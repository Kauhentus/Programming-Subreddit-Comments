No, there are benefits in "one file per program approach" * you can Ctrl-F symbols in a any editor, even in browser * your module names are not contrintuitive, because they are absent * it's easy to "maintain" files up to 2000 LOC
Misleading?!
&gt; Noep
Haha, not quite. As far as data flow goes, Perl chains applications better than Haskell composes functions, but neither is very good at a heavily compositional style. I just like both languages for different reasons. &lt;shrug&gt;
It is interesting to compare the type-system(s) in this thesis with [System FC](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/FC) (the core type system used to explain the advanced type-system features of GHC), but my understanding is that there no yet a consensus on some of the underlying questions, so this is probably moving territory right now. This work mostly uses *coherent* abstraction (on constraints that are known to be satisfiable), while it could seem natural to take type equalities as parameters that we're not sure ever hold. These coherence questions are also touched on by the recent work on [kind equalities for FC](http://www.cis.upenn.edu/~eir/papers/2013/fckinds/fckinds-extended.pdf) (which was [discussed on r/haskell before](http://www.reddit.com/r/haskell/comments/167c21/down_with_kinds_adding_dependent_heterogeneous/)), so we can expect interesting developments in any case.
&gt; prettyPrint q &gt; | length (snd q) == 1 = [] &gt; | otherwise = concat [ ip, " has visited these pages:\n" &gt; , concatMap pages (snd q) &gt; ] &gt; where &gt; pages x = concat [ " [" &gt; , show . snd $ x &gt; , pluralView $ snd x &gt; , fst x &gt; , "\n" &gt; ] &gt; ip = fst q &gt; pluralView x = if x == 1 &gt; then " View] " &gt; else " Views] " Ugh. All those `fst`s and `snd`s. This is what pattern matching is for. prettyPrint (ip, pages) | length pages == 1 = [] | otherwise = concat [ ip, " has visited these pages:\n" , concatMap prettyPage pages ] where prettyPage (page, views) = concat [ " [" , show views , pluralView views , page , "\n" ] pluralView 1 = " View] " pluralView _ = " Views] " Clearer, and shorter too. Could do with a type signature. `length pages == 1` is, as you note, potentially quite inefficient; I would remove the check from this function and instead ensure that ips I didn't want to print were filtered out before they were passed in. ---- &gt; prettyPut :: String -&gt; IO () &gt; prettyPut x &gt; | null x = return () &gt; | otherwise = putStrLn x I would use `unless` from Control.Monad. prettyPut :: String -&gt; IO () prettyPut x = unless (null x) (putStrLn x) If I was feeling particularly fancy/obfuscatory, I might even write prettyPut :: String -&gt; IO () prettyPut = unless &lt;$&gt; null &lt;*&gt; putStrLn I don't think this last version is generally accepted as good style though. ---- That's what jumped out at me.
Compaction is fine if you only use the structure ephemerally (which granted, would fit with the normal use case for a cache), but if you are using it persistently, it means that lookups become O(n) in the size of the log, because you can't amortize compaction over the production of the log.
Since modules is the only way to make abstract data types in Haskell, you are forced to break your code into modules.
A bad example, alas. It illustrates how you would solve the problem in Haskell, but not necessarily the best way to solve the code in the "real world" (which tends to get spooked by Control.Applicative and operators like '&lt;$&gt;' It is similar to the following example. Knuth once wrote an article on literate programming using wordcount as a case study. Doug McIlroy's classic review of that article is worth reading here (where both the original article and the review are present): [http://onesixtythree.com/literate/literate2.pdf](http://onesixtythree.com/literate/literate2.pdf). Money quote on p. 478 : &gt; I found Don Knuth’s program convincing as a demonstration of WEB and fascinating for its data structure, but I disagree with it on engineering grounds.
Strictly speaking, `3` is compiled to `fromInteger (3::Integer)` (as Porges said below), thus there is a conversion, and that makes it polymorphic.
Thanks for that! Do you mind if I use the changes that you recommend and add it to the code on my site/github?
What is wrong with demonstrating "trivial" things in Haskell? Haskell is a language for general programming, and I find there are not enough tutorials and examples about how to use Haskell for "trivial" things. Surely not every Haskell blog needs to be talking about category theory, advanced algorithms, and complex data types, right? 
I don't mind: you're welcome to take them.
That sounds reasonable. I guess you will also distribute it as a separate package?
I actually like `unless &lt;$&gt; null &lt;*&gt; putStrLn` because it reads quite naturally as "unless the value is null, print it." Of course, that's a matter of habit as any code style is, but if I got to decide, I would make it accepted!
Nothing is wrong at all with it. There can never be enough tutorials and examples. I think what /u/not_nought reacted to was the usage of the word "real world" which tends to imply more complex problems.
Yeah, most likely it will come as a cabal package as it needs a haskell program to communicate with the server, which simplifies the Elisp significantly (and the hypothetical Vimscript or Sublime code that someone may want to write). For newbies we'll provide a binary of this program too, and a sample Emacs archive so that even an Emacs newbie can get setup with a Haskell environment painlessly. 
Generally, someone who writes a whole application in a single file doesn't care enough about modularity to want abstract types.
I always wondered what an OS would look like if it would incorporate a VM-like behaviour, such as creating snapshots, reverting and branching them, etc. There wouldn't be such a great need to reinstall systems from time to time - you could just revert to the first snapshot. But I didn't have much time to think through this, so I'll just leave it here as a food for thought. (Also, I think it would be a good idea to treat programs as packages, but I guess I'm just stating the obvious here.)
I assumed "real world" here was in opposition to "ivory tower". This is something anyone could see the value in, unlike another zygohostomorphic Fibonacci implementation. 
Was I ambiguous? I assume you are referring to the sentence &gt; the type of 3 says that it can produce a value of any type a satisfying Num a, in particular it can also produce a value of any type a satisfying Floating a. I don't think the sentence is ambiguous, but I can understand how you could have difficulties deriving its implications. If I had written the similar statement &gt; if `foo` can produce a value of type `MyNum`, then it can also produce a value of type `MyFloating`. Then indeed, that statement would have implied that every `MyNum` is a `MyFloating`, because we don't know anything else about the value produced by `foo`. However, what I wrote had the extra keyword "any" in it: &gt; if `foo` can produce *any* value of type `MyNum`, then it can also produce *any* value of type `MyFloating`. This version does not imply that every `MyNum` is a `MyFloating`. If every `MyNum` were a `MyFloating` (but not vice versa), then in addition to the many values which would be both in `MyNum` and in `MyFloating`, there would also be values of type `MyFloating` which were not also of type `MyNum`, so `foo` would not be able to produce them. Note that I have switched to subtypes instead of type classes. I hope I have made things simpler, not more complicated!
[Here](https://github.com/meiersi/HaskellerZ/blob/master/meetups/20130829-FPAfternoon_Lambdachine/20130829-FPAfternoon_Lambdachine.pdf?raw=true) are the slides from Thomas's talk at ZuriHac a couple of months ago.
In c++ or delphi, I would be almost ok for a 2000 LOC file. But in haskell (or python, etc) ? IMHO, A 100 lines in haskell is equivalent to 500 lines in C++/Java (gross gross estimates). So 2k LOC is huge ! The benefits you cite are dwarfed by the advantages of easier maintenance, even for a simple one-man project. Just try to find a tool that cover your search need : vim , emacs, geany, whatever. And the point n°2 is backward, naming is a way to capture related functionality, if you have hard time naming something, there is likely something wrong with it. 
Ah, how the trolls like to hide behind opaque handles.
The question is whether readers will correctly deduce that `&lt;$&gt;` and `&lt;*&gt;` refer to instances of `(-&gt;) String`, or misread the code and assume they refer to instances of`IO`. You could explain in a comment, but all other things being equal, IMO it's better to rewrite the code to be more explicit. One day most people will use an IDE that displays the type as a subscript to the operator or something, and this won't be such an issue any more.
I think this is a neat little script. I don't get why anyone would complain that it's not "real world" enough. Granted it's not a complex piece of engineering, but it still solves a real problem.
My mistake. You are right I was referring to your [earlier example](http://www.latermuse.com/static/ron-simpleuniqueipsystemscript.html). From your other comments, it is clear that you are taking a tutorial approach to Haskell using simple examples, and that is a good goal. Seen that way, these two examples are fair game. I suppose I am reacting to my own path learning Haskell. I'm now reasonably good at it, but in my early days, I remember being annoyed at the amount of abstraction upon piled abstraction that I had to know before I could get even a trivial amount of stuff done. I still feel that way for many projects when contrasted with equivalent Go/Clojure/Ruby code. 
&gt;zygohostomorphic Fibonacci implementation. Zygohistomorphic, please.
[ICFP 2012 postgraduate student poster session. Thomas Schilling.](https://www.youtube.com/watch?v=eLj6JtA8jAA)
Notice how the commenter implied you can use just awk for pattern matching. Why would anyone be masochistic, like that?
Sometimes I read articles and think: well if I could understand all the words that you are saying then I probably wouldn't need to be reading this article :)
Paolo, it's just me and my ipad or the math glyphs doesn't render at all?
They don't render for me in Firefox 25 on linux either.
What's a good book to learn about these things? Not category theory in general, but functor and monad algebras in particular.
I also prefer this approach, but it can be a bit more cumbersome in some cases.
It works fine for me, linux, firefox 25?
Interesting point. Why is it you can't amortize log compaction? 
Mac Lane has a chapter on monadicity, IIRC, but a better and more comprehensive reference is probably Borceaux's Handbook of Categorical Algebra, volume 2. There is also a nice gentle introduction in Awodey's book.
it's mathjax. is noscript blocking it or something?
Makes me wonder if C would have been successful if it had required an advanced mathematics degree to understand it.
So, let's say that you create an empty cache and that you do enough lookups that on the next lookup a compaction will happen. Then, if you only do lookups on this version, every lookup will cause a compaction.
Least and greatest fixpoints happen to coincide in Haskell (CPO), but that is not the case in general. Pipes are completely iterative monads (F \inf X = nu Y. X + F Y), not free monads (F * X = mu Y. X + F Y).
I added a more informative error message for terminal sizes. The versions of deps are just what cabal gave me - I'll have to look at that a bit closer at some point. I thought x.x.y was just for bugfixes though.
hfov from hackage doesn't build on recent ghcs I think. I sent an email to the maintainer but it hasn't been updated yet.
If you would like to package a program together and not have to deal with "use this app through your browser" stuff, you could check out node-webkit to get a simple web front-end. 
Thanks!
Actually, pipes are better represented as mixed inductive-coinductive types: mu Y. nu Z. X + (A -&gt; Y) + B x Z. The idea is that there should always be a finite number of awaits between any two yields. This ensures that composition is always productive. 
Renders fine for me in Alien Blue on an iPad.
This something I been wondering about: there are four (co)free (co)monads: free monads, cofree comonads, free completely iterative monads and cofree recursive comonads. All these use one quantifier, does (co)free things occure at deeper nestings as well?
I don't know. I'm actually not very familiar with completely iterative monads at all. Do they arise via some monadic adjunction as well? Do you have any references?
There is a series of papers by Milius et al about them, but they are too hard to understand for me. More recently, and closer to functional programming, there is a paper called "Unifying Structured Recursion Schemes" by Hinze et al. In section 6.1, they mention free monads and cofree comonads and that they arise from the two adjunctions Free -| U and U -| Cofree respectively, they also mention that each of these adjuction also give rise to a comonad and monad respectively, but that these are "less interesting" and therefore ignored... I haven't checked, but I suspect these might be the cofree recursive comonad and the completely iterative monad.
He's got a proper job now (doing haskell!), so I think it's gotten moved to the back burner. I'll let him tell the details himself.
Thanks! Looks like a really interesting topic.
Why couldn't you keep the compacted version around for the next lookup? Any pure functional data structure would have to return a new version of the map on lookup. If you had a MRU list factor of 2*n, you would have to do a compaction every n lookups at most, depending on how many of the n keys are actually used. I'd be more concerned that every time I want to insert an element into the cache, I'd have to compact to know which element to remove. My cache will be around for the lifetime of the application and always be full after startup, meaning insertion requires deletion to keep the memory consumption constant. It's a really interesting design, though. I initially started with a Map + Sequence doing something similar to this, but couldn't really figure out all issues. Right now, I'm following the advice from edwardkmett / DanielWaterworth and am implementing a custom HAMT. It's really amazing how readable the source of Data.HashMap.Base is! I was perfectly able to understand this data structure from it. I think I can get away with a simpler implementation, though. I can cut some corners as memory usage and insert/delete speed is not that important.
There's nothing wrong with a single file, it's just that most Haskell implementations don't all multiple modules in a file.
This is a talk about FP Complete. Slides are available at: http://cufp.org/sites/all/files/slides/2013/lebovitz.pdf 
I'll look into it, but I can't say I'll find anything soon. I've been in a bit of a slump with Emacs since before I discovered CEDET. But if someone will remind me I'll try to get it set up and work on finding/writing guides to share over the holiday. I should have some more free time starting around the 20th of December.
Works for me now, must have been something intermittent. 
&gt; Want to write a small GUI thing but forgot to sacrifice to the giant rubber duck in the sky before trying to install wxHaskell or Gtk2Hs? I made many generous offerings to the giant rubber duck in the sky. Then, cabal failed to install gtk2hs and vomited all over my existing packages before screeching to a destructive halt. Woe. Maybe I should give threepenny a shot. How is the FRP interface coming along? Also, can I use a backend that isn't Snap?
&gt; cabal failed to install gtk2hs and vomited all over my existing packages before screeching to a destructive halt. I should disapprove of the mixed metaphor, but somehow it only enhances the vision of carnage.
Cool. I am really happy to see such attentive contributions. Bingo, **defaulting** is the keyword I am looking for. Great! In http://www.haskell.org/onlinereport/decls.html#sect4.3.4 it has been stated: "If no default declaration is given in a module then it assumed to be:", " default (Integer, Double)" Thank you! &gt; unifcation gives the constraint set I have to think about this. I guess there is a related important concept behind, which I should know.
[HalVM](http://halvm.org).
I agree that having to get the compact and traverse the MRU log to find the LRU key for eviction is suboptimal. We can improve this by also caching the LRU key (in a Maybe). If we happen to use the LRU key, we will invalidate this cache by replacing it with Nothing. When we need to evict an entry from the map, we only need to traverse the LRU log if the cached LRU key is Nothing. I expect this would significantly reduce the need to traverse the MRU log in the average case. We could go even further by caching multiple LRU keys (in a list). Then we only need to traverse the MRU log when all of the cached LRU keys have been invalidated. If number of cached keys is small and independent of the size of the map (e.g. 3 seems reasonable), there is still only a small constant time overhead to checking/invalidating them. 
I don't know if you're using chrome because sometimes chrome doesn't load youtube videos for me while firefox does.
The `webkit` package on Haskell works fine… Once you've successfully installed it, anyway. So maybe if node-webkit is easy to install and portable that would be a good way to distribute your apps as "real" apps.
Yes, Huffman trees are the right choice if you want to use trees. But nothing prevents you using from immutable arrays in Haskell, solving the indexing problem in a more traditional way. I wonder which is faster in practice? In an array approach with multiple repeated samples, one typically computes an array with the CDF in it and then does binary search on it. log_2(n) splits is suboptimal compared to Huffman, but array accesses are fast, and log_2(n) is so small that constant factors matter. Maybe fastest of all would be to store the tree in an array manually, ordered so that the most likely branch is on the left (aiming to maximize cache locality).
&gt;How is the FRP interface coming along? Nicely. For a FRP neophyte, getting to a working application was quite painless. You will find it familiar if you already tried reactive-banana.
 itemFromMongo [nameField, commentField, statusField, countField] = Item { itemName = fromText nameField , itemComment = fromText commentField , itemStatus = read (fromText status) , itemCount = case value countField of Int32 c -&gt; fromIntegral c _ -&gt; error "gee whiz type safety sure is great" } where fromText a = case value a of String a' -&gt; Text.unpack a' _ -&gt; error "broken db" formatting makes a world of difference
I meant that the surface of the water function would be unimodal and the smallest unimodal function covering the original, the same as convexification, but with unimodality as the target property. A unimodal function holds no more water, so when you fill it up you make it unimodal.
IIRC, selecting a random element with non-uniform distribution can be done in constant time with linear-time and space preprocessing. I'll try to find a link when I'm not a phone.
acid-state stores data in memory *and* persists it to disk, which I think explains pretty much everything about its standings in these benchmarks.
Here we go, [Vose's Alias Method](http://www.keithschwarz.com/darts-dice-coins/).
I see. I misunderstood what your point was. 
I'd point out that HDBC and HDBC-mysql are not particularly performant. I haven't benchmarked HDBC-mysql against HDBC-odbc and mysql-simple myself, but a couple (trivial) benchmarks between HDBC-postgresql and postgresql-simple showed HDBC was about 4x slower. And postgresql-simple isn't particularly fast either; postgresql-simple was about 20x slower than straight C and libpq. (although, one could argue the comparison between Haskell/postgresql-simple and C/libpq isn't exactly "fair", but it does place an upper bound on how much faster postgresql-simple could be.) I want to port the benchmark I do have to Haskell and postgresql-libpq and Python and psycopg2 to get a better feel for where things stand now, and write more benchmarks to get a more complete picture of performance, so I wouldn't take those numbers as anything too definitive. But honestly postgresql-simple is fast enough for me at the moment, and I have more pressing matters to attend to.
The instances you have right now could be factored Right now you have the notion that a Season has a temperature, but you currently require s and t to infer s' and t', but you can go t -&gt; s and t' -&gt; s' so you can go from temperature to season, so your s t -&gt; s' t' and s' t' -&gt; s t links are needlessly restricted. If I were determined to write these instances I'd just say `s -&gt; t s' t', t -&gt; s s' t', s' -&gt; s t t', t' -&gt; s s' t`, since given any one parameter you can actually know all 3 of the others.
YES! This is the beginning of what I've been hoping for for a while. Ideally, I want something like fish but implemented in Haskell. I'm definitely forking this. I cannot wait to put some work into the idea.
The lack of a SOAP library for Haskell could be a barrier to adoption in the enterprise.
You can comment out that type signature or switch it over to the suggested sig and it compiles and seems to run fine.
&gt; Sometimes I read articles and think: well if I could understand all the words that you are saying then I probably wouldn't need to be reading this article :) It gets better. I've managed to get to the point where I can understand all of the words, though not necessarily the sentences...
SOAP is one of the ill-conceived technologies where I hoped if one ignored it long enough, it would go away eventually... and luckily, most public web-services offered by the major players such as Google, Reddit, Facebook et al. are JSON based interfaces. So why can't the "enterprise" world follow suite?
Echo ignores whatever it gets on stdin and prints its argument on stdout. Yes is more like replicate from Data.List. The analogy is kind of weak, because I'm confusing arguments and pipelines, but whatever.
Cool, thanks for the pointers!
You're right that HDBC-mysql and HDBC-postgresql aren't fast. That said, they've received a lot less love than HDBC-odbc, which should outperform them both. A while ago I started writing [hdbc-performance](https://github.com/zenzike/hdbc-performance), which was intended to be a simple benchmarking tool to compare HDBC-odbc and HDBC-postgresql.
There is not much textual description on the core language, more like usage examples. Seems to have quite a few overloads per operation - couldn't find bind yet :)
I was thinking of the alias method, as I linked in a sibling comment. I was not aware of condensed table; it does not seem to be equivalent. In fact, the alias method looks superficially very similar to the square histogram method described in the same referenced paper.
Great GHC api example code as well. Thank you for releasing it.
I looked at the paper again when I got home and I think I was wrong. The "less interesting" comonad and monad will both be on the category F-Alg(C) -- which isn't what we are looking for (we want them to be on the category C). I think the two papers "Infinite trees and completely iterative theories: a coalgebraic view" and "From Corecursive Algebras to Corecursive Monads" might be more relevant... I don't understand either, but a quick glance suggests that they don't arise from adjunctions?!
Even so, you have an alarmingly large number of partial function calls. 
The directory package switched to using time instead of old-time some year ago. Try a newer GHC.
Thanks, I'll definitely have a closer look at that at some point. I can't say I've ever really tried setting up OBDC, so the guide is helpful.
Is it based on term rewriting?
&gt; Why do I get this ? ...because the author was too lazy to declare truthful build dependencies, thereby shifting the burden to its users to figure out how the heck to make it compile based on cryptic compiler messages... ;-)
Awesome! I think, though, to stand a chance at practicality, it would need to not require quotes around strings (and allow backslash escaping of spaces) in the common case (requiring quotes in deeply nested contexts might be okay). I look forward to seeing where this goes.
* HSH: https://github.com/jgoerzen/hsh/wiki * shellish: http://hackage.haskell.org/package/shellish 
I cannot resist to quote what they write about [functional programming](http://reference.wolfram.com/language/guide/FunctionalProgramming.html): &gt; Functional Programming &gt; Long viewed as an important theoretical idea, functional programming finally BECAME truly convenient and practical with the introduction of the Wolfram Language. (emphasis added by me) 
It seems on the newer GHC 7.6 that `old-time` is now gone and replaced with `time`, so ibotty kindly submitted [some CPP jiggery pokery](https://github.com/chrisdone/hell/commit/9d8a5a8e2137a4eb1bb8226110683f2a2badd2d2) to handle both 7.4 and 7.6+. :-)
Cool! Mind telling what libraries this use?
I'll read about alias method. Thanks for the pointers
This is working quite well, Latin jazz was maybe asking a bit too much !
I added [support for using shell libraries](https://github.com/chrisdone/hell#using-shell-libraries). That way, one can experiment a little bit before deciding to integrate one more tightly. (E.g. `shellish` has its own notion of the current directory which would require integration.)
Oh, true. Try an older GHC. ;-) Or, port hell to use `UTCTime` using my `time-compat` package to support old `directory`.
It's fixed now, someone submitted a pull request.
shellish hasn't been updated in years, you might try the modern fork: https://hackage.haskell.org/package/shelly
Hmm, using it on Take 5 was ... interesting :)
Thanks. I have another doubt. Is it possible, besides identity and associativity, to affirm the existence of inverses using monad algebras? (Maybe that's what the "Monadic functors" section says, but I didn't get that part.) And what if we wanted to define semigroups instead of monoids?
I don't think it really likes [Beethoven](http://chordify.net/chords/beethoven-symphony-7-allegretto-mvt-2-smalin), and even classic [demoscene music](http://chordify.net/chords/purple-motion-second-reality-mister-potatoman) is confusing, but it definitely likes [socialist songs](http://chordify.net/chords/bots-zeven-dagen-lang-vinylflavoured). But I have to say I'm really impressed how much it understands of [Hocus Pocus](http://chordify.net/chords/focus-hocus-pocus-pim-pellikaan).
&gt; I thought it was just pipe dreams being shared on #haskell &gt; pipe dreams I see what you did there...
What are the best kind of songs for it, overall? Tried with two songs and it didn't work so well. Maybe it's the rhythm? Chords changing too fast get lost in between?
With the following wrapper I'm able to get Shelly to function well: https://github.com/chrisdone/hell/blob/master/src/main/Shelly.hs chris@retina:~$ hell-shelly Welcome to Hell! chris:~/$ ls "." [FilePath "./.VirtualBox",FilePath "./.xchat2", [snip] chris:~/$ cd "Emacs/" chris:~/Emacs$ ls "." [FilePath "./.git",FilePath "./.gitignore",FilePath "./main.el", [snip] chris:~/Emacs$ One thing that would probably be handy is to be able to export environment variables similar to how `EXPORT` will persist things into the environment afterwards in bash. If there was a shelly command that would return what it considers to be the current directory and its environment then I could persist that without.
GHC is mentioned in 'The Haskell ecosystem'. Thanks anyway for pointing out. I'll consider moving it also in the Haskell projects section. 
Little noise, lots of signal, and songs with a lot of unison seem to work best. So get a song with a strong bassline/treble and with really simple harmonies. Look at the behaviour on [Pachelbel's (ever-annoying) Canon in D](http://chordify.net/chords/pachelbel-canon-in-d-original-instruments-walvis2007): the analysis works okay for a big part of the piece, but near the end, where the violins start to play more complicated variations (I guess?), the analysis can no longer focus on the basso continuo (cello, lute, harpsichord) and starts making odd guesses.
&gt; ...because the author was too lazy to declare truthful build dependencies, thereby shifting the burden to its users to figure out how the heck to make it compile based on cryptic compiler messages... ;-) I could've been much lazier and written or shared nothing at all in my limited free time. With the GHC API use, I didn't even expect people to be able to build it, which is why I linked to a Github project that explains what it's like to use. Don't be so snarky.
Great collection of resources, bookmarked. Thanks!
I hate you (in a jealous way)! You stole my name for the exact same idea. :)
Yes, and apparently Wolfram also invented symbolic programming, 55 years after McCarthy specified Lisp. From the [language overview](http://reference.wolfram.com/language/guide/LanguageOverview.html): &gt; uses its unique concept of symbolic programming to add a new level of flexibility to the very concept of programming 
I agree that when you need to explain in a comment, you're better of rewriting into a more sensible version. However, I think that as long as people avoid this specific use case, readers will be more likely to incorrectly deduce the correct instance. The more you do something, the better other people will become at reading it, and this is one of those small things where I'm prepared to sacrifice a little initial legibility for the greater good.
"define on the fly a String for every unbound word in the input" that's sounds like a great idea in my opinion. 
Minimal barriers to interoperability is an important element of freedom which github preserves, while certainly not being entirely Free-as-in-Freedom.
Thanks for the response. I am going to give both your implementation and ghci a shot for some simple shell scripts. For now I can't shake the feeling that it would be really nice for this to be a small extension to ghci so that everything else could be reused (including escaping a regular shell).
No.
The language is built on reducing function expressions like `f [x1,x2,x3,..]`. [EDIT: f is a function, x1 is an argument. Square brackets are for function evaluation, not lists.] The standard evaluation procedure (from the docs) is • Evaluate the head of the expression. • Evaluate each element in turn. • Apply transformations associated with the attributes Orderless, Listable, and Flat. • Apply any definitions that you have given. • Apply any built-in definitions. • Evaluate the result. Haskell is a one-lambda pony by comparision, but I think this has led to a deeper understanding of modularity, albeit without the nice graphics and lots of other walled garden costs and benefits. 
Generally, not just in Haskell, there is extra overhead in parallel solutions that may not be overwhelmed by the improvements parallelism offers. For this reason, as always when optimizing performance, it's utterly necessary to benchmark your code in reasonable settings to determine whether parallelism is actually producing a win. That said, there are a number of simple reasons why that overhead might be dominating. In particular, there are the simple questions that should be checked such as ensuring that you're compiling with the threaded runtime (`-threaded`) and that you tell it to use multiple processors (`myprog +RTS -N3 -RTS arg1 arg2` versus just `myprog arg1 arg2`). There is also always a tension between producing too many or too few "sparks"—potentially parallel computations. The scheduler continues to improve to handle more sparks with greater grace, but more sparks leads to more overhead and thus is at the core of the tension. Tools like threadscope can be used to analyze exactly how much computation is being distributed over your cores and to manage the overhead/parallelism tension with greater illumination. It's highly recommended that as you try to optimize your code for a parallel environment that you take a look at its performance frequently using that tool. Also, Simon Marlow recently published a book that goes through all of this is great detail—[Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929.). I highly recommend it!
I'm actually currently reading this very book. And I compiled the program like you mentioned. ghc -O2 xyz.hs -threaded -rtsopts -eventlog and running the program like ./xyz 40 +RTS -s -N2 -l And threadscope shows program using 2 processes. And 2 sparks are produced(as it should) one is converted and other is fizzled. 
Is this even new? Looks like some small changes to the same language that's been running in Mathematica for like 10 years.
The Alias method is a beautiful algorithm. 
&gt; And hell wants it the old way &gt;&gt; Try a newer GHC &gt; &gt; won't help And the name already begins to shine :)
You might want to have a look at plush, a recent Mark Lentczner project. It's a shell that leverages a browser based gui to support all kind of reimagined history and multiplexing features. To support it he implemented a posix compatible native Haskell shell. That required a deep dive into the posix standard and the source code of other shells. He even designed it using "POSIX" monad to allow IO to be swapped out to support pure tests. It might aid the integration of haskell syntax into "normal" shell operations. 
Awodey is in my opinion the best reference for the foundations. It's a very well written text.
If only one spark converts then only one bit of work was (potentially) useful. That could explain the slowdown—your parallel code is still effectively single-threaded and you're feeling the pain of the parallel RTS. I'm not completely sure why one spark would fizzle, though. That occurs when the spark tries to compute something that's been evaluated between spark creation and conversion. It may be that due to the recursive nature of fib, `fib (n + 2)` almost immediately ends up computing `fib n` which is already being evaluated by the first spark.
! Neat method. &gt; Second, and perhaps more surprisingly, this algorithm has been known for decades, but I had not once encountered it! Considering how much processing time is dedicated to simulation, I would have expected this technique to be better-known. My sentiments exactly. Thanks for the link.
He went all out with it. https://github.com/chrisdone/hell/blob/4a3cbba18511e27e1092ccef55c3f7e3c8844736/src/Hell.hs#L31
Yeah. “[brillant](http://thedailywtf.com/Articles/The_Brillant_Paula_Bean.aspx)”. Like [Gimp](http://www.gimp.org/). Or [Grub](http://www.gnu.org/software/grub/). Or other [genius](http://weknowmemes.com/wp-content/uploads/2012/08/genius-meme.jpg) names.
[GHC has the ability to overload strings.](http://www.haskell.org/ghc/docs/7.0.4/html/users_guide/type-class-extensions.html#overloaded-strings) And many languages have bare strings. I think there’s a way to do away with the quotes.
If I remember right, I think BeOS had a filesystem that stored mime types as part of the metadata for each file. In a sense, it's kind of strange to think about files having a "type" in the same way in-memory datastructures have a type. Basically, the definition of a file is that it's of type ByteString, and what we really mean when we say a file is of type "jpeg" is that it's a ByteString that can be successfully parsed into an image using a parser that conforms to the jpeg standard. I could see a typeclass-ish system for common file types as something that could be pretty useful. You might even be able to do something like type inference; if one program can write any kind of image on stdout, but it's being piped to a program that can only accept tiff images on stdin, you might want the first program to automatically select tiff as the output format without any intervention at all from the user or programmer, and if the output program can't produce a tiff for whatever reason, you could have the shell spit out a type error and fail.
&gt; leverages My potential λ-boner just shriveled up and died.
About a “billion” people had that idea. Especially since many of those people weren’t exactly quiet about it.
Any shell that wants to be taken seriously should support bash’s programmable completion.
Is that a grammar shrivel or a technology shrivel? :-)
see https://code.google.com/p/plush/
&gt;It may be that due to the recursive nature of fib, fib (n + 2) almost immediately ends up computing fib n which is already being evaluated by the first spark. I can't get why this should matter but it might have something to do with it. See, I changed the program wee bit, like this rfib :: Int -&gt; Int rfib 81 = 1 rfib 80 = 1 rfib x = rfib (x+1) + rfib (x+2) parallel :: Int -&gt; Int -&gt; (Int, Int) parallel x y = runEval $ do m &lt;- rpar (fib x) n &lt;- rpar (rfib y) rseq m rseq n return (m,n) sequential :: Int -&gt; Int -&gt; (Int, Int) sequential x y = (fib x, rfib y) Now, during parallel execution time is 3.42 total and 2.00 elapsed, whereas sequential one is 3.20, 3.14, although one spark is still fizzled.
&gt; Is it possible to affirm the existence of inverses using monad algebras? Yes. You need to consider the "free group" monad instead of the "free monoid" monad. You can manually check that everything goes through more or less the same as before. The endofunctor is now F X = 1 + X + X^2. One nullary operation and one binary operation as before, plus one unary operation for inverses. &gt; And what if we wanted to define semigroups instead of monoids? Same story, only the functor is now simply F X = X^2. The "free semigroup" monad is the functor that gives you non-empty lists. Unsurprisingly, it is possible to abstract over all these examples, by defining some notion of "algebraic theory" and then prove once and for all that those theories give rise to monadic adjunctions. I'm planning to explain this idea in more detail in future posts.
Using Inline pragma seems to work, now total time of parallel is as much as that of sequential which used to be double without inline pragma. Why is inline pragma helping here? 
HarmTrace being GPL, do you have the (relevant) Chordify sources accessible under GPL?
What parts of the site &amp; backends are written in Haskell? Only chord analysis? Or the whole site &amp; backends?
MongoPseudoDB + PHP + GHC ;] Great, really great. http://chordify.net/pages/about/ Technology behind Chordify We’ve made our best to make our website look simple and easy to use, even though there is some complex technology behind Chordify. Our website is built with state-of-the-art web development techniques, like HTML5 audio etc. Behind the scenes, we use the sonic annotator for extraction of audio features. These features consist of the downbeat positions and the tonal content of a piece of music. Next, a Haskell program HarmTrace then takes these features and computes the chords. For this to happen HarmTrace uses a model of Western tonal harmony to aid in the chord selection. At beat positions where the audio matches a particular chord well, this chord is used in final transcription. However, in case there is uncertainty about the sounding chords at a specific position in the song, the HarmTrace harmony model will select the correct chords based on the rules of tonal harmony. Chordify fosters open-source software. Not only do we use open-source software packages like GHC, PHP, SoX, sonic annotator, MongoDB, but we also give back a large share of the in-house developed technology to the music information retrieval research community via open-source software projects like HarmTrace and scientific publications. Chordify could not have been realised without the support of Utrecht University.
I'm not yet convinced that bog standard strings are so terrible. I'm trying it out.
Sorry! I don't hang around on IRC a lot so I had no idea.
Isn't LevelDB and variants the new hotness in this space? Would be curious about how it fits in this space....
I imagine the 5/4 time messed with it somewhat...
I'm not sure this project is worthy of the name. So the name might still be up for grabs.
It's a web service, no need to do that unless it's under the Affero GPL.
Fair, but it still involves me writing a block of manual conversions from the incoming list to an `Item`. My point there was that `acid-state` gives me that for free, and the conversion from HDBCs immediate format was a bit simpler than the same step for the Mongo data.
Nope. I may not have mentioned it, but I wanted an out-of-the-box config test of each of them. It might be interesting to do the fully tuned test after poking each storage option to its maximum, but I won't be doing that one (not least because I don't know enough about the internals or maintenance of each of the systems involved).
The cons operator (:) is used to add an element at the beginning of a list. For example: &gt; 42 : [1, 2, 3] [42, 1, 2, 3] You can use (:) multiple times to add more than one item: &gt; 127 : (0 : [0, 1]) [127, 0, 0, 1] That part should be fairly straight-forward. Now here's the fun thing about (:): you can use it to *split a list apart*. When you use it in a position where a pattern is expected (e.g. in the parameters of a function), Haskell will split the list for your. For example, you can write the `head` function (which returns the first element of a list) like this: head (x:rest) = x &gt; head [1,2,3] 1 Now, how does that work? Well remember just now, we used (:) to add an element at the front of a list, so let's rewrite our call to `head`: &gt; head (1 : [2, 3]) That's exactly the same, wouldn't you agree? Now look at the correspondance between this call and the definition of `head`; there's the name of the function, then an element, the colon, and the rest of the list. So in `head`, `x` will be bound to the part to the left of the colon, 1, and `rest` will be bound to the part to the right of the colon, [2,3]. Now, what about recursion? Well, a list in Haskell is basically just a bunch of application of the (:) operator. &gt; [1, 2, 3, 4] == 1 : 2 : 3 : 4 : [] True So a common recursion pattern for lists is to do something to the head (left part of the colon), and recursively call the function on the tail of the list (right part of the colon). Let's look at a function to count how many elements are in a list. Let's start with this incorrect definition: myLength (x:rest) = 1 + myLength rest How do you calculate the length of a list? Well you start by splitting it at the head, so you have in `x` one element and in `rest` all the other elements. So how long is the list? It's one more than the length of `rest`! That's the recursive part. Now, we'll get to a problem really soon. Let's look at a trace: myLength [1,2,3] == myLength (1 : 2 : 3 : []) == 1 + myLength (2 : 3 : []) == 1 + 1 + myLength (3 : []) == 1 + 1 + 1 + myLength [] == OOPS!! Problem! We defined `myLength` to look for the colon; but now with the empty list that pattern is not there! So we add an extra case to the function, called the base case: myLength [] = 0 myLength (x:rest) = 1 + myLength rest What is the length of an empty list? Zero, of course! So if we do the trace again: myLength [1,2,3] == myLength (1 : 2 : 3 : []) == 1 + myLength (2 : 3 : []) == 1 + 1 + myLength (3 : []) == 1 + 1 + 1 + myLength [] == 1 + 1 + 1 + 0 == 3 And there we are. A basic recipe for recursive function is: 1. Figure out what's the base case (aka the case where you don't have to do any work) and write it down. For lists, the empty list is almost always the base case. 2. Once you have your base case, try to figure out what you'd do if you had elements in the list. It usually involves doing something with the head of the list and calling the function with the rest of the list. Recursion can seem daunting at first, and sometimes the syntax can be bit problematic. Keep practicing and you'll eventually get it. It certainly took me a while, but now I don't even blink when writing recursive functions; I got enough practice that I have a good mental model of the whole thing.
Hey DaOrkyMan, First, I think you have to understand that the cons operator is for working with lists, so for example, 'a' : 'b' : [] would translate to "ab" or ['a','b'] These would all be equivalent. Remember that the cons operator adds something to the begining of the list. So now for your actual question. Recursion! Recursion is essential for repetition, so for example looking at calculating the lenght of a list you might want to iterate through the contents of list one by one untill all items were counted so you might have something like: length :: [a] -&gt; Int length [] = 0 -- base Case! Empty List length (firstItem:restOflist) = 1 + (length restOfList) Now the role of the cons operator here is to pattern match, which is a huge part of haskell functions, so knowing that the parameter is a list lets us pattern match and bind different parts of the list being passed in to names, (eg firstItem and restOfList); If the list fed in is empty, return zero. Else, take the list and split it into the fashion above, firstItem and remaining items. The big things here are to establish your basecase, eg, when the list is empty in this length example, and the pattern matching to breakdown the parameters. There are alternate ways to do this, such as length [] = 0 length list = 1 + (length (tail list)) You may also want to take a look at: http://learnyouahaskell.com/chapters http://learnyouahaskell.com/syntax-in-functions#pattern-matching -- Pattern Matching http://learnyouahaskell.com/recursion#hello-recursion -- Thinking Recursively. 
I think you meant: `'a' : 'b' : []`
I've said a couple of times here on /r/haskell that I consider the list's special syntax sugars to be a mistake; this is the sort of thing I'm talking about. It looks too much like magic at _exactly_ the wrong time in a person's Haskell education. DaOrkyMan, I'm going to assume, perhaps incorrectly, that you've seen the usual syntax for pattern matching a value and thereby deconstructing it: data Thing = Thing Int String thing = Thing 88 "Hello" showThing :: Thing -&gt; String showThing (Thing i s) = show i ++ ", " ++ s By pattern matching, we bind `i` and `s` to the Int and String elements of the Thing. Lists get special magic that makes them hard to understand at first: headZero :: [Int] -&gt; Int headZero (x:xs) = x headZero [] = 0 But it turns out that we can make this look more normal by writing it in the following manner, which is also perfectly valid Haskell: headZero2 :: [] Int -&gt; Int headZero2 ((:) x xs) = x headZero2 [] = 0 This makes it more clear, I think: You have a "List of x" (spelled "[] x") from which you will extract an x. A List is either a "cons cell" with an element and a list, or "nil", the empty list. The "cons cell" has the constructor called :, but despite the colon it's just a constructor like Thing. Lists are: data List a = Cons a (List a) | Nil Only Haskell has special magic syntax that I believe applies to nothing else that turns `List a` into `[a]` and `Nil` into `[]` (which I think would be otherwise illegal even if it wasn't already claimed). Cons is :, but it is parsed as an infix operator. It is, however, a constructor just like `Cons` in the above. Parenthesizing the operator like that allows you to use it as a normal function; note this is applicable to any operator: Prelude&gt; (+) 1 2 3 Prelude&gt; (+ 1) 2 3 With that desugaring in mind, Haskell lists should cease being so magical. I'm going to guess that once you see the spelled out List definition as above and the decoder ring that your problem is resolved. In table form: type [a] type List a constructor Cons infix constructor : constructor Nil [] (You can make infix constructors yourself; unlike the other bits of sugar you can use that bit yourself.) Oh, and `Cons`, by the way, is [an _ancient_ name](http://en.wikipedia.org/wiki/Cons), along with [`car` and `cdr`](http://en.wikipedia.org/wiki/CAR_and_CDR) (which you, mercifully, won't encounter in Haskell). I don't like these names, because they are accidents of history, based on assembly instructions and macros from the IBM 704. We should rather give them good names today, when it's no longer an imposition to type a real word instead. (Note I'm _not_ objecting to the use of `:`; it is sufficiently common to justify a short name. I'm saying I don't really like calling them "cons cells", which is a very jargony name for a basic concept, and puts an unnecessary barrier up for learners.)
You're welcome!
Aptly named. Haha just joking, nice job! Wonder how far it will go, as compared to zsh and bash.
what were the results? 
There is an argument to be made that the base case of sum is a singleton list.
( r a -&gt; r b) -&gt; r (a -&gt; b) Is that actually inhabited by anything useful in the r != Identity case?
and that argument is wrong. Since lists can be empty that function is not total. Here you need a monoid.
Why is it assumed that the sum function must be total?
that is what makes it a "function." Non total "functions" are destructive. If something claims the type `A -&gt; B` it should be defined at all `A`, otherwise it is lying.
I completely. However that does not change the fact that partial functions exist and are fairly commonplace. And since partial functions have their uses, even if you don't like them, then the argument remains. Our distaste for the possibility of failure is shared, but it is not enough to dispel the alternative definition from my mind.
The author probably shouldn't refer to the Haskell language as he. Also, I'd usually say "a Haskell compiler" not "an Haskell compiler". Content looks awesome though! 
A nontotal sum is pathological. It's not the possibility for failure that makes such a definition of sum wrong, it's that it is tantamount to a mathematical lie.
Why would you use a partial sum function when you could use a total sum function instead?
Have you considered accepting bitcoin as well as paypal?
Inversely, companies outside the US probably don't want to trust sensitive data to a US-based cloud.
It seems that `moeb traverse` is `loeb` but with side effects!
True, but lame.
An excellent answer! I guess that extending a Haskell one-process environment model to a whole OS might be a prolific approach (the same way as extending a single-task OS combined with powerful IPC primitives and the common abstraction, POSIX, gives us Unix). Still, it's not clear how to treat networking, that's the OS-wise analogue for IO monad. A sent network packet cannot be undone or made persistent. 
We call HarmTrace as an executable; we don't dynamically link to it.
You can make any partial function into a total function by choosing values for the inputs that are missing. That doesn't mean you should though.
HarmTrace seems to be the only Haskell component. Everything else is apparently written in PHP or C++.
&gt; I don't want to reimplement the entire library in Haskell. &gt; Hi, can you rewrite the entire SFML library in C and then optionally make a C++ interface to it please ? ಠ_ಠ
Looks ok on my iPhone (Safari)
This indeed was an attempt to troll.
How have I never heard of this? It's *much* farther along than any othe Haskell shell project I've come across. The web interface really has a lot of potential, I can see it being very very cool.
That's not true in general (e.g. `head`). What's wrong with `sum` being a total function? Why is `sum [] = 0` wrong?
English is not his first language. “He” in this context means “it”. Of course, Haskell Curry *was* a he.
Recursion in Haskell works the same way as in other languages (ignoring compiler optimizations). For example consider the recursive definition of factorial: f(0)=1 f(x)=x*f(x-1) In Haskell we would write: f 0 = 1 f x = x*(f (x-1)) We also have recursive data-types, such as the list. All a recursive data-type is is a datatype that references itself. For example, consider a linked list. Each node in a linked list contains a value, as well as a reference to another node in the linked list. We can express this in Haskell as: data List a = Cons a (List a) This data definition says that a list of type 'a' contains 1 value of type 'a', and another list of type 'a'. This definition only allows for infinite lists, so we also define another constructor for empty lists: data List a = Nil | Cons a (List a) To write a function for lists, we can use pattern matching. For example: head (Cons x y) = x tail (Cons x y) = y head returns the first element, while tail returns the same list but without the first element. Notice that neither of these functions are defined for the empty list (Nil). Using this, we can try define a function to take the sum of a list of numbers. sum (Cons x xs) = x + (sum xs) sum Nil = 0 as you can see, sum is defined as the first element, plus the sum of all the other elements. We also define the sum of Nil to be 0. This is how Haskell defines lists, expect that it uses special syntax, so that ":"=Cons, []=Nil. You can also construct a list by doing `a:b:c:[]`. This is because ':' is defined as an infix operator that evaluates its right side first. To clarify, we can add parantheses: a:(b:(c:[]))
`pwd` gives the working directory. I just released a new version with a `get_env_all function` for giving the environment. Note that with shelly you should now be able to pipe into a shell commands in hell. `setStdin it` could be useful.
You make a good point about head. I see how my statement was wrong. However, there are SOME functions that are partial, and could be made total, but aren't, like `tail`. So the question is, if sum doesn't have to be total, why should it be? In my mind, intuitively, you cannot sum numbers that do not exist. So when given an empty list, returning any number is a little bit of a lie. Of course, returning 0 makes a lot of sense because it is the identity but I'm not convinced that the partial version is less correct than the total version. Why do you think `sum [] = 0` is correct? The only reason I can see is that 0 is the identity.
Awesome! Thanks.
You don't have to be that brave! Check out elm: http://elm-lang.org mario example: http://elm-lang.org/edit/examples/Intermediate/Mario.elm They have some good material to introduce you to FRP.
Okay, well here is an answer that relies on [loeb](https://github.com/quchen/articles/blob/master/loeb-moeb.md) for self-referentiality. ~~Maybe someone smarter than me can tell me if the actually memoizes or not.~~ I threw it some trace statements and it seems like f was only called at most once for each floor, so this is memoized. import Data.Array import Data.Ix import Data.Maybe solve :: Integer -&gt; Integer -&gt; Integer -&gt; [Integer] solve n i j = i : solution n j ! i solution :: Integer -&gt; Integer -&gt; Array Integer [Integer] solution n end | end &lt; 1 || end &gt; n = error "No such floor" | otherwise = loeb $ go where go :: Array Integer (Array Integer [Integer] -&gt; [Integer]) go = listArray (1, n) $ map f [1..n] f :: Integer -&gt; Array Integer [Integer] -&gt; [Integer] f x arr | x == end = [] | otherwise = let upstep = x + 2 downstep = x - 3 uppath = arr !? upstep downpath = arr !? downstep goup = case (uppath, downpath) of (Nothing, Nothing) -&gt; error "Elevator unable to move" (Nothing, _) -&gt; False (_, Nothing) -&gt; True (Just a, Just b) -&gt; isShorter a b nextstep = if goup then upstep else downstep shorterpath = if goup then fromJust uppath else fromJust downpath in nextstep : shorterpath (!?) :: Ix i =&gt; Array i a -&gt; i -&gt; Maybe a arr !? i = case bounds arr `inRange` i of True -&gt; Just (arr ! i) False -&gt; Nothing isShorter :: [a] -&gt; [a] -&gt; Bool isShorter _ [] = False isShorter [] _ = True isShorter (_:xs) (_:ys) = isShorter xs ys loeb :: Functor f =&gt; f (f a -&gt; a) -&gt; f a loeb x = go where go = fmap ($ go) x `solution` builds an array of size n of lists of integers. At index i is the stops you make on the elevator. So if there are 10 floors and you are trying to get to floor 5, you have solution 10 5 = array (1,10) [(1,[3,5]),(2,[4,1,3,5]),(3,[5]),(4,[1,3,5]),(5,[]),(6,[3,5]),(7,[4,1,3,5]),(8,[5]),(9,[6,3,5]),(10,[7,4,1,3,5])] `solve` just indexes this array. `solution` is generated like a spreadsheet using `loeb` (I just found out about loeb a couple days ago and its pretty cool). This is brings in the self-referentiality and memoization. I had to write an `isShorter` method because you need something that can tell you that a finite list is shorter than an infinite list. Comparing length obviously won't work. This is where the lazy evaluation comes in. I used array just for speed but list should work too. Reminder: This returns the steps, not the number of steps. However, that is easily changed =P
Btw, I am a Haskell noob: does (!?) already exists somewhere for arrays?
I'm trying to get the game to compile, but I'm stuck. I have c2hs-0.16.5 installed into the sandbox, and am trying to install ncurses-0.2.7, but I keep getting the following error: &gt; cabal: The program c2hs version &gt;=0.15 is required but it could not be found. Could you possibly document the versions that you're building against? Thanks.
Not sure if this would apply but I've always found the nexus idea in [Trouble Shared is Trouble Halved](https://www.cs.ox.ac.uk/people/ralf.hinze/publications/HW03.pdf) to be a very interesting example for how to do dynamic programming in fp, though it does seem like it's not easy to generalize. 
Using chaosmasttters minimal function you can do this: import Data.Array data Natural = Next Natural | Zero toNum :: Num n =&gt; Natural -&gt; n toNum Zero = 0 toNum (Next n) = 1 + (toNum n) minimal :: Natural -&gt; Natural -&gt; Natural minimal Zero _ = Zero minimal _ Zero = Zero minimal (Next a) (Next b) = Next $ minimal a b infinity = Next infinity dp :: Int -&gt; Int -&gt; Int -&gt; Int dp n = let arr :: Array (Int, Int) Natural arr = listArray ((-2,1), (n+2,n)) [ f i j | i &lt;- [-2..n+2] , j &lt;- [1..n] ] f :: Int -&gt; Int -&gt; Natural f i j | i == j = Zero | 1 &lt;= i &amp;&amp; i &lt;= n = Next $ minimal (arr ! (l,j)) (arr ! (r,j)) | otherwise = infinity where l = i + 2 r = i - 3 in \i j -&gt; toNum (arr ! (i, j)) solve100 :: Int -&gt; Int -&gt; Int solve100 = dp 100 This version is indeed memoizing and also quite fast! Evaluating for instance *solve100 1 100* gives the result 52. I have no idea if this is correct though (I did not check if the given recurrence relation was correct). edit: The only difference with chaosmasttter solution is that instead of calling *f* directly in the recursive calls of *f*, the array *arr* is indexed instead. This array memoizes the results of calls of *f* and this function will be called at most one time for each pair of integers. Also I'm checking that *i* is within the range *[1, n]* since a recursive call might request the answer of, for instance, (-2, 3). There is no need to do the same check with *j* given that the function was called with a sane initial value for *j*. edit2: You can probably add *Infinity* as a constructor of the *Natural* type (or use *Maybe Natural* instead?) and return this instead of using the *inifinity* value. Then if this constructor is matched in *minimal* then the other number can be returned instantly instead of traversing all *Next* constructors first.
&gt; In this pearl we discuss an intriguing application of nexuses; we show that they serve admirably as memo structures featuring constant time access to memoized function calls. I think this is what I'm looking for, thanks! I'm reading it now. Edit: This is probably the right direction. The tree method for fibonacci might work although I agree that it's not easy to generalize. Tomorrow I'll try and model all the floors as binary trees, with the left node being the lower floor we came from (n-2) and the right node being the upper floor we came from (n+3) and the root node being the target floor. I think using nexus sharing as described in the pearl you linked would be the solution that I was looking for. The difficult part, which I'll leave to tomorrow, is how to have nexus sharing with only one of the children, since the other has to be computed. I currently have some lines like this but I don't know what to pass the `lowerFloor` and `upperFloor` definitions where I have the `_`: type Floor = Tree (Int,Nat) elevators :: Floor elevators = thisFloor where thisFloor = elevator (7,Zero) lowerFloor upperFloor -- these two lines are a problem lowerFloor = elevator (7-2,Succ Zero) _ thisFloor upperFloor = elevator (7+3,Succ Zero) thisFloor _ elevator :: (Int,Nat) -&gt; Floor -&gt; Floor -&gt; Floor Thanks for the pointer, this is finally starting to feel right.
I see. I still wonder what it is that prevents the State monad version of my example to be compiled to equally efficient code. Unfortunately, I don't know all that much about the internals of GHC, but shouldn't it detect that the state is updated and the previous version never accessed, performing the modifications in place? In other words: how exactly does the low level memory look like in the State monad and my HashMap state on an update?
Nothing prevents a state action from running up to a given point, then taking the current state, saving it, trying something then going back to the previous state. That is a lot harder when you have to save the entire world, and don't know what all the references you have floating around are in ST! ST is solving an easier problem in many ways and so can do so more efficiently.
Thank you. So would you happen to know of a straightforward way of making my particular example faster without using the ST monad? On a related note: I am sure you have been asked this before, but I cannot find it: could you please add modify', the strict version of modify, to the State monad, or would this break all kinds of things?
You'll get slightly better performance with a CPS'd state monad. newtype State s a = State { runState :: forall r. (a -&gt; s -&gt; r) -&gt; s -&gt; r } instance Monad (State s) where return a = State $ \ k s -&gt; k a s State m &gt;&gt;= f = State $ \ k s -&gt; m (\a s' -&gt; runState (f a) k s') s evalState (State m) s = m const s modify' f = State $ \k s -&gt; k () $! f s gets f = State $ \ k s -&gt; k (f s) s That should probably knock off about a third to half of the speed loss for using State, getting you closer to the ST performance. Re: `modify'`, that request would have to run through Ross Paterson as he still controls `transformers`. I merely expose the functionality he offers through classes in the `mtl` these days. I personally have no objection to adding it. You can probably write a default definition for it slightly more efficiently in terms of the `state` function though.
Great, thanks, I will try it out. Could the original State monad be replaced by the CPS-State so that all can profit? I also can't find a package for it on Hackage. I just emailed Ross about modify'.
This sounds like something that /u/Tekmo would be interested in, since he's thinking about making a new transformers package with more optimal implementations.
That was mainly to fix `WriterT`'s performance. I've been pretty happy with `StateT`, but I don't use it for really high-performance applications.
For safe versions of the prelude functions and more, you can use the [Safe package](http://hackage.haskell.org/package/Safe-0.1/docs/Safe.html).
It is also inhabited in Const. And in some things which are not quite Monads (like a well-type byte code). It is the signature for 'lam' in "finally tagless".
I'd also very much like to have a modify' in the standard library. If there are faster, well-known implementations for State/Writer, why are those not used in the transformers package? I find understanding the overhead of monads and monad transformers to be a rather difficult topic, any baseline improvements that would just make these standard building blocks faster is very helpful.
I usually use vectors to structure high-performance computations using their purely functional interface. If that doesn't work then I use ST as a last resort.
That's a neat package! Unfortunately, I can't find anything for arrays in there.
Oh I thought you were using []. Didn't pay too much attention. Either ways, it's no shame to define these small functions for yourself. I recognize the feeling though, when I'm working with Haskell everything is so neat and if a function is missing you feel like you've overlooked something. When I'm working in Java or Python I'll hack before looking for an answer more often than not.
"native haskell solution is imperative." Pun intended?
The CPS'd state isn't always a win. It is here for much the same reason you can use `ST s`.
I don't follow. You can use ST s to implement algorithms that rely on true mutable state, which you can't do with CPS'd state, right? When is CPS'd state not a win?
Oh my god. :&lt; Stop saying this. Just stop. Note that *this is not* a rage at you, /u/vincentrevelations, I'm just mad every time I see misquotation of / quoting Knuth out of context and implying that he said optimization is just evil… **He did not**. Please, see [original paper, page 8](http://cs.sjsu.edu/~mak/CS185C/KnuthStructuredProgrammingGoTo.pdf) for reference. &gt; The conventional wisdom shared by many of today's software engineers calls for ignoring efficiency in the small; but I believe this is simply an overreaction to the abuses they see being practiced by penny-wise-and-pound-foolish programmers, who can't debug or maintain their "optimized" programs. In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal; and I believe the same viewpoint should prevail in software engineering. (…) &gt; There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.
Andrea Di Persio. Italian. Closest to neutral pronoun is masculine “il”, and (I live in Italy) they say “Askell”, because ‘h’ is silent in Italian. You are now reading his blog post in an Italian accent.
Hehe, but this *is* about debugging and maintenance, in de facto standard libraries, nonetheless. Lots of people are still working with this code, so it needs to be understandable, reliable, and recognizable. Come to think of it, I actually wouldn't mind a fixed high-performance/ugly-code Transformers fork on Hackage. But I'll fix the quote. I didn't even really know it was a Knuth quote, for shame.
I understand that Haskell has a long history of not prematurely settling for a suboptimal solution, but I wouldn't consider Reader/Writer/State as 'novelty high abstraction packages'. They are widely used idioms, part of the Haskell Platform and described in every introductory book.
Is this the kind of solution you wanted? You should be able to replace Vector with any "indexable" functor (i.e List, Map, Array) The only requirement I can see is that the evaluation of min is lazy, so I just copied chaosmasttter and implemented natural numbers à la Peano. import Data.Vector (fromList, (!)) import qualified Data.Vector as V import Data.Function (fix) data Nat = Zero | Suc Nat deriving Show minNat (Suc x) (Suc y) = Suc (minNat x y) minNat _ _ = Zero toInt :: Nat -&gt; Int toInt Zero = 0 toInt (Suc n) = succ (toInt n) -- called as "elevator start dest floors" elevator j i n = toInt $ ((\x -&gt; fix (\r -&gt; fmap ($ r) x)) $ (fmap (transFunction n) $ fromList [0..n]) V.// [(i, const Zero)]) ! j transFunction n x v = Suc $ minNat (if x + 2 &lt;= n then (v ! (x + 2)) else (fix Suc)) -- If out of range, infinite steps (if x - 3 &gt;= 0 then (v ! (x - 3)) else (fix Suc))
That's true, but they're quite new. It's just a concept that worked from the first go, much like STM. As you can see, though, there's still a lot to improve.
I actually agree that it's likely worth it in this instance - I think a good case can be made, and you've got the seeds of it here. I don't think I agree with your assertion here in the general case, though - it depends on whether the speed gain in the library is likely to mean an appreciable speed gain any critical path in a client application. Even then, ease of maintenance should be considered.
I agree. `ST` plus a single `STRef` is isomorphic to the `State` monad. To me, the `State` monad feels more natural and easier to understand, because it is built out of pure operations. I only use ST when I need to optimize; e.g., if I need an `STArray`.
When it is more obfuscated, so always. Well, at least the underlying library code is more obfuscated, which is still not a win. Unless you need to optimize and the CPS version helps you, in which case it's a win.
I understand, thanks!
You must be great at parties.
It should be balanced against understandably however, which will effect those same thousands of people.
I can't say that I'm a huge fan of Perl these days, but it was the first language I learned and also the first language I used in a professional capacity. But still, I think there is a strong similarity between Perl and Haskell. In particular, the spirit of TIMTOWTDI is strong in both. Both Perlers and Haskellers understand that whether you implement a computational pattern using objects or functors or anonymous functions, etc, it's still the same pattern with different notation. This seems to lend itself to flexibility in notation, and ultimately the proliferation of "line noise" operators I tend to just ignore, given that it is just notation that plumbs the pattern together. I'm not sure if this is a language issue so much as a community one -- but it does seem to me that many users of other languages don't get it. You can think about your programs at a more abstract level if you abstract away from the specifics of an implementation of plumbing. That said, I find that Haskell's types make refactoring from one implementation to another easier than Perl, since it's harder to miss a case/detail.
I'd respect this move a lot more if it allowed push to public repos on github (and ideally gitorious). As is, cool, I guess, but mostly meh... Personally, I get too much work done when I'm without internet for reliance on an online IDE to be a good idea.
You could grab [lens](http://hackage.haskell.org/package/lens) and use the [ix](http://hackage.haskell.org/package/lens-3.10/docs/Control-Lens-At.html#v:ix) traversal. Something like `arr^?(ix upstep)`.
Thanks! I totally forgot about lens!
GHC abstracts hardware threads. In this case, the number of hardware threads will be the same as the number of cores. Also, especially when there's no IO, unnecessary context switches in the abstract threads are avoided. This can still be worse than a single threaded implementation, of course.
The CPSed state monad is a mixed blessing. I had a program where profiling indicated that a lot of time was spent in the state monad. I switched to the CPSed state monad, and profiling showed that it was 30% faster. Great! But when I turned off profiling the CPSed code was 10% slower than non-CPS. Moral: never trust intrusive profiling. 
It might be, it's a little dense in the elevator function. It only returns the result for one floor but it think it's the answer I'm looking for if it only calculates each floor once (i.e. it memoizes). It looks like the elevator function might be able to be modified to return the Vector of floors instead of the single floor we're looking at but again it's hard to tell.
Ok, I think I understand this. I'm pretty sure this is what I was looking for, in particular: data Floor = Floor { name :: Int, goUp :: Floor, goDown :: Floor } deriving Show allFloors :: [Floor] allFloors = map create [0..] where create i = Floor i (allFloors !! (i + 2)) (allFloors !! (i - 3)) although you go on to use a more sophisticated representation the core of it is the same. `allFloorsZ'` creates an infinite list of floors from zero up where the value of the floor is 0 if it's index 7 or `Succ $ natMin (value' $ goUp' floor) (value' $ goDown' floor)` otherwise. It also uses nexus sharing to make sure that floors are only calculated once (i.e. for memoization). It's unfortunate that the logic for the size of the step in each direction (2,3) is separate from the logic of the cost (`minFloor`).
I definitely agree that it is temperamental. Here I benchmarked with `time`, so not very intrusive, but it was definitely just taking his microbenchmark. Note: if you actually eta-reduce the code above, return a = State $ \ k -&gt; k a State m &gt;&gt;= f = State $ \ k -&gt; m (\a -&gt; runState (f a) k) it slows down rather than speeds up the code. =/
I believe that this is just the Mathematica "language" that has been finally given a name ... http://en.wikipedia.org/wiki/Wolfram_language
That's really terrible. Is there any hope of reducing the fragility of inlining (which I assume this is a symptom of)?
I know this is not what you asked for at all but this has been bugging me the whole night so here goes: this problem can be solved in constant space and time. Here is the optimal function in haskell: opt :: Int -&gt; Int -&gt; Int opt i j | i == j = 0 | i &lt; j = case (j - i) `divMod` 2 of (n, 0) -&gt; n (n, 1) -&gt; n + 3 | i &gt; j = case (i - j) `divMod` 3 of (n, 0) -&gt; n (n, 1) -&gt; n + 2 (n, 2) -&gt; n + 4 Note that this only works if the number of floors is greater than 4 (some problems can't be solved if there are 4 or less floors).
&gt; `ST` plus a single `STRef` is isomorphic to the `State` monad. Which suggests the exercise of implementing `State` in terms of `ST`: {-# LANGUAGE RankNTypes, ScopedTypeVariables, DeriveFunctor #-} import Control.Applicative import Control.Monad import Control.Monad.ST import Control.Monad.Reader import Data.STRef newtype State s a = State { unState :: forall i. ReaderT (STRef i s) (ST i) a } -- GeneralizedNewtypeDeriving doesn't work here because of -- the higher-rank type. deriving Functor instance Applicative (State s) where pure a = State (pure a) State ff &lt;*&gt; State fa = State (ff &lt;*&gt; fa) instance Monad (State s) where return = pure State ma &gt;&gt;= f = State (ma &gt;&gt;= unState . f) runState :: forall s a. State s a -&gt; s -&gt; (a, s) runState (State ra) s = runST go where go :: forall i. ST i (a, s) go = do ref &lt;- newSTRef s a &lt;- runReaderT ra ref s' &lt;- readSTRef ref return (a, s') get :: forall s. State s s get = State go where go :: forall i. ReaderT (STRef i s) (ST i) s go = do ref &lt;- ask lift $ readSTRef ref modify :: forall s. (s -&gt; s) -&gt; State s () modify f = State go where go :: forall i. ReaderT (STRef i s) (ST i) () go = do ref &lt;- ask lift $ modifySTRef ref f return () I looked into implementing `StateT` in terms of `ST`. At first I thought it'd be impossible for the same reasons there's no `STT` transformer, but it looks like that's no obstacle because you can duplicate the `STRef`. Now the question is whether there is some trick to avoid duplicating the `STRef` except when it's really necessary—otherwise you aren't doing any better than plain old `StateT`...
The fact that zero is the (additive) identity should be sufficient. Of course, things like this are always an argument about what you think the most sensible definition is, but it seems like returning the identity is sensible behavior in the `[]` case. It becomes a bother to reason about the sum function otherwise, like: `sum xs + sum ys == sum (xs++ys)` - *unless* `xs` or `ys` is `[]`, then it's bottom.
The sum function must be really total, and give zero for the empty list for all the mathematics to work properly. Same way the empty product must be 1.
Now you've made me wonder how Argentines pronounce 'Haskell'...
Well, the explicit nexus isn't required at all: allFloors :: Int -&gt; [Int] allFloors lvl = map toInt cache where cache = map go [0..] go i | i == lvl = Zero | otherwise = Succ $ natMin (cache !! (i + 2)) (cache !! (i - 3)) 
If we want a version that returns the whole vector elevator i n = fmap toInt $ ((\x -&gt; fix (\r -&gt; fmap ($ r) x)) $ (fmap (transFunction n) $ fromList [0..n]) V.// [(i, const Zero)]) It is memoized; what (\x -&gt; fix (\r -&gt; fmap ($ r) x) does is take some (Functor f) =&gt; f (f b -&gt; b) -&gt; f b, i.e it make a functor from a self referential definition of a functor. (I think the common name for this is löb). So what we do is build a vector of functions of how to calculate it (hence why I map my transFunction over the vector). We then replace element i with a function which throws away its argument and returns 0. (i.e the base case for our recursive function) After running löb, we now have a vector of unevaluated thunks. Upon attempting to read an element, the thunks will be evaluated in a breadth first pattern (because of the definition of minNat) starting from the element you tried to read. Once a thunk has been fully evaluated, it stays evaluated (and of course, a partially evaluated thunk doesn't lose its progress either), giving us something we might loosely call "memoization"
It does not make sense to do that. Read the repa chapter in Simon Marlow's "Parallel and Concurrent Programming in Haskell". http://chimera.labs.oreilly.com/books/1230000000929/ch05.html 
Seems nifty, but [0 .. Vector.length v] ? Really?
Here at least a gross simplification of what is going on applies. GHC won't inline a function that isn't fully applied to all of its arguments, so the eta-expansion causes more things to be elegible to inline. 
How I enumerate a vector was not really meant to be the interesting part of the blog post :) Earlier I use `zipWith` and I would really do so here in practice, but this is less characters and fits better in a blog post.
Oh, ha. Indeed :)
That's an impressive amount of work, but... probably the entirely wrong direction to be taking. Your problem is probably right here: data Action = Action { actionType :: Text , actionActor :: Text } Right there at the first "Text". It is unlikely that your actions are truly that unconstrained. You probably ought to be declaring an action type: data ActionType = Increment | Decrement | Delete | Whatever Presumably `actionActor` is not that unconstrained either, and so on. Finish declaring your types strongly enough, and testing becomes nearly redundant for JSON encoding; the biggest mistake I've ever made even with yet more complicated structures than what you showed there was accidentally misspelling the object keys, or mismatching the key spelling between the encode and the decode step. I suppose you could also forget to serialize part of the object out; strong typing makes forgetting to serialize something in virtually impossible. You can solve that with a library that helps you declare both directions at once via isomorphisms, which I thought I saw on /r/haskell once before but can't seem to turn up on Hackage.
On that note you've also got `s = sections ! sections`...
&gt; I also have to be certain that actionType value is serialized into the "type" key The biggest reason of using the free monad approach is to avoid duplication in the test code but is Quickcheck really the best way to test the API? The ToJSON instance definition already does a pretty god job of defining your interface and after we write it for the first place almost every change we make on it will surely break the interface, making the tests a bit redundant (they might as well just complain everytime you edit the ToJSON instance and show you aq diff with the last revision). I'd imagine the tests would help more if you were using some ad-hoc algorithms to write out the JSON and frequently optimized those for performance, meaning that you had to constantly be careful of breaking the interface. Overall, I think a test on the JS side would do a better job of catching API breakage - not only would that test what the client gets to actually see but it also stresses some extra layers of the API (JSON serializer, web server, etc)
I'm just commenting to thank our two fellow haskellers for making these casts. This is a really nice addition to the community, cheers guys!
Very interesting although I don't understand everything 100%.
&gt; The biggest reason of using the free monad approach is to avoid duplication in the test code I don't think that is the biggest reason - the biggest reason is how much can be expressed in tests, and also in making the result of the tests useful. &gt; after we write it for the first place almost every change we make on it will surely break the interface, making the tests a bit redundant How will you know it's broken the interface? &gt; Overall, I think a test on the JS side would do a better job of catching API breakage - not only would that test what the client gets to actually see but it also stresses some extra layers of the API (JSON serializer, web server, etc) There are full integration tests, but they are not exhaustive in all the possibilities that of objects that can be serialized - because to be exhaustive you get a combinatoral explosion. What I am doing here is trying to guarantee that the smallest components I have to do what I expect, and then I can put more trust in the composition of the entire system.
&gt; Well, sure, but this falls into the "trivial" bucket to my mind. Once you write the code to do that, there it is. In the time it took to write the test, you could just check. Anything that might happen to that code ("accidentally deleted", etc) might equally happen to the test code, too. It is either a bug or it isn't - I don't see how the symptom of the bug changes how important it is. Anyway, once it's written it is only there as long as I trust myself or my fellow developers - and frankly I trust none of us. It's true that test code can bit rot too, but at least having tests insures me against accidental deletions because now I have to accidentally delete things twice. What I'm not seeing is how you envision types can remove the need for what I am testing. I am not testing that pieces inside my system fit together, I am ensuring that I am exporting the correct public facing schema. That is, I am literally testing that I have been able to type the string `"type"` correctly (and have got it right for the rest of time). The best I can imagine is that I decompose my `Action` record down into pairs of (`ActionKey`, `Value`) and ensure that the decomposition is correct, and write a test that my `ActionKey -&gt; String` mapping is correct, but that's horribly complex above what I've done (which is at least separate code that doesn't impact the implementation of the API at all).
Another option would be to make an json encoder which also makes a schema, and check schema instead. Or generate js decored/encoder from.
Recursion could be a little bit hard when you're starting, if you're used to imperative programming. Another factor I found is that a lot of times it's explained badly. I found Paul Graham's explanation (I think it was in [ANSI Common Lisp](http://www.paulgraham.com/onlisp.html)) an aha! moment. The idea is that a recursive function acts over a recursive data definition. So, these functions follow (in most cases) the structure of the data. For example, as more than one has commented in this thread, a list really is: data List a = Nil | Cons a (List a) which means that a list of type 'a' is either the value Nil or a value Cons that contains a value of type 'a' and a list of type 'a'. So, most functions that operate over lists will be of the form: somefunc Nil = ... somefunc Cons a as = ... (Note that data is a type definition, so 'a' and 'Cons a (List a)' denote types. In somefunc 'Cons a as' is a value). Another example, a binary tree could be defined as: data Tree a = Empty | Node a (Tree a) (Tree a) so a (recursive) function template over (recursive) trees would be: somefunc Empty = ... somefunc Node a left right = ... Now, the good part. Recursion will make *easier* to program those functions. The "trick" is: 1. Begin programming the simplest case (called the base case). In a list it'ss Nil, in a tree it's Empty. 2. When programming the other cases, assume that the function is already working. In other words, you can (and should) invoke the function inside its definition to solve subproblems. For example, let's make a function that returns the sum of all nodes on a tree containing numbers. (1) The base case. What is the sum of an empty tree? sum Empty = 0 (2) The other case: sum Node a left right = a + .... What should we add? Since left and right are trees, and we can assume our sum function is working, the answer is simple, we just invoke sum over each tree: sum Node a left right = a + (sum left) + (sum right)
Both Lee Pike and Tom Hawkins have done work in this area - Lee's latest work is http://smaccmpilot.org/index.html. Tom presented his work at CUFP 2008 - http://cufp.galois.com/2008/schedule.html. His eDSL *Atom* is on Hackage.
BTW - by "this area" I mean vehicle control with formal verification rather than "plugins". (To develop a compiler, I think you'd have a lot on your plate with just the first two rather than all three).
The typeclassopedia needs to be included. http://www.haskell.org/haskellwiki/Typeclassopedia
Roughly speaking the above formula arises at the call-site to `divide`, where we want to check that the *actual* argument `z` satisfies the requirement that it be non-zero. Thus, the solver checks whether that-which-is-KNOWN-about-the-actual =&gt; that-which-is-REQUIRED-for-the-parameter. the LHS i.e. what is known at the call-site is: * `z == 0` * `x &gt;= 0` (as it is a `Nat`) * `x &lt; z` (from the output type of `foo`) the RHS (i.e. what is required) is `z /= 0` Does that help? Thanks! 
And it would stay true whatever you put on the right side. "Falsity implies anything", as they say.
All episodes have been great so far, thanks for organizing and recording those! The bit that really caught my attention were the mentioned improvements to stack traces and debugging. Is the plan, basically, to provide normal debug symbol databases which can be processed by standard system tools like gdb / atos / addr2line? That would be amazing, as the current -xc RTS option is pretty crummy and you're completely out of luck when your Haskell program crashes due to a segfault (unsafe vector operation or something like that). Getting just a symbol for the PC would already help in those cases, maybe even valgrind would get a bit more useful with Haskell programs. I've written my own profiling tool that uses standard debug symbol databases and callstack traversal to profile C/C++/ObjC/etc. programs. I'd be interested in making this tool work with Haskell programs. I can see how GHC could make the address -&gt; symbol translation work, but I can't imagine how a Haskell program would support call stack inspection using frame pointers or CFI based traversal etc. What's on the CPU/system call stack for the RTS / compiled Haskell code must have little correspondence to what's actually going on. If somebody in the know could provide some additional detail / links, that'd be great!
An usual occurence for me in these parts. At least it`s quite common to have a blog post from someone else the following week to clarify some things
&gt; implementing swagger in Haskell bless you
Feel free to write one. We have plenty of things you can choose to build it upon: Web frameworks, XML parsers, etc. That said, I personally would be content to never interact with SOAP again. ;)
&gt; Well then, is there a way to prevent LiquidHaskell from telling lies? That is, can we get Milner’s well-typed programs don’t go wrong guarantee under lazy evaluation? &gt; Thankfully, there is. And the solution is? To add a predicate for convergence/divergence?
This is a very nice first program. Your style is idiomatic and clear. Since you are asking for general advice, I recommend writing a property-based test suite using [QuickCheck](http://hackage.haskell.org/package/QuickCheck). For example, you could begin by writing a property that states that `rotateChar n c` is always a letter when `c` is a letter.
I also developed something that renders LaTeX snippets to images and embeds them in my blog pages: http://liamoc.net/posts/2013-11-13-imperativereasoning.html But it's very hacky, depends on a hastily written C program and is unlikely to be useful for more general audiences.
HAppS continues on as [Happstack](http://happstack.com/) which is actively maintained and yes, I think the comment is referring to AcidState.
User studies are not the only criterion for determining suitability for teaching. That said, we are doing extensive studies.
Borrowed from Racket, and languages like D appear to have it too.
Racket responded to that decades ago.
Correct. For now, our refinements will be dynamic.
That's precisely right.
But this overlooks the cost of type debugging. I think the right perspective is to assume there will not be a single ideal solution -- each group of students and their faculty (assuming a traditional setting) will have their own comfort point at which to make the switch (including at time zero).
But we compile to JavaScript via Racket, and may soon have a direct JavaScript implementation. We're not as biased as you think, but we're also not exactly the same as the Racket team.
Our language is eager, it puts a lot of emphasis on testing, and our refinements are currently checked dynamically. Our types are gradual, and our plan is to always keep the overhead of types (meaning cognitive overhead) low. This means we'll trade a less sophisticated type system for one that has low friction for the user (especially in terms of errors).
This is a good characterization.
Needless to say, there's overlap between the Pyret and Bootstrap teams (namely, me), and we'd very much like Pyret to useful for Bootstrap teachers. I really don't understand your comment about FRP. I don't understand how Bootstrap (the curriculum) is a "naive FRP system". Understand that my experience with FRP comes primarily from having designed two languages with very particular characteristics (FrTime and Flapjax) so we may be using different meanings for the term.
Such is life in the world of static checking. Note that in general, if I understand correctly, proofs of such contracts would not depend on the arguments being convergent. So there shouldn't be an added burden of proof. But contracts, such as this that do rely on the fact that the argument is convergent, would require you to prove it. In the end I don't see any way to avoid that. To be honest my question wasn't so much if convergence/divergence needed to be considered, but if the programmer would have to deal with it by hand by proving such predicates, or if it could be done in a more clever way. That said... my knowledge of LiquidHaskell is pretty shallow and I thought about this for a few minutes only :P 
I'd also suggest Control.Applicative syntax, makes things clearer in my mind eg. import Control.Applicative getDict :: FilePath -&gt; IO (Set String) getDict = fromList . words &lt;$&gt; readFile 
Over all rather nice code, I only have two suggestions First countRealWords should probably have it's arguments flipped. If you consider partial application, it's more likely that you will be changing the word not the dict. Second you can clean up the decode functions a bit like follows (assuming the first change) comp = compare `on` (countRealWords dict) 
With a zipper? http://www.haskell.org/haskellwiki/Zipper For example: http://hackage.haskell.org/package/rosezipper-0.1/docs/Data-Tree-Zipper.html I think that does what you need. But walking down, doing work, and walking up again is of course rather wasteful. So you usually just walk to where you need to be, do you work, and stay there… like a cursor… which a zipper is. It’s really easy to implement something like that on your own once you know the basic ideas. 
I think you cannot get a better introduction to this than that: http://learnyouahaskell.com/zippers 
Actually, I only made a couple of my functions point free after lpaste complained about "Eta Reductions," but those do look much better point free. Your decode function looks much nicer too. Thanks!
I think /u/kamatsu is alluding to the halting problem. edit: at least it feels feesible to detect when a function depends on it's argument, and if does not, drop it from analysis.
The correct version doesn't seem particularly more clear to me. getDict = ((fromList . words) &lt;$&gt;) . readFile In fact, as much as I like applicative syntax, that looks significantly worse to me.
I definitely will. In fact, the main reason I'm learning Haskell is because I want to learn how to write correct code. After that, perhaps I'll formally verify it in Coq :)
Yes, you're on the right track -- not a predicate, but a check for convergence.
&gt; caesar :: Int -&gt; String -&gt; String &gt; caesar = fmap . rotateChar I'll be honest, I like the non-point-free version of this better. With the point-free version, I need to stop and think about how the first Int gets applied to `rotateChar` which then gets passed to `fmap` before the `String` argument is considered by the function. Perhaps this is intuitive for experienced Haskellers, but I have trouble with function composition of non-unary functions. Also, why use `fmap` instead of just `map`?
True. I only meant it should be evidential. Also, unfounded claims can still be correct—even the most empirical design is guided by intuition.
Thank you very much, Simon! It was real pleasure to listen. And I'm really curious with Haxl, since everyone (and myself) these days deal with multiple backends and multiple servers of data, it would be nice to be able to express yourself so that it gets implicitly concurrent. 
&gt; I need to stop and think about how the first Int gets applied to rotateChar which then gets passed to fmap before the String argument is considered by the function. This is indeed a new way of thinking to basically anyone who hasn't programmed Haskell before, but I recommend training yourself to think this way as it can be extremely helpful.
Probably beginning of next year.
That would be good if OP wants to walk around the tree in some arbitrary order and insert nodes in various places. I suppose that could be the case, since OP was so vague, but it sounds unlikely.
The TikZ manual has a whole tutorial about Petri nets, if that's what floats your boat.
IMO, it's not hugely surprising that refinement types detect evaluation order: this also happens in intersection types. You can work around this by controlling the positions at which you do intro/elim of the refinement types -- see Joshua Dunfield and Frank Pfenning's [*Tridirectional Typechecking*](http://www.cs.cmu.edu/~joshuad/papers/tridirectional-typechecking/). They worked this out for cbv, but a dual treatment for cbn ought to exist. 
I liked the non-point-free version better like you in the first look, but actually the point-free is clear as long as you think of it this way: * rotateChar takes an Int and returns a function which acts on a Char. * fmap takes a function and returns another function which acts on a String. Indeed, there is no need to think that the String argument kind of avoids rotateChar in order to reach fmap =&gt; it's just that fmap returns a function, and caesar is just this function.
&gt;Also, why use fmap instead of just map? `fmap` works on any functor, `map` is just for lists(a String is a list of Char). When used on a list `fmap = map`. Personally I always use `fmap` because it is more general.
Yes I understand he is. 
&gt; IMO, it's not hugely surprising that refinement types detect evaluation order No sure what you mean. Do they? It doesn't seem like they do. That would be a huge problem given that the semantics of Haskell do not specify evaluation order at all. 
Right, thanks!
When you add the obvious rules for union types, then substitution does not preserve typability. The key point is that you need the disjunction property -- if a term has `e : A v B`, then we need to be able to infer that either `e : A` or `e : B` hold. However, this is not true: e.g., if `e` is a variable `x : A v B`, you can't do any better. As a result, it's unsound to add the obvious rules for unions to a nonstrict language. (This is what I mean when I say they detect evaluation order.) Making things sound usually requires either a value restriction or a covalue restriction (ie, restriction on the evaluation contexts the rules can be used in). See Barbanera and Dezani-Ciancagliani's *Intersection and Union Types: Syntax and Semantics* for a discussion of this point. 
Is there any case where you'd specifically want a *map* instead of an *fmap*?
&gt; When you add the obvious rules for union types, then substitution does not preserve typability. The key point is that you need the disjunction property -- if a term has e : A v B, then we need to be able to infer that either e : A or e : B hold. Yes. &gt; However, this is not true: e.g., if e is a variable x : A v B, you can't do any better. Not sure what this means... a variable's type doesn't need to be inferred, it's simply assumed. Maybe it'll be clear once I read the paper. &gt; As a result, it's unsound to add the obvious rules for unions to a nonstrict language. (This is what I mean when I say they detect evaluation order.) I also don't understand this, since strictness is orthogonal to evaluation order. Both strict and non-strict languages may have their evaluation order specified or not. E.g. OCaml, unlike SML, [doesn't fully specify evaluation order](http://caml.inria.fr/pub/docs/oreilly-book/html/book-ora029.html). &gt; See Barbanera and Dezani-Ciancagliani's Intersection and Union Types: Syntax and Semantics for a discussion of this point. Thank you for the reference. I'll read it soon. 
It's not invisible, just whitespace.
Great. 
To me the issue isn't quite evaluation order but the (potential) divergence, which is "masked" by cbv but which one must deal with head on in cbn. Thus, I'm not sure one can solve the problem by controlling the positions, because its: (a) hard to connect syntax and evaluation order with CBN, (b) worse, often we want to talk about things that are never actually evaluated (e.g. ghost variables). Still, thanks for the links! Perhaps I've not fully understood you and the links will make matters clear. And of course if you can figure out a dual treatment that would be great!
Not really, but say you have a function `f :: Int -&gt; Int`. Now define funca = fmap f funcb = map f If you don't provide any type signatures, GHC will infer the types. The type of `funca` will be `(Functor f) =&gt; f Int -&gt; f Int` while the type of `funcb` will be `[Int] -&gt; [Int]`. If you *know* you want your function to operate on lists, using `map` can be nice since the compiler will complain of you pass anything else. If I wanted this behavior I would still use `fmap` though. I would just give the type signature explicitly: funcc :: [Int] -&gt; [Int] funcc = fmap f now `funcc` and `funcb` are identical. However in general I think you would want your functions to be as general as possible.
 tell w = RSST $! \_ (s, ow) -&gt; return ((), (s, ow ++ reverse w)) I don't think this has the strictness properties that you want. The `($!)` does nothing because its second argument, the lambda, is already in whnf (and the RSST constructor is already strict because it is a newtype).
What makes onmach's suggestion so bad that everybody is trying to find a way to fix it?
`constE` doesn't do what you think... it makes an event that just triggers once, at the start. And `ship` shouldn't be an event at all, it's supposed to be the ship's position, right? Make `ship` a behavior. Currently it is an event that never fires, which is why nothing ever gets drawn (the behavior returned by `game` always has value `return ()`). Assuming you want the ship to move "continuously" while an arrow key is held down, you need some kind of "clock" event that triggers every time step. Use it in `move` to produce an event of ship position updaters and let `ship = accumB 100 move`.
I suggest you get reactive-banana-sdl from https://github.com/orclev/reactive-banana-sdl or https://github.com/JPMoresmau/reactive-banana-sdl (my version is reactive-banana 0.7 compatible). It hasn't been released on Hackage yet but hopefully it will be soon. Either use the library directly or look at the code to see how things are done. I have a little game based on it at https://github.com/JPMoresmau/typeclass. 
&gt; There are times when I'll change the data structure from a list to something else and when I do the code continues to work. Just change `map` to `fmap` when that happens. You'd have to change the type signature anyways. There is no gain (and some cost) to writing `fmap` instead of `map` when you know the functor is `[]`. In this particular case the most likely change is replacing `String` by `ByteString` or `Text`, which you can't use `fmap` on anyways (as they're monomorphic containers). So this "premature generalization" is particularly misguided.
Thanks! I actually made ship a behaviour first then made it an event since I couldn't figure out how to trigger the returned type. This explains it! EDIT: The problem is now I don't know how to convert my 'Event t ()' (that occurs each frame) to 'Event t (Int -&gt; Int)'. Apply typeclass came to my rescue! :)
Interesting. I wonder how this compares to other looping mechanisms e.g. arrow, pipe, frp signals.
I think you can do myStartupHook = spawn "xsetbg ~/somebackground.png" &gt;&gt; setWMName "LG3D"
Solved my problem exactly. Thank you! I'll sit down and see what's Haskell is all about one day. My CS friends were very excited to learn about Haskell, but I never understood why. They always mentioned how hard it was to learn. However xmonad is far too useful to not understand.
Another way to write the above is: myStartupHook = do spawn "xsetbg ~/somebackground.png" setWMName "LG3D" They mean the same thing. I'm just letting you know because if you have to add more commands later, you can just add more lines to the "do" block. I think the do syntax is clearer for this example. Anyway, I can't encourage you enough to try Haskell. It is true that it can be challenging to learn, but it transforms the way you think about programming.
yeah. for example you can't (at least, not in my attempts), do any IO with moeb traverse.
The two notations are completely equivalent; the `do` block is what Haskell people call "sugar" - syntax sugar for what the compiler expands to the `&gt;&gt;` version behind the scenes. And I second the 'learn Haskell' advice.
The phrase "I've largely abandond this project" says to me that it won't.
I just saw this and thought it was strange that nobody had posted it in this subreddit when it was published.
Which software is he using? I've seen it before, but a long time ago.
You could just make one of the "operations" digit concatenation.
Tidal https://hackage.haskell.org/package/tidal
You mean in Expr? Yes, that will work. Initially I thought to add concatenation in Fours, but I wanted it to keep its values finite. 
This exactly what I was looking for. I hope anyway. Thanks!
Sounds like shit
And if I understand correctly 'm &gt;&gt; k' is sugar for 'm &gt;&gt;= \\_ -&gt; k'. Sugar all the way down. I'm surprised this community doesn't have diabetes.
The symbolic programming they are talking about is symbolic rewriting. It is like partial evaluation, not for optimization, but rather because functions are only partially defined.
Yeaaaaa, I don't know
Ah, so syntactic sugar has a proper meaning? I always thought of it as a cute term relating to code reduction. Glad I found out before making a very nerdy faux pas. 
One overcomplicated approach might be a type-level natural index, which lets you specify a global limit on the number of fours across your entire expression. data Nat n where Zero :: Nat Z Suc :: Nat n -&gt; Nat (Suc n) -- Interpreted as a number of fours and where the decimal point falls data Fours n = Fours { howMany :: Nat n, decimalPos :: Fin (Suc n) } data Expr n where Num :: Fours n -&gt; Expr n Sum :: Expr x -&gt; Expr y -&gt; Expr (x + y) ... 
If data don't fit into memory available doesn't make processing such data senseless. My question is not about does it make sense or not but how. Especially how to deal with data chunks in Haskell.
There's no any hint there.
Sounds like a kind of party I wouldn't mind. Also the bit at the end made me chuckle a little: *This article was amended to represent the fact that the artists mentioned weren't creating music with HTML.* 
The pipes library is designed specifically for this purpose.
It does, my friend, it does. They call it 'lens'.
I agree. The haskell-platfrom used is a little old. And code should be listed in the book to get better reading performance. 
I'm interested in code generation for GPU, so pipes is not an option for me.
May be relevant: [twitter chat with packtauthors](https://twitter.com/packtauthors/status/405260315924758528)
Apparently there's a PDF version with embedded code listings, but the Kindle version doesn't have them. See my comment with the link to the twitter conversation.
I've asked the maintainer (orclev) if I could take over, and I use this library for my own games, so it most definitely will.
You'll still have to have a library that chunks the data, and that chunking has to be done on the CPU side.
Well, IO is applicative, so you might be able to, just not in the same way as in your `mloeb`: toeb = moeb traverse :: (Applicative f, Traversable t) =&gt; t (f (t b) -&gt; f b) -&gt; f (t b) toeb [(const getLine), (fmap (go . head))] where go str = putStrLn str &gt;&gt; return str would be an action that read a line, printed it back, then return a list containing two copies of it. 
One complication is that there are an infinite number of expressions even with only one 4, viz Num F, Sqrt (Num F), Sqrt (Sqrt (Num F)), Sqrt (Sqrt (Sqrt (Num F))), ... So you can't just generate all the expressions with one four, followed by all the expressions with two fours etc.
I find that it makes sense to use the general `fmap` when I use a functor in the same way that I always use the general bind `&gt;&gt;=` for monads. &gt; There is no gain (and some cost) I would guess this (very small, I assume you mean the indirection from `fmap` to the specific map function of the functor) cost only occurs when the function actually works on every functor and is optimized away when the function only works for a specific instance anyways. And in that case you actually gained something for that cost namely generality.
If you have two infinite sequences and you wish the enumerate both of them in the one sequence you can always alternate between them. This way you don't have the problem you get by concatenating two infinite sequences with never reaching the second sequence. Example: -- Problematic nats = [0..] ints = nats ++ map negate nats -- We never list any of the negative numbers -- No Problem nats = [0..] ints = 0 : concatMap (\a -&gt; [a,-a]) [1..] -- ints = [0,1,-1,2,-2...] There are more efficient ways to implement it but given enough time the above sequence with reach any integer, positive *or* negative. This can be generalised to an infinite amount of infinite sequences quite easily. You simply increase the amount of sequences you are alternating between one by one, e.g.: [x_1_1,x_1_2,x_2_1,x_1_3,x_2_2,x_3_1,x_1_4,x_2_2... ^ ^ ^ \___________|_______________|__ Introducing a new sequence Given enough time this will reach any number in any sequence in your infinite list of infinite sequences.
I was also contacted by them several times in this way. I wouldn't go as far as calling this a scam, but they seem to care primarily about the quantity of published books, not their quality or the reputation of their brand. If the book sells well, they benefit; if not, they don't lose much.
They've been pinging me for about a year now as well.
Wonderful, thanks! I'm amazed to discover that the solution uses the basic constructs of the language. That's a testament of the expressive power of Haskell (and my current limitations on thinking about applicatives and friends).
I was contacted to review it, but when I show my interest they said "sorry this has been already reviewed by another person". All this in a 24 hours time-span..
could I have your opinion on the following solution ? liftList :: Int -&gt; IO [Int] liftList n = go n (return []) where go 0 = id go n = liftM2 (:) getInput . go (n-1) it seems pretty natural. I tried benchmarking it along with the other solutions and it seems to compete reasonably well in running-time but I have no idea how to check for the memory consumption... 
“Algorave”, ahahaa. I like it! I'm still waiting for a producer with some existing skill to try their hand at livecoding. Most of the ones so far _sound like they're livecoded_, and not a good way. The samples are always too harsh and curt, the beats change too often or feel random (because often they are) and out of synch, and the livecoder doesn't know how to transition smoothly. I think there's an audience for this early-90's-acid-sounding style, and a much wider audience for the live-coding style (although I'd prefer a better display), but at present it just doesn't sound good enough. It should sound decent [like this](https://www.youtube.com/watch?v=tbKRCTOGnRw). Is there any live-coding setup out there that integrates the editor enough so that you can see what pieces of code are doing what when? E.g. let's say I have some code to model a tracker: every (nth 5) $ do d808 x2 amen sample ohyeah x2 snare rush snare As an audience member I really want to see the things that are currently active being boldened or bouncey or something, e.g. every (nth 5) $ do d808 x2 **amen** sample ohyeah **x2 snare** rush snare So that when the livecoder is making changes, it's easy for me to see what changed. If it's sufficiently easy to follow, I could even shout out amendments to make. Compare with e.g. the famous [Tone Matrix](http://dagobah.net/flash/ToneMatrix.swf) that was popular some years back, with this *very simple* UI you have both a beautiful display and a beautiful sound. E.g. [I just played with it for a couple minutes.](http://chrisdone.com/tonematrix.ogv) Play with it yourself for ten seconds and you'll have something that you like very quickly. I find it quite addictive. You can adjust it live and do "live coding” with only a trivial grid. If I were going to start a livecoding system, I'd investigate how I can merge the tone matrix concept UI with Haskell code, i.e. replace the grid with an AST. I'd probably use Emacs as the editor, but export the contents of the buffer regularly to some other program that would display the code in a pretty way with OpenGL and such.
OK, nevermind, it's just the same as the naive listRecurse!! and I guess It should be liftList :: Int -&gt; IO [Int] liftList n = go n where go 0 = return [] go n = liftM2 (:) getInput $ go $ n-1
I was contacted about reviewing this book, and writing and reviewing a book on Haskell data analysis. I'm not sure it's a scam, but a way to do low quality , high margin textbooks?
My first thought is, i'd be interested in the extent to which this is possible, given that [Java is the Kingdom of Nouns and Haskell(ia) is one of the Functional Kingdoms](http://steve-yegge.blogspot.com.au/2006/03/execution-in-kingdom-of-nouns.html); i'm guessing there's a form of impedance mismatch involved.
Pakt have kindly sent me the PDF version. The formatting is much better in this one and there are in line code samples too. (I was the one on the twitter conversation whining about the formatting)
&gt; avoidance of success hahahahaha
Maybe [Frege](https://github.com/Frege/frege) can help? &gt; Frege is a non-strict, pure functional programming language in the spirit of Haskell... &gt; Frege programs are compiled to Java and run in a JVM.
Nice ! It would be really nice to have a good Haskell live coding platform, i.e., select code and execute, with code completion, help etc, so I'm not talking about just using ghci directly. 
"The samples are always too harsh and curt, the beats change too often or feel random". Well, did you consider that some people like harsh sounds and non-obvious ryhthms ? There's plenty of people doing music with those qualities that are not live coding.
Could you use a templating language? I quickly [hacked together a templating mechanism](http://lpaste.net/96288) that might help you. Would it help to develop this further?
It's available on internets (so you can leaf through before buying, you know). I don't know what is the target audience of the book. 3 pages of screenshots about how you install haskell, then 3 pages about what is monad, and suddenly we crunch some data from database (3 pages). Not worth your time to even download. 
quick note : mk_pipes = mapM_ mk_pipe
hey, thanks.
&gt; I find that it makes sense to use the general fmap when I use a functor in the same way that I always use the general bind &gt;&gt;= for monads. There aren't too many specialized `&gt;&gt;=`s to use anyways. The only one I can think of is `concatMap` which is not too common (particularly as list comprehensions cover many of its uses). In general I have no problem with using overloaded type class methods at a known specific type. I just think it's clearly inferior when there is a convenient alternative that is specialized to the type you know you have. I'm willing to bet there is an "instance" (sorry) of a specialized fmap that you use all the time: function composition `(.)`. It is fmap for the functor `(-&gt;) r`. Can you imagine writing `fmap` or `&lt;$&gt;` everywhere in place of `.`? Isn't it better to separate those fmaps that definitely are function composition from those that probably/definitely aren't? Why should map/`[]` be any different? &gt; I would guess this (very small, I assume you mean the indirection from fmap to the specific map function of the functor) cost only occurs when the function actually works on every functor and is optimized away when the function only works for a specific instance anyways. And in that case you actually gained something for that cost namely generality. I'm actually talking about the human cost of using `fmap`: more work for the reader to understand the code and worse error messages when the code is not compiling yet. You're right that there is no runtime cost when using `fmap` at a known functor, assuming any reasonable level of optimization.
There seem to be a lot of interest in writing these kind of programs recently. (Thinking of those two earlier this month who led to the creation of Hell.) EDIT: Mentioned it on IRC, and a friend of mine told me he had translated a few shell scripts of his using shelly and (his words) "It is a lot better then bash. That is for sure.".
How about tailing the actual files and write to whatever service we want? Should be easier than this.
It's not quite ready for public consumption, but [Ermine](https://github.com/ermine-language) is another possibility. It's another non-strict, pure functional language, but it compiles to an extended lambda calculus and has an interpreter in Scala.
great link! thank you for sharing it.
&gt; Edit: i'd like to TRANSLATE the code, not run it in the JVM What exactly do you want to do? Take random Haskell code and compile it to Java source code? Have a Haskell edsl that *generates* Java code? &gt; The reasons for this are multitude and i won't really bore you with them. Does it basically come down to "This problem would be cleaner in Haskell, but we've been mandated to use Java because there's already a bunch of Java code?" In that case, you might be able to convince management to let you use Scala. With libraries like [scalaz](https://github.com/scalaz/scalaz), Scala basically drags Java halfway to Haskell.
Some people will simply rewrite the Haskell into Scala or Clojure, a few big companies that have posted here do this. Haskell is awesome to write and code with and Scala is "enterprise"
&gt;In general I have no problem with using overloaded type class methods at a known specific type. I just think it's clearly inferior when there is a convenient alternative that is specialized to the type you know you have. But surely there is a specialized bind function for every monad and a specialized map function for every functor. The question is whether they are exported from the modules which contain the instance specifications or not. If using the specialized functions really were the preferred way, wouldn't package authors in general export them from their packages?
Generally speaking code generation is only profitable if you are targeting something that can't be "library-fied" or if the problem domain can be expressed in a declarative notation much smaller than an operational solution. Parser generators were once a classic example, but now "in language" parser combinators are competitive in brevity and can use the host language's library capabilities. If you want to generate arbitrary Java code you will always be less productive than your co-workers: to build a generator you are still building the target code just at a meta-level (more work); debugging means debugging the target code and updating the generator (more work) and finally because you are using the idioms of the meta-language you are likely to take less advantage of idioms of the target language (lower quality work for your co-workers to deal with). If you are genuinely serious then my best advice is look for a new job. 
Files will take space on the disk.
See also [nginx_syslog_patch](https://github.com/yaoweibin/nginx_syslog_patch).
I'm coming from both C and clojure, looking for the same type of conciseness and efficiency that I have in clojure in a language that more closely interacts with the machine. People seem to like go for this approach, I'm a bit underwhelmed when coming from clojure, haskell on the other hand fits the bill perfectly. Admittedly I turned to rust first, haskell won me with the very extensive extension library, for instance I loved the fact that Inotify has its own lib (using it for another project).
If you need a FIX parser, there is: http://hackage.haskell.org/package/fixhs
I would like to stress out that I was not asking about my particular implementation in the first place, I was just wondering why this approach hasn't been taken as it seems like a clear win. I had an answer today on IRC : the writer monad is correct by construction (you can't change the past). This could be achieved with the approach I outlined by hiding the constructor, and providing a restricted API, but that is just not as nice. 
About the Java thing, you can try putting `export _JAVA_AWT_WM_NONREPARENTING=1` in your `.xsession`. It should fix the gray Java windows without having to lie to X about which WM you use :-)
What does it even mean to translate code? Turn idiomatic haskell code into idiomatic java code? It doesn't really make sense; idioms in haskell are completely anti-patterns in java and vice versa; they are polar opposites. any java code generated will not only look ugly and be unmaintainable, but will also most likely be extremely inefficient and unoptimized and run horribly as well.
&gt; But surely there is a specialized bind function for every monad and a specialized map function for every functor. The question is whether they are exported from the modules which contain the instance specifications or not. Normally these definitions are written "inline" into the instance declaration (see for instance [Maybe](http://hackage.haskell.org/package/base-4.6.0.1/docs/src/Data-Maybe.html#Maybe)) and so there is no name for the specialized function at all. It would be hard to come up with names for every specialized version of every type class function that didn't quickly become unwieldy. That doesn't mean we should refuse to make use of the ones that do exist like `map` and `(.)`.
note how many lines it has. haskell is a nice language to write in...
it is trolling if you don't elaborate.
Well, ok, I was just lazy. This is going to be a bit polemic, I apologize upfront for it. Coding is an activity with requires concentration, therefore a relaxed environment. Following a train of thought is nearly impossible when there's to much noise around you. And I don't like the whole "oh is this cool, I'm such a hipster/nerd" stuff going on. I mean, do people really believe that programming is a "cool" social and creative activity? It might be creative but only for the design, not the programming part. Programming is a technical process, much like engineering. Or have you ever seen mechanical engineers taking their drawing boards to a club?
&gt; I'm not sure it's a scam, but a way to do low quality , high matching textbooks? Depends on how you define "scam", I guess. They intentionally produce a product of poor quality in the hope of catching some readers unaware. The author probably doesn't get a good return in either -- I'd be wary of anyone who writes books for them. 
I haven't heard it yet. Was the same argument also used against testing - you can't exhaustively test a system so you shouldn't bother?
I don’t like Learnyouakaskell. It tries so hard to be “cute” and “funny”, that it comes of as annoying, dumb and fake. I’d appreciate a Monty Python remake of it though. :)
Yes, programming can be a creative activity which combines technical knowledge and artistic sensibility into just one activity. Just google "computer music", it goes back to the 50's. 
Well, in any case you have to walk that structure to change it. There’s no way around it. Look at how the Map type is implemented internally.
I hope this is useful, and I plan to x-post to /r/altcoin after my timer expires. There are plans to incorporate the authenticated API, but that requires https (which Network.Browser doesn't support) and a good HMAC-SHA512 routine. I think the https support will come though http-streams; I'd love suggestions about what package(s) to use for the HMAC-SHA512.
One thing that threw me off was seeing the type definition of Template as [Node], and then seeing type signatures of your bringing-it-together functions, expecting to see [Node] in there somewhere, but only finding Splices and Splice and being confused by their kind. I just came across this in mightybyte's Heist talk at nyc-haskell that helped me make that connection: type Splice m = Node -&gt; m [Node] (or more accurately) type Splice m = HeistT m [Node] Looking at the Heist sources now, it seems that things have changed a little bit more and the Splice type isn't *quite* as simple (there's the n m type variables). But just having that simplified definition demystified the somewhat mysterious Splice type. There's still the SplicesM and Splices types for me to be confused about. I don't know if your intention is to bring newbies up to speed in detail - but if so then it might help to have a sentence or two about the types Splice and Splices. Your blog post great, thanks for sharing! I've been eating up all the Heist and Snap tutorials I can find lately. edit: link to Doug Beardsley talk: http://vimeo.com/59215336 
The logic is: You're going to need to write tests anyway. These tests will cover everything the type checker would find anyway. Thus, the types cost you but have little or no benefit. The mistake is that the tests would *not* catch everything the type checker would, unless you have a lot more tests which are more expensive and tie down the code more heavily. I personally trust barely-tested Haskell code more than I trust typically-tested code in a dynamic language.
 Prelude Data.String Data.Text&gt; Data.Text.toUpper (fromString "baﬄe") "BAFFLE" That's nice. 
Too bad. Good topic for a book.
Unicode is broken. On one hand, we have programmers who know too little about the intricacies of text encoding (as indicated in this article), and on the other hand, we have the Unicode Consortium, which lacks any vision on what Unicode should be.
Yes I have heard very similar argument about testing. Along the lines of it takes so much time to write automate tests and you already checked the output when writing the function so why waste the time.
Thanks for the feedback! I'm working on some compiled Heist examples too so I'll figure out a way to work this in. I am still learning Heist too so it's definitely not an all-encompassing tutorial, but I'd like to try to help more with the official docs if I can get a sense of what questions beginners have. This post only covers the things I got hung up on when I got started.
surprised nobody has mentioned this but list comprehensions are considered unidiomatic if they are just maps or filters. instead of everyCaesar str = [caesar n str | n &lt;- [1..26]] try everyCaesar = map caeser [1..26]
Hey, I responded to your e-mail and updated the README.md in my repo to note that you're the new maintainer and that the latest version of the code should be downloaded from your repo. Thanks for picking this up, I'm glad someone is finding it useful.
It seems very strange to me to include ligatures in Unicode; I would think that would be properly handled in the text rendering algorithm itself.
Sweet! Looking forward to it.
IMO people should stop focusing on the correctness argument for types. Types are good for many things, and correctness is not even the thing they are most good at. In many cases tests actually do catch virtually all type errors. The point is that types catch the low hanging fruit with way less effort, i.e. sanity checking not bug catching. However, there are several things that are at least as important: types guide program design, they provide documentation and aid readability, they allow for better performance, they enable type classes, and they provide a concise notation for the contract between a module and the consumer of a module that is machine checked, they enable easy &amp; safe refactoring, they enable accurate autocompletion &amp; go to definition features in an IDE, etc. Types are however *not* particularly good at catching non-trivial bugs, so I think it's a good idea to stop focusing on the least convincing argument for types.
Something like this one, it is written with Snap: https://github.com/lyntonzhang/blogmd I feel like I still need more more guides after reading the two beginner books. 
http://www.unicode.org/faq/ligature_digraph.html &gt; The existing ligatures exist basically for compatibility and round-tripping with non-Unicode character sets. Their use is discouraged. No more will be encoded in any circumstances.
snap is a fairly large project and web framework with a very active community here on reddit, I would start out by learning snap first then going onto blogmd
I decided to translate Okasakis book, Purely Functional Data Structures into Haskell (it was SML). It's a must read for anyone wanting to write any serious functional data structures. And it led me to learning about optimizing Haskell, a nontrivial topic.
Sorry, Purely Functional Data Structures
Tha’s a bug. The correct result would be "ẞ" which is the rather new but nontheless correct uppercase "ß". It’s on the [NEO](http://www.neo-layout.org/) keyboard layout, so I can enter it like any other letter.
Dance party.
Don't be absurd. I'd write a piece of Haskell code to choose the optimum song to play at any moment if I had a dance party.
To anyone wondering: He means [the clipart code pages](http://www.fileformat.info/info/unicode/block/miscellaneous_symbols/list.htm). I mean it’s nerd-cool, but do we really need a [skull and crossbones character (☠)](http://www.fileformat.info/info/unicode/char/2620/index.htm)? Let alone a… fuel pump character: ⛽ It’s too small in comparison anyway! Not that that’s the biggest problem here… ;) Edit: Props for Reddit, for allowing them. At least *somebody* can handle Unicode! ^((I’m looking at you, Slashdot!)^)
Are you going to release the source code for that? I've been considering reading that book but would prefer doing it with Haskell.
That's a pretty bold statement about Unicode! Care to back it up?
Don’t blame your willful ignorance on others. Unicode is separated in pages and planes. Usually you only need the first plane. And then you make a whitelist of just the sensible pages. Finally, since the very first page equals the ASCII characters, you have to block everything below 0x20, and un-block "\r\n\t\v" plus whatever you need. And guess what: Unicode already has type info on every character! So you can already tell it to filter everything that’s not a normal character. So the typical “Uuh, but there are insecure characters in there; hurr, durr!” argument is bullshit, since many of those already existed in ASCII, and I don’t see anybody bitching about them being “too hard” to filter out. Like "\NUL\SYN\ACK\ESC" ++ repeat '\b'
From my point of view, if you really want to use types to enforce correctness then you need more powerful type systems, like dependent or refinement types. Unfortunately, there are still lots of open problems in this direction though.
My poor solution is to write library code, which I keep discarding because.. * I had a better idea * What I did was stupid * What I made was not quite what I originally wanted So far, the adventure has been more rewarding than the results. I am learning a lot by my ridiculous challenges, I do not suggest being adventurous rather than productive, but I feel that it has helped me in my continuing maturity,
Coding your first project, of course. ) Don’t get caught in the feeling you’d never be good enough. Otherwise you’ll some day find yourself reading the book “Tutorials for tutorials for dummies — Start learning learning learning today!” and thing “How will I be able do do *anything* without it!?” ;)
But that's semanticallly correct. When you yell that phrase nobody would assume _modest_ beer consumation...
I honestly wouldn't be learning much if I were staying within the limits and patterns of what I've read about. At least not as fast. I remember my first few days after reading LYAH, I spent literally hours trying to figure out what the compiler was saying to me :-) 
If you got a a hard time getting started with writing your first project I highly recommend reading [Write Yourself a Scheme in 48 hours](http://en.m.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours). In the beginning of this year I could not program at all. After reading this tutorial I got how you really *program*, taking a program apart and write the simple pieces before fitting them together. Now I've manage to write IRC bot, BF compiler, Pong game and such beginners programs and that is thanks to that tutorial.
&gt; For an ADT like your example Expr, where no other types appear in the constructors, this can be done by building all possible derivations one level at a time. I think I can see two problems here—one minor, one less so. First problem: you're assuming one extra condition that the type has nullary constructors. (To be fair, a comment later in the code example acknowledges this.) So the technique breaks down for types like this: data Silly = Foo Silly | Bar Silly deriving Show This is a perfectly legitimate Haskell type. I can define values of this type and write well-defined functions over it: foos, bars, foobars, barfoos :: Silly foos = Foo foos bars = Bar bars foobars = Foo barfoos barfoos = Bar foobars elimSilly :: (r -&gt; r) -&gt; (r -&gt; r) -&gt; Silly -&gt; r elimSilly f b (Foo x) = f (elimSilly f b x) elimSilly f b (Bar x) = b (elimSilly f b x) sillyToList :: Silly -&gt; [Bool] sillyToList = elimSilly (True:) (False:) example1 = take 10 $ sillyToList foobars -- [True,False,True,False,True,False,True,False,True,False] But I can't do this: enum :: [Silly] enum = [Foo, Bar] &lt;*&gt; enum example = map (take 5 . sillyToList) (take 5 enum) -- hangs In fact, the set of values of `Silly` is isomorphic to the set of infinite binary sequences, which is uncountable, so of course we can't enumerate it. But now the second, somewhat bigger problem is that your `Rec` type "contains" my `Silly` type as a subset; i.e., there is an injective function from `Silly` to `Rec`: sillyToRec :: Silly -&gt; Rec sillyToRec = elimSilly Nah Nuh So Haskell's semantics admits values for `Rec` that your code does not and cannot enumerate. But of course if we ignore these cases you indeed succeed at enumerating all *finite* values of `Rec`.
Some people are doing exactly that. http://algorave.com/
Great, I will give it a try.
My suggestion is to get involved in one of the many really nice existing projects in the Haskell community. Besides quickly achieving a feeling of growth and accomplishment, you'll also have a great time!
Of course...I am an idiot.
I don't know if that's true for German, but it's not true for Unicode. From UnicodeData.txt: 00DF;LATIN SMALL LETTER SHARP S;Ll;0;L;;;;;N;;;;; 1E9E;LATIN CAPITAL LETTER SHARP S;Lu;0;L;;;;;N;;;;00DF; U+1E9E lower cases to U+00DF, but U+00DF does not upper case to U+1E9E. From [SpecialCasing.txt](http://www.unicode.org/Public/UCD/latest/ucd/SpecialCasing.txt): # The German es-zed is special--the normal mapping is to SS. # Note: the titlecase should never occur in practice. It is equal to titlecase(uppercase(&lt;es-zed&gt;)) 00DF; 00DF; 0053 0073; 0053 0053; # LATIN SMALL LETTER SHARP S Instead, it upper cases to U+0053 twice. Also, from [the proposal for adding U+1E9E](http://std.dkuug.dk/jtc1/sc2/wg2/docs/n3227.pdf)^PDF! : &gt; The capital ẞ may under no circumstances change the case mappings of existing data stock: &gt; –toupper (“ß”) remains “SS”; &gt; –tolower (“capital ß”) results in “ß”: there is no rule that toupper (tolower(X)) must give X again, 
Why do you need that many modules?
My copy of the book already has Haskell source code for all the data structures in the appendix.
That depends what aspect of quant finance you want to learn. The rough subject areas as I see it are * Derivative pricing - Stochastic calculus - PDEs - Monte Carlo - Interest rate curves * Quantitative trading - High frequency (order book modelling, exchange arb, pricing, risk control) - Medium frequency ("day trading", "stat arb" etc) - Low frequency ("value investing", "carry", "trend following" etc) There are many books covering all these topics and more. Common languages used tend to be C++/Java/C# for derivative pricing, Python/Matlab/R for quant trading research and C++/Java/C# for execution, although there are exceptions (e.g. Jane Street use OCaml for quant trading, Barclays Capital and Standard Chartered use Haskell for derivatives pricing, Tsuru uses Haskell for quant trading) Let me know if you're interested in recommendations for any of these areas/languages.
This is what I am keep doing the las three months, and I am not happy because I made little progress compared to my experience of learning Python/C .. kind of wanna give up now.
I chose to implement a small toy project, then re-write it several times as I learned more Haskell, to hopefully produce code that would become more idiomatic with each iteration. I also read others peoples code for a bit. 
I agree. The article is very useful as an introduction and an overview, and this would make a great addition.
Data.Text will do everything fine **if and only if** you know what you're doing. And you will likely need Data.Text.ICU. The problem here is not library, it's how you treat your data. If you're using any function that access string by index and think that's fine, — I've got bad news for you. **Edit**: example ------------------- h&gt; let s = "S̷̨̖̻̙̪͎̏ͧ̆͊̅ͦ̿ͅt̡͚̺̤̗̞̼̉ͭ̓̽ͣ͐͗̉ͨ͗͋̎͋ͬ͝͠r̛̜͇̯̖̪̈ͩ̇̆̀̾̇͂̆͜i͐͒̏ͨ̚͏҉̡̹͓̯͎͎͈̪̩͞n̼̮͉̣̰͙̘̬͙̯̪̮͍̺̹̬̩͊̾̎́̌͒͊̀͆ͯ̿͘͝g͛ͮ̈́͛̍͋ͮ͋̿ͭ̈́ͯ҉̵̴̩̱̝̗͚͙͘͜" :: Text -- a simple string, 254 bytes in utf-8 h&gt; length $ chars' s 6 -- 6 graphemes h&gt; T.length s 130 -- 130 code points h&gt; TIO.putStrLn . ("\t\t\t"&lt;&gt;) . head . chars' $ s S̷̨̖̻̙̪͎̏ͧ̆͊̅ͦ̿ͅ -- first grapheme. But why'd you need it? 
Next step is to write some code! Preferably something you need yourself, so you have the motivation to do it.
What I can accept about Unicode is the compatibility with ISO/IEC 8859-1, because it spares all of more headaches than it creates. I can also accept there should be isomorphisms between some other well-established character encodings. But let's take a look at a *few* specific examples that make me doubt the competence of the Consortium to create an "character encoding to end all character encodings", which is clearly their ambition: * First off, there's the whole LTR/RTL issue, which existed from day one of Unicode. I get that the UC would like to have a simple character encoding, with context-free codepoints, but that's barely possible with the reality of the written word. The current 'solution' of Unicode, [RLO](http://www.fileformat.info/info/unicode/char/202E/index.htm), is so poor that it even [helps disguising malware](http://boingboing.net/2011/10/03/unicodes-right-to-left-override-obfuscates-malwares-filenames.html). What UC should have done, in my opinion, is to let go of the idea that text is context free, and to encode spans of codepoints with the writing system they're coming from. Yes, this makes Unicode more complicated, but the writing systems *are* complicated. Yes, this threatens the self-synchronizing feature of UTF-8, but with clever engineering, this wouldn't be a problem. No, this doesn't necessarily break text or even words with mixed scripts, it just makes it more expensive to encode, which is fair given how much it occurs. An upside to this idea would be that writing systems with higher codepoint numbers would no longer be penalized, since they could live in the 7-bit range, a palette size that is often more than enough. **UC shows that they are too naïve to deal with the complex task of standardizing text encodings.** * Then there is the inconsistencies in letting compatibility encodings live alongside an encoding with a single vision on text encoding. For instance, [Arabic](https://en.wikipedia.org/wiki/Arabic_script_in_Unicode) has seven blocks of characters. The first block is compatible with [ISO/IEC 8859-6](https://en.wikipedia.org/wiki/ISO/IEC_8859-6), which is a contextual encoding of Arabic -- which is a great idea for a writing system so sensitive to context as Arabic. Then there are a few presentation forms blocks, which override certain contextual forms that exist in specific languages that use Arabic script. The latter could've been designed better, but all in all: contextual scripts are nice. But let's look at another contextual script: Greek. At the end of a word, the minuscule sigma gets a different form. If the context rule is so simple, why have only presentation forms? Make a few compatibility codepoints for [-7](https://en.wikipedia.org/wiki/ISO_8859-7) instead. **Again, UC shows that they had no vision from day one. They just concatenated differently styled character encodings and hoped for the best.** * Emoji. What the fuck. So while many writing systems, organic (Welsh), fictional (Klingon) and technical (IPA) alike, can go fuck themselves (Welsh must use Latin even though [their writing system](https://en.wikipedia.org/wiki/Welsh_orthography) has letters (not ligatures!) that don't exist in Latin, Klingon doesn't have a block, IPA is scattered all over the BMP), a company can buy themselves into the Unicode standard with a *very poorly designed* set of codepoints? **UC shows they don't they're dealing with more than just character encoding. It's about language, and they're ignoring how important that is.**
Changelog: Changes since previous release: * Type classes now implemented as dependent records, meaning that method types may now depend on earlier methods. * More flexible class instance resolution, so that function types and lambda expressions can be made instances of a type class. * Add !expr notation for implicit binding of intermediate results in monadic/do/etc expressions. * Extend Effects package to cope with possibly failing operations, using "if_valid", "if_error", etc. * At the REPL, "it" now refers to the previous expression. * Semantic colouring at the REPL. Turn this off with --nocolour. * Some prettifying of error messages. * The contents of ~/.idris/repl/init are run at REPL start-up. * The REPL stores a command history in ~/.idris/repl/history. * The [a..b], [a,b..c], [a..], and [a,b..] syntax now pass the totality checker and can thus be used in types. The [x..] syntax now returns an actually infinite stream. * Add '%reflection' option for functions, for compile-time operations on syntax. * Add expression form 'quoteGoal x by p in e' which applies p to the expected expression type and binds the result to x in the scope e. * Performance improvements in Strings library. * Library reorganisation, separated into prelude/ and base/. Internal changes * New module/dependency tree checking. * New parser implementation with more precise errors. * Improved type class resolution. * Compiling Nat via GMP integers. * Performance improvements in elaboration. * Improvements in termination checking. * Various REPL commands to support interactive editing, and a client/server mode to allow external invocation of REPL commands. 
Perhaps we should have some new terms. Haskell could be called a strongly verified ("functional" and "statically typed") and highly convenient ("high-level") language? "Dynamically typed" would be unverified, "low-level" would be highly inconvenient, etc..
I have been doing that for a while, now I think I still need more input.
to go through the list, haskell's `text` gets the normalization wrong. which is by design, as far as i understand. there is `text-icu`, that handlles the normalization correctly. note, that the blog post does not mention locales, which is essential for some things...
SQL
&gt; haskell's text gets the normalisation wrong Actually AFAIK it doesn't do normalisation at all. And I wouldn't say this is wrong
&gt; there is no rule that toupper (tolower(X)) must give X again, This should be prominently documented. 
I feel that the best (maybe only?) way to learn is to gradually bigger projects; during those you will be inevitable confronted with real-world issues (sometimes unsolved real-world issues), from which most of the learning comes. Books won't teach everything, and there are not many Haskell books anyway. I also found that doing projects whose target audience is yourself is a very good (the only?) way to keep motivated. Based on these, I'm not sure you actually need more input. If you still feel you need more input, perhaps you could explain more about why you feel that way.
Then we indeed have a very different experience. In my experience NULL errors are an annoyance at development time, but they are almost never the kind of bugs that show up well tested code.
Seems to be a vocabulary shrivel.
well yes. i meant it does not get the normalization cases from the blog post right. so we agree, i guess.
This pretty much how I learned (and continue to learn) Haskell.
**1. Work on a project that is "too hard" for you.** "What?!", you say. Yes, you heard me correctly. The best way to push your abilities forward is to work on something that seems out of reach. All the biggest improvements for me have come when I just dove into a project that I had no idea how to do. **2. Work with other people.** Programming languages have such a wide space of possibilities that it's not possible for a single person no matter how talented to explore them all. Case in point: Simon Peyton-Jones recently mentioned in his lens presentation, "I don't think we had any clue, when we were designing Haskell, that you could do *this* kind of thing." Every programmer has a unique set of experience and things they've learned over the years from the school of hard knocks. By collaborating with other people, you allow yourself to learn from all of their experiences. One of the best ways to do this is to work on open source projects with people who you respect. **3. Work on the same project for an extended period of time.** Working on something for a long time forces you to deal with the impact of the previous decisions you made. Even if you do point #1 and work on hard projects, if you just drop them once you get the first sort-of-working version, you'll never learn what practices are good and bad for long-term ease of maintenance. In my experience, I've always learned the most from multi-year projects. Also, if you've done a good job with points 1 and 2, then by working on it for an extended period of time you'll be extracting much more value for the time than you would have otherwise.
No you're not
Not to sound like a broken record, but did you go through the typeclassopedia as well?
FWIW, Mercury doesn't have Prolog's "cut."
http://semantic-domain.blogspot.co.uk/2013/07/what-declarative-languages-are.html
I've had success with loebM as follows: loebM :: (MonadFix m) =&gt; [ [a] -&gt; m a] -&gt; m [a] loebM f = mfix $ \a -&gt; mapM ($ a) f The context is here: http://lpaste.net/50589 I was using it to tie loops during the effectful creation of references in a monad (usually ST or IO for their refs). It worked quite well and created fast traversing linked structures. This came up in discussion at http://blog.sigfpe.com/2006/12/tying-knots-generically.html
I tend to think of declarative programming as programming in a way that provides more information. This has the effect of providing leeway in the manner in which is a program is executed. EDIT: I've thought of a better way of stating this: "Declarative programming is programming by expressing intent".
you mean this? http://www.haskell.org/haskellwiki/Typeclassopedia http://www.haskell.org/wikiupload/e/e9/Typeclassopedia.pdf I think typeclassopedia is one of the important ideas that did not mentioned in the two beginner books, and I believe there are still lots of them.
Thank you, just downloaded this two books.
XSLT :-D
Absolutely. Dependent types are a very different story. The kind of errors they potentially catch go way beyond sanity checking.
great, just read the source code, there seems not much documents.
things like type safe, phantom types, type family, GADT, quasi quote ...etc, I just google this and google that, but I feel like I don't have a comprehensive understanding of all of this.
To understand what "declarative" means you need to look at history. The term got popular in the 80s when we needed a name encompassing both functional programming and logic programming. At the time it was LP that was popular, and FP wanted to jump on the band wagon. So a term including both was needed. So for me it's just a name for FP+LP.
A more nuanced argument might be to consider the cost models: Software ought to work correctly and that, for all the correctness properties you ought to verify, types and tests are two options for verifying them that often have strikingly different costs. Therefore, in programming languages with powerful static type systems, you can achieve a higher level of verification at a lower cost by choosing to verify certain properties by using types instead of tests. Whether to use types, then, is not an all-or-none decision. It's an optimization problem over all of your verification options in which you want more verification for less cost. And being able to purchace iron-clad verification at dirt-cheap prices for many properties is why types are often a big win.
When somebody claims that "tests will cover everything the type checker would find anyway," I ask them to show me how they would write tests to verify that their software is free of XSS vulnerabilities. (You can do this with types.)
yes to "This problem would be cleaner in Haskell, but we've been mandated to use Java because there's already a bunch of Java code?" i'll try to talk to my team leaders and see what they think - i think converting scala to java would be easier too
oh wow thanks that looks promising, i'll try to expand on it!
#2 is really really important. I like to think that my one trick is taking everyone else's tricks and making them my own. =)
Sure, but what does it mean for a language to be imperative or declarative?
Likely just the typical structural corecursion checking like Agda.
Thought provoking! The instinct is to declare that languages that express "what" are declarative and languages that express "how" are imperative. But it turns out that it's hard to make rigorous a distinction between "what" and "how"! The canonical examples of declarative systems are databases / search engines, but it's hard to see how these relate to programming in general. Even SQL can become quite "imperative" once the queries become more complex / have multiple stages / etc. An attempt for generalization from a different perspective: the type is "what", the term is "how". Halting problem notwithstanding, having a system automagically derive terms from types is "declarative" programming. Not aware of such systems, is there any good literature on the subject?
"type safe" i don't really understand, but the others you will meet in real-life projects... or at least that is my experience. - - - Phantom types: when you use a type parameter which is not used in the right-hand-side, for example: newtype Phantom a = Phantom Int This can be used for extra type safety, for example you can put a physical unit as the type parameter. Example: data EUR data USD newtype Money a = Money Double convertToEUR :: Money USD -&gt; Money EUR This way you cannot confuse EUR and USD. - - - GADTs: when you have a parametrized type, but you want to restrict the output for some constructors. Typical example is safer encoding of expressions or abstract syntax trees in general. Example: data Expr a where EInt :: Int -&gt; Expr Int EBool :: Bool -&gt; Expr Bool EAdd :: Expr Int -&gt; Expr Int -&gt; Expr Int ENot :: Expr Bool -&gt; Expr Bool EIf :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr a - - - quasi quotes: This is useful for introducing custom syntax. For example you don't want to write expression using the above data type, but want to use a "normal" syntax instead. You can write a parser which converts the "normal" syntax to the datatype above, then use this parser *within* your Haskell source code (not in the same module though), with a QQ-syntax like this: [$myparser| if (not b) then 5+x else y |] or something like that. - - - Type family: there are two versions of that (plus data families also). Standalone type families are basically functions on the type level (but you can only do a very limited set of such `type functions`). You also do this within a type class: class Collection c where type Elem c toList :: c -&gt; [Elem c] This is very similar in power to multi-parameter type classes, but in *practice* not everything can be converted from one to other. See [http://www.haskell.org/haskellwiki/GHC/Type_families](http://www.haskell.org/haskellwiki/GHC/Type_families) for more details, it's quite long to explain (and I'm not an expert anyway). - - - (there may be mistakes in the above, this was written without checking out stuff)
I wanted to try out Idris a few times, but it is very hard to compile (at least on non-linux platforms). I see that there is an OSX binary now, but there should be also a Windows binary build.
I tried to put "export _JAVA_AWT_WM_NONREPARENTING=1" in my ~/.bashrc file before I made this post, but it did not work. You tell me I need to put it in my ~/.xsession file. I'll give it a go when I'm at my work computer next week.
I can't wait to play with the interactive editing!
What? The compiler has nothing to do with it. Haskell has an operational semantics (not formally defined, but irrelevant). Therefore, we know the order in which our code will be evaluated. It can be (and is) reasoned about straightforwardly.
I worked on a project where I had to generate java code from UML-specifications. I had to do it java, but always felt that it is not really nice to write the java generator in java. I thought about making a DSL with Xtext but then moved on to another project. Haskell seems to be the right thing to do such things. I made a git project for the java templating (https://github.com/drever/JavaTemplate.git). I also changed some minor things and implemented the function call rendering. If you want to expand the code feel free to ask!
I think the problem here is that 'control flow' is slightly vague. Most operational semantics give one possible way for a program to evolve, and the compiler should certainly produce code which produces the same effects as the operational interpretation of any source program. But most operational semantics don't include optimizations like strictness analysis (if they did, you'd want to prove them correct w.r.t. a simpler semantics), and so we don't usually take operational semantics seriously w.r.t evaluation order (except as a *lower bound* on performance). So in the sense that you consider *evaluation order* a part of *control flow*, then you don't know in which order your code will be evaluated. Also, many operational semantics have non-deterministic reduction rules (e.g., for parallelism). So the intuition is that purity means evaluation order matters less &lt;=&gt; more possible (correct) reduction sequences/more correctness-preserving code transformations &lt;=&gt; more declarativeness. If you're still unhappy, maybe I'll put that forward as my (still vague) "definition".
The checking in Agda is *not trivial*. I'm hoping that idris does something simpler/more principled. (I'm really just curious)
There can be a Windows binary build if someone makes one! I don't have a Windows machine to make one on myself, however. Nor would I know how to even if I did, come to think of it. If anyone succeeds in making a binary build, please let me know!
Being that its Thanksgiving day, I would like to express how thankful I am for this. I find Idris very fun, interesting, and educational and I am glad it exists. An Emacs mode is real icing on this very delectable cake.
If you're going to include optimizations, then imperative programs don't denote accurate control flow either. Compilers are free to do plenty of reorganisation of code, regardless of the "declarativeness" of the code. 
From section 8 of Peter Landin's seminal *[The Next 700 Programming Languages](http://www.scribd.com/doc/12878059/The-Next-700-Programming-Languages)*: &gt; The commonplace expressions of arithmetic and algebra have a certain simplicity that most communications to computers lack. In particular, (a) each expression has a nesting subexpression structure, (b) each subexpression denotes something (usually a number, truth value or numerical function), (c) the thing an expression denotes, i.e., its "value", depends only on the values of its subexpressions, not on other properties of them. &gt; &gt; It is these properties, and crucially (c), that explains why such expressions are easier to construct and understand. Thus it is (c) that lies behind the evolutionary trend towards "bigger righthand sides" in place of strings of small, explicitly sequenced assignments and jumps. When faced with a new notation that borrows the functional appearance of everyday algebra, it is (c) that gives us a test for whether the notation is genuinely functional or merely masquerading. Then in section 9: &gt; An important distinction is the one between indicating what behavior, step-by-step, you want the machine to perform, and merely indicating what outcome you want. Put that way, the distinction will not stand up to close investigation. I suggest that the conditions (a-c) in Section 8 are a necessary part of "merely indicating what outcome you want." The word "denotative" seems more appropriate than nonprocedural, declarative or functional. The antithesis of denotative is "imperative." 
Yeah, I'm using the documentation in the linux headers, linux/input.h and uapi/linux/input.h, no reason at the moment to copy it over. Also there are a few resources scattered online. If you decide that you want to help, write me a pm and we will talk about this stuff and how we can procede and I'll also write a TODO list tomorrow. 
Hmm, that's inconvenient. Oh well. It's clear that we need to make installation easier for someone who just wants to pick it up and have a go (that's part of the reason for making external dependencies optional and making an OS X package, in particular). This will only get more important as things get more stable and better developed. So it's not that we're forgetting about other platforms, just that the first priority is to get it working at all, then to get it working in more places and more easily installable.
Ultimately, declarative is something of a value judgment more than a technical idea. It's based on what the language feels like to use and is thus as much about psychology as mathematics. Just like obscenity, I can't define it, but I'll know it when I see it^1. However, this does not mean it's completely undefined: we can still come up with mental tools that let us think a bit more systematically about what "declarative" feels like. For this, I think focusing on denotations--as suggested in the post--is a great place to start. We can say that a declarative language is one that has a "nice" denotation. This just shuffles the subjectivity around; mathematical niceness is not exactly a rigorous idea either! But I think it also makes the problem easier because evaluating the simplicity and elegance of a mathematical model seems more approachable than trying to evaluate a language's features. Conal Elliott has proposed replacing "declarative" (or maybe even "functional"!) with "denotative" as in having a nice denotational semantics. I rather like this idea, but I also think that trying to introduce a new word is even harder than nudging the meaning of an existing one ("declarative"). In fact, I think "elegance" in math and physics is similar to our notion of "declarative". Elegance is hard to define, yet most people find it extremely important. Elegance is also often relative rather than absolute: many things are only elegant because of how well they fit with existing theories; without that context, they might not seem elegant at all. Declarative languages are like this because they are implicitly compared to the most common language of the day. Also, does Haskell actually have a definitive evaluation order? My impression was that anything non-strict would work and that GHC is already more sophisticated than plain laziness. (With optimizations like strictness analysis that I'm only vaguely familiar with.) ^1: I actually really dislike this idea of "I will know it when I see it", especially in a court case, because it just punts on actually defining something and because reasonable people will disagree about what they "know". But it's a great illustration of how the definition might be subjective and how subjective things can be really hard to deal with.
How don't they? We can tell purely from examining the syntax a suggested control flow for any Haskell or ML program. Purely _operational_ details exist too, in things like `seq` and the parallel strategy monad.
Haskell has a definitive evaluation order just as much as ML or C do -- Haskell must behave consistently with universal lazy evaluation (barring things like `seq`), even if not all expressions are suspended as thunks. ML and C must behave consistently with eager evaluation, even though some computations could theoretically be delayed or not computed at all if it turns out they're not needed and it makes no observable difference to the program's semantics. It could be argued that ML, because it has an actually defined _eager_ operational semantics, is a bit stronger than that, but most ML compilers correspond to that semantics only up to observation.
But we do know that our code will be evaluated in an order that is observationally equivalent to the evaluation model of the operational semantics of our language, which is _not_ defined by the compiler (we have a formal operational semantics for the unique blend of polymorphic lambda calculi that Haskell uses, which more than suffices as a model for evaluation of haskell programs, and the whole kit and kaboodle for ML).
Sure, I'm saying what order to do things. If I am using ML (because it's a bit easier in this compact example), I could write: let x = a in f x and I am _saying_ "reduce a as much as possible, call the result x, reduce f until you get a lambda, apply to the lambda to x, and reduce as much as possible." This is a _sequence of control flow steps in the operational machine of ML_. Just as in an imperative language, when I write: int x = a; f(x) I am saying "evaluate a, push the value and the current program counter onto the stack and jump to the entry point of f". These are steps in the C (or what have you) abstract machine. There's really no difference between the two on a fundamental level.
Once again, I am not talking at all about compilers, but operational semantics. Operational semantics capture precisely the _semantics_ of our expressions in terms of _machine operations_! The fact that we have an operational semantics for these "declarative" languages means that what we are expressing is still just a sequence of steps on some abstract machine.
But we have other semantics for them, as well - why privilege the operational one? When writing (pure) Haskell code, I usually think primarily about the result of the computation (data) and only secondarily (i.e., when trying to fix space usage) about evaluation.
Partly. There's special functions for comparison (that can do it even without normalisation first), you don't need to store everything normalised. And think about our ideal world with instance Ord Text. Without storing locale inside Text,… oops
toCaseFold? Or anything else. I don't remember anything else...
 h&gt; let [y1,y2]=["\8491","\197"] :: [Text] h&gt; UC.charName . T.head &lt;$&gt; [y1, y2] ["ANGSTROM SIGN","LATIN CAPITAL LETTER A WITH RING ABOVE"] h&gt; TF.print "{} {}\n" [y1,y2] Å Å h&gt; Data.Text.ICU.compare [] y1 y2 EQ 
So without such prominent documentation you expect `toupper(tolower("a"))` to return `"a"`? 
Well, you can't have a proper inverse with idempotent functions, but many people would at least expect `toUpper . toLower . toUpper` to be equivalent to `toUpper` and `toLower . toUpper . toLower` to `toLower`.
The blog post is getting at this in point #7: it is unclear that such a definition really distinguishes imperative languages from functional languages. Expressions in imperative languages also have denotations, and the denotations also only depend on the denotations of subexpressions. The difference seems to be that in functional languages the denotation of an expression is in some sense 'simpler' or closer to the value that an expression computes, than in imperative languages. In functional languages the denotation of an expression is just a function from lexical environment to values, whereas in imperative languages it is a function from lexical environment and heap to value and new heap.
We also have denotational semantics of imperative languages, just like we have denotational semantics of functional languages.
"The RSS version is much faster than the RWS one … except for lists !" Why?
With a state based writer, all `mappend` operations will be with singletons on the right. So that means that you are calculating ((((a ++ b) ++ c) ++ d) ++ e) Each (++) takes time linear in the length of the first argument, so building a big list takes quadratic time this way. With a normal writer you can get a more balanced tree of `mappend`s, depending on the structure of the monadic code.
&gt; So Haskell's semantics admits values for Rec that your code does not and cannot enumerate. But of course if we ignore these cases you indeed succeed at enumerating all finite values of Rec. Yes, good point. Only the *finite* derivations in Rec and Silly will appear in the enumeration. Since Silly has no finite derivations, enumerating them in this way will not work.
For me the informal meaning of "declarative" language is "without the significant sequence". Take for example config files. It does not matter if one section comes before or after another section and within each section it does not matter if one defined parameter comes before or after another one or is even missing completely (default params) In sql statement it does not matter which inner or left join comes after which and it does not matter which criteria in a where clause comes after which. That for me is a trait of a declarative language. 
Exciting! I was just reading over Oleg's page on this topic and would love to see more discussion. In particular, I tend to see language ASTs in initial style still and I'd like to know if it's just a matter of familiarity or if there are practical drawbacks of the finally tagless style.
I'd be interested in Medium Frequency and *maybe* low frequency, if you're willing to give advice there. My motivations are to create a hobby that is challenging mathematically and programmatically but is still approachable without massive resources. I've only recently started my research (I've been reading [this book](http://www.amazon.com/Quantitative-Trading-Build-Algorithmic-Business/dp/0470284889) and watching videos from [this coursera course](https://www.coursera.org/course/compinvesting1)). I'm more interested in it as a hobby than a money making scheme so I'm hoping to implement everything in Haskell. I don't know how feasible that is yet. Knowing which websites and forums to visit regularly might be more helpful than any particular book, however. /r/quant is kind of dead.
I tend to think of "declarative" as being, more or less in spirit, as the same thing as "logic". Not as in "logic language" but as in "first order logic". Raw specifications of properties completely devoid of anything resembling algorithms. In this sense, it's certainly compatible with some amount of what he says there, but only to an extent, because obviously when you write functions, you write algorithms, at least in Haskell.
Wrong. The ẞ got created, because of the problems that replacing it by SS caused, e.g. in names and brands. As I said, it’s rather new, and up until its conception SS was indeed correct. *Was*. Not anymore. It’s just a lowest common denominator fallback now, in case the font doesn’t support it. Barely any Germans heard about that change though. (Who the hell follows such changes anyway? ;) But I have already seen it in advertisements. P.S.: Please provide arguments to support your statements, if you don’t want to look like a troll, religious zealot or simply a really really dumb person. Because actually, your comment did not contain any arguments, so there was nothing to invalidate in the first place, as it already was invalid. I could have said nothing, or just the first word of my comment, and you’d still lost the argument.
In ML, I would imagine the denotation is still heap -&gt; new heap, thanks to refs. 
Right, now we're getting somewhere - but imperative languages have other semantics as well. So, is the definition of a declarative language based on the semantics people prefer to use? This is obviously a domain-specific preference, so I don't see how the term "declarative" has much usefulness, as it doesn't characterise anything particularly neatly.
Just so you know: I upvoted your comment above. It makes good point. You still should not have generalized your original comment so much. As you should have know how it made you look: Exactly as you described. So again, don’t blame your willful ignorance on me. I’m simply assuming the statistically by *far* most likely case, given the way your comment is written. In fact I had a discussion with CmdrTaco from Slashdot, stating exactly what you said in that original comment. And he *meant* his inability to properly filter characters. I’ve seen that with anyone who ever disliked Unicode. He happy; You’re the exception! Act like it!
I'm fairly certain that imperative programmers don't think about the denotational semantics of their programmers (I rarely view sequencing as composition of semantic functions ...). You could probably characterize the 'imperative thought process' as either operational (keeping track of the state) or axiomatic. I'm tempted to suggest that there's something either simpler or more flexible (resilient to code changes? e.g., reordering declarations matters less in the absence of state) about the functional/denotational approach - maybe that it's more syntax-driven? See the Landin quote on this page ... All this might be a mere recapitulating of RWH's thought process leading to the blog post in question ...
I intend to ultimately consider the pros and cons of both. For the most part, I think it's just a matter of familiarity, although when you want to interpret the same syntax multiple ways, an initial encoding is certainly convenient (or, actually, *required*, but it can come in forms that are convenient for mixing with final encodings).
Does it break any laws?
If anyone around here thinks live coding is cool they might want to check out ChucK: http://chuck.cs.princeton.edu/doc/language/
The productivity checker for codata right now is essentially a placeholder for something better. 
I'm sorry about that - I thought that Idris was fairly well known in the Haskell community by this point. I guess I thought wrong. See my first comment for a bit of explanation, and I'll think about this for next time.
We do have a [Windows installation guide](https://github.com/idris-lang/Idris-dev/wiki/Idris-on-Windows) on the Wiki. Did you see that? It's also quite possible that it needs updating, since we moved to Trifecta and gained new dependencies.
But that's in the `text-icu` package. I guess i misunderstood your comment. Thanks for the example, i did not know charname. I had not searched for it though.
As you have also followed the discussion about it, you might have noticed the controversies about that proposed change. Can _you_ provide a reference that it's mandatory? The only mandatory ref i know is unicode which tells it's still 'ss'.
I understand the basic notions of the theory, but I still don't get how you can produce monads such that their algebras form a "nice" algebraic theory. I do understand that T-algebra on lists is precisely the monoid algebra, but I couldn't come up with any other examples of T-algrabras representing some meaningful theory, even though I tried to study this topic myself. For instance, which monad's T-algebra forms a group algebra? Is there an algorithm for constructing monads for arbitrary theories?
Yes, I wonder what a good starting point would be, the yi editor?
In my case the connection with early nineties dance music is highly intentional, it's what I grew up with.
I don't expect it to, as it *should* behave just like state, but I did not verify it formally.
Afraid it's still in the æther. Once I finally finish one of my posts--soon enough, hopefully--I'll definitely submit it to reddit.
Strongly checked, checked, verified, error-preventing, something along those lines.
At least Frege has: - a lang spec and other documentation - an online REPL - an Eclipse plugin - a working, downloadable compiler (in Frege) and standard library - example code for running examples All of this public and open source, of course. Not to offend anyone, but all that is known about ermine is that Edward Kmett said it exists?
Frege does exactly what you want. It can translate almost all of Haskell 2010 to Java **source code**. That being said, the generated code is not intended for human use. And I doubt any pure functional code ever would.
`let` is always `let rec`
That's why you ought to compile with -Wall which will warn you about that problem...
That's one good reason to always compile with `-Wall` or even `-Werror`.
If you turn on warnings you will get a warning whenever you rebind a name.
If you want to do "deep" pattern matches then finally tagless poses a bit of a challenge. For example: f (C1 x y (C2 z)) = ... is really tough to write in finally tagless. I wrote about this a while ago with some helpful examples: [Transforming polymorphic values](http://martijn.van.steenbergen.nl/journal/2009/10/18/transforming-polymorphic-values/); and [GADTs in Haskell 98](http://martijn.van.steenbergen.nl/journal/2009/11/12/gadts-in-haskell-98/)
Which tends to violate beginner expectations a lot. let x = 1 let x = x + 1 print x Infinite loop, not `2` (with the second `x` shadowing the first). let foo = 2 + 3 + 4 Syntax error, not line-wrapped expression. I’d rather have a distinction between singular/plural and recursive/nonrecursive `let`, just so I don’t have to answer the same questions over and over. 
You can always use the identity monad :) runIdentity $ do x &lt;- return 1 x &lt;- return (x + 1)
This is just one of things Haskell does to haze you during your initiation. I have done exactly the same thing. I was working on a multi-threaded app, I thought I had a deadlock somwhere. It turns out it was just an typeo in one of my "let" statements. Wasted several hours. Now, whenever my program freezes, this is **the first thing** I check my code for. Compile with -Wall (as many have already said) and see if that is the cause. Also, monadic notation does not have this problem. For example: a &lt;- return (a+1) will do what you expect and increment **a**. So here is a trick I sometimes use when readability is more important that concise code: myPureFunc :: Int -&gt; Int myPureFunc x = maybe x id $ do x &lt;- return (x*3 + someConstant) x &lt;- return (mod x anotherConstant) x &lt;- return (updateMy x :: Int) return x The **maybe**, **id**, **return**, and **($)** functions are all in Prelude so it works everywhere without any imports. What it does is, **maybe** expects it's third parameter to be of type "Maybe x" where "x" is the same type as it's first parameter, "Maybe" is also a monad so you can use procedural notation. So the type inference will see the procedural notation and understand that you are just writing a long "Maybe" expression where every **return** statement is replaced with **Just**. Of course, concise code is usually the most readable. As you become more familiar with the various coding techniques, you will find yourself relying on code like this less and less. 
`updateMy . flip mod anotherConstant . (+ someConstant) . (*3)` 
Aw come on, don’t get me started about `return`. :P
It does have to do with laziness, because this would have failed much sooner if it was strict. It was hard to debug because the issue (me trying to use the name) occurred way later in my code.
Thank you, I'll use that.
@tomejaguar Yes, I know. I was trying to provide a simple example of how you can re-bind a variable with the same variable name being used on both sides of the left-arrow token. I like writing my equations in point-free style and all on one long line because it is just so cool, and very satisfying to do that. But sometimes I need to consider that other people (or my future self) might not quite understand what I was trying to do and force myself to make the code more verbose for the sake of understanding what happens in the code step-by-step.
My app is multi-threaded and that was partly what made it so difficult to debug.
Actually in my case they usually are recursive. I tend to write my bindings using case whenever I want to be sure I don't introduce recursion, and I've used -Wall for everything for so long that I can't remember the last time I've been bitten by this issue.
One nice result of Haskell's default is that you're free to order definitions however you like, especially at the top level. OCaml wouldn't have had this freedom anyway because bindings can have effects, but Haskell modules take advantage of it very frequently.
Usually I try to set it up so that I'm destructuring something, but that does never introduce a cycle, and makes sure that the few cases that -Wall wouldn't catch due to multiple uses of another variable are handled. e.g. https://github.com/ekmett/lens/blob/master/src/Control/Lens/Internal/Indexed.hs#L244
What you say is basically true, but comes from a different angle than I would. I would say that the final style forces you to think compositionally. Your "Transforming polymorphic values" constant folding example can be written as a final encoding with a fair bit less work than you implied in your post: data Optimized a = Static Int | Dynamic a foldConstants :: Num a =&gt; Optimized a -&gt; a foldConstants (Static x) = fromIntegral x foldConstants (Dynamic x) = x binop :: Num a =&gt; (forall b. Num b =&gt; b -&gt; b -&gt; b) -&gt; Optimized a -&gt; Optimized a -&gt; Optimized a binop op (Static x) (Static y) = Static $ x `op` y binop op x y = Dynamic $ foldConstants x `op` foldConstants y instance Num a =&gt; Num (Optimized a) where Static 0 + b = b a + Static 0 = a a + b = binop (+) a b Static 1 * b = b a * Static 1 = a Static 0 * _ = 0 _ * Static 0 = 0 a * b = binop (*) a b a - Static 0 = a a - b = binop (-) a b fromInteger = Static . fromIntegral instance Var a =&gt; Var (Optimized a) where var = Dynamic . var I haven't thought much about the stepwise evaluation example brought up in your "GADTs in Haskell 98" post. If you would like, I could do so later.
In OCaml I am bitten by shadowing at least as often as I am by recursive let in Haskell. Just writing a locally sensible binding inside some larger expressing can break a reference later in the expression to something bound earlier in the expression. Also, with the possibility of shadowing I can't just find any binding in scope with the name I'm looking for. I have to find the *last* one, which is much more annoying. 
I agree. I think explicit recursion should be discouraged, so when you actually write something recursive you should be punished (by having to write "rec"). 
Being able to shadow bindings (or put declaration in arbitrary order) is only somewhat related to having recursive bindings. You can ban recursion, but still have order in the binding group be irrelevant.
Ah, you're right of course. I concede that my point is pretty much irrelevant. 
Hi, sorry for the curtness of my reply. I know what you were trying to do. I was just trying to point out that if you write in point free style you *don't need to name variables* anyway, solving the problem of shadowing. I should have said a few words to that effect ...
In case there are any haskell newbies around, it might be worth pointing out that the problem is only because of the evaluation order. a ++ (b ++ (c ++ (d ++ e) would be sequential in a lazy language.
I don't know if it's just me, but I would've found let much clearer there: Indexing mf &lt;*&gt; Indexing ma = Indexing $ \i -&gt; let (j, ff) = mf i (k, fa) = ma j in (k, ff &lt;*&gt; fa)
I've had many of these errors too, when I started out.. turns out I just needed to redefine the idea of a 'variable' in my head.
Or http://supercollider.github.io/ together with https://github.com/miguel-negrao/FPLib :-)
Interesting trick. Looks much better than the lambda hack I used to use: let (|&gt;) = flip ($) in 1 |&gt; (\x -&gt; (x + 1) |&gt; (\x -&gt; ...))
Perhaps, but then had I accidentally reused a variable then I'd have a silently cyclic definition. The version I wrote is much closer to the core that it generates, and I spend a lot of time reading core, so it, perhaps, has warped my tastes a bit. I also personally tend to avoid 'let ... in' simply because I can't come up with an indentation scheme for it that I like. Shallow? Yes.
I've only run into this in the repl. Haskell's repl betrays quite a few reasonable expectations, unfortunately :(.
We are advertising 10 fully-funded PhD students at the University of Nottingham in the UK. Applicants in the area of functional programming (www.fp.cs.nott.ac.uk) are particularly encouraged! -- Graham Hutton
It turns out that Template Haskell seems to do the things I wanted to do there. Here is a [tutorial](https://github.com/leonidas/codeblog/blob/master/2011/2011-12-27-template-haskell.md). I guess I will look more into that instead of writing everything from scratch.
Even then, I think myPureFunc x = updateMy ((x + someConstant) * 3 `mod` anotherConstant) is more readable. (I mean, than the `maybe x id` hack -- pointfree is probably better here)
No problem, no offense taken.
Argh, my intuition is failing me. I can't see why with a lazy writer the associativity would change.
I don't think that will work, with all the dependent external libraries which cause the main pain here (maybe there is also some dreaded diamond dependencies with the haskell deps, I don't really remember)
Thanks guys for doing this, and I don't want to sound unappreciative, but some of the questions about applicatives early on didn't really make sense and seemed a bit obtuse. Monads, even with an applicative superclass, don't _inherit_ any behaviour from applicatives. You can't define `&gt;&gt;=` in terms of `&lt;*&gt;`, but you can define `&lt;*&gt;` in terms of `&gt;&gt;=`. It seemed like the hosts got themselves confused and it was a bit frustrating to listen to. Mind, I may have misunderstood what the hosts were asking.
I've seen it, but I think I disregarded it... (yes, I know...). When you have both mingw and cygwin, and then ghc's own mingw on the top of that, things start to be unpleasant. Trifecta I have installed according to ghc-pkg, maybe it was libffi where I gave up. Honestly I don't remember, but in any case it's too much pain, especially for just trying out. I understand that making binaries for every platform is not your favourite evening program, but normal users will at the best case try "cabal install idris", if they already use Haskell, but probably not even that. So if you want users, you should provide binaries.
&gt; TL;DR: GHC HEAD (but not GHC 7.8) will soon support OverloadedRecordFields, an extension to permit datatypes to reuse field labels and even turn them into lenses.
A question: looking at this example from the OP— nameDefined :: (r { name :: [a] }) =&gt; r -&gt; Bool nameDefined = not . null . name —does this only work if there is a record with a `name` field in scope? Or does the `r {name :: [a]}` in the signature cause `name` in the definition to have the special record-lookup behaviour? (Instead of the '"name" not in scope' behaviour.)
It's worth pointing out one library that already goes a step further with very nice extensible records. https://hackage.haskell.org/package/vinyl
I was surprised someone said they had to invent live coding, since it's been happening for decades with supercollider :( but I love the fact that this is getting some traction and is using Haskell. Props.
&gt; You can actually force other people to write correcter code. This is a great observation.
How does this compare to Ermine's row types? Adam's extension has often been described around here as very conservative. That leads to the obvious question of "what does Ermine allow you to do that OverloadedRecordFields does not?" Very excited about this extension BTW.
These new record constraints look like a nice approach to structural typing in a Haskell-y way. Can you define a constraint for multiple fields simultaneously, e.g. `(r { name :: String, age :: Int }) =&gt; ...`? It's a pity the record update syntax isn't generalized by this extension, is there any hope of that making it in eventually?
There is a reference to a capital eszett in this Wikipedia article: http://en.wikipedia.org/wiki/Capital_%C3%9F 
The point is that if Functor -&gt; Applicative -&gt; Monad then each typeclass would only need to implement its unique behavior (`join` in the case of Monad) and could "inherit" the rest from its superclasses. For instance, `a &gt;&gt;= f = join (fmap f a)`. The proposal outlining these changes is http://www.haskell.org/haskellwiki/Functor-Applicative-Monad_Proposal
but ẞ is included. even duden says, that `SS` is proper capitalization of `ß` in all but geographical names... that might change in the future of course.
The REPL (GHCi) is just run inside an implicit IO do block.
Yes, the syntactic sugar supports multiple fields. The difficulty with record update is that the traditional syntax is so expressive, it's hard to generalize! We could could support a fragment of it, e.g. single but not multiple record updates (so `e { x = t }` is polymorphic in the type of `e` but `e { x = t, y = u }` is not). It's not clear where to draw the line, however.
They actually raised an interesting question though perhaps not worded in the clearest way: are there applicatives that are not the applicative portion of a monad? The answer is yes.
Well, if we can ever get around to extending the assignment sugar to use the [`exposed`](https://github.com/ekmett/lens/issues/197) machinery we talked about, but that should definitely be kicked down the road. =)
&gt; I like to think that my one trick is taking everyone else's tricks and making them my own. When you're not computer-sciencing, do you do impressions? That's an uncanny rendition of my old dissertation advisor.
Ah, this looks really awesome. I haven't played around with supercollider at all, but it looks worth digging in to. Slightly unrelated, I've seen people live coding with csound even. And at some point I saw and embedding of csound into an ableton rack.
I don't see how this Cython version is more high-level than C. It's almost C. 
I wonder how true the wording is in that quote. Just about every time I've done a band/music interview the final result was so paraphrased and hacked around I didn't really say any of it. I'd wager that the actually quote was originally more like "Live coding didn’t really exist, in Haskell."
I updated the post, and they are indeed faster.
&gt; (Has r "name", t ~ FldTy r "name") I find this a bit odd. What is `FldTy r "name"` when `Has r "name"` is not satisfied? Doesn't the, uhm, 'existence' of `FldTy a n` already imply `Has a n`? 
Terrifying. Subtyping ahoy!
It hurts in GHCi, because it leads to an infinite loop. One could ask the question if the runtime could do better. Whenever an expression must be evaluated in the evaluation of that same expression, there is surely something wrong, and the runtime could shorten the non-termination a bit. 
So to be clear, `name` can't be a function with extra logic in it? If your code requires smart constructors you can't write getters/setters except in the module where the data declarations are in scope? Thus these new records can't be composed? For example: data Person = Person { name :: String, yearOfBirth :: Int } f :: r { age :: Int } -&gt; ... so a smart getter `age` which computes the current age based on `yearOfBirth` can't be used as a record constraint? It's a neat first step even if that's the case. EDIT: Wait, since the record is a typeclass, you could probably create another typeclass which exports the `age` function. class HasAge a where age :: a -&gt; Int f :: HasAge a =&gt; a -&gt; ... I can't tell if this is a good way to do this or not.
Not sure what this has to do with explicit recursions?
The ``collatzLen`` function is straight C in the Cython case. It's not a fair comparison to Haskell, unless we write that function in C and call it via the FFI since that's basically what Cython does.
I have been feeling the very same thing about the indentation for 'let ... in'. 
How does that play with type inference (in Ermine)? My knowledge of the area comes mostly from Daan Leijen's Morrow papers, which didn't deal with concatenation
Well, the simplest story you can tell about `let` vs `let rec` in OCaml is that with `let rec` the names you're binding are in scope on the rhs of their definitions, and then in both cases is in scope from the end of the binding group onwards. It would be quite a bit more complicated for the rule to instead be that a name bound without `rec` is in scope both above and below its binding except in the bodies of its (transitive) dependencies. Now that I think about it though, nothing forces you to think of `rec` in terms of scope. You could simply get an error saying "I've detected a recursion that you weren't explicit about, please use `rec` or reconsider your code". 
I can't get Haskell to be anywhere near as fast.... What are you doing that I'm not?
It doesn't. What he meant was that they were both equally slow because the performance penalty of the left-associative list concatenation outweighed the penalty from laziness.
however ghc -O2 -fllvm -fforce-recomp collatzhs.hs ... and $ time ./collatzhs 1000000 (329,837799) real 0m0.265s user 0m0.260s sys 0m0.003s 
Well, thanks for sharing. I guess.
Are you talking about the specific program in the blog post? The post is pretty specific, what are you not doing that he is? :) One thing that will make a big difference is using `-fllvm`: GHC's native code generator doesn't know how to implement `even` as a single `test` instruction and instead emits some awful integer division instruction sequence. I haven't looked at the assembly in this case but I would guess that that contributes a large amount to the difference between `-fllvm` and the default `-fasm` observed by /u/corvinusz.
No parallelism? 
You cannot let *x = x + 1* if you don't mean it.
I think he did the C version first in this post : http://www.reddit.com/r/haskell/comments/1rqf37/an_example_of_fast_numerical_computation_using/cdpz3f6 with a time of 0m0.203s.
You can use this feature just for better name resolution, without ever writing any record polymorphic functions.
Is there an example where I can see Ermine doing joins, intersections etc.? I'm curious as to how the types work out ... 
The way we did it was a key to making a decidable inference algorithm. It plays quite nicely with inference, though asymptotically in terms of compile time there are some corner cases.
Dan Doel gave a nice demo using it in the middle of http://www.youtube.com/watch?v=QCvXlOCBe5A
I'm pretty sure that's not what I'm supposed to do in /r/haskell but I can't help it, so I made the C version faster. There's a memory trade-off though. The idea is that once you hit a number you've already checked in the sequence, you already know the length from there down to 1. I'm pretty sure the same technique can also be used in Haskell to speed it up. http://lpaste.net/96397 I spent just a couple of minutes on it so there might be a bug somewhere. Measurements: time ./original 1000000 (329, 837799) real 0m0.183s user 0m0.180s sys 0m0.001s time ./new 1000000 (329, 837799) real 0m0.017s user 0m0.015s sys 0m0.001s However, I'd like to say that this is not exactly a good example or a meaningful benchmark. 
Aaah, I must have missed that. Thanks :) EDIT: hmm, for me the C implementation is still quite a bit faster...
You could make a `HasAge` typeclass yourself, but you would have to write trivial instances for every datatype with an `age` field. What would be nice is to specify such "virtual record fields" by giving instances of `Has` and `FldTy` yourself, like this: type instance FldTy Person "age" = instance Has Person "age" where ... This is currently forbidden, but we could imagine lifting the restriction. The trouble then is that it does not bring `age` into scope, as you identify.
That's what I would've expected. Some syntax for record projection would be nicer, though, you're right. But this is still tons better than what we have right now, so, thank you for making it work! :)
Why isn't it another parameter of `Has` with a fundep, I wonder?
See Chapter 7 ("GHC Language Features") of the GHC manual.
it _is_ what cython does.
you missed a `Elem` in the third row. nice example and good explanation!
I see the difference (I know that in the second assertion `~` is sort of a grokked typeclass), but if `FldTy a n` has some value (namely the type of the named field), wouldn't `(t ~ FldTy r n)` imply `(Has r n)`, making `(Has r "name", t ~ FldTy r "name")` a pleonasm?
See also [Fun with Type Funs](http://research.microsoft.com/en-us/um/people/simonpj/papers/assoc-types/fun-with-type-funs/typefun.pdf).pdf
It is interesting that you got such a speedup from `stream-fusion` given that `base` uses build fusion which I'm told should have similar effects, although with varying trade-offs between the two. On the other hand it seems /u/corvinusz got similar speedups with the `Data.List` version.
But sometimes the only constraint you want _is_ a field with a name and a type! You can build more restrictive things on top of this, but you can't do the converse.
&gt;a_n+1 = a_n/2 if an is even, else (3a_n + 1)**/2** &gt; This is a slightly optimized version of the sequence in the Collatz conjecture. (Emphasis mine.) Because the number after an odd number is always even, you can always divide it by two without affecting termination.
What are those `Ptr Int` arguments for? Can you post the C code?
Why not have an explicit combinator for recursion? let x = loop $ \self -&gt; 1 : self
https://gist.github.com/llelf/7722337
Did you consider http://hackage.haskell.org/package/lrucache? I note it isn't in your benchmarks. Also, your code would be much more useful for the haskell community if it was on hackage (says the person who does exactly what you did).
I did, in fact I link that package and mention in both the documentation and source that LRUBoundedMap_LinkedListHashMap is in based on / inspired by it! ;-) Since that container is virtually identical to what I benchmarked (but uses the slower Data.Map instead of Data.HashMap), I didn't think it was important to include it. The only reason that this is not on Hackage is that I'm not sure if it's universally useful. The feature set and performance is clearly tailored to my use case. If some haskellers think it's useful, I would consider releasing the CustomHashedTrie on Hackage, as it is the fastest and most polished of the implementations.
Ok, just seems important to me to confirm success, and the scale of such.
This was a great talk showcasing how monads and typeclasses help organize e.g. varying security policies across different clients / different roles in a client's organization. And there are a couple of hilarious quotes. &gt; For this particular case, we found ourselves leaning on the type system of Haskell so heavily that, you know I've spoken to everybody on my team and we all agree that we could not have done this as badly as we did without the type system.
I'd agree in principle, but the lrucache uses Data.Map, and needs on average five O(log n) operations from it for doing a lookup + updating its internal LRU linked-list. In the benchmark results you can see that a single lookup in a plain Data.Map already takes significantly longer than the entire lookup + LRU update for the CustomHashedTrie. So that's why I didn't look much further into this data structure. I mention in the docs that this approach + a mutable hash table + pointer links would likely be significantly faster, if one can live with a non-pure design. I also can't use that container unmodified as its insert function does not return which element it dropped. It would be easy to write a quick wrapper which does that, of course.
This might be competitive, but I am not sure. import Data.Word import Data.Bits collatz :: Word -&gt; (Word, Word) collatz k = go k 0 0 0 k where go 1 !max !maxlen !_ !_ = (max, maxlen) go !n !max !maxlen !curr 1 | (curr + 1) &gt; maxlen = go (n-1) n (curr + 1) 0 (n-1) | otherwise = go (n-1) max maxlen 0 (n-1) go !n !max !maxlen !curr !x | x .&amp;. 1 == 0 = go n max maxlen (curr + 1) $ unsafeShiftR x 1 | otherwise = go n max maxlen (curr + 1) $ unsafeShiftR (3 * x + 1) 1 main = print $ collatz 1000000 
I think I can see where this concern is coming from. If all record-using functions become record polymorphic (because that is the lazy solution), and short, generic names are commonly used for field names thanks to clever name resolution, then we could end up with a lot of code that *happens to work* thanks to massively overlapped name usage. You see this in other languages all the time. The good news is that this doesn't have to happen. Just do the right thing: include type signatures on top level definitions. If you do this, you won't be surprised by the types of values your function is working with, but you will be able to use non-prefixed field names without having to put in much effort. Haskellers value static checking too much to accidentally give it up.
There's also the Han unification controversy, which is partly an artifact from when Unicode was supposed to be a 16-bit fixed-length encoding. 
What is the Idle GC bug he refers to?
The hacker news thread (https://news.ycombinator.com/item?id=6822901) about this post brings up a question about why the stream-fusion library isn't default. Are there cases where it's meaningfully worse using the fusion library? (I get that there may be overhead with very small list use cases). Should I just choose to use the stream-fusion version of list by default for all problems I'd use a normal list for?
Uh, we have that. import Data.Function let x = fix $ \self -&gt; 1 : self main = print $ take 10 x `let rec` is usually defined in terms of `let` and `fix`, though not necessarily implemented as such. 
Honest question: what would you use this kind of setup for? Are there advantages to just installing GHC the usual way?
This is a variation of what happens any time you have two types whose representation is the same, but you want to make a nominal distinction. You might `newtype` an `Int` just to make sure that particular uses are intentional rather than just type safe on a low level (e.g. age vs temperature vs distance in millimeters, etc.).
I like to have as much control as possible over my compilers and what they come with; when you use repositories, there's also that third managing party around. A couple of scenarios: 1. In a month or so, 7.8 will be released. I know exactly where my files are, can bootstrap 7.8 with 7.6, and then delete 7.6 without any traces. 2. When 7.8 is out, I may also want to have a 7.6 installation alongside (as fallback or for testing). Using the repositories usually means you can upgrade to, but not add, a new version. 3. Repositories are usually pretty outdated/"stable" - someone else made the version choice for you, but sometimes that's not what you want. 4. Knowing how to compile GHC may be useful in case you want to change something one day :-)
&gt; the instances of Has and FldTy are generated automatically by the compiler, and cannot be given by the user Does this mean libraries like vinyl or HList, which have their own implementations of a "Has" class, will not be able to make use of the `r { name :: [a] } =&gt;` sugar you're adding? Also I will point out that the "type changing lenses" issue/complexity is not a problem with lenses provided in HList (http://hackage.haskell.org/package/HList-0.3.0.1/docs/Data-HList-Labelable.html) 
 type W = Word32 next a = Just . join (,) $ (if even a then a else 3*a+1) `div` 2 len :: W -&gt; W len 0 = 0 len i = fromIntegral . V.length . V.takeWhile (/=1) . V.unfoldr next $ i resultQ :: W -&gt; (W,W) resultQ n = foldAllS max (0,0) $ runIdentity $ R.computeUnboxedP vec where fun (Z :. i) = len &amp;&amp;&amp; id $ fromIntegral i vec = R.fromFunction (Z :. fromIntegral n) fun :: Array D DIM1 (W,W) http://lelf.lu/files/fast-num-haskell-bench-N2.html Next step: accelerate? But nvidia drivers hate me today it seems. 
Contributing to small (such small that it is possible to study the code in 1-2 evenings) projects.
I don't know any way to do thus with ghc. But I know of one implementation that provides this feature. 
I had this vague idea a long time ago that there should be a kind of context you could use for this kind of stuff, something like a "special" implicit parameter you could import the name of from a Debugging module. Something like (?Debug.Wherever.codeContext :: Debug.Wherever.CodeContext) (I'm not sure if implicit parameters can be given module prefixes, but anyway.) It would be special in the sense that any function which wants this context in its type must have an explicit type declaration. If any other function uses a function with such a context in its type, without itself declaring a type with it, then the function used gets the implicit parameter automatically passed in with a value appropriate for its use site. My point here is that this would *allow* you to explicitly declare such a context, thus creating a function which can give messages depending on the site of its *caller*. Some examples for how it might work to define this for some functions that I'm sure many people have wanted to work this way: undefined :: (?codeContext :: CodeContext) =&gt; a undefined = error $ "Undefined value at " ++ show ?codeContext head :: (?codeContext :: CodeContext) =&gt; [a] -&gt; a head (x:xs) = x head [] = error $ "Head of empty list at " ++ show ?codeContext The intent here is that if you take `head xs` where xs is an empty list, you'd get the file and line number of where you actually used "head". (Although you *should* usually use pattern matching instead, in case anyone objects.)
Why must you taunt... :)
What location would here' :: (Loc -&gt; a) -&gt; a here' = id . here give? The location of the declaration of `here'` (where `here` was used)? Or the location of the use of `here'`?
&gt; ... and the documentation step when installing new libraries is usually much shorter than the actual compilation. Heh, I have over 200 packages installed and the "Updating documentation index" step often takes longer than the rest of the build process combined. I should try to fix that some day...
is the cabal haddock step parallel yet?
 funcs = [length, reverse, head] results = map (\f -&gt; f []) funcs Where would the "usage of head' be tagged. When it is used by name, or when it is finally applied?
&gt; The OverloadedRecordFields extension does not attempt to generalise [record update] syntax, so a record update expression will always refer to a single datatype, and if the field names do not determine the type uniquely, a type signature may be required. Just wondering what the reasoning is for this decision. If polymorphic accessors are so valuable, why not polymorphic updates?
I'm guessing there would be some crossover between Symbols and DataKinds, can you give some explanation as to when you would prefer Symbols?
The location of `here'` .
But you can't export macros from a module (shadowing they are macros to the outside) I believe. Currently I'm thinking about: - making `here` be `(Maybe Loc -&gt; a) -&gt; a` - importing a dummy `here` always passing Nothing - running a custom preprocessor step on my codes to replace `here` with the appropriate CPP pieces. Still not satisfying though.
Some clarification would be welcome on that point. Which parallel framework he refers to? As a random data point, I tested Monad.Par with GHC 7.6 up to 24 cores, and it seemed to scale nicely.
Perhaps we should have two pragmas, one that gives you the full deal and one that only gives you the improved name resolution?
Simon Hengel was working on [REWRITE_WITH_LOCATION](https://github.com/sol/rewrite-with-location) — I think that would solve your problem. [Here](https://github.com/sol/ghc/commits/rewrite-with-location) is his branch implementing REWRITE_WITH_LOCATION in ghc, but from what I can see it hasn't been merged into ghc trunk for some reason (and so probably won't be in 7.8).
The first three screnarios don't require compiling your own GHC. When installing a binary release, you can say `./configure --prefix=...`.
Write a "real" program with a Haskell framework like [yesod](http://www.yesodweb.com/). Either re-implement something you've done before to see how to your existing mental models apply with these platforms, or start from scratch with something not too complex (e.g. a Twitter clone).
I don't see how changing the algorithm on the control lets us evaluate optimisation techniques on the other implementation...
In case anyone hasn't mentioned this yet, the reason that the binary is so much larger is because the Haskell version has to carry around the entire haskell runtime environment.
Perhaps he is referring to the scaling limitations of the pre-7.8 I/O manager http://haskell.cs.yale.edu/wp-content/uploads/2013/08/hask035-voellmy.pdf
&gt; Is somebody trying tot **re-implement Haskell on the type level** ? (And has anyone started re-reimplementing it on the kind level? :P) for a very long time now, yes - haven't you seen this yet? re kinds, they're talking of collapsing kinds and types into typekinds in the future.
nope, maybe row polymorphism, but not subtyping. The former is much more benign.
only, with O(n) access in the size of the record, rather than O(1). 
It's a lot like strings versus sum types. Sum types can offer more safety via more static information, but they mean you have to encode everything up front. We might write a sum type for the names of the week days, but we'll probably use a string type for filenames. We could write data Symbol = Age | Name and promote that to a kind, but then we'd have to do that for every record field we ever use, and we'd have to import each of these symbol kinds whose fields are in use, and we'd need something like a type family to map record types to symbol kinds. In the end the whole exercise is futile because we haven't actually solved overloading of records - the `age` field uses a specific `Symbol` kind and can't be used with any other record, and what's more we'll still need some way to disambiguate `Age` from any other `Age` that might be part of a different kind.
C has allowed defining variables sensibly, rather than on top since 1999. Why do the silly thing of listing all variables on top?
I forgot about that. When you have lots of libraries the process takes longer, that's true. Edited.
I think the idea is that you're always expressing a denotational semantics and an operational semantics simultaneously. If the former is simple and direct and the latter complicated and indirect, you're "declarative". If it is the opposite, you're "imperative". Haskell's operational semantics are far more complicated and indirect than C's, while the denotational semantics are much simpler and more direct (at least when avoiding the imperative subset).
The whole libgmp.so.3 issue is not an "end-all".. For example, when I compiled ghc 7.6.3, I ran into this issue, in spite of having installed libgmp3. However, the only thing you need to do is create the proper symlink, and voila. This was on ubuntu 12.04. 
Good point, I added annotations for things you only have to do if the binary download doesn't work.
The [haskell wiki on building](https://ghc.haskell.org/trac/ghc/wiki/Building) is very good. In comparison, this article is bad. It's got some technical flaws, and not as well written. The haskell wiki is also a good place to contribute to. Also, for newbies. On ubuntu 13.10: use package manager, install haskell-platform. The end.
How does this differ in practical terms from full-blown record subtyping?
At least two implementations provide it... :)
Since macros live at the CPP level, they know nothing about modules. You have to use #include
Rather than manually rewriting `head` and every other function that calls `error`, one could use [`mapException`](http://hackage.haskell.org/package/base-4.6.0.1/docs/Control-Exception-Base.html#v:mapException) to attach location data to the exception produced by `error`.
Aha very interesting. I'll have to read the blogpost. Also, four spaces at the beginning of a line (and ``'s line-internal) let you write in monospace.
The same thing as this extension but with explicitly declared ("nominal") overloaded record fields. E.g. record field name -- produces something like -- class HasName a where name :: ... data Person = Person { id :: Int, name :: String } -- produces something like -- id :: Person -&gt; Int; id = ... -- instance HasName Person where name = ... Then `name` is a properly namespaced thing, and so I can only write a function polymorphic over two record types `T1` and `T2` if they imported a record field declaration from the same place. That also gives me a single place to document the expected meaning of the field. If two libraries have different ideas about what `r { temperature :: Double }` means then it's hardly sensible to write a function polymorphic over records from both libraries! That's the behavior I generally expect from the Haskell type system: if I have interoperability between two libraries, it's because they imported their types and type classes from the same places. Not because they happened to pick the same name for their types or their type class methods. That's like the Python-style duck typing that I think TheGoodMachine is alluding to. 
thx, fixed
These `Has` instances don't obey the normal rule for exporting instances, right? (namely that any instance visible in a module is also visible in any module that imports it) I guess that seems okay given that you can't write your own `Has` instances, but if you could it would quickly lead to coherence issues.
I published our QuickFIX bindings and code generator (note: old but works) a while back.. https://github.com/alphaHeavy/quickfix-hs
You probably meant [`parBuffer`](http://hackage.haskell.org/package/parallel-3.2.0.4/docs/Control-Parallel-Strategies.html#v:parBuffer) rather than [`parListN`](http://hackage.haskell.org/package/parallel-3.2.0.4/docs/Control-Parallel-Strategies.html#v:parListN), but `parBuffer` didn't give me any speed-up either. These individual `collatzLen` computations are too small to effectively spark individually in any case. Better to manually split the range [1..1000000] into a reasonable number of subranges and compute the maxima of the subranges in parallel, or use a divide-and-conquer parallel algorithm that bottoms out in a serial computation at some reasonable subproblem size. I don't know for sure but I assume Repa does something like the latter for you.
I don't totally disagree but note that your "more idiomatic" suggestion is wrong :) while the list comprehension is the sort of thing that's very easy to get right on the first try. I think list comprehensions get more hate than they deserve.
&gt; Also the build guide is much less to the point for my use case (and particularly longer). To do a basic linux build, a the GHC wiki documentation is much more to the point and shorter, and complete. The crucial sections are quite obvious and I count 446 words compared to this link having 959. I don't know your "specific use case," but I don't think you've made an accurate general statement about the GHC wiki build guide. https://ghc.haskell.org/trac/ghc/wiki/Building/Preparation/Linux https://ghc.haskell.org/trac/ghc/wiki/Building/GettingTheSources https://ghc.haskell.org/trac/ghc/wiki/Building/QuickStart 
Ooo. That's clever. Are there any good examples of doing that? Might be very handy for me soon. 
That symlinks .so.3 to .so.&lt;installed&gt; though, in other words it points at something other than it should. At some point things may break, and nobody knows why. (In Haskell terms: this is the `unsafeLink` solution.)
I'm not certain this is what he's referring to, but we've found that the idle GC, for some concurrent applications, causes significant CPU load even when nothing is going on. An example is when you have a threaded web server on very low request volume. Passing -I0 to RTS completely got rid of this unexpected behavior in a few cases we've encountered at work.
Oh, I saw that one as a binary download too. That is what had me confused.
I think this is it: https://ghc.haskell.org/trac/ghc/ticket/915 Duncan's thesis has some possible solutions from what he said, but using Stream Fusion hits a wall in some cases, particularly ones like: let foo = doListThing (xs, ys) = unzip foo in ... do things with foo/xs/ys ... IIRC. This is because stream fusion doesn't really work well when you fuse a producer into multiple consumers, like in the example. Ben Lippmeier has done some work on 'Data Flow Fusion' which solves this problem in the new versions of Repa, for example. You can find a paper here: http://www.cse.unsw.edu.au/~benl/
&gt; A package manager installs to a local folder. Aptitude is a package manager. Aptitude installs in global folders. (At least by default. My apt-fu is weak.) Other than that it seems like you're largely writing in anger, and I'm sorry for upsetting you. I often like to have guides in addition to the official documentation; in case that one does not mention a strange error I encounter, it's good to have other perspectives on the same thing. For example it doesn't seem to be so easy to automatically install documentation for all your libraries. (Try wiping your .ghc/.cabal, then `cabal install haddock` -- no documentation for it or the deps will be generated, even if you've reinitialized your cabal config to build docs. Haven't found an explanation for this yet. If I do, I can't add it to the GHC build guide because it's not GHC. It's an issue somewhere in between GHC, Cabal and Haddock, and there doesn't seem to be a right place to put it.)
If you don't want to do all this, you are welcome to use [FP Haskell Center](http://www.fpcomplete.com) which runs in the cloud install-free.
Yes.
Right. An instance is visible exactly when the corresponding record field is visible, to maintain module abstraction boundaries. Writing your own `Has` instances would indeed be dangerous (though not as dangerous as writing your own `FldTy` instances, which could cause inconsistency). That's why they are currently forbidden.
I should have been clearer about update in the blog post! Polymorphic update **is** supported, with some limitations (you can update only one field at a time, and there are restrictions on the type of the field). Because of these limitations, we don't generalise the existing record update syntax: instead, you can use some sort of lens to do polymorphic update if you want it.
&gt;When 7.8 is out, I may also want to have a 7.6 installation alongside (as fallback or for testing). Using the repositories usually means you can upgrade to, but not add, a new version. This is exactly why you should use a *purely functional* package manager such as [Nix](http://nixos.org/nix/) or [Guix](http://www.gnu.org/software/guix/).
Unfortunately I haven't used NixOS, so I have no idea what awaits me when I install it on my main computer. Standard distros (Suse, Debian, ...) provide me with a certain sense of comfort that lots of people have used them, and many problems I'll discover are already solved somewhere on the internet. I'm past the point where merely setting up my system is part of my hobbies, I want a system that works reasonably well out of the box so that I can tweak is when I feel like it (and never *have* to). I think that's the main reason most people don't try out new and interesting distros. You can of course previre an OS by using a VM, but I can never take that as seriously as having it actually installed and depending on it. Video is flickering in the VM? Whatever. Video is flickering in the OS? Better fix that.
Glad this is happening again!
Thanks for doing this, I'm very excited!
Something I've had fun with over the last few days. It scratched an itch and now that I've mentioned it you might find you have an itch there too. Comments, critique, etc gratefully received.
This is a trivial example, but the point is that func by name and func by application may be far apart, may even travel through many intermediary functions. * context by name use: you know where the name was used, but you don't know where it was finally applied, or where its arguments came from. * context by application: you know what function, and you know the arguments, but you dont know where that function came from. Either way, without having some way to tag and trace the flow of values through your program, you end you with uncertainty in either call site or bind site.
 update_person id name p = let p' = p { id = id } in p' { name = name } Ive just updated two fields seperately, using polymorphic field update. Would it type check? What would be the type of the function? Would it be thus? update_person :: (r { id , name }) =&gt; r -&gt; r
We try to minimize the performance penalties in `vinyl`. For small records, there should be no overhead at all (see the benchmark in the package).
Nice!
After seeing all that projects that interact with git repo via `readProcess "git"` I wonder if there some git binding that allow you to parse whole repo to a single monstrous GIT_DATA in `IO` and then work with it purely. With nice functions like `branches :: Git -&gt; [Branch]`, `blame :: Git -&gt; File -&gt; (Revision, Author, String)`, etc. There definitely should be some
It's been a year already? Wow.
This is exactly what we saw, as well. It seems to be caused by having a large resident set with intermittent usage (perhaps triggering expensive GCs frequently). We didn't investigate the issue much beyond determining that -I0 eliminates it.
I have two cliches for you: "Ask and ye shall receive", and "be careful what you wish for". module Main where -- This was just discussed, so I assume you already know what it does. loeb ss = go where go = fmap ($ go) ss evenCell i ss = ss !! (i `div` 2) + 1 oddCell i ss = ss !! ((3 * i + 1) `div` 2) + 1 cells = const 0 : const 0 : zipWith ($) (cycle [evenCell, oddCell]) [2..] main = do print . maximum . take 1000000 $ loeb cells It doesn't quite meet the spec, but it's close enough. After spinning for several seconds, it tells us what we already knew: badcollatz.exe: out of memory
It's not clear that this is an api limitation, but rather an implementation shortcoming. Do you have more resources pointing to this problem? It's something I'd probably be willing to send a pull request to vinyl.
Well, I can fix this. I'll just use a list of arrays. module Main where import Data.Array import Data.Word type W = Word64 data IL a = IL W (Array W a) (IL a) deriving Show instance Functor IL where fmap f (IL len vs next) = IL len (fmap f vs) (fmap f next) fromList as = go initialLength (split initialLength as) where initialLength = 1024 go len (bs, cs) = IL len (fromList len bs) (go (len * 2) (split len cs)) split len = splitAt (fromIntegral len * 2) fromList l = listArray (0,l-1) toList (IL _ vs next) = elems vs ++ toList next (!?) (IL len as next) ix = if ix &lt; len then as ! ix else next !? (ix - len) loeb ss = go where go = fmap ($ go) ss evenCell i ss = ss !? (i `div` 2) + 1 oddCell i ss = ss !? ((3 * i + 1) `div` 2) + 1 cells = const 0 : const 0 : zipWith ($) (cycle [evenCell, oddCell]) [2..] main = do print . maximum . take 1000000 . toList . loeb $ fromList cells Alas, something is being overly strict, I think: poorcollatz.exe: &lt;&lt;loop&gt;&gt;
If memory serves correctly, than 24DoH is how I got involved in Elm. Such a great idea and I'll be doing it again!
Silliness aside, this works: module Main where import Data.Array.IO import Data.Word collatz :: IOUArray Word64 Word64 -&gt; Word64 -&gt; IO Word64 collatz a i0 = go a (advance i0) 1 where go a i len = if i &lt; i0 then then fmap (len +) (readArray a i) else go a (advance i) (len + 1) advance i = (if even i then i else 3 * i + 1) `div` 2 main = do a &lt;- newArray_ (0, 1000000) writeArray a 1 0 let loop i (maxVal, maxIx) = do val &lt;- collatz a i writeArray a i val let mm = if val &gt; maxVal then (val, i) else (maxVal, maxIx) if i &lt; 1000000 then loop (i+1) mm else return mm print =&lt;&lt; loop 2 (0, 1) There is still a speed problem, though. I'm too lazy to track it down: $ time okaycollatz.exe (329,837799) real 0m0.510s user 0m0.015s sys 0m0.000s
Did you even read the links? Nix works fine on several distributions, including Debian as you mentionen. You can download a binary package [here](http://hydra.nixos.org/release/nix/nix-1.6.1).
Thanks very much! I really enjoyed last year's installment and I'm looking forward to the next :)
Also note (since I felt a negative tone to in your reply), that I mentioned it simply to point out that what you work around in the tutorial is a solved problem by academia. I also found it funny that it's solved using functional principles as this is /r/haskell/. Did not meant to sound like a "use gentoo it's 1337!!1!!" kid.
The sad thing is that neither of the uses here require any internal knowledge of git whatsoever. One finds the relative path to the nearest .git directory and the other finds the named reference for HEAD, both of which can easily be managed in a few lines of code with native Haskell. 
it has been said that the reasonable limit is `O(log(n))` though. which might of course be ok.
This would be monomorphic in the extension as currently implemented, because traditional update syntax is still always monomorphic. To get polymorphic update you would need to rewrite the code to use something like lenses. You could then get a type like this: update_person :: (Upd r "id" a, Upd r "name" b) =&gt; a -&gt; b -&gt; r -&gt; r Here `Upd r n a` means `r` is a record with a field `n` that can be assigned type `a`; it's a counterpart to `Has` that I didn't talk about in the blog post. There's more detail about how this works [here on the GHC wiki](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Plan#Limitedtype-changingupdate).
Well, it would depend on the strict language I suppose but the statement above might appear to process e.g. 'e' four times but in Haskell I don't think it would be processed at all until something tried to read that far into the resulting list. In haskell everything (that hasn't been explicitly set to strict) behaves like a python generator. So (++) is [] ++ ys = ys (x:xs) ++ ys = x : xs ++ ys You could read (:) on the right side of the (=) as 'yield' so what this basically means is that (++) is a generator that will return every member of the first list in turn and then every member of the second list. In a strict language I would expect (d ++ e) to be processed first, or at least before 'b'. In haskell I would expect this to not be processed at all unless 'a', 'b' and 'c' were all consumed first.
[Here](https://www.reddit.com/r/haskell/comments/1rnz7p/a_faster_rws_variant/cdrm27r) is what I meant. Is there a problem in my understanding here?
In my case, .so.&lt;installed&gt; was .so.3. It just wasn't symlinked. 
ocharles, can you add link to blog root from individual post page, please? It would make it much easier to navigate.
Do you already have enough topics for the month or are there days to fill?
Hm, in retrospect I think I saw the links but thought they would lead to some generic description about Nix (which I'm already familiar with), so I skipped them. Now after reading they sound quite nice though!
Oh life, why do you always get in the way of making a calendar-like page.
This is a great feature! It also helps with certain other issues I've run into, especially regarding 'safe' serialization routines.
I think I'm ok for topics, but if you're eager to write something then I won't stop you! Drop me an email at ollie@ocharles.org.uk and let's have a chat.
I agree with the conclusion: &gt; if I have interoperability between two libraries, it's because they imported their types and type classes from the same places. Not because they happened to pick the same name for their types or their type class methods. That's like the Python-style duck typing that I think TheGoodMachine is alluding to. Although I think people should just write the type class themselves if they want record polymorphic functions (i.e. no new syntax for defining such classes/instances). My response was more towards the other part of the parent comment, about the whole design being bad. I think the design solves the more important problem (record field names are a pain) quite well. I don't care much for the polymorphism and will probably disallow it in my style guide until I can be convinced it's a good idea.
In practice O(log(n)) might still be 10-100 slower, depending on the constants.
Unintentional or not, that's a great pun :)
I haven't done a huge amount of web programming but I chose Happstack (pretty much at random over Snap -- I knew I didn't want all the mysterious things that come with Yesod) and I found that Happstack.Lite makes getting started very easy.
I also looked at happstack, the code looks clean and there is a really nice crash course. The only thing that kinda bums me down is that there is no hot reloding. I know that there is 'happstack-plugins' but it is outdated atm and I could probably achieve the same thing with some task automation tools but I am just to lazy. 
Programming languages are tools. Just like any tool, you can do with them what they were intended for, or bend them to a completely different usage. Do you think they intend to release high quality code by following this process? Of course not, so your arguments are actually not attacking them since they're not _coding_ in the way you mean it. They are bending the tool for artistic purposes, which is BTW a very fundamental element in art. How do you think painting was invented? By using _natural_ pigments, i.e substances serving originally _another_ purpose in nature than that of being spread over walls/canvas/etc. So no, coding doesn't require any of this at all. Producing reliable software does. It is not what they intend to do here. And I'm pretty sure Alex McLean started by using the approach you recommend to develop tidal, _in order_ to be able to do live coding afterwards. Quite paradoxical...
You can't say if value terminates or not, but for some cases you might be able to approximate: for example if f has a form f x y z = f x y z then it won't ever terminate. There are some program analysis methods that allow you to be sure that something diverges and say "not sure" for other cases.
Couldn't open the video, but this immediately jumped into my mind: http://youtube.com/watch?v=iSmkqocn0oQ 
Haskell is useful *because* the real world is all about side effects. In my opinion, Haskell is the only language that gets side effects correct, since it reifies side effects as first-class values within the language. This makes it far easier to manipulate and reason about side effects since they are ordinary values rather than mysterious machinery lurking in the background of my program.
Very strange. His name is on the Haskell 98 report, so he does know that imperative code can be written in Haskell. Perhaps he's playing up to his audience.
There are several Yesod tutorials hosted on FP Complete's [School of Haskell](https://www.fpcomplete.com/school). I authored a [tutorial series](https://www.fpcomplete.com/school/advanced-haskell/building-a-file-hosting-service-in-yesod) for them that could be considered a complete example. Advanced usage is not covered, but perhaps seeing a project being built from start to finish would be helpful. See the [source code](https://github.com/snoyberg/haskellers) to www.haskellers.com for a complete example being used in production. Type errors can be tricky to work through. If you're having difficulty working with a 1.1 example, the [migration guide](https://github.com/yesodweb/yesod/wiki/1.2-upgrade) and one of Michael Snoyman's [blog entries](http://www.yesodweb.com/blog/2013/03/yesod-1-2-cleaner-internals) explain what to expect.
I happen to disagree with him on this point, but he does seem to honestly believe it these days.
You can have various heuristics. All proof assistants have to have some way of asserting that things aren't bottom. They usually do this by lobotimizing recursion or similar so that all functions are continuous (in the domain theory sense). In Haskell I suppose you could tack on some engine to check for proper (co-)recursion, but it will always miss some cases without an explicit proof. 
To be precise, you cannot write a program that will take as input ANY program in a Turing-complete language and ALWAYS give the correct answer as to whether the input program will terminate. If you relax some of the constraints on the problem (using an input language that is not Turing-complete, limiting the form of input programs, or allowing the output to be something other than 'yes or no') you can still do useful termination analysis on programs.
Thank you for your well-written answer. I appreciate that. Putting this into an artsy context makes more sense to me. However, despite being especially grumpy and neophobe that day (happens, I normally try harder to be open-minded), my main concern is that many young people won't see what engineering sciences are really about and pick up wrong career choices. Spending three or four years studying a subject without obtaining a degree is (imho) a major set back in ones life. [Edit] Also, had I know that this is a project by [Alex MacLean](https://en.wikipedia.org/wiki/Alex_MacLean), I'd have taken it more seriously. I know, that's odd, but talk about credibility. I thought it was one of those boring Start-Up fads.[/Edit]
Och, to *misrepresent functional programming* depends on what you mean by functional programming. Erik has been around a lot longer than the shoehorn of "functional programming" to mean "pure functional programming": http://wcook.blogspot.co.uk/2012/07/day-functional-programming-changed.html
As Harper acknowledged, Chung-Chieh Shan had the best response to this post. What, if anything, is a tall man? Harper is asking for a sharp definition that exactly rules in or out every case. But the concept is useful even if it's vague.
I really like Erik Meijer, but something about this talk is really off. Especially about the part of the functions that accept unit, and return unit. createProducer :: t -&gt; (() -&gt; IO t) consume :: t -&gt; IO () With kinds it's pretty obvious how they would behave (in a specific scenario), and I know he knows about them since I learned Haskell from his videos. The only possible theory I see is that MS crew tortured him because of praising Haskell that much, instead of their own mainstream languages; and forced him to absorb those "penalties" by giving a talk in which he dismissed it. I'm no conspiracy theorist, but this is the only video I saw where he bashed Haskell. And that comes from someone whose seen way too many of his videos and interviews.
His lectures about functional programming in Microsoft's "Channel 9" were hugely informative when I started learning Haskell. http://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals
I see two possibilities. The first is that he is making those remarks to win over the audience. The second possibility is that he has an honest appreciation for side effects after spending years trying to avoid them. So maybe he is trying to prevent programmers to go overboard with an obsession with purity when one can gain many of the benefits of functional programming by keeping a more pragmatic position. 
I have no doubt it would be easy to abstract out the snap reloading code and let it work with Happstack too. Somebody should get on that :-)
It's worth mentioning `vector-space` here too. `linear` assumes your vector types have kind `(* -&gt; *)`, while often you'll have monomorphic vectors which you need `vector-space` for. I'd love to hear from anyone else who has had experience working with both of these libraries and managing the difference between polymorphic and monomorphic vectors. (Unrelated: I have an implementation of monomorphic `Quaternion`s that might be worth throwing into `vector-space` if it were desirable.)
Why do you care? If he doesn’t have valid arguments (which I assume), there’s nothing to discuss. You don’t have to attempt to refute every disrupting troll, you know? :) It’s not like you could change the views of a believer with rationality anyway. And really there are better things to do than to change a mind that doesn’t want to live in reality. If he *does* have valid arguments (no idea, I’m not wasting time on long chaotic ramblings): Nice! We learned something! In any case: ***Who*** said it [is absolutely and completely irrelevant to the validity of an argument](http://en.wikipedia.org/wiki/Ad_hominem).
I, for one, am surprised by how many people consider this an important feature as no one has actually bothered to complain on the mailing list. I wish people would complain about things more instead of suffering in silence. 
I'm in the same case and had the same question, glad we had some great answers.
ocharles, I think you forgot to update your /blog index page.
This video is from 2012, I am fairly certain I saw other videos from 2013 where he showed his appreciation for Haskell. Let me check and get back at you...
Indeed, just a filled cache on my end I guess.
Preferably, the techniques will say "definitely terminates", "definitely doesn't terminate", or "not sure". It doesn't have to be *incorrect*, it will just be *unhelpful* at times.
A type system is totally allowed to do that as long as it stays usable. Regular Haskell also rejects a bunch of classes of programs that would actually work fine.
Is there actually a definition of functional programming? Immutability and limitation of side effects are standard parts of structured programming, see const-correct C code for details. Recursion is one of the ideal methods for handling lists and some algorithms, see C's implementation of qsort for details, also any tree library. Higher order functions are a beautiful abstraction for events and callbacks, along with a number of other constructs-see C events, C qsort for details Laziness is a high percentage performance improvement and adds a lot of potential optimizations and elegant solutions, see "thunking," manually in C for details. Is C a functional language? I think basically everyone would take issue with that. The usual answer that you will get is that the point of Functional Programming is about functions, not code functions, math functions. The ideal is so called "declarative programming," related to logical programming. The idea is "what not how," i.e. what the result is, not how to get there. Unfortunately this, for various reasons, basically doesn't work in any sort practical manner in the general case.
A life support machine is, to my knowledge, not real-time enough that a GC-run would be a problem, but yes, space leaks could be problematic.
Can't you cheat and use a GADT? I haven't looked into Linear, but unless it needs `Functor` instances it seems doable. I mean something like: data FakeGenericVector a where Vec :: Vector3Double -&gt; FakeGenericVector Double or more generically: data FakeKind type arg a where Enrich :: type -&gt; FakeKind type arg arg
If you guys knew Erik this discussion wouldn't happen. :) Of course he knows exactly what Haskell can and cannot do. But Erik also likes to be a contrarian. 
@ocharles, your link to linear's Hackage page is broken.
They're all based on `class Functor f =&gt; Additive f`, so it'll take more than that, unfortunately.
Well, let me preface this with the fact that I haven't really followed or paid much attention to Erik in recent years, so I haven't heard what the OP is talking about myself. However, I wouldn't rule out the possibility that he's adapting his message to his audience. It's often a very effective political tactic, for better and worse. And such is the nature of politics that one tends to be influenced to some degree or another by the group you run with. Maybe it started out as a little lie to increase his credibility in the communities he sought to influence, and perhaps it grew into more than that. 
I saw a talk by Erik at YOW almost a year ago today and caught up with him a little afterwards. There was quite a bit of cheekiness / not-quite-trolling along similar lines, in the middle of a talk on comonads. I think the comments on side effects started a little after that. I vaguely remember that he had previously discussed that futures or promises were comonadic, and then corrected himself because the failure cases and asynchronous nature of them can play havoc with the laws. I can also kind of see that if you have a significant Haskell background and you're developing a C# library with a lot of theory behind it that you might keep reaching for things you learned from Haskell and end up getting burned because of the lack of purity. I'm wondering if one of those two things planted a seed. I don't know if genuinely believes that Haskell is useless though. Either way, he seems pretty hardcore / awesome.
Can you elaborate on what you mean by "vector types have kind `(* -&gt; *)`? Maybe you can give an example of something you can do with vector-space that you can't with linear?
I'm all for contrarianism but in this case he's being 'contrarian' in service of conventional wisdom!
&gt; I'm not a computer scientist, but isn't it true that we can't prove that a computation will terminate? We cannot prove that every possible syntactically valid terminating Turing complete program will terminate. We can prove that some useful subset of syntactically valid terminating Turing complete programs terminate. Edit: clarification due to kamatsu.
If we define "definition" as "a meaning of the word that the vast majority of the software engineering community agrees with, precisely", there isn't a definition of anything. My goto example is "object orientation"; everybody agrees what that is, right? Well, [no](http://mumble.net/~jar/articles/oo.html). I honestly can't come up with a single definition of any major term that "everybody" (or even a reasonably large subset thereof) would agree with. The most "complicated" term I've ever thought of for which there is broad agreement is "byte"; some might argue that isn't quite exactly an "octet" but that's a very small view now. Especially if we discount those who are being pedants for mere pedantry's sake.
He's being a contrarian to what he used to be. :)
&gt; Thankfully, you don’t have to worry about that - because Edward Kmett has already done it for you. This will be the beginning of every Haskell tuto for the next 5 years (at least).
It's fairly uncontentious to define "functional programming languages" as the family of programming languages deriving from the Lambda Calculus (give or take a bit of argument over Lisp). Such languages share a *family resemblance* (see http://en.wikipedia.org/wiki/Family_resemblance) with each other but they may not share all features (e.g. NESL is a functional language without higher order functions!). The language list in the FAQ from the old days of Comp.lang.functional worked like this. I guess "functional programming" is idiomatic programming in a "functional programming language". I'm satisfied for myself that a more "essentialist" definition doesn't exist.
Works now! :)
I listen to the linked presentation in the background and some of the comments that he had mid and near the end seem to support the adapted vocabulary. 2013-12-02-13-00-34--6h - Patrick Wheeler 37 minutes in is when he starts talking about Haskell. approximate quote: ~ &gt;We should not try to hide effects, we should make them explicit. ~"the real value is make a distinction ... super important in functional programing separating interfaces from implementation. I think functional programming has more todo with this then side effects." ~"functional programming is all about being aware of hidden assumptions and then making them explicit or ignoring them when they really don't matter." - " In OO this distinction is often blurred."(distinction from interface and implementation.) ~ "One strong point of functional programming is that it forces the distinction between interface and implementation." ~ 36:35 ~ "Thats the thing I miss most from functional languages" referencing higher kinded types, example is foldMap from Data.Foldable.
Right, right, you can always expand your responses beyond yes/no to defuse the Halting Problem, basically by rejecting the premise of the halting problem.
Doesn't Microsoft contribute heavily to Haskell? And leverage a lot of the concepts and structures? (Linq, ReactX, Entity Framework etc.)? Why would they force him to absorb the penalties while using his work?
It is certainly not necessary to have a bottom value—you can make it impossible to construct terms which do not terminate (with the hazard that some terms which incidentally terminate will not be expressible).
It's AGPLv3. My cabal is from Debian Sid: cabal-install version 1.16.0.2 using version 1.16.0 of the Cabal library So, it doesn't support the AGPL value for the license key or I would have put that. I will make that change in the future. I included the GPL because the AGPL makes reference to it (special exception for combiing GPL and AGPL works) and I thought it useful to have available. I don't know anyone that would consider legal matters without a solid internet connection, but it's there if you need to review it when offline. I personally prefer the GPL (over BSD-style), mainly because it means that if I end up downstream of myself license-wise I can make further changes or revert mis-features. Also, I'm fine sharing intellectual property without monetary compensation, but using the BSD license "tastes" to me like working for free. I could be convinced to change the license (it's all my original code), but it would require some compensation, probably in the form of money or code. Thanks for taking your time to review and comment.
This story is actually becoming quite long in the tooth. Once category theory stops getting thrown in the faces of everyone who touches Haskell then these kinds of comments in videos will stop and not until then. It seems to me that Erick M. is more lashing out against the category theorist Coup d'état than expressing distaste for Haskell. ;-)
Can I ask in what context `linear` was developed? I quickly glanced at the source code, and it doesn't seem like it's a very high performance library. I work in a lab where we re-implement MATLAB, and 50% of our focus is making matrix operations as fast as possible. Is `linear` meant for smaller problems, or exploration problems where performance is a secondary concern?
Any chance I could convince you to store those lists using an org-mode compatible format? Also, why store in .git when you could simply store it at the project root (parent of .git) as a file named "TODO.org" (github will render .org files). This would also work for multiple branches as the file can change between branches (but might run into "please commit first before checking out branch x" issues when you want to quickly capture other TODOs). I've also thought about a file parser for scraping TODO: comments OUT of source files into a central TODO file. Then in Emacs org-mode when you are C-c C-c'ing the done items it could go through and remove the TODO comment (would probably need to ID tag the TODO - I can see this getting messy). Anywho, some ideas! This is really cool though and I'm definitely using it.
Error message? OS? cabal --version? ghc --version? Did you install the GTK+ dev packages? Did you cabal install gtk2hs-buildtools and add it to your PATH?
&gt; Who said it is absolutely and completely irrelevant to the validity of an argument. In a purely formal, 2-valued logic sense, yes, but see [this](http://lesswrong.com/lw/aq2/fallacies_as_weak_bayesian_evidence/); once you throw probabilistic reasoning into the mix, this no longer holds. If someone who has, in the past, demonstrated themselves to be reliably correct on a topic says something on the topic, you should assign a significantly higher prior to it than if someone with no expertise said it.
In fact there is a theorem which basically says something along the lines of There is no useful property that is decidable for all programs In particular, absence of memory leaks, small memory usage, returning an int, returning a non-zero number are all instances of an undecidable problem. However, in many cases humans and computers can, in fact, determine these facts for all *practical* instances of programs we actually write.
The context was the lack of a canonical low-dimensional linear algebra library on hackage. I don't really see how `linear` could be seen as a MATLAB replacement unless you only use MATLAB for small vectors and matrices. For those uses, `linear` being slow would be news, and benchmarks attached to Issues on the repository would certainly be appreciated.
Could you please explain what you mean by "Yesod… codifying 'industry worst practices'" ?
Are there algorithms with random space leaks? Or better put that would be very difficult to find in testing but appear in the wild? Because in my experience, if you have a space leak problem it is very obvious from the get go. In addition, data coming into a life support system tends to be very deterministic. What I mean by that is you are usually taking in sensor readings which have a set scale and rate. So finding space leaks seems like it would be even easier in this scenario.
Thanks for the feedback about low-dimensional matrices/vectors. My group interacts with people from other disciplines and they all seem to have these huge matrices, which is why I was curious.
Yeah, that may be out of scope for `linear`. Some of the sparse matrix stuff seems quite neat, but I've not been involved in that. Larger linear algebra workloads are possibly still best handled by `hmatrix`, but there is hope for a new library focused on big matrices coming soon (paging Carter).
And to address the question exactly, you certainly can prove that a specific program terminates, or doesn't.
I started out with Happstack and built a couple sites like that but I really wanted traditional DB with haskell types which yesod made dead simple. It is a really nice framework and a great launching point for bringing new developers into the haskell world because there is a lot of vocabulary crossover with Ruby, NodeJS etc... What exactly do you mean by advanced topics? Have you looked at the cookbook or the user group... things get pretty gnarly in those places.
I'll keep an eye out. Our project currently has a JIT for MATLAB and two backends to compile MATLAB to FORTRAN 95 and X10. I'm part of a new team to target JavaScript, but if Haskell becomes an interesting compilation target... ;)
Usually Haskell is the one used to craft compilers.
Our infrastructure is in Java. 
Comments and critiques welcome. A lot of the code is in the RandT monad because I needed to generate the display lists. I tried generating a DelayedGraphics type in the Rand monad that had instructions to create a display list in the IO monad but it was more annoying that helpful I thought.
I installed gcc as it is said [here](http://stackoverflow.com/a/19529693/1268079). And I find it is the same gcc as xcode provides. The error can be found [here](https://gist.github.com/eccstartup/7764212).
Yep, `gitlib` is very close, but it doesn't have functions described above
Why `base ==4.5.*` ?
i just let them be autogenerated by cabal. I probably should make them less strict but I'm unsure how far I can go without breaking it.
&gt; createProducer :: t -&gt; (() -&gt; IO t) What's the purpose of the () there? That seems stupid in a lazy language.
I think gtk2hsC2hs might be using clang. On OS X I have been using macports installed GHC, GCC and GTK+. I have to build it with... cabal install gtk3 --with-gcc=gcc-mp-4.8 Try using --with-gcc to point to a command that runs gcc not clang.
Do you mean "decide" or "prove"? Of course we can't prove that every possible Turing complete program will terminate, because not all turing complete programs terminate. The interesting part is deciding whether it does or it does not.
Great work! Working on a toy project with Gtk2Hs now and it's quite helpful.
Unfortunately this approach has a major drawback compared to ```data V3 = V3 !Float !Float !Float```. The monomorphic type, when compiled with ```-funbox-strict-fields```, is more time- and space-efficient. In inner loops, or with large numbers of elements, this is practically necessary to get good performance. Of course it does make it more difficult to have e.g. ```someTransform :: Real a =&gt; V3 a -&gt; V3 a```, so there's definitely a trade-off.
I think ocharles translation from Kmett-speak to ~~dirty peasant~~ English should be merged into library docs!
I know the purpose of this is really to demo linear, but there's no need to transpose the matrix, you can simply have the 'transpose' argument - the 3rd - to glUniformMatrixMxNfv be true/1. So in that code, instead of with (distribute $ triangleTransformation t) $ \ptr -&gt; GL.glUniformMatrix4fv loc 1 0 (castPtr (ptr :: Ptr (M44 CFloat))) We can have with (triangleTransformation t) (GL.glUniformMatrix4fv loc gl_TRUE. castPtr) -- no point annotating the pointer's type, that's already known -- edit: looks like actually the type of the matrix is inferred from the above snippet, -- but i suggest instead of "newIORef 0", that "newIORef (0 :: M44 Float)" be used Although I'd avoid having to allocate memory on every glUniformMatrix4fv call by keeping around a pointer to an array, that we write the matrix to.
Erm, Erik was a large part of the [initial push _toward_ category theoretic methods](http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf).
Haskell allows code that has side effects, but there's still some room for that not to be a straw-man argument. Haskell doesn't need to make side-effects impossible to make them difficult to work with. Having effects controlled by the type system means there's more work to do to write effectful code. Some things are verbose and ugly, such as using `IORef` for mutable data structures. Obviously that's just the balance between up-front effort and paying later for doing the quick-and-dirty thing now. [Worse can be better](http://www.jwz.org/doc/worse-is-better.html). I'm not advocating this view, and don't know if Meijer would - I'm just saying a gap exists. Actually, in another video (where IIRC Meijer murders a soft toy to make his point) I thought there was something odd about his expression. Perhaps a bit I-know-this-is-stupid. 
Yeah, but this type of talk and important people leaving Microsoft Research, makes you think... Joking. Before I get called paranoid. 
It seems Erik found this thread: https://twitter.com/headinthebox/status/407742948348948480
Awesome! I have been thinking lately about recursion schemes and associativity, and this was a welcome reinforcement. The associative versions can also benefit significantly from parallelism, for large enough input. Of course a list, being linear, is not natural for taking advantage of this; `Vector` would do much better.
Great! I installed gcc-4.9 with homebrew, and `cabal install` everything with `--with-gcc=gcc-4.9`. No error this time! Do I have to run the program generated with `gcc-4.9`? How?
This is the bit I love about 24DoH - I get to learn too! I knew it was possible somehow to indicate to OpenGL that the matrix was transposed in reference to the default layout, but I wasn't actually sure how to do this. I remember when I wrote this code a few months ago it was almost midnight and I was in a "argh, just take my code!" mood. Thanks for the tip on how to do this a bit more correctly :)
That looks fun! More examples would be nice.
The problem is that, when I launch `ghci`, as a default, it will work with `clang` or `gcc`? I don't know how to choose.
I happen to work a lot with erik meijers products: a) early research stuff b) LINQ,Rx,async And i am a fan of his work, in all regards but: I often asked myself stuff the OP indicated --- and i am confused as well. His real opinion is absolutely unclear to me. He used to like evaluation instead of stepping like a turing machine, he used to like reasoning about code, he used to like structuring effects. Especially Rx throws everything at you at once: effects, push semantics, concurrency, scheduling, higher order functions and effects. I am happy he was the one working on that stuff- i am sure he can handle all the hard stuff. However, lets get to more religious stuff: he used to like orthogonality in languages, everything is the same, or everything can be expressed with composition of simple things. The languages he apparently fell in love with are exactly the opposite - everything (list comprehensions, async,..) is different, nothing the same, and all is a huge beast of engineering. He went all the way from functional fundamentalist to wizard of all effects at once because they are so real.
I find really nice that linear uses typeclasses, so that you may reuse plain vectors like math vectors. But those polymorphic functions work only on Data.Vector.Vector. For OpenGL access purposes, it would've been could if they were defined by using the Data.Vector.Generic.Vector class, so that we may also use them on Data.Vector.Storable.Vector. *EDIT:* I just saw that Linear.V.V (the type-sized variant) is just a wrapper around Data.Vector.Vector, and that it defines a Storable instance. May help here to ease communication in one direction: Haskell to C. But for the other direction we need Data.Vector.Storable.Vector to wrap a pointer returned by C code and use it like a vector.
that is something it should have though...
FYI that's Rice's theorem.
But that doesn't detract from the interest of his work.
I smiled when I saw Bob post on the comments: &gt; Eventually people will come to understand what I've been saying about Haskell for twenty odd years.
I came in here to comment on the very same line! It's pretty much the standard mantra these days.
He is living the mantra, "applied duality". What's the dual of functional purity?
It is true that libs like vect or Vec already includes projection and translation 4x4 matrix computation which linear lacks (or indirectly proposes, via mkTransformation, which I don't find as clean and formal as things like ```translation (x, y, z) `times` rotation (a, b, c) theta```). So we have Vec, vect &amp; vector. We still need "vecto", "ve" and "v". Guys, we should really take 5 minutes to discuss library names...
A bistable process.
It's been a while since I worked with them, but I'm pretty sure we have abstracted out the `SDLRenderer` and `SDLWindow` entirely, so that bit is pretty much done. You can obviously also create OpenGL contexts, as demonstrated in this post. There is also some rudimentary support for event handling in terms of (at least) keyboard and mouse input. I think the sound area is still TBD.
well if you have better name ideas for a *vector library*, I'm happy to hear... At least `vect` doesn't steal a common name. 
&gt; [leak in node.js](http://www.joyent.com/blog/walmart-node-js-memory-leak)
[Moose led to Mouse led to Moo led to Mo (led to M)](http://search.cpan.org/~mstrout/M/lib/M.pm)
&gt; As a note, Byte does have a technical definition, it must be at least 8 bits in C and satisfies a few criteria, in C# it is exactly 8 bits, Java as well etc. That's what I mean, though; it doesn't have _a_ technical definition, it has a _suite_ of technical definitions. The reason why I say we generally agree is that while C supports &gt;8 bit bytes, "nobody" really uses them anymore. Byte and octet are synonyms for all major and the vast bulk of minor platforms and languages. &gt; There are a number of generally agreed upon facts, spaghetti code bad, Ah, but try to get a _definition_ of spaghetti code out of people. See, I'd use one inspired by Dijkstra's "Goto Considered Harmful" paper that actually has a specific meaning, but many people will use it to refer to conceptually unstructured code that "merely" very hard to understand... and honestly, if it came to a vote, I suspect you'd see the latter definition win, despite the former's superior academic credentials and history. (And that's not even necessarily wrong; nowadays it's pretty rare to encounter "true" spaghetti code by my definition, where people are actually `goto`ing every which way and using nothing but global variables.) I was talking specifically about _definitions_. Things that are ill-defined: "Object oriented", "strongly typed", "manual memory management", "functional programming", "well-factored", "variable". You might say, "Ah, I know what that one is, I will post my definiton." This isn't the first time I've made this point online. And what often happens are two angry posts about how some particular term is actually well-defined... with the two people using incompatible definitions. A lot of people think their definitions of the relevant terms are much more universal than they actually are. I'm not making a moral claim here, I'm making an _observational_ claim: There is a great deal less consensus about almost every software engineering term than most programmers seem to think.
Cool, I've rarely seen Reddit Gold in this channel! 
Actually, assuming the combining function is expensive enough to warrant parallelism, then `[]` is probably just as good as `Vector`: While it is true that you need to linearly traverse the list to set up the parallelism, once the parallel tree is set up - computation would have the same degree of parallelism as it would with `Vector`.
We deliberately require parametricity over the content type. This matters to us as we make matrices by using `f (g a)`. It s at first blush limiting, but it also enforces that the library plays well with things like `ad`, and introspection on it in EDSLs, which matters more to me than 20% performance for the boring flat Double case, since I _can't_ say these things in other libraries. YMMV. Folks have been talking about adding monomorphic vector support to `linear`. I'm somewhat neutral on the proposal.
The things I tend to manipulate with linear are vectors of AD variables, or vectors in EDSLs, vectors of vectors in matrices. Those things all go out of the window when you bake in the choice of `Float`.
Send a patch!
We've been talking about adding support for more 'monomorphic' vector types. The issue is that this is immiscible with the current matrix representation which is to use nested representable functors. Due to representability `f (g a)` is isomorphic to `x -&gt; y -&gt; a`, making the matrices very easy to manipulate. As storing more interesting contents in my vector spaces and exploring representability was my original motivation, I'm content not to solve _everyone's_ problems, but I'm not averse to taking patches that improve generality without too much of an impact on existing users.
I didn't realize there was anything bad about Rails, other than someone talking about Haskell advantages vs Ruby. What's wrong with code in templates? Basically, sorry, I'm new enough to this to not understand any of your points here. I would really like to understand though, so if you could possibly expound, I'd really appreciate it!
Interesting. What is the advantage of doing it the `linear` way?
Does that hold for *every* specific program?
Interesting. Thank you! I kinda followed that, and it helps me have some perspective. I know though that Yesod uses PostreSQL which is certainly robust anyway. I happen to love the overall format of the Shakespearean Templates, although I am not knowledgeable enough to weigh in on the signifance of the mixing in of code there. I'm working on a site where I'm just dabbling with code, mostly doing the *other* parts of the work, and my partner is coding it in Yesod, so I'm kinda following along…
You get all of the normal `Functor`, `Applicative`, `Traversable`, `Foldable` machinery aligned with your linear algebra. You get polymorphism in the component type which, as ekmett says below, is useful for working with, say, matrix derivatives via `ad`. You also get a very different representation of matrices; compare [Data.LinearMap](http://hackage.haskell.org/package/vector-space-0.8.6/docs/Data-LinearMap.html) with [Linear.Matrix](http://hackage.haskell.org/package/linear-1.3.1/docs/Linear-Matrix.html).
&gt; However, in many cases humans and computers can, in fact, determine these facts for all practical instances of programs we actually write. Or we can make compilers only accept a subset of programs for which the problem *is* decidable. For example, type checkers will not accept perfectly valid programs which it cannot prove return the right type, like foo :: Int foo = case True of True -&gt; 1 False -&gt; "Never reached, but not an Int"
Edward the video is clear -- he calls category theory a "Rabbit Hole". Not to say that category theory is bad it just not a "hackers" tool. Library writers who have the knowledge to make use of it are more than welcome to do as they please. I just think category theory may be too prevalent in the Haskell community which actually does more harm than good. I could be wrong. 
A few notes: 1. You've got currying backwards. 2. Haskell doesn't have weird syntax. Actually, Haskell has very little syntax at all. The operators are mostly library defined, and not part of the syntax. 3. What is your target audience? You start by explaining what Haskell is (as if you were targeting the guys who take their first fumbling steps) and then you go into the gory details of the definition of a monad, skipping over any practical examples of monads (as if you were really targeting people who have been writing Haskell for a while.) I'm not saying it's a bad article, I'm just saying it's a little all over the place and could use a clear focus, I think!
Oh god, I'm trying to get *away* from my 6 years of Perl background!
And you're right. I started with what I felt was weird compare to other imperative languages I used to be coding in. And monads are the most weird among those things. Thus, exactly that structure of an article. But the idea is to continue with a practical example of a new monad which hopefully will make the series more consistent. Initially I thought doing it all in one article, but it turns out to be quite lengthy already. Thanks for your input, I will try making it more focused next time. Maybe you can volunteer reviewing next post before I will publish it? I would really like that...
Ish. In particular, you can't declare types, classes, or instances in an IO block. Still, that's mostly a good way to think about it (with the additional note that things showable things are printed, and "IO a" is showable as " "a").
Great to see GTK+3 support! Does it work with glade?
I didn't catch that you intended for it to be a series! As a series – with some structure – I think it could become *really* good. I'm unfortunately not at all qualified to do a proper review, but I'd be glad to drop some hints off where I can! Edit: Some general tips: * Pin down your audience. Are they comfortable with algebraic data types? Have they used `Maybe` before? Have they done I/O? Are they comfortable with various monads? * Decide how far you want to take your audience. Do you want to reaffirm the little they know about monads? Do you want to make them monad power users? Do you want to gently introduce them to the theory? Do you want to take them through only the theory? Do you want to teach them how "real world code" can use the fact that some types are monads? * Create a road map from your audience to your goal for them. Beginning from what they know, how are you going to lead them through what they don't know? The more specific you can be here, the better. Create a user-friendly version of this and include it at the top of each post! * Remember the keep your posts coherent all the way through. Each post should be a logical succession from the previous one. This is where the road map comes in handy – it stops you from wanting to write everything about everything in the first few posts. * Decide how much you want to hand-hold your audience through the experience. Are you willing to sacrifice time and brevity for taking it very gently, or do you want to write a series which drags the audience along through all the amazingness you want to show them? * Mind the time and space you have set out for yourself. If you only want to write 2000 words, you shouldn't try to cover too much. When you have gone 80% of the way, it is time to start wrapping up, leaving the audience with something tangible they can take with them and a warm fuzzy feeling of understanding. It's okay if some bits in the middle are difficult, as long as long as the beginning and ending are friendly!
Praised be every one who worked on this release. I love to see GTK+3 support. Great work!!!
Nice work! Tasty looks interesting, I'll have to check it out. How do test trees interact with [doctests](http://hackage.haskell.org/package/doctest) ? Is there any way to get them to play nicely together? I do love my doctests.
Meijer murders a soft toy in the linked video, just past the timestamp.
I'm not saying that he isn't making that argument now, just pointing out the irony. =)
Unfortunately, doctest's library API is very simplistic, and doesn't allow to integrate it properly. If someone could make doctest expose test results (or, better yet, the tests themselves) in some structured format... and then someone would write a tasty wrapper... that'd be nice :)
So you assume that for every program that does terminate there *is* a termination proof (in classical logic). That's rather bold.
This counterargument does not apply to what godofpumpkins said. They didn't claim that there is an automatic termination checker which can be called as a program and answers the question "termination yes/no". Instead, godofpumpkins suggested that for every program a clever enough mathematician can with certainty answer and prove whether or not the program terminates. The standard argument about the Halting problem doesn't apply to this claim. But checking out Gödel's incompleteness theorem might be a good idea. 
He's a vegetarian who eats fois gras. :)
Yes, you just need to strike a balance between input size, cost of the folding operation, cost of traversal, and cost of parallelisation.
Well like Obi-Wan says, it kind of depends on your point of view. The compiler tries to check well-typedness of your program but fails for a "non-practical" instance like the one above.
I think it should work with glade using gtkbuilder files ([gtkbuilder demo](https://github.com/gtk2hs/gtk2hs/tree/master/gtk/demo/gtkbuilder)). There is no "glade" package for gtk3 though (libglade is deprecated).
Thank you! These are probably the most valuable notes I have ever received/found about blogging and Haskell particularly. Thank you again, I will try to keep up.
 Prelude Test.Tasty Test.Tasty.QuickCheck&gt; defaultMain $ testProperty "Numbers are positive" $ \x -&gt; x &gt;= 0 Numbers are positive: FAIL *** Failed! Falsifiable (after 6 tests and 2 shrinks): -1 Use --quickcheck-replay '5 299289546 40686' to reproduce. 1 out of 1 tests failed Are you perhaps experiencing [this bug](https://github.com/feuerbach/tasty/issues/16)? Be sure to install tasty-0.4.1.1.
Ghent is a really nice city
Yep. The success of Rails has supposed a step back for haskell web development. In the old days, WASH, a Haskell web framework had tons of innovations in the genuinely functional style of continuation-based web frameworks. It was discontinued because everyone wanted to imitate the MVC-Rails model in fashion. That, IMHO is no bad, but it fall short in what can be expected from the Haskell language and the Haskell community. HappStack is the one that tries to use the extra level of the Haskell language. The other two try to cover the gap between no Haskell knowledge and a running application in the most straight path possible with proven techniques of other languages. Don't misinterpret me, I think that the three frameworks are well though, modular, and efficient, but you do not stare at them and say "Wow!!! this is beatiful and powerful. That changes my mind!. I have to understand and use this somewhere" like you would do at other haskell technologies and developments. I´m the developper of MFlow: It is a continuation-like framework. Its syntax is clean. it is a true DSL made of combinators for creating web applications. It is not as mature as these three developments and has a very small community: basically, me. But no other web framework offer the [10+ features that are mentioned in the demo site](http://mflowdemo.herokuapp.com/) . Among other things, no broken internal links, HTML templates editable at runtime, reusable flows, back button management, full REST, routing and navigation in a navigation monad. multiple flows inside a page with auto-refreshing of widgets and other single page development features. Persistent user session data in normal haskell variables and horizontal scalability. I think that it covers the "Wow!" magic of Haskell programming that complement the industrial strength and experience of the three main web frameworks. It is a true [DSL for web applications](https://www.fpcomplete.com/user/agocorona/MFlowDSL)
He's suggesting that for a beginner. When a beginner it makes sense to focus on understanding one area of the language at a time, so it doesn't matter so much if one postpones learning about the "best" monads for a particular task.
I definitely agree with * Make specific data types first, parameterization can be done later and * You probably don't need your own typeclasses yet Too often I see beginners overengineering in these directions.
Thank you very much Hamish. This was really awaited, in particular for cabal 1.18 users. I will give it a try very soon on a new project.
&gt; So how do we deal with this in Haskell? I mean, unless we compile separately for those languages, toUpper, toLower :: Text -&gt; Text can't be fully correct pure functions. I'm guessing we just ignore the three languages with special rules. Data.Text.ICU.toUpper :: LocaleName -&gt; Text -&gt; Text
&gt; You have to understand how a monad works before you understand `getLine &gt;&gt;= putStrLn`. &gt; I think it’s important to quickly put one’s hands on the mtl monads, because they’re an important part of what Haskell is. I, and I think the OP, disagree with both of these. I personally rarely use State, Reader or Writer. I very, very rarely use mtl. Learning something new can be overwhelming, especially something as unusual as Haskell! It's nice to break it into smaller, manageable chunks.
I use mtl all the time, and I, too, disagree with /u/_skp. &gt; You have to understand how a monad works before you understand getLine &gt;&gt;= putStrLn. A monad is just an abstraction, albeit a useful one. You don't need to understand abelian groups to count the change at a supermarket.
This is Ubuntu logic again. Haskell is for maximum efficiency, emergence, elegance and power when coding. If you want to stay brain-dead (not judging, these are your words), Haskell is the wrong language, mate. Use… I don’t know… Python? (Not that that’s a bad language, obviously.) But don’t mess up our language to force it into your mindset. It’s not made for you! Leave it be! It’s bad enough already, that Ubuntu messed up Linux to appeal to the “loud idiot” type, and that Mozilla did the same to Firefox to become iChrome. I have no idea why you think limiting yourself is a good thing though… I mean what’s next? Refusing to use anything but your feet to get anywhere? Using an abacus again because it’s “simpler”? Also: This is the slowest blog site I have seen in a long time. A loading splash screen? On a web site? Seriously??
Interesting that in the recently posted video Simon Peyton Jones says more or less the same thing: http://www.reddit.com/r/haskell/comments/1s2fb1/sexy_types_are_we_done_yet_a_discussion_with/ "I think we undersell types because we mostly represent them as a way to be against things, to reject bad programs [...] one thing that I really think therefore is design time. I used to be asked 'in functional programming, what do you do instead of UML?' [...] Then I realized, types are our UML, that's what we do when we design programs. Maintenance is another undersung aspect of types."
&gt; For a beginner though, even a long time beginner like myself, it can be intimidating and it can discourage you from learning more about this wonderful language. He isn't advocating limiting yourself. He's saying all the haskell related stuff can be a lot to take in and that it's ok to take baby steps.
Let’s generalize it: Whenever you do something, start with the most low-detail version of it possible. If you write a story, start with one word, sentence, paragraph that contains the whole story. If you paint a picture, start with one color, shape, 32px canvas for the whole picture. If you write a program, start with one function, interface, module.
If you don’t know what a monad is, how could you know what `getLine &gt;&gt;= putStrLn` does? You have to get the definition of `(&gt;&gt;=)`, and it’s directly related to `Monad`. For the supermarket example, of course you know the abelian property that states – as applied to your example – that you can swap money values. So sure, you don’t have to know abelian groups, but you have to know its commutative law. And it’s interesting to discover an abstraction of something you already know (e.g. non-empty list for `Semigroup`).
Just a note, it's Peyton Jones not Payton-Jones.
Suppose I define andThenRun :: IO a -&gt; (a -&gt; IO b) -&gt; IO b andThenRun = (&gt;&gt;=) justTheValue :: a -&gt; IO a justTheValue = return Then I can use `IO` without knowing anything about monads or knowing what `&gt;&gt;=` is.
What's the problem with Ubuntu?
Sounds a lot like the kind of Haskell I write every day.
Where the space is non-breaking.
What part is confusing you? Note, you'e using `unsafePerformIO` so all bets are off. :)
The output differences based on whether the function is pointfree or not
Hipsters gonna hip.
I'm sorry for offtop, but I'm using Ubuntu instead of Windows because it is much harder with Win to compile libraries, etc, and Ubuntu instead of True Linuxes because I don't like to feel myself a systems administrator at full-time work.
I would say that limited modules for language extensions is actually an advanced user thing. For beginners, sticks to the syntax in the report until you've got a good handle on that, at least.
Foo is in value form, which means it will be evaluated once (and your unsafe stuff executed once). Bar is syntactically a function, so the body will be evaluated every time you apply it to an argument. It's just a GHC thing. Note, however, that you can't rely on this behavior for unsafe side effects. Many optimizations can change this behavior. 
CAFs are only evaluated once (with ghc), so if you have a top level definition `x = e` then `e` will only be evaluated once. A (syntactic) function body is evaluated every time the function is called.
I just copied what was on the page :) But yes, you're right - I should have corrected that instead of just blindly copy/pasting.
&gt; You probably don't need your own typeclasses yet I agree with this. One of the more common mistakes I see beginners make is to try to implement everything using type classes and turning on 10 extensions to get things to work, when plain records would have sufficed.
Just to confirm it installs fine in cabal 1.18 with sandbox, on linux. Be sure you have proper dependencies, install gtk2hs-buildtools, and add .cabal-sandbox/bin to your path. Then cabal install gtk should work as expected. Now if the gtk2hs-buildtools step was hidden, it would be very cool. But this is more a cabal related request :)
What is CAF short for?
Constant applicative form
Actually I parameterize things by default. The extra flexibility imposes more restrictions upon the implementation, which gives me more confidence I haven't screwed it up in an obvious way.
Agreed. It "Just Works" and that's good enough for me. For some sufficiently flexible definition of just works
And Conal Elliot's [Pan](http://conal.net/Pan/papers.htm), for that matter. (dat 90's webpage)
Me too, minus the part of sticking it in I/O first.
But with a proper divide-and-conquer algorithm (with logarithmic critical path length) there are no combining functions that are too cheap to parallelize, provided you have a sufficient number of items to combine. You could get a good speedup just summing integers in an array, for instance. (Well you'd probably be limited by bandwidth to main memory to read the array, but not by general parallel synchronization overhead.)
Watching Erik Meijer and SPJ play off each other is actually pretty humerus. I actually appreciate his making some very far contrarian points since it gives Connor and Simon a chance to really get to the essence of why types matter.
I really don't see where he's coming from though. "Types do not eliminate all bugs, so we might as well skip them completely". To me that's like saying that the police will not prevent all crime so we should not have a police force.
I took your statement (or rhetorical question?) after "I think so?" as implying that you believe that for every program that is terminating there is a termination proof in "our (classical) logical axioms". Doesn't Gödel's incompleteness theorem directly reject that idea? (Since "our (classical) logical axioms" are such that all proofs built from them can be recursively enumerated, Gödel says that there is a true statement about arithmetics that cannot be proven using those axioms. So a program that terminates exactly if, in fact only because, that particular statement is true, will be a program that *is* terminating but of which it is not possible to *prove* that it is terminating.) If that idea was not what you meant, then I don't know what you meant (also not after the comment I'm replying to here).
And tests don't eliminate all bugs, and specifications don't eliminate all bugs, and syntax checkers don't eliminate all bugs - so! We should just `/dev/random &gt; asm` and enjoy the fruits of our labour.
Link for the lazy (PDF): http://www.dcc.uchile.cl/~gnavarro/ps/cpm12.pdf
I don't know where /u/TobyGoodwin's tutorial is, but you might like sigfpe's [The IO Monad for People who Simply Don't Care](http://blog.sigfpe.com/2007/11/io-monad-for-people-who-simply-dont.html).
I liked that Erik brought up type debugging. I would really like a type debugger, especially as, like Simon said, type systems like Haskell's become more sophisticated but the error messages become more cryptic. For example, you can often create a scenario where a simple Int ≠ () type error will cause 6 other completely unrelated type errors to occur in the same module due to type families. You peer at the errors in confusion, and then realise that it's the same old problem, and sift through the errors to find the real one. That's a pathological example. The average type error isn't much more helpful. It's the equivalent of a runtime error with no stack trace. So like Meijer says, a debugger that steps you through, shows you “Here's what I'm trying to unify, here's the things I _could_ probably unify, here are the instances I could resolve if it wasn't ambiguous”, etc. to the extent that I don't have to figure out what's going on by myself. It's not a feature whose use would diminish over time, either, I think. Newbies would love it, but Haskellers don't make it easier on themselves as they increase in competence, because they start doing more exotic things, so they still would want help and elaboration. It's especially bad at present with, e.g., GHCi, that suddenly stops giving you any type information when you get a type error. One of the _key_ places you _really_ need some type information about what you're doing and it's gone, you're in the dark. It's crazy when you think about it. It's definitely an area I'm interested in improving and would enthusiastically applaud any work done in that area. At one point I fiddled in Emacs with a redundancy setup, where you'd run two GHCi's, one to compile, and another to `:load` to keep the type information handy. I should resurrect that idea, because it drives me mad sometimes! But perhaps the real solution is to force GHCi to retain type information.
Great article. I just completed a big port of all my interpreted splices to compiled splices. It turns out, at least for me, compiled splices make my code much more clean/neat. I'm a huge fan.
I was also just thinking that this looks like composing free monads!
Nit - Oleg's comment says "Manly", you probably wanted to type "Mainly"
I think a novice needs to (fairly soon after LYAH stage) at least have an idea for what mtl can do for you. Personally I don't see how I could have much enthusiasm for Haskell if I didn't at least have ReaderT. You don't need it for toy programs or euler excercises but even relatively simple applications are going to need configuration data and I'd really get tired of passing it around explicitly to every function like some bag lady with a shopping cart in which she stores every possession she owns. 
The blog post writes "extensible-effects is a direct implementation of the concepts discovered in the paper". Not really. The paper iirc didn't discover or present any new concepts, just a new library. The study of effects including basically how to compose them in this way goes back to the mid-late-90s in the modern sense and back to the late 1950s in the broader sense. Not a value judgment on the approach, which I have yet to try out seriously, but I do want to be clear there's an apparently nice library, but very little in the way of "novelty."
Looking at the paper, there is an example that can't be expressed with monad transformers with co-routines, in section 5. I'd start there!
The comment says Manly, so I said Manly. I'll put a [sic] on it - I don't like changing other people's words.
The same sentence ends with "in the pape." rather than "in the paper." I'd say explored.
Didn't realise the typo was in the original. Thanks for writing all of these posts.
This is weird and stupid off topic, but I recognize your username from a SO question from a while back that I was impressed by. Just wanted to say hi!
Done. Thanks!
 &gt; Compiled splices only introduce one major caveat: they won’t stop you from declaring splices with the same node name, and it will happily let you overwrite duplicate values. Perhaps one could declare "dummy" top-level splices to serve as namespaces, that contained all other splices as local splices. I haven't tried it, though.
The strong form of the claim appears to be, if you are going to have to write tests for your code anyway, then, since the tests you are going to write anyway are going to catch all errors that your type system would catch, then there is no point in using the type system. The weak form of the claim appears to be, if you are going to have to write tests for your code anyway, then, with less effort than using a type system, you can write additional tests that will cover all the errors that your type system would have caught, so there is no point in using a type system. I personally don't buy the claim in either the weak or strong form (I believe I use type systems in such a way that catches errors that I would never have considered tests for). I hope I am not misrepresenting the claims.
It looks very much like a "free monad of coproduct of functors"-construction, which is essentially the same as a free monad, only more modular, because the coproduct is built-in. This is, of course, very different from monad transformers, which there is no elegant foundation for (unless you include hoist). It is an elegant method and obviously the next step after using free monads to enumerate the list of commands. Also, note that any monad can be seen as a quotient of the free monad of its underlying functor, so you don't lose power in neither values nor deconstruction functions, though you might end up with too much power in deconstruction functions.
&gt; It's especially bad at present with, e.g., GHCi, that suddenly stops giving you any type information when you get a type error. Are you aware of `-fdefer-type-errors`?
*waves*
What would the type signature be if you wanted verboseAddition to be an IO action? Can you add a `main` to your example code to help me understand how this is used?
This series of blog post has become a reason for me to not look forward to Christmas (since then I'll have to wait one more year for the next). Thank you and keep up the good work!
will this timeout if my build takes longer than 15 minutes :|
Yes. But it caches things, including the cabal sandbox from your previous build so new builds are incremental. Actually I'd appreciate it if you tried one of your bigger apps on it to see if it works with real apps. I only tested it with hello world type stuff. You can also use an Anvil build server to create Heroku slugs with no timeout. I haven't tried it yet, but it looks pretty smooth. https://github.com/ddollar/heroku-anvil
A 'type debugger' is a solution to a problem that should not exist in the first place. This problem (of having to decipher type errors) is entirely artificial, caused by the fact that the compiler is run in batch mode. That is, you write a whole bunch of code, then submit a blob of text to the compiler, which reports errors. A semantic editor in which assembled programs typecheck by construction entirely sidesteps the issues, and is a way nicer UI and beginner friendly to boot!
Yes, I'd say you have it right. The innovation is that the syntax is actually decent, whereas if you actually implement this sort of thing using the Data Types a la Carte sort of approach, you get a boatload to plumbing code and it quickly becomes an ugly monstrosity.
It's pretty easy to have better type error messages than ghc has. But ghc doesn't retain enough location information to generate them 
That depends on the language. Unless your language supports complete type inference, you can dig yourself into a hole where you have a part of your program that still needs to be filled in but there is no way to fill it in and have the program type check.
&gt; ghc doesn't retain enough location information What do you mean by this? In my experience, the issue is that GHC either (as stated) displays things downstream from the type error that aren't actually "wrong", or emits some bizarre message about, say, missing instances, when the mistake I made was actually much more mundane. It seems more of a human usability issue. GHCs errors aren't "wrong," they're just presented poorly. Unless that's what you meant.
just have a symlink from gcc to gcc-4.8 or whatever. it'll look for the first gcc in your path, problem solved
To be fair you'd likely use Frama-C or SPARK/Ada. I'm not betting (or flying) on Haskell planes or life support machines, unless it starts spitting out deterministically scheduled code. (I prefer Haskell on all other non-hard-real-time, non-safety-critical cases)
No I don't think so either, though the work on [Galois's ivory](https://github.com/GaloisInc/ivory) for using embedded Haskell domain languages to emit C is an interesting approach.
How is that related? I *want* the type errors immediately.
Ideally, yes. See: [Lamdu](http://peaker.github.io/lamdu/).
Thanks. I've been trying to figure out how a [particular library](https://github.com/unfoldr/Salsa/blob/master/Foreign/Salsa/CLR.hs#L81) was performing its initialization only once. Didn't know ghc handled CAFs in that way.
Manly effects - brutal yet handsome.
These are great, highly recommended for beginners! Jekor also have a series called "Code Decustructed" where he deconstruct and teach how some projects works inside. I was a bit skeptical to a video tutorial format at first (especially on YouTube) but this really shows that videos can be great for teaching.
Perhaps, but as I said in a sibling thread, although I have an awareness of what mtl can do for me, I use it very, very rarely.
Just like Oleg.
oh yes, you certainly do get much plumbing code! i was excited when i first read about data types à la carte, but became mildly disappointed when trying to use its approach for a medium-size application. fwiw: the paper mentions data types à la carte as inspiration. afair ekmett mentioned, that data types à la carte has major performance drawbacks, which extensible effects avoid.
If you head over to https://github.com/ocharles/blog/blob/master/code/2013-12-04-extensible-effects.hs you can find the code. Does that help?
And, as I said, I reject the implicit presumption of the types-versus-tests dispute, namely that types aren't for anything, only against. Even if you're pro-types, folks, please don't fall into the trap of thinking it's all about policing errors. Types are for having to write less of the program. Think about all the different things a function like traverse does: which of them you get is chosen by type, because types represent not only data with a representation, but moreover, data with computational structure. There are languages which still have last century's types, much to the irritation of their users. Let's not waste time on arguing why the advantages of those systems outweigh the manifest nuisance of them. Let's argue on the basis of types as they are today, and think boldly about how to make types work even harder for us in the future.
The "problems" he presents with monad transformers are in sections 5.1 and 5.2 of the paper. The one in 5.1 uses `ListT m a = m [a]` so I am automatically suspicious. The documentation for `ListT` states that it's only a valid monad transformer if `m` is commutative. I think `Error TooBig` and `Identity` *are* commutative, so this is probably a fine usage, but I am cautious and will have to think further. The one in 5.2 uses `Reader` in order to simulate some sort of thread-local *modifiable* state. This seems unwise. I'll have to investigate further! http://hackage.haskell.org/package/mtl-2.1.2/docs/Control-Monad-List.html#v:ListT
In the back of my mind I knew I'd read something that dealt with Oleg's objection in 5.1. Sure enough, it was Dan Doel who posted the nice article linked from here: http://www.reddit.com/r/haskell/comments/1n7w5p/monad_transformers_and_static_effect_scoping/ Basically it comes down to `catchError` having too big a type signature. I'm very impressed by the neatness of the `localCatch` solution that Dan proposes.
Maybe you could write it as "Ma[i]nly".
GHC doesn’t retain enough information to produce, let alone inspect, the chain of inference that led to its conclusion of inconsistency; nor does it allow you to proceed very far beyond such a conclusion to the point where no more consistent judgements can be made at all, *in addition* to the fact that it’s not perfect at presenting the information it *does* have.
When ghc says something like "Cannot unify Bool with Int" it tells you the location in the program when it discovered this. This location is not very interesting. Much more interesting are the locations where Int and Bool were introduced in the program (by a variable or constant). Showing those locations is a huge improvement, but as I understand it, there is no location information available in the representation of types during ghc's type checking. Similarely, when complaining "No instance Num Bool" you want the locations of where Num and Bool come from. Again, a huge improvement. We have a Haskell compiler that provides these locations, and have had relatively little complaints about type errors even from novices.
Hello, I'm using http-conduit / conduit to make requests to a REST API and download images. I wasn't really sure how this affects me, how WAI / http-client is related to the APIs I'm using. From what I understand, this basically replaces some of the underlying layers of APIs I'm using? Anyway, I decided to have a look and updated my packages from hackage (taking the latest conduit, http-conduit, attoparsec-conduit, authenticate-oauth etc.). After resolving the usual cabal mess and fixing a few trivial compilation bugs, it seems I have issues with the new versions. I get a ton of these exceptions: HttpException: InvalidStatusLine HttpException: ResponseBodyTooShort HttpException: OverlongHeaders HttpException: IncompleteHeaders I've never seen any of these before. Going through my logs, the only errors I'd get before are 404 / access denied status code errors and the occasional connection timeout when doing too many requests. I'm also seeing some of those: IOException: threadWait: invalid argument (Bad file descriptor) After my programs goes about its business for a minute or two, all connection requests fail for a while with this one: FailedConnectionException interspersed with this one: HttpException: InternalIOException send: invalid argument (Socket is not connected) Finally, every once in a while: ZlibException (-3) (The REST API queries are gzipped, the other ones might be as well, didn't check) I did not see any of this behavior with the old &lt;2.0 version (been developing / testing this code for weeks, running hours at a time, hundreds of thousands of requests, zero issues). Can anybody tell me what's going on here? How can I make my code work again with the newly released versions?
Aha! That's a good workaround. Thanks.
&gt; Tested end-to-end.. nothing you can get further from the types. Hahahahhahahahah. Good luck for 100% execution path coverage.
It's entirely possible that there's a bug in the new version of the library, it obviously hasn't received the same level of testing as previous versions. Can you provide a reproducible test case that shows the problem?
that's a great example! for future posts, there is a big test suite shipped with the "canonical" redo implementation by apenwarr.
ocharles is like the Santa of Haskell.
Okay, I've only ever used GHC. I'll have to look into that.
That is the file I was referring to. Can you please add `main` to it so that it is runnable?
One thing I'm always worried about with Haskell on Heroku is the slug size limit. Haskell applications can get big very fast while Heroku limits application size to 300mb.
Or, to make a cricket analogy, pro-types folks should also think of types as letting one program "on the front foot", rather than the back. http://www.urbandictionary.com/define.php?term=on%20the%20front%20foot (edit: 20 seconds after writing the above, I found myself quizzing hoogle "[Maybe a] -&gt; Maybe [a]"... ah!)
It would be really helpful if the links to the full source were at the bottom of the post. If I were learning Haskell I wouldn't immediately intuit that (.=) comes from Data.Aeson for example. I found them on the Github tree for the blog but had to dig a bit. [2013-12-05-scotty.hs](https://github.com/ocharles/blog/blob/master/code/2013-12-05-scotty.hs) Thanks again for this amazing series.
Could you add a github issue about this please? (and include the original cabal file and the constraints that fixed it) Perhaps I'm running cabal update at the wrong time and it's not aware of the newest packages...
It seems the issue is easily reduced to a small program: https://github.com/blitzcode/http-conduit-bug-reproducer Using a local sandbox and switching between the old and new http-conduit, I have it either reliably run till it hits a 404 or show the same seemingly random errors as in my actual program. When whipping this up, the bug did not seem to show up before I put in the async. Note that the program is not compiled with the threaded RTS (my actual program is). Hope this helps!
[Here's a bare-bones Happstack app that compiles and runs through this buildpack](https://github.com/tel/happstack-heroku-test).
Submitted