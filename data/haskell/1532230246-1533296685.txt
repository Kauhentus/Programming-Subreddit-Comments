JavaScript is in a unique situation among programming languages in that it has a relatively impoverished standard library, and extending it is difficult due to the nature of its distributed implementation, complicated standardization politics, and backwards-compatibility concerns. For a concrete example of the latter, see how [merely adding `Array.prototype.flatten` to JavaScript’s standard library caused a significant debate](https://github.com/tc39/proposal-flatMap/pull/56) because of an existing library that already added that method with non-standard behavior, and adding it to the language could break existing code. For this reason, things that really ought to be in the standard library just aren’t. Packages arose to fill in the gaps, but compromises are almost always made to fit into the existing ecosystem, rather than trying to redo everything from scratch, and without an overarching vision, a patchwork of packages developed that often have holes of their own. In most other language ecosystems, the standard library is generally much more complete, and libraries are generally expected to provide a useful complement of exports. Haskell’s standard library includes most of the functions that would come from an npm micro-package, so the issue just doesn’t come up nearly as often. If you *do* feel like a generally-useful utility function is missing from a library, don’t create a whole new package just to hold that one function… **just submit a pull request to the original package to add the function!** If it’s truly useful, it will probably be accepted, and this is far more useful than creating an entirely separate package when the functionality really belongs in the library itself.
The memoized value is carried by the `let` in `Memoize ()`. All the other instances are composing the function to memoize `f` with constructors. If you unfold the recursive calls of `memoize`, you get a trie, i.e., a tree where every maximal path in that tree corresponds to one input value, leading to a leaf of the shape `let b = (f . constr1 . ... . constrn) () in const b`, where `constr1 . ... . constrn` is the constant function equal to that value.
I don't mean to ignore type signatures, I mean it's hard to research the meaning and implications of the type signatures because the meaning is encoded as a grammar over punctuation (the way we express typeclasses, existentials, and fundeps, for example) that you can't easily "type into google" unless you know the right vocabulary words to match the punctuation.
You certainly can parse into the desired AST. Or you can write a conversion function from your grammar AST to your desired AST. It all depends on you. I'd say writing a conversion function might be easier, though I can't say for sure for your situation.
I had used ocaml module on toys a bit, and had read about their theory (again abit) at ATAPL, it seems nice to me, but OCaml/SML ppl seems to be in love about it. Does it work significantly better in larger size project, and does the OCaml people use the same module signature in different library implementing the same feature (for example, if I come across two ocaml gui library, will they use the same signature, so I can swap to another one with literally just one line of change?)? That would sound magical to me.
I agree with this. It's hard to give the formal criteria of what is a package. I don't like tiny packages because every good package takes quite a lot of effort to maintain: support multiple GHC versions, write proper documentation and tutorial, write decent tests and probably benchmarks, etc. It's better to have single good package rather than multiple bad packages. For example, there's this package and I don't like the fact that this is a separate package: * http://hackage.haskell.org/package/safe-foldable On the other hand, I have my own very small package: * http://hackage.haskell.org/package/hash-store But it's already going to be used in different packages and there're plans for more improvements and additions. So probably makes sense to have a separate package?... When I want something useful, I probably will try to find existing package where I can add this functionality (something like [`containers`](http://hackage.haskell.org/package/containers) package) and will open issue to discuss the proposal. Anyway, in our company we use custom prelude so we can always put small general and commonly used functions into it. With `base-noprelude` trick it's quite convenient to extend custom prelude for exact package needs. 
Very cool! Do you have any rules for cleaning up generated files? I.e. if I remove a Markdown file, can the generated HTML be auto-removed (like `rsync --delete`)?
I'm trying to use `nix` for building two local packages (one depends on another), and got strange shadowing issue: libb &gt;&gt; nix-shell --command zsh these derivations will be built: /nix/store/2ddm8i6my675a7lncm4frkcasprf5gxl-liba-0.1.0.0.drv /nix/store/7wmsgz49759pff7ikky2nksxn8cckr63-ghc-8.4.3-with-packages.drv building '/nix/store/2ddm8i6my675a7lncm4frkcasprf5gxl-liba-0.1.0.0.drv'... setupCompilerEnvironmentPhase Build with /nix/store/nv4h00ma4lmwj9bnc8dhf9nzyj28d6ds-ghc-8.4.3. ignoring (possibly broken) abi-depends field for packages ignoring (possibly broken) abi-depends field for packages unpacking sources unpacking source archive /nix/store/nf0yf8xsi7l92ark5q08ddhrkh2syy9b-hi source root is hi patching sources compileBuildDriverPhase setupCompileFlags: -package-db=/private/tmp/nix-build-liba-0.1.0.0.drv-0/setup-package.conf.d -j1 -threaded Loaded package environment from /private/tmp/nix-build-liba-0.1.0.0.drv-0/hi/.ghc.environment.x86_64-darwin-8.4.3 &lt;command line&gt;: cannot satisfy -package-id liba-0.1.0.0-inplace: liba-0.1.0.0-inplace is unusable due to missing dependencies: aeson-1.2.4.0-KPiiXcK6dELrtdAOauWhV (use -v for more information) builder for '/nix/store/2ddm8i6my675a7lncm4frkcasprf5gxl-liba-0.1.0.0.drv' failed with exit code 1 cannot build derivation '/nix/store/7wmsgz49759pff7ikky2nksxn8cckr63-ghc-8.4.3-with-packages.drv': 1 dependencies couldn't be built error: build of '/nix/store/7wmsgz49759pff7ikky2nksxn8cckr63-ghc-8.4.3-with-packages.drv' failed I've reported this question as an issue here https://github.com/Gabriel439/haskell-nix/issues/56
That might be the only way to achieve what I wanted. I'm [checking](https://github.com/snoyberg/xml/issues/130) with the package maintainers to see if there is a plan to update the whole package to `MonadUnliftIO`.
If I recall correctly, some in the C++ world advocate using exceptions only for rare hardware failures, contract violations and the like. Your app should almost always execute without any exceptions, to the point where throwing an exception is something log-worthy. Thus, reading from a file handle may throw an error because the file disappeared during the operation, but it should not throw because of EOF. Is this how you decide on IO exceptions vs Maybe/Either/MonadError?
By "static" web server you mean a server for static pages?
Yes.
Ooh, this is nice! Thanks for sharing /u/int_index! It somehow flew under my radar in the past days.
If you want to learn haskell, I'd recommend [http://learnyouahaskell.com/](http://learnyouahaskell.com/) Monoid is a well known abstraction in haskell land, so I assume that you actually want to learn haskell, read an article which isn't aimed at beginners and express your frustration by criticizing.
I think it would be more productive to use a neutral tone, especially in a reply to a provocative comment.
LYAH actually is not a good tutorial since it's very old and outdated. Haskell changed a lot.
Oh we can handle concrete examples all right, but good abstractions have a much longer shelf life than specific instances.
TL;DR: `Functions: "\b&lt;args&gt;\b[ \t\n]+::"` `Types: "(data|newtype|type)(\ +)\b&lt;args&gt;\b"` `TypeClasses: "class(\ +)\b&lt;args&gt;\b"` `Constructors: "\|[\t\ ]+\b&lt;args&gt;\b"` The one looking for a data constructor is pretty terrible, how do you search those?
Looks like a decent improvement to this Emacs package: * https://github.com/jacktasia/dumb-jump
No nothing yet! Seems a bit like that would be a shake concern and I haven't seen any shake primitives for that yet. You could always delete everything and rebuild, not ideal but it would work.
One small side note that might be of interest for people reading this article. I used the silver searcher for awhile, but I recently discovered another tool called [ripgrep](https://blog.burntsushi.net/ripgrep/) which is much better. It's faster and it doesn't have the [issues with gitignore behavior](https://github.com/ggreer/the_silver_searcher/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+gitignore) that have plagued silver searcher.
It's possible without any problems! You can have type ExchangeMap = TypeRepMap (Money.ExchangeRate "USD") -- :k (Money.ExchangeRate "USD") == Symbol -&gt; * So then you should be able to do `lookup @"EUR"` or `lookup @"BTC"` to get exchange rate from `USD` to your currency. If you're interested in complete example, you could open issue on GitHub and we will try to give complete example.
I agree, this does seem possible.
I'm using `ripgrep` for more than a year and I really like it! Also, it's written in Rust (and silver searcher is in C) and I like to endorse better technologies ^_^
This seem really cool! Is the source published somewhere?
Silly question: why are they called "Finger Trees"?
Super nice! Would be incredible if it could round trip and reinsert the correct imports when functions using them reappear. 
Should be possible, at least with some variant of `-fdefer-type-errors` to get the module through the renamer.
I use a version of this monoid (on arrays) when working with cache oblivious maps. https://www.youtube.com/watch?v=WE2a90Bov0Q It combines with a functional version of the Overmars and Van Leeuwen dynamization scheme to make a pretty efficient map!
#### [YOW! Lambda Jam 2014 - Functionally Oblivious and Succinct - Edward Kmett](https://www.youtube.com/watch?v=WE2a90Bov0Q) ##### 502 views &amp;nbsp;👍6 👎0 *** Description: This talk provides a whirlwind tour of some new types of functional data structures and their applications.Cache-oblivious algorithms let us perform o... *YOW! Conferences, Published on Jan 22, 2015* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details. Respond 'delete' to delete this.) ^(|) [^(Opt Out)](http://np.reddit.com/r/YTubeInfoBot/wiki/index) ^(|) [^(More Info)](http://np.reddit.com/r/YTubeInfoBot/)
It depends on what kind of package you're making. I don't think anyone wants a left-pad package, but some small packages are quite useful: * Packages providing orphan instances. Often two packages don't want to depend on each other, so a third package is made to provide instances for them. For example, I find [iproutes-aeson](https://hackage.haskell.org/package/aeson-iproute-0.2/docs/src/Data-Aeson-IP.html) very useful * Packages with a tightly defined goal. If your package claims to give me a list of ISO-3166-2 country codes, that may be all it needs to do. I think there's a place for both smaller packages (e.g. a list of ISO-3166-2 country codes and some instances) and larger ones (e.g. a package that can give me the regions of every country and convert between ISO formats). Converse: avoid "kitchen sink" packages that give me random utility functions. * Small packages supporting larger ones. Examples of this are packages trialling functionality that could be worked into a bigger package later, or functionality that is close to, but not within scope of the original project (for example, if someone views their package as a low-level crypto package, and your package wrapped it to provide utility functions like \`hashPassword\`, that could still be useful (assuming the original package doesn't want to offer those higher-level functions.
Each node has a fixed number of children which the creators called 'digits'; i.e. fingers; basically just relates to the shape of the actual data-structure
Code: [ [Untyped](https://gist.github.com/ekmett/f081b5e36bac3fed1ea6b21eb25327c6) | [Typed]( https://gist.github.com/ekmett/ac2bef9de19881d6286044a06936dd55) ] 
`toSort` is really inefficient for lists, because you're only adding one element at a time. It effectively degrades to an insertion sort. It'll work much better on a balanced tree. Alternatively, you can use the `Monoid` instance of a priority queue, which is very similar to what I did in my performance improvement to /u/ElvishJerricco's `Traversable` sort (that actually used `Applicative` rather than `Monoid`, but you don't need that here).
Yup it's pretty terrible, not really the point of the post though, I just like exploring monoids. I originally designed this monoid for use with a fingertree which would be combining caches of subtrees and thus would be doing more actual "merge"ing
[YouTube link](https://www.youtube.com/watch?v=PwD7D7XUzec)
Can't figure out if this is sarcasm or not...
I'm serious, I haven't really ever seriously dived into GHC core.
`haskell-ide-engine` seems to be a very interesting choice. I used it with `lsp-mode` and `lsp-ui` in Emacs and it works perfectly for me. 
Just a nitpick: this is merge, not mergesort. Mergesort is the full divide and conquer sorting algorithm, merge is just the conquer step. 
I like ripgrep a lot. I've been using it with [vgrep](https://hackage.haskell.org/package/vgrep) — it needs the `-v` option, as in rg -v search_text | vgrep
Sounds like this might be a good tool for haskell-ide-engine or maybe just in the vim and emacs (and vscode) haskell modes.
Supported formats?
Nice! Any plans to add this to hvr's `haskell-ci` so that one has one script that generates files for both Travis and AppVeyor?
.srt format only. I totally forgot to mention it. Have added it to the landing page "how to?" message now.
Thanks. Any plans for other formats like ass?
Thanks. Here is the source repo [https://bitbucket.org/sras/subtitle-fixer](https://bitbucket.org/sras/subtitle-fixer). 
Have never come across that format. Let me see if it can be quickly added. 
Very cool. The demo loaded instantly and worked great on my phone.
I've currently reached a stage in Haskell where I've tackled (arguably) difficult concepts, such as Recursion Schemes with F-Algebras, Monad Transformers, GADTS, Phantom Types etc. So far, I've avoided learning Category Theory in formal detail. I'm aware this question has been asked in various forms, however, am I at a stage where my progression as a Haskeller/functional programmer is significantly obstructed or slowed by my lack of Cat theory knowledge?
awesome! Thank you for writing and open sourcing this tool. Real world projects like this are a fantastic thing to have more of
Android, IOS, or something else?
It's a new Moto G6 with Android 8.0.0 apparently. Using chrome
How do I keep GHC up to date using Stack?
Yeah, that's part of the plan! Just working out some things around the version numbers on chocolatey and the script will be updated :) 
Adding imports on-the-fly would be really nice! This is the ultimate goal (so we can forget about writing imports manually). It's not immediately clear to me how to write _round trip_ or how to use `-fdefer-type-errors`... Adding imports requires to implement more complicated logic. Maybe `smuggler` can collect information about removed imports to return them back... For now closest solution for this problem seems to be copy-pasting and changing `printMinimalImports` function from `ghc`: * https://github.com/ghc/ghc/blob/2b1adaa7817c453df868d928312a9a99a0481eb1/compiler/rename/RnNames.hs#L1452-L1522 GHC already has ability to report imports which should be added. And since we use GHC API, it should be possible to do similar things.
Honestly I think people dramatically overstate the importance of category theory in Haskell. You can be an absolute expert at Haskell with almost zero category theory knowledge, and you won't have had much more difficult of a time getting there. The only time it will seriously hurt you is when you see a library deeply rooted in category theory that does something you want, but such libraries usually only come from Ed Kmett :P I think knowing category theory can solidify knowledge you may already have about Haskell-isms like monads and monoids, but it's far from necessary to understand those things.
You can always use `--compiler ghc-x.y.z` to tell Stack to use whatever version of GHC you want (supposing it can find a release of that version configured for your system). Or you can add `compiler: ghc-x.y.z` to your project's `stack.yaml`.
Is there a way to do this globally without that flag?
You can probably put that `compiler: ghc-x.y.z` line in the global project file `~/.stack/config.yaml`. Not sure how that will interact with projects; they might use the GHC from the project's `stack.yaml` anyway.
On Hackage now [http://hackage.haskell.org/package/instance-map-0.1.0.0](http://hackage.haskell.org/package/instance-map-0.1.0.0)
&gt; the return value of a function cannot depend on the value of its inputs. s/return value/return type/ Otherwise Haskell doesn't seem to be such a useful language at all :P
You can only replace a recursive call with a jump when it is a tail call. Otherwise, it needs to be an actual call (or you need to perform some other optimization that turns it into a tail call).
Is there any up-to-date tutorial you recommend?
can you link to your emacs configuration?
Haha.. yes that would be a pretty severe restriction. Thanks, fixed now.
You'll see the Haskell book recommended a lot as it's currently one of the best solutions out there. A lot of people find it crushingly verbose and long winded, but I never found it to be that way personally. There's another book floating around that's quite new and ~300ish pages but I don't remember the name of it... I vaguely recall the author being Russian but I could be mistaken.
This is neat, but my favourite smuggler is still [`acme-smuggler`](https://hackage.haskell.org/package/acme-smuggler).
wow, just saw this, it is fantastic - can't wait to dive into this after work : )
Sounds like you and I are interested in overlapping problem spaces :) You might want to take a look at [exinst](https://hackage.haskell.org/package/exinst), which uses the singleton approach to solve this exact problem. I gave [a talk](https://www.youtube.com/watch?v=PNkoUv74JQU) on its implementation.
Why do you use `hash-store` rather than the `pluginRecompile` field of a plugin to specify the recompilation behaviour?
Because it doesn't work for `ghcid` with `pluginRecompile`. `smuggler` changes content of the file by writing new content back to Haskell module and this leads to infinite loof for `ghcid`. So there should be logic to not change content of the file if content is the same as it was during previous compilation. Setting `pluginRecompile` to `purePlugin` is not enough.
Thanks a lot, that's reassuring! I was hoping to do my masters thesis about applications of hylomorphisms with neural networks, and was hesitant about the lack of elaboration with its relationship with cat theory, affecting the quality of my thesis. Clearly I would make the effort to delve into the material as necessary - but was interested in what would be your personal take on this? Is academic research into functional programming a entirely different problem w.r.t my original question?
I wouldn't say it's entirely different but I would definitely be more inclined to research category theory for a thesis. I don't know your subject that well, so all I can say is that if it feels like you're shoehorning category theory in where it isn't directly meaningful, it's probably not necessary. This would be true of any tangent in an acedemic setting, I think.
You can specify a hash yourself for this more complicated logic. Could that be sufficient?
This is phenomenal, amazing work! Not sure how I missed it. I think making the available via Choco addresses a large set of businesses with a “windows first” mentality, and hopefully eases one of the hurdles for adoption. 
The [`removeLegacy` function in Milkshake](https://github.com/emilaxelsson/milkshake/blob/master/src/Milkshake.hs) does what I was asking for. I haven't had time to play with Slick yet, but it seems like `removeLegacy` could be used.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [emilaxelsson/milkshake/.../**Milkshake.hs** (master → e25d86b)](https://github.com/emilaxelsson/milkshake/blob/e25d86bc28741da71d5e2eb73303846e53ff8c81/src/Milkshake.hs) ---- 
I assume you're talking about `MaybeRecompile` constructor of `PluginRecompile` data type. Using this constructor still requires to calculate `Fingerprint` of file. This is problematic because `pluginRecompile` has the following type: pluginRecompile :: [CommandLineOption] -&gt; IO PluginRecompile It doesn't know for which file it needs to calculate `Fingeprint` so I have no ability to calculate hash for current file manually and specify it. The only option is too decide whether I need to write to file is to do it inside `typeCheckResultAction`. Another note: `Fingeprint` uses MD5 for hashing and `hash-store` uses Blake2B. Blake2B is supposed to be faster than MD5. And in the future Blake2B will be replaced with `murmur` hash in `hash-store` (because it's the fastest hash and we don't need cryptographic properties).
Exinst does look cool and I'll have to take a closer look. 
In the following project - [https://github.com/jonoabroad/simplified\_cabal\_example](https://github.com/jonoabroad/simplified_cabal_example), the tests can not "find" the \`Main\` module. Is this due to my misconfiguration of cabal file or should the executable contain no testable code - being a entry-point into the code and all testable code be in a library, similar to [https://github.com/willprice/haskell-cabal-example-tests](https://github.com/willprice/haskell-cabal-example-tests) 
tl;dr: historical reasons `Monad` has, for silly historical reasons, a `fail :: String -&gt; m a` method. The MonadFail proposal’s purpose was getting this method out of the class, thus the `MonadFail` class was born. In order to maintain compatibility, nothing else was changed about it – the pain was having `fail` in `Monad`, and not really its existence per se. I don’t think the proposal would have gone through in any way had we changed `fail` in any way.
Thank you, Excellent work! i have trouble setting up proper window CI for some days, this definitely solve the headache.
The problem with `fail` is that it was initially added to the `Monad` class, even though it doesn't make sense for many monads. (The reason for this was for handling incomplete pattern matches.) The [MonadFail proposal](https://prime.haskell.org/wiki/Libraries/Proposals/MonadFail) fixes this by removing `fail` from `Monad` and putting it in a separate class, `MonadFail`. In order to have the simplest migration for legacy code, it makes sense that it should have the same type signature. If you want to throw exceptions in a monad, it would be better to use `MonadThrow` from [Control.Monad.Catch](https://www.stackage.org/haddock/lts-12.2/exceptions-0.10.0/Control-Monad-Catch.html) in the exceptions package, which has: throwM :: Exception e =&gt; e -&gt; m a
I think the haskell ecosystem is currently migrating away from `exceptions` and `monad-control` towards [`unliftio`](https://github.com/fpco/unliftio/tree/master/unliftio).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/unliftio/.../**unliftio** (master → c786990)](https://github.com/fpco/unliftio/tree/c7869907f6b372f74c1c58b20767be937d0eb1d9/unliftio) ---- 
There weren't any 'problems', it's just didn't feel intuitive enough (it seems to be made for certain workflow and not for the fully automated workflow). There are some 'automatic' things regarding certificate renewal that you'd like to be more configurable. However, it is definitely possible to make it work, it's not as if it didn't work.
So the overall idea is that this is for backwards compatibility reasons? A very valid argument, but would this design make sense?
Ah, so \`MonadThrow\` really follows the design I'm looking for I suppose.
Sure, it would make sense, but there are lots of others that make sense too. I’d be strongly in favor of not having `fail` at all, anywhere, since I do not think implicit failure in do-notation is good design. We could insert an `Exception`, but we could also use anything `show`able or `mzero` and so on – you’d have to find something clearly better than the alternatives. If all you want to have is throwing/catching capabilities, there’s [`Monad{Throw,Catch}`](https://hackage.haskell.org/package/exceptions-0.10.0/docs/Control-Monad-Catch.html), which is specifically for throwing/catching, and has nothing to do with do-notation desugaring.
I think I've just realized I'm not entirely clear on the relationship between `Monad{Throw,Catch}` versus `MonadFail`. As I understand it `MonadFail` allows for leveraging failing behavior when desugaring do-notation. Shouldn't `MonadFail` then subsume `MonadThrow`? They do the same thing from an operational perspective. Does that make sense?
Yup, that's right.
I don't think this is as widespread as your comment seems to suggest; `MonadUnliftIO` has some simpler types, but at the cost of less functionality. Lots of times you really and truly do want `MonadBaseControl`.
Thanks! A few of them are already on my reading list, especially TAPL, but it's so daunting, I think it's gonna cost me a year of my life to complete that :).
Hey! I have added support for ssa/ass subtitles. These subs are a bit hard to find. I have tested it with a couple of them that I could find, and it seems to work. 
I have yet to encounter a scenario where I felt MonadBaseControl was a sane answer. If the transformer stack is complex enough that MonadUnliftIO doesn't work, then it is too complex to sanely reason about generically lifting arbitrary non-algebraic effects.
Hey, just tried the tool, but it seems that the javascript you have is not working properly. POST https://sras.me/subfixer 400 (Bad Request) send @ jquery-3.3.1.min.js:2 ajax @ jquery-3.3.1.min.js:2 fileReader.onload @ subfixer:76 load (async) (anonymous) @ subfixer:73 dispatch @ jquery-3.3.1.min.js:2 y.handle @ jquery-3.3.1.min.js:2
On [Hoogle](https://www.haskell.org/hoogle/) or [Hayoo](https://hayoo.fh-wedel.de/) you can search for type signatures. [This page](https://wiki.haskell.org/Keywords) has an overview of keywords and symbols, and [this page](https://downloads.haskell.org/~ghc/8.4.1/docs/html/users_guide/glasgow_exts.html) is - I believe - a complete list of language extensions.
Yeeeah, I think /u/ElvishJerricco got it right - you're copying the array again and again and again, which is slow, while in the C version, you're just mutating a single value in an array, which is constant-time. So, two obvious ways to fix it. (a) use a mutable array type, or (b) drop the array approach - instead of allocating an array in advance and then filling it out, see if you can figure out a way to build up a linked list by consing values onto it, which is also constant time.
Yup, it would make sense to use MonadThrow if we did not have `fail` but wanted similar behavior.
Great analysis! There's one thing, I don't understand how did you got to the conclusion from the frequency histogram: &gt; So, you can read 90% of the Haskell files on github using only 10 extensions, and 95% using only 10 more! Shouldn't this be more like: &gt;10 most frequent extensions are present in 90% of Haskell files ? You were only measuring occurences, not counting files with the amount of `LANGUAGE` pragmas inside them. 
If I understand correctly, this only parses the case where each extension has it's own LANGUAGE pragma. What about `{-# LANGUAGE Ext1, Ext2, Ext3 #-}`? Also, the `LANGUAGE` keyword can be lowercase. I don't know if you accounted for that.
Would it be Possible to share the sub file? I think the parsing is failing on your sub for some reason.
Until recently, the entire Yesod ecosystem was using `MonadBaseControl` and `exceptions`. Now all of that has moved over to `unliftio`. That is a pretty big shift of weight.
Nitpick: some flags are on by default, so by stripping out the flags which begin with `No`, your analysis skips common extensions such as `NoImplicitPrelude`.
I'm planning on working on a similar thing at the moment, thanks for sharing.
&gt; Also, the LANGUAGE keyword can be lowercase. In fact, all pragmas are case-insensitive.
GitHub searches are case insensitive, and search by tokens, not exact string. See, e.g. [https://github.com/search?q=LANGUAGE+overloadedstrings&amp;type=Code](https://github.com/search?q=LANGUAGE+overloadedstrings&amp;type=Code) (if you're logged into gh). The last result on that page matches `{-# Language NoImplicitPrelude, OverloadedStrings #-}` The obvious downside is that this probably also matches code that looks like \`let msg = "This language supports overloadedstrings"\`, but I think it's a pretty reasonable estimate.
&gt; Cabal files. Yeah, that would be interesting to add and see how much it changes. I'd expect at least the relative frequency to be similar, but this could very well increase how frequently extensions pop up. But I think that it's probably much more common to enable extensions on a file-by-file basis than a project basis. It would be cool to see what effect this has, but it's probably not doable with the GH API. &gt; You were only measuring pragmas occurences, not counting files with the number of LANGUAGE pragmas inside them. Hmm, the way I understood it, github's global code search only gives you one hit per file, but I could be completely wrong here. &gt;10 most frequent extensions are present in 90% of Haskell files. I guess what I said wasn't quite correct, but this isn't it either. What the histogram shows is the fraction of haskell files that have the language pragma *out of the total number of haskell files with language pragmas*. So if you look at, say, `GADTs`, around 8% of haskell files that use pragmas use GADTs; that is, 92% don't use `GADTs`. `OverloadedStrings`, by far the most popular, occurs only about 25% of the time. When I was writing this I was imagining a hierarchy of language extensions, so if you enabled one you had to enable all the ones before it, but that's clearly wrong. I'll try to rewrite that paragraph to be both clearer and more correct when I get a chance.
Yeah, I originally made the big plot with all of them, with that exact extension in mind, but it turns out that none of the `No...` flags are very frequent, and nobody wants to look at a plot with 235 elements.
I didn't for a moment mean to imply that you should re-write it. This work is really cool. DT is the way of the future, and it's great that you are exploring it in this way. If "it's not about inconvenience of type-level programming", then why not just make each lookup O(log n)? That would solve the problem. It's easy to do in TH. We still have a long way to go before compile-time programming at the type level will become nearly as convenient as TH is now. And the relationship between type level programming and macro programming is still not fully understood. Great work like this library is a big step forward.
Hi! If you give my code a try I'll be glad to have any sort of feedback.
As a relatively new to haskell i wonder why those extensions aren't part of the language - it seems like almost anyone uses them anyway.
I'm using sdl2 and sdl2-mixer. I have this weird bug where the music stops playing at a random point. I can "fix" the bug or make it worse (e.g. music stops even earlier) by deleting random parts of the code that has nothing to do with sdl-mixer code. This makes me think it's some kind of garbage collection or C-related bug in how the Haskell sdl2 bindings work with the C libraries. Any ideas on how I can debug this? I'm not familiar with Haskell debuggers other than generous amounts of Debug.Trace, which may not really help me here.
How do you imagine O(log n) lookups in a type like `a -&gt; b -&gt; c -&gt; d`? You'd need to put `a,b,c` into some sort of a tree-like structure instead of a curried type, and generating the expression that would flip the arguments would be still O(n) worst case because you've got to skip all preceding arguments. I don't see how TH would be of any help here.
Because Haskell the language is defined in a specification document called the "[Haskell 2010 Language Report](https://www.haskell.org/onlinereport/haskell2010/)", which is written by a committee and so moves very slowly, while those extensions are, literally, extensions to the language as defined in that report. They are supported by GHC but not necessarily by other Haskell compilers. The next edition of the report is expected to incorporate the most common of those extensions into the language proper.
Is it known when next edition will be issued?
I'm surprised to see `Strict` in the top 10
Yep, looks like that could work! I'll check it out later, might be worth borrowing that. Anything that runs in shake should be compatible!
It is [2020](https://prime.haskell.org/wiki/Committee).
Oh, that's long time
Not really: the committee is really slow, folks say.
Maybe try picking whichever of `X` and `NoX` is higher, and plotting that?
I can meaningfully `fail` in something like `Either String`. It isn't big enough to hold every possible `Exception`, though. If all you _want_ is a string about the error, then `fail` as it exists is the right-sized abstraction. It demands far less of the implementor, uses a much cheaper code path, etc. It also has other benefits, such as that it is something that can be implemented in the language standard. It doesn't use anything we don't fully know how to standardize today.
It'd also be interesting to see extension use at the package level of granularity. For example, the graph has `FlexibleInstances` in second place, but often that will only need to be enabled in one or two modules that define instances alongside either classes or types - but those modules are usually the really important ones.
There currently isn't any requirement that the control flow of `fail` and `throw` have anything to do with one another. They both have the same general feel, but whether you can `catch` a `fail` and how, can vary fairly drastically from monad to monad. We do not today have a class constraint that says 'MonadFailsAsAnIoException`.
This subreddit is not an appropriate place for feature requests; we'd have way too many such posts if it was. Please find a project-specific venue; in the case of slack, the appropriate venue would be its [github issues](https://github.com/commercialhaskell/stack/issues) page. An issue about [bringing back statically-linked executables](https://github.com/commercialhaskell/stack/issues/4088) already exists there.
IIRC, GHC indeed emit `JMP` for entering a function call, so technically *we just convert the recursive function call into jumps*.
Actually, this is the case in Coq for functions that return things of kind `Prop`. It is consistent in Coq to assume that every object in `Prop` is equal.
Would that not make it possible to make the typechecker diverge 😉?
My personal verdict is that Category Theory is overly hyped. However given the fact that you hope to work with recursion schemes in the future(as stated by you in a later comment) you will find most of the recursion schemes papers(by Gibbons, Hinze, Wu, Hutton etc) are loaded with basic Category Theory. Most of those papers have basic introduction to the parts of CT that they use, but it might help you to formally learn it from a book. If you are interested to know about general programming in Haskell, you can tackle that with absolutely no knowledge of category theory. The lens library is the only *major* Haskell library that I encountered whose foundations are based on Category Theory. Even then, I myself did a literature review and survey on profunctor optics and I would say it does not require a very in-depth understanding of CT. Also, I myself personally invested a lot of time in formally educating myself about Category theory and I was quite disappointed by the return on investment on that time. My personal suggestion would be to spend that time learning to work with graphs and other advance data structure etc and also acquiring the ability to express imperative algorithms(presented in most algorithms books) in a functional way.
How have I not even heard of these until now?! Thanks for your post, these look really cool. Here's a more technical paper introducing them (using Haskell): http://www.staff.city.ac.uk/~ross/papers/FingerTree.pdf &gt; Abstract: We introduce 2-3 finger trees, a functional representation of persistent sequences supporting access to the ends in amortized constant time, and concatenation and splitting in time logarithmic in the size of the smaller piece. Representations achieving these bounds have appeared previously, but 2-3 finger trees are much simpler, as are the operations on them. Further, by defining the split operation in a general form, we obtain a general purpose data structure that can serve as a sequence, priority queue, search tree, priority search queue and more. 
The cabal file specifies the dependencies of the different build targets. The test suite `spec` depends on `base` and `hspec`, so only modules defined in these packages are visible. I don't know of a way to depend on an executable, but you could create a library that both the executable and the test suite depends on.
Don't use stack.
Is it picking up the word strict and assuming it's an extension?
Next stream is scheduled for 5PM EDT
I know right? Of you'd have told me I can use the same data structure for arrays, heaps and text editors I wouldn't have believed you. I'd love to see more data structures that are "parameterized" by other structures like this!
Yes, that's likely. GitHub's search is quite fuzzy.
What would be the cost of getting rid of `fail` altogether? It would on one hand make the implementation of Monad more principled and on the other enforce more consistent exception throwing standards.
Glad you find it useful :)
Yeah, Chocolatey is getting increasingly popular. which is a good thing :)
&gt; What would be the cost of getting rid of fail altogether? Enormous compatibility problems, especially with older libraries. A language extension to disable implicit failure in do-notation was implemented at some point, but never made it into GHC HEAD I think.
I see you use ``` plugin = defaultPlugin { typeCheckResultAction = smugglerPlugin } ``` but why not ``` plugin = defaultPlugin { renamedResultAction = smugglerPlugin } ``` It seems that you should only worry about name resolution, but not type checking. 
splitting out into a library was my thoughts as well, I'm a noob so wanted to be sure I wasn't missing something. Thank you.
Would it be possible to use something like this to detect unused language extensions?
Yes, this makes me doubt a lot of the results. Safe, Unsafe, etc. probably fall prey to the same problem. (That isn't surprising; SafeHaskell is nowhere near so popular as to occur in 10% of source files!)
You really don't want `Sort` to be a be a `Functor`, since, for instance, `fmap (fromList [3,1,4,1,5,9] !)` won't preserve sorted order.
Quite right! Good catch, not really sure why that was in there to begin with.
It's not quite production ready (maybe 65% of the way there) and it's currently much slower than Silver Searcher but our [refactorio](https://github.com/superpowerscorp/refactorio/) tool might be helpful in some of these scenarios. For example searching for type constructors looks something like: refactorio -t src --search --haskell _Module.biplate._TyCon.end and likewise applications of type constructors can be singled out via: refactorio -t src --search --haskell _Module.biplate._TyApp.end with the results looking something [like this]( https://i.imgur.com/dNpAbz4.png).
I think that `hlint` and `stylish-haskell` already can detect and remove automatically some unused languages pragmas for you. `smuggler` uses GHC API to detect unused imports. I didn't see any ghc functions that can detect unused language extensions. But if GHC has it then it's quite possible to remove redundant language pragmas.
Documentation for `renamedResultAction` was quite confusing to me. In docs I see that it's `Maybe` of action (which is already weird) and type of `renamedAction` doesn't contain `TcGblEnv` which is needed in order to report unused imports. * https://github.com/ghc/ghc/blob/master/docs/users_guide/extending_ghc.rst#source-plugin-example Working with source plugins is already hard because there're not so many documentation and examples. Now after your comment I see that in `ghc` source code type of `renamedResultAction` has changed. It doesn't contain `ModSummary` but this should be manageable. * https://github.com/ghc/ghc/blob/1a79270c72cfcd98d683cfe7b2c777d8dd353b78/compiler/main/Plugins.hs#L60-L61 Could you share any benefits of using `renamedResultAction` instead of `typeCheckResultAction`?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**extending_ghc.rst#source-plugin-example** (master → f0d27f5)](https://github.com/ghc/ghc/blob/f0d27f515ffbc476144d1d1dd1a71bf9fa93c94b/docs/users_guide/extending_ghc.rst#source-plugin-example) ---- 
We could try to add this to `summoner` in order to generate better CI for Windows! * https://github.com/kowainik/summoner
You could try to use `hgrep` to search inside Haskell code: * https://github.com/thumphries/hgrep
That's funny, this is one of the first things I built in Haskell about 10 years ago. However, it was just a hacky script, not a web service.
With the current situation besides `cabal` file one needs to check `package.yaml` which is using `yaml` format, so basically these extensions could be in any `.yaml` file now. And an interesting fact that they can be in all mentioned files + in the module itself, so calculating the sum is not the fair thing to do here. Not to mention that there is a bunch of hackage projects that are on the other version control systems. So, in fact, it's quite tricky to get the information that will reflect the precise situation in Haskell world. 
&gt; it does not use cabal at all (it supports Nix and Hpack configurations... Why you need to fracture Haskell ecosystem even more? :cry:
edited to reflect that this is not the intention
https://github.com/parsonsmatt/servant-persistent/blob/master/src/Config.hs
Very cool! Can you explain something about the inner workings? What’s the trick? One derivation per source file? Using nix does mean that you cannot prune the build when output hashes are the same, right? So you might need to rebuild revdeps even when one dep produces the same output for different inputs. Is that a problem in practice?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [parsonsmatt/servant-persistent/.../**Config.hs** (master → 9229cb7)](https://github.com/parsonsmatt/servant-persistent/blob/9229cb78b0e86a4a1839beccef20fae7a033956e/src/Config.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e2xnskk.)
Thanks, but I have would liked to sort out that how can i use the " withMySQLPool " and run the all program in the " ([Pool](https://hackage.haskell.org/package/resource-pool-0.2.3.2/docs/Data-Pool.html#t:Pool) backend -&gt; m a) " action. I think it's maybe more safty because the resources will be released automaticaly. But this solution is correct for me and I have to going to continue my work and I will use this approach now.
&gt; Very cool! Thanks :) &gt; Can you explain something about the inner workings? What’s the trick? One derivation per source file? Exactly, each source file is extracted into its own derivation and compiled as a `.o` in a derivation. In the simplest case, what drives the build is the `main` file (equivalent to cabal's `main-is`). The import list is parsed and any local file that matches the module name is built and added to the (parent) module compilation (this of course works recursively if the child module has imports of its own). For executables all object files are then gathered and linked together. Doing things this way has some issues, mostly regarding GC and having to rely parsing the Haskell files, but it works pretty well in practice. I'm open to suggestions on how to improve either. &gt; Using nix does mean that you cannot prune the build when output hashes are the same, right? So you might need to rebuild revdeps even when one dep produces the same output for different inputs. Is that a problem in practice? True, and in that sense snack doesn't do as good a job as haskell rules, for instance. In _theory_ I see two solutions: either use `readFile` on the object file and use that as the input to the parent module derivation. Alternatively I seem to recall that there's a `nix-store` operation that aliases a derivation with the hash of its output. In _practice_ I have no idea how to deal with that in a nice way. Any help appreciated :) 
In that case I think you can write `withMySQLPool _ _ $ \pool -&gt; runReaderT main' pool`, with `main' :: ReaderT ConnectionPool IO ()` is your app.
&gt; Exactly, each source file is extracted into its own derivation and compiled as a .o in a derivation. Awesome, I’ve been waiting for someone to do something like this. Do you think we can translate that to the way `haskellPackages` works? I imagine that might give us awesome parallelization of big Haskell builds, as long as the constant overhead of starting up one derivation per module is not too high. &gt; True, and in that sense snack doesn't do as good a job as haskell rules For onlookers: this is https://github.com/tweag/rules_haskell &gt; In theory I see two solutions: Nix does not support references by output hash (at the moment, who knows whether this will ever change). Any solution in that space is a giant hack, I’m afraid. :(
The key of the map would be the text of label, and possibly a type representing which other arguments have already been instantiated. The value would be a GADT that provides the function for applying the given argument at the appropriate function type. This obviously is not special to TH. If you can do something like that at the type level, that would be amazing! But it seems like it would be a lot harder, given that we don't yet have a standard type-level map with a simple API, or all kinds of other type-level glue code that would be needed to make this work.
The key of the map would be the text of label, and possibly a type representing which other arguments have already been instantiated. The value would be a GADT that provides the function for applying the given argument at the appropriate function type. This obviously is not special to TH. If you can do something like that at the type level, that would be amazing! But it seems like it would be a lot harder, given that we don't yet have a standard type-level map with a simple API, or all kinds of other type-level glue code that would be needed to make this work.
this PR adds `builtins.intern` which would help as well: https://github.com/NixOS/nix/pull/1502
Is there some kind of a sparse set implementation available as a library somewhere? I have a `Bounded`, `Enum` type and I will be storing collections of this type that are locally dense — that is, consist mostly of contiguous blocks, with only a few *"lonely"* points, — so I thought it would save space if I store them as a set of intervals defined by `(Int, Int)`.
Let's say I use some encoding to represent the type-level map you describe. Where would it show up in the API? As I said, we're dealing with `a -&gt; b -&gt; c -&gt; d`, not `Rec '[a, b, c] -&gt; d`. We don't have the luxury of using a `Map`.
Found the commit I was referencing, with that new nix-store op: https://github.com/edolstra/nix/commit/236e87c687065c2aa935542533b1a65282a73347
Could you show an idiomatic use of `fail` in some important/central library, preferably, in something which you think cannot be expressed otherwise?
No, I don’t know any such usage of `fail`, and everything using failable patterns can be trivially implemented manually, x:_ &lt;- doStuff -- becomes x &lt;- doStuff &gt;&gt;= \y -&gt; case y of z:_ -&gt; z _ -&gt; (fail action) The problem with compatibility is that the burden of proof is on the person introducing the change – you have to prove to me that nothing will break. And mind you, this includes blogs, books, code snippets, IRC logs and what have you.
Thx, I tried it but something was wrong because I couldn't fix the "abigous ..." problem, I'll take a chance again ...
Let me just casually put that here: &gt;Profpatsch [13:00] @zimbatm I want to see a mode where you have packages `p` and `q` and module `q.A` depends only on `p.B`, and GHC is able to compile `q.A` only with the interface file of `p.B` (edited) &gt; That would give a pretty awesome “only compile the modules you really need, down through the stack, if possible in parallel” functionality. &gt; I’d be okay if that means we can only do static binaries. :P &gt; nasm [13:05] @Profpatsch that's what snack does &gt; Profpatsch [13:05] w00t &gt; nasm [13:05] there's no concept of a package or library &gt; Profpatsch [13:06] You completely ignore GHC’s package libraries? &gt; nasm [13:06] the snack "package" is just a common config for a bunch of modules &gt; yep &gt; Profpatsch [13:06] oh. That’s kind of a game changer, is it not?
You could try [`Data.IntervalSet`](http://hackage.haskell.org/package/IntervalMap-0.6.0.0/docs/Data-IntervalSet.html) in the `IntervalMap` package.
I quite like this, as it can be overwhelming to remember how to refer to things in the various helper languages of a language’s ecosystem.
So, I'm trying to use this package in conjunction with Reflex+dom, and I am getting some intimidating-looking type errors even though I think I am using it per instructions: let counter :: MonadWidget t m =&gt; "from" :? D t Int -&gt; "till" :? D t Int -&gt; "timing" :? D t Float -&gt; "run" :? E t () -&gt; m :$ D t Int counter (argDef #from $ constDyn 0 -&gt; from) (argDef #till $ constDyn 0 -&gt; till) (argDef #timing $ constDyn 0 -&gt; timing) (argDef #run $ never -&gt; run) = constDyn $ constDyn 0 defs = defaults c &lt;- counter -- ! #from (constDyn 0) ! #till (constDyn 10) ! #timing (constDyn 0.2) ! #run never ! defaults -- ! defs ddisplay c Errors: https://gist.github.com/Wizek/b63135ccc0f02f801aadfec35d66b9d0 Any ideas what these type errors might mean? If I un-comment `! #from (constDyn 0)`, then it compiles. But then I can't use optional parameters, or can I?
I would also like to say it is not always clear whether a given extension will be liked, until people are using it in real every day code. Every one of these extensions have a chance of being superseded by something better, even ones that have been around for a long time. For example I remember TransformListComp generated a lot of excited buzz when it was being developed but no one really uses it. Whereas TypeApplications has proved to be immensely popular, and there are proposals out for more ways to use it, one of which I think might largely obsolete ScopedTypeVariables, another currently popular extension.
If anything is weird about the API, now is the best time to file a bug and request changes, they might make it into the next release! I guess it’s just a conceptual difference, in that the typechecked code has *more information* that you do not need. If GHC were different one could imagine that there are modules that GHC can rename (and thus allow your plugin to work) but not typecheck; if that were the case there would be a real advantages. But at the moment renamer and typechecker run together, so I believe that cannot happen.
* One from `IntervalMap` package is a set of intervals allowing overlaps, so probably not the one OP wants. * One from `Ranged-set` package is based on a sorted list of disjoint intervals. Membership test is (at least) O(n). * One from `data-intervals` package is based on a `Set` of disjoint intervals. Membership test is (probably) O(log n), although there is no documentation about performance. * [http://hackage.haskell.org/package/intset] (seems great but no commit since 2013) * [http://hackage.haskell.org/package/step-function]
The error appears to be that `! defaults` doesn't know when to stop looking for more parameters. Look at this part of the error message: ./proj/spec/ReflexDomSpec.hs:645:14: error: • Could not deduce (Named.Internal.WithParam' (Named.Internal.Decide Named.Internal.Defaults (m0 (D t0 Int))) Named.Internal.Defaults (m0 :$ D t0 Int) (m (Dynamic t a0))) You can see that the decision procedure tries to look into the `m` parameter. That's because we can pick `m ~ (-&gt;) ("param" :? ty)` and then `! defaults` would have to use the default parameter for `"param"`. Expect to encounter this whenever you have any function that is polymorphic in its return type. Regular parameters will work, but default parameters need to know when to stop.
This is really cool! I don't think I'll be an early adopter for this tool (I have enough problems with the ones I use already than to invite even more trouble into my life) but I really like the concept. I guess once I hear some positive reports of those who early-adopt in comparison to `stack`/`cabal new-build`, I'll give it a try.
Alright, so, is there an easy or not so easy way to overcome this? Or does that mean that named is fully incompatible with reflex-dom and possibly many other libraries? If the latter is the case, that would limit its usefulness quite a bit unfortunately since these more complex library-using functions would benefit the most from named/optional parameters I imagine.
It's something I didn't think about. As a workaround I can only suggest creating a wrapper around your monad: ``` newtype M m a = M { runM :: m a } deriving newtype (Functor, Applicative, Monad) counter :: forall t m . MonadWidget t m =&gt; "from" :? D t Int -&gt; "till" :? D t Int -&gt; "timing" :? D t Float -&gt; "run" :? E t () -&gt; M m (D t Int) ```
It's something I didn't think about. As a workaround I can only suggest creating a wrapper around your monad: newtype M m a = M { runM :: m a } deriving newtype (Functor, Applicative, Monad) counter :: forall t m . MonadWidget t m =&gt; "from" :? D t Int -&gt; "till" :? D t Int -&gt; "timing" :? D t Float -&gt; "run" :? E t () -&gt; M m (D t Int) Then `! defaults` would stop at the `M` wrapper.
That worked, thanks! I wonder if it is possible without that workaround as well somehow.
Mods please delete.
We need a delimiter one way or the other. I think the best we can do is to create this newtype: ``` newtype Result a = Result { getResult :: a } ``` and then modify `!` to unwrap `Result` automatically. So if you have ``` fn :: "param" :! a -&gt; Result b ``` then `fn ! #param a` would give you `b` rather than `Result b`.
I think writing with a final encoding gives you this kind of partial evaluation for "free," or, more precisely, forces you to do this work already.
I've happened to read through a couple and have 'learned' basic Category Theory, but mainly conceptually for purpose of learning as I find it easier getting my head around something without involving extensive formalisms. Although I haven't invested as much time as you, I too had the same thoughts - I guess I never really found anything motivational amongst the subject unfortunally. That last suggestion is brilliant and hadn't even crossed my mind, thanks load - that'll be on an immediate to-do list :) ! I appreciate your input.
Ah, as I thought as much, thanks! I agree, its quite a shoehorn - it might be unfortunate that I lack the knowledge in order to find/know what is directly meaningful. I thought that neural nets exhibited a really nice pattern that looked so naturally like a recursion scheme, so I'm pretty much chasing a butterfly!
Can this do the same thing as ccache does for C/C++ based projects where _if you recently compiled this exact same code, it retrieves it from cache_? I find that incredibly useful when working on code (e.g., switching between different branches, add test code, remove test code, &amp;c &amp;c).
To help out with errors like this, it's best if you post the source code and the full error message. /u/tungd is right. The transformation is basically going from this: foo = do pool &lt;- createMySQLPool ... doStuffWith pool to this: foo = do withMySQLPool ... $ \pool -&gt; doStuffWith pool
**EXPUNGED!** 😉
Thx, I tried the second one but got error. I'll try it again and post the error if isn't working ...
I consider it bad style to not put the extensions in each source file – you woudn’t put your `import` statements into `.cabal ` files, would you? But how common is that point of view? What are the arguments *for* putting them into the `.cabal` file.
There is a risk of that, yes. But innovative approaches and experiments should still be cherished! Consolidation can (and should) happen later!
Thank you, these comments are very useful. Also, I think it is possible that the links you put in brackets are not marked up correctly: the closing bracket appears to meld with the link.
Thank you, this looks promising.
Fixed, thank you.
&gt; one of which if I understand it correctly, I think might largely obsolete ScopedTypeVariables That sounds crazy! Do you have a link?
This is great. Let’s have more Haskell graphics and cross platform material!
It is [this](https://github.com/ghc-proposals/ghc-proposals/blob/master/proposals/0015-type-level-type-applications.rst) proposal. It is a little above my pay grade, but if I'm not mistaken instead of having a `forall a.` that has scope around your entire function the entire where clause, you could just have `@a` in your argument list in a few places and it'd be in scope only where you needed it.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc-proposals/ghc-proposals/.../**0015-type-level-type-applications.rst** (master → 5483317)](https://github.com/ghc-proposals/ghc-proposals/blob/5483317baac45bafdd16b470fc8b7ee7c56dcfc8/proposals/0015-type-level-type-applications.rst) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e2ycks1.)
It builds the function in question but it doesn't fuse it like having a JIT will, which removes all this extra calling overhead from the mix.
It does: *if you recently compiled exactly this transitive closure of inputs (say: code, build descriptions and dependencies), it retrieves it from cache*
I wish there were a version of `FingerTree` that used `Semigroups` instead, as the only time `mempty` is used is when the whole `FingerTree` is empty; you can get around having `mempty` in subtrees. This gets around a lot of the issues I'm seeing in the code for, say, the `IntervalList`, where they have to explicitly add an element that acts like `mempty` and they have to write `split` functions to handle it even though it can't actually happen.
Cool. It's been a while since I futzed with RebindableSyntax. Can it handle (&gt;&gt;=) for indexed monads now? The rank 2 type used to trip it up badly.
Yes, I was assuming the host language has a JIT in order to get the benefits.
Would GHC benefit from having a JIT? I always assumed it was unnecessary, but now I'm starting to wonder.
Should I change my OS to NixOS if I'm a Haskell user? How would that help? Is it recommended to use it as a primary OS on my laptop? Any information about the relation between Haskell and NixOS is appreciated.
Maybe (hopefully) somebody else can comment here with knowledge of how to get this working, but from browsing discussions and docs it seems like you're better off going with a Nix-based build instead of Stack when trying to set up a Reflex project. I don't have experience with a from-scratch Reflex project, but I've been trying out https://github.com/obsidiansystems/obelisk and it works fairly well with ghci. The flow I've gotten to work there is: 1) `ob init` to set up project 2) `ob repl` will get you into a repl where everything is (hopefully) loaded 3) alternatively, `nix-shell -A shells.ghc` to get into a shell where you have `ghc` and `ghci` based on what the project is. Then you can start whatever IDE you want from that shell and they should have access to `ghc` and `ghci` if they need it. 
What was the problem? Worked just fine for me [in 2014](https://gist.github.com/gelisam/9845116).
This sounds a lot like IPFS and similar products. Any plans to leverage or be compatible with them?
Weirdly, I was thinking of almost exactly this on a bike ride home a few days ago. Maybe I should think of other good ideas.
If anyone has the answer, the question was also posted at https://stackoverflow.com/questions/51505171/how-can-i-use-ghci-with-my-reflex-project-using-only-stack-yaml (and you can collect points there)
I should add that I am currently having lots of fun developing `reflex`-stuff using just plain `cabal` (although it helps to use the development version of `cabal-insteal`). See https://github.com/nomeata/ghcjs2gh-pages/
Indexed Monads in the style of Kleisli Arrows of Outrageous Fortune: https://hackage.haskell.org/package/index-core-1.0.4/docs/Control-IMonad-Core.html Their bind has a higher rank unlike Atkey style indexed monads.
We're really not better off with Nix, we've been down that road and we're rejecting Nix.
I think you mean [proposal #155](https://github.com/ghc-proposals/ghc-proposals/pull/155) possibly in conjunction with [proposal #126](https://github.com/ghc-proposals/ghc-proposals/pull/126/).
Somewhat relatedly, I always wanted Monad operators that can be used for both normal and indexed monads. My attempt had a closed type family to dispatch and a wrapper that uncurries potential indexes. It was horrible to use but ghc got a lot less fragile with type hacking since the early days of TypeInType.
I have got nice results with SVG manipulations via `threepenny`. Used transforms, etc., but no gradients yet.
For the specific problem that inspired this, something like the nar (nix archive) format also seems applicable.
I'd be curious to hear why, if you don't mind sharing.
nmattia showed an early version of snack to me at ZuriHac and I was blown away. Very interesting work! @nmattia does snack automatically handle .hsc files? I wonder if it's already capable of building complex projects like our [haskell-opencv](https://github.com/LumiGuide/haskell-opencv).
probably off-topic for this post, but is there a graphql client in/for Haskell?
There is a sorted type that is a `Functor`, though: newtype Sorted a = Sorted { runSorted :: (a -&gt; a -&gt; Ordering) -&gt; [a] } Its `Functor` instance works pretty much like you'd expect.
For this sort of thing, yes. There is also something to be said for a tracing jit eliminating all the indirect blind jump overhead of a spineless tagless g-machine, incrementally closing the world. This would get you somewhere between the computational models of jhc and ghc without the ahead of time whole program compilation pass.
Then in that case I think we're in wild agreement. My next step is hopefully to more or less finally taglessly encode the evaluation of this and get an evaluator that does everything I want. There seem to be a few more steps in how to get closures to work right, etc. but if I can find the right hooks for the big step semantics, I think that is the right general approach.
Suppose you're writing a complex but effect-free component of a larger system: yourCode :: ComplexOperationSpec -&gt; LargeBlob -&gt; LargeBlob Do you instinctively reach for, e.g., the `State` monad as an architectural aid? Or do you try to keep everything non-monadic as long as possible?
Thank you. One of your other projects had the clue I needed. I was missing a flag.
&gt;This is a Haskell-specific role: you will either need significant prior Haskell experience, ideally in a production setting, or an equivalent level of skill in another functional language (OCaml, Racket, etc.) and the ability to ramp up quickly. A degree of knowledge of programming language theory is also essential; prior work on a PL implementation is ideal. This sounds far pickier than the posted job description: Requirements * Experience with Git and GitHub. * Familiarity with basic functional programming concepts, e.g. , function application, function composition. * Able to communicate complex problems, solutions, &amp; questions clearly to diverse audiences. * Willing to experiment, make &amp; learn from mistakes, admit need &amp; ask for help, state &amp; invalidate hypotheses. * Passionate about building products that developers love. How willing are you take dedicated and fast learners in lieu of specifically-applicable experience?
Wow, that is quite a gulf.
I would appreciate it, if you would stream at a lower bitrate, if possible(not very good internet here). 
Usually I put most common and non controversial extensions inside `.cabal` file. Like: `-XOverloadedStrigs`, `-XGeneralizedNewtypeDeriving`, `-XDerivingGeneric`, `-XLambdaCase`, `-XTypeApplications`, `-XScopedTypeVariables` and even `-XRecordWildCards` (yes, I know that `RecordWildCards` are actually controversial but they are great when used properly). So the main reason is convenience. If almost every file in your project uses these extensions then writing extensions manually in every file is extremely inconvenient. Though, some extensions are still better to put in files, like `-XTemplateHaskell`, `-XDataKinds`, `-XTypeFamilies`, etc. They bring rather complicated features and usually not all files use such advanced features. Extensions are not added on-the-fly for your, you need to write them manually. After code refactoring some extensions can become irrelevant for some files but there's no tool to clean up all redundant extensions automatically. And removing such extensions can lead to version control system conflicts when working in a team. Unlike imports, extensions doesn't really help readability. Language extensions are unambiguous and don't bring overlapping features. And if you don't know what feature is used then seeing 10 extensions at the top of your module doesn't really help you to understand what exact language pragma brings this feature. Btw, I even remember your proposal about library-defined language extensions. It's very useful! But it has same cons as putting just usual extensions into `.cabal` files: * https://github.com/ghc-proposals/ghc-proposals/pull/92 Regarding putting imports into `.cabal` files: language extensions and imports are different from usability point of view. But if you use `base-noprelude` trick, you can put all imports into your `Prelude` and reexport different things for your package. It's very convenient when you use explicit imports and have 10 lines in each file with same imports.
I guess I am worried about using files outside cabal (just run `ghc` on them), of when you copy them into an other project, or when you process them with a non-cabal aware tool (`hlint` for example). But maybe most people don’t do these things anyways…
Usually when this happens it's because the position was tailored to an individual but they need to make it look fair even though the position is already "filled"
I've discussed IPFS with a few people. The biggest concern is that our use cases of downloading many small blogs seems to be relatively slow for IPFS. There also isn't any library in Haskell available right now for IPFS as far as I know. The plan is to initially have our own network protocol on top of HTTPS, but look into adding support for IPFS in the future. The local storage would stay as-is regardless for efficient querying.
I found this page detailing the file format: https://gist.github.com/jbeda/5c79d2b1434f0018d693 It does look like it solves the same kind of problem originally described, though of course it doesn't support the file deduplication. It should be easy to high export to and import from this format, which would hopefully make it easier to have Nix use Pantry in the future.
Don't use stack.
Done: [http://hackage.haskell.org/package/sv-1.1](http://hackage.haskell.org/package/sv-1.1)
&gt; open to remote work, though only from the locations listed here. I'm curious, what rules some US states out?
I think this is the first time I've used a Haskell build tool and it just worked without me fiddling around for hours. Great job!
Some more background for casual readers: https://github.com/NixOS/rfcs/pull/17
I’m new to Haskell and functional languages in general. I come from experience in Python, C++, and a little JavaScript. What is the best way for me to wrap my head around functional programming? I’ve heard the Haskell community doesn’t take kindly to OO folks, so if this isn’t the place to ask, just comment that and I’ll leave. 
I'll see if I can cut the bit rate a bit for the next stream. I cranked it way up after I had some quality issues during the first unrecorded stream and haven't really tweaked it to try to find a comfortable middle ground.
Nice! I've played around a bit with levity-polymorphic data structures a bit, trying to see what things would look like for `Maybe` and the like, but this is a lot cleaner than my previous attempts at just being polymorphic between LiftedRep/UnliftedRep, etc.
Can you elaborate? The post doesn’t look tailored to me—a large portion of people who have been on this sub for any length of time would meet most of the bullet points.
Are those actually the ones from the paper? It’s been a while, but I’m pretty sure the one in the paper actually has the state index type depending on the previous value (hence, “demonic” bind). To me these look like they just have fixed state indexes.
Tools like `hlint`, `stylish-haskell` and even `doctest` allow you to specify extensions in configuration files for them, so this is usually not a problem. In cases when you're copying files from one project to another within single organization or across your personal projects, there's usually style-guide or your own preferences regarding default extensions, so this is not a problem as well. Crossing these boundaries requires extra effort, but personally doesn't happen often for me. Regarding running only with `ghc` — this might be a problem. But I usually don't work with project files outside project. Inside project you have commands like `cabal new-repl`. It's hard to work with package module outside the package even with specified extensions anyways. So I see what can be inconvenient when users don't specify extensions in files. But these use-cases almost never happen during my workflow.
Is it same as https://arxiv.org/abs/1710.09756 ?
Is there a specific reason why France is not included in applicable countries?
`haskellPackages` has a function called `callHackage` that you can use for this. haskellPackages.callHackage "mypkg" "1.0.0.0" {} But nixpkgs is all about reproducibility, so even the hackage db used is pinned, via [all-cabal-hashes](https://github.com/commercialhaskell/all-cabal-hashes). So for new releases, you'll have to bump that. let hp = haskellPackages.override { all-cabal-hashes = fetchurl { url = "https://github.com/commercialhaskell/all-cabal-hashes/archive/6557c4177f2b7db92675c9183621bac449aa2018.tar.gz"; sha256 = "1h8dyx2w7417b96p1fwz0mld5rajfmxj13qhhzd2kf7q4ba1k2r2"; }; }; in hp.callHackage "mypkg" "1.0.0.0" {}
Disappointing.
Yeah, to me it just sounds like they want candidates with experience at an intersection of Haskell/FP and programming languages—which is exactly what I'd expect from the job description and project.
&gt; In practice, I've found Esqueleto more productive and reliable for my work. How so? Your comments are not useful for anyone wanting to decide which Haskell SQL library to use for their app. Please excuse my frustration, but you seem to have a lot of experience with this stuff, and yet you don't offer any concrete critique of the alternatives to Esqueleto. I'm in the middle of deciding which SQL library to use for a Haskell app, and I would really appreciate a more thorough critique of the alternatives to Esqueleto. 
Yes.
I think the best way is to read material about the language, concepts and idioms, write trivial and non-trivial code, and read code of other projects. Also, try to treat it as learning something new instead of learning the same thing you already know in different syntax. If you need recommendations for these things feel free to ask. I don't think that's true generally. I think most people in the Haskell community have previous (and current) experience in OO languages. There are some clashes of opinions and frustrations from both sides though because they see things differently. I also think this is a good place to ask your question.
It generally depends on the amount of usage and boilerplate, and the semantics of what i want to achieve. If you have: let (result1, state1) = f x state0 (result2, state2) = g x state1 (result3, state3) = g x state2 You probably want \`State\`. I generally begin without using \`State\` and add it if and when it starts looking too hairy.
Can you share your learnings, I'm also interested in this
I'm not GH-related but I guess they don't have any local incorporation hence cannot provide you with a french contract. I guess you still could build your own SASU/SARL and bill them for your services. This is quite common while working for a non-french company while staying in France.
There was talk on ICFP 2016 about SuperMonads that generalise all types of monads: * https://www.youtube.com/watch?v=HRofw58sySw
Interesting, I'd like to learn more. Where is `haskellPackages` and where is `callHackage` defined?
/u/profpatsch: regarding your comment there: https://www.reddit.com/r/haskell/comments/91f5r7/snack_incremental_nix_builds_for_haskell/e2xslal/ while I thought this was a crazy idea (although an interesting one) Bas convinced me that it was worth a shot and basically came up with what it now looks like. Thanks, Bas :) /u/bas_van_dijk: Good question, I actually don't know how .hsc files are built. Could you [open a ticket](https://github.com/nmattia/snack/issues/new) with a link to a particular commit of opencv you'd like to build? I can then give it a shot.
Author here, ask if you want to know anything else about Wire. (Or maybe not Wire 🙈)
I am interested too. I don't see a lot of countries. How does GH decide which countries they should hire from.
On a related note: when I was reading the paper, I didn't quite understand how one might work with non-natural multiplicities other than ω. The paper talks about the possibility of introducing another multiplicity β for borrowing - what does it mean for a value to be used exactly β times?
I wrote an [old article](http://comonad.com/reader/2007/parameterized-monads-in-haskell/) on that approach before I knew better, right after I joined the community. Life lessons long since learned: Type inference for them is awful as it doesn't flow into expressions. If you consider writing n^2 interactions for the mtl bad, then writing the (2^n)^2 bind instances to try to lift from monomorphic reader, and other combinations of effects into larger combinations of effects is considerably worse, and still ignores the fact that said lifting isn't unique as things like exception handling and state don't commute.
Kudos for the great job ad. This matches really well with my background and goals, and haivng the code open source is a huge plus for me. I'd apply in a heartbeat if I didn't already have a good Haskell job. One question: &gt; an intermediate-level Haskeller to work with us on the backend; Does that mean an expert/experienced/senior developer would not be a good fit for this position? (e.g. maybe because of the salary range etc.) Or do you actually mean "intermediate or above"? 
Those are the same as in the paper. McBride builds up a 1-parameter version and then uses "natural" transformations to sweep it under the rug. It is a bit clearer if you write out the type for an indexed fmap: Longhand it looks like: ifmap :: IFunctor m =&gt; (forall i. a i -&gt; b i) -&gt; m a j -&gt; m b j but with type a ~&gt; b = forall i. a i -&gt; b i it looks more recognizable. ifmap :: IFunctor (a ~&gt; b) -&gt; m a ~&gt; m b similarly ireturn :: a ~&gt; m a ibind :: (a ~&gt; m b) -&gt; (m a ~&gt; m b)
/u/nmattia [done](https://github.com/nmattia/snack/issues/33). Good luck, and let me know if you need some help. 
Oh, thanks, I'll try it soon! Does it mean that I cannot get a new version from Hackage until there is a new version of `all-cabal-hashes` is ready? Isn't there anything to get a tarball directly from Hackage and do `callCabal` on it? I thought of something like that, but if it doesn't exist, then there must be a reason? 
Intermediate or above, yep. We don't need expert Haskell knowledge but if you're an expert, you probably have a lot of industry experience in general and that would be pretty cool to have. (Not to mention that experts seem to grasp things much faster.) Whether we can pay substantially more to an expert is a different topic, and one I don't know much about :(
Typically it is additional costs of dealing with additional tax and legal overhead to working in a new state.
*[[32:53](https://www.youtube.com/watch?v=t0mhvd3-60Y&amp;feature=youtu.be&amp;t=1973)]* .. but now this type will mean there is a check that we don't accidentally, as some compiler optimization, duplicate the world (which is known to be difficult or destroy the world (which is known to be undesirable).
Wait, what's not clear to me is that yes, what you've written here with `ifmap` looks exactly like the Haskell-flavor of natural transformation, I agree; but that's not my understanding of what's being done in the paper. In the paper, his arrow looks like this: `(~&gt;) : ∀ {i : Type} -&gt; (f g : ∀ (x : i) -&gt; Type) -&gt; Type`. To me these don't look like the same thing -- your definition has `f` and `g` being from `Type -&gt; Type`, but his arrow is indexed on some type `i : Type` and the `f` and `g` are from *values* of that `i` to `Type`, i.e., `i -&gt; Type`.
The ad is lovely.
&gt; Looking at the repo, my guess is [this](https://github.com/mlitchard/reflex-todo/commit/a5cacaa9881543cb597275da8221bca18b0c3b39) commit fix his problem.
Weirdly, I was thinking of almost exactly going on a bike ride while sitting around in my underwear a few days ago.
If `pkgs` is your `nixpkgs` alias, then these are `pkgs.haskellPackages` and `pkgs.haskellPackages.callHackage`.
I know at least one of the guys there, and if I extrapolate from there, it must be a great team!
Can anyone recommend a very simple graphic library? I just would like to plot single pixels and lines on my screen. Thanks!
Sure, just clone the repo in the initial message. I fixed my stack.yaml file. The downside is you'll have to constantly maintain it. However in my estimation, if you are trying to produce a work product, there's no time to learn something new. Now, if you are engaged in exploration, maybe investigating nix has value.
That's right.
I’m reading “learn you a Haskell for great good” which was recommended to me. I’m a bit at a loss of what I can actually *do* with Haskell, and since I don’t really understand its usefulness I’m finding it difficult to see why anyone would actually use it. The syntax seems convoluted, and sure, it is faster to do many simple things in Haskell than in an imperative language but from what I’ve seen the sacrifice is readability. For example, I’ve seen these examples of Haskell code, that apparently does something useful in 3 lines but is completely illegible. This is probably because of my lack of familiarity with functional languages, but in any other language I don’t know I can look at a function and see what it does. For example, this little snippet on the Haskell website: primes = filterPrime [2..] where filterPrime (p:xs) = p : filterPrime [x | x &lt;- xs, x `mod` p /= 0] What does any of this mean? I think there’s a function. And there’s some stuff with division, but that’s all I understand. Also thank you for the advice. 
Weirdly, I was thinking of almost exactly you sitting in your underwear while avoiding good ideas a few days ago.
Yea there's `callCabal2nix` if you just wanna build any source folder. haskellPackages.callCabal2nix "mypkg" (builtins.fetchTarball { url = "https://hackage.haskell.org/..."; sha256 = "..."; }) {}
Those are the sorts of small things that make SPJ talks so enjoyable to me. 
So a few things: Imo LYAH is not a good book to learn from as it only gives the illusion of learning - it gives very shallow understanding to Haskell and has no exercises which leaves the reader frustrated and feeling "i read the book but i don't know how to write Haskell". Better resources include haskellbook.com or haskell wikibook. I've also heard some people found 'get programming with haskell' and Graham's book useful but i have not read them. Regarding usages: https://github.com/Gabriel439/post-rfc/blob/master/sotu.md I find Haskell to be best in class in terms of readability. Due to it's syntax and properties. It might just be that you are iust not used to ot yet. Maybe this guide that I wrote can help: https://soupi.github.io/soupi/reading_simple_haskell Most of your example you can try again after looking at my guide expect for the `[...]` stuff that is called a list comprehension which is a really elegant way to describe complex lists in a pretty math like syntax, but it's honestly not very common in most domains. Hope this helps. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/post-rfc/.../**sotu.md** (master → 1be40de)](https://github.com/Gabriel439/post-rfc/blob/1be40de1212f90fa221079f88c528bea5f831965/sotu.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e30l259.)
Gloss
No, unfortunately not. It's spread over a bunch of papers, and never complete. I discussed this with Richard recently, and yet, would be good to have something like that!
I would recommend against this book, since it does not come with any exercises, starts off by only explaining how to play around in the repl, and is out of date. I think there is a book recommendation thread like once every two weeks, so searching for "haskell book" or "learn you a haskell" (people keep asking if it's still relevant) and sorting by date should lead you to them. When I started Haskell, I first read LYAH as well, and almost quit because of it. Regarding the `primes` function, let's break it down. `primes` is not a function, since it doesn't take any arguments. You can check this yourself by typing `:type primes` in *ghci*. But `filterPrime` *is* a function. The first think `primes` does is call `filterPrime`, a locally defined helper function in a *where block*. `[2..]` denotes `[2,3,4,5,...]` an infinite list of numbers. Note that we *know* that the list is non-empty. Technically, `filterPrime` is a *partial* function, in that only the equation `filterPrime (p:xs)` is defined, but not `filterPrime []`. Remember that data [] a = [] | a : [a] is defined with *two* constructors, the empty one (`[]`) and the non-empty one: `(:) :: a -&gt; [a] -&gt; [a]`. I digress. I just wanted to make clear that this is code that is supposed to look pretty and concise, but don't let that fool you into thinking that Haskell is all about one-liners. As I said, `filterPrime` *pattern matches* on the second constructor for lists and gives the name `p` and `xs` to its two arguments. Pattern matching is all about asking "which constructor has this data been created with". In this case, the `(:)` constructor used for making a non-empty list. It's called "cons" (for construct) by the way. In the first iteration, `p = 2` (a prime number), and `xs = [3..]`, the rest of the infinite list. Now, what we want to do as result is return a list of prime numbers. We know that `p` is prime, so it comes first: `p : _`, where `_` is a hole we have to fill in, a list of more prime numbers. We create this list by calling `filterPrime` again, with another (non-empty) list as argument. This list we create by a so-called *list comprehension*. In `[ x | (x :: a) &lt;- (xs :: [a]), (p :: a -&gt; Bool) x]`, we create a list by * Taking each element from some other list `xs`, calling it `x` for every iteration, * Check if some predicate `p` holds for this particular `x`, and * return it unmodified if `p x = True`, or discard it if `p x = False`. The expression `mod x p` can be written `x \`mod\` p`, but they denote the same thing. Any binary function can be written in *infix* style like this. Anyhow, the predicate `(x \`mod\` p /= 0) :: Bool` is used to determine if we want to put `x` into our list of primes or throw it away. I hope this helps somewhat, and don't let this example discourage you. Look at how many things you may not yet be overly familiar with: * List comprehensions and list construction syntax like `[2..]`, * Haskell's concise function notation (`lhs args = rhs`), * recursion, * pattern matching (generally on sum types with several constructors), * the habit of giving very short names to function arguments, like `(x:xs)`. The last one may be justified if you have some type (expressive) signature directly above your function. Here you have none, so you have to "typecheck" in your head to make sense of what `xs` is supposed to be et cetera.
Would it be easy to get a visa for a spouse as well?
Hi Artyom, I'm wondering if you could give advice to a developer early in their career on how to move towards opportunities like this. Currently I'm finishing up a bachelor's degree in mathematics and I'm doing work that involves data science and automating the webscraping of AJAX- and WebSocket-based APIs. I have a fascination with functional programming and my studies in algebra give me an advantage in understanding the theory behind it. However, my ability to write useful Haskell code is quite marginal compared to my skills using an OO/scripting language like Python. I was thinking that working on a Haskell library would be a good thing to do. Do you know any open-source libraries that could benefit from someone like me who has a solid mathematical/algorithmic foundation but who is not yet especially proficient with the language?
This is offtopic, but how is [your book](https://intermediatehaskell.com/) coming along?
Maybe. It took 12 years between C++98 and C++11. It took about 10 years between C89 ("ANSI C") and C99. Some languages iterate faster, but generally don't have a detailed specification. Java did 12 "major" versions in 23 years, but some of those were mostly growth of the standard library, with very few core language changes. Also, most people seem not to care about the Haskell Language Report really. GHC hasn't actually implemented any version of it for a while; I'm fairly sure there are "pathological" Haskell2010 programs that the Applicative-Monad-Proposal broke and I know the Foldable-Traversable-Prelude changes made things non-Haskell2010. Even older than that, we have changes to Num no longer requiring / implying Show, and IIRC something about operator sections.
Hi folks, I work with Python and I'm finishing up a mathematics undergrad. I don't have a lot of proficiency with Haskell, but I want to learn. Can anyone recommend an open-source library that needs more work which would benefit from my math skills, but which wouldn't require me to have a deep understanding of the language? I'm wondering whether I could learn what production Haskell looks like by making contributions like this.
Not necessarily easy (it depends on your location, education, experience, etc) – but whatever we do to relocate you, we'll do the same for your spouse/family. 
You should switch to NixOS if you're a *Nix* user. NixOS is a lot more beginner friendly than Nix package development (in large part due to the excellent manual and module system of NixOS), but it still requires some comfort with Nix. There is no relation between NixOS and Haskell except that Nix is a functional language (but it's untyped), and the package system has good support for Haskell. Main word of warning: NixOS is a pretty bare and extraordinarily configurable distro. It's probably most similar to Arch Linux in terms of its DIY attitude, except that it comes with a lot of higher level knobs and switches for configuring stuff more easily with Nix.
This is cool, but is it actually fast? Last time I tried to achieve incremental builds in nix by making each module its own derivation, cabal new-* was still much faster.
I'm not Artyom but I interview people for Haskell positions regularly. Go build something you enjoy building. It could be a game, a web app, a library to do some task you normally do in some other language, an API binding, etc. It really doesn't matter what you write so long as it is non-trivial enough to require competency with the language. The enjoyment is for motivating you to put hours into it. My code sample for my first Haskell job was an OpenGL toy.
That's why *sometimes* you'll see an explicit accumulator used. fac n = fac' n (n - 1) where fac' _ n | n &lt;= 0 = 1 fac' 0 _ = 0 fac' a n = fac' (a * n) (n - 1) Here, `fac` is small enough to get inlined, and `fac'` is tail-recursive enough to get TCO'd.
I agree that the Greenhouse description is much more vague than my description of the role; I’m working on getting that Greenhouse page fixed as we speak. Thanks for pointing it out. As regards dedicated and fast learners: it’s a case-by-case basis. Part of the interviewing process is determining how a candidate’s previous experience maps on to the work we do on this team. A lack of experience working on PL implementations, for example, isn’t necessarily a sine qua non, but it is a sign that a candidate will have an easier time ramping up with that experience under their belt.
&gt; just submit a pull request to the original package to add the function My experience with this has been very positive. You need to be willing to accept and act on feedback from the maintainer, but it's honestly not that hard to add stuff to existing packages.
He said he's not Artyom. Good question though... I'd like to know as well. 
&gt; Hi! I’m Artyom and &gt; Author here, ask... I think you have reddit user name confusion.
The overhead of creating derivations with the nix sandbox enabled can be quite high, because it needs to setup a file system tree and namespaces every time.
Thanks for the advice.
[removed]
That couldn't possibly be because the `MonadUnliftIO` creator has a large influence over the Yesod ecosystem. :P
&gt; I am mostly concerned with practical understanding of the way the compiler "thinks" of the code I give it, so that I can trace its logic and somewhat narrow down on the solution, rather than going by blind guesswork. You're in luck. Given a bit of practice your neural network is much much better at gaining an intuitive understanding of what the compiler "thinks" of your code than reading any number of papers could! I'm *not* being snarky here, BTW, I'm completely serious: Practice is *much* more valuable for these things than any number of papers. (Of course a "combined" resource with the actual rules would certainly be useful for those working with the type system *formally*, but that's not most or even the majority of Haskell programmers.)
Interesting, Haskell and blockchain and not IOHK
Strange to not divulge who they are - looks like http://kadena.io though.
Have a look at https://github.com/wchresta/linear-codes and see if thats something that would interest you.
I feel compelled to point out that there are certain groups of physicists who think the world is being duplicated all the time. Relatedly, the destruction of the universe could be exploited for linear-time sorting (by comparisons) if the former group of physicists are right, so... It may not be moral to do so, but it might theoretically be of enormous practical value!
A bit late, but what is the specific version used to build it? Building with tags/ghc-8.5-start still throws this error
The paper's version of borrowing is basically a scoped Omega. You have weakening and contraction (i.e. zero or multiple uses) but after the callback returns you can't have any references left. For instance Foreign.ForeingPte: withForeignPtr :: ForeignPtr a -&gt; (Ptr a -&gt; IO b) -&gt; IO b We touch the ForeignPtr afterwards to notify the gc that the ForeignPtr finalizers shouldn't be run while we mangle the raw Ptr. We can use the Ptr unrestricted during the callback but can't return it. Rust's also has mutable borrowing which doesn't allow contraction, not sure how that would fit without subtyping. 
Seeing this inspired me to add this issue, that I have been mulling for a while (posting it in full to save a click, but it might be best reply on `github`): [Allow changing nixpkgs to update Haskell package versions](https://github.com/input-output-hk/stack2nix/issues/113) Stack2nix is cool if you have to maintain a stack.yaml and want a reliable way to turn it into a nix file. However it does not work as a migration tool yet. For instance here is the haskell-ide-engine [stack.yaml](https://github.com/haskell/haskell-ide-engine/blob/master/stack.yaml). Stack2nix outputs a 1.59MB [nix file](https://github.com/domenkozar/hie-nix/blob/master/ghc-8.4.nix) that pins all the haskell packages. This means you can't use pin nixpkgs to get different haskell packages versions (you will always get the ones stack would have choosen. I think if `stack2nix` took a nixpkgs ref as an input it could get much close to the hand crafted translation. It could compare the nixpkgs derivations to the ones it would have output and only output the differences Here is an example of what I would like to get out of `stack2nix` for the `haskell-ide-engine` stack.yaml: with import &lt;nixpkgs&gt; { }; let rootName = name: builtins.elemAt (lib.splitString "/" name) 0; isValidFile = name: files: builtins.elem (rootName name) files; relative = path: name: lib.removePrefix (toString path + "/") name; onlyFiles = files: path: builtins.filterSource (name: type: isValidFile (relative path name) files) path; ghc = (pkgs.haskell.packages.ghc843.extend (pkgs.haskell.lib.packageSourceOverrides { haskell-ide-engine = onlyFiles ["LICENSE" "Setup.hs" "app" "haskell-ide-engine.cabal" "src" "test"] ./.; hie-plugin-api = ./hie-plugin-api; HaRe = ./submodules/HaRe; cabal-helper = ./submodules/cabal-helper; ghc-mod = ./submodules/ghc-mod; ghc-mod-core = ./submodules/ghc-mod/core; haskell-lsp = ./submodules/haskell-lsp; haskell-lsp-types = ./submodules/haskell-lsp/haskell-lsp-types; haskell-lsp-test = ./submodules/haskell-lsp-test; leksah-server = ../leksah/vendor/leksah-server; base-compat = "0.9.3"; cabal-plan = "0.3.0.0"; haddock-api = "2.20.0"; haddock-library = "1.6.0"; })).extend(self: super: { hlint = haskell.lib.dontCheck super.hlint; constrained-dynamic = haskell.lib.doJailbreak super.constrained-dynamic; haddock-api = haskell.lib.dontCheck super.haddock-api; haddock-library = haskell.lib.dontCheck (haskell.lib.dontHaddock super.haddock-library); ghc-exactprint = haskell.lib.dontCheck super.ghc-exactprint; leksah-server = haskell.lib.dontCheck super.leksah-server; }); in { inherit ghc; shells = { ghc = ghc.shellFor { packages = p: [ p.haskell-ide-engine p.hie-plugin-api p.HaRe p.cabal-helper p.ghc-mod p.ghc-mod-core p.haskell-lsp p.haskell-lsp-types p.haskell-lsp-test ]; }; }; } 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [domenkozar/hie-nix/.../**ghc-8.4.nix** (master → e3113da)](https://github.com/domenkozar/hie-nix/blob/e3113da93b479bec3046e67c0123860732335dd9/ghc-8.4.nix) * [haskell/haskell-ide-engine/.../**stack.yaml** (master → 03168cb)](https://github.com/haskell/haskell-ide-engine/blob/03168cb2e4c3fb32be2bb2182819f07b27f5f9f4/stack.yaml) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e319wsu.)
Can you mention some benefits that you get from using Nix? And do you know where can a beginner get help starting with Nix? There doesn't seem to be a subreddit for it, but there's one with a small number of subscribers for NixOS.
I am fairly sure you are correct, it is Kadena. BTW, I have played with their Pact language - pretty cool functionality and it is open source Haskell.
It is mostly because when things get fused together into the 'costate comonad coalgebra' version, you want the 's' first. This avoids some administrative plumbing/overhead for `cloneLens` and the like. Also, it avoids overhead when implementing the combinator here, as you need to apply the function partially to 's' in the environment before mapping it over the functor.
oh there are definitively more
&gt; Why annotations in one case, and a type in the other? Because the type can be implemented without compiler support would be my guess.
Thanks! I’ve checked out your website, and I found it very informative. I will look into Haskellbook in a moment. One more question: I’m using spacemacs as my main code editor. Are there any essential plugins for it I should be using for Haskell development? 
I’m gonna be honest I did not understand a word of your explanation, but I will come back and th to read it again once I’ve acquired a bit more knowledge on the subject. Thank you for your helpful advice on learning Haskell, I will make sure to keep it in mind. 
I understand it makes the implementation cleaner, but it seems to me it comes at the cost of making the API harder to use. I usually have functions of the type of `bst` laying around that I can use but I have to flip them to pass them to `lens` so it's the same overhead but now leaking out into the API. I'm any case it is not a big deal, I just wanted to understand why the current signature was decided on. Thank you for helping me inderstand.!
I'm pretty confident that's not the case here.
This is almost always direction from Legal. Hiring in the US, especially when you're supporting people's visas, is actually a very carefully regulated process (even though we then end up with employment contracts that are pretty awful compared to the rest of the civilized world). You need to be able to obey local, federal, state AND then employee resident country laws. Heck, many US companies can't hire remote folks from \*other states in the union\*. My startup couldn't hire outside of a handful of states once we ditched TriNet, often because of laws in those states. Federalism forever, amirite?
The OutsideIn paper isn't fully up to date, but it is the best comprehensive reference on the core algorithm in use: https://wiki.haskell.org/Simonpj/Talk:OutsideIn
As a counter to this, I went into NixOS without any Nix experience, but I expected to spend a lot of time reading manuals and fighting to get what I wanted. Reality matched my expectations but I'm very happy with NixOS now. I agree that it reminds one of Arch or Gentoo or similar DIY-ish distros.
You might be able to get a visa for spouse as well in Germany. If Wire pays above average, then you should be able to apply for an EU Blue Card (it has minimum salary requirements) in Germany that lets you bring your partner with you.
One of the things I find strange is people _insisting_ on using names like `Mappable` instead of `Functor`. Does it convey the fact that you have a `map` operation better? Sure, it's right there in the name! Does it convey the fact that you have laws associated with how things compose? Erm...no? Does using fancy words like `Functor` at the very beginning scare people away? The epsilon-delta definition of continuity is one example I've seen where people often get stuck when they first study calculus formally. I know personally when I read somewhere "intuitively it is a lax semi-monoidal gobbledygook" in a library's documentation, I raised my eyebrows and went to drink a glass of water (perhaps I will understand it later?). Does that mean that we should stop using the term `Functor`? Does that mean that we start out teaching `Mappable` and then gradually talk about laws and introduce `Functor`? I don't know. However, I do think that it would be great if people got together and talked to each other and tried to understand differing points of views, describing their teaching experience for different groups (e.g. teaching FP to math folks could be very different compared to teaching it to OO programmers), instead of claiming that X is the best in all situations.
Do you think it is possible at all? That is, does the current type system appear as a principled structure or more like an ad-hoc patchwork?
In a pure immutable language, how?
[@zimbatm's latest tweet](https://i.imgur.com/uvQMMC3.jpg) [@zimbatm on Twitter](https://twitter.com/zimbatm) - ^I ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
Nice write up, can’t wait for the next blog post about succinct data structure.
There are two things at work here, which I didn't have time to mention in the presentation. Firstly, *unfamiliarity* makes people feel deeply uncomfortable, so deep in fact that it comes from the hippocampus and triggers the fight-or-flight response. This seems to be exacerbated in more experienced programmers, who think themselves to be somewhat masters of their domain, and are quite prone to reflexive dismissal of foreign concepts. If they haven't already learnt it, and they have got this far without it, why would they need it? Once I understood this emotional response, it became a lot less frustrating to repeatedly witness it. Secondly, a lot of the FP stuff in particular has totally alien words. OO and GoF patterns were examples of foreign things that were generally dismissed at first, often quite loudly, but they used language that was more familiar. People knew what objects were, what factories and builder were, and the analogies were useful in overcoming the unfamiliarity. The mathy words don't help at all in acquiring an understanding. This is of course a good thing in some ways, if Monad had been called Burrito it would have been about as much use as many blog posts for acquiring a true sense of what it is or does. The mathiness also doesn't help due to many people's poor experience with western maths education. So many people I know profess to hating and/or sucking at maths, yet they are very good at problem solving and logic. When they hear these words come from a relatively obscure and particularly abstract part of maths, they think "ok, I must hate it, because maths". Interestingly, my experience teaching grad programmers has been they're usually pretty open to this kind of thing, and pick it up pretty quickly. They *know* they have lots to learn, and just get on with it.
I haven't tried spacemacs in a few years but i think activating the haskell layer should just work i theory? There are a few ways to get haskell stuff on vanilla emacs - haskell-mode, intero and dante. Many people use intero but I use dante mostly. I suggest activating the layer and see what you get and grow it organically with your needs.
Well not everybody HAS to learn FP. If, as an OO programmer, you are only interested in learning what you already know, FP is going to be dissappointing (no matter the names). Also I'd suggest keep using the mathy names. Not because they are so nice, but because their meaning is defined so rigorously. Mappable suggests something you can map over. But does not tell you much about the laws ( as stated by [**u/theindigamer**](https://www.reddit.com/user/theindigamer/) ). Calling something mappable is not a very friendly to newcomers, it suggests they see something they already know and will feel bad as soon as they realize they've been lied to because it involves more stuff that nobody and not even an unfamiliar name warned them about! Also we should not ignore folks out there, like myself in fact, that find FP especially more attractive BECAUSE of it's mathyness. Presenting a challenge for new stuff to learn and actually improve on the stuff I know already. Changing names to suit an audience may result in a bigger audience, but it will also mean losing another audience. Somehow this point is not being expressed a lot.
`Lazy a ~ (() -&gt; a)` is an easy encoding to make without any compiler support. 
Hello Haskellers, I am looking for code review, would it be possible to share my code on this thread in order to get some hint/feedbacks, otherwise, where is the appropriate place to do so?
Are those performance numbers for aeson or the extra inefficient representation under 'Why does this happen'? Because iirc aeson uses boxed Vectors, HashMap and Text instead of lists. Not sure how much parsing to an unboxed data type and streaming consumption would help? Probably at least lower peak heap usage but most intermediate representations except Vector probably can't fuse away? Anyway, looking forward to the next post! 
I wrote a function: is :: (a -&gt; b -&gt; c) -&gt; Maybe a -&gt; Maybe b -&gt; Bool is x y z = isJust . liftA2 x y z I, obviously, tried use the magic of partial application on it, and while is x y = isJust . liftA2 x y works fine, both is x = isJust . liftA2 x is = isJust . liftA2 fail to type match. Is there any smart way to make the compiler infer it?
You could use Alternative option combinators : https://hackage.haskell.org/package/optparse-applicative
Try using `Integer` instead of `Int`. `Int` has limited precision, but it's faster than `Integer` which had unlimited precision.
Try changing the numeric type from \`Int\` to \`Integer\`. \`Int\` is a fixed-size "machine integer" which is bounded. \`Integer\`, on the other hand, is an arbitrary-precision type that can take up more memory as needed to represent bigger numbers.
On mobile, but can you try it with Integer instead? I can't remember which ints are big nums Also, I think its an overflow rather than a loss of precision, got what you meant but its a somewhat different issue.
&gt; Hopefully, these examples have convinced you of the absurdity of using in-memory documents to represent large datasets. This, I think, is the main takeaway here, and it's not unique to Haskell. In a nutshell, if you're going to process large datasets, you will eventually have to adopt a streaming approach. It's not new wisdom either; SAX parsers for XML have existed for a long time. The interesting thing about Haskell, in this context, is that due to the language's non-strict semantics, we often get away with what looks like "load the entire thing into memory at once", because lazy evaluation effectively turns it into a streaming solution - take, for example, this program: main :: IO () main = do data &lt;- getContents mapM_ processLine $ lines data processLine :: String -&gt; IO () processLine ln = case words ln of ("hello":names) -&gt; do mapM_ names $ \name -&gt; putStrLn $ "Hello, " ++ name ++ "!" ("tell":name:msg:_) -&gt; do putStrLn $ name ++ ", there is a message for you: " ++ msg _ -&gt; return () -- not interesting So while this looks like a program that first reads the entire input into memory, then splits it up into lines, and then loops over the line to process them one by one, it really is lazy throughout, and what really happens is closer to this: - Read a block of data from input and append it to a buffer - If the buffer is empty, and no more input is available, you're done. - If the buffer contains a full word: - Is the word "hello"? - then: - read more input into the buffer until you have another word, or until you find the end of a line - if you have another word, print a "Hello, ..." message and loop - otherwise, this line is done, go back to the start - Is the word "tell"? - then: - read more input into the buffer until you have another word - print the word - print ", there is a message for you: " - read more input into the buffer until you have another word - print the word - read more input into the buffer until you have reached the end of a line - this line is done, go back to the start - Is it something else? - then: - read more input into the buffer until you find the end of a line - this line is done, go back to the start However, the reason this can amount to streaming here is because we don't need any lookahead - in order to tell which pattern to match inside `processLine`, we only need to fetch the first word - no matter what follows after "hello", we already know which code path to take once we have read "hello", so we don't need to read more input until we actually get to the part where we process it; and once we're done processing a line, we will not reference it again, so the garbage collector will pick it up, and the whole thing runs in constant memory no matter how large our input. But with JSON, this doesn't work. A JSON document is either an object or a list (`{ ... }` or `[ ... ]`); in both cases, in order to parse the document, we have to read the entire input, at least if we want to reject invalid JSON documents like, for example, this one: { "foo": "bar", "baz": "quux" ] That's a syntax error, and the correct response is to reject it at the parsing stage. But that means that before we can even think about processing the document, we must have evaluated the parsed document at least to the point where the entire input has been consumed. This, btw., is again not unique to Haskell, it's the same in every language - it's just that it's less obvious in Haskell that the usual non-strict semantics aren't helping us as much here. In order to fix this properly, we have to find a way to process JSON in a streaming fashion. Unfortunately, this means we can't wait for the parser to tell us that the document is valid; we have to do some speculative processing, and then be prepared to roll back in case of an invalid document. This can't be done with aeson, and it's kind of tricky doing it with Haskell's transparent laziness; personally, what I would do is roll my own JSON parser that takes a callback, with both the parser and the callback running in IO. Something like: data StreamingJSONEvent = ScalarText Text | ScalarNumber Number | ScalarBool Bool | ScalarNull | EnterList | ExitList | EnterObject | ExitObject | EnterKey Text | ExitKey streamingProcessJSON :: (StreamingJSONEvent -&gt; IO ()) -&gt; String -&gt; IO () This is essentially the same thing SAX parsers do for XML: as you parse the input, you signal parser state transitions to the consumer through a callback. This way, the consumer can determine which parts of the document to skip and which to hold on to, while the parser simply chugs through the input in constant memory - none of the events requires lookahead in the parser, every one of them can be decide as soon as a token has been parsed. 
Ah, right, of course. With `Integer` it works just fine. Thanks!
You're right, `Integer` works fine!
Perfect, `Integer` works. Thanks!
No need to roll your own parser, you can use json-stream: http://hackage.haskell.org/package/json-stream
When would you say about about someone that he's an intermediate-level Haskeller? Which things should he know about and which should he be comfortable with?
But that isn't even close to viable for real world use, you NEED memoization.
&gt; In a nutshell, if you're going to process large datasets, you will eventually have to adopt a streaming approach. But I believe what OP is hinting at is that they will _not_ adopt a streaming approach, but rather use a more space efficient in-memory representation (not sure). &gt; we have to do some speculative processing, and then be prepared to roll back in case of an invalid document. &gt; personally, what I would do is roll my own JSON parser that takes a callback, with both the parser and the callback running in IO. But how do you roll back IO? How do other languages solve this? Is streaming parsing JSON even possible given what you said about unbalanced braces? Do they validate the document in a first pass and then do the parsing, knowing that it is valid? 
Cool! I'm looking forward for the 8.2/8.4 support to land!
Things I would expect (an incomplete list): * Can work with monad transformer stacks, can `lift` things around. Can use monad/applicative/functor interfaces. Can recognize when something is a functor. Knows things like `when`. * Used lenses, though doesn't necessarily understand how they work. * Used IORefs, concurrency. Aware that exceptions exist and should be handled sometimes, but might be afraid and/or distrustful of own ability to correctly work with them. * At least considers creating new typeclasses. Sometimes. Doesn't get confused around type variables either. It's just a cluster of correlated knowledge, of course – there are likely great Haskellers who write their own compilers and so on but have never used lenses or perhaps even never ventured outside of Haskell98. You can't neatly put everyone on a single axis.
Eh, maybe in 2019. Or 2020. At least for me, finding motivation turned out to be trickier than I expected. 
If you have a solid algorithmic foundation, you might try asking people who write algo/math libraries whether they need a helping hand. I mean, just email them and somebody might respond. Building something of your own is also good advice. I learned Haskell by reading Reddit and writing small projects. The downside is that it took me several years (inability to focus is great). If you're okay with internships (and possibly unpaid internships), you can just be blunt and write to all Haskell companies out there: "Hi, I really want to get better at Haskell and I can offer implementing math-heavy stuff and reading papers for you. Do we have a deal?" I think that Serokell could use a person like that, but I'm not sure. 
I've recently been doing some NLP and I did a wrapper to java-written Standford NLP command-line program [http://hackage.haskell.org/package/corenlp-parser](http://hackage.haskell.org/package/corenlp-parser) , hopefully that can be useful. \&gt; I wanted advice on how to expand haskell's NLP toolkit? I think writing wrappers around SpaCy would be great (but not around NLTK, which is not really a "production" solution, but is itself a wrapper most of the time), but the bigger thing would be to improve the situation around the ML world, making some sort of bindings/wrappers around the Python's popular ML libraries, or writing Haskell's own.
Yes, these performance numbers are for aeson. The sample data structure is provided for illustration only. Aeson is a widely used library meant to have good performance. It's quite possible that aeson has optimisations that reduce memory consumption, which leads me to suspect that more naive data structures such as what I described would perform much worse in both CPU and memory usage.
&gt; But I believe what OP is hinting at is that they will not adopt a streaming approach, but rather use a more space efficient in-memory representation (not sure). This is correct. The build up to how JSON is handled by the haskell-works parsing libraries will span over a number of posts. I will first be exploring how alternative parsing techniques apply with the CSV format and eventually come back to how JSON parsing can work with similar techniques. Also, the techniques I will described do not necessarily preclude streaming, although I won't make promises there just yet.
This is the usual solution. However, that is not the solution being (gradually) presented. If you need to only access some small part of the whole, streaming still requires you to touch the whole file. It helps address the constant factors, not the asymptotic amount of work on subsequent accesses.
This is true, SAX style parsers are a tried and tested way to work around whole document parsers and they definitely are the go to solution due to their simplicity where there are no other viable alternatives. Whilst SAX style parsers do solve the memory problem, they do not provided random access to data in the document, and are not nearly as convenient as whole document parsers. The question I will raise in future posts is can you get the convenience of a whole document parser without paying the Memory and CPU cost? It will be an interesting journey.
Aeson has the "cffi" option that improves performance for reading long strings. It is disabled by default, but depending on the type of input data it can give significant boost.
Wunderbar
&gt; Well not everybody HAS to learn FP. Weoo, *sort of*. I'd argue that FP is actually the foundation of programming, as it is a subset of programming, just minus the bit where mutation/side-effects happen. For instance, SICP spends the first 80-odd pages defining the pure core of Lisp, and then the next 250-odd introducing assignment and all the complexity it brings. It is perfectly possible to do first-order FP without a lot of the advanced category theory stuff. If you only have immutable values and functions you can go a long way in mainstream languages like Java etc., solving real problems like concurrency. What you don't *need* to learn is much of the higher-order things like category theory that allow for high degrees of abstraction and code reuse. I know plenty of extremely effective Haskell programmers who essentially use Haskell as a great imperative language, and yet are doing FP. However, I get your point. And I have zero interest in forcing things on people who are uninterested. &gt; Also we should not ignore folks out there, like myself in fact, that find FP especially more attractive BECAUSE of it's mathyness. Yep, totally agree. My point was just focused on the large subset of mathphobes. &gt; Nice talk by the way, enjoyed it. You're very welcome, and the compliment is very much appreciated, thanks!
In practice there are more programmers internationally who understand English than category theory. Names like Mappable and Combinable instantaneously give most people some intuition about what those typeclasses do. Then they can read the documentation to understand the specific laws. Names like Functor and Monoid give most people no intuition about what those typeclasses do. And they still need to continue reading to find out what the laws are. Consequently I would argue that Mappable and Combinable are better-functioning names: they give more people more understanding. I have a degree in comp sci, and a masters and PhD in machine learning. I’ve never been taught abstract algebra outside of Borel sets and measurability. Neither have the majority of my colleagues. If the aim is to increase the adoption of Haskell, it’s more feasible to change Haskell so it’s more understandable by the world, rather than wait for the world to change itself so it can understand Haskell better. 
Truth bombs : try to resist perfectionism and "shiny new toys" 
I really don't agree. I think the beautiful part of haskell is that we are actually able to express (an approximation) of these well-defined and well-understood concepts. They served us very well. If you look at Haskell-code, they are everywhere! I think using these words has 2 advantages: We can easily integrate more advanced concepts (just look at lens, it's actually solving a problem!) using the common vocabulary and directly profit from literature and well-understood concepts in this area. But I think the more important point is that alien words don't have an association. I really don't like Mappable for Functor, because there are a lot of valid Functor instances (that obey the laws!) that are not doing something you would describe as "mapping". the same with monad, where we completely fail to provide a metaphoric name. Just look at every monad-tutorial out there. The key is to stop searching for metaphors from the real world and start accepting that laws define structures and that everything that obeys the law is, in fact, a monad, regardless or type, structure, or skin color. We are not building factories or abstract factories, the thing *is* a monad. It's a property, not a decision.
you might ask in the dataHaskell gitter. some of us there work with NLP regularly. [https://gitter.im/dataHaskell/Lobby](https://gitter.im/dataHaskell/Lobby) (I reluctantly work in python for most NLP tasks.)
Well, \`Combinable\` is \`Semigroup\`, not \`Monoid\`. \`Monoid\` is \`CombinableWithNeutralElement\`.
Ah, yes. In that case, consider me interested :D
Your correction proves the point: it’s much less easy to make mistakes when names are so unambiguously prosaic. I was never curious about groups, since I didn’t need to know it to understand what a semigroup was. And the term semi here does nothing to indicate the absence of invertibility. I think we need to be open to the fact that the names chosen by mathematicians in the past might not be optimal for programmers in the present. 
The rollback is, by necessity, consumer-defined. Essentially, what needs to happen is that you assume that the document is valid, and when it turns out it's not, you abort and tell the consumer to roll back. But what that rollback involves is up to the consumer - if, for example, you're writing all that data to a database, then the logical thing to do would be to initiate a transaction at the beginning of the parse, and then roll that back when the parser calls for a rollback, or commit when you have gotten enough information to conclude that the parse has succeeded. If you're storing data in a file, then the rollback would involve deleting the file, or you might first store the data in a temporary file and "commit the transaction" by moving it into place (and roll back by simple abandoning or deleting the tempfile). If you're calling external services, then, well, you will need to figure out a way to roll back those effects - worst case, you will have to buffer your intended action, and only send them after the parse has succeeded. But either way, it's going to be application-specific.
I’m not entirely sure what you disagree with, and I pretty much totally agree with you. But, being reddit I’m sure we could/should confect something :-)
Does using explicit imports (`import SomeModule (someFunction)`) make the building process faster? If not, why should they be used (Beside avoiding ambiguity)?
&gt;If the aim is to increase the adoption of Haskell, it’s more feasible to change Haskell so it’s more understandable by the world, rather than wait for the world to change itself so it can understand Haskell better. I guess that the discussion should take this as a focus point. I work as a tester/QA-minded devops engineer. I think this world would be better off, a lot safer, if we'd get more well-educated developers. Many people can make an app that works, but very few seem to be able to make one that is relatively safe. I think learning Haskell, and learning about the math that governs it (not everyone to the same extreme levels as some.. obviously) would be helpful in this regard. Imagine cars, plaines, bridges and buildings being engineered in the same way much of our software is... pretty scary really! I wonder if getting more adoption for Haskell by "appearing less mathy" would be beneficial or actually the contrary ? Maybe we could aim for a way to get more devs actually interested in learning the hard stuff, versus finding ways not to (have to) ? So yeah.... what's the aim? If we do not agree there, then discussing the way to get there seems rather pointless. However it is nice to think about it, have the topic come up and see what we can learn.
&gt; But I think the more important point is that alien words don't have an association. I really don't like Mappable for Functor, because there are a lot of valid Functor instances (that obey the laws!) that are not doing something you would describe as "mapping". Boy, are you going to be disappointed when you find out what the single method of the `Functor` class is called.
How do you implement `seq` via typeclasses?
I will need to write an application that acts as a file-server; thus I'd need to do some 'seteuid' calls upon requests from diffrent users to impersonate file operations with them. The effective user-id should be per-thread; would it be sufficient to use 'forkOS' for the critical parts of code? Can I be assured that nothing else will run on the OS thread created by forkOS?
I love that `-o` is pronounced "lollypop"
Hey, h3st, just a quick heads-up: **lollypop** is actually spelled **lollipop**. You can remember it by **i in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Looks really nice.
I think it would be possible, yes! I don’t think there is too much ad-hoc patchwork, and if there is, then this would uncover it, which would be great.
delete [both are allowed](http://www.dictionary.com/browse/lollypop?s=t)
You can pull lenses out of my cold dead hands.
you are right. I think I got too carried away. But I think the argument still holds for monad.
oh, well i think i misjudget your comment. I also didn't make the connection between you and the video (although you mentioned it in the first sentence 😉).
As I understand it, it's mainly because it makes it (a lot) easier to see where functions come from; figure out where to go for more info; and refactoring. Just to name a few benefits I could think of.
Actually, this is a [total loss of accuracy, while preserving precision](https://en.wikipedia.org/wiki/Accuracy_and_precision). But, yeah, you overflowed `Int` which is either 32-bit or 64-but signed. If you need a range greater than that, `Integer` is the way to go.
You can query lambdabot on #haskell for that kind of thing: &lt;me&gt; @pl \x y z -&gt; isJust . liftA2 x y $ z &lt;lambdabot&gt; ((isJust .) .) . liftA2 Note that you were missing the `$` in the definition you gave. An expression of the form `g = \x -&gt; f x` can be *eta-reduced* to `g = f`. But what you really defined was is x y z = (isJust . liftA2 x y) z You can eta-reduce this once, but what you have left-over is *not* is x y = (isJust . liftA2 x) y which doesn't even typecheck. In cases like these, where you want to compose (curried) multi-argument functions, you often encounter these weird partially applied composition operator sections. As you can guess, things become quite unreadable quickly.
Thanks! 
'nix-repl' has tab completion and is a good place to explore nix. After starting 'nix-repl' do ':l &lt;nixpkgs&gt;' to load the nix package set
On top of making it clearer where code is coming from, it also defends against future versions of the imported module exporting symbols with the same name as other symbols your module is using. Without the explicit import, any new conflicting symbols will cause your module to stop building. So it's pretty critical to the PVP, since PVP doesn't consider new symbols to be a breaking change, but people don't really do it as often as they should.
I agree with you but I also think that the formalism isn't the best way to start learning. Java has CompletableFuture, Optional, Stream etc which all have map/bind/return. With some usage examples and operational intuition most people pick them up quite quickly. On the other hand it is very hard to get an intuition for Monad without knowing a bunch of examples to generalize from first. After having some intuition for Monad the laws are easy and you can compare it to Functor/Applicative/Comonad/Arrow and so in. Maybe monomorphized functions are the better way to learn? But map/concatMap are pretty much the only ones so there isn't much opportunity to generalize.
 class Seq a where seq :: a -&gt; b -&gt; b instance Seq (Maybe a) where seq Nothing b = b seq (Just _) b = b ... I guess
Exactly. Laziness is the feature that really needs baked in support in a mutable language, creating strictness is easy. 
Yeah, I that's why I've added "EDIT: I teached haskell to a few friends. I just ignored Monad, Functor etc. You don't need it immediately.". I also don't think it's best to introduce the formalisms. You have to learn how to interpret them. I think the best way is to just ignore them until they are literally intruding. Then they are more or less obvious.
I'll link [an older comment of mine](https://www.reddit.com/r/haskell/comments/7wmhyi/an_opinionated_guide_to_haskell_in_2018/du2506q/) listing some of the benefits of Nix / NixOS. The Nix manuals are excellent resources for beginners: - Nix: https://nixos.org/nix/manual/ - NixOS: https://nixos.org/nixos/manual/ - Nixpkgs: https://github.com/NixOS/nixpkgs
Brilliant build-up to the next post. Waiting for it.
My favorite Jurassic Park quote often applies to Haskell code I see in the wild: &gt; [They] were so preoccupied with whether or not they could, they didn't stop to think if they should.
Right, of course, it's the accuracy not precision. Thanks, and yeah you're right I overflowed `Int`.
Resist perfectionism, but don't write messy, unreadable code. To be able to refactor it later, you have to understand it.
This place is fine, or /r/haskellquestions, or the code review stackexchange.
Whoa! Obelisk is sleeeeeeek!
I haven't studied PL semantics in a while, so this might be wrong &amp; grain of salt &amp; all, but IIRC you shouldn't have lost progress and preservation, but your type system is unsound (Girard's paradox) and type checking may not halt.
Progress and preservation are properties of a language's semantics wrt. to some type system. You didn't define any semantics for your language, nor did you define your term-level syntax, so it's impossible to say, really. But intuitively, mixing recursive types with `-XTypeInType` (whatever that means for your language) should be possible in a way that progress and preservation are provable.
isnt soundness defined as progress and preservation? it is indeed type in type, but there is a bottom, so no need to go with girard paradox, and one can effectively do the same using fix. I think it is inconsistent (in Curry Howard sense), but soundness is still there. Regarding type checking may not halt: it is absolutely the case.
Will the interesting journey include editing, or just reading such documents?
the semantic is basic dependent type (no inductive, no universe, just type in type, forall might take val, and lambda unified with big lambda) with fix
I believe it's "type safety" that's defined as progress + preservation. Soundness is a term from formal logic (where progress and preservation don't exist), defined informally as "in this system everything that has a proof is true" (cf. completeness, "in this system everything that is true has a proof"). I don't understand why you believe Girard's Paradox doesn't apply to you (you may be right; again, I'm rusty), but presuming that it does: everything in your system has a proof, so it's inconsistent - there are proofs of P and \~P - and thus unsound (presuming that your semantics don't allow something to be both true and not true at the same time, I guess).
For the sake of completeness, export lists have their merits as well. In addition to being able to define abstract data types and structuring haddocks, GHC will be better able to specialize polymorphic functions.
The point of laziness is that you perform the computation *at most* once. You can't get that without either compiler support or mutable variables.
There _are_ efficiently editable "fingered" succinct data structures. [Farzan and Munro](https://www.sciencedirect.com/science/article/pii/S030439751000592X) give one such construction. It is a bit more difficult to construct than the usual succinct/compact indexed dictionary models folks tend to use though, and then everything kind of wants to live in something ST-like, but it is possible.
&gt; Names like Mappable and Combinable instantaneously give most people some intuition about what those typeclasses do. People keep saying this about Mappable, but I don't buy it. Outside of math and programming, "map" refers to a diagram that shows where things are located first, and maybe a plan second.
I've been thinking about this for a while. You might be interested in ΠΣΠΣ : Dependent Types Without the Sugar: https://www.andres-loeh.de/PiSigma/PiSigma.pdf
The high memory usage could be because you're using a parser. Why not regular expressions? [https://wbazant.github.io/blog/2018/04/26/how-to-parse-xml-with-regular-expressions/](https://wbazant.github.io/blog/2018/04/26/how-to-parse-xml-with-regular-expressions/)
FYI (F-OP's-I), PiSigma doesn't provide `fix` but rather permits full mutual recursion between named bindings in a context. I am not a very proof-proficient type theorist, but empirically PiSigma permits a lot of terms which "never normalize" --- both at the term level and at the type level (i.e. it's pretty easy to put the type checker into an infinite loop). As far as preservation goes; the paper discusses the fact that subject reduction for open terms doesn't hold in PiSigma. It's a neat little system, definitely worth reading the paper and playing with the interpreter!
Why not define different record types and have some optional cast between them? You could do something like... {-# LANGUAGE RecordWildCards #-} data Config = Config {configA :: Maybe Text, configB :: Maybe FilePath, configC :: Maybe Integer} data ConfigFoo = ConfigFoo {configFooA :: Text, configFooB :: FilePath} createConfigFoo :: Config -&gt; Maybe ConfigFoo createConfigFoo Config{..} = ConfigFoo &lt;$&gt; configA &lt;*&gt; configB You have the main config class that contains everything that is optional for the user to specify, and then have a new record for each API. All the maybe checks can now happen at one point when you start the API. The downside is that you'll need a new type for each new API, but really one would implement that in an ad hoc fashion anyway if they had different requirements.
[https://softwarefoundations.cis.upenn.edu/plf-current/StlcProp.html](https://softwarefoundations.cis.upenn.edu/plf-current/StlcProp.html) is where I get def of type soundness from, can I has source for the other definition? It definitely apply, all I am saying is that we already has fix, so no need to go all the way to Girard to construct paradox - fix id will do.
Thank you, I have asked a question [here](https://www.reddit.com/r/haskellquestions/comments/9270jn/code_review_looking_to_know_the_quality_of_my_code/).
Well I wish that it was like this - I ain't too great at the math part, and while I sometimes find articles doing mathematical proofs to explain even the simplest things intimidating - it still feels like a breath of fresh air to me - compared to a ton of rehashed OO literature. But currently being jobless - I can't say that going into the "how things work" isn't currently bringing any food to the table. Most of the positions available ain't ever gonna bat an eye at me telling them that I can argument my code decisions, instead of the seemingly required - I can name you the 5 last self-proclaimed best practices I've Googled.
what do those two values returned by the `mkStdGen` stand for?
https://books.google.com/ngrams/graph?content=lollipop%2Clollypop Language can change.
I think soundness means "type-safe", not consistency? Haskell's type system is sound but not consistent.
The [wikipedia article on soundness](https://en.wikipedia.org/wiki/Soundness) describes the property I'm referring to.
Probably because the strict languages you're thinking are poorly designed. You need compiler support in either case.
Progress and Preservation talk about the relation between small-step operational semantics and the type system. In this case, you are throwing away normalization (due to Type : Type), but you should still have progress and preservation if you set things up right. One thing to note is that with dependent types &amp; bottom, you lose decidability of your type system — your type checker may run forever trying to normalize a term that will never normalize. (A project that managed to reconcile this is ZOMBIE, by treating “total” computation differently from “partial” computation in the type system.) 
This argument is literally the entire point of the talk! If names are important, let's *not* invent entirely new sets of synonyms for existing concepts that have large amounts of pre-existing literature. Using the same language allows inter-disciplinary communication, and the original inventors of these things have invested huge amounts of time and thought into them. By using the original terms we can harvest that investment. By creating our own unique jargon (per language!) we wall ourselves into our own special gardens and shun outsiders, all in the name of making it "easier" for beginners.
Exactly. Try googling "mappable" and then try googling "functor" and decide which gives more useful results :-)
`mkStdGen` only returns one value which is of type `StdGen`. When you `show` that value, which is the default operation at the GHCi prompt, you see the two fields which make up the internal state of the random number generator.
Lots of people are telling you to use `Integer`, but you should probably use `Natural` (from `Numeric.Natural`) instead. They're both arbitrary precision, but `Natural` represents non-negative numbers. Since factorials can't be negative and you can't take the factorial of negative numbers, this represents the domain and range of your function better.
You give this up as soon as you have Type in Type. Mutual recursion doesn't add any computational power since you could simply define a tuple by mutual recursion.
I think others have answered the main question, but I'm also curious about why you chose to write the function that way. I'm new to Haskell and was wondering if there was something I wasn't understanding. I would have only thought to write something like this: factorial :: Integer -&gt; Integer factorial n = if (n == 1) then n else n * factorial (n - 1)
Don't you lose the ability to represent recursive types that aren't "normal"? For instance, look at the definition of a `FingerTree`: data FingerTree a = Empty | Single a | Deep !Int !(Digit a) (Seq (Node a)) !(Digit a) I don't think this can be done with a normal `Fix` type.
I'm also new to Haskell, but I would have written it a third way: fact :: Integer -&gt; Integer fact 1 = 1 fact n = n*(fact n-1) I think this is a lot clearer than both of yours, and uses pattern matching, which is very very cool.
What's the definition of Digit, Seq and Node?
&gt; You need compiler support in either case. You don't need compiler support unless you want `seq` to work on `a -&gt; b` or `Void`.
You dont fix on type and fix on type-&gt;type instead?
This is flawed. 'Mappable' never meant anything in the first place,and 'Combineable' sure as hell doesn't either. The major problem here is that people think they know anything at all, because they use bad nomenclature with no concrete definitions to describe poorly thought out and ill-designed constructs. Programming as a practical discipline has some of the worst nomenclature ever, nearly everything has at least 4 valid and completely different interpretations based off of context. Getting away from these concepts is constructive and serves an important purpose. I am all for using educational anaologies to ease introduction, but having a clear break from poorly defined concepts is a very valuable thing.
 data Digit a = One a | Two a a | Three a a a | Four a a a a data Node a = Node2 !Int a a | Node3 !Int a a a And for `Seq` there, I meant `FingerTree`.
Great! We need some good succinct data structure libraries!
Or `fact = product . enumFromTo 1`. 
can't you do this? data FingerTree' a f = Empty | Single a | Deep !Int !(Digit a) (f (Node a)) !(Digit a) data FingerTree = Fix Fingertree'
What people are you referring to? Every mainstream programming language – Java, Python, C#, Swift – has a map function. So amongst programmers, Mappable is far more comprehensible than Functor. In the general population, Combinable, Traversable etc are still comprehensible, but Reducible and Mappable probably are not. Then again, the general population also won’t understand what Boolean-expression means, so the point is moot. FWIW I do think map is a terrible name, both for applying things to values in containers, and for dictionary-like data-structures. It’s is their with buses and ports in the history of terrible computing nomenclature. However, as I mentioned above, it’s much more feasible to work with the world as-is than try to force the rest of the world to change. 
That makes sense. I didn't know about `Natural` type, but you're right, in this case it might make more sense.
Or `fact n = product [1..n]`
Have you updated your Intero installation?
hie + vscode works fine for me. I've also spent some time using hie + atom and ghcmod + atom and both of those worked fine. ghcmod + atom was the most featureful combination (things like inserting type signatures, different highlighting for imported identifiers...) but it got a bit too slow for larger projects so I've switched to hie. It still eats a ton of RAM but less than what I have at least.
You're right, this was not the best approach. The reason why I wrote it like this is because I'm coming from Scala where compiler wouldn't do tail call elimination. In this code `def fact3(n: BigInt): BigInt = if (n equals BigInt(1)) BigInt(1) else n * fact3(n - BigInt(1))`, you couldn't use `@tailrec` because (in compiler's words) `error: could not optimize @tailrec annotated method fact3: it contains a recursive call not in tail position`. That's not the case here, but I just wrote it like that out of habit.
&gt;23 hours ago OP's version is tail recursive, which is safe against stack overflows and slightly more efficient.
Some options that come to my mind: 1. Wrap each optional field in a `Maybe`. 2. Pass the arguments one by one (or in tuples) if a function does not need the whole record. 3. Provide a `defaultConfig` function to make it easier to fill all the record fields. 4. Start with the smallest record, add a type-parameter and an `extra` field that allows extending the record with the parametric type. This can lead to nested data-structures. 5. Use `-XDuplicateRecordFields` and define a different type for each set of fields that you need. Then access the fields using lenses or the `GHC.Records.HasField` class. 6. Use a library that gives you extensible records.
Not op but tail recursive functions are a lot more efficient. The operational intuition for the accumulator version is check: if (n != max) then goto loopbody else goto end loopbody: acc = acc * n n = n + 1 goto check end: R1 = acc goto stack[0] Which is the equivalent of a while loop in c. The non-tail-recursive function is something like: fac: stackcheck: if (stack_available &lt; 12) then goto gc else goto check gc: ... check: if (n == 0) then goto basecase else goto recurse basecase: R1 = 1 goto stack[0] recurse: push n R1 = n-1 call fac pop n R1 = R1 * n goto stack[0] Which is the equivalent of a recursive function in c with an extra stack check because the haskell stack is growable.
How is this a recursive type?
Sorry, totally detracts from the main points but there are some really neat CSS tricks going on there. I have a similar design on my page (https://two-wrongs.com/technical-writing-learning-from-kernighan.html) but I put all of my margin notes at the bottom of the page when viewed on a small screen, simply because I didn't want to use JS to reveal hidden content. This guy has turned the margin note number into a label for an invisible checkbox, and then reveals the hidden content with a sibling selector on the checked checkbox. I actually don't see a huge problem with it, other than perhaps screen reader users who are more familiar with footnotes being at the bottom of the document.
Actually kind of forgot the pain of making Hakyll output the right HTML that works with stock tufte-css, because I don’t want to maintain a fork
I've been using spacemacs and it does those things you wanted with minimal configuration, you just enable intero mode and thats it.
I mean, it is probably easier in the long run to output the right HTML, but it was psychologically simpler for me to just make my own implementation of margin notes...
Consider the definition of a functor `f` as "mapping" morphisms `(a -&gt; b)` to morphisms `(f a -&gt; f b)`. We can write this in Haskell as the type `(a -&gt; b) -&gt; (f a -&gt; f b)` But here the meaning of "morphism" and "mapping" are both represented as `-&gt;`, so we have lost something there. This is due to the fact that in the category `Hask` (or any category with [Exponentials](https://en.wikipedia.org/wiki/Exponential_object)) we can make the concepts of "morphism" and "mapping" coincide. (Funny enough - this is witnessed by the isomorphism of `curry` / `uncurry` mentioned here) However - in the theory of taking duals (reversing arrows) we need to keep them separate. A way we could express this more clearly is do define ``` `type Hom a b = a -&gt; b ``` where a morphism from `a` to `b` is now represented as `Hom a b` instead of `a -&gt; b`. Then the type of `fmap` is instead ``` Hom a b -&gt; Hom (f a) (f b) ``` when we take the duals we reverse only the `Hom`s. So the dual of a functor becomes ``` Hom b a -&gt; Hom (f b) (f a) ``` (since this is a just renaming `a &lt;-&gt; b` this shows the dual of a Functor is a functor) If you want to consider the dual nature of for instance `foldr` with `unfoldr` you'll need to set up the progress categories - and you might want to look into [Algebras](https://en.wikibooks.org/wiki/Abstract_Algebra/Category_theory) and Coalgebras.
**Exponential object** In mathematics, specifically in category theory, an exponential object or map object is the categorical generalization of a function space in set theory. Categories with all finite products and exponential objects are called cartesian closed categories. Categories (such as subcategories of Top) without adjoined products may still have an exponential law. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
**[Exponential object](https://en.wikipedia.org/wiki/Exponential_object)** &gt;In mathematics, specifically in category theory, an exponential object or map object is the categorical generalization of a function space in set theory. Categories with all finite products and exponential objects are called cartesian closed categories. Categories without adjoined products may still have an exponential law. ***** ^[About](https://www.reddit.com/user/ultimatewikibot/comments/90r969/about) ^| ^[Leave](https://reddit.com/message/compose?to=ultimatewikibot&amp;subject=Blacklist&amp;message=Me) ^[me](https://reddit.com/message/compose?to=ultimatewikibot&amp;subject=Blacklist&amp;message=Me) ^[alone](https://reddit.com/message/compose?to=ultimatewikibot&amp;subject=Blacklist&amp;message=Me) 
everything is up to date, i used stack lts for ghc 8.2 8.4 and 8.0 isntalled every intero on all platforms still problems
&gt;I think we need to be open to the fact that the names chosen by mathematicians in the past might not be optimal for programmers in the present. But, is there such a thing as "optimal for programmers in the present"? The premise that the old names aren't optimal leads to constant churn of the names as people search for ever better names. But, these are entirely subjective, so your great name isn't great for me. So, we develop entirely inconsistent jargon, and can no longer communicate effectively or read each other's code. Perhaps we should prefer consistency, and just demand that the reader learn what a `Monoid` is, and not have to know that it is a `CombinableWithZero` in lib X, an `AppendableIdentity` in lib Y and an `AbstractAssociativeBinaryFunctionProxyBeanFactory` in Spring! (oh and god help us when it comes to more complex types like `Group`, maybe `InvertibleCombinableWithNeutralElementClosureAndAssociativity`? I mean, there's a reason we have names, right?
The thing is, if you read English, Java-ish names like `CombinableWithZero` are much more comprehensible than `Monoid`, though `Reducible` might be better. Most of the rest of your names are nonsense strawmen. As easy as it is to parody Java naming conventions, they follow a formal structure and are comprehensible. Ultimately this is not a subjective discussion: if one were to launch a poll on /r/programming asking people to match names to descriptions, one could objectively determine which of Reducible vs Monoid were easier to understand to the population of programmers. 
&gt;`Reducible` might be better And you accuse me of nonsense strawmen :-) Surely a Reducible is a thing that can be reduced? How is a binary associative endofunction reducible? How on earth could I reduce it? What about that term invokes any sense of associativity? You are assuming at the least the reader is acquainted with the signature of a function such as: `reduce :: Foldable1 f =&gt; f a -&gt; (a -&gt; a -&gt; a) -&gt; a` in order to know what a Reducible is. IOW you are appealing to existing knowledge! I simply claim that naming is not in any way objective. It is clearly and purely subjective, and your test doesn't change that fact. You simply claim that popularity of a term would validate it. I don't much disagree, but simply wish to point out that the cost of turning your ad-hoc naming preference into the commonly preferred term, particularly in the presence of an existing well defined term, is rather large.
We’re both appealing to existing knowledge: you to the jargon of category theory; and me to English and idioms used in the majority of programming languages. The point is, which vocabulary is most widely understood. Naming might subjective, but comprehension is measurable, and hence objective. I can’t see a language becoming more popular by becoming harder to learn, and I can’t see a language becoming easier to learn by using less comprehensible naming. Regarding Reducible: if you talked to a reporter from Wired and asked him which Big Data DB had a Functor/Monoid execution mechanism, I’d wager they’d be pretty confused. If you instead specified a Mappable/Reducible mechanism they’d almost certainly say Hadoop. 
&gt;We’re both appealing to existing knowledge: you to the jargon of category theory; and me to English and idioms used in the majority of programming languages. Absolutely! I am not appealing though to the "jargon of category theory", but to strongly defined fundamental mathematical structure with enormous commonality across fields, as well as a huge corpus of literature on the subject. You are appealing to some much flimsier notion that some speakers of English, who happen to also be programmers, may have come across if they are using library that may actually be descended from something functional in the first place. Why wouldn't I have called "reduce" "transduce" in my library and hence have no idea what you mean? Maybe I called it "collect", and now my best name for Semigroup is Collectible. You think this is a strawman? This is the logical conclusion of your argument, and you justify this my believing names matter? I am not necessarily disregarding that using the precise terms makes learning harder, I am simply trying to de-emphasise the idea that the difficulty of learning these basics is the most important concern. We have schools that teach complex maths like Calculus to kids. Immensely useful general algebraic principles are also useful things for people to learn, and aren't *actually* that complex. Monoids are an extremely common property, and learning about what they are is extremely valuable. We commonly ask programmers to learn the API and idioms of various libraries, most of which share very little with the next library they need to learn, rendering that knowledge bespoke and quickly redundant. Learning these mathematical structures is often the opposite, broadly applicable and useful potentially for life.
Russian haskell community have it's own haskell based site [https://ruhaskell.org/about.html](https://ruhaskell.org/about.html) [https://github.com/ruHaskell/ruhaskell](https://github.com/ruHaskell/ruhaskell)
Hm.. the thing is that it works on my machine without any problems. So it's probably some messy config file somewhere.
Let me show you how to make the function completely point-free in a step-by-step fashion. is x y = isJust . liftA2 x y is x y = (.) isJust ((liftA2 x) y) is x y = ((.) isJust) . (liftA2 x) $ y is x = ((.) isJust) . (liftA2 x) is x = (.) ((.) isJust) (liftA2 x) is x = ((.) ((.) isJust)) . liftA2 $ x is = ((.) ((.) isJust)) . liftA2 is = ((isJust . ) .) . liftA2 
The axiomata defining the mathematical structure are wholly independent of the name of the typeclass implementing that structure. There is no fundamental conflict between mathematical rigor and naming. One could note in documentation that Reducible follows the Monoid laws from category theory, and then enumerate those laws. 
Thank you! I didn’t know about tail call elimination, but I knew something seemed different between our functions. It’s an interesting concept, and I think I now see why some functions that I’ve written have resulted in a stack overflow. Thanks for the explanation!
Sure, give it a bespoke and inconsistent name and hope that your documentation makes some connection to the actual name. What have you achieved? I'm not convinced you've achieved much at all. You think that names matter, and hence you should give your thing some weird name that makes sense to you, in your context. But even your example is laughably meaningless. Here's the google result: &gt;reduciblerɪˈdjuːsɪb(ə)l/*adjective* &gt; &gt;1.(of a subject or problem) capable of being simplified in presentation or analysis."Shakespeare's major soliloquies are not **reducible to** categories" &gt; &gt;2.MATHEMATICS(of a polynomial) able to be factorized into two or more polynomials of lower degree. So there IS a mathematical definition and it means something completely different to what you want it to mean. I'm getting a little of this game.
I'm curious about your comment here. I thought the function that /u/spinning-laef suggested can be optimised by GHC into a tail recursive function. I don't actually know that, since I'm also fairly new to Haskell, but I thought that was the case because his function doesn't return stack overflow even for pretty huge inputs. Perhaps it's just because the stack is big here, but the Scala version would definitely fail without tail call elimination.
Try `is :: (a -&gt; b -&gt; c) -&gt; Maybe a -&gt; Maybe b -&gt; Boolis = ((.) . (.) . (.)) isJust liftA2`
GHC dynamically grows the stack when we run out of space. There is a maximum but it's 10mb so you would need a bizarrely huge number to reach it and probably would run out of memory long before that when using Integer.
Huh, interesting! When I read the title, I thought it took both the video and the subtitle, and actually tried to fit the written text to the dialogue. This is much simpler, and might just be suitable for 90-99% of the out-of-sync cases.
Or `fact n = product [1..n]`, since this wouldn't require a huge stack.
Is everyone using intero in spacemacs now? I've been sticking with just the default haskell layer configuration, and recently after updating my emacs packages, flycheck seemed broken, so I had to roll back.
A very common issue I have with these tools is that they don't properly treat directory paths with space characters in them, which is very unfortunately something I cannot change. Maybe you have a similar situation?
For us who haven't watched the presentation, how much linear typing can we presently use? What milestones are still in the future?
nono, i usually avoid space in paths, i use \_ instead of space. i fixed it by running "stack new prname" and moving all my files there
oh thanks man, finally smth that works
intero is for stack projects
Both this and your initial claim about Type in Type being type safe but unsound are correct.
[removed]
Quite. Every programmer I've ever met who's worked with Hadoop has told me how confused they were when they heard it involved a cartographic approach to polynomial factorization.
Now that we have hnix is it reasonable to assume that we might get hnixos where configurations are done in Haskell/Idris/Dhall? [https://github.com/haskell-nix/hnix](https://github.com/haskell-nix/hnix)
The code actually is quite nice! Easy to read and understand.
But I like the esoterism of Haskell! It makes it seem more complicated, and by extension me more smarter, than it/I is/am!
It's eerie how similar this is to the blog post I wrote on [my own website](https://danso.ca/blog/intro/).
It is not implemented in ghc yet. [Tweag](https://github.com/tweag/ghc/tree/linear-types) is working on it.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tweag/ghc/.../**739d059259cbfa4d3394141d338690580e98f1a4** (linear-types → 739d059)](https://github.com/tweag/ghc/tree/739d059259cbfa4d3394141d338690580e98f1a4) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e353afa.)
[Appearently type safety = type soundness](https://cs.stackexchange.com/questions/82155/is-there-a-difference-between-type-safety-and-type-soundness)
I believe that in today
In today's Haskell, we'd write (~&gt;) :: forall (i :: Type). (i -&gt; Type) -&gt; (i -&gt; Type) -&gt; Type `i` is best imagined not as a *type* but as a *kind*. I don't know enough about traditional dependently typed syntax to understand what he's getting at with the `∀`s.
Nice! I've also been using a Hakyll for a few years, but I'm not doing anything really complex with it. My only problem with it is that the stack build seems to be enormous (both the output and the memory requirement) and I haven't yet found a good way to run the build on the droplet instead of locally.
Depending on what you are doing that might be better. In this case it's probably worse. Haskell does do TCO, but it's also lazy. So, it might be better to be productive rather than tail-recursive. For example: replicateTR n x = replicate' n [] where replicate' 0 res = res replicate' n acc = replicate' (n - 1) (x : acc) replicateP n x = replicate' n where replicate' 0 = [] replicate' n = x : replicate' (n - 1) The second is almost always the better Haskell function, since the WHNF of the result can be calculated in O(1) time, and the result can be consumed lazily. The first requires O(n) time to determine the WHNF, eliminating almost all the benefit of laziness. In fact, if you were using Peano Naturals, the implementation you provided would be better because it is productive. But, in the case of `Int` or `Integer`, neither solution is productive, so the tail-recursive one optimizes better. Of course, it you *know* that the result is *always* *entirely* consumed, productivity loses it's advantage and tail-recursion can be better again.
For other potentially [informative versions of factorial](https://www.willamette.edu/~fruehr/haskell/evolution.html). /u/AristaeusTukom /u/spinning-leaf /u/chetanbhasin 
Kinda funny that `mappable` is good but `functor` is "not". Both map and functor are names borrowed from math, it's just that one has gotten popular because of mapreduce and the other hasn't. I'll take the tons of existing literature over the "I think I kinda know what this might do." feels any day though.
Thanks for the responses all! I think this seems like the easiest solution, I realize now that there's no way around making every field `Maybe Type`. Just to follow this thinking a bit more in case it helps anybody else: I think this would work well with a helper method `hasFields :: Config -&gt; [Config -&gt; Maybe a] -&gt; Bool` that each function calls at the beginning, like: api_foo :: Config -&gt; IO () api_foo conf = do if (hasFields conf [a, b]) then return () else ... and since hasFields just works on a list I could combine it like... fooReq = [a, b] barReq = [c, d] api_foobar :: Config -&gt; IO () api_foobar conf = do if (hasFields conf (fooReq &lt;&gt; barReq)) then return () else do _ &lt;- api_foo conf _ &lt;- api_bar conf return () I guess the only thing this is missing is that it's pretty manual -- you have to manually combine the requirements, knowing that foobar calls both foo and bar. Ideally it would be nice to have this bubble up, i.e. that somehow foobar knows that it's calling foo and bar which have respective requirements that their fields be non-Nothing, so their requirement union has to be satisfied, though I'm not sure how to do this, or if it's possible at all...
Actually using that, but thanks. As usual when you have very special requirements, you end up having to hack a lot of stuff yourself
&gt; What people are you referring to? The people who have argued that Haskell shouldn't use names like Functor, Monad, etc. I haven't been keeping a list. &gt; Every mainstream programming language – Java, Python, C#, Swift – has a map function. &gt; &gt; So amongst programmers, Mappable is far more comprehensible than Functor. Your original argument was about "most people". Most people are not programmers. Traversable is another name that requires considerable context to understand its meaning. If you aren't familiar with the concept of "walking" through a data structure, it provides no intuition whatsoever. In contrast, Combinable is suggestive but vague. There are lots of ways things can be combined. Monoid is a precise concept with a fairly unambiguous name.
Thanks! Am I right in thinking that the first point is exactly what is shown by Girard's paradox, and the second is equivalent to saying you can replace mutual recursion by taking a fixed point (using "non-mutual" recursion) over a record/tuple?
Cool hacks! Hakyll can be a bit tricky; I gave up on customizing things in Hakyll a long time ago. Too much magic just makes everything tougher to understand. Just going to leave these other options here: - https://github.com/ChrisPenner/slick - https://github.com/ChrisPenner/SitePipe
On the first point not quite but close. Girards paradox doesn't directly imply that type checking is undecidable. There's a closely related proof that shows that so called "looping combinators" exist. That implies the system is Turing complete and thus type checking is undecidable. These looping combinators are however very closely related to Girad's paradox (or all the papers I've seen start from there anyway) Exactly what I meant on the second point.
No comment of mine mentioned “most people”. Please don’t misrepresent what I’ve said. I’ve consistently spoken about the intersection of people who are programmers and people who speak English, and argued that this is greater than the population of people who know category theory or Haskell. 
IIRC Haskell (when compiled) actually has an unbounded stack, so no maximum, at least until yo actually start running out of available RAM. 
Discord is proprietary software. :( Why not use free software?
I had no idea this existed. Thanks man.
You are absolutely right! GHC used to copy the entire stack when growing it leading to quadratic runtimes. Now it's a linked list of chunks so growing it repeatedly isn't as expensive. There still is max stack size -K, but the default is 80% of the physical memory! https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html#rts-flag--K%20⟨size⟩
I'm sure there are also some free software alternatives inhabited by the Haskell community.
https://www.reddit.com/r/privacy/comments/8lkb5s/friends_dont_let_friends_use_discord_the/
Reddit is proprietary software. :( Why not use free software? /s (I also despise discord... and reddit)
If only there was some internet powered relayed chat protocol, on which the Haskell community was one of the best in class.
Yep, #haskell on freenode works fine.
I would upvote this but it's full of `[deleted]` so doesn't actually add much to the discussion. Got a better link?
No. It's not my post under a different username so make what you will of it. I'm not trying to create FUD, just consider open source.
I come for the front-end functionality, not the text, and discord is so much funkier than `M-x erc&lt;RET&gt;`
Is loading the whole file into memory actually causing problems for you? If not, I would just take the memory hit. It seems like to support streaming in a file, your C library *also* has to support streaming in a file, which doesn’t seem super likely, and maybe not achievable, for OCR libraries.
#haskell is something special. Super helpful, active, and very hospitable to all knowledge levels.
Thanks for the response I think I asked the question poorly Tesseract returns the image text as an internally allocated string from the TesseractGetText function I don't know how to create a Haskell version of this string before freeing the original version, as Haskell can order things as it likes (ie. I'm worried about the lazy io problem) 
I use IRC a lot because it's the best free chat community right now, but I wish people would stop defending it as if it's good software. IRC is a garbage UI / UX and security was an afterthought.
Haha, yeah, security is terrible. But it comes with an Emacs client, which makes the UI/UX way better than the alternatives in my eyes :). Especially when it comes to talking about programming...
I would love to join the group I just want to know what are the real use cases of Haskell can we use it for building webapps 
Thank you so much, I finally get it now!
I tried to use the emacs client. But I desperately wanted a persistent chat log and the ability to log in from multiple devices. I tried to use znc to get the best of all of this, but that dramatically exacerbated the terrible-ness of the UI / UX.
I think that the familiar terms and appealing to existing intuition were a huge positive factor for the adoption of OO, but a negative factor in learning to use OO effectively. Sure everybody knows what an object is and the analogies seem to make sense so people feel comfortable jumping in and designing systems using OO. Things seemed to make sense because they fit with existing mental model so we ran right off the cliff with it. Objects weren't really just generic things and inheritance actually extends functionality instead of specializing kind. Things only got better when people started to formalize these concepts: Liskov substitution principle, SOLID, composition over inheritance. With the distinction between English object and OO object and a better mental model we got better OO designs. Was the swell in adoptions and eventual progress worth the mess it created early on? I don't know, but I personally prefer the small and clean approach. I think this is a fun excersize: How can we give better names to OO objects? For instance, should a java object be an MutableStateEncapsulator and a smalltalk object be a MessagePasserReciever? 
Well that’s what I’m fighting here—I don’t think it was intended to be a kind, it was meant to be a dependent type. I mean the paper says right in the introduction he’s using some Strathclyde Enhanced version of Haskell with dependent types. Whether that actually exists or not (I interpreted it as a joke), who knows, but it’s definitely not just normal Haskell.
&gt; IRC is a garbage UI / UX This. As a long time user (since late 90s) and client implementor (since 2014 I implemented clients in Java, Python, Rust) I hate IRC. It's lacking essential features for meaningful conversations that these days most alternatives (free/open or not) provide, like - Persistent chat history - Easy multi-user chat - Discoverable and easy-to-create channels - Threads - Clients that can render code blocks, quotations, images etc. - ... I think for the purposes of #haskell IRC is fine as you don't have long conversations there or need to see the chat history etc. but for anything more serious IRC just doesn't cut it.
&gt; IRC is a garbage UI / UX This. As a long time user (since late 90s) and client implementor (since 2014 I implemented clients in Java, Python, Rust) I hate IRC. It's lacking essential features for meaningful conversations that these days most alternatives (free/open or not) provide, like - Persistent chat history - Easy multi-user chat - Discoverable and easy-to-create channels - Threads - Clients that can render code blocks, quotations, images etc. - ... I think for the purposes of #haskell IRC is fine as you don't have long conversations there or need to see the chat history etc. but for anything more serious IRC just doesn't cut it.
We are already using free software. But if you want to reach more people you need to bring the discussions to them. Many people use discord and some may be interested in Haskell but haven't tried learning it because there are no Haskell communities on the platform they know. Having a discord server means low effort way for people on that platform to talk about Haskell and expand the Haskell community.
what's wrong with znc? You get playback buffer when you get disconnected and also chat log saved. :) Where did it exacerbate the UX issue that IRC has? And how is irccloud a solution? It only has apple and mobile software..
I'm fairly certain that `seteuid` is per process. I don't think there is a way to set it on a per-thread basis (with out without Haskell in the picture).
Playback is a god awful implementation of persistent log. Persistence and authentication both to the ZNC and the IRC servers are both infinitely worse than with solutions like IRCCloud. IRCCloud isn't a general solution, but it was a solution for me. I won't advocate that the benefits of IRCCloud outweigh the importance of free software, but it was a compromise that worked well for me personally.
FYI I think you duplicated your comment
agree, huge thanks to everyone involved
Thanks, fixed.
Matrix.org and eg the Riot client (desktop version) is a pretty good non-proprietary alternative. Matrix got a bit faster recently and is likely to keep improving. You can not only create new rooms, but it integrates quite well (usually) with the existing #haskell channel, adding persistence and the other modern features. Here's #haskell in the Riot web client: https://riot.im/app/#/room/#haskell:matrix.org 
Yes, it's a general purpose language so you can use it to build whatever you want.
Here's an archive link, posted by a commenter in the linked thread: https://archive.is/Q4N9J
I tried out matrix and really liked their concept, but their clients (or network) is abysmally slow. Opening a channel when you open the app will take ages :/
I can't stress it enough! So far it has been the most amazing group on the internet. No question is too dumb
It has improved recently, when did you try it ?
A couple of months ago, I think. Maybe I should give it another shot :) Right now though I do like my setup of Weechat connecting to slack, IRC and Messenger. Really started missing condensed chats and split windows after having used the slack client for a while.
Well, it is a type, but we can promote those. `i` might be `Bool`, and `j` and `k` GADTs like data J :: Type -&gt; Bool -&gt; Type where Yes :: a -&gt; J a 'True No :: J a 'False I don't know anything about this fancy dependent notion of "container", unfortunately, so I don't understand a lot of the rest.
Really I thought it was functional so you can just do scientific computing
Girard’s paradox by itself doesn’t imply unsoundness because one can work with a dialetheic paraconsistent logic. Girard’s paradox combined with the principle of explosion from contradiction does make it trivial.
GHCi is configurable; [here is a link to the docs](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html). You could bundle a GHCi script that creates a bunch of `IORef`s to hold the config values, and a generic "runner" function that takes care of passing the values in those IORef's to functions however needed.
Tweag's implementation is far enough along that I could use it to implement [my pet examples of linear types](https://github.com/gelisam/linear-examples#readme). The next step is to get the [proposal](https://github.com/ghc-proposals/ghc-proposals/pull/111) accepted, and then to merge Tweag's branch into the ghc mainline.
I kind of agree that one shouldn't make up new words for already existing concepts most of the time. I have two points though. 1. The speaker uses made up terminology himself when he reads the type signature of `&gt;&gt;=` out loud: "Takes a container with an a in it, a function ....". This intuition is justified in haskell because there, functors kind of behave like containers in other programming languages, and are fixed to endofunctors on Hask. It's a useless intution for other functors, e.g. for the homotopy group functor or the compactification functor. This is related to the next point: 2. Different names for the same thing can convey different ways to *think* about it. For example, what's usually called a "random variable" is formally a measurable function whose domain has the structure of a probability space. The name is justified because, in many ways, such a measurable function behaves like an actual value of its codomain. Thus, the name adds intuition and *does* matter. For a much better explanation of this point, see Thurston's *On Proof and Progress in Mathematics*.
You have to go up a level, and Fix there first, then you are fine. http://www.cs.nott.ac.uk/Research/fop/blampied-thesis.pdf is related. data Funky a = Singleton a | Funk [Funky (a, a)] -- * -&gt; (* -&gt; *) -&gt; (* -&gt; *) data FunkyHF a f x = SingletonF a | FunkF (f x) -- (* -&gt; *) -&gt; (* -&gt; *) type PrePairHF f x = f (x, x) type PostLisrHF f x = [f x] -- (* -&gt; *) -&gt; (* -&gt; *) type ComposeHF hf hg f x = hf (hg f) x -- ((* -&gt; *) -&gt; (* -&gt; *)) -&gt; (* -&gt; *) type FixHF hf f x = hf (FixHF hf f) x type FunkyF x = FixHF (ComposeHF PrePairHF (ComposeHF (FunkyHF x) PostListHF)) x type Funky' x = Fix FunkyF x ... or something like that. I've never worked all the way through it, but there's a number of papers that cover generalized folding on "nested" or type with "non-uniform recursion". Whereas most of the stuff on "recursive types" assumes a simple base functor.
Oh I entirely agree the use of "container" was regrettable, it made me wince when I reviewed the video, and I apologise for it. The preparation was rushed, and not all points were as well prepared as I would have preferred. I will though excuse myself – a little – on two grounds. Firstly, it *is* a programming conference with a largely programmer audience, and I was using merely as an illustrative point. Secondly, I don't think it is quite the same thing to use metaphor, simile or concrete example when explaining a concept – particularly a very abstract one. Different people have very different learning styles. Some prefer to start with an abstract concept, and then delight in seeing its repetition. Some like to start with a concrete example, and then use isomorphisms from others into their original intuition to help them think about the new ideas. And possibly more simply prefer to be shown multiple examples before being introduced the abstraction. Somewhat amusingly, I was taken to task by some of the linguists in the room for using the incorrect definitional terms for "word", where I should have used "lexeme". I felt this to be both chastising, and somehow supporting of my argument :-) Anyway, the main point of the talk isn't to *demand* fidelity to canonical naming, although I do argue that is a valuable trait when considering your local name for the concept. More, I am trying to point out that names *matter less* than types and laws as fundamental identifying features, where I give the definition of "matters" as "changing it has consequence". You are of course free to use aliases in order to aid your thought processes. I have found myself however introducing them far less frequently than in the past.
Why would one not want seq to work on avoid?
The entire point of this presentation is an attempt to reconsider the orthodox position that names are not the most important identifying feature of a thing. In programming in particular, we have a proliferation of different names *for the same thing*. And people justify these different names *because names matter*. You have consistently in this post merely restated the orthodoxy, and then tried to justify your favourite bespoke name for a couple of different concepts, without actually justifying your favourite names aside from they are the ones that are familiar to you, or some other *argumentum ad populum*. You are basically arguing for a continuation of your favourite cultural hegemon, and its imposition on the new generation. I don't really understand why you maintain this tactic, as the orthodoxy is the orthodoxy because that is what most people generally already believe, ie. you're *already in the majority*. Personally, I used to think the same way too, but experience and much thought has led me to revise my position – you cannot and will not convince me to revert simply by restating orthodox positions, and not engaging in good faith in any counter-argument.
I mean you might want that, my point is just that you can't really get it without baked in compiler support, because there is nothing to pattern match on. Every inhabited non-function type can implement `seq` in a lazy language without compiler support.
Okay, I see why. However, you can’t always implement seq in a lazy language. If such a language has a strict interpretation of pattern matching, then yes you can. But other lazy languages are possible where pattern matching is lazy such that ```haskell x == case undefined of Left _ -&gt; x; Right _ -&gt; x ```
Can you identify free software with an equivalent feature set and flexible moderation? IRC, you might suspect, is not something I consider because: * Very few IRC clients that are free software are as good as the discord web frontend. * None of them have voice options, which can be pleasant for teaching scenarios. * File sharing is MUCH worse on IRC. * IRC is (as a protocol) antiquated, inefficient, prone to identity theft, and every countermeasure to these is centralized to an unaccountable group of people anyways. Is there some new ActivityPub-based and more-decentralized protocol that would \*actually\* motivate this complaint beyond the realm of subjective ideological opinions about which small group holds power and into the realm of decentralization and federation? If so, I am \_there.\_
For the most part, the \*people\* in #haskell are great folks. That shouldn't be confused with the technology though. There are tons of great people in the community, and if a better platform emerges for them we should jump on it.
I mean you could defeat such a compiler by replacing `x` with a two different sufficiently complicated strings of functions that are both guaranteed to return `x`, and no compiler could out-smart you in the general case due to turing completeness. So you can always implement `seq` in a lazy language for inhabited non-function types.
No because in such a language, ```haskell case e of Left x -&gt; f x Right y -&gt; g y ``` would mean ```haskell (f ⊥ ⊓ g ⊥) ⊔ case e of Left x -&gt; f x Right y -&gt; g y ```
No because in such a language, ```haskell case e of Left x -&gt; f x Right y -&gt; g y ``` would mean ```haskell (f ⊥ ⊓ g ⊥) ⊔ case e of Left x -&gt; f x Right y -&gt; g y ``` Therefore, in the case where the value being analyzed is bottom, the result is `f ⊥ ⊓ g ⊥`, which is not always equal to bottom
So is your hypothetical evaluation model that you execute both branches of every encountered case statement in parallel, with captured variables set to `_|_`. Then you check the results of the two branches for structural equality, at which point you return them. Then if you ever encounter one of the `_|_` from the captured variable you go back and execute the case statement normally to find out what should replace that `_|_`. Because yes such an evaluation model could defeat `seq`, hell such an evaluation model would probably make even a baked in `seq` significantly less useful due to the ways it would try to get around having to actually look underneath it. It would also be an ungodly evaluation model with atrocious performance. My original point was (I supposed I could have been more clear) that a standard lazy evaluation model allows you to implement seq on inhabited non-function types. Not that you could do it in every lazy-like evaluation model under the sun.
I'll [quote you again](https://www.reddit.com/r/haskell/comments/91ycca/names_dont_matter/e329sz0): &gt; Names like Mappable and Combinable instantaneously give most people some intuition about what those typeclasses do.
Oh, cool, [another fork](https://github.com/parsonsmatt/vim2hs)! Mine just folded in most of the code that other forks incorporated, so yours is likely a better place to go. If you want I can add a note to my README to forward to your fork.
Awesome! Welcome to Haskell :D One note about Real World Haskell -- it is quite out of date! The general concepts and ideas are good, but in the 10 years since it was published, almost all of the library code won't work anymore. Feel free to join the [FPChat](http://fpchat-invite.herokuapp.com/) slack channel, there are a lot of that are happy to help in the #haskell and #haskell-beginners channels :)
If you'd like to do that, I think that would be cool.
Is there a type \`data D f a = a :&lt; (D f (f a))\`?
Is it [Breadth-First Numbering: Lessons from a Small Exercise in Algorithm Design by Chris Okasaki](https://www.cs.tufts.edu/~nr/cs257/archive/chris-okasaki/breadth-first.pdf)?
Yea Project Euler was one of the most fun things I did with Haskell early on. It got a little more tough with some of the more complicated algorithms than it would have been with other languages, but I was definitely more confident in the result. i.e. It took longer to get an initial solution, but it was more likely to be more correct with less testing. I still occasionally solve some Project Euler in Haskell to pass the time.
Unfortunately not. I also found this one when googling for labelling trees, but it is not what I am looking for. What I vaguely recall that sets the paper I am after apart is that, in the introduction, the author referred to some prior art with critique along the lines of *"the treatment is unsatisfactory, and we can do better"*, and that the overall tone was more conversational, kind of old school, as in many Haskell-themed papers from the 90-s, and not very technical.
Just a note from personal experiences: For a while I, like you, thought Project Euler would be the best way to learn Haskell, for most of the same reasons. However, in the several dozen PE problems that I did, no problem ever really forced me to do anything that I would have to do the moment I moved on to a larger piece of software (in my case it was a simple web API that had to query a database and return a JSON response). The real-world project forces you to do things like define custom data types, consider module structure, deal with IO, etc. Point being, Project Euler is a great way to get your feet wet with Haskell. But once you think you are comfortable, please do move on. You will rapidly learn quite a lot more when you write a larger program.
Yeah, Project Euler is fun if you want to do some math/algorithmic puzzles, but most probably won't make you better at software engineering.
That's pretty rockin'. Nice work! This is definitely a game changer. It also means I'll probably be able to use Intero on GHC. Living the dream! 
Perhaps it was [Reasoning About Effects: Seeing the Wood Through the Trees by Graham Hutton and Diana Fulger](http://www.cs.nott.ac.uk/~pszgmh/effects.pdf)?
Lazy IO isn't an issue with monadic IO - your `packCString` will happen before `TessDeleteExp`, because that ordering of effects is what monads deliver for us.
Surely with optimization turned on GHC will notice that the argument is always unused and therefore not bother to generate or pass a thunk for it?
I've found Advent of code to be a bit more varied and less math centered than project Euler when I want to try out some language.
&gt; Haskell is the best language I've used for it, bar none. This was the exact reaction I had my first time using Haskell, after getting frustrated with a project of mine that I had been trying to do in Python, and then Perl. Fast forward 12 years, I still think Haskell is the best language, and for almost all programming tasks. I feel a tremendous debt of gratitude to the guy who heard about the problems I was having and suggested I use Haskell, which was the first time I had heard about it.
Thanks for the fork. I mainly use Emacs, but in my vim config I had been using https://github.com/neovimhaskell/haskell-vim, which works in neovim and vim 8. Is there potential for a merge with that project? I mean, I'll give vim2hs a try. Performance is mentioned in the README and I wonder if vim8/neovim's built-in async execution can help smooth that out.
Did you forgot to include typeof.hs in the repo? I see only `typeOf.cabal` and `Process.hs`.
Narrator's voice : it was `traverse` all along. 
Hi, i’m an Haskell beginner and I would like to understand the coolness of what did. 1- I thought that ghc was already inside ghci? That ghci was a wrapper around ghc? I guess it is not. 2- Why would I run Ghci inside Ghci unless it is for academic purpose? 3- What does it allow you to do to load GHC in GHCI?
This is for working on the compiler itself, which is a Haskell project.
&gt;After D4904 was merged, I realized that -fobject-code should have been included in settings.ghci, and indeed it was in Csongor's version. Without it, you get errors like the following: [ 16 of 493] Compiling State ( compiler/utils/State.hs, interpreted ) Error: bytecode compiler can't handle unboxed tuples and sums. Possibly due to foreign import/export decls in source. Workaround: use -fobject-code, or compile this module to .o separately. Might it be possible to just add this to the files that need it, not all of them? Maybe `{-# OPTIONS_GHC -fobject-code #-}` behind some CPP or something? Dunno if that's possible. &gt; Are nightly builds of GHC downloadable from somewhere? If there are, then the following workflow could work for newcomers to GHC development: &gt; * Download nightly build of GHC &gt; * Load GHC into GHCi in \~10 minutes or less. I think that this would be far more appealing than the current recommended approach for newcomers, which is something like a 30 or 40 minute build process on my machine. This leads in to my other question somewhat: Why do you have to use ghc-stage2 for the parent GHCi? That rules out working on cross compilers, which rules out me using this for WebAssembly development. Why couldn't you use the host's GHC (I think this is called stage 0?) for the parent GHCi? Then users would just use the GHC on their system; no need to wait for stage 2 to build.
cool, so how people were already doing dev on the compiler before knowing how to load it in ghci?
I'm guessing they built the project and tested the binary.
Lots of patience and running make/tests - this should be way faster
Not sure that this is guaranteed, especially with more complicated cases.
I'd put this differently. Project Euler will get you doing different things in Have than most software engineering jobs, yes, so if you are learning Haskell as just a job skills, you will want to do other things. But it's certainly not just "getting your feet wet". It is, in its own way, far far more challenging and involved a task than Advent of Code.
I challenge you to create a reasonable benchmark in which this effect is measurable, let alone significant.
Just started skimming and noticed something incorrect right away. The const keyword in JS has nothing to do with value immutability, its assignment immutability. Meaning you can assign an object to a const and change properties on that object and you are writing perfectly valid JS. It is only if you try to assign another value entirely to the variable that it is a problem. 
I also want to do that, but I struggle xay too hard to find how to do X or Y in Haskell each time How do you know what to do? 
I'm pretty sure I mentioned that actually.
&gt; An immutable value is some data, which can be read, used to construct some other data, but not directly amended. Similarly, an immutable variable is one whose value can be observed but not actually changed. In a purely functional program, all variables and values are immutable. JavaScript supports immutable variables using the const notation. This is implying that const notation uses value immutability.
It's not that well phrased, I agree. 
I like the direction so far, I'm gonna do a closer read later. Would be cool if you do an Either or Maybe implementation in JS to better clarify because they can be confusing at first even though they are conceptually not complicated.
Thank's I added that to my TODO list :)
I think the host GHC could be used but additional steps have to be taken to make sure certain versions of packages are installed in the package database. Csongor's original script worked like that. 
Was it [Type-Safe Observable Sharing in Haskell](http://www.ittc.ku.edu/~andygill/papers/reifyGraph.pdf)?
i think i did, there's a binary available in the releases though until i fix it https://github.com/goolord/typeOf/releases
I sometimes get flack for this for being too meta, but I consider the "value" of a JS object to be its pointer. From my perspective the author is absolutely correct.
haskell-ide-engine is cool, but I'm reluctant to put anything in vim2hs that is likely to break or bog down performance. I think I'll try to contribute to haskell-vim, however.
Yeah, its correct that the variable contains a reference to the value rather than the value itself so in that sense even by changing a property of the object as long as the reference doesn't change it maintains value immutability. That trips people up a lot though and I don't think most JS developers think of it that way except when comparing objects, it's also kind of a bad implementation IMO. They provide Object.freeze as well but it only goes one level deep...
These people who say that names do not matter have never maintained a medium/large size program.
Really like obelisk so far :) What editor tooling so people develop with? Also, I felt the caching in Linux (Ubuntu) was a bit hit and miss, and when running on run, it took a loooooong time to compile it, because it pulled in a lot of external dependencies. Would be interested in hearing from other people trying it out :)
Thanks for the reassurance. If I understand correctly, since packCString returns a Strict Bytestring, the memory is immediately copied into the Haskell universe rather than being copied on demand. If packCString returned a Lazy Bytestring, there would be a chance for a use-after-free
whatever you reckon, kid
A lazy bytestring is a chain of strict bytestrings, allowing the tail of a chain to be a thunk. The blocks are not an IO type, so evaluating a lazy bytestring can't perform IO operations such as reading memory. You could contort things such that the bytestring IO isn't performed until later evaluation requires it, but this would show in the types, which would wind up something like `IO (IO ByteString)`.
This is exactly my experience. I did plenty of Project Euler problems in Haskell but I don't feel like I've actually learned that much. I still struggle with the concepts you've mentioned, and monads still escape me. A web API does seem like a good starter project to work with.
HNix is just another Nix interpreter. It enables writing Nix in Haskell no better than regular Nix enables writing Nix in C++ (which you can do, btw).
Thanks Chris! I haven't tried it with intero yet - definitely worth a shot.
As far as I can tell, this is indeed the typical workflow. It would be nice to be able to run tests against the GHC-in-GHCi. I have some thoughts on how to be able to do this in a fairly general way, so hopefully this will be possible at some point in the future
&gt; Maybe {-# OPTIONS_GHC -fobject-code #-} behind some CPP or something? I don't think that works. It used to cause panics - https://ghc.haskell.org/trac/ghc/ticket/10965 - based on my recent testing it doesn't seem to do anything. The tricky thing is that all of the dependencies of modules that use UnboxedTuples also need to be compiled with object-code. So if the pragma worked, then you'd need to annotate all of the dependency modules as well. Maybe worth a shot, but not sure if a patch like that would be merged. Could be a pain to maintain pragmas like that. I've opened [trac #15454](https://ghc.haskell.org/trac/ghc/ticket/15454) about automatically using object code when necessary, and byte code otherwise. I imagine that even with some automagic like that, some code changes to GHC will be needed to move unboxed tuples code, to minimize the amount of code that must use object code.
&gt; The tricky thing is that all of the dependencies of modules that use UnboxedTuples also need to be compiled with object-code Ah, didn't know that. Yea then pragmas in files definitely won't work. Any comment on the stage2 requirement and the RTS?
Oh, interesting! [The instructions in mpickering's email used `ghc-stage2`](https://mail.haskell.org/pipermail/ghc-devs/2018-May/015810.html) - and I assume there's a good reason for it. However, maybe it is just a bit easier setup than using it with the host's GHC, since it already has the right packages. Would be curious to know so that I can correct my post on this point, if necessary. I think it would be great to get this working with host GHC, and scripted, because I think it would really help lower the barrier-to-entry for contribution if initial setup was &lt; 10 minutes. I mentioned briefly in the post the possibility of using nightly GHC builds. One thing that occurred to me is that it might also be nice to allow downloading the object files. So, the idea is that you could download the object files and nightly ghc binary. This could allow you to get cranking on modifying GHC in less than a minute if you have fast internet.
`iover itraverse`, that is.
Great! If you join forces and a joint project emerges, I'd appreciate an announcement ;).
Related to https://elvishjerricco.github.io/2017/03/23/applicative-sorting.html ?
Hm, it didn't work so well for me, some feedback: * The performance is pretty bad while typing. Maybe only start searching until .2 seconds after I've stopped typing, or maybe just general performance improvements? * I searched for "Cryptography" but didn't get cryptonite as a result. On hackage it's the top result, which I think is the correct one. I can fix this by switching to fuzzy search, in which case it's the 14th result. If I search the string "cryptonite" on fuzzy search, it's the 15th result. * I think adding the downloads count and last uploaded way is worthwhile. That's really helpful to me in hackage search, so I can get at least a rough idea of how well maintained the package is * Clicking on the package in the list to reveal more information wasn't super intuitive, but it was very helpful. Very nice to have more information without opening a new page!
This does not apply to hacking on the RTS. It would be quite an amazing feat to be able to do incremental reload with a modified RTS. It is using the RTS that's already built into your `inplace/bin/ghc-stage2` RE the stage2 requirement, I'm not sure. It would be great to be able to use the host's GHC, because this could allow newcomers to GHC development to be able to start hacking much quicker. I have a nearby comment on the topic within this same thread - https://www.reddit.com/r/haskell/comments/92szf7/ghc_inside_ghci/e39ey3m/
Nope, I have never seen this one before in my life.
Could you maybe be a little more specific in what you are actually trying to do? In principle, something like the following low-tech solution isn't terribly hard after all: {-# LANGUAGE DeriveTraversable #-} {-# LANGUAGE DeriveFoldable #-} {-# LANGUAGE DeriveFunctor #-} module Bla where import Control.Monad.Trans.State.Strict import Data.Traversable data BTree a = Tip | Node (BTree a) a (BTree a) deriving (Eq, Show, Functor, Foldable, Traversable) addLabels :: BTree a -&gt; BTree (Int, a) addLabels btree = flip evalState [1 :: Int ..] . for btree $ \a -&gt; do (n:ns) &lt;- get put ns pure (n, a) testTree = Node (Node Tip "x" (Node Tip "y" Tip)) "z" Tip Then we have *Bla&gt; addLabels testTree Node (Node Tip (1,"x") (Node Tip (2,"y") Tip)) (3,"z") Tip There are a lot of papers dealing with various kinds of tree annotations, but all of them are more interesting than just "threading a source of unique labels through computations involving recursive data structures".
Shameless plug for my preferred setup https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8
I just discovered this doing some UI this past week (after using JS for years) and I'm so disappointed they continue doing types so backwards.
Check out typescript. They have mapped types which allows you to do something like this: type DeepReadonlyObject&lt;T&gt; = { readonly [P in keyof T]: DeepReadonly&lt;T[P]&gt; } This recursively marks every property as readonly. There are some edge cases to consider though, but gets you close. There is also immutable.js to consider which uses structural sharing.
yeah Typescript is good and fixes many issues. Many of those I think should be in the core language.
Also Elm is awesome too!!!
I would be very much interested in this. As someone who works in JS most of the time and tried dabbling into Haskell quite a while but quickly got turned off after spending god knows how long for cabal to install something I would like to give it a try once again. 
Didn't intero require a stack project? Maybe I'm mis-remembering something, but I think that was what put me off from using intero much, as it kept installing stack and differerent compilers all the time (and locking up emacs while doing so). I might also have been holding it wrong altogether.
I doubt that there are specific restrictions for cross compilation. You would of likely need a recent build -&gt; build compiler. But that one should be fine loading a the source for the cross compiler. /u/ElvishJerricco, ping me on irc, and maybe we can give this a try.
I'm not the author of this proposal, so I can't tell you much about why this solution is not good. Neither I'm an expert in GHC optimizations. If you want to get more detailed feedback about your solution to this problem, it's better to comment directly on GitHub under comments for this proposal.
same
&gt; The performance is pretty bad while typing I wonder how much of this is due to the fact that this is a GHCJS app.
I read your fix but... fundamentally, I don't think JS `const` is a great way to introduce the Haskell sense of immutability to JS programmers. A lot of JS programmers don't really understand JS `const` in the first place. Those that do understand it, understand it in a very JS context that is unhelpful in learning Haskell. For JS nerds, `const` maintains a distinction between primitive and object values which is not important in a Haskell context. I think you'd be much better off explaining immutability from first principles, with an aside noting the similarities and differences to the JS `const` keyword.
&gt; Clicking on the package in the list to reveal more information wasn't super intuitive, but it was very helpful. Very nice to have more information without opening a new page! It would be even better if this information is rendered correctly. Description field in `.cabal` files (where this information comes from) has the special haddock syntax, so it should be taken into consideration. For the moment it looks a bit messy.
Would be really nice if Hoogle could have similar features. For example, you type query like this: author:"Some Author Name" categories:CSV,JSON And it searches through package metadata as well.
I was thinking about something less scary but a bit more verbose, like \tree -&gt; evalState (traverse (\e -&gt; get &gt;&gt;= \(x:xs) -&gt; (e,x) &lt;$ put xs) tree) but this is certainly more golfy! 
This is a valid concern, but from what the JS test readers said, they don't have an issue understanding that part. 
Gloss is really simple and easy to use. There's also a nice pong tutorial on the internet that uses gloss if you want to make something a little more advanced than just pixels and dots :) http://andrew.gibiansky.com/blog/haskell/haskell-gloss/
Just out of curiosity, what channel are you playing the sound on? I also had an issue with sdl2 mixer where it garbled the sound on the Music channel so I off loaded the music on it's own channel, so the audio worked there. Although, I never quite figured out why the music was so garbled on the main Music channel. I think Haskell's GC shouldn't touch the audio stuff because it should be in the C static memory of a Haskell program. Is it possible you could share the code of the project? 
Hmm. Your test readers probably understand JS better than the general audience - after all, they volunteered to be test readers for a JS/Haskell book!
Yeah there might be some selection bias going on.
GHCJS is fast. You can write responsive apps without trouble. Just like Haskell compiled to binary, don't let its file size fool you into thinking it's expensive. I think it's more likely that the computation taking place is slow. If it's doing a text search through 10,000 elements in a list, then that can be a cause. I doubt it has some kind of full text search index.
Your book, but I'd really consider introducing immutability from first principles then comparing/contrasting with JS's capabilities. (Both `const` and things like `Object.freeze`).
Yeah I'll definitely consider rewriting that section to do exactly that.
GHCJS is generally like 20x slower than native in my experience. Don't get me wrong, it's remarkably fast for what it has to work with, but that doesn't make it fast. That said, it's more than fast enough for most web apps needs, and I agree with your assessment about this particular app's likely performance bottlenecks.
This scares me a lot more. I guess because I know what `unsafe` in the above is about.
You mean because of the refutable pattern matching?
You could add a chapter on constructs often used _by_ javascript programmers : For instance I write this code block a lot - like *very* often : const items = text .split(/[\r\n]+/g) // split lines .map(l=&gt;l.trim()) // remove trailing and end spaces .filter(l=&gt;l) // remove blanks .map(l=&gt;JSON.parse(l)) // parse .filter(o=&gt;o.balance&gt;0) // for instance ; This construct is the standard one for d3js for instance and feels familiar to a javascript programmer parsing json items. Of course this fits easily in explaining `.` in haskell, and IMHO gives very rewarding examples of composition. (2cents - and I like the idea of your book in general)
It's a lot more for me to digest. I know each of `over`, `unsafePartsOf`, `traverse`, `zip` - and I see what it is supposed to mean at a glance (Didn't type-check it to make sure the composition is exactly right).
The main source of performance penalty is actually network namespaced, those could be reused and sandboxing would be cheap. Just need to be done :)
Jeremy Gibbons I see https://www.cs.ox.ac.uk/jeremy.gibbons/publications/uitbaf.pdf and the "root" http://www.cs.nott.ac.uk/~pszgmh/effects.pdf
Thanks for the feedback! It's very helpful. &gt; The performance is pretty bad while typing. There are several tradeoffs here. Decreasing the number of results you show at once speeds things up but is less useful. Waiting longer after you type a key reduces CPU load and unnecessary calculations but makes it feel less interactive. It also won't change the perception of slowness if you type slowly. I tried to find a balance of these concerns when I was testing, but I was developing and testing on a pretty beefy machine. Here's a video showing the search speed for me: http://hexplore.mightybyte.net/search-speed.mov Maybe I need to test on a wider variety of hardware. &gt; I searched for "Cryptography" but didn't get cryptonite as a result. In my mind, this is the intended behavior. It's because it is using an exact substring search and it defaults to case sensitive (the button in the top right corner). I chose to default to case sensitive rather than insensitive because that restricts searches faster (another tradeoff). &gt; If I search the string "cryptonite" on fuzzy search, it's the 15th result. I put zero effort into ordering the search results. It's just alphabetical by package name. I added this to the backlog (https://gitlab.com/mightybyte/hexplore/issues). &gt; I think adding the downloads count and last uploaded way is worthwhile. That's really helpful to me in hackage search, so I can get at least a rough idea of how well maintained the package is Good point. I created [an issue for this as well](https://gitlab.com/mightybyte/hexplore/issues/3). However, this is yet another tradeoff. Adding this increases the amount of data I have to send and slows things down.
&gt;If the aim is to increase the adoption of Haskell, It isn't.
&gt;As easy as it is to parody Java naming conventions, they follow a formal structure and are comprehensible by English speakers used to other programming languages. They really aren't, speaking from experience. What you are familiar with is not what everyone is familiar with. &gt;one could objectively determine which of Reducible vs Monoid were easier to understand to the population of programmers. /r/programming is full of novices
Semigroup isn't the 'jargon of category theory'.
&gt;Every mainstream programming language – Java, Python, C#, Swift – has a map function. Recently, perhaps. &gt;So amongst programmers, Mappable is far more comprehensible than Functor. Calling any typeclass 'X-able' is awful naming. &gt;In the general population, Combinable, Traversable etc are still comprehensible, but Reducible and Mappable probably are not. Then again, the general population also won’t understand what Boolean-expression means, so the point is moot. They also don't care and aren't relevant. &gt;FWIW I do think map is a terrible name, both for applying things to values in containers, and for dictionary-like data-structures. It’s up there with “buses” and “ports” in the history of terrible computing nomenclature. However, as I mentioned above, it’s much more feasible to work with the world as-is than try to force the rest of the world to change. Map comes from, you guessed it, mathematics. A map is another name for a function. It's a good name, too.
The Emacs plugin uses stack to install and execute the Haskell package, but the Haskell package is independent of the build tool.
&gt; Maybe only start searching until .2 seconds after I've stopped typing I have a [nice little abstraction](https://pastebin.com/nVaYkVVb) in JS I use for this (and all sorts of other things, like download/upload liveness). It works off of a soccer/football juggling metaphor where you kick a ball up repeatedly until you miss, then it falls to the ground. So here, onCreate is nothing, onFall would send an HTTP request, timeout would be ~1s, and any time a character is entered you'd call `juggle.kick()`. As long as you keep kicking the ball (typing), nothing happens, when you let it fall (stop typing for 1s), an HTTP request is sent.
I still have no idea what cabal, stack or this nix thing is. Every time I try start to learn a bit more of haskell I have to randomly type in commands until something works. 
 mostOccurringElement' = head . head . sortBy (comparing $ negate . length) . group
This does not work as `group` only group subsequent items.
I commented in /r/rust too but again great article, and demonstration of writing traits to make code more readable.
Not sure this will work. `group [1,1,1,2,3,1,4]` gives `[[1,1,1],[2],[3],[1],[4]]`; it requires a sorted list for that to really work. I'd probably just insert (v,1) into a hashmap (from unordered-containers) for every element, combining with (+), and then use maximum over that hashmap
Here is a solution: mostOccurringElement = fst . maximumBy (compare `on` snd) . bla where bla [] = [] bla (x : xs) = (x, 1) : [if x == y then (y, c+1) else (y, c) | (y, c) &lt;- bla xs] However, note that this is a `O(n^2)` algorithm. And I am not sure you can do much better with just `Eq`. If you only want to deal with `Complex`, just unpack it into a tuple, then use the `Ord` instance of the pair, and repack afterwards. Another option is to use a hashtable. But this then requires `Hashable`. 
lol data Discrete a = Discrete { getDiscrete :: a } deriving Eq instance Eq a =&gt; Ord (Discrete a) where { (&lt;=) = (==) } mostOccurringElement' :: Eq a =&gt; [a] -&gt; a mostOccurringElement' = getDiscrete . mostOccurringElement . map Discrete
I wrote a frequency function that works on instances of `Eq` and then find the most occurring element from that list. frequencies :: Eq a =&gt; [a] -&gt; [(a, Integer)] frequencies [x] = [(x, 1)] frequencies (x:xs) = let (xfreq, yfreqs) = partition (\(e, f) -&gt; e == x) xs freq = case xfreq of [] -&gt; 0 ((_, f):_) -&gt; f in (x, freq + 1) : yfreqs mostOccurringElement' :: Eq a =&gt; [a] -&gt; a mostOccurringElement' list = fst $ head $ sortBy (\(_, f1) (_, f2) -&gt; flip compare f1 f2) $ frequencies list I'm not as good at knowing all of the functions available as some other people, so this might not be the most concise solution...
This might work with a bubblesort, though! (I believe bubblesort is one of the few sorting algorithms which actually works with just a partial order, as opposed to a total order.)
I'm not making requests for this stuff. It's all done in your browser to avoid network latency and needing to have more substantial server infrastructure.
Clever hack there!
Hey, one of the editors at Manning here. Sorry to hear you've had this experience with our MEAP program. We definitely feel it's better to develop our books out in the open. Sometimes they take longer than we'd hope though, I agree. Trust me when we say nobody wants our books finished sooner more than us :) I did want to note, if at any point you decide that you're unhappy with the pace or quality of a MEAP, we do let you refund or exchange at any point. More info on the FAQ here if you're interested: [https://www.manning.com/meap-program](https://www.manning.com/meap-program) I'd also like to point out that on [liveBook](https://livebook.manning.com/) you can now read any of our books in web form, for 5 minutes a day, every day. This is a great way to read a book in MEAP a little at a time, totally for free. This is a great way to get a feel for the pace we're releasing content before risking any money. You can read this book here: (https://livebook.manning.com/#!/book/haskell-in-depth)[https://livebook.manning.com/#!/book/haskell-in-depth]
When I call `waitToSetLock fd (WriteLock, AbsoluteSeek, 0, 0)` from the unix package, my process waits, but also uses 300% CPU and all my fans start spinning up. If I move the `waitToSetLock` call into a forked thread (forkIO and forkOS), the process sits nicely at ~0.3% CPU. What could be causing this? 
Here's mine mostOccuringElement l = fst $ maximumBy (comparing snd) $ map (\x -&gt; (x, length (filter (== x) l))) l
 import Data.List (group, sort, maximumBy) import Data.Ord (comparing) mostOccurringElement' :: Ord a =&gt; [a] -&gt; a mostOccurringElement' = head . maximumBy (comparing length) . group . sort 
Read the post before you reply please
 λ import Test.QuickCheck λ quickCheck $ \i a -&gt; (mostOccurringElement (i:a) :: Int) == mostOccurringElement' (i:a) *** Failed! Falsifiable (after 5 tests and 4 shrinks): 0 [1]
Jesus! with only knowing about map and fold you can give conferences about how little the names of map and fold matter, an therefore, by an obvious extension, names do not matter in programming at all. Take for example a function: factorial :: the signature Int -&gt; Int say it all, why we need to name it "factorial"?. let's call it cucumber. by the signature it is obvious that: cucumber :: Int -&gt; Int Is factorial anyway!!!!! being invited to Haskell conferences is is easy if you are cool and you say these crazy things that haskellers like to hear !!!
I work on a code base that is in the order of 100,000s in terms of LOC (quickly approaching 1M) and I haven't found a solution that's suitably quick or doesn't just slow me down. Most of the team are using Sublime with an internal syntax highlighter that has better goto-defs than the standard Sublime one. The rest of the team are doing their own thing with a mixture of Notepad++ and vim.
&gt; but each time it builds in the background, it creates some temporary directory that nix uses, and it never cleans up after itself, so I lose a couple of gigs of space per hour until I rerun nix-collect-garbage. If nix-collect-garbage fixes it, it's not a temporary directory, it's a derivation or source file. This seems very strange to me. Do you have any more info on what paths in the Nix store are being created and why? Maybe you could diff the output of `ls /nix/store` before and after a `stack build`?
You might as well define a pseudo-ordering (i.e. isomorphic to tuple where one dimension comes first). mostOccuringElement = ... maximumBy (compare `on` fakeOrder) ... where fakeOrder (i :+ r) = (i, r)
Maybe it builds the derivation for the source code itself, and the editor is creating backup files locally, which makes Nix to rebuild everything every time the file is changed?
Good point. It doesn't seem like it should create so much extra stuff under /nix/store, since I haven't changed any dependencies in a while. Maybe stack's nix integration creates its own package store? But then, nix-collect-garbage wouldn't fix it, either. I'll have to create a script that does something like what you suggest.
The source directory was my first thought as well, but not because of backup files. Putting your source dir in the Nix store will include another copy of your .stack-work dir, which will cause quite a bit of bloat. But I can't think of a reason for stack's Nix integration to put your source dir into the Nix store.
Can you share your `shell.nix`?
https://github.com/sfultong/stand-in-language/blob/14-intermediate-natural-numbers/shell.nix Note that running nix-shell directly and repeatedly doesn't use up any extra disk space.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [sfultong/stand-in-language/.../**shell.nix** (14-intermediate-natural-numbers → f7aad7d)](https://github.com/sfultong/stand-in-language/blob/f7aad7d86682e5b4a4cdb63dc371980e932c5156/shell.nix) ---- 
This seems like a great usecase for [discrimination](https://hackage.haskell.org/package/discrimination-0.3).
Yep. It looks like it copies the entire project directory into the nix store each time.
That's exactly what it seems to be doing.
Hm. That's quite odd. Are you using a custom shell file for stack's nix integration?
Thanks, using it right now.
According to the primops documentation, `Char#` is 31-bit and `Int#` is potentially as low as 30-bit. How is correctness preserved with `chr# :: Int# -&gt; Char#` and `ord# :: Char# -&gt; Int#` when `Int#` is only 30 bits?
Haskell 98 specifies `Int` (vs `Int#`) is at least 30 bits - this is the language definition and not what GHC actually does. GHC uses 32 or 64 bits depending on the architecture for both `Int#` as well as the boxed `Int`. In the .Prim module documentation there is mention "it can also be explicitly set to a smaller number, e.g., 31 bits, to allow the possibility of using tag bits. Currently GHC itself has only 32-bit and 64-bit variants". So if someone were ever to make that design decision they'd need to think about the new world where `Int#` is a smaller domain than `Char#`.
Yes. I posted it farther down. I'm not using haskell.lib.buildStackProject, so that might be causing some trouble.
Pretty cool. I'm curious about the performance difference (if any!) between the two implementations.
That filter makes it O(n\^2). Using `Map` is faster: mostOccurringElement = fst . maximumBy (comparing snd) . toList . foldr (flip (insertWith (+)) 1) empty
Thanks for your response. I'm looking at [`primops.txt.pp`](https://github.com/ghc/ghc/blob/master/compiler/prelude/primops.txt.pp), and it says: &gt; Currently GHC itself has only 32-bit and 64-bit variants, but 30 or 31-bit code can be exported as an external core file for use in other back ends. In the case where someone is exporting 30 or 31-bit code, it seems like the `primops.txt.pp` handles the situation inconsistently. For example, it takes the time to define `INT32` and `WORD32` to use either `Int#` and `Word#` on `Int32#` and `Word32#` based on whether `Int#` and `Word#` can hold 32 bits or not. This indicates that handling the case when it's smaller is important enough to actually be dealt with correctly. This is actually used when writing to an `Addr#` or a `ByteArray#`, but it doesn't seem to be used much elsewhere when it seems to be strictly required. For example, `decodeDouble_2Int#`'s documentation specifically states that `The next two are the high and low 32 bits of the mantissa`, yet it`s perfectly happy to put them into 30-bit `Word#`s rather than using `WORD32`. Similarly, `pdep32#` is `Word# -&gt; Word# -&gt; Word#` rather than `WORD32 -&gt; WORD32 -&gt; WORD32`. Is this inconsistency a bug that should be reported or fixed? Also, should an additional definition of INTCHAR be used based on whether `WORD_SIZE_IN_BITS &lt; 31`?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**primops.txt.pp** (master → 9d388eb)](https://github.com/ghc/ghc/blob/9d388eb83e797fd28e14868009c4786f3f1a8aa6/compiler/prelude/primops.txt.pp) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e3b45ta.)
Little helper here (O(n)): mostOccurs :: [(a, Int)] -&gt; a mostOccurs = head . sortBy (flip $ comparing snd) O(n^(2)) is the best you can get with just Eq. inc :: Eq a =&gt; a -&gt; [(a, Int)] -&gt; [(a, Int)] inc x [] = [(x, 1)] inc x ((i, n) : ins) | x == i = (x, succ n) : ins | otherwise = (i, n) : inc x ins mostOccurringElement :: Eq a =&gt; [a] -&gt; a mostOccurringElement = mostOccurs . foldr inc [] Using HashMap should give you O(n lg n) inc :: Hashable a =&gt; a -&gt; HashMap a Int -&gt; HashMap a Int inc x = insertWith (+) x 1 mostOccurringElement :: Hashable a =&gt; [a] -&gt; a mostOccurringElement = mostOccurs . toList . foldr inc empty Possible micro-optimization by pre-processing with `(head &amp;&amp;&amp; length) . group` (O(n)) in both cases, but not enough to bring down the complexity.
Would require a `Grouping` instance instead of / in addition to the `Eq` instance.
That requires `Ord`.
My IDE is Emacs + direnv-mode + nix-shell + direnv with `use nix` in my .envrc`. This means whenever I change file/buffer in Emacs, direnv is used to load all the nix-shell environment variables into Emacs's set of environment variables. Then I can just use tools like `projectile-compile` with `cabal new-build` to build my project and make use of emacs `compilation-mode`.
I *think* you get two kind errors there. `FingerTree' :: * -&gt; (* -&gt; *) -&gt; *` in the absence of PolyKinds. `Fix :: (* -&gt; *) -&gt; *` in the absence of PolyKinds So, `Fix Fingertree'` isn't going to work, and neither is `Fix (Fingertree' a)` at not with normal kinds. And, looking at it a little longer, I don't think PolyKinds changes anything.
I don't understand why you need that second definition for your product type. Can't you just assign the first definition an infixr and have `a :&amp;: b :&amp;: c :&amp;: d`?
Oh crap. I guess you could get away with just Eq and Hashable on a HashMap but I see how that's not preferable either.
You're god damn right. I changed type definition and blog post content. Thank you very much! 
I'm lost as heck. I want to bundle up a bunch of constraints (as a type-level list using `ConstraintKinds` into an existential and later recover specific ones; say, automatically provide a `Show` instance for `Exists cs` where `cs` contains `Show`. I remember enough tricks from hacking in Idris and type-level Haskell that it's no problem to build a proof of constraint membership in a list, nor to find these proofs or use them to extract constraints. The question is, what the heck do I do next? I'm really going in circles here; maybe I just need more coffee. {-# LANGUAGE GADTs, DataKinds, KindSignatures, ScopedTypeVariables #-} {-# LANGUAGE FlexibleInstances, MultiParamTypeClasses #-} {-# LANGUAGE ConstraintKinds, RankNTypes #-} {-# LANGUAGE TypeFamilies, TypeOperators, FlexibleContexts #-} {-# LANGUAGE TypeInType, StandaloneDeriving #-} import Data.Kind (Type) import GHC.Exts (Constraint) -- a set of constraints type family ManyConstraints (cs :: [Type -&gt; Constraint]) (a :: Type) = (r :: Constraint) where ManyConstraints '[] _ = () ManyConstraints (c ': cs) a = (c a, ManyConstraints cs a) -- an existential known to meet a set of constraints data Exists :: [Type -&gt; Constraint] -&gt; Type where Exists :: (ManyConstraints cs a) =&gt; a -&gt; Exists cs -- a proof that a set contains a given constraint data HasConstraint :: [Type -&gt; Constraint] -&gt; (Type -&gt; Constraint) -&gt; Type where HasConstraintHere :: HasConstraint (c ': cs) c HasConstraintThere :: HasConstraint cs c -&gt; HasConstraint (a ': cs) c -- pull a constraint out of a set, given a proof type family GetConstraint (cs :: [Type -&gt; Constraint]) (prf :: HasConstraint cs c) (a :: Type) = (r :: Constraint) where GetConstraint (c ': cs) HasConstraintHere a = c a GetConstraint (_ ': cs) (HasConstraintThere prf') a = GetConstraint cs prf' a -- this works, but doesn't appear to get me much! type family HasSome (cs :: [Type -&gt; Constraint]) (c :: Type -&gt; Constraint) = (r :: HasConstraint cs c) where HasSome (c ': cs) c = HasConstraintHere HasSome (a ': cs) c = HasConstraintThere (HasSome cs c) -- I'd rather, I think, have a class class HasSomeC (cs :: [Type -&gt; Constraint]) (c :: Type -&gt; Constraint) where -- maybe something like this? (doesn't compile) -- withConstraint :: ManyConstraints cs a =&gt; (forall r . (c a =&gt; a -&gt; r) -&gt; a -&gt; r) -- I want to be able to write (or derive!) one of the following! {- instance HasSomeC cs Show =&gt; Show (Exists cs) where showsPrec n (Exists a) = ... -} 
The intero executable behaves just like ghci (it's a fork of ghci). The emacs integration portion of intero uses stack + intero. It's really tricky to do things like reliably installing it and target / component selection without integrating with a particular build tool. Supporting all the many varieties of setups out there would be a lot of work.
so, how does this approach compare to this: https://github.com/travisbhartwell/nix-emacs ?
I came up with a very similar solution. A bit heavier weight for the task but it supports decoding as well by pushing the “key” into the type as well. https://hackage.haskell.org/package/composable-associations-aeson-0.1.0.0/docs/Data-ComposableAssociation-Aeson.html (I even used “User” as my example too!)
That sounds intriguing, but I'm not sure I could wrestle spacemacs into doing that.
I have yet to get it working, but i have high hopes for \`haskell-ide-engine\` and \`hie-nix\` with \`spacemacs\` and \`lsp-mode\`. This would probably address your stack-bloat in a roundabout way.
Hm. I don't see anything wrong here... Does running nix-shell directly and repeatedly use more disk space if you make a change to your source dir between invocations?
What is a "singleton on length-indexed list"? If you're talking about a library for fixed-length vectors, I usually use *vector-sized* which uses TypeNats, and *type-combinators* which use an inductive length type.
This might not solve your actual problem, but if you want to use it with `Complex`, you can just define a newtype wrapper with a dummy instance: newtype ComplexOrd a = CO { getCO :: Complex a } instance Eq a =&gt; Eq (ComplexOrd a) where x == y = compare x y == EQ instance Ord a =&gt; Ord (ComplexOrd a) where compare = comparing (realPart.getCO) &lt;&gt; comparing (imagPart.getCO) and then you can do: getCO . mostOccurringElement . map CO
Thank you for the note on jargon! This is a great way to add compatibility to certain types. By the way, do I have to use a record for adding a custom `Ord` instance? I'm thinking about using something like `newtype ComplexOrd a = CO Complex a`.
This looks cool! I never knew how to implement O(n) sort in Haskell without using array.
You don't need record syntax, but it makes the one-liner easier because you can write `getCO` instead of `(\(CO x) -&gt; x)` or using a let-binding. btw your syntax for the newtype is slightly off, you'd want to say `newtype ComplexOrd a = CO (Complex a)`
I guess it's a sorting stability problem?
Sure, but it comes with an instance of `Grouping` for `Complex`, so it actually solves the original usecase ;)
I doubt it. The original order of the list isn't helpful so the stability of the sort shouldn't be either.
You're absolutely right. Typed that on my phone.
But then since I didn't specify which element to return when there are multiple most occurring ones, I guess returning either 0 or 1 for [0, 1] works?
This is exactly what I am looking for!
That’s perfectly fine. It just didn’t work for my usecase; and didn’t work for ghc anyway. Maybe that changes now. When I was able to use it, it was great.
I appreciate your efforts but for me the description wasnt any easier.
Nice write up &amp; kudos for going the extra mile and implementing it in both Haskell &amp; Agda! Do you mind [submitting an issue](https://github.com/agda/agda/issues) about the missing cases warnings you get when you enable irrelevance? This sounds intriguing.
I generally try to avoid State, since it can encourage you write code in a very imperative sort of way. You can often times use folds and possibly monoids in its place which can help you write more concise code which is easier to reason about.
Sure! I'll try find a minimal example.
RemindMe! 2 weeks
I found that learning some category theory allows me to approach problems from different angles than I would be able to otherwise. Knowing about things like adjunctions and comonads can shine a light in hidden areas and help you reason about/simplify your code. I don't think it's necessary to learn CT in order to become a great Haskell programmer, but there are some great resources on the internet these days so why not?
The thing is, `IO` and `ReaderT r IO` are instances of both `MonadThrow`/`MonadCatch` and `MonadUnliftIO`, so if you use either of those monads underneath `ConduitT` then your code should still work fine. As far as I can tell, the `MonadUnliftIO` typeclass is supposed to replace `MonadIO`, so unless you are using `liftIO` somewhere in your function, you don't need to include `MonadUnliftIO` in the type signature.
Eagerly agree, but again not disparaging. I'm not sure I'm the target audience. (Statistical scientist Haskell learner rather than computer scientist Haskell learner)
Now you have two names for the same thing. That's not really helping, is it?
Nice article. I disagree with a couple points. First, and most importantly, exceptions are not really part of pure Haskell in the sense discussed by Moggi. They are more like another form of partial programs. Throwing an exception is only half of what makes monadic exceptions useful, the other half is handling them. This is what `Maybe` and `Either` let you do, most simply. Second, in the discussion of the lazy state monad, there is a comment about how evaluation isn't forced, and somehow that means it might not actually be a monad. This is a pretty deep misunderstanding; lots of monads exist without forcing values. In fact, the `mfix` combinator, the monadic lifting of the ubiquitous "Y-combinator" `fix`, relies to some extent on lazy evaluation. Certain other monads like the reverse state monad also rely on lazy evaluation to exist at all. What makes some structure a monad or not is simply whether it follows the monad laws. These laws correspond to some very basic rules about computation and inlining; things like `let x = exp; f(x)` is the same as `f(exp)`, and that function inlining (changing the associativity of the binding `;` between statements) doesn't change the meaning of a program.
Doesn't this work? It uses [Dict](https://hackage.haskell.org/package/constraints-0.3.2/docs/Data-Constraint.html#t:Dict) and overlapping instances instead of TypeInType. class HasSomeC (cs :: [Type -&gt; Constraint]) (c :: Type -&gt; Constraint) where -- Requires AllowAmbiguousTypes projectDict :: Dict (ManyConstraints cs a) -&gt; Dict (c a) instance {-# OVERLAPPING #-} HasSomeC (c ': cs) c where projectDict Dict = Dict instance {-# OVERLAPPABLE #-} HasSomeC cs c =&gt; HasSomeC (x ': cs) c where projectDict Dict = projectDict @cs @c Dict instance HasSomeC cs Show =&gt; Show (Exists cs) where showsPrec n ea = case ea of Exists a -&gt; f a where f :: forall a. ManyConstraints cs a =&gt; a -&gt; ShowS f a = case projectDict @cs @Show @a Dict of Dict -&gt; showsPrec n a 
Monads don't do sequencing. IO monad does. Monads are used to compose Kliesi arrows.
There are already multiple names in common use amongst programmers for all the standard parts of functional programming. F# has computational expressions instead of monads. Most languages have mappers instead of functors. Some languages (C++, Prolog) have entirely different meanings for Functor. The question then is which name to choose, and how. The general consensus in this conversation has been that language popularity and ease of learning is secondary to adhering to definitions from branches of maths like category theory (and dictionaries!). My view, the minority, is that ease of learning is important, and that one should use the names that programmers generally have already organically settled on and are likely to understand (map, reduce etc). Moreover I don’t think that changing names reduces mathematical rigor at all, particularly if one makes note in the documentation. The idea is gradual self-revelatory learning. From the name Combinable one gets the idea that they’ve found a combining typeclass, and from reading the docs via an IDE they confirm it’s what they need and additionally learn it corresponds to a semigroup with particular laws. One can choose the dogmatic approach, even if it results in a lack of popularity. There’s no doubt a satisfaction to belonging to a peculiarly educated clique. However absent a critical mass of programmers, libraries and tooling will be few in number and poorly developed, as demonstrated by the continually appalling numbers Haskell frameworks obtaining in the Techempower web-development benchmarks. Consequently, one will very rarely get to use Haskell at work. It’s worth noting that in many ways this is a bad time for Haskell. Impure strict MLs like Rust, ReasonML, F# and even Swift have many of the features Haskell once touted as unique selling points. Nowadays it has fewer ways of differentiating itself, and is still (deliberately and unnecessarily IMHO) difficult to learn and therefore hire for. 
Where are type level lists defined and what operations do they support? I am in a particular need for the concatenation. `'++` does not seem to work. I am rolling my own type family in the meantime, but it bothers me that I could not google up any relevant documentation. There is a module `Data.TypeLits` in `base`, but one thing it does not talk of is type level lists.
tbh, I've never really seen "smugness" from experts about monads.
I'm not so sure that monads aren't about sequencing. Category theoretically speaking, there is no generic swap arrow `m • m -&gt; m • m` with which to talk about commutativity, and I doubt you could construct one for any sub category of Functors due to the Functor laws (though I'm an amateur so I have no evidence of this). Speaking in terms of Haskell, the effects of the first computation *determines* the effects of the next. There is no Monad for which this isn't true, so sequencing seems inherent to monads. At best you can discard effects entirely, but then you're still left with the non-commutative composition of pure functions, which still cares about sequencing. In fact, generally speaking, I'd say category theoretical composition probably encodes sequencing, and monads are just the kleisli form of this. Please someone tell me if I'm just talking out my ass here :P
They're defined in Data.List, because they're just regular lists lifted to the type level (you need to enable DataKinds). This lifting only works on *constructors* (in the case of lists, `[]` and `:`), not on functions. I'm sure type-level `++` is defined somewhere, but I don't know where.
Thank you. I suspected as much. However, if these lists are actually identical to the ordinary ones, should not it be possible to run ordinary functions on them _(at compile time)_?
what :&amp;: does it mean ? what does it do ? haskell noobie here. 
In principle, yes. In practice, no, because this automatic lifting (currently?) only works on ADTs; any function you try to lift might involve something that isn't available at the type level, e.g. Int.
There's no magic -- it's just something he defined in his library/article. You could use words and get the same thing like this: `data MyProduct a b = MyProduct a b`, but then instead of writing `a :&amp;: b :&amp;: c :&amp;: d` you'd get to write `MyProduct a (MyProduct b (MyProduct c d))`.
It shouldn't be much work at all - you just need to have the `direnv-mode` package installed, and then run `(direnv-mode)` on startup to enable the minor mode everywhere.
Well, it doesn't look like nix-emacs can actually be used with anything that doesn't have direct support for it. That is, if something just uses `executable-find` then I'm screwed. I could advise `executable-find` to actually use `nix-executable-find`, but that's more work than just enabling `direnv-mode`. &gt; naively, it seems like you have to have an .envrc file in addition to a nix-shell in every project This is mostly true, but it's one line that I write once and don't really care about. In bigger projects that have multiple `.cabal` files, I use a mono repo and have a top level .envrc that puts me in a nix-shell that can build all of the `.cabal` files. `direnv` is smart enough about not reloading itself in each subdirectory.
Applicatives and Monads sequence effects in the same way that any monoid "sequences" values with (&lt;&gt;). This is not necessarily commutative so the order can matter, but doesn't have to. [1] &lt;&gt; [2] /= [2] &lt;&gt; [1] (Sum 1) &lt;&gt; (Sum 2) == (Sum 2) &lt;&gt; (Sum 1) IO is a good example of a non commutative monad since it executes across time and the effect of printing 2 statements in the opposite order is obviously different. Maybe is a good example of a commutative monad since it will fail regardless of whether the Nothing is its first effect or its second. If we disregard the way that values are threaded through these effects, all the applicative and monad functions (&lt;*&gt;) (&lt;*) (*&gt;) (&gt;&gt;=) have the same type: m -&gt; m -&gt; m (i.e monoid appending of effects)
Very informative and to-the-point post. Thanks!
Maybe is not a commutative monad. Just 1 &gt;&gt; Just 2 /= Just 2 &gt;&gt; Just 1
I was talking strictly about the effects being commutative, not the values that are threaded through them. Just &lt;&gt; Nothing == Nothing &lt;&gt; Just
Is there a unified way for talking about arbitrarily long products? Is this heading towards generic programming à la generics-sop ?
I don't think so. I don't know beautiful solution for open sums without typelevel-black-magic.
The non effectful part was what I said ensures a Monad is not commutative. Regardless of effects, composition is sequential.
Okay, I see. In case its of any benefit, I went ahead with a more hybrid approach using Haskell and Nginx (openresty) together. I'll post more details soon. Currently, its powering websites like these: corica.be, naturalcaffe.com etc. We serve hundreds of these domains and process thousands of reqs. per minute without problems.
Ah yes I see what you mean.
Thanks! By the way there is some documentation for rewriting but [it's in another chapter](https://agda.readthedocs.io/en/latest/language/with-abstraction.html#rewrite). Hopefully this gets linked/redirected to soon.
About purity and a particular type system, absolutely, but yeah not so much monads 
I'm playing it on the Music channel. I'll try playing it on another channel and see if it works! Relevant source code: [https://github.com/elben/pong-wars/blob/master/src/Main.hs#L554](https://github.com/elben/pong-wars/blob/master/src/Main.hs#L554)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [elben/pong-wars/.../**Main.hs#L554** (master → d3cabda)](https://github.com/elben/pong-wars/blob/d3cabdaabbe2e28502d393280d967d1500ea61ea/src/Main.hs#L554) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e3cl7za.)
Well I changed it to not play in the Music channel and things seem to work just fine now. Thanks for the tip.
They're semantically identical, but have some differences under the hood. In particular, lambdas will only share data when they are *fully saturated*. Which means you might see better performance if you write f a = \b -&gt; let x = someExpensiveComputationOver a in f' x b instead of f a b = let x = someExpensiveComputationOver a in f' x b when using `f` partially applied, eg `fmap (f 5) [big list]`
No problem :)
It's funny to me that everyone spends so much time apologizing for laziness and talking it down when it's actually a really important part of what makes Haskell useful in a lot of scenarios, and a small but important part of why its concurrency story is good.
I think the missing article probably refers to the {-# REWRITE #-} pragmas 
No. However, stack seems to generate a nix derivation where src is set to a path under /nix/store, where my entire project directory is copied. Maybe there's an environment variable that I can set in shell.nix that will set stack's src properly. Anyways, I cleaned out .stack-work, so it's no longer copying ~450MB each time, so the bloat is manageable for the moment.
Ah, so stack is the problem :P Can you give me any indication of what that nix derivation that stack is generating is? I'd like to try and fix that.
&gt; there is a comment about how evaluation isn't forced, and somehow that means it might not actually be a monad. This is a pretty deep misunderstanding; lots of monads exist without forcing values. That misunderstanding starts *early* in the article: "This is where monads rescue Haskell. The key relevant construct of the monad is what Moggi called the extension of a function f to a function f\*, i.e. the way functions are to be "lifted" to be interpreted in the monadic setting. It is defined as f\* c = let x &lt;= c in f x. This helps because it forces the evaluation of argument c in a predictable way." This is such an egregious error that I strongly considered removing my upvote or even downvoting the post. Monads aren't essentially about *anything*--not evaluation discipline, not referential transparency either. The fact that the Identity functor is a Monad is proof of that; it can't carry anything around that wasn't already in the language. Some monads ((-&gt;) e, ReaderT e m, StateT s m, ST, IO, etc.) are for referential transparency. `get` or `getTimeOfDay` can't be *just* an `s` or `DateTime`, or we lose referential transparency. Instead we apply a functor to those types and call them actions-returning-a-`s`. We also want to do "computation" with these actions, and it turns out the "computation" provided by monads was a pretty close match. (If you can do your computation without `case`, you get the Applicative model of computation. If you need fixed-points in your computation, you have to move up to the MonadFix model of computation.) Other monads ([], WriterT l m, Proxy a a' b b' m) are about entirely different things. But, the computation we want to do is sufficiently similar, that we also use monads for them. I like monad = context + computation, but it's useful to remember it's not the *only* context + computation abstraction available. Anyway, I'll leave my upvote on the article even though it's wrong, since it seems to have inspired some good comments. 
It doesn't have exactly what you want, but you should check out [kleisli-functors](https://github.com/ElvishJerricco/kleisli-functors) and the blog post it's based on.
This always really annoyed me about GHC but I don't really see a way around it. Is there a way to define the most general operation and expose partial application behaviors as needed? ie it seems like there's always a meaningful choice of `f a b c = ...`, `f = \a -&gt; \b -&gt; \c -&gt; ...` and everything in between which can't be changed without rewriting the entire function body. If we write f = \a -&gt; \b -&gt; \c -&gt; e f' a b c = f a b c is ghc smart enough to generate specialized code for f' without allocating intermediate closures? Is there a reliable way to get such behaviour?
&gt; But the rules of the language evaluation strategy, along with the further propagation of the flow of control will eventually lead to the replication of the argument This isn't correct. A pointer to the value might be duplicated, but the value isn't. You are more likely to get `movq (%rbx), %rax` `addq (%rcx), %rax` than something like `movq (%rcx), %rax` `movq (%rdx), %rbx` `addq %rbx, %rax`
This doesn't seem quite right. A representable functor `f :: Kleisli m -&gt; (-&gt;)` (assuming `(-&gt;)`/`Hask` and `Set` are the same) needs to be equipped with an object `RepK f` in `Kleisli m` and a natural isomorphism, with two halves: tabulateK :: forall b. Kleisli m (RepK f) b -&gt; f b indexK :: forall b. f b -&gt; Kleisli m (RepK f) b -- tabulateK :: Kleisli m (RepK f) ~&gt; f -- indexK :: f ~&gt; Klesili m (RepK f) You've got `RepK` and `indexK` right, but your `tabulateK` is incorrect, being `Kleisli m (RepK f) ~&gt; (m . f)`. I'm sure this class has its uses, but I don't think it can rightly be called `Representable`. Really, making `Vector` representable by fixing a size via global state is a horrible, ugly, no-good hack. I think you'd be much better of using something like [`vector-sized`](https://hackage.haskell.org/package/vector-sized), which gives you [`Vector :: Nat -&gt; Type -&gt; Type`](https://hackage.haskell.org/package/vector-sized-1.0.4.0/docs/Data-Vector-Sized.html#t:Vector). For a given `n :: Nat` with `KnownNat n`, `Vector n` is a nice, well-behaved `Representable` endofunctor. It is represented by the object [`Finite n`](https://hackage.haskell.org/package/finite-typelits-0.1.3.0/docs/Data-Finite-Internal.html#t:Finite) from [`finite-typelits`](https://hackage.haskell.org/package/finite-typelits) and the families of arrows [`index`](https://hackage.haskell.org/package/vector-sized-1.0.4.0/docs/Data-Vector-Sized.html#v:index) and [`generate`](https://hackage.haskell.org/package/vector-sized-1.0.4.0/docs/Data-Vector-Sized.html#v:generate).
I’m afraid I don’t know know about it’s development but, out of curiosity, do you have a use-case in mind?
https://pastebin.com/g9aHnJU2
I had a [stackoverflow answer](https://stackoverflow.com/questions/49368020/impose-nesting-limits-on-recursive-data-structure/49373746#49373746) a while ago that could have used them.
Yeah, that works! (Had to enable `UndecidableInstances` as well.) I hadn't thought of using explicit type applications or overlapping instances. I'm not sure I actually want to stick with this design, but this does seem to be a way to make it work.
You might like [data types à la carte](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf)
You can’t write a Functor instance for Either due to how application of type constructors work. You have to newtype it &amp; flip the type variables. 
I once saw the following type signature in a library: `parsingBase :: ( MonadPassManager m, ParsingPassReq_2 m , UnsafeGeneralizable a (Expr Draft), UnsafeGeneralizable a SomeExpr ) =&gt; AsgParser a -&gt; Text32 -&gt; m (a, MarkedExprMap)` How can I use this function if it looks like it can return anything? More specifically, how can it return `m` if `m` is just defined by its constraints? It's like `m` has no type... I just straight on don't understand this type signature :)
Author here. Thank you for the comment! I was trying to distinguish between scheduling the effects which monads need to do (at least according to Moggi) from forcing evaluation which is one obvious way of achieving it. But I don't think I was clear enough. Nevertheless my point stands. A lazy language needs a mechanism to schedule the interactions in computations predictably, and that is the role of the monads. Eager languages can add effects in a type agnostic way. 
It just means the caller chooses what the m and a are. Valid choices they have are any m and a that are instances of the mentioned classes. For example, if there were an `IO` instance for these `m` classes, you could use this in your main function, so long as you also picked a type for `a` that satisfied the other instances. Also as an aside, the code where you found that function is extremely high level, there are a lot of things in it that I don't understand either.
&gt; A lazy language needs a mechanism to schedule the interactions in computations predictably Not if it is pure. &gt; that is the role of the monads. Nope, still wrong. Haskell decided to use monads for this, a bit. It also uses monads for a bunch of other things. Clean uses uniqueness types, so you can definitely to it without monads.
There's a recent [`Bifunctor` instance](https://hackage.haskell.org/package/base-4.11.1.0/docs/Prelude.html#t:Bifunctor) that should do what you want.
You can control with that how keen GHC is to inline you binding ([SO answer](https://stackoverflow.com/a/46173048/388010), [GHC User guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html?highlight=inline%20pragma#inline-pragma)). There's no semantical difference at all, they get desugared to the same Core IR, *except* that the `IdInfo` of `f` (which is consulted by various optimizations, such as the Simplifier) has different inlining guidelines. When there's a fully saturated call site for `f`, GHC will inline its RHS into the call site and subsequently beta-reduce (e.g. apply) the exposed lambda abstraction with the argument. This allows other transformation to take over, but at as always there's the risk of code duplication (and resulting slowness) if you inline too keen.
GHC doesn't really generate specialised code so much as it just does or does not inline the binding, which you control with how you declare your function. See my top-level comment for details. That's also why [`(.)`](https://github.com/ghc/ghc/blob/5e103a1e8a5b23eafbaf825c255b919e8205d87b/libraries/base/GHC/Base.hs#L1293-L1298) is defined as it is.
The type of `fmap` is `Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b`. It is possible to instantiate `f` to `Either l`, yielding `(a -&gt; b) -&gt; Either l a -&gt; Either l b`, or to `(,) l`, yielding `(a -&gt; b) -&gt; (,) l a -&gt; (,) l b` (or equivalently, `(a -&gt; b) -&gt; (l, a) -&gt; (l, b)`), but it is not possible to instantiate `f` in such a way that the type of `fmap` specializes to `(a -&gt; b) -&gt; Either a r -&gt; Either b r`, `(a -&gt; b) -&gt; (a, r) -&gt; (b, r)`, `(a -&gt; b) -&gt; Either a a -&gt; Either b b`, nor `(a -&gt; b) -&gt; (a, a) -&gt; (b, b)`. You can, of course, write such functions yourself, you just can't use `fmap` to perform those transformations. I would also like to point out that if you define versions of `Either` and `(,)` which only have one type parameter instead of two: data MyEither a = MyLeft a | MyRight a data MyPair a = MyPair a a Then you can implement a `Functor` instance whose `fmap` transforms both parameters; `(a -&gt; b) -&gt; MyEither a -&gt; MyEither a` and `(a -&gt; b) -&gt; MyPair a`. The real `Either` and `(,)` are defined using two parameters because it is more valuable to be able to use two different types for the two branches/fields than to be able to transform both branches/fields at the same time using `fmap`.
Good game
Btw., GHC could potentally just float `x` out one level, [but doesn't do so intentionally](https://github.com/ghc/ghc/blob/a1a507a1faefef550378758f5228bd01c78c4f25/compiler/simplCore/SetLevels.hs#L355). I for one am not missing this, because you can introduce the desired sharing pretty easily through a let-binding. If this would be optimized by default, you would have no chance to tell GHC not to do it ([there are good reasons for that](https://ghc.haskell.org/trac/ghc/ticket/917)).
`Either l r` has two constructors: `Left l` referring to the left type variable and `Right r` referring to the right type variable. Because of Haskell's kind system, `Functor (f :: * -&gt; *)` can only work with `Either l ~ f`. We must pre-apply the type. If you want to operate on the `Left` side with a `Functor` instance, you need something to swap the types: newtype Flip f a b = Flip (f b a) instance Functor (Flip Either r) where fmap f (Flip (Left l)) = Flip (Left (f l)) fmap f (Flip (Right r)) = Flip (Right r)
Tracks really closely with my recent experiences in the ICFPPC 2018. Was using a `IntMap (IntMap IntSet)` for some things and it was fairly quick, but my `(Int, IOVector Word64)` replacement got about a 30% speed improvement. I did find the later much harder to experiment with in ghci, though.
You are not allowed to do the latter when doing record updates for function-typed fields. Proposing a GHC extension for this is on my middle-term TODO list.
Only the last type parameter can be the one that varies for Functor, because of the limited form of an "instance head" in the Haskell Report.
Other examples: `read :: Read a =&gt; String -&gt; a`, or `put :: MonadState s m =&gt; s -&gt; m ()`. It's best to think in terms of type arguments. The fully type is `read :: forall (a :: *). Read a =&gt; String -&gt; a`. You should consider `read` a function with *three* arguments: a type `a`, an instance for `Read a` and a `String` value. In the internal representation of GHC, `read` is in fact a function with these three arguments. If you enable `TypeApplications`, you can partially apply to the first two arguments, by `read @Int :: String -&gt; Int`. The second argument is filled in implicitly with the `Read Int` instance. In general, we have - Type arguments, which are by default implicitly given by inference, but can be also given explicitly with type application. - Instance arguments which cannot be given explicitly, and are filled in by instance resolution - Regular arguments which cannot be given implicitly, and must be written out by programmers. Likewise, you should consider `parsingBase` a function with eight arguments: two types, four instances, then two values. 
Well, if you have to deal with situations where you don't know the dimensions of higher-kinded numbers at compile time, and you'd like to have a somewhat unified api between 'fixed' and 'unknown' sized vectors, you have to swallow the monad somewhere. Current boiler-plate is to use (withSomeSing)[http://hackage.haskell.org/package/singletons-2.4.1/docs/Data-Singletons.html#v:withSomeSing], and, until DependentTypes hits, type-level continuation programming is too cruel and unusual. So I might try this out in [numhask](https://github.com/tonyday567/numhask/blob/master/numhask-array/src/NumHask/Array.hs#L168)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tonyday567/numhask/.../**Array.hs#L168** (master → e4a8b33)](https://github.com/tonyday567/numhask/blob/e4a8b331e63add893a1f6a1e29ad0c3baf5caf46/numhask-array/src/NumHask/Array.hs#L168) ---- 
You might get even better performance with a monomorphic data type, so the Int can unbox, like \`data Foo = Foo !Int !(IOVector Word64)\`.
Do we know how much of the performance that was gained was via the "mod x (2^n)" specialisation? Obviously it wasn't all of it, but that seems like a fairly small but useful improvement that could be made in GHC.
That's actually what I used, I just didn't want to type that out.
oh ok
One difference nobody else is mentioning is that: f = \a -&gt; ... May trigger the dreaded monomorphism restriction (if the type of the lambda is inferred to have type-class constraints). f a = .. Will never trigger the monomorphism restriction.
What is a functor? It is a mapping that takes all the objects in a category C into a category D. In the case of the standard Functor type class, C and D are exactly the Hask category (in other words: the `Functor` class encodes of endofunctors on Hask, the category of Haskell types). Thus a `Functor` instance must map each type to some type, and so must have kind `Type -&gt; Type`. A pair has a different (though related kind) `Type -&gt; Type -&gt; Type.` In order to make it an instance of `Functor` we need to partially apply the type constructor to something. For example we could apply it to `Int`, obtaining `(Int, )`, the functor that maps, `Bool` to `(Int, Bool)`, `()` to `(Int, ())` and so on. In other words, `(,)` is not a Functor, but `(T, )` is, for any type T. Similarly with `Either`. There are alternative encodings of functors that are not restricted to endofunctors, see e.g. [http://hackage.haskell.org/package/hask-0/docs/Hask-Category.html#t:Functor](http://hackage.haskell.org/package/hask-0/docs/Hask-Category.html#t:Functor)
You could try `unsafeShiftL` and `unsafeShiftR` as well. On occasions I've gotten large speedups from that.
Your "I would also like to point out" was the real eye-opener. Thanks!
A couple of potential points of failure I've found with Haskero: - Intero should to be installed in the _local_ stack environment, not global; i.e. from within the project directory, `stack build intero` (NOT `install`). - If you're looking at an existing project with dependencies, make sure they're built: `stack build --dependencies-only` - If your project dependencies _change_, the plugin won't immediately notice and will choke on what it thinks are invalid imports. VS Code "Reload Window" (from Ctrl-Shift-P menu) fixes this. Other than these (consistent, easily handled) caveats, I've had a pretty good experience with Intero. It's definitely not perfect but it's covered my needs pretty well.
Recommended for fixed shifts that are less than the bitSize of the known type. I.e. when it is easy for the programmer to correctly assert the safety, no reason to pay a potential safety cost. I definitely used those in my use case.
28.4ms using separate `div` and `rem`. (I would use `divMod`.) 7.6ms using separate `.&gt;&gt;.` and `.&amp;.`. (I don't know a way to combine.) So, that's a 3.7x speedup from just that change. 
I'm a programmer coming over from C with an interest to learn Haskell. I have no knowledge of syntax, data types, or anything else. Where do I start?
But I believe what OP is hinting at is that they will not adopt a streaming approach, but rather use a more space efficient in-memory representation (not sure). I'm not OP, but that gives me an idea for an alternative approach: if we assume that the content of the JSON blob (i.e. the strings themselves) are significantly larger than the amount of space consumed by syntactic overhead, then we could just memory map the file and construct `Text`s from pointers. Taking this approach a step further, one could envision a parser which only stored the offsets for the root object and its children, such that accessing any child node resulted in that subtree being parsed lazily. (i.e. it's a space-time trade-off, exchanging repeated computation for lower memory consumption). The main disadvantage is that this requires you to store the file on the file system somewhere, rather than streaming directly from S3. There's also the minor annoyance of needing to perform accesses in IO, unless you hide it with `unsafePerformIO`.
Great answer, thank you!
Could you make the graph scale be relative to the fastest instead of absolute? IIUC the thing you're trying to emphasize is the factor of slowdown vs choice of implementation and not the big O complexity.
Short version: Before learning Haskell, I mostly had experience with procedural languages (C, C++, Java, Python) and I found "Learn You A Haskell" an okay introduction to the basics. Long version: I want to make a strange recommendation – don't start with Haskell. I don't know your situation and experience, but I think my friend was recently in a similar situation to you, going from being a long-time C programmer to some fancy newfangled language. I recall that it took him a while to let go of C habits/patterns - passing in pointers, for-loops (python only has foreach and while), `src++ = dst++;`-style idioms, stuff like that. If you've been doing C for a long time, you might find it easier to try something like Python first to get into a more "carefree"/"I don't care about memory management"/"no-braces-or-semicolons ;)" mindset in an imperative/procedural setting. (No need to go into OOP/dynamic stuff too much, although if you find that you enjoy that, good for you!) Here's a bunch of more specific things to try: - Write some simple stuff first, and try to be idiomatic – use a guide to make sure you're not just writing C in Python. I recall that doing a CodeCademy course for total programming newbies was surprisingly helpful, as it showed me different ways to achieve stuff I already knew how to write in C++. - Write pure functions as much as possible – copy stuff all over the place, even though it might feel painful ;) - Structure your programs as a pure-functional core that just transforms data, and a thin IO layer that actually reads/writes stuff out to the outside world - Get some experience with "duck-typing" – it should provide a nice transition to Haskell's typeclasses - use `namedtuple` – basically immutable structs - Check out list comprehensions, tuple destructuring, lambdas/functions-as-normal-values-you-can-do-stuff-with, map, filter, itertools, maybe decorators - all functional programming patterns - Then, make that easier by using immutable data structures from the `pyrsistent` library IME, getting some experience with those things should make moving to Haskell much smoother. Hope this helps and good luck!
I can never remember whether `quotRem` or `divMod` is the fast one. I often go for `divMod` just because still think of the individual operations are "divide" and "modulo" not as "quotient" and "remainder". :/ But, I generally have little call for `quot`, `div`, `rem` or `mod` by themselves; I guess I use them for Haskell FizzBuzz, but that's about it. :P
**Short version** Before Haskell, I only had experience with procedural languages (C++, Java, Python, C) and I found "Learn You A Haskell" to be an okay introduction to the basics. **Long version** I want to make a strange recommendation – consider starting with something like Python, and then move on to Haskell. Of course I'm making a bunch of assumptions here as I don't know your situation and experience. I think my friend was recently in a similar situation to you, going from being a long-time C programmer to a very different newfangled dynamic language. I recall that it took him a while to let go of C habits/patterns - passing in pointers, for-loops (python only has foreach and while), `src++ = dst++;`-style idioms, stuff like that. If you've been doing C for a long time, you might find it easier to to get into a more "carefree"/"""high-level"""/"I don't care about memory management"/"no-braces-or-semicolons ;)" mindset in Python's more familiar imperative/procedural setting. Here's a bunch of more specific things to try: - Write some simple stuff first, and try to be idiomatic – use a guide to make sure you're not just writing C in Python. I recall that doing a CodeCademy course for total programming newbies was surprisingly helpful, as it showed me different ways to achieve stuff I already knew how to write in C++. - Write pure functions as much as possible - copy stuff all over the place, even though it might feel painful ;) - Structure your programs as a pure-functional core that just transforms data, and a thin IO layer that actually reads/writes stuff out to the outside world - use `namedtuple` – basically immutable structs - Check out list comprehensions, tuple destructuring, lambdas/functions-as-normal-values-you-can-do-stuff-with, map, filter, itertools, maybe decorators - all functional programming patterns - Then, make that easier by using immutable data structures from the `pyrsistent` library - Get some experience with "duck-typing" – it should provide a nice transition to Haskell's typeclasses. (No need to go into OOP/dynamic stuff too much, although if you find that you enjoy that, good for you!) IME, getting some experience with those things should make moving to Haskell much smoother. And there, even more wonders await ;) Anyway, I hope this will help you in some way. Good luck!
`quot` and `rem` are the fast ones, but `div` is the one that can correctly be optimized to a shift (negative numbers are tricky). 
I don't think it's necessary to learn an entirely different language before learning Haskell. All he needs to do is get "Haskell Programming from First Principles". That book has everything he needs to get started.
Consider this short module: 
Can you explain this in more detail? I don't understand. Searching for "lambda saturate" doesn't result in anything helpful.
It's rather nicer to write functions with multiple patterns using the second syntax. Compare: maybe x Nothing = x maybe _ (Just v) = v or maybe = \x m -&gt; case m of Nothing -&gt; x Just v -&gt; v They're similar in length, but the latter has a lot more syntactic noise.
"Saturated" in this context means "it has received all of its arguments". In the second example, `someExpensiveComputationOver a` is recomputed every time the function is called, even if `a` is the same. In the lambda example, the function explicitly does some work when it receives its `a` argument before returning a lambda that wants a `b`. If that lambda is called multiple times, it will reuse the same `x` instead of recomputing it.
I see that `fromIntegral` is used to convert between numeric types. But `fromIntegral` is implemented as `fromInteger . toInteger`. Is there faster way to convert `Word64` to `Int`? Maybe you can write something like this: w64ToInt :: Word64 -&gt; Int w64ToInt (W64# i) = I# (word2Int# i)
I believe on 64bit machines it will be compiled to no-op.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**Word.hs#L794** (master → 5e103a1)](https://github.com/ghc/ghc/blob/5e103a1e8a5b23eafbaf825c255b919e8205d87b/libraries/base/GHC/Word.hs#L794) ---- 
Thank you, that explanation approach was enlightening!
Are there any reliable tools for cross-compilation? I'm considering porting a CLI tool from Go, but cross-platform support is an important constraint and all the cross-compilation documentation I've seen so far requires fiddling with GCC.
Oh, that's really nice! I wasn't aware of such rewrite rules. In that case it's really cool that I can use `fromIntegral` without worrying to much about performance. I only fish that function had shorter name, something like `toNum` at least...
Oh yeah, that makes a lot of sense!
Damn the person who wrote the Monad definition on Haskell Wiki
Is uint64_t the fastest choice for the C implementation?
Even nicer with `-XLambdaCase`! maybe = \x -&gt; \case Nothing -&gt; x Just v -&gt; v Though, I personally would write it like this: maybe x = \case Nothing -&gt; x Just v -&gt; v
I'm a little sad that the first words after "Making Haskell as fast as C" is "imperative programming". Shouldn't functional programming also let us program code as fast as C?
Amy too Wong if I make a joke? Sorry. I had to. It looks like an interesting conference. Please downvote.
Since when does Amy sound anything like 'am I'? 
This whole exchange seems completely unnecessary.
Currently the major bottleneck at a low-level is memory, its allocation and caches. You need to control memory carefully, which you can in C, and not in Haskell. In an ideal word, the compiler can detect when it can properly reuse memory, allocate things on the stack, in practice this is really hard and time-consuming.
For absolute beginners I second [Learn You a Haskell](http://learnyouahaskell.com/) or [Real World Haskell](http://book.realworldhaskell.org/read/). For intermediate/advanced I'd recommend: \- [What I Wish I Knew](http://dev.stephendiehl.com/hask/) \- a great reference covering a variety of topics. \- [Parallel and Concurrent Programming in Haskell](https://simonmar.github.io/pages/pcph.html) \- the definitive resource on parallel and concurrent programming. \- [Category Theory for Programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/) \- category theory with Haskell and C++ examples. \- [The Haskell School of Expression](https://www.amazon.co.uk/Haskell-School-Expression-Functional-Programming/dp/0521644089) \- a classic text by one of the pioneers.
Yes, but what does that have to do with Imperative vs functional?
It's C vs Haskell, not imperative vs functional. Imperative language that do not fine control over memory like Java are also a couple time slower than C. 
If you're interested in skipping ahead a bit, video from my recent stream on succinct data structures, which covers some usecases for the rank-select structure that John is alluding to here, is available: https://www.twitch.tv/ekmett/video/291249101 It is a bit less focused than John's gentle introduction, but does dive into a bunch of applications of a good rank-select structure, if not how to build one per se.
I agree.
If that's, like, Archlinux's pacman and you're not paranoid, then you could try asking it to not check signatures – [https://wiki.archlinux.org/index.php/Pacman/Package\_signing#Disabling\_signature\_checking](https://wiki.archlinux.org/index.php/Pacman/Package_signing#Disabling_signature_checking). You'd just have to figure out where stack keeps the \`/etc/pacman.conf\`. Incidentally – if anyone know what is pacman doing inside of stack, I wouldn't mind being enlightened about the why.
You're right, it's not necessary – I only meant to say that if he's got deeply ingrained C habits and little experience with GC'd languages, taking a short detour on the way to learning Haskell might be helpful. Perhaps I could've expressed that better. If you disagree with that, could you say why? If, in you experience, C programmers have no problem moving to Haskell, please let me know – I'm always happy to correct my view of things.
I'm currently working on an unofficial guide to GHC's extensions, aimed towards beginners who are wondering what the hell is going on. Adding a stability indicator to this guide sounds like a really good idea. Afaik, GHC HQ doesn't commit to any stability guarantees for extensions, but since the whole thing is just my opinion anyway, that doesn't bother me too much.
I'd honestly rather we re-double the efforts on producing a new Haskell Report, and those that want stability can stick to the Haskell language as defined in the report, and those that want to research and use GHC (or other compiler) extensions.
I need that. 
then a precise schedule for the Haskell-Report (for example every 2 years) would be great!
I hope the author reads this: Have you had a look at easyspec as well?
&gt; guide to GHC's extensions https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions Does it need an update?
I think it would be needed simply to classify language pragmas as stable and maybe deprecate old, stable but increasingly unused ones 😉
This is not official of course but I often refer to it: http://dev.stephendiehl.com/hask/#language-extensions
&gt; I sometimes find it very hard to get an intuition about which abstractions are worthwhile. Well, yea. It contains the halting problem as a subproblem! The best way to cope with this impossible skill is to learn how to recognize a *bad* abstraction as soon as possible, i.e. learning which abstractions are "not bad (yet)" is easier than which are "good". A bad abstraction is one that brings you to grief and confusion. Often you learn this lesson violently, weeks after you implement it, on a deadline, at 3 AM. It's almost never clear at first glance whether an abstraction will work out unless it has a similar shape to some other abstraction you've met before. Even then, the slightest modification can ruin an elegant tool. As for recursion schemes, the inestimable John Wiegley has a great report from the trenches about them: http://newartisans.com/2018/04/win-for-recursion-schemes/
The C++ ISO committee adopted a 3-year cycle for standards starting with C++11. It has made a huge difference for the language, both for adopters of new features and for compiler and tool developers. It would be amazing to follow their lead here.
TIL. In any case, a new report is *overdue*.
I'd like this, but there's a lot in the way of adding things to the report. We currently have https://github.com/haskell/rfcs for proposals for Haskell 2020, but it looks like *nothing* has been accepted; is that accurate? This is in part due to a lack of clarity on just how an acceptable proposal becomes accepted, and in part due to the fact that most proposal discussions list more problems than solutions. There's a lot of extensions like BangPatterns where I'm ok with "just merge it the way it is now." But there are lots of complaints about these extensions' existing implementations, which is holding back the process indefinitely. That *would* be for the best, if it seemed like any progress was being made. But that largely doesn't seem to be the case. There's also not nearly as many proposals as there are extensions that people are interested in seeing added, because it's hard to write an acceptable proposal (i.e. "Please add ScopedTypeVariables" isn't enough). It'd be nice if every extension in GHC were documented as though it were a proposal to the Haskell report; then we could just branch off those and consider them for adoption without anyone having to go back and do this at a less relevant time.
Scheme (and SICP) was a perfect gateway drug for me: dynamically-typed, functional, automatic memory management, reasonably pure.
This is so weird. I haven't tried cloning your repo to reproduce the issue myself, but I can see no reason why that derivation would be added :/
&gt; -- | &gt; -- λ show ... (+2) $ 3 &gt; -- "5" Btw, while `λ` is a much better prompt than the default in `ghci`, the `&gt;&gt;&gt;` syntax is the one used by [haddock](https://www.haskell.org/haddock/doc/html/ch03s08.html#idm140354810775744) and [doctest](https://github.com/sol/doctest#usage).
What is the state of the art of Bazel for Haskell? For example, I would like a workflow such as: bazel cabal install aeson And get the aeson build from cache, if available, or build it and upload to cache.
I did the same and started with #haskell on freenode.net, "The Craft of Functional Programming", and "Haskell School of Expression". Man I wish I still had those books around for keepsakes.
Yes; that guide doesn't cover a lot of newer extensions (notably type families). I'm also trying to improve on its structure by partitioning the extensions into two tracks ('basic' and 'advanced') and linking to external tutorials where available. Indeed, a lot of those links go to the School of Haskell guide, whose explanations I find pleasingly concise.
If we can't agree enough to get the report updated, I'm not sure anything is really "green light" stability.
Yeah, you would think that, when using nix integration and a custom shell.nix, stack would let shell.nix generate a single derivation and not make some other, separate derivation. I should attempt to follow ocharles' advice. Let us all hope for a world where nix is the singular build tool.
I think there's a difference between "stable" and "ideal." The BangPatterns proposal got derailed by some valid criticisms, but the extension as it stands today would be totally stable and better than nothing. I'd love to see things perfected before being added to the report, but there are plenty of things that I'd be happy to get at all.
I'm not sure we should let the perfect be the rnemy of the good, even for the report.
Right. My point is that many of the proposals seem to have exactly that problem
Did you see the [LANGUAGE stability proposal](https://github.com/ghc-proposals/ghc-proposals/pull/85)? It is rejected because &gt; we cannot predict the future and hence cannot usefully designate some extensions as stable
Wow! What an amazing resource. Thank you!
Experimented with this a bit today, overall it does seem like a more elegant solution. However I still had to hack on flycheck a bit to get it to work in emacs with direnv (fix here -&gt; https://github.com/wbolster/emacs-direnv/issues/17). Also I'm not totally sure I understand what direnv allow is for - a security issue perhaps? Does it persist over time (it seems you have to direnv allow . whenever you change the envrc at least)?
I see. Well, you can write Python in a more functional manner, but you're right that it's sort of going against the grain of mainstream usage. I guess my picture of how things look from a C programmer's was a bit off. Thanks for the correction!
This. Unfortunately the standardisation process is stale (or even dead) because * There is no need for the new standard. Right now haskell is implementation-defined language, and most of the comunity is OK with that. IMO it wont change without an alternative compiler (it should be popular obviously.) * Goals are not clear. Why do we want particular feature to be standardized? Of course, everybody has an answer to this question, but there is no commonly accepted one. * The approach is... hmm... theoretical. E.g. they are not going to standardase GADTs, but require all exrensions to work nicely with it.
GHCHQ tries not to break code, but there are never any promises. I remember when Num lost its Eq superclass, which broke quite a lot of code but which was necessary. Haskell 98 code doesn't necessarily work today because of the AMP. Semigroup-Monoid has only recently played out and that also broke lots of code. All three of those have probably caused more breakage individually than any extension-caused breakage taken totally. So, yeah. I would have to ask: if we formalised the language extensions, cui bono? In terms of reducing practical breakage, my first target would be getting some kind of 'default superclass' mechanism to make refactoring class hierarchies less painful; language extension stability guarantees are pretty far down the wishlist there.
Indeed and the social effect of this change in C++ stewardship absolutely *cannot* be understated. The "semi-fixed" time-table is crucial. It doesn't actually matter in the big picture that some things end up getting postponed, even if it happens several (not *too* many!) times. The point is that people get excited about the new features that *are* delivered, proposers of features have a reasonable way to gauge how much time/effort they have to put in before they know if there's a *chance*, etc. Time-based releases are the *only* way to deliver.
&gt; Why do we want particular feature to be standardized? Stability and dependability, hence the thread. Same reason any feature is standardized in any language, protocol, or any other standard.
Just started Exercism haskell track. I got this simple task of checking an string input if the alpha characters are upper-case, if everything is just a bunch of white-space or if the string is ending in a question mark. I have no idea how to proceed... I expect it to be something like: check str | str ~ "^\s+$" = "all blanks" | str ~ ... | ... If anybody can help me I would be very greatfull 
but there's a schedule for the usual breakages in prelude (deprecating stuff, the AMP etc.) I would be happy with a guaranteed schedule. Also, i think Language Extensions are more complicated, because some of them are not syntactic sugar, but actually provide functionality that's not accessible otherwise and can influence the design of you solution.
Take a look at the base package `Data.Char`. I think you’ll find some helpful stuff there. (Provided you aren’t being asked to implement the functions in `Data.Char`)
Thanks!
Using a regex is one way to do it. But since that's an intro course, they probably intended you to use list functions – String` is just an alias for `[Char]`, so you can use all the usual list functions like `map, filter, foldl, all, any`. `Data.Char` has some useful functions for working with characters like `isAlpha, isSpace :: Char -&gt; Bool`. So you can do stuff like: `filter isAlpha "ab cd" -- "abcd"` `any isUpper "xYz" -- True` So you'd still have a bunch of guards just like what you wrote, but the checks would be done with list/char functions instead of regexes.
[removed]
This sounds good. Are there downsides? Would it be based on a ruling from the council or vote by the community?
Your blog post is great but what does the golden toilet mean
I will be covering parsing as specified by RFC escaping because that's what has been implemented and will be covered in a future post. For CSV we might want create it lazily as the file is parsed, and exploit some instruction level parallelism when creating the index. There is although the option to create and store the index along-side the CSV file so it can be re-used. There are reasons why using another may not be possible (for example the file was provided by a third party) and using a binary format may not necessarily save you from using a lot of memory. For XML &amp; JSON, escape processing can be done lazily when the field is actually access so escape processing doesn't have to be performed for fields that are never processed.
I highly recommend watching kmett's video. I'm currently part way through watching right now. Myself, I've actually learnt a lot on this topic from watching his older videos and then doing my own reading on it. Here's an older video, although the low compression quality will require some intense concentration. https://www.youtube.com/watch?v=uA0Z7_4J7u8 
Comparing the assembly generated by GHC for both versions ([Native.hs](https://github.com/hverr/intset-compare/blob/423c186331f3258dcad35047fe1faca8f7fed417/src/Data/IntSet/Native.hs) and [NativeDiv.hs](https://github.com/hverr/intset-compare/blob/423c186331f3258dcad35047fe1faca8f7fed417/src/Data/IntSet/NativeDiv.hs)) will show that the only difference is the absence or presence of the `divq` (on x86_64) instructions. So all of the performance gain was due to the specialization of the modulo operation.
Generally, functional data structures will be slower than imperative data structures^1. Of course, implementing imperative data structures require imperative programming. A better title might have been *Making Haskel as fast as C: Imperative data structures in Haskell* ^1: An excellent resource on understanding the tradeoffs between various programming models (declarative, functional, imperative and many more) is *Concepts, Techniques and Models of Computer Programming* by . He states in Chapter 4: &gt; Is declarative programming efficient? There is a fundamental mismatch between the declarative model and a standard computer, such as presented in [146]. The computer is optimized for modifying data in-place, while the declarative model never modifies data but always creates new data. [...] Now we can answer the question whether declarative programming is efficient. Given a straightforward compiler, the pedantic answer to the question is no. But in fact the practical answer is yes, with one caveat: declarative programming is efficient if one is allowed to rewrite the program to be less natural. Here are three typical examples: &gt; &gt; 1. A program that does incremental modifications of large data structures [...] cannot in general be compiled efficiently. [...] &gt; 2. [...] &gt; 3. A function that implements a complex algorithm often needs intricate code. That is, even though the program can be written declaratively with the same efficiency as a stateful program, doing so makes it more complex. This follows because the declarative model is less expressive than the stateful model. [...]
It's an example of something where it's cost greatly exceeds its utility. Sure, it works and does the job, but considering how much we'd have to pay for it to get the job done, are we sure there isn't a cheaper way?
Would it be reasonable to fix in GHC? At least for constants.
&gt; For XML &amp; JSON, escape processing can be done lazily when the field is actually accessed so escape processing doesn't have to be performed for fields that are never processed. Do you memoize? If so, you'll blow up your memory footprint if you access a significant fraction of the file. If not, you'll throw away performance if you access the same spot of the file multiple times. &gt; If there is a bug is in the binary translator, you will have to fix the bug and run it again and it is fairly costly and slow if you have many GBs or TBs of compressed CSV/JSON/XML file. If there's a bug in your bit-string generator, you will have to fix the bug and run it again and it is fairly costly and slow if you have many GBs or Tbs of compressed CSV/JSON/XML file. Just convert to a ProtocolBuffer, no rank-select trickery involved, and the costs in computer time are similar. You'll likely save programmer time, because they can use more normal accessors.
It also looks like I can't get warnings about unused imports when I do an implicit import.
I recon it is (or certainly very fast): When compiling using GCC 7.3.0 using `-O2`, the following assembly is generated: ``` _int_set_add_in_bounds: movq 16(%rdi), %rdx movq %rsi, %r8 movq %rsi, %rcx shrq $6, %r8 andl $63, %ecx movl $1, %eax salq %cl, %rax orq %rax, (%rdx,%r8,8) ret ``` On a side note: I discovered a nasty bug in my C code. When compiling the following line with GCC without optimizations: ```c uint64_t i = ... uint64_t mask = 1 &lt;&lt; i; ``` GCC will assume that 1 is a 32-bit integer, and will execute the shift instruction on a 32-bit register, before sign-extending the result to 64-bits (`uint64_t`). Thus if `i` &gt; 31, the result will be bogus. What should have been written was: ```c uint64_t mask = (uint64_t)1 &lt;&lt; i; ```
[removed]
&gt; Do you memoize? If so, you'll blow up your memory footprint if you access a significant fraction of the file. If not, you'll throw away performance if you access the same spot of the file multiple times. Yes you are correct. Memoization is very convenient if you're careful to not touch a significant part of the file, but does represent a memory issue. A memoized document that is fully evaluated will put you back into square one and you use as much memory as traditional parsers. The API I will eventually discuss exports cursors which don't have this problem and then has a memoized layer on top so that you could have the convenience for small chunks of your parsing. This gives you fine grain control over what you want to memoize and what you don't. &gt; If there's a bug in your bit-string generator, you will have to fix the bug and run it again and it is fairly costly and slow if you have many GBs or Tbs of compressed CSV/JSON/XML file. &gt; Just convert to a ProtocolBuffer, no rank-select trickery involved, and the costs in computer time are similar. You'll likely save programmer time, because they can use more normal accessors. If you store all your data in protocol buffers and load everything in the file into memory as a single document you still will have a memory issue. Also succinct data structures are surprisingly fast. Especially if the rank-select bit-strings fit in your L2 cache. 
[removed]
To elaborate further on the problem with the protobuffers strategy is that it does not handle large messages well. Even the authors of the protocol discourage its use for large messages: "Protocol Buffers are not designed to handle large messages. As a general rule of thumb, if you are dealing in messages larger than a megabyte each, it may be time to consider an alternate strategy" https://developers.google.com/protocol-buffers/docs/techniques Yes, in fact we do have such messages. 
Hello! I'm trying to use dependent types within Fix point types, in a catamorphism. For example, during a matrix multiplication I would like the dimensions of the outer matrix to be compatible with the inner matrix's dimensions - i.e. m x n should be followed by a n x j matrix. However I can't find a way to express this on the type level; clearly using (Matrix m n) as a functor won't work due to the type of Fix: newtype Fix f = Fx (f (Fix f) data Matrix m n k instance Functor (Matrix m n) where .. I thought of trying to define a different type of Fix, something like the following (but was unsuccessful): Fox f g = Fox (f (Fox g f)) Could someone help provide some solutions or tips? Thanks loads.
&gt; If you store all your data in protocol buffers and load everything in the file into memory as a single document you still will have a memory issue. You can access protocol buffers directly off disk, no need to load any part that isn't accessed.
Cap'n Proto or FlatBuffers can simply be mmap()'d and accessed directly without any parsing, and can handle very large messages.
Do you have any CPU &amp; memory benchmarks for this? It also sounds interesting.
Unfortunately no good implementations for Haskell :/
Nah, sorry. I actually don't have to deal with data sets where XML or JSON (de-)serialization has ever been a significant overhead that we needed to reduce at work. And, for "play" I'm more interested in Type Theory and Generic Programming, and not "big data" of any kind.
That's cool. We went the succinct data structure route because at the time there weren't any viable options from Haskell. We even benchmarked memory and CPU usage for the succinct data structure XML parser against other document and streaming alternatives and found the the succinct data structure library had better performance and memory profile. The solution we have is not too shabby and we're still on the lookout for more optimisations for succinct data structure parsing in general. There are also other reasons why this this solution was ideal for us. In the big data space the data we get is very dirty. We get things like: * truncated XML files * irregular files with wildly varying "schemas" because of a decade of receiving data from submitters of different versions * XML files that lie about what version they are And I say "schemas" because we didn't actually get a schema at all and the process of reverse-engineering the various schemas was quite involved and unifying them into something suitable for storing in a binary format is quite involved. We do actually generate a binary format for consumption by other tools, but the number of iterations we had to take to get this right mean that high performance, low memory textual document parsing was extremely helpful.
I believe all DOM rendering is done within `requestAnimationFrame` by default. So maybe postBuild represents an event that will let you start rendering in the next animation frame? i.e. maybe postBuild for a dom rendering block waits until after the animation frame to fire, and any DOM drawn after that will be put into the next animation frame. II'm not sure on any of this, so you'll probably want to double check me in the `ImmediateDomBuilder` implementation.
GHC does support cross compilation, though it's not exactly a dream to work with. Nix provides a pretty good cross compilation story, but that comes with the learning curve of Nix. /u/angerman has some [good talks about cross compilation](https://www.youtube.com/playlist?list=PLbjcAmsCYuS4xspQb5BHnIHhi4BrWteGz) that might give you an idea of how you want to approach this.
/u/ocramz I think this package from /u/int-index solves problem with arbitrarily long products: * https://github.com/int-index/union
Maybe as an alternative it's possible to create some fancy operator like `::&gt;` (or probably better) so instead of writing fn :: "param" :! a -&gt; Result b Users can just fn :: "param" :! a ::&gt; b But probably it will complicate logic much more...
I was so annoyed by `&gt;&gt;&gt;` that I patched my `doctest` to support this. I do not really use Haddock, and for illustrations in posts λ is fine, so I am happy with it.
Thank you for your efforts! As as `alga` user I'm looking forward to your results :) Side question: does anyone know where we can track the progress of [the various GSoC Haskell projects](https://summer.haskell.org/news/2018-04-23-accepted-projects.html)?
Hi, /u/mahsa_106l/ 😊 Your question is more appropriate for the monthly Hask Anything thread which is pinned to the top of the subreddit. Could you please ask it there, instead? Also, please note that it would help immensely if you formatted your code using a monospace font, by indenting the entire block by 4 spaces. 
This is a general issue with a range of msys distros. for some reason they don't come with the right keys. When I was last trying to figure this out, I found the following magic sequence got me up and running with keys and the like. I suppose in your case you can preface everything with `stack exec` to run it under stack? pacman-key --init pacman-key --populate msys2 pacman-key --refresh-keys pacman -Syu restart msys2 
btw, the session 7 on youtube is missed (sorry for OT :D
MonadFail proposal yet to come. A lot of code using partial pattern matching will start to hemorrhage warnings...
I don't really have an explanation for why that doesn't work, but you can enable the [MultiParamTypeClasses](https://wiki.haskell.org/Multi-parameter_type_class) GHC extension and add the state and event types as two more params to FSM. {-# LANGUAGE MultiParamTypeClasses #-} class FSM m s e where update :: m -&gt; s -&gt; e -&gt; m data State = Stopped | Started data Event = Start | Stop data Obj = Obj { state :: State } instance FSM Obj State Event where update obj Stopped Start = obj { state = Started } update obj Started Stop = obj { state = Stopped } and as an addition to that , as long as each `s` and `e` can always be determined by the `m` used, you can add [FunctionalDependencies](https://wiki.haskell.org/Functional_dependencies) to type check that the correct types are always used together {-# LANGUAGE FunctionalDependencies, MultiParamTypeClasses #-} class FSM m s e | m -&gt; s e where update :: m -&gt; s -&gt; e -&gt; m 
If you want to see how a functional Tic Tae Toe console game looks like, check my implementation among many. It uses as many functional features as possible in trying to avoid imperative style. The logic is completly separated in a state monad. Unfortunetly the input and the response is still mixed with the output inside a IO monad. The next version will be comonadic FRP and this is as "functional" as you can get [https://github.com/raducu427/tictactoe](https://github.com/raducu427/tictactoe)
Hey, raducu427, just a quick heads-up: **completly** is actually spelled **completely**. You can remember it by **ends with -ely**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Great ! A version 0.2 will normally come out soon ! Sadly there isn't any simple way to track the progress of GSoC projects. Students are only invited to write blog articles, but many does not. If you are particularly interested in one project, feel free to email the concerned student: I think it is safe to say that it will be happy to answer :)
My perrenial repost of https://github.com/haskell/rfcs/issues/15
Hello haskellers! I've been struggling with `Lens` as of lately, and currently I am stuck on trying to do `foldMap` "inside" a lens composition. I am sure this is trivial but I am lost in the ecosystem. I've got two types and would like to produce the following getter data Foo = Foo { _bar :: \[Bar\] } makeLenses ''Foo data Bar = Bar { _baz :: \[Baz\] } makeLenses ''Bar data Baz = Baz myFunction :: Getter Foo \[Baz\] myFunction = ... That is, I want to do something like `foo ^.. bar . "concatStuff" . baz` I have looked at folds like `foldMapOf folded` but I have not been able to type check. Any hints? Cheers!
In addition to u/ChrisPenner's answer, if you draw the structure of the tree, you see if looks like a tree with a central trunk with fingers/hands coming out each side (and the deeper the tree, the more the hands look like "normal" trees). At least that's my mental model of them.
It's for a call-by-value language (OCaml). For lazy evaluation try https://koko-m.github.io/GoI-Visualiser/ It's a bit less "user-friendly" but it handles all evaluation strategies in a provably optimal way. 
Self plug: https://gilmi.me/blog/post/2018/07/24/pfgames You will need to get more familiar with building Haskell to make a game, but it's possible. www.haskellbook.com can help you get there.
You could run the search in a Web worker to avoid blocking the page? 
Ah, my mistake. Sorry, I confused it a little bit. Sometimes happens when you're sleepy on the mornings :(
Awesome post! I have a rough understanding of rules but inline pragmas are a science to me - I try stuff and measure until it kinda works. This is in large parts because I don't understand how they interact. Like {-# INLINE[1] foo #-} foo = ...bar... {-# INLINE[~2] bar #-} bar = ... Does that mean the bar in will never be inlined? Also, how does phase control interact with rules generated by spec const/worker wrapper etc?
First game exercise from our course: https://gitlab.ub.uni-bielefeld.de/FFPiH/uebung2017-4 The follow-ups (parser, mtl, GUI,...) Are not open as they contain the example solutions of the previous, but I could mail you a zip of them if interested.
Woops. Misclicked the export and left it private. It should show up shortly.
I would expect `folded` to work in ths case.
This is a good tutorial (also contains links to other tutorials): * http://www.andrevdm.com/posts/2018-04-02-haskell-rogue-like.html
Very nice article! The problem with rewrite rules i think is ensuring their correction. I've been having the idea that in a dependent language like Idris, Coq or Agda, it would be possible to use proved preservation rules (like functor laws) as rewrite rules.
I would definitely recommend Elm as a starting point to writing games in an FP language! :-D Here's a couple examples of mine if OP is interested: - [Haze](https://github.com/basile-henry/haze): Infinite maze game, not sure it's the most readable code after some attempt of mine at making it faster :-/ - [Peg Solitaire](https://github.com/basile-henry/peg): Tiny solitaire game to experiment with drag and drop mechanics Hope this helps! :-)
I agree that a visual explanation would have been better, but I was too lazy to draw the diagrams. I like the way you've described it though, let me try to mention this in my post.
You are right, correction is very important, because RULES are imported even in the user's code. Giving a proof of it can be great for sensible libraries! (From memory there is some proofs for some RULES of \`map\` on the wiki, but I don't know if it was done formally).
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](http://hackage.haskell.org/package/base-4.11.0.0/docs/src/GHC.Base.html#map) - Previous text "map" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20e3gr8iz) 
[removed]
Sorry, I meant [~1], always get confused about which is exclusive. main :: IO () main = print foo {-# INLINE[1] foo #-} foo :: Int foo = bar 3 {-# INLINE[~1] bar #-} bar :: Int -&gt; Int bar i = i + 1 Checking the code is a good idea, had a look at ddump-simpl-iterations. foo get's optimized but the unfolding of foo doesn't because that might allow better optimizations later: -- RHS size: {terms: 2, types: 0, coercions: 0, joins: 0/0} foo [InlPrag=INLINE[1] (sat-args=0)] :: Int [LclId, Unf=Unf{Src=InlineStable, TopLvl=True, Value=False, ConLike=False, WorkFree=False, Expandable=False, Guidance=ALWAYS_IF(arity=0,unsat_ok=False,boring_ok=False) Tmpl= bar (GHC.Types.I# 3#)}] foo = GHC.Types.I# 4# Then we inline the unfolding of foo but we are in phase 1 and bar can't be inlined anymore: main _s26f= ... case bar (GHC.Types.I# 3#) of { GHC.Types.I# ww3_a26u -&gt; ... 
Hello everyone, I have been "learning" Haskell on and off for a couple of years now. Currently I am in graduate school and most of my time is running simulations (statistical physics). My simulations are in C++ and I chose it for its speed. However after going through Learn You A Haskell, I am amazed at how concise (and bug free) you can be. So I was wondering whether Haskell has some good numerical programming support (mostly matrix multiplication). Also, I was wondering how one gets around the immutable data types in Haskell. For instance, I sometimes have to modify elements of a matrix. Thanks and I apologize if these questions are too "nooby". Cheers. 
The type of `update` is `forall m s e . FSM m =&gt; m -&gt; s -&gt; e -&gt; m`. This means that `s`, in particular, is (implicitly) _supplied_ by the caller. It is an _input_ of the function, about which you can't make any assumptions, since it is arbitrary.
&gt; a small but important part of why its concurrency story is good Care to elaborate? I'm guessing you mean `par`, `pseq` etc. but I don't see why you couldn't do something similar in a strict language.
We actually do both. We use the succinct techniques while transforming data into a uniformed binary representation. That is, while your "just convert to a protobuf" phase, where you still need to read that XML. Imagine that you have a 300+MB XML file and you want to transform it into something. Now imagine that you have lots of them, with slightly different schemas. You want to transform them into some uniformed binary (perhaps even protobuf, although we use different) representation for future use. You want to do it fast. And you want to do it cheaply. We only deal with a subset (however large) of data in these XML files. There is more data in there which we don't use and we don't know how to make sense from, so we can't come up with a schema that correctly unifies this data. Therefore we _know_ that we will be coming back to the original data in the future. Not necessarily because of the bugs, however, we experienced that too :) This is _one_ of the problem we are dealing with. We measured a number of XML parsers and techniques, in different languages (including C, Scala, Haskell, even JavaScript ;)). We've tried different infrastructures for scaling this stuff (AWS Lambdas, Apache Samza, EMR, custom clusters). Succinct stuff works really well now compared to other alternatives when it comes to parsing XML files into something else. And there is room for optimisation, still. &gt; If there's a bug in your bit-string generator, you will have to fix the bug and run it again and it is fairly costly and slow. Not necessarily. We actually were saving indices separately for the future reuse, but we have found that building indices is a very quick operation, compared to the rest of the work (loading, performing the transformation, encoding the result binaries, etc.). So _for now_ we have even switched off persisting indices and simply build them on the fly. We may review this decision in the future, but it is like that now. &gt; Just convert to a ProtocolBuffer, no rank-select trickery involved, and the costs in computer time are similar. This is where I am a bit lost: the cost of converting to ProtoBuf is similar to what? We need read that massive amount of XML and to convert it first, and we need to do it quickly and cheaply. This is exactly what we use XML parser for (converting from XML to binary), and where we see our wins. 
&gt; Just convert to a ProtocolBuffer, no rank-select trickery involved, and the costs in computer time are similar. Another thing is, once you have that fast XML parser with performance characteristics similar to ProtoBuf, as you claim, why would you even cared to convert the data to ProtoBuf (work, time implied) and store it once more (cost implied)? If perf is similar, and you have a parser, why not just using it and make no unnecessary transformations? :) 
See [u/semi225599's answer](https://www.reddit.com/r/haskell/comments/93t0ri/pattern_matching_rigid_type_variable/e3gibv1) for how to solve the specific error. In particular, the `MultiParamTypeclasses` and `FunctionalDependencies` features allow multiple independent member types. &gt; how to achieve what I'm aiming for I'd be interested to see what you are aiming for. It _looks_ like you're writing OO code using Haskell — which is possible, but usually not the best way to write Haskell code. If you're interested, I'm sure somebody will happily chip in.
Ah, this is fun, I thought `bar` will be inlined. You are right, all of this has to be used with care !
I would make the records fields strict when we don't need them to be lazy, this way we have better performances, we prevent some space leaks, and we prevent some exceptions to pop up at unexpected locations. Also, instead of using "case ... of Left a -&gt; (...) Right b -&gt; (...)" you can use the Data.Either.either function, it makes the code more readable I think. Same for "case ... of Nothing -&gt; (...) Just x -&gt; (...)" where you can use Data.Maybe.maybe Other than that, I like the idea of the program! Out of curiosity, why did you chose a postresql database to store the temperatures?
I'll mention my own game engine which features music playing and the possibility to have multiple players connected through internet : https://github.com/OlivierSohn/hamazed 
I'll add a suggestion to use docker, you have 3 now :) I tried to make my project (haskel / c++) build on Heroku but couldn't make it work (I don't remember exactly why(. Besides, using docker is a more generic solution, so if I want to use something else than Heroku, it's easier to change. Hope this helps!
Thanks! I'll try your suggestions. I chose PostgresQL so I can query the JSON I stored along with the temperature, so I can experiment with it. PostgresQL is my default choice when it comes to databases, unless there is a compelling reason not to choose it.
The first four lectures of http://cis.upenn.edu/~cis194/ build towards writing Sokoban in Code World (https://code.world/haskell). Code World is a great environment to get something going quickly, including multi-player abilities.
Ok, It turns out that's the one I use in my project too, but had forgotten about it :)
I changed the data types into newtypes. Does that have the same effect?
[Nikki and the Robots](https://github.com/nikki-and-the-robots/nikki) is nontrivial 2D game. 
How is that different from `(,)`s?
You've seen this post? https://www.reddit.com/r/haskell/comments/80u5uq/build_large_polyglot_projects_with_bazel_now_with/
For a single comma, it's not different. But the Haskell comma has magical hardcoded behavior that gives us tuples, where `(a,b,c)` is *not* the same thing as either `(a,(b,c))` or `((a,b),c)`. This results in [stuff like this](https://hackage.haskell.org/package/product-profunctors-0.10.0.0/docs/Data-Profunctor-Product.html#v:pT62), since each tuple is a distinct type not composed of simpler tuples; in contrast, with a binary product, `a X b X c X d` is really just a combination of a bunch of simpler things as opposed to a new, magical hardcoded "4-product", so unlike that link, you'd only have to have one instance: "my thing respects the binary product", and you'd get n-ary nested products for free. For some nearly undocumented thoughts on how you can take this and run with it, [check out my Agda post on instance search with sums and products](https://identicalsnowflake.github.io/InstanceSearch.html).
Basically, yes. 
I am just trying to start hs, it would be useful to every one if properly commented
Agda can be used to formally prove list or vector type: [An Introduction to Programming and Proving in Agda (incomplete draft)](http://www.cse.chalmers.se/edu/year/2017/course/DAT140_Types/LectureNotes.pdf) by Peter Dybjer.
psst, it's a wiki :)
It's a bad reason to use a software because other people use it. It's still proprietary software. And by introducing a channel for functional programming on Discord, you just confirm those people who use Discord.
[@Int's latest tweet](https://i.imgur.com/Y2HPWsi.jpg) [@Int on Twitter](https://twitter.com/Int) - ^I ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
Thanks for mentioning your experience. I am definitely going to be migrating to docker after attempting (and failing) to link my project with a local install of tesseract. If I can't even do that easily, I can't trust it to work, especially when it takes 6+ minutes to test a solution through Heroku. I wonder if you ended up trying docker at any point, since I'm unsure of the difference between ```stack build --docker``` and ```stack image container```, but I'm sure I'll figure out a solution. 
noob question with barely any coding chops under my belt but i want to toy around with things... would it be easier and cleaner to just download an image of Ubuntu and do all fiddling with haskell within it (installing GHC, stack and all that) or screw all of these extra steps and just install haskell platform on my Windows 10 machine? seems like everything i read about haskell on windows is a PITA....
[`Data.List`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-List.html) and [`Data.Char`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Char.html) have everything you need. You can use [`filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-List.html#v:filter) along with [`isAlpha :: Char -&gt; Bool`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Char.html#v:isAlpha) to filter out only the alpha characters and then use [`all :: (a -&gt; Bool) -&gt; [a] -&gt; Bool`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-List.html#v:all) and [`isUpper :: Char -&gt; Bool`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Char.html#v:isUpper) to test them. Similarly, you can use `all` with [`isSpace :: Char -&gt; Bool`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Char.html#v:isSpace). There is also [`isSuffixOf :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-List.html#v:isSuffixOf). Putting it all together: import Data.Char import Data.List -- Remember String is an alias for [Char] alphasAreUpper :: [Char] -&gt; Bool alphasAreUpper = all isUpper . filter isAlpha allWhitespace :: [Char] -&gt; Bool allWhitespace = all isSpace endsInQM :: [Char] -&gt; Bool endsInQM = isSuffixOf "?" main :: IO () main = do l &lt;- getLine print $ alphasAreUpper l || allWhitespace l || endsInQM l
If you have any questions, let me know. Also checkout the #haskell-beginners channel on the FP Slack ([https://fpchat-invite.herokuapp.com/](https://fpchat-invite.herokuapp.com/)).
Indeed I have. Honestly I am hoping something exists that is much closer to zero-touch - after all all the information is available via the cabal files and cabal's database without any need for (manually) constructing a BUILD file. I know this is unlikely.
Question about the Haskell Report. I would like to help contribute to the Haskell Report. Not Haskell 2020 (if that is still a thing), but maybe the next one? So keep in mind the following questions have the scope of at least a few years: * What should I read up on in order to be knowledgeable and useful in making this sort of contribution? * What sorts of contributions should I get involved in now, that will be relevant to the Haskell Report later? * Do I need to be on the committee to contribute? If so, how do I get on the committee?
Haskell's RFC process is supposed to be inspired by Rust's RFC process. There was a talk given about Rust's RFC process not a long time ago - [why wasn't I consulted](https://www.youtube.com/watch?v=rdmpOktHLmM). From what I remember, they have team which must reach consensus; anyone on team can raise objection which must be resolved and they have final, limited, time period in which they can't make decision based on argument, that wasn't already presented.
Why even have `|` in that comment? `|` means Haddock so I’m guessing removing that will help.
&gt; "s should be allowed to be any type, why can't it be some State type?" That would be true if s where *existentially* quantified. All type variables in Haskell are *universally* quantified, so they have to work for *every* type. (There are tricks to implement existential quantification via universal quantification, if that's what you need, but those are in GHC extensions.)
Good bot.
Thank you, agcwall, for voting on CommonMisspellingBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Good human.
Good good human bot bot.
The recent info (from this week) not that many packages (anymore). Only 16 packages from the dependency closure of `pandoc`, `warp`, `snap`, `darcs`, `yi`, `lens` + something more.
Thanks for your answer. A follow-up question: How much one need to undertstand the advanced stuff in Haskell to be able to start writing "real" programs, not the tutorial-type ones? For instance, LYAH touches upon `IO` much later, which is usually a sign that the stuff is advanced. Cheers. 
Around when can we expect to hear back from you guys?
I think you can get going writing real programs before really understanding what you are doing. You will miss out on some of the more beautiful ways to frame problems, but getting comfortable enough with the IO monad to start using haskell like a typical imperative language is not that hard. For example: [http://blog.sigfpe.com/2007/11/io-monad-for-people-who-simply-dont.html](http://blog.sigfpe.com/2007/11/io-monad-for-people-who-simply-dont.html)
You need `IO` to do most useful things. `ST` is slightly more complicated, and `hmatrix` seems to rely on it. Fortunately, in Haskell, I've found that cargo-culting is comparatively safe, and the type checker yells at you when you do something wrong. I don't think `IO` is particularly advanced. It's kinda like understanding what the heck `public static void main(int argc, String[] argv)` is in Java.
I've tried to make a game before and it was quite difficult. Though if I were to try again I think I could get more progress. My advice: * Figure out which GUI backend works for you. Different backends have different approaches and I found that I was trying to switch between them, costing me a lot of time. * If possible, make the game on Linux (or OSX). Maybe it's just me, but I had a difficult time getting the GUI backend stuff to work on Windows. It was a lot smoother on Linux. * Don't worry about FRP, at least for now. When I started I spent a long time learning about FRP and which FRP packages to use. Unfortunately I don't think the FRP packages have matured enough to be useful. A lot of them have completely different styles and a lot of them haven't been updated in a while. * Your GUI backend will probably encourage you to do the time/GUI stuff in the do-notation (monadic style). 
Classic example of a simple 2d Haskell game: [lenses pong tutorial](https://github.com/ekmett/lens/blob/master/examples/Pong.hs) Fair warning : Lenses can be incredibly intimidating if you go too deeply into the documentation, but, in the context provided here, they're just used for property access and manipulation for record types. Sort of like "getters" and "setters" in OOP. For the most part, I suggest taking a look at this example because it's a simple, self-contained, single file example of a game using Haskell that has some pretty readable code, the fact that it resides in the lens repository is ancillary. 
FunDeps question - In the above, if you wanted: `s -&gt; e`, but had no opinion about possible variations of `m`, would that require an additional extension, or can fundeps handle the idea of one type not necessarily implying the other two?
[CodeWorld](https://code.world) includes a very easy framework to make simple networked games.
MSYS2 uses pacman. It should also come with make though.
Thanks, very nice description!
I would say don't pay attention to those who say it's PITA. Most haven't tried a recent release of the compiler since GHC 7. Just install platform and try it. If you find any issues report them. The port will never get any better if all people say is to "use Linux or WSL". At this exact point in time, the only things that are definitely broken are things relying on the unix library.
[removed]
thanks a lot, i'll try that first then.
my interest spinning up a VM is that i can keep things relatively simple and clean on my host windows box so i'd rather avoid WSL in itself
[removed]
[removed]
[removed]
Yes, Text is better than String : https://www.reddit.com/r/haskell/comments/3rrlih/text_or_string/ in Haskell, everything is lazy by default, and note that strict fields are not always better for performance, sometimes it's better to be lazy! Sometimes...
Looks like the formatting is off, I found that triple backtick is not supported here, however you can add 4 spaces before a line to make it appear like it's code!
&gt;To help figuring out such optimizations, GHC inlines your code. Because Haskell is pure, one can replace a function by its definition. I am not sure if being pure has to do anything with this. As far as I know, you can replace impure functions calls with their definitions as well without changing the behavior of the program.
This sounds cool. I'd suggest using ghci and just reloading the files that have changed but I'm not sure that scales well.
About "making ghc as fast as ghci", you could try the `-fno-code` flag, which keeps type-checking but suppresses code generation. https://downloads.haskell.org/~ghc/7.8.2/docs/html/users_guide/options-phases.html#options-codegen