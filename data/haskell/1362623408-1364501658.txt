&gt; "A value of type STM a is a transaction that returns a" This doesn't sound much like a denotational model to me. I think my approach to coming up with a denotation for STM would start with observing at least the following properties: * There are mutable references involved, so I would probably refresh my memory on existing denotations for languages with mutable references. * There is the possibility of nontermination (since you could always just `retry` forever), so the denotation would include some way of expressing that. If the model I come up with doesn't already have that ability, I could add it with a `+1`. * Since STM deals with transactions, the denotation (perhaps strangely) doesn't actually have to deal with concurrency! This is a pretty awesome property. I can probably model it as a sort of state transition without suffering from the same problems that the state transition "model" for IO has. * Nondeterminism comes into play, since I have choice in the form of `orElse`. Including this in the model might also cover the possibility of failure, which I already mentioned earlier. Edit: This doesn't actually sound like such a simple model, to me! This often happens when you try to define the semantics for something that already exists.
Don't worry, this was very helpful. This seems like it resembles a lot of my recent thinking on implementing functional reactive programming (not the specific stuff on vector clocks, but the general approach to design). For example, when I design some concurrent interface to some sort of game logic, I don't start by thinking about what the internal representation of the game logic should be. I instead just ask myself: What are the commands that the client uses to interact with the server and what do they mean? A toy example I like to use when testing FRP ideas is some server that can accept multiple clients. It displays a counter for each connected client and each client can either increment their counter or quit, removing themselves from the server display. So when designing the API for a client, I just define an init command: init :: Command (Command (), Command ()) And the meaning of init is (very vaguely) a command that initializes the interaction with the server, and then returns two commands, the first of which increases the counter, the second of which quits. Then once I've specified that behavior I can judge the correctness of the implementation by how well it maps to that meaning I've established. More specifically, I can then assign meaning to client interactions. For example, for the following "session": do (inc, quit) &lt;- init inc inc quit ... I can say that "means" that the client has connected to the server, incremented their counter twice, then quit. However, all of that is still vague and I'm reading that book you linked me to sort of understand things more rigorously.
Yeah, I think rigor is the main thing this explanation lacks. I think it still feels like you have started with the interface you want to have rather than a model that works well for the problem. The former is what, I think, tends to result in constructing a set of primitives without much intrinsic meaning and then turning them into some sort of free structure to give the interface some nicer properties.
I don't think that would have helped much. The real problem is having a powerful and easy to use interface as well as having it be more standardized so people can interoperate. Actually implementing the individual libs is easy compared to that.
What is being more standardized?
Well, hopefully one day people will come to a consensus on this steam library stuff so people don't need to write different versions of the same libs.
Not sure how you maintain that collection. But everything by this user will be about haskell, http://www.youtube.com/happstack also have some other videos here: http://www.youtube.com/watch?v=7Wmszk4wZxQ http://www.youtube.com/watch?v=ok_gf0QVLnE A search of your channel didn't seem to turn any of those up. but, looks like you have quite the collection overall! good job!
Probably the more interesting bits in here would be the `getEvent` implementation and the discussion of ignoring `inotify_rm_watch`'s `EINVAL` error condition in the source comments.
Probably the most interesting things here would be the discussion of [ignoring `inotify_rm_watch`'s `EINVAL`](https://github.com/lpsmith/linux-inotify/blob/787f5c01c7e634f55e48fd91bbe0f2c06333222a/src/System/Linux/Inotify.hsc#L170), the idea about an alternative Haskell representation of [watch descriptors](https://github.com/lpsmith/linux-inotify/blob/787f5c01c7e634f55e48fd91bbe0f2c06333222a/src/System/Linux/Inotify.hsc#L68), and possibly the implementation of [getEvent](https://github.com/lpsmith/linux-inotify/blob/787f5c01c7e634f55e48fd91bbe0f2c06333222a/src/System/Linux/Inotify.hsc#L221).
Surely there is no place for the () to be lazy in the first place. I have a feeling this is "just in case" voodoo.
Got 'em. Thanks!
In this case, I think you've clearly picked the wrong side of that balance. The single fundamental idea you're trying to get across here is that in Haskell, the notion of "function" doesn't include stuff like effects, state, or a definite observable time when that function is "called". That Haskell uses a different word for that kind of stuff, and separates the concepts. But when you call main a function, what you mean is that it's an action, with effects, state, and an observable time when it's called. That's a direct contradiction to the central point of your article. It's true that most of your readers will be familiar with main as a function... but this is exactly the *same* confusion that leads them to want functions in other places where actions are called for. In this case, if you want to accomplish anything, confronting that confusion is necessary.
I'll stick up for the opposite view for a little bit. Haskell doesn't have functions of zero arguments, in exactly the same sense that it doesn't have functions of two arguments. And yet it's useful to build an abstraction where talking about the arity of a function makes sense. However, in this case, I completely agree that calling main a function was reckless... not because it's inherently wrong to view values as functions of zero arguments, but rather because the people the article is talking to will be *looking* for main to be a function, and to them, the word function will mean a lot of other things: that it could have effects, that there's some discrete action of "calling" it... and these are the things we do *not* want to imply.
I always think a program like this: if True then &lt;correct expresion&gt; else &lt;expresion that gives a type error&gt; This program is always well-behaved but doesn't pass the typecheck Edit: sorry, not related to GHC
Yeah, I really liked that paper. I'm glad you liked it too. 
For those wondering why I need quasi-quotation support for Objective-C: https://github.com/mchakravarty/language-c-inline
Sounds like a great goal. What's missing is someone willing and able to do all the integration work and testing...
I believe hlint issues a warning for code like this.
To Unsafe.hs which is reexported from Heterogenous. 
It's to shut HPC up. If you don't force the () then it complains that the unit value returned from this function is never used. If you look closely there are quite a few little tricks I use to get rid of false negatives from HPC :(.
...and agreement on what the core design principles should be.
Just nitpick: Several places in the readme have `linux-notifiy` instead of `linux-inotify`. Otherwise, seems like a nice library.
djinn sometimes produces non-unique solutions. It's clever in that it tries to use all its arguments but it does not always give you what you want.
Cool, I like this idea!
&gt; for now we will focus on the abstract syntax, and deal with concrete syntax later... ...followed immediately by a concrete Haskell data type, and much mucking around with fiddly details of concrete Haskell syntax. OK, I understand what you are doing. That concrete type is a model for a small piece of abstract Haskell syntax. But perhaps just a few more words of explanation about that would be in order in an introductory text?
The GHC cross compiler to Android is working fine. You can install most cabal packages with little or no changes. As for actually writing useful apps, that requires more. The first step is a fully functional JNI binding, which I have nearly completed and will release soon. With JNI you can call any Java methods you want, so you can write full Android apps with nothing else. But it’s a little cumbersome. Step two is to create Haskell bindings for the entire Andriod API. I’m looking in to automating as much of that as possible because the API is huge.
Here is a fix for that http://www.reddit.com/r/haskell/comments/19qt0p/complete_parser_and_quasiquotation_support_for/
I wrote some benchmarks just to make sure I wasn't embarrassing myself (or, more to the point, Greg) with my work. It turns out the benchmarking real-world IO code is tough! Criterion is terrific at evaluating whether one function implementation is better than another in nanoseconds, but it's a poor fit (ha!) for making decisions where I/O time dominates. Even just talking to localhost on a massive sever machine with more spare CPU cycles than Deep Thought still involves context switches and the vagaries of different processes being scheduled if as and when. "non-pure" indeed. Let's go with "slightly" faster. Which is to say that a library built on io-streams doing non-trivial work performed quite well when compared to a similar full-service library, and meanwhile profiling showed that allocation was sane, performance was good, and there was no leakage even when huge data volumes were pumped through it. Which is precisely what we ask of a streaming I/O library. Nice work, Greg. AfC
My ghc cross compiler build tools are on github: https://github.com/neurocyte/ghc-android
I looked for something like that a few times. I expected it to be called `compose` :)
&gt; This language was inspired by twelf, haskell and agda. Sweet! I am familiar with the first, and I love the last two. I have many questions. &gt; Your programming language should be turing complete - totality checking is annoying. [...] Now any term must be terminating. Isn't that a contradiction? Or are you saying that some parts of the language must terminate, but not others? &gt; -- we can define subtraction from addition! &gt; &gt; query subtract = add (succ (succ zero)) 'v (succ (succ (succ zero))) But you're not actually defining subtraction here, are you? You are asking the proof-search to find the answer to a particular subtraction. Is either subtract or v' available in the rest of the code? With what type? &gt; Some basic IO: Using unix pipes, this Caledon can be used more seriously. Somebody plz write a wrapper? I'm not sure I understand what you want this wrapper to do. Are you saying that the query which follows that paragraph will perform the IO operations it implies, but the proof search will also print a lot of junk which the wrapper would have to remove? &gt; Shell commands: &gt; &gt; defn ls : string -&gt; string -&gt; prop not "ls : string -&gt; IO string -&gt; prop"? Does your system consider shell commands to be side-effect free, or is your IO system very different from Haskell's? (edit: formatting)
One of the things that Greg suggested I change in the API signature of **http-streams** was taking the client-to-server stream and changing it from `OutputStream ByteString` (which is what he hands to me after I open the socket) to `OutputStream Builder`. It took me a while to get my head around the wisdom of this, but it turns out that under the hood the internals of **io-streams** co-operates _very_ well with the internals of **blaze-builder** and **bytestream** to accumulate and then stream appropriately sized chunks of data, and prevents the problem of sending zillions of little strings one by one.
Ok, so here's something that confuses me. So what if the implementation perfectly matches the desired model? The reason I'm asking is that the `ProxyCorrect` implementation is correct by construction and exactly matches the set of supported features I had in mind. Like, I can understand the utility of this denotational design when the underlying implementation does not match the desired denotation, such as the `ProxyFast` implementation which has more power than I actually want to expose, but I use that extra power for efficiency reasons. But when I also have an implementation that exactly matches the desired feature set, does that mean that I could in principle say "`ProxyCorrect` is my model"? A lot of what you are saying reminds me a bit of the language that Heinrich Apfelmus uses when he describes `reactive-banana`. He'll say something like "Imagine that a signal is `[(Time, Event)]`" or something like that, and he even has a slow model implementation that he keeps in the library that people can use to reason about how the library should behave. So does that mean that I can just provide `ProxyCorrect` as the model implementation and that would suffice or is there something more that I am missing? I guess my question is what exactly about free objects is undesirable in the context of denotational design? I mean, there are ways that one could further restrict the `pipes` model, and I do provide some like `Server`s, `Client`s, and the other type synonyms, but those are still just free monads. I can't think of any useful way to further restrict the model that doesn't markedly depart from the spirit of the library and restrict its general applicability.
For `Watch`, you may consider offering both options: the safer one that includes the `Fd` inside of it for simpler deallocations and the unsafe one that takes up less memory for the purpose of storage within data structures. Then users can decide which version they prefer.
Ah, sorry. I thought reddit would alert me if it was.
Yes, I notice the same problem when tuning `pipes`. The moment you start doing any useful `IO` you immediately jump to the microsecond range per loop. I end up having to benchmark entirely pure code segments in order to get useful data for tuning the core implementation for `pipes`. Of course, that's good in a way because it means that the core implementation's delay is pretty negligible for most purposes.
You can recover the other version as `pipe return return`, but I guess you'd get what you deserve?
&gt; What happens if: &gt; &gt; * You remove the check 0 &lt; n I tried doing that and nothing happened. absoluteSum vec = go 0 0 --if 0 &lt; n then go 0 0 else 0 
Hi there, it's my first post on reddit. I saw some discussions about a CSS selector based template engine in r/haskell_proposals so I thought about sharing this with you. Disclaimer: I am a Haskell beginner, so expect some facepalms ;)
I'm usually inclined to make just about any change that allows lines of caveats &amp; explanation in documentation to be removed, but can't really judge the tradeoffs here. Could you hide `Inotify`, `init` and `close` if you build the API around a newtype-wrapped `ReaderT Inotify IO`?
Good catch! In fact that test is quite redundant. I wish I could claim this was deliberate, but in fact it was not. Sometimes these checkers can be rather too clever... I have updated the article and demo code. Thanks!
You don't _really_ want to make a for loop :), or even write explicit recursion. That's too low level. What is the code supposed to give you? The sum of the squares of all the numbers from 1 to n. Okay, so you know about sum, so that's good. How do you get the list of squares to sum? Prelude&gt; [1..5] [1,2,3,4,5] or Prelude&gt; enumFromTo 1 5 [1,2,3,4,5] The last bit is to square all the numbers. If you want to do something to all the items in a list, consider map: Prelude&gt; map (+1) [2, 3, 4] [3, 4, 5] Unless the assignment is to write explicit recursion, using maps and folds is the way to go. Sum is just a fold, so the sum of the squares of a range is really just folding + over mapping square over a range, which is just what the final code should look like. (At least how I would write it.) Does that help? *Edited because I'm bad at formatting code.
 1. 1 + 4 + 9 + 16 + 25 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = sum [1^2, 2^2, 3^2, 4^2, 5^2] = sum (powList [1,2,3,4,5]) = sum (powList [1..5]) 2. sum already written 3. TODO: write powList :: [Int] -&gt; [Int] 
This problem is fairly trivial, as long as you remember that Haskell is lazy, and know about map, take and sum: integers :: [Int] integers = [1..] -- list of all the ints map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f [] = [] map f (x:xs) = f x : map f xs map (* 2) integers =&gt; [2,4,6,8,10,12,14....] take :: Int -&gt; [a] -&gt; [a] -- function to control how much of the list we evaluate take 0 _ = [] take _ [] = [] take n (x:xs) = x : take (n - 1) xs take 5 integers == [1,2,3,4,5] Your problem can be solved fairly simply using sum, map, take and [1..].
There are two questions to ask yourself when solving a problem through recursion: First, what does your function do at the lower bound? In this case, what is the result of applying your function to the number zero? Second -- pretend for a moment that you know what your function does at some arbitrary number greater than 0. Let that arbitrary number be `n-1`, so we're pretending that the value of `sumOfSquares (n - 1)` is known. What is the result of `sumOfSquares n`, in terms of things you already know? If you squint a bit, you can see that this is the same procedure `sum` follows, except for lists of numbers, rather than numbers. The empty list, `[]`, is the smallest possible list. `t` is an abitrary list, and the definition relies on knowing what `sum t` is. `(h:t)` is a list one element longer than `t`. If you've ever taken a math course where you did proof by induction --- recursion is essentially the same process.
if you would like to show some haskell application or showcases, I shamlessly introduce a youtube video on my program here ;-D http://www.youtube.com/watch?v=Z2wzpyxsVSU 
Yes, term search can cause arbitrary IO. The purity is in the theorems: i.e. lambda evaluation/beta-eta conversion of a term of type "something -&gt; prop -&gt; prop" will not cause effects, but searching for the term "something -&gt; prop -&gt; prop" might cause effects. The reason I say this is ok is that types should be considered theorems, and proofs of theorems shouldn't involve your coworkers, although proving a theorem might involve your coworkers. &gt; is your term search going to be running random commands on my system in an attempt to find one which outputs "out"? Not *random* commands - well thought out commands. "Proof search" in a logic programming language is really a misnomer - it isn't really a random search. The axioms you give it end up looking much like imperative/functional code in do notation very good pattern matching. Take for example the following code. defn printForever : string -&gt; prop | _ = [A : string] printForever A &lt;- print A &lt;- printForever A You can read this code just like you would the imperative version: printForever a = do print a printForever a They would in fact execute similarly (albeit, my interpreter is WAY slow). The only times this gets messy is when pattern matching is involved - then you need to make sure the arguments are all unique to all possible cases, otherwise code could backtrack and io could be repeated. Other languages have solved this with uniqueness checking. This isn't necessary, as sometimes you want backtracking in your IO. 
Hmm... yeah, if I can find a few more videos of Haskell applications (perhaps an xmonad demo, plus there are some demos of 3D graphics code laying around), I could create a section for that. Cool! (Done)
Good to know. I have been working with one-based arrays lately so for a while I couldn't find the error in the second example either and started thinking I had gone mad :)
That's a bit silly. Sounds like HPC needs some love.
&gt; I end up having to benchmark entirely pure code segments in order to get useful data for tuning Greg suggested something to me that's on my TODO list — rather than pinging a real server (localhost Apache or otherwise), I need to write a `fakeConnection` or so that simply and immediately returns a properly formatted HTTP reply. That will take the I/O out of it, even if not taking the `IO` out. Which of course is a good idea; I haven't done it yet simply because I didn't want to be building against something that wasn't an actual web server. Contrived test cases always work; real web servers are often petulant. I should implement a fixed response benchmark this weekend. Might be fun. AfC
Ninja answer from the author of reactive-banana. Basically, everything will be fine once you grow your intuition a bit, so that the code you mention will become totally intuitive. The main step for that is to understand the applicative operations really well. When I am reading the example you mention, my mind simply replaces `&lt;*&gt;` and `&lt;$&gt;` by whitespace, i.e. function application.
It doesn't sound like your problem is with FRP, but with applicative style. (&lt;$&gt;, &lt;*&gt;, &lt;$, etc.) Applicative style is actually pretty sweet, but it may be *unfamiliar* to you. Like anything worth doing it takes some initial time to master the concepts. http://en.wikibooks.org/wiki/Haskell/Applicative_Functors http://learnyouahaskell.com/functors-applicative-functors-and-monoids#applicative-functors Often if something is an instance of Applicative it's also an instance of Monad, and so you often have the option to use do notation instead until you get familiar with Applicative. For instance, here's that line from your example: let c = colliding paddlePic &lt;$&gt; paddlePos &lt;*&gt; pure ballPic &lt;*&gt; ballPos and translated into do notation: do ppos &lt;- paddlePos bpos &lt;- ballPos let c = colliding paddlePic ppos ballPic bpos ... As you can see Applicative style is shorter, and the flow doesn't bounce around as much. If you're interested in Haskell, you're going to run into Applicative all the time. It's definitely worth getting a handle on.
Elm actually [looks rather nice.](http://www.ustream.tv/recorded/29330499) That's probably a better example to look at FRP because it's baked into the language itself, so there's not a lot of irrelevant line noise to wade through.
FRP is kind of and in-progress in haskell. Judging from the rapid user-friendliness advances made in the streaming data area iteratee/pipes/conduit/io-streams I have great hopes for FRP. If you have time. Maybe you could lend some of your brain's CPU cycles to the endeavor of making things easier for people like me.
What a fast response! It doesn't sound like you got this impression, but I just want to clarify that I don't mean to say anything bad about reactive-banana; the only reason I singled it out is because it has by far the most documentation with which to learn it. My perspective comes as someone who learned Haskell fairly extensively (up to monad transformers in my Real World Haskell book), completed a moderately complex project, then did other things for a year and... had to relearn almost everything I once knew. I think Applicative for example is very elegant conceptually, and I'm sure it's a good fit for your library, but it worries me that I have to put so much time into understanding it and then forget everything in a year. It just seems like quite the barrier in terms of practicality.
I am not sure whether the crypticness in the code results from FRP or just general operator-laden Applicative programming (and Lenses, too). In this particular example, it looks to me like due to Applicative style, not FRP itself. I had difficulty before to read codes with those operators, but now I feel comfortable now. I think that mental barrier against such common idioms will go away as one encounter similar examples more. As for FRP itself, I do feel that FRP code is often too dense. Often, the FRP examples maximize its conciseness to show how powerful it is. To be more friendly with someone like me in the earth, I suggest to write all the examples in FRP at least three times longer (which is already short enough). Please do not use view patterns and applicative operators other than `&lt;$&gt;`,`&lt;*&gt;`. (I think getting rid of `&lt;$&gt;`,`&lt;*&gt;` better) please use mappend instead of `&lt;&gt;`. please use `view`,`set`,`over` et al instead of operators in `Control.Lens`. Please show the types in the local definition. Please use qualified name if possible. For the FRP-specific operators like whenE, make a short comment on what it is, please. 
I'd say unfamiliar and unintuitive are not mutually exclusive. [Elm](http://elm-lang.org/) opted to introduce (&lt;~) and (~) in place of (&lt;$&gt;) and (&lt;*&gt;), and I think it makes a big difference: let c = colliding paddlePic &lt;~ paddlePos ~ pure ballPic ~ ballPos In many cases I prefer the various `liftN` functions though. To the OP, I'd say give Elm a try. I am the author of this project, and the main focus is making FRP really nice and easy to use, stripping out the non-essential Haskell concepts and paradigms and making what's left as clear as possible. [The video](http://www.ustream.tv/recorded/29330499) linked by chrisdoner elsewhere (thanks!) is a nice intro if you are interested in games. It will show how to make [this mario game](http://elm-lang.org/edit/examples/Intermediate/Mario.elm) from start to finish. [The code for Pong](http://elm-lang.org/edit/examples/Intermediate/Pong.elm) uses exactly the same structure.
&gt; cryptic operators like &lt;&gt;, &lt;$&gt;, &lt;$, #, and &lt;*&gt; (among others), I think what makes operators cryptic in other languages is that they are not used consistently. This does not seem to be the case with the Haskell I have studied so far. &lt;&gt; is going to be associated with monoid. &lt;$&gt;, &lt;*&gt;, &lt;$ is going to associated with applicative # is going to be some form of postfix notation. At least it usually is in the few graphics libraries i have used. If a library does not follow these standards or does not have a good reason for doing so then It would receive extra scrutinization before I used it casually/often. I have only read some of the blog posts on FRP only enough to think of it as operating on stream of events and that it allows for a more declarative style. I think I was able to understand what the paddleHit function does though a detail or two still escape me. If would be helpful and you are interested I can write down the though process I went through when reading the function.
Absolutely; I suspected FRP wasn't to blame, but rather the fact that FRP libraries need to use more intricate features such as applicative style or arrows. It's tough to define what's just "unfamiliar" and what is not intuitive enough. I learned about applicative style quite thoroughly a year ago, but I moved away from Haskell for a year and promptly forgot everything. I haven't had that experience with any other programming language.
Seconded! (by the author :P) The point of Elm is to make FRP as intuitive and low-friction as possible. If you want to see what Elm programs end up looking like, I'd look through these interactive examples: * [Mario](http://elm-lang.org/edit/examples/Intermediate/Mario.elm) * [Zelda](http://elm-lang.org/edit/examples/Intermediate/Walk.elm) * [Analog clock](http://elm-lang.org/edit/examples/Intermediate/Clock.elm) * [Turtle](http://elm-lang.org/edit/examples/Intermediate/Turtle.elm) * [simple animations](http://elm-lang.org/edit/examples/Intermediate/Physics.elm) Elm's [syntax](http://elm-lang.org/learn/Syntax.elm) is very, very similar to Haskell, so the learning curve should be low. More resources are available [here](http://elm-lang.org/). The language is quite young though, so while each 0.N version of the compiler is documented online and quite stable, there may be breaking changes between major versions (in particular, the next release (0.8?) will have a lot of improvements).
A question about Elm: how is it performance-wise? I'm trying to write a relatively high-performance simulator, and the thing I was most iffy about in terms of choosing Haskell over C++ was performance.
I think if anything, `Free` satisfies the role most analogous with lists. `Proxy` is more like `[F a]` for some `F`. It's the `F` that I'm more concerned about, since the `[]` doesn't actually add very much on its own.
I have not run into issues, but I also have not done extensive stress tests. For a language that compiles to JS, Elm has the advantage of being strict which should make a fairly large difference. That said, Elm's compiler is nowhere near as sophisticated as GHC and it's compilation target is nowhere near as fast as assembly. If it's any assurance, the next release of Elm is looking to be quite a bit faster than the current one. If the question is, "is Elm today as well optimized as Haskell and C++?" the answers is definitely not. As for the concerns about Haskell, I believe Haskell is very, very fast for a functional language (I have heard on par with Java and OCaml and such? I believe OCaml is also very fast).
That's a good point, and for the simple case I like the operators you picked for Elm. But you lose the left and right sidedness of the operators, so it seems like it would be harder to express things like: \*&gt; &lt;\* &lt;$ etc.? 
Well, that would be true if the only operation proxies permitted is the monad instance. I think the composition operations are the non-trivial structure that justifies the abstraction.
edit: oops, somehow my original comment got lost in the pipes of reddit for a bit, and I tried again. Carry on :P
While Signals in Elm are applicative functors, they are used in a very particular way. The goal of (&lt;~) and (~) was to make something nice for FRP, not to have a fully general API for applicative functors. In general I try to go for a set of orthogonal core functions and design them such that more complicated behavior is straightforward to define. I feel this makes it easier to remember what is in a library and easier to gain mastery of the relevant concepts. It is definitely a trade-off that needs to be addressed tastefully, but I currently consider the absence of those operators a nice feature :) This is also why Elm's Dict library does not have nearly as many functions as Haskell's Data.Map.
I rather dislike the asymmetric Functor/Applicative operators. Always takes a while before I understand what's going on, even if I know what the operators do (which took a while too)... especially if they're used on one line with other operators and I also have to think about precedence. Just sticking with the symmetric versions and putting in the `const`s explicitly makes much more readable code IMO.
I agree. precedence is awful with anything but standard operators.
I would say it's more an issue with the type resolution algorithm than with the type of `if_then_else`. Both clauses could be assigned the type `forall a. Show a =&gt; a` and the expression would type check. But the type resolution algorithm won't generalize that far.
I just did a little parser with `attoparsec`, and got a little dipping in the applicative style over there. In Yesod I saw a bit as well. As a new haskeller it is not straight intuitive, but for sure Applicative-style seems pretty much all over the place: therefore I dont get the worry of forgetting it, or not finding any other use for it. 
The problem is typeclass constraints in Haskell must be top-level. Your method, if I understand correctly, would lead to a type: Show c =&gt; Bool -&gt; (Show a =&gt; a) -&gt; (Show b =&gt; b) -&gt; c
I think `&lt;&gt;` is strictly better than `mappend`. It's not like `mappend` is a particularly obvious name, and `&lt;&gt;` leaves much less noise in the expression. If you don't know about monoids, `mappend` won't help, and if you do `&lt;&gt;` is just as clear. With `&lt;&gt;` it's much easier to scan and read even more complicated expressions. The `&lt;&gt;` highlights the structure of the code; `mappend`, being a full word, blends in with names actually part of the logic rather than the structure. Perhaps it makes sense to use something like `•` for `&lt;&gt;` to mimic mathematical notation. Also, that makes the grouping of the code even more obvious. As for making the code longer, I think the most important thing is to use intermediate names. Being a bit more liberal with those would probably be the biggest single win for readable code.
So most of the issues stem from applicative code. One solution I've seen is idiom brackets, but nobody seems to use them. I gather the idea is that you would be able to write f &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c as (| f a b c |) This is a much bigger deal for more complicated expressions. Has anybody actually used these for programming? Are them some problems with them? Also, applicative functors are sufficiently fundamental to programming that once you start using them, you'll use them everywhere. So you won't forget what `&lt;$&gt;` and `&lt;*&gt;` do.
I think the `#` usage comes from OCaml's syntax for working with objects and methods. With a flipped `($)` you can fake it with Haskell records. I find it pretty terrible, however. module Main where data Foo = Foo { member :: Int } foo :: Foo foo = Foo 3 main :: IO () main = print $ foo#member -- "3" where a # b = b a 
And people pick on Perl !
&gt; I'd say unfamiliar and unintuitive are not mutually exclusive. I'd say they are almost the same thing. "Unintuitive" is in my dictionary a synonym for "unfamiliar to the masses." Vim is considered unintuitive, since it's unfamiliar for most people. I know several people who would claim that programming in general is unintuitive since it's unfamiliar to most people. Blender is also considered unintuitive since it's unfamiliar to people who work with other 3D softwares. Which also makes "unintuitive" a pretty meaningless words for me when applied to a specific domain. I don't care how unintuitive it is as long as it is powerful, since I will be going to have to teach myself how it works anyway. "Intuitive" only makes sense when it refers to things that are shipped to customers which are not expected to pour in thought and very much learning before they use it. Things which are "intuitive" are designed to look and work like things that have been around for a long time.
Once you grok Applicative, you won't just forget it. You will see applicative patterns everywhere, even in code which doesn't explicitly support the concept of applicative functors. It's a little like learning an unintuitive but powerful piece of software like Vim. It is hard at first, but once you get it you want to use it everywhere and you will never really forget it.
&gt; `f &lt;$&gt; stream1 &lt;*&gt; ... &lt;*&gt; streamN` "zips" the streams together with f to create a new stream containing the results. On a side note: Thanks. I was looking for exactly this very recently, when I wanted to perform two I/O actions and couple them together into a tuple. I didn't realise it's just something like (,) &lt;$&gt; getLine &lt;*&gt; getLine Now I can go make my code better!
That translation isn't quite right - c is an action, not its final result. You probably want let c = do ppos &lt;- paddlePos bpos &lt;- ballPos return (colliding paddlePic ppos ballPic bpos)
Looks very neat! I like the ideas behind it, good luck!
You might say that. If you want complete safety, you could wrap OutputStream into a region-parameterized type to prevent it from escaping: pipe :: forall r. (RegionalOutputStream r a -&gt; IO r1) -&gt; (RegionalInputStream r a -&gt; IO r2) -&gt; IO (r1, r2) This is the same approach I've used [here](http://hackage.haskell.org/packages/archive/scc/0.7.1/doc/html/Control-Concurrent-SCC-Sequential.html#g:2) with some success.
Don't worry, we'll soon brainwash you into loving the cryptic operators. Yes, the are at first cryptic, but there are only a handful that Haskellers use, and once you have been "initiated," you *will* love them. /jedi-hand-wave
A value *of type* `()` could be bottom, but *the value* `()` is clearly not. *Some* sort of analysis should be able to see that there is no danger of crashing with `return ()`, given a suitably safe definition of `return`.
Often times the binary operation (for groups, monoids, etc) is denoted ○, so `&lt;&gt;` actually makes a lot of sense mathematically and it's likely one of the reasons that syntax was chosen.
Interesting, but why does the Elm site (and Wikipedia article) pretend like Haskell doesn't exist and that it just picked this syntax at random?
One of the arnows are more familiar than the other in the context of "up." Indeed have I never seen anyone use the arrow pointing down to indicate up before. Regarding your second example I'e seen variations of $ for function application way more than ^, so I find that more familiar and also more intuitive. My point is that 1. intuition can be shaped, by becoming familiar with things, and 2. everybody starts with slightly different intuitions. These points make intuition fairly irrelevant when you design things where people are going to have to learn anyway. ---- Edit: To expand on this, it's important to separate * intuitive (something you've seen before so you know how it works) * unintuitive (something you haven't seen before so you don't know how it works) and * counter-intuitive (something you've seen before so you think you know how it works but it turns out it doesn't.) Counter-intuition is bad (like assigning the meaning "down" to an arrow pointing upwards, or making the classic floppy disk "save" icon mean "remove paragraph".) However, things will always be unintuitive until you have learned them. For a new computer user today, having a floppy disk mean "save" probably doesn't tell them anything until they learn it means save. Yet it would be considered unintuitive to have any other icon than a floppy disk to represent the "save" operation. Similarly, having &lt;$&gt; mean "fmap" is unintuitive (*not* counter-intuitive) until you have learned what it means.
Looked into Mario example and I find it contr-intuitive a bit. For example, vx is set to x: walk {x} m = { m | vx &lt;- x, ... but vx is used as velocity x &lt;- m.x + t*m.vx It's kind of arcane math!
Thank you!
It's a little old, but I'm curious to hear people's thoughts about this.
it does (or something close enough)...and no
Right. ST-style types would be the clear solution, but probably worse than the problem in this case.
Clearly I'm biased. But it's a common experience for me to start a task with the intention of ensuring that SHE has nothing to do with it (to reduce cognitive load on readers), but the point at which I crack is the point at which I can't stand the applicative "swearing" any longer and bring on the idiom brackets (to reduce cognitive load on me). You can write... (|f s1 .. sn|) :: a t if `f :: s1 -&gt; .. -&gt; sn -&gt; t` and `si :: a si`, and you can also have (|s1 $$$ s2|) for infix `$$$`. If you want to include effects which don't contribute values, you can write stuff like (|f s1 (-p-) s2|) which will do the effects of `s1`, `p`, `s2` in that order, but apply `f` only to the values generated by `s1` and `s2`. There are issues with "tack brackets" `(-`..`-)` as a notation, not least the meaning of `(-3-)`, but some such facility is useful. If you happen to be using a monad, not just an applicative, you can use the postfix join to signal the point at which a computed computation is actually invoked. (|f s1 .. sn @ |) :: m t if `f :: s1 -&gt; ... -&gt; sn -&gt; m t` and `si :: m si`. I also baked in some support for Alternative. `(|)` means `empty`. `(|a1|a2|..|an|)` means `a1 &lt;|&gt; a2 &lt;|&gt; .. &lt;|&gt; an`. I often write parsers which look like thing :: Parser Thing thing = (| f1 this (-punctuation-) that | f2 somethingElse ... | id (-tok "("-) thing (-tok ")"-) |) which amounts to writing the productions of the grammar with the "noise" components tacked out and the semantic functions which build `Thing`s from signals neatly down the left-hand side. By Wadler's Law, there's plenty of scope for arguing about which precise symbols should be used, but I do find that the resulting code looks much cleaner. Of course, I would point out that if only types gave a clear separation of "effect description" and "value description", much less of this punctuation would be necessary. The notation is effectively needed only to mark contexts where the compiler should treat a type `f v` as an effect descriptor applied to a value descriptor. The point of idiom brackets is just to signal effect-value type structure for argument positions everywhere inside the `(|..|)` not further enclosed by `(..)`.
Except in Haskell 丄 is *bottom*, not top. Right? I've never seen bottom written as 丅.
I'm going to rewrite your `paddleHit` to use [idiom brackets](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/idiom.html). paddleHit :: Behavior t P2 -&gt; Behavior t P2 -&gt; Event t () -&gt; Event t (Velocity -&gt; Velocity) paddleHit paddlePos ballPos frame = let c = (| colliding ~paddlePic paddlePos ~ballPic ballPos |) response v@(unr2 -&gt; (vx,vy)) = if vy &lt; 0 then r2 (vx,-vy) else v in (| response (% whenE c frame %) |) Question: is this clearer than your original? (I should caveat that I'm not entirely happy with this syntax --- e.g. imo the magic prefix `~` operator should revert to being ordinary infix `~` if followed by whitespace, à la Template Haskell's `$` (Conor doesn't specify this), and `(%`...`%)` should be replaced by a similar magic prefix operator --- but I do think it's an improvement over the standard Applicative operators for anything other than trivial code.)
Uh... try recoding that example in perl and you'll see why.
That's what I meant.
Since applicative has become increasingly common in haskell code, when do we bite the bullet and just add idiom brackets as a GHC extension?
I always write `f &lt;$&gt; x1 &lt;*&gt; x2` to keep my code uniform.
The chinese characters are much more commonly 上 and 下, however, but your point still stands.
I want this greatly. We have syntax for Arrows and they're much less common.
&gt; I just want to clarify that I don't mean to say anything bad about reactive-banana; the only reason I singled it out is because it has by far the most documentation with which to learn it. No worries. :-) But here's the thing: you could have singled out *many other* libraries, like parser combinators of formlets for the *very same* operators. The only new operators I introduced in reactive-banana were `&lt;@` and `&lt;@&gt;`, which are variants of `&lt;*` and `&lt;*&gt;` and easy to understand once you are familiar with the latter. (In fact, they simplify FRP code even more.) You do have to spend effort to learn these powerful abstraction in the first place, but once you do so, you can apply them in many very different places. I don't know any other language besides Haskell that offers this kind of return. Perhaps more practice can help. Personally, I had a lot of fun reimplementing the `Prelude`. 
Additionally, mappend has as much to do with appending as return has to do with making singleton lists. 
Well, the problem with that is that most use cases would involve interfacing that newtyped-wrapped monad to other application monads. So you'd probably need to make it a monad transformer, something like newtype InotifyM m a = InotifyM (ReaderT Inotify m a) getEvent :: MonadIO m =&gt; InotifyM m Event But to be honest, I've soured on the idea of an application monad in many but not all cases for a couple of reasons: the problem of combining disparate application monads, the fact that adding features via monad transformers doesn't pay it's own way, dealing with exception handling, etc. So such a monad transformer might be worth considering adding to the base package as an optional wrapper. But I certainly couldn't require it, as it doesn't pay its own way. Sure, in some cases the GHC optimizer might be able to optimize the extra overhead of `InotifyM` in code that doesn't use Inotify ops, but these optimizations (like your observation regarding `Chan`) don't really work across module boundaries, because the compiler has no way of knowing whether the foreign code is going to use the parameter or not.
It's such a shame. It seems like everyone wants -XIdiomBrackets, but (paradoxically) no one submits feature requests, creates pages on ghcwiki, etc.
I think people are afraid to make a silly suggestion.
great_bushybeard: That's correct. That's why the tag-line is "Control your Arduino board from Haskell", not "Program your Arduino board using Haskell." Unfortunately, getting a GHC (or any other Haskell compiler) run-time on Arduino would be quite challenging, so directly programming Arduino using Haskell is currently a distant dream. Another alternative is to use a DSL like approach, where Haskell would generate the C that gets further compiled and put on the Arduino. This approach was demonstrated by Lee Pike earlier using the Atom DSL: http://leepike.wordpress.com/2010/05/31/twinkle-twinkle-little-haskell/ hArduino takes the approach of a "combined" eco-system, where the host-computer and the Arduino board cooperate. While you need to have a controlling computer at all times, it also gives you the power of both, all programmed using Haskell, to do nice controller projects.
&gt;I'd think "Haskellers know and non-Haskellers don't care". Not true. Non-Haskellers would certainly care about the immense resources available for helping to grok Haskell (which has become a sort of cottage industry), all of which would help them with Elm. Alternately, as a Haskeller, I would (have) appreciate(d) some form of confirmation that, "yeah, this is largely (a variant of) Haskell, so all that stuff you know there applies here".
That makes sense :) I'll add a note about this to try to get these benefits!
Do you have a link to that proposal? There are already 2+ implementations of manipulating html through CSS selectors in Haskell, although I forget if any specifically tried to do templating.
The proposal is here: http://www.reddit.com/r/haskell_proposals/comments/vx7bz/a_selectorbased_ie_css_templating_and/ I think I've found one implementation you are talking about, although that is CSS 2 only. Too late though since I already implemented almost all CSS 3 selectors which make sense in a non-browser setting. Anyway, it was fun.
&gt; The notation is effectively needed only to mark contexts where the compiler should treat a type f v as an effect descriptor applied to a value descriptor. I believe the terms "effect descriptor" and "value descriptor" are new as they are nowhere in your original paper. Could you please explain in plain language what these terms mean? As far as I could make out, `f` of type `a -&gt; b` is pure, so I don't know why it's called an effect descriptor.
From my understanding of what's going on at SXSWi, no, there isn't (but I might be wrong.) A lot of the presentations etc. seem a bit far removed from this kind of stuff. That said, if there are Haskellers around town for the next two weeks, and anyone would like to meet up for beers somewhere in ATX (downtown or elsewhere is fine,) I'd be more than happy to join in or help arrange a place to meet as I live here. Unfortunately I only *know of* one other Haskeller in town, and he's [a professor](http://www.cs.utexas.edu/~wcook/), so I couldn't guarantee a large audience in any case.
The theorists didn't *take* the symbols, they made up symbols that happened to look the same, but with the opposite meaning! The point is that the meaning of symbols is different in different contexts. &gt; But if 丄 means above then it corresponds to ↑ in my opinion because the pointier part is still pointing upwards. You're relying on the pointy = important heuristic again, which is not universal.
Hmm, that might be a typo. Haskell's type system is impressive, but I don't know of a type-level representation of water content.
That sounds even closer to a usable solution than I thought. Great.
My bad. The `f` in that last para is a hypothetical *type* operator, not intended to be cognate with the `f`s earlier which are term-level functions. By "effect descriptor", I just mean some type operator which describes some form of impure computation in the course of computing a value, be it a monad, an applicative, or whatever. Lots of Haskell types are of form `f v` where `f :: * -&gt; *` and `v :: *`. `[Int]` is of that form, with `f = []` and `v = Int`. `a -&gt; b` is of that form, with `f = (-&gt;) a` and `v = b`. Depending on syntactic context, these are treated either as pure value types (lists of integers, functions from `a` to `b`) or, when we write `do {x &lt;- blah; ...}` for `blah :: f v` , we treat `v` as the value type, which becomes the type of `x` and `f` as the descriptor for the monadic effects which are locally permitted (nondeterminism, reader of `a`). Similarly, when we work with (&lt;*&gt;) :: Applicative a =&gt; a (s -&gt; t) -&gt; a s -&gt; a t we are treating `a` as an effect descriptor and its arguments as value descriptors: `a` effects are permitted, and at the value level, we're just doing application. Of course, to Haskell, there's no such thing as a designated "effect" (apart from the blessed pure effects of nontermination and exceptions). It's just that operators like `&gt;&gt;=` and `&lt;*&gt;` and do-notation work with types which reflect that analysis of types `f v` as `f` for effect and `v` for value. Plenty of other functions treat types of form `f v` as if `f` is some sort of container data structure. It's in the eye of the beholder. And my point is that this fluidity in types is notationally problematic exactly because the only way we have to explain whether to treat types `f v` as foreground containers or as background effects wrapped around values is by explicit use of operators. If the types made a clear separation of what was supposed to be the effect part and what was the value part, then the ordinary whitespace could be overloaded as application for any notion of effect, not just the blessed "pure" effects, and we would be spared all this `&lt;*&gt;` noise. You ask for plain language. As I'm not you, I can't tell if I'm being plain to you, so I can't knowingly fulfill your request. I can only try variations in the way I put things and hope for the best. In the course of communicating in the present, I do not promise to use only phrases I have used in the past.
This is great! I'd love this for `#happs`, is it open source?
Here's 12~ years worth of IRC logs taken from tunes.org, indexed and browsable. More features will be forthcoming (mostly in the statistics department). Getting the data set imported and running fast, was the first step. It imports yesterday's logs once a day. I also thought +1'ing messages that would be a fun way to mark interesting conversations. I'll be integrating it with hpaste.org. The webmaster of [no longer running] ircbrowse.com, Kevin Rosenberg, gave me his blessing to use the ircbrowse name on a .net domain. I thought posting it to reddit would be a sufficient stress test. **Warning:** May consume lots of your time.
IME, this is more likely to turn people off of FP, since they get the impression "oh, so FP is this nichey thing we could do, but we use C++ because it's better".
Cheers! The [source is here](https://github.com/chrisdone/ircbrowse).
Is Tree enough of a middle ground?
This is really cool. I had no idea strings were immutable in all those languages.
Quite some time ago lambdabot's @quote database was wiped out, so we're missing quotes from a large part of #haskell (and other channels') history. Unfortunately, querying this service for `@remember` drops the @, so it is difficult to recover all the @remember commands in order to restore the @quote database to some semblance of its former glory.
It's against freenode policy to release public logs of channels.
erm, no. this is freenode policy: http://freenode.net/channel_guidelines.shtml As per those guidelines, the fact that #haskell is logged and the location of the logs are in the channel topic. everyone knows #haskell is logged, and folks tend to know that if they want to go 'off-log' they should take it to -blah.
can't believe I said boobies that many times.
I'd focus on a small subset of [Data.Map][1]'s external interface: insert :: Ord k =&gt; k -&gt; a -&gt; Map k a -&gt; Map k a delete :: Ord k =&gt; k -&gt; Map k a -&gt; Map k a lookup :: Ord k =&gt; k -&gt; Map k a -&gt; Map k a A nice application of this interface is parsing lexical scope: int a; int b; { int a; // shadows old a int c; } // c is no longer defined, and a is what it used to be With an immutable map, you can add identifiers to the environment, then roll back to the previous environment when you leave the scope. It is very easy to implement something like Map if you don't care about performance. In C++, it could be a linked list of key-value pairs. `insert` prepends a pair, overriding any existing pairs (since `lookup` would start at the beginning). Delete also prepends a pair, but uses NULL for the value. You could then hint at using a tree data structure. Draw a balanced tree. Show that to insert a new item, you only have to replace nodes from the item to the root. The new tree can just link to the unchanged nodes in the old tree, like in [this picture][2]. [1]: http://hackage.haskell.org/packages/archive/containers/latest/doc/html/Data-Map-Lazy.html [2]: http://en.wikipedia.org/wiki/File:Purely_functional_tree_after.svg
Yes, but no. The thing is that it does take effort to learn them, but once you grok them, they make your life a lot easier than any well-named alias could. In this case, it's important that the operators in question can be written in infix notation. For instance, the introduction of `&lt;@&gt;` enhances code readability significantly. One could argue that it's the same for learning arcane syntax in Perl or keyboard commands in Vim, but it's not. In Haskell, you always learn something profound that goes well beyond syntax. 
One area where persistent data structures may offer advantages in both time and space usage is when searching a game tree and updating the game state - beats having to copy the whole game state at each node. This would apply for example if the game state can be represented as a persistent tree map.
For a particularly elegent purely-functional Tree structure, you should have a look at [1-2 Sibling Trees](http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CDUQFjAA&amp;url=http%3A%2F%2Fwww.cs.ox.ac.uk%2Fralf.hinze%2Fpublications%2FBrother12.pdf&amp;ei=kRc7UfnVI4WRrAGXuYDIDA&amp;usg=AFQjCNHcd4oEerQXenefR3zcNcvd7JfbPw&amp;sig2=hq4_QTd7LzmzcveL1CfhVg&amp;bvm=bv.43287494,d.aWM&amp;cad=rja) The implementation of these in a GCed language with ADTs and Pattern Matching is *kisses fingers like italian chef*. I've implemented them in C with ref-counting. It was mind boggeling and poorly performant :(.
This is unfortunate. Thanks for reminding me to not ute #haskell anymore.
#haskell has always been logged, and published online.
I haven't come up with a good exercise, but GCC has [an extension](http://gcc.gnu.org/wiki/TransactionalMemory) that allows you to use [STM](http://en.wikipedia.org/wiki/Software_transactional_memory). You might be able to do something interesting using purely-functional data structures and STM.
Here's a CSV export of all `@remember` and `@forget` commands: http://ircbrowse.net/exports/quotes.csv The search is based on a word index by Sphinx, otherwise it would be rather slow. It's possible to setup other search types and even a regex search, but I suspect the use-case is smaller than the effort required to do it performantly (e.g. [Google Code Search's algo](http://swtch.com/~rsc/regexp/regexp4.html)). The regex command to do that CSV takes 8 seconds. That sucks, and would be easy to abuse. I've considered listing these on the site, too, browsable.
So now we have the overloaded semicolon, perhaps we also need the overloaded space.
Ruby Strings are mutable
I think the mathematical definition of a variable really should be called an "unknown" instead. The programming definition seems much more appropriate for the word "variable" since it varies throughout the lifetime of the program.
Teach GIT as a functional data structure, you can progress quite quickly through the dimensions.
I was a TA for Jay McCarthy (Brigham Young University) when he taught an intro to programming in C++ course. He basically taught it as if it were Racket, which baffled some of the "I already know C" students but worked well overall, imo. You should check out his curriculum and/or get in touch with him. He teaches immutable cons lists and trees, and delays teaching mutation at all until the end of the semester. http://faculty.cs.byu.edu/~jay/courses/2011/fall/142/course/
Why avoid #haskell, simply because it is logged? (note: reddit is also "logged"...)
Searched for myself. I am getting duplicates and triplicates for recent stuff. Looks like the dupes start around March 6.
Ha, you're right. Can you tell I spend my day to day in python land?
I will speak up as a voice against idiom brackets. I believe that `(&lt;$&gt;)`and `(&lt;*&gt;)` are good enough and don't require learning extra Haskell syntax. Beginning Haskell programmers frequently encounter two stumbling blocks: * Too much syntactic sugar and too many idioms to learn before they can begin to mentally parse other people's code * Difficulty understanding parsing errors Adding idiom brackets compounds these problems for a very minor syntactic gain. We often forget that when newcomers look at Haskell code they have NO CLUE which things are functions, which things are reserved tokens, and which things are syntactic sugar. This makes the language very unapproachable for beginners.
Ah, cheers. I'll add some duplicate checks for the import.
I found that as well, I think it is something about how the browser or OS handles simultaneous key presses. I initially used space for run, and it also had weird interactions with the arrow keys. What platform are you on?
This is awesome, so thank you for that... but I'm going to be That Guy and point out that lambdabot also understands ?quote and ?forget.
And now you've said it again!
I just added [a nick cloud](http://ircbrowse.net/nick-cloud). Gonna add a word cloud too. =)
It is *really* interesting to see what the channel was like back when it first started. I expect it will be a valuable resource when stack overflow falls short. Thanks for this.
No they're not. All strings are duplicated before being mutated in Ruby (Source: Rubinius)
 x = "abc" x &lt;&lt; "d" print x # x is "abcd"
Sorry, I see what you mean now. However at the VM level, what I said still holds true.
Awesome! &lt;3
Could be, good point. On the other hand if OP could show, that some restrictions are cool because they make programming easier &amp; compiler's job easier, then... FP is a natural evolution from C++ ;)
You should remember to properly format your code for reddit by putting an additional 4 spaces in front of each line of code, or putting backticks ` around an inline snippet. However, it looks as if you want to output a particular string based on the value of `x`, but you have just about everything wrong that you could. There is no way this code would compile, and there's a much easier way to express this, with [guards](http://learnyouahaskell.com/syntax-in-functions#guards-guards): myGrade :: Int -&gt; String -- Always put a type declaration myGrade score | 0 &lt;= score &amp;&amp; score &lt; 60 = "You got an F" | 60 &lt;= score &amp;&amp; score &lt; 70 = "You got a D" | 70 &lt;= score &amp;&amp; score &lt; 80 = "You got a C" | 80 &lt;= score &amp;&amp; score &lt; 90 = "You got a B" | 90 &lt;= score &amp;&amp; score &lt;= 100 = "You got an A" | otherwise = "Error: Score either negative or greater than 100."
Much better! Nice work. :)
As a first step, perhaps you should ask yourself this: what is `x`? Is it a number, or a string?
I haven't been particularly assiduous in pushing idiom brackets as a language proposal, partly because I can use them anyway, and partly because I'd rather think about what to do in some future language where the type system manages effects-vs-values more neatly, so the whole thing becomes beside the point. When I teach Haskell, I certainly don't inflict my idiosyncratic preprocessor on my students. But when I write Haskell, my priority is usually my own code comprehension, rather than accessibility to beginners. As long as I can still use idiom brackets in my own code, I'm quite happy for you to have your way here.
This is awesome! Do Simon Marlow and Simon Peyton-Jones use #haskell? Does anyone know their usernames? Thanks :)
Incidentally, are you in the quantitative finance space?
&gt; So the Universe we are living in is being constantly modified by Haskell programs. That's why being a Haskell programmer feels like being the Master of the Universe. :)) 
Funnily enough, that's very close to how I actually learned math: as a totally abstract game that you played. I viewed all math problems as fun puzzles to solve and I never needed to give meaning to any of the equations. That would have been like asking me what is the meaning or significance behind solving a Sudoku puzzle or playing a video game. Anyway, that might explain why the word "unknown" feels more natural to me, because I've always viewed it as a puzzle to solve, and "unknown" fits that metaphor quite well.
Sure! If you can PM me an email or somesuch we can coordinate after you get here (sorry for getting back to you late; I was out of town for a day myself!)
I learned a lot from reading the previous publication of this text: http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way To me it was the best introduction I found to simply get me started; I'm glad FPComplete embraced it. 
For comparison, the [yesod web framework](http://www.yesodweb.com) has two CSS [templating languages](http://www.yesodweb.com/book/shakespearean-templates) - Cassius and Lucius. Lucius is the one that is mostly used nowadays.
Haha, is that some kind of #haskell meme?
Linux 64bit, xorg-server 1.13.3, Firefox 19.0.2 (and same behaviour in Chromium 25). Interestingly, it works if I switch ctrl to space. PS: I just checked this in xev: Indeed, even this X server here is inable to see Ctrl-Up-Left. That's a funny thing to know about.
I don't really think the word "unknown" captures the meaning of "variable" in mathematics. It clearly captures some uses, but not things like "random variable" or "independent variable". A variable might well be known! I think "variable" is a good word, since what you have is some relationship (equation, inequality, function) between multiple things that is true for all legitime values of those things. Thus, they vary. They are not necessarily unknown. A variable may or may not be something you solve for.
A shachaf meme :D http://ircbrowse.net/browse/freenode/haskell?q=they+are+so+easy
I think there's probably _some_ comparison. =P
&gt;Finger trees seem too complicated I guess that takes [hash array mapped tries](http://en.wikipedia.org/wiki/Hash_array_mapped_trie) off the table? These are the most interesting persistent data structures IMO. Their time complexity (log32N) puts them well ahead of anything based on binary trees.
Wrong forum - try stackoverflow.com 
Those libraries do an entirely different thing. Those are for injecting variables into a stylesheet, mine is effectively a subset of jQuery for server side.
In terms of performance, OCaml basically behaves like what you would get if you naively added higher level features to C. It's fast if you write low level code, but the compiler will not optimize away levels of abstraction, so if you write high level code it will be much slower than the equivalent code in Haskell (compiled by GHC, at least) would be.
&gt; apart from the blessed pure effects of nontermination and exceptions I always thought these were indistinguishable in otherwise pure code (and therefore would have just bundled them both together as "nontermination"), yet you seem to have intentionally distinguished them as effects. Is there a reason why?
FP *is* a nichey thing we do even in Haskell - OK, a *big* niche, but you can't write that much practical code without using the IO, ST and similar monads. Actually, given that the context relates to algorithms and data structures, it's worth mentioning that a few problems that have efficient solutions using mutable data structures have no such efficient solutions in the purely functional world. Examples include hash tables and union find. Actually, Haskell provides purely functional dictionaries that are competitive with it's own ST-monad hash tables, but apparently [that's only because the ST-monad hash tables are pretty slow](http://stackoverflow.com/a/10897664/180247). As I've started on the Haskell-hate (even though I quite like it), laziness is a problem for algorithms, though of course laziness isn't a requirement for purely functional coding. The problem with laziness is that performance issues are not referentially transparent. Using the value of a variable may look like an O(1) operation, but the computations that derive that value may have been deferred until now by laziness, so it could be O(anything) depending on the context in which the your variable reference occurs. Time and space analysis in a lazy language pretty much always has to be amortized analysis, and it's difficult to restrict the scope - you might have to analyse the whole program. The problem is deeper than that, though, as even when you get the correct performance bound, the order in which things are done is relevant to an algorithm - if you aren't doing things in the right order for an algorithm, you've implemented the wrong algorithm. With laziness, the order of execution for non-trivial code isn't obvious - it's determined by data dependencies in a non-referentially transparent way, not by the visible structure of the code. And there even seems to be a cultural aspect to this, where Haskell programmers have a blind spot WRT the accuracy of their algorithms. Take for example the purely functional "quicksort" given in may Haskell tutorials. It simply isn't the quicksort. First off, the genuine quicksort is an in-place algorithm (it rearranges data within a mutable array), which is clearly incompatible with a purely functional algorithm - there's often a nod to that at least. Secondly, although the quicksort is sorting by partitioning, it doesn't follow that every sort by partitioning is a quicksort. The original paper describing the quicksort specified a particular partitioning algorithm and particular variants, and the approach used for the purely functional "quicksort" is completely different. Issues relevant to the performance of the purely functional "quicksort"... 1. Because it operates on lists, not arrays, it doesn't have the same locality guarantees as the genuine quicksort - lists probably won't be scattered randomly through memory, but you can't be sure, and the more you share chunks of lists the less you can keep complete lists contiguous in memory. 2. The head of the list is taken as the pivot for performance reasons, which means that randomizing pivots and similar tricks aren't applicable. These are precisely the tricks that make a robust high-performance quicksort possible, and a lot of the original quicksort paper was devoted to discussing them. 3. Because a new list is produced as the result of each partition-and-concatenate step, there's lots of dynamic allocation of memory that doesn't happen in the genuine quicksort. The compiler might be able to optimise this, but even if that's possible, it depends on the context in which the code is used. 4. In the genuine quicksort, each partition has two pointers moving towards each other until they meet. This means there's only one pass through the complete array. In the purely functional "quicksort", the partitioning is done with two separate passes through the list in the same direction - it has to be because the lists are unidirectional (hence the wrong partitioning algorithm). The optimizer may or may not be able to optimize this into a single pass. 5. [added in edit] The above don't even consider laziness. The genuine quicksort runs one partition at a time. The pure functional "quicksort" *appears* to do the same, other than details of how that partitioning is done, but laziness potentially interleaves partitions, adding overhead for the usual implicit data structures to defer execution to keep track of that. Other completely unrelated computations may also be interleaved, depending on how the resulting list is used later on - if you initially only use the first 5 items from the result, then initially only enough sorting to provide those will be done initially. Because of the tangle of data dependencies relating the output list to the input list, that leaves a complex tangle of deferred computations represented by an implicit data structure until some more items are needed. I say "potentially" because it depends on strictness analysis, and because explicit strictness can be specified to avoid the issue, but if the laziness were doing a useful job then that couldn't apply anyway. This isn't the only example - there's a paper about the [sieve of eratosthenes](www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf) for example, which Haskell programmers have traditionally implemented wrongly, getting the wrong complexity as a result. Personally, I observe that even Simon Peyton Jones has stated something along the lines of laziness probably being a mistake except in that it forced purity - the use of the typesystem to keep pure and impure code separate may never have happened if Haskell was strict, and ML is evidence of that - strict and impure. Every time I make observations like these, I learn a few new things about why I'm at least partly wrong, but for me the key point remains. Pure functional by definition limits your options, so you simply cannot implement mutable data structures in purely functional code (though the ST monad is no great hardship, despite some deliberately verbose syntax). However, laziness makes Haskell an inappropriate language for studying algorithms - the order of execution is hidden by too much implicit indirection and there are too many hidden overheads for deferred execution, so the algorithm you get isn't the algorithm you see and time and space performance analysis is unnecessarily difficult. [added] - So Haskell is evil? Not at all. Although performance can be unpredictable and is difficult to formally analyse, it's generally very good, and most code isn't that algorithm-theory or performance obsessed as long as it gets the job done. One well-known paper claims Haskell is the ultimate scripting language, and I think there's a good point there. Sure, static typing isn't normal for scripting languages, but Haskell is extremely good for easily writing things that are sound, and fast enough. That "easily" is more dependent than language- and paradigm-specific knowledge than usual, but still true. Haskell is also very well suited to large-scale software. There's just this particular thing that Haskell isn't very good for, which for the most part isn't that big a deal, though I do wish Haskell was strict-by-default with laziness hacks rather than visa-versa. 
&gt; However, laziness makes Haskell an inappropriate language for studying algorithms s/algorithms/real-world time-and-space analysis/ or possibly even s/algorithms/complexity analysis/
I think the point applies even if you treat algorithms in a cookbook kind of way. After all, performance can be a correctness issue. One little nasty in lazy evaluation is that it can turn a correct algorithm into one that's broken because of stack overflow, basically because of the implicit data structures that are built for deferring execution. Those can get arbitrarily large, and when the evaluation is triggered, that means arbitrarily deep nested calls. Of course in theory that just means you need to learn the alternative patterns that don't cause these issues - avoid tail recursion and use corecursion instead, IIRC - but that still means you need a different cookbook. In Haskell, the traditional cookbook doesn't give the results it promises. 
Yeah, the trick is to carry moist things in containers. Because they Maybe Moist. 
http://hpaste.org/82140
The way I did it was probably not the right way, but simple enough. I used `unsafeInterleaveIO`, which allows you to perform input sort of lazily. I defined a list which contained all of the input ever, and then just folded that with the logic to get a list of game states, which I fed to the thing that presented them to the user. Lazy evaluation is a little creepy sometimes. ---- Edit: Basically this: getCommands = unsafeInterleaveIO $ do c &lt;- getCommand cs &lt;- getCommands return (c : cs) main = do commands &lt;- getCommands let gameStates = foldr gameLogic commands mapM_ drawGame gameStates ---- (Yeah, I know I could write getCommands = unsafeInterleaveIO $ (:) &lt;$&gt; getCommand &lt;*&gt; getCommands I just don't feel completely at home with using applicative style on infix operators. If anyone know a better way to write it, please tell me.)
If you decide not to bless them, they should probably be modelled differently from one another.
The HAM part isn't particularly educational there but tries are cool and can be explained in a few minutes, with the same asymptotic complexity.
Is there support for tuples with idiom brackets? (|a,b,c|) or maybe (| (a,b,c) |) to mean (,,) &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c
Pretty cool. Is there any way to filter out all the joining/quitting/ping messages? With enough people in the channel, it can be a bit overwhelming, especially since most people don't talk much. +1'ing messages does sound like a neat idea. StackOverflow chat supports this, and it does give an interesting perspective on recent interesting conversations in any given room. Also, since you're an Emacs expert: is there some easy way to do this sort of filtering with Erc? 
Ah, I see. What's the state of Haskell/Scala/FP in the finance world? My impression was that management types were extremely averse to anything but C++, but this was a while back. Perhaps things have changed since then.
Why \`liftM\` ask instead of asks?
It is *far* more frequent for lazy programs to take less (asymptotically) stack space. See map f (x:xs) = f x : fmap f xs as the quintessential example. I can think of *no* example to your claim in a purely functional language that arises if your cost model doesn't provide zero space tail calls. That is because all such examples would involve code that would stack overflow anyways--the maximal depth of delayed computation is proportional to the depth of the constructing computation. Further, in almost all cases GHCs optimizer makes the tail recursive examples strict. Not to say space isn't an issue. I would not advocate a lazy language for an algorithms course, but lets be clear, call by need is asymptotically optimal w.r.t time, and stack usage is normally asymptotically better than cbv. 
Neat. I think around this time last year I also did a [SDL snake clone](http://quasimal.com/projects/level_0.html), haha. In Main.hs, you have in `handleInput` a whole lot of guards like that read `| key == SomeKeySym`. It's a good idea instead to use a case expression: case key of SDLK_a -&gt; ... SDLK_q -&gt; ... This probably has no impact on performance, but remember that case expressions are more "primitive" than `(==)` and `Bool`s, as well as being a bit less visually dense.
Why isn't it on hackage? It's harder to try your game if it's just a github link. It only takes a few seconds to upload to hackage, and it saves gobs of time and effort for the many people who want to try your code. If your code builds and you're ready to share the link to github, you're ready to upload to hackage. It's also harder to read the code because there are no haddocks. Haddocks get generated and posted automatically when you upload to hackage. Well, actually your code isn't commented yet anyway, so that's not an issue yet :). In the comments you do have, though, you should get in the habit of writing -- | board size instead of -- board size so that the comment will show up in the haddocks.
I think Simon Marlow came on once or twice. I don't think SPJ knows what an IRC is.
The current implementation supports pairs, with (|a , b|) = (,) &lt;$&gt; a &lt;*&gt; b There's no reason (other than implementation effort) why it shouldn't support larger tuples. The rule of thumb I'd like to reach is that between the outer (|..|) and any inner (..), things in argument positions are typed as effectful. But I only implemented prefix and single-operator infix application.
Splitting hairs much?
see also the (for now one) comment(s) on lambda the ultimate http://lambda-the-ultimate.org/node/4694
You could look at the directed acyclic graph git uses as a data structure for its repository and how that enables sharing between commits.
Er, this submission titled monad-subclasses links to a package MONOID-subclasses. No Monads in evidence.
I had to remove the version restrictions in the cabal file to make it build on Debian testing. It compiles and runs fine with `mtl == 2.1.1`, `containers == 0.4.2.1`, `SDL == 0.6.3`, and `base == 4.5.0.0`.
349 lines according to `find src -type f | xargs wc -l`. Indeed very nice.
With methods such as `foldl` and `mapM`, the `FactorialMonoid` typeclass looks like it should be related to `Foldable` and `Traversable` somehow (albeit that the latter two are for higher order types).
Sorry about that. I posted the link before my morning coffee. There seems to be no way to edit the title now. 
Some of these seem werid, but love `MonoidNull` and `CancellativeMonoid` having a place to live now :)
TOO LATE I HAVE SEEN THE SUBMISSION AND I AM APPALLED*!*
Re-submitted because of the wrong title the first time around. Sorry. 
OK then, I haven't timed that particular inconvenience, but there are many others, too. That is the standard way to make packages available in Haskell. To me, someone who doesn't upload to hackage either is just getting started with Haskell - which is fine, and welcome! - or is sending a message that they really don't want to cooperate fully with the community. Their code isn't searchable or installable in the usual way, can't as easily be re-used by other packages, doesn't have haddocks and source code available in the usual place, doesn't have a quick view of versions and dependencies in the usual format, doesn't show up in the results of automated tools like dependency and reverse dependency statistics analysis, etc. It's so easy; why not make things easy for people?
If you mean that all possible work is written somewhere in the program, that's true - there's some overhead for deferring work, but that's a constant factor time overhead. No work is invented. But the same applies for strict evaluation. That's fine if you're amortizing over the whole program, but as I said, it's hard to limit the scope of the analysis. If you have a performance-critical piece of code, it's hard to ensure that no deferred computation leaks into that - simply referencing a variable could trigger some huge slow deferred computation. It's a bit like naively implemented garbage collection cycles, but the issue is deferred computations, not deferred freeing of memory. So if your analysis needs to be no better than you'd get assuming strict evaluation for the same structure of code, that's fine - the order of evaluation will be different, but the same asymptotic result applies. But if there's supposed to be a performance benefit to lazy evaluation, you've got a lot more work to do - you need to know which work *will* be avoided in the context of your complete program and to account for that in the analysis. Otherwise, your asymptotic bound is exactly as it would be for strict analysis - you've just added loads of constant factor overheads for all the deferred evaluation. The order of computations is also often important for locality - for using cache effectively. Of course that just means lazy evaluation isn't an optimization, even though that's often how it's advertised - the real benefits are that it can give the correct result in cases where strict evaluation fails, and that it can help with composability, that it can make cyclic references in pure functional data structures possible etc - that it enables some idioms and patterns that don't work with strict evaluation. That's good and interesting, but laziness doesn't need to be pervasive to get those benefits - it could be targeted where needed. And the need isn't that extreme - most languages get by perfectly well with strict evaluation. So basically, pervasive laziness means significant costs are added pervasively, then some of those costs are eliminated again by strictness analysis and some more are eliminated by hard developer work adding in explicit strictness where needed. And work that should have been done at a convenient time can still be saved up then triggered at the worst possible time if you miss something. All that to get occasional special-case benefits that most programmers are willing and able to do without, as evidenced by all the strict languages. That's why I think it would be better to be strict by default, but with laziness hacks. 
So it behaves more like a mutable wrapper around an immutable string (like `org.apache.commons.lang3.base.MutableObject&lt;String&gt;`)? x = "abc" y = x x &lt;&lt; "d" puts y # output: abcd
irc and reddit, they are both lenses (pun intended) through which I view and interact with the Haskell community, with equal disregard to the level of logging.
&gt; It's worth noting that a class instance does not need to satisfy this law: &gt; factors (a &lt;&gt; b) == factors a &lt;&gt; factors b Would it be safe to say that *when observed under `sort`* that this law must be true? In other words: sort (factors (a &lt;&gt; b)) == sort (factors a &lt;&gt; factors b) I was going to say "when observed under `nub`", but `sort` makes a stronger assertion about the number of duplicates. Of course, this requires the additional assumption that the elements must have an `Ord` instance.
Thank you, fixed ;)
No. Consider the `Product` instance: factors (Product 0 &lt;&gt; Product 6) == [Product 0] factors (Product 0) &lt;&gt; factors (Product 6) == [Product 0, Product 2, Product 3] Another instance that doesn't satisfy the law is `ByteStringUTF8`, which is really only a representative example of a large class of eminently practical data types. If you read UTF-8 from a file in, say, 4 kilobyte blocks, there is a significant chance that one of your blocks is going to end in the middle of a UTF-8 encoded character. The last factor of that block is going to be a malformed character. But concatenate that block with the following one and the character prefix combines with the first factor of the next block to become whole again. 
I thought too that it's for functionality, but if that's the way to cooperate with the community, then it should appear there as soon as I get a username/pass to HackageDB. Also, I added some Haddock comments, thanks for the tip ;)
Fixed.
I've seen that clone ;) I changed to 'case ...', it does look better ;)
Hackage was never meant for those purposes and is not well suited for either of them. Hackage is for sharing the libraries and programs themselves, viewed at a higher level. It's built on all the metadata used to build a package, not just the source code. That is a significant difference. Even if we only look at the effort saved, and assume it is as small as chrisdone claims, that could already be significant. After all, github's huge success was built around the perceived collaborative advantage of the pull request workflow over `darcs send` and `git send-email`, even though at first glance the savings is only a matter of a few keystrokes. But it is more than a matter of saving a few seconds. Having packages organized by machine-readable metadata on Hackage adds a huge amount of collaborative value over github, and huge potential for building more collaborative tools on top of it. The metadata also can include the source code, as a link to a service like github, so that people can also collaborate at that lower level. But that's only one detail. For defining releases, what we have currently is the Haskell Platform, which is only for the most stable and popular packages. The FP Complete team is working on Stackage, and I suspect they have more than that up their sleeves. We were supposed to get a whole slew of other tools with Hackage 2; unfortunately we're still waiting for those. I think there is still a lot of room for innovation in the area of defining releases. But Hackage 1 is not that, other than saying in the human-readable package comment that this upload is a stable release, or just the latest build of HEAD with some recent bug fixes, or whatever.
You need to put back ticks around the definition
Operators are infix, so you could define something like: (#) :: Rank -&gt; Suit -&gt; PokerCard r # s = PokerCard { rank = r, suit = s } 
Why did you decide to make a binary operation &lt;/&gt; as the definition for a reductive monoid? Would it have been possible to make a unary function "inverse", and then have a default implementation of: a &lt;/&gt; b = a &lt;&gt; inverse b That's what I've done when implementing groups before, and it seems more flexible to me, but I'm wondering if it doesn't make sense in this case.
That is the standard mathematical definition of a group, yes. The problem is that most practical monoids are not groups: in particular, you can't define a reasonable `inverse` for lists and other sequence types. 
It'll still require backticks when used. What you're proposing is only so the definition can match the expected usage.
Well, at least *I* find the backtick convention ugly and rather awkward to type. (But not so ugly and awkward I ever would've brought it up unprompted.)
If the name consists of letters, it is a function which requires backticks to be called infix. Example: of :: Rank -&gt; Suit -&gt; PokerCard r `of` s = PokerCard { rank = r, suit = s } If the name consists of symbols, it is an operator which requires parentheses to be called prefix. (@@) :: Rank -&gt; Suit -&gt; PokerCard r @@ s = PokerCard { rank = r, suit = s } I'm sorry for being terrible at coming up with operator symbols. I leave that to someone else.
But if inverse :: a -&gt; Maybe a doesn't that make the inverse method equivalent to the &lt;/&gt; method for reductive monoids? (I miswrote my code above to not take this modified inverse function into account) 
Can I mix and match letters and symbols?
Nope.
I settled on ``Ace `o` Spades``. By the way, what's a programmagic way to build a deck given the enumerator records for Rank and Suit? I imagine it would involve `fromEnum`/`toEnum`.
Well, no. Even if you restrict yourself to Abelian monoids, you can't write `inverse` for types like `Set` for example. Here's a GHCi session: $ ghci Prelude&gt; :module Data.Set Data.Monoid.Cancellative Prelude Data.Set&gt; let a = fromList "abcde" Prelude Data.Set&gt; let b = fromList "bcd" Prelude Data.Set&gt; a &lt;/&gt; b Just (fromList "ae") Now try getting the same result using `inverse :: Set -&gt; Maybe Set`. What would be the value of `inverse b`? 
http://stackoverflow.com/questions/10548170/what-characters-are-permitted-for-haskell-operators
Thanks, I'm honored! Seriously, though, really nice application.
Do a list comprehension: let deck = [ rank `o` suit | rank &lt;- enumFrom Ace, suit &lt;- enumFrom Spades ]
Backticks are hell to type on many non-US-English keyboard layouts.
Here's a hacky solution with some extra typing, but without backticks! I assume you have derived `Enum` for `Rank`. data OF = OF ace :: OF -&gt; Suit -&gt; PokerCard ace _ s = PokerCard Ace s -- or point-free two :: OF -&gt; Suit -&gt; PokerCard two _ = PokerCard Two -- or with const three :: OF -&gt; Suit -&gt; PokerCard three = const (PokerCard Three) -- you get the idea! -- the rest in one line: four,five,six,seven,eight,nine,ten,jack,king :: OF -&gt; Suit -&gt; PokerCard [four,five,six,seven,eight,nine,ten,jack,king] = map (const . PokerCard) [Four .. King] -- now you can write pokerDeck = [ ace OF Spades, two OF Spades -- and so on ] The OF datatype isn't strictly necessary, but prevents confusing (but very metal) stuff like `ace "Motorhead" Spades`. You can still write `ace undefined Spades`, you can get around that by writing `ace OF = PokerCard Ace` I think (pattern-matching semantics has always been a weak point of mine, you might need a `case` statement instead). If `of` wasn't a keyword, you could even write `of = OF`. ----- EDIT: Just thought of this utterly evil hack to get rid of the 'of' entirely, and using numbers for cards: {-# LANGUAGE FlexibleInstances #-} instance Num (Rank -&gt; Suit) where fromInteger n = (undefined : map Card[Ace .. King]) !! (fromInteger n) Now `2 Spades :: Card` typechecks (you need the explicit type!) and is what you think it is :-)
most keyboards can only generate one or two simultaneous key presses of keys in one group, with group being the layout of how your keyboard was wired internally.
see web-plugins which (is meant to) support(s) dynamic loading of modules as well. http://hackage.haskell.org/package/web-plugins-0.2.1
I'm going to offer a different perspective on this general issue, which I mostly hold: As far as laziness goes, it's a matter of philosophy: laziness is better from a high-level, semantics-oriented standpoint but strictness is easier to work with for performance. I almost never care about getting the best performance, but I *do* care about writing cleaner, more expressive, less coupled code that's easier to reason about, which laziness helps. And when I *do* care about performance, I'm willing to use a profiler or even do my own code generation (for sufficiently complex tasks). As a simple illustration, I think it's much easier to add strictness to a lazy language than vice-versa. Ignoring some issues with ⊥ (which we *can* ignore in practice), forcing something is a transparent action. The consumer of the data does not know the difference. On the other hand, in a strict language, making something lazy tends to require quite a bit of ostentation and, critically, needs some cooperation at the call site. Perhaps there's a way around this--maybe if you had a language that was both pure *and* strict--but it seems to be more annoying in practice. Also, again from the semantics point of view, non-strictness allows more programs to terminate and has less expressions map to ⊥. So, in a very specific sense, it's strictly more expressive. In my experience, it's also more expressive in the sense that it gives me more freedom to write and organize my code in what I find to be the most natural fashion. In general, I want to worry about fewer things. Since performance doesn't matter for the vast majority of code, things like the order parts of my program get evaluated in just shouldn't matter. In fact, I don't even want to worry about *whether* something gets evaluated: if I need it, it should; if I don't, it shouldn't. A non-strict language lets me do this. I actually started taking this for granted shortly after starting to use Haskell: I can do things like have local bindings to computations that do not happen every time a function gets called. This means I can extract an expression out into a where or let block without thinking about its context--one less thing to worry about! Really, that's the core reason I like purity, laziness and types: there's far fewer things to consider when writing my code. I don't have to worry about the order of evaluation. I don't have to worry about *what* gets evaluated. I don't have to worry about things getting evaluated multiple times. I don't have to worry about distinct parts of my program affecting each other. I don't have to worry about nearly as many runtime errors. I can write my code with far less thinking, far less incidental complexity, than in any other language. Also, FP is *not* a niche thing. Instead, it's a different *foundation* for programming. Instead of thinking about programming as a series of steps for the computer to take, we can think of it as transformations between data types. FP combined with type theory plays a very similar role in programming to set theory in math. IO and ST? Those are the niches: we only need them for very specific sub-tasks. And they happen to fall very well into an FP model--we can essentially express imperative ideas in terms of the simpler, more convenient functional foundation. Even if they're not actually implemented that way. More generally, I think that's a very important insight: functional programming as a different foundation. Most people seem to think of FP as "normal" programming with state taken out. This is only made worse by a bunch of "multiparadigm" languages which are mostly just imperative languages with some cursory, functional-inspired features tacked on. The important insight is that functional programming is different at its very core, a completely different way to view the world. This view, I think, also helps laziness make more sense. So yeah. I don't think FP is a "niche" at all, and I *do* think laziness by default is a good thing. Or, more accurately, I think that non-strict semantics are good: whether the language is actually lazy or uses some cleverer evaluation strategy is entirely an implementation detail. All that matters most of the time is whether the code *means* the same thing.
Your editor probably has support for character substitution in one way or another. For me, é § è ç à are the primary characters on the number row, but I don't ever use them in code. I should probably listen to myself and do this right now.
LambdaCase syntax is \\of, not \\case, isn't it? (I've not installed 7.6 yet, so can't check.)
It's `\case`; I made sure to check the [User's Guide for 7.6.1](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#lambda-case) while I was writing this.
You're right. But I never really liked the look of `case`. I think guards and function pattern matching are often easier to read. A lot of times I'll define a function instead of a `case`, if that helps. But if things get really tough - I might even use `(==)`. If `(==)` is the way to make the code most readable, then avoiding `(==)` is just an optimization. I guess my point is that it's all a matter of taste.
Some people (like my self) find list comprehensions a needless complication to Haskell syntax. You can just use the list Monad! deck = do rank &lt;- enumFrom Ace suit &lt;- Spades return $ rank `o` suit List comprehensions are just some syntactic sugar for the above. Alternatively you could be very concise and use Applicative style. Applicative style is nice for when you really don't need to bind a variable, you just want to feed it straight into a function. deck = o &lt;$&gt; enumFrom Ace &lt;*&gt; enumFrom Spades And all uses of enumFrom can be rewriten as [x ..], so: deck = o &lt;$&gt; [Ace ..] &lt;*&gt; [Spades ..]
In that example, should Nothing v be just Nothing edit: formatting
Cool! TIL about postfix operators and tuple sections. You could make the lambda-case example a little prettier, though. {-# LANGUAGE LambdaCase #-} import Control.Monad (forM_) main = [Just 1, Just 2, Nothing, Just 3] `forM_` \case Just v -&gt; putStrLn "just a single number" Nothing -&gt; putStrLn "no numbers at all"
This is a good idea but I feel like it should be built on top of the `semigroups` package instead of Data.Monoid. (&lt;&gt;) makes sense in too many cases where you lack an identity, and it's such an attractive operator that I am fully for `semigroups` claiming it. In addition it looks like some of these classes make sense for semigroups, too.
Cool looking 
MultiWayIf example looks like case. It would be better to rewrite it to use different comparisons. 
Where can I download it?
I've got trouble figuring out the usefulness of unicode syntax. I can't seem to find a decent way to input the symbols... Using alt codes from memory or using a (clickable) chart would be too slow. Am I missing something? Is there an automatic hconvert2unicode tool to pretty up my syntax?
 suit &lt;- Spades wouldn't work, should be let suit = Spades or suit &lt;- return Spades or (possibly what you intended) suit &lt;- enumFrom Spades
What if we change inverse to: inverse :: a -&gt; Either a a where Left indicates the "negative", and Right indicates the "positive"?
Thanks! I took your advice.
I changed it to better reflect what guards can do.
Not that I know of. Some of the symbols (like `≤`, `≥`, and `≠`) are typeable on a Mac keyboard using the `option` key, though.
There's persistent &lt;http://hackage.haskell.org/package/persistent&gt;. 
Don't confuse [simple and easy](http://www.infoq.com/presentations/Simple-Made-Easy)! If you want easy, try the [alligators](http://worrydream.com/AlligatorEggs/).
&gt; If an expression can be evaluated and terminate then some non-strict order will do so, but not necessarily the one that Haskell chooses. This is false. If any evaluation order not evaluating under binders results in termination than CBN results in termination, and Haskell's "non-strictness" is denotationally equivalent to CBN. 
If `Semigroup` gets merged into `base`, as I think it should be, I will gladly introduce `Reductive`, `Cancellative`, etc. as a replacement for `ReductiveMonoid`, `CancellativeMonoid`, etc. They would all subclass `Semigroup` instead of `Monoid`. (The `FactorialMonoid` class is naturally a subclass of `Monoid`, so that's a different matter.) As things stand now, however, I wanted to stay as close as possible to the orthodoxy. If it proves itself in the wild, I'm hoping the package could eventually become a part of the platform in some form. I have to choose my battles. 
That's because `&lt;/&gt;` is not total, but `CancellativeMonoid` *is* `Group` since it requires the totality of `&lt;/&gt;`.
Update: Three tutorials are up now. (I just published one on list comprehension extensions.)
Sometimes when I look at pages on there, the code flat out isn't visible. On Chrome 24 running on Kubuntu 10.10.
Can you think of a simpler graphical notation? Fewer graphical elements to represent the same structures?
What does the word "optimization" mean? Often we use optimization to refer to something merely getting faster/better by a little bit. This is not what I mean when I say that call-by-need is optimal. I mean that in a precise way. An optimization in this sense is something that picks "the best" or "the most" along some criteria. CB-need is (asymptotically) optimal along the criteria of *total work* or equivalently, sequential time. This asymptotical optimality is the normal meaning of optimal in computer science--it is what we use when we say that "merge sort is optimal" or "splay trees are optimal for all access patterns." I am perfectly happy to instead say "CB-need is within a constant factor of asymptotically optimal with respect to work" if that makes you feel better. I think this unnecessarily complicates things, but no biggie. This is *not* to say CB-need is optimal among *all* dimensions. It is not space optimal, or optimal w.r.t to opportunities for parallelism, or optimal w.r.t. cache or IO or anything else. CB-need does better than people often act along most of these dimensions, but that is not the optimality claim. What I mean is merely that in a *formal* sense, CB-need is an "optimization" over *all* other reduction strategies. Don't confuse CB-need/lazy evaluation with non-strictness. The benefits of composition, etc, arise from non-strict evaluation. The "natural" kind of non-strict evaluation is call-by-name (CBN) which is a perfect dual of the call by value popular in most language. CBN is the oldest and perhaps the most canonical evaluation order for the lambda calculus. The CBN/CBV distinction is about *semantics*. The CBN/CB-need distinction *is not observable* in a pure language, and is thus simply a question of *cost model*. Although Haskell does not specify an evaluation order (but the denotation means it has to be equivalent to CBN), and GHC does not do CB-need precisely, it is still the appropriate starting basis for asymptotical analysis of Haskell programs. The optimality of CB-need is easy to intuit: in CBV each possible reduction happens exactly once. In CBN each possible reduction happens 0 or more times. In CB-need each possible reduction happens 0 or 1 times. Theorem 1: Let a "legitmate" evaluation of an expression is an sequence of beta reductions converting that expression to a value such that no reductions happen under binders. For any expression, if there exists at least one legitimate evaluation, than CB-need will evaluate that expression to a value with work proportional to the shortest of all legitimate evaluations of that expression. Theorem 2: The above property does not hold if we replace CB-need with CBV or CBN. These are left as problems for the reader. Given these results we see that if a program runs in O(f(x)) under CBV it runs in O(f(x)) under CB-need. Dually if a program runs in \Omega(f(x)) under CB-need it runs in \Omega(f(x)) under CBV. This means that if what we care about is an upper bound for work, we can always start by pretending our language is CBV. We can't go the other way. CB-need might be **infinitely** faster than CBV. A terminating program under CB-need may not terminate under CBV. const 1 loopforever That is strict evaluation **inserts extra work!** Note: that although CB-need might take *arbitrarily* more space than CBV, if CBV takes finite space so does CB-need. *Arbitrary* is smaller than *infinity*, so this doesn't simply disappear into a "Space/time" duality. Most people recognize the fact above. What should be more widely known is that situation is better than that. I will attempt to communicate how this works, but I don't think my explanation of these ideas are that great. Let us consider only amortized upper bounds (of work). Most of the time in complexity analysis this is what we care about anyways, also compilers can screw with your lower bounds, and GC means that you have to deal with amortization. I think this is reasonable. Single numbers don't cut how long it takes to compute something under CB-need, since `fst (take_short_time,take_long_time)` is only `take_short_time`. We need a more complicated way of talking about time. The same problem shows up in CBV, however. Functions that take functions and return (data structures containing) functions need to account for how long they take in terms of the incoming functions, and need to describe how long the outgoing functions take. We care about how long it takes to *use* a value under different access patterns. We can assign to any value a "cost structure" based on the type structure: The cost structure of a product type is the tuple of the cost of forcing that thunk, and the cost structure of each of its components. The same construction is used for sum types. On the other hand, a function type's cost structure consists of 1. the cost of forcing that thunk 2. a function from cost structures to cost structures. We will, for the moment, ignore sharing. This is because sharing in data type (`let a = exp in (a,a)`) is only going to make things asymptotically faster, and that can be ignored and still get correct (but not tight) upper bounds. If you think of some complicated set of nested product types, than the time it takes to force a leaf (assuming we do no other computation) is the sum of the values to that leaf in the tree. The point of this model is to allow more interesting cost analysis than "just pretend CBV." Observe that this model is compositional. Knowing the model for subexpressions gives us a strategy for figuring out the model for bigger expressions. What is more, we can reason about blocks of code using sharing to get lower bounds, so long as we can cast the input and outputs of those blocks of code in these terms. Lemma 1: in the case of a product or sum type, we can "move work" up from one of the children thunks to the parent. This is because, given any access pattern we will need to evaluate the parent before we can evaluate the child. Lemma 2: insertion of an operation with the cost semantics of "deepSeq" (forces everything recursively) can never make a program asymptotically faster in this model. This is easy to show inductively by lemma 1. deepSeq simply moves all of the cost to the top of a product/sum based type. As such, it is an accounting trick and can be removed without slowing down the program (asymptotics again). So here is the trick. Say you have a function you want, and you want to know how long it takes. You can always just pretend *that function* to be CBV by inserting accounting deepSeqs in appropriate places. The point of this was simply **you don't have to do whole program analysis**. You can assume CBV most of the time, and only use the CB-need structure to get tighter bounds on small pieces. You do get constants. I concur. Although GHC is pretty amazing in the effectiveness of its strictness analysis. Strictness analysis is actually not so hard these days, and so mostly the overhead is zero. I know of know equivalent result for automatic insertion of laziness. All of this was about **work**. You also talk about cache. The cache story is more complicated. And that is that cache effects, and memory IO, and all of that mess, are products of a particular * architecture*. Caching is the ugly solution to the von Neumann bottleneck. It isn't a fundamental part of computation the way work, space, and even parallelism seem to be. And because it is a product of instruction oriented machines, Blelloch and Harper notwithstanding, it is fundamentally imperative in nature. You need to analyse imperative programs to really think about the details of particular cache architectures. So, cache aware programming is going to tend to be imperative. C'est la vie. We have imperative sublanguage. The ST monad is pretty solid and using Haskell like it is C is not so hard. It is possible that there are opportunities for *cache oblivious* lazy algorithms, but I am not sufficiently an expert in that area. On the other hand, non strict evaluation allows for *compositional programming* and combined with purity and effect tracking this many enables library specific optimizations. This can lead to much better code from a cache perspective, since your loops can get fused for you. Now, some Haskell libraries are even getting prefetch instructions into the compiled executables. I think the future in cache tight scientific programming is going to have Haskell beating out all but the most tuned C and Fortran programs because of these optimization frameworks. My own preference would be to not have to choose between lazy and strict evaluation. Ideally we could have this encoded via polarity in the type structure. How it is, I use both strict and non-strict languages, and since most language are strict, Haskell being non strict is desirable *even if* strictness is preferable most of the time (which I do not believe). But, I have found it much easier to do strict programming in non-strict languages than vice versa.
What's wrong with both being superclasses of `Group`?
Consider an operator that has two arguments. One terminates, the other doesn't. Happily, in this case, once you have successfully evaluated one of those arguments you won't need to evaluate the other - perhaps it's a boolean `and` where the successful argument happens to evaluate to false. Perhaps it's a multiplication where the successful argument happens to evaluate to zero. Which incidentally may not even prevent the second argument from being evaluated (testing for the zero is extra work) but lets ignore that. The trouble is you don't know which of those arguments to evaluate first - you don't know which one will succeed. In the [Gentle Introduction to Haskell 98](http://www.haskell.org/tutorial/), you can find a simple example on page 17. Two following two implementations of the prelude `take` function are shown... take 0 _ = [] take _ [] = [] take n (x:xs) = x : take (n-1) xs take1 _ [] = [] take1 0 _ = [] take1 n (x:xs) = x : take1 (n-1) xs As this tutorial says... &gt; We see that take is "more defined" with respect to its second argument, whereas take1 is more defined with respect to its first. It is difficult to say in this case which definition is better. Just remember that in certain applications, it may make a difference. (The Standard Prelude includes a definition corresponding to take.) That is, the functions `take` and `take1` given here attempt to provide the same mathematical function, but each prefers a different evaluation order - in this case particularly for the pattern matching. Each will succeed in cases where the other fails to halt, depending on which of the two arguments happens to fail to halt, which in general is unknowable due to the [halting problem](http://en.wikipedia.org/wiki/Halting_problem). Saying "non-strict" or "call-by-need" does *not* fully define the evaluation order, and the choices that remain can make the difference between a program that works and one that fails to terminate. And if you're relying on laziness to ensure termination, for non-trivial code, you have a very complex web of data-dependencies to consider, including many that run through library code that you didn't write. 
Hey, he might just be working in an implicit [indexed state monad](https://gist.github.com/pthariensflame/5054294). ;)
Simpler /= simple.
Add a third dimension for types.
those are different functions, that is different lambda terms. There is only one evaluation order here. It is interesting to think about how CBN might not be "lazy enough." We can't (deterministically) do better, but if we have a proof that these two functions produce the same thing when they both produce values, Conal Elliott's `unamb` operator allows for amazing things. In this case takeLazier n xs = (take n xs) `unamb` (take1 xs) but, `unamb` is a fundamentally different kind of creature than what we are used to. With a reasonable cost model, `unamb` lets you solve NP hard problems in polynomial time! It doesn't solve the halting problem (you don't have to, you just run them in parallel). But, that is beside the point. That is, this is a non sequitur. If your program terminates with CBV it terminates with CBN but not vice versa. Given a lambda expression, either CBN terminates, or nothing terminates.
`lempty` and `mempty` would be different names for the same thing.
I plan on reading this more thoroughly, but I love this kind of thinking, taking one kind of abstraction and translating it into another. It shows neat patterns you couldn't otherwise see. 
&gt; What does the word "optimization" mean? Often we use optimization to refer to something merely getting faster/better by a little bit. This is not what I mean when I say that call-by-need is optimal. I understand that. If you evaluate the same expression both strict and lazy, the lazy version may do asymptotically less work, and never does asymptotically more. That sounds like a great advantage, but I don't think it really is. Programmers are perfectly capable of writing code without specifying huge amounts of redundant work. And even optimizers for strict languages can eliminate a lot of the redundant bits and pieces that still occur. Sure there's some nice idioms and patterns that exploit laziness, but targetted laziness would do the job. &gt; That is strict evaluation inserts extra work! No, strict evaluation doesn't insert extra work - it only fails to eliminate extra redundant work you happen to specify. Sure, if you specify an infinite loop, strict evaluation will fail to halt - so don't specify an infinite loop! After all, as I've pointed out elsewhere already, if you're relying on laziness to ensure termination in a non-trivial program you'd better be damned careful - left-to-right vs. right-to-left order can result in a program succeeding or failing to halt too, and due to the halting problem you can't know which to choose in general. &gt; We have imperative sublanguage. The ST monad is pretty solid and using Haskell like it is C is not so hard. I'm not convinced that's imperative enough. It doesn't force all computations to work in a specified order - only the primitive actions. Everything else in those expressions is lazy by default as usual. For example, take a look at the documentation for `modifySTRef` [here](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-STRef.html) - it isn't applied strictly, so you can easily be writing a deferred computation rather than an evaluated result. OK, the fix there is nice and easy, but that's just using mutable data structures. Even when you're e.g. writing an interactive program with heavy use of the IO monad, you may still want to use pure functional data structures. Even when you're using `ST` and/or `IO`, it's still easy for laziness to save up huge amounts of work until the worst possible moment. &gt; since most language are strict, Haskell being non strict is desirable even if strictness is preferable most of the time I disagree because laziness is not the only distinctive feature of Haskell. Even if evaluation were strict, keeping pure and impure code separate using the type system is still a good thing, even though as SPJ suggests it would probably never have happened. I've used a bit of ML and a bit of scheme, many times over the years. The first functional language I learned (a bit) was actually Miranda, a couple of decades ago. None of these had much appeal for me. Haskell does. Actually, I remember reading somewhere about a language that's basically a strict-by-default Haskell dialect. But looking for the reference (it might have been [the Disciple language and DDC compiler](http://www.haskell.org/haskellwiki/DDC)), I also found a broken news.ycombinator.com link with a Google snippet suggesting that the next version of Haskell would be strict by default. Maybe my imagination (it went away when I refreshed and I can't find it again) or maybe a past/future April fools or something ;-) &gt; non strict evaluation allows for compositional programming I agree, at least to a point, and it sometimes bugs me when I end up writing the same code several times in C++ because sometimes I need the extra results and sometimes I don't want the overhead of computing them. And I even recognise that this would still happen if you could target laziness where needed, because library function developers wouldn't know when the caller needs it. But then again that's hardly an everyday issue, and usually computing any extra results just to discard them is usually a trivial overhead - if most of the work wasn't necessary both ways, they wouldn't be extra results of essentially the same computation. 
Thanks for the expansion! This really sewed it up for me: &gt; By "effect descriptor", I just mean some type operator which describes some form of impure computation in the course of computing a value, be it a monad, an applicative, or whatever. I had to read the following many times to get a clue: &gt; If the types made a clear separation of what was supposed to be the effect part and what was the value part, then the ordinary whitespace could be overloaded as application for any notion of effect So just to check if I'm understanding right, suppose we have a list `[a]`. Applicatively, it's ambiguous whether to treat it as a list-effectful `a` or a null-effectful `[a]`. So apparently, this ambiguity is really what's blocking the overloading of ordinary whitespace for greater good.
In a language that didn't penalize deep accurate class hierarchies, I would agree. Haskell is not that language. I've built such a hierarchy before on multiple occasions. It is nigh unusable for library implementors. You get zero code-reuse, and every point in the lattice of laws and members requires an instance. As you add dimensions of laws the curse of dimensionality gives you an explosive number of points. Consequently, in Haskell, I've taken to adopting a more pragmatic approach, which is that I generally try not to introduce any type class that merely introduces a law, unless that law is accompanied by an operation. I have high hopes for the work that Jacques Carette has been doing to make principled ways to talk about lawvere-like theories as his notion of classes, his approach does seem to scale to big sets of laws, and he goes through and painstakingly names every law he introduces and plays cute games with pullbacks on his algebraic specifications to get it all to work out, but that isn't Haskell.
You really do want the inverse relative to something. Consider the integers under multiplication. The only items that have proper inverses are 1, -1. However, we can still do integer division when given a divisor. His is basically the same API I expose for this purpose. =)
&gt; those are different functions, that is different lambda terms. There is only one evaluation order here. You are missing the significance of the pattern-matching order. Choosing to match left-to-right is just as arbitrary as choosing to evaluate the arguments of an primitive operator left-to-right. Besides, unless you can magically choose which implementation to use, the problem is ultimately exactly the same - whether the evaluation order is chosen by the language, the compiler, or your particular function implementation, there is no one perfect evaluation order for all contexts where you might re-use that code. Each order will fail for some cases where an alternative order would succeed. &gt; That is, this is a non sequitur. If your program terminates with CBV it terminates with CBN but not vice versa. Given a lambda expression, either CBN terminates, or nothing terminates. You're assuming the same code either way, which completely misses my point. If you're writing for strict evaluation you simply won't add all that redundant work assuming that laziness will eliminate it. It's normally very easy to see when a strictly evaluated expression will halt and, just as important, when it will fail to halt. With strict evaluation an infinite loop is an infinite loop, so you don't hope that laziness will eliminate it - you simply don't write infinite loops. Writing code that guarantees to terminate with strict evaluation is mostly pretty easy, and when you know an expression will evaluate successfully using strict evaluation, you also know it will evaluate using lazy evaluation, but you're not *relying* on lazy evaluation to ensure termination. Meaning non-strict evaluation is safe but, in halting terms, unnecessary. So the point is basically that, outside of some particular patterns and idioms that exploit laziness (and which don't need the laziness to be by default) if you want to ensure termination you pretty much end up writing code that would work for strict evaluation anyway. BTW - there seem to have been quite a few non-termination bugs in GHC, and Google has no trouble finding old non-termination bugs in darcs, xmonad and more. Of course all non-trivial code has bugs, and Haskells fine-grained static typing does a good job of preventing bugs, and people who live in C++ houses and all that - but non-termination bugs do seem to be a real-world issue in Haskell. 
After more searching, it seems that next-Haskell-strict-by-default was confusion - either mine or whoever wrote the line I now can't find again. What I did find was this... &gt; Any successor language [to Haskell] will have support for both strict and lazy functions. So the question then is: what’s the default, and how easy is it to get to these things? How do you mix them together? So it isn’t kind of a completely either/or situation any more. But on balance yes, I’m definitely very happy with using the lazy approach, as that’s what made Haskell what it is and kept it pure. http://www.techworld.com.au/article/261007/a-z_programming_languages_haskell/?pp=7 Well Haskell already has support for strict and lazy functions - lazy by default but you can force strictness where needed. He does say "So the question then is: what’s the default", but shows no sign of concluding that the default should be strict. 
Can you nest multiway if's?
I was actually just looking for the same thing. chrisdone's snap-app package has some very simple migration code for postgres: http://hackage.haskell.org/packages/archive/snap-app/0.2.1/doc/html/Snap-App-Migrate.html And here's an example migrations list from ircbrowse: https://github.com/chrisdone/ircbrowse/blob/master/src/Ircbrowse/Model/Migrations.hs
For me, the alligators just confused the issue. Then again, I found the lambda calculus itself intuitive without any visual aides, so I'm not probably not in its audience. 
I have used [dbmigrations](http://hackage.haskell.org/package/dbmigrations) for a couple of years in production. It it based on hdbc, and works as advertised. That said, I can't recommand it, because managing the graph quickly becomes to much a burden, to the point that after a few monthes we started linearizing the whole graph, defeating the point of the library. But even then, exploring recent migrations remains cumbersome. 
Additionally to being hard to input, they're hard to read. Most Unicode symbols render poorly on most systems. The font weight differs from normal characters, the kerning is off or aren't even monospaced, vertical spacing is awkward and some symbols require a fallback font different from your normal code. Simply put, it's ugly.
I expect laziness in Haskell, and don't find things like the semantics of `modifySTRef` surprising (or at least not now, I might have been burnt by it once). I learned to use `$!`. Laziness makes me more productive. But that is not why I am here. Your initial claims about it being hard to reason about the work performance of lazy code were my complaint. &gt; But then again that's hardly an everyday issue, and usually computing any extra results just to discard them is usually a trivial overhead - if most of the work wasn't necessary both ways, they wouldn't be extra results of essentially the same computation. The way I program, it is. 
I think I've come up with [a simpler one based off of it](http://i.imgur.com/XvrWiOa.jpg), but I still have to check with more cases and decide on a convention. It's the same structure, but feels a little more organic and less scary. At least, to me.
Great. I had seen this presentation at a different conference posted before, but the quality was not very good. I'm glad to see it posted with high quality audio and slides.
There was quite a large (bikesheddy) discussion when the feature was implemented in which almost every imaginable option was proposed. Personally, I was for `\of`, but `\case` got the support of the implementers.
Yes, but multiway if's don't affect layout so you'll probably need to use parenthesis to ensure the clauses are associated with the right if.
&gt; It's the same structure Then it's not more simple.
I tried doing [`λn.λf.λx.n (λg.λh.h (g f)) (λu.x) (λu.u)`](http://i.imgur.com/pszXz0L.jpg) and I couldn't really tell if I was doin it rite. There doesn't seem to be an example similar to this.
&gt; Haskell is not that language. Is there a language that it could be though? Is Jacques Carette working on an actual language, or is this mostly theoretical work? Even so - is there anywhere we can see that work? I'm very interested in rich hierarchies.
I don't even know what this is saying. Some of it is clear, but some of it is incomprehensible, much like the original. :(
I do agree, though am a bit fond of ∀ which doesn't suffer in the same way as the various Unicode arrows.
Am I misreading or are λx.λy.yx and λx.λy.xy represented the same?
My script reproduces all of his examples up to Omega, so it is with confidence that I tell you: almost! *********************** * *********************** * * *********************** * * * * *********** *** *** * * * * * * *********** * * * * * * * * * * ***** * * * * * * * * ***** * * * * * * ***** * * * * * ***************** * * * ********************* * * 
Ahhh right, so it was the lambdas on the right I confused. Nice!
For me it helped to adjust the dimensions of the lines a bit, like so: S x ─┬─────── y ─┼───┬─── z ─┼─┬─┼─┬─ ├─┘ ├─┘ ├───┘ (for `λx.λy.λz.(x z)(y z)`) Basically, draw one horizontal line for each variable captured by a lambda, then to apply e.g. `x` to `y`, draw vertical lines down from each of those variables' lines and connect the two with a new horizontal line. That's pretty much it; each "bottom-right corner" is an application.
I believe the lines connecting `(λu.x)` and `(λu.u)` with what they're being applied to should go straight down the middle, where you have just a stump (rather than to the left). This is what I got (with added labels for the variables): n ─┬──────────────────────── f ─┼────────┬─────────────── x ─┼────────┼──────┬──────── │ g ───┬─┼─ u ─┼─ u ─┬─ │ h ─┬─┼─┼─ │ │ │ │ ├─┘ │ │ │ ├─┘ │ │ ├────┘ │ │ ├───────────────┘ │ ├──────────────────────┘ Edit: well oops, looks like I should've refreshed before I posted...
Ah, of course. Thanks.
There is nothing on their [website](http://keera.co.uk/blog/) and nothing on their [GitHub](https://github.com/keera-studios).
Why aren't you using liquibase still?
Have you ever heard of the [compose key](http://en.wikipedia.org/wiki/Compose_key) in Linux? I have caps lock mapped to it on my keyboard. I don't generally use the UnicodeSyntax extension, but if I find myself working with code that does use it, I use the compose key.
I basically get the system. I just don't think it's very simple. It's a bit problematic. At the very least the stark geometricity of it is confusing. It needs to be visually neater. But I also don't really think it's clear how whole lambda terms behave in this system. The outgoing lines should be for return values, but they also seem to be used for the whole lambda term. This is a problem.
Functional befunge!
https://github.com/ekmett/algebra/blob/master/src/Numeric/Partial/Group.hs
Maybe we could actually spare the long horizontal bars? S x ┬ y ┼───┬ z ┼─┬─┼─┬ ├─┘ ├─┘ ├───┘ Or even: S x ┬ y ┼ ┬ z ┼ ┬ ┼ ┬ ├─┘ ├─┘ ├───┘ Here “beginning vertical lines” would signal abstraction, “ending vertical lines” would signal use/application.
Ok, the former would then get somewhat verbose (I hope I got it right?): x ┬ ┬ y │ ┬ │ ├─┘ │ y' │ │ ┬ │ ├──┘ └───┘ While drawing this, I see what pygnisfive was getting at :) So it looks like the horizontal bars for abstraction are indeed necessary.
Mm, I'm not proud of it. It's not particularly decent, and it's very specific to my snap-app lib which is in turn depending on my pgsql-simple library. I would be interested in one similar for postgres-simple. It doesn't have to be clever. Migrations generally start high and decrease over time as a project matures, so simple is fine. I think what was said about the dbmigrations library corroborates this.
More of an implementation concern, but how would you deal with the case of multiple derived classes providing (possibly different) implementations for the base class? For instance class Semigroup a where mappend :: a -&gt; a -&gt; a class (Semigroup a) =&gt; Monoid a where mempty :: a class (Semigroup a) =&gt; SomeAlgebraicStructure a where {- class functions -} instance Monoid [a] where mempty = [] mappend = (++) instance SomeAlgebraicStructure [a] where mappend = {- implementation possibly different than (++) -} That might not make much sense for the list instance of `Semigroup`, since other implementations probably won't be logically valid, but it may be reasonable to do for other type classes.
These are really nice tutorials. Thanks for investing in Haskell.
It lead to problems with people looking for how mappend was implemented: they would first go for Semigroup, if it's not there, they might look in Monoid, but how many people would look in MonadWriter, where by your proposal it could well be? What if you use an obscure class that has a Monoid depend, or possibly one you've written yourself and don't export? It could very well be difficult to find where the definition of a function is with this extension.
The only sensible answer is that this would be an error, as you've given conflicting instances for `Semigroup`. I don't see a reason this error would be tricky to detect... indeed, it seems like desugaring in the obvious way, by turning the one instance declaration into multiple, does the right thing.
This is what I was thinking.
You just provide a default fmap in the Monad class. Then, unless there is an explicit definition given somewhere else, use the default one.
I like the looks of groundhog. http://hackage.haskell.org/package/groundhog
This is a good point. Maybe haddock would be able to somehow automatically point to the correct spot.
That's exactly right, yes.
This is how it works in the [Frege programming language](https://github.com/Frege/frege). The frege compiler does not check for implemented instances, but for implemented class methods from super types. If everything is there, it creates missing instances on the fly. This works, because instances are name-less. However, Frege's type classes are not as powerful as GHC Haskells. (It's basically Haskell 2010, minus restrictions on the use of type synonyms). So I cannot say if this approach could co-exist with GHC's more advanced extensions. 
Right, thanks for the catch.
Didn't Conor McBride suggest something similar the last time there was a big squabble on the Haskell mailing lists about the Functor &lt; Applicative &lt; Monad hierarchy? I think it may be implemented in SHE.
I just had a look at the liquibase website. This tool looks absolutely insane.
I have to say I love your humorous bits. When I clicked the "elegant" button on the File Input line, I laughed out loud. Thanks again for sharing these with us!
awesome - thanks
I see these .hi files all over my projects, but I don't properly understand what's inside of them beyond "compiler junk." Where can I find more detailed documentation?
https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/superclass.html
Strictness doesn't just mean `$!`. It also means `seq`, `deepseq`, `$!!`, bang patterns and *all* the strict variants of library functions. Of course in a strict-by-default language, the need for non-default laziness wouldn't end with a `lazy` pseudo-function, but the point is that adding explicit strictness isn't as trivial as you make out. 
Insane as "not sane" :) The design does not make sens to me. The fact that the logic must be expressed in XML, while SQL is a high level and standardized language, sounds particulary insane to me indeed. 
XML is a poor choice, but SQL is inadequate to express changesets.
http://hackage.haskell.org/trac/ghc/wiki/Commentary/Compiler/IfaceFiles is the go-to place about that I guess.
I have your `gtraverse` as `gtraverse (For :: For GTraversable)` in my [one-liner package](http://hackage.haskell.org/packages/archive/one-liner/0.2.1/doc/html/Generics-OneLiner-ADT.html#v:gtraverse). It is implemented with `buildsA`, but a direct implementation like you suggest is probably faster.
Nice paper - I was inclined to point out Keenan's representation when I first saw your submission (even though I find it less than obvious, the Mockingbird paper is well known). The bubble notation seems pretty nice but I felt a bit like I was reading a paper about xeyes :-). I like the compactness &amp; simplicity of your notation. Have you considered how reductions could be animated?
Yes, I considered animation and put a sample one on my webpage at http://www.cwi.nl/~tromp/cl/graphical.html, but still find it rather unsatisfactory. Diagrams are mainly meant as a static notation.
I'm not really a fan of these database-agnostic/ORM abstractions. They suffer from the lowest-common-denominator problem. For me, the relational model is elegant and Postgres seems like so far-and-away the obvious choice for a data back-end that I'm not very interested in using anything else. Persistent seems to give me a only little subset of Postgres. That said, I'm a fan of the rest of the Yesod components.
Trivial reasons: I don't want XML everywhere and I don't want to install or maintain a Java stack. Other than that, Liquibase works fairly well.
Heh, I just noticed your username and remembered your short binary coding of lambda terms influenced a part of my thesis. Small world :-).
Interesting. I do wonder whether there's scope for a real theory of patches for databases. The "dependency" idea is not one I've worked with. I'm not sure exactly what a dependency means in this context. What does it allow you to do? The documentation for this library seems to be scant/missing. Liquibase has the concept of patch "preconditions", which is basically a set of select statements which determine whether the DB is in a state such that the patch is runnable. For example, an "add column" patch would have as a precondition that the column does not yet exist. Another, possibly complementary, reasoning could have each patch list the things it touches, e.g. migration touches --------------------------------------------------- ----------- ALTER SCHEMA foo RENAME TO bar; foo CREATE TABLE foo.bar ( ... ); foo.bar ALTER TABLE foo.bar ADD COLUMN baz ...; foo.bar.baz ALTER TABLE foo.bar DROP COLUMN baz; foo.bar.baz ALTER TABLE foo.bar ALTER COLUMN baz ...; foo.bar.baz CREATE SEQUENCE foo.seq INCREMENT 0; foo.seq ALTER TABLE foo.bar OWNER TO baz; foo.bar INSERT INTO foo.bar ...; foo.bar (?) DELETE FROM foo.bar ...; foo.bar (?) UPDATE foo.bar ...; foo.bar (?) This could give you guarantees that certain patches can be concurrent (i.e. the order in which they run doesn't matter if the things they touch are disjoint).
Seems *extremely* experimental. How does relational stuff work in the presence of sum types? E.g. what does it mean to join? I'm not sure.
Your little migration library looks pretty similar to mine. This simple "serial list of migrations" approach seems most practical. All things considered, I'll stick with my dirty 50-line solution and maybe release it if it looks at all respectable some time.
I think the tricky bit comes if the Monoid instance is in package A, the SomeAlgebraicStructure instance is in package B, and then I'm trying to build package C which pulls in dependencies on A and B in an way I don't expect (or even want). Or we could say that providing a parent-class method in a module which does not define either the type or the parent class is a new type of orphan instance, so you at least get a warning.
By deforming the alternative fixpoint diagram a little we get something that actually looks like a Y: ![Imgur](http://i.imgur.com/1l3iFV9.jpg) Might make a nice logo... Can someone tell me what's wrong with my image syntax? I followed http://daringfireball.net/projects/markdown/syntax#img but fail to get an inline image...
For those who missed the announcement on haskell-cafe (like me), here it is now. Great news! Now cabal-dev installs directly via `cabal` again, without having to get it from github.
Do you happen to know if there's a tool out there for tracking the name mangling/lifting between source code and what you see in Core/Iface? Figuring it out isn't *too* bad once you get used to it, but if there were a tool other than intuition, then presenting Iface information would make for a nice IDE tooltip...
I guess it's time to move to 7.6
Good idea! Didn't quite work for me, but this did: pokerDeck = [ r `o` s | r &lt;- [ Ace .. ], s &lt;- [ Spades .. ] ]
[ x .. ] is just syntactic sugar for enumFrom x, so that should definitely not be what broke it.
It should build there too. Bug reports if you find otherwise :)
I have been annoyed before by how haddock doesn't give source links for instances, so I would say this is already needed.
&gt; Strictness doesn't just mean $! Yes. But *most of the time* haskellers want to/expect to keep their lazy coinductive data unforced. When using the ST monad, `$!` gives an easy way of having the reference cell be evaluated--which was your complaint before.
You're adding additional features now that are somewhat less clear. It's fairly obvious (to me, at least) what it means to define superclass methods in an instance declaration: it means also derive the superclass, and define the methods that way. It's far less clear what it means to define a *default* superclass method in a class declaration. Does that mean: * Any declaration of `instance Monad Foo` also defines a `Functor` instance? If so, that break huge amounts of existing code that explicitly declared `Functor`. * This only generates a `Functor` instance in case there's no `Functor` instance already around? This still breaks a lot of existing code that defines orphaned instances for `Functor` (or even worse, `Applicative`) -- something that is fairly common to do because many older libraries don't have instances. * That the dictionary generated depends on whether a `Functor` instance is available at the call site? That breaks coherence, and can lead to all kinds of bad things.
&gt; Or we could say that providing a parent-class method in a module which does not define either the type or the parent class is a new type of orphan instance, so you at least get a warning. It's actually pretty much exactly the same type of orphaned instance. And yes, leads to the same kinds of problems. All that changed was the need to repeat the `instance ... where` line for the superclass.
Certainly the proposal as written here doesn't require any kind of change to the `.hi` file format. It just allows you to define instances of entire class hierarchies more concisely (and incidentally, happens to be source-compatible with moving methods to superclasses). There have been other proposals involving default implementations of superclass methods in the *class* declaration, which would indeed be intrusive.
Interesting, I haven't seen your library before. Some questions: 1. What are the advantages to making the class constraint an associated type, rather than a class parameter (as in syb-with-class)? 2. Why do you need to treat recursion specially? Would there be any problems with mutually recursive types? I also started to write a generics library: https://github.com/feuerbach/traverse-with-class.
Sure. Suppose in one module you have: module A where data Foo a = ... instance Monad Foo where ... And then in a different module: module B where import A instance Functor Foo where ... This is currently valid Haskell (though you will be warned about the orphan instance if you turn that warning on). But suppose you refactored Monad to be a subclass of Functor, providing a default implementation of fmap. Presumably, module A would get that default implementation, since you have no way of knowing that module B will ever exist when compiling A. So now module B fails to compile because A already exports a Functor instance for Foo.
You would still have to hint the system so that "ADD COLUMN baz" is applied before "DROP COLUMN baz". So really, a simple linear history system is fine. Next time I really need that in haskell, I'll define a trivial text file format, with a parser that can extract successive migrations from it : --: 1 :-- ALTER SCHEMA foo RENAME TO bar; --: 2 :-- CREATE TABLE foo.bar ( ... ); --: 3 :-- ALTER TABLE foo.bar ADD COLUMN baz ...; I think we can afford to store identifiers for *all* applied migrations (rather than only the greatest), so that we can remove the constraint that identifiers must have total order. That way, an identifier could be a short changelog, if the user wants so. *edit : typos*
May is a good month for hackathons! Europeans, I'll see you in [OdHac](http://www.haskell.org/haskellwiki/OdHac)!
Yeah, does anyone know who that /u/simonmar troll is, always spouting off about GHC too? ;-)
Thanks for the glad hand. Some questions. Should all superclasses give rise to automatic superclass instance generation? How do I switch off superclass instance generation (e.g. when you make Monad and Traversable instances which compete to make Functor)? How do you know your proposal does not break existing code? It'd be lovely to make progress on this issue. I bet HQ would implement this functionality if the community reached a consensus. But there do appear to be corner cases and conflicts to resolve. The [relevant page](http://hackage.haskell.org/trac/ghc/wiki/DefaultSuperclassInstances) on the GHC Commentary records at least some of the discussion. *(Edit: added link to Commentary page)*
I'm leaning towards hsenv these days.
In persistent's defense, you don't have to use the ORM abstractions, you can write raw SQL as well. And Felipe Lessa has done some pretty interesting work [in this area](http://blog.felipe.lessa.nom.br/?p=68).
The answer on #2 is: The recursive positions don't always do the same as the top level instance. I noticed this when I wanted to implement the uniplate methods. But these methods don't make sense for mutually recursive types. In that case you can just implement `buildsA`, `buildsRecA` has a default implementation in terms of `buildsA` that throws away the function for the recursive positions. I like how in your library you can choose which subparts are interesting based on the class you're implementing, that could really speed things up in some cases. The answer to #1 is that I did not see the advantage of using MPTCs, and I prefer associated types in general. But now I'll have to rethink that. I'm not so sure what advantage you get by using an implicit parameter?
Why so? I'd like to give it a go, but it appears that hsenv is not Windows compatible. IIRC, some time ago I decided to get the hsenv sources and try to modify it to build on Windows, which I succeeded in doing, but didn't have enough time or motivation to fully verify the functionality.
It seems more complete, as the whole tool-chain needs to call `cabal-dev x` or you have to specify `-package-conf …` to all the tools. With hsenv it does that using environment variables without any need to change the way you invoke things. And `cabal-dev ghci` just broke for me recently due to the way it handles the piped process and some version mismatch.
It is definitely exploring some less tested ideas, but I like the approach it is taking and think it is worth watching.
I'm using the sandbox support in the github version of cabal-install and so far it's been pretty good in spite of it being still unfinished and under development.
What's the experience like and features? Does it handle having a completely empty (except base) package set, or does it always include the main user/global packages? That's a [problem in cabal-dev](https://plus.google.com/114991347543804898741/posts/B6f9hCnTk6f) and I suspect hsenv (but haven't checked yet).
I know this isn't strictly Haskell, but I feel Idris has sufficient overlap with Haskell that this video may be of interest to many people here. If you feel otherwise, please let me know (:
Idris is just fine.
&gt; Does it handle having a completely empty (except base) package set, or does it always include the main user/global packages? That's a problem in cabal-dev... I agree that's a problem. It doesn't affect me though. My workflow is: install haskell platform, cabal update, cabal install cabal-dev, and never use cabal again. That process only takes a few minutes, so re-installing HP and GHC is very cheap and I can do it all the time. E.g., when I make a mistake and type cabal instead of cabal-dev. :)
Great to hear! Looking forward to that going mainstream soon, I hope.
How does hsenv works with emacs haskell-mode? Can you configure haskell-mode to use hsenv? Can you run 2 separate projects in emacs using hsenv? 
Really? I understood it's just a different style of working. I could give you a long list of reasons why I wouldn't want to switch from cabal-dev to hsenv, but I won't bother, because I assume it's just a question of learning the other tool and getting comfortable with it. Kind of like with version control systems. One sure advantage of hsenv over cabal-dev is for people coming to Haskell from Python or Ruby, where the sandboxing tool is very similar to hsenv. That's not me though. The Github README casts doubt on whether hsenv would work on Mac OS X or Windows. That seems weird to me - I can't imagine why it wouldn't work on Mac OS X, at least. And if something as crazy complicated as Cygwin can be gotten to work pretty well on Windows, how could it be that something as simple and elegant as hsenv can't be gotten to work? But in any case, cabal-dev works flawlessly on all three platforms. Are you completely fluent in both tools and if so, can you point to real reasons why one is better than the other?
What do you mean by "all the tools"? I just run cabal-dev all the time, and it works. Many haskell tools that cause automatic builds, like keter and yesod, have --dev flags that cause cabal-dev to be used. Others, like xmonad, anyway just run ghc directly without cabal. ghci is a problem, though. It is sometimes fiddly about dependencies, and in that case can take a little bit of work to get `cabal-dev ghci` to run properly. If that's a total non-issue in every case for hsenv, then that's a definite advantage.
`GTraversable Monoid` defines how to do something *using* `Monoid`. E.g. if you want to "double" each element of the data structure (sorry, couldn't come up with a better example involving Monoid), you do let ?c = Proxy :: Proxy Monoid in everywhere (\x -&gt; mappend x x) a
&gt; I'm not so sure what advantage you get by using an implicit parameter? Often in a function you work only with a single constraint. It may get tedious to pass those proxies around. Here you just bind it at the beginning and the rest is magic. Or you even let it propagate — like in the definition of `everywhere`. (`p` is analogous to your `For`.)
&gt; One sure advantage of hsenv over cabal-dev is for people coming to Haskell from Python or Ruby, where the sandboxing tool is very similar to hsenv. That's not me though. I recently abandoned octopress in favour of hakyll, and one of the aspects in its favour was how foolproof cabal-dev was. I don't know ruby and the suggested rbenv broke so frequently and so mysteriously that I could never actually do any blogging.
Idris is definitely on topic here. It's written in Haskell, and it's partly based on Haskell. Thanks for the link.
&gt; Are you completely fluent in both tools and if so, can you point to real reasons why one is better than the other? I've been using cabal-dev for a few years. I use it at work and for personal projects. I think 100% of the Haskell projects in my control are in cabal-dev. The parts that bugged me a little are: * Having to invoke `cabal-dev` specifically instead of just `cabal`, `cabal-dev ghci`, `cabal-dev ghc-pkg` — and I seem to remember passing specific options to these because cabal-dev ignored them. * The way `cabal-dev ghci` would at one point seem to load all the packages up front, making start-up and restart times slow as hell. * Having to specifiy `-package-conf` to tools and any tools I'm writing, like Fay. I work on Fay with cabal-dev but commands require special invocations and we added an environment variable for the package-conf to avoid some work. So we're already half way to reproducing hsenv. Positives: * The `cabal-dev add-source` command is convenient for using your own local libraries. I use this a lot at work. * One can symlink `cabal-dev` in five projects which all share the same package set. I do this with Fay and its related projects. Someone showed me hsenv in person and I quite liked the look of it. I just started using hsenv for my new dedicated host on ircbrowse.net and hpaste.org and such. It worked, it was convenient. It also supported using different GHC versions. Given that I have restrictions about the GHC version I can use at work, this is useful. I think the symlink thing mentioned will work just the same across projects. The only thing I'm not sure about is `add-source`. &gt; The Github README casts doubt on whether hsenv would work on Mac OS X or Windows. That seems weird to me - I can't imagine why it wouldn't work on Mac OS X, at least. And if something as crazy complicated as Cygwin can be gotten to work pretty well on Windows, how could it be that something as simple and elegant as hsenv can't be gotten to work? But in any case, cabal-dev works flawlessly on all three platforms. I don't think they've tested it on OS X. I expect the shell scripting and environment parts are what would be hard on Windows, but everything is hard on Windows. &gt; because I assume it's just a question of learning the other tool and getting comfortable with it. Kind of like with version control systems. I don't think it's as arbitrary as order of use. There are genuine technical differences as above. Anyway, I said _leaning_.
This doesn't play very nice with multi-parameter typeclasses. A contrived case: class Multi a b where multi :: a -&gt; b -&gt; Int class (Multi String a, Multi Bool a) =&gt; Foo a foo :: a -&gt; Int foo x = multi "xyz" x + multi False x instance Foo MyType where -- how do you define the two versions of multi??
Got it.
OK, cool, thanks. &gt; The cabal-dev add-source command is convenient for using your own local libraries. I actually use yackage servers for local libraries. That way I can move around from machine to machine and only update the repo I am currently working on, without keeping track of every local dependency. I only have to make sure that I am connected to the right yackage server for that project (since I have several). I imagine that technique would work fine for hsenv too.
&gt; One can symlink cabal-dev in five projects which all share the same package set. I do this with Fay and its related projects. Oh, nice idea, thanks.
I still don't quite understand how the "then" list transformation works. Could someone fill in how the unsugared "bnodo" function has too look? {-# LANGUAGE TransformListComp #-} orig = [(x,y) | x &lt;- [1..2], y &lt;- [1..3]] {-a = [(x,y) | then reverse, -- disallowed at the start x &lt;- [1..2], y &lt;- [1..3]] -} b = [(x,y) | x &lt;- [1..2], then reverse, y &lt;- [1..3]] bnodo = undefined c = [(x,y) | x &lt;- [1..2], y &lt;- [1..3], then reverse] cnodo = reverse $ [1..2] &gt;&gt;= (\x -&gt; [1..3] &gt;&gt;= (\y -&gt; [(x,y)])) (I really appreciate the guide btw!)
It always includes the global packages (but not the user packages) as there's no real way of excluding them.
Really enjoying this so far! Hope to see more of Idris on here in the future :)
Oh yea, I always forget it's written in Haskell! Something almost paradoxical about having a more powerful language written in a language that I already view as extremely powerful... but it's gotta be written in something, after all (:
I've been using hsenv on Mac OS X for about a year; no problems.
I haven't used it from Emacs yet, but once I do I'll add hsenv support.
I stick to hsenv for 1 definite reason, and 1 less definite: * Definite: I can use a more recent GHC, since hsenv support specificying a tarball containing the latest compiler installed in the sandbox I create * Less definite:I like being able to share a sandbox across projects--but without just sticking to the latest Haskell Platform, which is usually several revs behind GHC et al. But hey, if cabal-dev can use a different GHC in the sandbox, I might be persuaded. :)
One of the reasons why I set my keyboard layout to US (even though I'm European). The brackets [ ] and braces { } are also much easier to type on a US layout.
I have a toy language that I play with every once in a while when the class hierarchy headaches in Haskell get to be too much for me, but what I give up to get it to work is too much. I don't know if Jacques has released anything publicly. I saw an early pre-alpha a couple of years back and really liked the direction he was going.
Why are you using that for Y, and not `λf.(λx.f(x x))(λx.f(x x))`? That's what I always thought was known as the Y combinator. And besides, it would make your Y diagram look more like a Y. Just curious.
So far it's a good start. It will automatically build all my local dependencies if I "cabal sandbox-add-source" them, which was the first thing I was wanting. It's still missing the ability to manually uninstall packages in a sandbox, which is crucial when developing large multi-project systems. I've also been missing ghci support, but I think tibbe &amp; co have been working on that. The ability to copy sandboxes would also be nice as that would allow me to save time building a big dependency chain.
Sure, in fact there is only ever one valid Functor instance for any type. But the same is not true for Applicative.
Really?! First, I've never heard of the only one valid instance of Functor thing before. Got a link to a proof somewhere? Second, does it ever make sense to have an applicative not defined like this: pure = return (&lt;*&gt;) = ap ap :: (Monad m) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b ap = liftM2 id
One thing which does not work (or at least did not work with its precedessor) is trying to use packages which compile their configuration files, like xmonad.
Enjoyed a lot. Looking forward to the following series. Just a minor question. Can Idris show the code behind the tactic script?
This causes even more type envy. Augh!
Yes, you can either evaluate the name, or use the (undocumented) REPL command ':di' (for 'debug info'). I should probably tidy this up, because it's nice to see.
Don't think so: all the :.'s are in the typeclass constraints.
I tried to rewrite all the TransformListComp parts without the sugar, do these look correct? Also there is a mistake in the guide: The example for "then group using clauses" is impossible as "(groupBy (==))" is not of the right type ("Eq a" can't be used) [foo | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi, then f, xj &lt;- xsj, ... xn &lt;- xni ] == [foo | f x1 &lt;- xs1, f x2 &lt;- xs2, ... f xi &lt;- xsi, xj &lt;- xsj, ... xn &lt;- xni ] ------------------- [foo | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi, then f by exp, xj &lt;- xsj, ... xn &lt;- xni ] == f (\(x1,x2,...,xi) -&gt; exp) [(x1,x2,...,xi) | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi] &gt;&gt;=(\(x1,x2,...,xi) -&gt; [foo | xj &lt;- xsj, ... xn &lt;- xni ]) ------------------- [foo | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi, then group using f, xj &lt;- xsj, ... xn &lt;- xni ] == map unzipI (f [(x1,x2,...,xi) | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi]) &gt;&gt;=(\(xs1,xs2,...,xsi) -&gt; [foo | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi, xj &lt;- xsj, ... xn &lt;- xni ]) unzipI :: [(t1,t2,...tn)] -&gt; ([t1],[t2]..,[tn]) ------------------- [foo | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi, then group by exp using f, xj &lt;- xsj, ... xn &lt;- xni ] == map unzipI (f (\(x1,x2,...,xi) -&gt; exp) [(x1,x2,...,xi) | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi]) &gt;&gt;=(\(xs1,xs2,...,xsi) -&gt; [foo | x1 &lt;- xs1, x2 &lt;- xs2, ... xi &lt;- xsi, xj &lt;- xsj, ... xn &lt;- xni ]) unzipI :: [(t1,t2,...tn)] -&gt; ([t1],[t2]..,[tn]) 
There are at least two possible `Applicative` instances for `[]`. So yes, for any type isomorphic to `[]`, it might make sense to provide an orphan instance for `Applicative` that isn't the same as the `return` and `ap` for someone else's `Monad` instance. It wouldn't make sense to use them at the same time, but it might make sense to indirectly import modules that declare them both. It's definitely dangerous, but possible. And then, crucially, even if the orphan Applicative is compatible, the compiler cannot possibly check that fact in general. As for there only being one valid `Functor` instance for a type, I don't have a link to a complete proof. Maybe someone else can jump in with one. Google is failing me at the moment...
I prefer Y = λf.(λx.x x)(λx.f(x x)) being one variable shorter. As Wikipedia says "This fixed-point combinator is simpler than the Y combinator, and β-reduces into the Y combinator; it is sometimes cited as the Y combinator itself". I disagree on looks: ┬─────┬──── ┼─┬─┬ ┼─┬─┬ │ ├─┘ │ ├─┘ └─┤ ├─┘ └───┘ has too wide a base, and too tall a left edge.
Loving it. The last bit scared me a bit, though. using (xs : List a) data Elem : a -&gt; List a -&gt; Type where Here : Elem x (x :: xs) There : Elem x xs -&gt; Elem x (y :: xs) What I find disturbing is that `xs : List a` appears to be defined outside of the scope where `a` is bound. Obviously for simple programs like this it's not much of a problem, but for more complex programs it could create some nasty subtle bugs.
I think it's actually automatically binding `a` at the using declaration. Exactly the same thing in Agda is: module _ {a : Set} {xs : List a} where data Elem : a → List a → Set where Here : ∀ {x} → Elem x (x ∷ xs) There : ∀ {x y} → Elem x xs → Elem x (y ∷ xs) 
Hey! Sorry it took so long to respond. So here's some sample input: http://pastebin.com/NXqE6za4 So then I do: string &lt;- readFile "pokemon.xml" let root = fromJust $ parseXMLDoc string writeFile "replicatepokemon.xml" $ ppElement root And I get this as output: http://pastebin.com/V4jNWW7L This can't be right can it? Edit: Using showElement gives me this: http://pastebin.com/GdkBqZTY Edit again: Apparently it validates with my pokemon.rng file... but is there any way I can get rid of the ugliness at least? 
Yes, just [use this](https://github.com/tmhedberg/hsenv/blob/master/hsenv.el). Then you can do `M-x hsenv-activate` and select your environment. Your REPL etc will be updated. It just does this by modifying your `exec-path` locally (it doesn't override anything in `haskell-mode` really.) I don't know about the second question. Probably not from my understanding of how it works. NB: that 'fork' is pretty much the official repo now.
&gt; The only thing I'm not sure about is add-source. I think this is pretty much replicated by just activating the sandbox via `source` and going into that directory and `cabal install`ing it. `add-source` conceptually adds an extra entry in the set of packages for `cabal` to track when you later `cabal install`, while this really just blindly installs it up front, though. It's not 1 to 1 but it's roughly what you expect I guess. &gt; I don't think they've tested it on OS X. It works fine on OS X in my experience. I submitted some bug reports I think. Windows users are out of luck (surprise.)
Well, that's because `rbenv` is not a package sandboxer. It is a *ruby installation* manager, making it easy to quickly switch out versions and Ruby implementations for testing, development, etc. Each of these ruby installations has their own set of (global) gems. `rbenv` does not fix the sandboxing problem. This is actually a thing because - quite unlike us - they actually have competing Ruby implementations with lots of differences, and `rbenv` is tremendously helpful in making them all easy to use. What you're actually supposed to use is `bundler` to manage local gemsets. It is not entirely like `hsenv` or `cabal-dev`, but it is actually quite nice for what it does. Particularly, when you specify dependencies and install them it keeps track (in the source repo) of the exact dependency chain used, to make sure all developers/users get a consistent environment when they `bundle install` later. It can then be upgraded in lockstep with all developers, etc. This always ensures consistent versions, reproducibility of builds, failures, bugs, etc. There is no analog in cabal as far as I know. We just do the whole dependency thing differently. I say this because it's possible you were doing something wrong and were unfamiliar with the tools. It is very robust and good at what it does in my experience with it at home, work, and elsewhere (and indeed, it works well even for very large projects.) And I can say the very same thing of `hsenv` which I also use pretty much every day.
&gt; I think this is pretty much replicated by just activating the sandbox via source and going into that directory and cabal installing it. add-source conceptually adds an extra entry in the set of packages for cabal to track when you later cabal install, while this really just blindly installs it up front, though. It's not 1 to 1 but it's roughly what you expect I guess. Yep, not the same thing.
So it's not as scary as it looks. Like other dependently-typed languages, Idris has implicit arguments. This is to prevent absurdly large applications of trivial things. A fully-explicit list instance (using *Cons* instead of *(::)* for easier reading) might be something like: foo : List Int foo = Cons Int 1 (Cons Int 2 (Cons Int 3 (Nil Int))) This is because type and value parameters are basically the same thing. Note that due to the structure of *List*, where the second argument to *Cons* is always of the type given in the first argument, we don't need to provide the first argument when the second's type is available. This is a bigger deal for more complicated types like *Elem*. *There* has the following type when all implicit arguments and applications are made explicit: {a : Type} -&gt; {x : a} -&gt; {xs : List a} -&gt; {y : a} -&gt; Elem {a=a} x xs -&gt; Elem {a=a} x (y :: xs) This would be insane to write when the compiler can figure it out. While other dependently-typed languages such as Agda require you to write all of those implicit arguments, Idris follows roughly the Haskell convention that lowercase free variables are automatically converted to implicit arguments, but this only works when Idris can find out what their types should be. A "using" block is just a way to say that, in the body of the block, when certain names occur as implicit arguments, they should have certain types and come in a certain order. It's just a readability thing. edit: I am bad at Markdown
I fully admit to not understanding the ruby toolchain. All I did was follow the instructions given for installing rbenv and then using bundler as you said. But after restarting my machine some session details had been reset I think I couldn't be arsed fighting with it. I had already stumbled across two annoying bugs in the default markdown renderer so I figured trying something else wouldn't be a bad shot. 
http://dafuq.jpg.to/
spam
This seems like a great exercise for those coming from a python background. I would recommend defining the operators at precedence 6 or stricter, because (:) and (++) are infixr 5. This will enable you to write expressions like: ghci&gt; [1, 2, 3, 4, 5] !@! (2,4) ++ [5, 6] [3, 4, 5, 6] without parentheses. The standard operator (!!) uses infixl 9.
Good idea, I changed them to 8, so they're still lower precedence than !! so it's possible to index into sliced lists.
I feel compelled to add that I don't think you should really use these functions when working with lists. I just think it's great to try these kinds of things when learning a new language. Be aware that this implementation must traverse the entire list even when the slice is at the beginning (because you use the length function). Consequently, it will fail for infinite lists.
That's just the next challenge. Support infinite lists!
For the function it seems more natural to have the range as the first argument. Am I wrong? Maybe you are just reflecting the Python function which I don't know. 
If you wish to index into sliced lists you should declare them infixl 9 (the default associativity and precedence for operators). If you do so, then the following: [1, 2, 3, 4, 5] !@! (2, 4) !! 1 will be equivalent to: ([1, 2, 3, 4, 5] !@! (2, 4)) !! 1 as you intend. However, if (!@!) has precedence 8, (!!) will be applied first, resulting in a type error: [1, 2, 3, 4, 5] !@! ((2, 4) !! 1) -- oops, (!!) is not defined for (a,b) Declaring them with the same precedence also enables you to write: ghci&gt; [[1, 2], [3, 4, 5]] !! 1 !@! (1,3) [4,5] taking a list from a list of lists, and then taking a slice from that. The key here is the left-associativity and equal precedence of the operators.
I did submit a pull request to haskell/haskell-mode to try and integrate them so that you could have a hsenv attached to a session (and therefore a different hsenv for each session) it was sort of working for me a few months ago, but my elisp is pretty weak. Last time I looked the internal structure of a session in haskell-mode had changed enough that it wasn't easy to fix but there might still be some worthwhile stuff in there.
Yeah, that makes more sense for partial application.
You might be interested in taking a look at how indexing happens in the [lens](http://hackage.haskell.org/package/lens) package. `elements` allows for indexing based of `Int -&gt; Bool` [1..9] ^.. elements (`elem` [1..2]) [2,3] or interval n m x = (n &lt;= x) &amp;&amp; (x &lt; m) -- [n, m) in interval notation [1..9] ^.. elements (range 1 3) [2,3] or &gt; import Data.Sequence &gt; import Data.Sequence.Lens &gt; fromList [1..9] ^.. sliced 1 3 [2,3] I think you would have to build you own negative indexing however. I do not see many people complaining about it absence, though I do miss it every once in a while having used both python and mathematica.
Without a way to distinguish between infinite and finite lists, negative indexes are quite dangerous to use in Haskell. Python gets away with it because "lists" are indeed "arrays". I just went and checked [itertools](http://docs.python.org/2/library/itertools.html), and it doesn't have anything to "index" into an iterator at all, let alone do negative indexing. Which sort of corresponds to the fact that `!!` in Haskell is probably a code smell, too, for anything beyond screwing around in GHCi.
I can not think of the last few cases where I missed them. Next time I will write it down and see if the disre stand the test of time. The main reason I think think of negative indices being more dangerous then say `reverse` is there is little difference between the safe syntax, positive indices, and the dangerous syntax, negative indices. There is not much there to raise any alarm bells, doubly for people coming form languages where negative indices can not cause this problem due to the lack of infinite lists. Using different syntax for the two would solve that problem, but it does not seem worth it. A better solution would be use a different data structure that enforces it to be finite.
The problem with this is the use of type families. I initially tried something similar for type classing pipe operations, but it failed because type families are not injective. For things like `MonadLayerControl` you sort of get away with it because at least one term in the type signature is not the result of a type function, so the compiler can latch onto that for type inference. However, with pipes it does not work because every term in the type signature becomes a type function and the compiler has nothing to work with to infer which type class instance to use. I would really prefer something like "type lenses" so that we could define things more naturally, like: class MonadError m where throw :: e -&gt; (err := e) m catch :: (err := e1) m -&gt; (e1 -&gt; (err := e2) m) -&gt; (err := e2) m Heck, if you did it that way, you wouldn't even need a `MonadError` class. You could just define a `Monad` class that uses a lens to point to where the monad return value is supposed to be located: class Monad m where return :: a -&gt; (ret := a) m (&gt;&gt;=) :: (ret := a) m -&gt; (a -&gt; (ret := b) m) -&gt; (ret := b) m ... and then `MonadError` is just the special case of switching the lens to point to the error value. Of course, what I'm describing is nonsensical because how would the compiler know which lens to select! This is one of the reasons that I'm very unsatisfied with Haskell's brand of type class inference.
I'm not sure how much the following type signature might add, but is newtype Iso m = Iso { getIso :: forall a b . (m a -&gt; m b, m b -&gt; m a) } layerInvmap :: Iso (Inner m) -&gt; m a -&gt; m a possible?
&gt; The problem with this is the use of type families. How so? Do you mean the `Inner` and `Outer` type families of `MonadLayer` and `MonadTrans` respectively? `MonadLayerControl`'s `LayerState` is actually a data family, and that whole type class (apart from `zero`) is basically a rip-off of `MonadTransControl` from the `monad-control` package anyway, which is used by lots of stuff without problems. Do you have a concrete example of how the non-injectivity of type families would cause problems for the `Inner` and `Outer` type functions? Or maybe you could show us how your attempt to do something similar caused you problems with the `pipes` library? (By the way, the design decision with which I'm least pleased is that `MonadTrans` is instantiated on `* -&gt; *` types rather than `(* -&gt; *) -&gt; * -&gt; *` types, and the `Outer` type family is a hack to get around that. I could change this (and maybe I would change `Inner` to a fundep if I did this, I'm not sure) were it not for GHC bugs [#2893](http://hackage.haskell.org/trac/ghc/ticket/2893) and [#5927](http://hackage.haskell.org/trac/ghc/ticket/5927), which I understand caused you problems with your efforts to type-class the `pipes` library as well (I discuss this in the big `Documentation.Layers.Overview` module for which Hackage has unhelpfully not generated the haddocks yet). I would have absolutely no idea how to go about fixing these bugs (sure even simonpj isn't sure how to go about it, judging from the comments on those issues), but maybe we could somehow team up to get them fixed! :))
You *really* ought to wait for the documentation to appear on Hackage before announcing it to the world; many people will likely see that there is no documentation up, think something along the lines of "Oh well, not worth my time then" and then move on to something else.
Hmmm, interesting. That's pretty simple and it does makes it much clearer what `layerInvmap` means than the current type signature. It makes it slightly more awkward to call `layerInvmap`, but not my much, and it's rare you would need to call it directly anyway. For `transInvmap` though you would need something like: &gt; newtype Iso i i' = Iso { getIso :: forall a. (i -&gt; i' a, i' -&gt; i a) } Which isn't really that much nicer than just "inling" `Iso`.
Thank you, exactly what i was looking for.
Would it work to make `GTraversable c` a super class of `c` if needed instead of requiring `GTraversable c` for subterms?
Excellent! :-)
Well you could still only have just one `Iso` and just change the signature of `layerInvmap` to: layerInvmap :: Iso (Inner m) (Inner m) -&gt; m a -&gt; m a Or you could even just call it `Auto` (an automorphism is an isomorphism from a category to itself) and do: layerInvmap :: Auto (Inner m) -&gt; m a -&gt; m a Which isn't *that* goofy! Maybe if you did: type Auto m = Iso m m It wouldn't be so bad. I'm really on the fence about this. EDIT: I've spent so much time working on the documentation for this package that I'm writing Reddit comments in Haddock.
What do people think of the following? type Iso m n = forall a. (m a -&gt; n a, n a -&gt; m a) type Auto m = Iso m m type Homo m n = forall a. m a -&gt; n a type Endo = Homo m m layerInvmap :: MonadLayer m =&gt; Auto (Inner m) -&gt; Endo m layerMap :: MonadLayerFunctor m =&gt; Endo (Inner m) -&gt; Endo m transInvmap :: (MonadTrans m, MonadTrans n, Outer n ~ Outer m) :: Iso (Inner m) (Inner n) -&gt; Homo m n transMap :: (MonadTransFunctor m, MonadTrans n, Outer n ~ Outer m) :: Homo (Inner m) (Inner n) -&gt; Homo m n liftInvmap :: MonadLift i m :: Auto i -&gt; Endo m liftMap :: MonadLiftFunctor i m =&gt; Endo i -&gt; Endo m
I wonder if `Homo` should be `NaturalTransformation`/`Nat`/`η`? I'm not sure about `Iso` because isomorphism is more general than something restricted only to monads.
Hmmm, yes, natural transformations seem to be an appropriate concept here. (I'm not any kind of expert in category theory, I've no formal education in any of this stuff, I'm still kind of figuring it out.) But in a way, it doesn't really matter if I'm using exactly the correct terminology here: what I'm proposing are throw-away type synonyms whose only purpose is to make the type signature of the above functions clearer, and I think they do that (the terms I've appropriated from category theory certainly approximate the actual meaning of those types).
I this is possible in any language that implements the implicit calculus of constructions (like caledon, or Agda kind of): defn monad : [m : {ret : prop} prop] ({a} a -&gt; m { ret = a }) -&gt; ({a b} m {ret = a} -&gt; (a -&gt; m {ret = b}) -&gt; m {ret = b}) -&gt; prop 
A reply, but really a "related": is there a central place, perhaps on the Haskell wiki, that expresses all of these category theory terms in Haskell (assuming there are many more outside of the ones expressed here)?
I don't know myself, like I said in another comment I'm pretty new to this stuff too. That particular set of terms I found on the [Wikipedia article on Endomorphism](http://en.wikipedia.org/wiki/Endomorphism): there's a nice little diagram there in the section at the top of the article. The relationship between these concepts is pretty graspable I find.
I spent a long time getting a "Can't unify Int with Nat" error because the Nat constructor is a capital O but I was reading his example as a zero. &gt;_&lt;
Just on software engineering grounds I'd advise against the "negative index means from the end" semantic. It *always* leads to unexpected errors.
So the key problem I encounter when type classing pipe operations is specifying where to locate the input and output type variables for both the input argument pipes and the output composed pipe. If I use type families to simulate lenses, I get something like the following pseudo code (simplified to unidirectional pipes): class Pipe p where (&gt;+&gt;) :: (io := (a, b)) p -&gt; (io := (b, c)) p -&gt; (io := (a, c)) p The two input pipes and the output pipe are all wrapped in non-injective type functions, so the compiler cannot infer which class instance to select.
Start from idris-tutorial.pdf, it's quite short and teaches well, you'll avoid these kind of mistakes.
I think it always matters to use correctly terminology because that's when you can leverage intuition off users. If you start calling things by the wrong name, then you will confuse people.
I don't know... But what are you trying to achieve with that?
Have a look at http://www.yesodweb.com/
For a relatively "thin" server I'd just use [scotty](http://hackage.haskell.org/package/scotty).
Why not go all in and use fay [1] on the client side too :-) [1] https://github.com/faylang/fay
&gt; I would prefer if the merged version did not have this GTraversable c restriction for subterms Any particular reason? I've recently made `c` a super class of `GTraversable c`, so now we don't have an (explicit) `c` restriction for subterms. The advantage is that it also works with classes that we have no control over (like Monoid).
Yesod and Scotty seem nice. You can also try write your own with plain fgci, i.e. Not so good example, but my first steps: https://github.com/DeTeam/h-fcgi/blob/master/hello.hs 
I've used Yesod quite a lot. But, personally, for something like this, I'd probably use [Snap](http://snapframework.com/). This is just because I've found it simpler and more convenient than Yesod when I need something lighter-weight than a traditional server-side-heavy application. By "simpler", I just mean that I write less and more readable code.
I've been using a stack of Snap, Aeson and control-lens to quickly build JSON RPC layers at work. I've been thinking a lot about how this could evolve in the direction of a restful API with computed routes instead just the static endpoints I've been using. I don't see any reason why Yesod couldn't easily replace Snap in the stack. I'm using Snap for historical reasons. The introduction of Lens and the improvements to aesons ability to strip out Haskell record naming conventions have almost completely eliminated the need for extensive boiler plate in this kind of work. I'm planning to add do some blog posts with sample code as soon as possible. 
For completeness, there's also happstack. It's a bit lighter than snap and yesod, and rigging it up with aeson should be just as straightforward.
What's control-lens?
Recently I've been experimenting with Snap in combination with Ji, and I've found it very promising so far. Setting up snap (even on windows with Haskell Platform) was a breeze. Ji likewise is easy and comes with some pretty cool examples out of the box. I'm using them to avoid messing with GUI toolkits on windows. I like the idea of Ji pushing GUI updates to the browser from within Haskell. It allows me to have a single package and single exe that encompasses both backend and GUI (in my case, I'm communicating between the two using Chan, but I'm open to other suggestions). You also get type checking between GUI and backend code, which I'm not sure you can do with Fay since it's compiled separately as I understand it. Disclaimer: I'm not super knowledgeable on either Fay or Ji. With Ji I just set up my rudimentary Chan last night, so..
I use `snap-core`, `digestive-functors-aeson` and `aeson` as the stack for my current little web service.
I think he means `lens` (I.e. Control.Lens)
I write my apps against `WAI` directly. I use `yesod-routes` for routing.
I would suggest you look at [ekg](https://github.com/tibbe/ekg). Ekg is not a web server library or framework. But it is a simple example of a JSON API implemented with Snap.
If I generate an instance for say `Monoid (a, b)`, I'd like to only have the constraints `Monoid a, Monoid b` and not also `GTraversable Monoid a, GTraversable Monoid b`.
I don't think I've seen anyone write a 3-4 line web service in snap or happstack. That doesn't mean its the right tool for OP but scotty is pretty darn concise. If you're familiar with Ruby web frameworks, it aims to be analogous to sinatra, for quickly prototyping simple web services, whereas the other web frameworks in Haskell are more targeted at Rails, a more general purpose web framework. I'm writing a library for spinning up a mock web server in a thread for testing. I've found scotty the best choice, as its an extremely thin wrapper over the meat of the code. I probably wouldn't use it as an API though.
I'll throw my hat in the ring for Yesod as well. I tried out Snap, and while its nice, I really love yesod's static routing DSL. You can achieve something like it in Snap but the amount of glue code I was writing to do that made it not worth it to me. Beyond that, Yesod seems the most similar to Rails (in philosophy, not necessarily approach), which I work with every day for my day job, so it was easier for me to grok.
&gt;I don't think I've seen anyone write a 3-4 line web service in snap or happstack. A 3-4 line web service in scotty does almost nothing though doesn't it? Can you post the 3-4 line web service in question so people can translate it into snap and happstack for comparison?
I can recommend [happstack](http://happstack.com/page/view-page-slug/9/happstack-lite-tutorial).
If you haven't yet, you should also check out [School of Haskell](http://www.fpcomplete.com). Our tutorials (even web tutorials) use "active code" that can be modified and tested within the browser. So far it's mostly Yesod, though we're hoping to see more [Snap, Happstack, etc](http://www.fpcomplete.com/user/snoyberg/random-code-snippets/snap-and-happstack) tutorials on the way as well.
Here's [Scotty's example](http://hackage.haskell.org/package/scotty) in Snap: {-# LANGUAGE OverloadedStrings #-} import Snap.Http.Server import Snap.Core import Data.Monoid main = httpServe (setPort 3000 defaultConfig) $ route [("/:word",word)] where word = do app &lt;- getParam "word" writeBS $ "&lt;h1&gt;Writing my " &lt;&gt; maybe "app" id app &lt;&gt; " was a Snap!&lt;/h1&gt;"
&gt; If you're familiar with Ruby web frameworks, it aims to be analogous to sinatra, for quickly prototyping simple web services, whereas the other web frameworks in Haskell are more targeted at Rails, a more general purpose web framework. Yesod is. Snap isn't.
The mock HTTP server I'm writing is only a few lines in scotty because all it needs to do is define a single catchall route and then pass the request into the mocker which does the brunt of the work. The use case calls for the most concise web server DSL possible because all I'm using it for binding to the port and parsing a connection. Scotty is pretty much a closure over an IORef. I'm still working out the design but there's some code [here](https://github.com/MichaelXavier/HTTPMock/blob/master/src/Network/HTTPMock/WebServers/Scotty.hs). The fact is this problem does not call for a web framework, and scotty/warp definitely perform well enough for the sake of testing, so I have no reason to switch to something like Snap/Happstack.
Oh right, duh!
Ah, I see from the email thread linked at the wiki page that some poor fool volunteered to moderate the discussion on this ;-) What decisions remain to be made, your linked proposal seems reasonably worked out? Of the options listed for handling clashes of explicit and implicit instances, the discussion seems to support option 2 (which sounds about right to me). What else (apart from the creation of a TRAC ticket) needs to happen?
It looks like there is an XSS vulnerability there, where 'app' is written directly. 
So, the same as the example it is supposed to be replicating?
You might want to add `deriving (Enum, Bounded)` to both `Suit` and `Value` to simplify card and suit evaluation functions. With these instances, `unkeyedDeck` can be simplified to `[ Card v s | s &lt;- enumFrom 0, v &lt;- enumFrom 0 ] ++ jokers`. The problem spec specifies exactly 2 jokers, so I'd define `Card` accordingly rather than let `Joker Char` leave potentially hazardous junk in the state space. A lot of people won't be that interested in the actual problem, my guess is. If you could summarize it, or describe a small part of it and draw attention to the relevant code section, you'd get much better feedback.
&gt;The fact is this problem does not call for a web framework Yeah, I think the issue is that you think snap and happstack are analogous to rails. They aren't. Like chris shows above, using snap-core is virtually identical to using scotty. &gt;so I have no reason to switch to something like Snap/Happstack. Nobody was suggesting you switch, just correcting the misconception that scotty is more minimal or concise.
In case you're new to Haskell web dev, this is _not_ idiomatic code. The done thing is to use an HTML combinator library or templating library. I use [blaze-html](http://hackage.haskell.org/package/blaze-html), for example. E.g. [this](https://github.com/chrisdone/ircbrowse/blob/master/src/Ircbrowse/View/Overview.hs) is the source for [this](http://ircbrowse.net).
the mapping of morphisms from an endofunctor on hask has type `f a -&gt; f b`, is that an acceptably precise answer? 
Yeah, this is a good way. I also found this http://stackoverflow.com/questions/841851/how-do-you-combine-filter-conditions
Oh, well... `last` then :p
Or even more concisely: {-# LANGUAGE OverloadedStrings #-} import Snap.Http.Server import Snap.Core import Data.Monoid main = httpServe (setPort 3000 defaultConfig) $ pathArg $ \app -&gt; writeBS $ "&lt;h1&gt;Writing my " &lt;&gt; app &lt;&gt; " was a Snap!&lt;/h1&gt;"
Oh! I see now. But how to extract `GTraversable c` from `c` (e.g. in `everywhere`)? It could have been easier if `c` was of kind `Constraint` — then we probably could require `c` to be of the form `(c', GTraversable c')`. But `c` has kind `* -&gt; Constraint`. Any ideas? (It might be doable using quantifiers from the `constraints` package, but I imagine the package would become harder to use.)
This is a really cool aspect of Idris. When I have the time I think the first thing I'm going to do is attempt to implement linear lambda calculus as a dsl. The STLC example in the video makes that seem possible (unlike in Haskell) because of the way typing environments are explicitly handled.
On the other hand... Maybe I can live without those combinators. You often want some custom recursion scheme anyway. I'll try to remove that constraint and see what happens.
It depends what is really meant. If what is meant is a natural transformation, then we ought to restrict m and n to be functors and for the morphism to preserve that functorial structure. If what is meant is a homomorphsim of monads, then we out to restrict m and n to be monads and for the morphism to preserve that monadic structure. All monad (homo)morphisms are natural transformations, since we can uniformly define `fmap` using the monad operations, and therefore preservation of monad operations also preserves `fmap`; however, not all natural transformations are monad (homo)morphisms... Either way, the documentation should be clear about what exactly is expected to be preserved, since we can't encode all of the requirements in the type.
Elm is really awesome for the browser side http://elm-lang.org/try
Sorry for this being off topic, but what is the meaning of the ~ in context specifiers such as: &gt;class (MonadLayer m, m ~ Outer m (Inner m)) =&gt; MonadTrans m where I've tried googling for 'Haskell tilde' and I only found articles about irrefutable patterns. 
This is why Haskell is lazy by default.
You need to mark used variables, but it shouldn't be hard. See http://okmij.org/ftp/tagless-final/course/LinearLC.hs My current project for which I have been using Idris doesn't do anything fancy like that (just an ordinary functional language really), but I have plans to use Idris with more interesting calculi, including some with multiple forms of binders in addition to being substructural (in some cases).
Applicative idioms seem to be spreading quickly. I wonder what it would take to get hlint to recognize and suggest them as replacements.
There's nothing new here, but it's awesome to see people discover safe coding practices. Well done OP, spread the word and don't stop here!
As NotAbel says it indicates equality, and it's part of the TypeFamilies extension. It's particularly useful with associated types, but can sometimes be useful on its own. 
So is this actually a "problem" with op's library or just yours? It would be a shame if people were discouraged from trying this out just because they heard Tekmo say it had some problem.
Both. As far as I'm concerned there is still no good solution to this problem.
&gt; Also, Haskell is not lazy. It is non-strict. Pedantry is petty. The broader context is control structures reified as values. The most fundamental examples are function values. For instance, if' :: Bool -&gt; b -&gt; b -&gt; b if' True x y = x if' False x y = y if' True 1 undefined &gt; 1 In a strict language, if' would need to accept explicitly thunked values in order to behave like a control structure. Every lazy (ahem, non-strict) data type is a reification of a control structure. Take again, for instance a lazy (ahem, non-strict) list, which reifies a coroutine that produces a perhaps-infinite sequence of values. A strict list does not. This link is rather basic in comparison to how deep the types-as-control-flow hole goes.
&gt; This link is rather basic in comparison to how deep the types-as-control-flow hole goes. Indeed. He only mentioned Maybe, which doesn't require laziness. So I think your comment was of out of context. I didn't quite know what you were going on about so I decided to be complete, might it be relevant to any further discussion (big chance when we're talking about control). It was not my intention to be an ass. 
No worries. Maybe would be less expressive if `Just` were a strict function. The author's example does not use laziness, but when we use data structures to shape the flow of our program in general it is laziness that enables us to do so. Actually, I would say it is strictness that would prevent us from doing so.
I'd be interested in an example where Just benefits from laziness. I use it often so I maybe I just don't notice it.
I'm still not really convinced that this is a problem in practice with `layers`. I've been using the library myself and haven't run into any problems caused by `Inner` not being injective. I did try to do the same sort of thing with the `pipes` library though, and you're right, it doesn't work. {-# LANGUAGE RankNTypes, FlexibleContexts, TypeFamilies #-} import Control.Proxy.Core.Fast import Control.Monad.Trans import Control.MFunctor class ( p ~ ProxyConstructor p (OutputUpstream p) (InputUpstream p) (InputDownstream p) (OutputDownstream p) (ProxyMonad p) (ProxyReturn p) , Monad (ProxyMonad p) , Monad (ProxyConstructor p (OutputUpstream p) (InputUpstream p) (InputDownstream p) (OutputDownstream p) (ProxyMonad p)) , MonadTrans (ProxyConstructor p (OutputUpstream p) (InputUpstream p) (InputDownstream p) (OutputDownstream p)) , MFunctor (ProxyConstructor p (OutputUpstream p) (InputUpstream p) (InputDownstream p) (OutputDownstream p)) ) =&gt; Proxy p where type ProxyConstructor p :: * -&gt; * -&gt; * -&gt; * -&gt; (* -&gt; *) -&gt; * -&gt; * type ProxyMonad p :: (* -&gt; *) type OutputUpstream p :: * type InputUpstream p :: * type InputDownstream p :: * type OutputDownstream p :: * type ProxyReturn p :: * request :: (ProxyReturn p ~ InputUpstream p) =&gt; OutputUpstream p -&gt; p respond :: (ProxyReturn p ~ InputDownstream p) =&gt; OutputDownstream p -&gt; p (&gt;-&gt;) :: ( Proxy p' , Proxy p'' , ProxyConstructor p ~ ProxyConstructor p' , ProxyConstructor p' ~ ProxyConstructor p'' , ProxyConstructor p'' ~ ProxyConstructor p , ProxyMonad p ~ ProxyMonad p' , ProxyMonad p' ~ ProxyMonad p'' , ProxyMonad p'' ~ ProxyMonad p , ProxyReturn p ~ ProxyReturn p' , ProxyReturn p' ~ ProxyReturn p'' , ProxyReturn p'' ~ ProxyReturn p , OutputUpstream p ~ OutputUpstream p'' , InputUpstream p ~ InputUpstream p'' , InputDownstream p ~ OutputUpstream p' , OutputDownstream p ~ InputUpstream p' , InputDownstream p' ~ InputDownstream p'' , OutputDownstream p' ~ OutputDownstream p'' ) =&gt; (InputDownstream p -&gt; p) -&gt; (InputDownstream p' -&gt; p') -&gt; (InputDownstream p'' -&gt; p'') instance Monad m =&gt; Proxy (ProxyFast a' a b' b m r) where type ProxyConstructor (ProxyFast a' a b' b m r) = ProxyFast type OutputUpstream (ProxyFast a' a b' b m r) = a' type InputUpstream (ProxyFast a' a b' b m r) = a type InputDownstream (ProxyFast a' a b' b m r) = b' type OutputDownstream (ProxyFast a' a b' b m r) = b type ProxyMonad (ProxyFast a' a b' b m r) = m type ProxyReturn (ProxyFast a' a b' b m r) = r fb'_0 &gt;-&gt; fc'_0 = \c' -&gt; fb'_0 &gt;-| fc'_0 c' where p1 |-&gt; fb = case p1 of Request a' fa -&gt; Request a' (\a -&gt; fa a |-&gt; fb) Respond b fb' -&gt; fb' &gt;-| fb b M m -&gt; M (m &gt;&gt;= \p1' -&gt; return (p1' |-&gt; fb)) Pure r -&gt; Pure r fb' &gt;-| p2 = case p2 of Request b' fb -&gt; fb' b' |-&gt; fb Respond c fc' -&gt; Respond c (\c' -&gt; fb' &gt;-| fc' c') M m -&gt; M (m &gt;&gt;= \p2' -&gt; return (fb' &gt;-| p2')) Pure r -&gt; Pure r request a' = Request a' Pure respond b = Respond b Pure (The error message this generates is totally insane by the way, it's several megabytes long.)
No, it's beautiful! I pushed it with some minimal changes.
`mplus`? Okay, technically not benefiting from `Just` being lazy, but instead from `mplus` being lazy. Still.
That is pretty awesome that you wrote that out. Layers works because for the effects you chose to model one of the types is not a type function, and the compiler uses that to infer every other type. However, even in those cases type inference can still fail. For example, consider the case where you have: class C a where type T a :: * f :: a -&gt; T a instance C Int where type C Int = String f = show If you write: f (1 :: Int) ... then the compiler can infer which f you meant because it knows the type of f's input and can infer f's output, but if you write \x -&gt; f x ++ "Hey" ... the compiler cannot infer which f you meant because the compiler cannot work backwards from the output type of f to the input type because the output type is a type function of the input type and there could be many input types that the type function T maps to a String. With pipes, everything in the type signature (both inputs and outputs) is a type function, and the compiler cannot work backward from any of them so type inference is always guaranteed to fail. So functions that use type families only work if they have at least one type to work with that is not the output of a type function.
https://www.fpcomplete.com/school/pick-of-the-week/type-families-and-pokemon
Haskell with Pokemons! This touched my heart! I love it!
I implemented linear lambda calculus as a DSL, the only downside is I couldn't overload function application so I had to use "a $ b".
in Idris? You can use applicative functor syntax `&lt;$&gt;` and then you have idiom brackets. Inside the brackets you have white space equal to your overloaded function application. The syntactic overhead ends up being pretty minimal. 
no, in Haskell.
Thanks a bunch... actually what I did to fix it (in case you were curious) is I wrote a function to traverse the entire Haskell representation recursively and retain only the Elements, and not CData and stuff (and also set all the line information to Nothing). Then I used ppElement to print it back out and it looks perfect.
Basically any modern dependently typed language can implement a type safe STLC. You can fake it in Haskell using some of the fake dependent type gadgets hanging around. It's basically one of the go-to examples for any dependently typed language.
Thanks to type families, I finally understand Pokemon :-)
The enthusiasm is contagious. Very entertaining talk. Who's the "David" fellow that he's calling upon from time to time?
...as he had to skip a few slides, is there a link somewhere to the current version of the slides that were presented in that video?
Thanks, that's actually a good point. I should use it more often.
Given the [list of presenters](http://skillsmatter.com/event/java-jee/functional-programming-exchange) and the fact that he talked to him about Scala, I'm guessing David Pollack.
Agreed. Something like this will be a lot easier to read: data Suit = Clubs | Diamonds | Hearts | Spades deriving (Eq, Enum, Bounded) data Rank = Ace | Two | Three | Four | Five | Six | Seven | Eight | Nine | Ten | Jack | Queen | King deriving (Eq, Enum, Bounded) data Card = Card Rank Suit | Joker Bool deriving (Eq) -- | Constant: suit modifier suitModifier :: Suit -&gt; Int suitModifier s = 13 * fromEnum s -- | Get the value of a card cardValue :: Card -&gt; Int cardValue (Card rank suit) = 1 + fromEnum rank + suitModifier suit cardValue (Joker _) = 53 type Deck = [Card] unkeyedDeck :: Deck unkeyedDeck = [ Card rank suit | suit &lt;- enumFrom Clubs, rank &lt;- enumFrom Ace ] ++ [ Joker False, Joker True ] 
COMIC SANS AGAIN ARRRRGH It had to be said.
nice tool! btw, maybe you could color packages not yet in the platform, but bundled with GHC (such as `Cabal` or `ghc-prim`) as well but perhaphs in a different color?
&gt; Aliens abducted Simon Peyton Jones And the abduction was somehow bee-yotched. You give aliens far too much credit if they can't bungle up every once in a while. ;)
What you have is a free monad transformer in disguise: import Control.Monad.Trans.Class import Control.Monad.Trans.Free import Prelude hiding (fail) type FailList a e = FreeT ((,) a) (Either e) () emit :: a -&gt; FailList a e emit a = liftF (a, ()) fail :: e -&gt; FailList a e fail e = lift $ Left e Using that you can build the list using `do` notation: failList :: FailList Int String failList = do emit 1 emit 2 fail "What?" emit 4 `fail` will short-circuit everything after it, so the `4` will never be emitted. The above evaluates to: FreeT (Right (Free (1,FreeT (Right (Free (2,FreeT (Left "What?"))))))) Or you can use `pipes` to accomplish the same thing: import Control.Proxy type FailList a e = forall p . (Proxy p) =&gt; Producer p a (Either e) () emit = respond fail e = lift $ Left e The advantage of using `pipes` is that `pipes-3.2` will include `ListT` like operations so that you can manipulate it using the `ListT` monad, too!
I'm starting to think *everything* is a Free Monad (Transformer) in disguise!
There's a reason we call it "free"!
There's also the [List](http://hackage.haskell.org/package/List) package that implements a "proper" ListT.
The very first message: "If the Haskell language can be considered a temple for the worshipers of functional programming, then this import / export stuff is definitely the sewer underneath it!" -john peterson
Alas, the lost art of book review. Still, a collection of links is better than none.
Interesting --- I had always heard one of the reasons that `Monad` isn't a `Functor` is just an accident of implementation. It seems from [this post from Paul Hudak](http://code.haskell.org/~dons/haskell-1990-2000/msg00453.html) that monads and functors are discussed but the actual term functor isn't used. He uses `Mappable` to generalise `map` over a container type. This would certainly explain things. At what point did the `Functor` typeclass come into being? This suggests a period of time where `Monad` existed but `Functor` didn't --- was there ever a `Mappable` which got discarded? Edit: Also people have apparently wanted to rework `Functor`/`Monad` and the `Num` class since the late 19th century. How much less breakage if it had been done 15 years ago! :-)
It is a good talk but I am a bit disappointed that higher rank types weren't included. In my opinion, higher rank types have an exceptional power-to-weight ratio compared to other type system extensions. It's the first extension I think of beyond Haskell2010.
Perhaps you could use: type FailList a e = ([a], Maybe e) Although this will not work if you want the error to be lazily detected.
I have read in a few places that facts in harder to read fonts are more likely to be retained: http://www.princeton.edu/main/news/archive/S28/82/93O80/ Which may speck to the reason comic sans and other similar fonts continue to exist. Used for the right purpose they may be more effective.
Suggestion for skim-ability: provide type signatures. Bonus: For more complicated signatures, provide sample input/output runs. "The haskell way" (such as there is one) is that type signatures tell you most of what you need to know to * understand the code * write the code * not lose focus on the big picture 
That's a good idea! When I wrote the code (~ a year ago) I was just starting the language and just thought: "Hey, if the compiler can infer it, why bother". What a fool I was :) I shall add them tomorrow, thanks very much. Edit: Added type signatures
I have not read the few articles on the subject in the while but I remember the source of the annoyance being described as the brain needing to do more work to understand difficult to read text. The idea being that the brain is a resource hungry organ so it instinctively is trying to minimize it use in any case the benefit is not instinctively understood. The annoyance is barrier put up so if no other overriding concern over comes along no time is wasted on what would have been hard, resource intensive process.
Their weight is quite heavy, with the loss of type inference and various trivial-looking refactorings breaking.
Where's the source?
You don't lose type inference for Hindley-Milner types and in my experience top-level signatures are usually adequate for getting everything to type check. Sometimes it's required to give a let binding a type signature because GHC doesn't generalize lets anymore but that's orthogonal to rank-n types. In fact, that change was made to facilitate GADTs when GHC moved to System FC. I feel like that's heavier than a few type signatures! I am going to assume by refactorings breaking you mean things like (.) can't apply to higher rank functions. Yeah, that's a pain. Do you have other examples off the top of your head?
I would love a good talk on higher rank types - I'm actually using some, but don't fully understand what I'm doing, which is a bad sign. What I'm doing is based on [this Stack Overflow question](http://stackoverflow.com/questions/9220775/how-to-use-a-proxy-in-haskell-probably-using-a-higher-rank-types-extension) I asked a year ago. I understood the answer well enough to work with it, and I understand that essentially there's some Prolog (or predicate logic) going on in the type-system but some aspects that are implicit without rank N types may need to be explicit to get them right with the extension. But I don't use it enough to build an intuition for what it can do, and therefore I don't find uses for it. I still just have the one special case where I stumbled into it by accident. 
SPJ does all his slides with Comic Sans. Its on purpose to troll people like you :)
&gt; Do you have other examples off the top of your head? One of the canonical ones is using `($)`. In particular, things like the refactoring: runST $ blah blah blah vs runST (blah blah blah) There are a number of other standard issues like that. For more examples, just search for any of the papers on the various partial-inference techniques for System F (e.g., MLF, HML, Boxy Types, OutsideIn(X),...). One of the HML papers had a really nice list of examples, though I can't seem to find which paper it was at the moment.
One of those little co-incidences - after watching this video and getting that reminder about static typing as a maintenance tool, I just found [this link about test-driven development](http://www.reddit.com/r/programming/comments/1aepv9/tdd_bypasses_your_brain/). I thought the basic point could apply to static typing as much as it does to TDD. As SPJ says in the video, static typing gives confidence when making sweeping changes, just like having lots of unit tests. And there's certainly a widespread culture of type-driven development in Haskell, where the types are written first and the code is written to fit. The key point is this paragraph from the link... &gt; However, developing in a test-driven way can result in an overreliance on tests: when a test starts failing (and you know the test is correct), there is a tendency to mindlessly modify code until the test passes. Furthermore, true 100% code coverage by tests is a myth: no matter how many good tests you write, not all cases will be covered. Therefore, mindlessly modifying code until all tests pass is likely to introduce new bugs for which no tests exist. 1. It's possible to be over-reliant on static typing - to mindlessly modify code until the type-checker stops complaining. 2. Static typing obviously doesn't give 100% coverage and isn't meant to. The [top comment](http://www.reddit.com/r/programming/comments/1aepv9/tdd_bypasses_your_brain/c8wswud) is clearly as good a counter for static typing as it is for TDD - I really *don't* believe the GHC team implements sweeping changes by mindlessly modifying code until the typechecker waves a white flag. Still, I just thought I'd offer this up for comments - is type-driven development related to test-driven development in an interesting way, or is this just one of those pointless observations? 
- Haskell Road to Logic, Math, Programming http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.9312&amp;rep=rep1&amp;type=pdf - GHC Chapter in "Architecture of Open Source Apps" http://www.aosabook.org/en/ghc.html - Book of Exercises, in Spanish http://www.glc.us.es/~jalonso/vestigium/peh-piensa-en-haskell-ejercicios-de-programacion-funcional-con-haskell/ - alternate URLs: Marlow, Parallel/Concurrent: http://ofps.oreilly.com/titles/9781449335946/index.html https://github.com/simonmar/par-tutorial *roughly 92%* ------- Also this ginormous: http://alexott.net/en/fp/books/
I believe GHC has special support for that.
Here you go : https://github.com/Twinside/DiffTimeline
&gt; Static typing obviously doesn't give 100% coverage and isn't meant to. Please explain. Do you mean that a type checked program is not guaranteed to be correct? I for my part think that static typing and tests supplement each other. The type checker first flags all lines where some change elsewhere requires adaptions. The test cases do their tests, as usual.
&gt; Please explain. Do you mean that a type checked program is not guaranteed to be correct? Yes - some errors aren't detected by static type checking. Many of those can of course be detected by unit tests. But I still agree with the quote about tests never being 100% either - 100% line coverage or even 100% decision coverage aren't really full coverage. You can have an error in a single trivial line of code with no decisions at all, and seemingly fully tested. For example, the test ensures that with inputs `2` and `2` you get the correct answer of `4`. But is that computation an addition, a multiplication, raising to a power, or something else? If you pick the wrong one the code is in error, irrespective of the seemingly complete coverage - it still takes at least one additional test to spot the problem. Absolutely complete coverage, to me, would mean complete coverage of all possible combinations of inputs - and even that's just for a system that cannot have state. Coverage of all possible combinations of inputs in all possible states - without knowing the implementation and therefore without knowing what states it might be in... clearly impossible. So it's a good job full coverage isn't really necessary. There's a level where adding more tests is paranoia - the probability that they'll detect an error doesn't justify writing and maintaining the tests themselves. That seems also to have an analogy in static typing. Haskell is exploring the space of type systems that are a bit more sophisticated. It's possible to have a language that proves absolutely that the specification matches the implementation, but that means the specification is as complex as the implementation - or even a bit more so. That means the specification probably has as many bugs as an unchecked implementation would have. And that means coding the implementation to match the specification would reproduce those bugs. So that means it's maybe important that static typing shouldn't try to catch all errors - specifying types must be simpler than specifying the complete behaviour of the program or you can lost the benefits, at least assuming the development is driven in a no-brains way by the tests/types/specification. Annoying fact for some - the same issue exists with requirements documents, and yes, they're usually buggy too. 
Applications ARE[for] invited for a Research Assistant on an EPSRC-funded project "A Theory of Least-Change for Bidirectional Transformations".
&gt; Yes - some errors aren't detected by static type checking. In the video, Simon says just that, specifically that the claim "Well typed programs can't go wrong." is an humorous exaggeration. Yet, static typing as maintenance tool is still invaluable. If only to help you deal with the class of very trivial errors like mixing up a T and a Maybe T. Your time is just too valuable than to expend it on writing test suites that catch such errors. You want to concentrate on the more interesting (logic) errors the type checker can't see. 
It looks very nice from the screen shots. Why is this not uploaded to hackage? What is the clojure stuff about?
When I experience really weird goings on with the platform, I usually just wipe the entire platform and reinstall... Hasn't happened to me for a while and I'm sure there's a better way, though :) When it comes to cabal-dev, I compile from the github repository: git clone https://github.com/creswick/cabal-dev.git cd cabal-dev cabal install
Clojure stuff? There is no clojure to bee seen in the project, or did you mean something different than the language? For the hackage part, right now the build is to cumbersome to be put on Hackage.
Getting cabal-dev from github used to be the only option, since for a long time the hackage version was so out-of-date that it wouldn't build on any recent combination of GHC and/or Haskell Platform. However, the cabal-dev developers at Galois have recently remedied that. They have uploaded a recent version to hackage that should work in any reasonable modern environment. And they have announced that from now on, they will try to do a better job of keeping the hackage package current and supporting the platforms people are currently using. So right now it looks like the best bet is the hackage version. If it doesn't work for you, post a bug on github.
That symlink business sounds like it will indeed cause you problems. One thing to watch out for is that for a working cabal, you need not just one executable but four: cabal, ghc, ghci, and ghc-pkg. They all need to be compatible with each other. (Which is why I try to avoid upgrading cabal-install in between GHC upgrades.) The cabal-dev package itself installs three executables, all of which need to be on your path and compatible with the above four in order for cabal-dev to work: cabal-dev, fake-ghc-cabal-dev, ghc-pkg-6_8-compat. I am using homebrew, and cabal-dev works fine for me out of the box, even though I too have a slightly unusual symlinking setup for homebrew: I have homebrew installed on an external disk, with the symlinks adjusted as suggested in the homebrew installation instructions. On the other hand, I did not attempt to upgrade cabal-install...
If you're working on Google Code Jam, you can use the `GoogleCodeJam.hs` file I used last year: https://github.com/rampion/GoogleCodeJam
It's a local competition, but this is really helpfull, thank you !
I really appreciate that someone took the time to write this out, but I have two complaints. My first attempt to add electric pokémon resulted in a runtime(!!!) error because I wrongly assumed that by not copy pasting an implementation of pickWinner into class Battle Fire Elec, it would use the default specified in the battle class. Second, as I alluded earlier, the solution was to copy paste a bunch of code and change the word "Water" to "Elec" and *nothing else*. If you look at the code on this page, a huge amount of it is literally copy pasted boiler plate. How is this helping us? It didn't even catch my error at compile time. Define an `Elec` type however you'd like. Then give it a Battle instance: instance Battle Elec Fire -- pickWinner is missing but with the default Winner a b type, there's only one implementation possible instance Battle Fire Elec where type Winner Fire Elec = Elec pickWinner = flip pickWinner Now run it and watch it burn when it comes time to pick a winner. So i thought ok fine, I'll just specify a default implementation for pickWinner in the Battle class, not sure why they didn't do that to start with. pickWinner a b = a Nope. Can't deduce (a ~ Winner a b). Ok fine. pickWinner a b = Winner a b -- using the type synonym from the class Nope, type constructor Winner is not defined. What is this? Are we really forced to paste the default implementation into every instance? Doesn't that defeat the entire purpose of this exercise? Someone help me out here.
To be fair, I ditched all the writing part of hit and massively rewrote the interface, it seemed a too heavy change for me to push upstream. That being said, If you think these changes are interesting I'd be glad to work with you in order to push them upstream. (for reader information the hit fork used by Difftimeline is available at https://github.com/Twinside/hit-simple , name changed to avoid name clash)
Nice! What about some icons identifying each message platform (reddit, so, mail-list, etc.)? And just header is actually not enough - it will be better to have smth like preview.
I [knew](https://twitter.com/christopherdone/status/311982712229621760) this was going to happen. Well done :)
&gt; What about some icons identifying each message platform (reddit, so, mail-list, etc.)? I think it's a good idea. =) Like [this](http://jbotcan.org/hub/). &gt; And just header is actually not enough - it will be better to have smth like preview. Hmm. I'll see what can be done. It depends on the particular source of content, of course. I'm scraping Github manually, for example.
&gt; Is there a page somewhere that aggregates all of Haskell's community output into one place? Yes, [Planet Haskell](http://planet.haskell.org/). &gt; it seems hard to get an RSS/Atom feed As for any Planet feed, the URLs for the [rss feed](http://planet.haskell.org/rss20.xml) and the [atom feed](http://planet.haskell.org/atom) are completely standard. Even if you're kind of out of things in the world of aggregators, Planet Haskell and its feeds don't seem very hard to find. Did you look at the list of community resources on the front page of [haskell.org](http://www.haskell.org)? Did you do a Google search? &gt; A more carefully filtered list of blogs, because Planet Haskell contains non-Haskell-related stuff in it. The maintainer of Planet has worked very hard over the years at building the selection offered and tailoring it to the needs of our community. That said, I'm sure there are other tastes, and no one is stopping you from publishing another feed. But don't you think you should be in touch with the maintainer of the existing one before unleashing such harsh public criticism? Perhaps he would be interested in implementing some of your suggestions. The maintainer is Antti-Juhani Kaijanaho of the University of Jyväskylä in Finland. Antti-Juhani is a long time member of and respected contributor to not only the Haskell community, but also the open source community at large. &gt; Whipped it up in… 9 hours Nice. Compared to all the other aggregators out there, are there any novel features? Of course, being in Haskell is already a very nice feature, in my opinion. But again, how does this differentiate from Sigborne's very nice [feed](http://hackage.haskell.org/package/feed) package on hackage?
True, but the roots of these kinds of issues date back to the early 20th century. Haskell Curry and Saunders Mac Lane were the last two American Ph.D. students at Göttingen in the late 1920's and early 1930's, before the department unraveled after the rise to power of the Nazis. They both did their doctorates in mathematical logic under David Hilbert, Paul Bernays, and Hilbert's student Hermann Weyl, and both were deeply influenced by the philosophy of mathematics at Göttingen. The two did not overlap at Göttingen - Mac Lane arrived shortly after Curry had left - but Mac Lane [mentions](http://www.ams.org/notices/199510/maclane.pdf) that he knew of Curry, and it seems likely that he knew of Curry's work. So it is not surprising that deep connections were later discovered between Curry's combinator theory and Mac Lane's category theory.
Didn't see this. It's great. :)
Inside of class when the goal is to teach techniques for solving equations, I just wish teachers were more thoughtful in their words, and communicated clearly that they are asking a specific question - what are the possible values of x - from among the many that can be asked that involve an equation of some sort. Outside of that specific situation, though, there are many things that can be done. I have spent about three years now volunteering at schools to teach inquiry math electives that encourage students to think and write about questions using expressions and variables, and very rarely does the problem come down to solving the equation for a variable. The last school year, I extended that into a class in computer programming with Haskell, which was essentially an exercise in describing situations and desired results using algebraic reasoning. These kinds of things are far more confusing if the basic terminology and modes of thought are designed to be specific to a specific kind of problem. Hence the objection to "unknown".
The problem with planet haskell is that most feeds it aggregates are either off topic or overly academic.
Right, but then we would never get into trouble by accidentally deriving the wrong instance of Applicative from a Monad instance because the Monad instance doesn't exist.
I've been thinking for a while as I follow a couple of online AI/ML courses that some of theses types of processes would be elegantly expressed with FP. Is anyone using haskell for things like this in anger anywhere?
&gt; As for any Planet feed, the URLs for the rss feed and the atom feed are completely standard. &gt; &gt; Even if you're kind of out of things in the world of aggregators, Planet Haskell and its feeds don't seem very hard to find. Did you look at the list of community resources on the front page of haskell.org? Did you do a Google search? Did you actually read the text you are replying to? :-) It said that it seems hard to get an RSS/Atom feed from YouTube. I don't know whether that's true or not. But at least you should crititcize him based on what he was saying, not based on some strawman. I don't see where he claimed (as you imply) that getting an RSS/Atom feed for Planet Haskell is hard. 
So crude. That's not the kind of attitude that will help in growing the community. OP: A week won't be enough to learn all the tricks and to have a proficient style, but it's good for a general approach using eg. the first chapters of Learn You A Haskell. As tew88 said, if after a week you like it, you'll want to spend more time on it.
Pong in a week is a long shot. That requires mastering monads, including state/maybe/io and perhaps a graphical library. 
I don't know if this actually changes anything, but I meant that I hoped to be proficient enough to write Pong *after* spending a week teaching myself Haskell.
&gt; &gt; Is there a page somewhere that aggregates all of Haskell's community output into one place? &gt; Yes, Planet Haskell. Hm, no. Planet Haskell only aggregates blogs. &gt; &gt; it seems hard to get an RSS/Atom feed &gt; As for any Planet feed, the URLs for the rss feed[2] and the atom feed[3] are completely standard. I was talking about YouTube. Read my post properly. I said: “YouTube — but it seems hard to get an RSS/Atom feed.” &gt; Even if you're kind of out of things in the world of aggregators, I'm not. &gt; Planet Haskell and its feeds don't seem very hard to find. They're not. If you looked at [the linked submission](http://haskellnews.org/grouped) you'd see that I'm _using_ those feeds. &gt; Did you look at the list of community resources on the front page of haskell.org? Yes. &gt; Did you do a Google search? Did you properly read anything I have written? &gt; &gt; A more carefully filtered list of blogs, because Planet Haskell contains non-Haskell-related stuff in it. &gt; But don't you think you should be in touch with the maintainer of the existing one before unleashing such harsh public criticism? Perhaps he would be interested in implementing some of your suggestions. Harsh? I'm stating a simple fact. Planet Haskell consists of blog posts by Haskellers, not necessarily always about Haskell or Haskell news. This is besides the point, anyway. Planet Haskell is orthogonal to Haskell News. &gt; The maintainer is Antti-Juhani Kaijanaho of the University of Jyväskylä in Finland. Antti-Juhani is a long time member of and respected contributor to not only the Haskell community, but also the open source community at large. Great. So? &gt; &gt; Whipped it up in… 9 hours &gt; Nice. Compared to all the other aggregators out there, are there any novel features? Well, it doesn't just aggregate RSS/Atom feeds. It's specific to Haskell, it knows about Haskell and can do clever filtering. &gt; But again, how does this differentiate from Sigborne's very nice feed package on hackage? Again, did you read anything, _anywhere_? Haskell News [_uses_](https://github.com/chrisdone/haskellnews/blob/master/haskellnews.cabal) the `feed` package. It's a library. Haskell News is a service. Your post is completely baffling. I don't even. `〈(゜。゜ )`
I don't think working towards a specific goal is the way to learn a new language. You can do that when learning one you already know - namely yet another imperative language - but when you're getting into something entirely new, learning it for its own sake is probably what most people do. At some point, you'll realize that you're not a beginner anymore, and writing Pong is super easy. Compare it to learning French - you like it, you'd like to be able to read it, you keep learning. Rarely I have met people who started learning it just to understand Lagrange's original papers.
This is awesome! It demonstrates once again why Rank2Types/RankNTypes is the extension I would most like to see stadardized. One thing: why not make squash a member of MMonad, so that implementations may be written in terms of that instead of embed (where desired)?
I should really figure out the answer to that question. I'm not a GHC hacker, although I know plenty of people who are GHC hackers. Some action may have been taken already. (I won't name names, but I will ask them nicely.) Option 2, where instances from prior modules or explicit instances in the same module take priority but cause a warning, is indeed the preferred compromise. Correspondingly, adding default superclass instances for existing superclasses shouldn't break existing code (or do anything), as the defaults should always be explicitly overridden. Also, making a new class a superclass to old classes should be at least mostly ok, as long as a default is given. However, adding an old class as a superclass-with-default-instance of another old class is likely to cause the same sorts of problems you get when one of your favourite private instances gets added to the library. It's awfully tempting to suggest that we should have a way to mark an instance as low priority, to be discarded (with a warning) if already provided by an earlier module or by a high priority instance in the same module (the Option 2 criteria).
The original reason was that embed is more efficient, but I can add `squash` to `MMonad` with a default definition in the next patch.
Which is exactly why I defined that FailList type the way I did. Actually it's slightly more general: it's if you want the work to be done lazily. Checking the errors up front is incompatible with doing the work incrementally, you have to allow for discovering errors along the way.
`(Maybe e, [a])` is not equivalent operationally. You cannot do incremental processing with the `(Maybe e, [a])` style. You're welcome to try it, but check that it is indeed incremental and that it doesn't retain garbage any longer.
I'm not sure why people want to turn it into something else. What's wrong with it as is? It's simple classical lazy functional programming.
It would be interesting to see how you would implement this after another year of experience in the language... a future blog post? :-)
Is `embed` always going to be more efficient? Or just it happens to be on the current members? Also, I note that the `EitherT` monad is not a member. Was that just to avoid the dependency for now?
I can't say for certain, but I am reasonably sure embed will be faster. Just compare their definitions in terms of each other: squash = embed id embed f t = squash (hoist f t) It seems like passing an `id` argument to embed will always be more efficient than a hoist followed by a squash. Also, no `EitherT` because of the dependency. I want EitherT added to transformers.
I disagree. I think nothing focuses the mind more than having a concrete goal to work towards. Sure he will make mistakes and do lots of unidiomatic things, but he will learn from those mistakes and become a better programmer as a result.
Judging by the fact that you want to write something like Pong, here is a quick and dirty progression: * Read "Learn you a Haskell for great good" to learn Haskell basics * Read "You could have invented monads" to learn about monads * Read "Monad transformers - step by step" to learn about monad transformers Then you should be able to whip up a quick program using the `StateT GameState IO` monad, also known as the "simple game" monad.
For just one week, I suggest the recent School of Haskell. You won't have to install anything, you could code from your browser. The website is mostly dedicated to beginners. You should give it a try. https://www.fpcomplete.com/school
That would be interesting. I'd have to wait a while as the code is still, to a certain extent in my head. Also I'm not sure how I would do it differently, although that may be due to the problem I just mentioned. I'll probably write about something different first (partly to improve my explaining skill). Thanks for the interest :D
This is really incredible. I think I mentioned in one of my blog posts that I didn't think this kind of thing was possible. Also, this is how I thought monads worked when I first started out (I thought &gt;&gt;= mapped between monad types) and it was really confusing. Great looking package. I'll have to check this out! Thanks again, Tekmo!
Salary: £29,541 - £36,298 p.a. Ouch. It saddens me that you can have a much better life by being a Java developer :( I'll admit I don't really know what a research assistant does though, so maybe that salary is low for a reason.
Talk to me like I'm stupid: What's the theoretical background for `StateT` not being an `MMonad` and `ContT` not even an `MFunctor`? I tried writing `squash_StateT` and saw clearly that I can't, but I'm looking for the higher-up view.
I'm currently adding more and more transformers to what has come to resemble a monadic Jenga tower of dizzying height, so this library appears at the right time.
My experience with all of the languages I'd worked with before Haskell is exactly as you described. However, my experience with Haskell was quite different. It isn't a matter of learning a few bits of syntax, but rather an entirely different way to view problems. Six months later I was quite productive, and it has impacted how I think about a lot of problems. For instance, in lots of languages people talk about code reuse, but seldom realize it on any large scale. In Haskell I find that I'm able and willing to reuse most of the code that I've written over the last 6 years. To me that is an amazing feeling, because I no longer feel like I'm treading water when working on a project. I can factor out small components and because they are implemented parametrically and without side-effects, they are suitable for a much wider array of situations. The parametricity guarantees from Haskell make it easier for me to write much larger programs that I can trust to be correct, and support much easier refactoring in the large. Laziness is a fundamental component to the code-reuse story in Haskell. By making _simple_ lazy algorithms, they inherently fuse themselves away, often making the composition of two very general things faster asymptotically if they were written individually. In a strict language, if I write a `sort` routine for a list, and then write another routine that knows how to take the first 10 elements from the list, I'm still having `O(n log n)` or `O(n^2)` to sort, then taking my ten elements off. In a lazy setting that can optimize into a quickselect with no additional code or implementation effort on my part, smoothly degrading from the O(nk) selection bound towards the other bound as you take more elements... because it doesn't do the work to calculate the portions of the list you don't look at! The nice thing about it is that I was able to take some of that insight back to the other languages with me. It really changed the way I think for the better, but to really fully internalize Haskell will be an on-going concern. Learning Haskell isn't a matter of simply writing C or Python with Haskell syntax.
That makes me reconsider riding my skateboard in to work every day. ;)
You won't learn Haskell in three days, but that's all you need to become an addict :)
It requires use of a single Monad, if you write it in IO, and you might not even have to be aware of the word 'monad.' The resulting code won't be idiomatic but what do you expect after a week?
Paul Hudak's original book "The Haskell School of Expression" has an implementation of "paddleball in twenty lines". It's quite likely this used a GUI library which is no longer supported... therein lies perhaps Haskell's biggest problem if you want to code game-related stuff - it doesn't have the level of industrial support that more mainstream languages have particularly for GUIs and interactive graphics (though there is a reasonable SDL binding). 
The problem is that we have the types: ($) :: forall a b. (a-&gt;b) -&gt; a -&gt; b runST :: forall c. (forall s. ST s c) -&gt; c Thus, in order to pass `runST` as the first argument to `($)`, we have to instantiate `a ~ (forall s. ST s c)`. However, in general, it's a bad idea to instantiate type variables with quantified types. (Where by "bad" I just mean that it raises a number of difficult issues; not that it's evil or anything.) However, this is precisely one of those things that we *want* to work. Thus, even though it's problematic, all the major techniques will try to find some way or another to make it work. The operative word being "try". The fact that it works in current versions of GHC says nothing. If you go back and test older versions of GHC (say, all the major versions from 6.6 and higher) you see it flipping back and forth between whether it accepts `runST $ foo` or not. Each one of those flip-flops is due to GHC replacing the inference algorithm it uses; some can support this example, others can't.
Aight, that makes sense to me. Thanks!
Hey David, thanks for putting these up! //Adam from the Scala course
Does this mean that you can always work around `runST $ foo` in the versions it doesn't work by adding ugly manual annotations?
FYI, for the exercises and slides related to this workshop: https://edwinb.wordpress.com/2013/03/15/idris-course-at-itu-slides-and-video/
Most of the time, when you learn a new language, it's usually just another permutation on the following options: * Curly braces or significant whitespace * Interpreted or compiled * Object-oriented, imperative, functional, or multi-paradigm * Dynamically or statically typed Haskell deviates enough from the above formula to make it difficult to transition to it from other languages. There are a few concepts that you will probably have never seen before: * Monads (i.e. overloading imperative notation) * Algebraic data types * Enforced purity * Laziness * Higher-kinded polymorphism * Type classes. Technically, lots of other languages have this, like Java's interfaces, but they don't emphasize them as heavily as Haskell does. ... and that's purely the language. Haskell libraries do some really incredible things that you will not see in other languages.
GHC has some pretty kick-butt profiling options. Have you explored using them to figure out where your slowdowns are?
In my opinion, if it only takes a couple days to "get" a language, then that language doesn't present enough new ideas to be worth learning. I'm more interested in languages that will teach me a lot I don't already know. But don't be discouraged; you should be able to start writing useful code right away, provided that you start with a suitably small scope.
Neither term is really ideal. But generally "variable" refers to the fact that the statement or claim is quantified over some set of values. x + 2 - 1 = x + 1 is a true statement whenever x is replaced with any complex number, but it wouldn't make sense to call x an "unknown" since there are multiple values for which the equation is true. On the other hand, when a statement is true only for a single value of the "variable," then it doesn't make much sense anymore. There's only one value of x for which x + 1 = 2. In this case, "unknown" does seem more logical. Ultimately, "variable" is used as a synonym for "element" (of some set), and this language make sense in all contexts, but it may come across as pedantic ("Find all elements x of the set of complex numbers such that x + 1 = 2" vs "If x + 1 = 2, what's the value of x?").
I don't know if the code of my simple 2d platform game can help you that much, but perhaps to get a feeling how to handle input and state changes: https://github.com/dan-t/layers The general structure is, that there's first of all the game data, which is called AppData in my case. data AppData = AppData { ... } To get a mutable game data, which is mutable from the update loop of the game and from the input handling of the game, the game data is put inside an IORef, which is just a mutable variable. type AppDataRef = IORef AppData In my case the AppDataRef is than put into a State monad, but for your first Haskell program I would leave the State monad out and just work with the IORef. main :: IO () main = do aRef &lt;- newIORef AppData { ... } initGLFW aRef -- see below gameLoop aRef gameLoop :: AppDataRef -&gt; IO () gameLoop aRef = do update aRef render aRef gameLoop aRef update :: AppDataRef -&gt; IO () update aRef = modifyIORef aRef $ \appData -&gt; -- modify your appData render :: AppDataRef -&gt; IO () render aRef = do appData &lt;- readIORef aRef -- render your appData The input handling is done with GLFW (it's the package glfw-b on hackage): initGLFW :: AppDataRef -&gt; IO () initGLFW aRef = do -- initialize GLFW setMousePositionCallback $ \x y -&gt; modifyIORef aRef $ \appData -&gt; -- do something with x, y and appData I hope that you can take something out of this. Greetings, Daniel 
IIRC, Johan Tibell was asking things around about Iface files on one of the GHC mailing lists. I suppose that's the best place for you to go ask these questions, because I don't know of such a tool. I suppose there is a module somewhere in GHC's source code that handles this though.
I've haven't look what kind of changes you put in, but i think we could talk about it to see how hit API could be improved and hopefully merge things.
Woops! Good catch!
thanks, i've added those packages, but put them in the platform for simplicity.
Paraphrased: "Well, there are lots of ways of deriving monads. Hm — how many people here have written a monad tutorial?" ;)
I am a haskell amateur at best; but I did have a breakthrough some time back in understanding Monads as computation in a context. The idea of Maybe wrapping a value in a context simply means that with your value, you also have the 'knowledge' that there may not have been a value at all. That is the context, "there may or may not have been a value". Which is obviously useful when writing code; since if something is *not* wrapped in a Maybe, than you know it will *always* have a value.
Is there video or audio to go along with these anywhere?
&gt; The idea of Maybe wrapping a value in a context simply means that with your value, you also have the 'knowledge' that there may not have been a value at all. That's just a restatement of what Maybe is doing. The issue is "context", which I feel is highly inappropriate for this. The word has a very well established family of meanings: the set of variables in scope at some place in a program, or the "rest" of the value/computation at some point in a value/computation. The use in the slides is highly atypical, and makes no sense even if we ignore well-established uses.
I guess I am not familiar with its 'very well established family of meanings'; but the idea of a Monad representing the context of a computation actually makes/made perfect sense to me and demystified monads for me. I am not sure what other word would be useful here
What term would you use instead?
I'm not sure you need a term.
I actually found the slides pretty good and they're probably at the correct level for the audience.
It's more like monads _are_ the computation, if anything.
I'm not sure I like saying that monads are computation. Monads, being functors, provide a computational form for non-computational (or boringly computational) domains, I feel. I think it's better to view monads as perspectives. `Maybe` is the perspective that the computation might fail.
Interesting! What about MApplicative? Or MMonadTrans? Are those sensible too? If the latter is, there might be an infinite hierarchy of applying the same idea over and over again. 
I'm not going to bikeshed about monads, though I knew a comment like this would arrive at some point. You could just easily say “Maybe is a computation that can fail.” Etc. etc. That's why I said “if anything”, like, _I_ wouldn't say “computation”, but if we're going to use any of the terms on his post… Buh! It's just word games fluff. _We_ know what monads are. My point was… lost. I don't care anymore. Carry on without me!
Aww, I was just trying to have a discussion. :(
Sure, that's the place to ask; but you seemed knowledgeable so I figured I'd give it a shot :)
The thing you'd have to annotate is the `($)` in order to specify that you do in fact want to instantiate `a` at a polymorphic type. But giving that annotation will be tricky just because `($)` is an infix operator, and using it in prefix form rather defeats the purpose... of course, that's a syntactic concern rather than a real theoretical concern. IIRC, all the inference algorithms I've seen (for full RankNTypes) would allow you to say: (($) :: ((forall s. ST s c) -&gt; c) -&gt; (forall s. ST s c) -&gt; c) runST foo and have it work. Though I could be misremembering; some of the less-robust algorithms may require monomorphizing `c` too--- I know the decision procedure for Rank2Types can run into bizarre issues like that. (Which is one of the reasons why, even though Rank2Types are technically decidable, you're better off just trying to go for RankNTypes if you actually want to make your users happy.)
First, just to warn you, it looks like the project is going in the direction of only supporting OpenMPI and mpich2, which might not be the environment in which you will be running it. It is for this reason that I eventually just wrote my own small C shim to handle the stuff that I needed; you can see the C code [here](https://github.com/gcross/Visitor-MPI/blob/master/c-sources/Visitor-MPI.c) and the Haskell code that wraps it [here](https://github.com/gcross/Visitor-MPI/blob/master/Control/Visitor/Parallel/BackEnd/MPI.hs) (scroll down to Foreign functions). Second, if you look [here](https://github.com/bjpop/haskell-mpi/blob/master/src/Control/Parallel/MPI/Fast.hs) --- starting at line 623 --- it looks like the Fast module will use the Storable representation of the Haskell values, which should be equivalent to the C representations. Don't use the Simple module to communicate with C code because Simple uses the Serialize typeclass which is not guaranteed to produce the C-layout of values. But again, while I looked at haskell-mpi I never actually used it in a project so value my advice less than that from anyone else here who has more experience with it. :-)
Where can I read about the problems with linear implicit type parameters? I found the GHC docs on them and the Monad Reader issue, but neither explained what was wrong with them.
I think you can do MApplicative if you first introduce the `Pair` of two monads: -- Should be a newtype, but I'm lazy type Pair m n r = (m r, n r) instance (Monad m, Monad n) =&gt; Monad (Pair m n) where return a = (return a, return a) (m, n) &gt;&gt;= f = (m &gt;&gt;= fst . f, n &gt;&gt;= snd . f) Then I THINK MApplicative would be: class (MFunctor t) =&gt; Applicative t where unit :: Identity a -&gt; t Identity a mult :: Pair (t m) (t n) a -&gt; t (Pair m n) a I might have `unit` wrong, but I think `mult` is right. However, I haven't tried to see if the current `MMonad`s satisfy the above `MApplicative` yet. The above is just a wild stab in the dark. I also definitely don't know if there are `MMonadTrans`es yet. However, I'm pretty sure there is such an infinite hierarchy, which can probably be made finite with kind extensions like `PolyKinds` or something in a similar spirit.
It was actually Edwin who did all the work - he recorded them and put them up. I just stalked him on Twitter until he posted a link to the video, then copied it here :-)
I already said elsewhere that _I_ wouldn't use the term “computation”, but if it's been introduced, might as well go along with Wikipedia's: “In functional programming, a monad is a structure that represents computations.” So I suggest you guys correct [that article](http://en.wikipedia.org/wiki/Monad_\(functional_programming\)), and add “computations” to the list of [What Monads Are Not](http://www.haskell.org/haskellwiki/What_a_Monad_is_not). That said, let the hair splitting continue without me!
In my opinion, what's important about Applicative is not that it's a monoidal functor, but that it cooperates with `apply :: F B^A * F A -&gt; F B` and `pure :: A -&gt; F A`. So I guess that MApplicative should be something like type Exp m n a = n a -&gt; m a class (MFunctor t, MonadTrans t) =&gt; MApplicative t where -- MonadTrans is for lift = pure apply :: t (Exp n m) a -&gt; t m a -&gt; t n a Edit: disclaimer: I wrote this after no more than 10 minutes of thinking after waking up, it might contain not even wrongness. 
Yes, it is intended that you can send data between Haskell and other languages for MPI processes. There is an example in the test directory which shows this for an array of integers: https://github.com/bjpop/haskell-mpi/tree/master/test/examples/HaskellAndC I haven't tried an array of doubles recently, but if it doesn't work, that would be a bug and we should try to fix it.
You are right, the Simple module does not produce language-portable messages, but the Fast module does. It is our intention for the library to be portable across MPI implementations. Although, as far as I know, it has only been tested with OpenMPI and mpich2. If you've found that it does not work with other MPI implementations please submit a bug report to the github project, or to one of the authors.
Looks really cool, will try it when get home.
Long live the lenses! Is Post-mortem section in the end a joke?
Post-mortem -&gt; Postscript?
I'd caution that MPI itself is not really designed for that kind of hetrogeneous service approach.
Any value is a structure that represents a computation. f :: Int represents the computation of an int, so the statement is not inaccurate, although misleading.
The type signatures in the post are a good start. But this: class PlayableGame a index tile player piece ... What is `a`? Only when 1/3 down the post does state get Ctrl-F mentioned for the first time. Don't get me wrong -- this is a quality article. But the surface area for entry is small. One needs to understand * the theory behind the model (board game abstraction), * the actual game, * implementation of the actual game, * and last but not least, Gtk. One solution is to serialize the material into shorter, narrower posts.
Yes! Late night writing goofy titles. I'll change it.
I don't know anything about template haskell, how would I replace this to make it work without template haskell? $(makeLenses ''Location) 
Note that `Control.Lens` exports `Control.Lens.TH` so the second import is redundant.
You can compile with `ghc -ddump-splices` to find out what code it generates. It's probably something like, latitude = lens _latitude (\loc lat -&gt; loc { _latitude = lat }) and the same for `longitude`.
I think the code would be clearer if it would use type families instead of functional dependencies: class PlayableGame a where type Index a :: * type Tile a :: * type Player a :: * type Piece a :: *
`unit` should be `unit :: Const () a -&gt; t (Const ()) a`. (`Identity` is the initial monad, and you need the terminal monad here.)
You! I really liked those tutorials and I'm sad there weren't any more after the second - do you ever plan to continue the series?
[Learn You a Haskell](http://learnyouahaskell.com/) is how I learned (it's in the sidebar). Be warned, though: Learning Haskell is a long haul. If you're looking to pick up a language in a week you're not going to like Haskell. If you like thinking about things differently and aren't afraid to be *totally and completely lost* at times (better yet, if this excites you) then you're likely to enjoy Haskell.
Enjoyed this, very nice to read on mobile. What presentation library is this?
&gt;At its simplest, a lens is a value representing that maps between a complex type and one of its constituents. Could you reword this sentence? It doesn't parse as is, and I get a couple of slight differences in meaning depending on how I read it.
The word "representing" shouldn't be there. The phenomenon is the reverse of. "Value" otoh is just careless. Everything in Haskell must surely be *something* and that *something* surely has to be *value*, right? What else can it be? 
~~The code samples don't show up in Opera by the way, I had to use Chromium to read it.~~ EDIT: Seems to work now. The type of `latitude` is given as `Lens' Location Double` and the following paragraph talks about Doubles as well. Unless I'm misunderstanding something those should be Arcs. Other than that it looks good. I'm awaiting a follow up with advanced features.
Not many type classes give Num a run for its money!
Thanks, I'll take that out.
I plan on doing that in another one or two installments. The problem is that the inner workings of Control.Lens are very generalized which make them very difficult to follow. I agree thta degOfLatOrMinuteOfLong is sort of meaningless. The point was that you can get compositions, products, and coproducts of lenses suggesting that they're a nice kind of value to pass around.
Nice, glad to see some more people not drinking the FRP coolaid! Great work!
Could someone explain (simplifying as necessary) why lens composition is read left-to-right instead of right-to-left? That really doesn't sit right with my current (clearly faulty) intuition. I'm guessing the definition of Lens' has something to do with it.
I actually meant to delete the "that", it's changed now. As to why I stressed "value", not *everything* in Haskell is a value. In particular, what is "record setting syntax"? Clearly we can use it to build a value, a function `(part -&gt; whole -&gt; whole)`, but by itself it's not really... anything. To me that's an enormous value of lenses—they provide values which track access to various components of complex types. This gets even murkier when you start comparing it to the OO world, which I mostly resisted doing here. In the OO world, getters and setters are method associated with an object... they are not "free" values.
Haha I didn't write any of that, sorry for the confusion. I'm just a happy reader of it who thought the OP might get some use out of it too.
Hey, come on, FRP is pretty awesome even if it's having some teething issues. Just like any truly novel technology. (And most of *my* problems with it have been due to the Haskell GUI toolkits rather than the FRP abstractions themselves.) And, again like anything novel and different, the problem is always that *not enough* people are using or developing it, not the opposite! You'd think somebody using Haskell would be acutely aware of this problem: after all, a surprising number of people think the same thing about the whole language despite the fact that it isn't all that popular. That's certainly not a good thing!
It is just because of the way lenses are defined. You could wrap it in a newtype that reverses the order of composition if you preferred. newtype Opposite c b a = Opposite { unOpposite :: c a b } instance (Category c) =&gt; Category (Dual c) where id = Opposite id (Oppposite f) . (Opposite g) = Opposite (g . f)
Oh right. Gotcha. And here I was envisaging something subtle involving the nature of foralls and so on.
s/mɑnæd/moʊnæd/
What's your setup like? I'd like to replicate the issue and file a bug report.
Whatever was causing the issue with the code samples seems to be fixed now. I'm on Opera 12.14 on Arch Linux 64bit if it still matters. 
&gt; forming a pair of glasses or just binoculars?
A *value* in Haskell is the result of *evaluating* an expression. See the connection? So when one says "X is a value", the context should always provide information about what *expression* X is a value of. The guts of a lens is that it's ultimately a data structure, in most incarnations. Now it's true that it may be more helpful to explain the rationale behind a lens. Here's an attempt: "A lens is a pure and functional way of querying and mutating data in a part/whole relationship with other data." That weasel-y "way" still crept in, but I hope you get the idea.
I am too lazy to fill out a full bug report but just to quote from the .cabal file: if flag(mpich14) extra-libraries: mpich, opa, mpl else extra-libraries: mpi, open-rte, open-pal So the build explicitly requires either mpich specific libraries or open-mpi specific libraries. By contrast, my small snippet of code only requires the mpi library.
Yeah, I had no interest in describing the internals of `Control.Lens` yet. I will soon, though! More important to me is the way it feels—the denotation perhaps. Having a lens as a value, an independent object, something that you can carry around separate from the value of inspection, is a part of the value of lenses. The idea of "value", to me, supersedes "way" because of functions like `view :: Lens' a b -&gt; a -&gt; b`. What is the first argument to `view`? The lens, a value.
Chris and I know each who the other is :-)
I notice you mix the technical meanings of "value" (operational and denotational) with the lay meaning of "value" (something desirable). Do you see anything up with that?
For curiosity, it actually outputs code to the effect of latitude :: Lens' Location Arc latitude f (Location lat long) = (\lat -&gt; Location lat long) &lt;$&gt; f lat {-# INLINE latitude #-} which is a direct implementation of what the underlying `Lens'` function-forall-functors type. In short, in fmaps the setter into the functor returned by a function `f` applied to the "gotten" value. The function `f` is very interesting, though, since it has a type like `pure` but `forall f. Functor f` instead of `Applicative f`. In truth, it's not possible to construct such a function, but it prevents us from using any "special" property of the functor—all we can do is `fmap`. That code might as well be the implementation of `lens`, too. lens :: (s -&gt; a) -&gt; (s -&gt; a -&gt; s) -&gt; Lens' s a lens get set f loc = set loc &lt;$&gt; f (get loc)
I agree, but on the other hand if you are going to be working on clusters than using MPI can be easier than figuring out the details of how to find and communicate with the other nodes, and it is practically guaranteed that MPI will be installed.
Why is it safe to regard `v0` as a `Word64` in `countDigits`? I don't see how `countDigits` could return the right result for `Integer`s larger than `maxBound :: Word64`.
wouldn't that be like two telescopes :P
This kind of application looks like a good use case for [hsqml](http://hackage.haskell.org/package/hsqml) - have you looked at that at all? The example program for hsqml is also [a board game](http://hackage.haskell.org/package/hsqml-morris).
[decimal](http://hackage.haskell.org/packages/archive/attoparsec/0.10.4.0/doc/html/Data-Attoparsec-ByteString-Char8.html#v:decimal) from Attoparsec is quite slow. In one application, profiling revealed that parser to be the major CPU bottleneck.
With a 64 bit address space you're guaranteed that no number (that can be converted to a string) could be large enough to need more digits than can be counted with a 64 bit number. Remember, this function will force the argument, so you'd crash with OOM if you ever ran into a number big enough that its digit count would overflow a Word64. Of course, if you ever port this to a 128 bit machine, these kinds of tricks would have to be updated.
That's the argument, not the result *Main&gt; countDigits (maxBound :: Word64) 20 *Main&gt; countDigits (1 + fromIntegral (maxBound :: Word64 ) :: Integer) 1
Hmm, wait.. In order for something to have 2^64 decimal digits, you'd need 2^64 bytes to store the string (1 byte per digit), which you wouldn't have. EDIT: nevermind, I realized that it was the argument that was a Word64, not the number of digits.
Thanks. I've submitted the issue to the github repository for haskell-mpi.
Exactly; that's what I meant. /u/ssylvan's point would stand if we were talking about the first argument to `go`, not the second one.
Excuse my ignorance but, what does "greenfield" mean in this context?
Very worthwhile article, thanks!
If your Integral happens to be an Integer you're overflowing quite heavily, there. You could also use log, and it very likely makes sense to special-case Integers to use raw gmp. 
Kudos to both in that case :)
&gt; sending a message that they really don't want to cooperate fully with the community Seriously? That's overdramatic in my view. Many people just won't have thought it needs uploading, or not got around to it yet – so just ask for it. Only if they say no can you really start drawing conclusions about their ulterior motives.
One quote of the week! This is an outrage. 
In this case the project is being developed from scratch. It's not maintaining and building upon an existing codebase or debugging and updating legacy code. This means you can have (some) say in the design decisions etc. and not have to spend as long working out what the hell the guy before you, or in some cases, the guy 10 years before you was thinking.
Until an efficient implementation of FRP has been created, which has proven itself to be capable of withstanding the considerable strains of real world application development, it is hard to see FRP as even being a well defined programming model. I think that philosophical pondering of application development models is very useful, and has benefited Haskell a huge amount. But that doesn't mean that we should all bet blindly on the unproven horse of FRP. FRP hasn't proven itself yet, and until it does I am very happy to see people succeeding at non FRP based models. Re your quip about GUI tool kits. Imperative GUI toolkits are presented as a set of mutable objects which get drawn to the screen. An FRP application, in order to use such a toolkit, must be deranged, so as to represent, not behaviors, but a set of mutations to those objects which then coerces those objects into acting like the behavior you are trying to create. This is an extremely indirect path. It would be better, not to use an imperative toolkit at all, but to represent your behaviors as images which get drawn directly to the screen(using a framework like Cairo, OpenGL, ect). However, this still doesn't solve the problem with directing everyone to place all their eggs in the FRP basket. It is still insane to have every Haskeller trying at FRP when other models work well. I am sometimes afraid FRP will be the death of Haskell, as people assume that only the impossible FRP grail can be used to achieve interactiveness.
&gt; the claim "Well typed programs can't go wrong." is an humorous exaggeration. For what it's worth, I think this was originally said with a *very specific* meaning of "go wrong", for which it's actually true. As I understand it, it means that the type system is "sound" so that e.g. evaluating a type-correct program produces a type-correct result, and primitive operations are never invoked with inappropriate arguments (which would mean the semantics defined no way to continue – the program gets "stuck"). Although I suspect that even then it was meant at least somewhat tongue-in-cheek.
Very nice video editing skills :)
&gt; The problem is that the inner workings of Control.Lens are very generalized which make them very difficult to follow. It’s only March, but I suspect that’ll be in the running for Understatement of the Year. A few months ago I stared at the library’s type signatures for many hours until — well, until they either started to make sense, or else I had sufficiently internalized my confusion. In truth I began my own tutorial then, not to demonstrate the use of lenses but rather to motivate the type signatures for those who (like me) possess that horrible combination of insatiable curiosity and modest intellect. It’s surprisingly difficult to get from the desire for better getters/setters to type Lens a b c d = forall f. Functor f =&gt; (b -&gt; f d) -&gt; (a -&gt; f c) So I look forward to the later installments.
Yes sure, in that sense it is even true. If f is Int -&gt; Int, then f 5 will not return anything but an Int. I do not want to downplay the importance of this in any way, but it simply does not mean that you can't easily write programs that can fail: f :: Int -&gt; Int f 0 = 42 main = print (f 5) Note that, from a practical perspective, it doesn't matter if your program fails because f 5 diverges, or if it fails due to type errors.
It's [Revealjs](http://lab.hakim.se/reveal-js/).
Here is a link to the related package http://hackage.haskell.org/package/maxent
To get a head start, read [1], [2], and [3]. They're fairly abstract, but van Laarhoven is good at exposition and O'Connor has some breakthrough paragraphs. [1] http://twanvl.nl/blog/haskell/cps-functional-references [2] http://twanvl.nl/blog/haskell/isomorphism-lenses [3] http://arxiv.org/abs/1103.2841
We could make that too. binocs :: Lens' x b -&gt; Lens y b -&gt; Lens' b c -&gt; Lens (x, y) (c, c) binocs l r b = l . b `alongside` r . b
Story: I wrote the first version write after watching [Team America](http://www.imdb.com/title/tt0372588/), and one of my friends saw the american flag I had doodled and called the language amerilog. 
The reason composition comes out as *just* function composition is fairly subtle. To get a head start, read up on what I mentioned [here](http://www.reddit.com/r/haskell/comments/1anf0n/school_of_haskellbasic_lensing/c8zs6tr)
Do they really mean "&lt; 5 years" experience?
Oh, the (.) is actually just function composition, not some generalised version of it (like, say, fmap, or the Category version Tekmo gave)? In that case, the ordering really is confounding my intuition. I guess I've got some reading to do. Cheers!
The functions you're seeing in that article cannot be used on non-fixed-width integers, as they are not public.
There's no standard pronunciation, I pronounce it how it would be pronounced in Latin, i.e "o" as in "pot". This gives it assonance with monoid (which isn't a moanoid) and monomorphism.
Then put them in a typeclass, so it becomes self-documenting and idiot-proof?
youtube video directly: https://www.youtube.com/watch?v=NzyDgzPMbVw
Actually, my problems with the toolkits haven't been issues with impedance mismatches or anything like that. In fact, for the most part, Reactive Banana at least is very good at wrapping up the toolkits in a very simple but effective way that keeps me from worrying about specific details. The FRP library does not need to be deranged at all--all the mutable object stuff is simply below your level of abstraction. It's really not that indirect at all. I just write my FRP code and it ultimately gets translated down to events, mutable variables and polling, but I don't have to worry about *how* that happens. This is just the same as normal GUI programming where I write things in terms of events and never worry about how they translate down to functions called from an event loop or anything like that. My problems with the toolkits have been quite a bit more boring than that: they are very difficult to build, laying out my widgets is annoying and (at least with wx) I had problems finding certain things--namely a good web view for rendering HTML and CSS. I've already used FRP for my own relatively simple GUIs, and for those it's already strictly better than normal event-based programming. Admittedly I haven't tried anything super dynamic, but I don't have to: a very large proportion of UIs in practice are actually relatively simple. FRP is already perfect for those. Just because it isn't perfect yet does not mean it's some "impossible grail"! 
Really nice. Just sat through all the Haskell ones.
From the [README](https://github.com/mmirman/caledon): &gt; It's named caledon after the "New Caledonian Crow" - a crow which can make tools and meta tools. Since this language supports meta programming with holes, implicits, polymorphism, and dependent types, I thought this crow might be a good mascot.
http://www.stroustrup.com/whitespace98.pdf
Yeah... that's the wild part: it really is just function composition. Of course, function composition forms a Category so you can generalize it using the Category typeclass, but lenses actually compose as functions.
I wasn't correcting him, just making a quip about how odd it sounds pronounced that way. :P But, since you bring up standard pronunciations, there is actually a standard pronunciation, which is indeed [moʊnæd], at least going by Merriam-Webster: http://www.merriam-webster.com/dictionary/monad 
I'd say those words are not very well-defined in a language like Haskell. Because Haskell has no subtypes or mutation, there's really never a situation in which there's a semantic difference between the two. Hence, the right answer is basically that it doesn't matter. If you are asking about the implementation in GHC, then the answer is closest to static binding for stand-alone functions, and dynamic binding for methods of a class. Most of the time, anyway... depending on optimizations.
&gt;there's really never a situation in which there's a semantic difference between the two. I don't think I understand what you mean here. Consider: x = 3 f y = x + y z = let x = 4 in f 3 -- lexscoping: 6; dynscoping: 7
The question was not about lexical vs dynamic scope, it was about static vs dynamic *binding*, which AFAIK refers to which one of multiple overloaded variants of a subtype-polymorphic method is called. In some languages the compile-time type of a variable determines whether the subclass or the superclass is called. This is static binding. In others it is the runtime type -- dynamic binding. Many languages employ both static and dynamic binding in different situations. If this is what OP meant, then cdsmith is correct in saying that this isn't really relevant in haskell.
That's dynamic scope. A very different thing from dynamic binding. Haskell definitely has static scope (except explicitly with `-XImplicitParameters`)
Dynamic scope would be very much semantically meaningful. But dynamic binding is something different: it generally refers to the distinction between virtual and non-virtual functions in C++, for example.
No offense intended, have you considered other programming languages, e.g, OCaml or F#?
There was a Haskell cafe thread a while ago about this. UK dictionaries differ from Webster here.
For parsing, you **absolutely** want to be using [bytestring-lexing](http://hackage.haskell.org/packages/archive/bytestring-lexing/0.4.2/doc/html/Data-ByteString-Lex-Integral.html). While the parsers are given as operators on `ByteString`, it's trivial to wrap them for use in attoparsec. The `readDecimal` implementation therein has been [seriously optimized](http://community.haskell.org/~wren/bytestring-lexing/test/bench/html/) and it significantly outperforms *every* other Haskell implementation I've seen. (If you can improve on it, I would be *very* interested to see your code.) [When last I optimized it](http://winterkoninkje.dreamwidth.org/77021.html) the `readDecimal` parser outperformed the implementation in Warp and other web frameworks by a wide margin. I mention that mainly because parsing decimal numbers is a hotspot for web frameworks, so that's a very strong baseline to compare against. In addition, as of version 0.4.2, I've incorporated the optimizations mentioned in the OP's blog post. According to [my benchmark](http://community.haskell.org/~wren/bytestring-lexing/test/bench/packDecimal.html) the difference between the new version ("`packDecimal3`") and the one in version 0.4.0 ("`packDecimal1`") is only around 1.3x, so it's not nearly as impressive a boost as for `text`; but it's still a handy little speedup.
I can't say for the one-line implementation, but... I do have [some numbers](http://community.haskell.org/~wren/bytestring-lexing/test/bench/numDigits.html) for various implementations of arbitrary-base integral logarithms. Now, if you take the best one of those and use it in the context of the above post, you'll get something like "`packDecimal1`" in [this benchmark](http://community.haskell.org/~wren/bytestring-lexing/test/bench/packDecimal.html). And "`packDecimal2`" of the latter benchmark is what you get by replacing `numDigits 10` by the proposed `numDecimalDigits`. The move from "`packDecimal2`" to "`packDecimal3`" is what you get by additionally doing the `write2` optimization.
At least with UndecidableInstances, I can easily construct examples where there are infinitely many instances of a class, all of which are accessible at runtime. A mental model where all of these function calls are specialized and bound statically at compile time fails to explain how such a program could ever finish compiling. And, of course, in practice, type classes are nearly always implemented using dynamic binding techniques (dictionaries that are realized at runtime). Both of these facts make it very questionable to answer that Haskell always does static binding. If you look for specific test cases to distinguish which is going on, they don't exist: all programs in Haskell specify their behavior at a higher level of abstraction. The common trick is to "forget" the type information via subtyping... which isn't possible in Haskell. Incidentally, you could try to forget the type using a constrained existential type. Then you end up with the answer that Haskell does dynamic binding... but that's quite suspicious, since existential types can be seen as a syntactic sugar for keeping an explicit dictionary... and no one, for example, would argue that C has dynamic binding because you can pass around structs full of function pointers! All of this points to the notion that the question is too low-level for Haskell, and assumes a common vocabulary that no longer applies.
FWIW, I love attoparsec, but the thing to bear in mind is that attoparsec is, principally, designed as a *parsing* framework. For as fast as it is, once your interest moves to *lexing*, you can generally outperform attoparsec by designing dedicated lexers. Attoparsec is fast, in part, because it abandons a lot of the parsing niceties of Parsec (e.g., nice error messages); but you can be even faster once abandon the rest of those niceties (e.g., backtracking, choice,...).
One might say that static binding happens when you know exactly the type of the variable, and dynamic binding happens when you don't know the exact type but you know what type classes it belongs to, as type classes are akin to virtual method tables in C++ since, in both cases, a table of methods applicable for the type class are implicitly passed around through hidden arguments.
&gt; UndecidableInstances Okay, I wasn't thinking things through entirely there. The example I had in mind required existential types, so it's essentially the same example as later on. &gt; Look at the core output of any GHC program, the exact dictionary to use is passed in for any ad-hoc polymorphic call. Well, sort of... the compiler also quite commonly generates functions from one dictionary to another. But this is beside the point: in a lower-level intermediate representation of C++, the vtable would be constructed explicitly for all classes, as well. That doesn't change the fact that C++ virtual functions are always described as dynamic binding.
It's described that way in C++ because, in terms of semantics (not implementation), C++ has a notion of a "dynamic" type (represented by the vtable in the implementation). In Haskell, all type information is erased and classes are replaced with explicit dictionaries based on _static_ type information. Values don't carry around dictionaries with them to enable dynamic binding like in C++, so this really resembles static binding more than any other form of binding.
It would be quite interesting to see a comparison between the different FRP libraries when it comes to strenghts and weaknesses. Is it true that no FRP library is good enough for real-world use?
This talk is worth it just for the last 5 minutes, when the program Peter writes manages to outsmart the entire audience :)
It's come alive!
slides http://files.londonhaskell.org/2013/02/27/being-lazy.html
Not really. There is more than one open position so there is some flexibility but the initial brief was an Associate level hire and that is normally someone with less than 5 years experience.
Be careful, or you could get an arrow in the eye.
Thanks for doing this. I have tried to understand lenses by reading the Haddock docs, and came to the conclusion that somewhere in there were some really powerful ideas just struggling to get out.
I always considered “lazy” to be the wrong word here. Efficient is when you save the unnecessary work. Lazy is when you also save *necessary* work. And thereby harm yourself. Haskell is not lazy. Haskell is *efficient*.
Ha! +1
It's interesting that the job is about re-engineering automation scripts from a dynamic language into Haskell. Having recently written some automation scripts in Ruby which orchestrated integration tests, I sometimes missed the assurances provided by a statically typed language. Simple things like the script not failing midway of a long execution because of a misspelled variable. Of course, that particular problem is solved in Ruby through unit testing, but I find these kinds of automation scripts quite cumbersome to unit test. As for why Haskell as opposed to Ocaml or F#... I think the segregation of pure from impure code thanks to the IO monad opens interesting possibilities. And packages like process-conduit (http://hackage.haskell.org/package/process-conduit) are making easier to invoke external programs from Haskell.
Is there a video / audio / transcript that goes with this? It seems great, but some more words might help.
Unfortunately the audio wasn't recorded properly, but there is a video ... somewhere...
Lazy is sometime the appropriate intuition. Indeed, someone lazy will delay as much as possible the work it has to do, sometime in the hope that others will do it for him at some point because of pressure. But eventually, if really forced, he will do the work. Often late because of being overwhelmed at the very last moment. This is pretty much the pitfall of laziness in multi-threaded systems. Some threads build-up work as input allows to construct it, but it does not get reduced immediately. Instead, some other thread that need a more evaluated form of the work will unexpectedly have to perform the actual computational work, and output will come late because of work congestion.
Very nice work - Selecting this for [School of Haskell](https://www.fpcomplete.com/)'s [Pick of the Week](https://www.fpcomplete.com/school/pick-of-the-week)
Link to the source http://conal.net/talks/folds-and-unfolds.lhs
All these questions, and more, answered definitively in the Haskell report! http://www.haskell.org/onlinereport/haskell2010/ from there we see (http://www.haskell.org/onlinereport/haskell2010/haskellch2.html#x7-180002.4) that &gt; An identifier consists of a letter followed by zero or more letters, digits, underscores, and single quotes. Identifiers are lexically distinguished into two namespaces (Section 1.4): those that begin with a lowercase letter (variable identifiers) and those that begin with an upper-case letter (constructor identifiers). Identifiers are case sensitive: name, naMe, and Name are three distinct identifiers (the first two are variable identifiers, the last is a constructor identifier). so... as many characters as you want! all the characters!
https://github.com/ghc/ghc/commit/5319ea79fa1572b7d411548532031f9d19b928c6 &gt; Nullary (no parameter) type classes are enabled with `-XNullaryTypeClasses`. &gt; Since there are no available parameters, there can be at most one instance &gt; of a nullary class. A nullary type class might be used to document some assumption &gt; in a type signature (such as reliance on the Riemann hypothesis) or add some &gt; globally configurable settings in a program. For example, class RiemannHypothesis where assumeRH :: a -&gt; a -- Deterministic version of the Miller test -- correctness depends on the generalized Riemann hypothesis isPrime :: RiemannHypothesis =&gt; Integer -&gt; Bool isPrime n = assumeRH (...) &gt; The type signature of &lt;literal&gt;isPrime&lt;/literal&gt; informs users that its correctness &gt; depends on an unproven conjecture. If the function is used, the user has &gt; to acknowledge the dependence with: instance RiemannHypothesis where assumeRH = id 
If the point of nullary type classes depends on their instances only being defined in programs (and not libraries), do we need to prevent nullary typeclass instances in libraries from being (accidently) uploaded to Hackage? 
This is actually very useful. A nullary type class is essentially a **module interface**. Since it can have at most * ^ 0 = 1 possible instance, that behaves as a module implementation. Note that it can even have types.
There isn't much reason not to use passphrases like that instead of the mishmash with numbers and symbols in it too -- apart from the fact that many sites place dumb restrictions on passwords, like disallowing spaces and forcing numbers or symbol characters to occur in a password that's over 20 characters long.
Just use [diceware](http://diceware.com).
I don't see any point in enforcing this.
In the example of the Miller prime test, can you select the fallback version if you don't want to assume GRH? Without oleg-ery, if possible! Also, for some emotional reason, I'm extraordinarily worried about orphan nullary type class instances. No parents, no arguments, and no way to newtype around them - what are they going to do? ---- In all seriousness, if you write a "constructive" library and someone in your hierarchy decides to sneak in class ExFalsoSequiturQuodlibet where efsq :: ((a -&gt; Void) -&gt; Void) -&gt; a instance ExFalsoSequiturQuodlibet where efsq proof = &lt;some absurd nonsense&gt; Can you do anything against that? Or am I worrying about nothing?
Could you give a simple example? The `RiemannHypothesis` example somehow doesn't strike me as something I would write! 
Why would you worry about efsq (which doesn't have a nice sensible definition anyway) and not false :: forall a. a false = false ...?
Good point! Clearly, I haven't thought this through entirely. Still, something feels fishy, but I'll withhold judgement until I see more of the intended uses.
In any case I don't see how the class/instance mechanic makes any difference. class ExFalsoSequiturQuodlibet where efsq :: ((a -&gt; Void) -&gt; Void) -&gt; a instance ExFalsoSequiturQuodlibet where efsq proof = &lt;some absurd nonsense&gt; is much the same as efsq :: ((a -&gt; Void) -&gt; Void) -&gt; a efsq proof = &lt;some absurd nonsense&gt; 
Well, I don't know if you're aware of [the comic that popularized this argument](http://xkcd.com/936/). I remember when I saw the comic, my reaction was that it's not spelling out the most important assumption for the proposal to work: the four common words must be chosen truly at random. If you allow *any* input from the user (like, "do you accept this one or should I generate another?"), the entropy of the passphrase plummets: 1. Users will pick passphrases with more frequent words over alternatives with more infrequent ones. 2. Users will prefer passphrases with word orders that match grammatical constructions of their language. E.g., English adjectives precede the noun they modify, so users will prefer passphrases that reflect that. 3. Users will prefer passphrases where the words bear a semantic relationship. E.g., "brown dog" will be preferred over "blue dog." 
Huh, I think this will make an excellent interview question: write a program to return four random words from the dictionary file, but which: 1. Runs in constant space compared to the size of the file. 2. Scans the file only once. 3. Does not have prior knowledge of the file's size.
Hmmm. I have thought before that I would nice if it were somehow possible to set (global?) "default" values for implicit params, but it were still possible to explicitly override them if necessary. This almost acomplishes that, except there's no way to override the defaults.
This is just a generalization of reservoir sampling. You immediately accept the first four words in the dictionary and then each successive word has a diminishing probability of displacing a currently selected word (selected at random).
Learning new words is a definite side benefit. One idea that struck me as a possible simplifying approach depends on whether `top4 = take 4 . foldl (flip insert) []` is optimized to use O(1) memory. So maybe I'll check it out. And, if I were interviewing you, I'd say. "That's cool. Now you're given an additional requirement - the words must all be distinct. Apart from repetition, all possible sequences of 4 words must be equally likely"
If you implement reservoir sampling correctly then it automatically satisfies both requirements. All selected words will be distinct and every word in the dictionary has an equal probability of appearing in the result. It's a bit counter-intuitive at first until you figure out what the probabilities are still equal, but it's a rather clever algorithm.
My password safe automatically generates and saves random passwords for me, so there is no need to remember any (except the password for the wallet itself, of course).
It's actually a very well-known interview question.
I'm not exactly sure what AshleyYakeley means, but here's an example that I think illustrates the point: imagine we have some library that does some useful thing, but some part of it is application specific and requires whoever uses our library to include some functionality. With nullary type classes, our library code can look like this: class Foo where foo :: Bar -&gt; Int foobar :: Foo =&gt; Bar -&gt; Int foobar x = foo x + 1 If someone wants to use `foobar`, then they have to provide an instance of `Foo` and implement `foo`. instance Foo where foo = const 3 Alternatively, if we have some base code that can be done in different ways, we can do the same thing and simply have multiple modules for the different implementations. Then users can import the base module and whatever implementation they want to use.
Care to compare/contrast with liquidHaskell (http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/)? 
It should be noted that the random generator used in System.Random is not cryptographically secure so it should not be used for password generation. It's better to use something like Crypto.Random.
What widgets require a mutable internal state? I mean, you'd think say a text editing area would, but what stops you from keeping the cursor position, selection, ect, in the main program state and handling key events directly to change those things?
&gt; Runs in constant space relative to the size of the file. I read this and thought --- isn't that what's implemented here? Constant "relative to the size of the file" is a strange way of putting it. It makes it seem like you need a strict read of the entire file, rather than lazy loading, which would be relative to the size of the largest random index generated. Maybe just "in constant space"?
Why a *diminishing* probability? Wouldn't that bias the selection to words occurring earlier in the file? 
It gives the same error as it used to, except with "Use -XNullaryTypeClasses to enable nullary type classes".
Nope. The reason it works is that words occurring earlier in the file also have a higher chance of being displaced by a later word. The actual probability you use is that if you have k slots and you are at word number n, then you select it and displace an existing word (selected at random) with probability k/n. I highly recommend you start with the case of 1 slot (i.e. k=1) and calculate the probability of inclusion of every word after each step.
beautiful! Thank you. When you put it that way, it would be biased toward the end if you *didn't* use a diminishing probability. 
Actually, it is not about segregation in this case. Our system is huge and complex, and any solution has to run a) cross-platform, b) have good web support, c) lots of open source libraries. So a) rules out F#, b) and c) rules out OCaml. Usual toolbox is Snap/Yesod + Hackage + GHC on Linux/Solaris/Windows. Also, we already have ~1.2M lines of Haskell... (and maybe 1% the amount of that in F#)
The only overtly haskelly bits I can think of are linq, async and RX... But I'm neither a haskell or C# expert!
It's probably more productive to just pretend that it's a somewhat nicer Java than a very crippled (from the point of view of FP) Haskell.
I think what you are talking about is F# not C# F# indeed has some things in common with haskell (but it's more like Ocaml) True you can indeed do some simple FP (as in every languague nowadays) but it is a pain because the type-system gets very verbose (Func&lt;Func&lt;A,Func&lt;B,C&gt;,D&gt; ....) but F# is really nice and has a really strong OSS community backing it) You can learn/try a lot here: http://www.tryfsharp.org/ and it's not that different to learn (of course the frameworks are different but to be honest: the documentation and what comes with it (.net framework) is much better for your everyday - non-ebony-tower - work) (BTW: yeah I saw that you "have to" do it in C# but as F# runs on the same env. you might speak to your Prof. or whoever gave you the assignment and have him/her let you do it in F# too - worth a try - we all had to do it in our jobs :D )
Would this be a good way to do dependency injection in Haskell?
I think there's a reasonable C# subset for functional programming, so if you stick to that you should be able to pick it up relatively quickly. Read up on: * [LINQ](http://code.msdn.microsoft.com/101-LINQ-Samples-3fb9811b) -- you can use the query comprehension syntax, or the regular first-class function syntax. The former is sugar for the latter. * [Tuples](http://www.dotnetperls.com/tuple) * [Lambdas and delegates](http://msdn.microsoft.com/en-us/library/bb397687.aspx) (delegates are first-class functions) * Parametric polymorphism is known as [generics](http://msdn.microsoft.com/en-us/library/0zk36dx2.aspx). You can place generic parameters on methods/functions, and type declarations. * The standard [System.Func* and System.Action* delegates](http://msdn.microsoft.com/en-us/library/yxcx7skw.aspx). These are delegates with generic parameters. Most first-class functions you deal with will be one of these two classes of delegates. * [Type constraints](http://msdn.microsoft.com/en-us/library/d5x73970.aspx) Caveats: * lambdas and delegates are *nominally* typed, so you can't implicitly or explicitly coerce a delegate of one type into a delegate of a compatible signature, ie. System.Predicate&lt;int&gt; is signature compatible with System.Func&lt;int, bool&gt;, but they are not interconvertible without doing some magic like I do in my [Sasa.Func.Coerce](http://msdn.microsoft.com/en-us/library/yxcx7skw.aspx) library function. * methods and delegates are multiparameter, not curried like in OCaml and Haskell, thus leading to all the Func* and Action* overloads. * The "void" return type is not a type, so it can't be used as a generic argument. Hence the need for all the Action* delegate types distinct from the Func* delegate types. Action* differ only in the fact that they return void. * generic parameters on methods are strictly more general than generic parameters on delegates (which are types). Method generics support first-class polymorphism, while type declaration generics do not. * classes are always implicitly option types, ie. they are nullable, while struct types always have a "valid" value, ie. are not nullable. Struct types are then useful for eliminating null reference exceptions in programs, as long the default struct value is meaningful.
C# is very very similar to Java. There are a few things that have made their way from Haskell into C#, such as type inference and type-safe first-class functions (these are called "delegates" in C#), but only in a very mild form. Linq and Enumerables and such are possibly somewhat influenced by Haskell's monads, in how they hide the plumbing of a computation and present only a high-level interface, but the similarities are fairly far-fetched, and thinking of them as monads will probably not help you much. Extension methods are another feature with a slight Haskell smell to them, but again, this is quite far-fetched. For someone whose programming experience so far has been Java, C++, OOP PHP, etc., C# introduces some wild new concepts, but for a seasoned functional programmer, it feels more like "meh". Compared to Haskell, C# does have quite some extra things to offer, but those are from the realms of imperative and object-oriented programming, not functional. C# is, at its base, firmly rooted in a class-based single-inheritance OOP paradigm.
Real-World Functional Programming by Tomas Petricek. It introduces functional programming to C# programmers with a lot of examples in F# and C#. As *CKoenig* said, C# *supports* some functional programming techniques (there are lambdas, parameterized types, a kind of lazy evaluation, tail recusion, list comprehensions ...) but the syntax is way too verbose that this programming style will never become as pleasant as in Haskell. Anyway, most C# programming courses today only give Visual Studio assignments, using the provided automatic code generation ... nobody learn how to program in a C# course anymore. But if you have the "luck" to get an assignment which doesn't rely on the Windows-Forms/Data-Binding/ASP/Silverlight crap, maybe you can ask your teacher to do it in F#. It's a damn good language. Better than Haskell for a lot of things. The type providers feature introduced in the last year version of the language is probably the best improvement in "business oriented" programming language for a decade. Moreover, knowing Haskell, you can become fluent in F# in an afternoon.
Oops, I thought they meant including it by default. Ignore me, then!
That's good to hear. I imagine that code path will be exercised about 99% by people who forgot the type class parameter, and 1% by people who actually wanted a nullary type class. It's fairly common to see: class StringLike where toString :: a -&gt; String
&gt; Linq and Enumerables and such are possibly somewhat influenced by Haskell's monads, in how they hide the plumbing of a computation and present only a high-level interface, but the similarities are fairly far-fetched, and thinking of them as monads will probably not help you much. LINQ was explicitly inspired by the list monad in Haskell, and LINQ syntax fails to be a sort of monad comprehension mostly because C# can't support a generic equivalent of `return`. A couple guys on the C# team at MS wrote monad tutorials and everything when LINQ was first released. Note also that GHC's [generalized list comprehensions](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#generalised-list-comprehensions) appear somewhat influenced by LINQ. C# basically started as a cheap Java knock-off but the most interesting additions since then have consistently (and often intentionally) moved it in the direction of Haskell.
Yay!
If orphan instances are a bad idea, and there's no point in writing an instance of a nullary typeclass in the same module as that typeclass, that makes all non-useless nullary typeclass instances orphans, and thus a bad idea.
Or would be, if I knew Java. But even then I guess I'm interested in what those "somewhat nicer" things *are*. It's a fairly largeish assignment, so I'm trying to get to know the language a bit better, otherwise I'll just flounder around. I've already read a bunch about how it's different if you're coming from C++, now I'm trying the Haskell angle in case it'll give me more perspective.
Nah, I was in fact thinking of C#. As camccann mentions in another comment, and as I'd heard before, many of the features they added later felt/feel Haskell-inspired. So that's what I'm trying to learn more about. And not much chance of F# unfortunately.
How about providing a newtype for monad morphisms and having two versions of hoist, one for the monad morphisms newtype (without normalizing) and one for otherwise (with normalizing)?
The advantage of that is that it cleans up the type signatures and then you can weaken Rank2Types to PolymorphicComponents. However, there are two disadvantages: * Libraries would have to make `mmorph` a dependency to export their own monad morphisms. This is the big issue. * composing monad morphisms requires importing `Control.Category` Another approach I considered was to just to include an `unsafeHoist` method in the `MFunctor` class that defaults to `hoist`, but may be overridden for efficiency if it can take advantage of the knowledge that the argument is a monad morphism. Then the burden is on the user to prove that it is a monad morphism if they want the speed gain. However, even that approach has its disadvantages. For example, `pipes-safe` passes the monad morphisms in its type signatures internally to `hoist`, so then I would need to duplicate the entire API to provide safe and unsafe versions of every function. So, for me at least, I would prefer to duplicate `ProxyFast` (i.e. a `ProxyFaster` type) and then provide that in a base proxy implementation not selected by default, rather than duplicate the entire `pipes-safe` API. More generally, this issue is pretty narrow to just `pipes` and I don't anticipate other libraries having this problem where they even need to worry about whether or not a monad morphism, so I'd prefer to just add one module to `pipes` rather than impose API duplication on every library that uses `mmorph`. I would rather not impose additional API complexity on other libraries that use `mmorph` just because of an issue that really only affects `pipes`. So right now the choice is mostly between whether or not I provide an almost duplicate implementation of `ProxyFast` with an unsafe hoist or I just make `ProxyFast` unsafe and warn users about the issue.
In some programming languages, standarts and real life implementations can be different. Is there a limit for name length in GHC ?
This is kind of off-topic, but after thinking about this for a bit, and with [some](http://hackage.haskell.org/package/tagged) [help](http://hackage.haskell.org/package/reflection) from [edwardk](http://hackage.haskell.org/package/constraints)'s packages, I implemented [named and unnamed implicit parameters with defaults](http://github.com/duairc/implicit) (that can be overridden).
&gt; composing monad morphisms requires importing Control.Category This isn't really a disadvantage. `Control.Category` would ideally be in `Prelude`
I got curious of this approach, so I tried it on the German words file that ships with gnu-aspell. Turns out that the resulting phrases are actually hard to remember because that file includes all declensions and conjugated verbs. On the other hand, composed words really help: schlurfenden niederländisches leistungsfähigen ereignislosem unterdrückte Nahrungsvorräte erstrangiges Gleichverteilung erschieße Rekordergebnissen angekoppelte Zeitfallen vergeistigtem Zeitgenossen unkritischem Fußgängerübergang
Note that you can create nullable value types (including structs) using `Nullable&lt;T&gt;` or the equivalent `T?`.
It's very unwieldy. If you defined a composite widget, you would have to expose all its internal widgets' states via its interface. What would be the advantage over an imperative GUI tookit, where you *can* abstract out widgets without spilling their guts over all of your program?
Are you at all familiar with OOP in the nominal subtyping style? i.e., static types with explicit declaration of subtype relationships. It sounds like you're implying that you already know C++. If so, between that and Haskell you already know most of what's in C#. The only trick being which parts are which...
Learn about Haskell's sum types (i.e. `Either`) and how Haskell uses type classes instead of inheritance. Those will give you a unique perspective. Unfortunately, once you learn sum type you really start to resent programming in languages that don't have them.
The link to the tutorial is messed up. Any plans to get pipes into Stackage and put the tutorial (or new content) on the SoH? *[edit: Here's a thought: it would be cool if pandoc supported haddock markup.]*
&gt; I will try to get it added and write up a tutorial, although really I just want to get pipes-bytestring out ASAP because most of the examples I would like to demonstrate depend on that library. Sure! Why is that a separate package BTW? Are there compilers that don't support it, I wonder? You can't escape it on GHC, anyway.
The major thing that I notice... is that in parseArgs, you use do notation where it's totally unnecessary. This is a qualm about style, but simply including the expression, or using (&gt;&gt;), would be cleaner. https://gist.github.com/arbn/af4f777cd07e8c4b7c30 
The basic layout of the `pipes` ecosystem is designed like this: * `pipes`: This "ring 0" library focuses only on abstractions which are elegant (i.e. anything where there is only "one true way" of doing it). This library is also supposed to pave the way for the rest of the pipes libraries and convince people that the rest of the ecosystem is worth installing, so anything that makes it easier to install and less likely to conflict with existing package installations, the better. * `pipes-safe`, `pipes-parse`, `pipes-free`, `pipes-stm`: These are the "ring 1" opinionated libraries that introduce standard idioms for various things. For example, `pipes-parse` will introduce the standard way to deal with parsing, push-back and end of input. These are the libraries where there isn't "one true way" of doing things, but I still make a reasonable effort to try to get it right and keep it as elegant and simple as reasonably possible. Dependencies are less of an issue for these libraries, and `pipes-free`, for example, will have a `free` dependency. * Everything else (i.e. `pipes-network`, `pipes-attoparsec`, `pipes-bytestring`, etc.). These are the "business end" of the `pipes` ecosystem. I also tend to not release things until I judge them to be close enough to completion to be very low maintenance because I have extremely limited time available to program being both a graduate student and a father of two. If I were to release `pipes-bytestring` in its current state I know I would get a mountain of questions that I would basically be answering with "Just wait until `pipes-parse` comes out, it solves these problems", so I'd rather just wait until it is actually ready rather than release something half-baked.
The argument still remains that it would be difficult to read any of those passwords over the phone if the need arises.
Nice, but a little dated. Nowadays, some random bits read from /dev/random should be just fine, as long as you are careful not to write anything to disk (including swap). Also - making sure that no one is in the room and closing the curtains is no longer sufficient now that miniature surveillance cameras are so cheap. ;)
One of my hopes for [last year's Summer of Code](http://hackage.haskell.org/trac/summer-of-code/ticket/1609) was to eventually be able to embed runnable code snippets in Haddock. Sadly, the project didn't pan out.
This piece of the pipeline: foldl (++) "Solution:\n" $ ... can be rewritten as append "Solution:\n" $ concat $ ... to save on the O( n^2 ) retraversal of the initial portion. I can't think of a place where you'd actually want `foldl (++)`, and I'm inclined to regard it as a universal code smell. 
I agree, FRP sucks :) Edit: OK, that was rude. But I think this example shows my point about FRP being flawed rather well.
Very nice, but... Custom Setup.hs files make cabal packages more fragile in several ways, so it's usually recommended to try to avoid that. The `extra-html-files` way is better, but as Brent points out, it limits the package to recent Cabal versions, and it's usually a bad idea to upgrade Cabal except at the same time as a GHC/Haskell Platform upgrade. What versions of Cabal support `extra-html-files`, and how do older versions of Cabal react to it? Do they just ignore it? If so, fine. Otherwise, I hope this feature does not become widely used until the Haskell Platform supports `extra-html-files`. EDIT: As of now, I imagine that the auto-generated haddock documentation on Hackage will stop being generated for packages that try to use this.
The sample on the linked blog page is a PNG.
Right now, `extra-html-files` is only in HEAD. Older versions of Cabal will print a warning but otherwise ignore it. I'm pretty sure it won't stop Hackage from building documentation (though I haven't tested it).
It would be useful to see absolute performance numbers. Benchmark against native bytestring for example. I am suspicious there are non-trivial overheads being introduced.
OK, great, then people can go ahead and start drawing their Haddock diagrams! The potential audience will gradually expand during the next year or so. Please remind us every once in a while about this neat feature.
Isn't there some way to get rid of that `dummy#` component that doesn't do anything (from the user's point of view)?
I would like to incorporate external renderers in some backends. This could give you LaTeX in your diagrams. It is listed among our [GSoC ideas](http://www.haskell.org/haskellwiki/Diagrams/GSoC).
&gt; bytestring with lazy IO wins, followed by pipes, followed by conduit. Although bytestring had no significant outliers, pipes moderate and conduit severe. I don't fully understand how that works in criterion, or what it means, but doesn't it make the numbers kind of useless?
Also machines, and the older iteratee packages! In fact, it would be nice to have a benchmark suite involving all the options and a rich set of test cases.
I don't use only `criterion` when I benchmark things and I have several alternatives I use, including applications I have written and the venerable `time`, and all of those measurements closely match `criterion`, so those numbers are accurate enough for real world purposes. I don't know what the origin of the outliers are, but if I had to guess it would be because `pipes` and `conduit` are heavier on the garbage collector which might be triggering randomly throughout the profiling process.
Thanks for reminding me. `io-streams` comes close to `bytestring` in performance: ... , bench "iostreams" $ withFileAsInput inFile $ \hIn -&gt; withFileAsOutput outFile $ \hOut -&gt; connect hIn hOut ... benchmarking iostreams mean: 494.5905 us, lb 491.2628 us, ub 499.2152 us, ci 0.950 std dev: 20.11511 us, lb 15.26249 us, ub 25.48040 us, ci 0.950 found 9 outliers among 100 samples (9.0%) 9 (9.0%) high severe variance introduced by outliers: 37.563% variance is moderately inflated by outliers There is no comparison for the latter benchmark since `iostreams` does not manage resources.
:) It's not as bad as it sounds. You just learn to program in your head more and then put it down in code as quickly as possible when the kids give you a break.
How about this? swap :: Int -&gt; Int -&gt; [a] -&gt; [a] swap i j xs = low ++ [xv] ++ mid ++ [xu] ++ high where u = min i j v = max u j (low, xu:xs') = splitAt u xs (mid, xv:high) = splitAt (v-u-1) xs'
Minor bug, should be "`v = max i j`". But yeah, that's a good readable version. I was trying to turn it into a version that made the minimum number of allocations, but I couldn't come up with anything faster than the version I posted. Just as well, I should've just switched everything to `ByteString` instead of messing around with trying to make it fast with regular `String`.
Right, I mean on the Haddock pages. Sorry, should have been clearer.
Hmm, yes, I suppose we could easily get rid of that.
This is exactly what the pipes library does, except without type families or MPTCs (because they don't work if you want to preserve the ability to be composed).
I recently used `conduit` to work on something so here are some thoughts on how to use it (in its current state) to get what you want: If you write, index = awaitForever $ \i -&gt; -- Note that the operations get/put need to be lifted -- to be evaluated in the underlying monad. n &lt;- lift get lift $ put (n + 1) yield (i, n) You get, index :: (Num s, MonadState s m) =&gt; ConduitM t (t, s) m () -- Specialization: index :: ConduitM t (t, Int) (StateT Int IO) () You want something that is `ConduitM t (t, Int) IO ()`. Consider `transPipe`. transPipe :: Monad m =&gt; (forall a. m a -&gt; n a) -&gt; ConduitM i o m r -&gt; ConduitM i o n r Looking at the type signature, a first thought might be to pass `flip evalStateT 0` there, and that would indeed give you `ConduitM t (t, Int) IO ()`. However, if you run it, everything will have the index `0`. Example: &gt; sourceList ['A'..'Z'] $= transPipe (flip evalStateT 0) index $$ consume [('A',0),('B',0),('C',0), ...] That is because `evalStateT` is being used here multiple times to map each `StateT IO a` to `IO a`. What you want instead is to stay in the `StateT Int` monad. sourceList ['A'..'Z'] $= index $$ consume :: StateT Int IO [(Char, Int)] Now you can `evalStateT` at the top. &gt; flip evalStateT 0 $ CL.sourceList ['A'..'Z'] $= index $$ CL.consume [('A',0),('B',1),('C',2),('D',3), ...] You can use `transPipe lift` to lift streams from `ConduitM i o IO a` to `ConduitM i o (StateT Int IO) a` when connecting to `index`. Also, `Data.Conduit.List` provides `mapM`: mapM :: Monad m =&gt; (a -&gt; m b) -&gt; Conduit a m b It executes operations in the underlying Monad (so you don't need to lift `get`/`put`). You can rewrite `index` as: index = mapM $ \i -&gt; n &lt;- get put (n + 1) return (i, n) So, it's either this, or the loop you had originally. For something as simple as a counter, the original loop might be better. Side-note: My original use case required something similar to maintaining a counter. Something that bit me in the ass was `put (n+1)`. This remains unevaluated until the `evalStateT` so you'll build up a very large chain of thunks `((...((n + 1) + 1)...) + 1)`. Instead, you want `put $! n + 1` to evaluate the `n + 1` right away. 
Nice. One point: aren't you measuring strict bytestring IO there though? You would need Data.ByteString.Lazy to measure streaming file throughput.
I see, so instead of putting monad transformers on top of `ConduitM i o m`, you can use `transPipe` to slide the transformer right beneath it in the transformer stack, and achieve the same effect. index :: Monad m =&gt; Conduit i m (i,Int) index = transPipe (flip evalStateT 0) . awaitForever $ \i -&gt; do n &lt;- lift get lift $ modify (+1) yield (i, n) That would make `MonadConduit` pretty redundant. Your warning about avoiding thunk build-ups is well put.
You can view the original one [here](http://swizec.com/blog/aho-corasick-string-matching-algorithm-in-haskell/swizec/5376) where the code is actually copied correctly.
Well, not exactly. As I mentioned, that will result in everything having the same index because `evalStateT` gets called separately for each item. So, what you do is, `transPipe lift` *other* sources/sinks/conduits into the `StateT Int m` monad to connect them to index. So, if you have a source and a sink over the `IO` monad, mySource :: Source IO Char mySink :: Sink (Char, Int) IO String You cannot directly connect them to `index` which is of type (specialized): index :: Conduit Char (StateT Int IO) (Char, Int) You want to lift `mySource` and `mySink` into the `StateT Int IO` monad. transPipe lift mySource :: Source (StateT Int IO) Char transPipe lift mySink :: Sink (Char, Int) (StateT Int IO) String Then you can connect them using `index`. transPipe lift mySource $= index :: Source (StateT Int IO) (Char, Int) index =$ transPipe lift mySink :: Sink Char (StateT Int IO) String So, you can complete the chain in either direction: transPipe lift mySource $= index $$ transPipe lift mySink :: StateT Int IO String transPipe lift mySource $$ index =$ transPipe lift mySink :: StateT Int IO String So, you get a computation in the `StateT Int IO` monad which you can evaluate using `evalStateT`/`runStateT`/`execStateT`.
reading comprehension fail on my part, thanks for trying again.
&gt; And, again like anything novel and different, the problem is always that not enough people are using or developing it, not the opposite! The real problem is that there's not enough interactive software written in Haskell in the first place, with or without FRP.
And on Hackage: [http://hackage.haskell.org/package/implicit-params](http://hackage.haskell.org/package/implicit-params)
Tangentialy related to haskell, but I thought it would be in the vein of interest of many here. If this is inapropriate, let me know.
# **Fixed your link** I hope I didn't jump the gun, but you got your link syntax backward! Don't worry bro, I fixed it, have an upvote! - [I bit.](http://cstheory.stackexchange.com/a/17014/14334) ^Bot ^Comment ^- ^[ [^Stats ^&amp; ^Feeds](http://jordanthebrobot.com) ^] ^- ^[ [^Charts](http://jordanthebrobot.com/charts) ^] ^- ^[ [^Information ^for ^Moderators](http://jordanthebrobot.com/moderators) ^]
Thanks. I was in a dreadful hurry...
Could you give an example of this with `pipe`?
And [here](http://www.reddit.com/r/haskell/comments/10zqla/ahocorasick_string_matching_algorithm_in_haskell/) is the previous discussion.
Here you go: import Control.Proxy import Control.Proxy.Trans.State index :: (Monad m, Proxy p) =&gt; () -&gt; Pipe (StateP Int p) i (i, Int) m r index () = forever $ do i &lt;- request () n &lt;- get respond (i, n) put $! n + 1 &gt;&gt;&gt; runProxy $ evalStateK 0 $ enumFromToS 'A' 'E' &gt;-&gt; index &gt;-&gt; printD ('A',0) ('B',1) ('C',2) ('D',3) ('E',4) The `pipes` tutorial describes this in much more detail in the [Extensions](http://hackage.haskell.org/packages/archive/pipes/3.2.0/doc/html/Control-Proxy-Tutorial.html#g:14) and [Local State](http://hackage.haskell.org/packages/archive/pipes/3.2.0/doc/html/Control-Proxy-Tutorial.html#g:16) sections.
Thanks for taking the time to write that, that's very nice.
My pleasure!
Is this just an epsilon-NFA^1 that stores actions at particular nodes such that, if they are reached, a value should be emitted? ^1 - I realize it's deterministic, but it's not a DFA. The failure transitions correspond to epsilon-transitions in an epsilon-NFA.
Very neat.
I'm not really sure I understand the reasoning behind this. If I read `she` but then read an `r` the word I'm reading is obviously not `hers`: it's `shers` or something. And if `shers` was a real word (or prefix of one) it would be in the dictionary in its own right.
Some further discussion on napierian functors here: http://stackoverflow.com/a/13100857/371753 Working through representability in Hask is great fun, btw. All cartesian closed categories are enriched in themselves, which is terribly handy in such circumstances.
Is there some combinatorial species trick to perform so that `Log (Maybe a)` "becomes equal to" `a - a^2/2 + a^3/3 - a^4/4 + a^5/5 - ...`. The coefficients seem to indicate that 'circular lists' are involved, somehow. But other than that, I'm lost!
&gt; What would be the advantage over an imperative GUI tookit, where you can abstract out widgets without spilling their guts over all of your program? It sounds like you just want a way to express that your program is parametric over widget state that you don't care about, ie. instead of getting rid of this state/not exposing it, make the state opaque to code that shouldn't care about it.
I remember looking at an earlier version of this and finding it absolutely obnoxious, but a few iterations later it's looking pretty good. And it's not even halfway done yet! I can't wait to see how this book will eventually end up. EDIT: also, TIL about using unsafeCoerce to check if a list is null. I'm totally using this from now on.
At the bottom of page 10 (section 2.2.2).
Ah. For some reason Ctrl-F doesn't find that.
That sounds something like what the School of Haskell does. Maybe it's possible to use some of their code for Haddock. (Is it even open source?)
Just FYI, I hear that `LogMaybe a` is more precise than `Log (Maybe a)`, especially on small types.
Oh cool. I had tried reading that before, but was not at all ready for it. Level up!
&gt; EDIT: also, TIL about using unsafeCoerce to check if a list is null. I'm totally using this from now on. Hope this is a joke. Use null to check if it's empty.
Don't worry, in the text it is very clearly a joke.
Right! That's basically all I'm looking for. Which parts are which. I already know C++ and Haskell very well, so there's probably not much in it that's new outright. But it's a big alien environment and I feel lost without a map. :)
No such luck unfortunately - ASP it is. 
Awesome, thank you! I'll read them for sure.
Log (List a) ≅ a + a\^2/2 + a\^3/3 + ... That's the species of all cycles. I also have no idea what it means.
I like it, pretty example heavy, there are definitely those out there who would benefit from this approach. I think that there needs to be more of a style difference between the compiled code examples and the ones that are to be run through the REPL. Also if the examples were included as attachments with the PDF that would be a definite +1. Hopefully the author has the fortitude to finish this up.
If this ever becomes reality, please return to edit this comment. I would be interested.
I think the radius of convergence is pretty meaningless in this context, since the series are (usually?) cut off at some finite point.
Sorry, I know! I was just making a subtle reference to the difference between log(1 + x) and log1p(x) on small values of imprecise types like IEEE floats :)
Numerics never was my strong suit :-) My R-o-C remark wildly misses the point!
Yeah, I think the example problem was really poorly chosen. Does anyone find f = (5+) . (8/) easier to understand than the original?
The arithmetic example is not a good one, but I will often write something like f s = length (head (words s)) only to convert it to f = length . head . words later. I find the latter *much* easier to read. 
There's probably something that can be done with [`reallyExtremelyUnsafeCanYieldFalsePositivesAndFalseNegativesPtrEquality#`](http://www.haskell.org/ghc/docs/latest/html/libraries/ghc-prim-0.3.0.0/GHC-Prim.html#g:20), but I recommend using [the `StableName` abstraction](http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-Mem-StableName.html). Haskell 'pointers' are fickle beasts: they can get moved around at almost any time by the garbage collector, or they might belong to sparks that don't get evaluated after all, for example.
`common` is not necessarily shared I think, although on a performance level it makes sense to do so. What you would have to do is have your intersection function compare not values, but also memory locations of the investigated cells. But this leads to another problem: an optimization allowed due to referential transparency may change semantics. memoryAwareEquals xx@(x:xs) yy@(y:ys) | &amp;xx == &amp;yy = True -- "&amp;" = "address of" | x == y = memoryAwareEquals xs ys | otherwise = False a = [1..] b = [1..] memoryAwareEquals a a = True memoryAwareEquals a b = ??? -- Bottom or True? A non-optimizing compiler will allocate `a` and `b` separately, so the result is `Bottom`; however, seeing that both `a` and `b` are `enumFrom 1`, it could optimize both `a` and `b` to point to the same thunk, in which case the `memoryAwareEquals` would of course yield `True`. **Short version:** Values are referentially transparent, memory isn't.
In the actual problem its trivial to get node identity -- just zip each list with [1..]. For algos such as the one described in that post, we don't need to magically recover shared identity from existing lists -- we can just stick some unique identity on nodes and then work with that.
Great example! In general, the conversion to point free reveals a simple linearity that you missed when first piecing the function together. You aren't converting to point free to impress your friends, the point free conversion is the byproduct of shedding superfluous decoration.
I'm not sure this should even be mentioned in a beginner book. I like the "there's a function `IO a -&gt; a` and it shall be named when you're ready for it" policy.
I don't see why this isn't a perfectly valid solution: common as bs = common' (cycle $ zip [1..] as) (cycle $ zip [1..] bs) where common' is the algorithm described in the linked post.
I find the biggest readability problem in my and other's code is mixing point free and pointed styles. Only rarely is it easy to follow a semi-pointfree definition which declares one or two variables and leaves the others free. The counter examples, for me, tend to be when it's easy to think abut the first arguments indexing a set of pointfree-defined functions. Like replicateM n = sequence . replicate n
When will it stop if there are no common nodes?
Won't that just give each list the same sequence of identities? How do you use those to find the intersection?
Never mind, I misunderstood the question. He wants to detect identity after the identity information has already been lost (since if it weren't then presumably all you would have to do is just zip each element with its identity).
Hm, well. I haven't used C++ in ages so I'm not sure what to mention. What comes to mind first is that C# has garbage collection and its "generics" are roughly parametric polymorphism as in Haskell, rather than how template stuff works in C++. Most of OOP-y stuff is going to be close enough to C++ that you should be able to figure it out using the language reference on MSDN. Everything inherits from Object and reflection can poke around with type information stuff at runtime, so imagine if everything in Haskell had implicit instances of `Data` and `Typeable`. Oh, and most variables are actually implicit nullable references, that's kind of a big thing that might differ from what you're used to. So if you assign to fields on a mutable object passed to a method as an argument it changes the object itself, but if you assign to the argument variable itself you overwrite the reference to the object. Feel free to toss any specific questions you have at me. My day job uses C# so I can probably explain most stuff from a Haskell-ish perspective easily. The C++ part I can't help with though.
I didn't pay much attention to efficiency writing this non-working illustrative code I guess.
I love the dedication.
A good rule of thumb is that if you ever section composition (i.e. `(f .)`), then you have gone too far.
This is generally how one does observable sharing in haskell btw: http://www.cs.uu.nl/wiki/pub/Afp/CourseLiterature/Gill-09-TypeSafeReification.pdf but again I don't think it is necessary for this case.
Just wanted to say thanks for doing this for newbies. You reviewed one of my projects a while back and I learned a ton from it, glad to see you doing it again. :)
&gt; this is not possible with pure FRP, because the value stream t.TextValue is not known at that point. Doesn't it retain purity if the value is simply "Unbound" until something becomes available? ie. it's lazily computed, as we would to resolve other sorts of recursive bindings.
I don't think so: purity implies that we can substitute equals for equals, yet in this example we cannot.
In general, identity doesn't matter without dirty tricks when everything is immutable.
Really? I certainly find `(f .) . g` much more readable than either `\x -&gt; f . g x` or `\x y -&gt; f (g x y)` (or, god forbid, `((.).(.)) f g`).
I prefer `\x y -&gt; f (g x y)`
I could see where that would be useful, and I wouldn't imagine it would be difficult to do. It could probably even be performed by a simple preprocessor.
Yes. I also found it strange that that was missing. It’s not at all hard to implement. The order of the backticks and brackets should be irrelevant for properly parsing this.
In this case, the problem makes explicit reference to identity, so it does matter. In other words, given this: a = 1 : 2 : 3 : c b = 9 : 8 : c c = [ 4, 5 ] versus this: a = 1 : 2 : 3 : 4 : c b = 9 : 8 : 4 : c c = [ 5 ] The problem gives different answers for these inputs, even though the values are indistinguishable if you respect referential transparency. Ergo, it's impossible to solve this problem without a notion of identity. You can definitely introduce an artificial kind of identity *before* building the data structures, for example by building the data structures out of uniquely numbered objects. And if you trust that artificial representative, then the solution can be correct modulo that trust. Another solution is to go ahead and use mutable references explicitly in your own list type. Eq on IORef does the right thing here.
&gt; the problem makes explicit reference to identity Well, implicit reference. It says that the lists "merge," which *could* just be equality. The accompanying diagram and implementation strongly suggests identity. But for identity, it's just not a relevant question in Haskell. Why do you care that the list nodes are the same? Maybe you're doing manual memory management or mutating them. Then it's important. In Haskell, you ordinarily can't do those things. (And if you really need it, you're already in the realm of dirty tricks.) OTOH, wanting to know the first point at which all remaining elements are equal, that could be meaningful in any language.
Me to - I can see immediately what it does, whereas the rest (other than `\x -&gt; f . g x`) were puzzles to solve. That said, as my experience with Haskell improves, that could easily change. I've been programming for a long time (though not long with Haskell), and when you're programming for real you write for other experienced developers to read, not for the day-one newbies. You don't put the people who'll be working with your code all the time at a sustained disadvantage just to help out the day-one newbies who'll only be day-one newbies for a day.
Also [the school of haskell](https://www.fpcomplete.com/school/how-to-use-the-school-of-haskell) offers some neat in-browser execution.
But without identity all we can ever do is confirm that a segment is equal. The values could eventually differ. eg: common = [4,5,6] br1 = 1:2:3:common br1' = 1:2:3:common ++ [7] br2 = 9:8:7:common br2' = 9:8:7:common ++ [8] Finding the intersection of `br1` and `br2` will produce the non-empty result `common` but trying the same with `br1'` and `br2'` won't work.
If I understand correctly, `` `(`` is the opening operator and ``)` `` is the closing operator. Which should be unambiguous. It's a PITA to type in markdown though :P
FWIW, there is also [hugs](http://tryhaskell.org/hugs/) compiled to JS, too. But it requires Firefox (it's fastest in there) or Chrome, not because these browsers support more features, just because it's a lot of code and Opera/Safari can't handle it. Not exactly GHCi. It supports IO. &gt; do writeFile "hello.txt" "hej!"; readFile "hello.txt" &gt;&gt;= putStrLn → hej! But sometimes it might respond with “Never gonna give you up”, because Hugs sometimes runs quit() when it runs out of memory/needs to GC. Just run the command again.
Your second example (`f`) is actually not valid Haskell.
I'm not sure the benefits outweigh the cost of more baroque syntax. Am I right in thinking that the parens are only there so we can nest backticks inside backticks? * ``y `f x` z`` seems clearer than ``y `(f x)` z`` * ``(x `(`f` y)`)`` looks like an exercise in obfuscated code Otoh, what is wrong with let on2ndResult = (.).(.) in f `on2ndResult` g (or some more suitable name)? 
There's no character accepted from the input when that transition is followed. In implementation terms, that probably means that the failure was detected via a lookahead - whatever character was taken from the input was put back before following the failure transition. You could maybe construct a model where failure items are present in the input stream and accepted when a failure transition is followed, but that ignores the issue of how failure items are added to the input stream. 
http://ghc.io/ http://ideone.com/
&gt; But it requires Firefox (it's fastest in there) or Chrome Seems to work fine in Safari 6.0.3, FWIW.
inb4 Agda mixfix operators. This proposal corresponds to `_⟨_⟩_` from [Function](http://www.cse.chalmers.se/~nad/repos/lib/src/Function.agda). **addendum**: seeing as bash et al. went from backticks to `$(…)` to avoid nested backtick hell, shouldn't we do something similar, if we're going to go down this route at all?
My comment was addressing correctness, not performance. Whatever... 
You could just define infixr 8 ⟨ infixl 9 ⟩ (⟨) :: a -&gt; (a -&gt; c) -&gt; a -&gt; c a ⟨ f_b = f_b a (⟩) :: (a -&gt; b -&gt; c) -&gt; b -&gt; (a -&gt; c) f ⟩ b = \a -&gt; f a b I probably got the fixities wrong, but you get the idea. There is probably a package on hackage that does exactly this. Upon actually trying, it seems that Haskell doesn't allow these as operator names :(. You'll have to settle for something like `≪` and `≫`.
Suppose that we have FRP combinators, we have an input stream coming from the OS containing key/mouse/other events, and we have to define an output stream of bitmaps that will be displayed. So our main function looks like this: main input = ... compute reactive output bitmap ... Could we build a GUI framework on top of this? The answer is obviously yes, because we can just simulate an imperative program by writing a single reduce over the input stream that computes a new state based on the previous state and each input event. But could we build a compositional GUI framework based on this? The FRP examples I have seen are nice because you can do functional graphics, for example a circle following the mouse and a rectangle: bitmap = circle mouse.x mouse.y `over` rectangle 0 0 100 100 It would be nice to have the same kind of composition for GUI widgets. The trouble is that GUI widgets need to know their parents to receive events. If you have this: b = new Button(); Then what is b.ClickStream? It can't be anything yet, because which events it will get depends on where the button ultimately ends up in the widget tree. So when b gets put into the widget tree, then events will be sent to b.ClickStream. But this cannot be implemented on top of pure FRP, because in pure FRP an event stream has to be fully determined before it is used. So what you can do is give each widget constructor its parent in the widget tree as an argument: b = new Button(parent); But now you end up with recursive bindings everywhere, because the parent also needs to know about the button to be able to display it. And what happens if you pass in the wrong parent? How do you do dynamic widget trees? It becomes incredibly messy. Maybe you see a solution for this?
Why do you say that Haskell isn't friendly to REPL UIs? I'm quite happy with tab-completion on function names and so on - sure it could be better, but it could be lots worse too!
There is also https://github.com/shapr/ghclive which was a work in progress at the end of last summer that lets you set up a session on one machine and collaborate with it over the internet. It is probably lacking the security features you'd want for such a scenario though.
It is really cool that you answered that.
&gt; The solution relies on the fact that once you've identified that the lists merge, they must be the same from that point onward, and you need not even look at the rest of the list. Ahhh, yes. I concede, identity is definitely critical. &gt; No comment on whether it's useful. It was just a puzzle, so usefulness was never a requirement! I think it's worth identifying when a puzzle could never serve any useful purpose, especially if it's an extension of the puzzle.
Huh. I guess I just assumed that, since section syntax worked with backticks, so would conversion to prefix. I see that I was wrong
I've wanted this for things like `liftM (||)`. For example, if you lift things into the (r -&gt;) monad, you can do things like f `liftM (||)` g Right now you have to either write it prefix, or define a local new operator.
There are several versions of this floating around. You can implement it with a data structure and a function or with a pair of functions. IIRC, Chung-chieh Shan first proposed this around 2006 or so.
In the same section, it says `null xs` is better than `xs == []`. Why?
These don't work with the current version of GHC, or even with Haskell Platform.
I'm surely not nearly as qualified as you for any of this, and perhaps you go more in depth later in the book. To me laziness is one of the most exciting features of haskell. It allows you to program by constructing infinite lists, filtering those and then heading as many items as you need. Maybe I'm doing it wrong or misunderstanding something... Also awesome idea, and I can't wait to spend more time reading it over!
&gt; Also, Haskell requires parentheses around negative numbers, for some reason. Are you unaware of the reason? Either way, this gives that impression to the student, which is probably undesirable.
I actually don't know the reason :-) I don't mind revealing to my students that I don't know everything. But I maybe sometime I'll dig into this question.
&gt; If you don’t know about instance declarations in Haskell, please go and read about type classes. Seems like this is something they need to know about. I thought the book targeted people with some Haskell exp?
Yeah I understand both of those things, thanks for the explanation, and I look forward to reading more of your book!
You should Ctrl-F "negative" on this page -&gt; http://book.realworldhaskell.org/read/getting-started.html
The magic happens in the implementation of the `HasTrie` typeclass, which might be spread over multiple pieces of code depending on the type of the domain of the function you're trying to memoize. It looks like the idea is to build up a data structure which can be used to store the calculated values of the memoised function by looking at the type of the domain. For example, if I can memoise something of type `a -&gt; c` and something else of type `b -&gt; c` then I can certainly memoise something of type `Either a b -&gt; c` by storing a pair of memo structures. Similarly, if I want to memoize something of type `(a, b) -&gt; c`, then I can create a memo structure for `a` whose values are memo structures for `b` with values in `c`. `memo` is indeed pure, but you might also think of it as several pure functions, each of which memoizes a particular type of function. The typeclass machinery is what hooks up the actual implementation.
Negative numbers in Haskell are one of the few instances of magic in the language. The language doesn't support prefix unary operators, so negative numbers would normally be interpreted as subtraction from a positive number. To support negative numbers working in a reasonably intuitive way, the form `(-1)` is treated specially as a negative literal instead of being a partial application of subtraction. Without the parentheses the language must treat `-` as an operator the same way it treats any other operator. As a weird artifact of this, you can have spaces between the `-` and the number and it's still treated as a literal due to how Haskell deals with whitespace with operators. `(- 1) :: Num a =&gt; a`
But that doesn't explain how any particular thing is memoized. It can't be turtles all the way down.
Ok, I suppose I should have added: there are concrete implementations of memo tries for basic domains like `Int` and `()`.
But as I understood it that was the entire point of the question. At least it's the bit that I'm interested in. From what I can tell the implementation takes advantage of the memoization done by the compiler itself, similarly to Edward Kmett's excellent answer on Stackoverflow: http://stackoverflow.com/questions/3208258/memoization-in-haskell/3209189#3209189
Ah, yes, that makes sense. I know about the use of (+1) for increment and (+) for addition function. I suppose (-1) is a strange special case! It should be a function that subtracts its argument from 1. Very strange! It's not just magic, its a hack!
My audience (undergraduates) will almost certainly never have heard of *kinds* in the sense you are mentioning (values, types, kinds, etc). I'm using the word in the informal sense. I'm trying to avoid unnecessary jargon, and also allowing myself to use technical words with their ordinary meaning. For example, I just call "x" a "variable" rather than using the more obscure word "identifier".
Check out nixos.org and the papers listed at: http://nixos.org/nix/docs.html It takes a functional approach to package management and deployment. As an alternative you might want to check out http://docker.io/ That said I am not sure most of your concerns apply to Haskell. There are probably more receptive places with that consideration in mind. 
Just a follow up: it occurred to me that something similar to your original sample can achieve this in a fashion much simpler than my previous suggestion. index :: Monad m =&gt; Conduit i m (Int, i) index = evalStateT go 0 where go = do v &lt;- lift await case v of Nothing -&gt; return () Just x -&gt; do n &lt;- get put $! n + 1 lift $ yield (n, x) go Basically, this time, instead of building the conduit over the `StateT Int m` monad, we do it the other way around: `StateT Int` transformer over the conduit. `Conduit i m o` is a synonym for `ConduitM i o m ()`. So, `go` ends up as, go :: StateT Int (ConduitM i (Int, i) m) () The conduit operations (`yield` and `await`) need to be `lift`ed but not the state operations (`get` and `put`). &gt; sourceList ['A'..'Z'] $= index $$ consume [(0,'A'),(1,'B'),(2,'C'),(3,'D'),...] This is better than the previous code because you no longer need to `transPipe` other conduits to `StateT Int m`.
 -- without InfixExpressions ((.).(.)) f g -- with InfixExpressions f `((.).(.))` g Both these read terribly. It's an abuse of point free style, just use named arguments dammit! :)
The trie it uses under the hood is never inserted into. Instead, it is generated from the very beginning fully populated with every possible input! This is not crazy because it generates it lazily.
Or https://github.com/GaloisInc/HaLVM
Woah...
you should also checkout [elm](http://elm-lang.org/). very similar to haskell, compiles to js, html, &amp; css
If your students build non-trivial languages as exercises, they (and you) might benefit from [bnfc](http://bnfc.digitalgrammars.com/). From a grammar file, it generates a lexer/parser as well as datatypes and builders for ASTs. I'm a TA in a course at Chalmers Univ of Technology, Gothenburg, (where the tool comes from) where we use it to teach a course that is somewhere between yours and a compilers course. As labs, students implement a type checker and an interpreter for a simple c-ish language, and an interpreter for a minimal Haskell like functional language. The coursebook: http://www.digitalgrammars.com/ipl-book/
xs == [] requires the element type to have an Eq instance.
a^n /n = n items, with no information available to distinguish a "first" element bags (i.e. sets with duplicates) are [something like](http://strictlypositive.org/derivcont.pdf) a^n /n! which means there's no information about the item order either. Note that this is *e*^a ;)
That is not quite correct. You can e.g. write x = -1 just fine. Or even x = -y where y is not a literal. The special treatment is only to make negative numbers work intuitively when inside parentheses. Haskell treats unary - as having precedence like the corresponding binary operators + and -. This is what you would expect if you look at how polynomials etc. are written in mathematics. The reason this may be unintuitive to some is that in many programming languages (probably inherited from C) unary - binds much stronger, so that you can use it like 2 * -2. This is not possible in Haskell and you must then use parentheses around -2.
&gt; If I run "cabal install xmonad" then I don't end up with a source dir that I can modify and recompile. I still must download xmonad as a tarball if I want to hack it. If you run "cabal unpack xmonad" it will give you the source directory. Running "cabal install" from within that directory will compile your modified version.
Fascinatingly enough, `ghc` is actually great at optimizing `StateT`, so much so that I sometimes get *faster* code when I write things the naive way rather than hand-inline the state passing.
Interesting! One of my favorite courses as an undergrad was pretty much about this topic, in which we built interpreters, type checkers, garbage collectors, etc, and learned about languages in general. Although in our case, we used Racket and kinda sorta followed the [PLAI](http://cs.brown.edu/~sk/Publications/Books/ProgLangs/) book. Btw, if you know Robby Findler, he still teaches it. Languages didn't seem as arcane (though there's still vast swaths of magical territory) after that, haha
But if you are developing non-trivial browser-based applications then on the user side you are restricted to using either JavaScript or something that compiles to JavaScript, so in that area Haskell has already lost just like every other non-JavaScript language.
Have you looked at Yesod? I like that I can functionally output correct Javascript and HTML from within a Haskell driven web framework.
I haven't, but consider an application with a very rich graphical interface like this: http://www.stellarium.org/ Are you saying that all (or at least most) of the front-end JavaScript code that would be needed for a hypothetical browser-based version of such an application can be generated automatically by Yesod?
&gt; It is obviously deficient in many respects but there are many well-paid great minds hard at work solving those limitations, so it is only a matter of time. As an aside, could you tell me to what projects/groups you are referring but this remark? I ask because I have not heard of anything that makes me particularly enthusiastic about the a future where all applications run in a browser.
That's actually more of an algorithmic problem. What I do in that case is define locally backtracking parsers and then compile them to non-backtracking parsers that clear the saved state. You'll see much more of this pattern when I release `pipes-parse` very soon.
From an efficiency standpoint, I'm referring to attempts to improve Javascript's performance such as by Google (V8 and, to a lesser extent, Dart) or Firefox (asm.js). From a portability standpoint, Google has a vested interest in improving Javascript so it can deploy more applications that can bypass the control of walled gardens owned by competitors such as Apple and Microsoft.
Yes. However, I imagine that Javascript will slowly evolve to a more efficient subset that compiles to better code or be replaced by another language that will, and then the browser will just be a glorified virtual machine, distinguished solely by its large mindshare and install base.
I see your point, thanks.
Ah, now I see what you are getting at. That definitely makes sense, though I have trouble seeing us reaching such an end in the forseeable future since the various browser vendors show no sign of being willing to cooperate on moving away from JavaScript to something that is more like a virtual machine --- i.e., I respectfully think that you are being optimistic on this front but I would be delighted to be proven wrong. :-)
okay, but you can have local (dynamic) evidence
Woah... + 1
Have you heard of [asm.js](http://asmjs.org/spec/latest/)? It's pretty much exactly what you're describing. It's been getting lots of attention lately, since Firefox either has or will soon have an implementation.
Then again, [Hypercard](http://arstechnica.com/apple/2012/05/25-years-of-hypercard-the-missing-link-to-the-web/) was first released in 1987 and the browser still has to catch up on some aspects, mainly programmability by laymen and WYSWYG when it comes to designing layout. (The CSS `div` dance is just horrible.)
nixos looks very interesting. And it might well establish itself as a distribution, thus increasing diversity. BUT THAT DOESN'T help me distribute packages to debian!
Automatically? No. Functionally using Yesod's Julius at compile time? Yes (if I understand correctly). Here is a snippet of JavaScript using Julius: $(function(){ $("section.#{sectionClass}").hide(); $("#mybutton").click(function(){document.location = "@{SomeRouteR}";}); ^{addBling} }); There should be type safe interpolation at compile time. Here is a good link on the JavaScript problem: http://www.haskell.org/haskellwiki/The_JavaScript_Problem But, if you really like functional programming, and don't want to write JavaScript, take a look at WebSharper (which uses F#): http://www.websharper.com/samples/Formlet Here is a JavaScript snippet from WebSharper: [&lt;JavaScript&gt;] let AddressFormlet () : Formlet&lt;Address&gt; = let inputF label errMsg = Input "" |&gt; Validator.IsNotEmpty errMsg |&gt; Enhance.WithValidationIcon |&gt; Enhance.WithTextLabel label Formlet.Yield (fun st ct cnt -&gt; {Street = st; City = ct; Country = cnt}) &lt;*&gt; inputF "Street" "Empty street not allowed" &lt;*&gt; inputF "City" "Empty city not allowed" &lt;*&gt; inputF "Country" "Empty country not allowed" I like this a bit better, but I haven't played with F# at all.
Pixel exact web sites can be made, but they are unaccessible to people like me who like to use 2x or more magnification. I won't use a site on which I have to scroll horizontally to read. It is just too slow and tiresome! I know that most people are good at reading size 12 fonts, but when I read something with size 12 fonts my eyes move around and I get lost on the page. I know I'm in the minority here, I even understand that there is a possibility that I'm in such a minority that it would be worth it to make the web unaccessible to me. Here in Prague, for example, it isn't worth it to make buildings accessible for people in wheel chairs, so most buildings are not, and wheel chair people are confined to their homes. This is fine I guess, though, since there aren't that many people in wheel chairs, and most of them are too old or too sick to go out much anyways. This leads to the segregation of the handicapped. Luckily for me, today 99% of websites work well with text only zooming. I'm using reddit right now zoomed in text only. It works very well.
Variables and identifiers are *different* things. One is not a more obscure word for the other.
Yes, and I like the premise of it.
Yes, I do know Robby. Great guy! My book can be viewed as a version of PLAI that uses Haskell instead of Racket.
I am struggling with how to deal with parsing. Thanks for the pointer. I was hoping for something a little more lightweight.
&gt; However, in order to be useful, programs must interact with the outside world. Haskell solves this problem using monads to capture details of possibly side effecting computations — it provides monads for capturing State, I/O, exceptions, non-determinism, Pet peeve: the only one of these monads that captures interations with the outside world ("side effects") is `IO`
You might have side effects with large monads too. Say you have a State monad and call two functions that operate on the same State. You don't know what modified anything. Same goes for ST or every other monad holding some state. With state, you don't know what a function is goind to do with that state. There you have your side effects. It's not only about IO.
nix is a package manager, so you can install nix(the package manager) on debian and install your package/application.
Again, I don't want pixel-exactness either. It's just that HTML totally messed up the trade-off, creating more work for everyone while still not achieving the benefits of stretch-based layout. A lot of websites still specify pixel values because that's the easiest way to actually get a layout to display correctly. Ironically, the only thing that benefits from the markup are Reader-type plugins that strip away all of the layout.
I am pretty sure we could come up with a much better layout technology than HTML+CSS, and still retain the ability to build accessible web sites that degrade gracefully.
Okay, cool. I just skimmed paper very quickly but will take a closer look. :)
Obviously parsec comes to mind. So does uu-parsinglib which has some nice features for parsing languages and providing tips on how invalid syntax could be fixed.
Not necessarily. A state monad can easily be an appropriate "subset" of IO. The interface for accessing state that's mutable both inside your program and externally is identical. Similarly, non-determinism might be affected by the outside world. The result is what we call concurrency. In that case, the program... do student &lt;- students scores &lt;- testScoresFor student return (student, sort scores) Could give you the results in an arbitrary order determined by the OS scheduler and other forbidden details. It's not the pure List monad (and there's probably a `runNondet : List a -&gt; IO (List a)` to run it) -- but it's still a non-deterministic monad. Random which pulls from, say, an *actually* random number generator device on the system is another example where you can have a side-effectful thing that isn't just IO.
I do this. DigitalOcean has $5/mo vps's that will work just fine for this purpose.
It's not clear to me that I could model `Pipe` capabilities using this system, specifically the ability to be composed. I feel like this approach is narrowly specialized to replacing just a few monad transformers and not solving the problem in general.
It doesn't try to solve the problem in general, and explicitly says so - it is already known that some effects can't be modelled algebraically, like continuations. Algebraic effects don't replace monad transformers, but rather can work with them.
&gt; If Haskell doesn't hitch itself to the browser then the language has no future. I agree in spirit (the browser is the UI platform), but there's plenty of interesting things to do in the backend (where people now use C++, Java, Python, and Ruby).
That still doesn't help me make it easy for normal users to install Haskell software :( 
I don't know for some forms of software it is the easiest method. It was the easiest method to get gtk working on my mac with Haskell as far as I can tell. Either you: * Depend on external programs, in which case you have to modify your deployment for every distribution you release for. * Have everything be internal, written in haskell and have to rewrite considerable functionality. HalVM has to recreate some functionality to work on top of Xen hypervisor and does not currently have a method of calling external C libraries, with out the pain of making it able to run on top of Xen itself as well. * Have a package management system encapsulate all of your dependances so they are easy to distribute, which is what Nix does for you. Cabal may be that one day but it is not now. If you search the web you can find the mantra "Cabal is not a package manager." So it does not seem to be intended to solve the solution of package management as a variation of Greenspuns law. Most people end up doing a combination of point one and two as far as I can tell, which for each package is not a mountain of work but it does need to be kept in mind each time.
Als you could just package nix for debian which would make it work, though you would still have to deal with the overhead nix would cause.
As a non-expert, I am still trying to understand the lay of the land when it comes to effect systems. I would like to have a more detailed explanation of your systems theoretical foundation. Are you basically working with Lawvere theories? Is the system more, less, or equivalently expressive to the [12]? (or are they actually comparable?) Admittedly my ignorance is large enough that will not give me very much, but it helps to start to connect the dots. Maybe someone can clear up some confusion I have over terminology. What is the difference between an "effect system" and a "theory." Are "algebraic effect systems" related to "varieties" in universal algebra, or is there an equivalent universal algebra definition?
That's the situation right now, but it's only a matter of time before people demand both portability and off-line functionality.
&gt; state monad can easily be an appropriate "subset" of IO. &gt; Similarly, non-determinism might be affected by the outside world. Sure, they *could* be, but they're not. Only `IO` itself (and `ST`) actually capture outside world interactions.
The State monad is just a newtype over pure functions. It *could* be implemented using mutation, but it's not.
Not at all true. There's plenty of things in Haskell that have measurable real-world side effects in pure code :)
Doesn't BNFC generate inputs to other parser generators, which also have restrictions on the language that they can parse (e.g. LL(1))? This wasn't at all clear from the documentation. I would love to have a GLL/GLR parser, but I haven't found one. I'm investigating Parsec, and it seems to be working well for me.
I mean the thing that we normally call "variables" in algebra and in most programming languages, including Haskell. The word "identifier" is a narrow technical word that should be confined to discussions of parsing.