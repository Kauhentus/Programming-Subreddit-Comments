&gt; how long the superexponential time actually is. Except on days when I'm an ultrafinitist, infinitely shorter than forever. Thanks.
I... don't think I agree. Once I would have, but I think I've changed my mind. I have never written Haskell code where the correctness of a component depends on whether exceptions reset state or preserve it; most of the time, the ordering just affects whether I get the state back after the overall computation errors. I'm sure that there is a short snippet of code which uses exception handling non-trivially to get different answers depending on the transformer stack ordering (if you have such an example, I'd like to see it), but I don't think it's a problem in practice, especially if the `MonadError` class is used only for `throw`. Extensible effects have this problem and don't seem to view it as a big deal, and the advantages of leaving the effect stack abstract are rather large. I recall that /u/edwardkmett endorses the type class approach; I wonder what he has to say about non-commuting type classes.
&gt; 2) With explicit values for exceptions, you will need to plumb them around. Machinery like `EitherT` helps, but it can be inconvenient to use unless it's already baked into your monad. With normal exceptions, no plumbing is necessary. Not exactly intended as a criticism, but this is directly antithetical to the essence of Haskell; just replace "exceptions" with "IO" or "state" and you get justifications for OCaml. Personally, I think that the error model is described [here](http://joeduffyblog.com/2016/02/07/the-error-model/) (long read; skip to the conclusion): * Sometimes, return codes are used (`Either` and `Maybe` are just improved versions thereof) * Other times, for recoverable errors, _checked_ exceptions are used * Unchecked exceptions are uncatchable and terminate the process (so are referred to as "panic" rather than "exception" to avoid the implication that catching is possible) * To avoid checked exceptions and return codes but still handle a swath of failure cases, the function can take a handler object as a parameter (possibly in a `Reader` monad)
Are exceptions required in Haskell to do certain things? I'm coming from Rust which does not have exceptions and seems to get along just fine without them.
I wouldn't know, I didn't do anything special. 
Shouldn't the tutorial use Stack instead of the Haskell Platform in the Setup section?
&gt; Yeah, ok, call me when you can keep a lambda term running longer than every star in the sky. Okay, I guess. By the precondition, this call cannot happen until every star in the sky is out, so I have to wonder what I'm calling you on.
My point exactly.
The Haskell Platform includes Stack now
Briefly: I guess he refers to `String`, `Text` and `ByteString` (don't know the other two). `String` is a linked list of `Char`s, so access is O(n), plus in general lists are better suited to represent infinite streams rather than finite chunks of data. Some parsers e.g. Attoparsec work with `ByteString`, whereas it would be best to process text with `Text` (constant time access and a number of other perks)
&gt; (don't know the other two) Lazy `Text` and lazy `ByteString`.
There's an Emacs stylish package you can install that just works. I bound C-c C-s to run stylish and then save the buffer and I've been very happy with it. EDIT: Actually, it might just be part of haskell-mode. Try using the function `haskell-mode-stylish-buffer`.
There's a [new compiler](https://github.com/evincarofautumn/kitten/pull/152) being worked on and also the revamping of the website. I'm fairly excited about that :) Some may be interested in the new permissions system discussion for kitten discussed [here](https://www.reddit.com/r/programming/comments/4mzlvi/10_lesserknown_programming_languages_worth/d41x2w8?context=3).
Hmm... it looks like it was trying to link against a gcc installed by brew? Where did your cabal come from and did it involve brew being on your system? (Note: there's no reason not to just grab the latest cabal executable from https://www.haskell.org/cabal/download.html which I'm pretty sure will _not_ have this issue)... Alternately, if this was an upgrade to El Capitan, then it could be due to SIP now disabling overrides to library paths being propagated downwards (LD_LIBRARY_PATH, DYLD_LIBRARY_PATH, and DYLD_FALLBACK_LIBRARY_PATH in particular I think). That particular message coming from the cabal executable itself makes that seem slightly unlikely however?
And, another: https://github.com/dbp/hworker/blob/master/test/Spec.hs
No documentation?
This seems like a minor convenience. Sorry, we can't spend the time to address every one of those.
Docker or nix. I am surprised there is a case where ghc breaks on mac os update. If such a circumstance does happen, then we should have it detect this case and tell you to re-setup your ghc. For now, to do that you just delete stuff out of `~/.stack/programs/.../` as appropriate (be sure to get the .installed files!) OP, please open a ticket with specifics if this affects stack as well. We do a sanity check after ghc is setup. Perhaps this sanity check could be re-used for detecting when your setup is borked and recommend that you re-setup.
Yes, I'm aware of that trick, it's pretty clever! Thing is, as far as I know there isn't a library for it, and as far as I've seen people don't do that. I haven't played with this approach much, it would be interesting to see how far it would go in practice. Part of the reason I haven't tried is that it seems like a fancy solution for safety which only covers the simple cases. It'd be worrying to me to adopt a solution that allows you to make mistakes while having the semblance of safety. Why a semblance of safety? I think it's why people don't actually use this. It has a crucial flaw - the exception throwing checking is done lexically rather than paying attention to your actual program flow. "Throws" constraints just bubble up to the top regardless of whether or not the callsite is involved in execution of the function. This makes it really really easy to make the following errors: 1) Declaring that your function throws an exception when it never will. The constraint just goes to your top level function, it is not an analysis of whether it's actually thrown from a direct invocation of the function. It is particularly easy to make this mistake when just copy-pasting the type inferred by GHC. This is particularly the case with heavily polymorphic code - easy to not consider the Throws constraint if its buried amongst others. 2) When you already call some function that has "Throws FileNotFound", it will also satisfy the constraint to a call to some other function. This can make it easy to miss that you didn't handle the case as described in (1). Since the constraint is easily satisfied for things that it should not be, it's easy to miss usages of "Wrap"-like newtypes in order to convey the information of exception-throwey-ness. This newtype wrapping is generally ugly and inconvenient. The article attempts to downplay this flaw &gt; Instead the type of returnAction will be &gt; &gt; returnAction :: Throws HttpException =&gt; IO (IO ByteString) &gt; returnAction = return (simpleHttp "http://www.example.com") &gt; &gt; which has the Throws annotation on returnAction itself; this means we can make the annotation disappear by adding an exception handler to returnAction even though it’s never called (because returnAction itself never throws any exception). &gt; &gt; This is somewhat unfortunate, but it occurs only infrequently and it’s not a huge problem in practice. If you do need to return actions that may throw exceptions, you can use a newtype wrapper such as Wrap that we used internally in rethrowUnchecked (for much the same reason): I think it'd occur frequently, and so would be a problem in practice. Also, in other languages, checked exceptions have [not been found to be a panacea](http://stackoverflow.com/questions/613954/the-case-against-checked-exceptions). Many of the criticisms there are specific to Java failings, but many also apply to "Throws". There's another reason to not do this, which is that it relies on some pretty intense magic for `catchChecked`. Perhaps overly pedantic, but I'm not sure if the interaction between existential constraints and `coerce` is defined. Perhaps we can define this to be valid for classes with no methods, but yikes!
If I need a known order for effects to commute I make a class that includes the ordering in its semantics and lift it as needed. If you go off and work with "effect handlers" you have all the same ordering issues. As a tangent, one real downside to the current `(MonadState s m, MonadReader e m, HasFoo s, HasBar e) =&gt; ...` approach is that if that function is recursive, it pretty much can't inline or usefully specialize. More recently I've been playing around with viewing the category of monad transformers with monad transformer homomorphisms between them as a monoidal category and playing with lenses into the category of monad transformers. This lets me talk about ways to pick out 'part' of the monad transformer stack in a way that accomodates orderings. I've yet to get this nicely packaged up, and viewing things this way doesn't play nicely with ContT (like many effect systems), but it does let me code to a concrete monad transformer stack and define the 'lens' that lets me swizzle out all the parts of the larger monad that I need in order to run the smaller concrete monadic computation I want. Since it is a concrete computation written against a concrete monad, the compiler can actually optimize it even when it is recursive.
I don't know that there's an easy solution to this problem. Regardless of how it's done, you'll always have to manually specify the rules for when cache needs invalidating. Unless there's some way to abstract which events cause invalidation. Can you maybe specify what those are a little bit for us?
[removed]
Nice! Would be very cool to replace the python script with TemplateHaskell so it automatically runs on compile :-)
Text has no constant time access. It's a common mistake to treat UTF-16 as if every codepoint can be represented with two bytes. attoparsec takes both ByteString and Text. There's also nothing that makes Text the naturally best choice for it.
When I first heard about BDD it seemed like QuickCheck style property-based testing. Do you think that's accurate or are they more different than that?
GHC does not come with cabal / stack, making it fairly useless on its own. The Haskell Platform now is basically a minimal installer, not coming with a set of libraries by default.
One approach could be to define an inductive typeclass over the datatype you intend to store in redis giving a "hash" of its structure. You could write this "fingerprint" library using generics. Then, you could write the hash into a local file or the like (or even into redis itself) and dynamically at app startup check if the hash matched the current hash, and if not, invalidate and overwrite the hash with the new one.
Closely reading this I formed a strong impression: Haskell in production is ready to shine and equipped to solve a large swathe of problem domains. I'd love to see this material form part of a modern real-world haskell.
&gt; define an inductive typeclass over the datatype What does that mean?
I'm not familiar with TemplateHaskell unfortunately - from what I've seen it looks quite gnarly. I'll happily take on a patch though :)
whats a docker, and why would i want to use it
Nice! That matched my expectations, from talking with other folks who teach people Haskell - but it's always good to have more anecdata :) 
There's some stuff that is a lot less transparent without asynchronous exceptions. But they are not strictly necessary. Anything you can do with (synchonous) exceptions you can do with some combination of Either and/or Cont. So, in theory, no. In practice, both the standard library and many popular libraries use exceptions (*generally* in IO) for quite a few failure cases.
Often you'll have to mash together two APIs that use different string types, which might be inevitable but just feels icky.
Edited my post to include more context. 
To Rails, having a wrapped IO monad is pretty close to magic. ;)
I can't think of anything that does what you want out of the box, but some of the papers on Lvars and might shed some light on how you could use a expression language to capture the dependency tree of your data.
That links to an Apollonian gasket demo, which is different than circle packing. BTW, Brent Yorgey has a nice [series of posts](https://mathlesstraveled.com/tag/apollonian/) on Apollonian gaskets.
Well-known meme: &gt; There are only two hard things in Computer Science: cache invalidation, naming things, and off-by-one errors. 
Something like toJSON or Show or Hashable typeclasses -- where instances for types are defined "inductively" in terms of the things they contain. So e.g. instance (Fingerprint a, Fingerprint b) =&gt; Fingerprint (a,b) where fingerprint (a,b) = hashPair (fingerprint a) (fingerprint b) or the like. (I'd actually write this using `Proxy` rather than actual datatypes, because the function depends on the types involved and not the values of individual inhabitants, etc. But the above code is simpler for illustrative purposes).
Is it just me or is this a pretty dismissive answer? I wouldn't file quality-of-life bugs against *any* product if that was the response to be expected.
Consider meditation. By all accounts it's a wonderful practice, but people who have tried to get me into it were so dogmatic and downright pushy that I refuse to try it on principle. The very word leaves a bad taste in my mouth. Maybe some people feel like that about Haskell, the same way some people feel about vegetarianism and polyamory. Anything being sold as enlightenment will face visceral pushback from those who feel it is an attack on their way of life.
Hakyll has a Compiler monad that handles automatic dependency tracking. Might be interesting?
It is somewhat annoying - I find myself continually having to (re-)search for [this page](https://wiki.haskell.org/Base_package), due to having a sieve-like memory. For example it would be nice to just state which GHC versions a library supports directly in the `.cabal` file. 
I've just had an ingenious idea. How about a package called `ghc-dependency` which has versions that correspond to GHC version numbers and depends precisely on that GHC's base package?!
I think your (and /u/rpglover64's) points make a lot of sense. Do you have any examples for making your own class to encapsulate the ordering? I thought typeclass-constraints are (usually?) resolved at compile-time, are you talking about explicit inlining and specializing through pragmas or automatic optimizations? Lensy transformer stacks sound interesting, I'd like to read more about that!
Lovely.
took your haskell-do-info and altered it slightly, here's my current haskell-related forms from my init file: https://gist.github.com/unhammer/edd926fff70af71d1c77ed73461d55a9 (I don't use spacemacs, but I do use `use-package` and `evil-mode`.)
&gt; Not exactly intended as a criticism, but this is directly antithetical to the essence of Haskell /u/rpglover64 knows the truth.
Short answer: There's no compelling reason for `base` version to match with GHC's version. For one, `base` follows the PVP, so it could be that we don't need a major version bump, but GHC's version would force us to do one. And it would be rather limiting for another reason as well, as we couldn't have done what was done for GHC 6.10.1, when two versions of `base` were bundled: `base-4.0.0.0` and `base-3.0.3.0`.
I don't think that's a good idea for what we're trying to evolve GHC's boot libraries into... specifically, the current correlation between `base` version (and other boot libraries) and GHC version will become weaker if all goes according to plans (and this also means, that in future we won't be able to use `base`'s version as a proxy in `.cabal` version contraints for GHC's version, nor `__GLASGOW_HASKELL__` as a proxy for `MIN_VERSION_base(...)` anymore)
Well yes, a general solution is AI-hard. However, writing a "how I converted my Python tooling to Haskell and you can too" blog has a known general approach with signature blogThis :: Notebook Scribbles -&gt; EnglishDictionary -&gt; Snippets Code -&gt; IO BlogPost (it has to be in IO because the side effect of sitting down to write it is a RealWorld thing).
That's another question, do `GHC.*` or `Foreign.*` belong to base?
And now we have two problems …
According to this plan, how does one then create a build dependency on specific versions of GHC? Will there be some magical pseudo-package for that?
&gt; [this page](https://wiki.haskell.org/Base_package) [which in turn links to [this more comprehensive page](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Libraries/VersionHistory)] Wow, thanks for that link! I didn't even know those pages existed. That is extremely useful.
Not only. They can be various kinds of tests, such as QC properties or traditional unit tests. The point is that the tests are specified using a human-readable DSL which specifies required behaviors of the function.
oh, thanks!
Ah, so an interactive demo wouldn't make sense because the radii are fixed.. TIL :-)
I know, and I understand; it is, however, an example of a workflow not supported, and a symptom of the fact that it's not straightforward to add your own templates. This may be a documentation issue, but I'm not sure. I'm also aware that the pull request will take all of 2 minutes to write, but it's not clear to me that it would, or even should, be accepted.
Nope I just tried random permutations of local/remote, relative/absolute path, with/without extension, etc :-)
This was a great read!
OK, very cool, that covers language features. How about GHC bugs? Of course the best approach is to make GHC totally bug free. But, just theoretically, let's imagine that we do not achieve that immediately. How do I avoid a GHC version which I know has a bug that my package tickles?
I'd think any instance that can be generically derived. If you use a proxy rather than an actual value, you can descend both sides of a sum type too.
Wow.. very exciting. I'm actually glad you didn't pick my project, as these are all quite interesting, and I would be more happy to have them available to me. Bravo
Case in point.
I totally understand where you're coming from.
 I've needed `parseExp` and `substituteSynonyms`, but not `checkConstraint` or `unifyTypes`. Can you give some use cases? hmm, `checkConstraint` might let you conditionally declare instances. 
You can do `--help` to find out more about the various commands. The `TEMPLATE_NAME` section is the relevant one here. &gt; stack new --help Usage: stack new PACKAGE_NAME [--bare] [TEMPLATE_NAME] [-p|--param KEY:VALUE] [DIRS] [--solver] [--omit-packages] [--force] [--ignore-subdirs] [--help] Create a new project from a template. Run `stack templates' to see available templates. Available options: PACKAGE_NAME A valid package name. --bare Do not create a subdirectory for the project TEMPLATE_NAME Name of a template or a local template in a file or a URL. For example: foo or foo.hsfiles or ~/foo or https://example.com/foo.hsfiles -p,--param KEY:VALUE Parameter for the template in the format key:value DIRS Directories to include, default is current directory. --solver Use a dependency solver to determine extra dependencies --omit-packages Exclude conflicting or incompatible user packages --force Force overwriting an existing stack.yaml --ignore-subdirs Do not search for .cabal files in sub directories --help Show this help text Run 'stack --help' for global options that apply to all subcommands. I always have a hard time with the stack docs myself, but leave notes for my future self [in blogpost form](http://www.christopherbiscardi.com/reproducible-tutorials-with-stack/) often. 
Official Image is PR'd at https://github.com/docker-library/official-images/pull/1848
I agree and am generally all for documentation. In this case, and frequently with docker, the Dockerfile is concise up-to-date and exact.
&gt; "every package with 'ghc' its name" http://hackage.haskell.org/packages/ + CTRL-F 
I know. But, that page loads lazily, and has duplicates.
Yes, the primary usecase I have in mind for `checkConstraint` is for omitting instances if they already exist. It could also be used to omit trivially satisfiable constraints, reducing output code, though that's not so important. As for, `unifyTypes`, for your entertainment and horror, here's the TH [unifyTypes](https://github.com/ekmett/lens/blob/bdb7e9142e83ff92a3f914a7ffc7ea4f5582a6d9/src/Control/Lens/Internal/FieldTH.hs#L493) function in lens.
The author of the post is a longtime troll. They produce good work at times in their language of choice at any given moment, but will turn on it and scream bloody murder about how useless that language was the next moment. Their claims about "idomatic" haskell are usually bogus, and their claims about what they have "found" are based on deliberately unfair misrepresentations. That said, the claim that there are no purely functional versions of certain data structures and algorithms that have the same complexity as mutable versions is true. Although its better to say that there are certain data structures / algos for which _immutable_ versions don't keep the same performance profile. But that doesn't mean haskell much less FP is fundamentally flawed for those algos and structures -- it just means you use the mutable versions of them. And FP in general and haskell in particular have no problem with mutation -- in haskell you just get to be more granular and controlled with it through explicit management of effects. Here is one particular wrong claim that jumped out at me: "There are no purely functional concurrent collections" because "By definition, immutable collections cannot support concurrent mutation". This is terrible logic-chopping. The genuine question is "are there purely functional data structures suited for concurrent use" which of course necessarily means in the context of living in some mutable reference shared between threads or the like. Depending on the scenario, there are _lots_ of structures, both common and exotic, which are suitable for various purposes in this regard.
I don't think the Win32 bindings impose any additional thread constraints, so just consult the regular Win32 docs to see which APIs are thread safe, which use TLS, can only be called from the main/GUI thread etc.
&gt; Should IOExceptions generated by underlying libraries be captured in the HttpException type, or kept as raw IOException values? In this context what does **generated by underlying libraries** actually mean? Would it encompass an `IOException` thrown by a socket disappearing, for example?
As a user of the HTTP libraries I'll anyway need to deal with the fact that each request might throw some kind of exception. It doesn't really strike me as so strange to also throw an exception for non-200 status codes. At least now there's just one mechanism to report errors? i.e. look at this basic function that makes a REST API call: https://github.com/blitzcode/hue-dashboard/blob/master/HueREST.hs#L63 Currently I need to handle `HttpException`, `JSONException` and an error reply from the API endpoint. If we return the status code another way, I guess I'd need to add yet another error check for this type of error.
Does GHC do this, for any situations or algorithms? 
Yes, that's exactly what I mean. Within http-client, exceptions like "invalid HTTP response format" are already turned into an `HttpException`. The question is whether we should capture a failed socket action and return it as an `HttpException`, or leave them as the original `IOException`s.
That was my original logic in the design. On the flip side, in many cases people need the full response body and/or headers to deal with a non-200 response, in which case they need to use `checkStatus` to disable this exception behavior and check the status code manually themselves, so it's not really helpful. You could still argue that failing loudly as we do now makes people mindful of the possibility of a non-200 status code.
Ok, I see. It's basically whether you consider a non-200 response a fatal error like a failed connection that should just abort the entire current sequence of operations or simply a change in your control logic, like trying to obtain authorization after a 401 response.
Now when [this PR](https://github.com/snoyberg/http-client/pull/179) is merged, one can enable his preferred behavior on per-manager basis. Does it means that everybody is satisfied? IMO it is a question of backward compatibility now -- I'm OK with any default if it is easy to change it, but breaking existing code is the worst way to fix things. Also, people argue that http client library should not handle any status code specially because &gt; "everything is "success" - it's up to me how to interpret the response body/headers But then http client should not handle redirects too, which would be silly. So the only argument for non throwing exception on non-2xx status I see is that most of libraries in other language doesn't do that. But it is pretty weak argument -- you are expected to read documentation before using any library, and the behavior is clearly stated in haddock.
lol I tried to give $50 but stripe was in "test mode". I'm in Greece, if relevant. edit: upper-right has `TEST MODE` after clicking `Donate`. Confirming the donation pops-up like "non-test card used in test mode". But then below `Donate`, it says: Fri, 17 Jun 2016 10:09:56 GMT: Your donation was successful -- thank you! 
The W3C fetch API returns a promise which resolves with the Response when the status code is success, otherwise the promise is rejected with.. a TypeError. This would correspond to what http-client is doing now. I personally like the approach that Servant has taken, where it automatically handles standard responses, but gives the user full access to the response in the form of an `Either`.
I'm somewhat in favour of a `MonadError HttpException m`, as that both forces me to handle the failure, but also allows the exception to propagate while keeping some book-keeping. I can get an `Either` out by just immediately running with `runExceptT`, which is what I'd do in the majority of the time. This is like using `try`, except I can't forgot to do it (unless of course I'm already working in a monad that has that specific constraint). Ultimately I don't think that failing to get a 2xx status code is an error though, nor do I think that a 5xx response is an error - at least not in the context of `http-client`. As far as that library is concerned, it's doing its job - I'm not sure it should be mixing concerns by making decisions based on the contents of responses. All `http-client`s job is is to understand the HTTP protocol in a type-safe Haskell-y way. The only errors that would come from `http-client` are genuine errors to speak the protocol - decoding errors, sockets being abruptly closed, etc.
upvote for not only opening up a trac ticket, but also linking to it.
Thanks for the even simpler example! I've updated the Trac ticket with this info.
The idea is that many (most?) IO operations like opening files seem to throw an IOException when they fail. The HTTP library already has an exception wrapping a wide range of these error conditions: https://hackage.haskell.org/package/http-client-0.4.28/docs/Network-HTTP-Client.html#t:HttpException Now some people argue that an non-OK HTTP response code is not the same as say a failed network connection, and simply another valid response. The argument against this is that there's already an error reporting mechanism (the exception) and returning an HTTP error code would create two places where errors are reported.
Thanks! Is that Agda? &gt; The internal logic of this HashTable (how it relates to itself via constructors) is in fact linear -- for any individual h : HashTable you have, I can construct it efficiently with mutable operations. Can you explain? Are you saying the compiler automatically/statically generates a mutable representation/initialization? Seems hard. i.e. lookup "a" (Delete (Insert (Insert Empty "b" "2") "a" "1") "b") becomes what?
[removed]
We're currently having a similar discussion for the venerable HTTP library: https://github.com/haskell/HTTP/pull/102 I'd be quite happy to align HTTP with whatever http-client decides if that seems sane (but I think I'd always want non-200 responses that come from an actual server to be returned directly).
[removed]
Catching `SomeException` is discouraged and almost never the correct solution (https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Exception.html#g:4). The issue is you usually want to catch all errors resulting from your actual HTTP requests, but you do not want to catch other unrelated exception. No point in notifying the user and/or scheduling a retry if CTRL+C is the culprit of the exception.
Here's my attempt. {-# LANGUAGE FlexibleInstances, FunctionalDependencies #-} import Prelude hiding (Fractional(..)) import qualified Prelude -- | -- &gt;&gt;&gt; :set -XFlexibleContexts -- -- &gt;&gt;&gt; let six = 6.0 :: Double -- &gt;&gt;&gt; let two = 2.0 :: Double -- &gt;&gt;&gt; let zero = 0.0 :: Double -- -- &gt;&gt;&gt; six / Just zero -- Nothing -- &gt;&gt;&gt; Just six / zero -- Nothing -- &gt;&gt;&gt; Just six / Just zero -- Nothing -- &gt;&gt;&gt; -- &gt;&gt;&gt; six / Just two -- Just 3.0 -- &gt;&gt;&gt; Just six / two -- Just 3.0 -- &gt;&gt;&gt; Just six / Just two -- Just 3.0 class Fractional a b c | a b -&gt; c where (/) :: a -&gt; b -&gt; c instance Fractional Double Double (Maybe Double) where _ / 0 = Nothing x / y = Just (x Prelude./ y) instance Fractional (Maybe Double) Double (Maybe Double) where mx / y = do x &lt;- mx x / y instance Fractional Double (Maybe Double) (Maybe Double) where x / my = do y &lt;- my x / y instance Fractional (Maybe Double) (Maybe Double) (Maybe Double) where mx / my = do x &lt;- mx y &lt;- my x / y I used Fractional instead of Num, because that's where `(/)` lives. I hid the `Fractional` and `(/)` definitions from the Prelude and reimplement my own in terms of the Prelude's. I also had to give explicit types to my literals, because if the left operand is an Int and the right operand is 42, the compiler doesn't know whether to parse the literal using the `Num Int` instance or whether to fruitlessly search for a `Num (Maybe Int)` instance. Finally, I had to specialize everything to `Double` and I'd probably have to duplicate the entire setup for every concrete numeric type I'm interested in. That's because if I try to write generic instances, then the `a a (Maybe a)` instance overlaps with everything else. Unless I turn off the functional dependency, in which case I'll need even more type annotations. In other words, there's still a lot of room for improvement :)
I'm not sure how you'd have the same operator for these different types: (Num a) =&gt; Maybe a -&gt; Maybe a -&gt; Maybe a (Num a) =&gt; Maybe a -&gt; a -&gt; Maybe a (Num a) =&gt; a -&gt; Maybe a -&gt; Maybe a Seems like a lot of trouble. On the other hand, a newtype wrapper: {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Control.Applicative newtype ApplicativeNum f a = ApplicativeNum { getAppNum :: f a } deriving (Show, Functor, Applicative ,Alternative, Monad, Eq, Ord) instance (Num a, Applicative f) =&gt; Num (ApplicativeNum f a) where (+) = liftA2 (+) (-) = liftA2 (-) (*) = liftA2 (*) abs = fmap abs signum = fmap signum fromInteger = pure . fromInteger instance (Fractional a, Alternative f, Monad f, Eq a) =&gt; Fractional (ApplicativeNum f a) where fromRational = pure . fromRational x / y = (y &gt;&gt;= div) &lt;*&gt; x where div 0 = empty div y = pure (/y) instance (Floating a, Alternative f, Monad f, Eq a) =&gt; Floating (ApplicativeNum f a) where pi = pure pi exp = fmap exp log = fmap log sin = fmap sin cos = fmap cos asin = fmap asin acos = fmap acos atan = fmap atan sinh = fmap sinh cosh = fmap cosh asinh = fmap asinh acosh = fmap acosh atanh = fmap atanh type Safe a = ApplicativeNum Maybe a Might give you what you're looking for: empty == (1 / 0 :: Safe Double) pure 2 == (20 / 10 :: Safe Double) pure 6 == (2 * 3 * 1 :: Safe Int)
For this purpose, I'm unconvinced of the utility of having a bytecode and then trying to enforce all the invariants in the higher language. I think they're going to need to be built in a more integrated manner. This is a practical objection, not a theoretical one. Theoretically invariants can be written into the higher level, but if the lower-level bytecode wasn't written carefully with the target invariants in mind then it will likely be the case that the invariants that can be offered by the higher language will be compromised in practice. For instance, the hard stop on "gas" provided for a computation may manifest at the higher levels of the guarantees in _really_ unpleasant ways and translate to surprisingly weak guarantees being available for the higher-level language to promise at all, to say nothing of the endless other details imposed by the already-existing bytecode. It's easy to run out of "gas" in the higher level language if, in order to maintain the target constraints, it has to wrap every operation in some sort of defensive code to deal with the possibility of running out of "gas", or any of the other endless possible flaws in the bytecode, basically multiplying the "gas" usage by a non-trivial constant factor out of the gate. (Or potentially _worse_ than a constant factor.)
Ideally their blockchain VM should refuse to run code if it doesn't provide a machine-checked formal specification of it's behaviour. I'm not sure this would require anything as fancy as a dependently typed language, as you could just have a simple imperative language with a formal semantics and prove whatever specification you want w.r.t. those semantics in more conventional HOL-style theorem prover. The comment in the linked thread that formal verification "just proves the equivalence of a formal specification and a program." then isn't an really issue because the specifications would be public and I can choose whether on not to use a financial service based on how much I trust its specification. It's still research level stuff but not too far out of the realms of possibility. Seems like an interesting project at any rate!
It's hard to change existing blockchain tech, and I fear that there's too much complexity in such a proof system, which makes me think that it's more likely that we'll have some off-chain verification that potential users can look at... But there's something called TauChain that integrates a blockchain with some kind of type theory stuff (I just heard about it today, don't know if it's vaporware).
As I said on my HN chain too, it's not hard to imagine something like this being very big in 50 years. I'm not down on the entire idea. It just may be a case where it's 5-10 years too early, and it may also be something that needs to come out of the gate pretty much correct at this level, just as it needs to come out of the gate cryptographically correct. I have to admit to some skepticism over Ethereum at this point, but I hope its likely-eventual-failure doesn't ruin the entire idea forever the way such things sometimes can.
This is one of the reasons I like the rust language. No exceptions. It merely returns a result type from any function which can fail, like nearly all IO functions, and those errors can be mapped and merged together in the case of writing a series of statements by using a macro (try!). You can still unsafely unwrap their eithers or maybes, but then it is obvious looking at the code that you've papered over an error, unlike with exceptions which hide them all for no good reason. It turns out that having an escape hatch is very useful for that initial bit of writing your code, when you don't want to deal with hard errors. I desperately wish such a thing could happen in haskell.
If I had to do this, (/?) :: Fractional a =&gt; a -&gt; a -&gt; Maybe a l /? 0 = Nothing l /? r = Just (l / r) I see no reason to overload existing `Prelude` typeclass to do that. Moreover, `Fractional`s already have value to represent zero-division, which works well.
What is it supposed to be doing?
Please also tell us how you're running/compiling it. Optimizations?
No optimisations. Just `runhaskell inversions.hs`. And for Python `python inversions.py`.
`[]` are linked lists. use `Vector` for O(1) access. https://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector.html
The difference is purely that `CoRec` is supported by the entire `vinyl-plus` library. I could have used either one to the same end. But `vinyl-plus` just has a number of convenience functions that I figured I might as well get access to. Plus, Oleg's isn't on any Hackage package, and I'd rather not publish a package for it when something already exists.
I would guess it couldn't be integrated into existing tech, but the idea would be that the system that actually checks the proof objects in the blockchain can be simple, even if the system you use to create the proof objects is arbitrarily complicated.
This seems very off topic for /r/Haskell.
To be fair, we do that with `MonadError` and `ExceptT`, which in my opinion are pretty much always the right choice over unchecked exceptions. I think people are slowly starting to move to them, but we still unfortunately have a lot of people using unchecked exceptions.
The Gas concept doesn't come from Solidity, as it is just a high level language similar to JavaScript, but with static types, that compiles to the EVM, which is the bytecode for Ethereum. On the EVM, each executed instruction requires burning a fixed amount of Gas. Gas is obtained by converting from Ether, which is a digital money like Bitcoin. So, tl;dr, computing costs money and is measured by Gas.
One thing I notice is that you are recomputing `length` a lot when you could just compute it once at the beginning and pass it around. You are spending something like O(n log(n)) on `length` when you could spend just O(n).
&gt; Arrakis teaches the attitude of the knife — chopping off what's incomplete and saying: "Now it's complete because it's ended here." It proves algorithms terminate by stabbing them if they run too long. It's an effective final fallback, but theoretically uninteresting. This trick is generally covered in the first "real" computer science in college course immediately after the halting problem is covered, because it's one of the basic quasi-answers to the HP: "just cut it off after a certain number of steps and declare it doesn't seem to halt". (The trick is used in real code in more than just Ethereum; one of the basic tools encompassed in the security buzzword "advanced threat protection" is running potentially malicious code in a VM environment with a certain amount of resources, and waiting for it to do something bad. This then turns into the obvious arms race between the VM emulators and attackers, but it does provide non-trivial protection, because at any given point in time a lot of attacks are running around that aren't necessarily the latest &amp; greatest.) Solidity is, unless I missed something big, "just" an imperative event-based language that compiles to a bytecode that Ethereum runs. There's nothing particularly interesting about it from a PL-theory perspective. (That is not, by itself, a criticism. There's a strong case to be made that in contrast to my first post about TFP and FRP and other things so theoretical that even the local community would hesitate to build a large program on them, that you should build on the most solid technology available at the time. However, IMHO, Solidity isn't it.)
This person's version of your `countSplits` function is a lot faster: https://gist.github.com/MastaP/2035851#file-inversions-hs-L67 
I think we simply have two camps diverging, building two separate ecosystems. To provide you with few examples where libraries switched from explicit error status to unchecked exceptions, compare [mongodb-1.*](http://hackage.haskell.org/package/mongoDB-1.5.0/docs/Database-MongoDB-Query.html#v:access) and [monogdb-2*](http://hackage.haskell.org/package/mongoDB-2.0.10/docs/Database-MongoDB-Query.html#v:access) 
Does haskell have the equivalent of Scala's Try? It seems very intuitive for me.
I'm reminded of [this](https://github.com/HIPERFIT/contracts).
I agree.
Is this spam? This has nothing to do with haskell.
I get a stack overflow when I run this on large input, so I doubt it's purely to do with optimizations.
&gt; I'd really like my financial language to be a total functional programming language, but that's still research-level right now; normal programmers are basically incapable of writing it right now, to say nothing of normal financial people. That's not true at all I think. You can write lots of perfectly good code in a total functional language without learning any new things at all. Given the restricted domain of financial contracts, I suspect that that suffices!
Great talk! Have you tried the "constrained, single-constructor" `CoRec`? data CoRec :: (k -&gt; *) -&gt; [k] -&gt; * where Col :: (t ∈ ts) =&gt; !(f t) -&gt; CoRec f ts I find it cleaner, and it might be faster. e.g. handle :: Rec (Handler f b) as -&gt; CoRec f as -&gt; b handle handlers (Col variant) = h variant where H h = rget variant handlers https://github.com/sboosali/vinyl-sum/blob/master/sources/Vinyl/CoRec.hs#L51 
100000 numbers. 
(I see. Only took a glance; it came up with Project Euler for me when learning Haskell).
I know, I've done the same https://github.com/sboosali/vinyl-effects/blob/master/sources/Vinyl/Effects/Language.hs#L59 Never benchmarked the difference, but I'd guess that performance is similar, given `RElem` is recursive. But, all work that goes into `RElem` speeds up all uses of `rget`. For clarity, compare `vinyl-plus`'s `handle`, which is recursive: https://hackage.haskell.org/package/vinyl-plus-0.1.1/docs/src/Data-Vinyl-Prelude-CoRec.html#coalesceBy 
I'm pretty sure this data type is isomorphic to Either. Then again, it looks like one would get more mileage out of ExceptT.
`IO (Either e a)` APIs are a pain and `(MonadError e m, MonadIO m) =&gt;` can get awkward. I'd prefer `EitherT HttpException IO`. 1. `do`-notation short-circuits on failure, like exceptions 2. easily recover exceptions `eitherT throwIO return :: EitherT HTTPException IO a -&gt; IO a`. 3. easily recover the original `runEitherT :: EitherT HTTPExceptions IO a -&gt; IO (Either HTTPException a)` 4. easily transform status codes into errors `mapEitherT (fmap (\case Left e -&gt; Left e; Right a -&gt; if is2xx a then Right a else Left a;)) :: EitherT HTTPException IO a -&gt; EitherT (Either a HTTPException) IO a`. I think of it as "converting errors into warnings" and vice versa. It's easy for the library author to wrap their failable bindings with `EitherT`. 
FWIW: abotu `ImpredicativeTypes`: [Trac says:](https://ghc.haskell.org/trac/ghc/wiki/ImpredicativePolymorphism) &gt; We've made various attempts to support impredicativity, so there is a flag -XImpredicativeTypes. But it doesn't work, and is absolutely unsupported. If you use it, you are on your own; I make no promises about what will happen. GHC docs from ~2006 to ghc7.10.3 doens't mention much. [Impredicative Type sections doesn't seem to have changed at least since GHC-7.4.2](https://downloads.haskell.org/~ghc/7.4.2/docs/html/users_guide/other-type-extensions.html) [GHC 8.0.1 has better docs! in a sense they say directly:](http://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/glasgow_exts.html#impredicative-polymorphism) &gt; However the extension should be considered highly experimental, and certainly un-supported. You are welcome to try it, but please don’t rely on it working consistently, or working the same in subsequent releases. See this wiki page for more details. --- Not all extensions are created equal. Already mentioned `ImpredicativeTypes`. `PolyKinds` didn't work well in first version it was introdued. `TypeInType` will most likely have some tweaks. I'm a bit doubtful if `OverloadedLists` won't never change. OTOH I'm quite sure that `DeriveFunctor` works expectedly. But I do agree, we should avoid dramatically change extensions, e.g. `PatternSynonyms` got quite a lot of power lately, and we have no way to tell Cabal whether we need newer versions of that extension. --- So if you rely on "experimental" extension (there should be some stability indicator), you have to specify which compiler you use. You can tell Cabal now: if !impl(ghc &gt;= 7.8 &amp;&amp; &lt;8.0) buildable: False Yet, I'm not sure whether solver knows about this. Similarly you can exclude buggy versions of ghc as well. --- There are bugs in GHC, or something changes: previously accepted programs might be rejected. (IIRC something happened at 7.0 when more programs could be accepted, but some older ones needed more type annotations). IMHO you cannot ever 100% assume that your code will work with newer version of the compiler. Luckily GHC development is quite open, and we get early previews, and some people even spend their time building portions of Hackage/Stackage to track those hidden bugs! And those changes to Cabal i.e. specifying which extensions you use, would help those people to find your package on Hackage without downloading whole Hackage. Or if `base` could survive without major bump thru new version of GHC, it would be even easier to test more!
Got it. Since the `MonadState` was exposed, I thought the user was meant to pass in their own cache. i.e. you and `Fraxl` might both access it in their own ways. 
Ah. I see. But now I have a separate implementation using `Data.Vector` where `length` is supposedly constant and the time is worse. Let me add the gist to the post.
Yea I used `MonadState` because I think it's important that the user be *able* to initialize the cache themselves, or store the resulting cache (Haxl does this for replay-ability). But the usage of it is still very specific to `fetchCached`.
That said, caching is merely a substitution on the data source. You could implement a custom caching system relatively easily. This sort of composability is what I like about Fraxl. As another example, you could substitute a data source with a logger that logs every request and the results, without having to change a single thing in the rest of the code base.
Hello. I think that your edit is wrong, it may be `A[i] &gt; A[j]` Some have already pointed out the use of `runhaskell`, which is interpreted, versus a compiled version. This is a small part of the problem. Some other have pointed that the python list, which are contiguous array, are faster in random acces and length calculation than the haskell, which are linked list. The equivalent of python list is `Vector` is haskell. I'm adding a few point to help you in your search for an answer. First of all, always write a simple and naive implementation of the solution and benchmark it. For example : inversions' :: [Int] -&gt; Int inversions' [] = 0 inversions' (x:xs) = (length (filter (&lt;x) xs)) + inversions' xs This solution is simple and will help you to have a baseline of performances, it runs on 2.9s on my computer using an inptu of 39000 values. You can use it in your tests to ensure that future solution are working correctly. This solution is `O(n^2)` In comparison, your solution runs in 2.6s on the same dataset. I tried to understand your code and recognize a merge sort, so we can hope for a complexity of `O(n log n)`. How to get an interesting idea of the performance you can achieve ? First just do a rough estimation using the complexity, if `O(n^2)` (my solution) is 2.9s, an `O(n log n)` solution should runs in something close to `2.9s * n log n / (n ^ 2)` or `2.9s * log n / n` or `2.9s * log 39000 / 39000 = 1ms`. This is a rough approximate, but clearly we are far from it. Another way to do this approximation is to test the performances of a merge sort. Actually `Data.List.sort` is a merge sort and this function sort my 39000 values in 120ms. Actually I was not satisfied, because 120ms is too much and I benchmarked correctly the code and realized that 115ms was used to parse the input data and that the sort was done in 5ms, so close to our approximation. So we realize that we must get a result in a few miliseconds to count the inversion. Actually for this dataset, the parsing will be the bottleneck. But I didn't give any solution yet and I won't. But the source code of `Data.List.sort` is available here https://hackage.haskell.org/package/base-4.9.0.0/docs/src/Data.OldList.html#sort and is actually short, so you may be able to adapt it easily. But we can continue our discussion. Why the hell is your code slow? To understand it you can benchmark it using the tool at our disposal. First compile it with profiling `ghc -- -O2 -prof -caf-all -auto-all inversions.hs` and run it. You'll get a file called `inversions.prof` in the same directory, and for me it contains this: Fri Jun 17 20:59 2016 Time and Allocation Profiling Report (Final) inversions +RTS -p -RTS total time = 3.75 secs (3751 ticks @ 1000 us, 1 processor) total alloc = 378,599,080 bytes (excludes profiling overheads) COST CENTRE MODULE %time %alloc countSplit.r Main 94.7 3.1 main Main 3.3 63.9 countSplit Main 0.6 21.2 countSplit.leftSplit Main 0.3 2.0 countSplit.rightSplit Main 0.2 1.9 countInversions.left Main 0.2 4.0 countInversions Main 0.1 2.7 individual inherited COST CENTRE MODULE no. entries %time %alloc %time %alloc MAIN MAIN 55 0 0.0 0.0 100.0 100.0 main Main 111 0 3.3 63.9 100.0 100.0 inversions Main 113 0 0.0 0.0 96.7 36.1 countInversions Main 114 78623 0.1 2.7 96.7 36.1 countInversions.whole Main 130 39310 0.0 0.0 0.0 0.0 countInversions.right' Main 126 39311 0.0 0.0 0.0 0.0 countInversions.(...) Main 124 39311 0.0 0.2 96.1 28.4 countSplit Main 125 585517 0.6 21.2 96.1 28.2 countSplit.right Main 134 256341 0.1 0.0 0.1 0.0 countSplit.rightSplit Main 133 275841 0.2 1.9 0.2 1.9 countSplit.r Main 132 275841 94.7 3.1 94.7 3.1 countSplit.left Main 131 250554 0.1 0.0 0.1 0.0 countSplit.leftSplit Main 129 270365 0.3 2.0 0.3 2.0 countSplit.l Main 128 270365 0.0 0.0 0.0 0.0 countInversions.left' Main 123 39311 0.0 0.0 0.0 0.0 countInversions.w Main 122 39311 0.0 0.0 0.0 0.0 countInversions.(...) Main 121 39311 0.0 0.4 0.0 0.4 countInversions.right Main 120 39311 0.1 0.0 0.1 0.0 countInversions.r Main 119 39311 0.0 0.0 0.0 0.0 countInversions.(...) Main 118 39311 0.0 0.4 0.0 0.4 countInversions.mid Main 117 39311 0.1 0.2 0.1 0.2 countInversions.left Main 116 39311 0.2 4.0 0.2 4.0 countInversions.l Main 115 39311 0.0 0.0 0.0 0.0 CAF:main1 Main 108 0 0.0 0.0 0.0 0.0 main Main 110 1 0.0 0.0 0.0 0.0 CAF:inversions Main 106 0 0.0 0.0 0.0 0.0 inversions Main 112 1 0.0 0.0 0.0 0.0 CAF:lvl2_r8oi Main 105 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Exception 101 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Handle.FD 100 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Handle.Text 99 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Encoding 93 0 0.0 0.0 0.0 0.0 CAF Text.Read.Lex 91 0 0.0 0.0 0.0 0.0 CAF GHC.Conc.Signal 89 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Handle.Internals 86 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Encoding.Iconv 77 0 0.0 0.0 0.0 0.0 This shows that 97% of the time is spent inside `countSplit.r` which includes a call to `length`, wich is `O(n)` as u/erunna suggested... Changing this piece of code to propagate the length that you compute only once and divid when needed is a 5 lines diff. Finally the code runs in 195ms for me, which is now acceptable. Optimising it more will be a more complex task, but will be achieved by understanding the code of `Data.List.sort`. I hope you got some clues from this message, but the idea here are: - Always have a baseline when profiling based on an initial naive implementation - The naive implementation will help you to test later - Know what you are targeting and why - Use the profiling tools. Have fun.
Ah, I see what you mean. This was a pretty neat article on the subject: https://www.schoolofhaskell.com/user/snoyberg/general-haskell/exceptions/catching-all-exceptions
Could you post a fixed version with minimal changes? I tried the change you suggested and I didn't see any difference.
&gt; Changing this piece of code to propagate the length that you compute only once and divid when needed is a 5 lines diff. Could you post your change?
There is `ApplicativeDo` in GHC 8.0. Not quite what you're asking for but it helps {-# LANGUAGE ApplicativeDo #-} x a' b' = do a &lt;- a' b &lt;- b' return (a / b) &gt; :t x x :: (Applicative f, Num n) =&gt; f n -&gt; f n -&gt; f n Of course in this case, standard Applicative style is better: `(/) &lt;$&gt; a' &lt;*&gt; b'`. But if you want to use Nothing, you can use the Alternative type class {-# LANGUAGE ApplicativeDo #-} x a' b' = do a &lt;- a' b &lt;- b' if b == 0 then empty else return (a / b) &gt; :t x x :: (Monad f, Alternative f, Num n) =&gt; f n -&gt; f n -&gt; f n That's the bare minimum for doing the divide by zero check, and it actually does require Monad. So that should be a hint that there's no Applicative style for doing the divide by zero check.
Third try - at least it now gives the same answer as the python one. countSplit :: [Int] -&gt; [Int] -&gt; ([Int], Int) countSplit = go 0 where go ny [] ys = (ys, 0) go ny xs [] = (xs, ny * length xs) go ny (x:xs) (y:ys) | x &gt; y = let (zs, c) = go (ny+1) (x:xs) ys in (y:zs, c) | otherwise = let (zs, c) = go ny xs (y:ys) in (x:zs, c+ny) 
Cool presentation and library! I have a question about law-breaking `Applicative`s. It's something I've [thought about before](https://tangledw3b.wordpress.com/2015/02/12/applicatives-monads-and-concurrency/). I don't really like the law-breaking even though it's not so bad. But why not use a newtype, where the original type implements `Monad` and its `Applicative` superclass is law-obeying and the newtype does not implement `Monad` and has the concurrent `Applicative` instance? This is what `ZipList` does or more relevantly, `Concurrently` from the `async` library. With `Haxl` I can see the motivation because the whole point is to hide that you're using concurrency from a bunch of Facebook programmers who are writing spam fighting logic and don't want to know these kinds of details. But for a library where correctness and cool algebraic tools are a priority, I think a small mental cost of annotating concurrency might be a good choice.
Sometimes the compiler can't tell. I sometimes put bang patterns on arguments I expect the compiler to identify as strict just to ensure that it's behaving.
Yea, I could have just made the Applicative effects be done in `Ap`, then used the traditional free monad around that. But having to reason about that was counter to the point of getting concurrency for free, so I dismissed the idea. It's possible that would be the better approach though. That said, breaking that law this way is extremely inconsequential. As long as it is morally correct, which it is, it's fine. I don't imagine any problems will come up as a result
This is exactly my opinion on the likes of Bitcoin and other spin off crypo-currency. Financial institutions don't just appear out of nothing, when they do it's a red flag to investors that something fishy is going on. The same goes for "beta" or "patent-pending" software, you just don't know what you're buying into until the hype is already over. It kind of hurts my hope that a more technology-advanced, transparent economy could exist, but we'll get there by gradual change just like everything. 
You're right but there's so much more sanity going on here than over there. 
I totally agree with that assessment. However, I still think we should use exceptions. Summarizing some of my earlier points: 1) We're already stuck with them 2) They're fast, don't add overhead 3) They're convenient I suppose it's mostly point (1) - yes, it is anti-thetical to the essence of Haskell, but we're stuck considering them anyway. It's far from trivial to design something better that has more safety. I think we can get more safety with the existing mechanism, but the libraries have to be written and used. If the essence of Haskell is grounded in history, though, I think one could pedantically argue otherwise, though. I heard it was designed from the beginning to be lazy and partial, and this was part of the original intrigue. The partiality probably came before the ability to catch exceptions (I dunno if that was possible before the IO monad).
Nice quick approach. Though when there are duplicate elements it gives a different answer. (but then again you could argue the input should be a permutation)
This: https://gist.github.com/jstimpfle/f708125e7fe2ec92e36584c65537208b is where I challenge you to go Haskellers. Runs in 30ms for 100000 random integers on my old system.
Not sure about ActiveRecord, but I'm pretty sure Spring keeps your transaction state in thread-local storage. It's just maintaining a bit of dynamically scoped state, it's not a hard problem.
Just to avoid any potential misunderstanding. I'm not at all involved in ethereum development myself, just quoting Vitalik here. I suspect one would likely need to look at what they currently have and then solicit feedback from the community and developers with a proposal once things have calmed down a little. Whatever the research problems are, I think there's probably no domain with a more immediate need for the solutions. My understanding is that the properties that need proving have more to do with modeling financial contracts and less to do with the underlying blockchain technology if that helps. (At least, in this particular case...) EDIT: Here's some concrete problems that they've had to deal with: https://www.reddit.com/r/ethereum/comments/4omdlf/to_kickstart_the_building_safer_smart_contracts/ Don't ask me what any of that means :)
Thanks, I didn't realise it would be possible to do that with just a newtype wrapper.
Designing a language is one thing, and verification is another. The two are sometimes related, but are by no means joined at the hip[1]. Building a model of the virtual machine in TLA+ and using it to verify compiled contracts seems like a very feasible effort. Other kinds of tools, like static analysis for the bytecode, can make some forms of verification under some circumstances easier and faster, but a TLA+ spec is something that can be done right now, and for a very reasonable effort (probably less costly than coding a new EVM implementation, possibly much less costly). In addition, a high-level formal spec (in TLA+ or Alloy) of complex contracts is probably also the responsible thing to do, regardless of the language you end up choosing to code it. Both of these tools are significantly simpler and significantly easier to learn than Haskell, so people here really shouldn't have a problem learning and using them. They do, however, require careful and precise thinking, which no tool could possibly eliminate if a good program is what you want. Of course, verification always means ensuring a program satisfies a certain spec. No verification procedure would tell you that the program may do something undesirable if you don't specify in any way what kinds of things are undesirable (e.g., you'd need to say that a participant ending up with all the money is undesirable). Coming up with a good, reusable list of undesirable properties may be the lion's share of the effort, and is a domain knowledge problem rather than a technical problem. -------------- [1]: 99% of verified code is not written in the kinds of languages you mention. Production verified programs are written in languages like Ada SPARK, SCADE (well, this one is FRP-ish), C and Java. It is true that some languages may be easier to verify than others, but the difference may not be as big as some people imagine, due to very real theoretical limits (that tie the cost of verification to the to the number of states the program has rather than the language it's described in). Writing code in a total language has little to no effect on the effort required to verify it (it is usually a requirement for languages that are intended to be used as theorem provers for general mathematical theorems, where infinite recursion of a proof has no meaning and results in a logical inconsistency).
&gt; it will likely be the case that the invariants that can be offered by the higher language will be compromised in practice Which is why you need to verify at the bytecode level. It shouldn't be too hard, especially for programs that are quite small. Of course, higher-level tools that help you make sure you've written the right thing in the first place can also be extremely useful. They, too, can be new languages or verification/design/modeling tools for existing languages.
&gt; You can tell Cabal now: &gt; &gt; if !impl(ghc &gt;= 7.8 &amp;&amp; &lt;8.0) &gt; buildable: False &gt; &gt; Yet, I'm not sure whether solver knows about this. `buildable: False` tells cabal only that a single component of a package is not buildable. It's not the same as introducing an unsatisfiable constraint such as `build-depends: foo&lt;0`.
I think the issue alluded to here is that compiling to byte code means you'd also have to verify the behaviour of the EVM implementation and its spec to even have any of the guarantees mean anything. 
&gt; forces you to be aware of each type of exception It is not the case for `std::io::Result`, it always uses the same `std::io::Error` type to indicate failure. Different IO functions may fail in different ways, but they use the same error type. You can check what exactly went wrong using [std::io::ErrorKind](https://doc.rust-lang.org/std/io/enum.ErrorKind.html), but you do it in runtime. It is almost exactly how `SomeException` works in Haskell, so I don't think it is what /u/onmach meant. &gt; I don't personally see IO as an indicator of exceptions though. Why? I find it pretty useless to have "can perform IO" thing without "can fail" because non-failing IO actions are very rare.
 type IOorFail a = IO a Problem solved?
Sidenote, why doesn't `mtl` have a convenience function for that? liftEither :: MonadError e m =&gt; Either e a -&gt; m a liftEither = either throwError return do val &lt;- liftEither =&lt;&lt; somethingThatReturnsEither
`Result` is used in Rust to tell the programmer "Hey, you **have** to deal with this error, even if all you do is crash". `IO` doesn't do that at all. `IO` just lets you be on your merry way, and doesn't enforce any rules about checking for exceptions. This is a logical, mechanical difference. Just because `(&gt;&gt;=)` and `try!` have the same behavior doesn't make them the same. There are plenty of examples of patterns that behave the same, but which force you to reason about them differently. `IO` code is not reasoned about in the same way as `Result` code, even if `try!` makes it possible for them to behave the same.
OP, can you explain why this is related to Haskell? 
Yes, I was too rude and schematic. I'm very sorry for that. I will try to be more assertive, but i will not go deep in the details. since detail by detail, anything can be reasonable in certain cases. What is wrong is the whole stance. What is wrong is the whole thing. The attitude. A look at your program and any non partisan haskell programmer will desist immediately from doing any serious work with Haskell. He will have the same impression that has destroyed all the reputation for haskell in industry for decades: That for printing in the console it is necessary to have a doctorate in maths. And I'm sorry, this impression is right. Not by the language itself, but by the very bad and false pedagogy of many haskellers. This is the ping pong program in Erlang: ping(Receiver) -&gt; { pong, Receiver } ! { ping, self() }, receive pong -&gt; io:format("Got pong!~n") end. pong() -&gt; receive finished -&gt; io:format("Pong finished~n"); { ping, Sender } -&gt; io:format("Got ping!~n"), Sender ! pong, pong() end. Imagine that an experienced production programmer interested in Haskell or a decision maker in a company look at this code and compares it with yours. You know -but the above guys don't- that, using cloud-haskell, this same thing can be done in almost the same number of lines and with similar clarity, but with better type safety. I don't even mention [this other library]"(https://github.com/agocorona/transient/wiki/Transient-and-the-Actor-Model-(Erlang-OTP,-Cloud-Haskell,-Scala-Akka)") that convert the problem into something trivial. But you RUINED that opportunity for haskell because you made headlines in the topic of distributed haskell, presenting convoluted arguments and artificially complex code that -supposedly- promises to simplify (!??!) programming using cloud Haskell. And -worst of all- you received dozens of votes. What do you think that the programmer or the decission maker will do? He will write a report: "Using Haskell for our distributed problem: case closed" Haskell keep kidnapped by such pedagogy that discard what Haskell already has and emphasize last hour occurrences, fashionable items of the month, convoluted arguments with long clauses of unproved statements extracted from the imagination and not from months or years of experience in real cases. Haskell is kidnapped by academicist puritans that comes every now and then with a new revelation of ten cool lines of code that destroy what others have done, because the community tend to follow more the last cool guy in the town who can assemble the longer line of dot composition that type match than someone with experience in the problem. I'm waiting for the day when Haskell will be liberated from such pernicious community
&gt; try! You can't. I wish Haskell supported identifiers that mix alphanumeric with non-alphanumeric characters. e.g. the `predicate?` and `mutation!` convention in lisp.`
The same in Rust (with a bit better API, and more portable): https://play.rust-lang.org/?gist=89beb3eb5606881d32664248e5661288 The compiler is probably keeping the [idx1] and [idx2] bound tests, so it could be a little slower than the C version.
&gt; Just because (&gt;&gt;=) and try! have the same behavior doesn't make them the same. So the difference is not in behavior, it is in our minds! That is exactly what I'm trying to say. &gt; There are plenty of examples of patterns that behave the same, but which force you to reason about them differently. Yes, haskell syntax is less suggestive, but I'd not say it forces us to any specific reasoning. It is often hard to change your mind, but it is possible. &gt; IO code is not reasoned about in the same way as Result code Right. And that is exactly what is wrong with haskeller's approach to `IO` -- they don't admit that `IO` indicates failure. By design `IO` includes all possible side effects, and failure is a side effect too. If you accept that, then exception handling in haskell will become easy and logical. I'd say that `IO` in Haskell is used to tell the programmer (among other things) "Hey, you **have** to deal with this error" (the "crash" part doesn't applies here because we use exceptions where Rust uses panic.)
 - (whole , w) = id $! countSplit left' right' + (whole , w) = id $! countSplit mid left' right' in (whole, l + r + w) -countSplit :: [Int] -&gt; [Int] -&gt; ([Int], Int) -countSplit [] y = (y, 0) -countSplit x [] = (x, 0) -countSplit (x:xs) (y:ys) +countSplit :: Int -&gt; [Int] -&gt; [Int] -&gt; ([Int], Int) +countSplit _ [] y = (y, 0) +countSplit _ x [] = (x, 0) +countSplit len (x:xs) (y:ys) | x &lt;= y = ((x: left ), 0 + l) | otherwise = ((y: right), 0 + r) - where leftSplit = countSplit xs (y:ys) - rightSplit = countSplit (x:xs) ys + where leftSplit = countSplit (len - 1) xs (y:ys) + rightSplit = countSplit len (x:xs) ys left = fst leftSplit right = fst rightSplit l = snd leftSplit - r = (snd rightSplit) + length((x: xs)) + r = (snd rightSplit) + len I added a `len` argument to `countSplit` and use it instead of the computation using `length` which was responsible for the `O(n * n)` complexity. This allows me to solve the 100k problem (using my own dataset) in 0.5s. However the profiling gives 60% of the time inside the parsing, so it may be interesting to improve the parsing. `read` is notorious for being slow because it use `String` internally which are usually slow for huge amount of data, so I gave a try with `Data.Attoparsec` and `Data.ByteString`. I simply replace your parsing which was: parse = liftM (map (read :: String -&gt; Int) . lines) getContents by: import qualified Data.Attoparsec.ByteString.Char8 as P import qualified Data.ByteString -- ... parse :: IO [Int] parse = do content &lt;- Data.ByteString.getContents let Right res = P.parseOnly (P.decimal `P.sepBy` (P.char '\n') ) content return res It is a bit more convoluted, but I'm using `getContents` from `Data.ByteString` and then I'm creating a parser which parse many `P.decimal` separated by a `P.char '\n'`. This drop my running time to 0.3s for 100k items and the parsing only account for 15% of the running time, so time for more optimizations on your function. I put everything in a github repo https://github.com/guibou/inversionsCount/commits/master Good luck. 
I thought this was r/DunderMifflin for a second... Glad I'm not the only one intersecting Haskell and The Office!
Hi , Just finding out if there is any far connection for Haskell to this. I mean future web development architectures.
&gt; But still, I wouldn't say IO is a good vehicle for error handling. I know that this opinion is widely accepted by haskellers. But I never saw a single valid argument for it yet :( &gt; Regardless, this doesn't change the fact that Result enforces compile time guarantees about error handling that IO simply does not. If you mean "if you don't accept `IO` as failure indicator, then `IO` doesn't enforce compile time guarantees", then fair enough. It makes no sense to argue with opinion, lets just agree to disagree. But if you mean "even if you accept `IO` as failure indicator, `IO` still doesn't enforce compile time guarantees", then I don't understand it. If you just need a syntactic indicator for propagating exceptions, then try this: newtype Result a = Result (IO a) deriving (Functor, Applicative, Monad) runResult :: Result a -&gt; IO a runResult (Result a) = a tryIO :: IO a -&gt; Result a tryIO = Result main :: IO () main = runResult $ do str &lt;- tryIO $ getLine tryIO $ putStrLn str But `Result` here is completely trivial, it adds no extra functionality and no extra safety. It is just forces you to write `tryIO` for purely syntactic reasons. I don't see what kind of bug it can prevent. 
Lexifi, a financial contracts language based on OCaml, is a commercial shipping product and uses higher-order approaches to model observables/events.
I'm new to Haskell, can you explain monads to me like I'm a first grader? 
Let's be pragmatic.
Haskell? It's that academic language, right? 
The difference is that a function that throws an exception in haskell is IO a, whereas a function that throws an error is Result&lt;A, SomeError&gt;. You can't run the function in rust without acknowledging the fact that you are ignoring an error. But in haskell it is quite possible for you to run the function, get an answer and never know there could have been an error. The fact that these errors tend to only happen around IO code in haskell helps narrow down where an error could occur, but there's a lot of IO code in a real production system, and I've had intermittent run time breakages due to exceptions I didn't know could occur or ones that I did know could occur, but forgot to catch. An API in haskell could use the same error handling rust does, but it is opt in, and a lot of the community is opting out. In rust there's no risk of that. And that's nice.
Powerful and easy to use library, documentation is currently lacking, but you can easily follow the types after reading this delightful paper.
Programming with state is more intuitive because underneath, computers are a big state machine.
Exception: Prelude.head: empty list
Real programs need to interact with the outside world
Monads are like burritos
Functional programming? No one programs in lisp, man.
Explain a Functor? That's easy! It's like a box.
Failed, modules loaded: none.
Where do I find Haskell's symbolic debugger?
&gt; If Haskell is so great, why isn't everything written in Haskell? s/everything/anything/
A monad is a value in a context.
So the context is the box.
Your solution is 15ms on my dataset. My new haskell solution is 78ms ;) Well done ;) https://github.com/guibou/inversionsCount/commits/master
New version with a running time of 75ms. (edit: it is 75ms, not 0.75ms... Sorry... I should have typed my input with `Numeric.Units.Dimensional`)
I think you mean forM_
Facebook uses Haskell. 
&gt; `Result` at least forces you to `try!` it So does the `Result` newtype in my example above. What additional value it adds except few keystrokes to type `tryIO $`? What bugs does it help to prevent?
I tried this in Haskell and it's got the same problems as JavaScript: [] == "" 
An AppendableMappable is just an Appendable in the ObjectyArrowy of SelfTransformables.
[glpk-hs](http://hackage.haskell.org/package/glpk-hs) comes to mind. Depending on how big your problem instances are, GLPK might be good enough.
&gt; It's easy for the library author to wrap their failable bindings with `EitherT`. I would say the opposite. It's easy for the library user to wrap `IO (Either e a)` with EitherT. That way when the giant transformers EitherT/ExceptT fiasco comes up, the library author is unaffected.
Does it run on JVM? 
http://cdn.pophangover.com/wp-content/uploads/2013/12/brad+pitt+seven+gif.gif
Types don't really help to catch bugs because people almost never pass booleans to functions that take integers.
No. The library author does it once, the library users do it every time. Ditto compatibility. You don't find `IO-Either` APIs a pain?
&gt; The claim that Haskell is pure is false because there is unsafePerformIO. this I like
A monad is a space suit full of burrito ingredients.
But allocation *is* mutation.
`f :: a -&gt; ExceptT SomeException IO a`
Haskell is the perfect language.
Comic Sans
5x seems pretty nice (although the C code is not only faster but also easier to read). Did you measure the parsing overhead? Maybe you can come even closer.
Make is better than Cabal.
I know Haskell sucks because I tried writing something trivial - the Sieve of Erastothenes (a "hello world" of math really) - in it and it was *[so difficult](https://wiki.haskell.org/Prime_numbers#Using_ST_Array)* to do it efficiently!
It's annoying because it's true.
The parsing account for 15% of the total running time. I totally agree that this code is totally unreadable. Mostly because there is no special operator in haskell for `ST` and `Vector`, so you have to live with the `V.unsafeWrite vector idx value`. I tried to build a custom DSL https://github.com/guibou/inversionsCount/blob/syntax/inversions.hs#L30 and this is "better". Perhaps someone more used to `ST` may gives tips. Actually, this is the kind of code for which I'm not ashamed to use the FFI and call C or Rust from Haskell. I'm sure we can come closer to your C reference, however this will imply reading the GHC Core language to understand what is really happening and... Well, I'm not motivated now ;)
&gt; You don't find IO-Either APIs a pain? No. They're the most flexible and don't impose a choice of error handling mechanism on the user.
This appears to be undocumented (sorry!) but to enable haddocks by default put this in your `~/.stack/config.yaml`: build: haddock: true I implemented the original minimal `--haddock` support for Stack and unfortunately not much has been added to it since then. I'd love to go back and add all the features people have requested, but time has not been forthcoming lately (even better would be reviewing PRs adding those features). I think one of my original mistakes was relying on the Cabal library to build the haddocks, which has an impedance mismatch with how Stack should be doing things. That's part of why some of the limitations are there (such as having to rebuild the package in order to build the haddocks; I couldn't find a way to avoid that at the time). So probably the first thing to do is just have Stack do all the haddock stuff without using the Cabal library. Once that hard part is done, adding more features should be much easier. 
&lt;insert anything Jon Harrop has ever said here&gt;
Is there anywhere in the UK I can pick a copy up? Can't find it anywhere.
I like parentheses.
Let me revise my sentence by adding "without recursion" to the end :P
Why would that annoy /r/haskell?!? It gets asked a lot, but if anything, you should take it as a hint about deficiencies in Haskell documentation, instead of being annoyed.
Currying and partial application are the same thing.
I have doubts that any amount of documentation could avert this one. (It's an understandable mistake. I am not blaming the learner by any stretch.)
omg
&gt; I never used a functor in Ocaml. Ha! Not sure if that was intentional.
Some good tasks here http://www.haskellforall.com/2015/12/how-to-contribute-to-haskell-ecosystem.html
Haskell exists to make Haskellers looks smart.
Awesome! I'll check out the sample and see if I like it.
From what I gather, there should be 2 possible points of entry to the API: 1. A function that returns a `Response` type that includes both `StatusCode` and `Maybe Body` (the body of the response, which may be included or not). If the user wants full control over the response they should use this one. 2. A function that returns a sum type, with either an error (in non 2XX cases) or the body of the response. If the user just wants to handle the "happy case" and not care about handling status codes then they should use this one. Regarding having `http-client` just being a type-safe wrapper for the HTTP protocol then the 1st option should be sufficient as the "default", since in HTTP as long as you get a valid HTTP response it is can be considered a "success", no matter what the status code is. But to make it more user-friendly the 2nd option can be used as the default.
This sounds useful. Is it available?
I reckon the most useful answer is, use `&lt;-` inside a `do` block.
Haskell doesn't pay the bills.
And non termination is a side effect
Can be generalised as "if you know what you are doing you don't need type safety"
Oh that makes me fume
It seems like Markdown would be more suitable here, depending on how much different kinds of graphics you need. However, this is a great example of how easily extensible Haskell DSLs are. HaTeX is awesome for document generation. I've used it in combination with Markdown multiple times. 
 You can't tell what is a function call without parenthesis around the arguments.
Author here. Feedback is welcome! This was my first try at doing anything slightly nontrivial with servant. I was pleased to learn about how the HasServer typeclass works and how this relates to the universe pattern, which I learned about in the context of Idris.
Love this one. I almost get personally offended by this. &gt; "So is *my* name not good enough? Would you like me to change it to, say, ShortWhiteAmericanDudeWithWifeAndThreeKids?"
This is the moment when Haskell sits you down in a chair and asks you to pick between a red pill and a blue pill.
Haskell sucks because it has partial functions. Commentary: 1st world problems here folks. Try writing a total function in Python/Ruby/JS. At least Haskell *has* total functions... :P
Yes, although some parts aren't quite as decoupled as I would like to. * Here are the notes: https://github.com/NorfairKing/the-notes * Here is the bare-bones back-end: https://github.com/NorfairKing/LambdaTeX * Here is the back-end for graphviz code generation: https://github.com/NorfairKing/haphviz 
Doesn't it have to be untrue to qualify ?
sed s/probability/Probability/g &lt;old.txt &gt;new.txt
No, the P as in P(X = 5).
Seems like it's not just the language subreddits either. I saw it on [/r/feedthebeast](https://www.reddit.com/r/feedthebeast/comments/4ods8p/piss_off_rfeedthebeast_with_one_sentence/), too.
This is almost true. The compilation phase of Haskell catches a *bunch* of my stupidity. If you only see what makes it into my commits, you'd think me much smarter than I am.
Consider using a bit of quasi-quotation for this, because the double quoted escaping and multi-line strings don't seem so nice.
F#, but it is on dotnet. Not sure if it is a deal breaker for you. Scala, but it is a horribly butchered language in the name of endless concessions to object oriented nature of java.
"So I just used `unsafePerformIO`"
Combine this with my old shot at this with servant 0.4 and you've got some nice modern authentication scheme :) https://github.com/arianvp/servant-jwt-example I really like the universe explanation you're giving here! I'm currently dabbling around in Idris and planning to write a servant clone in it. Usually, universes in idris are closed. Whilst Servant's universe is very open. It's really easy to extend the servant EDSL. how would you go about encoding this openness in idris? Usually you define some datatype X which is your _closed_ universe and you write interpretations to type for it. But how do you extend X elegantly?
No wonder we got a Javascript backend before a Java backend...
And burritos, of course, [are like monads](http://www.cs.cmu.edu/~edmo/silliness/burrito_monads.pdf).
In fairness, type errors are a fairly small (but rather impactful) subset of the mistakes most people make. You're way more likely to make algorithmic mistakes... at least I am. Comparing two like quantities with &lt; when you meant to use &gt; is perfectly valid typewise after all.
It's everywhere, most of the video game subreddits are doing it too. 
Well yes, but this annoys me when I hear such a thing!
Programmable semicolon !!!!!
I've actually also done this before! Some people don't like my code style, and it was a slightly old version of servant, but it worked very well! Here's the implementation: https://github.com/hherman1/SashaAsherArtSite/blob/master/src/Auth.hs It took quite a bit of work but was very rewarding and made it quite easy to implement security. As proof, check out the simplicity of my secured API routes: type GetUser = WithAuth :&gt; "get" :&gt; Capture "uid" UID :&gt; Get '[JSON] (Maybe User) getUser :: SID -&gt; UID -&gt; UID -&gt; Serv (Maybe User) getUser _ _ uid = lift $ Sql.getUser c uid You can see servant interprets the `WithAuth` type in routes as a `SID -&gt; UID -&gt;` (session ID and user ID). The auth handler takes care of failed authentications on its own which is ideal for *this* backend because it was a RESTful API. This makes working on the site super simple! 
The general counterpoint is that with a powerful enough type system, even those algorithmic errors are type errors. The name "type system" is a bit unfortunate, because in reality it's a proof checker of varying sophistication.
Oh dear god not the "our brains are meant to think imperatively". Normally spoken by someone with 15 years of experience of imperative languages.
"And now I get a file already exists exception."
That's an optimization concern, not a semantics issue. ;)
It seems to me that the only alternative that is "mainstream" enough is OCaml. I don't know much about it, but it might scare python users for other reasons. For example, doesn't seem to have near the same popularity as Haskell, and just doesn't seem very mature. On the other hand it is very well established for some use cases. Most people I met that are OCaml'ers do static analysis for example.
What is the best GUI library for Haskell? I. e. see first 5 minutes of this talk: https://media.ccc.de/v/rustmcb.why-port-10k-loc-to-rust
forM is just a flipped version of mapM: it maps over the list "passing the monad through", yielding something like m (t a). forM_ does the same thing, but then throws the result away, yielding a m () but still keeping the effects. forM_ is thus closer to for loops, because in imperative languages for loops don't return a value either.
I'm including a reasonably expanded version... roughly, anything you'd write down what it was if you were working on paper. 
Maybe not documentation, but better education of software engineers could prevent a large portion of these questions. 
Sure. The idea is not to make it impossible to create bugs, the idea is that bugs should stand out as relatively obvious. `fromJust a + 3` is way more obviously partial than `a + 3`. This is similar to how formal proof systems don't make it impossible to create incorrect proofs, but they do force you to support yourself on invalid assumptions to be able to create incorrect proofs. "I am an elephant. If I am an elephant, I have a tail. Therefore, I have a tail" is a completely valid proof which might lead you to conclude I have a tail, but it rests on an obviously invalid assumption. It's easier to fact-check assumptions than... well... the entire proof.
Isn't the correct response to this: "that's what a runnable Haskell program is"?
I think this is how some people feel about (and how others sell) the language as a whole.
It's funny, many of these annoyances seem to be downstream results of bad Haskell pedagogy/salesmanship.
Partial application is never used actually.
Ah, I see my suggestion was successful ;)
You can always screw up the logic, but a smart modelisation can help tracking logic errors. For example, quantities (like Float) with an associated dimension can show you many cases where you are wrong. Storing a distance instead of a squared distance somewhere, or adding time and speed instead of multiplying. I usually observe that programmers are proud of their work when they wrote a type which is able to work for many of their use cases. For example, a `data Point3 t = Point3 t t t` which can, providing the right functions, works as space coordinate, direction, normal, color in different color space, intensity, flux, radiance, speed, acceleration... That's a good type for a library, but it should not be used as is in an application, it should be wrapped in as many types as use case you have and only the needed functions should be exposed. For example you don't need a `subtract` function between two points, you need a `directionFromTo :: Point -&gt; Point -&gt; Direction`. Yes, you can still screw the ordering of the two arguments, but at least you will not subtract a point and a color.
I suppose you mean that Frege isn't GHC.
Or "just blame the programmer"
&gt; Standard Chartered is listed but, again, this is little more than Neil Mitchell and Lennart Augustsson. lol
Maybe [elixir](http://elixir-lang.org) supposedly is one of the new breeds. But, personally, I am also a python programmer and the transition was not that hard. Just some reading and you are good to go. [This ](http://learnyouahaskell.com) book is really good to start of. With something like 2 hours daily I would bet that in a week you will feel pretty confident using Haskell. And the extended vocabulary really helps at parties. (lol)
Which is exactly why I hate using Python for projects larger than 500-1000 lines of code. In 500 lines, I *might* be able to keep all the types in my head. At 50k? Not chance in hell. I love Python for &lt;100 scripts but at my grand age of 33, I can't remember all the types and state of everything. And why should I if the IDE / compiler can do it all for me?
How about `Flatmappable` for the Scala crowd?
why does haskell have so many strange named functions? (im really a beginner :))
I have now a solution which beats your C implementation on my computer. https://github.com/guibou/inversionsCount/commits/master Your version is compiled with `clang -Ofast -march=native` benchmarking ./inversions &lt; data time 16.21 ms (16.11 ms .. 16.31 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 16.17 ms (16.12 ms .. 16.30 ms) std dev 201.3 μs (100.5 μs .. 379.8 μs) benchmarking ./a.out &lt; data time 16.35 ms (16.30 ms .. 16.43 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 16.37 ms (16.33 ms .. 16.43 ms) std dev 121.4 μs (91.09 μs .. 168.6 μs) `inversions` is the haskell one. `a.out` is the C one. I'm happy because I learned something doing this ;)
I'm now at ~16ms, beating on average the C implementation provided in this thread. I think I'll write a blog article about this (But I need to open a blog first) soon. I'll keep you aware. In the meantime, you can have a look at the github account, the code is available without any comment ;)
/u/haskell_oxford in /r/haskell, /r/programming, and /r/programmingcirclejerk: 2016 Indictment
My problem with `TypeError` in Python is that it usually gets thrown several layers of call-stack below where the actual bug occurred. Duck typing: all the worst parts about type-classes, with none of the upsides.
Because we suck at naming things
is this like the comparison of .each and .map in ruby?
Haven't used Haskell much so far. What's the problem with `Num`? Is there a bug with it, or is it just too general to be safe?
and is this underscore a general convention or just for this function? thanks for your answer!
It's general, or at least it's used for other functions in the module: if you look at [Control.Monad](http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Monad.html#v:forM-95-) you'll see other functions with the underscore to signify that they throw the result away, like zipWithM_ or sequence_, always accompained with a non-underscore version.
ah ok, thanks for your answer! :)
You can write your example as data Foo a where Bar :: (a ~ String) =&gt; Int -&gt; Foo a Quux :: (a ~ [Int]) =&gt; Bool -&gt; Foo a so the same thing is happening in both cases.
It's not useful in practice, since Haskellers are more interested in implementing next abstract type theory paper rather than improve things like running in production or debugging.
I think we agree on a lot of this stuff! Have we spoken before?
Should head [] = []?
We both know that lisp is more expressive than haskell.
It really depends on what you want from this new language. If you don't mind OO, favour practicality over elegance, wish for more static types, have a need for go routine and channel, `Go` might be a good choice. Particularly as a Python replacement as I see it as a better `Python` myself (at least for non scientific or non scripting work). For the record, `Go` is well established and heavily used in the sphere of what is called "devops" (`docker`, `consul`, `kubernetes`...). On the other hand, `Haskell` is a wonderful language to learn and could be suitable if you strive for a statically typed and functional language. As far as my experience goes, Haskell is a wonderful medium but I would be careful to introduce it within a Python team (particularly if the team is not already receptive to functional ideas). If you do introduce `Haskell` I would say you need someone on the team that knows the language quite well already in order to drive the team in the right direction. If your team is already sold on the strongly/statically typed purely functional - as you put it - I don't think the `Haskell` route would be that hard then. I don't know them but `Swift` or `F#` might be good candidates too. In case you don't care much about statically typed, love the functional paradigm and don't mind the JVM, `Clojure` is an interesting option. `Rust`is also quite an interesting one but it is more a replacement for `C++, C` type of code (at least for now in the early days) As a final note, `Purescript`is quite awesome for web development but then again `Typescript` might be a more practical choice there. Really it all depends on what you do with the language and the sensitivity of your team. As much as I like `Python` for instance I believe it has been a poor fit for software such as `Ansible` or `Salt` (same for the `Puppet`/`Ruby` combo).
Thanks for the suggestion. Trying this yields the error below. If I am reading this correctly then I could go and write an instance in bytestring conversion for Maybe Integer but this would still mean that the header would be output, simply with no content if I decide that Nothing is the empty string. For headers that have to be of a certain format I'm not sure that's really an option. &gt; No instance for (bytestring-conversion-0.3.1:Data.ByteString.Conversion.To.ToByteString &gt; (Maybe Integer)) &gt; arising from a use of ‘addHeader’ &gt; In the expression: addHeader (Just 0) &gt; In the second argument of ‘($)’, namely &gt; ‘addHeader (Just 0) $ [r1, r2]’ &gt; In the expression: return $ addHeader (Just 0) $ [r1, r2] 
 benchmarking ./inversions &lt; data time 16.09 ms (15.98 ms .. 16.19 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 16.05 ms (15.99 ms .. 16.15 ms) std dev 180.8 μs (97.75 μs .. 273.4 μs) benchmarking ./a.out &lt; data time 9.347 ms (9.196 ms .. 9.557 ms) 0.998 R² (0.995 R² .. 1.000 R²) mean 9.237 ms (9.191 ms .. 9.343 ms) std dev 184.1 μs (79.33 μs .. 332.9 μs) Well done ;)
But the Haskell language is wonderful. Is the Haskell community -that is misguided by wannabe academicians that hate industry- the one that makes Haskell a mess. They want haskell as a dead language like Latin, to be used only in cult ceremonies and holy and useless papers and libraries with poor quality that nobody should understand, since the universities pay for it and you can look like an intelligent person. 
How are you going to hire Haskell programmers? Literally no one uses it!
&gt; But the Haskell language is wonderful. Is the Haskell community -that is misguided by wannabe academicians that hate industry- the one that makes Haskell a mess. Solipsism. Haskell doesn't look for these people, but it has a reputation for attracting ignorant neophytes with a remarkable ability to inflate and embellish their own successes, resumes, abilities at the expense of others.
[removed]
This sounds a bit like the [Ghostbuster ICFP 2016 paper](http://www.cs.ox.ac.uk/people/timothy.zakian/ghostbuster.pdf) in spirit.
Mostly because it groups a seemingly random set of operations together that its instances must implement, and you usually end up with types with clear semantics for `(+)`, `(-)` while `abs` or `signum` doesn't make any sense. A lot of people consider `Num` a wart in the language, and the numeric hierarchy should more or less follow the algebraic structures in Mathematics (groups, rings, fields, etc) instead. It is probably a bit too late to change, though.
I used to not like lisp parentheses as well, but if you use something like paredit or smartparens packages from emacs it greatly enhances writing, navigating and editing lisp code. You never write all those parentheses yourself. With those packages IMO it's easier than editing scala code. I spend 10+ hours a week coding in scala and lisp each.
D'oh!
o rite
oh man not again...
An `IO String` is a string that needs to do IO to have a value. A `String` is one that doesn't. How would one convert a string that needs IO to one that doesn't?
If you *must learn* Haskell for your degree then is it for a set module that has a recommended text book? If so use the recommended textbook. Haskell in the wild is a large and complicated beast, learning it in two months by yourself without a structured path would be a big challenge.
&gt; I wonder if there is a strongly/**statically typed purely functional** language that is tailored to replace Python? No. There isn't. I'm sorry. stpfp is still kinda niche and most of the stuff in Haskell is there for a reason. I don't think people are going to support only a subset of the language just to serve as a stepping stone to a different language when stuff fails to scale (expressively). What you can do is restrict yourself to a subset of the language and use it where you need. For example, just using something like [turtle](http://hackage.haskell.org/package/turtle-1.2.8/docs/Turtle-Tutorial.html) to write shell scripts. A few different options is [teaching Haskell yourself](https://chadaustin.me/2016/06/the-story-of-haskell-at-imvu/), direct people to quality resources, start small by just using something like `turtle` for simple tasks or simply choose a different language which will give you a different set of features that your coworkers might want. Besides, you still haven't told us *what* makes Haskell scary IYO so we can try making it less scary. Even if it might not help when there is prejudice, it might help beginners who want to learn.
Absolutely true. An example of that is the article of this link below. I invite the people that practice the sport of negative voting without commenting to take a look: https://www.reddit.com/r/haskell/comments/4n648f/distributed_programming_in_haskell/d4e4od9 And think if attitudes like these really contribute to the success of Haskell or simply are self-promotion using last time occurrences at the expense of the Haskell language. That such destruction of the reputation of haskell and cloud haskell for real cloud computing by sinking it under a pile of foam received almost 100 votes is disgusting for me. 
There is a tag on stackoverflow http://stackoverflow.com/questions/tagged/servant
Perhaps, but it's far from a catch-all. Consider a hypothetical beginner trying to understand `map`. getPrompt :: String -&gt; IO String getPrompt x = do putStr (x ++ " ") getLine main = do let prompts = map getPrompt ["foo?", "bar?"] return (map putStrLn prompts) Error message: map.hs:8:24: Couldn't match type ‘IO String’ with ‘[Char]’ I can't figure where to put the `&lt;-` inside the `do` to make it all better. :P
I think that's actually [this question](https://www.reddit.com/r/haskell/comments/4ooive/annoy_rhaskell_in_one_sentence/d4ebxfl).
Doesn't GHC [often] optimize away the intermediate data structures in Generics? I wonder how well in practice.
If you are lucky, the SMT solver will recognise the structure and employ an LP (or MILP) solver as a theory. But if you actually know that the problem at hand is LP, going via SMT seems unnecessary. I understand that this may still be a good idea (engineering-wise) if the SMT bindings are already in good shape -- I never used them. I am not very familiar with z3, but I think Yices includes an LP solver as a theory.
eh, laziness complicates debugging. if the expression (no statements) isn't forced, it won't be printed. 
to be fair, it is. i'd like: {-# TOTAL head' #-} head' = head to not "totality-check". 
Yep! `Servant.Server.Experimental.Auth` saves you from having to implement your own combinator and `route` method. There is some wiring you need to do, but at a high level most of what you do is implement a function `Request -&gt; ExceptT ServantErr IO a` (an `AuthHandler`) and then wire everything up. WRT the above example: * `AuthHandler`: https://hackage.haskell.org/package/servant-server-0.7.1/docs/Servant-Server-Experimental-Auth.html is implemented here https://github.com/myfreeweb/sweetroll/commit/8103812a3833789b8fa12488a857ec20f52ccffd#diff-b037ab9af7336c9fa8e606e3a9a5e589R32 * wiring (type instances): https://github.com/myfreeweb/sweetroll/commit/8103812a3833789b8fa12488a857ec20f52ccffd#diff-b037ab9af7336c9fa8e606e3a9a5e589R26 * wiring (context): https://github.com/myfreeweb/sweetroll/commit/8103812a3833789b8fa12488a857ec20f52ccffd#diff-5f5e3251524f4ca40f29cd56e7cd6f5eR116 Super stoked to see people using this!
Simplifier usually does good job here and eliminates intermediate steps -- but not always. There's no guarantee that all your intermediate Generics data will be vanished in the generated code. This also means that you either have crazy compile times (when GHC is able to optimize Generics away) or runtimes (when it cannot) ;-) 
the reference of "kqr's name" is updated with the new referenece to the new name.
Well, yes. But I use it to see when exactly the expression is being forced.
i hate this! `x == null` can be `True`, `x == undefined` is always itself `undefined`. it's infectious.
Actually you are right.
&gt; you don't need a `subtract` function between two points, Yes you do. &gt; you need a `directionFromTo :: Point -&gt; Point -&gt; Direction`. If by "direction" you mean "Vector" and by "directionFromTo" you mean "subtract". [Torsors, torsors everywhere](http://math.ucr.edu/home/baez/torsors.html). 
Thanks. I now know what EndoFunctors are.
*Eratosthenes
I use TDD with 100% test coverage so type systems are unnecessary
looks like z3 does :) https://github.com/Z3Prover/z3/tree/96e157e201308d7beba556f62b145edc9263d0a7/src/math/simplex looks pretty basic though :) 
Best option (IMHO): http://hackage.haskell.org/package/derive
Yeah that would be convenient. That way I can just throw `{-# TOTAL main #-}` on all my programs and never have to worry about errors.
Thanks for sharing the question. Here is my take on it. https://gist.github.com/huseyinyilmaz/9f1139c2e2719b55160fd111270dfb88 
Except that in Haskell it is actually true, not some random noise that appears from type coercion.
Cabal Hell is very, very real T_T
/r/DnD as well.
Your variable names suck. It's pretty hard to forget what, say, ball.centerpoint is (it's a point, specifically the point at the center of a ball.) If you give things terrible names in haskell a mess... what is a Qx? Hell if I know. The fact that the compiler won't let me treat it as an Hd doesn't really help.
It's actually consistent, just different. Those functions that go into the same place an imperative construction would go get the same name (even if their meaning is nearly the opposite, like return.) Monadic functions usually come with a variant ending in _ that throws the results away.
The one is funny :p
A couple of things: *Add type signatures to your functions and use less general types* Right now they have quite general (inferred) types and GHC might not be able to optimize away the overhead of the dictionary passing that implies. Example of current type: ``` orbits :: (Eq a1, Num a, Num a1, Ord a) =&gt; a1 -&gt; a -&gt; a -&gt; a -&gt; a1 ``` You want to mention `Int`, `Double`, etc explicitly in the type signatures. *All function arguments need to be strict* Currently there's lots of boxing going on. Add a bang to every function argument. *divMod might be slow* If the divisor isn't statically known the compiler will add an unnecessary branch.
I recently stumbled upon this old SO answer, and I think it's actually still pretty good end-to-end advice. http://stackoverflow.com/questions/1012573/getting-started-with-haskell In terms of new stuff since it was written, Stephen Diehl's list (more of a fantastic reference than a step-by-step guide IMO) and the new Haskell Book are good additions to look at.
It's also very, very rare these days. The term refers to a specific set of circumstances that used to come up quite often before. Beginners use it any time they fail at Cabal. It's like someone mistyping `pip froze` and calling it "pip hell". No, it's just a typo.
Which points out how important it is to separate identity from value!
If you basically write the types of values in the name of variables, what benefits do you get from dynamic typing over static typing?
This is why you should also have tests to verify your logic is correct. Static typing gives you a lot of good stuff. It helps you filter out bad values, it gives you guarantees, it guides you in development, it serves as a kind of documentation and api, but it's not magical. Want to verify your logic? Simply use tests :)
Yeah, but let's say that I want to make a new ball. What do I pass the constructor for the centerpoint? Is it an (x, y, z) tuple? Or it a Point object? Maybe either works. Or maybe it has to be a PhysicalPoint (which derives from Point)... And the docs just describe it as "the point at the center of the ball".
Yeah no problem. I just don't have experience with dynamically typed languages so I was interested. Basically if there is no benefit then static typing in strictly superior in that specific case, as you get a lot of help when refactoring. For example, if you wanted to change centerpoint to something else like centerpoint3d you'd have to change the name, then find all the instances where it was used and change the name there, and if it was put in a variable you'd have to change that too and start tracking that as well, and you get no help whatsoever. And if you miss something your program will fail at runtime (someday, somewhere, maybe). With static typing the program will not compile and will tell you where changes are needed. These changes can, more often than not, be done mechanically without much thought. I'm sure there are other aspects where dynamic typing brings value, but this doesn't look like a good system to work with IMO.
The referenced github issue contains the comment &gt; I think it's the `tryAny` in `Data.Yaml.Internal.decodeHelper_`, which uses `waitCatch`, inside of decode's `unsafePerformIO`. Plus, you know, laziness and stuff. And sure enough, there's `unsafePerformIO`s involved in the code: encode :: ToJSON a =&gt; a -&gt; ByteString encode obj = unsafePerformIO $ runResourceT $ CL.sourceList (objToEvents $ toJSON obj) C.$$ Y.encode decode :: FromJSON a =&gt; ByteString -&gt; Maybe a decode bs = unsafePerformIO $ either (const Nothing) id &lt;$&gt; decodeHelper_ (Y.decode bs) decodeEither :: FromJSON a =&gt; ByteString -&gt; Either String a decodeEither bs = unsafePerformIO $ either (Left . prettyPrintParseException) id &lt;$&gt; decodeHelper (Y.decode bs) can somebody ELI5 me why those defintions need no `NOINLINE`s and if those `unsafePerformIO`s can be removed? If they can't be removed, can the scope of `unsafePerformIO` be reduced or is there no benefit to keep `unsafePerformIO` scopes as small as possible?
Have you read any of the code? This is really a non-issue.
Indeed. And even then, with `NOINLINE` but in a concurrent setting, `someByteString` might still be evaluated twice. Unless one passes `-feager-blackholing`: https://downloads.haskell.org/~ghc/7.10.3/docs/html/users_guide/using-smp.html. And even with that flag, I'm really not sure this will *guarantee* single evaluation.
Why did you pick the name "Otoke"? My first reaction was the korean word 어떻게 which means basically "What to do?"
Now it's starting to make sense to me, thanks! So... using `unsafePerformIO` as a way to aggregate actions which from the outside appear referentially transparent sounds a lot like what `runST` is for, no? Would there be any benefit to using `runST` rather than `unsafePerformIO`?
TIL about eager blackholing, and that `unsafePerformIO` isn't as non-dupable as I thought. Thanks for the link.
This is a great suggestion, and what I'm planning on going with, thank you for making it! It has minimal impact on users and allows for a gradual transition away from the current behavior. I'll likely be marking the `IsString` and `Default` instances as deprecated as well, and in a future major revision update them to use the new behavior.
Correct, but it's preventing that double evaluation via blockholing, which without eager-blackholing may not work correctly if there are two separate threads running concurrently both accessing the same value. At least, that's my understanding based on learning on this feature 37 minutes ago :)
TIL that `unsafePerformIO` incurs a thread synchronization cost. So if `NOINLINE` is fine then you probably (but not always!) want `unsafeDupablePerformIO`.
Right, at least that would explain why it's a primop. But reading the comments in the file Yuras pointed to, I'm getting the feeling the "noDuplicate" thing is a pretty strong guarantee. So either there's a documentation bug lurking here, or `unsafePerformIO` somehow implies eager blackholing for that particular thunk. Either way, even if it's just implemented using blackholing, it might still not be safe unless blackholing adds a write barrier. I don't know if that's the case, hence my comment earlier about how even eager black holing might not be give you a guarantee.
Aha! Reading https://github.com/ghc/ghc/blob/master/rts/PrimOps.cmm#L2242, we see that `noDuplicate#` ultimately acquires a lock, via `threadPause`. So yeah, there is synchronization going on and there's more to `noDuplicate#` than just checking the blackhole bit of the thunk. I assumed it was just doing the blackhole checking.
You don't need to reflect servant version in your package's version, there are `build-depends`for that (which are visible on Hackage page!).
I ended up writing a quick bash script which took a package name (with autocompletion), used `ghc-pkg field &lt;package&gt; haddock-html` to find the documentation location, and open a web browser at that location.
No, even eager blackholing doesn't prevent double evaluation (there is a small race window). But `noDuplicate#` acquires [an exclusive lock](https://github.com/ghc/ghc/blob/c88f31a08943764217b69adb1085ba423c9bcf91/rts/PrimOps.cmm#L2252) when setting blackhole up, so double evaluating is impossible any more.
You will probably be interested in this corner case too: https://ghc.haskell.org/trac/ghc/ticket/11760 It is not directly related, but it is enlightening. 
For mine and others knowledge, could you set up a case that illustrates this? This doesn't seem obvious to me.
&gt; How \[Result&lt;A, std::io::Error&gt;\] is different from IO a where SomeException works as std::io::Error There is a fundamental difference between unchecked exceptions in any language and Result types in rust. In rust result types cannot compile until either you deal with them, or explicitly and verbosely ignore them. In rust you will either have caught every possible error at compile time or you will have a bunch of unwraps in your code that you can see quite plainly in your code. In haskell or java or any other language that uses unchecked exceptions you can have code that looks correct, compiles flawlessly, but could blow up. Uncaught unchecked exceptions only error at run time, which is too late! &gt; Do you know all ways read way fail? I don't care what the docs say. I cannot use read without acknowledging its result type. Once I have its error I can access its errorkind if I want, or I can just merge all of the ways it could have errored into my own error type. I could in fact nest the io::Error into my own type so that code further down can know the root of the problem. No information is lost, no surprises to be had.
I have attempted to keep version numbers synced between packages but I've always regretted it.
Is the forking trick still necessary now that we have `SomeAsyncException`? I often use `try`, check if I caught `SomeAsyncException` and rethrow, otherwise I return `Left e`, where `e :: SomeException`. For example, do mentry &lt;- try (liftHandlerT (runExceptT m)) case mentry of Right (Right entry) -&gt; fmap (entry :) (worker jobs done) Right (Left failure) -&gt; do logErrorNS "uploadZipToS3/worker" failure worker jobs done Left unexpectedFailure -&gt; case fromException unexpectedFailure of Just (SomeAsyncException{}) -&gt; throwM unexpectedFailure Nothing -&gt; do logErrorNS "uploadZipToS3/worker" ("Unexpected failure when forming ZipEntry: " ++ tshow unexpectedFailure) worker jobs done 
I know. I was just following suit with the other servant packages and being hopeful =P
Awesome! Thanks! Should I drop a link to the YouTube video anywhere in the proposal? There doesn't seem to be a particularly fitting spot for it.
That's exactly what my new work on exceptions is about: leveraging the type based approach instead. The problem with relying on types is that there's nothing that enforces it: you can `throwIO` a `SomeAsyncException` or `throwTo` an `IOException`, and nothing in `Control.Exception` will stop you. I'll probably put a link to my prototype library on Twitter in the next few days, and after getting some feedback write up a blogpost about it. FWIW, with these kinds of cases I find ViewPatterns to be a nice way to avoid inward-creeping indentation: Left e@(fromException -&gt; Just SomeAsyncException{}) -&gt; throwM e
You mean `-feager-blackholing` compiler flag? Yes, it is safe from duplicate evaluation without the flag. And after rereading the code I think `unsafePerformIO` doesn't use blackholing at all, it just locks the thunk directly. (As usual, it is just my understanding of the `GHC` internals. Don't blame me in case I'm wrong :) )
Yes it's based on SPJ / Eber paper on Financial Contracts. Their combinator approach doesn't permit arbitrary contracts though 
[`snap`](http://hackage.haskell.org/package/snap) and [`snap-core`](http://hackage.haskell.org/package/snap-core) have been on different version numbers for awhile now and it hasn't caused us any problems. If you put proper version bounds on your dependencies, then it shouldn't matter if your version numbers diverge. The version bounds always serve to document what's going on. 
I agree with you, it's very problematic. In defense of `Control.Exception`, there was historically no such thing as `SomeAsyncException` (and there still isn't an `IsAsyncException`). Also, with the current setup, there's no way to prevent synchronously throwing an async exception. My workaround for this is to have a wrapper for `throwTo` which detects if the given type is synchronous, and if it is, wraps it up in another datatype which is asynchronous.
A footnote in the description should be fine.
&gt; Author: simonpj !
&gt; `throwTo` I think we should just get rid of async exceptions. That "solves" the problem. ;)
Our brains are meant to webscale.
Does [this](http://www.well-typed.com/blog/2015/07/checked-exceptions/) checked-exceptions machinery perhaps have a place in your re-imagined exceptions API? I don't think it's made its way to Hackage yet, but it is inside hackage-security [here](https://github.com/well-typed/hackage-security/blob/master/hackage-security/src/Hackage/Security/Util/Checked.hs).
Are there any core or close-to-core developers willing to set up a Patreon and use it for income to work on GHC and its surrounding infrastructure? More importantly, how many people are willing to contribute to such a thing? I'd happily throw in $10/mo.
Whatever the platform used, as long as it is simple and easy for spontaneous contribution! (please no bank/account numbers, or boring forms to fill out, or strange, insecure-looking web sites to enter payment details into)
Seconded.
Relevant: http://lukeplant.me.uk/blog/posts/why-learning-haskell-python-makes-you-a-worse-programmer/
This is a great idea! 
What's a good (secure/reliable) way to pay one or more salaries from distributed sources that does not rely on banks ?
When working with tightly coupled/related packages (which could be one, but for sake of external dependencies or something else are split into many) one would like to - be able to easily to make cross package changes - have separate versioning of each package And then one obviously want to have source version control in place. So the options are: - monorepository - easy to make cross package changes - versioning is cumbersome. Cannot have tags for each version, one solution is to have `foo-vA.B.C.D` and `bar-vU.X.Y.Z` tags. Also history is polluted with not-so-relevant information. - separate repositories - more difficult to make atomic changes. In GitHub flow that would result into cascade of pull requests - git submodules or stack.yaml with git locations bringing separate repositories together helps a bit. - obviously repositories are separate and versions are easier to track. And the choice depends on what's more important at the moment. In the early stages development of package collection, the easiness of making cross package changes is probably more important than precise version information. Iterations mosty likely introduce breaking changes all over the place anyway. Yet when collection matures changes become more local, so probably separate repositories would serve purpose better. Also "shaving-off" packages on the boundaries of the dependency tree (whether they are "leaves" or "root"), like is done with `servant-cassava`, `servant-blaze` and `servant-lucid`, is an option: at least the part of API used by them seems to be quite stable already. 
Have a look at [SBV](http://leventerkok.github.io/sbv/).
While I don't want to discourage the idea, I think you need to be rather flexible in what the benefit is. If the model is just "pay someone to work on GHC", then it's tricky. Suppose, for instance, you raise $1000 / month from various people. Great! But it's not going to pay someone's full-time salary. And if you're looking for someone who has another full-time job, then offering these people a part-time salary for GHC work probably won't work. There are legal issues with employment agreements and non-competition clauses. Aside from that, money probably isn't the main impediment to their contributions anyway! If you compromise on who you're looking for, you might be able to have a half-time student, but then it's not clear if the time demands on people like SPJ go up, or down!
It could, I think it's orthogonal to what I care about. I'm worried about the current situation of Haskell runtime exceptions, which are unchecked. I'm focused on making those work coherently and make it easy to get right. Explicit/checked exceptions is clearly something many people are very interested in doing, and Edsko's blog post demonstrates a means to get these two worlds to somewhat coexist.
Right, the ideal person for something like this is someone who does contract work to make ends meet, not someone with another full-time job.
Was on /r/math as well, it all started on /r/cars.
I hope you're not serious!
It didn't say annoy the internet in one sentence. 
I am, though I know it won't happen in GHC. Asynchronous exceptions were a bad idea to begin with, and other languages with them have encountered even more pain than Haskell. I have "solutions" for all the motivations behind asynchronous exceptions.
I would imagine that is far more error-prone than simply using Debug.Trace.
I think /u/frublox is saying that it's bad when the error type is `SomeException`. If it's something like `ParseError` or `e`, then you can't just replace it with `throw`/`throwIO`.
There are other downsides to `ExceptT e IO` besides using `SomeException`. [This post](https://www.schoolofhaskell.com/user/commercial/content/exceptions-best-practices) does a better job of explaining it, IMO.
You don't need to use `MonadThrow` or the `exceptions` package, really. The usual `Control.Exception` and `IO` will also work.
&gt; Both...insist on ticket numbers of the form #NNNN. It's worth noting that GitHub lets you write "GH-NNNN" as well.
[removed]
A lot of the work is not glamorous and boring and really tedious, which is one thing to keep in mind. It requires insight and some cat herding, but those can be taught. But, if you want to crowd source things, I think a better idea is to stick to driving initiatives for real deliverables, rather than open ended "pay the developer" funds. It's much easier to convince people of, and I think has a lot of benefits (including more money, going to a wider amount of people). An example is technical documentation, mentioned in the blog. Rather than fund like $500 a month for some seemingly infinite amount of time to get some open-ended stuff done, it'd probably be better to allocate like, 2-3k or something and say "We'll pay out for someone to do dedicated work on our technical documentation for 2-3 weeks". Some benefits: - Much more clear value add and deliverable schedule. I have no idea if the $500/mo I get is going to the things I really want, or anything I want at all, or if the people even have time that month on such thin margins. - Selects for more relevant criteria from a wider pool (Can this person write well? Do they have a track record as a contributor to other projects? Do we have faith on delivering in that timeframe, at that cost?) You don't need to be a really good Haskell programmer to do this task. If we only select for GHC experts, the money will keep going to the same places. That's maybe OK, but I'd personally prefer to spend it in ways that can also help educate and expand the hiring pool for more inexperienced people (Haskell jobs face so much competition that inexperienced people *very easily* get priced out, due to no fault of their own, except maybe "I didn't spend every weekend writing Hackage libraries to prove I'm great") - Related to #1: Much easier to estimate costs and development velocity. If you pay people $500/mo, that's only like 4-6 months until you would have paid them $3k anyway. But there's no guarantee that the development velocity is as good - you may spread out a *lot* of things on that $500/mo, leading to very little time on "Technical Documentation rewrite". Maybe that person got married during month 3 and took more time than you thought. Think about why people dislike it when things like single-licensed software becomes SaaS: part of it is that SaaS drives revenue *without* giving a clear schedule or development velocity. Will that $500/mo be a waste this month, or the greatest $500/mo you ever spent? It's a toss up, from month-to-month, because it's unclear how the money is allocated. With licensed software, value-add is part of the *deal* because if I upgrade my license (read: fork out the cash), it better be worth it. In general I think something like this could work for specific things. The amount of money in this area is slim and it's not coincidental; almost any Haskell programmer or Haskell company can add more value to their *core products* by doing anything else vs working on GHC. You can add more value as a janitor probably; it's not "economically sensible" to invest lots in PL development.
&gt;Besides, you still haven't told us what makes Haskell scary IYO so we can try making it less scary. It's not so easy to make Haskell less scary. Better tools (e.g., stack or a better IDE) will help. [Better compiler messages](http://elm-lang.org/blog/compiler-errors-for-humans) is definitely helpful. One major scary part about Haskell is its terminology. This cannot be easily changed. Monads and arrows are propagated through the whole library and documentation. Whatever library you want to use and whatever book you start studying tells you about those terms. See Elm's terminology to see what I mean. After all, I am not sure if making the language less scary for others is the best idea. Haskell is the language of elites. It is pioneer in many area. Haskell should stay Haskell. We don't really want to make Haskell another Python. That's why I am asking for another language that borrows most of the goodies from Haskell, but is not as scary. This language does not have to be the pioneer or the language of elites. It needs to be focused on practicality and simplicity. 
&gt; SimonPJ spends about half his time on GHC or GHC-related things, so that counts. I find it rather difficult to evaluate the nature of academics' contributions to software development. We have a relatively similar situation in the OCaml community, with historically a core kernel of academic maintainers and other non-academic maintainers -- plus external contributors of all kinds. Some questions one may ask to evaluate whether someone counts as full-time worker or part-time worker or volunteer is: is contributing to the maintenance what this person is *paid* to do? Is it part of the professional recognition that they get? In my experience, in the case of academics the answer is "essentially no" to those questions (I don't know about the specifics of the situation between Simon Peyton-Jones and MSR, of course.) On an abstract level, everyone inside the academic community knows that Simon Peyton-Jones (or other academic contributors to open-source projects, such as Xavier Leroy and Damien Doligez in the OCaml community) is doing a great job on GHC maintenance and has been doing so for years, but at the end of the day you will have a hard time finding a research institution that clearly states "part of your job as a researcher is to develop and maintain software on the long term" (I know of none), and of academic panels that recognize reward in a non-marginal way such effort. I think that many academics thus think of their own involvement as a volunteer effort: they are not doing this "as part of their job", but *aside*. They may in fact be working on it during their work-time (however that is defined), but they perceive it as a personal indulgence, they do it because it's fun, that delays some other "real work" that needs to also be done -- and this mixture of recognized-real and voluntary work tends to sprawl well outside any well-defined notion of "work time". There may be some guilt coming with this activity -- when it delays or slows research output, or is decided over other obligations such as meeting submission deadlines, sending reviews on time, etc -- along of course the pleasure and interest of contributing to an exciting project. (Well just like any other significant open-source contribution, no matter what the professional role of the contributor, there is guilt coming both ways: when we work on the project instead of something we should prioritize, or when we don't work on the project and other contributors or users then face problems that we could solve. ) If Simon PJ suddenly stopped any involvement in the maintenance of the GHC codebase, but kept helping along with the design of Haskell language features -- with the accompanying stream of publications documenting that process -- would people really notice? Inside the Haskell community, some may notice (although it's a testament to its strength and diversity that many actually wouldn't), but among, say, the people that evaluate Microsoft Research, or the researchers that attend ICFP or related conferences, would that impact how they perceive SPJ in a significant way? If the answer to that question has reasonable chances to be "no", then it probably means that the "volunteer" status is closer to the nature of this contribution than any full-time or part-time label. In the end I think the strongest indication of how SimonPJ perceives is own contribution status is his own words in the blog post, which rather clearly indicate volunteering situation -- rather than a half-time duty.
&gt; With something like 2 hours daily I would bet that in a week you will feel pretty confident using Haskell. You must be joking? I had been a full time Haskell developer for over a year and I am not sure if I can call myself confident using Haskell yet! I have a PhD in computer science by the way.
&gt; If you don't mind OO ... My question is specifically about "strongly/statically typed purely functional languages". `PureScript` is exactly what I am not looking for. It tries to adapt almost a full-fleged Haskell to web envoronment, including many of the ghc extensions. In contrast, Elm tries to do the same in a simpler manner. 
&gt; Why not create a concrete proposal... Can't tell if mailing list trolls have moved to reddit or this is sincere :P
Interesting. I've messed around with this general approach with two experimental (and very unfinished) projects: - https://github.com/jgm/grammata - https://github.com/jgm/HeX I still like the idea of using Haskell to define macros with typed arguments. 
This is sincere. I don't follow the mailing list closely. Have you posted one there? I've read your blog post. I've read Simon's reaction. I haven't seen a concrete proposal, but many ideas that could form a proposal.
Appreciation: confirmed. SPJ continues to be a class act.
Under those conditions I am not even a python programmer so forgive me for being so naive
The actual hard part is coming up with enough money to just pay people to document stuff.
My async exception story isn't far off this one - all tracked in https://ghc.haskell.org/trac/ghc/ticket/10793. My workaround is at https://github.com/ndmitchell/shake/blob/master/src/Development/Shake/Internal/Core/Pool.hs#L183, which is somewhat in the same spirit as yours, although in this case GHC could do better.
Here's the problem. I could conceivably help people get started with GHC dev. I'm already sponsoring some (non-GHC) open source Haskell development (out of my personal income, I work full-time) and my coauthor and I sponsored the new Haskell.org webpage as well. I'm not _going to_ until they show more willingness to make the development process more accessible. I used to be an avid Phabricator user, but I was using it in a private startup where I knew I could onboard new developers in person. I would never use it in an open source project that is this labor-starved. Until they show more give on that and peripheral issues with GHC dev, I can't justify throwing new people to the wolves like that. It'd be worthwhile for them to show more openness to using Github just a signal.
You write: " I feel like I'm up against a stone wall of unwillingness to admit responsibility." But Simon writes: "None of this is an argument for the status quo. Simon, Ben, Austin, Herbert, and I have been talking about a rather more systematic process for new features. We'd like to learn from experience elsewhere, rather than reinvent the wheel, such as the ​Rust process. Please suggest processes that you have seen working well elsewhere." So I think that makes clear a significant openness to better ideas on how to shepherd proposals. 
Then why did he not actually address any of the specific items I laid out in actual bullet points on my blog post? If Simon and Simon are blockers, who does he think is going to make such decisions? I also backed up that opening comment of mine with several instances of what I described. Did you read them? EDIT: To clarify, to me taking responsibility involves action. Others may not see it that way; if you don't like the word responsibility, then substitute your preferred word for doing something because you are in a position to have an effect.
I don't think what you're saying makes sense. Given that what he said in the blog post contrasts with the reality he participates in, how can I have faith that the work they are engaged in is directed in a way I advocate for? I know he means well, but am I to ignore what he says in favor of my faith in his good intentions? If I say nothing now, and the next step is to continue the good work they've done on getting widespread buy-in on new features, for example, we will have gained precisely nothing because that is not actually something GHC devs do even if Simon thinks it is.
&gt; Is there some fundamental reason we can't have a GHC Generics-like API for generating code at compile time? Can you be a bit more precise about what you're wanting here? I agree that it would be nice to have the ease-of-use of GHC generics combined with the speed of TH-generated code, but it's not obvious at all to me as to how one would accomplish this. After all, we need TH for compile-time compilation because Haskell isn't a homoiconic language like Lisp, so how would you propose declaring what code should be created at compile-time?
&gt; [I want to] give you the opportunity to be more humble ...via a steady stream of bitter, self-aggrandizing "*I'm* one of the good ones!" snark.
Monads are like horses. If you're in a horse, and that horse is in another horse -- you're still in a horse!
Sounds like a tutorial for infinities.
When a database detects a deadlock, typically it selects only 1 or a number fewer than all of the deadlocked processes to terminate. Why couldn't GHC do something similar?
Paying for improving documentation is a great idea. There are many graduate students who are working on/with several Haskell libraries. We can pay them for improving documentation on the parts of haskell they are familiar with (and then it can be checked/edited by the community at large). It will also be a lot cheaper.
Do you have a proposal for a typeclass more useful than Traversable?
You are pointing at the single most annoying misfeature of Haskell (or rather its standard Prelude). I have lost count of the instances in which I have been deterred from experimenting with different container types and implementations because the current requirement of either giving functions with identical semantics different names or requiring qualified imports. The additional effort of memorizing the various variant names and/or fixing every single instance of use is just too much trouble too often. What should be no more effort than changing a single 'type' line instead requires editing half the lines in the module. The worst part of this design error is that it is that it would have been so easily avoidable: Just put more the generic functions into type-classes (and/or make use of type-classes), either existing ones like FTP (where that makes sense) or new ones, if necessary. And, no, this does not necessarily result in performance degradation. If you use INLINE, SPECIALIZE, or (in the worst case) RULE pragmas, the compiler can almost always figure out the correct dispatch at compile time.
One of the popular typeclasses for dealing with this is [mono-traversable](http://hackage.haskell.org/package/mono-traversable-0.10.2/docs/Data-Containers.html), but I would like to mention that ad-hoc typeclasses to provide functionality without laws might actually lead to less maintainable code in the long run. Because typeclasses are global and magic, they actually very easily lead to unmaintainable code that's more difficult to reason with. Typeclasses excel when you can write polymorphic code that you can re-use, but when you write polymophic code, you actually have to be able to reason with them and think about what they do. You have to have laws...or else your polymorphic code really has no meaning at all. Writing Monad-polymorphic code or Monoid-polymorphic code helps you reason about your code and gives you stronger confidence on the behavior of your functions, and even aid in type safety. Writing container-polymorphic code in a typeclass with no laws means that your functions really literally don't mean anything. You can't reason at *all* with what your polymorphic functions are doing. There are no guaruntees on the semantics or how different methods interact. I think it makes more sense to write monomorphic code, because every container's methods have their own semantics and meaning. It's really impossible to reason with "inserts", "removals", etc. in *general*. They all interact in different ways for different containers. Using the specific function for that container makes more sense and gives you more clues to tell you what the semantics of what you're writing with means. It gives you more readable and maintainable code that is easier to reason with and refactor.
Here's my perspective on the matter. You observe some frustrating dissonance between Simon's stated criteria for when a feature goes in, and what actually is observed to happen for actual GHC development. But I think it boils down to the fact that SPJ is not the final gatekeeper: by in large, changes to GHC are self-policed by those with commit bits. I think we all aspire to the standards of feature-ship that SPJ espouses, but we are not all Simon, and we make mistakes. ;) SPJ very rarely vetoes feature commits which are actually pushed to master. I went and searched through my mail history this sort of thing. Here is a specific case of this happening: https://ghc.haskell.org/trac/ghc/ticket/9793 and there is also a related near miss: http://comments.gmane.org/gmane.comp.lang.haskell.ghc.devel/10097 . Actually, as far as I can tell, all the other SPJ reverts this year were just because someone pushed code that was just wrong (e.g., broke validate). I think there's definitely room for "the rest of us" to rise to SPJ's standard. Let me point out one personal example where I think the process "worked" (it depends on what you think of as working). In https://ghc.haskell.org/trac/ghc/ticket/10117 Snoyman identified a problem with GHC's behavior and called attention to it as important. I agreed it was important, and then I spent the rest of my afternoon crafting an alternate proposal. I posted it and tweeted about it. SPJ eventually came by and asked, in the usual mysterious way, what the "principle" of the design was, and what exactly is "refined". When I eventually came back around to this again, I spent another full afternoon pondering what exactly was meant here... and eventually came up with the final form of the proposal, which was far simpler than what I had originally proposed. Quite a lot of cycles were spent... and this is just for warnings behavior, and I haven't even implemented it yet. And to this day, SPJ is the only one who has given substantive feedback on the proposal. I can't imagine what the burden is for bigger more complicated features. (Or maybe I can: c.f., Backpack, which has been in development for TWO YEARS, and I'm still trying to convince Simon it's simple enough to put into GHC.) Or, maybe, a PhD student with commit bits will just push it, and that will be that. What do you think is more likely? So what does this mean? Maybe what this means is that GHC has gotten too big for "all people with commit bits have unilateral access to master", and we need a different development style. (I might mention the Linux kernel development model, with lieutenants, not because I know anything about it, but because I've heard about it.) Or maybe what GHC needs is a competitor, the same way that clang was the best thing to happen to gcc. I don't know! It is all very frustrating and I do not know the slightest what to do about it. But here we are.
I believe the logic here is that doing so would be nondeterministic, and having programs fail differently for each run is far worse than reliably doing something we don't like.
&gt; An oft repeated explanation of gcc's apparent development doldrums before clang was that those with stewardship were deliberately sabotaging a potential surrounding ecosystem in order to maintain personal control for ideological reasons. I think it's a real shame if that is the direction the GHC team is going in. Oh c'mon. You _know_ that's not what Edward was suggesting. :-) 
+1 qualifying is seriously under rated.
I usually shorten the qualifier: import qualified Data.Set as Set
Get `text` into `base`, for starters
Wouldn't it be nice if we could just write (with equivalent semantics): import Set Agda got the syntax right for modules as far as I'm concerned. Unqualified import would be open import Set And we'd do away with the prefixes.
How, may I ask? A quick grep (lines with `String` vs. lines with `-&gt;`) estimates 20% of types in GHC to involve `String`.
Boy, I'm sure glad that all the Maps have a unified interface now. This solves everything!
Should we look at how `swift` deals with strings? Deal with grapheme clusters, and abstract the concrete representation away from any user facing code, unless explicitly requested? 
Your notes about the `StringLike` and `ListLike` classes make sense. My intuition is that if the rewrite rule system is ever amped up ([#9601](https://ghc.haskell.org/trac/ghc/ticket/9601)) enough of those problems could be solved that it'd at least be faster than `fromString/toString` all over the place. For (2), what about `t[original_name]` and the like to distinguish `ByteString` from `Text`? A lighter-weight version might be one "regular" version of a function and one generic. Or maybe just make a `Module.Text` for `Text`, etc. as is often currently done.
&gt; I don't think anybody is doing a great job at cataloguing specs and design debates, and this internal clutter becomes a thorny shell for outsiders. Agreed there -- even trying to reconstruct the LC story was difficult. I think the idea of what's called a "rust-like" process isn't bad. (although I think the lineage probably goes back through PIPs in python and SRFIs in scheme. I like HIP as an acronym if we go that way). So the "wiki source is definitive" doesn't seem bad to me, but there needs to be a place for attendant chatter and argument as well -- maybe using the same wiki more like in the old ward's wiki style or nlab style with a place for arguments and disagreements is ok. On the other hand, this is a place where a git-style interface shines so you can have PRs with attendant "meta-argumentation" against markdown docs, and effectively more granularity throughout. But that's just the refinement phase. A better mechanism for final vote tallies (even if they're only informational and core gets the final say, which I'd lean towards) is also important I think as we scale up. But I also worry about uneven distribution of polls and the question of people with "skin in the game" vs. passerby in this regard. Perhaps the same sort of formal structure we've moved towards elsewhere would help -- a designated but _known_ "in-group" (in this case those with commit bit) who have the decisive say amongst themselves, but a wide set of "temperature taking mechanisms" beyond that.
https://jaspervdj.be/posts/2011-08-19-text-utf8-the-aftermath.html
[removed]
I doubt that this would aggrandize me. I'm as bitter as the reality of the haskell community is, and this is the only way to break the circle of auto complacency. Truth makes you free.
&gt; SimonPJ spends about half his time on GHC or GHC-related things I'm quite curious, then, about what SPJ spends his time on that is not GHC-related.
I don't think any proposal improving the String/Text problem can be successful with current haskell. I do hope that backpack changes that. Of course it has to be around for quiet some time before it can even be considered honestly.
But how to discriminate between functions or constructors having the same name but different signature? E.g. `map`
I don't see how changing the import syntax has anything to do with that?
Have you tried running this micro-benchmark with optimization (i.e. compiling the programs with some non-zero level of optimization instead of running in ghci)?
&gt; Just to be painfully pedantic, I will outline one option: the wiki is central. Trac tickets with a proposal must link to the wiki, and the manual imports wiki content to ensure the wiki is up to date, Notes should be links to wiki pages. Sign-off on a design happens on the Wiki so it is not debated on Phab, nor revised on one of the mailing lists. I've tried this approach (or *very* similar, at least) in a medium-sized team (5-10 people) and it's just painful for everyone involved. Among other things it suddenly makes it extremely dangerous to do wiki maintenance such as moving pages around -- resulting in huge swaths of broken links, etc. The Notes (or equivalent) *must* stay in the code. EDIT: I would actually go the exact opposite direction: Specification-level documentation starts on the Wiki and it then *moved* to the code repository/reference documentation. Leave a pointer in the wiki, if desired, but no detailed specficiation-level documentation should remain on the wiki since it only adds ambiguity and confusion.
`:kind!` didn't work for me in GHC 7 either, fwiw.
@pdexter the problem is the filtering order, case1 perform n mod7 and n/7 mod3 but case2 perform n mod3 and n/3 mod7 (case1 is doing n+n/7 divisions when case2 is doing n+n/3 divisions)
Are your "solutions" real solutions that could be used? At this point I'm convinced that asynchronous exceptions are absolutely essential. For example, in the system I work on at Facebook we use allocation limit exceptions (rather like timeouts) to prevent runaway computations from degrading performance. You might say "just fix your runaway computations", but that's just not practical in a system as large as ours that is moving as rapidly. Multiple times we have had people accidentally create infinite loops in the codebase, these get caught by the allocation limits, and the worst that happens is that our monitoring shows some errors for some requests. It just isn't possible to do this without async exceptions, as far as I'm aware.
To be fair, Bryan was objecting to changing the API for Text, not to using Text in base APIs.
To me this seems like a problem far more general than just a few poor choices in Prelude. The core issue is being able to express logical structures in one place (`List Char`, `Nat`, etc) but attach an efficient (and possibly not 100% accurate!) interpretation of these structures when we compile. Do you have any thoughts on this in general?
Sometimes you get the more experiential version: "I never make mistakes that a type system would catch"
...and there's also the quite related `type FilePath = String` issue (and there was an attempt at a modest proposal for that one: https://ghc.haskell.org/trac/ghc/wiki/Proposal/AbstractFilePath) In the case `String` vs `Text` a basic question is wether to overload the few existing functions specific to `String` or whether to simply add new variants of those functions (I'm thinking of e.g. `putStrLn`) for the sake of backward compat.
Check transducers out. They provide a compostional interface for list operations. (http://hypirion.com/musings/haskell-transducers)
First of all, stop conflating ByteString into this. It's just about Text vs String vs Vector Char vs any other reasonable data structure for text. Use the right data structure for the job, etc. Is needing to convert to pass to others' libraries really the biggest problem? ByteString is pretty much universal vs the much less used [Word8], Vector Word8, etc. Still use the right tool for the job, but usually that's ByteString and most people use it.
One other thing. Even if you don't want to actually use string-conv in your code, the package is still useful because if you need to do a string conversion and you don't remember which function to use, you can just look in [the string-conv source](https://github.com/Soostone/string-conv/blob/master/Data/String/Conv.hs#L63) to find out! it serves as a convenient reference for all the conversions between the five common string types.
Now now. The suggestion on hand wasn't exactly fully baked. It consisted of a long indecipherable mailing list thread and a Trac ticket that nobody ever contributed to. I hope you'll agree that the bar ought to be higher than "Greg gets excited and makes an ill considered fragment of a suggestion for how someone else should spend their time".
For those going "Oh god, transducers, what the hell are these things anyway": http://beerendlauwers.be/posts/2015-12-09-transducers.html
&gt; this is a place where a git-style interface shines so you can have PRs with attendant "meta-argumentation" against markdown docs, and effectively more granularity throughout. Yeah, it'd be nice if the wiki was in a git repo people regularly checked out. &gt; A better mechanism for final vote tallies (even if they're only informational and core gets the final say, which I'd lean towards) is also important I think as we scale up. This is what I want, too. It would mean that the proposer wouldn't be so easily waved off by a committee saying, "There's no broad support," which, today, is an entirely meaningless phrase given the orders of magnitude different participation levels we see in different places. 
Well, for allocations, I would actually track what operations can allocate and have them work in `Maybe` / `Either` / something else. The same thing I would do with other actions that can fail. This is the worst motivation of the 4 given in the original paper. There's no reason you can't do this and still have a garbage collector.
Why not https://hackage.haskell.org/package/EdisonAPI-1.3/docs/Data-Edison-Assoc.html ?
I agree that important should be qualified by default. But, I don't think they should be aliased to their list name part ("Set" for "Data.Set"). The whole "Data." prefix should mostly be dropped, but that's a choice made by the hackage uploaders.
&gt; we absolutely must figure out a plan to fix the String/Text situation I don't think the "fix" is to deprecate either type though. There are definitely places where we currently use `String` were we should use either `Text` (`Monad.fail`)or `ByteString` (`FilePath`), but I think there are still good places for `String` (or at least `[Char]`) being used. The only thing worse that have 3 string types would be having one that is used for all 3 purposes, and therefore ends up being bad at all 3 roles.
I've not looked at their API, but your descrption sounds like what I think programmers want. The internal representation should probably be strict finger trees of fixed-grapheme-cluster-count chunks[1], with some special casing for small values. The API would reflect that, having only operations that can be done cheaply in that representation, and indexing would count grapheme clusters, not code points or bytes. Extracting code points would be am explicit transform to `Traversable f =&gt; f Char`, and extracting bytes should be through explicit encoding. [1] Or maybe just as RRB tree of grapheme clusters?
So, you don't believe in function extensionality? ;)
It seems like for optimality you'd have to not only have a good idea of the a priori chance a predicate "fires" but also pairwise correlations between all your predicates.
Can anyone please explain why we cannot have prim ops for String just like for other primitive types? Just bake the String type into the compiler, have efficient low-level implementation, and support the existing String API transparently. Then, we don't have any API migration problems and we get efficiency.
&gt; Extra points for seeing why that is slower than your case 2... It's faster isn't it? The reason being that the `mod 7` test excludes more numbers than the `mod 3` test, so fewer numbers are tested twice.
Whatever problems those conversion suites have, Haskell would benefit from having a sanctioned text format and conversion suite on base. This way, they would at least stop multiplying. (And yes, I'm guilty of that.)
Yeah. `CString` is implicitly encoded: newCString :: String -&gt; IO CString newCStringLen s = getForeignEncoding &gt;&gt;= flip GHC.newCStringLen s https://hackage.haskell.org/package/base-4.9.0.0/docs/src/Foreign.C.String.html#newCString 
I certainly don't disagree that the current situation is... very suboptimal. &gt; Your suggestion of moving the wiki content to the code, for instance, should be enforced in some way, perhaps by a garbage collecting spider that would raise a stink about pages with a design tag that have no link to the source tree, or that link is broken or not reciprocated (I.e. The linked source file doesn't link back to the wiki page) I think this one can be done pretty simply. Add a checklist to all new-feature code reviews: "[ ] Has all specification been moved from wiki to documentation?". Likewise add a checklist item for everything else: "[ ] Has specification been updated, if applicable?" (Not a GHC developer, just in case that wasn't clear.)
The "have efficient low-level implementation" seems to be the problem. Consider the `Nat` type which is implemented like `Nat = Zero | Succ Nat`. There's currently no way to do something like "represent this type as an Integer" in GHC and I think doing so might open its own can of worms.
I believe instance specific implementations of a "classy" function that's not in the class definition. For example, something like giving a lookup table for `fibonacci :: (Eq a, Num a) =&gt; a -&gt; a` when used with `Int` (because so few Fibonacci numbers fit into `Int`) but a more advanced function for `Integer`.
The way I see it, there will never be any meaningful competitors to the Text library and it's good enough, so there's no point holding out for a better solution. The only operation that is more efficient for Strings is cons, and even in that case I would still prefer a Builder over a String.
The [Core Libraries Committee](https://wiki.haskell.org/Core_Libraries_Committee).
The instances in that package for `String -&gt; ByteString` using `ByteString.Char8` are not smart, they snip each char to 8 bits. It should instead go `String -&gt; Text -&gt; ByteString`.
Green-field and brown-field are probably what they're referencing.
Green as in green field development, i.e. starting from scratch without legacy. Brown is the opposite: working in an already existing code base.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/annoyinonesentence] [\/r\/haskell](https://np.reddit.com/r/AnnoyInOneSentence/comments/4p5a6p/rhaskell/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Cool, thanks!
One very encouraging feature of this conversation is how many people actually care about GHC's development, and making it more approachable, rather than simply caring about *using* GHC. Thank you! As I said in my post, and as Ben has said on this thread, we are developing a process for adopting new features for GHC. We'll publish a proposal and invite feedback. Meanwhile, please do offer models that you think work well. (No one has done that yet, unless I missed it.) One thing I didn't mention in my post is that GHC is a research project as well as a compiler used by (hundreds of) thousands of users. To take a recent example, there was not much user pressure to introduce the TypeInType extension, which was a major change that Richard Eisenberg pushed into GHC shortly before Christmas. It was the fruit of a multi-year conversation, including several papers, in which Richard and Stephanie convinced me that it was (a) simple enough and (b) useful enough to go into GHC. During that conversation the details changed a *lot*; it was time well spent. It significantly extends the power of GHC's type system. But it was undoubtedly destablising; it led indirectly to the `$` fiasco, for example. Being a research project AND a production compiler is tricky, and I'm sure we don't always get it right. But I do think it's an unusual and rather wonderful thing. Anyway, back to the point: stay tuned and we'll push out that proposal for comment in a week or two. Simon PS: very exciting: practically my first Reddit post ever :-)
There's a great function that's polymorphic over all `a`: unsafePerformIO! /s
*shrug* I really don't think everything useful has to be in base. I do wish that Edison were part of the platform.
I'm not sure why that's hand-wavy at all. Allocations can fail, therefore they exist in a type were failure can the signaled. I routinely do it in C++, I see no reason you couldn't do it in a pure language with an even better type system.
That seems to be the catch-22: People aren't talking enough about this before the `text` modules are in `base` but they're not putting them in `base` because they haven't been talked about enough. I don't think this can be solved well without some key feature being added to GHC (like type internal representations, better type hinting, or instance specific implementations), but I'd like to get started with adding library support. Edit: I mean I'll happily work on making `Text` and/or `ByteString` versions of `String` functions if given a good place to start.
Looks cool! Found a typo, in case the authors are here: On page 8, it says "It has one law, stating that reverse is idempotent" - "idempotent" should be "involution" here (idempotent would mean f(f(x))=f(x)) 
Probably the best thing you can do to help on an individual level is, if you happen to be a library author, encourage good practices. Don't use `String`, and don't contribute to the proliferation of `StringLike` classes out there.
I asked a friend what's the most important thing about software engineering they learned at their current (systems programming) job, and the response surprised me. It was: "Just how fallible we all are". Getting someone interested in better systems programming languages (like Rust) is straightforward: find a C++ programmer, then convince them that their own C++ code is full of undefined behavior and they just don't know it yet. The second step is the problem. Everyone thinks they've (finally, this time for real) found a safe subset of C++ and they're done writing bugs forever. 
Specifically it doesn't run raw SQL queries and instead uses some DSL that can be modified, extended, and in general interacted with. 
Why does this complaint is considered annoying? Of all the stuff in this thread this issue seems like a real problem.
Which encoding should they use? 
We had a GSoC project for doing just that that I mentored for Jasper back in 2011. It ultimately made everything slower, and made it a lot harder to deal with format conversions, because things like iconv liked UTF16 input, so in the end we rejected merging the bulk of it and just cherry picked the performance improvement components. Another hazard: If you unpin the memory for bytestrings to get better GC control over them then folks who currently use them with `mmap`'d data get hosed. =/
You can do that today with RULES. However, in the case where `fibonacci` is used as a polymorphic type that just _happens_ to be called later at `Integer` the specialization may not kick in. 
If I recall correctly, no code would have been broken; the type of $ would just look weird.
So you are saying that current convention is against incorrect usage of `unsafePerformIO`... While true, it is as comforting as having faith in pureness of any C function. I simply can't understand why is this not a huge issue for users of language that puts so much emphasis on statically verified purity :/ To put it another way - why does Safe Haskell seem to be mostly ignored? Is this due to popular extensions (TH?) not compatible with it?
My work is almost exclusively in Python these days. Despite this, I can use many of the ideas I learned from Haskell. The comments in my code mention ADTs, I use list comprehensions in lots of places, most of my code would be statically typed if Python had a type checker. I keep telling my coworkers about the _Maybe_ monad and how their use of None is reminiscent of it, and they're catching on. Python is easy to read, easy to write, and has pretty good library support. It even comes with its own BBP-like controversy, namely whether to upgrade to Python3 or stick with Python2.
I use Haskell at my full-time job. If I had to "fall back" from that for job reasons, I'd probably find a different career focus. Maybe move to systems programming in Rust or C++ if I could swing it. I've moved a couple startups over to Haskell, most of them were on the JVM (Clojure, Scala) before. I don't think you should pick your job (or tools) based on a community survey.
Sure, you would expect things like Text.length to be a little slower with UTF8, but in most real-world usage (again, I claim) the benefits due to the reduction in memory usage will win out. I'm expecting to get a solid 10% win from going to UTF8 for our app. Perhaps if we were processing mostly 4-byte UTF8 chars the story would be different, but we aren't, and I suspect that's the case for most real-world usage. Of course we'd have to do the ICU interface too, with conversions to/from UTF16 at the boundary with ICU, but I strongly suspect that's a tradeoff worth making.
"An involution" surely, as "is involution" is non-grammatical. Another typo on page 3: &gt; We consider this a small **prize** for the extra power theory exploration provides. That should be "price"
Something being a code smell just means it's a good idea to take a good hard look at your code, as there is a chance it can be improved. It doesn't mean that you cannot ever violate the "holy rule". It's entirely possible, for example, to have a long function that doesn't do anything complex. In such a case, it's not that bad to leave the function that way. However, in most cases a long function is just doing too much. Its work can be divided into logical units. This makes your code clearer, makes testing easier and enables reuse.
One way to prevent documentation to drift away from your implementation is to use doctests. The added benefit from them is that they provide readers with some concrete examples. For me, in a lot of cases some well-chosen examples do more to explain than abstract definitions.
If you can get the ICU bindings working nicely that'd go a long way towards making a UTF8 text binding more viable. The overall performance impact of UTF8 operations was pretty low. Halving memory usage could definitely make up for it if you use a lot of largish `Text` blobs and they are treated as mostly inert, of course if/when you go through ICU a lot then the trade-off is going the wrong way. As a vaguely interesting but largely irrelevant aside: I actually have a `Text`-like library that I never bothered to polish up and release that uses UTF8 text and a not-quite-succinct `poppy`-based rank structure (which gets tacked on when it encounters anything is outside of ASCII) to get O(1) length, O(log n) splitAt, drop, take, etc. By adding a supplemental succinct select structure I could get the latter operations down to O(1). This fixes the asymptotic performance of operations that want to cut positionally, but the constant factors obviously suffer some.
I actual get that the two filters is slower but optimising makes two filters behave identically to filtering on the conjunction. I had to increase testN to get any reproducibility as it was too fast to get good results in ghci. I am intrigued about how you get that result as I can't reproduce with any set of flags. What ghc version are you using? I am running ghc 8.0.1. When testing the compiled performance I was using `-O` and `-fobject-code` in ghci and I was using `-fbyte-code` without `-O` in ghci for testing interpreted performance. EDIT: The above was incorrect as made a typo in one of my definitions. I think that allocating two extra booleans is more expensive than one cons cell unless we inline `&amp;&amp;` but I am not entirely sure. Using `-O` definitely helps here as it changes the call to `&amp;&amp;` into nested cases without any allocation. The performance of unoptimised haskell code is very hard to judge because so many utility functions like `&amp;&amp;` and `$` are not inlined and incur call overhead. 
[removed]
Perfect. Thanks.
The allocations are completely implicit. What enforces they go into a maybe type? Does your server go up in flames if someone loops without putting their code in maybe?
What is this ICU thing and why is it so important to optimize `Data.Text` for it at all costs?
If I couldn't find Haskell work, but needed some kind of work. Most of my work lately is ops, backend. In the past I did some dist-sys/arch type stuff. I really enjoy that, but I mostly just want interesting/difficult work so I've been eyeing systems as an alternative.
&gt;&gt; we absolutely must figure out a plan to fix the String/Text situation &gt;I think there are still good places for `String` (or at least `[Char]`) being used. I think there aren't any cases where text or builder/text isn't a superior solution to string. 
Another question is to have a generalized form of Read and Show so that user-defined data may have a serialization to some more efficient form of string without extra effort. If this is not allowed, String will continue as the de-facto base case for many new haskell programmers and programs. Simply because it is easier. and the problem will be perpetuated.
I don't think you realize what you're saying. You've introduced maybe wrappers in virtually every subexpression! What you're left with isn't similar to Haskell at all.
&gt; I think there aren't any cases where text or builder/text isn't a superior solution to string. Lazy parsers or lazy consumers in general. Though lazy text is okay here, especially if you need more than one character look-ahead. I am also not a fan of the naming for MonoTraversable methods, so having something that is Traversable is slightly nicer.
RULES that seem to kick in by magic are why I rarely use them. If you know a good way to ensure they kick in without screwing up inlining and other optimizations, I'm all ears.
Pretty much the reason why I'm still having a C++ job. ;)
What ideas are those and what specifically makes them much more difficult to learn or use in Scala? Can you provide some concrete examples?
I've been loving full time Elixir for the last month. One of the few commercially viable languages that actually takes immutability seriously, good tooling, friendly syntax.
Congrats!
English.
Nice! Readable too.
At work, we're using Scala and JavaScript. I hate JavaScript. We're planning to replace it with Elm and that makes me very happy :)
I've been doing Haskell professionally for 6 years now. I would say that if you really want to get a Haskell job you need to be willing to move. There are some opportunities for remote Haskell work, but there are more that aren't remote. NYC, London, and Singapore are probably the best locations for finding Haskell jobs right now.
Seeing as there is apparently a performance difference between the orders of the tests, presumably list traversal doesn't completely dominate, at least in this case.
Would creating something like Data.ByteString.Unpinned be a way forword? Or would it be seen as polluting the namespace / adding extra maintenance burden?
&gt; Are the frameworks/libraries and documentation mature enough for enterprise development? "Mature enough for enterprise development" is fairly subjective, but in my opinion yes. I've been using it in production for years now and at several different companies. &gt; How would you compare Servant against Yesod/Snap? Servant is a higher level library that lets you describe the structure of REST websites with types. You can use it to auto-generate all kinds of things such as: servers, clients, API documentation, etc. Servant can generate server functions for the [warp](http://hackage.haskell.org/package/servant-server) or [snap-server](https://github.com/haskell-servant/servant-snap) web servers. &gt; Can Servant do most of what Yesod/Snap does? No. It is an API for describing websites. Yesod and Snap are web frameworks that include a web server. Servant can't serve web pages without using one of them. &gt; Is Servant used for dual applications (webservices AND websites) or only for webservices? It can be used for both. &gt; How would you compare Haste and GHCJS? GHCJS is closer to GHC than Haste is. There are some Haskell things (such as weak references last I checked) that GHCJS gives you that Haste does not, because GHCJS includes a shockingly complete implementation of GHC's runtime. In my experience this has meant that I can share more code between the front and back ends when I'm using GHCJS than when I'm using Haste. However, Haste's resulting javascript code will be smaller than that of GHCJS. Also, Haste may have improved in terms of code sharing since I used it. Here's a thread from a few months ago where the author of Haste replied to me saying that Haste had improved its code sharing story since I used it: https://www.reddit.com/r/haskell/comments/48mipd/status_of_fay/d0kuo79 &gt; How well does the Haskell ecosystem plays with databases (PostgresSQL, Redis, Cassandra, Riak, etc.) Just fine in my experience. There are Haskell bindings for all the ones you mentioned: * http://hackage.haskell.org/package/postgresql-simple * http://hackage.haskell.org/package/hedis * http://hackage.haskell.org/package/cassy * http://hackage.haskell.org/package/riak
On Haste vs GHCJS: [Here's a discussion from over a year ago](https://www.reddit.com/r/haskell/comments/31ui2h/whats_the_current_status_of_ghcjs_vs_haste/). Since then, I think people have only continued to migrate toward GHCJS. I don't have a lot of knowledge with Yesod/Snap, but I've been toying with Servant a lot and find the type-level APIs to be invaluable. [I wrote `servant-router`](https://github.com/ElvishJerricco/servant-router) as an example of this. You just get so much mileage out of being able to represent end-points this way.
C++ and Python for collaborative stuff. For quick personal programs, I use mostly Python and Haskell.
I don't know about should, but it (XMonad) works well for me. Pandoc is something else you might want to look into. Hakyll is great for static site generation.
It shouldn't. It should be least-common-denominator. ByteString can used for FilePath on all platforms. Plus, technically POSIX isn't just for UNIX systems. It was meant as a common interface for all OSes. The UNIX standard (maintained by The Open Group, holder of the UNIX trademark) has drifted from the POSIX one at several times in the past, though I think they are sync'd right now.
I just discovered `ExceptRT` and now I am very confused
I'd also add that a lot of companies using Scala really aren't using it as a functional language. Sure you'll use the collection api and for comprehensions when appropriate, but often will come up against a lot of resistance from the folks with a Java background when you try to pull things in a more functional direction.
I never liked the way scala does type classes. They aren't really part of the language, but rather a clever way to use of implicits. Algebraic data types (case classes) are also clunky in my opinion. That being said, I'd choose scala over java if I had that choice.
You can use my command-line benchmarking tool: `bench` * https://github.com/Gabriel439/bench
I haven't used Go much, but I think I would prefer Python over Go, because although Python is dynamic and unsafe, it does have the power and flexibility that comes from a dynamic scripting language. What are your thoughts on this? What do you like about Go?
Haskell's most natural habitat is most certainly Linux. You'll run into the fewest incompatibilities and best runtime performance, especially for servers. As far as I know, some of the more impressive I/O optimizations exist only on POSIX systems, but my info may be wrong or out of date. (Someone please chime in.) But Windows is by no means out of the picture. I've used Haskell (GHC) on Windows for years and even deployed a Haskell DLL as part of a larger application to thousands of customers with relatively little headache. Stack has made the Windows experience 10x better than it ever was before. There are still some headaches when linking in shared libraries, but they are rarely insurmountable. It sounds like you're building a server. Like I said, Haskell may not be able to give you the insane performance that it achieves on Linux, but it definitely works and works well.
YES. I am in the same camp as you I think. I learned Haskell a looooong time ago and now I'm getting "into" Scala at work due to resume driven development and I hate it. I think Scala pisses me off so much because it could have been just as good as if not better than Haskell. Instead you get CanBuildFrom and the rest of the INSANE standard Scala collections library, (mutable by default, and lol if you just import immutable.Seq and try to use it). A bunch of strange idioms like companion objects, self types, and the wacky for comprehensions. And god help you if you run into implicit resolution ordering issues. I miss Haskell so much.
Any particular reason for moving the Purescript over to GHCJS? I'm looking to run a bunch of AWS Lambda instances as .purs code but I've just barely started.
The danger is when you can observe the difference myid :: a -&gt; a myid x = x actuallyPlus1 :: Int -&gt; Int actuallyPlus1 x = x + 1 {-# RULES "myid/add1" myid = actuallyPlus1 #-} Now using `myid` is pretty dangerous from a parametricity standpoint. If you use it in a situation where a is known to be Int and the RULES fire, it'll start adding 1. If on the other hand you pass it to a polymorphic function and then depending on if the typechecker happens to figure out that it is actually going to be an `Int` at that point it might or might not add 1. This is "magic like whoa". You can do better by using Typeable to check the type, so that it shows up safely in the type signature. I simply offered up the RULES thing as the only form of "template specialization" you can get without affecting the type signature you gave -- and to show quite how bad it can be.
Standard Chartered has something like 3+ million lines of Haskell code and they have recently advertised Haskell jobs in both London and Singapore. New York is home of the [largest Haskell meetup in the world](http://www.meetup.com/NY-Haskell/) (I am one of the co-organizers) with [London having the second largest](http://www.meetup.com/London-Haskell/). I can think of at least 5 or 6 companies off the top of my head that are doing Haskell in NYC and there are probably others I can't remember or don't know of. Boston and the Bay Area also deserve mention, but they deserve mention in pretty much all conversations about tech jobs and they actually seem to have fewer Haskell companies than the other three that I highlighted. While there are several finance companies featured prominently in the list, I don't think Haskell leans towards finance as much as you might imagine from hearing those cities mentioned together. NYC in particular has quite a nice tech startup scene.
 lookup 10 (insert 10 20 empty) The main problem with classes that let you both construct and consume a structure is that someone sometime has to pick an instance explicitly. This makes a lot of code go from the sort of code you can infer to code you can merely check with explicit type annotations sprinkled all over it to indicate magic should happen there.
Off-topic, but I'm curious about this: &gt; "intermediate-level ideas" (typeclassopedia, pipes, free, the less magic parts of lens, etc) If pipes and free are intermediate-level, what is master-level? :S I would classify easier things as intermediate, e.g. monad transformers \^\^
I do a lot of (modern) C++ at work. I do enjoy the strictness-by-default, better memory layout, and guaranteed compile-time polymorphism (templates).
[removed]
Some nice Haskell packages require `unix` dependency and thus they won't build on Windows, that's the main pain spot. A lot of packages that require linking 3rd-party libraries (like SDL2, GLFW, gtk, etc) actually work out-of-the-box now, just fire up the msys2 terminal, `pacman -S` something, followed by `stack build` and it works. A few years ago, I have to manually copy &amp; paste headers &amp; libs.
Yeah I was using classy prelude and all the good stuff that comes with that today and it totally does what I wanted to have happen anyways. I had some problems with Vector, a library wanted unboxed ones in some places and boxed ones in others, but beyond that it was exactly what I wanted, to be able to look at a page of common functions and see the directory of instances that I can choose from. I also like the explicit Element type family better than the way I was considering doing it with the constraint. The consumption/construction problem is an interesting one. I've been thinking for a long time about a language which has a way of encoding efficiency of algorithms as well as properties preserved via functions in the type system (i.e f :: ( x :: Int) -&gt; Int &amp; [Monotone f, O(sizeof(x))] , and I'd imagine if you had that you could do some basic asymptotic analysis and choose the most efficient instance in the cases where you've left it ambiguous. This could probably be done in Coq now that I think of it, but I'm not sure of the reasoning process' complexity. I'm sure some bounded form of reasoning would help in a majority of cases.
As a person who uses NixOS, it's pretty painless for me to use programs written in Haskell, so I have most of them installed. Here's a list, roughly sorted in order of usefulness (to me) within each section: * General * [xmonad](http://xmonad.org) * [xmobar](http://projects.haskell.org/xmobar) * [taffybar](https://hackage.haskell.org/package/taffybar) * [git-annex](https://git-annex.branchable.com) * [pandoc](http://pandoc.org) * [hakyll](https://jaspervdj.be/hakyll) * [mighttpd2](http://www.mew.org/~kazu/proj/mighttpd/en) * [hledger](http://hledger.org) * [yeganesh](http://dmwit.com/yeganesh) (useful if you use dmenu) * [hbro](https://hackage.haskell.org/package/hbro) (looks promising but needs some love for JS and Flash support) * [mecha](https://hackage.haskell.org/package/mecha) * Compilers * [ghc](https://www.haskell.org/ghc) (of course!) * [Agda](https://hackage.haskell.org/package/Agda) * [idris](https://hackage.haskell.org/package/idris) * [ghcjs](https://github.com/ghcjs/ghcjs) * Dev Tools * [darcs](https://hackage.haskell.org/package/darcs) * [shake](https://hackage.haskell.org/package/shake) * [cabal](https://hackage.haskell.org/package/Cabal) * [stack](https://hackage.haskell.org/package/stack) * [leksah](https://hackage.haskell.org/package/leksah) * [yi](https://hackage.haskell.org/package/yi) * [packdeps](https://hackage.haskell.org/package/packdeps) * [hoogle](https://hackage.haskell.org/package/hoogle) * [bustle](https://hackage.haskell.org/package/bustle) (AMAZING if you ever work with DBus) * [hi](https://hackage.haskell.org/package/hi) * [omnifmt](https://hackage.haskell.org/package/omnifmt) * [cabal2nix](https://hackage.haskell.org/package/cabal2nix) * [hpack](https://github.com/sol/hpack) * [ghc-mod](https://hackage.haskell.org/package/ghc-mod) * [ShellCheck](https://hackage.haskell.org/package/ShellCheck) * [ghc-vis](https://hackage.haskell.org/package/ghc-vis) * [threadscope](https://hackage.haskell.org/package/threadscope) * [STGi](https://github.com/quchen/stgi) * [LiquidHaskell](https://hackage.haskell.org/package/liquidhaskell) * [HipSpec](https://github.com/danr/hipspec) * [hasktags](https://hackage.haskell.org/package/hasktags) * [hlint](https://hackage.haskell.org/package/hlint) * [stylish-haskell](https://hackage.haskell.org/package/stylish-haskell) * [hindent](https://hackage.haskell.org/package/hindent) * [halberd](https://hackage.haskell.org/package/halberd) * [pointfree](https://hackage.haskell.org/package/pointfree)/[ful](https://hackage.haskell.org/package/pointful) * [djinn](https://hackage.haskell.org/package/djinn) * Misc * [cgrep](https://hackage.haskell.org/package/cgrep) * [bench](https://hackage.haskell.org/package/bench) * [yaml](https://hackage.haskell.org/package/yaml) (has `yaml2json` and `json2yaml`) * [aeson-pretty](https://hackage.haskell.org/package/aeson-pretty) * [xml-to-json](https://hackage.haskell.org/package/xml-to-json)([-fast](https://hackage.haskell.org/package/xml-to-json-fast)) I can't vouch for the usefulness of these programs in environments where using and patching programs written in Haskell is less painless. Probably some of these packages won't even compile, but most will. As far as I'm aware, this list has most of the working and useful Haskell executables on Hackage, but if you know of one that isn't on this list, please let me know. EDIT: also, [here](https://gist.github.com/taktoa/92ab94e1ea617f6a96a4a9e23b8baaf4)'s my absolutely enormous `~/.nixpkgs/config.nix`, edited somewhat for brevity (some packages may not exist upstream, but all the Haskell ones do for sure) EDIT: added `bench`, `hpack`, `ShellCheck`, `LiquidHaskell`, and `HipSpec`; fixed links for `yaml` and `aeson-pretty` EDIT: added `omnifmt`, `cgrep`, `xml-to-json`, and `xml-to-json-fast`
Um, I won't try to defend many things in Scala, but you do realise that the Scala collections you're given by default are the _immutable_ ones, right? Also, for comprehensions are literally the same thing as Haskell's do notation, so I'm not sure what the complaint is there.
I find Scala to be the closest thing to Haskell that you'll ever find in wide use, and it is surprisingly close in expressive power and type system power. But, among other problems, you'll certainly have a hard time getting people to buy in to typeclass-style programming in Scala; they regard it mostly as a black art. Even the standard library doesn't place much emphasis on typeclasses. So you'll see a lot of mixin traits and classes that end up as giant bags full of methods from many different traits.
I have some good and some bad things to say about Go, but I think my experience is positive substantially because Go feels like a (mostly) marginally better C, and I know how to quickly build stable things of moderate size in that environment. I've never really figured out how to work in Python in a way that let me get shit done. I'm tempted to generalize to "scripting languages" broadly, but I once spent a year maintaining 60k lines of bash. Some of it may be the quality of the code-bases in question, but a combination of "everything happens elsewhere" and not actually knowing (beyond careful reading of code or possibly outdated documentation) the domain or range of any function really makes things hard to keep a handle on - especially in a changing code base.
To explain: this is the author of [Pandoc](http://pandoc.org), one of (if not *the*) most popular haskell projects in existence. He's a professor of philosophy. The point is that you can have lots of productive fun with Haskell, regardless of what your day-job is.
VB6 and C# for any new work.
`void*`... slightly worse ways of saying `Typeable a =&gt; a`. The problems inherent to the construct are minor compared to the problems derived from misuse, and at least I have the *rest* of the code comparatively pinned down. I certainly prefer the Haskell to the go.
That sounds like a challenge to me.
I agree. We use very few type classes in Scala; it's rarely worth the added complexity. And don't get me started on GADTs. But if you stay away from type magic, Scala is a decent functional programming language, as well as a decent imperative programming language. 
It might be the only way, but it's a last resort. See previous discussion about proliferation of string types.
For fancy IO, you really need epoll (Linux since early 00s) or kqueue (FreeBSD since early 00s, NetBSD, OpenBSD, or OS X), neither of which are POSIX. Strictly speaking, if you are limiting yourself to POSIX, then you are limiting yourself to select/poll. I don't know too much about Windows IOCP, just that it's a complicated API that is not immediately obvious how to navigate and I don't really have the inclination to figure it out. However, it would appear that IOCP leads to a rather different structure of the core IO components of a native IOCP program versus native KQueue/Epoll program, and some of those differences may well reverberate beyond core IO a bit. So yeah, if you want to write servers in Haskell and you care about IO a lot, Linux, FreeBSD, and Mac OS X would be easier choices, with Linux being the easiest. Solaris/Illumos might not be too bad if you are willing to put some work into it. But Windows appears to be an open problem that several people have put at least somewhat serious attempts into, and there hasn't been a result that's been widely advertised or distributed yet.
There's not a clear line obviously, but I don't consider myself near master-level for being able to use pipes or free. :) Understanding heavier category theory based libs by e.g. Edward Kmett, and all of lens is a different level IMHO.
The controversy was that it would be a big deal to beginners who have just installed ghc, have just been told that you program by 'following the types', and subsequently typed in `:t ($)` only to find a very ugly output.
Oh how I long for the day that Windows compatibility stops being provided by Cygwin and its ilk. Especially for things like Haskell that don't need deep OS call access like e.g. C does.
C# I would love to work in Haskell but I have 2 big things going against me: * No Haskell jobs near me * Can't show professional Haskell experience on my resume For the last point I know people say "just point people to your repo"... I have some Haskell code but no time for anything substantial like I would need to land a Haskell job with no professional experience in the language.
Why not go the other way: make `Data.ByteString` unpinned, and introduce a pinned variant in `Data.ByteString.Pinned` instead? It seems to me that those few cases which demand a `mmap(2)`able `ByteString`s could be made to require a different `ByteString` flavour, rather than have everyone pay the cost for an ability that isn't taken advantage of. Btw, there's also `Data.ByteString.ShortByteString` which is a very lightweight wrapper around `ByteArray#` which can bring significant performance boosts and heap-savings. It's great when you have lots of (short) data values which don't need to support slicing operations (like e.g. POSIX FilePaths, sha256/md5 sums, utf8/utf16/latin1 encoded text-strings used as map keys, and so on).
Thanks for your response. I've been meaning to ask a similar question and your response helps. However, I'm a bit confused about the part regarding Servant. You say it can't serve web pages without using Yesod/Snap and then you say it can be used both. Do you mean it can do both assuming it uses Yesod/Snap? A rather specific addition to the question may be "to what extend Servant lets you customise REST related aspects of HTTP?" For example when I search for optional Http headers and Servant I can only find a discussion where someone is asking if it is possible and there are no definitive answers (forgot where the thread was) So if I tried to replace an existing .NET Web Api based REST service with a Haskell framework, what would be a good candidate? 
Haskell and Purescript in London... Go on...
Client 1: Java 8, Javascript. Client 2: Java 8, Groovy &amp; Haskell. In the past: Java, Python, JS. Obviously shell scripting comes in to play here but it's not the majority language.
What am I missing about Scala's type classing? AFAIK, really Scala has just the ability to "alias" types: e.g. instead of using `(String, Int)` for e.g. a person, I can say `type Person = (String, Int)` in Scala; does its usage stop there? It's not like you have typeclasses and instances in Scala (though you have traits).
Hm.. do you find Go worth it? I started as a hobbyist programmer in Scheme and Haskell, but now I'm a software engineer doing Node.js. Most of our backend team though prefers Go so I've been learning it.. so far I just don't see the point. Even today it took 30 lines to do what I did in 10 in JS.. though maybe it's because I'm still learning Go.. How do you tolerate it as a functional programmer (if you see yourself that way)?
Is there `Text.Show` class with `GHC.Generics` instances?
Yes. It's 100% an engineering problem (unlike, for example, dependent types). And 100% necessary for a robust/complete IDE. 
While we're joking, why don't we use the Windows API as the portability layer and libwine to run on non-Windows platforms? It's basically the same thing.
(upvoting because interesting, but) Haskell is lazy, and functional, practically everything allocates. `Maybe` allocates! So if your computation successfully doesn't over-allocate, producing an `x`, it then allocates a thunk for `Just x`. (c.f. `finally action finalizer`, possibly `throw`ing during the `finalizer`) Even if `Just` were strict, it would allocate for the indirection; we don't have https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes 
Yeah I wasn't actually joking; I don't know much about this layer. I primarily do Haskell dev on Linux but I do dev' on Windows too and I don't have such issues as I'm not writing anything using the unix dependencies you refer to. Writing a RESTful server with DB &amp; AWS access works just fine on both, for example.
Hakyll is *amazing*.
If you refer to the Ubuntu stuff in the Windows anniversary update, then it's just another POSIX-like layer like Cygwin and won't resolve the issue of needing a POSIX-like layer.
In practice, native POSIX means Unix though.
Scala has full-fledged records and typeclasses, and also lets you specify multiple typeclass instances and also a custom error message if an instance cannot be found.
So you're saying that Haskell isn't WEB SCALE on Windows? Building network and DB packages on cygwin gave me some headaches when I first tried it a few years ago, indeed, but I guess that's a minor incovenience compared to degraded I/O performance.
I'm coming up on two years of full-time Haskell professionally. If you're interested in macOS/iOS development, Swift is a truly enjoyable language—it took a number of really important lessons from Haskell, OCaml, Rust, and plenty other languages. Swift 3 is a simply excellent language, especially in conjunction with Xcode playgrounds.
&gt; And please note that POSIX isn't sufficient for modern software even among Unix-like systems (e.g. neither epoll nor kqueue are POSIX)! Oh, it's way better than that: POSIX doesn't even reliably specify a way to *close a file descriptor*. There's literally no specification of what happens if `close(2)` gets delivered an interrupt (i.e. whether or not you need to re-`close(2)` or not if you get `EINTR`).
Cygwin is probably your best bet here. If you can run a VM (probably not), that might be the easiest solution. Load up Arch or NixOS and go hog-wild. EDIT: Docker on Windows may be a workable solution too.
[ShellCheck](https://hackage.haskell.org/package/ShellCheck) is a really useful tool to learn how to write more robust shell scripts (and fix the ones you have).
As someone whose primary environment is Linux but has had to wade through Windows issues my biggest recommendations are [Stack](http://docs.haskellstack.org/en/stable/README/) over Haskell Platform for getting started and [MSYS2](https://msys2.github.io/) over Cygwin.
The problem with a double standard is that it discourages new contributors, so eventually the pool of people willing to contribute dries up
Ruby (on Rails) and Javascript (AngularJS)
We have several enterprise products that we also release for Windows. Performance is good. We use cabal and it works great on Windows, just as on Linux. We have many library dependencies, including libraries that wrap a C API, and they work fine; the only one that was more complicated for us on Windows than Linux was text-icu, because the ICU4C DLLs could not be used. We had to use MSYS2 instead. But that problem is now fixed in current GHC.
I personally would recommend against Cygwin. Stack will automatically install an isolated instance of MSYS to get you a reliable state. Within MSYS you can use `pacman` to get packages, etc.
"An experimental Malfunction backend for the dependently typed programming language Idris [ 1 ] is available. This backend does not yet implement all Idris primitives, so cannot run all programs. Nonetheless, preliminary benchmarking of some simple programs (currently, Idris lacks a comprehensive benchmark suite) shows the Malfunction backend outperforming Idris's C backend by a factor between 3.2x and 14x."
I thought ByteString was pinned for copy-free FFI calls? Otherwise, how can you to pass a big chunk of data to C without copying it? And is there some documentation about the bad effects of pinning? Obviously it means the GC can't move it, but what problems does that cause?
I'm guessing this wouldn't really work for Haskell due to lazyness.
Thanks for your hard work!
You can express lazy computations in a strict language (and conversely) and a [strict Core intermediate language](http://research.microsoft.com/en-us/um/people/simonpj/papers/strict-core/tacc-hs09.pdf) has been proposed for Haskell in the past. So this could indeed work for a lazy language. Now it may be tricky to make it work for Haskell if earlier compilation passes make assumptions about the runtime and value representations (which are distinct from OCaml's runtime and value representation), and there is no clear reason why it would be more efficient than GHC's current backend -- although it would be interesting and amusing to compare, and symmetrically I've often thought of sending OCaml code to GHC's backend. Finally, the runtimes have different capabilities, in particular OCaml's runtime does not support parallel execution of gc-ed threads (OCaml and C can run in parallel, but not OCaml and OCaml) while this is one of the strength of the Haskell runtime. (Conversely, OCaml's runtime may have higher throughput in the sequential time and smaller pause times on some workloads.) So in any case using this backend would be unacceptable for many Haskell programs -- those relying on Haskell shared-memory parallelism for performance.
An interesting and important point is that OCaml and GHC made different choices of intermediate representations, OCaml's intermediate IRs are untyped while GHC's Core is strongly typed. Typed intermediate representations have many benefits (debugging, reasoning, some optimizations, etc.), and I would be extremely happy to see OCaml's implementation have a typed IR, but they also make it notably more difficult to re-use the backend for other languages, as other backends then have to perform a type-preserving transformation which can be very difficult (or impossible) without adding dynamic tests all over the place at the cost of efficiency). There has been other attempts at targeting OCaml's IR for other experimental functional programming languages -- Links, Eff, etc. -- and I think that's a very inspiring outcome. Get a strong compiler backend and runtime for cheap!
Isn't ExceptT with IO at the base considered an anti-pattern?
I'd never heard of that before (I'm not very familiar with OCaml or its ecosystem), but yeah, that looks like exactly what I had in mind!
Why would it be? `IO` sort of represents unchecked exceptions, while `ExceptT` represents checked exceptions. Doesn't seem unreasonable to make checked exceptions available to IO bound code
I primarily develop in a mixture of C++, Delphi, and PHP these days at work. However, I do have a few small work-related applications written in Haskell, mostly to do simple data transformations/analysis i.e. not-as-critical stuff. EDIT: I guess I didn't really answer the question. While the Delphi and PHP code I maintain sucks, I really enjoy writing in C++. Modern C++ (i.e. anything newer than C++98) really is a joy to work with. It's expressive and powerful, and it lets you open the hood when you need to squeeze every last drop of performance out of your code. Most of my C++ code is very high level and uses a ton of the STL.
Note that StrictCore has explicit thunks -- `\&lt;&gt; -&gt; expr` is a thunk (even though it looks and type checks like a lambda) that, when applied, handles the work sharing. (e.g. subsequent applications doesn't evaluate it again) Code generator needs to specially handle these type of values. (which is explicitly mentioned in the paper) So it's not like you compile a lazy language to a language that knows nothing about laziness.
Except can often be introduced by other stacks that you also happen to want to use with IO, and IO always has to be the base, so I can't see it being unacceptable. 
The garbage collector will not run while any foreign calls are occurring, and only begins running once all Haskell threads have synchronized at an allocation point. This means the GC cannot move your object while you're inside a foreign call - although it might mean that it is moved *right after* the foreign call is complete. Anyway, as stated, the vast majority of use cases that require passing buffers to C do not require pinned memory, only a block of storage for the duration of the call. Most of the times it's stuff like "Here is a buffer, run to completion and return a value with an updated buffer", or "Here's a buffer and a length of it, validate it". These require no copies or anything at all, regardless of whether the memory is pinned or unpinned. If you need `mmap`-able ByteStrings or ByteStrings that need to exist in-place for the duration of the program (like if you register a pointer with a C library that needs to stay alive) you can use a pinned version (or StablePtrs) or something explicitly. &gt; And is there some documentation about the bad effects of pinning? Obviously it means the GC can't move it, but what problems does that cause? The big problem is if you have a lot of long-lived ByteStrings, and they are introduced/released over the life of the program, you will cause a lot of heap fragmentation as the RTS keeps trying to find contiguous blocks to satisfy your requests, interspersed with all the other allocations going on (including very short, ephemeral ByteStrings, where the cost isn't so bad). In turn, that fragmentation can cause allocations to fail, and GCs to run more frequently. And a side effect of that is that due to the fragmentation, because GHC can't move or compact these objects either, they'll suffer from locality problems, too.
Bytestring does not do stream fusion, for one, so there's a fundamental point in the design of `text` left unaccounted for. But. I *think* at one point I talked with /u/dcoutts about this, and came away with neither of us sure if text's fusion framework is really worth its weight in that regard. `vector` has a much more powerful and general fusion library than others including "array recycling", which I'm not sure was ever adopted into `text`, so using it would give you all that machinery "for free", at least. At some point, there was also a floating proposal to have `bytestring` use `Vector Word8` as well, but that one seems as if it hasn't gone anywhere in a long, long time.
I wouldn't pick Go over Haskell, for most projects, given the option. I also wouldn't pick Go over C for projects that actually need C. I definitely find the static checking it affords to be an improvement (used wisely) over JS or Python, and it has a much better concurrency story than either (but not as good as Haskell's, except possibly around GC). It's true that it can be verbose, but the time to write 30 lines vs 10 isn't so much the issue as the ability to quickly make understandable changes to the code.
it's `unexceptionalio`, no?
I've been using both of them since I got these responses, mono travers able is more or less what I want 
At my previous job, I did use Haskell; now that I'm not employed there anymore, I have to make do with whatever meets my other criteria. As long as it's not PHP, I'm OK with it - the mainstream languages are all extremely similar anyway. I have worked professionally with JavaScript, Java, C#, C++, C, Python, Clojure, and ClojureScript in the past (also PHP, but I don't want to be reminded); of these, C++ is probably closest to how I want to program. Clojure I found particularly disappointing, but that might be the result of inappropriately high expectations - it's not bad, it's just not significantly better than the languages it set out to fix.
This will be my base monad from here on out!
&gt;[**Monad Transformers - Ben Kolera [57:23]**](http://youtu.be/pzouxmWiemg) &gt;&gt;Ben talks about monad transformers for configuration and error handling in functional code. &gt; [*^Brisbane ^Functional ^Programming ^Group*](https://www.youtube.com/channel/UC3eIPN3YtTqPDDBbDVPlpWQ) ^in ^People ^&amp; ^Blogs &gt;*^2,316 ^views ^since ^Mar ^2015* [^bot ^info](http://www.reddit.com/r/youtubefactsbot/wiki/index)
Is it possible to style gtk/glade to look like the native platform? 
I think he was referring to a well-known blog post (can't find it right now) that called `ExceptT SomeException IO` an anti-pattern
Yes, thanks! I fixed it
&gt; the notation of list and pattern matching could be used for any kind of container Well we already have ViewPatterns so I suppose you're suggesting even more sugar? &gt; This would make the program work for any concrete container. Yes, but likely not very well. E.g. `cons` for `Vector`s is `O(n)`, so a function that's `O(n)` with lists that uses `(:)` to build it up/traverse it suddenly becomes at least `O(n^2)`. &gt; I know that there are problems like bottom there... Lists are lazy by default most (all?) of the time in Haskell, so you'd have to translate all these programs to work on only lazy containers. @edwardkmett also mentioned: &gt; when you do this, OverloadedStrings punishes your users, forcing them to use explicit signatures on every string literal they pass you.
I don't know about OSX, but Gtk2 looks fairly decent on Windows. Gtk3 just looks like crap everywhere, even natively on Linux all the widgets and fonts are way too big and too spaced. 
Is virtualization an option? Last I heard, Hyper-V was capable of hosting Linux VM's.
I was assuming C would take or share ownership, and I think that still needs pinned memory. For example, "here's a buffer, display it in the GUI". It needs to keep ownership, and finalize via a FinalizerPtr when it gets a new buffer. It looks like StablePtr doesn't have a way to deref from C, or for C to mark that it's done with it, so it seems a ForeignPtr is still the only way to hand data to C without a copy. That said, I used to do this but now I just copy, because memcpy is really fast. I agree it's a minority case for ByteStrings, but I still think there needs to be a no-copy way to give away data. Likely that doesn't have to be ByteString's job, there could be a lower level library specifically for allocating and writing to a pinned buffer, or a Data.Vector variant.
Ah, I see. I missed that it's polykinded. That's very different.
Aura if you use Archlinux (https://wiki.archlinux.org/index.php/aura)
Great presentation, but I can not agree with the idea. Any medium large application may have many developpers and the maintenance may last for years. You can not know in advance how many configurations , states and effects in general you should need. An application could manage dozens, hundreds of states. Only stacking state transformers forces all of your team to be expert in monad transformers for recomposing and tweaking frequenty the stack to accomodating this or that state variable that may extend to the whole application. I can not think in a stack of dozens of monad transformers. That is unmanageable. The alternatives used more often are either a global state (that is pervasive unfortunately in many haskell developments) or the encoding them as parameters on some kind of configuration with registers. Probably a reason for the complain about the absence of extensible records comes from the inflexibility of monad transformers. I think that, definitively, monad transformers is at odds with any attempt to popularize Haskell in industry.
&gt; We'd like to learn from experience elsewhere, rather than reinvent the wheel, such as the ​Rust process. Please suggest processes that you have seen working well elsewhere. zeromq's C4 process could also give some inspiration.
"Typeclasses" in Scala is basically manual dictionary passing automated via implicit parameters; it's not an actual language feature. def foo[M](m: M)(implicit val monoid: Monoid[M]) While it's much less tedious than manual dictionary passing, it has the same tradeoffs in terms of the reasonability of code. For example, Set needs to hold onto its ordering so you don't pass in multiple different orderings.
&gt; Well we already have ViewPatterns so I suppose you're suggesting even more sugar? Yes, some sugaring for reusing pattern matching for lazy list to be applied to some general class (that may implement head, tail etc) so that code for strings could be made strict with some type annotation. View Patterns expressions may be the desugared code. The point is to allow to change for linked lazy lists to strict unboxed list with only type annotations, so the programmer could choose the better implementation for his problem. In the medium term the internal structure of the container could become a low level detail that should be left to the container itself. I can imagine adaptive containers that may pack and unpack temselves depending on the usage. Lazy lists do this in a certain way. I think that there are much to do on that.
Hmm, apparently POSIX did revise their standard to define what should happen in this case in 2012. In any case, this is a pretty interesting overview of the situation: https://lwn.net/Articles/576478/
How do you expect a`Ptr` to remain valid between the time `bytearrayContents#` is called and the foreign call if it's unpinned ? https://hackage.haskell.org/package/ghc-prim-0.5.0.0/docs/GHC-Prim.html#v:byteArrayContents-35-
I think the solution to this is a cross between *using* `mtl` style classes, and *writing* `mtl` style classes. On a certain scale, you can easily write all your functions like `(MonadState MyState m, MonadIO m) =&gt; m ()`. As the stack gets larger, though, you start to need to abstract all of that into an application specific stack. class (MonadState MyState m, MonadIO m) =&gt; MonadMine m where -- custom necessary operations What's nice about this is that you don't have to declare all your monad dependencies in the class head. Your instance head can require constraints to implement those custom operations, without exposing those constraints to other parts of the code. So it's a balancing act. How much do you put in function constraints? How much gets put in the class head? How much gets put in the instance head? You just have to be responsible in how you manage these things. I have a few rules of thumb. For example, I almost never require a `MonadIO` constraint *anywhere* except the instance head. This way I can define alternate instances of my type class in order to test my functions with pure monad stacks.
In my experience, I found it more hassle than it was worth, because I needed to implement all the special masking-aware `catch` and `bracket` semantics and the like to handle both sorts of exceptions, and it was rather a tricky business. There may now be packages that handle all that for you, but again, in my experience, getting that stuff right _without_ another layer of exceptions is subtle enough that I don't feel good about the possibility of incidental error being introduced.
I started a new job four weeks ago. Been coding almost exclusively in haskell since. The exception is a tiny amount of shell script for running tests and having to review some C code (for FFI). At my last job I also mostly did Haskell.
Great news!
Another option is to work with concrete transformers and massage them with [hoist](https://hackage.haskell.org/package/mmorph-1.0.6/docs/Control-Monad-Morph.html#v:hoist) from mmorph and [zoom](https://hackage.haskell.org/package/lens-4.14/docs/Control-Lens-Zoom.html#v:zoom) and magnify from lens. 
Doing this is simply not possible in a *safe* manner. I know in `transient`, you dealt with this by having type-indexed state, but this is not without its safety issues. In `transient`, failing to initialize state before accessing it results in silently terminating the thread. Plus, if two different programmers decide to add state of the same type, there's a conflict. These are easy mistakes to make, and there are *no* compile time insurance policy on them. The only thing the programmer can do to mitigate them is sprinkle in default values for when the desired state isn't present, and use `newtype` for every state added, which are not acceptable solutions. The only way to get this effect safely is to make a compile-time note of it that bubbles up to a point where state can be responsibly initialized. Trying to hide the state in between means that it's all too easy to generate stateful errors; the exact sort of problem Haskell is supposed to solve! Transformers achieve the effect safely and responsibly.
&gt; In transient, failing to initialize state before accessing it results in silently terminating the thread. You can give a default value to be used in case of not being initialized. and yes, you can force this at the compilation time in you restricted transient EDSL. "get" can be easily simulated with the same type safety at compilation time. Silent termination in the other side permits the creation of very expressive code using the alternative operator and this is the only reason why it is there, not because a limitation on the transient approach to state management but because it is a great advantage that enhances composability. &gt; if two different programmers decide to add state of the same type, there's a conflict. If they add state of the same type is because they want to update the same state. Otherwise they could easily use newtype definitions. This does not require any effort and favour good practices. In the other side, a stack of monad transformers can also be confusing if it has two or more states with the same type.
As I said in my comment, neither of those is an acceptable solution! Having to sprinkle default values wherever you might use state is completely counter to the point of having global state in the first place. Requiring the use of newtypes is a serious potential point of failure. Using a global type-indexed state system is just like using an untyped dictionary object from something like JavaScript, except that you don't get to use descriptive names; you can only use the type signature. It still makes no guarantees about the presence of the state (god forbid you accidentally index with `Int` instead of `MyIntWrapper`, and don't catch the mistake until its too late). Point being, you simply cannot rely on just *expecting* global state to be available correctly. To have any kind of safety assurance, you have to bubble the state up in the type signature. And this can be easily hidden behind a type class. &gt; Silent termination in the other side permits the creation of very expressive code using the alternative operator and this is the only reason why it is there, not because a limitation on the transient approach to state management but because it is a great advantage that enhances composability. This is not something that needs to be baked in to every monad stack. Most people don't need it. Transformers offer a better solution to that problem by allowing you to *choose* to use `ListT` for non-determinism, meaning you can't just silently fail without having explicitly made that possible.
&gt; The machine code that GHC generates is also a 'strict language'. Is it? How is that statement meaningful?
&gt; As I said in my comment, neither of those is an acceptable solution! Having to sprinkle default values wherever you might use state is completely counter to the point of having global state in the first place. It is not necessary. you give the default value once. Just like in the state monad transformer. As I said, I can define "get" for any type with the same type safety and giving a default value once: getInt:: Int getInt= getSData &lt;|&gt; return 0 &gt;Using a global type-indexed state system.... it is not global at all!!. It is monadic state. There is no global state anywhere. two branches of the computation in two diffeent thread can manage different values for the same type. You may have interpreted it wrong. "Silent termination" in transient it is not for creating some snippets in order to get rid of the treatment of Nothing. It permits the composition of multithreaded programs with asynchronous inputs and also permits the composition of distributed programs. I think that this is worth the pain. But you can get rid of termination not adding a new monad transformer, but supressing it, by programming in the inner state monad. proc :: TransIO x proc= Transient $ do -- no termination effect here ........ 
&gt; Previously, the state of the file descriptor was undefined if an EINTR was returned, but the new interpretation says that an EINTR return requires that the descriptor still be open. EINPROGRESS should be returned if the system call is interrupted but the file descriptor is closed awesome, literally the worst of both worlds: not only is the behavior wrong (IMO), there is *new* behavior to signal what probably should have been the original behavior anyway.
Like this? https://github.com/Soostone/string-conv/commit/ff0de8f68e478e102b25332562f485c0c10d93d2 I just pushed this change to a branch based on this comment. If this is a safer way to do it I'd be happy to make a patch release.
Yup, and ditto for the other direction. That is, `T.unpack . T.decodeUtf8` rather than `C8.unpack`
99% Haskell and 1% full stack Bash programming, but mostly just Haskell at Ambiata for the last year plus. I'd consider working in OCaml, Erlang or Lisp as well and probably would suffer through Scala if none of the other options were available.
Haskell, with a bit of Nix, and a smattering of Bash, Sed, Awk, and JavaScript.
Unsolicited advice: I'm happiest job-wise when optimizing for particular domains, rather than langs (although langs can give you a fascinating insight into how much leeway an org allows its devs). In the past, at this job, I've worked on and enjoyed using C++, Python, and Java for various projects. Currently I'm doing C# for Windows desktop stuff. Only reason I mention all this is that I usually don't mind using conventional langs if the project is small enough in scope and team, and I have considerable say in the architecture to keep it that way, conceptually. I admit I sometimes undermine attempts to use Node at work, but, hey, we all have our vices.
I'm mostly done. Did it for work so there's going to be a lag before I can get permission to release it BSD.
Given any suitable kind `k`, there is a category **Hask**-`k` where the objects are types of kind `k`, and the morphisms are the appropriate functions or natural-ish transformations. Given `t :: k`, your `hoid @t` provides the identity morphism of object `t` in **Hask**-`k`. (Suitable kinds are `*` and `p -&gt; q`, where `q` is suitable.) Anyway, this seems to be simpler: class IsHead (p :: kp) (q :: kq) instance IsHead p p instance IsHead p q =&gt; IsHead p (q x) hoid :: forall t a. (IsHead t a) =&gt; a -&gt; a hoid = id It requires `AllowAmbiguousTypes`.
&gt; (No one has done that yet, unless I missed it.) From /u/acow [further down the thread](https://www.reddit.com/r/haskell/comments/4oyxo2/blog_contributing_to_ghc/d4h1xyi), restated for convenience: &gt; I will outline one option: the wiki is central. Trac tickets with a proposal must link to the wiki, and the manual imports wiki content to ensure the wiki is up to date, Notes should be links to wiki pages. Sign-off on a design happens on the Wiki so it is not debated on Phab, nor revised on one of the mailing lists.
The problem is the misuse of interface{}{} driven by the lack of generics seems to be rampant. Perhaps this phenomena is local to me though.
Nice is single threaded iirc, which is why one of the tenets of node is (or was) "don't block the loop".
Why not have a type that acts like a pin for a `ByteArray#` and can be casted to `Addr#`, or used as one? When all the pins to a specific `ByteArray#` are garbage collected, it can be unpinned.
Yeah, I really don't like that `cons` and `index` on `Text` is O(n). Honestly, if I had my way, I'd want strings to be represented by fingertree-based ropes of arrays, for O(1) cons/snoc/uncons/unsnoc and O(lg n) split, concatenate, index, etc. You can even have it be UTF-8 interanally: Just have each array segment keep track of how wide each character is. So if you have a bunch of English text, then Greek text, then Japanese text, then back to English, then you have one or more arrays where the characters are one byte, then some where they're two, then some where they're four, then back to one.
OK, slight improvement: type family IsHead (p :: kp) (q :: kq) :: Constraint where IsHead (p :: k) (q :: k) = p ~ q IsHead p (q x) = IsHead p q hoid :: forall t a. (IsHead t a) =&gt; a -&gt; a hoid = id Now this gives nice error messages: test :: [Bool] test = hoid @Maybe [False] • Couldn't match type ‘Maybe’ with ‘[]’ arising from a use of ‘hoid’ • In the expression: hoid @Maybe [False] In an equation for ‘test’: test = hoid @Maybe [False] I think this is a better approach for error messages, rather than saturating the type with `Any`, because it will work with type of kinds such as `(* -&gt; *) -&gt; *`.
Node is a single threaded event loop. Go has N:M green threads like Haskell, and a fair attempt at smooth support for CSP.
Last time I tried GHCJS and JS interop, you needed to explicitly export values from HS to JS and manually dispose them (essentially playing manual garbage collector). Does the module do that automatically? Or does GHCJS not require that anymore?
I sympathize with your preference for language features. And I think `hoid` feels a lot like one, which is encouraging. The `hoid` versions seem more minimal to me than the alternatives you gave (e.g. the exposure of the arities). I anticipate that will be true for any case where the goal is just to fix the head of the type that's the domain or the immediate codomain. And FTP made that pretty common. Also, have you heard if signature sections likely to be implemented in GHC?
We should improve that (but we'll still need `Ptr`/`Addr#`-variants, as not every buffer can be a GC-friendly `ByteArray#` created in Haskell-land)
&gt; we'll still need `Ptr` / `Addr#`-variants ... yes, I wasn't implying that it would replace them completely.
Neither give good error messages with kind `(* -&gt; *) -&gt; *`. Consider: newtype T f = MkT (f Char) Yours gives: *Hoid&gt; :t hoid @T [True] &lt;interactive&gt;:1:1: error: • Couldn't match type ‘[Bool]’ with ‘T (Lookup T * [Bool])’ arising from a use of ‘hoid’ • In the expression: hoid @T [True] Mine gives: *IsHead&gt; :t hoid @T [True] &lt;interactive&gt;:1:1: error: • Could not deduce: IsHead T [] arising from a use of ‘hoid’ • In the expression: hoid @T [True] 
The site looks great actually! The thing is - pledging does not work. Every few days/weeks I login and try pledging for snowdrift and it always says: "We are redoing the pledging system right now" - waiting for the day when I can actually throw my money at you! :-) 
That's a pretty informative answer, and it helped me fix something up in my code. I've recently written something similar to the above, with the goal of specifying the associativity and precedence rules once and then being able to create a parser from it (using [the `parsers` package](https://hackage.haskell.org/package/parsers-0.12.3/docs/Text-Parser-Expression.html)) and a pretty printer for it (using [the `ansi-wl-pprint` package](https://hackage.haskell.org/package/ansi-wl-pprint)). I'd written some doctests for what I had, and had some QuickCheck for my specific uses of the machinery, so I was pretty confident - until I spotted that your `isParens` function is different to mine. So... I wrote some QuickCheck to create arbitrary operator tables, and then to create arbitrary expression from those operator tables. I checked two properties: that pretty printing and then parsing returns the original expression, and that pretty printing and then removing a pair of brackets either breaks the parsing or returns something other than the original expression. It turns out my version had some subtle bugs, and yours passes with flying colours. Yours also does the right thing if you throw AssocNone into the mix, for handling things like `==`.
&gt;you or him must recode all the instance methods again for this particular stack I'm missing what you're saying here. In the mtl style, all of your application specific code will only mention the part of the stack it actually deals with. If you add something new to the stack, only the places that care about that portion of the stack would have to be touched (what environment is there in all of software development where this wouldn't be the case?). Yes, the MonadMine might have to have some of its class methods updated, but it's just one location. Perhaps it would be easier for me if you gave a specific, simplified example of what you have in mind. If you're just adding a single new variable to the state part of the stack, that just seems rather trivial to me. If you add a whole new thing in the stack that *is* potentially substantial and *should* have a closer look, afaict.
Exactly what information do you have available at compile time with checked exceptions implemented like this? A type of `ExceptT MyException IO` enforces neither that the function will only throw `MyException` nor that it even throws any `MyException` at all. All you have created is a second exception mechanism on top of the first one that the function may or may not use. You'll still have to deal with IO exceptions. In terms of static safety it is exactly as binding as a comment in the haddock about which exceptions the function will throw. I'm all for as much static safety as possible, but this doesn't actually seem to give you any.
I really don't like the thought process that because unchecked exceptions exist, checked exceptions provide no safety. Checked exceptions ensure that you correctly deal with at least those exceptions that are checked. It doesn't enforce the unchecked exceptions unfortunately, but such is the nature of *unchecked* exceptions. This doesn't mean that the checked exceptions are worthless. Having them checked gives me guarantees about at least those exceptions. So, I generally avoid throwing unchecked exceptions. When using code that throws unchecked exceptions, I frown a little bit, then figure out the appropriate way to deal with them, whether that means rethrowing in a checked manner, or dealing with the exception directly. It's an unfortunate conversion due to what I see as a bad convention. But in the (surprisingly common) case that the `IO` actions that I'm using document no unchecked exceptions, you just get to live nice and easily in a checked world, buying you compile time guarantees about error handling. The usefulness of unchecked exceptions lies in asynchronous exceptions, in my opinion. Such exceptions are meant to be terminating, and shouldn't be caught anyway. For what it's worth, I think [this Well-Typed article](http://www.well-typed.com/blog/2015/07/checked-exceptions/) represents a really smart way to implement checked exceptions in a performant way. For the most part, I just default to `ExceptT`, but that could be a better solution. 
Awesome! It looks like there's a decent amount of overlap between bits of our projects - I'll have to keep an eye on that repo. That link also looks pretty handy :)
When windows gets "Linux support" won't this be resolved?
I already had some understanding of building a monad stack, but if got much better after reading this presentation. Thank you. 
So... does Wrinkl have a website? Is there an email address or web form to submit applications? Also, what does Wrinkl do besides author jobs descriptions loaded with buzz words? [From original post](https://www.reddit.com/r/haskell/comments/4pcwv1/haskell_software_developer/d4km6t0)
I find [shelltestrunner](http://hackage.haskell.org/package/shelltestrunner) quite useful. And if you like darcs, there's [darcs hub](http://hub.darcs.net). I would bet money there are some other useful executables on Hackage we haven't mentioned yet. 
Shake can do this: http://shakebuild.com/why 
I completely forgot about that and yet I use it.
Until/unless Zeal gets native support for Hackage, you can use [haddocset](http://hackage.haskell.org/package/haddocset) to generate docsets. Here's a partial copy of [my comment from another similar post](https://www.reddit.com/r/haskell/comments/4of53q/haddock_at_your_fingertips/d4cm7mi): ---- * Create your initial haddocset. stack exec haddocset -- \ create -t ~/.docsets/haskell.docset * Build packages in `stack` with `--haddock`. I often do, stack build foo --haddock Or to get all dependencies of my current project, stack build --only-dependencies --haddock * Afterwards, simply add packages to the docset. stack exec haddocset -- -t ~/.docsets/haskell.docset \ add -s "$(stack path --snapshot-pkg-db)"/*.conf That gets you the docs for the packages that are part of the stack snapshot. For the rest, do the same but with `local-pkg-db`. stack exec haddocset -- -t ~/.docsets/haskell.docset \ add -s "$(stack path --local-pkg-db)"/*.conf I'm using the `-s` flag to skip existing packages, but you can also use `-f` to overwrite them. Skipping tends to be a lot quicker, but you probably want the latter if you've changed the global stack snapshot you're using. --- &lt;/end copy&gt; Note that, - This gives you a single global docset you can throw into Zeal/Dash. - With Dash (probably with Zeal too), you have to quit the program before you can `add` new packages to an existing docset. Otherwise it will occasionally run into conflicts in accessing the underlying SQLite database. 
Allow me to sort of hijack this thread for discussing something I've been thinking about. [Portage](https://wiki.gentoo.org/wiki/Handbook:AMD64/Working/Portage) has a concept called [USE flags](https://wiki.gentoo.org/wiki/Handbook:AMD64/Working/USE). These are very much like the existing Cabal package flags, but with one crucial difference: packages can depend on other packages having a specific flag enabled or not. So, for example, you'd write (in pseudo-syntax) build-depends: aeson[+profunctors] to signal that you need `aeson` to have `profunctors` support. This could be used as a way out of the orphan instance dilemma: - `aeson` defines a package flag `profunctors`, off by default - `aeson` defines `Profunctor` instances and depends on the `profunctors` package iff the `profunctors` flag is set - a downstream application/library depends on either - `aeson[+profunctors]` if it needs the `Profunctor` instances (and is thus okay with the dependency on `profunctors`) - `aeson[-profunctors]` if, for whatever reason, it needs `aeson` to not export the `Profunctor` instances - `aeson` if it doesn't care This way, we get no orphan instances, but only those dependencies used (transitively) by an application must be built. Thus, libraries can freely interoperate without the user suffering from increased build times and such. Of course, the added flexibility comes with significant downsides: - The test matrix blows up even more because, in theory, each combination of flags should be tested. This isn't at all realistic, so one would probably end up testing with all flags active. However, most errors resulting from this should at least be compile-time. - There is more potential for dependency conflicts if one dependency requires `package[+flag]` and another `package[-flag]`. However, if flags are used as outlined above, `-flag` dependencies should be exceedingly rare. - Stackage and similar efforts, as well as binary distributions, have to turn all flags on and are thus no better off than in the status quo. Indeed, if library authors decide to add support for more libraries (and thus more optional dependencies) because of this, some Stackage users would be worse off. Nix could go the Stackage route or replicate the flag functionality. - With the system being as general as it is, it could be used for all sorts of complicated nonsense. I'd be sympathetic towards a more restricted design.
I'm not yet at the stage where I can apply to a primarily Haskell based role but that's a place where I would like to get later this year. Out of curiosity - and as a very rough estimate - what is the current market price of a salary for such a position? EDIT: Also, if I *did* have the skills, then I'd definitely have applied for this job. Your company and product sound amazing, and you've done a good job of explaining what the position entails. 
That's interesting... more important than avoiding orphan instances that way you could depend on a concrete configuration of a package. Like e.g. whether to use `tls` or `HsOpenSSL` which may result in different types being exposed by the package. That would be great to have.
Sorry for being dylsexic: It should be HRLMP, ''Haskell Road to Logic, Math(s) and Programming'', which is available online at https://fldit-www.cs.uni-dortmund.de/~peter/PS07/HR.pdf or in bookstores.
This module helps with exporting the values and calling them on the other side. It runs the GHCJS RTS side-by-side with the JavaScript code, so you should get away without the [`releaseCallback`](https://github.com/ghcjs/ghcjs-base/blob/master/GHCJS/Foreign/Callback.hs#L37-L47) calls. But **I don't know if you need to play GC**; you shouldn't have. The Haskell values aren't really exposed to the callers, there's an EventEmitter, onto which we attach a single listener for "call" requests and execute exported values that were stored in a global map. There's only a single point of exporting a function to JavaScript land, which is this listener. Everything else is happening on Haskell land and should work as expected. That being said, I've not tested if the GC works as it should.
To be clear, here's the thing... **This module doesn't allow passing thunks or Haskell values across the boundary.** All values have to be serialised onto JavaScript values. Correct me if I'm wrong, but I doubt that's expensive for most use-cases. It's what the edges of any browser handling library will do. - - - It might be confusing, but even the functions exported with `GHCJS.CommonJS.exports` aren't passed through the boundary. You can call them like you describe if you want, but you can also skip it through the wrapped functions, which treat Haskell land and JavaScript land as separate worlds.
Considering they are still looking for a CTO I imagine they don't really know yet what they do...
What if some version on `aeson` doesn't define `profunctors` flag, say the functionality isn't anymore optional? How Portage handles those cases?
How do GHC and Ermine handle this part of the compiler?
&gt; There is more potential for dependency conflicts if one dependency requires package[+flag] and another package[-flag]. You said this is rare, but I don't think it exists at all. I personally use -flags in ports, but it's to turn off things like X11 that would be on by default. I turn it off because I don't want to pull in X11, but if it was already there for whatever reason then I would drop the -flag. In Haskell, if we say that we only allow such "flags" to be for additional optional features and never for features that are on but can be turned off then this shouldn't be a problem. 
Very cool examples!
The easiest way is probably to have an update event loop (yes, they exist in Haskell too), where with each timestep/event you update the state accordingly. Essentially building a giant, more general, state machine - where transitions are triggered by said events. This wholly violates the idea of encapsulating different parts of your program (though it is still very modular) but with strong enough types, you can often encode most of what you'd want from encapsulation directly into the structure of your data. Sometimes all. This also puts you in a good mindset for thinking about it in an equational and immutable manner, which tends to be easy code to get right.
I don't think Purescript is bad, but it changes too fast and too opinionated for my taste. 
Functional Reactive Programming is one approach to dealing with state in functional programs. Although it's web-focused, I have a tutorial for getting started with FRP [here](https://github.com/reflex-frp/reflex-platform#tutorial).
I updated `GHCJS` to stack `lts-6.4` https://www.reddit.com/r/haskell/comments/4pih2i/ghcjs_stack_lts64/
You will probably have one big state and a function **PlayerInput -&gt; GameState -&gt; GameState**. The trick is how to decompose that function into manageable components that only know about the parts of the global state they actually need. Updating deeply nested records is a bit of a pain in Haskell. There's a solution for that called "lenses" (composable getters/setters) but unfortunately they are a bit above newbie level. I would try to get by without lenses at first. At the very least, you'll gain insight into their motivation. Remember that records can have type parameters and allow polymorphic update (you can change the type of a parameterized record using an update). This can be useful when modelling. See [this talk](https://www.youtube.com/watch?v=BHjIl81HgfE). Try to keep the main "update function" pure. That way it will be easier to switch presentation layers. A modelling gotcha: having a "shared reference" between two objects can be dicey if you want to update the shared value, because of purity. Sometimes, it's better to store an indirect reference, like some kind of id value that is later looked in a map. When you want to change the shared value, change the value in the map.
Thought you were at Twitter!
A thousand times this. Just expanding it, since the comment may be too intractable for somebody just learning Haskell, you'll loop over something that looks like this: main = game initialState game state = do displayState state i &lt;- getUserInput let newState = updateState i state game newState That's the overall idea. After a bit of practice, look at the State monad (and StateT) and the lens library.
Your program doesn't do the same thing as mine. No counts, no spawning, no RNG. It looks like the small portion of server logic that you wrote is actually longer than my Haskell logic. Give me a program I can run that actually does the same thing mine does. 
I lied a bit. The Haskell code also exports functions to look into the state, extract subparts from it and serialise to JS. The point is though, the (expensive) serialisation only happens when needed. Usually most of the UI does not need to be re-rendered, React optimises that away if you tell it that none of the data has changed. Only when the data changes React will ask you to re-render the UI and that's when I request that the data be serialised to JS.
IMHO (and you should rather talk to a lawyer if you need a more authoritative opinion): Yes, you need to make available all source-code you combined with the GPL-library that got translated by `ghcjs` into a combined work that got transferred and executed in the client's browser. See also [Implications of using GPL-licensed client-side JavaScript](http://greendrake.info/content/nfy0)
[removed]
I'd keep views in Haskell if the state is there... It sounds like a very expensive way of having `immutable.js` in a JavaScript app in terms of bundle size and development effort. Unless there's some generic "look into the state" and "extract subparts" thing that you can expose to JavaScript.
C++ post cpp11 isn't exactly a joy, but boy you can abuse that template specialization process.
Your definition of subarrays, the early inlining and pattern matching isn't really helping, but I guess that is the point of the exercise. I believe especially explicit recursion should be avoided for equational reasoning since it requires an inductive argument to prove it's equivalence with another term or a translation to a higher-order construct in which case you could have used that directly. But the argument can be fixed with a little handwaving. You left of here: mss (x:xs) = max (x + maximum (map sum ((tails . reverse) xs))) (mss xs) You can observe that while `tails . reverse` isn't exactly equal to `inits`, it is equivalent modulo inner and outer order. That is `[[x1,x2],[x3,x4] ~ [[x4,x3],[x2,x1]] mod permutation`. And `map sum` doesn't care about the inner order and `maximum` doesn't care about the outer order. If you prefer it more mechanically you can use `tails = map reverse . reverse . inits . reverse` and see the reverse's eliminate each other, bubble up and disappear. So we have: = max (x + maximum (map sum (inits xs))) (mss xs) At which point you can follow the original argument by Bird and end up with: = max (x + foldr f 0 xs) (mss xs) where f x y = max 0 (x+y) You can check that that `foldr f 0 (x:xs) = max 0 (x + foldr f 0 xs)` but we need `x + foldr f 0 xs`. In the case `x + foldr f 0 xs &gt;= 0`, it is indeed `max 0 (x + foldr f 0 xs) = x + foldr f 0 xs` but what if not. Well you can inductively proof that with base case `mss [] = 0` and `mss (x:xs) &gt;= 0` per hypothesis `mss xs = 0` the second argument `mss xs` in `max (x + foldr f 0 xs) (mss xs)` is always `&gt;= 0` and so tacking an additional `max 0 .. ` on the first argument has no observational consequence therefore in this context it is `max 0 (x + foldr f 0 xs) = x + foldr f 0 xs` so we arrive at: max (foldr f 0 (x:xs)) (mss xs) where f x y = max 0 (x+y) We can proof that `max x1 (max x2 (max x3 ...))) = maximum [x1,x2,x3,...]` therefore we get = maximum [foldr f 0 (x1:..:[]),foldr f 0 (..:[]),foldr f 0 []] where f x y = max 0 (x+y) Which is basically = maximum . map (foldr f 0) . tails where f x y = max 0 (x+y) = maximum . scanr f 0 Which are the last two steps of Birds Argument.
Not any longer! I switched a few months ago
This is a nice book: https://www.manning.com/books/functional-programming-in-scala You can technically replace the examples in the book with some Haskell examples and publish it as a Haskell textbook :)
`cabal` can have multiple versions of `ghcjs` just as well. Even with pre-cabal-1.24 the binaries are installed with a version suffix if we use your versioned src-tarball of ghcjs, e.g. ~/.cabal/bin/ghcjs-run-0.2.0.820160417-7.10.3 ~/.cabal/bin/ghcjs-boot-0.2.0.820160417-7.10.3 ~/.cabal/bin/ghcjs-pkg-0.2.0.820160417-7.10.3 ~/.cabal/bin/haddock-ghcjs-0.2.0.820160417-7.10.3.bin ~/.cabal/bin/ghcjs-0.2.0.820160417-7.10.3 ~/.cabal/bin/hsc2hs-ghcjs-0.2.0.820160417-7.10.3 ~/.cabal/bin/ghcjs-pkg-0.2.0.820160417-7.10.3.bin ~/.cabal/bin/haddock-ghcjs-0.2.0.820160417-7.10.3 ~/.cabal/bin/hsc2hs-ghcjs-0.2.0.820160417-7.10.3.bin ~/.cabal/bin/ghcjs-0.2.0.820160417-7.10.3.bin And `ghcjs-boot` manages a versioned folder as well: ~/.ghcjs/x86_64-linux-0.2.0.820160417-7.10.3 So there's no technical reason you can't have multiple `ghcjs` versions installed with cabal as well. And with the nix-store in future cabal versions we have the ability to isolate different versions of `ghcjs` executables (or any other build-tool for that matter) at the nix-store level.
They're still in pre-release "stealth mode", so I'll refrain from commenting on the business itself, but on the technical side, Obsidian (my company) has been serving as their interim CTO to help them get started quickly. We've helped them make the initial technical decisions and we're helping build the MVP. We also handle QA, devops, and other functions that don't need to be in-house just yet. I'll be happy to answer any questions you have about the technology, dev practices, etc.
are you fpnoob / stacklover? My experience echoes most others around here. I worked with fiddling cabal sandboxes and such before but stack simply works better. I'll check back when cabal has "solved" the problem.
&gt; I'm familiar with the more comprehensive way and I'm also a nix user. When cabal accomplished that goal I'll revaluate it and compare it to stack. For the moment though, stack solves my problems best. What on earth do you get from Stack if you're using Nix? In my experience, it's just an extra layer of indirection that provides me with extra compilation time compared to just using Nix. I only use it to package up things for people that _don't_ use Nix (for reasons I do not comprehend.) :-)
/u/tolsyz, any support for using this with a dev version of GHCJS? We have a forked version with an important bugfix that would be nice to use this with.
Well, honestly it's because I'm having trouble setting up a multi-source nix project despite reading the manual section on it and ocharles' wiki on it. I love the idea of nix, thoroughly enjoyed the paper, have committed some derivations in the past, but I'm having a very hard time adjusting to a pure nix workflow despite a large time investment. Stack didn't take long to figure out and use at all.
I guess, depending where the fix is, it will be a different approach. If it is just a `ghcjs`: it would be a repack, if libraries they would need to be patched. The scripts I use are in: https://github.com/tolysz/ghcjs-stack 
To me the deal breaker is that i have to be able use private code frequently. My company host a private git server like many others. stack support add git commit hashes directly as dependency. that solve most of the headcache when we use haskell in our project(you don't have to manually clone/pull...). That's been said, the first class support for vcs system is a key feature to any modern package manager(godep, npm...). I really want cabal support this. It there a issue report yet? Or should i make one?
Thanks. Servant sounds like worth looking at.
Unfortunately I don't know enough about Nix to answer that :-( How well does Nix handle solving individual install-plans for all of Hackage? I.e. currently about 10000 packages with about 70000 package versions? I read somewhere that Nix is already hitting scalability limits with the Stackage subset, so I'd expect this to be even worse with all of Hackage. And how would cabal's install-plan solver integrate with Nix?
oh hey, i was totally at the meetup(girl in the pawprint dress) didn't think to ask when i was there, but do you have any opinions on purescript as far as functional -&gt; js languages go? i was unaware of ghcjs before this talk, and id already been intending on picking up purescript. now im finding myself a bit uncertain haha
I've changed this line to `getDayList = [minBound .. maxBound] :: [Day] ` right now. But because of haskells lazyness, wouldn't it be regardless because I never pick a number greater than 6?
That's good to hear! Make pledging work, the other stuff can follow :-)
caution - rickroll! and not even close to funny
It's not completely unheard of. For example, `libreoffice-5.1.3.2` requires `x11-libs/cairo[-xlib-xcb]` (for whatever reason, but that's large software projects for you). But on the larger point, I agree that this isn't a problem as long as this feature is used exclusively for optional library features. The problem with that being, of course, that enforcing such social contracts never works perfectly and has educational costs which one would rather avoid if possible.
Your proposal is indeed simpler from a user point of view, as well as less general and more magic. The first quality is a definite advantage; the second might well describe a better tradeoff between complexity and generality. What I'm not quite sold on is the magic part: Under your model, adding a dependency can trigger the activation of arbitrary code in other dependencies, and I won't necessarily realise what's happening. This should not be a problem if the system is used only for the purpose you envision, but I'd still rather do things manually. (Then again, writing `[+profunctors]` for each dependency because I like profunctors would also get old quickly. I don't think there's a right answer here.)
Why iterate at all? `Day` has a derived `Enum` instance already (as it should), so you can just use `toEnum` instead of indexing into a list.
[removed]
Thanks! I couldn't imagine there is a way to derive an efficient algorithm from merely specification, that is, equational reasoning until I read this book. Bird's another book, *Pearls of Functional Algorithm Design* introduces tens of algorithms with equational reasoning and each reasoning is quite different from other. For me it was really hard to read and I don't know how he - and you - made each step of reasoning. How can I *train* equational reasoning? Is this a well-defined topic of computer science, or acquired from long period experience?
This looks _amazing_. Thanks so much for taking the time to upgrade from a personal project to an educational project.
Wow! ლ(✿◠‿◠)ლ This is a very impressive piece of educational work! I tried it locally just now. Can't wait to get more free time to go through it in detail.
You mean like [this](https://github.com/mstruebing/today.hs/blob/master/Today.hs#L14-L18)?
It is really cool! Interesting coincident -- I'm implementing STG right now too. How it compares to [miniSTG](https://wiki.haskell.org/Ministg)? Also re this [issue](https://github.com/quchen/stgi/issues/5): &gt; Writing a Haskell compiler to desugar them to STG is probably easier than writing those nested pattern matches by hand and debugging the result. You can use GHC as a [frontend](http://blog.haskell-exists.com/yuras/posts/turn-ghc-into-frontend-for-ministg.html).
Can anyone tell me how I should test this? Should I only test the 'api' --&gt; getDayFromTimestamp?
Thanks for explanation! Yeah, I mean sometimes you want to observe some particular case and more or less understand what's going on there in general by switching small parts, like going from lazy to strict. 
I got a chuckle out of the Lol Monoid instance. That, said, there's room for a ["Haskell for the impatient"](https://www.amazon.com/Scala-Impatient-Cay-S-Horstmann/dp/0321774094/)-style book.
Oh boy that's why I put programming in quotes, we weren't exactly concerned about such fickle things as the "real world" or being "productive" back then haha. I studied mathematics so the programming classes were taught like math
This is great! Could this STGi tool also be a tool for programmers to understand how to optimise their Haskell code for performance, and possibly even to exploit lazy evaluation? There are various places dotted around on the web that encourages the inspection and understanding of core if you want fast code (though very, very little on how to exploit laziness for faster runtimes), e.g. [here](http://www.haskellforall.com/2012/10/hello-core.html) [here](https://skillsmatter.com/skillscasts/6495-keynote-from-simon-peyton-jones) [here](http://stackoverflow.com/a/6121495/1526266) [here](http://hackage.haskell.org/package/ghc-core) [here](https://donsbot.wordpress.com/2008/05/06/write-haskell-as-fast-as-c-exploiting-strictness-laziness-and-recursion/) and [here](https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/faster.html) . For example in that last link (from "*Faster: producing a program that runs quicker*" in the GHC docs): &gt; If profiling has pointed the finger at particular functions, look at their Core code. lets are bad, cases are good, dictionaries (d.&lt;Class&gt;.&lt;Unique&gt;) [or anything overloading-ish] are bad, nested lambdas are bad, explicit data constructors are good, primitive operations (e.g., eqInt#) are good,… How much of "bad Core" (w.r.t runtime costs) is de-sugared whilst STG'ifying it, and how much "bad Core" is left exposed at the STG level? Could STGi be a tool to highlight code that may potentially be inefficient to evaluate compared to an alternative STG pattern with the same denotation?
In GHC pipeline STG is very similar to Core, except that most of type annotations are removed, every subexpression is explicitly named, and primitive ops and constructors are saturated. See [here](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/GeneratedCode) for details.
GHC 8 actually accepts that hand written instance. See https://ghc.haskell.org/trac/ghc/ticket/12120 for a similar bug. Is it actually an issue for you, or just a curiosity? GHC doesn't always strictly follow the standard, and fixing it would just break existing programs for little benefit. Edit: https://ghc.haskell.org/trac/ghc/ticket/8883 is also relevant.
I learned Haskell soon after getting introduced to programming through C++. To program in Haskell from the perspective of an absolute beginner programmer was mainly the experience of trying to program very basic things and getting hit with a ton of weird error messages from GHC that I didn't understand. I spent a lot of time trying to understand C++'s error messages in order to complete assignments, but Haskell took it to a whole new level. Type inference is NOT your friend when you starting out, neither is lazy evaluation. It's hidden information that affects how your program runs, so just because your code says one thing doesn't mean the compiler sees it the same way - and that was frustrating. GHC will frequently fail to take bad code and point out clearly why it's bad. Instead it will infer the types in a weird way, take issue with that, and then give you an error message featuring types that don't actually exist in your code. And then there's the parsing error messages. I stumbled early on how to actually format a module header in Haskell. I would write something like this: module solution import Data.List ... This would give an error like this: error: parse error on input 'solution' What am I supposed to do with that? So I google and check how I'm supposed write the module header in the first place - turns out it has to be upper-case. I write this: module Solution import Data.List ... Now I get an error like this: error: parse error on input 'import' "Wait, that didn't fix it. It just changed the error message. Maybe the upper-case wasn't the problem? What's wrong with my import? I've seen it work before..." This is what Haskell was like at first and this was just one trivial example, there were many others. The learning experience mainly consisted of being regularly paralyzed and terrorized by error messages. Despite this, there were aspects to Haskell that I really liked at that early point compared to C++ that convinced me to continue with it after the class ended. I found composition was significantly clearer and more intuitive than OOP - it just made more sense to build things with functions over data types than to use the contrived/over-engineered crap that was objects and classes. "Who the hell talks about the world as objects and classes? Why are we structuring a bank account as a class?" This was my impression of the paradigm. It seemed way more theoretical than functions and composition did, because functions and composition were familiar concepts from my high school and college math classes whereas OOP introduced a very weird way of talking about the world without really justifying why we should be programming that way. I got that it could help us structure our programs better and get good code reuse, but can't you get that with just plain old functions. It didn't make any sense. So I found learning how to program with Haskell was an experience of getting frustrated with the programming environment that I had to work with and understanding cryptic error messages, but at the end of the day there was something very interesting about Haskell that pulled me in from that point on. At the time it was just function composition, but the language started looking even more attractive as I used C to write parallel programs (the significance of pure functions) and refactor thousands of lines of Python code (the significance of pure functions combined with an expressive static type system). If I had to teach someone to program from scratch, I would go Python -&gt; Haskell -&gt; C. I've witnessed first-hand how quickly someone with no programming experience can actually get a program up and running with Python and start to feel that empowerment of "Wow, I could make the computer do *anything* I wanted...I could make *anything*". It's really cool to be sitting with a student when they have that realization. That would be your basic 101 introduction class. For the advanced programming class where students would traditionally get moved on to OOP-styled programming, I would opt for a language that focused on functional programming and function composition. Haskell is fine for this. Then, I would move a student on to C as I don't think you can have a complete education as a programmer and computer scientist with just garbage-collected languages. One of the biggest lessons my professor drilled into my head since day one was the importance of the memory hierarchy and how if you ignore it you're basically asking to be put out on the street and made very, very poor. Apparently it's not *that* apocalyptically bad because Ruby and Node.js exist, but it's still pretty important if you want to make high-quality software that others can build upon. Plus, knowing C is pretty useful for Haskell as it complements Haskell's strengths quite well. So yeah, getting exposed to Haskell early on in my education was overall a good experience and actually turned me into more of a functional programmer than an OO programmer, but I think it's important to keep in mind that at that experience level there's a lot of very basic things that are difficult to do as a newbie and Haskell doesn't exactly do anything unique to alleviate those problems. GHCi was pretty useful, but man....those error messages. By comparison, Python or some other dynamically typed procedural programming language is much more straight-forward to understand quickly. Where Haskell really starts to shine is when you get into topics of how to handle the complexity of software and dealing with modern software engineering problems. Basically, the moment you need to bring OO into your life is when you should jump ship to FP/Haskell. **Disclaimer:** At the time I was starting programming and learning Haskell, my world view had been heavily influenced by General Systems Theory and the like. So decomposing the world into systems, functions, verbs, etc. made a lot of sense to me, I might have just been weird at the end of the day.
People laugh at the titles of these books, but when you stop and think about it, 24 hours is pretty close to the amount of lecture you get in a 3-unit, semester-long college course.
&gt;I've recently stopped using nix for a while, after some dramas with OS X and with GHC moving to 8.0.1 in whichever channel I was using and me struggling to update bounds and various feature flags on the packages I was using. I'm sure I'll be back at some point though :) On osx I couldn't even build Darwin stenv or GCC so I didn't get far. I love nix, but I also want to get work done. 
You're right. GHC 8 does accept the second one. It's not an issue for me in a particularly bad way. I try to avoid `FlexibleContexts` in favor of the higher kinded typeclasses from `Data.Functor.Classes`. My expectation has always been that GHC would require me to enable an extension if it was going to derive something that required `FlexibleContexts`. It's not very difficult to just reason about the instance that GHC will produce, but it would be nice to get the error if it's doing something incompatible with the extensions I have enabled. It's not a huge problem for me, but it definitely seems like a bug that it doesn't require you to enable the extension.
Actually, I don't think it will look like magic. You don't notice that aeson doesn't have profunctors because you don't have them on your system so from your code there is no way to verify if they exist or not. Once you install profunctors (for some other reason, perhaps) then it appears that aeson always had them, you just couldn't access them before. What change would you notice?
running $ stack setup ends with: Run from outside a project, using implicit global project config Using latest snapshot resolver: lts-6.4 Writing implicit global project config file to: /home/pi/.stack/global-project/stack.yaml Note: You can change the snapshot via the resolver field there. Downloaded lts-6.4 build plan. Fetching package index ...remote: Counting objects: 218936, done. remote: Compressing objects: 100% (176463/176463), done. remote: Total 218936 (delta 58485), reused 172550 (delta 40275), pack-reused 0 Receiving objects: 100% (218936/218936), 51.29 MiB | 89.00 KiB/s, done. Resolving deltas: 100% (58485/58485), completed with 1 local objects. From https://github.com/commercialhaskell/all-cabal-hashes * [new tag] current-hackage -&gt; current-hackage Fetched package index. Populated index cache. Did not find .cabal file for type-eq-0.5 with Git SHA of 44ef3dae4dacdc2729a2fde3ced964bd52ae05c7 Right Nothing Did not find .cabal file for language-ecmascript-0.17.1.0 with Git SHA of 3f4043479db04f39fe3734b61ebfee6384a02596 Right Nothing Did not find .cabal file for ixset-typed-0.3 with Git SHA of e2f9019388d77b3fb23b2fb6395564a6cd5bd7d9 Right Nothing Did not find .cabal file for fay-builder-0.2.0.5 with Git SHA of 45d24fd3663ae74696726d87a783469679ad6472 Right Nothing Did not find .cabal file for fay-0.23.1.12 with Git SHA of 8dbcdfd039559115ca6928f4140cc880b1a4e6b3 Right Nothing Did not find .cabal file for HDBC-2.4.0.1 with Git SHA of df385b10294c417ae6f2667b9cbc7c020393722f Right Nothing I don't know how to install GHC for (Linux,Arm), please install manually Please help. 
This isn't quit the same thing you're asking about, but I teach Haskell to new programmers, around ages 11 to 14, all the time! Given the age, I'm particularly focused on resolving stumbling blocks in the process. (Okay, perhaps you shouldn't call it Haskell exactly since I've used the RebindableSyntax extension, a custom Prelude, and compiler message post-processing to make big language changes; but if it's compiled with GHC(JS), that makes it basically Haskell... right?) First of all, I agree with /u/0ldmanmike that error messages in Haskell are pretty bad. In fact, if I have to pick languages with awful error messages for beginners, I'd say C++ first, then Haskell in a close second. The reasons: * Curried functions guarantee that if you forget a parameter to a function, your error message will be a type error involving higher-order functions. * Type classes are the enemy of comprehensible errors. Once a type class creeps in, the compiler will try to unify anything with anything, and then complain about the missing instance in an unexpected place elsewhere in the program. * GHC makes a lot of suggestions that are just really awful for beginning programmers. Get your definition syntax wrong? GHC helpfully suggests that maybe you meant to enable TemplateHaskell. Stray comma? GHC helpfully suggests enabling TupleSections so that your syntax error becomes a type error with yet another unintended higher-order function! Other trivial mistakes can bring about suggestions to use DataKinds, etc. I strongly disagree with your proposed approach of learning math, then type theory and lambda calculus, and *then* programming for the first time. It's just not how the brain works. The goal in learning any programming language, especially for a complete beginner, should be to get something interesting to happen as quickly as possible, NOT to front-load theory which will be forgotten because of the lack of an advance organizer. Another observation that some Haskell programmers find surprising is that even students with no prior programming experience still struggle to express themselves with functional programming. This is no longer surprising to me. The abstractions of function and variable provided by functional programming are simpler than the alternative; but expressing ideas using simple building blocks has *always* been hard. It's a skill that needs to be learned. Fortunately, it's also a skill worth learning.
I learned programming starting with Haskell. Before that I was a mathematician, the kind that does math on paper with a pencil. This is partly a lie since I had taken some CS classes in high school and college that were in either C++ or Java and I had programmed a little in those and in JavaScript, but Haskell was the first programming language I really used "in anger", and was (still is) the vehicle through which I learned more broadly about programming.
Any chance of getting this for 64 bit arm (`aarch64`)?
I think you'd first need an ARMv8 GHC!
Wow, a Haskell job in Turkey! I am pleasantly surprised, good luck!
Haven't both Carmack and at least one other guy done blog series on game programming in Haskell? It is quite the change, but suitable levels of polymorphism (like lenses) make adding to the state not so bad.
I thought I did.... But now I realised I don't :( u@tegra-ubuntu:~$ uname -a Linux tegra-ubuntu 3.10.67-g458d45c #1 SMP PREEMPT Mon Feb 8 17:44:18 PST 2016 aarch64 aarch64 aarch64 GNU/Linux u@tegra-ubuntu:~$ file /usr/local/lib/ghc-7.10.3/bin/ghc /usr/local/lib/ghc-7.10.3/bin/ghc: ELF 32-bit LSB executable, ARM, EABI5 version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=c2002709f062e00b92536c9c6770e7fa9d96f7d8, not stripped 
This is excellent! The machine state transitions in [Evaluate.hs](https://github.com/quchen/stgi/blob/master/src/Stg/Machine/Evaluate.hs) are well commented and easy to follow.
&gt; guaranteed compile time polymorphism Can you explain? Like, monomorphization?
This is the second Haskell job in Ankara, actually. I worked at the first one for a while :-)
Sure, but the fact that you've chosen lambda calculus and type theory as your two subjects indicates that, at some level, understanding computation *is* your goal. When you have an environment in which your students can experiment with computation and get instant feedback, it's just a waste to make them learn it all in abstract terms first, and then spring on them at the end the instant-feedback environment that could have helped significantly back when they were struggling to apply the basic ideas. Neither of us is talking about software engineering, which is a whole other can of worms.
People laugh on aggressive marketing such books use. It's obvious to everybody except maybe newcomers in industry that it's impossible to learn technology on good enough level by any means by investing only 24 hours. Lectures are usually accompanied with exercises, course project and so on. And this is just a start. Python MOOC on edx is ~50 hours. Not even Haskell or C++ But content of these books is usually opposite to garbage title (not talking about this book), which is good at the end.
Nix doesn't use a solver to select install plans. Nix behaves more like Stackage where you have curated predefined sets of packages that build together. The main benefit of using Nix compared to Cabal is that it works for non-Haskell dependencies as well, like C libraries. The main disadvantage (in my experience) is that Nix does not support incremental builds below the granularity of a single package, meaning that if you change one file in a package you have to rebuild the whole package. In theory Nix could support more granular builds below the level of a package, but the `cabal2nix` tool does not support doing that since it treats the entire package as a single Nix build product instead of treating each module in that package as a separate build product.
Well I have been wondering what the `o` stands for as well. It completely distracts me from focussing on the real meat of your blog post ... I just thought what a stupid question that must be (but damn is this`o` for ... google it, saw the korean references ...). As they say "There are only two hard things in Computer Science: cache invalidation and naming things" ;-) 
@/u/snoyberg Do you think something like this would fit stack better?
The compiler specialises all instances at compile time. If you define a function which operates on Monad, and use it on IO and State, it generates two copies of the code and directly calls the right one.
Yes, it is frustrating how many "western" companies assume you're a cheap worker if you apply for a remote job. I hear explanations such as "in your region we pay workers such and such..", I explain back that one of the main reason I work remotely is that I don't work for "my region" wages...
Interesting, which company is that?
I think one way that wouldn't be too bad would be to have `@Type` essentially just be a type restricted `id` function. Example: input: `@String mempty` desugar: `(id :: String -&gt; String) mempty` result: `""` Another: input: `@[] . pure $ 5 + 5` desugar: `(id :: [a] -&gt; [a]) . pure $ 5 + 5` result: `[10]` Now for fmap for example, if you want to decide the type of functor you should just do `@[] . fmap` and GHC would notice that `[]` isn't a fully applied type, and fill the remaining slot(s) with just `t`. This would also work fine for `foo bar` as you can simply do `foo (@Type bar)` Sure it is slightly more verbose than `foo Type.bar` but I would say the vast majority of the time you won't even need to use `@Type`, whereas you have to use `Type.` EVERY time even when the type is obvious. Also IMO my suggestion is much more consistent and disciplined then just sometimes having to use a module specific function and sometimes getting to use generic functions, and then sometimes having to type those generic functions explicitly the old fashioned way.
If you change the stack then you need to create a MonadMine instance for that stack. And this means duplicating some methods definitions or referring to the methods of other instances and modifying some of them. All of this for every new effect that you may have . Taking into account that a real medium size program may have dozens of states managed in this or that part of the application and this implies the addition of a new monad transformer IF it is coded functionally, not using global state one get an idea of the magnitude of the problem and the overhead that a programmer or maintainer may have using that approach.
This is almost redundant with type annotation sections (::Type) and conflicts with the stuff ghc is bulling ahead with, so consistent or not would probably be dead on arrival with the folks who decide this sort of stuff. E.g. Personally, I'd have loved @ for passing Proxy arguments like we do today, no muss, no fuss, but that proposal went over badly when Stephanie and company already had a student hacking on type application. Also, the type application stuff can handle things that are currently impossible without proxy arguments or Tagged, while this can't, so this proposal addresses few of the reasons why they wanted @ in the first place.
&gt; reference And if you really need them, you may be able to get by with either `STRef`s or `IORef`s, though laziness makes these act somewhat unintuitively; i.e. the contents of a ref can be a thunk, just like every other lazy value in Haskell.
please please no more food analogies for explaining abstract algebra. They get extremely awkward very quickly .. other than that, good talk `^_^`
&gt; This is almost redundant with type annotation sections (::Type) Can you elaborate on that, would my examples work with type sections, and if so would you mind showing me what they would look like? If they would be similarly concise then I will go back to my original point of type annotations not being that bad, and IMO much much better than having to ALWAYS write monomorphic code, even when the polymorphic version wouldn't be ambiguous. &gt; Also, the type application stuff can handle things that are currently impossible without proxy arguments or Tagged, while this can't Can you give me an example? Is that something to do with the whole `show . read` issue where the type is technically `String -&gt; String` but at the same time not really. &gt; so this proposal addresses few of the reasons why they wanted @ in the first place. I mean I am pretty much solely talking about the IMO really annoying issue that is a substantial amount of code being less polymorphic than Java / Python etc. while the rest of Haskell is leagues ahead. And my suggestion seems to more or less fully solve that part, besides the whole `show . read` thing.
Currently, `stack new` -&gt; code.
The Jetson TX1 (which I assume you're using) until recently only had a 64-bit AArch64 kernel and a 32-bit AArch32 userspace, so for all intents and purposes it was no different than the TK1, userspace wise. That was fixed recently; although you'll have to completely re-image the board with the latest release to get an Ubuntu 16.04/AArch64 userland along with a kernel. You'll also have to grab an existing ARMv8 binary from somewhere, although it looks like [Xenial has a port for AArch64](http://packages.ubuntu.com/xenial/ghc) as a package.
[removed]
1. I create a git repository for my project. 1. I create a cabal file using `cabal init` and I add `ghc-options: -W -Wall` to the auto-generated file. 1. I write a hello-world, in order to avoid an [ENONSEQUITUR](http://c2.com/cgi/wiki?HelloWorld). (so technically the rest of those points are after writing my first line of code) 1. I create a sandbox using `cabal sandbox init`. 1. I add a dependency to [doctest](https://hackage.haskell.org/package/doctest) and I write a dummy failing test, usually `2+2=3`. I open a separate shell and I use [ghcid](https://github.com/ndmitchell/ghcid#readme) or [fswatcher](https://github.com/ehamberg/fswatcher#readme) to continuously run the tests. 1. I make the tests green, and now I'm ready to go! I don't pre-emptively add any pragma or import, I simply wait until an error message tells me that it's time to add one. I should probably add `ScopedTypeVariables` though, because it's one of the few extensions which changes the existing behaviour instead of adding new syntax, and so GHC cannot suggest it like it does with so many others. I also like having a [hoogle](https://www.haskell.org/hoogle) tab open, but that doesn't count as a step I do before I start a new project, because chances are it's already open since the previous project :)
Read the lecture notes from here and if you have time do the projects: http://www.scs.stanford.edu/11au-cs240h/
Well if nothing else, it destroys any kind of type inference. If you have a `s :: Set a` and a `f :: b -&gt; c`, then `#map f s` wouldn't tell even tell you that your `b` and `a` don't match but would instead give you an obscure `No instance for IsLabel "map" ((b -&gt; c) -&gt; Set a -&gt; d)` error.
I think you forgot a step =P `stack new` -&gt; Rename `Lib.hs` -&gt; code
So edit your `~/.stack/config.yaml` like this: templates: params: author-name: Your Name author-email: your@mail.com copyright: 'Copyright: (c) 2016 Your Name' github-username: yourusername category: Development And then you can create a new projec with: stack new my-project tasty-travis C.f. http://yannesposito.com/Scratch/en/blog/Haskell-Tutorials--a-tutorial/#creating-a-new-project-with-stack
`(+1) . floor . logBase 2 . fromIntegral`
I usually recommend [the CIS194 course](http://www.cis.upenn.edu/~cis194/spring13/lectures.html), it seems to have been useful to a lot of folks. It's a series of 12 small-to-medium sized lectures, each accompanied by homework exercises that are pretty great.
Specifically the linked course, Spring 2013.
That would give the wrong result when x is a power of 2.
Note that the "bit size" (which I interpret to mean the smallest number of bits needed to hold a number) is off by one from the 2^n &lt;= x &lt; 2^n+1 definition; I'll assume the latter. Going through `integerLog2#` as mentioned in another comment ought to be the most performant way to go, if that fits your criteria. This is slow (O(n) operations) but: ``length . takeWhile (&gt; 1) . iterate (`shiftR` 1)`` Going through Double as in `floor . logBase 2 . fromIntegral` introduces rounding issues when x is too big (the smallest value on which it gives the wrong answer is 2^49 - 1). You could probably do a O(log n)-operation binary-search-based thing if you wanted to stick with non-Integer-specific operations, but that wouldn't fit in a reasonably short line. 
I don't edit my copy, so that my code will be more Kmett-like.
I've got my editor setup to auto-insert a version of that whenever I open a blank *.hs file - now I just need to get the module line automatically inserted...
Only downside is that `Traversal` is more terrifying and `traverse` is no longer a `Traversal` (although `traversed` is).
I'm always torn about things like this... On the one hand, making it a total function is nice, but it's a programmer error to pass 8 as an index, and leaving it partial ensures that the error is detected.
&gt; MagicLabels would remove the "open-world" assumption associated with normal type classes Ouch. Good luck getting that working with the OI(X) constraint solver. 
1. Git, along with a nice `.gitignore` file. 2. `cabal init` along with a `LICENSE` file, `README.md` and `src` directory 3. `default.nix` using `cabal2nix`, followed by `cabal configure` from within `nix-shell` Once I have this set up, I create a module and load it into GHCi. Haskell's Emacs mode is smart enough to recognize I'm using cabal and nix, so it loads the module using `nix-shell`. I don't have any imports or extensions I *always* use. The one flag I almost always turn on is `-fdefer-type-errors`—I should put it into my global GHCi config, but I've been too lazy. I've thought about wrapping all of this into a script or having a template git directory I could clone from, but it takes so little time for each project that I haven't bothered.
Oooh, I like this. Using compiler internals ftw :)
Stack can watch source files and rerun your build or tests :-)
The "X" in OI(X) means that the constraints are independent of the constraint solver. I'm not saying implementing it is easy, but it's not particularly hard either (it's very similar to the existing HasField work, just a more extensive resolving strategy) 
I mean the solver as documented in the last half of the OI(X) paper, not the OI(X) system itself.
You should start a Haskell or Functional Programming meet up in your city. I started one in Sydney, Australia over 8 years ago and its still going strong.
Here is a free Haskell tutorial actually called "Learning Haskell": http://learn.hfm.io
A while ago now, we wrote a JFP article on our experience of teaching beginners Haskell: http://www.cse.unsw.edu.au/~chak/papers/CK02a.html (the preprint link has the full article without a paywall)
Maybe is a sum type, where `Nothing` is equivalent to `()`. type Maybe a = Either () a The dual of a sum type is a product type. type CoMaybe a = ((), a) So as far as I can tell, `CoMaybe` is just the `Identity` functor, right? Or did you have something else in mind?
There are some pretty big downsides actually. 1.) We don't know how to do indexed lenses/traversals this way. 2.) It doesn't cooperate with `base`, requiring all fresh plumbing. On the other hand, you can write `lens` compatible lenses with no extra dependencies at all. 3.) Extending (2) `traverse` isn't a `Traversal`. Going back and forth between a pure profunctor representation of traversals that isn't just 'representable profunctor represented by an applicative functor' is annoyingly tricky and doesn't play nice with any of the existing ecosystem. These are the reasons why we haven't adopted the 'pure profunctor' approach in `lens` itself. I like talking and thinking about lenses this way, because it lets us do things like unify `re` and `from` in `lens` and works in more categories, but it is a bad fit for such a library. A lot of the adoption of lenses comes from the fact that folks can at least supply them to their users without incurring crazy dependencies. We explicitly opted out of building things them way as well, because we didn't want to split the community. Re: `Strong` vs. `Star`, you can show the two definitions are equivalent, and that you can translate lenses back and forth. It is a trivial exercise using the fact that `Star` is `Strong` in one direction and using `uncurry` `dimap` and `first'` in the other.
You can prove it by careful application of the laws and extracting the `(s -&gt; a, s -&gt; b -&gt; t)` getter, setter pair that must exist in one representation, and you can show the other must be a provably equivalent application of `dimap` and `first'` with a similar pair of functions.
It'd probably be `(Void, a)` if you just want to sit there and flip products/coproducts, initial/terminal, from 1 + a to 0 * a, so it'd be a functor w/ no constructors. data Null a This comes up from time to time, but isn't very Maybe-like.
You just forgot that `1` is dual to `0`, that's all.
Is this a joke? It's like complaining that we're using reddit to discuss this instead of the "free and open" mailing lists.
No. Maybe fifteen years ago, but not once a giant community has formed and people are betting their livelihoods on it. Haskell is not the only project with a benevolent dictator. It's more about coherent vision and veto power, not freedom to add whatever toy feature you want.
I want to use it for CUDA, but so far I was having trouble getting all of the prerequisites I need set up. (Haskell was only a bit of the bother I was having)
Not a big deal, but I like the fact that in lens you can use traverse directly. It makes the library feel like a "natural" extension of the existing Traversable machinery, without discontinuity.
Wow, I just looked through some of code.world and it's a really cool approach to learning programming -- thanks for sharing! 
Why do you need a virtual machine?
Do you offer remote work as well? I'd be interested in part time remote work if possible!
Hi, I also work in Picus Security. Thank you for your interest! But we're currently only looking for onsite candidates.
Hi, The salary question is difficult to answer... The market salary depends on which (geographical) market you are referring to. We pay about as much for a Haskell developer as we would have for a developer with some other primary language (e.g. Java/C++/Python), if we had been hiring such developers. And thanks for the praise :) --Tage 
stack new, git init, vim project.cabal, gitignore.
Normally, you need a `contravariant` dependency for `Getter`s and `Fold`s and you need a `profunctors` dependency for `Iso`s or `Prism`s, but `Lens`es and `Traversal`s can at least be defined with nothing and for the longest time that was all anybody cared about. Re: traversals, the `Wander` class in `purescript-profunctors` is probably the best compromise we have for this in the vocabulary `purescript`'s type system allows. `traversed` is the `purescript` profunctorified `traverse` equivalent. So, yes, we can make up new names for everything, and new classes that we can use in this situation, but we then do _have_ to make up those new names and classes, and you'll still need `traverse` and the equivalent.
So you save namespace at the cost of type inference, the open world assumption, and the ability to reason at all about your code using laws rather than instance-by-instance reasoning. I'm going to run screaming now.
[removed]
Don't run away screaming just yet. ;-) The impact on type inference is a bit hard to assess. Clearly ambiguity problems can be introduced if one starts sprinkling `#` signs in front of all identifiers, but deploying them more conservatively might leave enough type information to make sense of the code. After all, it's very easy to introduce ambiguity with typeclasses (`show . read`) and yet somehow they are useful! I think the open world assumption is a red herring. Under this proposal, the `IsLabel` constraints would gain special constraint-solving behaviour that isn't really comparable to how typeclass instance resolution works. Just like the `HasField` constraints with ORF. On reasoning capabilities, it's true that if you see e.g. `#null` you might need to figure out its type in order to know which identifier had been chosen and hence reason about it. Ad-hoc polymorphism is certainly ad-hoc. But sometimes it's convenient...
The point of inferred typing is that you _don't have to_ annotate everything..
They are annotated for didactic purposes, and the linter sends just a warning, as you say. The typechecker and therefore the compilation will work nonetheless
Thank you ozgurakgun.
Should that be `(f ||| g) . Left = f` and `(f ||| g) . Right = g` for the final two equalities in the paragraph defining binary coproducts?
it's actually pretty good documentation - so even if I don't at first I annotate every top/out-facing function so the next one looking at it (probably me) does not have to ask GHCi for the signature. Also in some situations the error messages you'll get will make more sense (to you) if you annotate first
Unfortunately, local functions of a polymorphic function can not be annotated without the forakk extension. So most people (like me) don't annotate them but because there is no clear line but the opposite: they can't do it. 
Also the arrows of `f` and `g` should be flipped in the same sentence.
Also, lambda-bound variables.
Yeah, I was thinking about mentioning the `ScopedTypeVariables` extension. I'm not so sure though, if this is the reason why most people don't annotate local function definitions. Even monomorphic or independently polymorphic local functions I see rarely having a type signature. I think it could also be, because people may tend to structure sub problems into top-level definitions, beginning by giving each a type signature, which often makes local functions simple enough, that they don't need further specification. Furthermore, `hlint` and `ghc` only complain about missing top-level signatures, which makes it really annoying to be negligent for these cases ;-)
You didn't annotate `+` ;)
Don't be a tease tell us how ;-)
I was doing some hairy `conduit` processing with CSV data, `lifted-async` stuff, `conduit-async`, etc, and the type signature was changing pretty frequently as I was adding stuff to the end of the conduit for testing in the REPL. I deleted the type signature and everything Just Worked and I got to develop and change quickly. Once I nailed down the final implementation, I pasted the inferred type from GHCi and wrote a doc comment.
Is there a way to have your own templates? I'm not doing open source and not using github. All the templates make assumptions which don't apply. 
&gt; Let me summarise Yep, that's correct. Although I was thinking it would still look for explicitly-declared instances, as well as using the magic ones inferred from scope. &gt; Should it be permissible to quantify over unsolved IsLabel constraints? My thought was that it would behave similarly to HasCallStack, in that un-annotated usages would be solved (and fail), but that giving an explicit type signature for `f` would delay solving to the caller (and similarly, that too could be further delayed, up the chain).
I'm reminded of the type inference lecture in my PL class, where the prof described three languages on an axis of type inference. Most inferred was OCaml; middle of the spectrum was Java; least inferred was a hypothetical language which he called "Awful".
You're surely trolling?
Nice article. Do you have an implementation on GitHub or similar that we could take a look at?
I've had this discussion with Java developers a number of times. A lot of people dislike type inference because the clarity that type annotations provide is fairly valuable. And that's true. Being able to see the signature of a function right next to its declaration is extremely helpful. But it isn't *always* helpful. The type annotation is often just getting in the way, making the writer write more than they need, and making the reader read more than they need. The rule of thumb is that if it's a library function that you're exporting, you should annotate it. Exported functions need to be *very* explicit, in order to make sure everyone knows what's going on. It's localized code that doesn't need annotation. Bindings in a `do` block, `let` bindings, and `where` clauses are all places where you'll be really grateful for type inference. In these places, you're often making values whose types are really obvious. Imagine if you had to give type annotations to every expression in a `do` block. It's just cleaner to have inference for these areas.
I am in the minority. I don't annotate unless it makes a functional difference, and then I see whether the code can be made clearer to better infer the type. I don't buy the argument that annotating the type makes code clearer, because I can always get the type from the editor and repl.
Yeah, I forgot to switch symbols, while very mechanically search/replacing. I just wanted to highlight quite how mechanical the process is.
I have something similar. It doesn't exactly follow this scheme but it's also a bit older. https://github.com/NorfairKing/super-user-spark/blob/master/src/Arguments.hs It also doesn't use config files (on purpose).
Why a monad? Isn't `lens-aeson` enough for your needs? https://hackage.haskell.org/package/lens-aeson
Oh I see. You're asking in an abstract sense. Perhaps you'll find this article helpful: http://www.haskellforall.com/2015/01/total-100-exhaustive-pattern-matching.html
Is there a timeline for that? If not, is there any way for those interested in getting it sooner to help you out?
I agree, but in this case the question mark was clearly implied, at least for me.
That's the difference then. No cost to me. Plus, I get to see what the compiler thinks the type should be vs what I think the type should be which sometimes also reveals oddities of the code that even the type system doesn't catch. But don't mind me. I've found I do many things differently than the majority.
I cannot understand why. I've provided two pieces of working code, the former is efficient but hardcoded, the later generalize it but with runtime penalty. My question mark is clear to me. Is not for you?, Is not for them?, Ok, but downvote when they not understand...
I'm curious what you're doing that can't be done just with Stack?
I misunderstood what you mean with dynamic and static. If I see this right, you want to have compile-time guarantee of the number of parameters/elements. I think there is only Template Haskell e.g. via [HList](http://hackage.haskell.org/package/HList) to do it. I don't know enough about dependent types, but they may also provide an solution for your problem.
Using well-known function names defined in Prelude as parameter names and writing a `Num` instance for simple counting is very displeasing. A `Monoid` instance or just a simple add function would have been a much better option. When I saw the headline of this post I expected an scholar to present his work and not a question to a problem. Having my expectation shattered made me angry indeed and I think that other people may react the same way.
Where did you encounter CoMaybe? 
If the compiler can write all your types for you, the type system must be pretty stupid. (Paraphrased from Conor McBride, possibly misinterpreted) I write types to state my intent, such that the compiler can tell me when I write code that doesn't match my intent. I like local inference just fine, but I'm not comfortable with missing top level types any more.
Hence why the type of the function you actually call from the outside is `IO ()`. It bugs me that people use arguments like that when Haskell explicitly marks the single most central function to be a real word mutator / transformer (`RealWorld -&gt; ((), RealWorld)` or whatever).
I mean I get that partial functions are problematic or whatever, but I honestly don't think they are that big of a deal. Particularly because in places where I use them and know they aren't going to return bottom, having to use `Maybe` instead would actually make things worse. Because then I have to handle `Nothing`, but `Nothing` means I fucked up my code pretty badly and so the best thing to do in the case of `Nothing` is `error "You fucked up kid"` at which point the original partial function is going to be easier to debug, particularly if there are multiple such functions and I want to know which one failed. Propagating `Nothing` up the stack in situations where the programmer fucked up does nothing but make the code harder to fix. With that said safe functions should absolutely live alongside the partial ones, for situations where `Nothing` is meaningful and not programmer error (mostly thinking about dealing with some sort of user input or external system).
The very simple API in the example? Since performance probably. Bigger APIs which you can extrapolate this blog post to? All of the usual Haskell advantages.
I'd say the best way is to just write a newtype over ReaderT itself. newtype NwMonad a = NwMonad { runNwMonad :: ReaderT NwConfig IO a } deriving (Functor, Applicative, Monad, MonadIO) But really, I'd urge you to use [Ether](https://int-index.github.io/ether/), as it makes composing monads of the same type painless.
An aside: I'd recommend avoiding `do` notation for newbies, it obscures what monads are actually doing. You can rewrite that function like: ``` main = game initialState game state = displayState state &gt;&gt; getUserInput &gt;&gt;= \i -&gt; game (updateState i state) ```
The usual technique is this: newtype NwMonadT m a = NwMonadT { runNwMonadT :: ReaderT NwConfig m a } deriving ... I'll let you work out adapting the rest of the details - it's straightforward. The basic idea is that you'll need `Monad` or `MonadIO` constraints for some of your classes and instances. I don't remember exactly which classes you can derive automatically and which you need to write out explicitly; GHC will let you know.
Why make? Most people just use cabal or stack, and you are not doing anything significant beyond what those do. If you want to lay the groundwork for a more complex build structure as the project grows, consider [shake](http://shakebuild.com) instead.
I started at [Habito](https://www.habito.com) about a month and a half ago; we're a fin-tech startup if you're feeling buzzwordy and a mortgage broker if you're not. We're based a minute's walk from Aldgate East tube, just off the end of Brick Lane. We use Haskell (with Postgres, hosted on AWS) on the backend and Purescript (with React+Redux) on the frontend. The dev team is four people, including me, and including the CTO. We're not actually looking for more devs at the moment, but are always willing to hire outstanding candidates; so if you (or anyone else) are interested, e-mail our CTO will@habito.com with your CV and see what happens. (Sorry for slow reply.)
You can fix that by writing the instance in the following way: instance (a ~ b, c ~ d) =&gt; IsLabel "map" ((b -&gt; c) -&gt; Set a -&gt; Set b)
We default to assuming `application/octet-stream` when the content-type is missing, as suggested by RFC 7231 3.1.1.5. I'm inclined to say that, insofar as you can, push back against clients that send a request body without a Content-Type header; it'll save you the trouble of having to worry about backwards compatibility in the defaulting rules, and avoid situations where you assume a content-type and then end up returning a 400 Bad Request for a decoding error when in reality the mistake was not an ill-formed body but in assuming the incorrect content-type (which will be very hard for your client to figure out). If you must default, /u/seriedivergente 's suggestion is one possibility. The other is adding `OctetStream` to the list of content-types (and add an appropriate `MimeUnrender` instance for it and `String`). I myself am not inclined to make the behavior on missing content-type headers configurable (I think it's just an accident of how other web frameworks work that this is often the case), but you could also open an issue to open up discussion about it.
But this is once per new employee / change to tech stack? Or really once per project?
For a concrete transformer stack there's little benefit, `ether` is useful when working with polymorphic transformer stacks. However if you want to lift not only `tell` but also `listen` or `pass` for one `WriterT` over another `WriterT` you'll find very quickly that there's no `liftListen` or `liftPass` for `WriterT`. For that purpose I have a library [`transformers-lift`](https://hackage.haskell.org/package/transformers-lift) that gives you a comprehensive set of lifting functions (and I use it in `ether`).
In case it wasn't clear, the point of this post is in replacing the Ruby implementation with a totally Haskell implementation, without having to implement everything at once.
At the moment it doesn't introduce any overhead and is as slow as nested transformers without `ether`. I have a work-in-progress lib `ether-flatten` that solves this, it's in the same GitHub repo as `ether`.
Was just followin suit with OP
This post was intended to serve as a proof of concept for taking some old legacy API and replacing it, endpoint-by-endpoint. Really, you can get this benefit from any language that supports a reverse proxy, but it's unlikely that many can have one with low enough overhead to be worth it. There's the correctness aspect from types, and the client/doc/swagger/etc. generation stuff that Servant can do. Performance is a component too. For what it's worth, here's the output from `httperf` on hitting the `/` route from Ruby. Keep in mind this is `WEBrick` which is *not* great, but I'm way too lazy to setup puma on this right now. λ ~/Projects/incremental-servant/ index* httperf --port=4567 --num-calls=500 httperf --client=0/1 --server=localhost --port=4567 --uri=/ --send-buffer=4096 --recv-buffer=163 84 --ssl-protocol=auto --num-conns=1 --num-calls=500 Maximum connect burst length: 0 Total: connections 1 requests 500 replies 500 test-duration 19.961 s Connection rate: 0.1 conn/s (19960.9 ms/conn, &lt;=1 concurrent connections) Connection time [ms]: min 19960.9 avg 19960.9 max 19960.9 median 19960.5 stddev 0.0 Connection time [ms]: connect 0.1 Connection length [replies/conn]: 500.000 Request rate: 25.0 req/s (39.9 ms/req) Request size [B]: 62.0 Reply rate [replies/s]: min 25.0 avg 25.0 max 25.0 stddev 0.0 (3 samples) Reply time [ms]: response 1.4 transfer 38.5 Reply size [B]: header 282.0 content 66.0 footer 0.0 (total 348.0) Reply status: 1xx=0 2xx=500 3xx=0 4xx=0 5xx=0 CPU time [s]: user 6.18 system 13.78 (user 30.9% system 69.0% total 100.0%) Net I/O: 10.0 KB/s (0.1*10^6 bps) Errors: total 0 client-timo 0 socket-timo 0 connrefused 0 connreset 0 Errors: fd-unavail 0 addrunavail 0 ftab-full 0 other 0 And the Haskell `index` branch, where it's doing the `lucid` HTML templating: λ ~/Projects/incremental-servant/ index* httperf --port=8080 --num-calls=500 httperf --client=0/1 --server=localhost --port=8080 --uri=/ --send-buffer=4096 --recv-buffer=163 84 --ssl-protocol=auto --num-conns=1 --num-calls=500 Maximum connect burst length: 0 Total: connections 1 requests 500 replies 500 test-duration 0.061 s Connection rate: 16.3 conn/s (61.5 ms/conn, &lt;=1 concurrent connections) Connection time [ms]: min 61.5 avg 61.5 max 61.5 median 61.5 stddev 0.0 Connection time [ms]: connect 0.1 Connection length [replies/conn]: 500.000 Request rate: 8136.6 req/s (0.1 ms/req) Request size [B]: 62.0 Reply rate [replies/s]: min 0.0 avg 0.0 max 0.0 stddev 0.0 (0 samples) Reply time [ms]: response 0.1 transfer 0.0 Reply size [B]: header 143.0 content 77.0 footer 2.0 (total 222.0) Reply status: 1xx=0 2xx=500 3xx=0 4xx=0 5xx=0 CPU time [s]: user 0.02 system 0.04 (user 32.5% system 65.1% total 97.6%) Net I/O: 2240.7 KB/s (18.4*10^6 bps) Errors: total 0 client-timo 0 socket-timo 0 connrefused 0 connreset 0 Errors: fd-unavail 0 addrunavail 0 ftab-full 0 other 0 Here's output from just running the reverse proxy: λ ~/Projects/incremental-servant/ index* httperf --port=8080 --num-calls=500 httperf --client=0/1 --server=localhost --port=8080 --uri=/ --send-buffer=4096 --recv-buffer=163 84 --ssl-protocol=auto --num-conns=1 --num-calls=500 Maximum connect burst length: 0 Total: connections 1 requests 500 replies 500 test-duration 19.984 s Connection rate: 0.1 conn/s (19984.3 ms/conn, &lt;=1 concurrent connections) Connection time [ms]: min 19984.4 avg 19984.4 max 19984.4 median 19984.5 stddev 0.0 Connection time [ms]: connect 0.1 Connection length [replies/conn]: 500.000 Request rate: 25.0 req/s (40.0 ms/req) Request size [B]: 62.0 Reply rate [replies/s]: min 25.0 avg 25.0 max 25.0 stddev 0.0 (3 samples) Reply time [ms]: response 39.9 transfer 0.0 Reply size [B]: header 290.0 content 66.0 footer 2.0 (total 358.0) Reply status: 1xx=0 2xx=500 3xx=0 4xx=0 5xx=0 CPU time [s]: user 5.76 system 14.20 (user 28.8% system 71.1% total 99.9%) Net I/O: 10.2 KB/s (0.1*10^6 bps) Errors: total 0 client-timo 0 socket-timo 0 connrefused 0 connreset 0 Errors: fd-unavail 0 addrunavail 0 ftab-full 0 other 0 There doesn't seem to be any appreciable overhead from the reverse proxy in this case, but I don't really trust the benchmark.
How do you know what to annotate with if you can't get the compiler to generate the type signature for you?
Oh, so it's just a bug. Thanks!
Are you looking for [` bound`](https://hackage.haskell.org/package/bound) or it's [tutorial](https://www.schoolofhaskell.com/user/edwardk/bound)? It's Lens-like in that /u/edwardkmett is inolved in both.
If you use nginx , you need to update the nginx configuration every time you add an endpoint (updating two different places rather than just one). And you might get it wrong. And you'd need to run nginx in your test suite to make sure you don't. While nginx obviously has uses (e.g. when which requests get proxied where is a much more static thing), here it seems much more sensible to go the route presented in the blog post.
This is what I didn't like about `ApplicativeDo`. I understand the value of turning existing `do` expressions into Applicative form, but I just think the special casing by GHC is terrible. I would have much rather seen a keyword like Scala's `yield` introduced that behaves like `ApplicativeDo`'s desugarring for `return` / `pure`.
Because that's the only one i knew in general programming. And I did not know enough about cabal until now, but wanted to write a small tool.
`foo :: (MonadReader r m, HasConfig r Config) =&gt; m ()` This does the right thing without too much worry.
Probably related to [this](https://github.com/yesodweb/persistent/issues/516) (and [this](https://github.com/haskell-servant/servant/issues/286))? After these changes, `Handler` is no longer a `MonadBaseControl IO`, so you'll need to have the underlying monad for the `ReaderT` be something else than `Handler`/`ExceptT` (e.g. `IO`).
Well each project varies. Certainly we can't share a setup across projects, they require different tooling, versions, OS even...
You can specify a template file directly instead of the name of one from the stack-templates repository in your stack new command. You can also specify custom variables using the -p option to stack new.
Haskell has terrible performance is confusing and it never works because it uses Category Theory, while C is closer to the metal and it is exactly how computers work inside and how we think
Thanks for the explanation :) 
What I dislike is changing the fundamental meaning of what are usually ordinary functions. Having a keyword makes it much clearer, and prevents issues like OP's
It definitely looks related to that. Thanks for the links. I will read through and try to implement something based on that.
Actually, that's completely unnecessary. If you just have `MonadReader Config m =&gt; m ()`, then `ask :: MonadReader r m =&gt; m r` will do the right thing even if the stack is `ReaderT Config (ReaderT SqlBackend IO) a` ((nevermind this is wrong))
[removed]
Well but that won't work if you have to call a `MonadReader SqlBackend m` function, which I presume is OP's problem? Otherwise, yes, the outermost `ReaderT` takes precedent, which obviously doesn't compose desirably. OP could also have the `ReaderT SqlBackend` on the outside, preventing the use of `Config`.
Now that I'm testing this, it's not working like I thought it did. Oops.
ghcmod, at least with my setup in vim, successfully returns types for me like maybe 20% of the time.
No, consider this program: foo = do x &lt;- something if x then bar else baz something :: X Bool bar :: X Int baz :: X Int instance Applicative X There is no way to compile `foo` as an applicative expression, even though the final expression has type `X Int` in the environment `x :: Bool`. The problem is that `x` is used outside of a `pure` expression. This expression only works if `X` is also a monadic type. The general rule is that the shape of an Applicative computation only depends on the shape of its arguments, not their contents. Lets say `X = []`; then if `bar` has 2 elements and `baz` has 3 elements, the shape of `foo` depends on the *contents* of `something`, not just its shape--that's too powerful for Applicative.
&gt; (Paraphrased from Conor McBride, possibly misinterpreted) Interesting, where does he say this?
Writing `Makefile` instead of a proper `.cabal`?
&gt; nothing I found on Hackage or in `base` allowed me to work with C pipes and raw file descriptors What do you need that for? What's wrong with `Handle`? I thought perhaps speed but then your [getLine](https://github.com/deech/hs-popen/blob/10b1663a997715e298504a9a1eeee30a03ecb8bc/src/System/Process/Popen.hs#L43) returns a `String`. If you need help changing that to return a `ByteString`, BTW, I can open a PR. I'd probably rename it to `pget` or something and have it take an `Int`, because [your impl](https://github.com/deech/hs-popen/blob/master/cbits/PopenShim.c#L20) doesn't necessarily return a whole line but 1024 characters of a line, or a whole line including the '\n' character, which is rather fiddly. BTW [the references here seem](https://github.com/deech/hs-popen/blob/10b1663a997715e298504a9a1eeee30a03ecb8bc/src/System/Process/Popen.hs#L54-L64) to refer to an `Examples/` dir which doesn't seem to exist. I think it should be `cbits`.
Right, but you're talking about software packages. I'm talking only about libraries. You have a point about social contracts, but the feature could be implemented in such a way that it can only be optional. There is no way to say "do this unless told not to" or to say "please exclude this feature".
&gt;If you change the stack then you need to create a MonadMine instance for that stack. If you *change* your stack then you need to *update* the MonadMine monad instance, no? But you update that one location and locations that need take advantage of the change. Everywhere else is unchanged. The story is perhaps a bit easier in an OO / imperative language but to get that ease of use you're giving up all the safety that the monad approach is buying.
Sounds like this Mac-specific Cabal bug: https://github.com/haskell/cabal/issues/2715. I don't know how to work around it. On the other hand, you're using `SDL 1.2.15`, i.e. SDL 1 branch which is old and deprecated by SDL authors. You may consider switching to SDL 2, i.e. use `sdl2` haskell package and `sdl2` brew package. I can confirm I use these on Mac without any problem.
By introducing a keyword with exactly the same name as a regular function. This is the problem: "return" and "pure" have very different semantics depending on whether you type them in a do-block or not. If a keyword is being introduced, it shouldn't have the same name as a normal function and disambiguate by heuristic-based context.
Some recommendations can be found from http://haskelliseasy.com/
I find this a bit overstated. Being able to create executables for both Linux and Windows is a selling point of Haskell for me. Perhaps the trickiest thing for the OP will be setting up something like postgresql-simple or sqlite-simple.
I can scrap the GUI if it will add too much complexity :) Sadly, it HAS to work on windows. If it's too big of an issue I will have to find a different project
Thanks this looks like a lot of fun and an accessible ramp into language design for a beginner. Nice articles for beginners are always useful. It nice when someone higher up lowers down a ladder for the rest of us.
No, it wouldn't. You added redundant annotations for no reason. Not having type inference would still free you from all annotations on the second line.
There sadly isn't yet a reasonable story for GUI on Windows, everything else (including working with a database) isn't really a problem.
Why not a CLI ? That would remove a massive amount of complexity.
What about [fltkhs](http://hackage.haskell.org/package/fltkhs) ?
Cross platform UI is probably easiest with something like threepenny-gui.
I take the dubious credit for complaining to SPJ when I was a beginner and convincing him to add the special case typing rules for `$` to make `runST $ ...` work "as expected". Now I cringe...
As far as I understand inference refers to finding types you don't know, i.e., of variables without annotation. if you already know the type of a variable, it is trivial to add an annotation to all of its occurrences... I think I understand your point, I just think you're talking about something entirely different of what OP had in mind... pretty sure he meant about top level type annotations, which just fill the types of bound variables... 
My wild guess: you're not seeing the raw output. The browser you're using is actually modifying the DOM it receives because it is invalid (unclosed &lt;span&gt; tag, perhaps ?). Try Ctrl+U (under Firefox) to see the actual HTML sent by the server (what is shown in the "Inspect element" drawer is the parsed / transformed markup).
You can use a tool like [Fiddler](http://www.telerik.com/fiddler) to see exactly what is being sent over the network.
Hi gdeest, The actual output is what I got from the page source of Firefox. The closed span did not fix the issue either.
Thanks for the response. Unfortunately, Fiddler does not run on my system - OpenBSD.
I feel bad saying this since I know the author actually cares about Windows users, but this http://hackage.haskell.org/package/fltkhs-0.4.0.9/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:7 is not good. You shouldn't be forced into a whole new environment on your machine (MSYS) just to be able to use a library. Stack includes a MSYS installation and it would be really great if you could just open a terminal there and `pacman -S` what you need. However, the last I've heard is that it never works like that. I don't really know what can be done about this. As far as I can tell, nothing will really change until Windows gets a proper package manager for native libraries. I've attempted to provide something similar with https://hackage.haskell.org/package/native where you'd only need a single command to get the libraries you want but someone still needs to write an actual working script that downloads proper binaries for a library and puts them in the right place. What I have works fine for simple things like libcurl, but anything more complicated doesn't really work.
&gt; Monoid a =&gt; Monoid (Map k a) Yes please! I don't understand why the default instance doesn't work that way.
Well then just use `curl` to see the exact HTTP response
Yea it's not hard to package a GHCJS app in Electron. It's not particularly performant though, considering the cost of compiling to JS
If the performance was the only concern left with regards to GUI I'd be a happy man :D
&gt; *IMPORTANT:* it XSS-sanitizes every bit of text in the Object passed to the template. Your [`Servant.EDE.HTML`](http://hackage.haskell.org/package/servant-ede-0.5.1/docs/Servant-EDE.html#g:1) value is getting sanitized. Serve up a `Tpl MyHTML` if you don't want your `ToObject` to be sanitized. Something like (not tested)— data MyHTML instance Accept MyHTML where contentType _ = "text" // "html" /: ("charset", "utf-8") type UserAPI = "user" :&gt; Get '[Tpl MyHTML "user.tpl"] User
/u/begriffs The description should be clarified that the latest stuff that beats GNU grep is not widely applicable since it only works for a very narrow set of state machines that are not as easy to encode using the nice algebraic API
Got it, I have changed the description. Not sure if my change will propagate to links already shared on twitter though. Sorry about that!
Somone's already mentioned it, but [threepenny-gui](https://wiki.haskell.org/Threepenny-gui) is the easiest way to get started with UIs and build UI apps that run cross-platform. Native GUI toolkits are a bit of a nonstarter. GtK is probably the best-supported and even if runs on Windows, it'll look ugly and be a complete pain to work with. The other alternative is using GHCJS and compiling to JavaScript (perhaps with [reflex-frp](https://github.com/reflex-frp/reflex)). You could then package it up using Electron, which is how the Atom editor works. There are limits on performance, but it shouldn't be *that* bad. If you want to give it try (and you're not *developing* on Windows), [reflex-platform](https://github.com/reflex-frp/reflex-platform) makes it easy to install all the relevant pieces to work with reflex and GHCJS. reflex-platform won't work on Windows simply because it uses the Nix package manager to install everything, not because any of the Haskell-specific parts won't work.
This solved my problem. Thank you very much.
Because of Rice's theorem - any nontrivial question about the behavoir of code is as hard as the halting problem, when taken across all programs. EDIT: Seriously guys, this is a lot harder than general AI or reversing entropy. Those are just varying degrees of monumental difficulty/improbability. This problem is undecidable. 
Well, let's take a step back. What does it mean to abstract over something, especially to do it well? It's a slightly more formal, more computery notion of *understanding* something. To build an abstraction based on lower-level ideas, we build a *deep, useful understanding* of those ideas. Is the problem of understanding low-level ideas hard? Of course it is! That's why programming and computer science are ultimately creative disciplines. Going *from* a clear understanding of something coupled with a formal model (ie abstraction in the CS sense) to the low-level thing is a reasonably direct operation. I mean, in practice, it's often non-trivial, but it's always a journey with a clear starting point and a clear goal. Going the other way is a lot harder because we don't know what the goal is or how to get there or even if it's possible. Think by analogy to NP: it's easy to *verify* a solution works but hard to find a solution in the first place. What you've noticed is the same dynamic writ large. This also explains why all the approaches we have to doing this (machine learning, neural nets, program synthesis... etc) are all so limited and primitive and yet are *also* such big advancements: it's an incredibly general, difficult problem. It's basically the problem of "how to do intelligence"!
This was a great presentation. I love the enthusiasm, the relatively clear code and the obvious practical utility! I'm also impressed you get get such good performance out of such simple code.
Nice, thanks for sharing this!
Thanks :)
The slides for the talk are located here: * https://github.com/Gabriel439/slides/blob/master/regex/regex.md
It's licence is a bit unclear, does Ambiata own your software if you use it? Do they own Reddit if the source is accessed as a link in this post? ;) Just make it BSD or MIT and enjoy free community support and the influx of ideas of others, make it closed and place it among other closed ones.
Pretty sweet ! I actually have been looking for a library like this.
&gt; The B language And here I thought you were referring to https://en.wikipedia.org/wiki/B_(programming_language)
Using Num typeclass for regexes is hardly elegant.
To each his own. Some people say inventing your own infix operators is hardly elegant. Or you make your API really verbose. You can't win.
Props on presentation and notes and article. Always glad to be able to digest things quickly (especially topics like these) because of good organized information.
&gt; Stack includes a MSYS installation and it would be really great if you could just open a terminal there and pacman -S what you need. However, the last I've heard is that it never works like that. It pretty much does for a couple of months now, modulo `stack exec -- ` in front of the `pacman`. (Or `stack exec sh` and do whatever.) For example: http://redd.it/4jpthu
I knew about that, but I also wanted to stick to the names used for the mini langauges introduced in the usual PLT textbooks and course materials. Starting with the B you linked to would have been a much harsher introduction to DSLs :) 
Two things: * Did you try switching the resolution to SD? There's a gear icon on the player which allows you to switch it. * Have you tried the download link? Chrome allows me to play the video straight without having to download it when following the download link below the video player. Also can you tell me more about how it craps out? Does it start playing for a while first, or not even get started? I do want to fix this and make the playback perfectly smooth.
&gt; As evidence, you can see that there is no avalanche as there was with Java That's because the entire advantage of Java at the time was the "write once, run anywhere" meme. It had nothing to do with the expressiveness, maintainability, and certainly not the speed of the resultant programs of the Java language. That's what the PFP advantage is: expressiveness, composability, and maintainability. There is a very real and attainable business value in this, but the cost is unorthodox reasoning about how we approach business problems, and that's always a hard sell to management.
Awesome stuff. Let's keep up the good work and build up more ML on top of Haskell!
The newly updated license looks similar to BSD and MIT.
&gt; That's because the entire advantage of Java at the time was the "write once, run anywhere" meme. It had nothing to do with the expressiveness, maintainability, and certainly not the speed of the resultant programs of the Java language. But that's not true. Sure, that was the tagline, but that wasn't why companies adopted Java, certainly not for server side applications. Back then, as today, most companies used a single OS for their servers anyway. We (back then I was in a large organization in the defense industry) made the decision to switch from C++ to Java (circa 2003, I think) when the evidence for significantly increased productivity was overwhelming, and could not responsibly be ignored. &gt; That's what the PFP advantage is: expressiveness, composability, and maintainability. Expressiveness is an inherent property; whether or not it is an advantage requires evidence; it may well be a disadvantage (it's like saying that Java's advantage is that it has classloaders -- trivially true, but maybe not an actual advantage). The rest are *purported* advantages. There is little evidence (to wit, none) that Haskell code is more maintainable than, say, Java code. I can easily explain why it would be more maintainable and why it would be less maintainable. Empirical evidence is required to settle this issue.
What do you mean? I have an idea and those are the requirements. I want to use Haskell to learn the language.
Thanks for the info! I think I'll go for a CLI. A GUI would add too much complexity without giving any real advantage
0 for match nothing, 1 for match empty sequence is a bit wacky to me. I see `(+)` defined but not `(-)`. I was expecting a full instance of Num, otherwise why bother using it. But that doesn't take away from the talk. 
I think it's definitely worth spending some more time on. Maybe have a look for other sources of information? The [Haskell wikibook entry](https://en.wikibooks.org/wiki/Haskell/Classes_and_types) is pretty good, as is the explanation in the [CIS194 course](http://www.seas.upenn.edu/~cis194/spring13/lectures/05-type-classes.html).
Typeclasses in Haskell are a way to allow functions to be able to take more general arguments. Consider this example implementation of the `length` function: length :: [a] -&gt; Int length [] = 0 length (_:xs) = 1 + length xs This takes a list of values of type `a`, and returns its length. Notice how we don't specify the type of the values, but allow a list of any type. We do this using a type variable, in this case, `a`. length [1,2,3] --&gt; 3 length ["hello", "world"] --&gt; 2 Let's say that we want to have a length function that only works on lists of numbers. How would we do this? In this case, we simply have to change the type signature to: lengthOfNums :: Num a =&gt; [a] -&gt; Int lengthOfNums [] = 0 lengthOfNums (_:xs) = 1 + lengthOfNums xs The thing on the left side of the `=&gt;` is a typeclass constraint. It means that instead of allowing any type to fit into the variable `a`, the type must be an instance of the typeclass `Num`. That is, `a` must be a number. Let's try our example usage again: lengthOfNums [1,2,3] --&gt; 3 lengthOfNums ["hello", "world"] --&gt; &lt;interactive&gt;:1:1: error: No instance for (Num [Char]) ... Now, if we try to use our function on a list that doesn't contain numbers, we get a type error. This is good! It can help us catch errors at compile-time, instead of run-time. Typeclasses are really useful for writing general functions, that is, functions that work on lots of different sorts of data. This example is somewhat contrived, but the typeclass mechanism is really useful for many things. Typeclasses essentially allow one to write functions for whole groups of types, and have the same functions work for each type; instead of writing a version of the function for each type. Does this help? It's hard to know what parts of the concept you don't understand, unless you help us out by specifying more. 
Perhaps would you get more answers if you explained your current understanding, no matter how approximate it may be. I'll try a quick explanation anyway. 1) A type class is not a type: it is a programming *interface* parameterized by a type. 2) If SomeClass is a type class, and *a* is a type, the fact that *a* is an instance of SomeClass is written: SomeClass a 3) In your example: class Numberish a where fromNo :: Integer -&gt; a toNo :: a -&gt; Integer The interface of the type class "Numberish" is given by two functions: fromNo :: Integer -&gt; a toNo :: a -&gt; Integer Where *a* must be understood as a placeholder for types for which a Numberish instance is defined. 4) In the following: newtype Age = Age Integer deriving (Eq, Show) instance Numberish Age where fromNo n = Age n toNo (Age n) = n A newtype wrapper for Integer, Age, is defined. Then, a Numberish instance is defined for type Age by giving a definition for fromNo and toNo. Note that the types of forNo and toNo in the typeclass definition are: fromNo :: Integer -&gt; Age toNo :: Age -&gt; Integer ie. the same as: fromNo :: Integer -&gt; a toNo :: a -&gt; Integer where Age has been substituted for *a*. 5) Finally, typeclasses can be used to program in a polymorphic way. Look at the following type signature: sumN :: Numberish a =&gt; a -&gt; a -&gt; a Everything on the left of "=&gt;" is called *constraints*. This function cannot be used if these constraints are not matched. In plain english, this can be read as: "sumN is of type a-&gt;a-&gt;a provided a can be proven to be instance of Numberish". Finally, the function definition: sumN a a' = fromNo summed where iA = toNo a; iA2 = toNo a'; summed = iA + iA2 Note that the functions toNo and fromNo are used without actually knowing what type *a* is. 6) In: k :: Numberish a =&gt; a -&gt; a k a = a k is simply a special version of the identity function (id) for Numberish instances. That's the gist of it. Hope it helps !
This is really cool. Seems like most of the hard work is done, with some kind of wrapper over cublas for matrix multiplication and (accelerate?) for other things it'd be as good as any framework. Would be nice somehow to de-couple the learning algorithm (SGD with momentum) from the gradient estimation too... 
Who knows maybe it is a Crazy James Bond GPL Affero, "You can see the source code but then I own you... licence? 
Nice, but I don't get how `satisfy` is working in the final code. It seems to be hard coded to state 0, but the `satisfy` could appear in any position. I looked through `match` thinking it must be shifting the states down as it consumes the regex but I didn't see that happening anywhere.
It serves a similar purpose to classes in object oriented languages, but is a compile-time construct. It might help to see the contrast: OCaml also uses inferred typing but it doesn't have type classes. So `+` for `int` is spelled `+` and `+` for `float` is spelled `+.`. There is also `print_int`, `print_string` and so on instead of just `print` or `show` as you're able to have in Haskell due to overloading enabled by Typeclasses. So yes, you'll need to learn them if you want to learn Haskell. EDIT: Actually, giving it a second thought, Typeclasses in Haskell are not like classes in other languages. They're very much like Interfaces but with the ability to specify a default implementation for the "methods". That is, you can have Typeclass hierarchies, one kind of data type can "implement" multiple different and unrelated Typeclasses at the same time, you can write code to an Typeclass instead of a concrete implementation, etc., etc.
You are awesome :)
Type classes are implicitly passed in records. The function `sumN :: Numberish a =&gt; a -&gt; a -&gt; a` takes two a's and returns an `a`, but it also takes an implicit argument of type `Numberish a` (this is indicated by the =&gt; instead of -&gt;). The `Numberish a` is a record containing the `Numberish` functions for type `a`. When you use `toNo` and `fromNo` in the `sumN` function they are looked up in the record that is passed in implicitly. The class definition is like a record type definition. The instance definition constructs a value of that record type, and sets it up so that this value will be passed in implicitly in functions that need it.
FTFY: &gt; Typeclasses in Haskell are a way to allow functions to be able to take more *specific* arguments. When you restrict the type argument to be numbers then they are more specific not more general. General: The type argument 'a' could be anything. It's more general. length :: [a] -&gt; Int Specific: The type argument 'a' ist restricted to types that implement the type class Foldable. It's more specific. length :: Foldable t =&gt; t a -&gt; Int
I think we're talking past each other. I'm saying "update existing", you're saying "create new" but if I look at a text diff of your "create new" won't I just see a line or two appearing to change in the existing monad instance? Of course, technically, one instance stopped existing and a new one was created but from the perspective of the source code, it's not a big change and nothing unexpected, no?
`x` is polymorphic - it can be whatever monad the context calls for. It can even assume different monads in the same expression: [ head x ] ++ [ fromJust x ] 
when you say `fromJust x`, you are evaluating x, but you aren't *mutating it*. As such, x remains polymorphic until a function demands that it not be. By analogy: Prelude&gt; let xs = [1, 2, 3] Prelude&gt; head xs 1 Prelude&gt; tail xs [2, 3] Just as when typeclasses were involved, just because we evaluated xs doesn't mean we are mutating it, which is why we can continue to call multiple functions that operate on xs original (and only) value. Perhaps this will confuse you further, but your experiment would continue to work with new monads: Prelude&gt; let x = return x Prelude&gt; import Data.Either Prelude&gt; isRight x True 
Not in a real file, but it works in the shell for some magic reasons... If think the `MonomorphismRestriction` may be responsible for something. https://wiki.haskell.org/Monomorphism_restriction 
A typeclass constraint is like an extra parameter to a definition. One implementation of typeclasses (which I believe GHC uses internally) is to convert to “dictionary passing style”, in which the implementation of a particular instance is passed as an argument to a function requiring some instance (and then inlined, &amp;c.). data MonadDict m = MonadDict { monadReturn :: forall a. a -&gt; m b , monadBind :: forall a b. m a -&gt; (a -&gt; m b) -&gt; m b } maybeMonadDict :: MonadDict Maybe maybeMonadDict = MonadDict { monadReturn = Just , monadBind = -- ... } listMonadDict :: MonadDict [] listMonadDict = MonadDict { monadReturn = \x -&gt; [x] , monadBind = -- ... } x :: MonadDict m -&gt; m Char x dict = (monadReturn dict) 's' So retrieving the value of `x` at different types in your code is equivalent to calling the function `x` with different dictionaries here: fromJust (x maybeMonadDict) == 's' head (x listMonadDict) == 's' 
It works in a real file as well unless the monomorphism restriction kicked in. But it only kicks in according to *syntactic* triggers: `x` was declared without syntactic parameters (on left side of `=`) and without a type signature, and had class constraints. All these conditions are true in your file so it indeed kicks in. If you add a type signature for `x` (or disable the monomorphism restriction), it will not kick in.
Everyone explains that "x is polymorphic", which is true. I'd like to add that one way to think about it, is to think of `x` as a *function* from the type you want to instantiate it at, to the actual value. So: `fromJust x` is really: `fromJust (x @Maybe)`. `head x` is really: `head (x @[])` 
[removed]
If you run this: Prelude&gt; let x = return 's' Prelude&gt; import Data.Maybe Prelude&gt; head $ fromJust x Gives an error because you mutate the x and receives the return type of *fromJust* type which *head* can't use.
What happened to me was about halfway my fans became really loud and video playback got choppy. Audio kept going just fine. I didn't explore further because I just downloaded the video and watched it that way. Edit: Firefox on Debian testing... I think.
The combinators `star`, `plus`, `+` and `*` shift states. So if you construct the regex `satisfy (=='B') * satisfy (=='y') * satisfy (=='e')` (corresponding to the string `"Bye"`) the last two `satisfy` will have their states shifted by `*`.
Thank you.
[TypeApplication](https://ghc.haskell.org/trac/ghc/wiki/TypeApplication): @maybe 
I have found repeatedly that trying to find the right job for a tool is much more difficult than finding the right tool for the job.
Isn't providing implementations made easy by passing them in as arguments?
Hm, that *function* actually clicked, thank you. It's like `Monad a` has a list of types it can be evaluated as, and the concrete instances of monad for list and maybe actually provide how that could be done: `fromJust x` leads to `fromJust (x @Maybe)` based on `fromJust` signature, which in turn unwraps to `fromJust $ ((return 's') @Maybe)` which is actually an instruction to call `return` of `Maybe` on `Char` 's', correct? I guess the @evincarofautumn explains further, I don't quite get the "dictionary" metaphor just yet. 
Num is often not fully defined. Perhaps it is still a bad thing, but it is common.
This is rather straightforward stuff as libraries go, but I'd still be interested in any critiques or suggestions. Especially, but not only, if you're someone who might use it. :)
How does it compare to [regex-tdfa](https://hackage.haskell.org/package/regex-tdfa-1.2.2), especially performance-wise? I agree that the algebraic approach makes for a neater eDSL.
Yes but `f`, the function driving `satisfy` is hard coded to check that the input it is looking for is accepted at state 0. You can shift the previous state 0 to be state e.g. 5, but how can you make the local `f` function suddenly pattern match on 5? This is what's not clear to me. EDIT: Nevermind, got it. In `satisfy` it looks like the code else shift nL (fR i (s - nL)) Is always ensuring the original `f` will end up seeing 0.
`-Werror` too.
In your edit, it looks like you've modified `Numberish` as follows: class Numberish a where fromNo :: Integer -&gt; a toNo :: a -&gt; Integer heck :: Integer -&gt; Integer (It would be nice if you had mentioned this to us, so there would be less confusion.) Here's the problem: type classes exist to be resolved based on *types*. The idea is that the compiler has several definitions of these functions to choose between, and it should try to choose the right one based on the type you expect it to have. So, for example, if you use `fromNo` in a context where the expression should be a `Year`, then it will choose the implementation for `Year`. But if you use it where it expects an `Age`, then the compiler will choose the implementation for `Age`. Your `heck` function, though, has just one type, `Integer -&gt; Integer`. There is no way for the compiler to choose between many implementations based on type. So it will never be able to choose the appropriate instance! To be more verbose, suppose you wrote this: class Numberish a where heck :: Integer -&gt; Integer instance Numberish Age where heck n = n + 1 instance Numberish Year where heck n = n + 2 myFavoriteNumber = heck 5 What do you expect your favorite number to be? Should the compiler choose the `heck` implementation for `Year`, and give `7`? Or should it choose the `heck` implementation for `Age`, and give `6`? There's no way to decide! So the program is ambiguous, and must be rejected. The way this gets resolved is that class methods have to mention the type variable in their types, so that the compiler can make a choice.
Answering a different question from your edit: &gt; `instance fromNo n = Year n` Why is the type `Year` written on RHS? Actually, it's the *constructor* `Year` that's on the right-hand side. Recall that when you write: data Year = Year Integer You are defining a type called `Year`, with a constructor *also* called `Year`. That constructor converts an `Integer` into a `Year`. So the reason it exists on the right-hand side above is that `fromNo` needs to return a `Year`, so you need to build one.
[removed]
Isn't the problem here that you've typed `'s'`, not `"s"`. I.E. `Char` not `String`? Prelude&gt; let x = return "s" Prelude&gt; import Data.Maybe Prelude Data.Maybe&gt; head $ fromJust x 's'
I'm reading the book too, but I'm a bit ahead of where you are. Let me try to help. &gt;After reading a little bit , what I understand is that &gt;1. `class` defines a new type class and its methods. That's correct, and a type-class is a set of types which all support curtain functions. It's a family of types. &gt;2. `newtype` is used to define a new type (like we use `data` maybe ?) Yes, `newtype` is a special case of `data`. `newtype` can only cope with types that have a single unary constructor, whereas `data` can do that and a lot more. So why have `newtype` if `data` can do the same job? It's more efficient. As there can only be a single value in a `newtype` it becomes an alias for that type. &gt; But then I am not understanding the syntax of it &gt; newtype Age = Age Integer deriving (Eq, Show) Age is a type that has a single constructor, called Age, which takes an integer. It will be stored as an integer, because `newtype` rather than `data`, but will be type-checked as a separate type. &gt; why is the type `Year` written on RHS? `Year` in this case isn't a type, it's a constructor which returns a value of that type. Unfortunately it's common to name the type and the constructor the same (like the Age example above) and this can be confusing. &gt;And when I did this it gives me an error &gt; instance Numberish Year where fromNo n = Year n toNo (Year n) = n heck n = n So here you're saying that `Year` is a member of the `Numberish` family (The `Year` type is an instance of the `Numberish` type-class). To qualify as a member of that family it has to have definitions for the functions defined in the `class`. `heck` is not in the class definition, and so doesn't belong in the instance. Hence the error. `Year` might be a member of several type classes and so would have several instance blocks written for it. Other functions not related to the type class are written outside of the instances. Hope that helps
Touché.
Neat. Some time ago, I also used Thompson's construction to build an NFA based regular expression engine as well ([parser](https://github.com/jason-johnson/frobo/blob/master/src/Text/ExpressionEngine/NFA/Parser.hs), [matcher](https://github.com/jason-johnson/frobo/blob/master/src/Text/ExpressionEngine/NFA/Matcher.hs) (I know, the code needs to be cleaned up)) , also skipping the last step of converting an NFA to a DFA. I like this strategy because the engine will only iterate the target string once no matter what: when the string is finished, whatever matched matched. I took a different strategy than you have here because the driver for me was to be able to efficiently combine lots of regular expressions into a single one (which means I must be able to examine structure looking for duplication). I also implemented grouping for it, though you should be able to do that the same way I did. I have not yet looked at performance, and my current implementation uses a ListT to handle traversing all applicable states "at once". Of course this means an expression like `a*b` on `replicate (10^8) 'a' ++ "b"` would generate around 10^8 duplicate states. I planned to use Oleg's set trick to do the trimming between elements of the string (which is what your strategy is directly doing).
In fact regular expressions are closed under negation, `negate` is possible to define such that, so `r + negate r ≡ star dot` and `match r x = not (match (negate r) x`. Also as `1 + 1 = 1` in regex algebra, `fromInteger` can be defined for all `Integer`s. With this definition of `negate` the interplay of multiplication, addition and negation is a quite different what you would expect from say integers, but `Num` doesn't state any laws... `abs` and `signum` is unfortunately something you cannot define. Interestingely something like `x + x = 2 * x = 1 * x = x` works. Actually alternative "multiplication" for regexes would be intersection (which is well defined as well), that why I'd prefer define regexes as a `Lattice` for union/intersection and `Monoid` for concatenation.
Thanks for the reply. Sorry for the confusion , but I was stupid enough not to change the type class definition for `heck` . The example you gave helped me understand much better.
Thanks for the answer, but what is an Interface ?
Glad it helped. I'd say make sure you're solid on the concepts in each chapter before moving on. The book builds chapter by chapter. For example if you moved on from where you are, the next chapter is the monoid type-class. If you're shakey on type classes in general, talking about a specific one is going to be confusing. Do the exercises! Be careful with the language used. Words like class &amp; instance have very specific meanings (as you've seen), but often different to what you may be used to. The author is good at explaining, but if you skip read thinking "I know what a blah is" you'll go wrong. The next few chapters give basic "patterns" for code. First couple (monoid and functor) are relativity straight forward, second two (applicative and monad) I found more difficult, but not because the concepts were hard, just that I felt I was juggling a lot of new stuff when I got there. That's about all I've done right now. 
I guess I'm just confused as to why `return x` can evaluate to something which is both a list and a maybe value. That's so odd to me.
I can write here a bit of a summary of what I've learned since then. Be advised though that I'm still in the learning phase, and in the previous post I was mostly just reinventing known things. Essentially, it all boils down to efficient reduction for lambda terms. One can use explicit syntactic substitution for normalization, but there are *abstract machines* that are much more efficient and also simpler in regards to naming and scoping. In the 80s and 90s there was a lot of work done on efficient compilation and/or interpretation of functional languages and a number of different abstract machines were created, for example GHC's STG, OCaml's Zinc, Clean's ABC, etc. Some simple machines are described neatly [here](http://gallium.inria.fr/~xleroy/mpri/progfunc/machines.2up.pdf). When doing dependent type checking, we need to do reduction, of course, and "checking without substitution" can be realized by using an abstract machine representation for terms instead of a pure syntactic representation. In fact, it's possible to generate machine code from terms in a JIT-like manner and run those when doing type checking. [Here's](https://depositonce.tu-berlin.de/handle/11303/3095) a work that demonstrates this for a simple type checker. Also, there's a project called [Coqonut](http://www.maximedenes.fr/download/coqonut.pdf) for developing a JIT compiler for Coq that works this way. The earliest reference I found for typechecking-via-abstract-machine is [Coquand's algorithm](http://www.cse.chalmers.se/~coquand/type.ps) from 1996, which has a very neat Haskell implementation in the appendix, and is almost the same as my type checker in my prior post. The same algorithm is extended later in [Mini-TT](http://www.cse.chalmers.se/~bengt/papers/GKminiTT.pdf), and also used in [this paper](http://www2.tcs.ifi.lmu.de/~abel/msfp08.pdf). Also, note that normalization-by-evaluation and abstract machines can be very similar (or even operationally the same) in this context. See [this gist](https://gist.github.com/AndrasKovacs/9892ec465fb32619902fc10e8003a286) for a small example. There are some pros and cons though. NbE allows us to easily compile binders into actual functions in the metalanguage, so we can make use of existing efficient interpreters and compilers. However, the `Value -&gt; Value` functions in NbE hide the underlying `Term`-s that are being interpreted. With abstract machines and explicit closures, `Term`-s are visible, and we have greater flexibility when doing reduction, for example we can easily add extra rewrite rules or rules for symbol unfolding. With NbE, if we want to parametrize reduction, we have to do that by cramming more information into the input type for the semantic functions, so instead of `Value -&gt; Value` we have `ReductionOptionsAndValue -&gt; Value`, which forces us to precommit to a fixed set of reduction options. Right now I'm thinking/learning about the feasibility of *only* using abstract machine representations in a more sophisticated dependently typed checker. The references I mentioned before only present simple "core" type checkers, but in a production-strength system we'd like to have metavariables, unification, type-directed eta-conversions, retained unreduced terms and a bunch of other stuff. I do not know at the moment, and I plan to investigate next, the extent to which Coq and Agda are already using abstract machine reduction during type checking. Coq has a fast bytecode interpreter, and Agda has a `Reduce` monad which I think is probably some sort of environment machine, so they both use AM's to some extent, and I would like to avoid reinventing the wheel. 
~~Probably because your time complexity changed: `O(n) -&gt; O(n^2)`~~ ~~Remember that `length` is `O(n)` for lists.~~ Oops, I thought `isPalindrome2` was called recursively. `isPalindrome` runs in `O(2*n)`. - `O(n)` to reverse the list. - `O(n)` for the full comparison. `isPalindrome2` runs in `O(2*n + C)`. - `O(n)` to calculate the length - `O(n/2)` to perform the `splitAt` - `O(n/2)` to perform the half-length comparison. - `O(C)` to perform "optimization" calculations and branching. `isPalindrome2` is slower because your extra code cost something. I would guess the branching is the biggest factor. The modulo operation might be costly also. For a definitive explanation, you would have to look at the Core generated by each piece of code. 
Think of an interface as something you gain by restricting the the type of a value. The restriction is expressed by a constraint. For example something :: a can be any value somethingelse :: Numberish a =&gt; a can't be any type, it must be one that supports the functions, or methods if you prefer (toNo, fromNo), that you have listed in the type class called Numberish. It is work to define typeclasses, but it saves you work later if you want you define functions that aren't committed to a specific data type but aren't general enough to work for any data type whatsoever. Later, if you want to generate a really long thread, ask the community when it's a good idea to define typeclasses versus just implementing a record. Or whether your type classes should be accompanied by implicit "laws". 
I see hmatrix as a convenient, batteries included library for one off matlabesque things but generally not suitable for production use due to poor performance, partial functions, and generally iffy API design. Id suggest checking out Hblas, repa, or other options if you haven't already. If you have, what made you pick hmatrix? Other than that this is pretty cool. I want to see more of this kind of thing done in haskell. There are definitely advantages to Haskell, the ecosystem just needs to catch up to the strengths of the language.
This error is because you bind `x` to a concrete type when you evaluate `fromJust x`. You also have other type errors in this fragment that are revealed subsequently, so here's a fixed version: v :: (Char, Char) v = let x :: Monad m =&gt; m Char; x = return 's' in let val = fromJust x in let val2 = head x in (val, val2) main = print v The key difference is I give an explicit polymorphic type to `x`. Without that, the type inference engine will bind the type of `x` when it encounters the first use in `fromJust x` to `Maybe Char`. You can see this yourself by replacing the result expression with `_` to make a hole: Found hole ‘_’ with type: Maybe (String, String) Relevant bindings include val2 :: Char (bound at test2.hs:9:17) val :: Char (bound at test2.hs:8:13) x :: Maybe Char (bound at test2.hs:7:9) v :: Maybe (String, String) (bound at test2.hs:6:1) In the expression: _ In the expression: let val2 = head x in _ In the expression: let val = fromJust x in let val2 = head x in _ This also shows that `val` and `val2` are both of type `Char`, so `(val, val2)` is type `(Char, Char)` and not `Maybe (String, String)`. Instead of giving `x` an explicit type, you could also enable the language pragma `NoMonomorphismRestriction` and the type inference system will give `x` a more general type: Found hole ‘_’ with type: Maybe (String, String) Relevant bindings include val2 :: Char (bound at test2.hs:10:17) val :: Char (bound at test2.hs:9:13) x :: forall (m :: * -&gt; *). Monad m =&gt; m Char (bound at test2.hs:8:9) 
Keep in mind that `String` is `[Char]` which is basically a linked list of characters. You can't do random access on a list, and operations like length are O(n). If you actually care about the performance of string processing, you should use the [Text](http://hackage.haskell.org/package/text) library. If you're sure you don't care about stuff like unicode, you can use [ByteString](http://hackage.haskell.org/package/bytestring) instead. Both of those allow random access and O(1) length.
why don't you move pragma extensions to .cabal file ?
Why did you think it would be twice as fast?
Given O(1) length and indexing I believe the best you can do is O(n/2). Since `String` has O(n) length and indexing, I think the best you can do is O(2n).
The trick is that `return` looks like a function of 1 argument, but is implemented as a function of 2 arguments. So `return x` is a function: * when eventually given a `Monad_Maybe_instance` argument, it returns a `Just x`. * when eventually given a `Monad_List_instance` argument, it returns a `[x]`.
I'm not the original author, but the coding standards at Ambiata (both the author and I work there) specify that language pragmas should be in the source file rather than in the cabal file. I don't see any good reason to change this. 
O(n/2) = O(2n) = O(n), I think he was after performance rather than asymptotic complexity. I actually don't get why so many "O"s are being thrown around when OP wanted to make the code 2x faster (no matter what complexity he starts with, he would stay in the same class after making the code run faster by constant factor).
Ok, more comprehensive response. There are some languages pragmas which are general benign, like `OverloadedStrings` and could be put in the cabal file with no ill consequences. Others, like `TemplateHaskell` should *only* be enabled for files that actually *use* `TemplateHaskell` features. Enabling `TemplateHaskell` for files that don't need it will result in slower compiles and potential debugging problems when the parser accepts valid TH expression in a file where there is not supposed to be any TH. Similar arguments could be made for advanced features like `PolyKinds`, `DataKinds` etc. Once you decide that a decent number of the language pragmas should be placed in the source files, it makes sense to put them all there. 
Text does not allow random access any more than String. It'll be faster because of the denser memory layout, but complexity is still O(n)
So I guess this happens any time you construct a value out of completely type-class generic functions, and I've just never experienced it because I tend to stick an implicit type signature along the way?
Huh, `unsafePerformIO` can implement `unsafeCoerce`. Thanks!
This isn't Hacker News. Putting "Show" in the title is a non sequitur.
Nice work. Did you read Simon Marlow's book on Haskell concurrency?
As you said I wanted to make the number of comparisons half, not time complexity, without too many overhead. I wonder if there is a faster way than `xs == reverse xs`for plain `String` or `ByteString`.
I found that `length` for String is O(n) but length for `Data.ByteString.Char8.ByteString` is O(1). But changing from `String` to `ByteString` already gave me enough speed up :)