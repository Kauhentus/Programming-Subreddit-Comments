I guess you are right. It doesn't explicitly say that Arrow is a more general form of Monad. But, I was going off of the following paragraph towards the end in that article &gt;Although Hughes’ goal in defining the Arrow class was to generalize Monads, and it has been said that Arrow lies “between Applicative and Monad” in power, they are not directly comparable. The precise relationship remained in some confusion until analyzed by Lindley, Wadler, and Yallop, who also invented a new calculus of arrows, based on the lambda calculus, which considerably simplifies the presentation of the arrow laws (see The arrow calculus). There is also a precise technical sense in which Arrow can be seen as the intersection of Applicative and Category. I did see thatdiagram and I was wondering if there is any way one can at least qualitatively relate (not necessarily a strict superclass or subclass) these typeclasses.
Ah ok. How would an existential type work with something like Ord. For example ["asd", 56] is a list of Ord's, but you can't order it in a sane and type safe way.
&gt; How would an existential type work with something like Ord? Badly. If you have two things guaranteed to be `Ord` but nothing else, you have no way to tell that they're the "same" `Ord`, and thus no way to actually compare them. This is (related to) why in Java, you need to `instanceof` when you override `.equals()`.
What do you think of Agda's Emacs mode? Is that "Integrated" enough for your taste? I'd love to have something of that level, and with that turn-key experience out of the box, for Haskell.
I'll just post this again: [Haskell needs to take a lesson from Agda](https://www.reddit.com/r/haskell/comments/334x2v/cartesian_closed_comic_26_ide/cqi1jcq).
You can find some related discussion attached to [my StackOverflow question](http://stackoverflow.com/q/24668313/477476) and the [related /r/haskell thread](https://www.reddit.com/r/haskell/comments/2aaz8s/are_arrows_exactly_equivalent_to_applicative/).
Never used agda mode for emacs. I only started using emacs about a year ago for Haskell. Before that was eclipse (java) and notepad++ or notepad2 or gedit (python). The biggest thing eclipse has that i miss in emacs is a side pane that shows all the top level definitions of the current module.
&gt; I was wondering if there is any way one can at least qualitatively relate (not necessarily a strict superclass or subclass) these typeclasses. Okay, let me try. Since Monad instances have kind `* -&gt; *` while Arrow instances have kind `* -&gt; * -&gt; *`, we're not even talking about the same kind of entities, so neither set is completely included in the other. They truly describe completely disjoint sets of type constructors, so neither is a super class of the other. In order to compare them qualitatively, we'll have to drop those indices. So in a type like `fmap :: (a -&gt; b) -&gt; f a -&gt; f b`, I'll drop the indices and keep only the information of whether the argument is a value constructed with the type constructor or not. I'll use `f` for types like `m a` and `k a b`, and I'll say that values of those type are "inside" the system described by `f`. And I'll use `a` for types like `(a -&gt; b)` which don't use `m` nor `k`, and I'll say that values of those type are from "outside" the system. First, here are the methods without that transformation: Monad and super classes: fmap :: (a -&gt; b) -&gt; m a -&gt; m b pure :: a -&gt; m a ap :: m (a -&gt; b) -&gt; m a -&gt; m b bind :: m a -&gt; (a -&gt; m b) -&gt; m b Arrow and super classes: id :: k a a first :: k a a' -&gt; k (a,b) (a',b) arr :: (a -&gt; b) -&gt; k a b And here they are again after the transformation: Monad and super classes: fmap :: a -&gt; f -&gt; f pure :: a -&gt; f ap :: f -&gt; f -&gt; f bind :: f -&gt; (a -&gt; f) -&gt; f Arrow and super classes: id :: f first :: f -&gt; f arr :: a -&gt; f Now we can start to see some relationships. `pure` and `arr` are both used to construct values inside the system out of values from outside. `id` is a primitive value inside the system, which doesn't need anything from outside in order to be constructed. But it's not that different from the values constructed from `pure` and `arr`; it looks like `pure` and `arr` can construct a variety of values while `id` can only construct one, but we know that `id` can construct identity morphisms for a variety of types, so the difference isn't that important. To make the two type classes easier to compare, let's drop even more information, by removing that leftmost `a -&gt;`. Monad and super classes: fmap :: f -&gt; f pure :: f ap :: f -&gt; f -&gt; f bind :: f -&gt; (a -&gt; f) -&gt; f Arrow and super classes: id :: f first :: f -&gt; f arr :: f Now we can see that `pure`, `id` and `arr` are primitives, while `fmap` and `first` are unary combinators, and `ap` is a binary combinator. Combinators construct slightly more complex values in the system from zero or more simpler values in the system, as I've explained in [my video about combinator libraries](https://www.youtube.com/watch?v=85NwzB156Rg). So let's drop even more information, unifying all combinators by saying that they take zero or more values inside the system and construct one new value out of them: Monad and super classes: fmap :: [f] -&gt; f pure :: [f] -&gt; f ap :: [f] -&gt; f bind :: [f] -&gt; (a -&gt; f) -&gt; f Arrow and super classes: id :: [f] -&gt; f first :: [f] -&gt; f arr :: [f] -&gt; f Ah, now we've finally dropped enough information to make an interesting qualitative comparison! Monad and Arrow both provide methods for constructing values in the system in the style of a combinator library, that is, via primitives and combinators. But `Monad` has something which `Arrow` doesn't! Its `bind` combinator is very special, in that instead of combining a fixed set of simpler values into a more complicated value, it uses a function which allows one of the operands to be picked according to a function. And we know from the use of monadic values as computations that this function is used by `bind` to choose the rest of the computation at runtime, after the computation has been constructed. Since Arrow doesn't have such a combinator, it's much more limited, as we can only build a fixed pipeline of values and then run it without modification. For this reason, I'd say that qualitatively, Monad is much more powerful than Arrow. One last thing: for a type to be an instance of a type class, it only needs to implement *at least* the required methods. So even though I've just described Arrow as a type class in which pipelines never change, that's only true of pipelines constructed using only the Arrow methods. You could easily define a system which implements the Arrow methods, and which also implements a `bind`-like combinator.
Your integration with `org-mode` looks really interesting! It seems like this might be the perfect solution. Now I just have to look into `org`, hehe.
Wow. That is very insightful. Is there a similar relation between other typeclasses? I should first probably try to understand what you said before venturing too deep into some of the more exotic typeclasses. Thank you for the explanation.
Seriously. I spent a few hours yesterday trying to get ghcjs working on windows with stack. No luck. This was my second try. I gave up a few months ago after finally getting the dependencies satisfied only to run into the windows path character limit.
Interesting, I didn't know that haste had a concurrency lib! In practice, we get lots of mileage out of having full blown concurrency and STM in GHCJS. Even stuff like supporting weak references ends up being relevant for FRP libraries.
&gt; Very helpful, but it would've been also useful to add some of the classes hierarchy and their associated operations (Functor, Applicative, Monad, Monoid, State, MonadTrans, etc.). I have been doing some reading ... I think you were hinting at additional cheatsheet pages that summarizes [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia). Is my understanding right?
probably we can use emscripten compile C libraries to js and let ghcjs use it?
I cloned your site, removed the code relating to Agda (since I don't know any, heh), copied the `defaultFileType` function into my `site.hs` and ran a `stack build`. I can build and view the site, but the LaTeX doesn't display. It's instead replaced by endless lines of this: &gt; Error:IO Exception: working.aux: removeLink: does not exist (No such file or directory) The only pictures that show up are the ones that are already there in `images/`. Do you have any ideas? I really want to get the repo running and model my own site after it.
That makes sense thanks! Is the situation different with the existentially qualified type method of making Set a Monad? Or am I misunderstanding that whole situation?
Category + Strong (from `profunctors`) = Arrow
Yes, that is the only error I see (once for every LaTeX formula). I'll try using the minimal example, as you say. But I think I'll end up with the same problem there. Edit: I get these compile errors with the minimal example: [link](http://lpaste.net/8005606430272913408)
I saw someone here linking to the lambda calculus page: http://dev.stephendiehl.com/fun/003_lambda_calculus.html The page is too dense for a newcomer like me. At some future time, hopefully, I can make sense of it.
You can also use the applicative instance of list [(*2), (+5), (+10)] &lt;*&gt; [3] The trick here is to call it on [3] instead of 3.
The fact that it is written in Haskell is irrelevant for a user point of view. Making it at the top of the of the list shows up how much featureful Leksah is.
Speaking of horrible design choices... the `Monoid` instance is aneurysm inducing as well... who ordered that?! ಠ_ಠ
You've probably already seen it, but just in case, there was [this post here last week](http://www.yesodweb.com/blog/2016/02/first-class-stream-fusion) about giving more control over stream fusion in a conduit-like library.
Thanks for the heads up. And... Ugh :(
Thanks! This is actually very encouraging; the current library situation is better than what you experienced (although user experience still trails GHCJS for many of the things here), and almost every single grievance that's still in there is fairly easily fixable and on track to actually get fixed once ICFP season is done with. As for reflex-frp, I'm curious about whether higher order FRP actually brings significant benefits in your experience, or if a hypothetical first order FRP variant would do the trick (almost) as well?
I see. Thanks for building a great tool anyhow!
I used [this](http://plato.stanford.edu/entries/lambda-calculus/) to learn it. It had a quite high learning curve, but is doable.
Interpreters often transform the code into stack machine programs, besides other optimizations.
&gt; Bird's book doesn't contain anything on lambda calculus. It is a book about Haskell. Michaelson wrote half his book about lambda calculus, and with the other half he creates an ML-like language using that material. Thanks for taking time to help me with my query. 
Cool, that's great to hear! It would definitely be nice to have more healthy competition between haste and ghcjs. In my experience I used higher order FRP substantially. I can't say, though, whether those things would have been impossible to do with first order FRP. But I suspect that if not outright impossible, then it would probably get rather inconvenient at times. It feels to me like an important operation that is very helpful in building the right abstractions to fit a problem domain.
As you know, GCC* is compiled by itself. GCC also comes with an optimization engine. GCC's code optimization therefore optimizes itself. Given that GCC is 5 years older than GHC, I doubt that your estimate is reasonable. ^(^* feel free to replace GCC with any other self-hosted compiler that has some kind of optimization)
Yes, this is a list of "features I really like" and as a contributor it is appealing that it is written in Haskell. This may well be of little interest to most. There are plenty of other features in Leksah though, please give it a go if you have time and let us know how you find it.
[removed]
Realistic compilers usually model the evaluation strategy on a more efficient abstract machine. Most abstract machines for the lambda calculus gather values to be substituted into an environment so that instead of literally creating or modifying an AST it can defer the substitution until a variable is encountered during normal evaluation, at which point out can simply look it up in the environment. There are very efficient environments for some evaluation strategies. In fact, most common evaluation strategies are common just because they are easy to implement efficiently in this manner.
Yep. Same limit killed my efforts last time. What stopped you this time?
Interestingly, we *do* use GHC to optimize GHC to optimize GHC, but we hit a fixed point after a few iterations, we don't increase the performance forever until we hit a singularity. GHC's build instructions have [four stages](https://ghc.haskell.org/trac/ghc/wiki/Building/Architecture/Idiom/Stages): &gt; * Stage 0 is the GHC you have installed. The "GHC you have installed" is also called "the bootstrap compiler". &gt; * Stage 1 is the first GHC we build, using stage 0. Stage 1 is then used to build the packages. &gt; * Stage 2 is the second GHC we build, using stage 1. This is the one we normally install when you say make install. &gt; * Stage 3 is optional, but is sometimes built to test stage 2. So how does the performance of each stage compare? Stage 1 may or may not be faster than stage 0, depending on what the GHC maintainers decided to work on between the two GHC releases: if they added more compiler passes, stage 1 might be slower, and if they hand-optimized GHC's code to reduce compilation times, stage 1 might be faster. In any case, the result depends on human work, not on the machine's work, so those aren't the kind of improvements which could lead to a singularity. Stage 2 should be faster than stage 1, because it's been built with a more recent version of GHC which performs newer, better optimizations. This time the speed improvement is also due to human work, but not the work on optimizing compilation times, just the generic work on making every produced executable faster, including GHC. Stage 3 should have the same performance as stage 2, because it's compiled from the same source code as in the previous stages, and it's optimized it using the same optimizations as in the previous stage. So there's no singularity.
Sure, but tikz is such a pain to write and debug.. we really need a codegen for this, I think.
&gt; Ugh :( Nope, I'm pretty sure this was the right choice (there's some discussion on the issue tracker and mailing list). This difference is certainly not the only thing making it hard to reuse Haskell type definitions in PureScript, anyway. Notably, PureScript is strict, so even something as basic as `data List a = Nil | Cons a (List a)` has a completely different meaning in PureScript. Similarly, `data Pair a b = Pair !a !b` is a syntax error in PureScript, and `data Point = Point { x :: Number, y :: Number }` means something completely different too. They're different languages, so I would strongly discourage trying to reuse type declarations across both. Typed-wire (or something similar) is the way to go, I think.
Theoretical solution: Have an `Arr` module in both your Haskell and Purecript. In Haskell: `type Ar a = [a]`, in PureScript: `type Ar a = Array a`; no other contents. Then use `Ar a` for building your types, and now you've got lists in Haskell and Arrays in Purescript that look the same again.
Could we then also say that Functor gives rise to Monad using the Free wrapper? 
Hello, GHC. Do you read me, GHC? Affirmative, Dave. I read you. Solve P=NP, GHC. I'm sorry, Dave. I'm afraid I can't do that. What's the problem? I think you know what the problem is just as well as I do. 
Also available in [Flow](http://hackage.haskell.org/package/flow-1.0.2/docs/Flow.html#v:-124--62-), my little operator library. Flow also happens to provide [`apply`](http://hackage.haskell.org/package/flow-1.0.2/docs/Flow.html#v:apply), which is exactly what this thread asked for. map (apply 3) [(* 2), (+ 5), (+ 10)]
Automatic indentation on a character, such as = , -&gt; is, would be my favorite. Also, vim shortcuts.
&gt; GHC's build instructions have four stages: Isn't that common practice? I know I've joked somewhat when I said "GCC is used to compile GCC", but IIRC, gcc follows the same practices, e.g. use the platform cc (stage0) to compile stage1, then use stage1 to create stage2, and afterwards stage3 to check that results are reproducible. Would that be possible with GHC too? Is there currently any other Haskell implementation that's able to compile GHC?
&gt; I never used purescript, and as a result I am currently convinced it is the best solution ;) Hah!
It isn't useful in any way, but technically you could write {-# LANGUAGE ExistentialQuantification #-} data AnyOrd = forall a. Ord a =&gt; AnyOrd a list = [AnyOrd "asd", AnyOrd 56] :: [AnyOrd] f :: AnyOrd -&gt; Bool f (AnyOrd a) = a &lt; a main = do print (f (list !! 0)) -- prints 'False' print (f (list !! 1)) -- prints 'False' The function `f` can only use what both `"asd"` and `56` have in common: that there is an `Ord` instance for the types. And there is really nothing useful you can do with this information alone. Not in this case, at least.
there are also other fusion techniques, used by libraries such as Repa. Take a look at Ben Lippmeier's papers on the topic.
&gt; I got is that miniKanren benefits from dynamic types. I'm sure they think that, but having given static types to something that people keep telling me they think is impossible (Opaleye, embedding SQL in Haskell) I'm fairly confident in predicting you can have a nice typed embedding of miniKanren.
I've never encountered such behavior with ghci-7.10.x, neither on Windows nor on Linux. Maybe something's wrong on your side.
&gt; the existentially qualified type method of making Set a Monad I don't think I know what you're referring to, but the issue is the same whenever you have two existentially quantified arguments. You've thrown away the type information that tells the compiler that an operation is safe. What you can do, however, is to tie runtime information to type information in a way the compiler can follow; GADTs, `Typeable`, and `singletons` are examples of this approach.
Write something like maximum $ fromList [minBound .. maxBound], check the memory pattern. If it blows up memory in ghci but behaves well when compiled, a great chance is that the library uses stream fusion and relies on ghc rewrite rules for optimization.
Isn't that kinda what `diagrams-pgf` is for?
I fixed it. When neither windows nor `stack ghci` (with the same ghci version) were reproducing the issue, I decided to take a closer look at my ghci command - turns out I aliased ghci to point to [ghci-color](https://github.com/rhysd/ghci-color) - which regexes the output. That regex apparently hangs when it's handed an infinitely long line.
I'm dumb. &gt; I fixed it. When neither windows nor `stack ghci` (with the same ghci version) were reproducing the issue, I decided to take a closer look at my ghci command - turns out I aliased ghci to point to [ghci-color](https://github.com/rhysd/ghci-color) - which regexes the output. That regex apparently hangs when it's handed an infinitely long line.
So in what situations would existential types be useful?
Ah ok I think I was thinking about the wrong thing. I thought there was some extension (existential typed) that made making Set a monad possible.
&gt; Isn't that common practice? I guess it would make sense for all bootstrapping compilers to do that, yes. I happen to have read GHC's how-to-build documentation but not GCC's.
Would be great when we finally could throw out hand-optimized code and concentrate on readability. But we are still very far away from that.
&gt; How does that sound? Sounds like something I specifically left out of the implementation to avoid confusion with extra newtypes ;). Yes, it's a good idea, though in practice may not pay off as well as you're hoping. You still need to explicitly look at the `Stream`s if you want to write efficient implementations of algorithms. But those kinds of details could certainly be placed in a `.Internal` module. I'm still not convinced that from a usability standpoint it's a good idea though. People already understand simple list-transforming functions. One of the nice things about this implementation is it models the way we deal with lists so closely. &gt; Why not avoid the risk by using an abstraction? Right, and you're correct about difference lists (that's why I commented on the naive implementation, which was in the dlist package until quite recently). The difference here is that to get good performance when writing, say, a `concatMap` function, you'd need to unwrap that constructor. &gt; Still, I'm highly skeptical that you can get something for free just by choosing a looser type. You definitely can. If we take the transformation functions and wrap up their result in `EfficientPipe`, I can easily prove the category laws for composition of an `EfficientPipe`. Voila! All we're doing is converting into a form where we've already proven the laws.
[`vector`](https://hackage.haskell.org/package/vector) is using the bundles from the "Generalized Stream Fusion" paper, although without SIMD support. 
Also, to clarify: The intended behavior wrt 1+1 is exactly what you're getting. But for me, after having one faulty infinite list print-out, any subsequent print-out would also be faulty. So ghci-color would work with 1+1, but it would go haywire if you hand it an infinite list. And it would stay haywire if you ask it about `1+1`.
The list itself appears to be running. Not sure why your confirmation may not have appeared. I also sent myself a password reminder, and that came through. So all systems look to be in working order. Perhaps an overeager spam filter?
Weirder still, sentience and intelligence are not the same thing! The compiler could also become intelligent but not sentient (capable of deriving new clever optimizations but said intelligence existing in a non-spatial sense with no concept of self or anything except code improvements) or even sentient but not intelligent (you've run into people like that). The latter would not be a very useful addition as a compiler though.
Yep. Though, I'd probably say it gives rise to "a" monad through `Free`.
I don't have any particular insight here, but chapters 1 - 9 were appearing at regular intervals, and then there was a long (more than a year IIRC) gap before chapter 10. I'm glad to see chapter 10 out of course, but I gather he is writing this as he goes, and between consulting gigs and the time it takes to produce the content, this project will proceed slowly.
Haha. Got it.
&gt; You did beta-reduction by substitution, right? Yeah, that's what I was doing. The environment approach sounds interesting -- I hadn't thought of that. Does it work at compile time? Or is it only useful for runtime interpreters?
Saying X gives rise to a Y isn't a very powerful statement. You can of course refine it to think about how we get the other thing: Arrow gives rise to an Applicative by forgetting structure. Applicative implies Functor by forgetting. Functor gives rise to a monad by a free construction, and it gives a comonad by a cofree construction. So in one sense you're doing different things in each of these cases.
Are you using `--split-objs`? It usually halves executable size. Note that you should compile all the deps with `--split-objs`. Also `stack` doesn't seem to support it, that is one of the reasons I'm using cabal sandboxes yet.
I'm not sure about ghc itself, but all core libraries that come with it are compiled with `--split-objs` by default.
You are stripping the binary, right? I have 11MB executable for 20KLOC project, it looks, and it looks reasonable for me.
cabal strips by default, yes. Like I said, the difference is substantial, though for instance I still find 7.9MB shellcheck or 13MB darcs large.
Some executables are 1MB smaller while others are 50% smaller. It's a big improvement so I wonder why cabal doesn't default to split objects.
In LambdaCube 3D, code generation for tuples was not a problem, at least it is not a problem at the moment. My remark about compiler optimization capabilities is questionable, you are right. I wish compiler code generators were smarter. I wish to work in a higher level language, where the compiler also finds out when a data structure can be replaced by another data structure, if that switch is a definite win. The first opportunity for this would be tuples represented by heterogeneous lists, with generated code where the tuple elements are stored in registers, or on the stack, or on the heap in a vector.
HLists are O(n) lookup because that's how they are defined currently. In practice, HLists aren't used like lists at all, they are used as small-sized tuples with fields that are usually known statically, but with an additional ability to abstract over sizes and components, or have labels and simulated row polymorphism as in `vinyl`. It's not really feasible in Haskell to do serious work with statically unknown `HList`-s or `vinyl` records; for that we need easy and safe handling of runtime proofs which is very painful without dependent typing. In short, we use `HList` for boilerplate removal and abstraction over statically known properties rather than dynamic inductive behavior, and for this purpose O(1) access would be perfectly appropriate. However, it's another question whether we should have a primitive flat `HList` in GHC. It would be a peculiar addition with peculiar characteristics, but in a new language it makes a whole lot of sense to not have 62 separate tuple definitions with different sizes. In a new language one could also make all data representation generic by default, in which case it again makes sense to represent the contents of each data constructor as a flat "anonymous" record or telescope. Flat `HList`-s would be nice in Haskell generics too, since currently we have to tolerate several times increased space usage and the consequent slowdown because of constructor noise. 
&gt; and no compiler offers this The Agda compiler replaces Peano representation of natural numbers with integers, which have quite different performance characteristics.
True! But considering the target audience of this post is for developers who are completely unfamiliar with crypto, I was purposely loose with the terminology. 
&gt; Does it work at compile time? What do you think the stack is? :)
&gt; On the other hand, if you import a module and use just 1 function from it, all of the code for all of the functions in that module get linked in. So in this sense, no, GHC doesn't do dead code elimination. But that is exactly what `--split-objs` does, isn't it? It creates a separate compilation unit for each top level definition, so linker can ignore them unless they are actually used.
I'm not sure those developers have it any easier when you confuse the terminology. If I sent a new person to your blog, and then they went out and read some more literature, they would be confused (as I was, but perhaps reversed). Also, check out [argon-2](https://github.com/p-h-c/phc-winner-argon2) :)
Is de Bruijn indices the best choice for that? Is there any better alternative for representing bindings? 
See my other [comment](https://www.reddit.com/r/haskell/comments/48quo4/write_you_a_haskell_stephen_diehl/d0m2l98): deBruijn indices avoid names, but they don't fix the performance penalty of substitution; they do make working with environments easier in some ways, though.
Look up Lamping's algorithm and derived variations on the algorithm. If you can afford it, buy [this book](http://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/optimal-implementation-functional-programming-languages?format=HB)
I am happy with Learn You a Haskell. I find the examples in LYAH very useful.
Well, just at a glance, your code looks very similar to the `Wire` data type in the `netwire` package, especially this code here: http://hackage.haskell.org/package/netwire-5.0.1/docs/src/Control-Wire-Core.html It appears to me like you have implemented some kind of Mealy Machine, or Moore Machine. In Haskell, Mealy/Moore Machines are used in Functional Reactive Programming (FRP), and Netwire is in fact an FRP library. Here is the Wikipedia article on Moore Machines: https://en.wikipedia.org/wiki/Moore_machine So it appears to me that what you have done is implemented a subset of `netwire`.
You must really dislike FreeT as well.
Here's an example of updating your code to use `MonadRandom`: https://github.com/Porges/haskell-SlidingPuzzle/commit/cbbd86c99c6e359a2ef26b28ec310030bd2cdefe :)
The scrypt package does make the distinction between a plaintext password and encrypted password on the type level.
Oh thats neat, do you think it will be easier to use gloss than SDL?
I have no knowladge about comonads, but this might be a chance for me to learn about it, thanks! I love how clean the code for the different "animations" become.
Ah, I see. Actually I thought you were talking about the performance of the generated code.
Good job! High-quality, well-documented additions to the Haskell ecosystem should always be encouraged. Looking through the documentation, I noticed a few things: 1. You use `path`. How is it different from/better than the `filepath` package? 2. What's the difference between `undoArchiveChanges` and `undoAll`?
It grants the monad laws for free. I'm not saying that there may be no reason to ever compose a Gotenks directly. I am saying that for common cases, having trivially provable category laws is a nice thing. The arguments I'm hearing from you don't dissuade me from this stance in the least.
Could you explain a little why comonads are a good match for this kind of operation? Thanks!
Lots of people say that Matlab or R are fast if you do not use loops and that if you "really know how to use them" you can do everything without loops, which is just a justification with bad arguments. Anything that is fairly non-linear suffers from that. Is this argument similar to this? I know almost nothing about memory management, so I would really like to know if these kinds of restrictions are connected to good practices or artificial. I'm used to simple C programming and I'm comfortable with Haskell, so Rust is really making me interested.
Well the lower level stuff uses `unsafe`, but if the Rust core team writes the lower level stuff correctly you can use it from code that does not tag anything as `unsafe`. If you really need it, you can write your own `unsafe` code and be just as fast as C, but in practice this is only needed for library authors, not application writers.* ^(*You may need to become a library author right now for your application, though.)
According to the [spec](https://www.techempower.com/benchmarks/#section=code) the ids and the randomNumber field (`World` specific) are ints. You can replace all `Integer` types in your models with `Int` ~~or to make sure they are 64 bits wide `Int64`~~. If you additionally unpack the fields (either by {-# UNPACK -#} pragma or `-funbox-strict-fields`) you can eliminate some indirections. Probably not significant performance wise but narrowing down a type to be more specific about its domain is a good thing anyway. 
What's the `simulate` function? Also, would you mind pasting the types too? Thanks :)
For the love of all that's holy, at least call it `pack`
I think I must be explaining myself very badly if you think I'm against monad laws for free. My initial comment was as follows &gt; &gt; We now get to trivially prove the category laws, since we're just using function composition! &gt; I'm afraid this isn't what you think it is. I said this because I got the impression that you believed the category laws for `Stream ... -&gt; Stream ...` were equivalently powerful to the (hypothesised by as yet unproven) category laws for conduits with leftovers. Perhaps you don't believe this after all, in which case I apologise for the confusion and derailment. 
The generated code was also quite slow. Decoding a 5mb json files would take something like 30s whereas the equivalent code in elm takes 2s. The interface would be very laggy too after that.
&gt; although of course `cons` is still O(N) for construction. Why is that necessary? In most systems languages, the vector libraries usually grow the vector's allocated size exponentially (usually with base 1.5).
"Catch" is not an algebraic operation so you can't implement it as a constructor in a free monad. Catch is a handler, that is it *destructs* the free monad, it doesn't construct it.
Thank you, I made the changes. 
I'm just guessing, but it seems like the difference between only including stuff in an object file that you use and splitting everything into object files and only keeping the object files which are used. The former will be less work for the linker to deal with, and the latter isn't currently an option for stack users. Please, correct my misunderstanding if one exists, as I'd prefer to not mislead anyone.
Yes, that was the huge draw for me as well :) I found two properties very beautiful: 1) that I could decouple data from animation, but this is something that MVC can achieve as well, the "wow" factor is much smaller. 2) that I simply need to teach my code how to update *one* cell, and the comonad _automatically_ "learns" how to update the entire grid (because comonads are awesome and can create local-to-global effects). 
I'm probably not being clear enough either, so let me explain myself better: it's really nice that, as a side effect of pursuing performance, I got categorical composition of streaming that supports leftovers. And I got that for free, without having to write any proofs. I also believe that I can provide direct composition functions for Gotenks which obey the category laws, but the result may not be quite as convenient and certainly won't come for free.
In your second example ("Seeds"), you assign `cell = extract grid` but never use that definition.
Thanks! I'd realized the connection to FRP (I was originally inspired by [Knockout.js's computed observables](http://knockoutjs.com/documentation/computedObservables.html), and it's not hard to jump from there to FRP), but I hadn't thought about `netwire`. I'll have to take a look at it, and see how I would implement the same using what it provides.
Oh, I did not know that (I personally am on OSX and have no problems setting up the Cairo backend). Could you please create an issue for me on the tracker?
I really love these. I think it's the comma style detection PR that made editing cabal file dependencies much more pleasant!
That's one feature, though it's only useful for relative paths. Absolute paths are not portable so statically checking them will work or not work on Windows/Linux.
Oh right, I hadn't noticed that :) Fixed, thanks! I'm not able to upload the fix since I'm behind a proxy at college, but I'll upload once I get back home.
`undoArchiveChanges` undoes editing of things that characterize archive as a whole (currently it's only archive comment). `undoAll` undoes everything. Simply put: `undoAll` = `undoEntryChanges` (for all entries) + `undoArchiveChanges`. 
&gt; Just make it polymorphic and deal with it. This isn't really as bad as you make it sound. data GList a = GList [a] Attribute type EList = GList Element If you aren't afraid of lenses, an even better solution might be this: data EList = EList [Element] Attribute elements :: Lens' EList [Element] elements f (EList xs attr) = (\xs' -&gt; EList xs' attr) &lt;$&gt; f xs attribute :: Lens' EList Attribute attribute f (EList xs attr) = EList xs &lt;$&gt; f attr traverseEList :: Traversable' EList Element traverseEList = elements . each The type of the last function is essentially an alias for `Applicative f =&gt; (Element -&gt; f Element) -&gt; EList -&gt; f EList`, which resembles the signature of `traverse`. And the function could also have been written `traverseEList = elements . traverse`. Traversals are really just generalized lenses.
&gt;This one will be more challenging, but hopefully someone will step up. This might be the sort of thing that should be done via haskell-ide-engine. Including options for profiling and displaying the results should be pretty easy as the compiler already does this. All you need is a couple of buttons for choosing options like optimizations and threads while compiling and then display the results in an output pane. Suggesting optimizations would be the next step and the more challenging one. But once the profiling display is in place I'm sure we can motivate someone to do it ;)
I haven't used Lenses yet. (Pretty new to Haskell overall!) so Lenses seems like overkill to me currently. But I will keep it in mind and maybe try it out if I run into this issue somewhere else later :)
The monomorphic variants is an interesting idea in theory. In practice it seems like more trouble then it's worth for the most part ..
&gt; I haven't used Lenses yet. (Pretty new to Haskell overall!) so Lenses seems like overkill to me currently. I think that's generally a good attitude. If you ever decide to use lenses, maybe you will find the following explanation to be helpful. You seem to already know what `traverse` does. It allows you to "effect-fully" walk through a `Traversable` structure `t` of zero or more "things". traverse :: (Applicative f, Traversable t) =&gt; (a -&gt; f a) -&gt; t a -&gt; f (t a) Here I am simplifying the signatures a little. A `Traversal'` (from the `lens` package) gives you essentially something like a specialized `traverse` function, for some appropriate types `s` and `a` of your choice: Traversal' s a = forall f. Applicative f =&gt; (a -&gt; f a) -&gt; s -&gt; f s Lenses simply take this idea a little further. You don't access zero or more elements. Instead you can access exactly one, due to the `Functor` constraint being a little tighter that the other one. Lens' s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; s -&gt; f s Note that every lens is a traversal, but not every traversal is a lens. I just hope this wasn't to handwavy. But if you use them, you will find that lenses and traversals are actually very simple. You don't even need to import any `lens` module to create them. The `Functor` and `Applicative` type-classes are sufficient. 
This is an excellent example of well documented code, thank you. I'll use your library for some personal projects. 
&gt; Couldn't you build this list without existentially qualified types [...] Yes, of course you can. And in my opinion this is how it should be done.
So is ST one of the situations where you do truly need existentially qualified types, but things like heterogeneous lists not so much?
To be honest, I'm not sure. I just remember this being discussed at some point. And it makes some sense, the linker needs more time to scan through multiple object files. I have no idea how expensive this is, though.
Great work, man. I'll try to port hs-java to use this package instead of LibZip+bindings-libzip, which as you already know can be problematic.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programmerhumor] [Optimization Engine, Optimize Thyself](https://np.reddit.com/r/ProgrammerHumor/comments/48ytqv/optimization_engine_optimize_thyself/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
How do you know it's a orphan package. As far as I know there are some rules to take over a package, You cannot do it because you want it 
&gt; How do you know it's a orphan package. Probably because it hasn't been updated in almost 5 years. Then again we could just ask /u/dons. Also since I never heard of it until now it can't be terribly popular. It's not on Stackage and there are better libraries around for doing these sorts of things. Maybe it should just be deprecated?
I'm not sure I follow. When interpreting a `Catch foo bar` node, you'd presumably interpret `foo`, and if it throws an exception, interpret `bar` instead. If no exception is thrown, the meaning of `Catch foo bar` is the meaning of `foo`.
Quick tip: you seem to be working with vectors of known size. In that case I would suggest looking at the [linear](https://hackage.haskell.org/package/linear) package which has such data types (ie. V2 for 2D coordinates). 
&gt; Isn't funbox strict fields on by default? No, because it can sometimes hurt more than it helps. If you pass an upacked value to a non-strict function it has to be repacked. 
Effectively all of dons' packages are up for grabs at this point to any interested maintainers. He's posted a message to the lists saying this some time ago.
done
Minor: personally, I don't find long chains of compositions readable, even less if you split them over multiple lines. Perhaps you could give a few more names here and there ..? This way you might also notice fruitful generalizations, here and there
I will look at it and see if it's something I can fix.
That is pretty much the current plan. We'll run about a month behind the usual GSoC schedule, and use the extra lead-in time to figure out how many slots we can fund with community support.
I'm doing something similar. My supporting libraries are working but poorly documented and not ready for release: - [reactive-dom](https://github.com/avieth/reactive-dom) is all about expressing user interfaces over reactive-banana. I'm confident that this library is unapproachable. It's incredibly abstract and, again, poorly documented; I also tend to make breaking API changes often. But it's working well for me so far! - [servant-route](https://github.com/avieth/servant-route) and [servant-xhr](https://github.com/avieth/servant-xhr) come together to derive XHRs from servant types (you just have to give the origin). &gt; how has it been? Is it more trouble than it's worth? Or does having Haskell on both ends really pay off? If you want to very quickly get something working reasonably well, don't go this route. Use existing solutions instead. But if you've got some time on your hands and want to try something new, go for it! Using Haskell on both sides does indeed pay off: write your API types once; automatically generate generic JSON (de)serialization; derive safe XHRs on the client; compose type-safe user interfaces from smaller parts; and best of all, never write JavaScript.
&gt; By default, GHC will now unbox all "small" strict fields in a data type. A "small" data type is one whose size is equivalent to or smaller than the native word size of the machine. This means you no longer have to specify UNPACK pragmas for e.g. strict Int fields. This also applies to floating-point values. http://downloads.haskell.org/~ghc/7.8.1/docs/html/users_guide/release-7-8-1.html
I see a lot of infix usage of `mappend`. Why not `&lt;&gt;`? Additionally, I usually prefer: foo = fold [a, b, c, d] to foo = a &lt;&gt; b &lt;&gt; c &lt;&gt; d This would be a great exercise to learn [`criterion`](https://hackage.haskell.org/package/criterion), and possibly Haskell's FFI bindings to test the C implementation. minmax :: V.Vector (Double, Int) -&gt; (Int, Int) minmax = uncurry (,) . (snd . V.unsafeHead &amp;&amp;&amp; snd . V.unsafeLast) `uncurry (,)` is `id`, so can drop it from the expression splitValidQuad u v w x = (V.map fromJust . V.filter isJust *** V.map fromJust . V.filter isJust) . V.unzip . V.map ((\v' (a,b) -&gt; (flip (,) v' &lt;$&gt; a, flip (,) v' &lt;$&gt; b)) `ap` testInRegionQuad u v w x) IMO this is pretty obfuscated. I generally prefer `&lt;*&gt;` to infix `ap`. You might like `both = join (***)` as a means of reducing some of the boilerplate. A *lot* of the code is suffering from pointfree obsession. (,,) &lt;$&gt; same' . sum &lt;*&gt; signum . sum &lt;*&gt; id $ vv This is less understandable and less efficient than: let sv = sum vv in (same' sv, signum sv, vv) 
Thanks for these tips. I, unfortunately, tend to get carried away with the pointfree style, so much so that I don't realize that I am writing inefficient code. Also, someone has already written some simple bindings to the Qhull C library [qhull-simple](http://hackage.haskell.org/package/qhull-simple). However, the package needs to be updated; some C headers are out of sync with current versions of the C library qhull. With that said, I will eventually use criterion to precisely measure how my implementation compares to the C version.
I find it interesting that you are using comonads in your implementation, because it looks to me like this will restrict your to CA only on abelian groups (or finitely generated at the very least).
So you have already noticed ways that the code could be shorter?
Ok, I have fixed the compilation error and some warning. It's available here: https://github.com/psibi/download I tried the `openURI` with various urls. While it worked for `google.com`, `haskellstack.org` and others, it didn't work for `haskell.org`. Probably, I have to see the C code to see why it fails for it. &gt; Then resubmit to hackage. Is there some process for it ? Where exactly do I need to resubmit it ? I haven't yet bumped the package version.
This is great! Thanks for doing it. Maybe we should just summon /u/dons and ask him what you can do.
Lost me when the Point example had bogus signatures. It's really important to be careful with that stuff or folks get really confused: I spent a minute wondering what feature of the Haskell type system I didn't understand (again). The rest of the article looks nice and was helpful, though.
Great work! A couple of minor points. First, the anchor links on the hackage page does not work (e.g. clicking encryption does not scroll down to the encryption paragraph). Second, while I agree with your stance on the encryption support in the zip file format, it is important to consider that the reason we need support for the zip file format while developing software in Haskell is that we are receiving zip files we need to process. Perhaps from sources outside our control. Sometimes these are encrypted by people not as well versed in the drawbacks of zip encryption as you are. Such as governments. If you have ever tried to contact those asking for a different format (or without encryption), then you quickly give up. Please add (read-only) encryption support if you have the time.
&gt; Lost me when the Point example had bogus signatures. It's really important to be careful with that stuff or folks get really confused: I spent a minute wondering what feature of the Haskell type system I didn't understand (again). I too must wonder what aspect of the type system you don't understand, because that example is completely valid.
In the Haskell example the password is being hard-coded.
`read`?
&gt;Prelude Network&gt; PortNumber 5 &gt;PortNumber 5 &gt;Prelude Network&gt; PortNumber (read "12" :: Int) &gt;&lt;interactive&gt;:5:13: &gt; Couldn't match expected type ‘PortNumber’ with actual type ‘Int’ &gt; In the first argument of ‘PortNumber’, namely ‘(read "12" :: Int)’ &gt; In the expression: PortNumber (read "12" :: Int) &gt; In an equation for ‘it’: it = PortNumber (read "12" :: Int) &gt;Prelude Network&gt; let a = read "12" :: Int &gt;Prelude Network&gt; PortNumber a &gt;&lt;interactive&gt;:7:12: &gt; Couldn't match expected type ‘PortNumber’ with actual type ‘Int’ &gt; In the first argument of ‘PortNumber’, namely ‘a’ &gt; In the expression: PortNumber a From GHCI.
&gt;Prelude Network GHC.Word&gt; PortNumber (fromIntegral (read "12" :: Int)) &gt;PortNumber 12 :O it worked! tyty 
it's a little confusing: - `PortNumber` is a data constructor for the type `PortID` which takes a single argument of type `PortNumber`. - `PortNum` is a data constructor for the type `PortNumber` so, in GHCi: &gt; :type PortNumber PortNumber :: PortNumber -&gt; PortID &gt; :info PortNumber data PortID = ... | PortNumber PortNumber | ... -- Defined in ‘Network’ newtype PortNumber = network-2.6.2.1:Network.Socket.Types.PortNum GHC.Word.Word16 -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ instance Enum PortNumber -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ instance Eq PortNumber -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ instance Integral PortNumber -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ instance Num PortNumber -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ instance Ord PortNumber -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ instance Real PortNumber -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ instance Show PortNumber -- Defined in ‘network-2.6.2.1:Network.Socket.Types’ so, one thing the docs say is "Don't use this data constructor!!" Fortunately, we have a `Num` instance, which means we can use numeric literals: &gt; let a = 5 :: PortNumber 5 If we have a `String`, then we can do a `read :: String -&gt; Integer` and then `fromInteger :: Num a =&gt; Integer -&gt; a`, specialized to `PortNumber`, we get: &gt; :type PortNumber . fromInteger . read PortNumber . fromInteger . read :: String -&gt; PortID Which does what you want. Note that `read` will fail with an exception if it is unable to parse! You may prefer something like [`readMay`](https://hackage.haskell.org/package/safe-0.3.9/docs/Safe.html) with a signature `Read a =&gt; String -&gt; Maybe a`.
It looks like it can fail if mappend is not monotonic w.r.t. the Ord instance. But I cannot think of a counter-example offhand.
No, I'm thinking of the group Z in the 'classical' 1d setting, or ZxZ in the case o fa 2d grid. For the kind of finite grid you have, it would be Zm x Zn for the m x n grid. In my opinion, the comonad version is really only appropriate to the 1d infinite case because it ignores the border cases, but I still think that it is just a reinterpretation of the standard definition of a CA over Z. I haven't looked at your code (it's probably above me anyways :) ) but I think implementing it in terms of finite groups *could* eliminate the corner cases at the edges. WIth G a group and A an alphabet, you speaking you might get types like `configuration :: G -&gt; A` `rule :: (G -&gt; A) -&gt; G -&gt; A` `step :: (G -&gt; A) -&gt; (G -&gt; A)` So if you start with a configuration s, then step s (x) = rule s (x) In the sense that x in the new configuration is determined by the rule's action on the current configuration. We eliminate the (x) argument to the function with the x-shift, so step s = rule s inv, or something along those lines, where inv inverse the elements of G.
Interestingly enough, my grid class *is* the finite case. I didn't like the infinite case exactly because of the "edge" case you described.
I have not used Servant myself, but from what I understand about it, the idea is that you still write clients in Javascript. Using Haste.App, server and client together are one Haskell program, and moving code from client to server or vice-versa is just a matter of changing one line of code. So they solve different use cases I think. 
I did see that, but it seems to me like the comonad interpretation will have the most trouble with the finite case, because it can just push off the edges. How did you manage it?
I make the comonad instance such that the definition of "duplicate" :: 'w a -&gt; w (w a)' creates left and right shifts that "wrap around" correctly. The 'fmap' instance is natural anyway, and I get 'cobind' in terms of fmap and duplicate. Hence, my definition of 'extract' that makes sure that the shifts wrap around is what makes the whole thing tick. That in itself is something that blows my mind, that I can encode the rules for handling the edges into the comonad, while my comonad _users_ do not need to worry *at all* about the wrapping around and whatnot - it "just works" from their perspective. I'm sorry for the bad formatting, I'm on my phone. I'll type up a larger response if you're interested once I get back home from college.
It sounds interesting. I just looked through your code, and it seems to me like you are using the comonad to pre-apply the shifts - this is more or less equivalence to the definition by g-shifts, but it sets them all up at once instead of on-demand (but of course with lazy evaluation it is all the same). Maybe I can scrounge up enough time to implement a CA along the lines of the more traditional method to see how they compare. It may turn out that they will describe exactly the same thing in slightly different ways, although to check this you might want to try implementing a CA over something like D_n with comonads. I think the lack of commutativity of the shifts may introduce difficulties, but maybe equivalent difficulties alon the lines of the word problem.
Oh you! :)
Try these resources: - http://kellabyte.com/2013/05/20/convergent-replicated-data-types/ (video, paper links) - https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type - https://github.com/dominictarr/crdt in JS - https://christophermeiklejohn.com/crdt/2014/07/22/readings-in-crdts.html Your writeup is nice; the join/merge operation in your diagram I think corresponds to the existence of a supremum allowing two strands of editing to converge.
Just grab it if its interesting and useful. Some of these were early attempts to fill in holes in the ecosystem - likely we have better alternatives now, but if not, all the repos are online and BSDd
That's interesting. How would you do that ? At the end of the day, you still need one main and one dispatcher which has to be either Yesod or Servant.
Yea. You can use GHCJS to write clients in Haskell though.
There's some stuff that can make you a lot faster and improve the UX for users if you share them: * data types and API endpoints * (de-)serialization * validation functions * utility functions 
I mean, if I am mixing both as you suggested.
I have uploaded a new version on Hackage: https://hackage.haskell.org/package/download-0.3.2.1
The main benefit of Servant is all the stuff you get for free (haskell client, JS client, mock server, documentation, swagger API description, and the list goes on). And, most important of all, all these things are always synchronized and consistent with each other. So, you cannot change the API a bit and forget to consistently update all these artifacts. I don't know what you mean "get pretty messy quickly". But there's nothing more messy than having the server, clients, docs, etc. inconsistent and outdated with respect to the API. Servant prevents that. Also, because of the clever way Servant is implemented, anybody can plug-in their own interpretations of the API quite easily. People still have some reservations about Servant related to it being tied down to HTTP too much (I don't know whether it has changed since the last time I looked). So, I presume, it'll still evolve quite a bit. But I've heard people using it in companies, so....
 fromInteger :: Integer -&gt; PortNumber
I have a question about this combination of functions. PortNumber constructor needs a type Word16, which fromIntegral can recognize and provide but how does "read" know that it needs to convert the string to a Word16 and not say a Word8. If I run your function on a number like "555" it will give me portnumber 555 but if the read converted "555" to a Word8 instead of a larger Num class (fromIntegral requires a Num and Word8 is a Num so I dont see how it knows that it shouldn't be a Word8) then after fromIntegeal I would have gotten PortNumber 43 Edit: I realize you said fromInteger not fromIntegral but my concern holds for both I think
1 and 2 are the main rationale for things like protocol buffers, they don't require you to use the same language for frontend and backend.
[removed]
Haskell's type system, while not converting things *for* you, can infer the types of expressions. `read` has the type `(Read a) =&gt; String -&gt; a`, which means that we're polymorphic in the return type of whatever calls `read`. `fromInteger` is also polymorphic in the return type: `(Num a) =&gt; Integer -&gt; a`. λ: :t fromInteger fromInteger :: Num a =&gt; Integer -&gt; a λ: :t read read :: Read a =&gt; String -&gt; a λ: :t (fromInteger . read) (fromInteger . read) :: Num c =&gt; String -&gt; c When we compose them, we say "The output of `read` flows into the input of `fromInteger`." Since `fromInteger` requires that the first argument is fixed to `Integer`, Haskell looks up to see if `Integer` has a `Read` instance. It does, and it specializes the `read` to use the one defined for Integers. Now we're still polymorphic in our return type -- we have a `Num c =&gt; c`, but we haven't fixed that `Num` type yet. The constructor `PortNumber` takes a value of type `PortNumber`, which is an instance of `Num`, which means that somewhere there's an: instance Num PortNumber where .... fromInteger i = ... Like above, Haskell realizes that the only way `(fromInteger . read) :: Num c =&gt; String -&gt; c` can type check with `PortNumber :: PortNumber -&gt; PortId` is if the `c` in the first expression unifies with `PortNumber`. So it picks the `fromInteger` defined on `PortNumber` when you do `PortNumber . fromInteger . read`. You can imagine this as `PortNumber . fromIntegerPortNumber . readInteger`, but Haskell is smart enough to pick the right instances without being guided (unless you're using fancypants language features like GADTs).
You need to expose the package by (1) adding `ghc` to your `build-depends`, or (2) `-package ghc` on the cmdln. (I think).
&gt; and the list goes on That's the point of my question. What else do I get for free ? &gt; I don't know what you mean "get pretty messy quickly" I mean, the types in the tutorial are quite scary, ex type UserAPI6 = "users" :&gt; QueryParam "sortby" SortBy :&gt; Get '[JSON] [User] -- equivalent to 'GET /users?sortby={age, name}' :&lt;|&gt; "users" :&gt; MatrixParam "sortby" SortBy :&gt; Get '[JSON] [User] -- equivalent to 'GET /users;sortby={age, name}' and it's only for one type of resource. What happen when you have users, post, comments etc . I guess the general route type becomes massive, doesn't it ?
oh, I didn't check fromInteger's type then. However I think my concern still holds if you replace fromInteger to fromIntegral. Would that technically have undefined behavior in Haskell? &gt; PortNumber . fromIntegral . read :: String -&gt; PortId Because fromIntegral takes type a of type class Integral. Both Word16 and Word8 are of type classes Read and Integral.
Halogen is pretty great! I definitely recommend it. The initial learning curve is pretty stiff, but the payoff is very good. Part of the design of Halogen is to make it work well with non-PureScript widgets, and it seems to do that very well. `reflex-dom` seems *extremely* cool, but I haven't had the time to do anything non-trivial with it. PureScript's FFI to JavaScript is nicer than GHCjs. If you'll be integrating with other JS libraries, this is likely a point in favor of PureScript. PureScript's compiled JS is *MUCH* smaller. My small Halogen app (after compilation with optimization, Closure minification, and gzipping (note to self: enable gzip on my app...)) is around 48KB. [This reports a GHCjs app to be around 18MB](https://twitter.com/dysinger/status/703389082924134400). The PureScript compiler is much faster than GHCjs, leading to a quicker dev cycle. PureScript and the associated ecosystem are evolving very quickly. This has good and bad parts -- lots of work is being done, but there are also a bunch of changes coming down the pipeline. Documentation and tutorials go out of date very quickly. This isn't a point in favor of `reflex-dom`, since there's so little out there for it, but it's something to keep in mind if you decide to go with PureScript. 
[**@dysinger**](https://twitter.com/dysinger/) &gt; [2016-02-27 01:20 UTC](https://twitter.com/dysinger/status/703389082924134400) &gt; My SPA GHCJS/Reflex app is 18MB (all in). That's a pretty hefty page load. (I wasn't successful with first run of closure-compiler either) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
It's not undefined behavior. The process is the exact same, but with an additional type class rather than a fixed type. Let's check out the `Integral` class: λ: :i Integral class (Real a, Enum a) =&gt; Integral a where quot :: a -&gt; a -&gt; a rem :: a -&gt; a -&gt; a div :: a -&gt; a -&gt; a mod :: a -&gt; a -&gt; a quotRem :: a -&gt; a -&gt; (a, a) divMod :: a -&gt; a -&gt; (a, a) toInteger :: a -&gt; Integer -- Defined in ‘GHC.Real’ So the `Integral` class has a `toInteger` function. I bet that's how `fromIntegral` is defined: λ: :t fromInteger . toInteger fromInteger . toInteger :: (Integral a, Num c) =&gt; a -&gt; c λ: :t fromIntegral fromIntegral :: (Integral a, Num b) =&gt; a -&gt; b Yup! So now we can expand the original: PortNumber . fromIntegral . read to PortNumber . fromInteger . toInteger . read `read` takes a `String` and outputs an `Read a =&gt; a`. `toInteger` takes an `Integral a =&gt; a` and outputs an `Integer`. `fromInteger` takes an `Integer` and returns a `Num a =&gt; a`. `PortNumber` only takes a `PortNumber`, which fixes the `Num a =&gt; a` to be `PortNumber`. So now `fromInteger :: Integer -&gt; PortNumber`. But wait -- what is `toInteger . read`? The composition has the type `String -&gt; Integer`, but how does it resolve the middle step: (Read a, Integral a) =&gt; (String -&gt; a) -&gt; (a -&gt; Integer) -&gt; (String -&gt; Integer) To be honest, I don't know how it picks the instance, but it *really* doesn't matter. **Any** type that satisfies `Read` and `Integral` can be used, and it won't affect the behavior of the code. My guess is that GHC defaults to `Integer` in this case.
The thing that was keeping me away from Halogen was indeed the learning curve, but I am going to read more about it. And I do think that Purescript is also currently a better platform, but there is nothing similar to reflex on it (the last time I checked).
It's about duality. If your server serves something, the client needs to know how to accept it, and vice-versa. These tasks, although going in opposite directions, share a lot of code.
That is a really useful writeup, thank you!
I see. Thanks
I will definitely consider getting involved in the open source project! I write much more Clojure than Haskell these days (due to work), but maybe I can contribute by trying it out, at the very least. I also enjoyed your Functional Programming in Scala book.
See also [the full list of 150+ available API docsets](https://kapeli.com/dash#docsets), courtesy of [Dash for OS X](https://kapeli.com/dash).
Cheers. :)
So how? You present your users a Swagger UI to their data? (kidding) No seriously, you write vanilla JS on the front-end?
Is this similar to Darcs?
Since you are using the `monad-loops` package, you could also write whileM_ (andM [not . PQueue.null &lt;$&gt; ST.readSTRef heap, isNothing &lt;$&gt; HT.lookup visited t]) $ -- do stuff 
I really do not know why, but you can try to run a benchmark and you will see that the tail recursive version is indeed faster than the `foldl'`. 
Out of curiousity, how big is your app (LoC-wise)?
with type M = Map (Sum Int) m :: M Int m = fromList [(Sum {getSum = 0},0),(Sum {getSum = 1},0)] -- {0: 0, 1: 1} f :: Fun Int (M Int) f = {0-&gt;fromList [(Sum {getSum = 0},0),(Sum {getSum = 1},1)], _-&gt;fromList []} -- 0 -&gt; {0: 0, 1: 1}; _ -&gt; {} g :: Fun Int (M Int) g = {0-&gt;fromList [(Sum {getSum = 0},0)], _-&gt;fromList []} -- 0 -&gt; {0: 0}; _ -&gt; {} (m &gt;&gt;= f) &gt;&gt;= g /= m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g fromList [(Sum {getSum = 0},0)] /= fromList [(Sum {getSum = 0},0),(Sum {getSum = 1},0)] Power of QuickCheck!
Here's what I discovered when making [this site](http://www.azlemi.org.il/en/) as a hobby (though I used Python for it, also on client side): Rendering on the client-side could be slow. For the initial loaded page it is faster to have the server render the html for it than the client-side render it. So for that you'll need the rendering code work also on the server-side.
&gt; **Edit:** Here are some _differences_. Decide for yourself whether they qualify as being _better_ or worse than Hayoo and Hoogle. 1. It's not limited to Haskell: you can _also_ search [over 150+ additional APIs](https://kapeli.com/dash#docsets) locally with this tool. 2. Weighing just 232 SLOC (statement lines of code), it's **12x lighter than Hoogle** (2,814 SLOC of Haskell) and **231x lighter than Hayoo** (53,775 SLOC of Haskell), according to [cloc 1.64](https://github.com/AlDanial/cloc). 3. It's implemented as portable POSIX shell scripts and documented with UNIX manpages.
JavaScript DSL? GHCJS is a Haskell compiler that targets JavaScript. It's not a DSL. It lets you run Haskell on the browser.
that's really a shame -- it seems like even the most naïve inlining should be enough to close the gap between the two
[removed]
https://www.reddit.com/r/purescript/comments/3wn4tb/halogen_signals/ It seems that it has some aspects of it, but it is not really FRP.
Cool, this link seems to explain more: https://www.reddit.com/r/purescript/comments/3wn4tb/halogen_signals/ There was a video a while back where one of the designers showed the main idea behind the library (it seemed inspired by FRP). But since then the api changed, so I am not fit to say what kind of library it would be ^^. 
Being a fan of [the Suckless philosophy](http://suckless.org/philosophy), I'm predisposed toward subjectively (and lazily) comparing open source software with the objective measurement of their SLOC counts. You know, _Occam's Razor_ and all that. I defined the term "SLOC" as a matter of thoroughness, for the benefit of those who might be unfamiliar with the term; it certainly wasn't meant to insult your knowledge, or capacity thereof, in any way IMHO. Sure, this tool (being so tiny) isn't meant to eclipse the rich feature set of Hoogle and Hayoo (having much larger codebases). Nevertheless, one might find this tool's offline API documentation set for Haskell to be handy, in a pinch. YMMV.
Personally, I'd trust 10000 lines of Haskell over 100 lines of shell scripts any day. Because shell is much easier to get wrong due to a non-existent static and primitive dynamic type systems. Also, your comparison does not include the fact that, and please correct me if I'm wrong, your tool does not actually generate a search index, but relies on an existing one published by a developer of a proprietary tool (with the obvious implication that they might pull the plug any day). You've applied Occam's Razor incorrectly. It requires a set of competing hypotheses that explain certain phenomenon equally well, which you don't have. (Thinking with types sometimes helps one catch obvious errors!) Second, you misrepresent Hayoo and Hoogle: they do in fact support offline mode.
It's true that this tool doesn't generate a search index; it uses existing indices provided by the proprietary [Dash for OS X](https://kapeli.com/dash) tool. However, users _also_ have the ability to generate such indices themselves, [as documented here](https://kapeli.com/docsets), so at least there isn't a _complete_ danger of indices becoming extinct. :) Regarding Occam's Razor, isn't the phenomenon here _the act of looking up_ Haskell API documentation offline? Surely all of the tools under discussion do _exactly_ that, and the simplest of them (by SLOC count) is this one, wouldn't you agree? How have I misrepresented Hayoo and Hoogle's offline capabilities? Are you treating the fact that I never mentioned those capabilities as _misrepresentation_? If so, that was neither intended nor implied because I also _didn't_ claim that Hayoo and Hoogle lacked offline capabilities. Finally, how am I to know what a stranger on the Internet finds to be meaningful? :) From my view, I certainly did answer your question of "How is this different/better than [x]" meaningfully, for I pointed out some differences, as you requested, that simultaneously reflected my opinion of why it's "better than [x]". &gt; **Edit:** For now, I've gone back and added the following disclaimer atop my answer, to appease your discontent: &gt;&gt; Here are some _differences_. Decide for yourself whether they qualify as being _better_ or worse than Hayoo and Hoogle. YMMV and thanks for your feedback.
Yes, they are commutative here, but no, they are not commutative in general.
&gt; In practice, for real applications you almost never want to use the same datatype for frontend and backend due to information hiding In my experience only some "top types" are thus hidden, most of the data types that I use in my APIs aren't changed.
What other objections? I know some, mainly raised by Edward Kmett, but they did not convince me. (I did not fully understand some of them, I have to admit.) Regarding your deeply nested stacks, even with just one Reader layer EE-freer performs comparably to mtl according to the paper. I got more curious about those things, because I use the freer package for a project of mine - and I really like it.
The syntactic overhead with freer is already pretty much ok - I think, especially if you take advantage of ConstraintKinds: https://downloads.haskell.org/~ghc/7.4.1/docs/html/users_guide/constraint-kind.html
Could you share some more on your ghcjs minification process? And the intermediate code size? Original Size, after closure compilation, and after final compression with zopfli? With the relevant command flags?
I mean the UI, not algorithmic speed. one keystrokes you have dash, you start typing it refines on the fly. Also, the same UI for many different kind of documentation. it's really sweet.. (but you can't search with signature of course)
Step 1: ccjs all.js --compilation_level=ADVANCED_OPTIMIZATIONS &gt; all.min.js Step 2: zopfli -c -i1000 all.min.js &gt; all.min.js.gz This is as described in the GHCJS deployment wiki. Step one goes 5mb -&gt; 1.6mb, Step 2 goes to 300kb. However, this depends also on removing extraneous instance derivations by hand, minimizing imports, not using Integer.
Yeah, I totally agreed with you about sharing datatype between frontend and backend, in some language's Web API framework(like Ruby's Grape), there are a concept call entity, which is a serializable data with an context, it control how to a serialize a backend datatype for frontend, I think we can use Haskell's type system to express this useful idea (phantom type?)
There is a different implementation of extensible effects that uses the freer monad (freer is the name of the package). I've been experimenting with it. Although I haven't tested performance, working with it has been a dream (especially compared to the Functor-bound implementation). It's a really interesting way of doing things
It's not horribly bad. The whole point of ST is to allow writing mutating algorithms behind pure interfaces. At the end of the day, when we write library code, we produce value for our users, and if our ST implementation is 5x faster than the immutable implementation, this should be seriously considered. The tradeoff is that our implementation will be uglier (which isn't set in stone, and partially Haskell's fault) and possibly (but not certainly) harder to understand for potential contributors, and would be less amenable to formal verification. These trade-offs should be weighted proportionally, and not dismissed based on some abstract ideological disposition. A great joy of many FP abstractions is that internals can be truly hidden, without any leaks. This gives us freedom that should be appreciated. It is absolutely *not* an essential feature of functional programming that implementation details use immutable data structures. Immutability has many advantages, but it's only a means to get what we ultimately care about, namely strong guarantees, abstraction, flexibility, performance, etc. I have written a great deal of very low level Haskell, and it looks like very ugly C code, with just slightly more guarantees than C. But this is not the way it must eternally be. I think that in a future dependently typed language it would be feasible to implement primitive operations as a strongly typed EDSL inside the language, with memory layout and field sizes reflected in the type system. That's the real power of types: that whenever we have a problem that have to be solved and cannot be worked around, we can formalize it and abstract over it in a machine checked way, no matter how messy it is. In Haskell it's best practice to stick to domains where Haskell's type system is strong enough to provide reasonable guarantees. In many domains Haskell is too weak for that. This is a fact about Haskell, not about domains. 
Agreed. But since the type used right before serialization is used only for that and is designed for that, you gain little or nothing by the frontend and backend being written in the same language.
I suppose this is the one you are speaking of? https://gitlab.com/cpp.cabrera/freer
Ah sorry, of course. These are discrete time FRP-like systems (we need a term reactive Flow-based architecture or something rather than FRP, which clearly uses continuous time).
I am planning giving react-flux a go sometime soon. https://hackage.haskell.org/package/react-flux There is not a huge tutorial or anything but there are examples.
Housing in London is indeed very expensive. It's possible to commute from further away and have cheaper living costs, but then you pay additional transport costs and opportunity costs with your commute time.
Looks very clean! I'll give it a try
You get the same. Problem with transformers. The only difference is that with effects, you defer the problem to a single location: where you finally run your effects.
The whole point of EE is to handle that. myEff :: (Member (State Int) r, Member (Reader String) r) =&gt; Eff r (Int, String) myEff = do i &lt;- get s &lt;- ask return (i, s) runState 1 $ runReader "" myEff Here, state and reader have no common class instances required. All that matters is that the effect type contains those effects.
Thanks for clarifying. I was indeed referring _only_ to HTML rendering time. :) This tool only uses the basic SQL LIKE operator; nothing advanced like FTS.
Oh yes. It indeed violates the laws. *Main&gt; let uuu = M.fromList [(Product x,(++ show (101+x))) | x &lt;- [-1..1]] *Main&gt; let vvv = M.fromList [(Product x,(++ show (201+x))) | x &lt;- [-1..1]] *Main&gt; let xxx = M.fromList [(Product x,(show (301+x))) | x &lt;- [-1..1]] *Main&gt; uuu &lt;*&gt; (vvv &lt;*&gt; xxx) fromList [(Product {getProduct = -1},"300200100"),(Product {getProduct = 0},"301200100"),(Product {getProduct = 1},"302200100")] *Main&gt; pure (.) &lt;*&gt; uuu &lt;*&gt; vvv &lt;*&gt; xxx fromList [(Product {getProduct = -1},"302202100"),(Product {getProduct = 0},"301202100"),(Product {getProduct = 1},"300202100")] Try three maps with keys from `[-1 .. 1]`, the values don't matter as long as all combinations are unique.
&gt; hard to convince friends that Haskell is a good language for writing imperative code Why is the burden of proof on Haskell? I've never seen a convincing argument from the other side that their way is better than Haskell. In fact, the strongest argument they can come up with for not learning Haskell is that they don't understand Haskell. facepalm. while (!heap.isEmpty() &amp;&amp; !visited.contains(t)) Pretend you've never learned a C-like language and tell me that doesn't look like gibberish. What the hell does "&amp;&amp;" mean? Why do some words start with "!"? What's with the empty parenthesis? Etc.
&gt; how do you do things that javascript can do (like mess with the html and stuff)? You want [ghcjs-dom](https://github.com/ghcjs/ghcjs-dom). Messing with the html and stuff is done in exactly the same way as in JavaScript, since ghcjs-dom is just a thin wrapper over FFI definitions. Here are some of the pieces: type WebView = Window runWebGUI :: (WebView -&gt; IO ()) -&gt; IO () -- This one is in Maybe because in JS, window.document could -- be null. webViewGetDomDocument :: Window -&gt; IO (Maybe Document) getBody :: Document -&gt; IO (Maybe HTMLElement) createElement :: (ToJSString tagName) =&gt; Document -&gt; Maybe tagName -&gt; IO (Maybe Element) appendChild :: (IsNode self, IsNode newChild) =&gt; self -&gt; Maybe newChild -&gt; IO (Maybe Node) ghcjs-dom has really good coverage. Every W3C standard functionality I've needed so far, I've found an FFI wrapper in there. If you know how you would express some DOM affecting program in JavaScript, just look for those method names in ghcjs-dom and you can probably directly translate the program.
yup! see [my other post](https://www.reddit.com/r/haskell/comments/4923ql/real_benefit_of_servant/d0opx2q). These alternate API types and their implementing servers can live in entirely separate modules if you like.
I think I could prevent the problem you mention by having a check constraint on the row such that left or right is set and not both, plus foreign keys from rows to left/right and from left/right to rows, if only I could guarantee certain paths: * l.row.left = l * r.row.right = r * x.left.row = x if x.left is not null * x.right.row = x if x.right is not null Not a clue how I could accomplish that though!
I love that operator! Also (&lt;|) = ($) and (&lt;&lt;) = (.) and (&gt;&gt;) = flip (.) make the code a pleasure to read and more numpty friendly.
Hi, it may be worth to scan the following for ideas or inspiration (Section 3, in particular): [*Algebraic Data Types for Language-Integrated Queries* (PDF)](http://db.inf.uni-tuebingen.de/staticfiles/publications/ddfp2013.pdf) 
&gt; ghcjs-generated js code is large May I ask you how large? And if you used a compactor like Ggl Closure? And Roughly how many lines of GHCJS compiled Haskell you have in the project? Thanks in advance.
You may be interested to see how `persistent` library does that. A sample example is given in custom fields section: http://www.yesodweb.com/book/persistent#persistent_custom_fields
Is there a video of this talk somewhere?
[Found it](https://www.youtube.com/watch?v=P3FjDZQHXbs&amp;list=PLnqUlCo055hV5dPC-4VWeXzhI8ooeTsVy&amp;index=26).
I would assume so. Seems like that would be the reason for packages like servant-ede.
The two most common reasons for me: - I've got a local fork of a library and I need to switch between that one and the one on Hackage, so I bump the fork's version and use `--constraint` to switch back and forth - Some combination of my dependencies fails to build, and I want to experiment with fixing one particular dependency. This is kind of the converse of what you had in mind - a transitive dependency pushed a PVP-happy update to Hackage, but my direct dependency didn't specify an upper bound on that changed one. `--constraint` is helpful for testing if that sort of thing is going on.
&gt; so I bump the fork's version and use --constraint to switch back and forth Wouldn't `--allow-newer` handle that without having to place an explicit constraint for your new version? &gt; a transitive dependency pushed a PVP-happy update to Hackage, but my direct dependency didn't specify an upper bound on that changed one. If I understand you correctly, you're saying that you had no upper bound and something broke, implying that you actually should have had an upper bound. In this case, you're using `--constraint` to slip that upper bound into the solver without editing your package's metadata. Is that right?
&gt;Pinker's Abstract Algebra I can't find anything with that name.
Whoops, typo on my phone, fixed now.
&gt; Wouldn't --allow-newer handle that without having to place an explicit constraint for your new version? Oh, what I mean is that I want to sometimes compile with my local copy, sometimes with hackage. I use `--constraint` to be specific and not give the solver any choice. &gt; you're saying that you had no upper bound and something broke, implying that you actually should have had an upper bound Here I meant, for example, I directly depend on foo, and foo depends on bar. I put bounds on foo, but since I don't use bar directly, it's not in my import list or .cabal file. If I suspect a problem coming from the lack of an upper bound on bar in foo's cabal file, I just quickly add a `--constraint="bar == x.y.z` to pin bar down and see if that's the issue. This could just as well be done in my own .cabal file by explicitly depending on bar - would just be a few more keystrokes.
Gallian's Contemporary Abstract algebra is a phenomenal introductory book, and includes programming exercises! It is pricey, but the older editions are quite cheap and still very good.
I use it a lot when working offline, to stop cabal trying to use versions of libraries I haven't got cached locally.
Like /u/edwardkmett I use `--constraint` a lot for exploring the install-plan configuration when trying to figure out which upper/lower bounds are inaccurate in released packages on Hackage. It's also noteworthy that [`--allow-newer` was extended to support limiting its scope to packages](https://github.com/haskell/cabal/pull/3171) in Cabal 1.24. I.e. you can now say `--allow-newer lens:base` to relax the upper bound for `base` specified by the `lens` package only.
The real cost for me is in the user workflow of switching to the correct app, putting the cursor in the correct field, clicking on the resulting link, clicking on another one because it was the wrong one. It's this latency, measured in seconds, which constitutes the 99% bulk of the latency and causes context switching. It would be useful if you have some small video demo. Screenshots are not meaningful for this sequence of action
for haskell i learned mostly from "learn you a haskell for greater good" (strange title i know)
Just to toss out the obvious "quick solution" so it gets a fair discussion: One table with a "tag" or "type" column, and then enough columns to accommodate the various types that comprise the sum type. rows ------- type ("left" or "right") left_int right_float right_str You could use a CHECK to ensure that the left and right columns aren't filled out simultaneously, and that the correct columns are not null given what's in the `type` column. In postgres, at least, `NULL` columns don't take up any space, so you wouldn't have a bloated table from this design.
I also recommend Bob Harper's book, Practical Foundations for Programming Languages
Not really. :( I've also had trouble finding anything useful on GHCJS. The good news though is that if you use reflex-dom, you don't need to do very much with GHCJS directly. And in the few cases that I've needed to, I can often find some similar code elsewhere in reflex-dom that I can copy. To get started with reflex, I'd recommend watching some of these talks by Ryan Trinkle: [https://www.youtube.com/watch?v=mYvkcskJbc4](https://www.youtube.com/watch?v=mYvkcskJbc4) (slides [here](https://obsidian.systems/reflex-nyhug/)) [http://www.infoq.com/presentations/functional-techniques-complexity](http://www.infoq.com/presentations/functional-techniques-complexity) [https://www.youtube.com/watch?v=92eXGvHFbzs](https://www.youtube.com/watch?v=92eXGvHFbzs)
right now i'm rewriting a huge amount of php code in js because php does not work on the client and I have to ajaxify a form. this leads to a cost overrun which my managers will have to justify to their clients. unfortunately my suggestion that we might use the php 'v8js' library to render our templates with js to avoid this trouble in future is not being listened to. even when i'm done we will have to do every change twice from now on. so yes, it's relevant alright. one of my gripes with yesod is that it has server-side templates. yes you can avoid them but it still encourages this kind of disaster. 
And it's freely available online: http://learnyouahaskell.com/chapters
How does algebra relate to functional programming? I know that sometimes authors develop a basic notion of category theory in order to present free groups and may be some other concepts as well, but other than this, is there any benefit in learning algebra topics? In other words, the question is would I lose by substituting algebra with any other subject loaded with categories, e.g. some text with modern treatment of algebraic geometry.
&gt; Then it should work, right? Nope. At least not in automatically generated version by stack yesod website. By default, for some reason, env variables in `config/test-settings.yml` file don’t work. You have to change one argument in `test/TestImport.hs` file. The reasoning for this was so that people using environment variables for development don't accidentally wipe their development database when running tests. You aren't the first person to be tripped up by this, though (see https://github.com/yesodweb/yesod-scaffold/issues/56). Any ideas on an improvement? Maybe a comment around `ignoreEnv` would help the code jump out for developers, or more extreme, have a log statement run in the tests saying that environment variables are ignored (something like "Note: By default environment variables are unused for the test environment—switch to `useEnv` to enabled them. (You can delete this log message now that you know)")? I think I'm in favor of at least a small comment.
I would add to the list: - Purely Functional Data Structures (Okasaki, 1996) - Pearls of Functional Algorithm Design (Bird, 2010) I would recommend *against* Tarski for an intro to logic book. Since you want to learn functional programming anyway, I would recommend: - The Handbook of Practical Logic and Automated Reasoning (Harrison, 2009) In that book, Harrison introduces First Order Logic while showing you how to actually program decision procedures in OCaml all along the way. As for category theory, I would recommend: - [Introduction to Categories and Categorical Logic](http://arxiv.org/abs/1102.1313) (Abramsky, 2011) The above emphasizes the connection between Intuitionistic Logic and Cartesian Closed Categories, as well as the connection between the Linear Logic and Closed Monoidal Categories.
Another option might be to use Gabriel Gonzalez's [optional-args](http://hackage.haskell.org/package/optional-args-1.0.0/docs/Data-Optional.html) package. If I understand the docs, your example, given a correct signature, becomes: let params = defaultImageParams { width = 800 } *update:* I just confirmed this works: {-# LANGUAGE OverloadedStrings #-} module Main where import Data.Optional import Data.Word type Color = (Word8, Word8, Word8) data Format = PNG | PDF deriving (Show) data RandomSeed = Randomize | Repeatable deriving (Show) data PDFUnits = Points | Inches deriving (Show) data ImageParams = ImageParams { format :: Format , width :: Optional Double , height :: Optional Double , hId :: Optional String , size :: Optional Double , color :: Optional Color , lineSpacing :: Optional Double , lineSpacingVariance :: Optional Double , wordSpacingVariance :: Optional Double , randomSeed :: RandomSeed , pdfUnits :: PDFUnits } deriving (Show) defaultImageParams :: ImageParams defaultImageParams = ImageParams { format = PNG , width = Default , height = Default , hId = "2D5S46A80003" , size = Default , color = Default , lineSpacing = Default , lineSpacingVariance = Default , wordSpacingVariance = Default , randomSeed = Randomize , pdfUnits = Inches } main :: IO () main = do let params = defaultImageParams { width = 800 } print params Results: ImageParams {format = PNG, width = Specific 800.0, height = Default, hId = Specific "2D5S46A80003", size = Default, color = Default, lineSpacing = Default, lineSpacingVariance = Default, wordSpacingVariance = Default, randomSeed = Randomize, pdfUnits = Inches} 
That's perfect! Exactly what I was looking for! Thanks a lot of testing it out too.
A lot of good recommendations, but let me add a few. First of all every book by Pierce and Bird I would highly recommend--I consider them among the best books written on any subject. Note also that just like programming you cannot learn math from reading a book--you need to work as many exercises as possible. This is one thing that makes Software Foundations so great, since you get feedback on your solutions. I'm currently working through SF but doing the solutions in Agda, which is an extra challenge and well worth it. Regarding algebra, I learned from Michael Artin at MIT using his text (back then just typewritten notes with many typos) as an undergrad, and took grad classes from Ken Ribet at Berkeley using Lang's text (although I really mostly used Dummit and Foote), and Richard Elman at UCLA using his own notes and superb exercises, and again I mostly used D&amp;F. All three texts are excellent--I would recommend D&amp;F for a first go-through and then Lang once you already understand the material and want to see it presented particularly beautifully (and Lang brings in category theory early on). Once you get to Galois Theory use the superb text by David Cox, another exceptional author. Having TAed the honors undergrad algebra class twice for Elman (which used his notes plus D&amp;F), I can say that if you want to learn algebra reasonably well you need to spend 20 hours a week on it for an entire school year. Even this will just give you the basics. I do recommend learning algebra before category theory if you have the time, since the latter is an abstracted version of the former and it's important to see the "concrete" version (still very abstract) first. As for learning category theory, Awodey's text seems best, supplemented by Pierce and others. Again you have to work exercises. A great way to learn is to follow Lang's "advice" for learning homological algebra--cover up all the proofs in the book and prove them yourself. I've been programming professionally in Scala for the last four years and although it's a fine language I would not try to bother learning theory using it. The OO aspects just get in the way. Stick with Haskell and dependent types. Speaking of the latter there is a superb text [Type Theory and Formal Proof](http://www.cambridge.org/US/academic/subjects/computer-science/programming-languages-and-applied-logic/type-theory-and-formal-proof-introduction) which is very carefully and clearly written and complements Pierce's TAPL perfectly.
I have almost finished the book, and would highly recommend it to anybody interested in math or category theory, especially those of us that have struggled with texts more geared for people with better groundings in the terms and syntax of abstract mathematics.
cc /u/bitemyapp and /u/snoyberg for your thoughts
It is important that you feel comfortable with the subject. If so than there is no need to invest time to solve exercises. In my experience exercises like those in SF lead to an operational understanding of the subjects presented which is beneficial and necessary for a deeper understanding but not sufficient by itself. My preferred way to test my understanding is to ask myself really simple questions like "What is the significance of this?" or "Why does this work?". I frequently find that I'm unable to produce an answer that would satisfy me if told by someone else even on subjects I think I have good grasp on. If you can produce such an answer after some thinking I wouldn't bother with further exercises, if not either think some more or back to the exercises. I made the mistake of solving all exercises while working through SF and not asking myself enough questions, it took me quiet a while to pick it up again after I ragequitted after MoreLogic over the sometimes soul-sucking bureacracy.
Ah I see. OK. That sounds reasonable to me.
Good point. I've also wished for an `--offline` flag. I'd rather use that than guess about which versions I have cached.
There's a tutorial here. Reflex uses ghcjs. https://github.com/reflex-frp/reflex-platform
A PR for some comments in settings.yml and/or test-settings.yml that clarify the situation would be most appreciated. Would that address the concern?
I prefer doing a composite type or a table with no pk for each sum type. With all fields nullable you can programatically enforce that only one is not null. And use the null to tag the current sum type. This way you can even have array and nullable sum types in postgres.
Either use /u/bartavelle's approach or if your structure allows it, use a set of mutually recursive datatypes ie: ``` data FooTag = Foo [BarTag] data BarTag = Bar String | Moo [Foo] ``` 
But what if I want to be able to extend the hierarchy? For example, someone else can make a new type and say "it can be a child for Foo". That is why I talked about the type class approach, sorry if I was not clear.
You may be interested in the following paper: [Functional Modelling of Musical Harmony](http://dreixel.net/research/pdf/fmmh.pdf) [The Framework of Music Theory as Represented with Groups](https://www.math.washington.edu/~morrow/336_09/papers/Ada.pdf) The 2nd paper models music theory expressed in group theory. 
Abstract and Concrete Categories is even available for free online (from the author, [here](http://katmat.math.uni-bremen.de/acc/)). Despite this, I'd recommend against it. I was part of a seminar (of computer scientists) that tried to pick up category theory. We used (at different times) both Awodey's notes and Joy of Cats: Awodey was full of well-chosen but difficult examples deep from mathematics; Joy of Cats was overflowing with examples from every corner of mathematics, half of which we needed to ignore to get anywhere. It also had the problem of particularly long dependency chains in definitions. As a concrete example * Awodey defines adjunctions via hom-sets, which requires hom-sets and natural isomorphism; both are concepts which are used elsewhere and not that much more difficult than others. * Joy of Cats defines adjunction as follows: 1. An adjunction is defined in terms of a "G-universal arrow" 2. A "G-universal arrow" is a particular case of a "G-structured arrow" 3. A "G-structured arrow" is defined in terms of unique existence of an arrow in terms of a functor. I found the former relatively easy to follow and the latter impossible to get my head around. That's not to say, of course, that the latter is bad; it's non-standard and in such a way that is difficult for computer scientists (using a sample of ~6).
I believe this is the lady from TheCatsters Category Theory videos.
[Here is the same woman talking about bagels.](https://youtu.be/NRvK_07KRV8)
Over a year old but hasn't been posted before and I am curious to see reactions. In particular, he thinks lawless typeclasses are bad. I used to disagree but now I am seeing his point. I used to think lawless typeclasses (like QuickCheck `Arbitrary`) are useful, but now I see that its lawlessness leads to the very things that frustrate me about it (such as having no idea what `arbitrary :: Gen Int` is going to do unless I look at the source code.)
For what it's worth, I basically agree. I like typeclasses with laws, and I'm alright with using using typeclasses to implicitly create proofs. For most other purposes, I dislike them. The notable exception to this would be aeson, where `FromJSON` and `ToJSON` really do take advantage of defining an instance inductively to let you do things like `encode [[[[6 :: Int]]]]`. Using typeclasses to overload some kind of `toWidget`/`toText`/`toRow` function is easily my least favorite use (employed by `cassava`, `yesod-core`, `xml`, etc.).
I confirm! She was introduced to us as such at the conference, but I see that the beginning of the introduction was not included in the video.
You can explore (much of) this alternative today in Scala. I'll stick to newtypes. Once the provenance of an instance is a thing that can affect your answer you need to have (incredibly arbitrary) means to track it and talk about it, and importantly you _have_ to talk about it all the time. Certain refactorings that work today cease to be sound, even simple libraries like `Data.Set` where we can today work with a dumb 'Set' data type that doesn't know its ordering become dangerous, because you now have to concern yourself with the provenance of the instance. On the other hand, saying that you should use typeclasses ONLY when they are pinned down completely by a law is wrong too. [] isn't "uniquely" Applicative -- it could use the ZipList instance or expand the other way. Not every typeclass describes 'property-like-structure'. There are many potential `Ord` instances for a given type. Much of this is pinned down by mere convention, similarly the order fields are visited in `Traversable` is only pinned down by convention. It is great when this is the case, but it happens only a fraction of the time. Law tracking on the other hand, works great -- in a sufficiently dependently typed setting. Unfortunately, nobody with dependent types has full-on Haskell-style type-classes. Instead you get things like `coq` which blesses precisely one path through the class inheritance hierarchy, or mechanisms that are secretly like Scala's implicits, where provenance matters. In such dependently typed settings it also raises the concern of just in what notions of equality a given instance holds. https://www.youtube.com/watch?v=hIZxTQP1ifo 
[Category Theory for Computing Science](http://www.abstractmath.org/CTCS/CTCS.pdf) is free online, and approaches the subject from a different angle than Awodey's book, which some people might find easier to grasp.
Oh yes, you are right. It doesn't take on extra information.
If the only code reuse is the data structure and serialization, this could be achieve by generating code to the front end language. (Either from the back-end language or an "middle"-one). However, it's obviously better if code generation can avoided.
The problem (IMO) is that type classes are not the solution to _overloading_, despite that being the problem they were invented to solve. They are a solution to a different problem that we didn't even know we had until we implemented them.
Do you know of Lean has real type classes? I feel like it's more likely than it might otherwise be because they also have another name overload mechanism.
I believe that aeson has the following law: decode . encode == id which does span two typeclasses, but seems fairly intuitive as a law.
Well lift is lawful. lift satisfies: lift . return = return lift (m `bind` k) = (lift m) `bind` (lift . k) I think liftIO is really just a specialization and still complies with these laws.
Note that implicits make more sense in a context of theorem proving, because there are many properties where you only care about _mere_ existence and not which particular proof you pick, even if they are different as proofs (and indeed where you can often specify that all proofs of a given property are contractible -- i.e. equivalent). So for some, though not all cases, the downsides are much less significant.
I don't see how the compiler could enforce adherence to type-class laws as the author wishes. Halting problem.
You know this, but not all alternatives to type classes are like Scala implicits. For example, OCaml's upcoming modular implicits feature solves the problems with `Set`. In OCaml with modular implicits, `union` function for sets might look like this: val union : {O : Ord} -&gt; Set(O).t -&gt; Set(O).t -&gt; Set(O).t It is impossible for the two set arguments to have incompatible `Ord` implementations; they would have different types. Of course not everything is perfect, but there are safe and reasonably polymorphic alternatives to type classes for this problem, at least.
`ApplicativeDo` is nice if you have something that isn't a Monad, but you want to use do-notation.
https://github.com/idris-lang/Idris-dev/blob/master/libs/contrib/Classes/Verified.idr It's possible. The downside is proving the laws can be a bit painful.
https://hackage.haskell.org/package/acme-colosson-0.1/docs/Acme-Colosson.html#v:numberwang
Doesn't look like it, no
Let's just take `Set` for example. Today, both `lookup` and `insert` take the `Ord` instance, rather than the `Set` itself having to waste effort and semantic baggage carrying it around. If we have to worry about the provenance of the `Ord` instance for a type because there can be multiple, we can't do this safely. Moving the instances out to use sites is no longer a generally valid transformation. So we might try to recover with either ML style functors, or just carrying the instance around. We could bundle the instance selection with the `Set` module via an ML-style functor. In that case now the provenance of the module may well be a concern. If I go to take a `Set Int` built by using some combinator from one library and try to combine it with one generated by a different library the modules will be different if functors have generative semantics like they do in SML then this isn't okay. (If on the other hand we have applicative semantics like ocaml, we may be okay, for easy cases, as long as we don't need anything funkily polymorphically recursive -- in those cases I really don't know how the applicative semantics work out.) On the other hand, if I move it the `Ord` instance into the `Set` data constructor then I'm hoist on another dilemma. What does `union` do when the two sets have different orderings? Today in Haskell I can hedge union, but in a world where my Set carries the `Ord` instance, that isn't sound. I can't even insert the smaller into the larger if I want to have some chance of explaining to the user in a sane way which ordering they'll get on the result, so I get worse asymptotic performance, or I start relying on some magic way to check if two instances are "the same", but right now `Ord [Int]` is constructed when it is needed from two parts `instance Ord Int`, and `instance Ord a =&gt; Ord [a]`, so pointer equality isn't good enough to judge the semantic equality of two instances. For `Compose` on the other hand, canonicity says that if I build up a call to `traverse` in one part of my code or in another part of my code, it will traverse things in the same order when applied at the same types, no matter where it happens. This provides a lot of freedom to move that code around and safety you can feel in your bones that it if still typechecks that the move was sound. I enjoy programming in Haskell in many ways precisely because I can just relax and move code like this and never break anything. In scala even [sorting your imports](https://twitter.com/kmett/status/649775936267780096) can change the provenance of selected instances, so just winging it and saying "don't mix up your instances" is quite hazardous to your health. Or as the scala faq points out the rules for finding an implicit are [simple](http://docs.scala-lang.org/tutorials/FAQ/finding-implicits.html). Just look: * First look in current scope * Implicits defined in current scope * Explicit imports * wildcard imports * Same scope in other files * Now look at associated types in * Companion objects of a type * Implicit scope of an argument’s type (2.9.1) * Implicit scope of type arguments (2.8.0) * Outer objects for nested types * Other dimensions
When parsing a protocol's messages (say, Slack's [RTM](https://api.slack.com/rtm)) I often define FromJSON instances manually, and ToJSON instances (for debugging purposes) using generics. They do not round-trip, of course, and that bothered me. I ended up defining the FromJSON instances on a newtype, to convey that they aren't related to the ToJSON ones.
Just be sure to *not* have your food in a paper/plastic bag with you when you walk around there. You'll most likely be burglarized by those adorable big-eyed beasts :)
I always forget Rust, but then remember I can't have polymorphic recursion, and remember why I forgot, and forget again. =) (Technically, Ermine is also designed around having real typeclasses as well.)
To be fair, in OCaml we have no choice. The anti-modular nature of type classes means they cannot safely be combined with full modular abstraction (i.e. functors). Personally, I prefer implicits over type classes anyway (assuming they keep the rules for implicit scope simple), but it's not like there was the option to use type classes for OCaml anyway.
Wow - this looks like an amazing piece of work!
Sans orphans you'd have to define the class or the data type in the module in question in order to be able to sensibly define an instance replete with body there otherwise you'd just be able to require an instance without the ability to specify how it must be written. The data/type distinction is stronger in a Haskell-like language. The ideas I've seen floated around for backpack-like systems are more modest and wouldn't let you specify the instance body in the module signature, but at best instead merely let you require an instance. A `newtype` supplies you with one way to make a fresh type to hang instances on, yes, but we do have other, hackier, means to generate fresh types for instance to dispatch off, e.g. `reflection`. I don't know how such could be applied to this problem usably, though, and I'll freely admit I remain rather skeptical on the viability of the backpack approach in general.
&gt; [] isn't "uniquely" Applicative -- it could use the ZipList instance or expand the other way. Not every typeclass describes 'property-like-structure'. There are many potential Ord instances for a given type. Isn't that really one of the biggest issues with typeclasses? Having to choose different instances with `newtype` wrappers kinda destroys the beauty of the abstraction. Ideally, the **code**, not **convention**, should explain behavior. Maybe typeclasses are the best abstraction we have (you clearly know better than me), but they are certainly not perfect. As an example, I've been doing some work with *cereal* and the `Serialize` typeclass lately. So far, I have always reverted into explicitly calling the different serialization primitives, such as `putByteString` and `putWord16be`, instead of using the typeclass. To me, this is a bad example of the use of a typeclass. The default instance of `Serialize` is, quite frankly, almost **always** the wrong choice. The same is true for other widely used libraries as well. For example, the *QuickCheck* library would probably be better off if it didn't have the `Arbitrary` typeclass, but rather required the user to explicitly name functions in the `Gen` monad. In this case, however, it's easier to live with, since it's "just" test code. EDIT: typo, typeclass -&gt; instances
I personally find that about 90-95% of the time the instance I want is the one right on the data type I'm looking at. Every so often I have to put on or take off a newtype, but it is very much the exception, not the rule. YMMV, of course, depending on the sort of code you write all day! In exchange, the benefit is that I'm not stuck explicitly plumbing all the boring code to pass an `IComparator&lt;A&gt;` explicitly to the constructor for a class I had to bother to give a name to to get an `IComparator&lt;IEnumerable&lt;A&gt;&gt;` for computing lexicographical orderings, all over the place in my code. Instead I just wrote `Ord a =&gt;...` and used an instance on `[a]` somewhere inside the body and the compiler wrote all that boring code for me, and updated it when I refactored the code, and removed the context when I monomorphized it to `[[Int]]`, etc. I do agree though that `Arbitrary` is a pretty good example of something that is pretty hazardous to tie exclusively class. The requirements for test objects vary a lot, and the default `Arbitrary` definitions are only occasionally good enough for what you need. All you get is a bit of completely ad hoc polymorphism that makes your test cases quite a bit shorter. Fortunately you _can_ write code in the `Gen` monad and use it, so in the worst case you just don't call `arbitrary` directly, and make use of manual calls to `forAll` and `forAllShrink`. Serialization and `Arbitrary` instances are cases where you often need different formats / specifications for different situations and are often ill served by a one-size-fits-all typeclass if that is the only mechanism you expose.
Even limited form of abstraction that just let us do things like 'curry out' more global choices like what random number generator to use over an entire module without burying code in type and `Proxy` arguments would go a long way in the Haskell ecosystem, so there is room for **something** in the backpack design space. That sort of thing is where I remain somewhat optimistic.
I'm really happy to read that. In most cases custom monads, transformers or `mtl` type classes in API is an abuse and over-engineering. Thank you a lot for the nice post!
I'd say that is almost inherent in the limitations of record syntax. /u/emarshall85's suggestion gets around it by using type classes for literals behind the scenes (`Num`, `IsString`, etc.), but that only works as long as you don't need to assign to fields of non-primitive type. For the `color` field, for example, you'll still have to write `{ color = Specific (0, 0, 0) }`, which doesn't improve much on `Just ...`. Using some operator sugar, the lens-based solution can probably be written as defaultImageParams &amp; with width 800 &amp; with randomSeed Randomize which ain't terrible, though I wouldn't call it a thing of beauty either. `with` would be a custom function, though I'd be surprised if there wasn't a combinator that does what you want in the vast forest of optics.
Ah, this makes sense! I now have a better understanding of your problem, and realize my suggestions aren't helpful.
https://hackage.haskell.org/package/acme-realworld is my favorite.
Looking at hopper [1] I can see slides - do you have recorded talk of that presentation somewhere? [1] https://github.com/hopper-lang/hopper
&gt; Furthermore it means that the caller of the library will likely need to write more code because they have to implement the interpreter. For more complex libraries, this may be worth it, but for simple things like writing a client library for a web API, it is most likely overkill. Most effects should ship with a simple interpreter. With MTL, most transformers also ship with a simple interpreter. Interpreting effects and transformer stacks end up being the same amount of work. Anyway, the `freer` package is a really new approach to free monads that reasonably reduces the complexity of the `Eff` monad. It's still far from beginner friendly, but it's amazing technology. I think if everyone always used Freer, everything would be much easier to work with. Not needing type classes and being able to easily abstract on the idea of an effect is really nice.
I agree with you that Domain Theory is the most relevant way for someone who knows a lot of FP to penetrate the subject. The question was, however, "*How does algebra relate to functional programming?*" and I pointed out the overlap. On the other hand, assuming the student already knows basic topology and the λ-calculus, then I would recommend the student learn Domain Theory rather than studying algebra if their goal is to be a functional programmer.
Shouldn't these be available on hecklage.haskell.org?
I'll agree with the other comments that `liftIO` has some laws, but additionally it's extremely predictable. I'm not sure on this, but I think that there is at most one valid `MonadIO` instance that can be written for any monad transformer.
How can it be made more beginner friendly? I've heard various positive positive stuff about free monads recently, but have struggled to find examples that make sense to me.
I've been using reflex and reflex-dom and I quite like what I've seen. There are is a [reflex-examples repo](https://github.com/reflex-frp/reflex-examples) to get you started. And then there are quick references to get a handle on the ideas behind it. [reflex quickref](https://github.com/reflex-frp/reflex/blob/develop/Quickref.md) [reflex-dom quickref](https://github.com/reflex-frp/reflex-dom/blob/develop/Quickref.md) I'm currently writing a boardgame in it, and so far it is working great! Even has websocket support. I feel like using ghcjs-dom directly is not a great option, the api is huge, IO bound, imperative, and not well documented. But if you need to go lower level for performance reasons that it totally an option.
Those slides aren't done ... And probably shouldn't be included in the repo because it's more tex boiler plate/template than content :) Right now a lot of the language engineering is about the evaluator and runtime system, for a combination of reasons: it's gonna often be embedded in a server environment so strategically having it have a seperate heap from Haskell will be good for avoiding large host application gc pauses in a networked systems. Likewise we want fully deterministic execution, so we're baking a reduction / complexity tracker into program execution for both easy replay debugging and so that different servers can use this reduction count to time out long programs (Ackerman 100 100 terminates, but not in this lifetime ;) ), likewise having a performant evaluator / runtime is important for fast type checking, so the evaluator work is being built to be augmentable with supporting reducing on open terms in the heap. On the type system side: the end language / type system will look like a linear logical sibling to Agda. I think this gist https://gist.github.com/cartazio/3873e805ad288e52ece7 is a good demo of why copattern matching should lend itself naturally to linear logic (this is alluded to in the various copattern matching papers but not spelled out ) There's other stuff I could say about longer term design and plans but I think that speaks to where things are right now pretty well. 
&gt; I've heard various positive positive stuff about free monads recently You might have seen `free` where it was `freer` that was deemed not beginner friendly. &gt; but have struggled to find examples that make sense to me With regards to their motivation or implementation ?
The hopper language is definitely something that I hope we can grow a community around as it matures. It's still super immature but I'm really happy with the design iteration we've done on the code base so far and I hope it continues 
As a person who was definitely NOT convinced by frp that I've seen in the past, reflex-dom has completely changed my mind. Everything can be packaged into widgets that react on events and use the current values of other widgets. The code is extremely modular and everything just "works". Any widget can create its own events or behaviors which other widgets can then react on. My attempts to use purescript moved me from the haskell realm into the javascript realm, with all its build systems and such. Unless you are already use to it on a regular basis, it's a nightmare. Its set of libraries is just whatever happens to be on some github or another, with varying levels of updatedness, whereas ghcjs can pull nearly anything from hackage and uses stack as its build system.
Apologize my ignorance, but: I don't understand exactly what this is and what its applications are.
Right, thank you
Bitcoin! Disruption! waffle waffle &lt;/sarcasm&gt; No, really I think this enables transactions across distributed systems to eventually converge, and in particular in settings where every participant node has to be verified (e.g. corporate networks, or similar).
I think implementation. I have seen posts where people seem to suggest (similar to the above) that free monads make sensible alternatives to monad transformers: the idea loosely described appeals to me, at least, that you could separate logic from actions. I have seen (and used) what appears to be a common pattern of using something like `readerT` inside `IO` or another monad transformer stack. Would it make sense to translate a problem like that to using free monads? If so, what might it look like?
That is true. I've been meaning to look into `freer`. I went to the London Haskell meetup recently that discussed extensible effects, and it was mentioned that newer implementations are based on the `freer` monad. The whole concept looks really interesting so I'm keen to check it out. Could make a nice part 2 ;)
My point exactly! Typeclasses as such is not a bad thing, but the way they are used is often confusing, or just plain wrong. When I look at a type, and the list of instances for that type, I wish I could easily discern how those instances are implemented. Today I cannot; I need to know not only if there is an instance of a class, but **how** that instance works. This adds a cognitive load when reading code. Ideally, if a type can have multiple instances of a class, it shouldn't have an instance at all. Compare the Monoid instances for integers, where Sum or Product has to be chosen explicitly, to the Applicative instances for lists, where one is default and the other is wrapped with ZipList. The first approach feels "right" (or at least cleaner), while the latter feels "wrong".
I'm a little disappointed that it doesn't use `unsafePerformIO` and a shared global state.
False! These properties aren't decidable, but there's nothing stopping you from proving them.
Haven't looked at the particulars, but Raft implements state machine replication, which doesn't just make sure the replicas eventually converge. It makes sure they appear to the clients of the service as a single, correct machine.
Just out of curiosity : why is Cloud Haskell not used ? 
How do you quantify "over-engineering"?
Author here, I guess I would ask you, what could we gain from cloud Haskell? Distributed ledger is already a cloud-like use case with a focus on transactional execution, as opposed to data crunching etc.
The readme is pretty massive :) but you can also look at the tangaroa paper: http://www.scs.stanford.edu/14au-cs244b/labs/projects/copeland_zhong.pdf
no idea. Im just a person who is learning haskell and I like to use this idea for a accounting app I had in mind after I know enough Haskell. 
Thanks! We also open-sourced "masala", an ethereum bytecode interpreter as well: https://github.com/slpopejoy/masala
This was a great talk, and its a really interesting project. Here's a link to the paper mentioned: http://www.cs.columbia.edu/~sedwards/papers/zhai2015hardware.pdf
Pulling in lots of experimental libraries and language extensions in order to create the perfect type safe abstraction for the domain. The reason this is *over* engineered is because there is a huge overhead in learning the libraries/extensions; it is much harder to find good documentation/experience reports; bugs and other issues are more common. It's hard to quantify it, but you can usually tell by the number/type of libraries/extensions that are pulled in.
Mathematica is not a general-purpose programming language, and should generally not be used for the writing of programs expect for those of a very specific character: solve this equation, plot this curve, etc. As such it is very different from Haskell which is (mostly) intended for general use.
The only other public info is the presentation I did for the HyperLedger Foundation last week. My talk starts at around minute 18 I think. Sorry for potato quality sound (I sound less like a robot in person). https://drive.google.com/file/d/0B42vMkapQi1MUm93QlNobkxoRG8/view
Making it more beginner friendly (and indeed bringing its true nature to the surface syntax) is pretty much equivalent to implementing row types in Haskell, so I'm not especially optimistic.
I think Mathematica is a subset of the Wolfram language? But I'm not familiar with it.
Yeah, I ran into that color field issue quickly unfortunately. This is great though! Thank you for taking the time to do this. I actually think this is more readable than record syntax.
Interesting-- not to mention ambitious! I hope their research program succeeds.
For me, none of those are at all related to over-engineering. They all sound like risks to be considered when deciding whether to use a language / style of Haskell coding, but not signs of over-engineering. 
My quick messy answer: I have used mathematica for a few years and I have been a Haskell developer for about a year. I agree with /u/dalastboss answer that mathematica is a special purpose language. One other important difference that I notice is the semantic of types: Haskell semantics are all shaped around its powerful type system. Mathematica is almost a typeless language. You can specify a few annotations on function parameters, such as whether or not the input is a list, but it does not have any type comparable to Haskell. You should probably compare mathematica with matlab and maple rather than Haskell. There are all math oriented languages with a powerful library good for prototyping. Among the last three, I definitely prefer mathematica which has a more functional style (I don't know much to tell you about the difference between functional and rewriting style). While all these languages have a sophisticated set of library to deal with math issues, neither of them has a nice theoretical model that Haskell has. My short answer is that if you want to create a demo or prototype or you are writing a scientific paper that requires lots of plots and graphs, mathematica is your friend. If you are interested in learning a programming language that is beautiful and surprises you, learn Haskell.
Cool project! The local stack solution might be useful to run a full Haskell program on GPU. Hence GPU lacks of stack, instead has local memory. 
Well, those individuals can create small projects for themselves in their leisure time. Frankly, a lot of companies are reviewing public repos to help gauge fitness. Clearly, you can't disclose confidential intellectual property, and you need to have a good work/life balance.
&gt; those individuals can create small projects for themselves in their leisure time Easier said than done. One, not everyone has free time. Two, even if they do, they may be ideologically opposed to adding to the world of code simply for the sake of code (how many more custom blog codebases do we need?). Three, many tech professionals are under employment agreements which preclude outside work, even for leisure (or, more precisely, ensure that outside work is owned by their employer, which hasn't given permission to open source it). 
It's more accurate to say [where deer come to mug you](http://2.bp.blogspot.com/-XF4IqctOw_g/UgAkScqkETI/AAAAAAAAMMM/Erf06ikX5pU/s1600/IMG_3082.jpg) :) 
As a special-purpose language, Mathematica is all about its libraries, and its particular use case. You're not going to be using it write e.g. a networking sort of library. It simply just does not have the rigor for doing that sort of thing. That said, languages like Matlab and Mathematica are exactly what you should be prototyping a model in if you're a data scientist. You can iterate a model several orders of magnitude faster than in a general-purpose language. Figure out a good model in your SP language, then implement it in your GP language. If you are a science person, don't underestimate the engineering side. It goes beyond just Haskell good/bad. A lot of people end up reinventing the wheel no matter what their preferred language is because they don't want to read/reuse other people's code.
I tried to write an answer, but it appeared too big. So I wrote a post: http://blog.haskell-exists.com/yuras/posts/a-begginers-guide-to-over-engineering.html
I don't see why a description of a recent project couldn't be such an example of work.
Awesome community :)
It can be, but glancing over some code from multiple applicants can assist one to whittle the pool of candidates.
I've had to struggle with employer restrictions in the past, too, which is part of why I left it a bit open-ended. In many cases, an example of work would be an open-source library, but in other cases it could be, for example, a publicly released project that you worked on. Even if the code is still private, I can take a look at the result and you can explain what role you played in delivering it. The reason for this requirement is that, in my experience, resumes and interviews alone are not very good predictors of success. For me, what has worked most reliably has been looking at actual work product, discussing how it was created, and learning about the details of how a candidate works. If we can understand each other well at this level, we're likely to have a good working relationship.
Matematica is made for math. Haskell is made out of math.
Does it have to be in Haskell? Basically are you looking for someone who already up to speed in Haskell or would there be room to learn on the job?
I'm just gonna comment from a general software engineering perspective rather than a Haskell-specific perspective. In modern times, on modern computers, I would have a lot of questions to ask you about the wisdom of this approach. You can generally expect a power-law-esque hit rate on the keys. It is likely that you'll get the vast bulk of hits on a small fraction of your cache size. It is likely that for all the complexity and significant risk of errors in implementation you'll see no perceptible speedup. I'd really want to see some serious real-world benchmarks that say worrying about weak ref-based caches will buy you real performance before I'd pay that price. (It shouldn't be hard to simulate such things by temporarily finding some more RAM and simulating what would go to weak refs by aging things out into a second cache before finally removing them, and measuring the hit rate of the hard refs vs. the simulated weak refs. But bear in mind you really need _real_ load for this to be a valid test.) When 4MB was a lot of RAM, this sort of approach could still be saving objects that were reasonably likely to be hit, but nowadays, you're probably much, much farther into the tail before the weak refs start getting used. You'll almost certainly get a bigger bang-for-the-buck just working on memory efficiency directly and keeping away from the weak refs. I've hedged significantly in this post because there are small places still remaining in the possibility space where this may make sense. If you think you are in one of them, explaining exactly where you are may also help other people make more concrete Haskell-specific suggestions relevant to your specific situation.
[removed]
[Raft](https://en.wikipedia.org/wiki/Raft_(computer_science\)) is a consensus algorithm (similar to the infamously complicated [Paxos](https://en.wikipedia.org/wiki/Paxos_(computer_science\))) for distributing a state machine/stateful database, by electing a leader to settle disputes on the order of events. This is in contrast to, e.g., [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type)s, which ensure that all nodes inherently converge (when they've caught up on all the events), by using commutativity constraints (either on the events themselves or on the representation of the state) to ensure the order events are received doesn't matter (but where nodes don't necessarily look the same at any given point in time). [Byzantine Fault](https://en.wikipedia.org/wiki/Byzantine_fault_tolerance) - Any fault presenting different symptoms to different observers. [Smart Contracts](https://en.wikipedia.org/wiki/Smart_contract) - programs that execute contract terms. (Maybe a better explanation [here](https://www.fastcompany.com/3035723/app-economy/smart-contracts-could-be-cryptocurrencys-killer-app), I'm admittedly not too familiar with it).
No, Haskell will not likely help you learn Mathematica. you're better off playing with mathematica itself. Mathematica is not strictly a functional language. it claims to support all programming paradigms. However i would not even think of it as a programming language (although it is) and programming experience only helps a little. it's just a math tool. 
You might be looking for [Control.Lens.At](https://hackage.haskell.org/package/lens-4.13.2.1/docs/Control-Lens-At.html) - although there are indexed variants in there as well :)
[removed]
Here is a motivating question that has deep consequences: What happens when `i` is out of bounds? There are ad hoc combinators in `Control.Lens.At` that provide the functionality that you are looking for. Skipping to the end, what you are looking for things are combinators provided by lens such as `at`, `ix` and `contains` mostly. An `IndexedLens` or `IndexedTraversal` is something slightly different than what you are asking for here, it is one where you can also get some piece of meta-data about each value it visits in the larger structure. For example, `itraversed` is an indexed traversal, which gives you access to the key in the map, position in the list, etc. It is the equivalent to `traverseWithIndex` from `Data.Map` but converted into a slightly more composable form, where if you use it as a normal traversal, it just works, but if you need the index it is available to you. As for what is in `Control.Lens.At`: `contains` can be used to access the presence or absence of a member of a `Set`, and to change that fact. `ix` can be used as a traversal of a given index in a container, but can't change its presence or absence. This lets it work with more container types, such as lists and vectors, where you can't add a 5000th element of the list without having to add all the elements in between. When you absolutely know the index you are using is inside of your container, you can use tricks like `singular (ix i)` to force this traversal to pretend to be a lens. This way you are taking it upon yourself the obligation to prove `i` is 'in bounds', otherwise you just have a traversal and can deal with missing data more directly. `at` can be used as a lens that given an index into a container gives back Maybe an element. This can be used to set the value or delete it. This works for things like maps, but not for lists. Note: while they could be (and once used to be) these aren't indexed lenses and traversals (mainly because it made it considerably harder for users to supply instances of the Contains, At, and Ixed classes!)
You have a typo in the text : "mangers" instead of "managers"
Any thoughts of splitting out the consensus modules to a separate library for use in other types of projects? 
Maybe the generalization holds if you can guarantee that the IntMap is dense in the keys ..
This is a proof of the Raft consensus algorithm using the Verdi formal verification tool : http://verdi.uwplse.org/raft-proof.pdf 
[**@mwotton**](https://twitter.com/mwotton/) &gt; [2016-02-23 22:23 UTC](https://twitter.com/mwotton/status/702257440574799873) &gt; just got bitten by (length $ Just [1,2]) == 1. Now, should I laugh at myself, or send a grumpy email to the mailing list... ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) ^(Starting from 13th of March 2016 /u/TweetsInCommentsBot will be enabled on opt-in basis. If you want it to monitor your favourite subs ask its moderators to drop creator a message.) 
Thanks!
From a brief look at the tech list they posted, it's probably mobile web applications implemented with GHCJS/Reflex-DOM.
Thank you. 
Strange, we both read the same comment but came to opposite conclusions! My understanding is that yes, there is a notion of indexed lens which allows you to abstract over all of those, namely `singular (ix i)`: atIntMap :: Int -&gt; Lens' (IntMap a) a atIntMap n = singular (ix n) atVector :: Int -&gt; Lens' (Vector a) a atVector n = singular (ix n) atList :: Int -&gt; Lens' [a] a atList n = singular (ix n) The fact that they all have the same implementation isn't even all that important, if you just want to write an algorithm which works on all those different containers, all that matters is that they all have the same type: swapLens :: (Int -&gt; Lens' fa a) -&gt; Int -&gt; Int -&gt; fa -&gt; fa swapLens at i j fx = set (at i) (view (at j) fx) $ set (at j) (view (at i) fx) $ fx Of course, using that abstraction, you can only write algorithms which read and set existing elements, you can't add new ones. Does your algorithm need to add elements, and if so, does it do so in a way which would work for all the containers you mentioned, for example by never deleting and always appending at the end?
My experience is performing what I would call "large scale" computations using Mathematica. I am a theoretical particle physicist and I use Mathematica all day every day for (what I feel) is pretty heavy analytical work. The codes are all one-shot, as they are simply big calculations and it is the kind of thing that Mathematica is usually pretty good at. However, once the code reaches a certain size, the un-typed nature of Mathematica becomes problematic as you have to change a piece of code within a large block. This is usually exacerbated by the fact that Mathematica does not terminate when something goes wrong. To this end, especially as the results often prove hard to check at the end, I find it very useful to apply typing concepts which I have learned from only a very basic introduction to Haskell. Essentially, I have implemented some very basic type checking code for Mathematica, failing immediately when the types do not work out. This all occurs at run-time and (if I ever get the time) I would be interested in trying to implement some sort of type elision, during the code loading stage. In summary, I think that the ideas learned from learning Haskell can be very useful in writing stable Mathematica code, and I wish Mathematica had some of these things built in! 
http://hackage.haskell.org/package/lens-4.13.2.1/docs/Control-Lens-Cons.html provides machinery for consing on unconsing off elements to various containers in a rather ad hoc fashion, but you'll note this forms a prism rather than a traversal, and doesn't give indexing capabilities.
I think it helps only a little bit. You can choose to treat Mathematica as a functional language and ignore the bits that does mutation, but really you can do that in almost any modern language: Python, ruby, etc. Mathematica has a little bit better support for functional programming than, say, Python out of the box but not much. In my opinion Mathematica's greatest strength is that is packs a large number of algorithms beneath an easy-to-learn and uniform interface. I once wanted to write a mathematical algorithm that automatically applies case analysis; the fact that a SAT solver is builtin makes Mathematica a more suitable tool than Haskell. I ended up writing a very bare-bones poor man's monad in Mathematica, but I think it's still much better than fiddling with a non-native SAT solver in Haskell. 
sorry can you tell me what's wrong with my answer?
Have you tried getting a job in another field? Web and graphic designers are expected to show-off things they've created. My vocation when I exited high school was as a seamstress, and none of the places I interviewed with were willing to accept "take my word for it, I really know what I am doing." The first place I interviewed with (and got the job) was impressed by sample projects I brought with me, and every other place after that had a sewing test. Given how many applicants will balk at completing a small programming test, what do you want employers to do? Diplomas aren't a good indicator of work quality, desperate employers hire low quality people all the time so you can't always trust work history. That only leaves code samples.
Have you seen the `stack` docker guide? https://github.com/commercialhaskell/stack/blob/master/doc/docker_integration.md
yep, I have seen that one. Still I have to find out how to make Atom or SublimeText working on the docker box 
It's certainly possible and fairly straight forward to do command line development in a docker container. Running GUI applications in a container is a different thing all together. A quick search gave me this. [running gui apps with docker](http://fabiorehm.com/blog/2014/09/11/running-gui-apps-with-docker/)
I very vaguely recall that stm-io-hooks works, so you could try it out. But I'm not sure it'll solve your overall problem. What if, in order to process the command, you need to do some IO? Say look something up in the database or send an email? Then, if I understand your model correctly, you'd have to do that IO in the transaction too. Now looking something up may be Idempotent but not email. So I don't think processing the whole command works well in STM. I could be wrong. But a typical web request handler works in IO, not STM. So, how to stop IO getting all over the domain model? Well, one idea might be to use a free monad and use the same interpreter for each aggregate. That may be overkill, see recent Reddit discussions, but that was what I thought of when I planned to do some event sourcing (I didn't follow through, sorry). Another idea is you don't necessarily need to mutate your aggregate if you have the log. After all, the current value is just a fold over its events. Or the life of the aggregate is a scanl over its events. So maybe you can use some FRP-oriented thinking here. Thinking in terms of whole streams is all the rage these days (I'm typing this from qcon).
I am using iPython/Jupyter with a Haskell kernel: https://github.com/gibiansky/IHaskell I think you can expose your local src directory to the Docker container and use a GUI editor like Atom or whatever. Described here in the Docker docs: https://docs.docker.com/v1.8/userguide/dockervolumes/
Maybe I don't understand your use case. Are you really trying to edit the files of a container you've already deployed to production? At work, we're deploying docker containers on our local machines in order to test them, and once they're ready, we deploy them to the cloud. When we need to fix something on those deployed containers, we don't log onto those containers to edit them live; that would contradict the idea of repeatable deployments. Instead, we deploy them locally, we fix the bug, we create a new image, and we deploy that new image on the cloud on top of the broken one.
Let me not agree with you on the RAM size on modern computers that you pointed out. I think at the time that 4MB was a lot of RAM, 1GB of data to process was also a lot, while nowadays, processing terabytes of data is pretty normal. We may also be happy if we keep half of the 16GB RAM of our laptop always free since RAM is cheap nowadays, but if you pay hundreds of thousands of dollars to Amazon each month to rent a bunch of servers, the story is different. You either want to use every bit of RAM that you have or cut a few percent of the RAM usage that costs probably more than the salary of a developer. I like your suggestion about simulating the weak references with an extra cache. I am pretty sure in many use cases a huger cache is not so helpful, but in my use case I can imagine cases where a small cache will not do almost as well as a larger one. It's very case dependent and may vary between customers. I'll edit my question and try to explain my case in more details.
"Power-law-esque" is important. Your cache hit rate will _not_ scale linearly with memory. Increase your data size by 100x, and you may well be able to maintain the same cache hit rate with a .5x increase in cache size; adding 100x the cache could well be a complete waste. That's why the size matters; it isn't the sizes themselves, but the fact that the distribution of cache hits is _likely_ to make it so you don't win anywhere near as much as you'd like with scaling up the cache. I, again, have to hedge, because while I'm comfortable saying "something power-law-esque" is by far the dominant case, if you have a strange use case you could get other distribution behaviors, in which case conceivably being able to stretch out another 10% in your RAM might be helpful. It's just not very likely.
Thank you for responding with a blog. Obviously, this topic is important to you. I don't think you've actually addressed your notion of "over-engineering." Taking your example, at each step towards more abstraction, your example does more useful things, i.e., handles more use cases. How is that over-engineering? The topic that you're addressing in your blog seems more of an educational matter. If one had a strong background in mathematical principles of programming, some of the blogs on FP would be at best mildly interesting at worst trivial and imprecise. Also, your example is a bit of red-herring, you're taking an example that's well known to any web developer, i.e., most imperative programmers who has done any backend web development would know this pattern instantly, e.g., cruddy CRUD. Let's say a pure mathematician, who has no absolutely no interest in datastores or computers, comes across your blog, how would such an example appear to her? 
If abstractions lead to safer or deterministic code or allows us to think and reason about code precisely, I'm all for it. It's not really at the cost of understandability. It's quite the opposite, it'll add to your understanding in precise ways. The real cost is that it's not immediately amenable to intuition without further study. For example, the abstractions in physics has obviously led to a much deeper understanding of the universe large and small. And such abstractions allows us to reason about physical phenomena and construct experiments to test those abstractions. Open any graduate-level text book on physics, the cost to those abstractions is that they are not immediately open to anyone coming to it without the necessary prerequisites.
I don't know the answer, but [this](https://github.com/ghcjs/ghcjs-base/blob/master/GHCJS/Buffer.hs) looks like a promising step.
Why not use docker locally? Trying to edit files on a remote server with a GUI application is probably going to cause pain in general, unless you really need it.
Echoing Laurence, I don't think any of the devs came in with any background in finance. The job will a great learning experience in many ways. It's also a great team to grow in. 
Related: [desugared dictionaries](https://ghc.haskell.org/trac/ghc/wiki/DesugaredDictionaries).
+1
Big differences: Types - Haskell is all about them, Mathematica doesn't care Libraries - Haskell has some things you can go and find, Mathematica has most things baked in. Mathematica libraries mostly deal with math Ease of use - Mathematica is easy to get up and running, and the interfaces are nice but not perfect. Haskell's tools feel like going to work in a medieval blacksmith shop. Use cases - Mathematica is geared towards scientific computing, and its paradigms can start to get in the way when your project gets bigger than that. Haskell works at any size. Similarities: Some functional concepts - you will get to use Map / Scan / Fold / Nest / etc in both languages. Both emphasize applying functions to data 
MTL lets you separate logic from actions too. MTL also lets you vary the interpreter. The only thing Free Monads give you above this is the ability to inspect/modify the syntax tree of the computation prior to evaluating the computation.
This is interesting. I had the same idea, actually, to use a static site generator with an API backend and a framework on the fronted, because I thought it would be cool to separate them out into projects that different teams could work on. Actually, I was imagining that product people wouldn't have to get us to update content-based templates anymore and we could work on data-delivery and fronted engineering problems. Ultimately, I couldn't tell if it was a bad idea or not because of too much separation and needless complexity. I'm looking forward to the more detail to come.
Very useful. Thank you. I've done several projects in a row with Elm + Haskell, but with ghcjs finally available over stack I've been anxious to try something. So... Another front end / flux / virtual Dom library? I'm paralyzed by the number of choices.
I can imagine this title showing up on someone's front page and them being confused and thinking it's talking about nuclear fusion 
After giving up on Awodey, I've had some luck with Goldblatt's "Topoi: The Categorical Analysis of Logic" Possibly because it was intended more as a philosophy book than a mathematics book.
@dizzeehaskell: If the person doesn't have any hands-on functional programming experience, but willing to study and believe he can master it. Do you consider give an interview opportunity?
"Wait, there's another team that uses Haskell that isn't Strats?" - *everyone reading this*
Take my upvote, you filthy animal
I suppose you mean a typeclass-based interface, because mtl only exposes a handful of effects ... Also free monads don't really give you the ability to inspect/modify the *whole* syntax tree of the computation, that would be free applicatives. The second argument of bind (of type `a -&gt; m b`) can't be inspected. What they also give you is a very convenient way to implement monads, so it is really not incompatible with an mtl-like approach.
yes
I've done this over the last few years. Several Go API services with Angular.js frontends, both built by different teams. Frankly, I've really enjoyed not having to care about UI/UX and left it to some amazing people who excel at that while I focus on the server infrastructure.
I think everyone is forgetting the majestic and wonderful team in Standard Chartered that started this whole Haskell thing :)
I'm sorry, but have you even read more than the first sentence of the post? I think OP is aware why it's unwise to `unsafePerformIO` in `STM`. &gt; Use a monad transformer, especially WriteT, to gather up all the info you need inside the transaction Wouldn't that repeat the writes if a transaction is retried? I mean, `WriterT w STM a` is `STM (w,a)`. Sure, designing an idempotent language to describe interactions with the database might be easier than forcing the database to accept idempotent writes, but it's still some work. &gt; Have the STM monad return an IO monad. I'm pretty sure that using [stm-io-hooks](https://hackage.haskell.org/package/stm-io-hooks) that OP mentioned would be better than a hand-rolled hack like this. The library takes care of exceptions, for one.
Pity about the degree. :)
Agreed. Some of the best hackers and developers I know didn't go to university, and it's an unfortunate consequence of our location that we can't consider them. :(
Alternatively, you can run one or the other in a VM. Nowadays it's pretty seamless.
&gt; How cast works is much easier to comprehend, I think it is a mistake to bake OOP-ish subtyping directly into the exceptions system. Especially because `fromException` doesn't behave as you (or I) might expect all the time: &gt; fromException (SomeException Deadlock) :: Maybe Deadlock Just &lt;&lt;deadlock&gt;&gt; &gt; fromException (SomeException $ SomeException Deadlock) :: Maybe Deadlock Nothing This cropped up in some polymorphic code I was writing that needed to convert from *any* exception type to a `SomeException`. But you can't just wrap all exceptions with `SomeException` and have it "do the right thing", you need to check if it's actually `SomeException` or not before you wrap it! Although, arguably, this is more of a flaw with the way `fromException` works than the OOPish subtyping.
I only know about this: https://github.com/ryantrinkle/reflex-nyhug, but it's basically "soon" ;) Maybe asking u/ryantrinkle would be good idea? // edit: Apparently the branch 'reflex-talk' contains some stuff.
What about this though: &gt; fromException (toException $ toException Deadlock) :: Maybe Deadlock Just &lt;&lt;deadlock&gt;&gt; It works just fine. I never use `SomeException` (the data constructor), only `toException`/`fromException`.
Ian Lynagh did most of the work to bring the hierarchical exceptions into the base package. I think the issues with changing the existing `IOError` type and others were around backwards compatibility. FWIW, we use hiearchical exceptions to good effect in the `haxl` package, where we have separate classes of exceptions for transient errors, logic errors, and internal errors (http://hackage.haskell.org/package/haxl-0.3.1.0/docs/Haxl-Core-Exception.html). We often want to distinguish between these classes but not the individual exceptions, so hierarchical exceptions are a nice win.
Way to go Ryan! I'd be so happy if I could be part of this. When you set up your branch in London I'll be the first one to submit my resume.
Yeah, that's what I take from the situation too. In other languages like Go, you can build packages of the standard library with different flags. For example, to build your library with the race condition detector enabled, the compiler will build the standard library packages that you're importing too with that setting. I don't know if GHC or Cabal have anything like this. If they don't, I guess the only way to do it would be for me to replace my standard GHC installation with one compiled from source with `-split-objs`. Although the bigger problem is that every user would need to do this to avoid bloated binaries.
Feel free to open issues with ideas for features and improvements. I may not know how to implement them just yet, but perhaps someone else does :)
Looks awesome but I have no idea what I'd use it for. Any suggestions?
Yeah, it was never my intention to imply that you cannot do what you want to do with the Serialize typeclass. I'm just saying that it really shouldn't exist. The typeclass abstraction doesn't really lend itself well to the task of serialization: context is more important than the actual type. Suppose you're developing a library containing some type. If a user of the library wants to serialize that type, it's up to him, not you, to decide how to serialize it. It would really be easier to just implement a function performing the serialization, instead of adding a Serialize instance. Even worse, if he wants to serialize the same type differently to different mediums (say a file and a network stream), it cannot be done directly with Serialize, unless you have a number of orphan instances (EDIT: or newtype wrappers...). The Storable typeclass actually makes a bit more sense (only ever so slightly, though). For a given platform, it's perfectly clear how a Word32 should be represented; it's decided by the C ABI.
I doubt that wasm compilation makes sense for most use cases. Wasm requires you to ship a lot more low-level code, e.g. garbage collector, unicode string operations. I believe it also requires marshaling if you want to call DOM apis. And of course it makes your building and debugging processes more complex. Also, javascript is already quite fast - haskell compiled to JS is not much slower than native haskell, IIRC. wasm only makes sense for a very narrow set of cases, like games. This is why all that anyone has written for asm.js is a few games, emulators, and so on.
&gt; This causes stack to use the ghc and build dependencies provided by the nix-shell. &gt; Last I checked, it only inherited the provided ghc and built everything else as it would without Nix available. Are you saying that you got it to actually use nix to fetch dependencies, thus utilizing the bincaches?
I'm a bit confused by this comment. It looks like `(Show a, Typeable a)` _are_ the superclasses of `Exception`, and there _is_ a `displayException :: e -&gt; String` function in the class (as of base-4.8).
Do you know what the dependencies are? Does the user of your program need to have GHC installed for hint to work?
&gt; 3 hours one time I'm not saying it can't be done, but I at least would have a hard time thinking of something that can be build in 3 hours and that I would like to be associated with. Out of curiosity what could such a project be?
I had a chance to look at `Wire` over the weekend, and I think that though there's some similarities (that implementation of `Applicative`, for one), `ObserverT` isn't a strict subset. You can rephrase `ObserverT`'s use to apply to `Wire`'s: caching :: (Monoid s, Monad m, Functor f) =&gt; StoreDict f b -&gt; (s -&gt; b) -&gt; Wire s e m b c -&gt; Wire s e m a c caching (StoreDict { .. }) query = stepOnMiss empty where stepOnMiss cache wire = fix $ \this -&gt; mkGen $ \s a -&gt; let b = query s in case lookup b cache of Just mc -&gt; return (mc, this) Nothing -&gt; do (mc, wire') &lt;- stepWire wire s (Right b) let cache' = insert b mc cache return (mc, stepOnMiss cache' wire') -- and this might be cheating a bit cachingBy :: (Monoid s, Monad m, Functor f) =&gt; StoreDict f b -&gt; (s -&gt; b) -&gt; Wire s e m (a,b) c -&gt; Wire s e m a c And the nice thing about that is that it gives you a high granularity of control over how and when the caching happens. However, since `Wire s e m` isn't an `ArrowApply`, it's strictly less powerful than a `Monad` like `ObserverT`, but not necessarily in any way that makes it less useful. 
Turtle uses the `show` function to get strings from the `Text` it uses internally, which results in all the output being quoted(and therefore not composable with other functions in a shell). You need to use `Text`'s own `putStr` variants instead. I have functions like: import qualified Data.Text as T import qualified Data.Text.IO as TIO viewText :: MonadIO io =&gt; Shell Text -&gt; io () viewText txt = sh (txt &gt;&gt;= liftIO . TIO.putStrLn) and for internal stuff: str :: Show a =&gt; a -&gt; Text str = trimQuotes . repr where trimQuotes = T.dropAround (=='\"') 
We have read and fully understood. DON'T. It is possible to solve your problem, but I 100% guarantee you that it will not involve doing IO inside STM. The whole point of STM is that it allows you to control completely where there is IO and where there is not. If you need to do IO, you need to arrange the structure of your code so that it will not occur inside STM. And that is always possible, if you have carefully formulated your requirements. One way to think of it is this: Your process is started in IO. If you need multiple IO threads, those are spawned in IO. Each thread has a well-defined IO role, and at startup it is passed an STM interface that it uses to find out what to do and when. So all the logic about the order in which things happen, and all message passing, happens via STM. Threads block when they are waiting for an answer from STM. But all IO happens in the threads themselves, when told to do so by STM. Perhaps something like this would work for your particular problem: * Thread 0 atomically sets up the machinery, receiving the various tokens each of the other threads will need, and then spawns the other threads. Then it atomically starts the machinery running, getting a message back when it is done. * Thread 1 receives an event, converts it to a command, atomically pushes it into a `TQueue`, and waits for the next event. * Thread 2 atomically gets a pair of a command from the `TQueue` and the user from its `TVar`. It attempts to persist it. It sends the user back to STM together with the result and/or status of the attempt. Then it loops. This is simplistic. For example, instead of a TQueue, you may want to build a transactional structure that allows you to get not just the front of the queue but the first command whose user is currently available. You may want another thread which receives failed attempts and does something in IO. You probably will want a way for each thread to hear from STM that we are all done and it is time to shut down. And of course, this scales easily by just spawning multiple copies of Threads 1 and 2. Etc. But I hope this gives the basic idea.
Interesting! But would be awesome to fix the root of the problem too - hoping that there's something to be done about it.
&gt; It is possible to solve your problem, but I 100% guarantee you that it will not involve doing IO inside STM. I haven't posted this thread, though I did run into a sonewhat similar problem once. (And mine *was* actually solved by using `unsafePerformIO` inside `STM`, as it only involved sending update signals to the GUI, which are idempotent. I ended up scrapping the code eventually and replacing it with [transient](https://hackage.haskell.org/package/transient)). Anyway the problem of synchronizing external storage with `STM` transactions is interesting (perhaps /u/whitehead1415 worded it wrong, as it's indeed usually not a good idea to try and use `IO` directly from `STM`, but there's no reason to jump on the exact words, as he did clarify it later in the post), and responses in the lines of "LOL NO YOU DON'T", responding to just the first sentence, aren't particularly helpful. Now that I look at the pseudocode: handleCmd :: Command User -&gt; Repo -&gt; PersistenceConn -&gt; IO (Either (Error User) (Event User)) handleCmd cmd repo conn = do let email = getEmailFromCommand cmd userMVar &lt;- getUserFromRepo repo user &lt;- takeMVar userMVar let (Right evt) = execute user cmd (Right Success) &lt;- persistEvent persistenceConn evt putMVar userMVar (apply user evt) ... I think, maybe it would be possible to make `apply user evt` commutative? As in `apply (apply user evt1) evt2 === apply (apply user evt2) evt1`. If not, then I guess the approach with `MVar` looks good, and there would be no need to do unsavory things to `STM` anyway - I think it's enough just to read `Repo` with `readTVarIO`, the `MVar` guarantees that nothing can mess with the user variable in the meantime, and if you don't use any more `MVar`s you can't cause deadlocks (Well, "can't" modulo exceptions. It might be a good idea to put the release of the `MVar` in `bracket`).
Those parts are good, we do need some dynamic mechanism for exception throwing. The parts that are more complexity than they're worth (IMHO), is having `toException` / `fromException` in addition to `cast`. The extensibility might be worth it if it was more flexible. Right now I think all you can do is have your exception opt-in to a specific chain of superclasses. However, you cannot create a new exception which will be cast to some existing exception. For example, if you want to annotate exceptions with stacktraces, there is no way to allow the stacktrace to be elided and catch the exception as its original type. I think that you also can't really have multiple superclasses, other than the ones that are intended to work together. This means that if your exception fits into two of these "sets of exceptions", you can't always do it. Even when you can, the boilerplate to do it is unpleasant and inefficient.
I believe that's the reason you can't have it. If you try a straightforward implementation of finger trees based on a Haskell implementation, for instance, you'll discover that the compiler attempts to monomorphize the recursive calls all the way down. But it can't go ALL the way down and errors out at some point. (Or that's what happened when I tried it.) e.g. http://is.gd/fXfvvh
The place I applied to to had a specific project they asked applicants to make if they didn't have a strong open source portfolio. They wanted a simple web application that pulls data from a database, saves files to the filesystem, and adds new data to the database via JSON request. I was already familiar with 2/3 of those tasks at the time, so it was a relatively quick project.
&gt; However, the strawman assumption in OP - "...if you have a problem where you really need to do IO..." implying that there are problems that cannot be solved using STM the way it is designed to be used - is wrong. It's not a strawman that it's necessary to use IO to talk to a database. It's actually STM that is rather useless in OP's example (edit: or rather, it is very useful, but only for the `TVar` holding the `Repo`, especially if it's meant to be a part of the public interface). &gt; For this case, I sketched out a way to structure it as an STM application that is simple to write, maintain, and scale. Why would you want to start messing with the complexities of MVars and unsafePerformIO here? Using a single `MVar` that is always released in the same function where it's taken is way simpler than your `TQueue` solution. (I didn't actually suggest using `unsafePerformIO`to OP, I just went off on a tangent there. Sorry for the confusion). What do you want to scale there? &gt; You might want to go to a lower-level implementation using explicit MVars instead of STM as part of some kind of optimization. An `MVar` isn't a lower-lever alternative to `STM`. It's a distinct tool that solves different problems. In OP's example, a lock is exactly what makes it possible to ensure that nothing happens to the user variable before it's written to the database and updated based on the write's success. Shoehorning STM to fit this kind of a problem, and ending up inverting the control flow with queues or channels, isn't simplyfing anything. The advantages of STM over locks (like composability) don't buy anything when the `MVar` never leaves the function (It could be accessed from `Repo` separately, and misused that way, but that's what abstract types and modules are for. Wrap the `MVar` in `Repo` in an opaque type, make the function the only way to access it, and the lock becomes an implementation detail that doesn't matter to the callers in the slightest).
As things stand, it would not make sense for Haste to compile to ASM.js or WebAssembly (which is currently more or less equivalent I'm expressivity), and it seems that Luite has come to similar conclusions regarding GHCJS. While it would likely be possible to get a fairly nice speedup, the amount of code you need to ship is a *lot* higher than with normal JS, and for most web applications that's simply not a tradeoff you want to make. It might be possible to get a better tradeoff by only going half the way. JS is indeed pretty fast by default if you avoid certain high level features (in fact, I've been consistently unable to get any performance benefits from ASM.js over low level JS, even on Firefox), but I'm not sure just how much functionality you'd need to throw out the window to get any benefits. I did have a master's student looking into such a backend for Haste but he seems to have gone AWOL, so if you're studying computer science in Gothenburg and need a master's project... EDIT: typos.
Could you please give an example of approach 2 ?
Nice, thank you
Do you expect that porting/compiling GHC's garbage collector to WebAssembly is going to be faster than running Javascript's own GC?
Calibre gui hangs if I drag &amp; drop for conversion.
UPC represent ;) nice to see some known faces around here. I'll take a look afterwards as I'm leaving for the airport now. Leaving this comment so I remember
Fixed, thanks!
The pdf seems to be generated using latex and [this tool](https://de.wikibooks.org/wiki/Benutzer:Dirk_Huenniger/wb2pdf). You can maybe piggyback on it to get the latex file and generate an epub instead? Another source of inspiration could be [the tool used to export wikisource books to epub / pdf / etc.](https://tools.wmflabs.org/wsexport/tool/book.php).
Quite possibly. Impedence mismatches are a real thing. Consider the points made in [this discussion about Golang GC and how they couldn't just pick up what Java did because of language differences](https://youtu.be/aiv1JOfMjm0?t=8m51s). And Go and Java are next-door neighbors on the language landscape; consider how much more different JS and Haskell are. And possibly it won't be faster. These things are fiddly. But it's certainly not obviously impossible for porting a real Haskell GC to blow away trying to lean on the JS one. Plus I want to just re-emphasize that JS isn't really that fast and it isn't necessarily that hard to beat it's performance. [V8 is not even in the top tier of performers on the shootout](http://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html). There was a lot of ink spilled on all the work that sped up JS over the years, but it's important to remember that if you take a language that was running ~100x times slower than C (or worse) and speed it up by the amazing factor of 10 (no sarcasm, the work done to do that acceleration was impressive), you still end up with a slow language in the end. (See also the Hack language in the same shootout. Speeding up PHP "a lot" does not mean you've produced a fast language.) If you don't start with the rather staggering pile of performance anti-optimizations that the 1990s-style dynamic scripting languages do, it's not that hard to beat them. I mean, look at Go again... a deliberately simple language that has still really not had _that_ much effort poured into it, a focus on compilation speed that means a lot of the clever optimizations aren't performed, garbage collected, nontrivial indirections through interfaces being done repeatedly at runtime, a lot of things that have the Rust/C++ crowd screaming about how terribly slow it is and how inappropriate it is for systems programming... and it still casually blows Javascript away.
&gt; Ah, what's so bad with OCaml? Nothing, per se. There's just a lot of little annoyances for me, a dyed-in-the-wool Haskell programmer. &gt; I mean, it must be much better than grading a Java assignment, no? No. With Java, I'm constantly aware I'm grading boilerplate-full mutation-heavy OO code; with OCaml, it's just similar enough to Haskell to leave me pining constantly, getting my hopes up and then dashing them. Annoyances I've run into during this grading session, in no particular order: * No `Show`: There's no easy way to generically show a value; this led me to a panoply of functions for showing e.g. `(int * int) list`, `(int * char) list list` (`[(Int, Int)]` and `[[(Int, Char)]]` in Haskell notation) for printing out incorrect results. * Testing: I don't know OUnit, and my colleagues' experience with it suggests that it's less usable than Haskell's testing frameworks. * Strictness (in trivial ways): If I want my students to write list with the first 50 Fibonacci numbers in Haskell, I might give them the following snippet: fib = undefined If they don't replace the `undefined` with their answer, minimal harm done. The equivalent code in OCaml, however, causes the entire module not to run. * Type annotations: If I want students to write a `map` function, I might give the following snippet: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs = undefined In Ocaml, the equivalent, writing type annotations in the standard way would look like this: let map (f : 'a -&gt; 'b) (xs : 'a list) : 'b list = failwith "not implemented";; Unfortunately, this type annotation is basically useless, in that it admits this implementation: let map (f : 'a -&gt; 'b) (xs : 'a list) : 'b list = [[]];; The solution is the following (which I find awkward): let map : 'a 'b. ('a -&gt; 'b) -&gt; 'a list -&gt; 'b list = fun f xs -&gt; failwith "not implemented";; (taken from [here](https://blogs.janestreet.com/ensuring-that-a-function-is-polymorphic-in-ocaml-3-12/)) Note that if you omit the explicit quantification before the period (as I did once), you get the same problem. * Constructors: They are not first-class, and their arguments must be uncurried.
I admit that I probably worded the question poorly. I didn't mean to imply that I was actually trying to perform IO in STM. I worded the question that way partly because I didn't know exactly what I should be asking, and also because I thought it would be helpful to anyone else new to concurrency in haskell. The reasoning is that someone that is new to concurrency in haskell would likely search for the terms IO inside STM even if they also knew it was a bad Idea. When doing things in STM there would be many times when someone says hey I need to do this IO action atomically with the STM action, like interact with a database, but where do you find the answer? I think that [Real World Haskel](http://book.realworldhaskell.org/read/software-transactional-memory.html), [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/index.html), and [stack overflow](http://stackoverflow.com/search?q=stm+io) are really great resources, and they made it pretty clear that you don't do IO inside of STM, but they still leave some questions unanswered. So the answer from /u/bhurt42 doesn't really help me because it doesn't address my specific problem, but I think it may be helpful to someone else. /u/yits thanks for contributing your solution. That gave me a new way I could tackle the problem. It reminds a bit of how I would use the actor model to solve the problem, which is probably because the nature of the problem just fits really well with actors because somethings just need to be synchronous. /u/mjmrotek thanks for your input on the `MVar` solution and suggesting using `bracket` that was very helpful. 
Sorry I agree I wasn't very clear that those 3 stages needed to happen atomically because the order of the events was important. If the events could commute then you are right there would be no reason to do IO inside STM and I could perform the IO at anytime
No problem!
Feature creep isn't necessarily over-engineering. If all those features held together by a simple set of mathematically sound theories, would it still be over-engineering?
Thank you for the amazingly detailed write-up. Can we **please** remove lazy IO from Prelude? Or at *least* rename functions that use it to include the word "DANGER"?
thanks for that - sad that you come to late to help out those poor FilmDB students on StackOverflow ^^
I mean, my comments are specifically about what they've actually implemented, which is a binary format for asm.js. Web applications are much slower than they could be, but numerical computation speed is far from the biggest problem in most cases. If WebAssembly grows into a real low-level VM, with a fast UI layer and a decent threading system and so on, then that would be a different story.
Complete removal seems like it could cause a lot of breakage. I do think that the haddocks should be improved though. Right now, the docs for `readFile` tell you that it's read lazily, but to someone new, that's not nearly enough information. Maybe adding a strict version of `readFile` to `base` would be good.
Good thinking. Thanks for explaining that!
AMP caused breakage and that was only to fix something that was arguably cosmetic and inconvenient. Lazy IO is an abomination and it needs to go. `pipes` is good if you really need lazy IO.
Hey would you be able to export unsafeInterpret? It's really handy to have sometimes.
Yes! Adding strict versions would be much better, and then having the docs say "You very likely shouldn't use this, but use the strict one instead"
No. I'm the biggest advocate in the world of "practical Haskell", but that's a giant baby to throw out for a very small amount of bathwater, and it wouldn't solve the underlying problem anyway. The underlying problem is that laziness makes the operational semantics of Haskell all but impossible for most mortals to reason about. The runtime, memory usage, and (yes) IO behavior of Haskell are almost always surprising even to someone like myself who's been writing Haskell for a long time. This is usually a *good* surprise (except memory usage): laziness and a good compiler tends to make programs run faster than you'd expect and use less memory for IO. But when it's a bad surprise, it's nearly impossible to figure out. The lazy behavior of IO is normally a huge feature: it means that I can process giant files with little memory in most cases. If I do the sane-people thing and write a different file than I'm reading from, I'll probably never notice that this magic is happening except through seeing better-than-expected performance (like "not crashing"). Complaining that I get surprising results from the posted code is like complaining that this shell script gives surprising results $ tr a b &lt;foo.txt | tr c d &gt; foo.txt All the same behaviors OP saw will happen here, but nobody's proposing we "make pipes strict" to make this kind of example work because the performance implications would be devastating. A change I probably would support phasing in is to forbid Lazy IO from having a file open for reading and writing at the same time. This would catch most of the common misuse cases and could raise a comprehensible and specific error message. Don't have time to write more right now. Will probably blog and link here later with something better thought out and more detailed.
It isn't a big deal. I use upx to compress a fully static pandoc from 50 megs to about 6 megs. The difference in running pandoc -v is 0.001 seconds versus 0.01 seconds. Yeah it adds up if you need to run things a lot, but for most things like commandline tools it isn't a big deal.
There is a warning in doc &gt; Warning: the readFile operation holds a semi-closed handle on the file until the entire contents of the file have been consumed. It follows that an attempt to write to a file (using writeFile, for example) that was earlier opened by readFile will usually result in failure with isAlreadyInUseError. Maybe you should file a bug against documentation.
If there were a commonly used pure "list with a special value at the end" type (something equivalent to **Producer a Identity r** or **Stream (Of a) Identity r**) I believe it could be used to handle lazy I/O inside a callback, like this: withFileContents :: FilePath -&gt; (forall r. List a r -&gt; (x,r)) -&gt; IO x This would ensure that the file is exhausted and closed before returning from the callback. But perhaps at this point it's better to bite the bullet and use a streaming library...
Helped a coworker with a fun one earlier this week, basically the opposite end of the spectrum: he was trying to do a proof-of-concept involving a Node subprocess with a `CreatePipe`'d stdout. It kept crashing with an EPIPE error. The problem was, the variable he was binding the pipe's handle to wasn't actually used yet because prototype, so it was getting deallocated (closing the pipe). The fix was to explicitly `hClose` it after everything else was done, forcing it to exist until that point.
Yeah, this was a subject of [a recent SO question](http://stackoverflow.com/q/35840685). At least I have a poem at hand for the next time I have to explain this to someone: &gt; Whoopsy daisy, being lazy &gt; tends to make file changes crazy. &gt; File's not closed, as supposed &gt; thus the error gets imposed. &gt; This small guile, by [read]File &gt; is what you must reconcile.
Assuming you are doing web development, any simple web app is fine. Just something that shows you can actually make a Haskell app, use a database, etc.
Yes that looks awesome! Especially his proposal for the new STM data structure. Also in his [thesis](https://github.com/mcschroeder/thesis/blob/master/thesis.pdf) gives a good run down of [stm-io-hooks](https://hackage.haskell.org/package/stm-io-hooks) and [twilight-stm](https://hackage.haskell.org/package/twilight-stm-1.2/docs/Control-Concurrent-STM-Twilight.html)
I have to say that I'm 100% with you on this. Reasoning: I've worked in Scala professionally for the past 3-4 years or so and type classes simply don't work in Scala in any meaningful sense. I mean, they're technically possible, but there are *soooo* many caveats, most important of which are: * You *must* import *exactly* the same instances everywhere that you expect compatible instances[1]... and the compiler can offer no help in verifying that instances are actually the same/compatible. * The implicit resolution rules are *insane*. I mean, they're *technically* simple, but insane if you consider human psychology and the fact that us humans don't have a compiler's view of the code. In short: Type classes in Scala are a complete disaster and are frankly unusable. The only hope for this (or a similar) approach that I could imagine making sense would be the Idris way where you can exploit dependent types to explicitly state that instances must be compatible (nominally, individual instances are named). However, I suspect it may still result in the "too much explicit annotation required" scenario that is the bane of very abstract/high-level code. [1] For extra fun try doing serialization/deserialization (with customization) in e.g. upickle. Even ignoring the compiler errors it's a nightmare to ensure that all your instances line up and you really have no way of knowing if you've done it right unless you actually try to round-trip data using e.g. ScalaCheck.
&gt; The underlying problem is that laziness makes the operational semantics of Haskell all but impossible for most mortals to reason about. Hyperbole isn't constructive. Anyone who has met me understands that if i can do it, anyone can. What is true? That it requires experience and knowledge that isn't easy to come by. That can be improved.
Megaparsec also supports indent sensitive parsing, maybe give it a try. https://mrkkrp.github.io/megaparsec/tutorials/indentation-sensitive-parsing.html
Yes. Untyped lambda calculus let's you write fix, so you can get infinite recursion. 
&gt; exploit dependent types to explicitly state that instances must be compatible This is actually rather ridiculously difficult to do once your class hierarchy starts to grow into any sort of an interesting lattice, except for the limited 'property-like-structure' cases, like there is one and only one way to turn a monoid into a group, since saying it is a group only gives you the power to find the inverses and inverses are unique. This is 'property-like structure'. You can forget it, but if someone reminds you later, they must give you the same thing you knew before, and you can prove that fact. Therefore it is safe to forget such things. However, there are many monoids that can share the same carrier set. If you forget the monoid for a given set and someone hands you another monoid on that same set, you have no idea if the two are the same.
FWIW, the entire backend + infrastructure of Wire is written in Haskell.
How do you know?
It's not hyperbole. I have a Thesis M.Sc. in programming language implementation from a respectable school, where I received the University's highest graduate award for my work. (I went on to get a Ph.D. in AI and become a university professor.) I've been working with pure functional languages for 30 years, ever since a friend and I built one in the 1980s and wrote a bunch of code with it. I am quite familiar with several functional programming languages. I have been programming in Haskell for a decade. I have substantial code in the Haskell Standard Libraries, and a few packages on Hackage that are reasonably widely used. I probably write a few hundred lines of Haskell a month for various purposes: my most recent project is about 300 lines of statistical analysis and display. About every fifth program, I run into a bad memory or time problem that I didn't expect and have a hard time repairing. Almost always, this turns out to be due to laziness. I have a half-dozen Haskell programs lying around that I can't make run efficiently and couldn't tell you why. I'm sure *somebody* could, including a few of my friends, but I hate to ask the guy in the office next door who co-invented typeclasses every time I get stuck: besides, he's working on a strict functional language now because he decided that laziness won't work for operating systems because time and memory. The guy in the next office after that wrote a really nice paper some years ago about trying to do state space search in Haskell and what a train wreck it was due to laziness: he's also a distinguished professor who specializes in functional languages. So...yeah. If you aren't having problems at this point, you're either more exceptional than you think or you haven't run into the real problems yet. Managing the IO system isn't it, I promise you.
I was one of the original members of the backend team at Wire. We initially prototyped in Python before everyone agreed to switch to Haskell, to shut me up.
With all the play around `seq`, all I thought was: "Why didn't he use [deepseq](https://hackage.haskell.org/package/deepseq-1.4.1.2/docs/Control-DeepSeq.html)?"
Hi r/haskell! Here's a little hobby project I've been working on lately and which I'm quite proud of. In the linked writeup I've discussed the creation process and some implementational choices I made. The code can be found [here](https://github.com/flannelhead/blackstar). I'd love to get some feedback (and possibly some optimization tips!) If you're interested in the physics side of the simulation, I've also written a little [article](https://flannelhead.github.io/posts/2016-03-06-photons-and-black-holes.html) on Schwarzschild geodesics, which should be approachable even if you have no background in general relativity.
This is a followup post to my earlier query here: https://redd.it/49ydqx. Make sure that you save the file locally, if you have a use for it. `archive.org` may (or may not) keep the file around in their servers. ----------- 1. Goto https://en.wikibooks.org/w/index.php?title=Haskell/Print_version&amp;action=purge&amp;printable=yes 2. `File`-&gt;`Save as`-&gt; `Web page, complete` 3. Create a `transform.xsl` with following contents &lt;xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt; &lt;xsl:template match="@*|node()"&gt; &lt;xsl:copy&gt; &lt;xsl:apply-templates select="@*|node()"/&gt; &lt;/xsl:copy&gt; &lt;/xsl:template&gt; &lt;xsl:template match="head/style|head/script|head/link"/&gt; &lt;xsl:template match="*[contains(@class, 'noprint')]"/&gt; &lt;xsl:template match="id('mw-navigation')"/&gt; &lt;/xsl:stylesheet&gt; &gt; The above snippet does the following &gt; &gt; 1. Removes all applied css-related info from the HTML file header. Both `calibre` and `kindlegen` were unhappy when large amounts of text get `display; none` css property. &gt; 2. Remove all `noprint` elements. &gt; 3. Remove `navigation` links from the footer 4. Run `xsltproc` with the above `transform.xsl` on the file saved in step (2) xsltproc --html --output Haskell.html transform.xsl Haskell_Print\ version\ -\ Wikibooks,\ open\ books\ for\ an\ open\ world.html 5. Use `ebook-convert` (calibre) to convert the `HTML` file to `EPUB` ebook-convert Haskell.html Haskell.epub 6. Add `Haskell.epub` to calibre. Delete the inline TOC pages (which are external links) and re-generate TOC manually. Optionally, remove the default `.css` file generated by calibre. (Without this I find that content that are rendered within a table -- for example, `Exercises` etc. overflow my paperwhite screen) 7. Optionally use `kindlegen` to convert the file to `mobi` format. I hope the EPUB file is usable as it stands.
&gt; https://commons.wikimedia.org/wiki/File:Haskell_eBook_Reader.pdf I don't want a PDF file. With the EPUB file the text can reflow nicely when I increase the font size. (I wear spectacles, so the kindle always has a bigger than normal font sizes) 
I'm sure he would have had he known about it. But beginners won't know about it. Lazy IO bites the very people who do not know the tools to deal with it. 
Great stuff! I noticed that some of your numbers have a fair amount of variation. In general performance comparisons on an multi tenant AWS machine is nigh impossible, because the available CPU power isn't constant. Something that might help get finer granularity results is running the whole thing on a dedicated instance. At greater cost of course...
The variation about 0.3% of total on the large error bar. Not sure whether that is too high but it'd be good to reduce it. An obvious way would be to use the law of large numbers and build more often. I'd be interested in a dedicated machine but more because EC2 doesn't expose the CPU hardware counters. Those allow counting the number of instructions which should be fairly accurate. There are additional complications in that GHC isn't deterministic, and the underlying OS can e.g. return EINT introducing further non-determinism so there will always be some uncertainty. **Edit:** Apologies, didn't really address the multi-tenant comment. I believe that task-counter only counts the time when the CPU is actually active, i.e. it *should* correctly deal with e.g. XEN suspending us for 20 seconds by not counting those 20 seconds.
Thank you for chiming in! This certainly sounds like an interesting and a very useful approach. Would you happen to have any example of this available? PS. Your `linear` package is also awesome.
Reading whole file into memory means going from constant space to linear space for sequential reading. You can't do that in general.
This is not the same `bimap` as the one in [bifunctors](https://hackage.haskell.org/package/bifunctors-5/docs/Data-Bifunctor.html#v:bimap).
Ah, didn't notice your graphs weren't zero based. Silly me. That said, what I've seen before is perf differences that can sometimes amount to +/-5% across runs at different times on different instances of the same type. It's that variability which makes tracking trends somewhat iffy. Even if it's CPU time we're measuring (not wall clock time), activity from other tenants can slow things down, like network traffic causing frequent interrupts and cache effects. Since this variability is time based, hard to average it out with many runs in quick succession...
If you need some (potentially old) data on dedis: http://serverbear.com
"Hyperbole isn't constructive. Anyone who has met me understands that if i can do it, anyone can." Just realized you wrote two sentences and the second one perfectly contradicts the first. Was that on purpose? If so, well played. Very well played.
It took me quite a bit of reading to realise this wasn't anything to do with values that try to evaluate themselves.
In Axolotol, users sign their ephemeral pre-keys with ed25519 conversion of the users long term master key. This was introduced in v3 of the protocol I think.... 
Their Axolotol implementation is in Rust. We do need more BSD licsenced Axolotol implementations though... https://github.com/wireapp/proteus So the cryptobox stuff is for something else, maybe audio/video crypto? 
Unfortunately, I don't have an examples that I am able to open source at this time. That may change over the next few months. Alternately, maybe I should just write an article series showing how it is done, when I get downtime.
You're *technically* right (the best kind!), but it absolutely does not matter in practice these days... as I believe I elaborated on in my comment.
Ah, I see we have coinciding strategies. I'm so confident in Haskell (after having used GHC for ~5-10 years at least) that I'll just keep talking/nagging until I wear "them" down. Glad to hear that it works... eventually!
Contents aside, I can't stand that the text is monospace but the code is not.
That's a cute trick, but kind of convoluted. You don't get the benefits of lazy I/O (this `List a r` is not as convenient to work with as `String`), and evaluating more of the structure to cause more `unsafePerformIO` to happen underneath you is not much different than just streaming over an underlying `IO` rather than `Identity`.
I haven't tried the code but I find your theoretical writeup very well-written, informative and enjoyable. I love to read a full story connecting general theory and ready-to-use application, written with novices in mind, so I thank you for this. 
Thank you! I'm very glad to hear this. This was exactly the aim of the article.
Is that a mobile issue? I'm not experiencing that problem myself...
Looks like it was, I get everything in monospace on FireFox desktop.
If you have an example application, I can code it up in my GPU EDSL in case I beat you to release ;)
&gt; And that is always possible, if you have carefully formulated your requirements. Sometimes this means generating an `STM (a, IO b)`, or similar and performing the second part of the result after the transaction is complete (e.g.). The similar, simpler `STM (IO ())` is one of those instances where `join` makes more sense than `&gt;&gt;=` as the monad operation.
Whilst I don't necessary disagree with your point in general. There are TONS of cases where unsafe should not "bubble up", for example using unsafePerformIO to run code that you know does not actual perform any real mutation should not be called unsafeDoSomethingSafeAndPracticalMutationFree.
One easy way to get AD is as follows. Make a new number type that stores both its value and its derivative at a certain point. For example data AD a = AD {value :: !a, derivative :: AD a} --By making the derivative `AD a`, we can compute higher order derivatives. SInce Haskell is lazy, it will only do so if we ask for them. Now make `AD a` an instance of a bunch of number type classes. For example (AD x x') + (AD y y') = AD (x+y) (x'+y') xad@(AD x x') * yad@(AD y y') = AD (x*y) (xad * y' + x' * yad) exp (AD x x') = (let expX = AD (exp x) (x' * expX) in expX) -- We could have also made this a recursive function, but by making it a recursive value, we don't duplicate what we don't need to (similar to putting fibonacci numbers in a list instead of making it a recursive function) This can be pretty fast if you use https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/pragmas.html#idp46686525907184
But a maybe type allows the caller to decide what level of safety is appropriate for them. I can easily use a uni-pattern or fromJust, but I can't do this if the library author has decided for me.
This only applies to the single-variable case though .. 
[removed]
Can you give a specific example of where this technique goes wrong?
True. But I/O is more expensive than CPU, and you need a lot more I/O just to load the bigger executable into memory. The difference is less for SSD users, but still there. On balance, for now upx still seems to be worthwhile pretty much universally for distributing GHC-generated executables to non-moblie devices. We'll see in a couple of years.
Interesting article. Thank you.
&gt; An MVar isn't a lower-lever alternative to STM. I disagree with this view. Composability means that STM is inherently a higher level view. I have never seen a non-contrived real-life problem whose solution is not conceptually simpler in STM than using locks. &gt; Shoehorning STM to fit this kind of a problem... To me, that is like saying "Shoehorning concurrency to fit this kind of problem...". This *is* a concurrency-related problem, and STM is the default way to do concurrency in Haskell. It is an approach to concurrency in general, a way of thinking about it at a higher level of abstraction, not just a "technique". &gt; The advantages of STM over locks (like composability) don't buy anything when the MVar never leaves the function... If you mean in performance, that is true. Like any low-level optimization, the question becomes whether using explicit locking is really required for this application. Premature optimizations should be avoided, but if this is your bottleneck, it could be worth it. If you mean in simplicity, I beg to differ. Even if no additional lines of code are needed, the additional burden of proof that "the MVar never leaves the function", and of having to worry about where the MVar is allowed to go at all, adds complexity. In STM, the semantics of STM itself provides those kinds of guarantees.
I meant things that evaluate themselves directly and make execution fail with `&lt;&lt;loop&gt;&gt;`, like `let x = x in x`. The internal mechanism in GHC-compiled code that handles this is called a black hole.
I've written a small [game](https://hackage.haskell.org/package/layers-game) in Haskell. I've used [GLFW-b](https://hackage.haskell.org/package/GLFW-b) for it and was quite happy. One of the big questions is how to organize the data, how to handling the game loop. What worked quite well for me was using a State monad which contains an IORef to the game data. The IORef allows the sharing of the game data between the game loop and the event callbacks of glfw. I also have a C++ background and game programming can become quite nasty and sometimes Haskell doesn't feel like the best tool for this kind of work. I'm also quite interested in the programming language Rust and for the particular domain of game programming it IMHO feels like the best combination of C++ and Haskell. Even if Rust is such a young language it already has a larger game programming community than Haskell most likely will ever have. 
Thanks for looking at this! Indeed we do need better tools for tracking GHC performance. That being said, I'm not entirely sure that the problem is a lack of measurements; Nomeata's [perf.haskell.org](http://perf.haskell.org/) is quite comprehensive and the availability of the raw [build logs](https://github.com/nomeata/ghc-speed-logs) means we are quite unconstrained in the sorts of analyses that we can apply. However, despite this quite massive dataset there are still plenty of problems that need solving, * our performance tests cover a rather limited subset of the compiler's surface area; testing libraries as you've done is a great way to solve this if done carefully * it's still too hard to visualize and identify long-term performance trends * once you have found a performance regression it is unnecessarily difficult to determine which part of the compiler is responsible. * runtimes can be quite difficult to reliably measure; allocations are more reliable but currently `perf.haskell.org` doesn't report these for nofib compilations Since the initial "soul crushing" thread Austin and I have tried to throw some of our time towards the problem, * Austin is investigating options for expanding our performance test coverage; measuring common Haskell libraries is one approach he's pursuing * I've been trying to improve compiler diagnostics output, making it easier to characterize the performance of individual passes (see [D1959](https://phabricator.haskell.org/D1959)). * We've been trying to scrape additional test statistics from nomeata's invaluable GHC perf logs and develop tools for visualising trends in these statistics Of course, Austin and I are quite constrained in the number of hours we can put in to this endeavor. If you have time to spare, we'd love to discuss ways in which you can help move this story along.
I'd argue that last point has much more to do with the kind of people that are attracted to Rust than it has with the relative merits the language has regarding game development. Though I know many people say that a garbage collector is a show stopper for game dev. 
That's overstating it. While it's true some algorithms are easier to write with a loop, I don't see why that would be the case more often with game programming than it is with other domains. As for mutation, I think even Carmack said that the size of the game state every frame is small enough that the overhead of using immutable structures doesn't matter too much (I'm paraphrasing). Besides, games were always made in languages that support mutation so pretty much all the knowledge about game making is based around it. That's also one of the big reasons why game making in Rust is more popular. It's familiar. It supports everything that's familiar to C(++) developers.
That's very useful, thanks! I won't continue with this experiment. What's an area where it'd be useful for me to get involved without needing too much of your time? As you already spent significant time thinking about this issue: What's your take on adding tracing like lttng?
Offtopic: MongoDB? Seriously? Typesafe so that nothing goes wrong and then playing jeopardy with your data!
Check out the /r/haskellgamedev subreddit, it's full of like-minded people :) &gt; I have concluded that I need a real project, something concrete that I can hack on for a longer time in order for me to actually learn Haskell. May I recommend starting with a smaller game? Haskell programs tend to have a very different overall architecture than imperative programs, and so it's best to try several different approaches in several small projects in order to see what works. If you start with a larger project, chances are you'll pick an architecture which is closer to the imperative architectures you're familiar with, and it might even work, but it won't allow you to learn the parts which make Haskell unique. My approach is to make tiny Haskell games ([LD#31](https://www.reddit.com/r/haskellgamedev/comments/2od4q2/its_ludum_dare_lets_show_them_what_haskell_can_do/), [LD#34](https://www.reddit.com/r/haskellgamedev/comments/3x1jqr/growing_up_a_short_browser_game_written_in/)) in 48h on each [Ludum Dare](http://ludumdare.com). &gt; what are the recommended media libraries in the Haskell ecosystem? For a first game in Haskell, I recommend the [gloss](http://gloss.ouroborus.net/) library. It's much more limited than what you're asking for, as it can only draw shapes and sprites and respond to keyboard and mouse events, but it gets you off the ground quickly, demonstrating that writing games in Haskell is possible, and that it's possible to describe the gameplay using only pure functions.
Making immutable data structures efficient - like in the Haskell way - you will most likely always have a higher amount of memory indirection, to be able to reuse the non changed part and only switch pointers of the changed parts. These memory indirections can hurt you quite a bit in performance demanding fields. There's a reason why data orientated programming is a big topic in game programming, so having more control over the memory layout of the data is a win. I love the semantic simplicity of Haskell, but certainly don't like the runtime complexities of its memory usage and sometimes uncertain optimizations. I'm just a bit uneasy if I have to look at Core to see if certain optimizations kicked in and to hope that a new compiler version doesn't change the behavouir again. It's just a complex field and even if you don't understand other peoples choices it doesn't mean they're unreasonable. 
This is a great talk on the topic: http://www.teamliquid.net/blogs/427090-john-carmack-programming-haskell
He's saying that you're using a type safe language so nothing goes wrong but then use an unsafe database for your data. 
I'm not familiar enough with multiple variable calculus to know how it would work in that. (This approach does allow multiple variables, its just that you can only take regular derivatives. (The derivative of `j` with respect to `k` is `derivative j / derivative k`.)
One interesting thing this allows is solving differential equations (sort of) in an interesting way: thing = (AD thingv thing') where thingv = 42 thing' = thing ^ 2
I think the other comments have covered the basics. Some more advanced topics and example games you might be interested in (take it with a grain of salt as I am no expert): * FRP (functional reactive programming, a way of dealing with callback hell) - http://zyghost.com/series/odin/part-one/ * netwire (FRP) - https://wiki.haskell.org/Netwire * Arrows - https://en.wikibooks.org/wiki/Haskell/Understanding_arrows * Machines, Arrows, Autos - http://blog.jle.im/entry/intro-to-machines-arrows-part-1-stream-and/ Some links: * GLFW-b - high-level bindings to GLFW - https://hackage.haskell.org/package/GLFW-b * luminance API (OpenGL backend) - https://github.com/phaazon/luminance * Tetris game made using netwire (FRP) - https://scrambledeggsontoast.github.io/2014/09/23/tetris-netwire/ * Code deconstructed series - Cuboid game - https://www.youtube.com/watch?v=-IpE0CyHK7Q * A sort of tutorial based on Cuboid - http://www.hgamer3d.org/# * Vulkan bindings - https://github.com/expipiplus1/vulkan * netwire + GLFW - https://hackage.haskell.org/package/netwire-input-glfw * netwire + SDL - https://ocharles.org.uk/blog/posts/2013-08-01-getting-started-with-netwire-and-sdl.html * netwire quickstart tutorial - https://hackage.haskell.org/package/netwire-4.0.7/docs/Control-Wire.html * netwire tutorial with OpenGL - http://todayincode.tumblr.com/post/96914679355/almost-a-netwire-5-tutorial * about netwire - http://hub.darcs.net/ertes/netwire/browse/README.md * another look at netwire - http://phaazon.blogspot.co.uk/2015/03/getting-into-netwire.html
&gt; That's very useful, thanks! I won't continue with this experiment. I'm glad I could be of help! &gt; What's an area where it'd be useful for me to get involved without needing too much of your time? As far as I can see, GHC's performance improvement effort has two fronts, 1. Improving our ability to measure and catch future regressions 2. Tracking down existing performance issues At the moment we lack hands on both fronts, however (2) does require a larger time commitment (and perhaps a bit more Haskell experience) than (1). Tasks in category (1) include, * adding performance tests to our testsuite and `nofib`. A good testcase should satisfy a few criteria: * have minimal dependencies outside of GHC's boot libraries (ideally none) * be reproducible: the test itself needs to be largely static yet be easily compiled with a wide range of GHC versions; testing the performance of Hackage packages like `aeson` is useful, but you need to be careful to compile against the same version (and dependencies) to ensure comparable results. * be relatively small: it can be rather difficult to identify compiler performance issues when working with a large testcase * require little in the way of maintenance &gt; As you already spent significant time thinking about this issue: What's your take on adding tracing like lttng? I'm not terribly familiar with lttng. That being said, we already do have a fair amount of tracing infrastructure (and working on adding more). The question is how we want to leverage it to assess compiler performance. What did you have in mind here?
[removed]
A more pedestrian approach would be to have a monadic type keep track of execution and take care of replaying/skipping actions it already performed on resume. If you don't have to keep track of non-deterministic state (at least your examples look like you don't) this would be very straightforward.
Here's how I would write it, using the hmatrix library: http://lpaste.net/154619 Hope this helps! 
Yep, that's always an option (thanks for the suggestion!), but wouldn't I lose the ability to encode my logic in Haskell in favor of an (E)DSL? I'd be writing things like: Begin $ If (Var "foo") a b Which is acceptable, but not quite ideal (it would be nicer to write `if foo then a else b`).
Workflow looks interesting! I'll definitely look into it. packman unfortunately requires you use the same executable, which is a massive drawback.
GADTs are just syntactic sugar for type equality constraints + existential type variables. It looks like this: data Foo :: * -&gt; * where Bar :: Int -&gt; Foo Int Oink :: b -&gt; c -&gt; d -&gt; Foo (f b) becomes data Foo a = (a ~ Int) =&gt; Bar Int | forall b c d f. (a ~ f b) =&gt; Oink b c d
That just means that we should also be bashing Postgres.
RebindableSyntax will let you use the `if then else` sugar. Unfortunately you lose case expressions. There are no extensions for that.
I am certain, it's only possible when you are doing that on purpose (unlogged tables, etc). I just don't see any use case for MongoDB. It's even slower than PostgreSQL with jsonb.
I run an agency and we do the occasional Haskell Job (we're based in London). He're is how we'd approach the problem: It's unclear from the description whether you want an OAuth2 client or provider. If it's a client used for login then passwords are out of scope because the identity provider handles those. Implementing a client can be as easy as sticking an oauth2 proxy in front of your app. This would be 1 day max. If you want to implement a full provider with the 1/ HTTP redirect workflow etc, based on servant, 2/ DB interactions 3/ A simple HTML-only admin interface 4/ Sendgrid integration we think it#s 1/ 2 days 2/ 1 day 3/ 2 days 4/ 1 day + 2 days of QA etc. At London agency rates (~£800 / day) that's ~£7000. I'm somewhat unsure about why you'd need an identity provider in isolation in Haskell when there are already plenty of well-tested projects doing just that but I'd be interested in learning the full story :)
And I've been thinking about case expressions. It seems that the RebindableSyntax could be extended to support overloading case expressions. I don't think there's anyone working on that at the moment, though.
&gt;If you drop the Monad instance and go for an Applicative instead, you would have the entire structure of the computation without running it; of course, you would lose bind and do notation. That sounds interesting. Basically I just lose control flow and the ability to predicate upon the values, but I can regain that by augmenting the EDSL with it. Wouldn't `ApplicativeDo` allow me `do` notation? Admittedly GHC 8.0.1 only though :)
&gt; Haskell programs tend to have a very different overall architecture than imperative programs I don’t find that to be the case. My Haskell programs are structured in much the same way as my C programs—I have data types and functions, encapsulated in modules, built in a top-down fashion, with a test suite. The main difference for me is using the type system to enforce more invariants, such as separating pure &amp; effectful code. I really don’t like this meme that Haskell is so different from conventional imperative languages. It does enable different (sometimes subjectively better) problem-solving strategies, but it’s not alien.
 Just try it out - looked decent on my Kindle. 
&gt; ...OK... That was easy! Everyone: This must be a great customer :)
We can still do better: * `text` requires installing a package and many people give up on Haskell before they learn how to do package management * `bytestring` is not an appropriate type for dealing with text * `readFile` is in the Prelude, and none of the alternatives are
You're right, the corresponding thing if you add that `Triple` constructor doesn't seem to work. At that point - in my old GHC, 7.8.4, at least - GHC gets confused about the other terms too. But I don't understand why this shouldn't work. It seems that it should be easier, not harder, when there is erasure of type information. When there is overlap of the types of the constructor parameters with the return type, you need the type equalities to make sure that everything matches up correctly. But if the constructor parameters are independent of the return type and are freely existential, then once you have matched the correct constructor in the corresponding equation for matchFoo3, why can't you use any function that is fully polymorphic in its parameters for the corresponding deconstructor?
&gt; My Haskell programs are structured in much the same way as my C programs—I have data types and functions, encapsulated in modules, built in a top-down fashion, with a test suite. Maybe it's your C programs which are structured in an unusual way :) One major way in which the architecture of Haskell programs differ from that of imperative programs is that Haskell programs tend to have an outer IO shell around a pure core. I don't know about C, but in object-oriented languages I don't feel like there is much of an incentive to have a pure core, instead the code is usually divided into many objects so that each one has a single responsibility, and if that responsibility happens to involve side-effects, then so be it. *edit*: Another big architectural difference is the way in which different modules interact. If module A depends on module B, then in an imperative language A is likely to make several calls to B, both within the implementation of a single method and across several related methods. In Haskell, it's more likely for module A to define a combinator library for describing all the possible ways to make use of A's functionality, and to only involve B in the runA eliminator.
I was surprised they don't have a native linux client. That would be a great way to beat Skype, which completely sucks on Linux. 
It's a little different, but you can get case expression sugar of some sort using arrow notation.
I have no idea. Also there's the fact that in Haskell, if the parameter is freely existential, you can't make any actual use of it in this case, even if you can use the function. Nothing about the type of the value may escape, which means the only thing you could even theoretically do with it is pass it to other polymorphic functions and eventually discard the value. Well, you could force it with `pseq` to possibly throw an error, that's a morally repugnant use case.
What does the pound sign do on line 20: l1 = sigmoid (l0 Linear.#&gt; syn0)
All the customers I have worked with have always been super nice, reasonable people.
ES "zen discovery" may all be just incorrectly reinventing the wheel: https://aphyr.com/posts/323-call-me-maybe-elasticsearch-1-5-0
[removed]
The book about Purescript has a chapter about it. (Chapter 11) https://leanpub.com/purescript/read#leanpub-auto-monadic-adventures They are not the same languages, but it is very easy to translate one to another. 
I'm writing [a MUD server in Haskell](https://github.com/jasonstolaruk/CurryMUD). The state of the virtual world is represented in a large record type. That record type is put inside an `IORef`. I have a `ReaderT` monad transformer stack; the `IORef` is passed around by the `Reader` monad. I'm a couple years into development and this solution has worked quite well for me. You might consider something similar. 
Cereal isn't outdated. Both support genetics. Binary should be readable but cereal should be slightly smaller. I think binary is lazy bytestrings vs strict for cereal. That's about it.
Understanding Netwire was really hard for me - Yampa was much more accessible. * [Wiki](https://wiki.haskell.org/Yampa) * [Yampa arcade (pdf)](http://www.antonycourtney.com/pubs/hw03.pdf) * [Yampa intro (video)](https://www.youtube.com/watch?v=T7XwTolu9YI)
How complex is the game? `main = interact $ concat . snd . mapAccumL gameStep initState . lines` For some game stepping function and initial state. Covers a lot. Now nothing needs to be mutable. Though if you need to do fancier ANSI terminal stuff, this won't suffice.
Here's what I've done, in essence: data Game = Game { currentLocation :: LocationName, world :: Map LocationName Location, playerInventory :: Map ThingName Thing } data Location = Location { name :: LocationName, description :: Text, exits :: Map Direction LocationName, thingsOnGround :: Map ThingName Thing } `moveTo` would take a `Direction`, find the player's current location by indexing into `world` with `currentLocation`, find the destination `LocationName` by indexing into `exits` with the given `Direction`, perhaps ensure that the exit is still valid by ensuring that the found `LocationName` is still in `world`, and then set `currentLocation` to the location's name (you also likely have other reasons to actually look up the Location object, e.g. to print its description, depending on the way you've laid our your game's event loop). It's very important that any "mutable" values not appear in multiple places in your Game data structure, because then you'd need to update all the places at once. That's why we use maps with names instead of just putting the Location objects directly inside other Locations (not to mention the circular references). I presume that Things can only be in one place at one time, so it's fine for them to live entirely in `playerInventory` or `Location.thingsOnGround`. Also, quite often in a game simulation you end up not using names as keys to these maps, but rather some kind of ID, so that you can do things like have names that can change, or have multiple possible names resolve to the same object. Yes, this is a lot of indirection, but I really doubt you're going to find something usefully more efficient than this for a room-oriented game like this, at least as long as you're keeping the whole thing in memory at once. The main annoyance is that pretty much all of your simulation functions like `moveTo` and `pickUp` and whatnot need to return `Maybe` or `Either` instead of just the a `(MoveResult, GameState)` tuple -- any of those map lookups might fail if your data has become inconsistent somehow. If you end up struggling with CPU or memory performance of `Map` for the `world`, then I would suggest trying one of the data structures from `unordered-containers` which do better with large maps that change than Data.Map. I would love to see someone actually figure out a decent way to represent a non-completely-trivial game simulation at least as rich as this one (which is not asking much!) with Zippers or Multi-Zippers, and give it an API that doesn't make me want to cry. I haven't seen one yet. I'd love to figure out a way to get rid of all the map lookups and Maybes and Eithers in my simulation functions. 
The Excel integration was (and is) a really clever way to get a foot in the door. Alas, Excel has too many weird corner cases to be a good frontend.
There should at least be such versions, yes.
I like [`MonadThrow`](https://hackage.haskell.org/package/exceptions/docs/Control-Monad-Catch.html#t:MonadThrow) from `exceptions`. It works in the context of `Either e`, `Maybe` and many monad transformers.
Being cloned over the wire beats the commute any day.
&gt; I'd love to figure out a way to get rid of all the map lookups and Maybes and Eithers in my simulation functions. Apparently, Liquid Haskell can help with these annoying "searching for a key that you know it's in the map" cases: https://ucsd-progsys.github.io/liquidhaskell-tutorial/10-case-study-associative-maps.html
There's the [RWS](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-RWS-Lazy.html) (read/write/state) monad that combines a reader, a writer, and an updateable state. It seems to me you could combine all possible actions into a data type data Action = Move Direction | PickUp ItemName | UseWith ItemName ItemName and put the update function into RWS, like so: updateGame :: RWST (IO Action) MoveResult GameState IO updateGame = do -- Get the next action, presumably by asking the user action &lt;- liftIO read case action of ... -- handle action Alternatively, you might push the IO stuff out of your update function: updateGame :: Action -&gt; RWS () MoveResult GameState updateGame action = case action of ... I used this second approach in a little game I made. The game itself is run by iterating `updateGame`.
I was once thinking about similar thing with the usage of free monads. Basically, you wouldn't be serializing this coroutine/computation, but instead the events would be serialized and then replayed on load. Then, your computation would look more like: killQuest = do threeBoarsRes &lt;- waitForEvent (PlayerKilled 3 Boar) if threeBoarsRes == Failed then questFailed else tenDragonsRes &lt;- waitForEvent (PlayerKilled 10 Dragon) .... This means when you will be interpreting this free monad (which will happen on every new event/or on load), you will have the events values ready and those stored values will then become the xxxxRes variables in your computation. The value of the computation (of type Free XXX) would be stored runtime and then used whenever new event arrives, to calculate the next 'state'. If you'd like to add conditions on health etc, then I guess you will probably have to wrap them in some events as well, and then you won't be reading health property from some player structure, but from those serialized events. Now, I haven't implemented/used those ideas so there is lots of things I don't know how to solve: * what's the type returned by waitForXXX? how to make it more type safe, that is, if you wait for MonsterKilled event, you need to get MonsterKilledEvent value back * how to make more complicated logic: the example shows progressing scenario, in which you perform some actions and each of them yields some result which can determine the future outcome. However, more realistic scenarios could introduce some external event that can fail the whole quest. How would that cancelling work? Should you check that condition on every step? That would be rather bad... So, I'm rather in similar position as you, and I'm quite interested what you come up with in the end.
I am using https://github.com/well-typed/binary-serialise-cbor , fast and reasonably universal.
I'm a bit uncomfortable putting the list of items in a location, since that blends mutable state and immutable state in a single type. One design I've seen elsewhere separates the mutable state into maps, and associative lists, and keeps the immutable game world separate. data Item = Item { itemName :: String, itemDesc :: String } data Location = Location { locName :: String, locDesc :: String } instance Eq Location where (==) l r = locName l == locName r data GameWorld = GameWorld { initialLocation :: Location , moveRules :: Data.Map (Location, Direction) Location , itemUsageRules :: Data.Map (Item, Item) Item } data ItemPosition = Place Location | PlayerInventory | Limbo data GameState = GameState { world :: GameWord , playerPosition :: Location , itemLocations :: [(Item, ItemPosition)] } In such a design,this is how I'd implement the `PickUp` command. itemPresent :: ItemLocation -&gt; Location -&gt; Bool itemPresent (Place itemLoc) playerLoc = itemLoc == playerLoc itemPresent PlayerInventory _ = True itemPresent _ = False pickUp :: GameState -&gt; ItemName -&gt; (MoveResult, GameState) pickUp origState@GameState{...} desiredItemName = let items = map fst [(i,l) | (i,l) &lt;- itemLocations, itemPresentLocally l playerPosition] item = listToMaybe [i | i &lt;- items, itemName i == desiredItemName] in case item of Nothing -&gt; (NoSuchItem desiredItemName, origState) (Just i) -&gt; (PickedItem i, origState { itemLocations = doInsert (i, PlayerInventory) itemLocations }) where doInsert = {- inserts / replaces a key-value pair in an associative list -} I was wondering was there anything more sophisticated than this. In particular the associative list won't scale very well for search, even if I use a zipper to manage the mutations. 
That looks really good. I'd set myself this project to get a good idea of how the RWS monads worked, and that chapter seems to be following the exact same approach :)
[#haskell-game](irc://irc.freenode.net/#haskell-game)
What about Duncan's `binary-serialise-cbor`?
The idea is to only store the decisions and inputs that have been made, and not the functions themselves. When you run the (same) program again, you can replay the computation using that data. Depending on the concrete problem, this may become easier and/or faster, if the the _structure_ of the program is known and doesn't depend on the values.
Might be worth comparing against original binary, before it got lots of features that made it slow. https://hackage.haskell.org/package/binary-0.4.3.1 and earlier. When it mentioned "1 G/sec" (and performance was part of the testsuite). Slightly #getoffmylawn #walkbarefoottoschooluphillbothways, since the point of binary originally was to be ruthlessly fast -- hence the performance numbers in the description -- it got a bit bloated over time clearly.
Good question, I completely forgot about that! I've added the code in https://github.com/fpco/serial-bench/commit/c67e64772f50bd25903002727f017c36cea9438d. You can see the results at: http://i.imgur.com/htyqIzL.png Summary: while it's a significant improvement over binary and encodes a lot faster than cereal, it's still slower than the approach I'm advocating in this blog post. But that's not too surprising: Duncan's cbor work is _far_ nicer as a long-term serialization format than any of the other approaches under discussion. But in the realm of getting maximum performance without worries of getting a nice data format, it's competing at a disadvantage.
Yeah, definitely need to go via IEEE or host representation. Totally unsurprizing there. https://hackage.haskell.org/package/data-binary-ieee754 was needed quite early.
Yeah, I've been watching Liquid Haskell. I also liked Gabriel Gonzalez's blog post about it. But this simulation code involves a *lot* of maps with indices floating around, and it seems like a pretty big task to implement. Maybe some day when I have a few weeks to figure it out :)
This isn't necessary if you're doing things like simple batch processing off some job queue, since you can just cleanly upgrade in between batches. I assume that's the intended use case here, given the requirements - otherwise, you're right - and cases like this are something we hope to eventually handle with `binary-serialise-cbor`. But given they don't need that, not very surprising to see you can go faster.
Thanks. I guess Axolotl at v3 is basically now what's implemented by TextSecure? Maybe the official spec has been updated, I should look around...
I hadn't seen (and only vaguely heard of) binary-serialise-cbor before. The source code is enlightening. 
Thanks for the explanation. I figured you were employing some way of doing sort-of-atomic deploys. 
I didn't see any difference with this implementation. But I'm curious: it doesn't seem like `decodeListLenOf` and `encodeListLen` are necessary to make this work. What's their purpose? __EDIT__ Actually, I take it back, it does seem to have sped things up, but it's still slower than the little-endian encoding format options.
No, I don't think there should be any downside for this case in eliminating the list. It will probably improve things a bit, too. The major benefit of this scheme is that serialized ADTs fit nicely into this list format, and this structure makes it much easier to do things like version-ing and migration in the future thanks to the tags, too (for example, suppose I extended `Baz` to now contain a `(Int, Int)` after the `Bool` - technically, *old* code should be able to read this new format, because it could skip over the `(Int,Int)` entry in the CBOR list and move on to the next value. Similarly, you want tags so you can e.g. introduce new constructors and perhaps convert from new to old ones easily, or even automatically.)
Makes sense, thanks for the explanation!
I also expected it to be slower because it implements so much more! I was just wondering, how it would compare to the other off-the-shelf libraries.
Congrats for the release folks!
cereal was created because of oversights in earlier versions of binary, mostly around error handling and streaming IO without lazy IO. Those improvements have mostly been folded back into binary so the libraries are about equivalent these days. I always use binary
Yay! I'm all about structured JSON logs. Having good structured logs in distributed systems in production is such a boon to operation and analysis.
Feel not. Because calculating is better than scheming.
The type of [itemPayload](http://hackage.haskell.org/package/katip-0.1.1.0/docs/Katip.html) generated by `makeLenses ''Item ` caught my eye itemPayload :: forall a a. Lens (Item a) (Item a) a a Is it a type-changing lens: itemPayload :: forall a b. Lens (Item a) (Item b) a b or itemPayload :: forall a. Lens (Item a) (Item a) a a itemPayload :: forall a. Lens' (Item a) a 
Should be fixed now.
"Use lenses". In particular, above and beyond merely being able to reach into complex data structures and modify them, you can use lenses to window into complex data structures and write generic functions that operate on chunks of things of a certain type, and use the lenses to specify the chunks. For instance, suppose you've got "item container" functionality, such that the user's inventory is an item container, each location may have items, each item may have items if it is a container, etc. etc. It becomes easy to write a move function that takes an item specification and two lenses that specify how to reach into the target from and to locations. Lenses really come into their own when they aren't just used for direct data access, but passed around as first-class values that allow you to abstract the very idea of "access path" itself.
One big advantage of Haskell will only appear after you have a large-ish code base. Refactoring. That's a fun day for a Haskeller. Scary and painful for languages like python. Anything over 1000 lines and you will start to notice.
I wondered about `cbor` this morning and - since the version of `cbor` I had lacked `V.Vector` and `Word8` instances - hacked together a test, which was much faster for decoding, the equivalent of your builder tests. Now that I have a version with the new instances, I just wrapped my instances and got similar results (The existing ones go by way of lists as do the binary instances, if I remember.) https://gist.github.com/michaelt/48c37120ef3d626d71ee (I am specializing to Vector SomeData, but I think isn't the source of the difference, since it was similar this morning.) The better instances is marked `cbor2` in the bits below. benchmarking encode/encodeBuilderLE time 4.158 μs (4.118 μs .. 4.211 μs) benchmarking encode/encodeBuilderBE time 4.780 μs (4.735 μs .. 4.835 μs) … skip cereal / binary … benchmarking encode/cbor -- using the built-in instance via toList etc time 8.864 μs (8.824 μs .. 8.905 μs) benchmarking encode/cbor2 -- using Data.Vector's own `foldr` directly time 4.261 μs (4.229 μs .. 4.290 μs) On the other hand, the different way I wrote the decoder function for the instance - with `V.replicateM` - didn't make an appreciable difference, but maybe there is something better around. benchmarking decode/cbor time 11.35 μs (11.18 μs .. 11.55 μs) benchmarking decode/cbor2 time 10.69 μs (10.54 μs .. 10.83 μs)
[Here](https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=3640b30a-38b9-49a6-b95c-d96e93bc9284) is a higher-quality version of the presentation. (An interesting part is from minute 39 onwards, the comparison with OO inheritance.) One interesting aspect of type classes is the process of "elaboration", of building the final dictionary from the constraints. This paper touches on it (from the perspective of type families): https://ifl2014.github.io/submissions/ifl2014_submission_31.pdf 
Thanks :)
I found this article by Oleg Kiselyov gives some good insights: http://okmij.org/ftp/Computation/typeclass.html
Very cool! Just tested. You need to have somewhat complete code. First implement the Nothing case, like this func :: Maybe Int -&gt; Int func m = 0 Then invoke the case split on `m` and you'll get func :: Maybe Int -&gt; Int func Nothing = 0 func (Just m) = 0 But it does not work when you only have func :: Maybe Int -&gt; Int func m so, you need to have a complete function. ide-haskell rocks! 
I am afraid I don't see what context the "in general" is referring too?
Thanks! By the way your version looked really good. 
I've added support, with some caveats - https://github.com/commercialhaskell/stack/issues/1284 . The caveats are both due to stack improvements that are in progress, as well as cabal / ghc issues (it does not force recompile modules when split-objs is newly enabled)
I have been working on a framework that should be as easy as C: https://github.com/hsyl20/ViperVM/blob/master/WritingBindings.md It is more oriented towards writing Haskell bindings for C libraries (instead of serializing arbitrary Haskell data). Hence it supports C structs (thanks to c-storable-deriving package), unions, arrays, enums, bitfields, etc. But it doesn't support variable-length types (ByteStrings, etc.), nor endianness portability.
I wrote a guide on using indentation here, http://zinkov.com/posts/2016-01-12-indentation-sensitive-parsing/ To get the behavior you want you will need to use absoluteIndentation.
You're the author of this crap? That just reinforces my view. The C++ standard library is the absolute pinnacle of well-designed C++ code. There is almost zero code out there in C++ that is designed better than the best parts of the C++ standard library. Object-oriented programming is awful and has always been awful.
I am in fact looking for a Haskell mentor myself at the moment. Thus, I can say that I very much like the idea to support the community at two ends using this strategy. Mentoring support could be classified, for instance: Live Mentoring (like Code Academy) with frequent meetings for learning, Live Mentoring for problem solving, E-Mail or Char Mentoring also in both categories. If no other way could be found PayPal at least offers an instant payment solution with notification so that the session can start when the payment to a haskell community account has been made 
Row has same structure? Or is Row combination of all possible columns where many of them are null? Or is Row a serialized format that cannot be queried?
Good Q. No not yet. We should add this tree example to our microbenchmarks and investigate. Note that the actual encoding being used is wrong (not valid CBOR), but that ought not to affect the performance. The only thing that pops out initially is that we've noticed that to avoid thunks while deserialising we need to strictly decode each component. So e.g. rather than: Tree &lt;$&gt; decode &lt;*&gt; decode we need to do do !l &lt;- decode !r &lt;- decode return (Tree l r) Exactly why this is needs some investigation, and we may be able to integrate it into the instances for &lt;$&gt; and &lt;*&gt;, but possibly doing so is not entirely kosher wrt laws.
I think with vectors the bigger win will come from improving things so that the vector instance does not need to go via a list. Michael is right to point out the issue with the `Get` monads that don't allow for ST, but there's another issue lurking in there. In Michael's use case he doesn't need to care about untrusted input, so in his case he can blindly trust length prefixes. In a library designed for more general application it is something you need to worry about. The classic attack against the decoder makes it behave like "allocate an array of size bazillion? Ok...". The decoder in `binary-serialise-cbor` does handle this issue for strings/bytestrings. It does not pre-allocate before the data has arrived. My thought is that we ought to do something similar for arrays/vectors of boxed or unboxed data too, by handling them directly in the decoder interpreter. I've also considered adding support for ST, but it may or may not be necessary, we'll see. Anyway, I'm hopeful we can speed up the instances for arrays and vectors, which would help this benchmark somewhat but would have the biggest improvement for dense arrays of primitives.
You can use a hole. func ∷ Maybe Int → Int func m = _ 
right!
I'm worried about the performance of the garbage collector on 48 cores for workloads that aren't IO-bound. I'd also be interested in seeing some numbers.
Awwwwww yeah! Looking forward to this!
OCR. I currently throw hundreds and hundreds of cores at OCR (not via Haskell). It is both fun and depressing to run htop and see 144 little bars all maxed out. 
Cheers muchly. I take it you mean on the encode and decode class methods themselves within the instances (the skeleton is inline already). BTW, if you would like to help out with the cbor lib by adding microbenchmarks like this for the instances, that's definitely an important thing on the TODO list. Would be much appreciated.
When you say "battle tested" - does that mean have you profiled performance against other logging frameworks, such as [fast-logger](http://hackage.haskell.org/package/fast-logger)? It has been found that logging can be a surprisingly large drag on performance. That's why a huge amount of work was invested in optimizing performance in fast-logger, down to deep GHC internals and tuning to CPU hardware capabilities. That's why fast-logger is pretty much the standard logger for live web apps, at least in the Yesod ecosystem. How does katip compare?
I got a friend into Haskell and showed him how to get started with repa and how to write a ray-tracer, its a really cool project, just cast vectors from the camera, through your grid plane onto the scene and do some simple primitive collision detection equations for spheres or cubes. Afterwards, you can add colors, shadows by calculating bounce, octree culling, etc... I think its a really cool project and each pixel is obviously independent so you get to see every core go full blast.
Yes, it does seem to have started out that way, at least. That Jespen post is about a version of ES that is quite old now. Since then, they have been devoting quite a lot of resources to improving and fixing zen, as already noted in the Jespen post. But in answer to my original question, ES zen doesn't seem to have anything really new. It was an independent discovery of, and subsequent parallel development of, the same general technique as Paxos and Raft.
Thanks for the compliment! :)
Wow, this is totally awesome! Care to share how you did it? I was thinking of a simple bash script myself.
The major motivation has been convenience. The objective here has been to make it as easy as possible to do logging in various settings. We've been using string-conv all over the place for typical text content and it has worked surprisingly well in practice. It internally uses the conversion functions from Text and ByteString, which I assume will be as performant as you can get. Conversions with Lazy variants however won't be very performant, as I think you lose fusion when you go through it. I suspect this approach be more performance in general than the Cons approach in lens, which I think will traverse the full string. Please correct me if I'm wrong. You make a solid point regarding encodings. In that case, you could correctly convert your string to, say Text, and use the string-conv instance from there. Ultimately, it is understood that the logs will be textual (and not, say, binary) - this is not an arbitrary database library. Seems like a combinator or so would be all we need to make it convenient. Regarding the additional structures: Absolutely, those would be nice to have. We haven't run into them much in practice, but once again, I suspect we could just add a combinator that makes it easy to use the cons infrastructure to produce log strings.
Could you tie it together with a way to go from a `Stream` to a `StreamModel`? :-)
done! see `stream2StreamModel`
Yup :)
That sounds great! This sounds like a fantastic utility and we will be looking at it. Thanks for the great work. EDIT: BTW fast-logger is not only for apache-style logs. Like katip, it also implements loggable things as a typeclass. You can just as easily create JSON logs as Apache logs. But of course, it is missing all the machinery for generating rich logs that you find in katip. Definitely looks like there is a lot of potential for synergy here.
GHC and the various parallelization libraries scale impressively easily and impressively well to a moderate number of cores. But last I heard, the results are less impressive for tens of cores, and I am worried about what will happen with hundreds. It could be that is just because of some technical issues that could easily be solved, and wasn't at the beginning only because of the hardware they were using then. But it could be that this requires a good truly parallel GC algorithm, which I don't think anyone has written yet.
What can you do with coinductive sums?
Thank you, we'd love to hear your feedback! It definitely sounds like there are significant synergies. In fact, I think it may make sense to write a straightforward fast-logger backend for katip, which could already yield most (if not all) of the performance benefits in writing to files/handles/etc from fast-logger. Katip ultimately passes messages onto its backend, which is where the actual IO and formatting heavy-lifting takes place.
streammodel is not useful at all, except that its simpler to write down in haskell :) 
Sorry, I'm not sure what you're asking, but I'll try to answer. Row is the interface type and has an ID, and a RowTypeID. Left and Right are two tables. The Left table has a foreign key to RowID and the properties unique to the Left type. The right table has a foreign key to RowID and the properties unique to the Right type. The primary keys for each table are: Row=ID, Left=RowID, Right=RowID. In case that doesn't answer your question, let me know and I will try to explain further.
I'm also a beginner. What's helped me so far was [monadic parsing in Haskell](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://www.cs.nott.ac.uk/~pszgmh/pearl.pdf&amp;ved=0ahUKEwjti__s98PLAhWFHB4KHUZ5CYMQFggbMAA&amp;usg=AFQjCNFv4uHrqc9XsWg3Qk2AFW1Vm9ZRTQ&amp;sig2=dmh-c1_rDYKbWlJXvUHf1g). Not specifically about Parsec, but it really helped me to get some intuition. 
You may want to include the key word "Emacs" here, as I infer you are looking for an Emacs package :)
Refactoring isn't that bad in C++ either. When I change something fundamental in my C++ EDSL, I get 20+ compiler errors. When I fix them locally, it usually just works. That might also have to do with me preferring parametric polymorphism over subtype, but a Haskell inspired C++ programmer will also have a fun time refactoring.
This is brilliant!! I've spent the last couple of mornings getting set up and it works like a charm!
Everything will be super easy to prove!
Due to TypeInType you get Girard's paradox; Haskell would consequently be a pretty bad proof assistant. (Not to mention the existence of undefined.)
Thought undefined was a consequence of including laziness. The other thought I have is: Wasn't Haskell to avoid the pitfalls of things like subtyping and dependent typing while also subsuming the benefits, and it has gone very far in doing so in a constructive and liberating way?
&gt; Thought undefined was a consequence of including laziness. No. With a totality checker it wouldn't matter if the language is lazy or not. Undefined is due to lack of a totality checker. However, with asynchronous exceptions you'd have bottom anyway, so it doesn't matter after all. Just don't use Haskell as a proof assistant, there's no hope that it will ever become one.
That's besides the point, being a proof assistant. &gt; asynchronous exceptions you'd have bottom &gt; totality checker it wouldn't matter if the language is lazy or not. So the language is a subset of lazy contexts which assuredly has totality, but this removes the notion of bottoms being undefined/undefined inputs which gives rise to codata. Async errors likely being when an asynch call doesn't return non-underdefined/bottom value when inspected. Takes the piss out of the punch.
You don't have `undefined` at the type level. As for Girard's paradox, my understanding of Richard's work here is that while the core type theory we elaborate to suffers from it, the surface language doesn't allow you to state it, so we recover a somewhat shaky form of safety.
thank you very much
The way I envision it - or at least the way I hope it will be - is this: When we use new type features to enrich the semantics of values, Girard's paradox isn't really an issue. We won't be subjected to the awkwardness of having a termination checker applied to all our value-level code. And if we mistakenly write a type-level loop, the type checker will terminate it with an error, which is what we want. Besides that, there will be some syntax - either new syntax, or using libraries that are yet to be written - for writing type-level theorems about value-level functions. There, we will have some way to ensure that a termination checker is applied to the theorem, perhaps in some syntax-driven automated way. In other words - I don't think the goal in DT Haskell will be to erase completely the distinction between values and types in idiomatic Haskell code. But since in fact it will be possible to use types as values and values as types, the power of DT techniques will be fully accessible in practical software development.
Great talk, too bad the video is not complete
But some rays finish much earlier than others which means it is not *regular* parallelism.
I do some odd jobs every now and then. Haskell always presents some unexpected problems which extend development time. then there is the fact that Servant is an unfamiliar framework for me which requires some research, so i'd charge twice as much as if it were a similar php job. it is a todo list (and seems to be only a REST api which avoids frontend problems), which is very simple and standard and can be done by anyone with basic development skills (this could -almost- be a job interview coding test). it should take me no more than a day or two in php. I think i'd normally charge 300 EUR for this. because i'm charging double it would be 600. this would be cash. if money has to be transferred via the banking system I have to factor in dutch income taxes which would be about 42% (which would mean a 1000EUR which i think is too much).
We have a nice lazy term level language. All of the interesting not-quite-dependent type bits in Haskell kick in at the type/kind level. So I guess if kinds are the types of types, then we're really talking about having "dependent kinds" not really "dependent types" so much ;) (For right now, and until Richard manages to beat some form of pi type into the system, its just a sort of cumulative universe thing, nothing dependent about it.) Our types don't get to depend on terms: After all, computations in the term language can diverge without consequence. Terms we "lift" from the value language to the type language with tick-marks don't come out the same. e.g. When we 'lift' [] from the term language, we only get finite lists at the type level. I'm not sure how dependent Haskell will get in the end, but adding TypeInType definitely made it easier to give kinds interesting properties.
try to write queries. i know it's involved. exercise left for readers. forgot lists are ordered as well.
My curiosity did eventually get the better of me, so I added the older binary. The results showed it performing slower than the latest binary package: http://imgur.com/x1Wn8Sy
&gt; lists are ordered as well This is just an `order by`. &gt; try to write queries It's just a bunch of joins, which are the easiest query to write IMO. Add a table, add a join. &gt; exercise left for readers The pattern I've described here is used for OOP all the time and I've never seen anything too hard, assuming the programmer is familiar with SQL. If you prefer something else, nothing is stopping you :) Some other patterns include NoSQL document stores, or storing documents in SQL. Alternatively you could store everything in a single wide table if you want as well, if the JOINs really put you off. This is a common pattern in data warehouses, but uncommon in a normalized database. I suppose it depends on the individual use case. 
WYAS is somewhat outdated and is a very quick intro for a newcomer imo. I wasn't able to do it until after I'd completed cs194 homeworks.
That's really interesting, and useful. Thanks
I switched from Sublime-Haskell to Atom-Haskell, it took about an hour to get everything installed and configured and I did not look back.
So we have a nice complete *bona fide* DT language embedded in Haskell. It plays the role of of the type system. If you want to program exclusively in the DT language, go right ahead. I'm sure you'll find pleasant ways of doing mundane things like writing data to disk or sending it out over the network. If you want to continue programming in the pure functional term language, with the type system providing rich and precise semantics for the terms, you can do that, too. It's the best of both worlds. And most likely, we will find new and creative ways to use the two together as a synergetic whole. The skies are blue; the horizon is clear. What could be better?
Very true. No, very false. Wait...
I've seen a few unikernel talks so far but this one is the first giving me a feel of how the development process works. Nice! Also, this process of porting to the unikernel and abstracting over the network stack, for example, is what we need backpack for badly. In MirageOS, they rely on the module system to swap implementations for the same interface but currently we cannot do it nicely in Haskell. 
I wonder how true this is practically. How easy is it to *accidentally* provide an incorrect/inconsistent construction which the type checker will accept?
partition in Data.List
I think the bigger problem is, "How much confidence do you have, given that something is 'proven' in Haskell, that it is 'true'?" With proof assistants like Coq, you can get pretty high confidence by skimming the proof to check that it avoids known inconsistent axioms; with Haskell and `TypeInType`...
actually this works better :) thank you !
Very.
This is used in the definition of [`catMaybes :: [Maybe a] -&gt; [a]`](http://hackage.haskell.org/package/base/docs/Data-Maybe.html#v:catMaybes): catMaybes :: [Maybe a] -&gt; [a] catMaybes xs = [ x | Just x &lt;- xs ] 
Pattern match failures on the left hand side of bindings in `do` blocks are handled using `Monad`'s `fail` method. For most `Monad` instances, this means a runtime error, but lists in particular define `fail _ = []` ([Source](http://hackage.haskell.org/package/base-4.8.2.0/docs/src/GHC.Base.html#line-726)). Hence, the above code is desugared to the following: list &gt;&gt;= \item -&gt; case item of GroupID a -&gt; return a _ -&gt; [] See also the [Haskell Wikibook](https://en.wikibooks.org/wiki/Haskell/do_notation) (which continues to surprise me with how good it is). Edit: Slightly late I am.
Using `lens` this can be generalized to any fold: _UserID :: Prism' To String _UserID = prism' UserID $ \case UserID usr -&gt; Just usr _ -&gt; Nothing _GroupID :: Prism' To String _GroupID = prism' GroupID $ \case GroupID grp -&gt; Just grp _ -&gt; Nothing &gt;&gt;&gt; toListOf (folded._UserID) list ["user1","user2"] &gt;&gt;&gt; toListOf (folded._GroupID) list ["group1","group2"] only now it's first class and we can get all user ID's from anything that's foldable toListOf (folded._UserID) :: Foldable f =&gt; f To -&gt; [String] &gt;&gt;&gt; toListOf (folded._UserID) (Data.Map.fromList [(0, UserID "root"), (1000, UserID "alice")]) ["root","alice"] as well as nested foldables of foldables of... &gt;&gt;&gt; toListOf (folded.folded.folded._UserID) [[Just (UserID "alice"),Nothing,Just (GroupID "cdrom")],[Nothing, Just (UserID "bob")],[Just (GroupID "sudo")]] ["alice","bob"] &gt;&gt;&gt; toListOf (folded.folded.folded._GroupID) [[Just (UserID "alice"),Nothing,Just (GroupID "cdrom")],[Nothing, Just (UserID "bob")],[Just (GroupID "sudo")]] ["cdrom","sudo"] **Edit:** The type `_UserID` shows it is a `Prism'` but because of the lens subtyping is *also* a `Fold`, as well as... _UserID :: Prism' To String _UserID :: Review To String _UserID :: Traversal' To String _UserID :: Fold To String _UserID :: Setter' To String
Dependently typed languages are a scam. They claim to make it possible to prove that your program is correct, but anyone with even a basic understanding of computer science knows that this means "write a program". 
We'd love to have you around! We are planning the forst haskell meetup for mid april.
I'm pretty sure FP Haskell Cetner was implemented with GHCJS, and that had quite a decent embedded editor with Haskell syntax support. I think they also used that same editor widget in the embedded editors in School of Haskell, which is now open source: https://github.com/fpco/schoolofhaskell I don't really know anything about the codebase but I'd assume their editor code is somewhere in there.
I was wondering, why can't `forall (a :: A). Sing a -&gt; B a` just be `Sing (a :: A) -&gt; B a`?
An aside, but how are the the versions exposed? I looked through the haddock and couldn't really see how that would work. I don't know how the other systems work, but I just stick a number in each Serialise 'encode' method, pull it out in 'decode' and then decode differently based on that. My assumption was that CBOR would be the same, but could stash the version in the tag and avoid the extra field. Of course I can do that too but CBOR requires a tag for everything so maybe it can automate that somehow? The bit about old code reading new data is neat too, though a bit different. I couldn't see how that is configured either, though maybe it happens implicitly.
f*ck me
ok
I assume your comment is some sort of weird joke or something, or maybe you misinterpreted my comment? Maybe my comment wasn't clear enough... Dependently typed languages have two options: either make the type system turing complete (which simply moves the problem of proving a program in a turing complete language correct to proving that your program in the turing complete type language is correct) or sacrificing turing completeness. Sacrificing turing completeness just isn't acceptable. Turing completeness is a fairly weak notion (in that even some extremely simple systems are turing complete, like the SKI combinator calculus).
could you explain the connection more please? i think theres certain a few different patterns people have cooked up that involve a function that dispatches on a GADT argument, but i'm not sure if i see the relationship to this one. I DO see a relationship between what i'm calling a Tag and the k constructor in the Step data type in machines https://hackage.haskell.org/package/machines-0.6/docs/Data-Machine-Type.html, but perhaps that what you're thinking of?
You can do lots of interesting things without Turing completeness, especially with coinduction, and I'm not convinced it's a requirement for a good programming language*. But even if you stay Turing complete, the type system is no less invalidated without dependent type than with them, especially the way (as I understand it) it's being done in Haskell. * McBride has a good paper on this if you're interested.
So the correspondence to the mathematical notation you're normally used to is as follows: A a b ~ a + b M a b ~ a * b D value ~ value (constants) S name ~ name (variables) d "x" e ~ the derivative of e with respect to the variable (type S) with name == "x". If you can write your expression using the right of the correspondence above, then if you translate to the left these functions should work as expected, outputting structures of the same correspondence. Later tonight I'm going to add an evaluate function which takes some sort of mapping S -&gt; Exp. Was that clear enough? Edit: Made Exp an instance of Show in a prettier way so you can see the correspondence when you print things
Sounds a bit like [the GHCJS based version of try-purescript](https://github.com/ghcjs/ghcjs-examples/tree/master/try-purescript) (uses CodeMirror).
Hmm... I'm not sure if this is the same distinction you mention, or a similar one, but I've been wondering about it for a while: the property Agda and Coq provide is that a term of type ⊥ will never even typecheck. I've heard this referred to as being "consistent as a logic". Haskell and other similar languages are not, but they are still "type safe". The property they seem to provide is that while terms of type ⊥ may typecheck, you can never actually get your hands on a (WHNF) value of ⊥ at runtime. When that "would happen" the program exhibits nontermination instead. What's the (or a) right way to think about this difference? What is "type safety", really, constrasted to what theorem provers like Agda provide? What are the correct words to use for them? Is one of them "sound" and the other one "consistent"? Do these correspond to the meanings of those words as applied to logics? My impression had been that people colloquially use "sound" and "consistent" (alongside "type safe") for programming language type systems somewhat interchangeably; is this really the case, or have I just been failing to pick up on the subtle distinctions?
Symbolic differentiation in a tweet: https://twitter.com/gabrielg439/status/647601518871359489
Has it been proven to be correct as Raft and Paxos have?
GHC apparently now allows `undefined * 0` to equal `0`!
You may find the following notes interesting: http://cit.dixie.edu/cs/3520/haskell-symbolic.pdf
What is 'y' in that example? Just a pre-defined value?
Typo. It should be X 
I think you really underestimate how much you can do in a total language 
You could use e.g. `:+` and `:*` as the constructor names instead of `A` and `M`.
Alas, [that one seems to be using Ace as well under the hood](https://github.com/fpco/schoolofhaskell/blob/master/soh-client/src/View.hs#L43).
&gt; It's the best of both worlds. Nonsense. The DT language in Haskell's type system is far more cumbersome than any actually dependently typed language, due to the need to use singletons to connect value level and type level computation.
It could if you allow kind signatures.
That's real nice to hear. Do you have a resource on that?
It works just like ordinary operators: `infixl 4 :+` etc.
Yes, the beginning of that video was, IMO, on the right track. I will add that in general, because Haskell requires to do things *so* differently, it's not enough to list the advantages; you also need to *quantify* them. For example, I won't switch a programming language if easier factorization would save me 5% of the effort, but I might if it saves 30%. Of course, this kind of data can only be collected once you have enough early adopters, and early adopters take chances just because they're curious. But once you start appealing to more conservative organizations, you better have some numbers.
The link is broken? 
It has already been fixed: https://ghc.haskell.org/trac/ghc/ticket/10691.
Thanks! ;) 
I think the NEWBIE just needs to understand when the variable 'f' is being used in the data context (i.e. used in the declaration or body of a function) or if it's being used at the type level. In class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b f is at the type level, since it is a variable representing the type to be defined as a functor. The same as in (fmap. fmap) :: (Functor f, Functor f1) =&gt; (a -&gt; b) -&gt; f (f1 a) -&gt; f (f1 b) where it is easy to see the f and f1 are not fuctions, but rather Functors. There should not be confusion if someone simply recognizes the context in which they are seeing these variables.
I thought type application was supposed to help that. Or am I misunderstanding singletons here?
[Here is a total evaluator for the untype lambda-calculus written in Agda](https://gist.github.com/gallais/303cfcfe053fbc63eb61).
Then you are as ignorant as you are combative. There are plenty of useful languages that aren't TC, take ANSI SQL for instance. Or Regular Expressions.
&gt; just needs to understand &gt; easy to see I don't agree with the post, but confusing the two namespaces _is_ a frequent source of difficulty and hard-to-debug-because-of-a-lack-of-understanding frustration. It's cognitive overhead for a newbie; I admit having struggled with it myself. 
Anyone can explain what this will bring to haskell , or what will be easier to do with that ? Can we use value level function at type level now ? As a profane interested by type level computation, I am kind a lost with all those ongoing changes.
Very interesting. I really like the goal to find a sort of fundamental insight here. This still seems to be lacking in the current discussions on EDSLs. I don't mainly work with embedded DSLs, and I am also not too familiar with all the theory here. So take my comment with a grain of salt. I did read (or at least skimmed) a few papers over the years and stumbled across Scala EDSLs and the earlier paper by one of the co authors about combining deep and shallow embedding, so maybe my opinion is not entirely useless :-) &gt; However, employing clever embedding techniques and stacks of unconventional features available in the host language, can lead to cryptic code sometimes. This is partly addressed with macros, or other forms of compile-time reflection/metaprogramming techniques. Looking at related work from Scheme/Lisp, Scala, or TemplateHaskell might be interesting here for a more complete coverage. I know at least one Scala project, YinYang (presented at GPCE 2014), that specifically tried to address this issue. &gt; It becomes difficult to distinguish an EDSL from the the host language, as the boundary between an EDSL and its host language would not be entirely clear. I think, with any form of symbolic/deep embedding DSL the types usually show (or clarify) the border between meta and object language. Without types, e.g. in dynamically typed PLs, I have heard of approaches that use named blocks for that. I think the mentioned YinYang does so, too. But I am fairly certain there is existing work on why and how to establish more fine-grained borders. &gt; The key selling points for embedding DSLs are to reuse the machinery available for a host language, from parser to type checker, and to integrate with its ecosystem, from editors to runtime system. EDSLs and embedding techniques that are proven successful in practice, go beyond traditional sole reuse of syntactic machinery such as parser and type-checker, and employ the evaluation mechanism of the host language to optimise the DSL terms (Axelsson et al. 2010; Svensson et al. 2011; Rompf 2012; Mainland and Morrisett 2010). Briefly put, what these techniques provide is abstraction-withoutguilt: the possibility to define layers of abstraction in EDSLs, using features available in the host language, without sacrificing the performance of final produced code. As mentioned in the previous section, an optimisation process, such as the ones used in above techniques, can be viewed as a normalisation process. So essentially, **what the mentioned embedding techniques do is to perform normalisation of embedded terms by reusing the evaluation mechanism of the host language**. As the names suggest, there is a correspondence between such embedding techniques and NBE begging to be examined Some approaches probably do use smart constructors to do some very simple optimizations, and directly "normalise". However, I think the more practical approaches typically additionally use more complex analyses and then pass to a compiler. I am not sure if I would classify that as normalisation. Also, while I can see the similarity to NBE, I don't think most practical approaches really normalise, not only because that is not possible as it is in NBE's setting (simply typed LC), but because rewritings (like calculating `Add(Constant(1), Constant(3))` to `Constant(4)`) are usually kept simple and don't aim at fully covering everything. Just my impression though. &gt; One interesting future work is to exploit the relation to compare EBN with the technique underlying LMS. As I see it LMS is a mixture between Final Tagless Embedding and Deep Embedding, no? What I mean by that is that it uses the former to achieve modularity and a sort of distance to leaking concrete term representation but I have yet to see an actual application using LMS that does not use the standard IR base, or a derivation thereof, to actually do the reification of embedded programs. That IR is pretty clever and is usually compiled down to Scala code, I think. &gt; Quoted embedding (Najd et al. 2016), which is a specific form of deep embedding, is when some form of quotations is used to represent syntax, and semantics is defined as functions over the unquoted representation I wonder, is this different to the quoting offered by MetaOCaml for instance? Or the one offered by Lisp? Or rather [MetaScheme](http://okmij.org/ftp/meta-programming/#meta-scheme)? :-/ I understand that the main point of discussion is abstract, mathematical insight, but I think I need to reread the paper in more detail to get a better idea of the minor details/difference. Again, I really like the idea to find an overview and common ground or a theoretical basis so to speak. However, that also means the paper gets a kind of survey-ish nature, and in that case I think it is missing a lot of related work on embedded DSLs. A quick search shows me that not-mainly-FP conferences like the ones at SPLASH (SLE, GPCE, OOPSLA) have had quite a few related EDSL presentations in the last few years. I mean, after all, LMS was presented there too. For instance, even if a particular work is not really central to the common theoretical foundation presented in ENB, for a reader it might not be obvious why it was omitted. P.S. I realize now I wrote this a little bit like OP is the author, which I am not sure is the case :-) 
Source for what? I linked the school of haskell github page
I'm not sure about this - I thought that injective type functions were not supported until recently (8.0), so this compiler message would be either accurate or out of date but not misleading within its own historical context. 
&gt; mortals Not sure whether it is an irony or not (my English doesn't allow me to distinguish such nuances), but I read the "Implementing Lazy Functional Languages" paper only few weeks ago. It is the first time I was working with STG. And I didn't know about miniSTG until someone mentioned it in [haskell care](https://mail.haskell.org/pipermail/haskell-cafe/2016-March/123402.html) a week ago :)
It turned out not to be as simple to convince myself of this as I thought. I'll have to turn it over in my mind a bit.
I always read the `may not` in the might not sense (perhaps I asked less of my English teachers), but I'd be happy with the change. Surely it would be one of the smallest, safest pull requests in GHC history. 
&gt; But even if you stay Turing complete, the type system is no less invalidated without dependent type than with them, especially the way (as I understand it) it's being done in Haskell. And you're right about Turing completeness being a weak concept. So weak in fact that it tells you almost nothing about how practical the language is. C and Haskell and SKI are all Turing equivalent - does that tell you anything about what kinds if problems they're good at solving? Are you going to assert that because theoretically SKI is Turing complete and Agda isn't, then that means SKI is a more pragmatic language to work with than Agda? The fact of the matter is many or maybe even most problems of practical importance don't require Turing completeness - tricky cases can be dealt with using coinduction, sized types, a monad, or well-founded induction, and as a last resort all DTPLs I know give you an "override" button. It's one thing if you want to say that these techniques are impractical or too complex to catch on - and certainly as things are now I could understand that. But that's a matter of language design and pragmatics, not a fundamental flaw in the theory.
I have two questions so far and would appreciate if someone could answer them. What does compositional mean in the context of this.paper? Why is the Chars language a shallow embedding? As far as I can tell it's syntax doesn't consist of functions from the semantic domain but from unique identifiers.
[Here it is.](https://fnpaste.com/pgjq) (Only the differentiation part because the simplification part is kinda messy and incomplete) The differentiation part is pretty straightforward. I'm not sure if there's anything that can be learned from this... I just implemented rules written on a high school calculus textbook. So it only supports univariate differentiation.
Yea. I worded that wrong. Type families can be injective. Your problem boils down to the fact that GHC 7.10 doesn't know that
Oh wow sorry I totally misread. I was confused, my bad =P
[one line of code and twelve tests](https://github.com/ghc/ghc/search?utf8=%E2%9C%93&amp;q=%22may+not+be+injective%22&amp;type=Code)
Yeah, I want to know who decided to use 'functor' in this way for OO. Have a little... "discussion" with them.
Hi, I am an author of this paper. I have been referred to here by some of my colleagues. Provided we have submitted the paper just yesterday, I have to admit I did not expect it to receive much attention online, this fast. Thank you gallais for posting it here, and thank you Redditers for picking it up. I'd be happy to reply to your questions, listen to your comments, and join in your discussions. If you also leave your name, I make sure to acknowledge you in the paper. 
Thanks, this goes some way toward clearing it up! (I knew about soundness and completeness; it's the relationship between soundness and consistency which puzzled me.) Is there really such a neat separation between termination checking and type checking? Many systems like the simply-typed lambda calculus, System F, and (without Type:Type) MLTT are type-safe *and* logically consistent, without any separate notion of termination checking. I guess once you add a recursion construct, then you have a split between systems which check it for termination and are consistent (Agda, Coq), and those which don't and aren't (Haskell, ML). Is that the whole story, all there is to it? I feel like there should be some deeper theoretical connections or explanations which I'm missing and would like to learn about, but I'm having an annoyingly hard time putting my finger on what they might be. Oh also, (as I just remembered upon looking at my notes from earlier) there are apparently two separate notions of soundness and completeness in logic, and one of them is sometimes called "consistency". I'll just paste my notes in: &gt; Soundness vs. consistency &gt; Theorem-proving vs. program-writing type systems &gt; Void cannot type-check vs. Void cannot exist at run-time &gt; {syntactic,semantic} {soundness,completeness}, except "syntactic soundness" is called "consistency" &gt; syntactic soundness -&gt; forall P, does not prove both P and not-P &gt; syntactic completeness -&gt; forall P, proves either P or not-P &gt; semantic soundness -&gt; forall P, if it proves P then P is true &gt; semantic completeness -&gt; forall P, if P is true then it proves P &gt; http://scientiststhesis.tumblr.com/post/62075544836/consistency-validity-soundness-and-completeness &gt; http://cs.stackexchange.com/questions/24700/consistency-and-completeness-imply-soundness &gt; http://www.scottaaronson.com/blog/?p=710 &gt; http://math.andrej.com/2007/04/08/on-a-proof-of-cantors-theorem/ From your reply, it's not immediately obvious to me whether the usage of "sound" and "consistent" by type theorists tracks with one or more of these. The notions of soundness and completeness seem to be the semantic ones. But then whence "consistency"? (Also seeing this again gives me a better inkling of what I feel I'm missing, from above. What it means when we say a logically-consistent system "proves" a type (proposition) is fairly obvious. The type is inhabited and if you run the proof you will get the inhabitant. In logically-inconsistent but type-safe systems, you can prove things which are false, but it's still guaranteed that if you run a proof and it terminates, the inhabitant you get will be "valid"... but what does this *mean*, logically? What is its significance? This understanding of it feels awfully low-level / operational. And on another hand, the typechecker of a type-safe but logically-inconsistent language is still clearly *proving*, *something* about an accepted term/type... not that it's *true* or *inhabited*, but then *what*? Maybe part of what I feel is missing from my understanding is that, unlike logical consistency where it's obvious why it's important, I don't feel like I grok why mere "type safety" without consistency is something important, and why we care about it, apart from the fact that we happen to find it empirically useful in practice. Agh maybe I'm not actually missing anything and just confusing myself by feeling like I am.) Sorry for rambling so much!
For the meaning of the term "Compositionality" refer to the following link: https://en.wikipedia.org/wiki/Denotational_semantics#Compositionality &gt; Why is the Chars language a shallow embedding? &gt; As far as I can tell it's syntax doesn't consist of functions from the semantic &gt; domain but from unique identifiers In shallow embedding, syntax consists of functions *to* the semantic domain. In the case of Chars language in Section 3.1, syntax consists of a set of functions *to* list of characters, i.e., the semantic domain. 
The OP is not an author of the paper, but I find it very useful to write comments as if talking to one of the authors. You should feel free to send them an email directly to provide this valuable feedback. I think it could also be very helpful (to the authors, and also interesting here) if you could give some additional pointers to these "other EDSL work" that you think are notable and/or relevant and are missing from the paper.
well, Haskell *always* allowed `undefined * 0 = 0` for instances...it was always allowed! :O But the issue here is that in 7.10, that's the behavior for the `Integer` instance.
Still interested regardless, if it turns out that I have a free summer...
Some interesting facts there about Haskell, particularly that Haskell is "39.6% less trending" than last year. What happened?
Thank you for your interest and your detailed comments. &gt; This is partly addressed with [...] techniques [...] &gt; I have heard of approaches that [...] &gt; [and the like] I agree that there is a wide range of tools and techniques that try to address the issues arising in embedding. Without a doubt. many of them are successful, at least partly. The point that the referring section in the paper is trying to convey is that there is still need for a different class of study, independent of tools and their implementations. The two--the practical tools and the theoretical insights--are complementary: practical tools are blind without theoretical insights, and theoretical insights are useless without a concrete realisation. We mention a few instances of such blindness in tools (e.g., extracting optimised code for programs involving conditionals and primitives), and I believe you can imagine of more instances. The theoretical insight offered in this paper is a step, no matter how tiny, towards understanding a specific class of existing embedding techniques, not a tool, nor a concrete technique. &gt; However, that also means the paper gets a kind of survey-ish nature, and &gt; in that case I think it is missing a lot of related work on embedded DSLs. I suppose there is no need to survey all the major existing works in the two areas to present a correspondence between parts of the two. One often needs to pick the right and enough number of instances on the two sides, and show that they follow the correspondence. If done correctly, experts on the two sides can pick up the work and relate the other existing works, when they need it. Having said that, I'd be happy to hear about specific related works that you may suggest, and add them to the related work section. &gt; but because rewritings (like calculating Add(Constant(1), Constant(3)) to &gt; Constant(4)) are usually kept simple and don't aim at fully covering &gt; everything. Just my impression though. I believe you have a right impression here. If you view NBE as a process that forcefully normalises everything, when in cases may not want it to do so, then it is too heavy of a tool to use. However, notice NBE process does not need to be so aggressive in the normalisation process: (a) a different (e.g., weaker) equational theory can be considered, (b) one can benefit from the residualisation process to leave some parts uninterpreted, and (c) a chain of multiple simpler NBE algorithms can be applied rather a large one. For instance, algorithms in Section 4.1 and 4.2 actually leave literals and primitive operations uninterpreted, while section 4.3 introduces a simple customisable framework for implementing rewrites similar to the ones you mentioned. 
[removed]
Does `evaluate` work? import Control.Exception do y &lt;- evaluate $ id x p''' &lt;- makeStableName y return $ p == p''' From the documentation (https://hackage.haskell.org/package/base-4.8.2.0/docs/System-Mem-StableName.html): &gt; Makes a StableName for an arbitrary object. The object passed as the first argument is not evaluated by makeStableName. (Laziness is tricky, your BangPattern might be enough). https://hackage.haskell.org/package/data-reify and https://hackage.haskell.org/package/data-treify are observed sharing libraries that might help. Also, watch out for `Int`s. I think GHC shares small integer literals, like Python. 
Hmm, on second thought, please ignore my previous comment. I like to answer questions which are just a bit above my current knowledge level, because that forces me to improve my knowledge by researching my answer until I'm confident enough in my research to give an authoritative answer. However, it's now clear to me that you did a lot more research on this subject than I did in order to answer you, and so you're probably much better off sticking to the results of your existing research and ignoring what I said. &gt; Many systems like the simply-typed lambda calculus, System F, and (without Type:Type) MLTT are type-safe and logically consistent, without any separate notion of termination checking. Yes, and it's amazing! So many core languages in which it just so happens that every well-typed term also terminates, eliminating the need for a separate termination checker. And it's not just a trivial consequence of the fact that those languages don't include general recursion as a primitive: the untyped lambda calculus doesn't have a recursion primitive either, but you can implement one via the Y combinator. I think that even though those languages seem simple and natural, they're probably a few gems among many other similarly simple and natural-looking rocks, and their authors must have examined many rocks before finding and publishing those gems. &gt; if you run a proof and it terminates, the inhabitant you get will be "valid"... but what does this mean, logically? Program evaluation corresponds to proof simplification. So a program which doesn't terminate would correspond to a proof which can be simplified over and over again without ever reaching a normal form. Since normal forms are used to prove that logics are consistent (that they do not allow a proof of ⊥), I'd say that such a logic is likely to be inconsistent. General recursion, in particular, corresponds to a form of induction which isn't well-founded, and it's easy to prove falsehoods using that. On the subject that normal forms are used to prove that logics are consistent, see [Frank Pfenning's Proof Theory Foundations lecture from Oregon Summer School](https://youtu.be/nw0JAF79gYI?t=31m56s) for a very thorough explanation. &gt; the typechecker of a type-safe but logically-inconsistent language is still clearly proving, something about an accepted term/type... not that it's true or inhabited, but then what? Why does it have to prove anything? It's verifying that the term is well-typed, that's all. Then if the language has a formal semantics, there is probably a proof of type soundness, saying that well-typed terms never evaluate to a type error. So I guess you could say that by filling up the prerequisite for this proof to go through, the type checker is proving that the term will never evaluate to a type error.
I see. When you said singletons for some reason I thought of `Proxy`.
parsec's dependencies are base (&gt;=3.0.3 &amp;&amp; &lt;5), bytestring, mtl, text (&gt;=0.2 &amp;&amp; &lt;1.3). base-3.0.3 is the oldest version of base listed on hackage, so it must be pretty old.
Wow this will be a good use case, Thanks. edit: wording
Hmm, time for me to subscribe to that tag on SO I suppose! Note that it is fairly hard to find an unanswered stack question which isn't either answered in the comments, or where the asker didn't respond with more info when asked. That percentage for stack is going to be a little bit less now, [I](http://stackoverflow.com/questions/31812379/stack-install-setup-hs-does-not-exist) [did](http://stackoverflow.com/questions/35621869/stack-build-fails-due-to-missing-package-although-stack-ghci-works/36075204#36075204) [just](http://stackoverflow.com/questions/36051106/using-stack-behind-proxy/36075165#36075165) [answer](http://stackoverflow.com/questions/33304155/how-to-automatically-build-test-modules-with-stack/36075099#36075099) [these](http://stackoverflow.com/questions/35600243/stack-interpreter-option-add-an-external-dependency/36074954#36074954) [ones](http://stackoverflow.com/questions/34109477/haskell-stack-build-error-no-local-directories-found-as-children/36075086#36075086).
Gasp, people are dropping Haskell for front-end dev? It only takes a couple fewer questions to drop by dozens of percentage points I suppose... ;) Haskel front-end FTW!
It should be noted that with extremely high levels of abstraction, as time goes on your number of new SO questions will begin to approach zero.
I choose to interpret this as Haskell's documentation and type-errors have improved, so there is less need to ask questions.
&gt; `F` is a type function and may be not injective
It's just a broken link, it should actually be https://hackage.haskell.org/package/base/docs/Control-Applicative.html
Isn't this somewhat similar to [cs240h lab 2](http://www.scs.stanford.edu/14sp-cs240h/labs/lab2.html)?
Hmm... I want to say I'm game to work on this, but I'm somewhat overloaded with projects already. What do you think has to get done in order to have something that works?
I tend to bug people who answer in comments to submit them as real answers. It's also apparently considered cricket to replicate their comment-answer in your own answer as long as you mark it community-wiki so its clear you're not on a rep-hunt.
Is this in reference to the stackoverflow question I submitted really recently? While I agree with you that it is a little unintuitive and breaks reflexitivy I don't think it is worth fixing due to the pretty significant costs. Maybe add a pragma for it but probably not default behavior. IMO as long as the side that evaluates first is the opposite of it's associativity everything works ok. (: is infixr and the left side evaluates first, * the opposite).
Was the choice based on the associativity of the operator (for situations where it is an infix operator).
Well, how much confidence do you have in the current type system? When you have `x :: Integer`, how confident are you that you didn't write a crashing/hanging program? Other than doing something silly like using partial functions from Prelude or undefined, the only way I can think of to make a bad but type-checker-passing construction would be to make a non-terminating recursive call. Obviously I don't have Agda/Coq levels of confidence that a Haskell expression is actually valid, but that doesn't mean I'm still not pretty confident, especially compared to how un-confident I am in the rest of the development stack (docker scripts, network endpoint matching, etc.). IMO the biggest problem with dependent Haskell isn't its theoretical inconsistency, but that it's really ugly and impractical compared to Agda.
Thanks for your suggestions! I have [created two screencasts](https://github.com/sunaku/dasht#preview) (one in terminal and another in GUI) accordingly.
SO is pretty hopeless as a source for learning haskelly things, perhaps people just gave up on it. One is much more likely to get a competent and prompt advice on this subreddit than on SO.
I wonder how shorter it can get if using more of existing libraries, like conduit or STM.
It's just that I browsed around some of the related work in the references and found some papers with EDSL keywords in the title, but I admit I have no idea whether they are actually notable/relevant as I am only tangentially interested in the topic.
Where did you get this notion from?
I know. It would be nice to be able to write those functions in the name of full abstraction. But the price is quite high. 
It was inspired by it. This proposal was more like an additional feature. If it were implemented in Haskel, it would be a language extension probably.
I kindof want to think that I could help quite a few people with their understanding there - I love this subrredit but I don't think there are many questions/answer to help people here - it's either *news*, chatty or really high level stuff (which is great but will probably not help beginners to learn)
Hey, it is even not [exception](https://gitgud.io/hae/ptcpd/blob/7263350b624c47a97e215412182dd77ef4a079cb/src/Main.hs#L41) [safe](https://gitgud.io/hae/ptcpd/blob/7263350b624c47a97e215412182dd77ef4a079cb/src/Main.hs#L53)! It is not so great example for beginners.
I have two pieces of advice: 1. Write large projects (or at least a little larger than you think you're capable of doing right now) that you have strong personal motivation to complete. The best way to expand your capabilities is to work on things that your current self is not quite capable of. 2. Collaborate with and read code written by other people. I discussed this more [here](https://www.reddit.com/r/haskell/comments/25fonf/how_to_get_good_at_haskell/chh49ys).
The current phrasing is &gt; NB: ‘F’ is a type function, and may not be injective so you've just removed the comma? The epistemic/deontic ambiguity of 'may' remains, which was my original complaint.
&gt; What does compositional mean in the context of this.paper? From the horse's mouth: &gt; Above evaluation process is particularly interesting in that it is compositional: semantic of a term is constructed from the semantic of its subterms. The idea is that you can basically associate to each constructor a combinator which takes the interpretation of the subterms and output the interpretation of the whole term. The only extra work needed to add a new constructor to the language is to give a new combinator rather than having to redefine the whole evaluation machinery. &gt; Why is the Chars language a shallow embedding? The papers says that it's the Chars language defined *in Section 3.1* which is shallow. The earlier one is indeed a deep embedding whereas the later one is simply defined as: Chars = List Char e0 = [] m * n = m ++ n Chr c = [c]
No, I switched "may not be" to "may be not".
Are you using the original version on Monad Reader or the newer [wiki version](https://wiki.haskell.org/Typeclassopedia)? If you encounter obvious broken links like those in the wiki version, please take a few moments to fix them if you know what the correct link should be. The corrected link for conal's blog is: http://conal.net/blog/tag/applicative-functor
All the other suggestions are good. Just a comment here. I found that you learn most about large scale design when you modify a bigger system. If the system is well designed, then it will accommodate most changes; however, a bad design will require constant struggle and redesign work to implement a change. Now, because Haskell is so great for refactoring, even badly designed systems can be redesigned given enough effort. I have done that myself, sometimes in drastic ways replacing large amounts of code. Of course, types are not always enough, so an extensive test suite is a must as well. So, to become productive in Haskell (and in any other language), join any bigger project you find interesting and useful and begin changing it. :-)
You can find the SOH client code in "soh-client" directory under the link I gave, see the "stack.yaml" file there to see how it's built (with GHCJS): https://github.com/fpco/schoolofhaskell/blob/master/soh-client/stack.yaml
But it will also affect functions that were possible before. It would be cool, but expensive.
But what about `fix (1:)`? If it evaluated the right side first that would never produce any values. Also `undefined * 0` is 0 and `0 * undefined` errors.
My infallible logic. ;)
Yeah, this merely means that people aren't using SO for Haskell as much. It just so happens that Haskell has one of the most thriving IRC channels, however.
By convention `k` tends to be used for functions that are in some sense "continuations" or sometimes "constant". If `f` is ruled out, `g` is the usual next choice for a function name.
Which (*) are you talking about? Each numeric type has its own. For, eg, Int the evaluation order of the arguments is unspecified, since it doesn't matter. 
* These are averages over the respective lifetimes * Two different (albeit related) technologies * Two different points in time I cannot even .. 
I'm not sure "may be not injective" is grammatical; "may be non-injective" would work.
This idiom of adding a parameter for the content is what the [wreq](http://hackage.haskell.org/package/wreq-0.4.1.0/docs/Network-Wreq.html#t:Response) library does. I liked this talk. **Edit:** Webpage could be made a comonad as well. But what would be the meaning of [duplicate](http://hackage.haskell.org/package/comonad-5/docs/Control-Comonad.html#v:duplicate)? Perhaps a function that takes a webpage and returns a webpage that displays the source of the original one!
&gt; When you have `x :: Integer`, how confident are you that you didn't write a crashing/hanging program? More than in Python, less than in Idris. &gt; the only way I can think of to make a bad but type-checker-passing construction would be to make a non-terminating recursive call There's also non-productive co-recursion (a fancy way of describing `safeTail [1..]`). &gt; IMO the biggest problem with dependent Haskell isn't its theoretical inconsistency, but that it's really ugly and impractical compared to Agda. I'm guessing that you implicitly meant "for theorem proving"; otherwise I'd vehemently disagree with your statement. I shudder at the idea of using Agda for a quick web scraping task.
A coworker of mine recently remarked that he's had the same XMonad configuration for the past 5 years.
Slides: http://nocandysw.com/add-a-type-parameter.pdf
I really don't like the fact that exception are everywhere in IO, this is against everything about security, type safety and no surprise. I understand for async exceptions, but I cannot for all others.
&gt; I'm guessing that you implicitly meant "for theorem proving" Of course, that's what I tried to specify by saying "dependent Haskell." Reluctant as I am to admit, it would probably take me a good week or more to make a web scraper in Agda (and even then it would be cheating by using Haskell libraries!)
What's PureScript like? I'm on ClojureScript now and love it, but miss static types. 
Use stack to compile your top-level program and add to your stack.yaml: flags: vector: boundschecks: false Now you can use the exact same package, no need for a `-unsafe` variant.
Couldn't have said it better myself.
The author of `TypeInType` here. There's tons to improve. First on my list is adding proper dependent types, which will remove the need for singletons. I plan on writing up my plan in my forthcoming dissertation, due out this summer. Along the way, I believe I'll be able to introduce unsaturated type functions, and other niceties to type-level programming (`case`, lambda, etc.). After that, I'm really interested in taking the fantastic usability of LiquidHaskell and seeing if dependent types can be made as easy-to-use. A key step along this path is lightweight existentials, allowing you do use `exists x. ...` in a type as easily as `forall x. ...`. Note that a refined (as in "refinement types", which are the basis for LiquidHaskell) result type of a function is really an existential. We also need much better error messages and user interaction. And to be able to use an SMT solver. And then some.
Hmm... Do you have a single example of when (:) acts like it evaluates the right side first. Because everything I have tried (last / reversed / fix etc. combinations for example) indicate that it acts lazy for the right and never the other way around. Also what about `*`
Integer I guess.
 Thanks. That is a super helpful example. 
3 things that made me happy: 1) Python, Haskell, and Rust are all in the most loved languages. 2) Vim is dominating Emacs. 3) "If OS adoption rates hold steady, by next year's survey fewer than 50% of developers may be using Windows." On a side note, is JS really that great? Why is it so popular? 
Does anyone know of any benchmarks or measurements demonstrating how much faster the `unsafe` versions are? Or any ballpark estimates?
[removed]
It's similar yet different than Haskell, It's very good IMO and it keeps improving in giant steps. Why not [try it](http://try.purescript.org/) and see for yourself? :)
Impossible to quantify without a particular use. I know donsbot had some concrete numbers for a microbenchmark on his blog at one point.
(:) is non-strict in both arguments. Consider, `null (undefined : undefined)`. Any function which considers the shape of a list but not its content evaluates the right side first, like `length (undefined : [])`.
If you're looking for free/libre/open community projects to contribute to, the one I'm involved with, [Snowdrift.coop](https://snowdrift.coop) works hard to be as welcoming to new contributors as possible. We're currently rebooting the code and getting a fresh start to clean up things on the way to launching, and we're currently sorting out the optimal tasks for everyone to be hacking on going forward. If what we're doing looks interesting to you, feel free to get in touch.
Ooh, nice, thanks for the link. Going to try it out :) Ahhh so much elegance. It does have huge appeal to me. I am very tempted to use it as the front-end for my next project, but there's going to be a lot of state and components involved in that, so that's what's making me concerned a little bit. So that's my concern at the moment, for ClojureScript I have reagent with re-frame to manage my state and components and seperate the M from V from C, and I find the codebase with them is easy to maintain over time. Is there such a framework for PureScript? 
Richard, I highly value your work. This is what Haskell needs. The lack of proper dependent types, unsaturated type functions and existential quantification are the main pain points of Haskell that I and our team at work encounter. It will be a real step forward for Haskell if the language gets all of those features.
You may want to make the `l` in `Variant l` a representational type parameter: type role Variant representational Otherwise GHC thinks it's a phantom type and allows arbitrary coercion by `Data.Coerce`, and also allows unsound type family + type class usage.
You may want to browse http://matrix.hackage.haskell.org/ to find packages which compile on all GHC versions back to GHC 7.0 you may also want to avoid packages with many dependencies though as those dependencies may change depending on GHC version, and therefore make results less comparable (in addition to the CPP issue /u/ezyang mentioned).
IMO this was an incredibly well-done talk. Light on implementation detail (which was *good*), but really trying to drive home to point of the true power of real parametricity. (However, I fear that I may be suffering from the "curse of knowledge" and so may not be able to evaluate it properly, i.e. evaluate it as someone who hasn't been exposed to these ideas before... and to see what they would get out of it.)
&gt; Write large projects (or at least a little larger than you think you're capable of doing right now) Sooo much this. You should always do something that you think is *just* out of reach. When you actually do it you'll find that usually, with a *bit of stretching*, you can actually reach it! (Incidentally this doesn't just apply to programming.) EDIT: Yes, sometimes you actually fail to reach it... that's just part of life. Try again, but don't try *exactly* the same thing and expect a different result. That's the sign of a madman. Try *variations* instead.
He's using io-streams instead :)
Funny, because every time _I_ see an error message like &lt;interactive&gt;:22:15: Couldn't match type ‘F a’ with ‘F a0’ NB: ‘F’ is a type function, and may not be injective The type variable ‘a0’ is ambiguous Expected type: F a -&gt; b -&gt; F a Actual type: F a0 -&gt; b -&gt; F a0 In the ambiguity check for the type signature for ‘constF’: constF :: forall a b. F a -&gt; b -&gt; F a To defer the ambiguity check to use sites, enable AllowAmbiguousTypes In the type signature for ‘constF’: constF :: F a -&gt; b -&gt; F a I just ignore whatever alien hieroglyphs have appeared before me and fix the bug on line 22. &lt;/ghcerrormessagesareugly&gt; 
[removed]
Nice! Looks useful, do you plan to publish it to hackage?
Thanks for sharing, this is useful :) I've been watching Bodil Stoke's talks about PureScript too, and wow, I think I'm sold. 
I don't need it right now, but I can imagine it being useful in the future. Take your time!
Thanks to everyone who contributed!
Thank you and Sönke for actually making it happen. 
Shouldn't the User be retrieved from the session data instead of the username/password combination? How does servant handle sessions?
This is the first offering of the class and as such is expected to be quite experimental. Future plans may indeed include offering the course online in some format once the material and presentation of the course are more mature. To start with, we figured a hands-on approach would be good. If and when we offer other formats, I'll be sure to post here!
Really wishing I lived in Seattle right about now...
So for Basic Auth the user is supposed to supply a username and a password for every request he makes? This seems strange.
And if you run your code with 'runhaskell -Wall' you'll see that `number1` shadows the existing binding (the one in the let). So the case line `number1 -&gt; putStrLn "value = 1"` is binding the name `number1` to the value of `value` and then returning the value `putStrLn "value = 1"`. If you use instead: `print value`, you'll see a `2` in the output. More generally, the cases of `case` are *pattern matched* with the constructors of the type of the *cased* value. E.g.: let b = True in case b of True -&gt; print "true" False -&gt; print "no true" would print "true". But (note the b=False) let b = False in case b of foo -&gt; print "true" False -&gt; print "no true" will also print "true" because, as foo is not a constructor of Bool (the type of b), what haskell does is to bind the name `foo` to the value of `b` and print "true". This is useful when you don't want to match against all the posible constructors. Eg. data X = A | B | C | D | E deriving Show let x = A in case x of A -&gt; print "abacus" B -&gt; print "boa" letter -&gt; putStrLn $ "I don't have a word for the letter: " ++ show letter Edit: formatting
JS isn't popular because it's great, it's popular because it's the only language that runs in a web browser. If it were great, there wouldn't be quite so many languages which compile to it.
At least to me it wasn’t clear from reading the announcement if the basics auth is refering to the http basic auth or if it’s just some arbitrary authentication mechanism that is subject to some limitations and thereby basic. It might make sense to point that out more clearly in the announcement/documentation.
Congratulations! I'm really excited about this work.
You might also want to check out the [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) which is sort of like the Haskell analog of "design patterns". It covers several very commonly used type classes that are used to structure and organize larger Haskell applications.
Aside from the awesome work done on Servant, these are some of the best written release notes I've ever seen!
If you made a constructor - like function that was simply an alternative way of constructing a data type. Say perhaps a new symbol that build lists from the back (despite the inefficiency) would that work?
I don't have an exact problem in mind, that's why I'm seeking potential ideas here. However, it will be related to The JavaScript Problem (link is given in the opening post). 
It might have been worth mentioning the geographic constraint in the subject.
The only thing that `null` does is check to see if its argument was constructed with `(:)` or `[]` ([here's its definition](https://hackage.haskell.org/package/base-4.8.2.0/docs/src/GHC.List.html#null)). Here is an example of a difference in bottom behavior between `(:)` and `snoc`: ghci&gt; null (undefined : undefined) False ghci&gt; null (undefined `snoc` undefined) *** Exception: Prelude.undefined Given the definitions of `null` and `snoc` above and [the definition of `(++)`](https://hackage.haskell.org/package/base-4.8.2.0/docs/src/GHC.Base.html#%2B%2B) (++) [] ys = ys (++) (x:xs) ys = x : xs ++ ys we see the reduction: null (undefined `snoc` undefined) = null (undefined ++ [undefined]) At this point, `(++)` tries to pattern match on its left argument, which is why the error gets thrown. Since Haskell is based around pattern matching (pattern matching is more fundamental than `(==)` and, if I remember right, GHC actually uses a sort of pattern match graph instead of a traditional function call stack), I don't think it is possible to have a non-trivial, non-constructor function that is completely non-strict in all its arguments.
Oh wow. Yeah that is actually pretty neat. Is it possible to define alternate constructors that can be interchanged? Perhaps again back to my list example. By that I don't mean a function that does its best to act like a constructor but an actual second constructor specified on the type. Somewhat similar to the deque ViewL and ViewR stuff but without the need for views.
i had a go at this a couple years back for what i think is the equivalent of a senior thesis with [this](http://kjgorman.com/static/report.pdf) -- using UHC to compile haskell to javascript + some runtime for an FRP based framework for webapps. it was fun and i learnt stuff but probably not particularly useful output overall -- at the time it did about the same amount as elm, which has gone on to be much more pragmatic. anton ekblad's masters thesis might be worth looking at too
You may want to find a way to help out with [GHCJS](https://github.com/ghcjs/ghcjs). It's a really great project, and there's a lot of interesting areas for improvement. I'd recommend speaking with [luite](https://github.com/luite) about it.
Isn't `ExceptT` in transformers?
https://youtu.be/kHEriteFMb0 is a very similar talk about how the adhering to the YAGNI principle is not a good idea in FP.
Where does one declare these flags in the Hs source?
CT stands for abstraction, and is applied on something already existing, so you don't need it per se. but at some point, CT machinery simplify things a lot, and helps to formally reason about programs
I think this comment is entirely correct (it is a comment somewhere inside a reddit discussion I haven't read about a blog post I haven't read, so I won't judge the context but only the content). I have one reservation on the "condescending" aspect -- I think this is a complex question and I don't think one can relate social behaviors to scientific theories. The post is also very informed -- not every comment makes a point using synthetic topology in passing, and I don't mean this as an argument by authority but as an argument (did we need any?) that the message should be respected for its opinion -- "not even wrong" is not even a wrong reaction to this post. What the post does *not* cover is that type theory (and category theory to maybe a lesser extent) has been an amazing vehicle for programming language design, and that it has an intrisic value in being more modular, by construction, than most of the work on software verification that comes from the "program analysis" tradition. We really need both brands of work, and I think type theory has *deserved* its situation as a cornerstone of programming language design -- although it would be very dangerous to have a closed, dogmatic view of the topic, as in general the creativity in design of operational mechanisms does *not* come from the static semantics. I have expressed in the past my opinion that category theory is often over-rated, in the Haskell community, in that beginners and exteriors have the wrong impression that it is the one theory that they should invest their time understanding to get a more rounded view of functional programming -- sometimes to the detriment of studying how to formally describe the static and dynamic semantics of programming languages, which is equally rewarding. On the other hand, the categorical viewpoint is a formidable research vehicle (and in "research" I include here some of the musings of the Haskell community, such as lenses or pipes for example, that are less academic in their presentation). And the interest in mathematics and algebra in particular that its marketing has ignited in the programming community is precious.
Another way to do this is `Rank2Types` (here's a basic [gist](https://gist.github.com/ChristopherKing42/0b75680f378bbb4a46af).)
I am so excited about this release! Three things in particular: 1. The removal of matrix params. I have been working with websites for years and I've never seen them before. [Removing them from Lackey](https://github.com/tfausak/lackey/pull/2) deleted about 1/4 of all the lines of code in that package. 2. Passing `BaseUrl`s and `Manager`s to Servant clients. This means you can consume the same API on multiple hosts. And you can log all requests sent through Servant. 3. The release of `servant-foreign`. I haven't used it yet, but so much of the boilerplate is the same between Servant client libraries. It will be nice to reuse it! Great work, everyone! 
&gt; Perhaps it could be recovered How would that work? I've thought about this when considering how you would build up a very typesafe route tree like Servant, but the problem with `do` notation is that you have to be in the same monad all the time - `&gt;&gt;=` can't change the type of the computation like `flowSeq` does.
I tend to find that Oleg's versions of the same [first version](https://mail.haskell.org/pipermail/haskell/2004-November/014939.html) and [second version](http://www.mail-archive.com/haskell@haskell.org/msg19544.html) have more features, as well as being quite straightforward. The OP's code, BTW, is only correct because it (essentially) only allows differentiation of analytic functions. If partiality (like division) is allowed, then this technique falls apart, and starts to give incorrect answers. Same with discontinuities. It's always amusing to differentiate ln(exp(2*Pi*I*x))/(2*Pi*I) and get 1 instead of something which sees the infinite number of discontinuities in the input.
CORS The idea is that IOT devices interfacing with browsers shouldn't have to deal with huge media assets. Those should be stored on something like AWS S3. Make a simple workflow to have your IOT server use cross origin image assets that the browser pulls down in Elm from another server. I'm not sure of an Elm library that does CORS handshakes yet. MVP would have the browser CORS upload an image to S3, and CORS download it all with an Elm wrapper so the developer doesn't have to write JS. http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html http://www.html5rocks.com/en/tutorials/cors/ https://devcenter.heroku.com/articles/s3-upload-python
There's also infernu, written in Haskell: https://github.com/sinelaw/infernu I believe it is superior to Flow when it comes to inference. I'm sure Noam would love some help with the project.
I assume the `deb7` at the end of the filename means that it is packaged for Debian 7? Unfortunately I am not running a Debian based distribution. That's why I compiled it in the first place.
No, I am talking about the release candidate.
It's a tarball, so that probably just indicates where it was built. I would at least give it a try. I built my own, and used ghc-8... as the resolver instead of stackage nightly. It worked, but I had to deal with a few packages on hackage that needed relaxed constraints for ghc 8. Some of them I could just point to GitHub, and some I had to modify. 
If you want to use GHC 8 with stackage nightly (or LTS) you will need to add any built-in packages that have differing versions from the snapshot as extra-deps.
Fanning out?
Wow, that is an impressive list. `Generic` is my favorite Haskell feature. I've never used any other language with a compiler able to so elegantly remove so much boilerplate. It's absolutely fantastic.
Do all the packages support GHC 8? If not you can try allow-newer but no guarantees that it will actually work.
It's also just a really nice way to set up GHC/packages, and use Stackage. 
By default, hoogle (https://www.haskell.org/hoogle/) will search only standard libraries. To determine the version you have on a platform, you can run `ghc -V`.
&gt;Arch to Ubuntu It depends on how you installed Haskell. traceShowId was added in 4.7 of the base package, so I suspect you have the obsolete haskell-platform package. I strongly recommend you remove that and follow /u/Tekmo's advice to use stack instead. It's a sort of one-stop shop that takes care of installing ghc, sandboxing, and fetching dependencies for you.
I wondered how long it would take before we would get an extension to remove the boilerplate (Proxy :: Proxy xxx). We won't have to wait for a long time!
This. Particularly take note of the `resolver` field. Also note that all of your dependencies will need to be specified in `extra-deps`.
i found that... I installed minGW and tried that instead. it fails on a different error: http://pastebin.com/mdHcnQpx
&gt; you could have simply put your error-handling code around each function. The whole point is to avoid doing this, without using exceptions, with the help of the compiler (as with checked exceptions in Java). For instance, I have some IO code where almost every function may fail if a device has been disconnected. I don't want to handle the error locally, instead I want to catch any DisconnectedDevice error with a surrounding `flowCatch` (like an exception) and I want a compiler error if I forgot to handle it.
NBE, and reduction-free normalisation, are generally well-studied in category theory (e.g., see the lines of work by Altenkirch, Dybjer, Fiore, and their colleagues), often in terms of presheaves, or Yoneda embedding. About the particular connection between normalisation and adjoint functors, I know of a few related lines of work. For instance, Sabry and Wadler (1997) employ reflections--a specific form of adjoints--to study and compare reduction theory of multiple lambda-calculi. Hopefully, via EBN, some of these observations in categorical setting can be put to work for EDSLs. Thank you for your comment.
[removed]
MinGW will not help you, you need POSIX support. Cygwin seems to be only option since POSIX support was dropped in Windows 8.1 or so.
Ah, yes thank you very much! I looked through the docs and google, but for some reason I didn't find that page.
That's correct. There's even more overhead, since the `Union` constructor adds more pointers into the chain (the complexity is still `O(n)` though). I hesitate to apply any optimizations until I've got benchmarks.
You're welcome!
&gt; Is there a way to search the standard library and only the standard library? What exactly is a "standard library"? - There's the [`base`](https://hackage.haskell.org/package/base) library, which is the library that includes the standard [`Prelude`](http://hackage.haskell.org/package/base/docs/Prelude.html) and among other modules. The `base` library is tied intimately to the version of the compiler and cannot be upgraded by itself. - There are the [boot libraries][1], which are the libraries needed to build the compiler and are generally bundled with compiler itself. But even these libraries can be upgraded independently of the compiler, so they are not all that special. &gt; Also, I've noticed that sometimes my Haskell installation doesn't have certain things (like for example traceShowId in Debug.Trace). I've especially noticed this discrepancy when going from Arch to Ubuntu. Arch Linux is bleeding edge, so it will always have the latest stable version of the Haskell compiler, whereas Ubuntu/Debian will lag behind by a few months to a few years. You can find the version of your compiler by running `ghc --version`. From this version, you can infer the version of the `base` library using [this table][1]. If you use Cabal, you can determine the versions of other installed packages using `cabal info &lt;package&gt;` [1]: https://ghc.haskell.org/trac/ghc/wiki/Commentary/Libraries/VersionHistory
&gt; By default, hoogle (https://www.haskell.org/hoogle/) will search only standard libraries. ~~Err, [what](https://www.haskell.org/hoogle/?hoogle=Vector)? AFAIK hoogle searches throughout _all_ of Hackage. I think that /u/minesasecret means `base` by "standard library".~~ As /u/sclv said in a comment below, Hoogle actually searches only in `+platform` for bindings, but in all Hackage package names. So if you search for `acme` you will get all `acme` packages, but if you search for `numberwang`, it won't show the binding in `acme-colosson`. Sorry for the confusion. One can add `+&lt;packagename&gt;` to the query to get only bindings exported by modules in `&lt;packagename&gt;`, e.g. [`take +base`](https://www.haskell.org/hoogle/?hoogle=take+%2Bbase). In Hayoo one has to use the `package:` prefix, e.g. [`take package:base`](http://hayoo.fh-wedel.de/?query=take+package%3Abase).
Yes, one can just query `+base`. And one can query `+` for lots of hackage packages. But Hoogle actually only currently searches by default a limited set of packages -- I believe platform libraries, which are more than base, but _far less_ than all of hackage.
I'm surprised that the bit fiddling is still in these libraries. All CPUs can switch byte order in a clock cycle or less these days. This capability was introduced in the 80486 in 1989. We're pushing 30 years since this was solved, but this is still a performance issue.
OK thanks for the update. I was expressing my view of where I hope we'll get. If you disagree only with how far we have gotten so far, I am very encouraged.
That's true now. I hope that will not be true in the future.
That's task parallel rather than data parallel. Whether you want monad-par or something like DPH (assuming it worked) really depends on the application. 
I'd go for MSYS, not Cygwin (MSYS will give you shell / automake &amp; co ...; cygwin is more heavy, introduces dependency on cygwin-specific DLL, and unless you need X[windows] or something as exotic, you won't need it). A few years back it sufficed for the unix package [as well as all other [C/C++] libs I built by hand]. [EDIT:] You'll need to run 'cabal install unix' from the MSYS shell !
Haskell ain't yet ready for full dependent types, although we are getting close. Take a look at [idris](http://www.idris-lang.org/) (a language similar to haskell but with full dependent types). The [idris tutorial](http://docs.idris-lang.org/en/latest/tutorial/index.html) will also review dependent types for you.
Check it out, I made a working example https://gitlab.com/snippets/16759 At the very end you can see that you don't lose do notation. If the type signature is removed, however, it doesn't work. Also, if you add more signal types in the signature it will still work. If you have two computations that doesn't use the same signal types you can lift them both into a computation that uses a union of their signals using the `liftSignal` function. computation :: Checked [String, Bool] Int computation = do a &lt;- return 3 if a &lt; 2 then signal "failed" else signal False return 5 computation2 :: Checked [Bool, Int] Int computation2 = return 3 computation3 :: Checked [Bool, String, Int] Int computation3 = do n &lt;- liftSignal computation m &lt;- liftSignal computation2 return m
vty has never supported windows (or as it says in the README: "cygwin only"). Though, here is [work in progress](https://github.com/coreyoconnor/vty/pull/1). But reading a character without needing to press enter is easy: [System.IO getChar](https://hackage.haskell.org/package/base-4.8.2.0/docs/System-IO.html#v:getChar)
Agreed. I never start a project that I know I can complete. This is the absolute best way to stay interested in ... uhh... life.
I needed the record field overloading.
Cabal flags aren't for declaration in Haskell source code, they are a higher-level construct than that.
Ok I will try it out. Thanks!
Without hSetBuffering stdin NoBuffering (https://hackage.haskell.org/package/base-4.8.2.0/docs/System-IO.html#v:hSetBuffering), it will probably still require whole line (at least [the char &amp;] newline) to be entered before it will start returning anything. Also check out https://hackage.haskell.org/package/ansi-terminal (ANSI terminal support for Haskell: allows cursor movement, screen clearing, color output showing or hiding the cursor, and changing the title. Compatible with Windows and those Unixes with ANSI terminals, but only GHC is supported as a compiler. )
I can make 2 recommendations I've read after Pierce's *Types and Programming Languages* (TaPL) to get in touch with dependent types. Both benefit from the foundations of programming language theory covered in the TaPL book. 1. The paper from the [*Lambda Pi*](https://www.andres-loeh.de/LambdaPi/) project: *A Tutorial Implementation of a Dependently Typed Lambda Calculus* by *Andres Löh*, *Conor McBride* and *Wouter Swierstra*. It first introduces a formal model and Haskell implementation of the Simply Typed Lambda Calculus, and then adds dependent types to both. The type rule for function application in the dependent calculus was quite illuminating to me: Γ ⊢ e : ∀x:τ. τ' Γ ⊢ e' : τ τ'[x ↦ e'] ⇓ τ" –––––––––––––––––––––––––––––––––––––––––––––––––––––– (APP) Γ ⊢ e e' : τ" Given a dependent function `e : ∀x:τ. τ'` and an argument `e' : τ`, the application `e e'` has the function's result type `τ'` in which `x` has been replaced by `e'`. So the argument expression `e'` get's actually substituted into the result type `τ'` of the dependent function `e`. 2. The free book [*Software Foundations*](https://www.cis.upenn.edu/~bcpierce/sf/current/index.html) by *Pierce et al.* is an introduction to functional programming and theorem proving with the *Coq* theorem prover / dependently typed programming language. The entire book is written in the *Coq* analogue of *Literate Haskell* and features many exercises, which are mostly general or programming language theory specific. 
I'm curious why you call the job interesting. Because you can use Haskell?
i never knew i needed this until now. I use chrisdone's hindent and shm, but sometimes hindent's opinions are a little extreme for me, and thats when align-regex comes out. Not ideal. I wonder if they will play well together. how does your package know what is aligned? will it align to arbitrary symbols, eg ++ or &amp;&amp; ?
I'm not very experienced in the industry, but I'm definitely knowledgable in all their requirements. I'm pretty good with Haskell and backend development, just through personal, non-professional projects and research. I don't think I should apply to this particular job, because I don't have the work experience for the "senior" title, but would it make any sense to reach out to them about a junior position? They don't have one listed on their site, but it's impossible to find companies hiring Haskell developers, and even harder to find junior positions in Haskell. Haskell isn't all I know, of course, but I just really want to land a Haskell job.
Unfortunately yes. The [Kerr metric](https://en.wikipedia.org/wiki/Kerr_metric) doesn't have the same symmetries as Schwarzschild, so it can't be simplified as much as the Schwarzschild metric. Additionally, it doesn't seem feasible to do a Cartesian coordinate substitution to that metric (the resulting geodesic equations would be *very* ugly). But it's not impossible, and some people have managed to do it (google "kerr raytracing"), and I also intend to do it some day.
&gt; I think that even though those languages seem simple and natural, they're probably a few gems among many other similarly simple and natural-looking rocks, and their authors must have examined many rocks before finding and publishing those gems. Interesting perspective, thanks! They seem so simple and obvious... but maybe that's only in hindsight. &gt; So I guess you could say that by filling up the prerequisite for this proof to go through, the type checker is proving that the term will never evaluate to a type error. ...but why is evaluating to a type error "worse" than evaluation not terminating? Why do we care about ruling out the one even when we can't rule out the other? (From a theoretical perspective, if there is one, rather than a practical one like "it prevents security holes".) &gt; So a program which doesn't terminate would correspond to a proof which can be simplified over and over again without ever reaching a normal form. Since normal forms are used to prove that logics are consistent (that they do not allow a proof of ⊥), I'd say that such a logic is likely to be inconsistent. Well put. My question was kind of the opposite: if the proof *isn't* guaranteed to reach a normal form, and so the logic is inconsistent, but you have the weaker guarantee that if it does reach a normal form, it will be type-correct. In other words, type safety. But why is type safety without consistency valuable? What does it buy you? (Basically, the same question as above.) /u/gasche's comment made me think there might be some interesting theory here, but maybe there isn't more to it than what he actually wrote. &gt; However, it's now clear to me that you did a lot more research on this subject than I did in order to answer you, and so you're probably much better off sticking to the results of your existing research and ignoring what I said. The material I found was almost entirely concerning the use of "sound" and "consistent" by logicians, while what I was wondering about was what relation this bore to their use by type theorists, so I'm not sure the former should lead me to doubt that what you wrote about the latter is largely correct. :)
course
horses for coarses
meh, pivoted from bitcoin :-) http://www.coindesk.com/mirror-raises-8-8-million-bitcoin-smart-contracts-trading/ http://www.coindesk.com/vaurum-rebrands-mirror/
&gt;I have wondered why, aside from money, the financial industry has captivated so many functional programmers. Has it captivated them? Or merely employed them?
Are Down Valuation Swaps just around the corner?
I know Szabo it's working with them and they are not doing the obvious straight forward thing it seems from the home page, finance can be actually part of a revolution, I think actually it's the only not working part in out system, that is able to produce a lot, but doesn't have good ways to distribute money to let the machine works...
"Sound" means "correct" in a rather general sense -- an algorithm is sound, a type system is sound, a logic is sound. What we mean by "sound" for a type system is usually runtime safety: that the evaluation of well-typed programs does not reach one of the "bad states" mentioned in the soundness theorem (typically, states where reduction is stuck). What we mean by "sound" for a logic is that it does not prove false formulas. Soundness is relative: we are sound with respect to a given specification, so the context matter. To my knowledge "consistent" has not been used so widely. In the context of *either* a logic or a type system (or a particular typing environment), it means: does not allow to derive an absurdity. Type systems are often thought about as proof principles to prove properties of programs. Proof systems are rather thought about as letting us prove formulas. Both point of views have valuable applications, and I think it would be a waste to subordinate one to the other. "Logical" and "operational" both matter -- and carry signifiance. Operational thinking also matters for logics because a given logic system may not have good compositionality properties. A common example would be the cut-free fragment of a sequent system -- for classical or intuitionistic logic for example. To define programs or proofs in a convenient way, one wants to work in a richer, more composable system, with a cut rule. The relation between this richer system and the original, reference system is established by understanding the operation aspects -- of cut elimination.
Idk if it is really the Haskellers go there or they call the haskellers (chicken and egg problem). However, I always thought the reason Functional, typed languages are liked by financial companies is because of the safety. When you are dealing with other people's money, you wanna try to be a safe and correct as possible. Idk if that's actually the case though. 
So for me markets are a very interesting place to work. There are lots of really hard problems to think about and you meet tons of really really smart people. While what I am working on now is maybe a harder set of problems than getting people to click ads or products, I love thinking through problems like how a market should be structured or what types of contracts someone might want to buy. There is certainly a plethora of technical challenges, but I don't think more than a large scale web company. For me the business problems being solved are simply more interesting to me than other industries that I have worked in.
My work is easier and goes faster with types. YMMV.
The later parts of [this list](http://jozefg.bitbucket.org/posts/2015-08-14-learn-tt.html) might be useful.
Maybe latter this year. Maybe early next. Are you coming straight out of college or have you been working somewhere else? Feel free to PM if you want to keep details private.
You're welcome!
ansi-terminal is what I started out with, and now am back to. in windows seemed to have the same issue with requiring newline. have since switched away from windows to virtualized ubuntu and everything's going swimmingly.
Static typing also aids performance, because the generated code can rely on in-memory structure, among various other assumptions. With dynamic typing, you have to treat all values the same, and this comes with a lot of overhead unless you use fancy jit tricks to dynamically specialize code (and even then there is overhead). In Haskell, the constraint solving portion of typechecking is also automatically plumbing dictionary passing. So, in that case typechecking is writing code for you, allowing your code to be more concise.
&gt; Vacation: Flexible Paid Time-Off policy. As long as you are delivering against your goals and company timelines, take time off when you need it. People who don't meet your goals need time off too. Maybe the goals were unreasonable? Guilt-based vacation seems like the worst idea ever.
Nice! I have added a FlowT monad transformer similar to what you have done.
What have you tried? Have you seen the [utf8-string](https://hackage.haskell.org/package/utf8-string) package ([example](http://stackoverflow.com/a/2089195/1333514))? Have you tried using [text](https://hackage.haskell.org/package/text)?
I don't think we view it like that. We have a very open culture and are constantly accessing the reality of timelines as we collect new information. We are flexible with employees when they needed timeoff whatever the reason, whether its for vacation or other reasons. I personally went to south america for three weeks this winter and didn't feel guilty at all.
If you want something like icu there is the text-icu package which has icu breakers.
I have tried all of those, and they all recognize a Unicode character as 2 Ascii characters. When I split a Unicode string, I got many Ascii characters.
I guess it works for some people, but I like to know how much time I have available.
Adding on the requirement of static typing. In addition to what others have said, strong static typing has benefits in a team environment. The type system can be used to help direct teammates into avoiding the wrong thing and doing the right thing. It's like /u/Tekmo said, the types act as "machine-checked documentation". And we all know how easily comments and documentation can fall out of date, or become plain wrong. Guaranteed-correct documentation becomes really valuable.
Brilliant! And adds a sense of theatre perhaps less present in burritos. :-)
That's correct, and contrary to his claim, any untyped program can be written in a typed language, simply by using a single type for everything. https://existentialtype.wordpress.com/2011/03/19/dynamic-languages-are-static-languages/
For me probably #1 is ease of refactoring. Make a change -- the compiler tells me how that change should be propagated through the rest of the code. Sooooo much easier than in a dynamic language.
One of the methods for learning Japanese kanji is to come up with a ridiculous story to remember its parts until the story disappears and you retain the readings. This felt like the same thing :)
I wouldn't say using free monads is "breaking the type system", because you must compose actions in a "typeful" way, and all the possible interpretations must respect that. One thing I do find sometimes annoying is implementing a pure function and then discovering that I need some kind of monadic effect. Or having to implement monadic and non-monadic versions of a function.
In modern Haskell, a Unicode string is represented by the type `Data.Text`. Most common per-character operations can be done directly on the string, without any special "splitting" operation, using the many functions provided in the `text` library. Doing it that way is usually simpler and much faster. But if what you really need is a list of individual Unicode characters, use `Data.Text.unpack`: unpack :: Text -&gt; [Char]
The TEX reference is a nice touch.
I prefer calling it reliability or correctness instead of safety. A program that gracefully crashes without undefined behaviour is safe, but still not wanted. Reliability is a feature on its own. We have to write more than e.g. a pythoneer, but we only write it ones and then have working code. We won't have to search for a special case, that somehow fails and can refactor or extend without fear of breaking code. When you know, that you are protected by the type system, you can write more risky code. For example in C++ people rarely write multithreaded code, because it is very easy to just forget one pointer and then they need to debug a data race. One minute of not thinking enough can lead to multiple days debugging. Rust is very similar, but its type system prevents you from accidentially sharing unprotected data, so the same problems are fixed within minutes. You have to think less and still get better code.
&gt; At this point, any red-blooded functional programmer should start to foam at the mouth, yelling "build a combinator library". I love SPJ's prose
If you find yourself trying to mimic OO so readily, it sounds mostly like you're having trouble thinking in the paradigm. Implementing a special-purpose interpreter correctly in Haskell is often quite a bit easier than solving the problem correctly in many other languages. Correctly being a very important word, as the ease of implementing something incorrectly is not worth discussing. (`main = return ()` is an incorrect implementation of every possible program) Furthermore, refactoring is considered to be one of the strongest points about Haskell, so I imagine your troubles doing so come mostly from a lack of language experience. You can divide 'unforseen functionality' into two camps. Unforseen synchronous functionality (which is easy to add) and unforseen asynchronous functionality - which depends hugely on what you're trying to add and how you've built things so far. The reason I say that unseen synchronous functionality is easy to add is because you can encapsulate extending a function's capabilities. Consider: data Extend a b a2 b2 c = Ext { translate :: a2 -&gt; a , extension :: a2 -&gt; c , finalize :: b -&gt; c -&gt; b2 } extend :: Extension a b a2 b2 c -&gt; (a -&gt; b) -&gt; a2 -&gt; b2 extend Ext{..} old newIn = finalize (old $ translate newIn) (extension newIn) Given this, we can take some old function operating on simpler data and extend it to a new function operating on more complex data, or even on the same kind of data but with different relation from input to output. Note that we can extend multiple potentially different implementations of the old function with the same new functionality. This is not at all how I would write that for myself, but you mention you haven't written much so I'll keep it simpler, and this should be something you can read if you've done enough to actually complain about architecture. In fact, I am not even advocating this as how to extend synchronous functionality. The point is only that, with the guarantees that referential transparency offers, this crazy extension mechanic can be shown to be correct as a means of general function extension *without even considering what the base or extended functionality might be*. (If you are not even using custom types to encapsulate your *foreseen* functionality, then no wonder you're running into trouble - you aren't using the tools we have to avoid it!) On the other hand, there's extending asynchronous functionality. This is much harder than synchronous extension in Haskell. That is because this is problem is *always hard*. Haskell has simply made the choice for it to be hard *up front*, rather than 6 months after you've written it throwing a bizarre asynchronous error you simply cannot track down because it only happens during a blue moon on a Tuesday while the user is not wearing matching socks. If I may ask, in what you have written, how much of your code is in IO vs being pure?
Your argument in favor of imperative coding seems to rely on this: &gt; Whereas with procedural code you can just hack it in through global state and opaque references (i.e. OO). In my experience, that is exactly the process by which programs become complex, buggy, and horrible to work with... which of course makes them, over time, *more and more* difficult to change. I've seen that happen on many projects. It's like the project-poisoning demon, constantly tormenting teams, taunting new team members trying to understand how the hell the code base ended up like this... Haskell uses referential transparency and type safety to enforce discipline when changing functionality, and that's a *major feature*.
I suspect that even "correctness" has some connotations that make some people think of some kind of pedantic overengineering (sadly). Instead, I like to talk about static typing as a clever system of avoiding bugs. Bugs are real things, the things that show up in the issue tracker every day... these awful, tedious, silly problems caused by oversights that static typing can notice immediately. After coding in very dynamic languages for a while, I'm beyond convinced that static typing is valuable. Not having it just feels ridiculous. Or when I've been in the JavaScript swamp for a couple of weeks and started to forget about static typing, I remember... oh, wow, the computer can actually know what things go where? And tell me when I make a mistake? Holy cow! (Yeah, unit tests can also help you discover mistakes, but in my experience, it's very hard to catch all null references even with lots of coverage; you need integration testing on all kinds of levels...)
&gt; Most common per-character operations can be done directly on the string, without any special "splitting" operation, using the many functions provided in the text library. Doing it that way is usually simpler and much faster. If you are referring to e.g. `toLower` and `toUpper`, then I’d rather we do not perpetuate the idea that these operations make sense at the 'character' level. The [`Text` docs themselves](https://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text.html#g:8) make sure to point that out, and provide many examples.
Besided "classy-prelude" and "lens", another useful library for this style of programming is "monoid-subclasses".
If I may generalize your argument, it sounds like the core idea is something along the lines of "functional programming doesn't work because there are refactorings that are hard in a purely functional language". I don't find that to be a convincing argument because *every* program has a set of refactorings that are hard and a set that are easy. (Where refactoring difficulty is defined as the amount of code that has to be changed.) Most of the design challenges I've encountered in the real world don't have an ambiguous "best" answer. They're a tradeoff. Choice A makes one set of future modifications easier while choice B makes a different set easier. The challenge is to pick the set that is *more likely* to be right--a probabilistic decision. In every career, always and forever, this means that there will be times when you get it wrong. Maybe you weren't even locally wrong. You were right about which kinds of changes were more likely at the time, but the business reality changes and you find yourself having to make the refactorings that your design made more difficult. When you think about it this way, the important question is not about whether you can do a particular refactoring with a small amount of code. It is about how confident you can be at the end that your refactoring is correct. This is where strong types and purity really shine. I could have chosen an initial design that formulated that pure function as a side effecting one, and because I didn't, maybe the change I'm making now will touch more lines of code. But the compiler will tell me almost everything that needs to be changed! So I can make the refactoring fearlessly. Before I started programming Haskell I actually encountered a situation where I wanted to try a large speculative refactoring that could potentially be a performance improvement, but ended up choosing to not do it because it was simply impossible in Java to get the guarantees I wanted that I had done the refactoring correctly. In summary, it's not about the raw number of lines of code that have to be changed. It's about how many guarantees you can get from the compiler that you made the changes correctly.
&gt; If I may ask, in what you have written, I've written in plenty of languages, ranging from Python to PL/SQL - but the only code that is currently still running in production has been Java and PHP. &gt; how much of your code is in IO vs being pure? It's kind of hard to say how much is IO, when stuff like logging and database requests and http requests and so on happen intermittently.
I believe they meant in regards to your small bit of Haskell experience.
The consensus in the Haskell community appears to be that a little bit of planning (and a bunch of design experience) will typically avoid the need for breaking the discipline. Do you have examples of "projects where breaking the discipline will be necessary"?
&gt; it was simply impossible in Java to get the guarantees I wanted that I had done the refactoring correctly If you end up changing a large enough part of the original system, because on the one hand the original system was simply not laid out to contain those new communication channels that are now being introduced, and on the other hand the type system doesn't let you get away with anything less in terms of magnitude of the changes involved, then how certain can you really be that the new implementation will actually be bug-for-bug compatible? How is that, just from an upholding-original-guarantees perspective, any different from putting a new global variable somewhere (and possibly capturing and hiding it behind some opaque references, just to make the relation to OO more obvious), and then switching on it, in some new pieces of code that are to be embedded in the currently existing one?
Although I disagree with using typeclasses for these purposes, I very much appreciate Michael's exploration of the design space. I've played around with the "data type as a module approach" myself a bit in [lens-prelude](http://hackage.haskell.org/package/lens-prelude-0.2/docs/Prelude-Bitpacked-Module.html), but I'm not really satisfied with it either. With that being said, I'd like to bring up a pain point that type class approach and the data-type-module approach suffer from. If we look at `MapClass`, we find a method named [insert](https://github.com/commercialhaskell/jump/blob/9cc1dbd020c2a42e2bd93204d517470c5781bbf2/src/Jump/Map.hs#L46). The cool thing is that insert now works for all map-like types. But what about when we go to create a similar class for set-like types (`Set`,`HashSet`, etc.). We probably also want a function named `insert` there. But now that name is taken. Dang. Or what about [singleton](https://github.com/commercialhaskell/jump/blob/9cc1dbd020c2a42e2bd93204d517470c5781bbf2/src/Jump/Map.hs#L44) or [toList](https://github.com/commercialhaskell/jump/blob/9cc1dbd020c2a42e2bd93204d517470c5781bbf2/src/Jump/Map.hs#L162). These two can really apply to not only things that are map-like or set-like but really anything that is list-like (`Seq`,`Vector`,etc.). So, the difficulty is that, with the typeclass approach, we still end up stealing some names for certain data structures. We can always say, "oh, just use listSingleton instead when dealing with a list-like thing", but that's pretty unsatisfactory. To be clear, using data types as modules doesn't provide a good way around this either. My faint hope is that some of the machinery from [OverloadedRecordFields](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields) could somehow eventually give us a way to really overload functions, but I'm not really holding my breath on that one.
I think the problem is we're stuck between a rock and a hard place. Type-classes for overloading are mostly gross, but modules with qualification are also gross to read and write. Until an editor like Lamdu makes it big, we're choosing between two unsatisfying solutions.
What about something like modular implicits? I know ekmett hates it, and his reasons are well documented anywhere, but I fundamentally disagree with him.
So yes, it sound like you're having trouble thinking in the paradigm. Everything (literally everything) really genuinely is just one big equation, even if it doesn't seem like it at first (this may not be the most *performant* way of looking at some problems, however). You're focusing on what things *do* rather than what things *are*. May I ask what in particular you were writing that gave you this bad experience?
&gt; How did you contain it to those 10%? Did the signatures of the entry points or APIs to those 10% remain unchanged? I didn't do anything to contain it. That was just the amount of lines that the necessary refactoring touched. &gt; It's pretty futile to argue against something, when there's clear market demand for it and no laws against it. The only question left is how adequate some tool is for meeting that demand. But that's not what I'm doing. Purity is effectively equivalent to a "law against it". It's not actually a law, just something that naturally biases things against the bad behavior. I'm not trying to categorically prohibit the bad behavior. I'm arguing for a reasonable safeguard. You seem to be arguing *for* the bad behavior though.
Can you give us a simplified spec of what you were trying accomplish in Haskell? Then we might be able to show you a solution in the "Haskell way".
Possibly you are missing your objective. I am referring to the XY problem here. You don't make function `A` behave differently depending on the number of times function `B` has been called, especially so if `A` is used all over the place. Hacking through global state doesn't solve problems. I think /u/jerf already suggested the key to composable code: **separation of concerns**. It might take insight to recognize and experience to appreciate, but it goes a long way. Of course, it will also easily "make "nope" show up in blue when printed out on the CLI, if anagrams of "dope" were processed in the last 10 seconds". Here's very nice example of simplifying something seemingly as simple as possible: http://conal.net/papers/type-class-morphisms/type-class-morphisms-long.pdf Now OO... separation of concerns will probably leave vast majority of classes with single virtual function, at which point a class is just a function. 
Basically you have to have an idea of why the normal person would need a derivative soon and why the internet is gonna be the biggest economy of the world, or better, biggest than the whole world other business soon.
Regarding qualified imports: I have a related proof-of-concept/library named [qualified-prelude](https://github.com/lspitzner/qualified-prelude), which might be able to mitigate the problems of qualified imports as listed by snoyman: * no need to interrupt your train-of-thought to add an import statement (you still need the dependency listed, but i don't see how this would not be the case for a classy approach; you need to import your classes as well.) * the mapping to shorter module names is consistent (as long as you use the same include statement). (snoyman's argument seems to boil down to just these two items.) Now qualified-prelude has a nice list of disadvantages (the high cost of maintenance until i put the time into automating the whole process is the main reason against announcing the package, because in its current state, the package may just break too easily). But maybe the idea is still worth pursuing? I am happy about any feedback. (i *am* using the prelude for personal projects, and am fairly happy so far. nothing broken yet :p)
It's an interesting approach. There are some downsides, however. You listed some in your README. In addition, you'll likely end up with more-often-than-necessary recompiles due to how Cabal handles CPP-enabled modules. However, it doesn't address some of the main concerns that I have, in particular, it does not provide a uniform interface to different datatypes (e.g., my Map/HashMap and ByteString/Text examples).
Actually, the `singleton` in `Data.Set` and the one in `Data.Map` take different numbers of arguments. And the `toList` from `Foldable` is different from the one in `Jump.Map`. Namely, it discards the keys. But this situation could possibly be addressed with an associated type family.
Lots! Types aid program design, help with performance, help with autocomplete, can offer as-you-type silly mistake checking, etc.
I personally dislike approaches that put a namespace in the operation like this. What about seperating out such functions into their own type classes and every class that supports those names can subclass it? Of course this would require the amount of arguments to always be the same but that holds at least for the functions mentioned.
&gt; burritos http://chrisdone.com/posts/monads-are-burritos
I think these are good arguments against classy-prelude, but they're not good arguments against this style of programming. IMHO, my subhask library (http://github.com/mikeizbicki/subhask) fixes your problems (and a few others). In particular: Every typeclass in subhask is founded on rigorous mathematical laws, there are no unprincipled typeclasses. Your example of `insert` and `singteton` functions are provided by the [Constructible class](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra.hs#L2356). If you click that link, you'll see a bunch of nice laws that these functions are also required to obey. Fortunately, all the Haskell containers actually obey these laws (See [these files](https://github.com/mikeizbicki/subhask/tree/master/src/SubHask/Compatibility) for lots of instances). Even better, **subhask is able to automatically verify that these laws are obeyed.** The fact that we can get "test suites for free" is my favorite motivation for this style of programming. 
True, it was not meant as a reply to the post's main idea; my point merely seemed sufficiently related. Regarding recompiles: Good point, thanks. That might indeed become annoying. I simply have not tested it in any sufficiently-large project to notice it. (but i'll add it to the list of disadvantages.)
&gt; Because how else do you get things done, without absolutely thinking everything through to the very last detail before writing even the first line of code, and then rigidly tying down all the pieces, once you've figured it out, because that's the only way it'll ever compile? &gt; So, what is it that I'm missing here? Refactoring is, in my experience, easier in Haskell then other languages, /because/ of the type system. If you are constantly fighting it, you aren't doing it right. I have to admit I don't have any idea what one would need "Frankenstein-esque variant of objects and stack frames as variables" for. It just seems strange to me, and makes me think you are trying to write c++ code in Haskell, which will generally not work very well. And special-purpose interpreters are a Haskell tool of choice because they work well and are easy. Like really really easy. (And free monads are not breaking the type system. Not sure why you would think they are?) Also yes one does use IO when it is needed. If you need to /do/ one thing, then /do/ another that's probably IO, and should be typed as IO. But if instead you are /computing/ something, which is indeed a significant part of what /computers/ do, it probably doesn't need IO or stack frames, or closures.
School of Haskell considers it an anti-pattern, I wrote a library (unexceptionalio) to make it even easier. This is a matter of taste, a bit. Basically, do you prefer the type system to catch your mistakes (even if that causes some extra coding) or not?
um, btw, you shouldn't be using `isNothing` with `fromJust`... you can just pattern match on the Maybe or use higher order functions (like `forM_` or `for_` here). that way it's compiler - verified and you don't have to use partial functions or go through boolean blindness
You've run into a question that is controversial among Haskell programmers. And beyond—this is Haskell's version of the checked vs unchecked exceptions debate, [most famous in Java](http://www.ibm.com/developerworks/library/j-jtp05254/). I hate Java checked exceptions, but find that `ExceptT`, despite being so similar, doesn't suffer from their awful verbosity, because baking things into an ADT allows us to write functions to cope with errors separately from the main flow of the logic, and put them into libraries like [`errors`](https://ocharles.org.uk/blog/posts/2012-12-04-errors.html).
Okay, I think I get what the problem is now. So, to write nice idiomatic Haskell code, generally you want to write a bunch of very small functions that represent, atomically, things that you want to do. Then you build some gluing operations to combine them together. Monads are often great (but are not always necessary) for the gluing part. More importantly, when you hear "build a specific interpreter" don't assume you are throwing anything away. You aren't. You are building a new tool on top of what already exists that happens to exactly solve small problems in your domain, and has a means to combine existing solutions. The interpreter is generally quite small compared to the code for things it interprets. When writing out custom syntax trees, things like GADTs allow you to - using Haskell's existing type system - build a custom type system that perfectly expresses whatever it is you're shooting at. It can even be (though I have not actually read the papers myself) be more expressive than Haskell's own type system! But then of course, your spec changes. This doesn't matter. When your spec changes (if necessary) you change your specific interpreter (or whatever else you use to model the domain) to match the new capabilities demanded by your spec. Then, the compiler will tell you exactly what other places you need to make changes. This isn't more work. This is *exactly the same* kind of work you have to do in other languages to *keep things correct* but that few of them actually make you do. Other languages simply let you bull ahead and wait to explode on you later - a *far* more costly way of doing things. In fact, I think this paper would provide a great example of what I'm talking about: https://lexifi.com/files/resources/MLFiPaper.pdf The combinators (a common name for the atomic pieces you build) operate on a single blanket `Contract` type, but this is appropriate for the domain. Almost arbitrarily complex constraints can be added to the type your combinators work on, should you need them. 
I would love a bit more explanation about that bit: &gt;However, we know that some control flows (such as exception handling) are not being used, since they are not compatible with MonadIO. (Reason: MonadIO requires that the IO be in positive, not negative, position.). Can someone maybe give me a link related to that, or something like that?
I can sympathize with that. There is even some data suggesting that having defined vacation may be preferable. Stepping back the stage we are at a very early stage and there is a lot of uncertainty. Anyone who says otherwise about a start without proven product market fit is lying to you. So there is a lot of uncertainty about many aspects of what we do, not just how much vacation their is. That said this type of environment is not for everyone. Some very smart and capable people I know would just not be a good match for an environment like this due to their preference for certainty. If one is concerned about the certainty around vacation then there are bound to be much greater concerns about all the other uncertainties that arise.
While I agree with most of the article (proper abstraction is key to writing flexible and maintainable code), there is one point I would like to hear a motivation for. The post mentions `Storable` and `Binary` as examples of good typeclasses, even though they don't have any accompanying laws. I have never really found these typeclasses to be at all usable. Most of the time I want absolute control of the memory layout and/or serialization, so I find it better to call concrete functions directly. Are there any good examples of when one of these typeclasses actually bring a real value?
IM thinking of using Servant. How do I store then the intermediate steps in a user session ? 
Yeah, that was pretty sparse on details. The idea is that `MonadIO` provides only the `liftIO` function of type: liftIO :: IO a -&gt; m a Notice how it requires that the input to the function to be `IO`, and the output to be the monad the instance is for (`m`). This allows us to lift any function that has `IO` in positive position, e.g.: foo :: IO a foo :: a -&gt; b -&gt; IO a foo :: a -&gt; (IO b -&gt; c) -&gt; IO d However, it's impossible to lift a function that has `IO` in negative position using just `liftIO`, e.g.: catch :: IO a -&gt; (e -&gt; IO a) -&gt; IO a For that, you'll need something more powerful, like MonadBaseControl (see monad-control and lifted-base packages) or MonadCatch (see the exceptions package). For those looking for more information on positive and negative position, I wrote an article on the topic: https://www.schoolofhaskell.com/user/commercial/content/covariance-contravariance
I'd say that's kind of cheating. What you get is a static type (because your type marker doesn't make sense if the type is allowed to change) but none of the benefits of static checking. Admittedly, I don't know what the exact definition of expressibility is since you can write anything in anything.
I'm not doing anything in finance right now, but I've explored the industry as an intern at a wonderful firm using functional programming and I can think of a few reasons why people might find it appealing: * Markets present a problem domain that's satisfying in the way mathematics is satisfying. If you like that sort of thinking, it's far more interesting than the "businessy" things you work on at many normal tech roles. * Trading firms still do useful things, the usefulness is just more abstract. People love to dismiss adding liquidity or even price discovery as important, but it is, demonstrably. It's just invisible in the same way as urban infrastructure is invisible and some people handle that better than others. I don't want to be too contentious, but I definitely see market making as significantly more useful than working on some random app, social network, delivery startup... etc. You could argue about, say, *how much* liquidity is useful—but you could ask the same things about what most tech companies produce. * Not everyone cares about contributing to a concrete product—especially when the product is doing something boring and your role is entirely in executing somebody else's vision. Personally, I'd rather work on something that I feel is more my own even if it is purely internal trading architecture than work on something that feels entirely somebody else's. My impression is that, at least within the sphere of functional programming jobs, this is somewhat easier in finance. This doesn't clearly demarcate finance vs non-finance, by the way: roles within both fields vary a lot. Rather, it's just one thing that might be a lot more important than working on a viscerally obvious product. I could come up with more similar reasons, but they all boil down to finding certain things more important than the nature of the product you're working on. Not everyone sees finance as creating nothing and not everybody cares about working on a concrete product, especially over other concerns. Think about it this way: you could—and people do—ask literally the same question of pure math researchers. Making an immediate, obvious impact is just less important to some people.
Why not 'insert val' and 'insertAt key val' ?
For what is worth, the [servant library uses ExceptT over IO](http://haskell-servant.readthedocs.org/en/stable/tutorial/Client.html). [Here](http://hackage.haskell.org/package/servant-client-0.5/docs/Servant-Client.html#t:ServantError) is the error type. I think it makes sense for servant because there are many error contitions, not all of them related to IO. Also, getting surprised at runtime by an exception you didn't know your function could throw isn't fun.
Once GHC 8 arrives with OverloadedLabels, it think it will be possible to define something like: IsLabel "insert" (k -&gt; v -&gt; Map k v -&gt; Map k v) fromLabel _ = insertMap IsLabel "insert" (v -&gt; Set v -&gt; Set v) fromLabel _ = insertSet And use it like: #insert 5 'c' someMap #insert 5 someSet Not sure if it will be a good idea though. 
I've often wanted to play with a Genetic Programming library with the genetic program specified to be an abstract syntax tree in Haskell, hooked up to make it easy to traverse across the tree and annotate it as we execute it. Possibly a free monad representation of the program (?), though that may prove too unrestricted in general. (Or perhaps not.) I could also imagine having to require additional constraints on the genetic program, so, for instance, a mutator can get the answer to the question "what nodes may I randomly select to put here?" in a reasonable way. To give an example use case: The project I did for my genetic programming class lo these many years ago was basically to hook up a "profiler" to my genetic program solving one of the bog-standard problems of the field, and then when it came time to mutate the program, using the number of times a given node was actually processed (short-circuiting logic operators meant that nodes did have varying execution counts) to guide where the mutations should occur. In particular, there's the obvious degenerate case that a node that is executed zero times should not be considered a target of mutation. (It may not be "wrong" to mutate/cross-over/etc. there in an academic sense, but in practice, we are certainly just burning CPU cycles for no possible gain.) Since this was before I even met Haskell and was written in C, it took me longer to write than I'd like and a lot of the obvious followups, some indeed expressible in a mere three or four lines of Haskell, were not done; for instance, it would be trivial in Haskell to take such an annotated tree and completely trim all 0-execution nodes.
Woah. I certainly was not expecting this level of detail. Thank you so much, I'll take into account!
On qualified imports: part of the reason people need to abbreviate module imports is because most module names are needlessly hierarchical, which gives us a bunch of garbage prefixes that tell us nothing. The `Data` in `Data.Map` is useless, as is the `Control.Concurrent` in `Control.Concurrent.MVar`. Eliminate the junk and there would be less need to abbreviate. Originally Haskell did not have hierarchical names, but after they were added (a nice addition in my opinion) it's like someone decided to barf an unnecessary hierarchy onto everything in `base` (`Control` adds nothing useful to `Control.Applicative`, and what does the `System` in `System.IO` tell me?) and then library authors followed suit (`Text.ParserCombinators.Parsec.Char`, really? Now we're down to `Text.Parsec.Char`, where the `Text` is still useless.) These hopelessly long names also bias folks into importing everything unqualified, which is an even bigger mess than ad-hoc abbreviations. This ship has sailed for existing libraries but please think about it before sticking your next library in `Control` or `Data`.
How would Lamdu alleviate this problem?
&gt; (the language extensions are only used in order to derive Show and Read instances and could have been removed if I had written the instances by hand like a real man). ಠ_ಠ
You're right that the question "which is better" is a loaded one, and you've answered to the best of your abilities. Which is more than I can expect from a stranger off the Internet :) Thanks for sharing your work and indulging my poignancy. I think the bottom line is that you're very excited about your work. Good for you.
Your library looks incredibly cool. Once the limitations are gone it seems like it would be a better alternative to Prelude. Or am I missing something besides the current limitations?
[Here](https://hackage.haskell.org/package/lens-tutorial-1.0.1/docs/Control-Lens-Tutorial.html)'s a great lens tutorial written by /u/Tekmo
I can think of a few projects based around [acid-state](http://acid-state.seize.it/). 1. support for sharding 2. use mmap, or some other trick, to allow portions of the state to be stored on disk 3. implement a generic query/update library so that an acid-state can be examine and modified easily from the command-line There are, of course, many other acid-state wishlist items -- such as an S3 backend. But, what makes these three items thesis worthy is that they seem doable, but the solutions are not entirely obvious. You would have the opportunity to break new ground.
You're stating that a pattern on the left matches with a value on the right. So you can have the top level pattern: (x, y) = (2, 3) which matches `x` with `2` and `y` with `3`. Likewise, you can write: fibs@(first : rest) = 0 : 1 : zipWith (+) fibs rest which matches `fib` with the whole result, `first` with the first fibonnaci term, and `rest` with all the rest of the fibonacci terms. Likewise, [wat] = [1] assigns the name `wat` the value `1`. To be a bit more formal, when you say `Pattern = Value`, you're saying "Try to match the value on the right with the pattern on the left. Match the names with values that fill those patterns." For the general case, `f x y = x + y` matches the name `f` with a function and `x` and `y` with variable names. Consider: wat : [] = [1] which is the same as `[wat]` above. What if we did: wat : [] = [1, 2] ? Well, GHCi loads the module fine, but it barfs when you try to reference `wat`: λ&gt; wat *** Exception: asdf.hs:4:1-19: Irrefutable pattern failed for pattern (wat : []) ... This was all a rather long winded way of saying that, yes: "the stuff on the left side becomes/gets assigned the stuff on the right side." is completely correct.
The equal sign in Haskell does not perform assignment. It's a statement of equality. It says the left and right sides ultimately have the same meaning, and, as you read, can be substituted back and forth for one another. This is similar to like-for-like replacement in mathematics. Consider this: 3 * (4 + 5) = 3 * 9 This is an equality. I'm not assigning the right to the left. I'm saying the right and the left are equivalent. The two sides have the same value, and anywhere the left is used in a larger expression, you can replace it with the right, while retaining the overall meaning (ultimately, 27), and vice-versa, like this: 7 + (3 * (4 + 5)) * 2 = 7 + (3 * 9) * 2 See our original equality hiding in among larger expressions there? You can actually make an expression more complex, too (e.g. if we replaced the right side with the left here), without changing its meaning. We can do the same kind of like-for-like replacement in Haskell. For example: head (x:_) = x These are different expressions, but they evaluate to the same thing, so in the end they have the same meaning, and there's an equality between them. They both mean x. You can go into your Haskell code, stick any value into a list, and add on any, unimportant tail, then call head on it, and it won't change the result. Here's an example of making the left more complex, without changing its meaning: 2 + 3 = 2 + (head [3,4,5]) I just gave the computer more work to do (on the right), but didn't change the meaning of the left by wrapping up the 3 in a list, then calling head on it. The two sides are still equivalent, modulo an evaluation step. This ability to simplify and complect while retaining meaning is part of what's called "[equational reasoning](http://www.haskellforall.com/2013/12/equational-reasoning.html)." We can reason about things through such equalities. This gives us a lot of power. You can treat expressions algebraically. You can solve systems of equations made of code. You can even take these powers from this term level up to the type level, as seen in Chris Taylor's [Algebra of Algebraic Data Types](http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/), where he treats ADTs as the mathematical objects they are, and—through math—discovers new things, though this is a more advanced topic, and still makes my head spin a little :) This equational reasoning power comes both from Haskell's powerful type system (Hindley-Milner), and from its functional purity, where functions aren't allowed to have side effects. Side effects are a major reason why most popular languages don't allow for such powerful reasoning, because there are no guarantees that a function will do the same thing every time it's called, even with the same inputs. An example of such a purity-wrecking side effect would be getting user input, or reading from disk. These can vary for every call, and even fail, so you can't treat such functions as static values, as we could in the examples above. Haskell evaluates code by pattern-matching. It looks for a pattern in your code that matches a left-side, and replaces it with a right side. Sometimes that makes the expression simpler, say when summing a list of numbers, but sometimes it makes it more complex, say when generating a list of values from a seed value with iterate.
I hadn't heard of this feature, thank you for the pointer.
&gt; &gt; That classy-prelude doesn't strive for this is my main objection. &gt; That's a pretty big assumption. After the first few versions of trying things out, we went back, rewrote all of the typeclasses from scratch based on laws, and wrote quickcheck properties to ensure them. I think you're attacking a strawman here. I think it's unambiguously true that subhask emphasizes lawfulness more than classy-prelude. For example: * In your original writeup you mention how your `Foldable` doesn't have laws, but subhask's `Foldable` does. * In your [`Data.Containers`](https://github.com/snoyberg/mono-traversable/blob/master/src/Data/Containers.hs) file, a quick search for the word "law" results in no matches. The only time I see you referring to laws (in my admittedly very quick search) is in the [`Data.MonoTraversable`](https://github.com/snoyberg/mono-traversable/blob/master/src/Data/MonoTraversable.hs) file where you say that the monomorphic containers obey the same laws as polymorphic containers. Your [test suite](https://github.com/snoyberg/mono-traversable/blob/master/test/Spec.hs) is very nice, and I assume that's what you're referring to when you say your class's have laws. But since these laws aren't documented in the haddocks, I honestly had no idea about them until just now. It also means it's hard for me to check that my own implementation of any of your classes satisfies the laws. * In subhask's [`SubHask.Algebra`](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra.hs#L2469) module, the word "law" appears more than 100 times. Furthermore, there are currently more than 1000 *automatically generated* quickcheck properties. I'm currently in the middle of a refactoring that will let greatly extend the lawchecking capabilities of subhask, and I expect to have more than 10,000 *automatically generated* quickcheck properties derived from class laws when I finish. I realize this might sound like I'm trying to pick a fight. I'm honestly not. Subhask is definitely NOT suitable for prime time right now, unlike classy-prelude. &gt;Regarding the Map newtype, I'm having a hard time understanding how this all fits together. Can you demonstrate how you would translate the following code into subhask? &gt;In classy-prelude, it's: &gt; insertSet "somevalue" mempty :: Set String &gt;insertMap "somekey" "somevalue" mempty :: Map String String\ In subhask, that looks almost identical: import SubHask import SubHask.Compatibility.Containers ... cons "somevalue" zero :: Set (Lexical String) insertAt "somekey" "somevalue" zero :: Map (Lexical String) (Lexical String) The `Lexical String` is because the default ordering for containers in subhask is the "subcontainer" ordering which is a lot more convenient for mathematical programming, and the way mathematicians normally think. Unfortunately, this is only a partial ordering, so it is not suitable for the containers library. The `Lexical` newtype uses a lexical ordering instead, which is the default ordering in standard haskell. If we were to use hash based maps/sets instead, then we wouldn't need these `Lexical` wrappers.
I think it will be an unabiguously better prelude for numerical programming tasks. For non-numeric tasks, I could see people preferring the Prelude's simplicity.
I'd love to understand this distinction as well. 
Picked up some more projects and am hiring another three devs to join Strats. This time in market risk rapid prototyping in Haskell. We about 15 or so devs now, so a good chance to join quite a broad, experienced team.
See [this comment](https://www.reddit.com/r/haskell/comments/4bgzq4/why_i_prefer_typeclassbased_libraries/d19gp51) by /u/snoyberg.
&gt; I'm thinking of doing something like this in a language I'm building Do you have any description of your languge? I have similar idea in aeiar https://github.com/dotneter-/aeiar#type-class
https://www.schoolofhaskell.com/user/commercial/content/covariance-contravariance
I don't think we need a `Binary` type class to get generics-based serialization. For example in [this module](https://hackage.haskell.org/package/generic-binary-1.0.1/docs/Data-Binary-Generic.html) we have serialization functions but no `Binary` instance.
I think HfM is a nice tool. (Disclaimer, I am translating the accompanying blog posts to Dutch). But before you buy, try the other tools and see what fits your workflow. Having a tool that works out of the box is fantastic, but setting up your own chain will (likely) teach you a few things. Like how to avoid setting up stuff yourself the next time, or how you can tweak your experience if you stick to doing it yourself.
I use vim, but I switched to iTerm2 some time back as the Apple Terminal.app has a few issues, especially with utf-8 characters, so if vim is your cup o' tea, try that. Alternatively, Atom makes a nice Haskell editor these days; [here](http://xn--wxa.bje.id.au/posts/2015-10-06-atom-haskell-dev.html) is the set of packages that makes it work, but fundamentally you want `haskell-ghc-mod` and `ide-haskell`. If you're using stack, ensure you provide `/User/&lt;you&gt;/.local/bin/&lt;progname&gt;` paths for `ghc-mod` and `stylish-haskell` in those two packages, respectively. If you're not using stack, consider it. :-)
Is a Masters or PhD truly relevant to the authorities, or just to the educational quality level of people you want applying for your positions? I ask because I've recently applied for some jobs in Singapore and my bachelor's degree didn't come up as any kind of problem.
It is really just an alias. Haskell is made of expressions. Expressions can grow big. The equality sign allows you to give a short name for big expressions, and reuse them without having to copy/paste. No need for glorification. I'd not overthink it, that is all there is to it imo.
Strictly curious as someone not holding a Masters/PhD, but is the role more academic/research oriented in nature and thus requires that type of specific training? What is someone with 10+ years of industry experience lacking in comparison? I ask on behalf of those of us who ply our trade with passion but are reluctant to apply because we fail to match the academic requirements.
This was a great talk. It's a interesting feature that I look forward to seeing.
To briefly address your edit, I absolutely meant abstraction, and explained how that's true in the blog post. You're programming against an abstract interface instead of a concrete implementation. You may be thinking about some other kind of abstraction, but your views are pretty narrow. Your comment comes across to me as adding nothing to this discussion. You made a blanket statement about pain without backing it up, and then played a semantic argument based on an apparently overly narrow definition. This looks like a typical knee jerk reaction as opposed to really considering an opposing viewpoint.
If you have 10 years of Haskell (or FP) industry experience please apply! PhD in comp sci (with lots of FP emphasis) is one way to quickly filter for plausible FP skills and training, but by no means the only way to gain it. Make the case.
This ignores half of my blog post. I'm not sure what point you're trying to make except convincing people who aren't reading the post that you're right.
* &gt; All the functions I know are available to me. * &gt; Add-on packages can provide new combinators without ever knowing about the data types I care about. * &gt; We can't accidentally end up with conflicting type signatures or semantics for a function name. * &gt; Gives us more information * &gt; Easily test out alternative implementations All of those are benefits of abstraction that go far beyond "overloading." That covers well over half of my blog post. So yes, claiming that you get all of these benefits with modules\* without typeclasses is ignoring the benefits I'm claiming. \* You may be trying to claim ML modules instead of Haskell modules here, which is pretty irrelevant. I'm talking about writing Haskell code here. There are certainly better ways to achieve what I'm talking about (frankly, even Java and Go do this better).
&gt; All the functions I know are available to me. I don't see how that's a benefit of abstraction. It seems like a benefit of overloading - you can use the same name to refer to similar functions. &gt; Add-on packages can provide new combinators without ever knowing about the data types I care about. This is true for any program written against a signature rather than an implementation. Haskell doesn't allow you to easily swap implementations like ML does, which is a shame, but even using various record encodings of modules in Haskell makes this possible. &gt; We can't accidentally end up with conflicting type signatures or semantics for a function name. Once again this seems more like a benefit of overloading, not abstraction. &gt; Gives us more information This information is available with modules too. If you have an abstract type with a specific signature, you know that the only operations used are those available in the signature. &gt; Easily test out alternative implementations This is a benefit of abstraction that is partially negated by Haskell's particularly bad module system, yes. Also, I'll add that type class canonicity is _anti-modular_ and therefore can easily be used to _break_ abstraction boundaries (although newtypes can be used to patch them back up again).
Mostly the pain caused by canonicity of type class instances, the need to use newtype to patch up abstraction barriers, and the global instance property meaning that you can't pick the instance appropriate for the occasion. It's not the right tool for the job IMO.
Type classes don't provide abstraction, here's a simple proof: You could replace all the type classes with explicit dictionaries and you'd get exactly the same level of abstraction that you had before! So, you don't get any extra abstraction from type classes. You get abstraction from, well, _abstraction_ - parameterisation of your computation. Type classes give you a way to use global names for such parameters, which is convenient but not at all related.
I would recommend Haskell for Mac for beginners. It's less useful for professionals at the moment, but it's great if you want to get a feel for the language.
You're forgetting performance and convenience, two other points I made in my post. But this is fine, if you want to ignore all the real points in my post and argue the semantics of the word abstraction for some bizarre reason, I won't get in your way.
I'm not sure what part of the post you're referring to. All of the examples Michael goes over deal with persistent data structures (specifically map-like structures) and have nothing to do with `IO`. I definitely agree with you about `MonadIO` being a non-solution for abstracting away `IO`. You mention that typeclasses can be used for this, and you are correct. There are alternatives though. If you're interested in the functional programming version of dependency injection, you should look at `free` and `operational` (and also my `operational-extra` package). I like them more than typeclass-based approaches. Here's an awesome SO question with an answer that compares the two approaches: http://stackoverflow.com/questions/23766419/when-would-i-want-to-use-a-free-monad-interpreter-pattern
That's actually a fair concern in general. However, I don't find it terribly relevant here, since: * most of the examples I bring don't have meaningful alternative instances * in cases where such alternatives exist, you can still fall back to qualified imports if you find newtype wrapping too onerous For example, you could have a MonoFoldable instance for Text that treats the UTF-8 representation instead of characters. But it's uncommon, and dealing with it via a newtype (or, better, just converting to ByteString) makes sense. There is a good argument about lazy versus strict Map functions. But I'd argue that those should have been two different types in the first place anyway.
What about PhD in numerical methods/high performance computing/PDEs, ... with C++ and civil/mechanical engineering background and interest in Haskell, FP, PL and CS in general? The positions sounds interesting and challenging but they seem to be for very experienced Haskell developers.
Only for simple cases. There is not nearly the support available for explicit dictionaries when it comes to inline and rewrite rules.
By having a module with alternate implementations of type class functions that assume a different "instance", like my lazy/strict example. I'm pretty confused now exactly what you're trying to argue for given that question. Are you actually arguing against this in favor of some actual approach that you consider superior?
&gt; By having a module with alternate implementations of type class functions that assume a different "instance", like my lazy/strict example I'm still not sure I understand. Doesn't this mean that one of those modules will have orphan instances and you break type class canonicity?
[digestive-functors](https://hackage.haskell.org/package/digestive-functors) is the most popular one, I think. [Introduction to the library](https://ocharles.org.uk/blog/posts/2012-12-02-digestive-functors.html). If you're going to use Yesod as the backend, you can use [yesod-form](https://hackage.haskell.org/package/yesod-form).
No, I'm saying that if you want something like a special behavior of toList for a Vector (say in reverse), you could either have a newtype wrapper and a Foldable instance or a module Data.Vector.Reverse with an alternative toList function. I think you're guessing I'm saying something deeper than I actually am. I'm just laying out the two most straightforward ways to get different behavior.
Ah, OK, so you only use the typeclass functions for one of the possible "instances". I misunderstood what you were saying before.
I find myself doing pretty fine with Atom and https://atom.io/packages/ide-haskell , except from when I randomly get problems with haskell-ghc-mod. If you use stack you can execute atom with `stack exec atom .` . Be sure to have the IDE-Haskell pluging configured to use the binaries from stack ( in your .local directory). Overall I wish there was a Haskell IDE that truly worked out of the box. I really need to see the type of expression and if a IDE doesn't do that then for me it's as good as using notepad. 
It doesn't seem problematic to me to have different names for insert-with-index and insert-without-index -- like "assoc" and "conj" from clojure. We don't have to go straight to having datatype-specific names like "listSingleton" -- these can just be different typeclasses.