Google is sponsoring ZuriHac this year - as they did for at least 4 years. I won't speculate about things you are speculating about, since neither of us really knows for sure. What I know is that multiple Googlers are using their 20% time to do various haskell-related stuff for many years now (including me) and the company does not stop or block us in any way. That's way more costly than sponsoring few students for a summer.
I don't like this typeclass anyway. Can't you make an instance of it for any possible `f : Type -&gt; Type`? And it has no laws?
ya, and even in cases where it is technically better, better by a factor of at best log(n) is pretty unimportant.
I don't really understand your comment, you seem to be contradicting yourself. What exactly do you have a problem with?
I don't think `Either` should have a monad instance *at all*. Errors should have went the Rust way, with a `Result` type with `Ok` and `Err` rather than bandwagoning on `Either` that is morally unbiased with `Left` and `Right`, but is biased in the prelude in a confusing way.
I claim that the type checker will pick up more incorrect uses of `Functor`, `Applicative` and `Monad` instances than it will of `Foldable`.
Holy crap, you weren't kidding, this stuff is sometimes ancient
I like how the top 10 most popular packages in Haskell are the packages for three *other* programming languages. (Elm, PureScript and Idris)
+1 Much more readable in my opinion
&gt; is how does it (if it does) really restrict the QueryArr world to just the things that can be compiled to SQL? It does, but not quite as you say. `QueryArr`s are not restricted to just the things that can be compiled to SQL, but you can only compile to SQL the things that can be compiled to SQL! It achieves this by only having base `QueryRunner`s that look like `QueryRunner (Column a) b`. That means you can only run `Query`s whose type parameter is a product of `Column a`. So you can make something like `Query (Either (IO (Ptr Int)) (STM Bool)` if you want, but you can never convert it to an SQL query (or indeed really do anything with it at all). 99% of the time when someone says "`Arrow` is too specific because it requires you to be able to lift an arbitary Haskell function" they're wrong. You can encode plenty of useful stuff with `Arrow` even in the presence of `arr`. 
Thanks /u/saurabhnanda. These are great!
In the advanced DB mappings section, under newtypes for primary keys, you mention: &gt; But it seems that Opaleye’s support for this feature is not really ready. So we will skip it for now. But in the same GitHub Issue you link (https://github.com/tomjaguarpaw/haskell-opaleye/issues/242#issuecomment-263217532) a suitable method is outlined, indeed one which I have found works well. What is it that you think is immature about this method? It isn't quite as neat as having a `FromField` definition or similar take care of it, but the additional boilerplate isn't so bad. You only need to substitute the newtype wrapper into your code, and in the table definition you put: tenantId = pTenantId . TenantId $ optional "id" You can use this with your `readOnly` function like: tenantId = lmap TenantId . pTenantId . TenantId $ readOnly "id" This is certainly worth it in my opinion: it's only a few lines and prevents situations arising (the comparison of two different `id` columns) which clearly shouldn't happen. edit: Or as below tenantId = pNewtype $ optional "id" or tenantId = lmap TenantId . pNewtype $ readOnly "id"
the issue isn't just about confusion, however. It's about being able to catch bugs in code.
Going back about 3 years (bear with my sketchy memory), I remember having to change an `Pipe blah blah ErrorT Something` into a `Pipe blah blah EitherT (Either ...)` and the resultant line took me a good day on `#haskekll` IRC talking to experts to figure out, with the end result busting out of ruler-80 and starting out like `lift.lift.lift.liftM .. join`. Yeah, the good old days.
Having users learn how to use lenses poses a much greater barrier to beginners than the type error that this student spent two seconds learning to fix.
&gt; 99% of the time when someone says "`Arrow` is too specific because it requires you to be able to lift an arbitary Haskell function" they're wrong. You can encode plenty of useful stuff with `Arrow` even in the presence of `arr`. Totally agree with this. It would be like saying `Applicative` is too specific because you can use `fmap` and pure. `Arrow` is far more useful to me *with* `arr` than without. If I want to encode arbitrary DSLs within Haskell, I can use a quasi quoter.
In the past 5 years, I had a bug exactly once using `traverse` on a tuple. I had tens of `&lt;loop&gt;` because of typos in `let` statements. I think the "pain" that these instances cause is blown out of proportion.
Which is almost as complicated?
&gt; toList (1, 2) = [2] is a pretty enormous problem. No it's not an "enormous" problem. You hit the problem once, you learn something, and move on. The absence of `let rec` bites me a lot more often (100x I would say).
Thanks that's just the kind of steer I was looking for. Makes perfect sense on reflection. I can marshal this data into ByteStrings instead of Text.
Thanks for that -- I think ByteStrings might be the right way to go in the final analysis. I'll need the toLazyByteString but I can elide the need to decodeUtf8 to create Text in the first place only to encodUtf8 it later. 
Yup, `Applicative` is a good analogy.
The API/implementation is the same, sure. But it has no dependencies. I assumed /u/Tysonzero was complaining about `lens`'s dependencies ...
Ok. [Haskell.org News](https://www.haskell.org/news) uses this Google Group as its source. Perhaps we should direct that to the actual archive?
Oh, I'm afraid there is one more case to consider: I'm just arrogant enough to submit this paper to ICFP thinking that it has a chance of being accepted...
Sounds awesome. I guess the best contributions are ones that improve upon existing work, rather than creating a new library for a new libraries sake. You guys might be able to stake out some corners of the Haskell world that need more love and haven't been addressed, as well, if they dovetail with your work.
That has not been true in my experience using foldable over stuff like `Maybe` all the time in production. I've had zero bugs of that nature. I think the problem is blown out of proportion
What you mean is, "Not only that, but we're bold, too!" ;)
Thanks for pointing out. "Advanced DB mapping" chapter is incomplete, but we cross our hearts and promise to update it by next week :)
Tom, I, myself keep getting confused with `Maybe (Column a)` in SELECT queries. How would you answer the following: &gt; What does having (Maybe (Column x)) during SELECT operations really mean? Does it mean anything in regular SELECT operations? What about LEFT JOIN operations?
&gt; dependently typed programing language with coherent type classes In a dependently typed language, you can ask the programmer to provide the uniqueness proof required for coherency. So, instead of focusing on type classes in particular, I'd think it best to have somewhat special treatment of uniqueness proofs. Although, you don't have to carry around some of the "baggage". I can't think of a strong reason to hide one, but you wouldn't have to force the export of particular uniqueness proofs, the way we do with instances in Haskell.
The most basic instances we use are not unique. For example, the instance of monad for list is not unique. Even though there often isn't a unique instance, there might be an instance that is more useful than the others. Requiring uniqueness lowers expressivity
Same here for nearly the exact same reason. The resource management is the key feature for and also stm-conduit is very useful in multithreaded applications (not that it would be particularly hard to implement in other implementations).
Coherency (local uniqueness) is the only thing that make type class constraints make sense. `Set` and `HashSet` would be very different beasts is the expectation was that the `Ord` / `Hashable` dictionary might change with each function call. Not requiring local uniqueness undermines correctness.
I should have mentioned the definition too: for_ a b c = traverseOf_ a c b for a b c = traverseOf a c b Or maybe it's better to not swap the last arguments, i don't know. With that you could write: for_ each [1..10] print And it won't typecheck if you use Either instead of lists.
Or you can even use `pNewtype` to make this even lighter.
`Functor` instances are always unique in the way you describe (modulo bottoms, etc.). This argument won't win hearts and minds among those who dislike the `(,)` and `Either` instances, though, as they can counter by saying that, weren't it for the "[forced] change of type parameter", `over _1` would be a perfectly good `fmap` for tuples -- which is, *in some sense*, correct. `Applicative` instances in general are not unique due to the choice of the order of effects, as you point out. There is no need for new laws to make them unique, though: both are perfectly legitimate structures that happen to arise from a single type, and it would be arbitrary to outlaw one or the other with a law. (We do have an -- unwritten? -- convention of sorts that `Applicative` instances use forward sequencing unless clearly labelled otherwise -- at least I can't recall ever seeing that not being followed -- but a convention isn't a law.) That point becomes more salient when we consider `[]` versus `ZipList`, or `Either` versus `AccValidation`, cases in which it is clear there are no grounds for considering any of the instances as unlawful. Multiple `Monad` instances are indeed much rarer, but not impossible either -- the reverse state monad comes to mind. --- This debate about `(,)` and `Either` rolls on endlessly because there is no blanket law that might settle it. No one complains about the default `Applicative` for `[]` replacing elements of the first list with their combinations with elements of the second one (and not the other way around), or about `fmap` for `Const a b` affecting the `b` rather than the `a`. However, some people do find the default instances for `(,)` and `Either` instances confusing, counter-intuitive, or error prone, while others believe they are reasonable enough and too useful to be eliminated.
&gt; Turned out he had used `concat`, but not on a lists of lists as he had thought, but on a lists of `Either a [b]`. In case it's not clear I guess the author means "not on a list of lists but on `Either a [b]`". But it's possible the student did something else related.
No it's a law documented in the haddocks of `Applicative`. If it weren't for that law, there would be no categorical reason for Applicative to be a superclass of monad. It's just a product of the fact that in Hask, a monad proves exactly one Applicative
I think your comparison to fgl in the intro is a bit facile. I think of fgl as the "raw primitives" and don't program "in fgl" in the same way I don't program in ASM even though that's what actually runs. It's very easy to write a smallish layer on top of fgl for supporting the two most common methods of using graphs: 1. with real data structures for labeling nodes and edges, or 2. where edges may have real data, but you want a cheap and easy "give me a fresh node with a fresh label" operation. For the former, see [this lpaste of mine](http://lpaste.net/179178) from a while ago. Doesn't support node/edge deletion but those wouldn't be hard to add; your `a` and `b` look like `1 -|()|-&gt; 2` and `a &gt;&gt; 3 -|()|-&gt; 1`. (I don't have any publically available code handy for the latter, sorry. But it's a trick we've all pulled at some point, either with graphs or elsewhere: use `State NodeID` where the `NodeID` is the next available node, and then the main extra operation is `freshNode :: State NodeID NodeID; freshNode = modify (+1) &gt;&gt; get` or so.) I guess what I'm getting at is that there's some deep semantic differences between your approach and existing ones; focusing on syntactic differences/minor engineering quibbles in the intro seems like you're missing the point of your own paper. =P In my experience, practical applications of graphs rarely have unlabeled nodes or edges. e.g. in circuits, nodes are labeled with the operation they perform, and edges are labeled by which "port" they plug into (whether they're the first or second argument to the operation). I suppose you could get away with it in the circuit case if all your operations were commutative, but *demanding* that seems unreasonable. Also a minor point about the discrimination package: linear-time sorting sounds so good, but please do a few small benchmarks before you dive deep. I was playing with it just this week, and in my (admittedly very simplistic) test discrimination used more memory and more time than `Data.List.sort`, right up to the point that my OOM-killer kicks in, so although it may scale better you may not be able to reach the scales where it matters.
Coherence is just local uniqueness. Coherence means there's only one instance visible from a particular module.
Yes I agree. I think the miscommunication here is definitional. I'm using uniqueness to mean that it is the only instance one could *construct* given the laws of a typeclass and you have been using it to mean the unique instance of a typeclass guaranteed by coherence. I'm not entirely sure how one could prove coherence of instances for a type class without a built in mechanism. People have tried to do this but it isn't possible to recover entirely.
I actually may write a rebuttal to that article at some point. 90% of what it mentions are "extensions", which obviously aren't going to be inter-compatible (trailing commas, comments, NaN, Infinity, capitalized True and False, hex numbers, etc.) Despite its flaws, JSON is still far, far better than a sprawling monstrosity like YAML (http://yaml.org/spec/1.2/spec.html). To be fair they aren't doing the same thing, because YAML is meant to be written by humans (and thus has comments) while JSON is really only for reading by humans, but generally isn't fun to write.
I could live with that, though I can't outright recommend it; breakage of existing packages would have to be carefully weighed against any benefits. After FTP was implemented, it would seem fighting against Traversable to be much more difficult, and going after Applicative seems even more difficult. Even if dropping Applicative could be justified, I would suggest adding a applicative dictionary as a member of Monad though. A default value could easily be provided.
I think local uniqueness no more difficult to deal with than global uniqueness. If global uniqueness has type `Unique a` then local uniqueness has type `Context a -&gt; Unique a`. But, in general I agree that both are hard problems.
This is right, but it might be worth phrasing it in a more explicit way. `(&lt;*&gt;) = ap` is a *Haskell* law that we impose *because* there is no categorical reason for `Applicative` to be a superclass of `Monad` -- no reason, that is, other than our wish that the `Applicative` of something with a `Monad` instance matches the applicative gotten out of the monad. That wish, of course, is very much a common sense requirement. Needless to say, our digression is relevant here because the surrounding debate can be framed in terms of how should the reasoning above involving `Applicative` and `Monad` be transferred to `Foldable` and `Traversable`. From this point of view, I would identify four broad stances: * Apply the same reasoning to claim that anything with a reasonable `Traversable` instance should have a matching `Foldable` one. * Apply the same reasoning, but claim that any `Traversable` instance that gives rise to an unpleasant `Foldable` one (for some criteria of unpleasantness) shouldn't exist in the first place. * Deny the legitimacy of `Foldable` and, on such grounds, refuse to apply the same reasoning. * Reject the line of reasoning as a whole, claiming that there are no grounds to generalise it to other pairs of classes.
I agree with most of what you're saying. But I would claim that the `(&lt;*&gt;) = ap` law is a bit more than just a wish. A `Monad` proves an `Applicative`, so we want that `Applicative` to exist somewhere. If we didn't *require* that instance to exist as a law abiding superclass, then anyone's alternate instance would be incoherent with the proof.
FWIW, this has never worked for me. ghc-mod keeps crashing.
In the case of `Either`, that is ruled out by the requirement that `foldMap = foldMapDefault`. (Pointing that out, of course, sends us back to your discussion with ElvishJerrico about `(&lt;*&gt;) = ap`, and to [my aside about it](https://www.reddit.com/r/haskell/comments/5x4yka/deprecate_foldable_for_either/degww2j/).)
there is https://www.stackage.org/haddock/lts-8.3/lens-4.15.1/Control-Lens-Fold.html#v:forOf_, no need to define it yourself.
I mean that J Harrop somewhere said that functional code is hard to understand compared with equivalent imperative code most of the time when doing real world programming, beyond de Fibonacci numbers. Your code seems very hard to understand for non haskellers, but it is a consequence of not separating the elements by labeling them with variables with names that are semantically significative, instead, you try to explicitly chain functional compositions beyond of what is reasonable and aesthetic. That is the cause of the difficulty. But obfuscation is a vice in so many haskellers that It is hardly noticeable. For some reason that I don't understand, some haskellers misunderstand streaming with putting as much code as possible in a single line. that psychological connection baffles me.
&gt; What you really mean is that a Monad instance chooses a particular Applicative implementation among those available that is consistent with the (&lt;*&gt;) = ap law. No. I don't. I mean the existence of a monad instance proves the existence of that Applicative instance, even if it hasn't been coded yet. You're right, we don't *have* to be coherent with this proof. I just think that would be a waste, and we'd lose a lot of generality and power in base if we did that (things like `traverse` would not necessarily work for any `Monad`, for example). It would also be far more confusing to me if a type had an instance that was incoherent with that proof. It's not that hard to use a newtype if you want a differently behaving instance.
&gt; That *can't* be a law about `Foldable` instances in general Indeed it can't, but in any case if that was the `Foldable` instance for `Either` the existing `Traversable` instance would be violating a contract it is supposed to follow.
Can you provide another (the other?) list instance that follows the monad laws?
But the machine does help the human. I use those damn instances for practical purposes and neither I nor anyone I know have ever actually gotten confused or a bug due to the instances. Now sure the first time I ever saw it I was like "huh, weird" and then I was like "oh, that makes sense, that is the only valid instance". I know I would never personally write such a function and make it return a tuple, and I would quickly notice if someone else did, and most likely request they change it. 
Which is a good reason for not using `Foldable` typeclass, but rather a `Fold` data type and having `Traversable` expose the compatible `Fold` as a member. We would no longer have `Traversable f :- Foldable (f a)`, but we would have `Traversable f =&gt; p f -&gt; Fold (f a)` or similar; i.e. a `Traversable` instance would still choose / select a particular `Fold`.
Alright sure. I'll just make all my projects and libraries depend on lens, and learn all the ins and outs of lens. Just because some people get surprised by some useful behavior that I have used and relied on, and have never personally been tripped up by. 
I mean it is the only remotely sane instance, that instance is not sane at all. If you want to be ridiculous like that basically only `Functor` actually gives rise to a unique instance for basically anything ever. So at that point you just don't want typeclasses, so you should probably switch to Idris or some other language that agrees with such a decision. 
you don't want short-circuiting do-notation, or only for Maybe?
Raised an issue here: https://github.com/haskell-infra/hl/issues/204 
&gt; `b &gt;&gt; node 4` ;-) Ah! So `&gt;&gt;` is almost `+` from the algebra. Almost because `node 1 &gt;&gt; node 2` is not exactly the same as `node 2 &gt;&gt; node 1`, although one could probably write the desired `Eq` instance if need be. &gt; Well, I think what this paper is about is advocating a different way of thinking about operating on graphs. fgl gave us the insight we needed for beautiful destruction of graphs -- namely, make an "inductive-like" facade that offers something like pattern matching -- and this enables the creation of graph consumers that are beautiful and efficient. Indeed. I don't mean to contradict you, but would like to note that with algebraic graphs you also have pattern matching on deeply embedded expressions `Expr` (S5.2), but it's not as convenient as with inductive graphs. &gt; Perhaps to that end it would be worth exploring a more complete/complex programmatically-constructed graph in the intro, especially if you can find one where the algebraic structure of the data you use to build the graph closely matches the graph term you produce from that data. Thank you, I'll think about it.
Where did I say that? `Maybe` is very reasonable to be a monad as there's only one type there. It's `Either` that is ambiguous because it has 2 types and neither are morally better than the other.
This looks pretty awesome from a bit of a skim - I'll definitely be going through this in more detail soon. I think it'll take me a while to digest / think about what I could do in an ML like language with subtyping. It'd also be cool to see how this would interact with typeclasses, or how the polarization and biunification pieces would effect / improve error messages in a language without subtyping. 
So what should it be called? `toListOfSomeItemsOfTheTypeOfTheRightmostTypeParameter`?
Idris does have type classes, they just call them [interfaces](http://docs.idris-lang.org/en/v0.99/tutorial/interfaces.html). You can only have one unnamed implementation of an interface, and you can explicitly pass named implementations if needed.
I mean I'm not an Idris developer, but you get what I am saying. Even with quite a few laws instances are rarely truly unique, they are just unique in the sense that only one sane implementation exists.
The look like featured packages rather than a top 10 per se.
...or just use purescript.
I think I get what you are saying and vehemently disagree. If we are going to assume coherence (local uniqueness) without proof, we are sacrificing correctness, and possibly even safety. For example, when you could write your own `Typeable` instance, you could use the holes in the coherence checker to write `unsafeCoerce` and segfault. I actually expect this to "get worse" when we add more dependent typing to Haskell. We should either establish uniqueness, or explicitly pass around the value we mean (and it still won't save us from bottom).
&gt; The plan is to create this for/during the Summer of Haskell so we already have it for the next GSoC application. That sounds like a great idea. A decent start might be to just create a wiki page, and email the maintainers of the top 50 library and developer tools projects at http://hackage.haskell.org/packages/top, asking them to each contribute an idea or two.
Scala had a symmetric Either type for a long time until they finally right-biased it in the latest version because right-biasing was more useful.
Cryptonite is much more mature and has a kitchen sink approach. They have implementations of any crypto primitive that you can think of. Raaz is not so mature. While maturity is a question of time and review, raaz is going to be opinionated in its support for primitives. We will not implement or even if we implement, we will not optimise any primitive which is know to be insecure or at least for which it is not easy to give secure implementation. For example, sha1 is depreciated and will not see any efforts to optimise. On the other hand we will try to optimise sha-2's blake2 (not yet there) etc. Raaz also attempts to use types where ever possible, even in low level pointer manipulation code, to avoid a lot of bugs. I believe this is the USP of the library. I have described that in some detail in the two blog posts below http://www.cse.iitk.ac.in/users/ppk/posts/2016-07-30-Why-another-cryptolibrary.html http://www.cse.iitk.ac.in/users/ppk/posts/2016-08-02-Handling-Secure-Memory-in-Raaz.html 
Taking the diagonal is another valid monad instance. The instance comes from viewing `[α]` as a subfunctor of `ℕ → Maybe α`. Since that is naturally isomorphic to `MaybeT (Reader ℕ) α`, I am fairly certain it should pass the monad laws.
I am not entirely clear on global uniqueness vs local uniquenes. I thought type class were globally unique and implicits were more local.
Maybe someone who take crap away?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/crypto] [raaz-0.1.1 released. • r\/haskell](https://np.reddit.com/r/crypto/comments/5xf5or/raaz011_released_rhaskell/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I'd love for all the scattered libraries like `witherable`, `These`, and the `Recursive` you mention to be collected somewhere -- libraries that solve an edge case where having it in one place provides a big advantage. Although `recursion-schemes` isn't an edge case.
For the low low cost of about 70 dependencies. ;)
If you're writing them as classes and instance clauses with the usual checks then everything is fine. data family Compose :: (j -&gt; k) -&gt; (i -&gt; j) -&gt; i -&gt; k -- k = * newtype instance Compose f g a = Compose { getCompose :: f (g a) } -- k = k' -&gt; * newtype instance Compose f g a b = Compose { getCompose :: f (g a) b } Making up some notation: -- k = Constraint class instance f (g a) =&gt; Compose f g a instance f (g a) =&gt; Compose f g a -- k = k' -&gt; Constraint class instance f (g a) b =&gt; Compose f g a b instance f (g a) b =&gt; Compose f g a b 
I think there's a good argument for keeping `Ord Int`. The rest I can do without.
Now we're talking
That approach, ret-conned 15 years or so into our culture, isn't without cost either. It comes at both a cognitive and runtime cost, requiring everyone to always worry which of two incompatible data types they are using, and to pay needlessly to convert between them. Moreover, when you define a type family to talk about 'sums' in more category theoretic terms you'll have to pick which one you want and half the users will be on the wrong side of the line. Type classes (w/out type lambdas) introduce a bias in the direction a to which argument is the argument fmap maps over. Trying to fix that by introducing type lambdas, and ignoring the original papers on the topic so you can have the 'unbiased' scala case, completely chucks type inference out the window, and leads to the situation they have today, where nobody writes remotely generic code that has anything to do with the useless Either data type they provide, because nobody can. People are forced to flee to inscrutable types like [\\/](https://oss.sonatype.org/service/local/repositories/releases/archive/org/scalaz/scalaz_2.12/7.3.0-M10/scalaz_2.12-7.3.0-M10-javadoc.jar/!/scalaz/$bslash$div.html) in scalaz to get work done instead, and all the problems I mentioned above remain in full effect.
https://hackage.haskell.org/package/husk-scheme
&gt;There are no global data structures in Haskell Sure there are. https://wiki.haskell.org/Top_level_mutable_state I get that there may be controversy over whether you *should*, but there's no question that you *can*.
Try some integration tests first, so you can at least check for breakage. 
It's based on the inductive structure of the Nats, and having the Ord on Int be consistent in the overlap. I could be convinced to drop it, though we would need to provide expose a compareInt primitive.
D'oh. You are right of course, this *is* `ZipList`. Now I have to check what did I miss that made me think it would be lawful...
Thank you, but this doesn't look neither idiomatic nor required by OP's question.
&gt; I'd appreciate it if Tom would explain all of it, though. Is there anything in particular you want explaining? &gt; I think it's mostly immaterial? I hope you're right. Ideally one shouldn't really have to understand `Profunctor`s to use Opaleye. I hoep that's the case.
Not saying it is, just that Haskell does not have any such limitation.
My example of another uniquely-determined instance in another thread was `Recursive` (from recursion-schemes). I'm not sure it's unique as-is but joining it with `Corecursive` and adding a few minor laws should suffice.
 Did you mean something like this ? funarray :: Map.Map String (IOArray Int Int) funarray = Map.empty storearray :: String -&gt; (IOArray Int Int)-&gt; State (Map.Map String (IOArray Int Int)) () storearray x value = do funarray &lt;- get put (Map.insert x value funarray) retrievearray :: String -&gt; State (Map.Map String (IOArray Int Int)) (Maybe ( (IOArray Int Int))) retrievearray arr = do funarray &lt;- get return (Map.lookup arr funarray) putmutablearray = do { storearray "value-table" ( newArray (512,512) 0 ) } getmutablearray = do { retrievearray "value-table";} putThenGet = do {putmutablearray; getmutablearray;} value :: BoardState -&gt; IO Int value (BoardState xloc oloc index) = do arr &lt;- (runState putthenget funarray) v &lt;- readArray arr index return v
`fail` can be handled in pure code for many monads. e.g. `fail _ = Nothing` for Maybe, so `fail "whatever" &lt;|&gt; return True = Just True` for that Monad. And it gets used in the desguaring of do notation. getThreeThings :: Whatever m =&gt; m [Int] do [x,y,z] &lt;- getThreeThings carryOnWith x y z becomes getThreeThings &gt;&gt;= \case [x,y,z] -&gt; carryOnWith x y z _ -&gt; fail "Incomplete pattern match at ...." On the other hand, `throwError` means you aren't ever handling the error in question in pure code. The `exceptions` package provides a more general `throwM` that can be handled by [at least one monad](http://hackage.haskell.org/package/exceptions-0.8.3/docs/Control-Monad-Catch-Pure.html), but now you aren't just dealing with strings, you _have_ to deal with any random exception type. MonadFail moving into its own thing means that we can support the do notation desugaring without requiring all the extra stuff that current ghc exceptions entail. MonadFail is standardizable, and moves an existing method out of Monad to where it belonged all along. It doesn't need anything that didn't exist 20 years ago. GHC exceptions need existentials, have Typeable overhead, aren't standard, etc.
I would like a proof of such uniqueness, as I said uniqueness is rare. And often overly restrictive. It is generally hard to come up with enough laws to guarantee uniqueness without losing too hard on the practicality side.
It's not even clear to know what you intend to do for a human being. I guess you want `StandaloneDeriving`
I love the way to generate the stream of all primes in Haskell.
`microlens` has everything you need to do this with nothing but dependencies on `base` and `transformers`. It doesn't currently have `forMOf_`, but it does have `traverseOf_` which is basically the same thing.
Generating zebra list (in Html using Hamlet ) &lt;ul&gt; $forall (x, zebra) &lt;- zip xs (cycle ["odd", "even"]) &lt;li class=#{zebra}&gt;#{x} 
Not too real-world, but to keep it simple: factorial n = if n == 0 then 1 else n * factorial (n - 1) vs. factorial n = if n == 0 then 1 else n * next -- Recursion taken out to make the non-recursive part clear -- The following is *not* an infinite recursion where next = factorial (n - 1)
FEI (for everybody's information) I'm sure OP is referring to this: &lt;interactive&gt;:3:10: Wildcard not allowed In instance head: _ =&gt; Eq (Foo a) 
Infinite list of random numbers: genList :: StdGen -&gt; [Double] genList g = (fromIntegral (i-mn) / fromIntegral (mx-mn)):genList g' where (i, g') = next g (mn, mx) = genRange g
That's called reactive programming
Since the compiler can work out what instances are required for my implementation to make sense why can I not omit them with`PartialTypeSignatures`? `Eq` is just an example. This question holds for other instances which the compiler does not know about and can't be derived.
It's a somewhat contrived way of doing this: data Stream a = a :&gt; Stream a deriving Functor stream :: (a -&gt; a) -&gt; a -&gt; Stream a stream f a = a :&gt; stream a (f a) idstream :: Stream Text idstream = pack . show &lt;$&gt; stream (+1) (0::Int) It's just a list that is necessarily infinite. You could use it by taking the first `n` elements, for example. takeStream :: Int -&gt; Stream a -&gt; [a] takeStream 0 _ = [] takeStream n (a :&gt; as) = a : takeStream (n - 1) as
Although it's actually not implemented this way, I really like that in theory you could provide a kitchen-sink API like [`disassembleMetadata` in `hdis86`](https://hackage.haskell.org/package/hdis86-0.2/docs/Hdis86-Pure.html) and only really perform the hard work of generating the [`mdAssembly` pretty-printing](https://hackage.haskell.org/package/hdis86-0.2/docs/Hdis86-Pure.html#t:Metadata) of a `Metadata` if the value is actually forced. That's beautiful: You don't pay in advance for stuff you don't need. If you don't need the pretty-printed assembly string, disassembling is quite much faster (too long that I used it to remember how much). You could of course achieve the same thing by explicitly annotating a field as lazy in a strict language, of course. Or just make pretty-printing a separate function, I guess... Which isn't done here because C.
Abstract machines for evaluation during dependent type checking. This is a use case where one *must* have laziness, and if there's no native support, one has to implement it. But in general I don't think laziness is a good default and nowadays I write `Strict` Haskell with sparsely and strategically placed laziness annotations.
I'm making a game in Unreal Engine (in Blueprints, specifically) and thanks to my Haskell experience, I've been trying to use pure functions as much as possible. One of the things I do is that every entity is updated by a function whose type is update :: Input -&gt; State -&gt; (State, Output) The input is for messages it receives from other entities / game state, the state is self-explanatory and the output is for messages it sends to other entities (such as "hey, I damaged you"). The idea, for example, is that it takes collisions in its Input and generates corresponding Outputs for that according to what kind of entity it collided with and its current state. One of my main problems with this approach is that I'm always evaluating the entire input every single frame even if I don't use it entirely. One of the fields, in particular, a list of all items in the game (which is used only in certain occasions) is very expensive to be computed like that. My point is this is wasted computing time that could have been avoided if it had lazy evaluation. My current workaround is to have that particular input be if-gated by one of the entity's outputs, which unfortunately makes the input always be available one frame later (which is also not ideal). So, this is not so much a case of using lazy evaluation successfully as much as wishing I had it.
Just sounds like something that no one's ever sought to implement. I see no technical reason this shouldn't be possible, though I'm no GHC developer. I'd say file a feature request!
Just have those functions take a callback from unit to the type. Boom laziness without the space leaks and unpredictability. Another way of doing laziness is the Idris way where you have compiler support for a lazy type. That's the best of all the worlds in my opinion. 
I agree. Evaluation is determined by which cells are in view. If a cell is not in view, and its expression is not referenced by a cell in view, it doesn't get evaluated. This is essentially lazy evaluation.
Well, in languages like Ruby, Python laziness can easily be done replacing variable by function (without arguments). Yet, pretty much nothing is lazy is those languages. A classic problem is processing in constant memory the result of SQL query. It is easily done in C/C++ using cursor, but (at the time) the only way was to use "batch" mode ,which doesn't compose at all. In other words, strict by default doesn't compose (or it does, making everything strict).
Couldn't you do the same in a strict language by having it take lambdas for arguments instead of values directly? It's more explicit too.
I think this should be quite doable! At some point (probably after the GHC 8.2 release), I want to implement [GHC Trac #13324](https://ghc.haskell.org/trac/ghc/ticket/13324), which asks for GHC to be able to infer wildcard types specifically for standalone `deriving` declarations: deriving instance _ =&gt; Eq (Foo a) (I'm much more familiar with typechecking `deriving`-related code than hand-written instances, which is why I want to tackle it first.) At the bottom of [#13324](https://ghc.haskell.org/trac/ghc/ticket/13324), I also ponder what it would take to extend this to *all* instance contexts, and not just derived ones. I don't think there's any theoretical challenge here—just a matter of getting the different bits and pieces of the typechecker to fit together.
That's your opinon. In my view the variable names could be even shorter. let zipBytes = Zip.fromArchive (foldr (Zip.addEntryToArchive . (\e -&gt; e {Zip.eLastModified = now})) Zip.emptyArchive zipEntries) let bytes = fromArchive $ foldr (addEntry . set lastModified now) empty entries
heh on windows i tried both, didnt have success with either so went back to ghc-mod
I think you can, though I find Haskell code is easier to read because you don't have to litter it with lambdas.
Yes, `aeson` only supports decoding and encoding `ByteString`s that represent UTF-8 text. If for some reason, a user ends up needing to decode json that has been encoded as UTF-16, he or she could use the functions in [Data.Text.Encoding](http://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text-Encoding.html) to massage the bytestring into the equivalent UTF-8 representation and then decode it.
Suppose that you're developing a data structure (Dist a) to model a frequency distribution (I used this for that Markov Chaining random text generator that everyone writes for fun). Two things you might want to do to it are (addItem :: a -&gt; Dist a -&gt; Dist a) and (getProbability :: Dist a -&gt; a -&gt; Ratio). If Dist a is just an unordered list of a, then addItem is just cons, which is very cheap, but getProbability requires you to walk the whole list. If Dist a is something like [(a, Int)], then adding is more expensive but getting probabilities is cheaper. What I came up with is something like this : data Dist a = Dist Int ([a], [(a, Int)]) addItem x (Dist raw cooked) = Dist (x : raw) (map (head &amp;&amp;&amp; length) . group . sort $ x : raw) That way you can have your cake and eat it too. If you add a bunch of items in a row, the expensive "cooked" thunk will get overwritten without ever being evaluated. If you then interrogate the data structure or a bunch of probabilities in a row, you only have to pay the cost of the sorting and grouping once.
nice !
How many times do you experience the example you gave? Is the function trick really that bad in that case? In my experience that's only like one in 20 functions. I hope you understand I am not trying to troll I just don't feel like I am smart enough to figure out the evaluation of Haskell functions. I wish it was easier.
I literally do it all of the time. It's not an exaggeration for me to say your proposal would going to break every single program I've written or worked on at a large scale (maybe except *some* of my FFI libraries), and the only upside would be I'd probably have to type `()` a lot more. That kind of "equational structure" you see in my example is something I use A LOT. &gt; Is the function trick really that bad in that case? In my experience that's only like one in 20 functions. It gets *very* tiring, is one of the main problems. But it still hurts in other places; for example, it is *very* common in Haskell to "let float" things all around the place, like in my example. It makes it more tedious to worry about this, when it's not really the goal of my program. But a bigger problem is, if you don't do this for everything, you either have to have "lazy" (`()`-ized versions) and "strict" (non-`()`-ized versions) of every function, IME. That's incredibly tedious, and you have to do it because *you can't predict the way people will put their programs together*. So what do you do? Force everyone into the world of `()` everywhere, or duplicate everything? Now how do you interface "lazy libraries" into "strict ones"? Write more adapter code, and repeat yourself. This is already a problem where we have strict-vs-lazy APIs in Haskell, admittedly. It's somewhat nicer because you can, to an extent, turn lazy APIs into strict ones, very trivially (`seq`, to an extent). There's also the argument our types do not reflect whether they are strict, but polymorphism over the "strictness" of a function still seems to be a VERY open point in the design space. &gt; I hope you understand I am not trying to troll I just don't feel like I am smart enough to figure out the evaluation of Haskell functions. I wish it was easier. No, that's fine. I apologize if I came off as snippy. Perhaps I am irritated because, in all honesty -- I think this thread is a perfect example of why you might be confused: because I think a large majority of these seem like "tricks" that just makes that one-liner simpler. You think, "Oh, it's a neat thing I can use to make some things more brief. Cool. But do I need it in general?" We get this question *a lot*, so while I admit I'm tired of it, there is clearly something to be said about "Why are people still asking this?" Perhaps we haven't actually answered it faithfully, then. But over time you realize it is *pervasive* in every piece of code we write, essentially. It is not a feature in the sense "Interfaces are a feature of language XYZ". It is a *way of structuring your programs*, and once that is clear -- it's more obvious that it's not only a tradeoff, but that simplistic "alternatives" (lazy lists, `()`-ize everything) are a poor approximation. Finally, I want to be clear that in other languages *you don't need laziness because you solve the problem differently*. I do not entirely "miss" laziness in Scheme, because I can often do a lot of the work with macros if I care enough. (It's more work IMO but not undoable). But in Haskell? It's a crucial part of the language, and one of the reasons I use it. Removing laziness would basically make me find an entirely new programming language, to be honest (or just make my own, since there aren't many alternatives!)
So even LEFT JOINs can't introduce a Maybe? They will be `Column (Nullable a)`?
Maybe that's just showing the limit of lazyness in strict language. Had you been using a lazy by default language, you would have known that the result was lazy had to be consumed. Having said that, in Haskell, the equivalent of HttpUrlConnection would probably be called in an IO monad, forcing its execution anyway.
Pretty often on Maybe, as I say in the mailing list
You would be correct if the same computation that is forced by your use of the value in Haskell wouldn't have to happen anyway in the exact same program in a strict language before you can even pass it as a parameter. The only difference is, that you sometimes might not have to pay the cost of evaluating it in Haskell while you always have to do so in the strict language.
If you don't have laziness by default, you can't have laziness at all. You can selectively force a lazy value to get selective strictness in a lazy language, you can not pass a lazy value through a single function that isn't aware of its laziness in a strict language.
But it's not a pure function, so why would it have to be lazy... Ah well, this is probably a specification thing and I guess I'm about two decades late to the party. 
I recently used laziness to simplify a graph traversal, where the output of some of the nodes depended on the output of other nodes. Without laziness, I would have had to explicitly go through the graph part by part, updating the labels of some nodes before others. Thanks to laziness, I was able to define the output graph *in terms of itself*, with all the dependencies handled for me. The logic was much more declarative and easier to read. Another example in my current work codebase is using memo-tries and lazy arrays for dynamic programming—I wrote an article about [lazy dynamic programming](http://jelv.is/blog/Lazy-Dynamic-Programming) a while back if you want more details. Infinite quadtrees for arbitrary-precision two dimensional indexing were the example that really got me excited about laziness. You could write a quadtree and, with some care on algorithms, you can work with infinite quadtrees, evaluating them as far as you need to display at a given zoom level. And if you extend this to zippers, you can have a quadtree with no top *or* bottom, letting you work with infinitely-sized, infinitely zoomable images—perfect for things like fractals or just for ensuring that images can be arbitrarily combined. This is a more general and elegant alternative to SVGs and the code for it looks far, far better with laziness. As others have noted, there are a *lot* of interesting uses for infinite lists—but most strict languages have some sort of streaming abstraction. What makes laziness interesting is that you can use the same sort of logic for *any* sort of structure.
While this is a very good argument for and example of lazy eval, I fail to see how its an argument for laziness by default.
Right, it's not. I was just too lazy to read the whole API...
Avoiding the challenges of mutability and thread safety are why we use pure languages.
&gt; Yet, pretty much nothing is lazy is those languages. Because these languages don't care about pure functions, which easily turns working with laziness into a a game of minesweeper. For example looking at an iterator might change the iterator. In Python: &gt;&gt;&gt; g = (x for x in range(5)) &gt;&gt;&gt; list(g) [0, 1, 2, 3, 4] &gt;&gt;&gt; list(g) [] I've already learned the hard way that the intuition I've build with Haskells lists simply doesn't work with Pythons iterators and generators.
Well since you replied here, personally thanks a million for writing this book. It allowed me to pick up Haskell which I had failed at earlier with LYAH and RWH.
I came to Haskell from a lisp background where there are essentially 2 ways to do what you're talking about (depending on what you want): a) macros, b) the `quote` function. One of the first things that struck me about Haskell code was how nice it was to let the language figure out where I needed quoting instead of having to get it right myself. It removed a genuine source of errors from my code.
While I agree with you to an extent, the problem is there is no easy way to tell this from the type. You either have to believe the docs, or look at the code yourself which can get really tedious. It's also really easy to reintroduce a thunk and make it non strict again.
In truth in a strict language you would likely just restructure the logic so the computation only runs when necessary which would probably result in a cleaner program anyways. The problem does not disappear in a lazy language, it actually becomes non obvious which code paths result in which evaluations. This is especially true as your application grows. That is in my experience anyways but I have only been using Haskell for five years or so and am by no means an expert. EDIT: Re-read your comment and I actually agree that the laziness can result in a "cleaner" program structure and I do find this very appealing. However I feel that Haskell hides some of the performance complexity away from you as well. Even though from the "code analysis" sense you describe the code is cleaner (less dependencies and nested logic) it is much more complicated in the sense of understanding evaluation. You say easy to force values, but when the code forcing the value is itself a thunk it's turtles all the way down. It's not always clear how to achieve what you want and to tell how things are going to be evaluated without some seriously deep knowledge of how things work. Especially since the optimizer can rewrite your code into something equivalent but vastly different.
I fully agree with you that this can be tedious, especially with data types from certain third-party libraries, where strictness considerations were at best an afterthought in many cases.
How does Haskell deal with the reasonable expectation that lazy output should actually happen? 
Correct.
Sure, so have a pure, strict, language and the "lazy x" type would take care of the mutability and thread safety for you.
Explicitly typed laziness in a pure, strict, language would remove the same source of errors (and also address the space leaks too).
As I've said elsewhere in this thread, emulating laziness in a strict language is far less useful than emulating strictness in a lazy language. In a lazy language, making a lazy function strict regardless of its implementation is as simple as `seq x (f x)`. However, making a strict function lazy in Idris basically isn't possible. It will always evaluate its argument before returning no matter what. If `(&lt;|&gt;)` is written strictly, it will never be able to short circuit on `Just x &lt;|&gt; y`. We pay a lot of costs for laziness. But I haven't found myself convinced that strictness can be as powerful in a pure language. Point being, I don't think strict languages have a suitable substitute for laziness, like lazy languages do for strictness, and the real issue is whether the costs are worth it (which I'm much less certain about)
What language are you thinking of?
I don't think you would generally use lazy I/O for output. If you have a case where you think it might make sense, please share more details?
&gt; In a lazy language, making a lazy function strict regardless of its implementation is as simple as seq x (f x) Yes, and that strictness is not reflected in the type of the function, something which we as Haskell programmers recognise as a Bad Thing^TM. 
&gt; a strict-first language has problems that are less fixable than a lazy-first language That's the part I don't understand. Why? It actually seems to me easier to make laziness explicit in a strict language than make strictness explicit in a lazy language.
&gt; until someone like Phil Freeman goes and uses it to solve the classic 'water-level' interview question[5] in an incredibly concise manner. Could you describe the problem? I've never heard of it and I'm having trouble deciphering what exactly that code is meant to do.
You're given a bunch of integer heights for a bar chart and want to compute how much water could get trapped on top of it. For example: [1,3,1,5,3,2,7] Now have it rain, pouring water down on the chart, where it gets trapped by other bars. Here it'd pool between the 3 and the 5, and the 5 and the 7. The water line would look like: [1,3,3,5,5,7] Finally, subtract those, and compute the total amount of water trapped in the chart: 2 + 2 + 3 = 7 You can implement this many ways, with varying degrees of efficiency e.g. by computing a pair of scans and then combining them, iterating a simulation of adding water to the top of a column until it reaches a fixed point, etc. so folks tend to like it as an interview question. The Tardis solution exploits laziness and a mixture of forward and backwards state to sort of fuse the construction of these two passes together. More advanced versions of the interview question may switch to computing it over a 2d grid, etc. Then the Tardis solution no longer applies. 
&gt; that instance is not sane at all Could you give a precise definition of "sane"? That would be extremely useful get us a long way to actually writing down what the `Foldable` laws are.
&gt; there is no categorical reason for Applicative to be a superclass of Monad This isn't true. In general, there is no relationship between monads and monoidal functors. But there is a result (due to Brian Day I believe) that in certain enriched settings then monads necessarily give rise to monoidal functors. The setup we use in haskell has this property.
I mean considering the original complaint (like the OP itself) was on the basis of practicality and confusing-ness and other non-rigorous terms. There really is absolutely zero burden of proof for me to come up with a hard law or some rigorous rebuttal. So I am just going to say that it is not sane for the same reason that making the ordering of `Int` (`-2 &lt; -3 &lt; 0 &lt; -1 &lt; 2 &lt; 1 &lt; 4 &lt; 3 &lt; 6 &lt; 5 ..`) isn't sane. Which is simply that it is not going to be what someone expects, even if they are familiar with how Haskell works, whereas `toList $ Right x == [x]` and `toList $ Left x == []` is absolutely what I would expect because I do understand how instance heads and such work.
&gt; same reason that making the ordering of Int ... But there's a very good, precise, reason for the [ordering of the integers](https://en.wikipedia.org/wiki/Ordered_ring) to be what it is. It's nothing to do with expectations. It's to do with mathematical properties. Now, is there a mathematical property that `Foldable` instances should satisfy? &gt; I do understand how instance heads and such work. Lucky you! I'd rather not have to, or rather, I'd prefer naive intuition to align, as much as possible, with type theoretic reality.
&gt; More advanced versions of the interview question may switch to computing it over a 2d grid, etc. Then the Tardis solution no longer applies. It'd be interesting to try to solve this with an n-dimensional tardis monad =P
That doesn't make any sense to me.
Yep. That's why the problems boil down to the tradeoff between costs and capability, as I said before. Mutability has **high** costs. I don't think laziness has costs nearly as high as that. I think the added capabilities are good. Regardless, my point so far has been that lazy-by-default is strictly more powerful than strict-by-default, even with workarounds like what Idris has; not that lazy-by-default is strictly *better* than strict-by-default. My personal conclusion based on my point is that laziness is the better default, because of my opinion of the tradeoff.
&gt; my point so far has been that lazy-by-default is strictly more powerful than strict-by-default ... because of my opinion of the tradeoff. Well I'd really like to understand your point of view better! If I came to agree with you it would stop me wasting my time thinking about what a pure, strict, language with explicit thunks would look like ... I'm hoping you'll convince me ...
Also, you don't get the strictness analysis and fusion to make laziness as efficient as possible.
You just have to remember to use the new API. And all libraries you use need to use it as well or else fun things can happen...
The point is having to use that trick is boring. A compiler can use strictness analysis to make those examples optimize away to nothing. Why should I do work that the machine can do for me? In a strict language, there usually is not any strictness analysis, so using the laziness there has a cost that is not present in Haskell. This further discourages laziness. Haskell does lots of fusion that is made possible by laziness and I lose that as well in a strict language. Basically, being strict loses on both performance and reasoning for functional styles of programs. The only area where strictness is a win is in memory critical settings, but in those situations even garbage collection is unacceptable and one really needs a fully affine type system to reason about space usage. 
Forgive my noobness, but how does "where" benefit from lazyness? I always thought it would just write inline what was in the where clause. A kind of syntactic sugar. 
I never quite understood why Windows didn’t just passively allowed &gt; 260 characters, but required you to opt-in with the ugly `\\?\` trick.
No, if you use a variable defined in a where clause multiple times, it is only evaluated at most once (if you don't use it at all, it's never evaluated). Consider f b = if b then g e e else c where e = expensive expression Then `expensive expression` is evaluated at most once if `b` is `True` and no times if `b` is `False`. If we inlined `expensive expression` it could be computed twice. It's equivalent to f b = let e = expensive expression in if b then g e e else c but in a strict language that always evaluates `expensive expression`, even if it's not needed.
So because that type of integer ordering has some neat mathematical properties (related to an unrelated typeclass, essentially `Num`) and someone comes up with a name for it then it is automatically mathematically rigorous to choose that one above all else? What about ordering under divisibility, it also forms some neat laws with `*`. if a &lt;= b then a * c &lt;= b * c if 1 &lt;= a and 1 &lt;= b then 1 &lt;= a * b Plus I don't think you were even considering suggesting that `Ord` should require an `Ordered Ring` (we should probably have an actual `Ring` typeclass before even letting the notion get close to our brains). So explain to me again how `Int` has a mathematically unique `Ord` instance.
To be honest, this and other similar observations are why my usual reaction to "haskell should be strict" is "I don't even know what you mean" (despite the fact that there do exist strict haskell variants).
It was a thing on the internet a couple years ago going by "the twitter water-flow problem" (or something similar). I tried a solution to a streaming version here which was fun http://brandon.si/code/writing-a-streaming-twitter-waterflow-solution/ though I'm sure I would panic if someone asked me to do that in an interview.
The issue with that kind of thing is that the exact "strictness" of a function can be ridiculously non-trivial. For example lets say we have some weird function like: myFunc xs :: [Int] -&gt; (Bool, Bool, Int, Int) myFunc xs = (True, null xs, length xs, sum xs) What exactly is the strictness of this function? So we have to limit what we mean by strictness clearly, one way could be perhaps compare what inputs need to be evaluated to `WHNF` to get the output to `WHNF`. Which isn't the worst idea: I think that could be feasible with a different `-&gt;` for always strict vs ambiguous arguments. Perhaps `!-&gt;` or something like that. So then we would have: seq :: a !-&gt; b -&gt; b (&amp;&amp;) :: Bool !-&gt; Bool -&gt; Bool One issue is polymorphism, sometimes different types will have different strictness for the same function, for example `(+)` with `Int` is obviously strict, but `(+)` with a lazy natural type is not.
I'd argue ```where``` doesn't work as well in other languages because of impurity rather than strictness. 
If you've ever written letrecs in scheme and had #f's pop up unexpected in code where that doesn't fit the 'type' then you'd think it was both. =)
Far from it. Monomorphic functions whose argument is `Seqable` would still not reflect their strictness in their type (and there are many other counterexamples which I won't bother listing).
&gt; being able to reason using totality and productivity Can you explain what totality and productivity have to do with space leaks?
It is a generic problem with self-recursive let bindings. Scheme differentiates let/let* and letrec/letrec* because the latter requires mutation and the former requires you to give your definitions in a strict order with no self-references. haskell's let is also a `letrec` just like `where` with minor differences in scope. You can write everything with one with the other. That said, where is part of a _statement_, while let is an expression, where scopes over |'s, this means this refactoring is actually potentially a pain in the ass. foo [c] | isAlpha c = ... | isPunctuation c = ... where ... foo xs = xs can share `where` definitions across guards. `where` is part of the statement of this pattern match, whereas `let` is an expression on the right hand side of the `=`. So to rewrite all the code you have written with where you might have to change the way the pattern matching is done and guards are set up. (Pattern matching will backtrack and cascade down to other cases below if all the guards fail.) On the other hand, writing all your code with where rather than let can require you to make new helper functions to attach the where clauses to. You really don't want to get rid of either one from the language, as both are useful in different circumstances.
Right up until you need to build a more complex instance off simpler parts. `Free Identity` is a nice partiality monad. that you can run for so many steps worth of work. That completely falls apart if `Identity` isn't a `Functor` though. `Free Maybe` lets it fail after any number of steps, and you can count Just's. `Cofree Maybe` acts as a non-empty list. It'd be a shame if it wasn't Traversable any more just because someone decided that the `forM myMaybe` pattern should be removed from the language. `Cofree []` is a nice rose tree. Kinda needs the instances on [] for it to make sense though. Explicit dictionary passing can be done as a replacement for type classes, right up until you need to know for correctness sake that the same dictionary will be passed at different call sites! The `Set` example of `insert` and `lookup` each taking an Ord dictionary and relying on receiving the same one both times comes to mind. You could force every Set to carry its notion of ordering around just in case, but then unioning sets becomes dangerous, which ordering wins? If you always pick consistently so you can reason about code you get worse asymptotics, etc. Otherwise you need to play some sort of game with baking the ordering w/ a ML-like module system, but then two different chunks of code that happen to return sets written by different people generate incompatible sets... where do you stop? If you have generative modules then they are going to be different types. If you have applicative modules you need to either pick up more and more module parameters for all the set types you use or be able to decide equality of functions somehow, or throw up your hands, write everything as if you have coherent instance resolution, tell people not to fuck it up, and hope they maintain perfect discipline.
Fully law abiding functor instances are unique. That said, there are a number of scenarios where folks use functor instances that are 'too defined'. data Pair a b = Pair a b instance Functor (Pair a) where fmap f ~(Pair a b) = Pair a (f b) for instance is too defined as `fmap id _|_ = Pair _|_ _|_` not `_|_`, but its useful because it behaves better under fixed points. The StateT monad (and its functor for consitency) for instance uses lazy pattern matches on the pair of its results internally. This permits foo = do foo; modify (1:) to produce results.
The Foldable instance is forced by having the Traversable. There is a requirement of consistency between them. Fortunately both `Either e`, and `(,) e` have 1 or fewer 'a's in them, so the uniqueness holds. =) Traversables have an explosive number of possible ways they can traverse, for any data structure with n elements it _could_ be any of the n! orders, and for each n, the instance could pick a different n! But at least in practice, convention supplies sufficient sensibility.
Just as `fmap = liftM` before, `(&lt;*&gt;) = ap`, `return = pure` became a law as part of the AMP. It was convention before, but without that law there is _NO POINT_ to having the inheritance relationship between these two classes as they'd be completely unrelated nonsense. If you have both a Monad and Applicative instance, then your definition must compute something equivalent to that. Now, the notion of equivalence is sort of up to the user, we don't have a way to check laws and functional extensionality, so for instance the haxl folks get away with saying that the number of data fetch passes isn't included in their semantics, but to do so they have to limit themselves to batching up queries that don't change the world they care about. 
Adding the Monad instances to the type forces laws on the Applicative instance to be consistent. Otherwise `traverse = mapM` and a whole bunch of other stuff breaks down and we'll forever have to keep a completely redundant set of things in the language for no reason.
If the compiler knows something is total or productive, it can be more liberal about doing optimistic evaluation without changing the semantics of the program. Totality and productivity make runtime semantics irrelevant (to a certain extent). All that changes with adding laziness is some things terminate faster. The step after totality and productivity is being able to reason about asymptotic cost of a function. With a form of laziness analysis, the compiler will be able to determine what should be thunked and what should not with greater precision. 
But Vector ∷ ℕ → Type → Type can
Absolutely. It is isomorphic to the reader monad (Fin n -&gt; a) e.g. `V n a` in `linear` has this instance.
The point I was making is just because an instance is not the only one could endow a type with doesn't mean that it doesn't make sense to pick one when one is clearly more useful (in this case allowing Haskell to provide a monad instance as well).
Now that I think about it, that is the only instance one could write for that type. The zippy instances are the only ones that will type check for this type. 
Yup -- whereas if you allow variable lengths you can end up with, so to say, gaps in your naturals, and things get awkward.
Yep. Showing it is just 'Reader' puts that fact in stark relief.
Unfortunately, it requires the instance to line up with `Applicative` if there is one. Why is that requirement there? I mean the class makes up its own operator for the operation, so why require that the operator line up with applicative as it is in a separate hierarchy. For example, `Event`s in FRP are monoidal in at least 3 different ways. 
Which in turn was bumped to https://github.com/haskellnews/haskellnews/issues/51
My justification is as I said above, the same as my justification for `Ord Int`. Neither has any concrete mathematical proof of uniqueness or supremacy (as I said, ordering by divisibility also has neat laws with appropriate operators from an unrelated type class, it might not have its own name, not that it should matter at all), both just are the most intuitive ones that can actually be implemented. So I am saying that I don't currently have concrete rigorous proof that `Foldable Either` is the only sane instance, although considering how silly the other instance is it is probably possible to come up with a decent law that gets rid of it. But my point is that you don't have a rigorous proof of the uniqueness of `Ord Int` so why should I have to provide one for `Foldable Either`. Unless you are potentially willing to part with `Ord Int`.
I don't think that is a fair comparison at all. Mutable vs immutable and lazy vs strict are not at all similar arguments. Mutability can straight up ruin correctness of code, but worse case scenario laziness just affects performance (constant factor due to thunks and such) / memory (not always a constant factory). Lazy languages terminate in strictly more scenarios than strict languages.
One doesn't have to think about execution order when writing where clauses.
Yeah I 100% agree, which is why it is always so frustrating when I hear people arguing again and again for strict by default, why can't people just let Haskell be Haskell, pretty much every other language is strict but I like the huge productivity I get in Haskell from laziness (and other features, but laziness is a big one) and would really rather it not be taken away from me. Luckily I know that Haskell moving to strict by default is literally never going to happen, but it's still pretty annoying seeing it argued for all the time.
When one knows the concrete type, being strict in it is a given because one can pattern match etc. Having to write strictness annotation would be quite painful for monomorphic functions. I think putting `seq` in a type class is enough. It creates a distinction between `foldl'` and `foldl` in type signature. Thus restoring free theorems. It disallows strict functors. Also, it eliminates most of the problems such as `⊥ ∘ id ≠ ⊥` because function types would not be members of the `Seq` type class. 
It's okay if you don't want discuss these issues rigorously. Just don't expect your opinions to have any weight. 
Could you post your stack file? I've tried following this: https://github.com/reinh/reflex-dom-stack-demo/blob/master/stack.yaml and I get this error: $ stack build Ignoring that the GHCJS boot package "aeson" has a different version, 0.9.0.1, than the resolver's wanted version, 0.8.0.2 Ignoring that the GHCJS boot package "attoparsec" has a different version, 0.13.0.1, than the resolver's wanted version, 0.12.1.6 Ignoring that the GHCJS boot package "scientific" has a different version, 0.3.3.8, than the resolver's wanted version, 0.3.4.4 Ignoring that the GHCJS boot package "case-insensitive" has a different version, 1.2.0.4, than the resolver's wanted version, 1.2.0.5 Ignoring that the GHCJS boot package "hashable" has a different version, 1.2.3.2, than the resolver's wanted version, 1.2.3.3 Ignoring that the GHCJS boot package "async" has a different version, 2.0.1.6, than the resolver's wanted version, 2.0.2 Ignoring that the GHCJS boot package "vector" has a different version, 0.11.0.0, than the resolver's wanted version, 0.10.12.3 Ignoring that the GHCJS boot package "text" has a different version, 1.2.1.1, than the resolver's wanted version, 1.2.1.3 Ignoring that the GHCJS boot package "dlist" has a different version, 0.7.1.1, than the resolver's wanted version, 0.7.1.2 Ignoring that the GHCJS boot package "pretty" has a different version, 1.1.3.2, than the resolver's wanted version, 1.1.2.0 Ignoring that the GHCJS boot package "containers" has a different version, 0.5.6.3, than the resolver's wanted version, 0.5.6.2 Ignoring that the GHCJS boot package "transformers" has a different version, 0.4.3.0, than the resolver's wanted version, 0.4.2.0 StateVar-1.1.0.1: configure data-default-class-0.0.1: configure dependent-sum-0.2.1.0: configure base-orphans-0.4.4: configure Progress: 4/47/home/cats/.stack/setup-exe-cache/x86_64-linux/Cabal-simple_mPHDZzAJ_1.22.4.0_ghcjs-0.2.0.20151029_ghc-7.10.2: streamingProcess: runInteractiveProcess: exec: does not exist (No such file or directory) /home/cats/.stack/setup-exe-cache/x86_64-linux/Cabal-simple_mPHDZzAJ_1.22.4.0_ghcjs-0.2.0.20151029_ghc-7.10.2: streamingProcess: runInteractiveProcess: exec: does not exist (No such file or directory) /home/cats/.stack/setup-exe-cache/x86_64-linux/Cabal-simple_mPHDZzAJ_1.22.4.0_ghcjs-0.2.0.20151029_ghc-7.10.2: streamingProcess: runInteractiveProcess: exec: does not exist (No such file or directory) /home/cats/.stack/setup-exe-cache/x86_64-linux/Cabal-simple_mPHDZzAJ_1.22.4.0_ghcjs-0.2.0.20151029_ghc-7.10.2: streamingProcess: runInteractiveProcess: exec: does not exist (No such file or directory) 
It doesn't seem that simple to me, unfortunately: $ /home/cats/.cabal/bin/cabal sandbox init Writing a default package environment file to /home/cats/sandbox/reflex-stack/cabal.sandbox.config Creating a new sandbox at /home/cats/sandbox/reflex-stack/.cabal-sandbox $ /home/cats/.cabal/bin/cabal new-configure Resolving dependencies... cabal: Could not resolve dependencies: trying: reflex-dom-0.3 (dependency of reflex-stack-0.1.0.0) next goal: webkitgtk3-javascriptcore (dependency of reflex-dom-0.3) rejecting: webkitgtk3-javascriptcore-0.14.2.1, webkitgtk3-javascriptcore-0.14.2.0, webkitgtk3-javascriptcore-0.14.1.0, webkitgtk3-javascriptcore-0.14.0.0 (conflict: reflex-dom =&gt; webkitgtk3-javascriptcore==0.13.*) rejecting: webkitgtk3-javascriptcore-0.13.2.0, webkitgtk3-javascriptcore-0.13.1.2, webkitgtk3-javascriptcore-0.13.1.1, webkitgtk3-javascriptcore-0.13.1.0, webkitgtk3-javascriptcore-0.13.0.4, webkitgtk3-javascriptcore-0.13.0.3, webkitgtk3-javascriptcore-0.13.0.2, webkitgtk3-javascriptcore-0.13.0.1, webkitgtk3-javascriptcore-0.13.0.0 (conflict: requires pkg-config package webkitgtk-3.0&gt;=1.1.15, not found in the pkg-config database) rejecting: webkitgtk3-javascriptcore-0.12.5.1, webkitgtk3-javascriptcore-0.12.5.0 (conflict: reflex-dom =&gt; webkitgtk3-javascriptcore==0.13.*) Dependency tree exhaustively searched. $ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.10.3 $ /home/cats/.cabal/bin/cabal --version cabal-install version 1.24.0.2 compiled using version 1.24.2.0 of the Cabal library Any ideas?
Is it possible to make "ring fenced" donations in order to have an influence over which projects benefit?
&gt; the current Ord Int is also not unique with the current Ord laws? This is completely true. In an earlier reply I wrote: &gt; As a total order, it is perfectly "sane" if it satisfies the laws of a total order, which any arbitrary rearranging of the Integers would. There are infinitely many implementations of `Ord` for Integers. There is, however, only one implementation congruent with all the other properties that Integers have. &gt; Do you want to add a hard Ord law, that if a type can be made into an ordered ring that it is a law that it must be? If I were to add any remotely quantitative requirement, it would be that the `Ord` instance for a type is compatible with a majority of the ordered algebraic structures that the type permits. Also if it wasn't clear, I'm not arguing typeclass instances should be unique. I think that the best default implementation for a type can be reasoned about on a case-by-case basis. In the case of `Ord Int`, there are overwhelming mathematical reasons to favour the canonical integer ordering. In the case of `Monoid Boolean`, it is equally valid (mathematically, and in terms of utility) to have `mappend = (&amp;&amp;); mempty = True` or `mappend = (||); mempty = False`, thus there should be no default. In the case of `Foldable (Either e)`, there is a valid yet completely useless instance, which would lead me to suggest that the abstraction itself is broken.
In what way is R lazy? I'm not saying you're wrong, it's just that from my use of R I can't see where the laziness is. 
&gt; There is, however, only one implementation congruent with all the other properties that Integers have. Can you give me some proof of this? Also what do you mean "all the other properties", what properties, and where in `Ord` does it say that these properties are more important than any other ones we could have with a different ordering? Is it just that more people know about them and they have names, that seems a bit arbitrary. &gt; If I were to add any remotely quantitative requirement, it would be that the Ord instance for a type is compatible with a majority of the ordered algebraic structures that the type permits. Majority? Which structures? The ones that have names and are on wikipedia? We can invent all the algabraic structures that we want, infinitely many, all with their own laws. Also "majority", "compatible", you are giving me nothing concrete or verifiable or quickcheckable or anything here. Give me something. &gt; Also if it wasn't clear, I'm not arguing typeclass instances should be unique. I think that the best default implementation for a type can be reasoned about on a case-by-case basis. In the case of Ord Int, there are overwhelming mathematical reasons to favour the canonical integer ordering. I agree, but again this is not rigorous (Which I think is OK, but you have to be consistent) and I think the current `Foldable Either` is also the one that should be favored out of all the possible implementations, the other possible instance I have been shown is far less useful and intuitive. &gt; In the case of Foldable (Either e), there is a valid yet completely useless instance No more valid than another ordering for int. You have still not come close to giving me an actual concrete law that all other `Int` orderings would break for me to concede anything on the "validity" of such a `Foldable Either`.
Apparently, R is lazy in its argument, meaning arguments of a function are actually thunks. Variables (assigned via &lt;- or =) are strict so (I think). You probably can try it by yourself.
Sure, if you have `Query (Column PGInt4, Column PGBool)` you can `fmap (\(x, y) -&gt; (x, y, Nothing)` and get a `Query (Column PGInt4, Column PGBool, Maybe whatever)`. But that's not really very useful. It's not that `SELECT`s have no way to omit columns, but rather it's that they have no *need* to omit columns.
Thanks for the suggestion. I probably should use Purescript before theorizing about pure, strict, languages. [As an aside I certainly dislike the look of `ArgumentDo` and this `_` thing, which presumably constructs a function with `_` as its argument. Maybe I'll change my mind when I use it.]
Late, but commenting in case this is useful. Haskell is a perfectly good language for writing cross-platform scripts in, especially if you take advantage of packages like [shelly][shelly] or [turtle][turtle]. There are a fair few pages on using Haskell as a "scripting language": here a few ([1][1], [2][2], [3][3]). It's pretty common to have scripts on your computer for all sorts of common tasks - doing backups, running common commands, monitoring or displaying log data - so why not make the most of the advantages Haskell offers, and try writing your scripts in it? I've slowly been converting existing scripts from `bash` to Haskell (and sometimes Python), and now have much more confidence in their behaviour. (Perhaps that's due to insufficient expertise with `bash`, but I don't mind; I'm happy to spend my time on other things.) [shelly]: https://hackage.haskell.org/package/shelly [turtle]: https://hackage.haskell.org/package/turtle [1]: https://docs.haskellstack.org/en/stable/GUIDE/#script-interpreter [2]: https://blog.otastech.com/2015/12/scripting-with-haskell/ [3]: http://www.haskellforall.com/2015/01/use-haskell-for-shell-scripting.html 
It's the same, but you had to do an analysis to put the computation of the expensive expression in the right place (this is, of course, a simple example, but you could have complex conditionals, each of which use a different subset of the intermediate values) and you have to put it there even if that isn't the clearest place.
&gt; `toList @(Either _) . pure == pure` &gt; toList @(Either _) /= const [] I was hoping for laws that apply to *all* `Foldable`s! I concede that not every `Ord` instance comes from a ring structure, but `Ord` does have other laws, after all.
By the way, the reason I would give for the `Foldable Either` instance being the unique sane one is that it is the only one that is compatible with the (unique) `Traversable` instance. Unfortunately this law doesn't extend to all `Foldable`s.
Why, a Star lifts a functor into a profunctor! What if I want a Monad constraint? It surely is not the Monad instance; that merely wraps the Star in a Monad!
But the existing laws of Ord don't cover Int's ordering uniquely as we discussed before. So we are both talking about laws and reasons for instances that aren't fully general. Neither of us can give a law for all Foldable/Ord instances that gives us uniqueness in our respective instances. But both of us can give good justification for the current instances. And yeah I forgot that Traversable was uniquely determined for Either. And obviously all Traversable and Foldable should agree. 
As far as I can tell, that's not possible, and I'm also not sure if that's benign for the community. We don't really select "projects": students just send in proposals and then the best proposals are picked. If you want to have an influence over which proposals get picked, you can sign up as a mentor.
Weight and rigor can coincide. However, weight and popularity coincide more often. Sanity is decided by consensus, not always backed by patterns of belief or action. His arguments mgiht not have to be mathematically rigorous in order to convince a sufficient number of the right people to make his opinion the only "sane" one.
Ok, I thought that, as you have a stack file, you actually were using stack and trying to make cabal freeze to distribute to other people. Now, that makes sense.
Nice! I've been contemplating a tool like this ever since Stack came out. Is it capable of giving version bounds that respects PVP (i.e. `base == 4.9.*` instead of `base == 4.9.1.0`)?
Honest question: Why don't you just use Stack instead of jumping through hoops to reduce the as you say huge pain with cabal a bit and be able to keep using cabal? What's keeping you from switching?
What's a string diagram? Wikipedia says [this](https://en.m.wikipedia.org/wiki/String_diagram), but that has nothing to do with Haskell and then you said you like category theory, so that can't be it.
Start writing simple programs. That's it. You need no category theory and you certainly don't need to read through HM type system proofs to begin with Haskell programming. It might help to know these things, but it's not a requirement.
I disagree with your example C code. That's not where I would write "e = expensive expression", it violates multiple good practices. The amount of lines in-between the expression declaration and usage needs to be kept to a minimum. Also, since it's only used inside the smaller scope (inside the if block), it's a better practice to declare it inside the if block. If it's used in multiple distinct blocks, but not others, it's best practice to declare it as a separate function and call the function when needed. Those practices both improve readability and performance to a same level as the Haskell code. With that in mind, I agree that the Haskell code is much simpler. I had to think about best practices to write good C code and in Haskell, I didn't have to think about it. 
I was thinking about it, but just like with integers, we have two possible monoid instances: `(empty, overlay)` and `(empty, connect)`. It's unclear why one of them should be preferred, so I think we could instead define two newtypes `Overlay` and `Connect`, mirroring `Sum` and `Product` newtypes for integers. We could then indeed define `forest` as a `foldMap`.
Well, the problem is there is no closure in C so how do you write those functions is they have arguments like `e = complicated_function(parameter 1, parameter 2, etc ...)` 
I'm sorry, I don't understand what closures have to do with this. int foo(int n) { int a = something; int b = something else; switch(n) case 0 : return e(a, b); case 1 : return e(a, b) + f(a, b); case 2 : return f(a, b); } int e(int param1, int param2){ //complicated expression } // same for f Isn't that equivalent? 
If types were strict by default, how would you use them as fields in a lazy type without forcing all but the last evaluation step?
This is awesome! If it could also go from `cabal.config` to `stack.yml` I would keep it permanently installed on my machine.
I mean Edward said in the talk why those alternatives had their own significant issues, I would rather not restate what he has already said. But I guess if despite their downsides you think they will be usable enough then go ahead and make a language that implements them. 
I know this was just an exercise for fun. But if anybody is looking to build a production rest API for postgres you should check out one that already exists. https://github.com/begriffs/postgrest I've been using it for awhile myself, very useful.
He's describing a problem that has _nothing to do with cabal itself_, but instead entirely due to interoperability between `stack` and `cabal-install` tools. Then he fixed the interoperability problem. EDIT: Sorry, I'm just annoyed that it looks like you're twisting his words. If it was unintentional I apologize.
Stuff like this is exactly why Microsoft has taken so long to reduce the limit. And it's not just opting in to long file paths: if the subprocess allocated MAX_PATH buffer to hold a filepath, passing a long filepath will cause it to just not work at all.
Not sure exactly what you're asking but it sounds like you will enjoy digging into haskell, and will appreciate the power to create nice abstractions that the type system gives you.
Doesn't do that now. Would certainly be possible.
First of all, I currently almost never use cabal directly, but instead use [mafia](https://github.com/ambiata/mafia) which is a more light weight solution to many of the problems that Stack solves. For those who haven't seen mafia, its a thin wrapper around cabal. So why maifa over stack? Well I joined Ambiata almost a year ago and we use mafia exclusively. Its deeply embedded in our work flow. Before I joined Ambiata, I used cabal sandboxes with multiple versions of GHC which I could switch between with a shell function. When I *tried* stack it wanted to change the way I worked and because I already had a solution to many of the problems Stack solves, Stack didn't offer me any obvious benefits. 
Wouldn't it be enough to just put [cabal.config](https://www.stackage.org/lts-8.3/cabal.config) provided by Stackage into your cabal project? That `cabal.config` contains a few more packages, but AFAIK it's not problem if your `cabal.config` locks down packages that you don't actually use.
I was actually just thinking of generating a `stack.yml` file with `extra-deps` entirely filled out. That way the stack user could at least build the application, even if they didn't get the reused of a LTS snapshot.
Doesn't the `stack.yaml` file require a `resolver` to be set? How would `jenga` figure out which resolver to use? &gt; That way the stack user could at least build the application, even if they didn't get the reused of a LTS snapshot. I thought Stack would already supply a way of stack-ifying a project.
Figures you would stop responding, you do realize that half of that previous comment was wrong right? Or are you just in denial?
I think they would be nice to have, like it's nice to have annotations when a function can do IO.
&gt; A prg based on ChaCha20 Cool, I was thinking of that. But I would like to see it with an interface like [tf-random](https://hackage.haskell.org/package/tf-random), for general (non-cryptographic) use. Also, I was thinking that maybe ChaCha20 is simple enough that it might be possible to convince GHC to registerize it and get a reasonable pure Haskell implementation. But that is just a dream, so far.
There used to be a link on the stackage site for each version, but it was removed at some stage.
Was this just an oversight or were the links removed intentionally?
Why do you think they would be nice to have? IO isn't an annotation. It is a monad. The point of IO is to preserve referential transparency. I don't see how these hypothetical annotations are comparable.
I was waiting for someone to catch me on that :) I wanted to validate that cat theory is not my problem, just the things it uses without explanation twists my stomach
Yeah, i did read "only in the category of types", which I did not really recall to be category theory, but i digress.
Yeah, sure, totally agree. In some cases using postgrest will save many days of your life :D I haven't aimed to teach how to build "real" REST API :)
Because in a lazy language I want to know when something is done strictly, and in a strict language I want to know when something is done lazily. It's a fair point about IO being to preserve referential transparency. But it's also useful as an effect type.
Once seq is placed under a type class, we have free theorems and complete expression substitutability. Why do you need to annotate strictness and laziness in Haskell? I would understand if we had a proper codata and data divide because then one could express things like traversing through an infinite stream is productive given a sufficiently productive Applicative, but we don't have such a vocabulary and I am not sure how Haskell will get there while maintaining backward compatability.
You do not need knowledge of abstract algebra to be an effective haskell practitioner. I personally enjoy using these concepts as tools for thought, but you can get tons done without them.
I don't think you have anything to worry about. For a long time I've held the belief that Haskell developers are easier to find than people think, and that the average quality of those developers will be much higher. In the past few months I've discovered that it is even easier than I thought! I don't have exact numbers, but I think we've hired something on the order of 10 Haskell developers in the last four months or so. There were several more we would have liked to hire, but weren't able to due to timing, location, etc. And we have not compromised at all on our hiring standards for the sake of numbers.
I wouldn't be intimidated by Ed's talk on lenses if I were you. You could easily find equally intimidating talks on the internals of the JVM or on the implementation of PostgreSQL, and yet thousands of "mere mortals" use those every day. There's nothing unique about Haskell in having very in-depth technologies. Which means (to answer your question), you can use Haskell just fine without knowing every detail. Just like any other language. Wanna create a web app with PostgreSQL? Great, `postgresql-simple` let's you write SQL yourself and execute it without any advanced type-level insanity. Just throw together some IO and bam, you've got an app; no indexed traversals necessary. What makes Haskell great is that it lets you do things the simple and familiar way, but provides you the tools to *learn* how to do it smarter.
&gt; Why do you need to annotate strictness and laziness in Haskell? Because operational semantics is not denotational semantics!
Bye
Stack is nice but really painful to use on a mobile tether (downloads soo much and throws everything in the bin a lot). For this reason I'd consider mafia, thanks.
https://www.youtube.com/watch?v=T88TDS7L5DY might act as a slightly better presentation on lenses from me. The NY Haskell talk was the first time I'd spoken about the lens library, and the abstractions used therein had yet to fully 'gel' into their final form. That said, my talks tend to be targeted at folks who already know Haskell really well, or are comfortable with the rewind button. =) I freely admit that others tend to do a better job of digesting that content and turning it into slower paced tutorials that are easier to follow along. (I did run a 3-4 hour workshop on lenses at ICFP a couple of years back that had enough time to let the topic breathe, but sadly, it wasn't recorded.) 
But, you could argue, they'll be over 1.5-2x as efficient and productive, so...
While you can probably find intimidating talks on JVM or postgres, I think they are a lot harder to accidentally stumble upon. For example, if you enter JVM into YouTube search, you get plenty of introductory videos with diagrams an pictures. However, if you enter "lens haskell", the very first talk is the one by Edward Kmett mentioned by OP. From the thumbnails all others look like similar talks. I hardly ever got an academic paper when looking for Java or SQL answers, but it's not an uncommon experience when looking for haskell information. 
That's more of a problem with the kinds of material our community has made available than with what that material should cover or how hard it is to learn. The reason that googling the JVM turns up simple results is because people have put enormous effort into providing beginner friendly material.
Like a Hoogle tailored to lenses?
There have been a number of different attempts to bang out a better operator sheet. I'm definitely open to improvements on that front. It is a wiki after all. ;)
Well, I find Hoogle difficult to use (types are a hard way to search for things, especially if the types are complicated). Really just want a table grouped by the monad classes I'm using that tells me all the things I can do, with fly-out examples. When I have a `StateT` I know I'm in a `MonadState`, but I don't know what I can do and I forget what operators I can use, so that's kind of where I want to start.
The only such trick I currently know of is to use foldl' :) And other strict evaluation stuff.
I think the lens wiki tabular form was on the mark, just the terminology a little confusing.
Note for ghc and cabal to migrate out of the testing/edge repository purgatory, more people need to validate it works. Its pretty easy to do now too: # apk --no-cache add --repository http://dl-cdn.alpinelinux.org/alpine/edge/testing ghc ghc-dev cabal For now however, ghc and cabal are ported to x86_64 and for armhf just ghc, cabal to come later. This porting is for ghc 8.0.2, and requires alpine linux 3.5. To build static binaries with ghc on alpine linux, please do not do what this post recommends: https://www.fpcomplete.com/blog/2016/10/static-compilation-with-stack All you need to actually do is change your Executable targets to include ld-options: -static like so: sed -i '/Executable .*/a \ \ ld-options: -static' package.cabal Note the r2 release of ghc is different from my prior ports in that the profiled libraries are split between ghc-dev, and ghc itself. This is similar to the debian ghc, and ghc-prof packages. In fact I basically stole the idea from them, (thanks for that by the way debian haskell maintainers!) I never wanted to become a ghc maintainer but just wanted a static pandoc binary and a version of Idris I can use to make static armhf binaries with. For now though if you could test that the upstreamed ghc and cabal work that would be great. Note, stack 1.3.2 is still available from my old port but I'd like to remove all those old ports from the internet as its costing me about $6/mo in s3 transfer costs. I'll get stack building once I get some agreement on how ghc produced things get built in alpine linux. Cheers!
That would be very nice! I would enjoy working on something like that, any idea how to accomplish this?
What's the percentage of that "not always desired"? With database schema, you can specify these attributes of very fields: * Documentation * Type * Validation No more Swagger, RAML, or other API specifications. Just provide database schema directly to API consumers.
&gt; `reflection` is a library rather than a separate language feature, which is more “elegant” in some sense since it’s a derived concept instead of a primitive. I disagree. I don't consider it any more elegant, since it relies on unsafe coercion. In fact, having to depend on another library for something that isn't elegant strikes me as *particularly* inelegant. They're equally tied to GHC (because of said unsafe coercion), so they're pretty much the same in this regard, as far as I'm concerned. Given that, I'd probably tend to choose implicit params should it be needed. The compiler can definitely optimize it much much better. However, there are some cases where implicit params simply won't work, while reflection will. Mainly instance declarations: {-# LANGUAGE TypeApplications #-} {-# LANGUAGE ScopedTypeVariables #-} import Data.Reflection import Data.Proxy newtype WeirdSum s a = WeirdSum a instance (Reifies s a, Num a) =&gt; Monoid (WeirdSum s a) where mempty = WeirdSum (reflect @s Proxy) WeirdSum a `mappend` WeirdSum b = WeirdSum $ a + b - reflect @s Proxy This instance allows you to effectively choose your zero at runtime, subtracting it for every addition. Can't do this with implicit params.
There are couple of design issues with randomness. 1. Firstly I do not want to expose any low level details like spliting or even seeding to the user, it is very likely that users (we should assume that they are not really crypto-savy) might get it wrong. In that sense the user should not and cannot make any assumption on the randomness. This I feel would avoid a lot of confusion. It should be possible to replace chacha with some other prg and the user should not be able to tell the difference. 2. The cprg should be able to secure its seed, in the sense that it should be able to prevent the seed from being swapped out to the disk. This mode of operation is required if we plan to use the cprg for long term key generation Regarding your second point on pure Haskell implementation, it is pretty difficult to get fast and secure implementation in just Haskell alone. Also if we do not care about cryptographic security, there might be better candidates (in terms of speed i.e) Having said that the current implementation is unsafe in multi-threaded environment. The memory used by the prg should be protected by a mutex to avoid random data being repeated across threads which it does not currently. I have not got too much time to think about it to come up with a clean solution as of now. The current plan is to get enough primitives out there so that we can (begin to) build a library for a protocol like ssh or noise. Once we start building such a library it should give enough feedback to iron out some of the api and other issues I hope. As far as I know there is only one down stream user of raaz so far (Joey Hess, keysafe). I am in touch with him for some unbiased feed back which should improve the API/security. If there are others like him I would invite you to give feedbacks.
Yes. &gt;All I'm saying is it would be nice if more people made all this knowledge accessible to mere mortals. Pedagogy is a skill as programming and computer science is. If you want good pedagogy give money to authors that do a good job. &gt;deciphering all the theory? A lot of Haskell's cutting-edge "theory" is really pretty disjoint from e.g. category theory. So it's really just a matter of sitting down with pencil + paper and making yourself understand it. 
&gt;types are a hard way to search for things, especially if the types are complicated Really? I think it's actually pretty amazing for certain things. For e.g. yesod maybe not, but that's why there's a yesod book. 
I have not, no. I'm not very familiar with CI(C)C
Could someone give a real world example of `reflection`, for example show how to do this runtime configuration thing that I'm assuming touches `IO`? (referenced a lot as a use case, but I couldn't find an example anywhere or a blog post on it).
For Ed's specific case of instances based on certain compositions of "underlying" instances, uniqueness proofs are probably the best solution. For a lot of the stuff in Lens, passing a dictionary around works fine.
&gt; The point I am trying to make is that it is often useful to pick an instance even if it is not unique. I don't think we should privilege instances like that. Instead, I think we should design type classes so their instances are unique, and when we can't we should use explicit dictionary passing.
I am not talking about coherence. Of course, we need instances to be coherent. What I am saying is, is that it is not wrong to choose an instance when there are other possible instances one could write while writing the code. I agree quite vehemently that the category of constraints must be thin. What I disagree with is requiring that the instance one writes for a typeclass be the only one one could possibly choose to write. I do not think this is damaging to correctness because the other instances can be accessed using newtypes.
I think in this particular case it's more about how Simon Peyton-Jones is just really amazing at making complex concepts very understandable to mere mortals (as I think readers of his papers can attest), and about how he was able to approach lenses from the perspective of an outsider rather than that of the implementor. FWIW, I've been working with Haskell for about 4 years now, part of that professionally, and only even bothered to learn lenses relatively recently, and still only used them *once* in production code. I don't consider the intricacies of our tower of abstractions as part of the core language, but rather as part of the ecosystem. You can use however much or little you want. There are very few that you can't really get around in order to be productive (e.g. Functors and Monads, and at least the concept of folds), but even then you don't need to know the category theory, but just how you use them in practice. It's a constant learning experience, and knowing more about such concepts does end up making you *more* productive, but in my opinion you should pick these things up as you go. When you can see repeating patterns in your code, you might want to look around or bit or ask someone to see whether there is an abstraction that does this work for you and then start integrating that into your work when it makes sense.
We would just use a newtype. In your solution we would pass in a different dictionary. In both cases, one has to do something fairly similar. I am not seeing the difference here.
[Here](https://productivedetour.blogspot.com.es/2015/02/a-dynamically-generated-fromjson.html)'s an old post of mine, referring to [this gist](https://gist.github.com/danidiaz/2bf98df3799c33ee5e9f). I use reflection to create typeclass instances dynamically; I have never used it for configurations.
I'm actually comparing the memory usage of Haskell and Common Lisp right now, using ghc and sbcl. Haskell is not really too bad, because there are a lot of situations where sbcl uses a lot of memory too. Both languages often use many times more than I expect. But maybe I just have unrealistic expectations of memory usage. And memory does keep getting cheaper. So, even though I know Common Lisp a lot more, and like it a lot more than almost anything, nevertheless I'm leaning slightly towards favoring Haskell now. Also, toy programs that use a lot of memory are often deceptive. The program not only spends time doing its work in all that memory, but also spends time building the data structures to do the work on. For example, when I create a 100,000,000 digit number string in Common Lisp, I can use (format nil "~d" number) or I can use make-string and use a loop to fill in the digits. Using format takes about 10,000 times as long as using make-string and a loop. And if you're not careful, you might accidentally include the time spent building that data, in the time to process it, in one language and not the other. 
So, correct me if I'm wrong, `s` is never actually a type we deal with, but a thing that's injected by `reflection` and simply by carrying `s` around in the type we can, at any time, read from `s` as if it were similar to a `Reader` monad? So `reflect` being like `get` but pure (and probably faster) and `reify` being really similar to `runReader`, but of course the difference here is instead of wrapping any of the functions in this non-monad, we're just holding onto it off to the side, so we don't have the overhead of the monad? `(Proxy :: Proxy s)` just looks like a doodad you can instantiate to represent a type without actually having a thing on-hand. This means you can refer to `s` without knowing its constructor (because it seems to be a phantom non-thing). All `Proxy` are the same, but not, because you're setting the type with `::`. Is this correct? If so, that's really cool, but I needed an explanation (with metaphor) compatible with my limited cognitive ability.
I wrote a blog post on this just recently: http://newartisans.com/2017/02/a-case-of-reflection/
Huh? It's not about anything extra being expressible. It's just information for the reader (and writer) of the program.
&gt; the toy programs I write tend to use orders of magnitude more memory than the same programs in other languages When you find such programs please post them so that (a) we can help you fix them (b) we know what space leaks tend to occur in the wild. &gt; I'm getting the impression I should learn a lot of tricks to work around such memory problems Probably just one trick: `seq` in the right place. Like I said, please post such programs here and we'll help you find the right place. Hopefully, with time, it'll be second nature.
Makes total sense. I still like my made-up version of events, though. ;)
Even without laziness you get the Set problem, where both insert and lookup expect the same Ord instance to be passed, where you need to be able to merge sets with union and intersection, and if they might have been constructed with different orders you can't do that with the same asymptotics. You can address this some of the time with ML style modules or adding some sort of type tag, but there is still a generative vs. applicative module problem there. Where one causes reuse and sharing problems and the other decidability problems.
I'll take the contrary position and say that I *do* actually think this is an inherent property of the language, insofar as "the language" includes the general style of doing things in the language. Haskell prides itself on abstractions that are reusable ad infinitum. For abstractions to be reusable, they need to be, for lack of a better word, "inspectable". In many dynamic languages, this is the default. Look at abstractions in e.g. Clojure or Python and you'll see people messing about with internal members whenever they need to use an abstraction in a new situation. I wish I had time to provide a concrete example but I hope someone else will. In those languages, it's essentially unavoidable that "if abstraction X doesn't provide the interface you need, it does somehow provide raw access to its internals and you can create your own interface to it." In Haskell, things are different. The default is that you *don't* and more importantly *can't* do any inspection to reuse abstractions. The type system prevents you. "What internal member? This is a completely opaque black box type. There are no internal members as far as you should be concerned." If an abstraction doesn't provide the interface you want, you're shit out of luck. Of course, you can always insert a conversion function `AbstractionX -&gt; AbstractionY` that attempts to create a different abstraction Y and proxies its operations through whatever interface is available in abstraction X, but this is inefficient both in terms of computer performance and programmer performance (there will be lots of conversion functions sprinkled around, which take time to read and write). So what do you do in Haskell if you want to create a very reusable abstraction? You come up with a representation that is as general as possible. This is what van Laarhoven lenses are. They are not "an implementation detail". They are an incredibly general representation of the lens interface. It's amazing, really, how well the entirety of the lens API is captured by what's essentially "a function that takes a contextful function and a value and will give you back a contextful value." You'll see this all over the place with Haskell. Find an incredibly general and compact (often "mathematical-looking") representation of your interface and its reusability will shoot through the roof. That's often a good thing, and often something Haskell programmers strive for. A lot of packages start out as "opaque black box" types with a set of functions that operate on it, but eventually they move over to the "mathematical looking" general representation that doesn't have to be opaque and their adoption increases overnight. Of course, yes, this means the "implementation details" leak. Except it's not really an implementation detail, it's just a single value that encodes the entire interface. You could easily create a package that wraps this representation in an opaque black box and define your own functions that operates on this black box. Will this be easier to use, and will it give more sensible error messages? Yes. Will it compose as well and be usable in as many different situations as the original representation? Not even close. Does it have a place in the Haskell ecosystem? Maybe. I think you should definitely try.
Exactly correct. `reflection` turns some value `x` you have lying around of any type `a` into a fresh type `s`, that can be reflected back down to the value level by using `reflect (Proxy :: Proxy s)` to get back your `x`.
Perhaps one real world use could be to layer your app in "plugin" layers, each with different states (but thanks to `reflection` it doesn't change the monad stack), and then squash it all together at compile time. Has anyone played with this?
&gt; that actually increases expressiveness Hmm, well I wouldn't refer to that as "expressiveness" .. &gt; if one is taking in an argument monomorphically one is fairly likely to be strict in it as one will have to pattern match to pull out information Nice observation! &gt; strictness information is path-dependent But so is purity information! I'm not suggesting that a full strictness specification be presented in the type. That would be madness. All I'm suggesting is that the type can indicate when an argument is *always* evaluated strictly. That's not hard or complicated. In a lazy language you're looking at the difference between `a -&gt; b` (`a` might not be evaluated before `b`) and `a !-&gt; b` (say) (`a` is always evaluated before `b`). &gt; API operational changes, which would previously be unobservable, would break people's strictness annotations Yes, and that's a *good* thing!
&gt; I disagree. I don't consider it any more elegant, since it relies on unsafe coercion. Is it not a type-safe API? Can you implement unsafe coercion between two types using `reify`/`reflect`?
&gt; Hmm, well I wouldn't refer to that as "expressiveness" .. It is expressiveness because one can make a `Functor` whose `fmap` executes in parallel and one will know that that is safe to do. &gt; In a lazy language you're looking at the difference between a -&gt; b (a might not be evaluated before b) and a !-&gt; b (say) (a is always evaluated before b). So what you want is a category of strict functions? This makes sense. I was actually just planning on implementing this in my experiments with a more categorical framework in haskell, so that I would have a category where strict functors would be law abiding. 
One important mindset, I think, is to start with the most important operators. No need to cover the full space at once. If 90% of people are content with the 30% most popular operators, it makes little sense to delay the release of a good cheat sheet just because it's hard to find a good way to fit in the last 10% of operators. Second, if an interactive cheat sheet is desirable, it may make sense to actually create a parser for a subset of some pretend imperative language. So when the user types something like for (player of world.players) { player.position = pickMove(player.position); } the cheat sheet can translate that into -- Monadic version, assuming some sort of state monad carrying a type with a world field: world . players . position %= pickMove -- Pure version, assuming a yourState variable with a world field: yourState &amp; world . players . position %~ pickMove or something along those lines. Not quite sure how to make things such as monad instances and whatnot clear, but I think if it was possible, a lot of people would appreciate something like that. (Edit: We're basically talking an optimising compiler, except it optimises for lens idiomacity rather than computer performance.) The kernel of an idea I like is to find a way to structurally represent an arbitrary lens-based expression that allows the system to map that to a representation an imperative programmer may feel comfortable with. It doesn't have to be a straight-up language-to-language conversion, it could also be a form that lets the user select what sort of components they want involved in the expression and then generate some variants based on that.
&gt; what you want is a category of strict functions? I want a datatype of strict functions and the facility to work over strict vs lazy functions polymorphically. Whether they're a category or not is not particularly relevant to me. &gt; &gt; Hmm, well I wouldn't refer to that as "expressiveness" .. &gt; It is expressiveness because one can make a Functor whose fmap executes in parallel and one will know that that is safe to do. Oh I see. The purity of the function expresses extra stuff you can do with it. Well, with a strict function I can pass it to a left fold and know it's safe to do so in the sense that I won't get a space leak in the accumulator.
&gt; This has nothing do with the strictness of the function passed and more so do with the strictness of the left fold function. Yeah, you're right. Maybe I can't make a suitable analogy between IO and strictness. Anyway, it's useful to see where things are strict. &gt; under O2, GHC is able to optimize the left fold problem away during strictness analysis. It's horrible to have to rely on the compiler to pick up an O(n) factor improvement in space usage. If optimizations don't kick in your code is broken and you have no idea why.
This line of thinking is really weird to me. Professional haskell devs are regular people, and they aren't any smarter than normal programmers. In fact, Haskell is unique in that lets you be *dumber* than other programmers. Haskell is useful because it lets you think less when programming, and eliminates huge classes of bugs right off the bat. You don't have to worry about bugs nearly as much, and you can think 25% of the amount you'd have to think when programming in a normal language. The idea that you have to be some sort of supergenius to be a professional haskell dev is the complete opposite of the truth. Professional haskell devs can be *dumber* than normal devs, and do better work. Haskell is *made* for people who want to think *less*. And no, the "theory" doesn't really come into play in day to day programming.
It seems like you're looking for something along the lines of LiquidHaskell. It basically allows you to write down compile time constraints like the one you're describing above. You can find it at https://ucsd-progsys.github.io/liquidhaskell-blog/ along with some examples.
Ah, so it is not possible by default?
Not that I'm aware of, but I'm not an expert on the matter. You should probably also have a look at http://dev.stephendiehl.com/hask/ - if there's something that does the trick then it'll probably be there.
You may want to post a screenshot of one of these diagrams because it's not at all clear what you mean.
Do you mean that this elegant interface vs. dirty implementation detail debate is largely subjective?
You could set up a [snowdrift.coop](https://snowdrift.coop/) project, I guess?
Was going to recommend `instance Num a =&gt; Num (Pattern a)`, but seeing that it's already [in the other forum](http://lurk.org/r/post/2JxfVIxSEZOiaESLbZ6S84), I'll just leave a link here. Thinking with the Haskell type system rather than against it is generally a good idea :P
I would argue that `overlay` makes for the more natural monoid instance. It reminds me of the `Monoid` instance for [diagrams](http://projects.haskell.org/diagrams/) where the monoid instance places one diagram on top of another and placing diagrams adjacent to one another is relegated to a separate operator.
I was going to suggest the same. Liquid Haskell is good at this kind of stuff.
Thanks, this is a really helpful explanation.
Thanks for that, that seems to solve the problem fairly nicely, although I wasn't expecting to require a library. I'm definitely interested in the nicer and more polymorphic approach, even if it may be a bit advanced!
You may like to read about [dependent types](https://en.m.wikipedia.org/wiki/Dependent_type). 
Ok here is the answer: instance Num a =&gt; Num (Pattern a) where negate = fmap negate (+) = liftA2 (+) (*) = liftA2 (*) fromInteger = pure . fromInteger abs = fmap abs signum = fmap signum instance (Fractional a) =&gt; Fractional (Pattern a) where fromRational = pure . fromRational (/) = liftA2 (/) Extremely happy about this!
Hi, I've been using your `alpine-ghc` Docker image to build static binaries for a few standalone scripts using `stack` and a shell script like so (this is just an excerpt): for f in ./src/*.hs do stack exec -- ghc -O2 -optl-static $f done I switched to the official `alpine` image and added the `apk` command in your comment, along with `curl -sSL https://get.haskellstack.org/ | sh` for `stack`. I got this error: Linking src/&lt;scriptname&gt; ... /usr/lib/gcc/x86_64-alpine-linux-musl/6.2.1/../../../../x86_64-alpine-linux-musl/bin/ld: cannot find -lffi collect2: error: ld returned 1 exit status `gcc' failed in phase `Linker'. (Exit code: 1) Adding `apk add libffi-dev` fixed this. The static binaries seem to be building and working just fine. Thank you for all your hard work on this :)
&gt; Even though from the "code analysis" sense you describe the code is cleaner (less dependencies and nested logic) it is much more complicated in the sense of understanding evaluation. I think you are spot on with this. This probably describes pretty much all high level language concepts, or any kind of abstraction in general. &gt; You say easy to force values, but when the code forcing the value is itself a thunk it's turtles all the way down. It might very well be the other way around. An underrated selling point of lazy evaluation is that you can get large values from functions without kicking of the avalanche of computation that might happen with strict fields. Instead you evaluate as you go, depending on what you choose to be lazy or strict.
&gt; I wouldn't be intimidated by Ed's talk on lenses if I were you. And I also wouldn't be surprised that Simon PJ gave an excellent introductory talk about lenses that made it click for the OP because that's what good educators *do*.
It is intimidating because hardware and cores and caches are involved.
Same for me.
I agree that `overlay` feels more natural as a monoid instance, and your `diagrams` analogy is good. But from the theoretical standpoint I'm not yet convinced. Note that `overlay` and `connect` have an interesting weak duality: if their arguments are non-overlapping then one can be expressed via the other using a sort of De Morgan's law: `x + y = !(!x * !y)` and `x * y = !(!x + !y)`, where `!x` stands for the *edge complement* of the graph `x`. This weak form of duality makes these two operations similar to Boolean disjunction and conjunction, which are dual and do not have a natural monoid instance.
Had a quick look and it looks like libffi didn't make it into the depends line in the upstreamed version. I'll have to find out why, until then what you did is correct. Old apk depends: https://github.com/mitchty/alpine-linux-ghc-bootstrap/blob/master/testing/ghc/APKBUILD#L26 New: https://github.com/alpinelinux/aports/blob/master/testing/ghc/APKBUILD#L38 
Would that mean a solution in n^2 time? I can only do it in n^2 log n (sort the heights, then add them one by one, keeping track of connected components).
duplicated: https://redd.it/5wyom2
Oh I should clarify. I'm also totally ok with that kind of usage. I just wouldn't call it "more elegant"
Oh? How so?
[removed]
My rule of thumb is that if something is composable (in the mathematical sense) then it's elegant 
[Monadic parsing in Haskell](http://www.cs.nott.ac.uk/~pszgmh/pearl.pdf)
I added a video to the post..
The safe implementation is described in the original paper. It's less elegant than `unsafeCoerce`, though, which is elegant in its own way - it takes advantage of the similarity between `=&gt;` and `-&gt;`.
A lot of answers here about the Bay Area. I'm curious if anyone has any similar experience to share about finding Haskell developers in the UK? I am learning Haskell, intending to use it professionally, but will find it hard to convince clients to let me use it if there are no Haskell developers around.
[removed]
I just realized from the description on the README that Jenga doesn't take into account the `extra-deps` section of `stack.yml` files. Is that right?
I've taught plenty of mere mortals to use Haskell, gizmos and all. No math involved really, just take it one step at a time.
It looks like you're trying to mention another user, which only works if it's done in the comments like this (otherwise they don't receive a notification): - /u/burntsushi --- ^I'm ^a ^bot. ^Bleep. ^Bloop. ^| ^Visit ^/r/mentionhelper ^for ^discussion/feedback ^| ^Want ^to ^be ^left ^alone? ^Reply ^to ^this ^message ^with ^"stop"
thank you for doing this. I've always wondered about the performance of reflex-dom but never got around to writing the benchmark myself. did you post this on the reflex-dom repo to see if you're doing anything terribly inefficient in your reflex code? are you measuring the time taken to parse and load the JS file as well?
It's usually a good idea to try some equational reasoning when you're not sure where your memory is going. Good old "replace a function with its definition" makes many things clear. Let's start with the naive definition of foldr. The version in `base` has clever optimization tricks, but we won't worry about that: foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b foldr f b [] = b foldr f b (x:xs) = x `f` foldr f b xs It's pretty clear that `minMax` is doing all the work. So let's step through `minMax` applied to the list `[a,b,c]`. It won't matter what a b and c are, so long as they have Enum and Ord instances. From `minMax`'s definition, `minMax [a,b,c] is foldr minMax1 (a,a) [b,c] We always work outside-in, evaluating the outermost function first, arguments only as needed. Here, the outermost function is `foldr`. Substituting in its (naive!) definition: b `minMax1` (foldr minMax1 (a,a) [c]) The outermost function is `minMax1`. Looking at its definition, we see that we need to pattern match on the second argument, in order to unpack a tuple. (This is something that ought to feature much more prominently in haskell tutorials: pattern matching forces evaluation.) So we evaluate `minMax1`'s second argument --- `foldr minMax1 (a,a) [c] --- and see if we get a tuple: b `minMax1` (c `minMax1` (foldr minMax (a,a) []) We're not there yet. So we force the argument one more step. `foldr minMax1 (a,a) []` is just `(a,a)`, so we get (b `minMax1` (c `minMax1` (a,a)) We *still* don't have a tuple as the second argument to the outermost `minMax1`. We're forced to evaluate the inner function call: (b `minMax1` (min a c, max a c)) Now we can finally apply the outermost function: (min (min a c) b, max (max a c) b) And now you should be able to see the problem. Notice that nothing has forced us to evaluate `min a c` yet; it's just a thunk. `minMax` builds a tuple of thunks; the longer the input list, the deeper the nesting of calls to `min` and `max`. We don't get the result of any comparison until we're done traversing the entire list. The fix is straightforward: minMax1 r (p, q) = p `seq` q `seq` (min p r, max q r) Which evaluates p and q before calling min and max. Making this change reduces the memory usage on a list of 10,000,000 random integers from ~3300MB to ~1300MB. The second version is still better --- presumably due to standard library cleverness in [the definitions of minimum and maximum](http://git.haskell.org/ghc.git/blob/HEAD:/libraries/base/GHC/List.hs) --- but the difference is fairly small. Now, with a little practice you won't have to be as detailed as I've been here. You'll be able to elide steps, and you'll develop an intuition for where excessive laziness is coming from. But I find it very helpful to have a simple, deliberate process to fall back on. 
Google "Functional Pearl Haskell", most of those are absolutely amazing
&gt; The idea that you have to be some sort of supergenius to be a professional haskell dev is the complete opposite of the truth. Professional haskell devs can be dumber than normal devs, and do better work. Haskell is made for people who want to think less. Good point. 
That is correct. Is the `extra-deps` section documented somewhere?
[removed]
There's also this, which is what's usually recommended to beginners: http://haskellbook.com/. 
Info about it seems to be spread throughout the docs, but this section should cover it fine: https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md#curated-package-sets. There's not much to it. Here's an example of them actually in use in one of my projects: https://github.com/seagreen/station/blob/master/stack.yaml#L4 In my experience so far almost every sizable project I do includes extra-deps, so they would definitely be something to add if you want to improve the robustness of Jenga.
Neither the original list nor its permutation can be sorted because they contain lambdas. EDIT: The reason I gave for this is wrong. The lists I have don't have lambdas; they have types. Since the universe of types is open, there is no definable ordering on types. However, it doesn't matter anyway, since a "sort then compare" approach decides whether to lists are permutations of one another but doesn't give you the proof term needed to turn one into the other.
Haven't you tried PureScript or Elm?
I had a similar problem: I'm parsing year values from a data file, and I wanted to limit the range for a valid year to be between 1800 and 2020 or so. I did this with a newtype builder function which does a runtime check and causes an `error` if out of bounds. I chose `error` instead of `Maybe` because it was most appropriate for this particular application: it's best if the program crashes and doesn't continue. https://github.com/dogweather/nv-statutes-haskell/blob/master/src/Year.hs
I couldn't resist (which is not to say I recommend this approach): {-# LANGUAGE ConstraintKinds, DataKinds, FlexibleInstances, GADTs , KindSignatures, MultiParamTypeClasses, PolyKinds , ScopedTypeVariables, TypeApplications, TypeFamilies , TypeOperators, UndecidableInstances #-} {-# OPTIONS_GHC -freduction-depth=0 #-} import GHC.Exts import GHC.TypeLits import Data.Proxy import Data.Type.Equality data Date d m y where MkDate :: ValidDate d m y =&gt; Date d m y data Month = Jan | Feb | Mar | Apr | May | Jun | Jul | Aug | Sep | Oct | Nov | Dec deriving Show type family ValidDate d m y where ValidDate 0 m y = TypeError (Text "Day cannot be 0!") ValidDate d m y = ValidDateHelp d m y (d &lt;=? DaysIn m y) type family ValidDateHelp d m y ok :: Constraint where ValidDateHelp d m y True = () ValidDateHelp d m y False = TypeError (ShowType m :&lt;&gt;: Text " "] :&lt;&gt;: ShowType y :&lt;&gt;: Text " does not have " :&lt;&gt;: ShowType d :&lt;&gt;: Text " days!") type family DaysIn m y where DaysIn Jan y = 31 DaysIn Feb y = DaysInFeb (IsLeapYear y) DaysIn Mar y = 31 DaysIn Apr y = 30 DaysIn May y = 31 DaysIn Jun y = 30 DaysIn Jul y = 31 DaysIn Aug y = 31 DaysIn Sep y = 30 DaysIn Oct y = 31 DaysIn Nov y = 30 DaysIn Dec y = 31 type family DaysInFeb leap_year where DaysInFeb True = 29 DaysInFeb False = 28 type IsLeapYear y = (y `DivisibleBy` 400) || ((y `DivisibleBy` 4) &amp;&amp; Not (y `DivisibleBy` 100)) type family x || y where True || y = True False || y = y type family x &amp;&amp; y where True &amp;&amp; y = y False &amp;&amp; y = False type family Not x where Not True = False Not False = True type DivisibleBy x y = Help x y 0 (CmpNat x 0) type family Help x y z b where Help x y z EQ = True Help x y z LT = False Help x y z GT = Help x y (z+y) (CmpNat x z) instance (KnownNat d, KnownMonth m, KnownNat y) =&gt; Show (Date d m y) where show _ = "MkDate @" ++ show (natVal (Proxy :: Proxy d)) ++ " @" ++ show (monthVal (Proxy :: Proxy m)) ++ " @" ++ show (natVal (Proxy :: Proxy y)) class KnownMonth m where monthVal :: Proxy m -&gt; Month instance KnownMonth Jan where monthVal _ = Jan instance KnownMonth Feb where monthVal _ = Feb instance KnownMonth Mar where monthVal _ = Mar instance KnownMonth Apr where monthVal _ = Apr instance KnownMonth May where monthVal _ = May instance KnownMonth Jun where monthVal _ = Jun instance KnownMonth Jul where monthVal _ = Jul instance KnownMonth Aug where monthVal _ = Aug instance KnownMonth Sep where monthVal _ = Sep instance KnownMonth Oct where monthVal _ = Oct instance KnownMonth Nov where monthVal _ = Nov instance KnownMonth Dec where monthVal _ = Dec val1 = MkDate @1 @Jan @2000 val2 = MkDate @29 @Feb @104 
If you don't believe the docs how can you analyze anything at all related to the computation required by various library data types and functions? For all you know `Data.List.sort` is bubble sort, or a deterministic bogosort.
Something like that. The 2d solution is much harder than the 1d solution, because you need to work harder to find all the pools.
I didn't mean to imply that I don't trust documentation (although I do not always, and diving into the source can often be very informative). Rather what I meant is that often that information is not even available in the docs. I have encountered code which uses strictness annotations but makes no mention of it in the docs. There is no way to know if the docs are telling you the whole story. If I was calling sort and it didn't tell me what sort method was used in the docs, I'd probably have to take a look at the source code no?
Without laziness you need to use mutation in an evil way, or [take two passes and miss the point](http://chris.eidhof.nl/post/repmin-in-swift/) entirely. This is sort of a classic example of laziness taking a two pass algorithm and turning it into a one pass algorithm.
So then your main issue is that it is hard to analyze performance without good documentation? That is the same in every language.
I've yet to see anybody actually follow up on the 'you could do this in a strict language' argument by actually, you know, doing it. People often trot out the 'you could do this yourself, explicitly' argument in languages like Scala where they have 'lazy val' lying around and the like. The problem is, nobody ever _does_ it in those languages enough that any library of lazy algorithms gets built up and then exposed to users. I literally can't find a single library that 'gets it right'. Moreover, things like short-circuiting evaluation tends to be in exceedingly short supply. e.g. `(&amp;&amp;)` forms a monoid, but in such a strict world, using it _as_ a monoid won't short circuit, because your `mappend` equivalent is strict in both arguments, unless you make an explicit 'short-circuiting monoid' that takes a lambda or `Lazy a` for the right hand argument. Nobody ever bothers to define such a construction, though! When you add to that the fact that these constructions typically have quirks, such as e.g. lazy val being a member of the object it lives in rather than part of the 'thunk' meaning that copying a lazy val to a fresh lazy val is actually an ever more and more expensive operation that holds onto more and more garbage, or the fact that the garbage collector in such a language isn't set up to forward thunks to their final answers, so you always forever pay 2x as much overhead in terms of memory access for thunk dereferencing, and you will necessarily face [Wadler's class of memory leaks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.5423&amp;rep=rep1&amp;type=pdf) unless you construct something that walks your heap for you, there are a lot of holes in this plan in practice that keep it from working out. Until someone actually constructs a decent library of such things, in a language without those problems, I'll continue to treat this as a straw-man argument, however well-intentioned.
You provably and necessarily lose a log factor on many algorithms in such a language.
We have Haskell as a strong existence proof of the latter. I can't find a single example of fully thought out 'laziness in the large' in the wild in the former. [Cross-linking to a reply elsewhere in this thread.](https://www.reddit.com/r/haskell/comments/5xge0v/today_i_used_laziness_for/deleyqr/)
Yes, and `(&amp;&amp;)` short circuits, because of compiler magic, but if you switch to using it as a `Monoid`, it can't, so using that to implement `all` and the like is dangerous, damaging code reuse.
hnnnnngh
&gt; The problem with the TF solution is that there is literally no code you can write with, say, monad transformers that is less verbose unless it doesn't mention the state/etc type at all. Check the signatures of the code in monads-tf against those in the mtl. Literally every single type signature gets longer. I will take your word on that, would you argue that the slightly longer type signatures are any less readable though? I personally find type families a lot more intuitive. put :: MonadState s m =&gt; s -&gt; m () Took me a while to get used to. Particularly since the injectivity is not explicit in the type. Give a state type and you will be given a monad type such that when combined the state type and the monad type are a valid monadstate pair, and each monad only has one compatible state. It is only really intuitive now because I just think of the s being a function of m haha. put :: MonadState m =&gt; StateType m -&gt; m () Reads quite clearly to me, you simply put something with the state type desired by the monad in question, and it gives you that monad. I do agree that more work should be done on improving the conciseness of type families, that is really my only qualm with them, besides that they seem like a better version of FD. And IIRC Injective TF's still aren't as general as they could be, and more work on that could maybe help with the multiple type parameters issue. &gt; Then there is the fact that with the FooC thing, you always wind up with one type parameter as the odd man out. Hmm... yeah that is a bit of a bummer, I like to think there is usually a type that deserves to be the odd one out. For example: (+) :: Addable a b c =&gt; a -&gt; b -&gt; c -- a b -&gt; c Is IMO much better represented as: (+) :: Addable a b =&gt; a -&gt; b -&gt; Add a b Even though it needs a few more characters. But I am sure that you can come up with real world examples where that is not the case, so I won't try and demand proof, although if you have any on hand I would appreciate it. Just to give me a bit more to think about.
Consider Stack's example basic Travis script to help keep the library compiling over time. I have been looking to publish more code lately, and have been looking at options with respect to stack, cabal version bounds, and testing against multiple build environments using multi-ghc travis or Stack's advanced example. I am hoping to reach Nix/Hydra eventually for builds and testing.
While you are correct in this specific case and I respect your point, it's an odd stance to be taking in "favor" of Haskell's evaluation model. Usually performance is not the reason people pick lazily evaluated immutable languages.
For those not familiar with his work, John was one of the authors of the [Haskell Report](https://www.haskell.org/onlinereport/) and one of the authors of the [Gentle Introduction to Haskell](https://www.haskell.org/tutorial/), which many of us remember as anything but gentle, among his many other accomplishments.
I freely admit I get way too much mileage out of that stupid example.
&gt; You surely wouldn't want the user to specify the whole operation each time! From the perspective of the user, the table schema is just an implementation detail, he just wants to get some data or perfrom some action. Of course! Which is why you write it up as a view and present it as such.
Agreed! The costs are quite high. I think the costs of laziness are significantly lower
Thanks, I'll check it out. Haskell libraries definitely seem to suffer from quite a bit of bitrot, as the language and tooling evolve. Travis indications are helpful there.
I suppose that under the belief system that surface complexity / redundancy is ok as long as we compile to a common core then having FD be syntax sugar for TF isn't too bad. I guess my view is that unifying surface syntax is really nice to have and worth prioritizing. I also dislike how if a library chooses to use FDs then you are stuck with them. As despite the extra few characters I do prefer a TF based transformer stack. Although I can see how with lenses FD might be more appropriate. What exactly is stopping FDs from being syntax sugar for hidden type families at this point? Would it be possible to make it so that you could write something like: class MonadStateTF m where State m = s ... instance (MonadState s m) =&gt; MonadStateTF m where State m = s ... To make that less of an issue? Is that part of the unification process you were talking about?
I ran into that very same thing in Ruby when I was trying to implement a DSL. I had no idea why it wasn't working right, and when I figured it out, I had to unroll a *lot* of code. Wasn't a fun time.
The fact that you get 'stuck with FDs' is an artifact of the fact that they are weaker in the current implementation. The comments I was making above about how to fix that would allow you to make an adapter that let you have your MonadStateTF as a derived thing off the fundep implementation. What is needed is that each fundep needs to become a hidden type family, class associated type families need to be allowed to reference class arguments that aren't on the left hand side of their = so long as those are functionally dependent on the stuff on the left of the =. Then you can build some machinery like: class MonadState s m =&gt; MonadStateTF m where type TheState m :: * instance MonadState s m =&gt; MonadStateTF m where type TheState m = s and look, you're out of "FD hell". This isn't legal today, but if internally GHC turned class Monad m =&gt; MonadState s m | m -&gt; s where ... into class (s ~ MonadState_1 m, Monad m) =&gt; MonadState s m where type MonadState_1 m :: * type MonadState_1 m = s -- not legal today, but make it so this does this definition for each instance automatically. ... then instance MonadState s m =&gt; MonadStateTF m where type TheState m = s becomes magically the perfectly legal instance MonadState s m =&gt; MonadStateTF m where type TheState m = MonadState_1 m Now library authors write instances for the classic `mtl` and the `monads-tf` variant that piggybacks this way and does this automatically works for all instances. No redundant code need be written, unlike today, and FDs cease to be a ghetto that traps you by the circumstances of your code's birth.
I work at Target on a new team using Haskell. We've found that hiring a small number of Haskellers is actually actively *easier* than the alternatives, because there are a lot of great people who are excited to use Haskell—more than the number of exciting Haskell jobs. This is in the Bay Area though. I don't think we've tried to hire any Haskellers in Minneapolis (Target's headquarters), but we have had difficulty hiring for other roles there.
proving is not the same thing as deciding. For any function `f : A -&gt; B`, `f` is equal to itself. I can incorporate this as a proof rule. That is very different from having a decision procedure that *decides* equality.
But how will you know that the same `f` exists in both lists without violating referential transparency? `a = \x -&gt; x` `b = \x -&gt; x` are not distinguishable from `a = \x -&gt; x` `b = a`.
It's possible to sort a list in such a way as to extract the permutation (in a non-DT setting, this is akin to `sort (zip xs [1..])`), and permutations compose. I'm not sure this is enough on its own, but it seems to get close. 
Ok I think I see what is going on. I thought the OP was talking about taking two different lists and checking if they were permutations of one another.
What was wrong with it? Just too complex and not deemed useless or could you do something bad with it?
 Very very sorry to hear this.《Gentle Introduction to Haskell98》 is such a classic that i always recommend it to people now and then, it's my haskell icebreaker. Don't hesitate if you think it's outdated in 2017, if you haven't learn haskell before then go and read this book!
More that it wasn't what most people expected, especially once your intuitions started to come from TFs.
I'll be sad that the prisms in `Control.Exception.Lens` will now be non-law-abiding, but still happy to see this happen. ;)
Ah ok, fair enough! Thanks so much for all the FD vs TF info and explanations. I think you have more or less convinced me that as long as we stick with Haskell's mantra of allowing surface complexity / redundancy, that FD and TF's should coexist and compile to a common core.
Lens documentation was late night reading for me! Very concise, but worth digesting the content as it was presented. I know I called for a better coverage of /u/tekmo documentation, however learning the hard way worth it.
Sounds like a Zappa quote
It looks like you're trying to mention another user, which only works if it's done in the comments like this (otherwise they don't receive a notification): - u/wook3r --- ^I'm ^a ^bot. ^Bleep. ^Bloop. ^| ^Visit ^/r/mentionhelper ^for ^discussion/feedback ^| ^Want ^to ^be ^left ^alone? ^Reply ^to ^this ^message ^with ^"stop"
I posted a new thread, "Another Haskell Mystery", which arose from experimenting with some of the suggestions people made here about memory usage.
Your suggestions and others make a big difference. But see my other thread, "Another Haskell Mystery".
Yes, and it makes a big difference. But see my other thread, "Another Haskell Mystery".
Eh, I'd reckon that you couldn't do much with lambda functions. As for type classes, I do think that the programmers I know don't utilize them effectively. Know any good resources for type classes? 
That's due to lazyness. `bigstring` is not a big string in memory, it is a lazy linked list of char. See for yourself : Prelude&gt; bigstring = show (2^(2^27)) (0.00 secs, 0 bytes) Prelude&gt; :sprint bigstring bigstring = _ Prelude&gt; take 3 bigstring "119" (2.41 secs, 114,435,528 bytes) Prelude&gt; :sprint bigstring bigstring = '1' : '1' : '9' : '6' : '3' : '8' : '0' : _ Prelude&gt; length bigstring The first `bigstring = ...` does nothing and takes no memory. `:sprint` shows the unevaluated thunk as `_`. Then I'm taking the first 2 elements of the bigstring. It takes 2.41s, just to build the big `Integer` you requested, this is not lazy. However, once that integer is built, building a `String` from it can be done lazyly and really quickly. If the compiler can see that your string is only consumed once, it will not store it in memory and everything will be processed lazyly. However, if you use the string more than once, is it is the case in the two lines of `main`, then the runtime must build the list at the first usage and save it for latter use. You may be tempted to say that haskell sucks here, but actually it performs better than many other languages because it allows this processing to be done in constant memory if your list is proceeded only once, when another non lazy language will eat the memory whatever the number of usage of the string you'll do (even 0). For a similar reason, if you inverse the two lines in `main`, you will get less memory usage. Because the line containing `head` only forces one character of your list, and the line using `midEnum` can later consume it lazyly. However, the haskell community usually don't recommend the usage of `String` (or `[Char]`) for big string, because they are a linked list, and hence exposes really poor memory performance and high memory overhead. Think about it, a linked list of `Char` is represented in haskell by something like `data ListChar = Cell Char ListChar | End`. This means that each Char will take: - One pointer for the constructor pointer (`Cell` or `End') - One pointer for the `Char` pointer - One I don't know for the `Char` value (I guess four bytes) - One pointer for the next `ListChar` Which means, at least 3 pointers and more or less four bytes. So a minimum of 28 bytes per char. So for a 40 million char string, it means at least 1.1 Gigabytes of memory. (Hey, I'm not that far from your 1.8 GB). You need to use something such as `Vector` or unboxed `Vector` which will pack your Char with a lot less pointers and in contiguous memory location. 
I guess you should better define what you mean by "know": is it "the bare essentials of the language" or "what it takes to be productive in a professional capacity"?
[removed]
There are all facts but the question is: how much of it can we ignore. Most successful libraries in other languages ignore all or nearly all those cases.
&gt; It works in all languages, is easier to understand, and gives you just as much safety. Well, it gives you as much safety as other languages have but it doesn't give you compile time safety (which is desired here).
This catches most cases but does catch last days of the month or February.
TBH your title is super confusing to me...
Yeah, the plan is to set up a github pages website. We don't have one yet though.
Write You A Haskell, http://dev.stephendiehl.com/fun/
This book is free, right?
I've also just started learning Haskell, and I'm working with a minimal installation: ghci, cabal-install, and vim. Works just fine. If you're not comfortable with vim, any other text editor is sufficient in the beginning. I wouldn't spend too much time on the environment before being able to write some serious code, tbh. 
Productive in a professional capacity. 
Agreed, you just need plain text editor with syntax highlighting and repl in the beginning. No need for any IDE or extra plugins.
I don't understand what you mean. Haskell *doesn't* make strictness explicit in types.
Cons: * Restrictions on installable packages. See [here](http://blog.haskellformac.com/blog/extra-packages-command-line-tools-in-version-11) for details. * Only one GHC version. * No hindent, hlint, stylish-haskell editor integration. * Autocompletion, shortcuts etc. probably won't get you typing as fast as you might be able to in emacs or vim. Pros: Only one, really. Really quick to run code in a project. In the normal write-compile-repl loop (which I use with spacemacs) if the code that I'm sticking into ghci is anything longer than a few lines, I'm stuck. Say I'm writing a parser for arithmetic expressions, and I want t play around with it. In ghci, I might write something like this: let x = Var "x" let y = Var "y" let exprString = "x + y * 4" parseWithVars [x,y] exprString But then, if I discover a problem in the parsing function, and I rewrite it, I have to recompile the code and write those four lines again. Or maybe I forgot to set `-XOverloadedStrings`. So I've got to do that in ghci as well. Or maybe I want to see the outputs of two separate expressions: let x = Var "x" let y = Var "y" let exprString = "x + y * 4" parseWithVars [x,y] exprString parseWithVars [y,x] exprString HFM makes this *so much* easier. I just write those five lines on the right-hand-side, and they are re-run, with output, every time I change something in the project. It might seem like a small thing, but for me it made the difference between frustration and progress when learning Haskell. The editor does a bunch of other cool things, like spritekit integration, but I haven't really used them. I still use spacemacs+intero for big projects, but any time I need to figure some code out, I'm always going to reach for the playgrounds. I would *probably* prefer to be able to stay in spacemacs the whole time, but until it gets something like playgrounds (maybe a clone of live-py-mode?) it won't be as useful for learning as HFM is.
The best thing about the bot is that a side-effect is that the user in question gets notified.
I concur.
Am I the only one that finds that code unreadable?
 data Stage i o = Stage (IO (Maybe o)) (i -&gt; IO o) instance Arrow Stage where ... (Just improvising) 
I tried intellij-haskell but I find that it lags. I switched over to VSCode + Haskero plugin + haskell-linter plugin and it works really well for me. I like that it highlights the error and shows the error reason. The setup is pretty easy too ... works on my first try :) EDIT: Since you are new and on Mac, here's an easy to install setup that works for me: 1. Install Stack via Homebrew 2. Install VSCode + Haskero plugin + haskell-linter plugin Done. :)
There is the ATS language which I think runs with no GC. I have not yet used it but would like to know if others have. http://ats-lang.sourceforge.net/DOCUMENT/INT2PROGINATS/HTML/book1.html My hunch is that if we only allow Inductive types a la Coq, since these types are structurally well formed, we can get away with no GC or at least trivial GC (ref counts). 
The key is how to construct a "recovery action" `IO (Maybe o)` for a composition. The idea is to try to recover the second stage, if it fails with Nothing try to recover the first stage and then apply the second stage transformation, else fail to recover. But now I realise the Arrow instance would not work because we can't define `first`. It is still a Profunctor/Category, though. 
I often look at the horrible mess of Perl code at my work and find new paths and preloaded data that are not used, many functions (re-)doing redundant things due to eagerness. I have a hypothesis that when a program is huge and obtuse, non-strictness can save a lot of resources for free that are almost impossible to save without very careful rethinking and refactoring of eager code. So, non-strictness seems to do the same to CPU usage that GC does to RAM: just write your code in straightforward way and let non-strictness/GC do the chores for you. And yes, you have to pay for that in demanding situations. Non-strictness in its current form in Haskell is not very pretty and may be not a pleasure to work with, but making everything strict is just plain luddism.
[Here's the code](https://github.com/zyla/type-level-web-server) and [here's authors blog](https://zyla.neutrino.re/posts/2016-09-06-tlh.html)
If you use recursion you have cyclic references.
It *does* statically analyze code, but it does so just to insert ref/deref statements. It's otherwise just reference counting, which as you say can't detect cycles.
Yeah, but that ‘code at compile time’ is bookkeeping code that helps it determine when to activate the deallocation code (which is also inserted at compile time). That’s why it’s called Automatic Reference Counting, because it inserts the code to do reference counting for you. I _think_ that doing it completely statically is not possible without something like substructural types.
&gt; I think that doing it completely statically is not possible without something like substructural types. What is "it" referring to here?
But functions don't get collected. It's only cyclic data references that cause problems.
&gt; ARC works by adding code at compile time to ensure that objects live as long as necessary, but no longer. &gt; to ensure It amuses me that they can say that and then proceed to discuss ref cycle memory leaks.
I think it's more about the jump in cognitive complexity in jumping from a series of programs writing to disk to Haxl. 
Reading your reply I immediately thought: Well just prevent sharing then by making bigstring a function of type `() -&gt; String`. However when I tried this with `-O2` GHC was too smart for me and I was unable to prevent the sharing easily. This was my last attempt: import Data.Foldable (foldl') import Debug.Trace data Pair a b = Pair !a !b midEnum :: (Enum t, Ord t) =&gt; [t] -&gt; t midEnum xs = toEnum (a + div (b-a) 2) where Pair e1 e2 = minMax xs (a, b) = (fromEnum e1, fromEnum e2) minMax (x:xs) = foldl' minMax1 (Pair x x) xs minMax1 (Pair p q) r = Pair (min p r) (max q r) -- Nothing changed up to here. ------------------------------- bigstring :: Int -&gt; String bigstring 0 = trace "forced" $ show (2^(2^10) :: Integer) bigstring 1 = trace "forced" $ show (2^(2^10) :: Integer) {-# NOINLINE bigstring #-} main :: IO () main = don'tShare bigstring bigstring don'tShare :: (Int -&gt; String) -&gt; (Int -&gt; String) -&gt; IO () don'tShare b1 b2 = do putStrLn.show.midEnum $ b1 0 putStrLn.show.head $ b2 1 {-# NOINLINE don'tShare #-} Even with this "forced" is only printed once, so `bigstring` is shared. Any solutions? edit: Ok, I found a way. But it would be nice if someone could comment on how to reliably prevent sharing with `-O2`. bigInt :: () -&gt; Integer bigInt _ = 2^(2^10) {-# NOINLINE bigInt #-} bigstring :: () -&gt; String bigstring x = trace "forced" . show $ bigInt x {-# NOINLINE bigstring #-} main :: IO () main = do putStrLn.show.midEnum $ bigstring () putStrLn.show.head $ bigstring () 
Is there some reason for the downvotes that I'm missing?
True. Just be careful about implementing half of what Haxl provides in an incremental way :)
Now i'll see, thanks a lot!
Unfortunately Haxl does not have a way to replace the `DataCache` with any sort of persistent storage interface (it's not abstract, and merely uses an `IORef`), so it fails on one of the important criteria. (Unless I'm missing something -- but I've been using Haxl for the past 3 days, and thought about making the cache persistent, and couldn't find a good way to do it, other than bulk loading/deloading every time I create the Haxl `Env`. Ideally the `DataCache` would have a more abstract interface to fetch cache entries...)
i'd almost be inclined to see it as a dependency tree &amp; just solve it with make or shake.
o7
Hakyll's [Compiler monad](https://jaspervdj.be/hakyll/reference/Hakyll-Core-Compiler.html) handles dependency tracking and caching of data. The cache is writen to disk. This probably won't be a perfect fit, but perhaps you can copy the code and tweak it until it is. For a pipeline, I know you can do stuff like "load all these files, run this transformation on them and collect the results, then save the results as snapshot A". I'll page /u/jaspervdj, he's the author. Perhaps he can explain a bit more.
Post your code, it will be easier. Also, you will need IO for that, and that's probably the source of your problem.
ARC and reference counting in general face problems which can be quite difficult to overcome. The most obvious is cyclical data-structures have to annotated or detected statically. The other problem is that with data shared among cores, reference count modifications have to atomic, otherwise you loose live data[1] or spuriously retain data. Following on from that leads to questions as to where the reference count modifications are. Implementing them naively can trigger large amounts of updates such as by walking a list which is retained somewhere else. Another fun quirk of reference counted systems, is that function with large amounts of local heap allocated data no longer return in constant time. Data-structures such as linked lists and trees have to be traversed and returned to the free-pool. You can optimize this by lazy freeing, but then you pay the price in several places. One of which is memory usage e.g. if you had a size discriminating allocator and it's base bucket size was 256 words for 2 word objects, a list of &gt; 128 2 word elements lazily freed hogs 512+ bytes until the allocator can free it. You then have to pay the price of time in freeing it either when an allocation is made not of that size, as you have to potentially pull more memory in to allocate an object (well, probably an entire pool). Or you pay the cost when allocating objects as you lazily de-allocate entire structures during allocation. The challenges in cycle detection and multiple modifications to reference counts in a short period of time (and from multiple processors) were addressed by IBM's Recycler reference counting system. There's a fair degree of literature out there on reference counting systems in functional language implementations, (LISP being the guilty victim). Memory management is one of those areas where the main questions are: Where do you want to spend time? Where do you want to spend space? [1] Been there, debugged that with a normal GC. It's an absolute pain.
Luckily sharing of large top-level declarations is rarely what people need when they desire to "control memory" in Haskell.
This is part of why I made [Fraxl](https://github.com/ElvishJerricco/fraxl). The caching layer is just a substitution, so you can implement your own or persist the default one. I need to update it for GHC 8 though.
 f i | i &lt; 10 = "hi" | otherwise = "bye" Semicolons are just an alternative to the layout driven parsing that is used mostly in conversation when using layout is clumsy. 
Not sure how that implies that "they don't have things figured out yet". GC _is_ the natural solution whenever a program has to allocate potentially cyclic data structures - the point is that for any *other* circumstance, you can do a lot better!
Doesn't Ur/Web work like this too?
More discussion about this: http://stackoverflow.com/questions/6208006/any-way-to-create-the-unmemo-monad https://www.reddit.com/r/haskell/comments/2g9akh/preventing_memoization_in_ai_search_problems/
Thanks! Thread killer if I've ever seen one.
&gt; Jargon is like shibboleth and often even becomes shibboleth, but it is not shibboleth exactly. I feel like the author wanted to use the word "shibboleth" and picked FP as their target.
I'm not the biggest fan of "functional programming" as a term[1], but it's got too much weight behind it. It's a term people already acknowledge and even understand—to whatever extent—and replacing it in the broader conscience not worth the time or effort. This is the same reason I'm willing to use "lazy" when I strictly mean "non-strict": it's not absolutely correct, but it's the word people know. If the specific difference between "lazy" and "non-strict" is important I'll spell it out explicitly, unless I'm talking in a context where people already expect and know about the distinction. [1]: I think it's not well-defined and the name is misleading. I wrote a bit about that [on Quora](https://www.quora.com/What-are-some-myths-about-functional-programming-and-functional-programming-languages/answer/Tikhon-Jelvis).
It's not meant to be an implication (or a criticism of rust). The reason I say they don't have it all figured out yet is that they still have proposals in the works for things like non-lexical lifetimes. I'm sure there are other things floating around that they are working on that I'm not even aware of. Another example might be that sometimes you want to borrow multiple fields at the same time in the same struct but you can't (directly) express that in stable rust right now.
Looked at Rust?
Yeah, you're right. You can have things of function type that are collected.
The link you provide shows that there are _no_ restrictions on installable packages, not the opposite. It just requires a little command-line work to install them :-)
Sorry to be ignorant but what is IO?
TL;DR: Functional programming has becomen a meaningless buzzword. Just as Object-Oriented Programming is.
I agree. I used to like ARC a lot until I found out about Rust and learned that it's just a bad version of lifetimes.
 data Day (dayOfMonth :: Nat) where Day :: CmpNat dayOfMonth 32 ~ 'LT =&gt; Day dayOfMonth However, this has the unfortunate effect that you can't write `day1 /= day2` in any useful way. 
I'm surprised no one has mentioned JHC up until now. https://web.archive.org/web/20160625205953/http://repetae.net/computer/jhc/jhc.shtml It's a Haskell compiler implementation that, as you can read in the above link, doesn't use a garbage collector. They use 'a variant of region inference' to handle automatic memory management. AFAIU this also allows them to forego the need for a runtime system, allowing them to compile `main = putStrLn "Hello, World!"` to 6,568 bytes vs 177,120 bytes for GHC 6.4. I really like the promise of increased efficiency of these techniques. Maybe someone can tell me why they are not as adopted as of yet, e.g. by GHC? Is it a lack of research/maturity? An undesirable increase in compiler complexity? Or something else?
Same here.
The reason I think this is possible is that for Inductive types in Coq you can only have structurally smaller subterms. This would mean for example there are no loops and that any constructor that you apply will be on terms of smaller "structural size". In such a case a ref count would do. Also consider implementing this is some thing like a graph machine. When variables come in and out of scope we can do this increments and decrements. I have not thought through all the details but that is why I said it is my hunch. Someone more knowledgeable can shed more light. May be /u/coremined 
I think you may miss some point of GC itself: GC is not only about free memory, but also about /optimizing/ memory layout. Using copy/compact GC, we can move linked data structure together to improve data locality, which is important to modern CPU with complex caches. We can also eliminate memory fragment: a problem which ARC/sweep GC or manual memory management introduced: the memory holes after you freed between other living objects, are not immediately usable by an memory allocator, simply because it may not large enough. While in haskell the allocator can do its job just fine by simply increment a pointer most of the time. Rust solve this problem by using a delicate memory allocator: jemalloc, which use many tricks to avoid memory fragment, but this definitely not comes free. I'd guess Swift or any other ARC language runtime will also have to use similar techniques. The pause GC introduced is another problem, I would recommend you to go and read [the GC handbook](http://gchandbook.org/) to get a better understanding of overall GC picture. ARC is also a kind of GC listed in the book, it has its own advantage and disadvantage ; )
Isn't that just `flip until`?
There's already [GHCJS](https://github.com/ghcjs/ghcjs).
oh nice, installing it. I am curious to see how readable is the output js.
IIRC basically unreadable. I feel like I remember someone saying purescript output is somewhat readable though if you want to go that route (it's also very possible I'm wrong and they just said purescript outputs less code)
I will definitely try both today and post the results. I wrote merge sort in Haskell and then translated it to js manually. I will post that too.
I would strongly recommend using a language you can use natively, and using idioms from that language. As much as I love Haskell, it's not always the right choice.
Note that ARC does not have great performance, and the performance gets worse with higher levels of concurrency. You have to fetch / add / store every time you copy a reference, and then fetch / subtract / store / branch every time you lose a reference. You also have to pay for the memory that the reference count uses, which in turn reduces memory locality, which decreases memory even more. Touching reference counts can also reduce the effectiveness of the processor cache. So ARC is only generally used with "larger" objects. I'm being intentionally vague here. But consider that in Swift you'll always use something like NSArray, NSDictionary, or NSString, but in Haskell you'll use something like [Char]. `"Hello, World!"` is a 13-character string so you'll have 27 reference counts--one for each character, one for each list node, and one for `[]` at the end. Maybe `[]` is shared, but still, that will ruin performance. It also turns out that GC is very fast. Ridiculously fast, even. Think of all the web servers running Java or C#. This isn't the 90s where the whole Java Applet would pause when it did a GC cycle. I'm not going to say something silly like GC is always better than ARC or vice versa, but most Haskell programs seem very suited for GC, even if they don't have cycles.
I wonder how this compares to blast and transient? https://github.com/jcmincke/Blast https://github.com/transient-haskell/transient-universe
Then GHCJS is a no-go. It's like reading assembly.
[Wagon](https://www.wagonhq.com) was a cracking SQL editor written in Haskell with an Electron frontend (I believe).
I haven't used it, but maybe https://github.com/jyp/dante this can help you out. It isn't live, but it does enable you to evaluate blocks of code right in the buffer. 
Seeing as the output is read by a human being, I would recommend TypeScript, &gt; version 2.0. You get some great typing capabilities, but it all gets erased quite cleanly when you transpile it back to vanilla JS. 
Here's an example for you: https://tolysz.github.io/reflex-todomvc/out.js It basically looks almost exactly like you'd expect compiled Haskell to look, only using Javascript instead of x86 assembly or LLVM or whatever. It's not very readable, but it's also not entirely impossible to make sense of bits of it if you really need to (especially if you start stepping through it with the help of a debugger). The running application: https://tolysz.github.io/reflex-todomvc/ The source code is here: https://github.com/reflex-frp/reflex-todomvc/blob/develop/src/Reflex/TodoMVC.hs While it's entirely impractical if your goal is human-readable Javascript, if all you care about is how the applications run, it's pretty great! We've been using it in production where I work (Obsidian Systems) and have built a few fairly sizeable apps with it over the last couple years. It's really nice to actually be able to use Haskell proper for both the frontend and backend of a web application, and to share code (and importantly data structures) between them. You can start out making some refactor on the backend, and eventually have type errors filter through to the frontend, and the compiler tells you everything that needs to change as you go. For example, a coworker and I sat down together and refactored an application that our team had been working on for a year in order to make it multi-tenant in about 3 days -- basically we changed the type of our authentication tokens and the type representing a connection to the database, and the compiler told us practically everything which needed adjustment across the entirety of the app. More recently we've also been able to compile our frontend to ARM code instead so that it can run natively on mobile devices. The majority of that work took place during a single week, but it was about 3 weeks for us to (mostly) implement a mobile design for the app, and get largely the same code running natively on iPhone and Android as well as in the web browser, with very few host-specific sections.
To make this a little more explicit, the function in the OP could be written as foo f r x = until r f x Here's [the docs for until](http://hackage.haskell.org/package/base-4.9.1.0/docs/Prelude.html#v:until) if you want to read more, but the type already mostly gives you what you need. Note: in this case, I'd probably just not bother defining `foo` at all unless for some reason it was much nicer to have the arguments in the other order. Instead, anywhere where I would write `foo f r x` I would just write `until r f x`, which is a little nicer IMO because it's a standard function instead of one you wrote yourself, so the cognitive load is lighter when reading the code.
I was reluctant to post this question here. But I am glad I did. Purescript seems to have everything I want from Haskell and outputs quite readable js. I don't mind explaining it to the interviewer if I can understand the output. Thanks 
not sure between direct translations and approximate translations. is this direct? Haskell code for merge sort module Main where main :: IO () main = do putStrLn "hello world" sort :: [Int] -&gt; [Int] -&gt; [Int] sort [] [] = [] sort x [] = x sort [] y = y sort x@(g:xs) y@(h:ys) = case g &lt; h of True -&gt; g: sort xs y False -&gt; h: sort x ys mergeSort :: [Int] -&gt; [Int] mergeSort [] = [] mergeSort [x] = [x] mergeSort xs = sort a b where a = mergeSort $ firstHalf xs b = mergeSort $ secondHalf xs firstHalf :: [Int] -&gt; [Int] firstHalf xs = take (length xs `div` 2) xs secondHalf :: [Int] -&gt; [Int] secondHalf xs = drop (length xs `div` 2) xs --------------------------------------------- js code translated from haskell code const merge = (x,y,sorted = []) =&gt; { if(x.length == 0 &amp;&amp; y.length == 0) return [] if (x.length == 0 ) return [...sorted, ...y] if (y.length == 0) return [...sorted, ...x] if(x[0] &lt; y[0]) return [...sorted,x[0], ...merge(x.slice(1),y)] else return [...sorted,y[0], ...merge(x, y.slice(1))] } const mergeSort = xs =&gt; { if(xs.length == 0) return if(xs.length == 1) return xs let a = mergeSort(fstHalf(xs)) let b = mergeSort(sndHalf(xs)) return merge(a, b) } const fstHalf = xs =&gt; { let l = xs.length return xs.slice(0,l/2) } const sndHalf = xs =&gt; { let l = xs.length return xs.slice(l/2) } let xs = [4,5,2,6,1,8,3,7,9] console.log(mergeSort(xs)) 
There is some project called [BuckleScript](https://github.com/bloomberg/bucklescript), which is a compiler from OCaml to JavaScript that outputs reasonable code, both in size and performance. Personally I cannot bear with OCaml’s syntax though.
Github user jwaldmann had already provided a PR for this around two years ago. I @mentioned him to see if he had some time to update it to work with the current branch. He has done so, and the new PR is now in review. Props!
I like the term denotative programming for this.
Stupid title, valid point. One never programs functionally for the sake of functional programming. 'Functional programming' itself never makes concrete sense.
I think it would be better to just have a safe form of head and tail and pattern matching as the default 
If we're talking about languages other than Haskell (you listening OP?), we must also mention http://fable.io/ for F# which keeps advertising how readable their JS output is --- can't vouch for it, but would be another "shortcut" option for OP I believe.
That looks fantastic! Thanks for the link.
What do you mean by "regular language" here? `word1|word2|word3|...` is still regular in the [Chomsky sense](https://en.m.wikipedia.org/wiki/Regular_language).
ARC is basically something Swift is forced into by dint of having to replace Objective-C. It isn't something you'd willingly force upon yourself if you didn't have to support that legacy. Performance is generally much worse than GC; caches get thrashed as reads become writes. In my experience using reference counted languages you quickly learn to not be so brave as you try valiantly to avoid having cycles leak, and then fail, and have them leak anyways.
The region inference never worked, so `jhc` just uses the Boehm conservative collector these days. Given the small scale of programs typically compiled with it, it gets by. The short version is that region inference and laziness don't mix, as laziness plays absolute hell with object lifetimes.
I'm not aware off any, and there's even an issue in lucid issue fracker: https://github.com/chrisdone/lucid/issues/16, if full correctness isn't required than by using e.g. tagsoup or xeno. I don't need such thing myself, but if someone has time to contribute, it would be great, sharing semiworking gist is a good start, no need to be perfect
I can express the morphology in one simple regular expression: `(C?VC?CV(VC)*(VC?VC?CV(VC)*)*)V` where C and V represents consonants and vowels. I can't generalize new words like this, and that's what I wanted to say.
Okay, so you had a simple regular language, and now you have a much more complicated, but still regular language consisting of a fixed number of words. How about a trie?
Translating idioms from one language to another with wildly different designs is rarely a good idea. The best you could probably do is try to minimize side effects into limited areas, but without static typing and type-class polymorphism, you're going to be very very sad writing Haskell in JavaScript 
Stack supports Nix with their "--nix" switch, which in theory gets you a solution to everything. There are a few people that talk about Stack breaking, but honestly that's maybe reason to file an issue with Stack, and see if you can get it fixed. So for me personally, I was already using Nix for a ton of other stuff at work (like C and Python build/packaging), and thought it would be nice to just not bother with Stack as yet another build dependency. Another reason is merely out of love for Nix (because it is a neat tool). If these feel like weak reasons, that's fine. I'd _never_ recommend a beginner start with Nix, and would always point them to Stack (as things stand now). Maybe that will change in the future as Cabal picks up more features from Stack and Nix.
By avoiding the need to turn HTML templates into haskell source code.
There are no definitive answers yet, but a number of alternative approaches. Some of us produced libraries and advertised them on the channel, others produced documentation or notebooks, which gradually appear on the docs page. So far all communication took place over Gitter but the mailing list will provide a more stable and official platform. As you can imagine, there are many gaps to fill in order to achieve that goal (an IDE with visual feedback, a flexible dataframe library, scalable numerical libraries etc.), and we are still quite few working actively on this. In general, in an open-source group, there is no "you" but "us", moreover each contributor works on these things in the spare time (apart from Theam.io who are working hard on the HaskellDO IDE and created the landing page). This said, we're very much open to suggestions, new contributions and constructive feedback.
Never saw Rumpus before, quite impressed. While it shines with 90th state of the art VR graphics I can't but help be impressed with the amount of work that probably was necessary to get to that point, from scratch probably.
I don't understand. What does the language have to do with the features of an application? Any application can be implemented in pretty much any language. There are no applications with features that no other language can do.
There is the [sciflow](https://github.com/kaizhang/SciFlow) library ... but I have not used it.
I don't think there's any such proposal, and it'd probably be a bad idea. There *are* cases where you really do want to catch absolutely **everything**, specifically certain types of bracket-like-things. The big issue IMO in the Haskell ecosystem is rather around distinguishing async from sync exceptions, and for that you'd need a runtime flag. (Type is not enough.)
How doe blaze deal with type-safety of the templates? Uses TH like Shakespeare?
Or to play Haskell golf: foo = flip until
Thanks, I will read through this and try to adopt it into my project. I am not a Haskell expert by any means.
[FGL](https://hackage.haskell.org/package/fgl) (Functional Graph Library) is based on this very interesting paper about encoding graphs inductively: http://web.engr.oregonstate.edu/~erwig/papers/abstracts.html#JFP01
It is a simplified version of Euterpea called SimpleEuterpea that my professor wrote. It isn't on github, but you can get it here: http://wiki.western.edu/mcis/index.php?title=Simple_Euterpea as a zip folder. I have it on my PC in the project folder for my stack project. I just need to figure out how to point to it.
Non-lexical lifetimes doesn't solve the problem of creating cyclic data-structures in Rust, they just makes it easier to write code operating on those structures, e.g. matching on the borrowed values of a borrowed `Option`/`Maybe` None of this solves the fundamental automatic memory-management problem of how to represent data with multiple owners, such as a cyclic graph. In these cases Rust has RC and [Arenas](https://github.com/SimonSapin/rust-typed-arena), or in extreme cases unsafe manual memory-management. 
I replied above.
I totally agree with their being benefits to using haskell source code. I use `blaze` and `lucid` pretty often, and I wrote [yesod-elements](http://hackage.haskell.org/package/yesod-elements-1.0/docs/Yesod-Elements.html) to make this style of write HTML available in my yesod projects. I don't like using the shakespearean TH stuff. But, my original point, which may have been lost, is that if I were in a situation where I was routinely having to convert large HTML files that someone else wrote into `blaze`/`lucid`/whatever, I would consider `heist` as an alternative.
*Hopefully*... I think it's one of those things that's annoying enough people that someone might eventually implement something better in anger. :)
Given that the stack.yaml `location` refers to a specific commit, pointing at GitHub doesn't introduce reproducibility issues as long as the remote repository isn't deleted and the commit isn't rewritten -- and, if you think that is a real risk in some specific situation, you can always point to your own fork at GitHub.
[removed]
Yeah, I agree
Can you show some examples of such optimizations and strictness analysis improvements? To me it seems like any optimization unsound in the presence of bottom is also unsound in the presence of an extremely expensive but eventually terminating (think heat death of the universe) expression.
Good to hear!
It seems that, in some circles, "functional programming" and "*pure* functional programming" have become conflated. LISP / Scheme / Racket are not pure (i.e. referentially transparent), but they are quite functional. I'm not sure if there's a pure, non-functional language. Maybe ATS? It seems functions as first-class values (lambda expressions, higher-order functions, closures inhabiting a uniform function type) tend to get added to pure languages quickly, even if they don't start with them.
However, I don't think anyone would be even remotely surprised if they learned that Agda, Idris, and Elm were written with C++. It's never surprising for an application to not be written in a certain language, because there's always multiple other languages that can do the job. My point is that we're not going to find some "killer application" that only Haskell can do, and "that no other language can do."
Check out this "algebraic graphs" library: https://github.com/snowleopard/alga. Comes with a few awesome blog posts too! One downside is that I don't think it's on Hackage yet, but no biggie if you're using stack.
/u/Tekmo, I have implemented higher-kinded graphs here: https://github.com/snowleopard/alga/blob/master/src/Algebra/Graph/HigherKinded/Classes.hs#L72-L82 And here is the issue where we can discuss this: https://github.com/snowleopard/alga/issues/9 I'd love to hear what you think about this.
To use those `git@github.com` links you need to set up a SSH key and, I believe, configure ssh-agent to authenticate you without a prompt. Alternatively, as you will only need read-only access for this, you can just use the https link, `https://github.com/codebikeclimb/SimpleEuterpea.git`.
&gt; Being optimistic is somewhat safer in a total setting. Is it though? I cannot think of a situation where the total language would perform an optimization that the non-total language couldn't. Optimistic evaluation works just fine in regular Haskell, hell you can even cheat a bit and immediately jump out of the optimistic evaluation when you encounter `undefined`, `error`, or `&lt;&lt;loop&gt;&gt;` &gt; Basically the step after totality is saying that this function has such and such cost. Now that I could see having much bigger implications for performance, but it also seems extremely challenging. And I am guessing that all real programs would cause the cost analyzer to have huge error bounds.
I don't really want to know what happened [here](http://i.imgur.com/nyT4vTo.png)
Like what for example? I find I learn languages much better by going through other source code and only really searching for what I don't understand. If it has a lot of non standard pieces though than I'd like to know what to watch out for. 
That's why you are an assiduous visitor of /r/basic 
Thank you! It was truly an honor to learn Haskell from one of the greats.
AFAIK, another name collision. However, Leibniz's monads are things you can't further split up, so in the sense that once you're within a Haskell* monad you're in there for good, the two notions align. Although, given that a CT/Haskell monad consists of multiple parts (a join function, a bind function...), by definition it violates the property of being a Leibniz monad. Anyone else want to jump in here and correct me or make a further point? You might want to read this if you're interested: https://dkalemis.wordpress.com/2013/11/23/the-correspondence-between-monads-in-category-theory-and-monads-in-haskell/ - if I'm not wrong the original term used for Haskell monads was just 'triple'. I'm willing to be shouted down on that point, though. \* I'm not willing to preach gospel on the CT variants this early in the morning, sorry :)
Awesome! Whats the interview process like?
&gt; you can always point to your own fork at GitHub I almost always do this for work code anyway since I'm often forking and patching something myself.
I can't say anything for react-flux, but we use GHCJS and Reflex in production at work. I'd definitely say they're production ready. But reflex is rapidly changing (if you keep up with the active branches) and GHCJS has a few pain points that can take some time to work around.
Not an expert, but some random thoughts: - you might want to use the internal module pattern if you want to test any of the private functions (may not be desirable) - I would have left this on github. It is well documented and has tests, but probably already covered by another library. Good to get hackage upload practice though. It might find a niche of usage with its very small number of dependencies, but more likely a smaller piece (the queue needed for astar or novel implementations of implicit or explicit graphs) would attract more usage.
Yes. As a casual Haskeller, but a long-time VS user, it is the most productive and comfortable environment I've used for coding Haskell thus far. I will also note that I have had better luck with Haskero than Haskelly to date.
Oh yeah! This makes me happy!
To expand slightly on that. Region analysis works reasonably well in the absence of mutation. But lazy evaluation is implemented using lots of mutation, which makes object life times unpredictable. 
Most cool -- be careful, sometimes more haste, less speed. Making quality an option looks like it could backfire (quite quickly).
It' possibly one of the first livecoding VR apps ... and it uses haskell ... and the dev made it open source! I find it difficult to understand why it hasn't gotten more excitement, particularly in the haskell world. 
I have not seen an announcement or anything... I guess that's why? :-)
Without the last line it can stream the result. With it, it has to hold onto bigstring in its entirety to reuse it to calculate the head.
My personal take on this is that if you're using a data source with side-effects (which your `myPutStrLn` is), then you probably don't want to use `ApplicativeDo`, or at least you want to sprinkle some explicit `&gt;&gt;=` around to enforce ordering when you care about it. Haxl by itself gives control over ordering by choosing whether to use `&lt;*&gt;` or `&gt;&gt;=`, but when you turn on `ApplicativeDo` you give up that control to the compiler.
Most "data with multiple owners" are definitely not cyclic, though. In these cases, refcounting works quite well even when the multiple-ownership makes the memory management not directly solvable by static analysis. --Of course using reference counting 'automatically' and pervasively, as in the *only* form of memory management (as seen in ObjC, Swift and in some newer C++ codebases) can introduce it's own inefficiencies and arguably isn't *that* much better than just using a garbage collector. But that's not really what Rust does!
Arguably, region analysis ought to still works well, under the common-sense assumption that any mutation is done under "exclusive" access for the portion of the object that is affected by the mutation. (This is exactly the restriction that Rust enforces using its borrowing system.)
[removed]
Do you have an actual complaint about my library and the rationale behind me developing it, so that it could be used independently in cabal-install? Of course I didn't cover the fact Cryptonite did not exist at the time I wrote this library, nor, I believe, did its predecessor set-of-libraries by Vincent support this signature scheme (I might be wrong, but at the time I believe I was the first to write any haskell package supporting Ed25519 back in 2013). Indeed, part of the reason I chose the "obscure ed25519 package" later on for Hackage (read: a well-tested set of C code used by many people and written by one of the world's most revered cryptograhers, with a very thin Haskell binding, which had already existed and been written by me for years at this point) was because it was minimal and offered very small, robust signature support, without dramatically increasing the size, dependencies, compile time, or scope of the resulting `cabal-install` tool (let's face it, shit doesn't compile quick for us -- adding 30 more modules that aren't needed is a complete net loss, if it can be avoided). Ed25519 also has ancillary benefits; for example, its short key size offering a very usable level of "copy &amp; paste", and easy key distribution, and signatures being small means they can be included in many places they otherwise wouldn't. It's also very fast (15x as fast as RSA-3072, roughly), so that's nice. Cryptonite is actually nicely put together (and minimal in dependencies), though at the time I think Vincent only released it months after our initial design, and even then, all we needed was actual signatures, so there's little need to adopt a larger package. In fact at one point I even wrote more code [to mimic OpenBSD's signify scheme](https://github.com/well-typed/libedsign) based on ed25519, which is more traditionally in line with how signatures behave in something like GPG -- signify's format I actually like quite a lot, and I think it's more featureful and robust. Though, I later abandoned this later on for a scheme based on simple, raw signatures, and TUF taking care of most of the harder work, as it was needless complexity in the face of the *actual* problem to be solved. (Of course, Cryptonite actually doesn't address or really "help" any more with TUF than `ed25519` does -- it's a protocol, not a primitive, so the package full of cryptographic primitives is ultimately irrelevant at some level). As a result, I actually believe I *resisted* the urge to re-implement too much obscure stuff, despite your suggestion. I'm eager in hearing detailed, technical complaints about my library and how it can be improved, based on your nitpick -- I even recently pushed [more documentation](https://github.com/thoughtpolice/hs-ed25519/commit/50266a0ef8969a018f78bd5902f7f5006210eeae) about how to safely deal with key material that had confused a user before. Of course, this took time, because hey -- I'm busy, and the library actually works, so for the most part I've been able to just let it slide. EDIT: Actually, I will admit, a security issue slipped by me in recent memory! It was, in some sense, theoretical -- one that would have caused signatures to never, ever work properly if you ever tested anything, and was something that was actually just as true of the C code ed25519 is built out of (hint: cryptonite is also using similar code, though I haven't looked, I bet it closed this gap), which assumes all buffers are 64-bytes long. [You can read 'em and weep](https://github.com/thoughtpolice/hs-ed25519/commit/07a09f4770a6dbbbb287c0c0e56381734c3ed1ae), friends. Actually, I have no idea if this is *really* a security issue in the sense it could expose key material (again, I doubt it, considering nothing would have ever worked) -- but I'm fair enough to consider *every* bug a security issue in a library like this. Of course, I expect these complaints to be rigorously founded, based on real security concerns you have with the implementation, its documentation, preferably with an analysis of where you think it could be improved. I wouldn't *require* a suggested improvement, that's ridiculous (it's a natural fallacy to suggest a such a requirement, as I'm sure you would agree) -- but naturally, since you are here nitpicking about such a detailed, nuanced field -- one which you are clearly concerned about, showing an already fascinating level of detail with your critique -- and with a suggestion like "go figure..." Well. You must withholding a secret of which I am not aware. As someone who has been interested in, and seriously studied security for nearly a decade -- I assure you, I'm interested in what you have to say. For example, perhaps you have identified a clear flaw in my suggestion of how to encrypt key material for safe on-disk storage, in the documentation (a scheme based on OpenBSD's `signify` format, in fact), and you have a robust attack for where it fails? Inquiring minds want to know. I eagerly await to learn from you and I hope that we can learn more from each other in the future, from interactions like this, as you clearly have skill far beyond my own. Please tell me what else I could improve, friend.
I think WebAssembly has received a lot of (currently) undeserved hype as a replacement for JS so you can write your webapp in whatever language you want. While hopefully this will be true someday, currently there's basically no bindings at all to major browser APIs (like the DOM), and until those arrive, it's honestly pretty useless. Moreover, creating those bindings is tricky, since the JS world is garbage collected, but WebAssembly (currently) has no support for GC. How exactly are these two worlds supposed to play with each other in any sane way? Hopefully we'll see real progress in these areas over the coming months, but given that this DOM/GC integration is [repeatedly marked as a pink unicorn emoji](https://github.com/WebAssembly/design/blob/master/FAQ.md#is-webassembly-only-for-cc-programmers) I'm not too optimistic.
I mean WebAssembly I think is more or less a fairly restricted subset of JS. So some of the JS features used by GHCJS may have been removed.
I was assuming that the long term goal was to make JS compile to / be interpreted by WebAssembly. Or just be an entirely separate thing to WebAssembly. So I see no issue with the whole GC vs no-GC thing. And I know that currently WebAssembly is still decently far out, it is important that Haskell interacts with WebAssembly as early as possible to make sure it implements the features necessary for it to be a decent compile target. It being low level by itself isn't enough, e.g C is not a great compile target for Haskell.
But, to do this you need a developer capable of building good abstractions. As I see you need somebody fast and capable of building good abstractions, and there is no reason (in my experience) to believe that those qualities cannot be found in the one developer.
I actually think that's unlikely. The stuff removed would have been high-level language features not needed for something closer to assembly language.
There is an issue for that https://github.com/erikd/jenga/issues/2 and progress is being made.
I'm fairly sure /u/luite2 - the author of GHCJS, has plenty of thoughts and/or plans regarding WebAssembly, maybe he can chime in.
AFAIUI there's an FFI-like thing to call JS functions. Could one not marshal your DOM-updates through that? I understand that this might be moderately less efficient than having direct access, but then even updating the DOM directly can actually be very inefficient due to relayout, etc. That's why the virtual-dom approach seems to be very popular... and I don't really see why a virtual-dom style approach would be precluded by the lack of direct WASM-DOM integration. Nor is GC particularly relevant to that. No, I think the major obstactle here is actually porting the GHC runtime.
&gt; I would think that Javascript is very close to WebAssembly It's not.
You just proxy the callback through the FFI via a string ID. People have been doing this with for **ages** in non-JS server+client frameworks like GWT.
It would be interesting to see the time and memory complexity of your algorithms expressed by referring to the functions `next`, `prune`, etc. For example: `bfs` presumably performs O(|V|) calls to `next`, where `|V|` is the number of vertices of the graph. The cost of `next` may not necessarily be O(1), and may also depend on the parameter, which makes the overall complexity analysis quite tricky. I can imagine using this library in cases I don't want to represent the graph explicitly, but I'd very much like to have some complexity estimates.
It's pretty easy to bind to the DOM with FFI... Not sure why people keep calling this a major problem. Edit: [For example.](https://github.com/AndreasMadsen/wasm-dom)
Am using it with Haskero and find it much better than atom, which I used before. I prefer VS Code over atom because it feels a lot faster &amp; more stable. Also it comes with built-in git support, and a good integrated terminal. 
YES! It's BACK! Great news. Thanks to all involved.
There were some posts on reddit a while back. I think it's just one guy so there's probably not a huge amount of resources to go beyond that by putting out major press releases and connecting with media outlets.
&gt; WebAssembly should be an alternative to using JavaScript and, honestly, this is what should have been done from the beginning That is an eventual goal, but they're taking many small steps to get there. At first, WASM is basically a replacement for asm.js. For now, it makes it easier to write number-crunching code that will perform well in the browser. With luck, we might have GC and DOM support sometime in the next 5 years. 
The problem with WASM is can't (yet) interact with the JS GC. The WASM/JS problem is similar to what you have when interfacing C++ and .NET code, or C and Java code, or C and Lua code. I'm a little familiar with all three of those. One of the big problems in all of these is with holding references to GC objects in the native side. The garbage collector doesn't just reclaim memory; it also moves values around in memory. But the garbage collector can't automatically know when your native code is holding a reference to an object, and it can't reach into your native code's memory to fixup references when objects move. In .NET, you handle both of these cases by "pinning" the GC object. This fixes its location in memory, and also prevents it from being GC'd. Of course, this ends up looking a lot like manual memory management - you have to ensure that you eventually unpin the object. In Lua, the interaction between the C runtime and the Lua runtime is mediated with a stack. There are also ways to get opaque references to Lua objects. It's more indirect than the .NET approach, but it's also easier to get right. &gt; You just proxy the callback through the FFI via a string ID. Sure, maybe the actual event wireup could be done with string IDs. But JS events usually come with arguments, and those arguments can be simple or complex JS objects. DOM events, for example, send an instance of a subclass of [`Event`](https://developer.mozilla.org/en-US/docs/Web/API/Event), which contains references to all kinds of complex, GC-managed data structures. AFAIK, WASM doesn't yet have a way for the WASM runtime to store references to GC values, except for imported functions. So sure, maybe you could use WASM as it exists today to attach an event listener to a DOM element, but your WASM code couldn't actually inspect the incoming event in any way. You'd need to write some JS shim to make it useful. &gt; People have been doing this with for ages in non-JS server+client frameworks like GWT. I'm not familiar with GWT. I thought it compiled all your Java down to JS. So sure, your GWT-compiled Java code can access things in the JS universe because your GWT-compiled Java code is running in that same JS universe. And both Java and JS are garbage collected, so the semantics are pretty close. But WASM is essentially a separate runtime that has different semantics from the JS runtime, so you need explicit interop points. And WASM doesn't have them yet. 
I will switch to it in 1-2 weeks, just a few line to change. Thanks :)
I've been using VSCode for nodejs stuff a lot lately and it's pretty nice. Going to give Haskero a try. I like having ghci + sublime side by side vertically and haven't been able to recreate that anywhere else so far.
What is the difference between them?
I was just talking with /u/luite2 about this. He mentioned a couple of challenges that may be difficult to find solutions for: 1. TemplateHaskell 2. JavaScriptFFI I'm sure Luite can give more detailed answers, but allow me to summarize my imperfect understanding. First, TemplateHaskell currently works by compiling the TH to JS, firing up a Node interpreter, running it, and getting the results back. This is obviously suboptimal. I believe it does this because TH allows you to do IO, and to do that correctly you need to be interpreting in the target environment. It seems to me that GHC should have a safe subset of TH that only allows pure operations, and for which it should be possible to accomplish the vast majority of the things we're currently using TH for. But apparently that has its own set of challenges. Second, if you're going to do production web development, you've got to have a JavaScriptFFI. Going straight to WebAssembly deprives us of the obvious approach to that unless we also leverage a JS -&gt; WebAssembly compiler which I'm sure would have its own set of challenges. I supposed the naive answer to both of these would be to just tack on a JS -&gt; WebAssembly layer at the end of what GHCJS is already doing. But that's rather unsatisfying.
If a troll is someone who make certain people feel the uncomfortable sensation of not being in a sheepfold, yes I'm a troll.
this is such a thorough owning, wow &gt; This user has deleted their account. Looks like you can add "Slayer of Trolls" to your resume
&gt; Is there a way to generalize iterating over a record type's accessors? Lenses, I think. However, if I'm understanding your issue correctly, why not just write a custom `Ord` instance for your `Field` type?
&gt; Are they tied to FP complete? I assume the user chose that name to position themselves as pro-stack and anti-cabal. I don't think they have any professional ties to FP Complete. 
&gt; However, if I'm understanding your issue correctly, why not just write a custom Ord instance for your Field type? To expand a bit on this, both the instances of `Foldable` and `Traversable` of `Data.Map.Map` and related functions such as `traverseWithKey` [respect the ordering of the keys](https://hackage.haskell.org/package/containers-0.5.10.1/docs/Data-Map-Lazy.html#v:toList).
I don't know, but I'd be surprised if it was actually tied to FP Complete proper. I'm pretty sure they're just a troll that likes to piss off the stack vs cabal folks. There's another troll that was more active last year, talking about how manual memory managment was Alpha and pure functions were weak and beta, but I don't think they're the same person.
what part of the bay?
If you google "webassembly GHCJS" you will see that I am not just guessing. In the past this WAS a problem.
Ok cool! Do you think it is a suitable compile target for Haskell?
Isn't it better to always modify the DOM via the virtual DOM and such for performance? The re-render stuff is interesting, I will keep an eye on that and maybe submit an issue if I run into problems. For getting GHCi to work I simply made the whole project based on GHC and not GHCJS. But then made a stackjs.yaml file that my little shell build script uses when calling setup, build and such, that calls into the GHCJS compiler. That way `stack ghci` and `hdevtools` both work fine. 
I thought part of the goal of WebAssembly was for JS to eventually compile to it / be interpreted by it? And wouldn't interacting with JS be an issue every language that compiles to WebAssembly would have to deal with? That doesn't seem Haskell specific. 
I think /u/fpcompletely may have tried to allude to [this other drama episode](https://www.reddit.com/r/haskell/comments/5lxv75/psa_please_use_unique_module_names_when_uploading/dbzhx91/) that occured a couple of weeks ago.
&gt; Actually, I think it's insane that this situation has been brewing for quite a while, and that this kind of shit talking and idiotic behavior has been repeatedly put up with ad nauseum by so many users here. You can go look up literally a dozen threads about all this stupid shit in the past, if you'd like. I'm not going to do the work for you. I don't know what the situation is, so I don't know what to even search for. That's why I was originally asking for more context to better understand what's going on. &gt; You should ask yourself why the user deleted their account, in fact, while I have no compulsion to do such a thing. I mean, if my response is so sanctimonious, so utterly over the top, clearly they have nothing to fear. They're anonymous anyway. Aren't I just hurting myself? It's all just insane dribble to you, right? Except for the minor fact that deleting their account deletes the trail of their behavior. It makes it easier for them to come back, to reinforce their narrative next time. They deleted their account, because actually being confronted directly for their behavior -- it robs them of everything. So the only thing they have is to start all over again. That's one interpretation. Another is that he/she was getting hate-PMs, or something else entirely. I don't know who this person/troll is, at all. &gt; From a person like myself with nearly a decade of insight into the community, I assure you -- I said nothing in my post lightly or happily. Of course I'm certain my experience and 10 years of insight mean nothing to you -- after all, I'm just insane and sanctimonious. I'm glad you have determined my anger is illegitimate. Thank you very much for that. Just because you've been around for 10 years doesn't mean you are always a pleasant person. Just because you made one post that I view as unpleasant and over-the-top doesn't mean I don't respect you as a person, or think everything you say is equally sanctimonious. I thought you were a great podcast guest, for example. I think the solution here, if there is one, is to bring to light any toxic or unacceptable behavior in a more friendly or agreeable way. That way, your point can get across without other anonymous people like me getting mixed signals from your caustic words. To the uninformed, _you_ come off looking like a toxic asshole troll, even though I know you are not.
Yep. It's somewhere on their list of post-MVP features, but I don't think it's a very high priority, compared to stuff like multithreading
FWIW, cabal and nix can also handle GitHub projects just fine. :)
Check out implementations with Python's OCR package Tesseract: http://resources.infosecinstitute.com/case-study-cracking-online-banking-captcha-login-using-python/
Will LLVM have a web assembly backend? Any chance that then Haskell code compiled with GHC and `-fllvm` could target wasm? 
Great stack! Can you say anything more about your ES/CQRS layer?
&gt; TH allows you to do IO, and to do that correctly you need to be interpreting in the target environment Is it actually defined in such a way that that's the only correct behavior? Or is it just a matter of otherwise needing to compile things for both build and target architecture? It seems natural to me that compile-time IO "should" be expected to be run on the build system.
It's hard because TH assumes that any TH code will be run on the same platform as the target. Meaning that if you're compiling to JS, the TH has to run in JS on a Node machine. If you're compiling to iOS, the TH would have to run on an iOS device, so it probably just doesn't work.
It already does. It's called Emscripten. But it's going to be extremely nontrivial to port GHC with it.
How come? Why doesn't the existing code outfitted by llvm pretty much work? Modulo some changes with what kind of IO interactions you can do. 
Are you guys ever planning on opening sourcing your internal Event Sourcing library? I have an [event sourcing library](https://github.com/jdreaver/eventful) of my own that I use in a few projects, and I just recently added postgres support. I would love to compare notes (or switch to yours entirely if I like it better, of course!). Event Sourcing and Haskell are a match made in heaven, I can't believe more libraries don't exist.
That seems totally fine as long as your language runtime can correctly unregister its use of a JS value. GHC's GC would just unregister anything it's not using.
This was the part of the interface that I was the least sure about. I put it as-is because I found that I often had multiple functions which would immediately tell whether I was at a dead end, and a function which cleanly generated the un-pruned "next" states. I thought that (for example) dfs next [at_dead_end1, at_dead_end2] at_end start was a lot nicer to type than dfs (filter ($st -&gt; not $ any ($ st) [at_dead_end1, at_dead_end2]) . next) at_end start In the case that you don't have any pruning functions, typing the extra `[]` isn't so bad in comparison, and is fairly low-line-noise as far as it goes. Having the extra argument does complicate the interface, but I thought that it was worth it for the convenience. I'm still not 100% convinced that this was the right call though, so I'd be interested in feedback on this.
It might be better to frame the main implementation just in terms of the `next` function, then add some combinators for building these. E.g. dfs (myNext `pruningWith` at_dead_end1 `pruningWith` at_dead_end2) at_end start
The integrated console is lovely, I have not found anything comparable elsewhere (including spacemacs). What I am missing is that lovely spacemacs SPC navigation. What I don't get though is why there is a debug button but no compile and run button?
I might be missunderstanding you, but why don't you just put them all in a list/array/vector and then call `sortOn _label` on them and keep them sorted? That way they stay in a fixed order and you can easily access them in that order whenever you need to.
I mean... yeah? That sounds about accurate. But the truth behind it is simple... posting online... it's a gift. Our gift. Let's never forget that -- and all post online, even more than we did yesterday.
Would you like to help make the GUI [part](https://github.com/deech/fltkhs) better?
Sounds good no rush. Hopefully by then it gets pushed into community.
Interesting! 1 year ago I also wrote a proof-of-concept (read: very incomplete) version of [that in NaCl](https://github.com/PkmX/naclffi) for C++, and you can do the reverse for callbacks written in C++.
["If you're having problems typing 'Ø', you can use only"](http://hackage.haskell.org/package/backprop-0.0.1.0/docs/Numeric-Backprop.html#g:3) Thank you! I massively appreciate this.
There are some places where lack of genuine dependent types is hurting such deep embedding of types. See for example https://github.com/raaz-crypto/raaz/blob/master/Raaz/Core/Types/Tuple.hs#L146 https://github.com/raaz-crypto/raaz/blob/master/Raaz/Core/Types/Tuple.hs#L155 I am sure this can be handled by GHC-pluggins but as of now I do not think it works very well. I do not know much about cryptol but my understanding is that it might have stuff like type level nats baked very much into its type theory and such stuff will be very natural to express. There are some solutions on the haskell side. 1. Start using a genuine proof assistant like coq and generate the actual code. I think we have not reached that point were we genuinely need so much power. This also has a slight problem because generating Haddock documentation will be a pain. So this is good only if every one directly refers to the coqdoc documentation. 2. Coverup most such issues using liquid haskell (which is one of the open tickets). Unfortunately, I have not been able to mix liquid haskell well at the package level and I will be happy if some one can help me out here. Any way thanks /u/joeyh for the free advertisement for raaz ;-)
Error: kvanb at entropy in ~/git/mineserver on master [!?] $ stack bench mineserver-nbt &lt;snip&gt; Error: While constructing the build plan, the following exceptions were encountered: In the dependencies for mineserver-nbt-0.0.1.0: mineserver-nbt-test must match -any, but the stack configuration has no specified version Plan construction failed. I've just been putting `mineserver-nbt-test` under build-depends for `mineserver-nbt-bench` which are both defined in `mineserver-nbt.cabal`.
While I don't quite buy what the author is selling, I do have counter-points to your two major points: * **Construction**: This fits the author's one use case for lists. First you efficiently build the list, without looking at it (although DLists support more construction patterns), and then you consume it into a more permanent structure. * **Infinite Lists**: Infinite trees, a la [this](http://stackoverflow.com/a/3209189/52310) SO answer are useful (and better) for many of the same memoization tricks.
Thanks I'll see if I can add a new shared library, seems like the best way. I don't want the app to depend on QuickCheck 
Here's another one: - Include the shared code as part of the library, but behind a flag which is off by default.
`stack` works with Haskell packages, which are defined by `cabal` files. If you don't have a cabal file, you don't have a package, and stack can't work with it. It seems like your professor has provided you with a single source file, lacking a cabal definition. They must have intended for you to copy the file directly into your project, and import it, rather than referring to it as a library.
Why would you need arbitrary data for benchmarks? That seems like it would make reproducible benchmarks very difficult
Don't , but it saves about 500LOC of test data and the arbitrary instances are tagged with a cost using 'reflection' to hint them how expensive a structure to generate . Should generate roughly the same cost data each time . If there is wobbling it would uncover an issue 
That's very cool! I suppose you could split the arbitrary instances/etc into a separate library and import that from your benchmark/testsuite, but that's not entirely satisfying.
May I add, some bad things like trolls for example, are actually symptoms of good things happening. It shows you are doing something to "offend" them. It is hell of a lot better than doing nothing and not getting trolled.
I'm so glad to read people are working on better regex packages/libs/APIs for Haskell. I've been definitely missing these coming from other languages. Off I go continuing to read the article, I just wanted to exclaim my initial excitement!
If you're using Haskell and you care about performance, you already made the wrong choice. Edit: yes, your down votes give me power! That said, many FP concepts map nicely to CUDA which is fast.
Awesome. That was a good guide for like big picture stuff.
In my experience, one of the reasons I am so reluctant to reach for a non-list data structure in Haskell unless I obviously need it is because the burden on me, the programmer, is frustratingly high. Consider a language like Java: perhaps I need a map? Well, I can just type `Map&lt;K, V&gt; hash = new HashMap&lt;&gt;();`, and my IDE will figure out the imports for me. After all that’s done, I can use lots of functions on my new hash with simple, reasonable names. I can do `hash.get(k)` on one line and do `list.get(n)` on the next, and I don’t have to worry about clashing namespaces. If I introduce a `Set` a couple lines later, I can use `set.size()` without worrying about qualifying it. Picking the right data structure for the problem is generally cheap, as long as I am familiar with the types available. Shifting gears a little bit, it’s also worth looking at Clojure for an example where choice of data structure is nearly effortless. Persistent vectors are a wonderful general-purpose data structure with great time and space complexity for a multitude of operations. Maps are, fast, ubiquitous, and even have their own syntax. Sets are similarly cheap and easy to use. When I need an iterator, Clojure’s lazy sequences are not only perfectly suited for the job, they are totally transparent. I can use all the same functions that I use on other data structures on them, without qualification. Neither of these languages are personal favorites of mine, and in fact, I try to avoid them as much as I can. In Clojure, especially, the shifting types without static typechecking can make it tricky to remember when you have a sequence and when you have a vector, something that can cause performance issues as much as anything else. A language like Haskell could provide ways to make the programmer much more aware of the conversions at play, but it doesn’t really do that, since it doesn’t make it easy to mix and match data structures like some other languages do. In Haskell, the barrier to entry is comparatively massive. - Having to import each data structure is, itself, a barrier that many languages don’t have, something that is multiplied by the fact that these data structures are often in separate *packages* that need to be added to the cabal file. It doesn’t help that there are often many choices per data structure type, and module names are wildly inconsistent. Tooling can help here, but this is a nitpick. The other issues are more important. - Qualifying everything is a huge pain. If you think otherwise, you are being naïve and idealistic. `reverse (take 5 (iterate (* 2) 1))` is simply more readable than `S.reverse (S.take 5 (S.iterate (* 2) 1))`, and it gets much worse when infix operators come into play. This is actually a symptom of a much larger problem, though… - APIs for data structures are wildly inconsistent. Operations that are not in some `base` typeclass are a complete crapshoot. Want to append an element? For `Data.Vector`, use `snoc`; for `Data.Sequence`, use `|&gt;`. Random access? `Data.Vector` has `(!)` (already a departure from `(!!)` for lists), but `Data.Sequence` uses `index`. This means constantly consulting documentation for each type of data structure, quite the departure from the ease of using lawful methods without worrying too much about the underlying type. - If you don’t think the above problem is bad, you may be more swayed by this: it’s impossible to write functions in Haskell that are generic on the type of data structure provided.^1 If you find yourself using a particular utility function all the time on vectors, then decide you want to use the exact same thing on a sequence, too bad! You get to write it over again. This might not sound *so* awful, since some operations need to be implemented in structure-specific ways to be efficient, but many high-level helper functions don’t need that sort of deep knowledge. In practice, this limitation puts an enormous burden on the authors of packages providing data structures, forcing them to reimplement the entire kitchen sink of useful functions, even when they could be obtained just as efficiently via composition of simpler primitives. I’m not saying that fixing this is easy. Trying to standardize an API around data structures can be extremely difficult, especially if you want to allow them to be suitably efficient. Still, it’s not impossible, and these shortcomings make mixing data structures in the same project/module—an obvious necessity to be able to pick the right structure for each job—far more inconvenient than it needs to be. I don’t blame people using lists in simple situations where the performance difference is negligible because dealing with doing it the right way was simply too obnoxious. --- ^1 Certain packages take steps towards making this possible of course, including `lens`, but none of them have the prevalence to actually fix any of the actual problems.
(I haven't even read the other articles yet.)
Thanks -- constructive feedback is always appreciated.
It's not from me, it's from *[type-combinators][]* :) [type-combinators]: http://hackage.haskell.org/package/type-combinators
It seems like the main thing needed is index based lookups and insertions. So a typeclass like: class Indexable t where type Key t type Value t (!) :: t -&gt; Key t -&gt; Value t put :: t -&gt; Key t -&gt; Value t -&gt; t Seems like it should alleviate this problem massively, `Monoid`, `Foldable` and `Traversable` should give you most of the other stuff. For example with `Set`, `Map` and `[]`: instance Ord a =&gt; Indexable (Map a b) where type Key Map = a type Value Map = Maybe b (!) ... -- safe lookup put ... -- `put map key (Just value)` to insert -- `put map key Nothing` to delete instance Ord a =&gt; Indexable (Set a) where type Key (Set a) = a type Value (Set a) = Bool (!) ... -- check if an element is in the set put ... -- `put set value True` to insert -- `put set value False` to delete instance Indexable [a] where type Key [a] = Int type Value [a] = a (!) ... -- standard indexing set ... -- standard assignment Now I suppose with lists you may argue that `(!)` should be total, but you should probably not have `set` take in a `Maybe`. So maybe the class needs to be generalized a bit. But having the type be the same gives you some laws like `put c k v ! k == v` (sans overflow / `_|_`) which is IMO pretty nice. What other operations do you think are needed that can't be efficiently built on top of indexing, folding, traversing or concatenation (Monoid)? I am sure there are some but I think the above is going to make life much much easier.
Backpack can do that. But if qualified imports are "too awful", mucking about with Backpack is almost certainly more awful. The overhead is probably worth it if you have a large collection of utility functions on data structures, but not if you only have one or two. One knock on effect of Backpack which might be useful is solving "APIs for data structures are wildly inconsistent": you'll want APIs to line up as much as possible, since that means more Backpack reusability. I found myself writing shims to smooth over API differences between packages and I think those shims might be useful regardless.
When one finally understands corecursion, it is customary to delete a corecursion tutorial from the internet. That explains the dearth of tutorials. That said, I liked [these slides](http://homes.cs.washington.edu/~djg/msr_russia2012/sangiorgi.pdf) about the related concept of coinduction. Especially the side-by-side definitions in slides 76&amp;77. 
Strongly agree with your suggestion. I have a personal hsfiles for stack which initializes my projects with an alternative prelude that makes many things much easier to reach for. 
That is like saying "If you are traveling by a car and care about not dying, you already made the wrong choice"..
 if it is not some form of container, the Foldable instance is useless.
&gt; insane, sanctimonious response It shut up an annoying troll, and was a fun read. I rate it 10/10.
Not misleading for `Functor` either: https://bartoszmilewski.com/2014/01/14/functors-are-containers/
List was just an example I could see it being much more useful for different types of Vectors, Matrices, Maps, Sets and Sequences.
Alright, so, I've read the article and gave the library a cursory try, and I must say I like it very much so far. Likely the most pleasant regex library I've tried for Haskell up until now. I especially like the replace functions, and that they support named groups. And that there is a replace as well that takes a function which returns a `Maybe a` to determine if/what to replace to. Very nice stuff. Looking forward to give it a more in-depth try on the next opportunity I need text matching, and I may be able to give you more feedback then.
There is a dialectical step involved here, though: before adopting this highly abstract notion of container, it is necessary to set aside any overly concrete pictures of what a container is supposed to be. To put it in another way, I suspect that changing the signature in the Prelude docs to `fmap :: (a -&gt; b) -&gt; container a -&gt; container b` would nudge many beginners towards a misleading picture.
Another example is non-empty or infinite lists, for the purposes of *understanding* the flow of types throughout your code, as well as their boundaries. If you ever see [] -&gt; error ... x:xs -&gt; ... or `(a, [a])` (or `([a], a)` or curried variant `a -&gt; [a] -&gt; ...`) see if [`NonEmpty`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-List-NonEmpty.html) fits, then **follow** the type errors. Same if you see pattern matching on `x:xs` but not `[]`, do the same with [`Stream`](https://hackage.haskell.org/package/Stream-0.4.7.2/docs/Data-Stream.html). These are both instances of `Cofree`! See if *that* generalisation makes sense. Etc. etc.
&gt; They deleted their account, because actually being confronted directly for their behavior -- it robs them of everything May I kindly ask to wield your mighty sword once more and apply your troll slaying skills [to this weirdo](https://twitter.com/realDonaldTrump)...? Pretty please....!
I think someone once told me that lists are more of a control flow structure than a data structure in Haskell. That seems to resonate with your comment. 
Not trolling here, but do you have examples of quasi-production Haskell code with high computational performance? I know of of many good libraries and applications that use Haskell to manage concurrency, and also of many papers that show proofs of concepts, but I'm trying to figure out where and how it's used. My own field is high-performance functional programming, so I'm trying to get the lay of the land.
The list of competing libraries should include [neural](http://hackage.haskell.org/package/neural)
&gt; Constructing linked lists is often way more efficient than building vectors or other non-list data types (of course, depending on your use case). Meaning they're often the best choice when you know you won't need random access, because the construction will be so much faster. I've created a benchmark with some data to demonstrate this: https://github.com/haskell-perf/sequences#consing I wouldn't call it perfect, but it's more than just a claim. Source [here](https://github.com/haskell-perf/containers/blob/master/Main.hs#L46).
I perfectly understand that. But then you use a mutable structure, you lose sharing, and stuff happens in ST or IO. That's just not the same thing as list append, and it's not fair to compare them!
Haven't needed to benchmark and finetune myself but from blog-posts I tend to get the impression that in order to get to "beat" levels of time-and-space-usage, you'll need to severely mutilate the code and algos using all sorts of spaced-out, carefully-placed combinations of strict/lazy annotations, a `seq` *here* but not *there*, a strict field *here* but not *there*, rearrange `let`s/`where`s for sensible sharing in "loops" (I think *this* really is something a linter/refactoring-tooling could do .. might have to look into that)
It looks like you're trying to mention another user, which only works if the username is typed correctly (otherwise they don't receive a notification): - /u/edwardkmett --- ^I'm ^a ^smartass. ^Bleep. ^Bloop. ^| ^Visit ^[/r/mentionhelper](https://www.youtube.com/watch?v=dQw4w9WgXcQ) ^for ^discussion/feedback ^| ^(You have now been subscribed to catfacts. Message "UNSUBSCRIBE" to stop.)
You have been successfully blacklisted. I won't bother you again!
I think by definition, C is medium level. Everything lower is Low-level and everything highet is High-level ;-)
That works for some cases, doesn't nest as nicely if you need that data ZOT :: Nat -&gt; Type where Z :: ZOT 0 O :: ZOT 1 T :: ZOT 2 Z :&amp;&amp;: O :&amp;&amp;: T :: (ZOT :**: ZOT :**: ZOT) '( '(0, 1), 2)
Well, the only time I had real performance issue using Haskell was when I actuall was using Vector (premature optimisation). All the performance bottleneck was in building the vector. I was using immutable vectors not mutable one. I switched to Sequence and the processing time went from about 10 min to less than 30 sec.
The "don't use list when you need to access them via index" is obvious, so I agree with that, but that's not what we use lists for (aren't we?). The "don't use list if you need to sort them" is rubish. 99 % of use case of lists are iterator, you need to process them all, and don't really care of the order. When you care of the order, then you can sort them, and still feed functions which expect an iteror (I mean a list). The cost of sorting the list is not more expensive that sorting a vector and you can produce a new "iterator" for function which don't need things in a specific order ... What's the problem ?
Sure, as I said the `vector` library does not help here (and for immutable vectors I don’t think it can). I was just trying to point out that this is not a problem of vectors (in the sense of storing a sequence as a contiguous block of memory) per se.
With the help of electron a SOTA haskell gui is very feasible. The main impediment is there are nearly 0 people with a design background in the haskell community to do it well.
Please as Bitcoin as a payment option for donations. If it was there I'd donate $50 immediately.
That is exactly what lenses do. ghci&gt; import Data.ByteSting.Char8 (pack) ghci&gt; import Control.Lens ghci&gt; "Hello World!" ^? _head Just 'H' ghci&gt; pack "Hello World!" ^? _head Just 72 
Well... In order for it to not be misleading, he had to virtually redefine what the word "container" means. Saying one thing but meaning something different than what most people will interpret is pretty much the definition of "misleading" =P
Not saying this isn't a big usability problem but doesn't [OverloadedLists](https://ghc.haskell.org/trac/ghc/wiki/OverloadedLists) mitigate some of the issues? If nothing else it at least provides a uniform destructuring interface.
I don't think it's a good example of high computational performance, though the numbers are impressive. Perhaps it's because I was expecting some hard number crunching example, mostly cpu bound.
But you wrap each remaining element of list in additional thunk. Ouch. So, with your solution your only hope is rule-rewriter firing.
&gt; the vast majority of *my* usage of list-like types is non-indexing computations like iteration; precisely what they say lists are good for I guess the article was written for newcomers, instead
This is a great idea. It would be interesting to add space performance as well if possible.
Is it possible to use the same unity for one benchmark (e.g. all Consing benchmarks use ns)? It's a bit easier to compare the numbers if you don't have to convert from ms/ns/µs. 
You got `?=~` wrong when you introduce its type :) Also, to at least provide some content: I'd really love to have a reasonable regex lib in Haskell, so much that I was even considering to write one myself, but now I'm grateful you came first.
Agreed, good idea, also for space metrics. Just came across http://neilmitchell.blogspot.de/2015/09/detecting-space-leaks.html yesterday and it annoys me just how easily space wastage sneaks into innocent elegant code until one finally has a good *intuitive, while coding along* grasp of how/when/why they occur (I got it in theory, but would love to evolve a brain ---or live-invokable-from-editor tooling--- automatism to not even incur them while writing code)
we actually were bought by box and support is discontinued. sorry!
I have just checked it against both `regex-tdfa` and `regex-pcre` and they both match the signature given in the article (to my eyes anyay). https://hackage.haskell.org/package/regex-tdfa-1.2.2/docs/Text-Regex-TDFA.html https://hackage.haskell.org/package/regex-pcre-0.94.4/docs/Text-Regex-PCRE-Wrap.html What is your source?
Oh yes, I mis-typed the '?=~' operator -- will get that fixed. Thanks for the feedback, btw!
Well, it's arbitrary precision for Integer, but not for Int, so you can choose a fast one by default.
the Integer type uses GMP, Int does not
This is a bit of a wild claim in my opinion. First you can only speak with certainty about locality of strict unboxed vectors, and secondly locality of lists probably depends a great deal more on how they are created and consumed and the GC characteristics of our program than any of us know off hand for a reddit comment, but my intuition is that lists are almost always layed out (mostly) contiguously in memory and seldom experience fragmentation. But I would be interested to hear if someone has studied this!
One thing I've found just now: https://github.com/BNFC/bnfc via http://bnfc.digitalgrammars.com/tutorial.html --- could be a stepping stone I'm looking into. Still, whatever ideas come off the top of your heads, shoot!
thanks for bringing this to my attention, it's awesome :)
I hate dealing with imports. At work we use a custom prelude which saves some imports bit causes other annoyances. I suspect some people use an editor and tooling that solves this for them, but honestly what would go a huge way for me is: - some facility to put qualified packages in scope everywhere (I could specify this once in my cabal file), and perhaps bring certain libraries into scope - a way for defining the default qualified name of a module _in the library itself_ which would be used above On the latter point, people already use fairly standard qualified names for the most used packages, and at least I rely on them when reading code, so why not make them part of the package
True, it's IO bound, but that is normal. As far as hard number crunching, I should say I don't have any kind of expertise, but my understanding is that this kind of stuff is only ever made fast (or: fastest) by implementing it in a low level language (that may be a library providing an interface in another language). (E.g., NumPy for Python is written in C.) I guess you're not going to beat that in Haskell without doing the same thing.
thank you
That's also how bytestring builders work in Haskell. Bytes are written to a buffer via unsafeIO. The buffer is popped off every time it's filled and the result is a linked list of dense arrays.
I recently [added](https://github.com/NixOS/nixpkgs/commit/dfcc9e2994eba3544ddfe303be5128ff17195ca6) `integer-simple` variants of GHC to nixpkgs. See the [manual](https://github.com/NixOS/nixpkgs/blob/master/doc/languages-frameworks/haskell.md#building-ghc-with-integer-simple) how to use one of these variants or how to get into an environment with Haskell packages compiled against `integer-simple`. The cool thing is that all these GHC variants and package sets are build by [hydra.nixos.org](https://hydra.nixos.org) meaning you don't have to build these yourselves.
Why isn't there a grow-able array list in the containers library? Honestly I don't even know where to find that on hackage. I mean something like Java's ArrayList which doubles the backing array in size when it crosses a load threshold so that you get constant time indexing and amortized constant time append.
That's is what [IsSequence](https://hackage.haskell.org/package/mono-traversable-1.0.2/docs/Data-Sequences.html) (in the mono-traversable) package . It looks like a good idea in theory, in practice (in my experience at least) it doesn't really work. It breaks type inference and error message are hard to understand.
Very questionable affirmations that do not apply to the case anyway. `container` is no less polimorphic than `t`.
Nice work. I have always wanted something like this to see the actual results. I have also created a PR adding [indexing benchmark](https://github.com/haskell-perf/sequences/pull/1) for sequences. Also keen to make this more comprehensive with the help of others. Free feel to add me there.
Also investigating [this](https://www.reddit.com/r/haskell/comments/2ig8ml/where_can_i_find_the_complete_bnf_syntax_of/) (so we don't duplicate discussion points ;)
There is bytestring and text, which are much nicer to use than a vectors of characters. They also have lazy versions and builders, something that is dearly missing in the vector package. Then there are the streaming libraries, which are basically lists interleaved with effects: streaming, pipes, and conduit.
Fixed!
+1 for the feature idea but please note that criterion is pretty much unmaintained these days and it might take a while to see a release. With stack, it's of course still easily possible to use the PR branch of criterion for the benchmarks.
Sure but for vectors you only have to hope that the data is local. For lists you also have to hope that the cells themselves are close by. That being said, cache misses do get a bit overstated. The only data that I've collected was running one of the valgrind tools on a Haskell program (used a lot of lists) and the number of misses was pretty much zero, though my data set wasn't really that big.
&gt; Backpack can do that. Last I heard, its signature matching can't handle subsumption of polymorphic types; won't this be a bigger practical problem for collections than for strings? Also, is it really at the point where you can seamlessly swap out a `Map` for a `HashMap`? &gt; But if qualified imports are "too awful", mucking about with Backpack is almost certainly more awful. I'm not sure of that, because the mucking around is in a different file, and doesn't impede readability.
Mutable vector?
The latest versions of containers feature a [foldTree](http://hackage.haskell.org/package/containers-0.5.10.1/docs/Data-Tree.html#v:foldTree) for rose trees, which is nice.
See the ListLike package.
The type classes don't exist as standard. You'd need to write them yourself, and they're not straightforward. Say you've handled `Map` and `HashMap` in your type class library, but now you want to treat `Vector`s as `Int` containers; are you sure you wrote your type classes correctly for that? Are you going to correctly handle unboxed and storable vectors too? What if you want a consistent interface between mutable and immutable `Vector`s? In addition to that difficulty, using type classes weakens inference, as others have said.
Are there benchmarks comparing gmp and integer-simple?
Because it's mutable. In Haskell, you should probably look at [Sequence](https://hackage.haskell.org/package/containers-0.5.10.1/docs/Data-Sequence.html).
Nope. You can implement it on top of mutable vector because they support a grow operation (which is just new + copy), but you have to keep track of load/capacity yourself. I mean it's easy to implement, it's just weird that I'd have to do it myself.
&gt; criterion is pretty much unmaintained these days I have opened an issue to address this: https://github.com/bos/criterion/issues/133
Stack saves a local copy the first time you compile.
Does [the report](https://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17700010.2) help at all? --- There's a goofy part of the Haskell grammar (layout in particular) that makes is unexpressable via pure (E)BNF. &gt; Note 5. &gt; The side condition parse-error(t) is to be interpreted as follows: if the tokens generated so far by L together with the next token t represent an invalid prefix of the Haskell grammar, and the tokens generated so far by L followed by the token “}” represent a valid prefix of the Haskell grammar, then parse-error(t) is true. &gt; The test m∕ = 0 checks that an implicitly-added closing brace would match an implicit open brace.
So I'd need a rule that turns `dropWhile` into `dropWhileFB` before inliner stage 1, and turns untouched `dropWhileFB` into `dropWhile` afterwards. So that's why they do it that way.
It's important to be aware that it can be very hard to get accurate benchmarks of lists. The rewrite rules fire very often in trivial code (which benchmarks tend to be), and far less often in nontrivial code.
you can better use `Foldable refligerator` to make your point. `container` is the word that everyone use to abstract a data struture which is instance of `Foldable`. Then why not include this semantic meaning in the signature of some abstraction that will be read by thousands of beginners in the years to come. NOT in your personal project, but in the most used module of Haskell? Because an absurd tradition followed brainlessly based on an outdated restriction on mathematical notation. Mathematicians had to strip semantic meaning in variable names for the sake of efficiency when making manual variable substitution on paper. This is no longer the case, and yet people supposedly intelligent, tradition averse and "disruptive" follow brainlessly that useless tradition.
can you explain what C++ namespaces are, and what they might look like in haskell? 
&gt; is the word that everyone use Always glad to entertain such notions.. I don't know how you define "everyone" but your definition clearly excludes the `base` package authors since they didn't follow "everyone's" *tradition*-uh-I-mean-style. Clearly following "everyone" would be less brainless than (what you *assume but cannot possibly prove* to be) "tradition". Do you have reason to think such "brainless people" won't be eager for your insights, though? Da ole beauty of open-source, [persuade-via-PR-or-fork](https://ghc.haskell.org/trac/ghc/browser/ghc/libraries/base/Data/Foldable.hs) as you see the need!
&gt; go off and write yet another PCRE-shim package for themselves and their fiends, leaving a fragmented regex-package space that continues to be dominated by the solid regex-base family Uh, what "yet another" package? There is one, The Only One, `pcre-light` that *everyone* seems to use, and I made [pcre-heavy](https://github.com/myfreeweb/pcre-heavy) on top of it to add useful functionality / syntactic sugar / etc.
no ableism
[removed]
Sequence indexing time drops on the largest benchmark. Seems even lower than vector. This doesn't seem right.
I'm not saying you can't use it, of course there is room :) But that whole multi backend `regex-base` stuff seemed complicated and unusable when I looked at it. Seems like the library introduced in the article fixes the usability! Too late for me though — one year ago I made `pcre-heavy` because that didn't exist, and I made it on top of `pcre-light` because it was possible for me to figure out how `pcre-light` works.
From [en.cppreference.com](http://en.cppreference.com/w/cpp/language/namespace): &gt; Namespaces provide a method for preventing name conflicts in large projects. &gt; &gt; Symbols declared inside a namespace block are placed in a named scope that prevents them from being mistaken for identically-named symbols in other scopes. C++ has no qualified imports, so all definitions from the included headers are simply dumped into the source file. Namespaces essentially provide an alternative to qualified imports. Syntax-wise I could imagine it to be implemented like this: module Data.Text (...) where ... namespace Text where ... pack :: String -&gt; Text ... For example within the namespace `Text` all functions (like `map`, `filter`, etc.) would refer to the functions from `Data.Text` and its sub-modules, without explicitly hiding the Prelude and related imports. Basically a general qualifier around all relevant identifiers within the namespace. Currently qualified imports aren't quite the same, because they are heavily constrained by the module hierarchy.
That makes sense, thanks.
I think these benchmarks are accurate. /u/eacameron's comment isn't really a good summary. The numbers show that lists are only fast at consing. For replicate (admittedly, an arbitrary function, but we can add more) and indexing they are slower than both Vector and Sequence. 
And the Haskell eco system is far better for those pcre- packages. Many folks, like Evans, just want PCRE and not have to worry about this multi-dimensional constellation of packages. pcre-heavy/light will be just the ticket. of course these packages heavily influenced regex and I hope both continue to feed off each other. Would be nice to see more being done with regex-applicative also. 
That's an interesting article; thanks for the link. (And yes, for something like the GADT `Expr` in the article, using a store/`Delayed` in a way similar to that would require the store extractor to reduce up to the point that GADTs would be unnecessary, which would indeed defeat the point.)
Once again, the base package authors followed the tradition of mathematics and in particular, the inventors of lambda calculus, who performed all equational transformations on paper. I think that this kind of admonitory rethoric is much more efficient to change what I consider wrong in this community. PR's to specific modules do not change cultures. 
It just shadows the already defined `otherwise`. You can use any name you like.
Starting a variable name with `_` means, to GHC, that even if this variable is unused, don't give a warning. This way you can have `_fooVal` as the pattern of the last branch of `case`, or write `f _foo bar 2 _baz` instead of `f _ bar 2 _`
Maybe that is necessary in some cases. Here's one where it was not: https://www.reddit.com/r/haskell/comments/120h6i/why_is_this_simple_text_processing_program_so/
Which? `Data.Sequence.index` has a log factor. I'm actually pretty surprised that it's that fast. I think it has something to do with how GHC manages memory.
 f = \case Just 1 -&gt; False Just 2 -&gt; True neverdothis -&gt; (neverdothis &gt; 4) /= otherwise Just like `neverdothis`, `otherwise` is just a variable. That is, `otherwise` is not a function or a form of special syntax.
It's fixed now.
&gt; Thinking about the syntax tree example, would it be possible to make catamorphisms for gadts? Yes. Though, you have to add another layer of abstraction. ;) GADTs generally aren't uniformly recursive. But, they are sometimes nested / non-uniformly recursive. You can operate on higher order functors (endofunctors on the category of Haskell `Functor`s), and get folding/unfolding based on right/left Kan extensions. I.e. [Initial Algebra Semantics is Enough](https://cs.appstate.edu/~johannp/tlca07-rev.pdf)
Surely that should be a fixable issue? Functions / operators that work on various data structures work fine in other languages.
Of course! Sorry, I just linked the first thing I thought of when I read "HAMT".
 pattern Otherwise :: a pattern Otherwise &lt;- _
&gt; other structures may give better wins still. Such as? I just replaced lists in my code where i used a lot of ++ to add to the end of list with Data.Sequence and got a huge speedup. I wonder now if a different data structure would have been better? 
Actually turned out to be trivial to support. Its fixed in git HEAD.
Oh shit. I knew the case statement was just binding a variable. Had no idea `otherwise` was an actual constant. Very cool
Hamlet has this.
Yeah :P
Boy did I trip myself up on that one. Thanks for straightening me out!
Thanks for the tip. It's always good to remember to take advantage of the compiler!
Documentation is high on their list right now. It's one of the next things they'll be working on. For now, you can use the quickrefs found in the Reflex and Reflex-DOM repos
My usage is very much "if it quacks, it's a duck." I'm not too well versed in what Pastro means theoretically =P I just used it to solve an engineering problem.
I am working (well, haven't touched it for some time) on implementing at pure (GHC) Haskell replacement for GMP https://github.com/erikd/haskell-big-integer-experiment/ . In the process I have done some benchmarking: http://www.mega-nerd.com/haskell-big-integer-experiment/bench-integer-20140920.html Integer-simple is significantly slower than GMP. I didn't benchmark multiplication because it seems to be `O(n^3)` rather that `O(n^2)` or something.
Arrows are so cool. I liked them since HXT appeared on Hackage.
(:
About `(***)`, `(&amp;&amp;&amp;)` and concurrency; if I'm not mistaken, whether concurrency messes with the semantics depends on the semantics of the arrow in the first place. For instance, if we're talking about a reader arrow, it's pretty clear that you evaluating the parallel paths concurrently will have no effect on the semantics. You might also be able to impose laws that make sure that the semantics are unaffected by concurrency. For instance, maybe the execution of your arrow involves network communication, but you might still constrain what kind of side effects are allowed to make sure that concurrency doesn't change the observed behavior.
I liked [Dan Piponi's Data and Codata](http://blog.sigfpe.com/2007/07/data-and-codata.html).
:)
The input file for happy is https://github.com/ghc/ghc/blob/master/compiler/parser/Parser.y, and the lexer is built from https://github.com/ghc/ghc/blob/master/compiler/parser/Lexer.x. 
I was just thinking about a way to talk to Neo4j (a graph database) in a programming language, and get most request as batched as possible, but without having to write them out individually. This might be a great step ahead of using cypher(their query language) and working on the results and then requerying. In contrast to relational stores, you can query relations and paths through your database directly, and it would be too nice to intermix it with some haskell snippets and then get the batching done in the background. background: I'd like to do some analysis on graphs containing a few thousand nodes (~ 10^4 - 10^5) and querying all the nodes at once would be too much, but the logic is too complex to do it all in cypher. This all just started out, and i need to read into their streaming api, but queries still need to be plumbed manually, so arrows might save soem time writing queries in the long run. EDIT: i just worked with profunctor-lenses from purescript. in the begining of the article i wasn't getting much out of it, but traversing and traversal's and that stuff is quite familiar EDIT2: really nice how you explained arrow-notation, i did not know how it works until now
Interesting -- I was circulating source code and arguing for using Tk to build Haskell GUIs in the 90s, and using the same pragmatic arguments to boot (though web interfaces were not an issue then).
Everyone loves `Profunctor`s these days.
starting with encouraging functional Scala :)
The uptick in profunctor posts around here lately is what convinced me to push this, rather than sit on it any longer =P It is weird how that happens though. This whole community seems to low key fixate on one topic every now and then.
What?
I wonder if this kind of approach would lead to something like a set of common signature packages, managed as curated sets. Something like all the different Preludes that have sprung up.
That's fantastic, I was looking for such material these days!! :D Thanks a lot! :3
Great \^_^ Please let me know if there is anything I can improve.
This is outstanding; thank you for sharing. Arrow notation is almost certainly undervalued, so it's good to see it explicitly pulled into profunctor exposition.
Yes, I imagined signature packages being for string-like things (or collection-like things), rather than bytestring-like things. Of course, the great thing about Backpack is that we don't have to choose in advance. Anyone can create a signature for anything and abstract their code over it without needing to coordinate with others or fit things into a hierarchy, and it shouldn't be too difficult to make an a concrete import abstract later on.
As long as you're still asking the question, the answer is "no".
I've modified your code a bit in order to use the more efficient `ByteString` type :) http://lpaste.net/353433
Hi Chris, I would like to cooperate on the 'serialization' comparison. There is already something here: https://github.com/tittoassini/serialization-bench (based on https://github.com/osa1/serialization-bench) It provides basic time/space comparisons for all major serialisation packages.
This link just serves up a blank page :( Anyone know anything about it?
That's awesome. How about moving/copying it into the haskell-perf org?
That might be what ends up happening. But unlike different Preludes, you can use as many signature packages as you want together, as long as they all agree on the names and types of the functions they require one to implement.
This is super neat! I also like how it's basically a template for however complicated of a repl that you want. Thanks a lot!
Thanks, sounds great!
&gt; Really do you believe that they all are complete fools? No, I actually think you're straight up wrong about this, but we'll just have to disagree on that I guess.
This seems like it would not handle Unicode/UTF-8 input very well. Seems like you would have to convert from UTF-8 encoded ByteString to Text or String in order to properly handle Unicode input, in which case you might as well read in Text or String in the first place. (See text package on Hackage). The other option is to simply be oblivious to Unicode and treat everything as raw bytes, but that might not work well in all situations.
Maybe if I can get approval to open source the work and then present the code publicly. We'll see what happens.
Excellent write-up! This is about the same reasoning and research I went through when designing the type system of [Kitten](https://github.com/evincarofautumn/kitten). I type the stack as a chain of product types (`,`), of kind `Stack -&gt; * -&gt; Stack`. All functions are implicitly “stack-polymorphic”, that is, polymorphic with respect to the part of the stack they don’t touch. The type `Int32, Int32 -&gt; Int32`, for example, is sugar for: &lt;S...&gt; (S..., Int32, Int32 -&gt; S..., Int32)` Where `&lt;S...&gt;` is the spelling of `forall s :: Stack.`. This leads to a small extension of Hindley–Milner that I dub *regeneralisation*. For example, the type of `map` is: &lt;A, B&gt; (List&lt;A&gt;, (A -&gt; B) -&gt; List&lt;B&gt;) As above, the function types are implicitly stack-polymorphic, so this is sugar for: &lt;R..., S..., A, B&gt; (R..., List&lt;A&gt;, (S..., A -&gt; S..., B) -&gt; R..., List&lt;B&gt;) But this is not the most general type possible: it restricts the functional argument of `map` to work only on a single stack state within the body of `map`, being whatever `S...` is instantiated to. This isn’t good, because it can lead to non-obvious error messages when a programmer simply calls the same function multiple times—similar to the monomorphism restriction. So we can *regeneralise* this type by shrinking the scope of the `&lt;S...&gt;` quantifier, giving: &lt;R..., A, B&gt; (R..., List&lt;A&gt;, &lt;S...&gt; (S..., A -&gt; S..., B) -&gt; R..., List&lt;B&gt;) This is a slightly more conservative rule than that used in Chris Diggins’ technical report, [Simple Type Inference for Higher-order Stack Languages](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.406&amp;rep=rep1&amp;type=pdf). In addition, Kitten is unique among concatenative languages in that it has an algebraic effect system similar to that of [Koka](https://www.microsoft.com/en-us/research/project/koka/) and [PureScript](http://www.purescript.org/), which turns out to be particularly elegant for a stack language. The kind of the function type constructor, `-&gt;`, is `Stack -&gt; Stack -&gt; Permission -&gt; *`. Kitten refers to effects as “permissions” because it was the most popular term among the people I polled—they indicate which effects a function is *permitted* to use. Built-in effects include `IO` (permission to break referential transparency, implicitly granted to `main`), `Fail` (permission to raise an assertion failure), and `Unsafe` (permission to break memory safety). So in fact the full type of `map` is: &lt;R..., A, B, +P&gt; (R..., List&lt;A&gt;, &lt;S...&gt; (S..., A -&gt; S..., B +P) -&gt; R..., List&lt;B&gt; +P) Without the stack variables, for clarity: &lt;A, B, +P&gt; (List&lt;A&gt;, (A -&gt; B +P) -&gt; List&lt;B&gt; +P) Here `+P` is a permission type variable. This signature says “`map` needs whatever permissions its functional argument needs”, so you can use it with either pure functions (`[1, 2, 3] {(+ 1)} map`) or effectful ones (`[1, 2, 3] { dup print (+ 1) } map`). Unlike Haskell, there’s no need for a distinction between `map` and `mapM`, and no difference in notation between pure and effectful code—to enable side effects, you only need to change the type signature. And permissions combine naturally: if `f` requires `+IO` and `g` requires `+Fail`, then their composition `f g` (or their unification, as in `if (x) { f call } else { g call }`) simply requires the composition of these permissions, `+IO +Fail`. An interesting side effect (ha!) is that because all terms denote functions, there’s no material difference between adding a constraint to a function type, and adding one to the typing context. So I plan to extend this system to represent [coeffects](http://tomasp.net/coeffects/) as well, such as implicit parameters and system capabilities. In fact, I’ve just started to co-author a paper about this. :) 
One thing I'm still not quite clear about.. in private (no other end-users) code-bases, if I "always ensure" all inputs and outputs (files and/or terminal/shell) "are in UTF-8 encoding" (or maybe I allow another encoding to be expressly specified via env-args), could I get away with just `bytestring` and skip `text` entirely?
Here are some equivalent questions to see if you can guess the answer yourself: * If I ensure all image inputs are 24 bit RGB jpegs, can I get away with just `Bytestring` and skip `Image` entirely? * If I ensure all sound inputs are VBR mp3 files, can I get away with just `Bytestring` and skip `Sound` entirely? * If I ensure all movies are 30 fps h.264 encoded, can I get away with just `Bytestring` and skip `Video` entirely? * If I ensure all configuration files are valid XML 1.1, can I get away with just `Bytestring` and skip `XmlTree` entirely? Anything that has to mention UTF-8 is not text. It is a bit pattern that can be interpreted as text. If you think you have too much spare time, of course you can do this interpretation of bit patterns (Bytestrings) yourself, just like you can write your own mp3 decoder. But if you want to be a productive programmer, you'll use the fantastic text library to convert bits to text that someone else has already written for you. Bits and bytes in a file "are" never anything. They're not a movie, they're not an image and they are not text. They can be interpreted into these things, and that's what the Text library does. 
As someone who tried to implement a set of lazy combinators in a strict language (C++), I entirely concur. It is freaking hard to add laziness to a strict language. Once you add concurrency to the mix, you can pretty much give up any idea that you'll get it right *and* make it efficient enough to bother. I'll admit, though, that making something *deeply* strict in Haskell is tough as well, since most of the core data structures are lazy (i.e. tuples, `Maybe`, etc.). But this seems to be worked around easily enough when needed.
Aha thanks! :) I've never seen the 'null' function before but I do agree that your version is just as clear when I know what each of the functions do
While trying to write an example, I actually discovered that there’s a bug in my implementation, haha, so that’ll need to be addressed. But basically, if you keep the HM restriction of not generalising lambda-bound variables, then without regeneralisation, simple code like this won’t work as expected: define twice_add (Int32, (Int32 -&gt; Int32) -&gt; Int32): -&gt; x, f; x f call + x f call In Haskell: twiceAdd :: (Int -&gt; Int) -&gt; Int -&gt; Int twiceAdd f x = f x + f x Because the second call takes place on a different stack than the first. You can work around this by storing the results in variables to get them “out of the way”, but you shouldn’t have to. 
Professor Peterson visited my college once to talk to us about Euterpea and the groundbreaking class at Western that used it. This is truly a tragedy.
Note that Bryan has rather vehemently defended his API decisions in `text` over time, "infelicities" or not, when folks have come in and pushed for him to arbitrarily change them to match `bytestring` or `base`. So you may be in for a fight. ;)
Well, Bryan can thinks what he wants, but if I'm going to get text and bytestring to play nicely under Backpack, I'm going to have to write shim packages to rename functions whenever there is a name conflict in any case :)
Looking into the demo program, `fd-example`, there's mention of a `unix` package, and no equiv for windows. If you look into the package `clipboard` on hackage, it builds with either the `unix` package or `Win32` package (it just allows pulling strings from the clipboard), depending upon the compiling host OS, maybe that's what you are looking for?
If you are indifferent to the character type, `vector` and arbitrary lists also need to be included and at this point I don’t think it makes sense to talk about strings, `Sequence` might be a better term.
Interestingly, there is a distinction. We should think of a string as a sequence which is *not* polymorphic over its elements, e.g., `data Str`; a sequence, on the other hand, IS polymorphic over its elements, e.g., `data Sequence a`. This is pretty important: you don't have a Functor instance for ByteString (ill-kinded!) but any list-like structure should support it!
Even with static linking, don't you just have to provide the .o file and some Gnu source code? How much of a burden could that be? 
I'd like different types for `ByteString` and `LazyByteString` as well as for `Text` and `LazyText`.
Do you have a preferred name for "monomorphic sequence"? I settled on `Str` because (1) it didn't conflict with anything that existed today, and (2) it seemed fairly self explanatory. I'll also remark that using the term "string" this way is pretty common in theoretical computer science, e.g. as in https://en.wikipedia.org/wiki/String_(computer_science)#Formal_theory
Perhaps we should have a signature for lazy strings versus strict ones. But String is soooo awkward &gt;.&gt;
Hm, naming things is hard :) `MonoSeq` and `MonoSequence` are both not taken as far as I can see but I doubt that these names are less confusing to a beginner than `Str`. Maybe `Str` is the best name after all and I just have to live with feeling slightly uneasy about it meaning monomorphic sequence.
Thanks a lot. I'm still a beginner and it helped me :D
I'm trying to get this to build in Nix, and I'm getting the following linker error: Linking dist/build/fltkhs-fluidtohs/fltkhs-fluidtohs ... …/dist/build/libHSfltkhs-0.5.1.4-…-ghc8.0.1.so: undefined reference to `Fl_Timer_New' …/dist/build/libHSfltkhs-0.5.1.4-…-ghc8.0.1.so: undefined reference to `Fl_OverriddenFile_Browser_New' …/dist/build/libHSfltkhs-0.5.1.4-…-ghc8.0.1.so: undefined reference to `Fl_File_Browser_New' …/dist/build/libHSfltkhs-0.5.1.4-…-ghc8.0.1.so: undefined reference to `Fl_OverriddenFile_Browser_New_WithLabel' …/dist/build/libHSfltkhs-0.5.1.4-…-ghc8.0.1.so: undefined reference to `Fl_OverriddenTimer_New' …/dist/build/libHSfltkhs-0.5.1.4-…-ghc8.0.1.so: undefined reference to `Fl_File_Browser_New_WithLabel' collect2: error: ld returned 1 exit status `g++' failed in phase `Linker'. (Exit code: 1) Any idea why that would be happening? Usually in my experience it results from modules not being in `exposed-modules` but that should cause failures on any platform. The Nix expression I'm using is: `haskell.lib.addBuildTools (self.callPackage ./fltkhs.nix {}) [autoconf fltk mesa libjpeg]` where `fltkhs.nix` contains the output of `cabal2nix cabal://fltkhs`.
They are monomorphic, unboxed, and packed. `UnboxedPackedSequence` is a mouthful, though :) `MonoSeq` is short, I'd like that one. 
Well then maybe all the string functions should be removed from `bytestring` that would never make any sense whatsoever for raw-byte-buffers from `bytestring`.. why are they even there --- as it stands, to the innocent observer: `bytestring` covers everything `[Char]` enjoys from Prelude/Data.List, sans the trivial un/lines and un/words, and even puts a few `strip`/`split` cherries on top. My continued ponderation: if the entire system is carefully controlled to only ever have one "encoding", there then obviously *is* no en/de-coding (necessary) Indeed, my main point is that a collection of string characters of length *n* shouldn't occupy much more memory than *n* bytes plus just a few more for whatever "internals" (like address or length or such). `text` isn't a hero in this regard either, but strict `bytestring` used with `utf8-string` is getting somewhere.
The "make backwards incompatible changes to the API" ship sailed a long time ago for both packages, of course, but the intent of the text APIs, where they diverged a bit, was to usefully generalize their bytestring forebears. There was never a goal of drop-in compatibility. I know that now makes things a little more awkward for Ed, so clearly his next project should be to go back in time a decade or thereabouts, and build backpack in 2003 or so :-)
Thanks for trying it out! I don't really know Nix but am happy to troubleshoot with you. I opened an [issue](https://github.com/deech/fltkhs/issues/53) on your behalf. 
It would be nice to have some kind of such string builder to allow efficient text creation: https://hackage.haskell.org/package/stringbuilder-0.5.0/docs/Data-String-Builder.html Like Java has `StringBuilder` class which stores array of chars internally and expand it if needed. Implementing something like this would be really cool! Also it'll be nice to make this _string builder_ work with `Buildable` type class from `text-format`: http://hackage.haskell.org/package/text-format-0.3.1.1/docs/Data-Text-Buildable.html#t:Buildable Using formatting libraries like `text-format` and `formatting` (`formatting` is better actually and `text-format` shouldn't be used) is really nice way to create preformated strings.
What are the minimal requirements for measuring allocation with `weigh`? Should the data be `NFData`?
Yep: https://github.com/fpco/weigh/blob/master/src/Weigh.hs#L126 It needs to be fully evaluated. Obviously, you don't have to, you could return a `()` and that would only measure forced things. But if you want to know how much space a thing takes up, NFData is the way to be sure.
Yes, you do understand monads!
I will look into microlens. That seems like a good idea. Others have mentioned that unique types would be helpful here, if only Haskell supported them. Automatic rekeying isn't specified in the protocol. They are monadic because they can fail. In particular, `readMessage` can fail to decrypt due to an invalid authentication tag, and `writeMessage` can fail due to nonce rollover.
fusion magic is, unfortunately, rather arcane and imo it shouldn't be relied upon; it's a nice feature, but getting the rules to trigger can be rough. instead, I would direct you toward the paper "Lazy Functional State Threads"-- sometimes, mutable algorithms really are the best ones, and Haskell's `ST` provides a very nice way to perform referentially transparent mutation. 
Well, bottoms always poop parties, but I don't think that's the end of the world. If you have some mechanisms like rollback, commit and retry for your side-effects, you should be able to do something sensible that align the concurrent and sequential semantics in case of errors.
Not to mention http://hackage.haskell.org/package/bytestring/docs/Data-ByteString-Builder.html Perhaps OP is referring to a list equivalent of these types, aka the `dlist` package (with its understandably wildly different API).
I'd wager that composition in general needs to be unrecoverable. Since `f . arr id = f`, `f` has to have some opaque wall of unstructured pure functions at both ends (in order to compose `id` onto it), which makes it generally impossible to recover information about what goes in between two computations in a composition.
It takes a string, e.g., compileRegex () "(foo)+". (The first argument is an option type, () requesting the default options.) The compileRegex function isn't part of the generic Text.RE API, but necessarily comes from the relevant backend with the =~ match operators.
i am new to Haskell: &gt;In this expression, we can’t possibly know which username we’d like to fetch until after we’ve already executed getFriends. So it’s not possible for this monad to batch those queries. Libraries like Haxl get away with breaking this law because they promise that their applicative instance is close enough. But it’s not strictly law abiding, and making that promise is dangerous. Plus, it gives the user of the library a false sense that whatever they do will be batched for them. In reality, you have to be careful not to accidentally use a monadic effect where you meant to use an applicative effect. Facebook uses -XApplicativeDo and their own prelude to make this especially rare, but it’s not a catchall. what does this mean and why is it ok?
I think you are confusing space complexity and the amount of allocation. Space complexity of Data.List.sort appears to be linear: https://ro-che.info/img/haskell-sort-space-complexity.png The amount of allocation only affects the run time of the algorithm: *all else being equal*, the algorithm with less allocation will run faster. But the particular Haskell implementation of a particular in-place algorithm that you have in mind may run faster or slower than Data.List.sort, even if it allocates less.
It's trivial, I know, but I'd rename `Bytestring` to `Bytes`. When programmers say "string" they generally mean "text", and that's not at all what an encoding-free flat sequence of bytes is.
About the only way I have ever seen namespaces used (as opposed to stuff in namespaces defined) is by essentially making them unqualified with 'use' in C++. Overall, having programmed both in Haskell and C++ for years I would say the C++ textual import system is infinitely worse than Haskell's module system.
Can you give an example that isn't just single-dispatch on the first argument?
Well, the Haskell report doesn't define allocation either, so I don't see how moving from resident memory to allocation makes your task any easier. For a particular implementation model (STG), it's quite easy to define an "idealized" (but inefficient) GC — one that runs after every reduction step and removes all dead data from the heap.
Sorry for deleting my comment, I felt it was unnecessarily harsh, didn't notice you'd replied. Would such an idealized GC be implementable without affecting time complexity of programs?
&gt; C is pretty high-level for a hardware designer As a rule most high-level language are never referred to as "portable assembly", unlike C.
Pure Haskell is relative. Using the FFI libraries you can write pure Haskell that is more like C than like idiomatic Haskell.
An unevaluated thunk has no memory overhead besides the typical question of space leaks. The indirection and cache locality of thunks is the same as with any pointer based language such as Java.
A word of advice.. if you're new to haskell, please don't spend too much energy investigating advanced posts such as this (especially on concepts like arrows) that have very little relevance to understanding the big picture of haskell. There is already a steep learning curve, and you don't need to make it any harder than it needs to be. Posts like this are advanced/theoretical, not pedagogical/instructional/prescriptive
I have been playing with putting tasty tests in my library code (similar in spirit to doctest). That might help here. You could avoid exposing the guts of your library, just exposing one top level testgroup/spec to your test suite. Alternatively you could have your test suite build the library in the test suite, swapping out one module.
Reminds me of SPJ's lecture to Erlang: SPJ: Here we have the `null` function. In Erlang it's called? Crowd: &lt;nothing&gt; SPJ: IsEmpty? Crowd: &lt;nothing&gt; SPJ: The function that turns an empty list to True? Of course you could pattern match, but you ought to add it to your base. So suppose we had `not . null`, and we wanted to make sure it generated the same code as a hand crafted `notNull`. Do you have this function `not`? Spelt "N-O-T", turns True to False?
Yeah, that all seems right. Since merge sort on lists doesn't involve cycles, I think reference counting might suffice to give it the right time and space complexity. I wonder if there's a GC that can do the same for all programs (both time and space should stay within a constant factor of best possible). It seems like a hard problem though.
I suppose not off the top of my head. But I mean isn't single-dispatch on the first argument more or less what we want. When do we need something much different than that? (setting / retrieving at keys, concatenating, folding etc. all fit under single-dispatch on the first argument).
Don't reinvent the wheel, just use 'vector' and 'vector-algorithms'. If you get a better implementation, fork and pull request please ! Edit: just looked at the source - looks very fast! Manual unrolling and everything.
I am disturbed that strict ByteString and lazy ByteString have the same name . Makes finding compatible libraries harder than it should be . Would be even better (in my mind, for reusability) if the strict version just didn't exist at all and people went to a little extra effort in their C interop (I think this is the biggest use case, libraries that require big contiguous buffers like crypto and compression) Does the strict version do anything you couldn't achieve with a very fancy NFData instance or a 'squash' function or something ? Should packages using strict ByteString instead use something like `array` or `vector` ?
Vectors are appendable with a copy; bytestrings are more efficiently appendable.
Is there any difference between a strict ByteString and an unboxed vector of Word8s? Maybe the splicing is better? In any case, I actually tend to avoid lazy bytestrings. If you're using laziness as a streaming abstraction, you should probably switch to an *actual* streaming abstraction. And otherwise, it can be unnecessarily complicated to reason about the performance of lazy bytestrings.
&gt; Is there any difference between a strict ByteString and an unboxed vector of Word8s? Maybe the splicing is better? Not sure. &gt; In any case, I actually tend to avoid lazy bytestrings. I think the misnomer is that a lazy ByteString is a streaming abstraction -- AFAIK it's not supposed to be. But lazy ByteStrings have very efficient data chunking, hence why the maintainers of `binary` have said on several instances that there's no reason to use strict serialization anymore i.e. `cereal` should die now that the lazy Bytestring `Builder` exists and is used by default for many libraries. As it turns out, `binary` is as fast as `cereal` now for all things and in some operations it's *several times faster* whilst supporting all of the same operations.
Piggybacking on this thread: What _are_ good producers and good consumers? I've never seen any definitions of these terms anywhere.
I always thought the major difference is that Strict doesn't support chunking and hence the append operation is not very efficient if the string ends up larger than the predicted chunk size whereas the lazy equivalent creates a lazy link rather than copying all of the old data.
Right that's why I said lazy bytestrings seem equivalent to lazy lists of strict bytestrings. Each element of the list is a chunk, and the list is lazy in the same way that a lazy bytestring is. At least, that's what I gathered from two minutes of reading that thread =P
[This]( https://jyp.github.io/posts/controlled-fusion.html) seems relevant.
This is fantastic! Good job! I'll be using this for my markdown files. Feedback: - In most apps in the file browser (e.g. the "open" popup) I can type to jump to a file/directory. This doesn't work in mdviewer. - I'm pleasantly surprised to see that I can scroll up/down with `j`/`k`. Not sure if this is done deliberately (vs. inherited from a libarary) but thanks for this :) I also need a rst version of this. How hard would it be to extend the app to render rst too? I'm assuming it wouldn't be too hard because pandoc can already compile rst to HTML, but I'm not sure. If it's not too hard I may give it a try this week.
For what is the book good for? I am a newbie in the ML world.
I understand that, but for generic usages of containers I feel like the type inference situation shouldn't be that bad as you typically are single-dispatching on first argument or perhaps on multiple things in sync (strictly better type inference). Can you give examples of code that you found cleaner without a generic container type due to type inference?
How about just f :: !ByteString -&gt; Parser a versus f :: ByteString -&gt; Parser a Seems to me that strictness/non-strictness is such an integral part of Haskell that we shouldn't have to invent new types to capture this. Rather we should reuse Haskell syntax, and annotate as we would with e.g. data structure fields.
I don't think you understand the problem. The bytestring package exports a `ByteString` type and two implementations; one in `Data.ByteString.Char8` (strict) and one in `Data.ByteString.Lazy.Char8`. The second is a list of the first. 
It's far from perfect but it's evolved into something we're quite happy with at this point. We have a type class Event which defines a few associated types like the type of an aggregate's ID, the aggregate itself and some other things like a type-level number indicating the version and a symbol indicating a type-level name for the aggregate (error messages, DB serialisation etc.). On top of this we build a class for commands which emit events and, separately, an mtl-style MonadEventStore class which provides methods for writing events, loading aggregates, etc. This has a couple of subclasses for back-ends which support snapshotting and projections. In the latter, we have a MonadProjectingEventStore which exposes a number of conduit Sources which stream events from the store based on various conditions. Projections are then written as conduit pipelines. Implementation wise, we currently have a single back-end that uses postgresql-simple. Here we lean on some of the associated type stuff from the Event class mentioned earlier to generate "type-safe" SQL queries for writing and streaming events. That way we can "guarantee" that projections never crash, only ever pattern-match the right event types etc. Serialisation wise we have a big event table using PostgreSQL's jsonb type with a few other columns tracking stuff like sequence number, IDs of who wrote the event, when it was written etc. We're planning to open source it but just haven't gotten around to it yet; hopefully in the not-too-distant future!
Yes, we are, though we've not found the time yet. I've written a few more details in reply to /u/b00thead's comment below.
Yes, but those you can already deterministically release with HOF and there are quite a few libraries available, like e.g. Control.Monad.Resource
FYI, I'm currently working on a new text package which directly use `ByteString` as compact representation and `[Char]` as stream representation. And i don't really think backpack is the way to solve `String` problem because we need **both** compact representation **and** stream representation to get things done and get things done fast. Why another text package you may ask? please read on because i'm really not trying to creating another representation at all ; ) I don't think we need `Text` as another compact representation because `ByteString` already provides efficient slicing and building operations, and `[Char]` is a quite nice stream representation based on `build/foldr` fusion. I just tag `ByteString` with a phantom encoding types(something like `Data.Fixed`) and provide slice operations directly on top of that(with encoding awareness like `utf8-string` package). and provide mapping/folding operations by adding `[Char]` decoding/encoding function(which should play nicely with GHC list fusion, well, at least i'm working on it). This idea is born with my daily work dealing with all kind of network and protocols, reading some data from database and assemble them into a JSON or something like that. I see no point doing decoding/encoding just to get a `Text`, but when i search the hackage to find something like `newtype TextualBytes a = TextualBytes { getBytes :: ByteString }`, surprisingly i found nothing. So i decide to cook my own. Finally, i still want to say there're some misunderstanding about `String`, because list is the most simple way we iterate things in haskell, just like for-loop or iterator in other languages. We should just take extra care because it can be easily eat your memory without fusion.
I am confused about that example. Consider: uncons :: Monad m =&gt; Stream (Of a) m () ⊸ m (Maybe (a, Stream (Of a) m ())) If I understand the paper correctly this does not prevent me from writing: unconsTwice :: Monad m =&gt; Stream (Of a) m () -&gt; m () unconsTwice s = do _ &lt;- uncons s _ &lt;- uncons s return () To quote: &gt; The linear arrow type A ⊸ B guarantees that any function with that type will consume its argument exactly once. However, the type *places no requirement on the caller* of these functions.
It is amazing to see that practical, industrial-level issues encountered by Haskell are being addressed with improvements to GHC of the highest grade. Of course, linear typing has been around for longer than that, but it is an extremely distinct approach compared to what other languages are doing.
But C libraries are linked dynamically by default as far as I know.
What do you mean by special treatment? I just tried and `libgmp.so.10` is clearly linked dynamically # echo 'main = return ()' &gt; Main.hs # ghc --make Main.hs [1 of 1] Compiling Main ( Main.hs, Main.o ) Linking Main ... # ldd ./Main linux-vdso.so.1 =&gt; (0x00007ffff1d97000) libgmp.so.10 =&gt; /usr/lib/x86_64-linux-gnu/libgmp.so.10 (0x00007f0680005000) libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f067fcfc000) librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f067faf3000) libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f067f8ef000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f067f6d2000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f067f308000) /lib64/ld-linux-x86-64.so.2 (0x000056056022e000)
Thanks. So if I have a regular expression entered by a user, it looks as if I should strip out any options and convert them to the option type, and pass the rest as compileRegex's second argument.
You might find the following useful: Philip Wadler's paper on Linear Logic: http://homepages.inf.ed.ac.uk/wadler/papers/lineartaste/lineartaste-revised.pdf and: http://www.pipeline.com/~hbaker1/Use1Var.html and: http://www.pipeline.com/~hbaker1/LinearLisp.html Idris's Uniqueness Types are cool too. They seem to be cousins of full-blown linear types: http://docs.idris-lang.org/en/latest/reference/uniqueness-types.html
If i remember correctly GHC links them statically on Windows. I might be wrong though and I don't have a machine to try it on.
Well it uses [pandoc](https://hackage.haskell.org/package/pandoc) — which can parse reStructuredText — so adding it shouldn't be too hard.
Thank you very much!
Every Paper SPJ has a part in is always so accessible and a pleasure to read... How does he do it?
Haven't read the paper yet, but my concern is that everyone's code uses `(-&gt;)` a lot. Every abstraction that anyone writes is now going to require some very deep thought on which instances of `(-&gt;)` to replace with `(-o)`. `Functor`, for instance. `(-&gt;)` appears three times in the signature of `fmap`. Which ones should be made linear?
Fox requires X11 to install on OSX and it has no GUI builder.
I have forgotten about this! I was using the license generated automatically by stack. I've changed it to GPLv3.
Thanks I missed that one. Sorry. I still think there should be an orthogonal way of declaring strictness.
The constructor behaves differently, newtype's argument is strict and never bottom. It might not be trivial for GHC to infer the need for laziness.
Sure, and the compiler does (under the name of sharing analysis). However, sometimes code is _only_ correct when something is used linearly. In that case a linear type system allows you to express that, at which point the type checker can verify it. 
&gt; This rises the question, why should strictness be hardwired into something that is supposed to introduce types? How would it not be? When you make a data type, you need to know its strictness semantics, because that's important in Haskell. We have to make a variety of behaviors possible in the most consistent way possible. I think the current system achieves that optimally.
I totally understand the need for the two different options, I am however unsure whether I (personally) like that it is done via keywords `data` and `newtype` which seem to me as very poor candidates.
I mean, data constructors are only one example of what may or may not be strict, why make two keywords to distinguish these two and leave other functions uncovered while they just as well may be strict in any of their arguments. Strictness seems to be a more general feat and as such could have been addressed independently of types. As a result one would define a type and the strictness of the constructor separately, in an analogous way to defining strictness of other functions. Not that there is a way for that (or `BangPatterns`?). But to me the current way seems very ad-hoc.
&gt; the notion of writing to and reading from a map from keys to values, so lists (key type is int), dicts, trees, and even sets (value type is either () or Bool depending on your approach). To be honest that seems like a rather ugly interface that would be hard to use in most situations compared to the existing type classes that do abstract over our existing containers. In particular an interface to single linked lists that only allows access via index feels like it would be practically useless since it only allows the operations with the worst performance.
You'd want to port [something like this](https://github.com/vim-syntastic/syntastic/blob/master/syntax_checkers/haskell/hdevtools.vim) to ale. It does work with syntastic though. I've never had hdevtools or ghc-mod work very well with my stack projects unfortunately.
FWIW, there's still a semantic difference between a `newtype` and a strict `data`. Pattern matching on a `newtype` behaves differently.
He also considered himself bad at math so he probably avoid cryptic overly concise mathematical style naturally.
Are you saying that your problems are purely syntactic? Like if we gave `data`/`newtype` better names it would have been fine? (I hope you have understood that there *is* a way to declare fields as strict in data (`data StrictBox a = StrictBox !a`), *and* that single-field `data`, even with strict fields (Such as `StrictBox`), are not equivalent to `newtype`s so both are needed, and cannot be special-cased. For comparison, suppose that in Java, when you call a function: f(u) where public void f(C x) and you modify a field of `k` in f, the semantics becomes: - This will normally change the field in `u` - But not if `C` has only one field, in which case the field in `u` does not change IMHO this is outrageous. Had single-field `data` been given `newtype` semantics, a less visible yet still way too confusing inconsistency would also occur. `data` is the 'normal' behavior. `newtype` should be used only if you are looking to wrap another type, say to distinguish `String` from `[a]` lists when serializing (`newtype JSONStr = JSONStr String`))
Cool! I guess that means this will all be in the upcoming GHC 8.2 release? ;) On a more serious note, is this supposed to make it into GHC eventually? If so, how would it interact with `DependentHaskell`?
How would you like it to be done?
&gt; Are you saying that your problems are purely syntactic? Like if we gave data/newtype better names it would have been fine? No. The bad names are bonus.
Lol so you aren't going to admit you were wrong. Man you really are just panicking and trying to deflect everything into a character attack against me instead of just admitting you fucked up. It's pretty darn hilarious. If it is such a beginner question why did Edward Kmett make a long comment that not once said this was wrong and definitely not for any reason you suggested?
I don't know, I am trying to understand it, I can live with it, yet it somehow feels off. I did not mean to diminish the usefulness of anything that made it to the report on haskell language. But as people realise connection between things and other generalisations I hoped for a either a fundamental reason for this distinction or a confession of imperfect design of the language with maybe pointers to the future. But if this in fact is the best way to do it, I am just curious why that is so.
Are you sure? I think that type for composition of linear functions is incorrect and needs to be (.) :: (b -. c) -&gt; (a -. b) -. (a -. c) (f . g) x = f (g x) which of course does not fit `Category` anymore (see page 1:5 of the paper, or section "Polymorphism" of [my blog post](http://www.edsko.net/2017/01/08/linearity-in-haskell/)). 
How in the world did you come to that conclusion? How is: vector V.! 5 map M.! 8 S.member 67 set Better than vector ! 5 map ! 8 set ! 67 And you do realize I am not proposing removing any existing functions except those strictly improved upon by the type class (so just indexing and modifying by index). I have no idea why you say "compared to existing typeclasses" since this is just one or two additional typeclasses that provide new functionality not already covered by existing typeclssses. 
`data X a = X a` being equivalent to `newtype X a = X a` means you lose the extra `_|_` on the outside. `X _|_` is distinguishable from `_|_`. e.g. The normal `newtype Mu f = Mu (f (Mu f))` doesn't put boxes in between layers, `data Mu f = Mu (f (Mu f))` does, and even if f was strict in its argument, the latter still gives you room to 'pause' and put in bottoms via `Mu _|_`. Both take exactly one argument, both provide semantics you need to be able to express. This actually matters in real code out there. You'd have no way to get those semantics back without either adding another constructor and screwing with totality checking or adding a useless extra field. A more plausible comparison, which doesn't affect the sort of language level denotational semantics interpretation of these things, is to compare `data X a = X !a` with `newtype X a = X a`, but there they have a difference operationally in the heap representation in existing the existing Haskell compiler we know and love. The former is a structure that contains a pointer to your 'a' where as the latter is just 'a' rebranded with no operational overhead. This _also_ actually matters in real code folks have out there, once the behavior of things around `unsafeCoerce` becomes a consideration.
Thank you. This point of view makes it much clearer to me. I will need to find some intuition behind the practicality. I am now convinced about the difference of the two things (for some time now), what I was concerned about is the way the distinction is made (and the way it is lost by the fact that dummies like me don't understand `newtype` completely and the syntax does not expose the difference in any way - I always took newtype ~ lightweight data)
Inferring linearity is widely believed to be an undecidable problem.
I was not suggesting to make data behave observably differently based on number of fields. As soon as someone pointed out the difference in pattern matching I agreed that it needs to be visibly marked. But why is single field a special snowflake and no similar behaviour can be generalised for more complex data, that is not obvious to me.
I was pretty sure that sharing analysis would only provide an approximate solution to the problem.
I haven't read the paper yet, but per that quote, their version of `(⊸)` is about capturing what it is that a function *does* with the value they receive (namely: use it exactly once), rather than saying anything about the received value itself. This makes sense from the perspective of trying to integrate linear and non-linear into the same language. You can view the `A -&gt; B` type as being a synonym for `!A ⊸ B`: which is to say that the argument of type `A` can be freely duplicated. Thus, your code could be more explicitly rendered as: unconsTwice :: Monad m =&gt; !(Stream (Of a) m ()) ⊸ m () unconsTwice s = do _ &lt;- uncons (unbang s) _ &lt;- uncons (unbang s) return () (where `unbang` is the name of the rule that lowers `!A` to `A`.) We can freely coerce a function of type `A ⊸ B` to one of type `A -&gt; B` by simply forgetting the knowledge that it uses its argument exactly once. Thus, (first-order) use of the lolli serves to provide information to the caller, without making any demands. (Higher-order use may make demands; following the usual game theoretic approach to explaining higher-rank quantifiers.)
This question is answered in sec. 2. We paid particular attention to make the new stuff compatible with the old as much as possible.
- `Matrix a` is the type of matrices containing elements of type `a`. - `Mat` is the sole constructor of `Matrix a`. It has a single field of type `((Int, Int), (Int, Int) -&gt; a)`, which is a pair consisting of (a) a pair of `Int`s and (b) a function of type `(Int, Int) -&gt; a`. Hence, every value of type `Matrix a` has the form `Mat ((n, m), f)` for some `n, m :: Int` and `f :: (Int, Int) -&gt; a`. - The `n, m` represent a matrix's bounds, i.e. the number of rows and columns. - The function maps indexes to elements, with the implicit contract being that it is only called for indexes within the matrix's bounds. For example, here is the 2 by 2 identity matrix: id2 :: Matrix Int id2 = Mat ((2, 2), f) where f (0, 0) = 1 f (1, 1) = 1 f _ = 0 If anything is unclear, feel free to ask.
Your operators in your example do radically different things. Checking if 67 is a member of set (an unordered container) is in no way comparable to getting the 5th element of an ordered container or to checking if the key 8 exists in a map. The point of the comparison was that modifying by index is one of the most useless operation to use generically because no two containers even share an index access with similar semantics and complexity. You have to mangle them almost beyond recognition as in your examples to even get something that looks similar. 
I guess they expected something like [this](http://julialang.org/soc/ideas-page.html)...
Monads do not express linear *resource consumption.* You could say they represent a linear ordering, but that's a very different problem.
`f . arr id = f` semantically, it doesn't mean you can't play semantically transparent dirty tricks if you happen to know `f` has no `arr` in it.
I guess. But I'm very wary about doing things that aren't directly law abiding, even when they're close enough. It usually indicates that you're trying to use the wrong abstraction.
Yes using react-flux at work it stands the test of real world requirements. React-blaze I don't know.
I'm using Reflex at work because I believe the abstractions are more composable and allow you to avoid having to manage a mess of callbacks.
Nice article. Note that Edsko's blog link isn't valid in the blog.
Is there a particular reason that this work is better done with a Uniq monad or something of the sort? The -o syntax will be yet another Haskell only syntax that new programmers will have to learn whereas another Monad is very Haskell.
Hmm, perhaps I should put total versions of these functions in the wrappers. Nice suggestion!
Thanks for considering it! I think there's a case for libraries exporting partial functions through an `*.Unsafe.hs` or `*.Scripting.hs` module, but I don't think they should usually be exported through the main library module.
Great comment, thanks! I've always wanted an in-the-trenches comparison between react-flux and reflex. &gt; It's just slower than updating things in-place. Re-rendering everything from scratch has a penalty. React uses a virtual DOM and performs in-memory diffs on that DOM to reduce the number of actual DOM modifications it needs to make. I wouldn't say it re-renders from scratch. However, I've noticed in the past that I have to help the diffs a little bit to get peak performance. &gt; React has been tested by Facebook and there's nothing new in the API that you have to worry about. If you want advice on how to do something there's a wealth of docs and blogs out there for you. I think this was meant to be a pro for React, not Reflex :) &gt; It mixes view/model/controller up. I know I said this was an advantage above, and it is, but you aren't forced or encouraged to separate them cleanly, so people will not do it, at least not in the same consistent way as in React style. So true. In a personal project, I tried to manually enforce the differences between these things, and it worked out okay. I bet making a large project with multiple developers in reflex will absolutely require some decision making here. I did do a project where I went with a React-style architecture in Reflex though. All of my global app state was passed into all top-level views via an event. Any sort of state local to a component wouldn't have to be exposed to this global state. This was a really nice tradeoff. &gt; It's a bit new, the haddocks API is mostly undocumented, and you might hit implementation bugs. Yep, you pretty much have to work off of the latest git versions AFAIK. &gt; So in summary, React and Reflex have issues and they both kind of solve each other's problems. I don't see a hybrid approach yet, so you might have to just assess the pros/cons and pick one, or use both, as I'm considering. If I had to start another project using GHCJS, I think I would try to formalize my React-style architecture in Reflex better. It really felt like the way to go.
I'm not sure if I should give the total versions the best names or not though ;) Do you have a preferred naming convention for "safe head"?
&gt; Your operators in your example do radically different things. For `Map`, `[]`, `Vector`, `HashMap`, `Seq`, I would argue they do very very similar things. For `Set` and similar I can see your point but still disagree. Morally a list / vector is a map from integers to values, and plenty of languages (see: JS, Python) use the same notation for accessing both. I think it is perfectly reasonable to do so. The main laws you want for indices are obeyed as long as you don't index out of bounds, and indexing out of bounds is always an annoying aspect of arrays anyway: insert m k v ! k = v insert m k (m ! k) = m I still think `set ! k` notation is pretty convenient, and it actually obeys the above law perfectly fine, in fact it obeys it even better than lists / arrays do. A set can be thought of as two things depending on the situation in which you are using it, either as an unordered container of values, or as a map from keys to just existence or nonexistence with no additional information (which you can also think of as a `Bool` or as a `Maybe ()`). For additional evidence of this being an accepted thing look at [Control.Lens.At](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-At.html). It defined `Set` the way I described. They did limit it to non-arrays so that the above laws (and probably some more) always held, which I could understand, in which case we should have a less constrained version for arrays, since it's less constrained perhaps a superclass would work.
That looks good so far. If you're filling a matrix with a given value "a," that means, for any index you give it, it's always going to give you back "a" (modulo out-of-bounds indices). I hope that gives you enough of a hint :)
The Aristocrats!
&gt; It mixes view/model/controller up. I know I said this was an advantage above, and it is, but you aren't forced or encouraged to separate them cleanly, so people will not do it, at least not in the same consistent way as in React style. Yea this can be a real problem in large code bases. You have to be somewhat disciplined with your architecture. I've found that my favorite way to handle things is to sprinkle little Elm architectures throughout. Basically, application state is passed in as a `Dynamic`, and UI/IO `Event`s are returned to upstream. At the root of whatever scope you find appropriate, you consume all of these events in order to correctly update the dynamic state that you're passing to all of them. This way, a widget doesn't have to concern itself with any of the corner cases and gotchas that arise from having multiple widgets interacting with each other; that concern is given to the upstream root that's managing the state. I'm not a fan of enforcing this style such that there is one root at the root of your application, as Elm does. But using small-scoped instances of this architecture in a moderate number of places is really nice
You may also consider Axiom: https://github.com/transient-haskell/ghcjs-hplay https://github.com/transient-haskell/transient/wiki/Transient-tutorial#web-programming-hplayground 
Does this enable mutable structures with immutable APIs?
I think they mean `LinearTypes`, although I'm still reeling a bit at the thought. In the other thread, I joked about this being in 8.2. But I didn't think it would arrive (if ever) before at least 8.6...
i got this: head :: Str -&gt; Maybe Chr unsafeHead :: Str -&gt; Chr ;-) 
.. I hope the complexity of this doesn't kill Haskell for the beginner. Not a fan of the syntax noise of `-.` used in this thread, I think `-o` at least looks a little better as it is symmetrical.
I think its important to underline that this doesn't let us write any code we couldn't today with any performance we couldn't today -- what it does is let us take certain invariants in API's that are maintained implicitly or by convention and force them into the type system. So I appreciate the comparison to region stuff a great deal -- the paper observes that many of the same use-cases can be handled by a lightweight typed-region system, but that its rather painful to do so. The discussion below about how the level on which this works is to e.g. enforce `withStream` takes a function that doesn't double-uncons is an example of this. I'd like to think that just this tech alone will let us write much more sophisticated stuff (doing its own memory management) in a genuinely safe way, and the examples suggest this, but I still am not so sure of how much this will suffice until I see it on larger scale use-cases. And even before we hit the RTS, I think the exploration of what compile-time transformations are possible on known-linear code will be another arena in which we may see some payout as a next step.
I have been messing around with react-flux and I haven't really had to use callbacks at all, can you give some examples? I had to use them a lot in reactjs, but with react-flux I just use lots of store actions, which seem way cleaner.
Can you give some concrete data types and examples? It's a bit unclear what you want still.
Yes, but IIRC, it's a performance antipattern, same as appending lists.
This article is 4 years old and reposted every couple months.
Do you mean it is submitted to the /r/haskell subreddit monthly? Do you know how I could find those previous posts? I'd be curious to read some comments and discussion around the topic from haskellers. I've tried looking at here: https://www.reddit.com/submit?url=http%3A%2F%2Ffsharpforfunandprofit.com%2Fposts%2Frecipe-part2%2F which only points to this post in /r/haskell.
Higher rank types and higher kinded types are not the same thing.
[Something seems fishy there.](https://www.google.com/search?q=site%3Areddit.com%20Railway%20oriented%20programming&amp;*&amp;rct=j) Google's results are much more in harmony with my memory of this than Reddit's. I don't even use F# and I've definitely seen this article multiple times before.
Here is a counter-point: https://www.google.com/search?q=site%3Awww.reddit.com%2Fr%2Fhaskell%2F+railway+oriented+programming I only see one submission dated back 6 months, plus this one.
Mutable structures with functional APIs? Yes.
That's the idea.
I am a bit late, but could you elaborate on abstract interpretation vs unification? What are the obvious deficiencies you are talking about? Are there any positives also (except easy implementation)? 
Everithing looks fantastic with lineal types and the syntax is minimally intrusive. The perspective of alleviating the GC load, increase safety, reduce latency and increase performance is almost unbelievable but the theory is simple and the promises are quite realistic. a ⊸ b Don't know it, but the lollypop operator is a major contribution to the enrollment of women in Haskell programming. All these sharp operators look very unpleasant for many women.
Even before venturing into reading the post, I would like to say a massive "Thank You" to Tweag and its team (really a bunch of excellent &amp; smart people!) for doing this. I think them embarking into this endeavour is not something one should give for granted, as the reality taught us how companies have been there before and decided to steer away from the problem by simply adopting a different technology (a choice totally justified when pressure builds up or Haskell is simply not the right tool for the job). If they would have decided to do this as well, it would have been a huge loss for the Haskell community, and I'm so glad that such talented people are pushing the boundaries of what's possible to with Haskell do up-to-date. Regardless of which kind of financial support they might be getting (if any!), I think trying to make ends meet by writing Haskell AND at the same time exploring novel ideas and features for a big compiler like GHC deserves nothing but sheer respect. _Chapeau, folks_.
Terrific post, thank you!
Will it provide unsafe operations? unsafeArr :: (a -&gt; b) -&gt; (a -. b)
The benefit of `-.` is that it's a valid operator name. `-o` would have to be special-cased which I'm not psyched about given the hassle around `*`
Using `Data`, from `Data.Data` you can use `toConstr` and `constrIndex`: Prelude Data.Data&gt; constrIndex (toConstr (Left "hello" :: Either String Bool)) 1 Prelude Data.Data&gt; constrIndex (toConstr (Right False :: Either String Bool)) 2 `Data` gives you enough information about the type to be able to get out the constructor count. The `Data` instance only exists if you ask for it with `DeriveDataTypeable` and a `deriving (Typeable, Data)` clause, though. You can similarly concoct a way to read out the constructors using `Generic`, but you need to remember to `DeriveGeneric` and add a `deriving Generic` clause. This has the benefit that you can set up the little typeclasses you need to handle types with functions in them. These is the sane ways to do this. Doing better requires evil knowledge of heap representations and exploiting dynamic pointer tagging, you'd need to ensure the result is forced then use an approach similar to [`unsafeGetTagBits`](http://hackage.haskell.org/package/tag-bits-0.1.1.2/docs/src/Data-TagBits.html#unsafeGetTagBits) modified to skip past indirections and look directly at the info table pointer if the dynamic pointer tagging bits aren't set, and to handle profiling, etc. In general there be dragons, so you're best off with the `Data` solution unless `Foo` contains functions, or a `Generic` solution, or writing a manual one-off conversion yourself.
This might be useless, and if so I apologise, but _if_ you're used to more conventional imperative languages you might define some structure like (C++ish pseudocode): struct Matrix&lt;A&gt; { int n, m; A elems[n][m]; }; A Get( Matrix&lt;A&gt; mat, int i, int j ) { ... } Where the `Get` does some obvious lookup into the array of `A`s within the `Matrix`. If you were doing a lot of work with identity matrices you could even make a special helper version of `Get` for some approriate numeric type `A`: A GetIdent( int i, int j ) { if( i == j ) { return A(1); } else { return A(0); } } You can absolutely define a data structure somewhat like this in Haskell too: data Matrix a = Mat Int Int [[a]] get :: Matrix a -&gt; Int -&gt; Int -&gt; a get (Matrix n m xss) = ... getIdent :: Num a =&gt; Int -&gt; Int -&gt; a getIdent i j | i == j = 1 | otherwise = 0 But really, `get` and `getIdent` are so similar it seems weird that we have to define them like this, it breaks code reuse horribly, and nested lists are a terrible choice of data structure to store our `a`s. We could make `Matrix` polymorphic over the data type, but then we have to deal with making `get` work for a variety of indexed lookup methods. Really, it would be much easier if we could just stick `get` (or `getIdent`) into the `Matrix` itself - functions are first-class citizens after all, we can pass them around and store them as we like. We can then do our simple things for `getIdent`, or close over our nested lists or vectors or whatever... data Matrix a = Mat Int Int (Int -&gt; Int -&gt; a) -- getIdent is so simple we can inline it as a lambda matId2 :: Num a =&gt; Matrix a matId2 = Mat 2 2 (\i j -&gt; if i == j then 1 else 0) -- the most unsafe matrix ever, will explode horribly, don't do this. -- but note we can close over the list in the accessor function, -- without mentioning [[a]] in the data constructor for Matrix explodyMatFromList :: [[a]] -&gt; Matrix a explodyMatFromList xss = Mat n m (\i j -&gt; (xss !! i) !! j) where n = length xss m = length (head xss) At this point, our `data Matrix` isn't really holding much of anything, just two `Int`s and a function. We can turn it into a tuple `(Int, Int, Int -&gt; Int -&gt; a)`, which is isomorphic to `((Int, Int), Int -&gt; Int -&gt; a)`, and then uncurry the function bit to end up with `((Int, Int), (Int, Int) -&gt; a)`. Wrap it in a newtype for a bit of safety and you end up with the original version, where `Mat` takes the above tuple of a pair of ints and function and gives you a `Matrix a`. Long story short, if you build the `Matrix` by supplying a custom 'accessor' function instead of simple data, you can do some interesting things. 
Please correct my understanding if wrong: 1. Linear Types come in two flavors: One where every argument has to be used exactly once, and the other, where every argument may be used an arbitrary number of times. 2. They introduce the arrow `-o` to impose the "use exactly once"-restriction on the left-hand type. 3. The arrow `-&gt;` does not change semantically, it imposes no type-level restriction on the arity of the left-hand type. Basically that is an opt-in of the more restricitive linear type by using the arrow `-o` instead of `-&gt;`. Compared to the approaches I have seen so far, that seems pretty elegant to me. 
I think you intended to say *prove* ? Nothing to show sounds a little rude :D
Do you mean like this: https://github.com/nomeata/ghc-proofs ?
You can have `ghcid` perform tests if there are no errors with `--test=exp`. Combine this with `quickCheckAll` and you can define properties that are automatically checked on every file change. For example: succ = (+2) prop_succ = \x -&gt; succ x == x + 1 return [] runtests = $quickCheckAll with the command `ghcid --test=runtests` 
I like it
How does this affect the upcoming Compact region feature in GHC? Should one choose this in favor of that then (and what would be the complementary use for that)?
Correct me if I'm not right. Couldn't the compiler could infer `-o`? So even if we declare a function `a -&gt; b`, the compiler could infer that its type is `a -o b` and so automatically take advantage of the memory management this provide. If I understood correctly `a -o b` simply means that `a` appear only once in the body of the function. It looks rather inferable to me.
I love the analogy!
This kind of development is the reason why I'm learning Haskell. Are there any resources, for programmers: (a) an introduction to linear logic and (b) examples of problems solved by applying linear logic with Haskell?
Infering linearity is an undecidable problem in presence of higher-order (or recursive) code.
You can say `write :: Int -&gt; a -&gt; Array a -o Array a` and you need to arrange the array to be unique when creating it. eg: `alloc :: (Array a -o k) -o Bang k`
How does react do that?
Wouldn't the first signature only prevent the array from being used more than once inside of the `write` function? I don't really get the `alloc` function.
Yep, this is another example where the library can only guarantee soundness if you promise to only use the function once. Being able to express and enforce that at the type level would be great.
It will be a part of the next version of GHC (8.2.1) * Description from [trac](https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-8.2.1#Back-endandruntimesystem1) &gt; This runtime system feature allows a referentially "closed" set of heap objects to be collected into a "compact region", allowing cheaper garbage collection, heap-object sharing between processes, and the possibility of inexpensive serialization. * [`Data.Compact`](https://hackage.haskell.org/package/compact-0.1.0.1/docs/Data-Compact.html) at Hackage * The paper [Efficient Communication and Collection with Compact Normal Forms](https://www.cs.indiana.edu/~adkulkar/papers/cnf.pdf) ^^[**PDF**]
Thanks for the link - I'm wondering how it fares against linearity when 8.4 comes out. It looks to me linearity is a cleaner approach, so I'm not sure what place that leaves Data.Compact in. Any clues for relative pros and cons between the two?
That was my first thought too - there's lots of fun to be had, esp if this starts affecting GC. So far it seems like the runtime representation of both is the same, so unsafeCoerce would do it, I think.
It's not at all production ready, but [phantheck](https://github.com/pseudonom/phantheck) is an experiment in that direction using GHC's typechecker plugin capabilities.
Thanks for the great paper! I'm in awe :) One question though: The paper has mentioned multiple times of the below pattern that helps to keep objects in the *linear* heap: alloc :: (T -o IO ()) -o IO () free :: T -o () But how can one actually start to implement these functions? Say I try a naive approach like alloc :: (Queue a -o IO a) -o IO a alloc f = f Queue.empty free :: Queue a -o () free q = ...? What I don't understand: 1. `Queue.empty` by its own allocates a queue on the *dynamic* heap; from what I understand in the paper that means in my alloc implementation the application of `f` would just operate on the dynamic heap in this case - right? (so my implementation would not work) 2. I can't think of ways to implement free - deep evaluate it? (obviously doesn't make sense) So are these two functions supposed to be implemented via FFI? It would be nice if there is some native way to declare 'I want this in linear heap'. Also would be nice if there is a `copy` function that goes from dynamic heap to the linear heap instead (in the paper the copy works the other way).
I've implemented this in C# at work in the past. It was really good for fall-through logic; i.e. does user have X1? else x1 error message; does user have X2? etc... It did however turn C# code from object orientated into functional code with a lot of Func&lt;T1, T2&gt; so I could pass around the functions. It feels more like a shiny feature in C# but is more akin to normal code in haskell. I'm not convinced it really offers anything within the haskell world. I think Darwin is right in likening it to Maybe.
Just use Unicode. It takes a few seconds to set up a shortcut to type it in one's editor of choice. There are only so many combinations of ASCII characters that don't look like operator soup.
I'd heard of `doctest` a few times so I made a [cabalized example](https://github.com/TomMD/doctest-simple-example) for my own exploration.
You should really add a README.md file to the root folder that giving a quick blurb of what the code is for. 
Nice. Thanks, I've always just had mountains of code everywhere and used make for compiling my and others projects. Is there a good resource for learning about what cabal does that's better? Thanks
There is something quite similar called [Canonical JSON](http://wiki.laptop.org/go/Canonical_JSON) for which an implementation can be found in [`hackage-security:Text.JSON.Canonical`](http://hackage.haskell.org/package/hackage-security-0.5.2.2/docs/Text-JSON-Canonical.html)
Cool! For comparison, here is the smallest `package.yaml` file for [hpack](https://github.com/sol/hpack) that accomplishes the same thing: dependencies: base library: exposed-modules: Experiments executables: experiments: main: Main.hs dependencies: experiments 
As mentioned, compact regions will be available GHC 8.2; linear types most definitely will not. And if the implementation proceeds as described in the paper, the first cut of linear types will not have any RTS support, so it will work well for managing memory that doesn't look like the Haskell heap, but if you want it to "look" like Haskell data structures, compact will better. But let's consider the end game, where you can declare Haskell values, use them linearly, and updates can be done in place (ala Cogent http://people.mpi-inf.mpg.de/~crizkall/Publications/cogent.pdf ). Compact regions still seem like an orthogonal mechanism: data inside a compact region can be referenced as many times as you want, and still get GC'd, it just gets GC'd as a whole. You can use references to data in a compact region non-linearly, which make things easier for certain applications. Still, it's interesting to think about, and may become clearer when people actually start writing programs with linear types in Haskell.
Awesome! Source? Can someone confirm?
So are you saying that if I'm able to take my existing Haskell program, and split it into small functions that only consume their arguments exactly once -- and annotate that using the lollipop thing -- then I'll get guaranteed amazing performance? 
&gt; I'm not sure what you mean by using bracket around wait/signal. I meant that wrote a little `withQSem` wrapper using `bracket` that acquires a semaphore, runs an IO action and releases it even an exception is thrown. Thanks for the link to Simon Marlow's fix on QSem -- it looks like `SafeSemaphore` is not really needed now? In the mean time I've been doing some more checking and I now not sure that QSem are causing this particular bug; I'll do some extra experiments. BTW, the application is used for automatically running tests on students programming assigments. The code in question is the `evaluateWith`function on https://github.com/pbv/codex/blob/testing/src/Codex/Evaluate.hs; this uses a quantity semaphore to limit the evaluation of too many simultaneous requests. 
Can you define and or quantify the scale and scope of what is meant by 'large', 'bug', and 'possible'?
What does large mean? In this context I would say that a fitting definition would be along the lines of: *too big to take all possible inputs/states into consideration*.
Great question. Definitely not, the goal is to pull in as little complication from Unicode as possible. There's an [issue open](https://github.com/seagreen/Son/issues/1) to discuss exactly how to order keys.
Canonical JSON was one of the things I researched before starting this project! As is noted in the hackage-security code it's not actually always valid JSON, so it wasn't an option for what I wanted. If anyone finds more subset-of-JSON projects definitely open an issue here: https://housejeffries.com/page/7
Thanks! -- fixed.
At the very least, you have cosmic radiation to contend with. It's physically impossible to shield an electronic device from all possible means of interference. Doing so would require going beyond the edge of the universe. When one bit flips due to this, it's possible for you to get a bug. I'd be very surprised if you could build a mechanism for detecting and correcting this that wouldn't also be affected by the same vulnerability. Therefore I'd say it's probably impossible to have zero bugs. More pragmatically though, I'd say there are so many points of failure in a modern application (hardware, kernel, OS, syscalls, libraries, layers and layers of libraries) that it's statistically infeasible that none of these things has any bugs that *will* affect your application. So I'd say that even if my previous paragraph is incorrect, the chances of someone writing software with failure rate of 0 are so low that I wouldn't expect it to happen within the lifetime of our planet. All that said, I don't really see what this has to do with Haskell.
`package.yml`files always seemed super cool and frankly better than cabal files, but I wonder how accepted they are within the community. * Is it a bad idea to use `hpack` for a library package? * Should I configure it to run `hpack` each time I modify the `package.yaml` to generate the cabal file? * Will stack prefer `package.yaml` files or cabal files? Sorry if you don't know the answers to these questions but I'd very much like to be freed of `exposed-modules` and `other-modules`
An idea I've recently heard is that since the vast majority of JSON is well-formed, JSON parsers should be optimized for speed in the presence of well-formed JSON. To that end, if 95% of JSON comes in a form that is twice as fast to parse if you take advantage of it, then the cost of backing up and more robustly parsing the remaining 5% is much more negligible than the benefits. It's like branch prediction. Point being, if I start emitting Son in all my applications and start taking the aforementioned approach to parse Son much faster, then the more external services that follow suit, the faster all these services will become. The advantage of this over just switching to faster protocol is that at no point did anyone break compatibility with JSON, so no one ever cause a compatibility disaster.
&gt; At the very least, you have cosmic radiation to contend with. I think it's reasonable not to consider a failure caused by cosmic radiation as a bug in your software! &gt; All that said, I don't really see what this has to do with Haskell. It's not really, I was just curious about the opinions of Haskell devs since Haskell already eliminates many classes of bugs that most developers have to contend with (for example, it seems likely to me that a JavaScript dev is less convinced bug-free software is possible ;-))
Ah, I see -- makes sense. Another way to manage the number of simultaneous evaluations is just to use a thread pool of a fixed size. I've also just used a Chan of `()` as a "poor man's counting semaphore" in the past. Take a unit when you start the job, replace it when you finish (of course guarded by bracket).
That's a way bugs can exist but does that mean bug-free is impossible? Is it not possible for none of those things to happen? Also - bugs can be fixed, there's nothing in the question about having to write it bug-free first time around; just about whether a large bug-free application can exist.
Or you can also just add an executable and no library section (after removing the dependency on the library in this case).
Cool, I'll probably switch over to `package.yml` files for future projects then. The `exposed-modules` system is frankly archaic in cabal files, it's time there was some way to automatically populate that for humans. 
I think that bug free software must logically exist(for any definition of bug free that is worth talking about), and therefore it can be written. The problem is knowing when some software you have is bug free at some point in time. This can further be broken down into two parts: 1) Knowing what you want the software to do and 2) knowing that the software does a specific thing (in particular what you want it to do). Formal verification solves the second one and really lets you know that your software does a specific thing to the extent that humans can really know anything. It comes at the cost of a lot of extra work though. 1) seems like a harder problem. To me it seems like the only way to solve it is to say that if a program satisfies its specification then it is by definition bug free, and any problem that might occur we call something else (specification error?)
we could perhaps ask /u/rahulmutt ;)
&gt; The exposed-modules system is frankly archaic in cabal files As is the fact that you explicitly have to name any dependency your executables or tests use that your library already uses. Same with default-extensions. And then there are the benefits of it being YAML - easy to parse in any language, a markup I already know, anchors and references...
If we can show that there can be a small bug free application, we can make any number of small additions to the application without introducing bugs and we say that a small application + a bunch of small additions becomes a large application then we basically have a proof by induction that a large application can be bug free. I think the first part is pretty clear, the second part is a bit tricky and the third part is a question of how we define "large application". e: Could be that some large applications can be implemented bug free while other can't.
Mistakes implies that devs missed something. But a lot of times there's simply nothing to miss. Requirements just change over time and an expected behavior becomes a "bug". It is impossible to build bug-free system for a very simple reason. We always work in the context of incomplete information. Therefore anything we build will always be incomplete in regards to functionality. 
... which would seem to suggest that if these were somehow marked in the type system, the compiler could do some incredible optimizations.
&gt; I'd be very surprised if you could build a mechanism for detecting and correcting this that wouldn't also be affected by the same vulnerability Do you think [ECC RAM](https://en.wikipedia.org/wiki/ECC_memory#Problem_background) is still affected?
Thanks
Given a long enough timeline I suppose anything is possible, however, quite unlikely? If you have a "large code base" then it's probably growing even larger with new features and fixes every week, at least in today's world. Maybe a good example you are looking for would be something like NASA command module C code? Where a mistake literally costs lives, and it needs to be checked and rechecked several times over before ever going live.
That's because, However being a really dumb article, the graphics are nice and the colors are appealing. 
You made a sexist assertion about how _operators_, of all things, are the things pushing women away from Haskell.
The term bug predates software: https://en.wikipedia.org/wiki/Software_bug#Etymology
What i said is based on my experience. Ask some friend women about it, instead of repeating boring ideological slogans. In case you have any.
Sorry; been a while since I logged in. The heavy reliance on type synonyms is generally unfortunate. The partial `nodeRange` is unfortunate. `match` and `matchAny` are weird; a modern Haskeller would probably expect them to produce `Maybe (Context a b, g a b)`, but instead they produce `(Maybe (Context a b), g a b)`.
I got overexcited. Android N and higher introduce some extra complications. But it's good to know we're working on it.
From my perspective, bug-free software, while technically possible in a theoretical sense, is absolutely impossible in the real world. As a codebase grows, the likelihood of it containing one or more bugs approaches 1. And in my opinion, this approach is very quick. I highly doubt there are many, if any at all, codebases of even ten thousand lines in the entire history of the world that are bug-free. At a million lines, forget about it. I guarantee the codebase has bugs, and probably a lot of them. So basically, there is no real absolute barrier to bug-free software. There isn't an amount of lines or any other physical metric where you can conclude with 100% certainty you have a bug. But it's so absurdly unlikely to not have bugs in nontrivial codebases that it should be considered impossible for any practical purpose.
Possible but not practical
Looks very nice. How similar is this to existing Haskell solutions? Is there anything implementing the Rx interface with the same nomenclature in Haskell?
Yeah, that's​ precisely my point. 'Bugs' in software don't come from some mysterious external force. They're mistakes, made by people.
Yes? Performing post mortem analysis to identify why and where mistakes got made is still important. My point is twofold- One, it's counter productive to say "this wasn't the Devs fault, the BA should've done a better job" but then go and name pure dev mistakes after some mysterious external force. Two, whether or not it's "your fault" that you didn't understand something is a question that you should answer only after you understand why you made the mistake - which should be a more complicated question than "was it in the requirements."
&gt; So are these two functions supposed to be implemented via FFI? I think that's right. Or something built into the RTS. But in terms of usage there shouldn't be a difference.
What a tragic waste of time.
It is just the Haskell Either type and either monad. But Microsoft people like to give different names like "worflow" and "computational expressions" for monads. Lots of "new" things in F# comes from Haskell. It is not a surprise since many Haskell core developers are MS research that is where F# comes from.
F# uses Module.bind instead of the symbol because it doesn't have type classes like Hakell which allows this symbol to be used to all monads implementations and also allows to write genric functions that manipulates monads and works with all monad implementations, but the access to the .NET api compensates this lack of flexibility.
you seem emotional about this topic
And a maze of twisty little "whitepapers", all alike, too. Here's hoping that Prof Wadler will bring some sanity to this field. Though pedophilia as a "phenomenon" is not a blockchain invention, but, tragically, much older.
Any error correcting code can only deal with a finite fraction of wrong bits over a given word : https://en.wikipedia.org/wiki/Forward_error_correction
There is this thing called Smart Contracts. It's like the new javascript for finance. It turns out there is a chance something like Haskell could become the industry standard. That's pretty good for everybody 
He's probably making bank off some Haskell bot he wrote.
There's a paper called "Out of the Tar Pit" that makes the argument that flaws in software often come from unnecessary complexity and that one way to limit this is to adopt functional programming practices, so it's not unusual to connect the two.
My guess is that the people, who claim it's impossible to write but-free software, are the ones who write relatively buggy software. Mistakes happen, sure, but claiming that the absence of bugs is an impossibility sounds more like someone wanting to protect their ego rather than capture a truth. Mistakes happen, and they always will. The only question is how long it takes for you to discover that you've made a mistake. The longer it takes, the more damage it does. Discovering a serious mistake five seconds after you type it in makes the mistake completely irrelevant. Discovering a mistake years after you make it, and it can be fatal to an entire company. 
Could you explain why that code works? I haven't done quickcheck before. Why is there the dollar sign. What does the return [] mean. Does quickcheck test all function that return Bool (_ -&gt; Bool) ? 
Please excuse my ignorance, but how hard can it be a hard problem to count how many times a variable occurs in a function body? 
The archaic bit is the UX, its needless bookkeeping from the developer when the vast majority of usecases would be solved by hiding `Internal.*`, even just adding glob support to cabal would go a long way. Having the granularity to specify each exposed module, in a centralized manner? Great. Imposing that all the time when 99.99% of usecases fall in some generic patterns? Not so much. As a side note: when I first started with Haskell I repeatedly hit issues about linking missing symbols... which traced back to not properly exposing modules, not that the error messages explained that.
That looks really cool. Although it isn't definitely suitable for test driven development, for few exemplary tests it is ideal and it even improves the documentation. Why isn't this feature already built into the language? 
Bitcoin has a number of flaws that are likely to sink it sooner or later - but it might be quite a ride before it gets there. The exchange of vast amounts of energy for bitcoin's security is one such flaw, and there have been innovations in how we look at trust models, and in what trust models are possible. But smart contracts are a non-starter, you cannot specify in precise detail every aspect of a contract without rendering it opaque even to experts, and there's uncertainty over wiggle room to assert that the intent of the contract and its behaviour have diverged - the contracts aren't actually smart, and can't reason about the intent of the parties involved. Still, one might be able to make a pile of money out of the idea before it's either supplanted by something workable but as yet unforeseen, or given up on.
Isn't it a problem that it tries to gimmick REPL? For example output of 2 and putStrLn "2" look completely the same in ghci. 
After the DAO debacle, I assume people realised the importance of carefully restricting the expressivity of the contract language. Perhaps Wadler's work with linear types is also relevant in that space. 
Thoughts, comments, and questions. First, typos: * Page 5: "To consume a pair exactly once, evaluate it and consume each of its components excatly once." (s/excatly/exactly/) * Page 14 (maybe): "This is not a problem in and on itself" (s/on/of/) * Page 15 (maybe): shared variable rule missing an equal sign * Page 16 (maybe) "by definition, in IO0 xw, x *must*" looks to be missing a space between "x" and "w" Questions: * The biggest thing I want to know is what types are inferred for `const x y = x` and `map f = \case {[] -&gt; []; x:xs -&gt; f x : map f xs}` * It's mentioned that linearity and laziness don't interfere; is it possible to force a linear value with `seq`? My intuition would be no, but there should be some alternative. * It seems that many use cases in the paper could benefit from a linear state monad; is there anything really stopping it? * How would I selectively add explicit non-linearity, like allowing a particular code segment to `drop`, possibly only certain types? * Speaking of explicit non-linearity, are we going to `unsafeDup` and `unsafeDrop` * Can we anticipate a nicer interface to mutable vectors and conversions between the `ST` API and the linear one?
SNARKs would help solve this problem partially
What are your assumptions? Let's say, for the sake of argument, that you're using formal verification methods and have proved your code correct, for some definition of correct. What's that definition? It starts with the assumption that the specification is correct. This pushes errors outwards, but it doesn't remove their possibility altogether. The generally higher level of abstraction employed at the specification level both makes for easier review and less chance of errors hiding in detail, but it doesn't eliminate the possibility. It continues with some assumptions you must eventually make about the compute platform on which you run: do you assume the CPU faithfully executes your opcodes? There are certainly contexts in which development teams _don't_ assume faithful hardware. Cosmic radiation effects are much more common outside an ionising atmosphere, so if you're operating there, you're probably designing in a way which handles a potentially high rate of random bit error. Down on earth, wifi networking software contends with a higher rate of bit error than spacecraft will see from cosmic radiation, and the source of that bit error doesn't really matter for the techniques to deal with it. The reason we have errors is one of cost, ultimately.
Because it depends on what functions you're calling, which may call your function back, etc.
&gt; We could, for instance, begin with cleaning up our language by no longer calling a bug a bug but by calling it an error. It is much more honest because it squarely puts the blame where it belongs, viz. with the programmer who made the error. The animistic metaphor of the bug that maliciously sneaked in while the programmer was not looking is intellectually dishonest as it disguises that the error is the programmer's own creation. The nice thing of this simple change of vocabulary is that it has such a profound effect: while, before, a program with only one bug used to be "almost correct", afterwards a program with an error is just "wrong" (because in error). - [E.W. Dijkstra](http://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html) Changelog: ## v0.1a - Fixed error in attribution - TODO: write a regression test for this
Yes.
&gt; however, quite unlikely And that's the question! Quite unlikely (Even, very very very very unlikely) is possible though, right?
I'm not sure I see how? The SNARK says nothing about the correspondence of the knowledge to an accurate representation of the state of the world. It only proves that you know something without revealing what that is. For a SNARK to represent an argument about the state of the world it would need to be something that would computationally infeasible to simulate, i.e. you actually need access to the system to perform the SNARK. Maybe the time evolution of a chaotic process? Not sure how you would form a SNARK from it though? But you're supposed to be able to construct a SNARK for any NP problem.
If you are concerned about the difference, ask to "show" or "print" the result and use that instead to distinguish. In practice, it has never been an issue for me. You do have types on hand, after all.