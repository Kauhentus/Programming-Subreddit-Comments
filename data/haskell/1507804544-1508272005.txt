`type Fix f = Cofree Void`, `type Fix f = Free f ()` like Edward Kmett pointed out somewhere on the internet.
Depends on what class of languages you're considering. My first though when someone asks for least crufty language would be Python, but that's of course a different category than Haskell.
Check out OCaml and F#
It is quite capable right now. The whole compiler and tools fit in a Docker image of less than 500 Mb and it is quite fast at compiling/linking (if etlas is used for all the process). There is a complete integration with other JVM languages and accept most if not all of Hackage. There are examples for using haskell FRP libraries as well as Apache Spark. forkIO as is now, executes in Java thread that are heavy, but soon it will have lightweight threads. There are ambitious plans for creating his own solution for distributed computing aimed at solving the same problems that Akka and Spark tackles. I think that it is in the right place an the right mentality at the right time for having success.
Ocaml and its sibling language, ReasonML. 
I write Python for a living. It's a very useful and powerful language with lots to love. But no way it's _less_ crufty than Haskell.
I've often wondered if it would be useful to have something like greet = \case Left -&gt; \error -&gt; "I don't know who you are because " ++ error Right -&gt; \name -&gt; "Hello " ++ name 
Troll post!
I think the constructor names in the definition of `CoAttr` are switched. The text and the following functions describe `Continue` as pure and `Stop` as layered.
So much salt, man.
Rust programmers laugh at your puny disdain for string types.
&gt; There are only three string types That’s two more than every other language out there. Plus there’s a _lot_ of code in the wild using ByteString for unchecked UTF8 handling (eg Cassava CSV parser) &gt; Just use the normal prelude This does become rather nightmarish when you’re constantly going between Text and String to name just one case. Plus the Haskell prelude has many unsafe holes that even Java lacks ( eg `head` throws rather than returning a Maybe/Optional) &gt; Hackage and cabal work quite well for me. If they’d worked equally well for everyone else, no one would have have needed to invent Stack. &gt; Make sure you're not using tabs and learn the [indentation rules. (see for example 2.7)](https://www.haskell.org/onlinereport/haskell2010/haskellch2.html) Haskell’s error reporting is quite vague, and does get line numbers wrong. Rust and Elm do much much better. OP’s criticisms of Haskell do have merit. The answer is there isn’t any good answer. Purescript did clean things up quite a bit, however OP presumably doesn’t want to generate JavaScript. Your suggestion of F# is probably best. 
For the spin benchmark, if GHC did fold it into a constant, that would be bad (for the benchmark at least) because then we wouldn't be measuring anything anymore. Also, your implication that hyperthreading may help more when the data set is larger than cache seems to be correct. With eight million machine words, I get: 1 Capability: 821.0 ms 8 Capabilities: 185.1 ms Which is a 4.43x speedup.
Rust really only has two string types.
Ah yes. String and &amp;str. OsString, OsStr, CString, CStr, PathBuf, Path, Vec &lt;u8&gt;, and &amp;[u8] are not strings.
Yes, it's possible, but it's going to be messy. Not because of the guards, but because you are using `n` multiple times, so if you're not going to use a name to indicate where all the uses are, you're going to have to use a lot of plumbing to get the three copies of `n` where you want them. Here's the most readable version I managed to write: import Data.Bool import Data.Tuple import Control.Arrow -- | -- prop&gt; f' x == f x f' :: Int -&gt; Int f' = ((id &amp;&amp;&amp; id) &amp;&amp;&amp; id) -- make three copies of `n` &gt;&gt;&gt; (first . first) (`div` 2) -- the first copy is `div n 2` &gt;&gt;&gt; (first . second) (3 *) -- the second copy is `3 * n + 1` &gt;&gt;&gt; (first . second) (+ 1) &gt;&gt;&gt; second (`mod` 2) -- the third copy is `mod n 2 == 0` &gt;&gt;&gt; second (== 0) &gt;&gt;&gt; first swap -- bool takes the False branch first &gt;&gt;&gt; (uncurry . uncurry) bool -- ((a, a), Bool) -&gt; a Alternatively, you could compose all the functions which get applied to each copy and use that instead of each `id`: -- | -- prop&gt; f'' x == f x f'' :: Int -&gt; Int f'' = (uncurry . uncurry) bool . (&amp;&amp;&amp;) ((&amp;&amp;&amp;) ((+ 1) . (3 *)) -- False branch (`div` 2)) -- True branch ((== 0) . (`mod` 2)) -- condition 
Isn't that basically just the lambda case extension?
You made me choke on my tea. To be fair though, a couple of those are just bytestrings.
To be fair, the list representation is more of an equivalent to `Iterator&lt;Char&gt;` which a lot of standard libraries have. It is just an extremely silly default representation. I'd also agree that lazy strings are the wrong data structure most of the time, though. `List&lt;T&gt;::get` can throw IndexOutOfBoundsException so i am 
I was wondering if the event coordinators had a contingency plan for what to do if he started starting about Scala's implicits. Like "oh no, not this again, cut the microphone and bring out the backup keynote speaker"
How are the lightweight threads going to be implemented? Via changes to the Eta runtime/jvm bytecode patching/something else?
Scala?
You forgot `Vec&lt;char&gt;` :p
Type class methods will be specialized at all types they are called at while their unfolding is in scope. INLINEABLE is enough to guarantee this. Manually specializing is only really worthwhile to avoid recompilation. As in, you use a type class to work for various Num types but never call it with Int in the library. If the library user uses it with Int it might be recompiled everytime that module changes, with manual specializing it will only be compiled once with the rest of the library.
The `\case` bit is `LambdaCase`. The `Right -&gt; \name -&gt; "Hello " ++ name` bit doesn't exist but seems like it might be cool.
Wait why would `sortWord16` have awful performance when `sort` has an `INLINABLE` pragma? This is one of the things that makes the possibility of link time optimization really attractive. If GHC saved all optimization passes for link time so it could do whole world optimization, it could eliminate duplicate specialization, reduce the *total* work required, leaving time to do more work specializing more functions. The problem is getting *incremental* work, but I think it might be possible to cache optimizations so that "incremental" actually just means "figuring out that we should use this cached state and go from there," sort of nix-style.
While you're wrong that PureScript is frontend-only (you can run it on Node, and I believe there's even a C++ backend), you're still effectively right: PureScript doesn't have anywhere near the level of support that Haskell has outside the frontend. It's lacking in libraries, and in particular in optimization. Haskell has a plethora of backend libraries and other things, and it's got a massive amount more optimization and performance-centric libraries.
&gt; Two types too many. I would say "one type too many." The difference between `Text` and `ByteString` is fairly important. I wish people would stop considering `ByteString` to be a `String` variant, because it's meant for very different purposes. &gt; Except that the Haskell Prelude suffers from the problem that it's mostly useless for anything but example code What? I have literally never used an alternative prelude, and have never noticed a serious problem. The fact that people consider the existence of alternative preludes a problem blows my mind. `Prelude` is not perfect, or even "great," but it's never gotten in the way, and the solution to literally any prelude problem is an `import` statement, which is super easy.
GHC will specialize type class functions if their unfolding is in scope. The unfolding is basically the original unoptimized code which allows more rewrite rules and optimizations to fire when inlined. We have the unfolding available: - in the module where the code is defined - in other modules if the code is marked INLINE or INLINEABLE Code is automatically marked inline or inlineable if it is small enough. If you want it specialized mark it inlineable. Manual specializing can be useful for libraries that use essentially closed type classes, for instance to overload a function for a couple Num types. If the functions aren't called within the library they would be specialized in the module where they are called and recompiled with that module. With manual specialization they would only be compiled with the library and when inlined.
I have no idea why it happens. It just does. It would be really neat if duplicate specialization could be eliminated though.
From a 10,000ft view, that sounds like a GHC bug. I can think of no reason why adding an `INLINABLE` pragma would ever *prevent* a function from being specialized.
I've tried several functional languages - multiple MLs, Haskell, Scala, Rust if that counts. IMO, the "tight", user-friendly options are Rust, F#, and Coq. They all have great tooling, great compiler errors, great documentation. On the other end, Scala and Haskell are "organic" in the worst way. OCaml is only slightly better - the build system story is awful. I have not tried Idris, Elm, Swift, Elixir, PureScript, etc.
I work in Scala. I hate Scala. I hate SBT, I hate the whole shebang. It's awful. Kotlin is not functional.
I'm going to assume that the OP isn't looking for nonstrict languages with monadic IO, otherwise he's pretty much stuck with Haskell.
Yeah I used promises or whatever and they were a pain in the ass. The whole thing seemed like a step backwards.
He said, standing in the porch dressed all in black.
&gt; There are only three string types That's five, String, ByteString, ByteString.Lazy, Text, Text.Lazy.
The problems of Prelude are more about what is there that shouldn't (like map restricted to lists) than what isn't there and could be solved with an import. That said, it's indeed not a huge problem. Irritating, maybe, but of little practical relevance. I just wish I could import a module hierarchy, because some times naming things is a problem.
Hrmm. Technically it takes a function, but it's not introducing any meaningful, extensible functionality because you still have to directly pattern match on the constructor. So it looks more point free, but unlike actually being point free, it's not part of a chain of composition, and so gets you little in terms of easier maintenance / authorship. bimap is better - I sure wish we could introduce something like a dynamic bi,tri,quadramap based on the number of constructors in the input Functor, but I don't see a way to do that without crazy boilerplate.
That’s why you need the runtime system. It’s not that different from C or Haskell. Who calls main?
Completely agree. Fwiw, even those problems can be “solved” with an import statement via `import Prelude hiding (...)`. This is of course really dumb =P
`bimap` is not sufficient because it doesn't allow you to escape the constructor. In any case, I don't see much advantage to either f g over \case { Left -&gt; f; Right -&gt; g } and I actually think the latter is clearer.
ByteString isn't a string type, like he said.
&gt; C programmer in me says, three types too many That may be the case. However how would you represent UTF-8 encoded strings and how would transmit said strings to a remote system in a safe way? 
An UTF-8 encoded string is a sequence of bytes. To transfer it, you send length and buffer. Sender and receiver both verify the absence of illegal code points. Though, in portable code you shouldn't assume Unicode. Instead, write locale-agnostic code that doesn't change a strings encoding unless absolutely necessary. 
Most of your issues sound to me like growing pains with Haskell. You may also want to use "stack" for your projects. 
https://stackoverflow.com/a/43203104/165806
Given what you've written, you effectively have 2 types: a sequence of sequences of bytes to represent UTF-8 and a simple sequence for transmission. &gt; An UTF-8 encoded string is a sequence of bytes. To transfer it, you send length and buffer Is this enough to recover the original encoding of the above buffer and length? Is it safe? 
&gt; I wish people would stop considering ByteString to be a String variant, because it's meant for very different purposes. Yes, it really ought to not have "String" in its name!
Performance-wise, is there any significant difference between strict `ByteString` and unboxed `Vector Word8`? If not, it seems like the only reason we even need `ByteString` is because of its string-like API, which might as well exist on `Vector`.
Try [Racket](https://racket-lang.org/). Also by cruft I assume you mean historical baggage. It's very unlikely you'll find a language + compiler + environment that brings all the advantages of GHC + Hackage without giving something up. So the answer to your question is what are you willing to give up? Semantics, laziness, performance (in class), types?
 &gt; doesn't allow you to escape the constructor. I don't understand what you mean by this. λ&gt;bimap (+2) ("This"++) $ Right " Works" Right "This Works" λ&gt;bimap (+2) ("This"++) $ Left 6 Left 8 Unless you mean in the definition of Either's Bifunctor instance? Or is there some other concept you're expressing that I'm not picking up on? 
There is no recoding going on and the length is explicitly transmitted. What problem do you imagine exists?
&gt; Rust is nearly as functional as you could get in the systems space. Actually, ATS has some pretty interesting ideas in that domain :) But Rust is absolutely the only "normal" systems programming language with functional paradigms. &gt;I'm going to assume that the OP isn't looking for nonstrict languages with monadic IO, otherwise he's pretty much stuck with Haskell. Monadic IO is often bested by what Elm/Idris/PureScript actually.
And no, I do not have two types. Where is the second type? The first type is a length and a buffer of bytes of whoch we believe that the contain UTF-8 encoded text.
A strict `ByteString` is a `ForeignPtr Word8` https://hackage.haskell.org/package/bytestring-0.10.8.2/docs/src/Data.ByteString.Internal.html#ByteString. I think being a `ForeignPtr` means it's pinned (never moved by the garbage collector). An unboxed `Vector Word8` is a `P.Vector Word8`, according to https://hackage.haskell.org/package/vector-0.12.0.1/docs/src/Data.Vector.Unboxed.Base.html#Vector. A `P.Vector` is one of these https://hackage.haskell.org/package/vector-0.12.0.1/docs/src/Data.Vector.Primitive.html#Vector and the underlying type is `ByteArray`, which wraps the primitive type `ByteArray#` https://hackage.haskell.org/package/primitive-0.6.1.0/docs/Data-Primitive-ByteArray.html. I have no idea whether how significant these differences are.
I mean that `bimap` always maps a `Right` to a `Right` and a `Left` to a `Left` so it's not as general as pattern matching.
Hm. It'd be interesting to find a good set of benchmarks to compare them. ByteStrings definitely are pinned, and I have absolutely no idea what the advantages and costs of this are.
One advantage is you can share it with other languages.
Plus, there's [Purerl](https://github.com/purerl/purescript), which is "a PureScript backend targetting Erlang source".
&gt; Where is the second type? A length and a buffer of bytes which we do not believe contain UTF-8 encoded text, presumably.
That is nice. This could be accomplished by wrapping the vector in a `ForeignPtr`, right? Though I guess that adds a level of indirection.
I knew I should have read the whole article before posting a comment :o
I don't know much about `ForeignPtr` but can you just "wrap" a Haskell value in one? It's not obvious from the API that you can or should be able to do that. http://hackage.haskell.org/package/base-4.10.0.0/docs/Foreign-ForeignPtr.html
I get the feeling people complaining about how many string types there are in Haskell aren't making much use of newtypes to create even more - for safety.
[I love Racket](https://www.reddit.com/r/haskell/comments/70b9j7/haskellers_do_you_still_code_in_other_programming/dn29iyb/), but Haskell is *way* less crufty than Racket, largely because Racket usually tries to maintain complete backwards compatibility. I doubt the OP will prefer Racket to Haskell based on the given criteria.
If you're using strings much, you really should have a lot *more* than three in your program. A `Password` is not a `Username` is not a `Filepath` is not a `URL`. Different things, different types.
Ooooh, ok, I see what you're saying. Yeah, I agree, an arbitrary way to pull a term out return it sans context probably is not a feature we really need - far too easy to accidentally break something. Explicitly declaring which `f` is probably better in the case of `(f n) -&gt; (n-&gt;x)-&gt; x`
ByteString processing is *much* faster than Text, to the extent that it's worth having a representation mismatch for performance and usability.
That's the first time I've heard Coq and "user-friendly" used together.
If you like Haskell, you just might **love** Idris. /r/Idris 
And for grapheme clusters Vec &lt;&amp;str&gt; and don't forget std::borrow::Cow!
I considered F# initially, but it does not seem to catch on. Seems like a bad investment of fine to me. 
https://github.com/haskell/cabal/issues/879
It was just a suggestion proferred with about as much thought as OP gave to their post. I clarify in the 2nd para that OP probably wasn't going to find what they were looking for. Sorry I was wrong on the Internet.
The creator of Scala hates Scala?
I haven't looked at it in awhile. It lacks some syntax extensions, which makes it incompatible with the Haskell ecosystem. As a project, it looks really cool.
CoqIDE is dope though, ProofGeneral too.
Haskell 2020 has stalled, and when a discussion about that stall was started, that discussion stalled. One member complained that the process of communicating on GitHub was "too heavyweight", which is fine, but I don't expect a committee member who cannot even find time or energy to communicate on GitHub to be of much help when it comes to actually writing a new standard. In fairness, I'm not doing anything to help either. I don't expect Haskell 2020 to happen. See: https://github.com/haskell/rfcs/issues/15
UTF-8 is the standard way to encode Unicode text as a stream of bytes. What else should I use?
So what's the difference in these types beyond their names?
&gt; "This code never errors, has no undefineds, and never throws from pure code" And is guaranteed to terminate. Which cannot be done for a turing-complete language like haskell.
Sorry for the slow response! Here are a few photos of the inside pages: https://imgur.com/a/J81RW The book is actually really good quality, both in terms of design and printing – definitely on par with any published paperback book, if not better! It doesn't look like it would fall apart soon either :) Thank you all for the nice comments and the bizarre number of upvotes (5th place on /r/haskell/top???) which of course belong to Bartosz, /u/hmemcpy, /u/codermikael and all the other contributors who made this book possible! It really shows what the community can do together (and how much we all appreciate Bartosz Milewski's work!) :)
let's see: * `True` and `False` are numbers, i.e. you can add them. * empty lists evaluate to false, and you are actually discouraged to write something more clear like `len(l) == 0`. * lambdas are restricted to single expressions, statements are not allowed. Makes writing HOFs much less appealing. Not saying that haskell is perfect by any means, but it's more consistent than python.
You might want to look into Eta as well. It's really close to being a port of GHC to the JVM. This crucially means it supports most of Hackage packages, which Frege doesn't.
IMO it is, conceptually, for the application developer. E.g. when reading in a file, createing JSON and outputting it... you deal with String, ByteString, and Text.
I think the "plethora" is part of the problem - too many options, and hard to tell which is well maintained, recommended, etc.
I'm kind of confused as to why anyone even wants to compile Haskell to the jvm. Can someone give me some use cases besides integrating with legacy code? Like cross compiling GHC is a pain but it's not so bad you need a whole new compiler.
Can you talk about why you hate Scala? On paper, it seems to have everything. Maybe it has too much?
&gt; Not saying that haskell is perfect by any means, but it's more consistent than python. Your post seems to equate "cruft" with weird semantics. The poster didn't actually list anything about semantics when using "cruft", he instead listed problems with the standard library, the package ecosystem, the plethora of non-standard string types, and poor error messages. I don't think your post really addresses any of these concerns.
From a bucket of bytes with a length how would the receiving end know it's UTF-8?
How does it compare to haskell itself? 
If you make a divisive post like this which requires significant effort from people responding to you, and then you don't bother to respond to anyone, you get a downvote from me. Don't be a bad OP.
My point was that’s a **much** better problem to have than not having options at all.
The receiving end knows because all of that is part of a protocol. If the receiving end doesn't know what the protocol is, you have far worse problems than finding out what the text encoding is. If you have a sufficiently generic protocol, you of course need to state somewhere that this message contains UTF-8 text, but as that's needed for every kind of datum and the way you phrased the question seemed to imply that both ends already know that they want to transfer UTF-8 encoded text, I didn't think there was any need to specify this any further.
Well, SimSpace is [looking for a frontend engineer](https://angel.co/simspace/jobs/210564-software-engineer-frontend) for our React frontend, and since we're using Haskell for the backend, we've been giving Haskell classes to other interested non-Haskell employees. So that would be one way! (We are also [looking for backend engineers](https://angel.co/simspace/jobs/64261-software-engineer-backend) for our Haskell backend. Boston or remote, but if remote, Canada and US only)
I am not entirely settled on semantic vs structural newtypes. That is, I strongly feel that Base64 and Base64Url should be newtypes around ByteString because the data has different structure. Same for validated vs unvalidated inputs. But should Name and Password be type aliases or newtypes? Type signatures stay readable either way and passing a struct is more readable than 5 newtypes. Still adds safety, though, and Control.Lens.Wrapped helps with the annoyance. Not sure where I fall on this.
oh, you think the `String` is your ally? You merely adopted the `String`. I was born in it, molded by it. -- Rust, probably.
Track Eta &amp; Purescript. I have a feeling they'll solve all these Haskell warts much faster.
So your logic is that if someone uses a type, then that type is a string?
It is a port of GHC running and producing code for the JVM machine...
The answer is "Yes." [The answer is always "Yes."](https://stackoverflow.com/q/13184294) But that doesn't mean you should do it. 
I heard in the gitter chat that it uses haskell itself: it uses a continuation monad to implement threading.
Oh nice. I didn't read that much about it since I'm at the bar. 😂
Many would consider `lens` to be an alternative prelude
Sounds like a great company, but you say "we are open to remote arrangements in rare special cases", which makes me think if my first name was Simon (😉) you'd find a way to let me work remote, but otherwise I'd have to live in Boston.
Awesome, thanks for the photos! Looks like they didn't print the images in color though... Still, it's good to know lulu makes a good quality print!
For this problem, your protocol can well defined by types. You can have Text for UTF-8, a triple (ByteString block, length of block, UTF-8 encoding). So all that your protocol has to encode is the triple. So, having 2 types of Strings can greatly help in thinking of the problem even transmission.
The kind of cruft listed by /u/kuribas is by far much worse than the "cruft" referred to by the original poster.
My protocols usually do not have a type system so they can be implemented in different programming languages.
I've started watching [this talk about LiquidHaskell](https://skillsmatter.com/skillscasts/10690-keynote-scrap-your-bounds-checks-with-liquid-haskell), and I find it interesting so far. Thanks for posting.
I'm not entirely sure if I would want to implement a protocol this way. Allowing arbitrary types to be described inside the protocol and sent sounds (a) very slow and (b) like a recipe for security problems. Note that in this example, your representation as a tuple actually doesn't work at all since the buffer length is needed to find out where the text ends and data after the text starts. I really don't understand what kind of advantage you expect from this sort of flexibility. Sounds fancy but kinda useless.
&gt; I'm kind of confused as to why anyone even wants to compile Haskell to the jvm. * JVM has a "battle-tested" GC and JIT. * Operations has a lot of experience and tooling for monitoring, tuning, and scaling JVM (particularly JavaEE) applications. * Incremental transition, allowing developer flexibility on the short term. It's not really "legacy" if it's a Java library I wrote 2 weeks ago, and it's better to be able to use that same library from Scala, Frege, and Eta than to be *forced* to rewrite it 3 times. And, it's not just in-house libraries; I get to use any/all of maven central e.g. JVM and CLR are both fine platforms comparable to native machine code. 
Interop isn't as good as some alternatives last time I checked. For example, if you "plug in" to a framework by extending a certain class, you have to have some bridge code written in a different JVM language in addition to any Frege code you write. I think Eta is *probably* a better choice if you want something Haskell-like on the JVM. Of course, if you've already got a lot of Frege experience, it's a different story.
It may not explicitly have a type system, but it's implicit--or do you never make a distinction between strings and integers in your protocols? You're free to write code as you see fit but there are real benefits to thinking in types even at the crude level of C.
&gt; It may not explicitly have a type system, but it's implicit--or do you never make a distinction between strings and integers in your protocols? Yes, I do. However, I don't see how this is relevant to the question. As marshalling code is always needed, the representation of data in a network protocol needs to have no relationship to the types used inside the program.
Do you have an example of some bridge code? Is it difficult or just tedious?
Impressed at how fast these went up
* SBT is hot garbage. "Simple Build Tool", my ass. * Compile-times are atrocious. * Defining a sum type is maybe five times more verbose than in Haskell. * There are too many ways to do any given thing. Tell me, when should I use a class versus a case class versus an abstract class versus a trait? * `Enumeration` blows. No branch checking in pattern matches. * Inexhaustive destructuring does not throw a warning, and instead fails at runtime. etc. For more: [Warts of the Scala Programming Language](http://www.lihaoyi.com/post/WartsoftheScalaProgrammingLanguage.html)
I no longer believe I understand what we're discussing.
&gt; &gt; Compile errors due to formatting but point somewhere else &gt; Make sure you're not using tabs and learn the indentation rules. (see for example 2.7) To be fair, on occasion parse errors are really really bad: * If you accidentally leave out the `do`, and there's a bind, it can tend to treat surprisingly large block of code as a pattern and throw an error about not being able to parse the pattern, it will complain about this at the beginning of the block, not at the pattern. This is because GHC parses patterns as expressions, and then converts them to patterns. The problem would still exist even if the conversion wasn't there, but it wouldn't happily parse in so much code as a "pattern" * Not really a parsing issue, but.. If you ever dedent a line to 0, and it happens to be a valid standalone expression, when TemplateHaskell is on, it will be interpreted as a TH splice. So the code before and after will be considered separately for checking. Often this causes a huge number of scope errors if the code before depends on the code after.
&gt; I'm kind of confused as to why anyone even wants to compile Haskell to the jvm. Say, for example, I have an existing good large app and I want to create a new plugin. As long as it runs in the JVM I could use any language. Using Haskell would be nice in this situation.
 &gt; If they’d worked equally well for everyone else, no one would have have needed to invent Stack. You cut off an important part of what I said, which was: &gt; I'm not sure what you're specifically struggling with. My point, and I should have been more explicit, is that just stating "Hackage" as a problem is not useful information. What about hackage is a problem for OP?
* F# being mutually intelligible with both OCaml and C# means that you're really learning three languages for the price of one. * It's also incredibly easy to learn. It's a lot like Python in that it exhibits pay-as-you-go complexity; you barely need to learn anything to get started, and the learning curve is smooth. * Though it is only available for Windows users, Visual Studio Community is an absolutely fantastic IDE; its F# support is unparalleled by anything in the functional world (though IntelliJ IDEA Scala support is starting to come close.) And *nix users get to use JetBrains' excellent Rider IDE. * Through Xamarin and Fable you have production-grade tools for targeting x-plat, mobile, and web. (Xamarin is notoriously fiddly, but it still beats writing your apps in Java and Swift.) F# is a fantastic language. Five years ago, it was (IMHO) by far the most productive functional language on the market. But Microsoft's been pulling out, F# jobs have been disappearing, and other functional languages have been picking up the slack (see e.g. Stack and Intero). So today it'd be an odd pick if you're looking for work. But for personal and hobby use, or if you're choosing your team's work language, it's an absolutely excellent pick.
I'm also thinking about using it but haven't gotten to it. I see people mentioning Eta. Eta seems to me like something allowing a Haskell programmer "target" the JVM. While Frege seems like a way for a Java programmer to jump into Haskell. So, Frege is more like Clojure, which I also like. What I didn't like about Frege, reading only some small tutorials, superficially, is that they don't provide "wrappers" for Java libraries. You're supposed to do yourself the Java - Haskell state monad mapping while it's clear to me this should be centralized somewhere. But it's probably not such a big problem.
Is there an easy way to specify "simple" subtypes as: data MyType = A | B | C data MySubtype = A | B ? In particular I'd like to define propositions and literals as: data Prop = Const Bool | Var Char | Not Prop | Or Prop Prop | And Prop Prop | Imply Prop Prop data Lit = Const Bool | Var Char | Not (Var Char) Searching the net I found that I probably could use GADTs for that, but maybe there's an easier way.
I think some people were using it to make a game engine for Android
I'd rather take a language with consistent semantics over a language which has a convenient libraries, but a flawed core language. Nor do I think the points mentioned by the OP are really inconveniences. It may be something that's surprises a beginner, but having both streaming and random access versions of unicode characters (Text) or binary bytes (Bytestring) is very useful. While a string as a list wasn't the best idea in hindsight, it is easy to teach. The extensions give us the great libraries such as vector and lens, and you don't need to be an expert in extensions to use these. And these extensions come in handy when you run into the limits of haskell (multiparameter typeclasses), or provide a cleaner way that's not supported by core haskell (i.e. view patterns, pattern synonyms, ...). The benefit of Python is that it is simple and easy to learn, but that's also its limitation. And limiting the language by design (lambda's cannot execute statements) makes it even less appealing for writing complex programs.
Neither do I. Very confusing.
&gt; I didn't see the string until I was a man. -- C programmers
Don't we all hate most languages?
Hmm, I didn't write that job ad, I should have read what it said before posting it! We do prefer local candidates, especially for less experienced developers since it's easier to mentor in person, but remote arrangements definitely aren't as rare as that text makes it sound. I count 6 devs out of 8 (4 out of 6 if we only count the Haskell devs) who are currently remote, and none of us are named Simon!
No. Not difficult, just tedious. It also means you'll never be dealing with just a single-language build infrastructure, which is not necessarily bad, but can have dark corners that a single-language build doesn't.
 I'm hoping to see some renewed interest once type providers are supported in .net core. The giraffe web framework adds some nice functional sugar over ASP.net core. 
Now I know why I asked... I confused Eta for Frege. https://github.com/Frege/frege/blob/master/README.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Frege/frege/.../**README.md** (master → 031b87c)](https://github.com/Frege/frege/blob/031b87cd7c2190a1ac77b13606f6c4dd31f46135/README.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply doaafiv.)^.
I know I do.
You can generalize your `CaseNum'` type like this newtype Case (c :: Type -&gt; Constraint) (xs :: [Type]) = Case (forall x. c x =&gt; x -&gt; x) For some inspiration, you might like vinyl's `AllConstrained`, used in [`rpureConstrained`](https://hackage.haskell.org/package/vinyl-0.6.0/docs/Data-Vinyl-Core.html#v:rpureConstrained): rpureConstrained :: forall c f proxy ts. (AllConstrained c ts, RecApplicative ts) =&gt; proxy c -&gt; (forall a. c a =&gt; f a) -&gt; Rec f ts or one-liner's, uh, [everything in there](https://hackage.haskell.org/package/one-liner-0.9.1/docs/Generics-OneLiner.html): combinators which take various kinds of polymorphic functions (such as `forall x. Num x =&gt; x -&gt; x`) and applies them to all fields of a record, using `Constraints t c` to mean that all fields of `t` satisfy the constraint `c`.
Types need not be confined your program, i.e., types can represent data in a network protocol that will allow you to reason about types in a formal and explicit manner. The notion of types in Haskell or any functional programming language is much richer than that used by C/C++. 
He's essentially no longer working on the original language and rewrote the compile in [dotty](https://github.com/lampepfl/dotty), which is scheduled to become "Scala 3" (or some such) in the distant future once it stabilizes. I don't think it's right to say he "hates" Scala, but he certainly believes that there is something wrong with its current state.
Oh, I'm thinking of [`StablePtr`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Foreign-StablePtr.html#t:StablePtr).
If you encumber your network protocol with the type system of one implementation, you just made a shitty network protocol that is very difficult to implement by any other program because other implementations have to replicate the possibly very complex type structure. I prefer to have simple network protocols where these features are neither needed nor desired.
Thank you! That is exactly what I was looking for. Let me see if I can use it.
There's a YouTube video of him giving a presentation at a college. It's basically him ranting for an hour with a powerpoint of reasons he hates Scala.
So using it Mac or Linux is a bit rough at the moment?
 n -&gt; ((n -&gt; p) -&gt; p -&gt; n) -&gt; p No matter how I try, I can't see how the last `n` is supplied by "the person calling the function". If I imagine having f :: n -&gt; ((n -&gt; p) -&gt; p -&gt; n) -&gt; p g :: (n -&gt; p) -&gt; p -&gt; n -- The second parameter of `f` I can see how my `g` will receive an `h :: n -&gt; p` (to which I will provide an `n`), but I can't see how I'll ever provide the last `n` to `g`?
I'm not working in it for a living, but if you want to PM me questions, I'll get back to you quickly. Although, if you can come up with the questions, there's also /r/haskellquestions where you can be mentored by the community. :)
Is it just because, ultimately, I provide the `g` and, therefore, its value?
Don't forget the compiler bugs!
The general idea is that applicatives are applying functions to values but with a little twist. Normally you would write a function call to look like f a b c but you can add a twist by using an applicative instance for the values, and instead writing f &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c The mechanical way to do this is to insert `&lt;*&gt;` between each argument, and `&lt;$&gt;` between the function and the first argument. The exact *sort* of twist this adds to the function call depends on the applicative instance. In the original comment we're using `n -&gt; a`, *functions*, which you can view as a value of type `a` that depends implicitly on a number `n` that we have to provide later. The upshot of this is that by doing function calls with `&lt;*&gt;` and friends, we can treat functions of type `n -&gt; a` more like normal values of type `a`. Another way to look at it is that applying functions with `&lt;*&gt;` and friends lets you work *under* the applicative in question. Here, it lets us apply the `bool` function to the `a` values *underneath* functions like `n -&gt; a`.
If you're looking for a well tended garden, then there's really nothing better than Elm. But it's for a specific type of crop, so you can't grow whatever you want in it. And that's a big reason why it lacks the cruft of most other languages. It knows its boundaries and optimized for everything within them. Haskell is far less bounded. I'd argue that it's purpose is to be able to support all sorts of programming ideas that may not ever find a practical purpose, or if they do, in a completely different form than whatever they were originally implemented with. Not the JavaScript flavor of the week, more like, the best of what academia has to offer. It takes whatever it can from it, whether it be in Math or CS or another discipline, and redistributes it to programmers. The future of Haskell isn't HaskellLite, and anyone holding out for it to cut down to a productivity maximizing size is wasting their time. You can be productive with Haskell, yeah, probably more so than any other language. But that's not what it's for. You have Elm for that. You have Haskell to make sure programming *itself* is constantly being enlightened with new ideas, and that they're not getting stuck or buried in academic papers. And to accomplish this, it always has to be moving. Stopping on occasion to trim cruft or to polish a finished product simply wouldn't be a very Haskell thing to do. Do I like this? No. I have my own frustrations with Haskell, and this is one of them. I think there's a better language gestating within it, however. But whatever it is, it isn't Haskell. So to everyone who wants something that feels like it's lacking a destination: stop looking at it, start looking beyond.
It's relatively nice if you're coming from the Java / C++ / JavaScript world. It's absolute crap if you've spent any time in the Haskell / Smalltalk / LISP world. Best I can say about it is that it's an escape hatch for functional programmers that are forced to work in the JVM. The tool support is crappy, and the attempt at unifying functional with OO has devolved into yet another hybridization that mocks them both. Your best bet is to use a curated set of libraries and stick to functional as much as possible. Typeworks seems to be doing the best work in that regard, but I don't think it can be considered a total solution as of yet.
I found out about this here https://hacktoberfest.digitalocean.com
&gt; And is guaranteed to terminate. I disagree. Non-termination usually is a bug, not a design choice. Requiring that code on which you depend be bug free is nice, but much more ambitious than ruling out (or even just detecting) a particular questionable design choice. &gt; Which cannot be done for a turing-complete language like haskell. I disagree with the presupposition that it needs to be done perfectly to be useful. Trivial non-termination like `let loop = loop in loop` could (and possibly should, judging from the number of times I've accidentally referenced a variable in its definition) be detected on a best-effort basis. 
This is depressing, but even more depressing is the fact that they're a committee of volunteers rather than a government funded institution. Something like France's IRIA. Meanwhile, fucking Oracle makes billions and it took them two decades just to implement severely bastardized lambdas. Where is the fucking justice in that.
I could write my application logic in eta and the ui in kotlin. (thinking Android dev here) That could be pleasant.
F# works perfectly well with Mono. Most of my F# development has been purely for Linux with absolutely no problems. The area you will run into issue with Xamarin. Still haven't figured out how to get it to compile consistently under Linux. But if you are looking to just do native desktop apps then F# and Linux are good to go. 
Tcl programmers laugh at your choice of having other types besides strings. 
So any single dimensional, homogeneous data structure is a string? 
I'm professionally writing Javascript and I haven't written a line of haskell in months. I think I'm starting to go through withdrawal. 
By the standards of someone who codes C# in Visual Studio or Java in IntelliJ IDEA, the F# story is pretty rough right now. By the standards of someone who codes OCaml or Haskell inside of Vim or Emacs... it's absolutely stellar.
You made me chuckle 
Or do use tabs... GHC doesn't care, only opinionated Redditors ;)
I mean, you can mix spaces and tabs and make it work but you have to know what you're doing. It tends to confuse beginners or people who aren't visualizing tabs in their editors. When it happens the syntax errors are pretty confusing if you aren't expecting it.
At least it also has "byte" :)
I wouldn't say either have less cruft.
Isn't `-fwarn-tabs` the default now? So GHC does care...
Missing sum types? Pattern matching? Static types? What else?
Made my day! That's a nice variety of Haskell media to consume greed
Cool, but why?
This is interesting, but why does the `pulls` endpoint say "Pull Requests" and return the same content here as the [issues listed here](https://github.com/issues?page=1&amp;q=label%3Ahacktoberfest+language%3Ahaskell+is%3Aopen)
1. This is cool. 2. Why?
Thank you so much for putting me on to this. What a great way to codify applications of rewrite rules. Reducing the application of these rules to graph search sounds like it paths the way for using Machine Learning to train Optimiser Heuristics. Am I missing some limitation?
why? because it is cool
Fair enough
actually eliminating the "tring" would be the best.
Coq is definitely not user-friendly but it compensates for it with its powerful tactic and other stuff. And CoqIDE vs ProofGeneral, I would chose ProofGeneral
&gt;Coq is definitely not user-friendly What's this meme about? I had an absolutely great time picking up Coq via *Software Foundations*, afterwards going on to write my own little projects. My metric being the amount of frustration per unit amount of work done, it's been great to me.
No, you're not :) in fact the follow up to that research dealt (somewhat indirectly) with that scenario. [The last paragraph of the abstract for this should head into the right direction, I think](http://www.cs.cornell.edu/~ross/publications/proofgen/). I'm very interested to see what an expert in Machine Learning could do with this approach to learning optimizations and structure and how they would embed that into a neural network.
Seems like a version of something like Alpha Go's Neural Heuristics would be valuable. I'm excited. I can imagine AIs that have multiple functions copying their source, injecting constant values for the particular problem that they need to solve, optimising them using this and then having a specialist program for performing a task without having to reason about the alignment of a new program.
Absolutely - when the very first thing the NixOS installer tells you is &gt; For partitioning: fdisk. with absolutely no help in what to actually do, you immediately lose a huge number of people who were interested in learning and immediately realise it's nowhere near being ready for everyday use (despite many people doing so). This was my first experience with Nix(OS) and it left a very sour taste in my mouth.
It looks like they did it as a learning project. I'm mostly basing this off of the readme saying that they moved on but wil keep updating it. Learning is always a good reason for projects like this, IMO.
"Tcl: Stringly Typed Programming for the Masses!" * John Ousterhout, probably
&gt; looking for a well tended garden Yes, you totally get me. Thanks!
&gt; Can someone give me some use cases besides integrating with legacy code? JVM and Java is more than a language, it is a platform and a infrastructure that provides a well documented common set of libraries that allows programs to be compiled once and be run everywhere. You cannot do it with Haskell as the ghc compiles to native code which is bounded to a particular processor architecture, operating system and executable format. If you only develop webservers and command line apps for Unix, you don't need to care about cross platform issues. However, if you intend to build apps that run on all platforms and also GUIs apps that runs everywhere, Java platform or any language that compiles to JVM may the best deal. Another problem with native applications is packing all dependencies and shared libraries for all supported operating systems. Again, it is not a issue for servers that only runs in one place, but it is crucial for games, desktop apps and business desktop apps.
Investment fallacy.
You can learn about Haskell, Rust and compilers at the very same time, seems like a good project
I like OCaml, but OCaml has as much, if not more, cruft than Haskell going by the OP's own priorities. For instance, take the issues in OCaml with the standard library which out-of-the-box provides just enough to write an OCaml compiler in, and nothing more. To get anything done you either have to resort to Jane Street Core or Batteries. OCaml then adds its own specific bits of cruft. There's multiple incompatible build systems, for example. OCaml strings are also not Unicode aware. You need to use Batteries or some other library to properly handle Unicode.
It would probably make even more sense to write a Rust compiler in Haskell :-)
I can't find it, but there's a video of him giving a presentation at some school. He spends the whole time tearing apart Scala and has a powerpoint to show his examples.
I don't know how MagicHaskeller works exactly, but type system and pattern matching seems pretty powerful combination for some "programs from data" kind of inference problems to me. Does anybody know if there any similar work done in this direction?
Right. My first rust project was a prolog interpreter. Just this week I was using it to learn more about proof search by extending it to be complete and sound on first order logic. It's been a great way to learn rust and prolog.
but mah learnin
It sounds really cool! Why prolog tho'? And is it open source? I would like to take a look!
/u/Syrak Thank you, it's working perfectly [now].(https://github.com/louispan/data-diverse/blob/23e0a52f2a13f5c8fdc09d74d9cdc2e5ecd31f28/src/Data/Diverse/CaseFunc.hs#L30)
[;)](https://github.com/pulls?q=label%3Ahacktoberfest+language%3Aidris+is%3Aopen)
Given that GHC probably contains many enormous space leaks writing a Haskell compiler in Rust actually seems worthwhile.
True story
Disclaimer : I'm not a professional in either, dabbling hobbyist, and Idris seems to have several potential limitations in production: ecosystem and maturity. But in *principle*, what I see is Idris standing on the shoulders of a giant -- Haskell. A language how Haskell "is supposed to be" -- if it had had all the experience gained exactly through its development and evolutionary process right at initial design, which is of course would have been paradoxical. 
An alternative to Haskell's runtime in Rust would also be an interesting project.
Is there way to put a negative constraint on a function like myApi1 :: ((MyClass c)) =&gt; c -&gt; IO () myApi2 :: (!(MyClass c)) =&gt; c -&gt; IO () I am using MyClass just to 'tag' some types and want to implement 'myApi2' so that it is valid only for types which are not 'tagged'.
You should probably x-post to /r/rust
I think the original compiler was written in OCAML, so... a partial win?
By this reasoning (rewrite in another language to avoid space-leaks), should we be creating another Haskell compiler in the first place? ;-)
You're welcome!
&gt; Edit: I actually found it) A non-constructive proof of existence?
Wow fast!
How do you come to this conclusion? If there were many enourmous ones I would expect that to be a major pain point. So either they would be fixed or discussed a lot more.
Is there anyone w
Two observations for discussion: 1. Pretty much every non-trivial Haskell program contains a space leak. 2. GHC uses vast amounts of memory and no one's really sure whether it needs to.
I agree that it would be nice to have a partial termination checker to catch things like `let loop = loop in loop`. That would be a wonderful addition to GHC in and of itself. If the thing you described earlier doesn't need to do termination checking, it seems like it could be done as a linter. Also, it's worth noting that `throw` is bad regardless of what context you're in since it's basically the same thing as `error`. The one that's ok is `throwIO`.
Rich seems to generally know what he's talking about, but I don't really recognise the problems he seems to imply types inherently have. Is there anyone who has used Clojure or some other lisp who can comment? I've never used a Lisp, so I find it difficult to say whether I'm not sufficiently enlightened to understand his point of view, or that there's more to it.
Summary: * Information is sparse, aka not structured. * Information is open ended. * We keep getting more information. * Sometimes I know n things about a piece of data and in other places I know n+/-m things. This is hard to model in static languages (inheritence, sub-typing, etc). * Product types are bad because they don't have names. Pattern matching to get out data is stupid and coupling to names which is bad. * No "compositional algebra" - I didn't discover what he means by this. * Clojure just uses dictionaries. * In summary: Positional semantics. Parametrization of types are positional (not named). * "Maybe String makes no sense" because your social security number is a string not a maybe string. "So I think static types are an anti-pattern because they introduce this coupling." He talked about coupling in another talk which I agree with. A row type is better than a product type (whether named or anonymous tuple), because it names its fields and you don't have to couple to ordering. Named parameters to functions are also mostly better, but with a few exceptions we make for single-argument functions or commutative functions like `+`. Aside from the positional semantics problem, I don't think he spends any time on actual staticness or type theory. Applying this to the pure statically typed world, I think it's very tractable with current technology. PureScript with its row types makes this style of programming easier, because you can have functions take (open-ended) records as arguments, and also put them in your sum types. Given a calculation like BMI: calculatePersonBMI = weightInKilograms / heightInMeters * heightInMeters And let's say we do "dog years" and combine BMI with (age/7), so the older it gets in dog years, the worse its BMI is. calculateDogBMI = weightInKilograms / heightInMeters * heightInMeters * age/7 Your code might traditionally in Haskell look like this: data Character = Dog Double Double Double | Human Double Double characterBMI :: Character -&gt; Double characterBMI (Human heightInMeters weightInKilograms) = weightInKilograms / heightInMeters * heightInMeters characterBMI (Dog heightInMeters weightInKilograms ageInYears) = weightInKilograms / heightInMeters * heightInMeters * ageInYears/7 If you add extra fields to Human or Dog, you have to: 1) go around and update all your code that pattern matches, and 2) make sure you get the order right when you do the update. So you might change the code to: data Character = Dog {weight::Double, height::Double, age::Double} | Human { height::Double, weight::Double} characterBMI :: Character -&gt; Double characterBMI c@Human{} = weight c / height c * height c characterBMI c@Dog{} = weight c / height c * height c * age c / 7 But with those record accessors you lose the ability to know whether the record is available or not for a given `c :: Character`. What if you apply `age` to a human, which doesn't contain an age? The output is not defined. But in PureScript, the above type has a different meaning. It means that the constructor `Dog` takes one argument, a record, and so does `Human`. And the records have different types. So the function becomes: characterBMI :: Character -&gt; Double characterBMI (Human c) = c.weight / c.height * c.height characterBMI (Dog c) = c.weight / c.height * c.height * c.age / 7 We can go on to do it for function arguments too: bmiImproved :: Character -&gt; Character -&gt; Double bmiImproved now prev = characterBMI now &lt; characterBMI prev We can screw up ordering of this function call, we can instead write: bmiImproved :: { prev: Character, now: Character } -&gt; Double bmiImproved diff = characterBMI (diff.now) &lt; characterBMI (diff.prev) At this stage we've removed any positional semantics and yet we have static type safety. I also don't feel like I've sacrificed anything to achieve this. I think Hickey's arguments against positional semantics are good, but the static typing points are a bit vague and uninteresting.
"We can't use static types for everything, so we won't use them for anything"
Author here. Was curious why this old project started appearing in my Github feed. Basically I wanted something to work just for the purpose of learning and I was tired of coming up with new projects every few weeks or months. So I figured a compiler would be cool and I was curious about how lazy evaluation worked as well as type unification so Haskell was a natural fit. I actually started to write it [in C++](https://github.com/Marwes/haskell-compiler-cpp) (would look better on the CV as there are more jobs in C++) but after getting tired of debugging a particularly nasty segfault I tried rewriting the parser in Rust and it grew from there. Currently though I have moved on to another compiler ([gluon](https://github.com/gluon-lang/gluon)). I should probably update the README since I don't actually need to update this anymore now that Rust is stable.
1. is a pretty bold claim, but 2. is just an artifact of GHC being a 25 year old code base. Rewriting it in Rust likely wouldn’t help that much more than rewriting it in Haskell.
&gt; I think Hickey's arguments against positional semantics &gt; are good, but the static typing points are a bit vague and &gt; uninteresting. Really great summary, thanks for this. I remember a Clojure proponent visited /r/Haskell and I tried to determine what he found so unpalatable about types. My conclusion was that he wanted polymorphic row types.
1. Neil Mitchell is indeed a bold guy. http://neilmitchell.blogspot.ie/2015/09/detecting-space-leaks.html &gt; Every large Haskell program almost inevitably contains space leaks. 2. What's your rationale for believing that massive memory usage is due to age? Do old programs generally use large amounts of memory? It seems very likely to me that it's got a few large space leaks.
I used clojure rather extensively for about two years, with very knowledgeable clojure veterans at my side, but it still basically felt like something halfway between Python and JavaSci ript plus a weird love for metaprogramming that, most of the time, seemed to introduce more confusion than it was worth.
&gt; Pretty much every non-trivial Haskell program contains a space leak. How are you arriving at this conclusion? Space leaks are pretty difficult to make in a GC'd language: you somehow have to leak so badly that the GC can't clean it up, so you have to do more than just create a reference cycle. You somehow have to create a permanent reference and then forget about it, which is not something easily done by accident in idiomatic Haskell code. Now if you're saying functions often use more memory than they need to, that makes sense, but that's not the same thing as a space leak.
Rexx laughs at Tcl.
What you are talking about is normally referred to as a "memory leak". In the Haskell world we generally use the terminology "space leak" to refer to the case "when a computer program uses more memory than necessary". See https://queue.acm.org/detail.cfm?id=2538488
The complexity of GHC’s technical debt makes it rather difficult to reason about its performance. That debt is due to age. And I’m not complaining about a new compiler. All I’m saying is that I don’t see any intrinsic value in doing it in Rust, in response to your comment that writing a Haskell compiler in Rust seems worthwhile. I think it greatly overestimates the power of space leaks to say GHC would be better written in Rust. If someone rewrote GHC in Haskell with a minor focus on performance, it would be a large project, and I think it would be fairly easy to make sure it didn’t have any (large) space leaks
&gt; If someone rewrote GHC in Haskell with a minor focus on performance, it would be a large project, and I think it would be fairly easy to make sure it didn’t have any (large) space leaks I agree. Perhaps you read something in to my original comment that I didn't actually say.
I know people abuse this term that way here when analyzing specific functions, but when talking about entire programs, that's definitely not what this phrase means. It refers to perpetually allocating more memory the longer your program runs; it does not mean simply using 30 MB when 10 MB would have sufficed.
I often like Rich's talks---he has some genuinely interesting ideas and opinions, even when I don't necessarily buy them wholeheartedly---but I gave up on this one about halfway through. I am always surprised at how he feels the need to express his dislike or disagreement with static typing with such aggressive sniping and derision---and it shows up both in talks like this (which is peppered with asides that clearly seem directed toward our community in the part I watched, and sounds like it turns into a jeremiad in the portion I didn't), and the one face-to-face interaction I had with him. It really comes off as insecure and defensive, which I think is unfortunate: it makes me less likely to pay attention to what he says in the future, and it seems to have become a habit of other people in the clojure community to emulate it.
&gt; Writing a Haskell compiler in Rust actually seems worthwhile. Assuming the value proposition of this statement is Rust, that’s what I’m disagreeing with.
&gt; I know people abuse this term that way here when analyzing specific functions That's rather strong language. The way I defined the term is the way the term is commonly used in the Haskell community. I've linked you to a paper published by the ACM that defines it as such. If you think we should be using a different definition perhaps you'd like to provide your own citations. &gt; It refers to perpetually allocating more memory the longer your program runs Indeed, and I suspect GHC does that. 
My understanding of the topic is that a space leak is when you use more memory *than you intended*, and a memory leak is a specific case of this due to a failure to release now-irrelevant resources. It’s not just that you used 30MB when 10MB would have sufficed. It’s that you really meant for you program to only take 10MB, but for some reason it’s using 30MB.
The value proposition of this statement is a using a strict language as an experiment in order to make a performance comparison.
Ah. Therein lies my misunderstanding =)
Awesome. Glad to hear the story behind. Its a great project to have worked on.
To me, many of his criticisms seem to be straw men: Java, anonymous product types, positional semantics, functions with too many arguments, etc. All of these things are things that I consider problematic. Java's type system is light years away from Haskell's, and is thus an invalid argument against static type systems. My threshold for converting tuples to named records has been monotonically decreasing over the course of my career. I don't consider `data Foo = Foo Int Int String Double String` to be a good pattern, almost always preferring to write named records. When function arguments get too long, I often create a record for them, which simultaneously reduces the number of arguments and gives them names. Record names in Haskell are composable because they're simple pure functions. His assertions about things being not structured and open ended really sound to me like assertions not about the whole world, but about the particular corner of the world that he has occupied. He prefers representing data as maps. Well, Haskell has maps. I can drop down to them whenever necessary, but I can also use its powerful type system when I'm working on problems where they make sense.
Some of these seem pretty silly, like `"Maybe String makes no sense" because your social security number is a string not a maybe string.` `SSN` should be 9 digits, and a record field of type `Maybe SSN` indicates the thought "I may or may not have the SSN for this record."
Oh, yes that makes sense. I agree with his original comment then, although using more memory than necessary is hardly unique to Haskell programs.
Ah, ok, in that case I misinterpreted your original comment. Yes, I'd agree that almost any non-trivial Haskell program uses more memory than necessary. I still think memory leaks should be pretty uncommon, though, even if they do occur in GHC.
As someone who does both PureScript and Clojure professionally that's _exactly_ what I wish I had in Haskell.
Yeah it would be very cool.
I actually see some sense in that.
&gt;&gt; My conclusion was that he wanted polymorphic row types. Like in frames? 
Has the video been removed?
There is a huge difference between a *a few large* and *many enormous* though. I don't doubt for a second GHC uses more memory than strictly neccesary. But the only perf related complaints I remember hearing so far where compile time related. Which to be fair can be related to leaks. And that seems to be more an issue of manpower than implementation language to me.
&gt; There is a huge difference between a few large and many enormous though. Oh really? How would you quantify that difference? :) &gt; But the only perf related complaints I remember hearing so far where compile time related. Lots of people would like to compile Haskell programs in low memory environments such as Heroku or other low memory virtual machines. &gt; Which to be fair can be related to leaks. Indeed. I suspect fixing space leaks in GHC will improve compile times. FWIW I don't know any of this for sure but it is my informed guess. &gt; And that seems to be more an issue of manpower than implementation language to me. Sure. Many respondents here seem to be assuming I've said "GHC needs to be rewritten", even "rewritten in Rust", or "Haskell is a bad language because of space leaks". I've neither said nor do I believe, any of these things. 
I've never understood the various row type encoding attempts well enough to fully grasp whether they're doomed but my guess is that you simply can't get good inference by (ab)using type level lists. You really need it baked in to your type system.
Gluon looks like a really cool project! Good luck with it.
No. 
Really? Knife handles don't prevent cutting yourself with the blade, therefore we shouldn't have knife handles?
I don't want my programs to be open, where anything can happen, I want them to be closed, where only things happen that I want to happen.
I could help you through the haskellbook.com. I think the book is a good vehicle for learning and I could try provide help and further explanations on top of that. To be honest you could just buy the book and join https://fpchat-invite.herokuapp.com/ and head over to the #haskell-beginners channel. You'll get help from myself and others.
Now we are in agreement. &gt; it seems like it could be done as a linter. Isn't that basically what Safe Haskell is? A linter with some extra compiler support?
&gt; Oh really? How would you quantify that difference? :) "Several to many large to enormous" :)
&gt; using more memory than necessary is hardly unique to Haskell programs. Not unique, but many ways of doing so are the direct result of laziness. Ed Yang has a good [taxonomy](http://blog.ezyang.com/2011/05/space-leak-zoo/). I prefer "thunk leak" to "space leak" because it's more specific and less misleading, and it's the one that's basically unique to Haskell.
I feel like dependent types would be a good fit for encoding data +varying degrees of knowledge about that data. 
I agree, this terminology is clearer to me.
There's no way to negate a constraint, AFAIK. I wouldn't be surprised if allowing it could lead to subtle bugs. However, if you're just tagging types, you can use a closed type family and equality constraints instead: type family Tag (a :: *) :: Bool where Tag Foo = True Tag Bar = True Tag a = False myApi1 :: (Tag c ~ True) =&gt; c -&gt; IO () myApi2 :: (Tag c ~ False) =&gt; c -&gt; IO () Of course, if your class has any methods, this doesn't work: `myApi1` won't have access to the instance. But this isn't very "good" Haskell, at least in my opinion; why do you need this in the first place? Can you give a less minimal usage example?
Sounds right to me! I'd appreciate any questions and/or comments about my experimental row types plugin. https://github.com/nfrisby/coxswain/tree/master/coxswain 
Sounds right to me! I'd appreciate any questions and/or comments about my experimental row types plugin. https://github.com/nfrisby/coxswain/tree/master/coxswain 
Sounds right to me! I'd appreciate any questions and/or comments about my experimental row types plugin. https://github.com/nfrisby/coxswain/tree/master/coxswain 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [nfrisby/coxswain/.../**coxswain** (master → bfd5777)](https://github.com/nfrisby/coxswain/tree/bfd5777964a5b4ba64e1bcc4cf18df1c9d8fc558/coxswain) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dobddr9.)^.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [nfrisby/coxswain/.../**coxswain** (master → bfd5777)](https://github.com/nfrisby/coxswain/tree/bfd5777964a5b4ba64e1bcc4cf18df1c9d8fc558/coxswain) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dobddsq.)^.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [nfrisby/coxswain/.../**coxswain** (master → bfd5777)](https://github.com/nfrisby/coxswain/tree/bfd5777964a5b4ba64e1bcc4cf18df1c9d8fc558/coxswain) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dobddu0.)^.
Short answer: No Longer answer: You shouldn't condition on the absence of a type class, because that breaks modularity, since now, if you compile with a module that produces an orphan instance, the semantics of the function change. You probably want to rethink your design, but if you _really_ want something like this, you might be able to use type families. {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} module Bar where type family Api1 a :: Bool type instance Api1 Int = True type instance Api1 Bool = False myApi1 :: (Api1 c ~ True) =&gt; c -&gt; IO () myApi1 _ = return () myApi2 :: (Api1 c ~ False) =&gt; c -&gt; IO () myApi2 _ = return () or {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} module Bar where type family Api1 a :: Bool where Api1 Int = True Api1 a = False myApi1 :: (Api1 c ~ True) =&gt; c -&gt; IO () myApi1 _ = return () myApi2 :: (Api1 c ~ False) =&gt; c -&gt; IO () myApi2 _ = return () 
What
You can use an explicit `Fix` combinator and functor coproduct, like so: data OpF a = Or a a | And a a | Imply a a deriving Functor data LitF a = Const Bool | Var Char | Not a deriving Functor type Lit = FixF LitF type Prop = FixF (LitF :+: PropF) Note that the shape of `Lit`'s `Not` constructor had to be changed in order to allows it to be used as intended in `Fix PropF`. The main downside of this approach is that you end up having to do a lot of tedious newtype wrapping/unwrapping; you might be able to get around some of that by sprinkling `coerce`s around. GADTs are probably a better option, though.
Every time I want to do this in Haskell, I cry a little. GADTs are probably the way to go if you can define all your constructors in the same place, but you will end up dealing with existential types, which sometimes make me sad. {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE GADTs #-} module Bar where data Prop isLit where Const :: Bool -&gt; Prop True Var :: Char -&gt; Prop True NegVar :: Char -&gt; Prop True Not :: Prop a -&gt; Prop False Or :: Prop a -&gt; Prop b -&gt; Prop False And :: Prop a -&gt; Prop b -&gt; Prop False Imply :: Prop a -&gt; Prop b -&gt; Prop False If you really want the `Not` constructor to handle single negation specially: {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE GADTs #-} module Bar where data IsLitTag = VarTag | LitTag | NonLitTag type family NegationTag a where NegationTag VarTag = LitTag NegationTag a = NonLitTag data Prop isLit where Const :: Bool -&gt; Prop LitTag Var :: Char -&gt; Prop VarTag Not :: Prop a -&gt; Prop (NegationTag a) Or :: Prop a -&gt; Prop b -&gt; Prop NonLitTag And :: Prop a -&gt; Prop b -&gt; Prop NonLitTag Imply :: Prop a -&gt; Prop b -&gt; Prop NonLitTag This approach is really useful sometimes, but it's pretty heavyweight, and it exposes a lot of the confusing parts of Haskell's type system. I'd recommend against it unless you're morbidly curious. For a long time, GHC didn't do anything fancy and just made more data types, so the simple approach is feasible.
His argument stems from a misunderstanding of static type systems. Even for his so called *open* data types you have to define a common interface to be able to do something useful. Otherwise all values are equivalent to the unit value, which bears no information except its existence. (Since he talks about information I assumed he means data types and not algorithmic extension.) If I want the same behavior in Haskell, I wrap an existential type with a type class constraint. Haskell's type system can easily give me better safety than Clojure because I can only pass in values for which the interface is well-defined. (That's probably what you meant.)
&gt; Names aren't first class Clearly he hasn't tried lens.
Yes this is a wart in Haskell. My coding guidelines require that sum types with records split everything up so you get this same behavior (albeit with much more boilerplate).
Also described as "Throwing the baby out with the bath water."
Oh. Rich is questioning static types. It must be a day that ends in a y.
I would *love* to see this (or similar) make it into normal Haskell usage.
&gt; On the other end, Scala and Haskell are "organic" in the worst way GHC benefits in some ways from being a target for people theses though it certainly runs slower for it :)
Another alternative might be using this in conjunction with futamara's projections and instead of having a "dumb machine", sticking an AI inside the machines while they work. The possibilities are limited only by imagination, really; it's a good time to be alive.
Maybe some day https://ghc.haskell.org/trac/ghc/wiki/Plugins/TypeChecker/RowTypes/Coxswain
&gt; But in principle, what I see is Idris standing on the shoulders of a giant -- Haskell. A language how Haskell "is supposed to be" Idris is strict while Haskell is lazy. Valid reasons for both, of course, but it makes a fairly large difference in practice.
It would probably be better to link to your wiki page instead https://ghc.haskell.org/trac/ghc/wiki/Plugins/TypeChecker/RowTypes/Coxswain
I absolutely love Idris but its ergonomics and tooling are not really there (particularly a cabal/Hackage equivalent).
&gt; There are too many ways to do any given thing. Scala suffers from the unfortunate problem that anything written in Scala could be written in a functional style *or* an OO style. So approaching a codebase you get fewer promises.
&gt; I cry a little I can see why, I thought it'd be simpler! Type families look really interesting though, but I'll probably just define literals as a different type, seems much easier. Thanks!
You could use MultiParameterTypeClasses and have a phantom type variable to distinguish which instance you want.
Functionally none, except that they are not interchangeable, which makes them pretty much exactly like lazy vs strict `Text` as far as the actual act of programming goes.
So these are not separate types in the sense that the underlying structure is not different.
Using the coproduct actually makes sense, although I really don't like `Not` being defined that way, but I can see the problem with `Not (Var Char)`, I'll look into it, thanks!
Indeed, and I'm having a really hard time if it's me who's misunderstanding him or whether he's actually saying some of these dumb things. (The reason being that he also often says a lot of very clever and interesting things.) Nowadays, I disregard anything he says about static-vs-dynamic type checking. I honestly don't think he is familiar enough with recent trends in state of the art practical type systems to have an informed opinion. (It's either that, or the type of work he does is *so* different from the type of mundane "practical" data-ferrying work that I do that we simply don't share enough implicit context for me to be able to understand where he's coming from.)
Potentially highly valuable too as if it worked out well, it could be reused in other implementations like GHC!
I want a faster `ghc`, not a slower `rustc`.
Indeed. Would you like "every prime-indexed bit" to be first class? Can do!
Very well put.
Why mix em? Just use tabs and only tabs. N tabs means you are in the Nth nested lexical scope, and everyone can make the indentation as wide or narrow as they want it to look locally.
I think /u/anacrolix may have thrown his computer out of the window after his sudden realisation :).
The underlying structure of everything is just bytes and pointers (aka more bytes), we use types to give these things different meaning at the language level.
This feels a bit ad-hoc, I think it should be integrated to the language itself with a nice synthax.
Damned classical logic.
Different type class instances should be different types. Consider `Map Int Int` - it has an internal structure that it needs to work correctly, and if you try to use some random ordering on Ints these assumptions will likely be violated. Simply using a different type for a different instance makes everything work properly.
Every language has maps. Row types, are some syntactic sugar for maps. What Rick demand is to use maps. But static typed programers don't understand this without some form of rigidity on top of that (some form of HList) and some syntectic sugar to avoid the disgusting untyped look of lookups. That destroys that flexibility that maps have and Rick demand. 
&gt; because it would not know what to put in for the underlying type class dictionary that gets passed around. No, that's not the reason. We genuinely do want each type to have at most one instance, it's a design goal not a limitation. See e.g [this](https://www.youtube.com/watch?v=2EdQFCP5mZ8&amp;feature=youtu.be) recent beginner-friendly explanation, or [this](https://www.youtube.com/watch?v=hIZxTQP1ifo&amp;feature=youtu.be) more well-known, more in-depth explanation.
The problem in general is called program synthesis, and there's some pretty cool stuff done for it. [For example.](http://comcom.csail.mit.edu/comcom/#Synquid)
This is very cool work. Thanks for sharing!
Can you give an example where we would get a wrong behavior with my suggestion?
Sure reversed ::Ord a =&gt; Ord a where a &lt; b = b &lt; a a = Map.fromList [(1,1),(2,2),(3,3)] b = Map.insert{reversed} 4 4 a 4 will now be inserted as the least element in the map, and now things will go wonky from here on out.
What I'm suggesting is one and only one *primary* class and many other secondary classes that have to be explicitly specified.
What does it mean for a class to be "secondary"?
&gt; "SPJ in an excellent series of talks lists these advantages of types... the biggest thing left out of clojure..." &gt; "The biggest merit he says is in software maintenence, and i really disagree with just a lot of this. it's not been my experience; the biggest errors are not caught by these type systems; you need extensive testing to do real-world effectiveness checking." I think Rich is dead wrong on types not providing good tools for software _maintenance_. He is attempting to discredit types because they do not catch all errors, or the toughest errors; that's fine, I don't agree with people that would say types do that for you. I think dynamism is great for speed of development and flexibility; but my impression about trying to debug and maintain clojure code is that one has to spend time tracing back the source of errors and shape of maps (pre-spec). Those issues are somewhat mitigated by having a good type system, and what you loose on the flexibility going with types, is a trade that I think returns much more that what you give up.
Additional question, also paging u/Solonarv, going back to the simplest case, why I can't simply do this: data MyType = A | B | C data MySubtype = A | B ? It's simply not implemented or it *can't* be implemented?
As someone who has used and contributed to some of these libraries, I can confirm that this is correct. I would add that performance is also a big unknown with a lot of these. If all the typeclass dictionaries inline correctly, things are fast, but if they don't, you're looking at a cost for each access that is linear in the size of the record. And I mean the bad kind of linear. Not like, "just increment this register a few dozen times". More like "let's chase pointers and blow out the cache". In a language with row types, people may write something like this: foo :: { x :: Int, y :: Int, z :: Int | rs } -&gt; Int With vinyl, this would instead look like: foo :: [ ("x",Int), ("y",Int), ("z",Int) ] ⊆ rs =&gt; Rec ElField rs -&gt; Int If the body of `foo` is small, then you can get away with just inlining this everywhere (and then hoping the dictionary gets inlined further). If it's large though, then inlining this in a bunch of places will bloat your executable. But this is the best case scenario. What if the function had more arguments and you wanted to partially apply it: foo :: [ ("x",Int), ("y",Int), ("z",Int) ] ⊆ rs =&gt; Char -&gt; Rec ElField rs -&gt; Int foo c r = ... main :: IO () main = do print (map (foo 'x') largeRecords ++ map (foo 'y') otherLargeRecords) Does GHC eliminate the typeclass dictionaries? Maybe. If everything's all in the same module and `foo` isn't exported, you have a decent chance that it'll do what you want. But you really don't know, so you have to read GHC Core to figure out. And vinyl cannot stop you from duplicating fields: myRecord :: Rec ElField [("x",Int),("y",Int),("x",Bool)] -- not good But to get back to your original point about type inference, yeah, it kind of has some weird corner cases where it's broken. When everything is monomorphic, it's all good. However, since vinyl relies on the [RIndex](http://hackage.haskell.org/package/vinyl-0.6.0/docs/Data-Vinyl-TypeLevel.html#t:RIndex) type family, which has a non-linear pattern match, to automatically resolve elementhood and subsethood, polymorphic fields can cause it to fail. (It's worth noting that it's not just a problem with vinyl. Every haskell library in this space uses either non-linear type-level pattern matching or `OverlappingInstances` and has this same problem). Practically, what does this mean? Things like this don't work: {-# LANGUAGE DataKinds #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE RankNTypes #-} module Example where import Data.Vinyl import Data.Vinyl.Derived replaceValue :: forall a rs. '("x",a) ∈ rs =&gt; Rec ElField rs -&gt; a -&gt; Rec ElField rs replaceValue r a = rput (Field a :: ElField '("x",a)) r tryToUseIt :: Rec ElField '[ c, '("x",a) ] -&gt; a -&gt; Rec ElField '[ c, '("x",a) ] tryToUseIt r a = replaceValue r a That one was a little underwhelming. It turned out to not be as bad as I remembered it. Here a better example an annoying inference problem. Let's first define a function: field :: forall s a. KnownSymbol s =&gt; a -&gt; ElField '(s,a) field a = Field a Now, in ghci we play around with this: &gt;&gt;&gt; field @"x" True :&amp; field @"y" GT :&amp; RNil {x :-&gt; True, y :-&gt; GT} &gt;&gt;&gt; :t field @"x" True :&amp; field @"y" GT :&amp; RNil field @"x" True :&amp; field @"y" GT :&amp; RNil :: Rec ElField '['("x", Bool), '("y", Ordering)] &gt;&gt;&gt; let y = Proxy :: Proxy '("y",Ordering) &gt;&gt;&gt; rget y (field @"x" True :&amp; field @"y" GT :&amp; RNil) y :-&gt; GT So far, so good. Everything has worked out smoothly except for the minor annoyance of having to specify the type of the `y` field (`Ordering`) to pull it back out of the record. Let's try nearly the same thing but with a number, and we're going to hop out of GHCi because it's monomorphization rules would give a misleading error: number :: Int number = getField (rget (Proxy :: Proxy '("z",Int)) (field @"x" True :&amp; field @"z" 42 :&amp; RNil)) This fails with: vinyl_inference.hs:18:20: error: • Ambiguous type variable ‘a0’ arising from a use of ‘rget’ prevents the constraint ‘(RElem '("z", Int) '['("z", a0)] (Data.Vinyl.TypeLevel.RIndex '("z", Int) '['("z", a0)]))’ from being solved. Probable fix: use a type annotation to specify what ‘a0’ should be. Although it's clear to us that we expect the value at `z` to be `Int`, GHC will not try to unify `a0` with `Int` (nor should it). So, basically, we can only use `rget` on fields that the record is monomorphic in. Support from the type system could fix this, and it could prevent duplicate fields. One other thing that's annoying is that, as people, we know that: x ∈ ys, ys ⊆ zs ==&gt; x ∈ zs No haskell records library gives us this for free. We have to prove it by hand, which clutters code and carries a runtime cost. Type system support for records could solve for the constraint on the right given the two pieces of information on the left. This would also be nice.
`Map` is not a legit comparison. The structure of keyed data is often not flat, or not useful to represent as flat. You have to make some pretty profound usability compromises if you're using `Map` or `HashMap` to contain arbitrarily nested (or typed) data - Your option is, essentially, to make wrappers of some other type and then try to implement an interface on top of that. Existing solutions to this problem in Haskell are clumsy and usually involve complex type system magic, some measure of variably severe performance cost, and weird language extensions. Although, with those solutions, the authorship experience can sometimes approach the relatively painless flexibility of having true row polymorphism, the number of considerations that you need to take into account can serve to limit the real-world applications of those solutions. It's a real problem that we have in the ecosystem. In Haskell, it's extremely easy to author abstractions and codify powerful rules - It's comparatively difficult to do a bunch of arbitrary one-off crap for note-taking, labelling, or other human-interface-y purposes. Sometimes, software design gets pretty arbitrary and one-off, and trading the ability to do 'fuzzy' things for more powerful systems of rules is not always a net win.
Basically, he said that the Maybe String is a feature of your protocol, but that optionality does not exist vis-à-vis the actual Domain. (e.g. maybe you haven't received a String for this person yet, but )
I don't buy the assumption that any program written in Rust will be faster than a program to do the same thing written in Haskell. Writing a compiler for Haskell is a highly non-trivial domain (where, for the most part, safety concerns are not related to memory safety or even higher-level notion of value ownership), and it may very well be the case that Haskell's ease and concision at expressing higher abstraction let people build a better, faster compiler than what you would get spending the same effort on a Rust codebase. I also think that compiling Haskell is also a relatively well-understood domain: many papers have been published about various parts of the system, there is a fairly well-defined type intermediate language with a robust design, and implementations of other parts abound. It is not clear what would be gained by yet another Haskell compiler (I realize of course that the posted project is a learning project, not necessarily meant to cover the full language and become a production compiler). On the other hand, compiling Rust is currently full of unknowns, and many part of the system would benefit from exploratory programming and exploratory design. For example, people are currently working on understanding how trait inference should actually work (Chalk), formulating it as Prolog-style proof search (in contrast, elaboration of type-classes in Haskell is a well-studied problem whose design space has been already well explored). Many parts of the lifetime system are also evolving fast or not-so-well-understood, and I think that there also much more to be learned about how to do good backend for a Rust-style language, or at least a more modular compilation strategy (`rustc`'s speed woes come in large part from the completely monolithic monomorphization design). The Rust designer community might actually benefit a lot from a smaller, more abstract, cleanly designed prototype of a Rust compiler, and that performance may not be the important feature there (so another language would probably work).
I mean you could always just throw it in an Aeson JSON object if you really want to.
Well there's your problem! Not enough Simons. 
Correct me if I'm wrong, but this kind of program synthesis constructs functions from specification. What is essential for GAI though is to synthesize functions from data. So an intermediate step is to derive a reasonable specification from data using some Machine Learning algorithm.
That makes some kind of sense, but it's just getting *really* tedious to be charitable when he's being so vague about things. (Honestly, at this point I think it might almost be a "cult of personality" thing :/. Incidentally, I wonder if the same has happened to Alan Kay.)
I'm not sure I get the joke here, but... this doesn't actually come up, like, ever? (I'm not upset or anything, but I just feel like I'm missing a good joke. I'm also slightly ~~drunk ~~tired and emotional which maybe a contributory factor.)
I agree completely. I wonder if Rich has ever had to program a SPA with plain JavaScript. (Choose whichever framework you like.) Doing that it *transformative*: You start out really happy for the first 3 months... you iterate quickly, AJAXing all the things, everything is swell... Now the client tells you that the backend services have changed a little bit... (dramatic) ... Now, you're fucked (I). You have to retest *every* single Ajax interaction... and you still don't necessarily know if you missed a newly added optional field. Ok, so you get over that crisis... get back into the groove... and you find out that you have to do a huge refactor of a class/component that's internal to your application... and then you give up.
(Self-reply just to have a clean separation from my previous component.) Given my many years of having these discussions, I'm now convinced that there's some sort of fundamental disconnect between the static/dynamic people. Usually I'd say it's the "PHP problem", but at least *some* knowledgeable people are touting "dynamic". However, those people *do* have a Cult of Personality thing going, so maybe that's the explanation... I dunno.
I didn't quite understand everything in your comment, so just to be clear: It's basically necessary to do it as a built-in thing? (I sort-of-understand the efficiency issue: HList is obviously going to be linear and if GHC doesn't inline accesses... well, goodbye performance. HMap might do "logarithmic", but HMap itself is... gnarly?)
Did someone call in for a `Map String Dynamic` ??
While the most common use of lens is to extract fields out of a record, it is capable of projecting arbitrary slices of a data structure out. So, for instance, [you can lens out individual bits of a number](https://hackage.haskell.org/package/lens-4.15.3/docs/Data-Bits-Lens.html) and you have full support for everything lens can do. From there it's just a short leap to doing something crazy like prime-indexed bits as a lens. The bizarrest part of my suggestion at all is that it isn't that bizarre and Haskell + lens would take it in stride, no problem.
Yeah, but how do you interact with that after you've instantiated it? The existing options are clumsy, and/or usually suffer from performance issues. Like, lenses are cool, but everyone conveniently seems to forget that updates with lenses involve making multiple copies of the structure over which you traverse. It's not entirely untenable - But it's not what I would call strong support for this sort of concept.
&gt; I don't buy the assumption that any program written in Rust will be faster than a program to do the same thing written in Haskell. Why not? Rust is explicitly designed to go fast safely. Haskell's able to go fast mostly because Haskell's design and position in academia enables a lot of neat compiler optimizations that you can get PhD students to implement. Even then, Rust's defaults are tuned for speed, while Haskell's are tuned for polymorphic FP.
Why is your monoid example better than the idea of the `Sum` and `Product` wrappers included in `Data.Monoid`? More generally, why are newtype wrappers more 'awkward' than your suggestion? Essentially, what you are doing in either case is supplying a context under which the operation should be executed - So, in both cases, you are saying 'For the purposes of this operation, please consider the `mappend` function to implement multiplication' Except you've also provided a whole new category of the type that you've wrapped, a category under which the monoidal operation has been qualified thusly. Unlike the category described by constraints, this category can be removed or added contextually. That seems like a much more powerful concept to me than making different flavors of class implementations, and it occupies just about as much space to author either way. 
Maybe I could get some of my idiotic Java friends to use it then. :P
What happens when you have functions that combine maps? union :: Map k v -&gt; Map k v -&gt; Map k v A runtime error if the `Ord` instances don't match? Maybe we want to have something in the type to make sure the `Ord` instances match up? But then we're back to where we are now—the `Ord` instance is uniquely identified with `Int` and if we want a different one, we have to wrap a newtype over `Int`.
I wanted to have a look at Ocaml but when I read the I need three stdlibs for basic string operations like lowercase, uppercase, split or utf-8 support, I purged every thought from my mind.
Yeah but then you have to work with those annoying functions that only give you data out of `Dynamic` if it's the right type... /sarcasm/
I think many people simply fail to correlate types with tests properly. Type systems *are* testing. They are just tests that you don't have to write! When you "wrestle" with the type checker to get something to compile, people complain. And then they go spend 3x that amount of time writing tests that cover almost the exact same thing...but worse.
I might start programming in a dynamically typed language when someone show me how I can write in a dynamically typed language effectively - having support as good as Haskell for: - Modeling my domain - Knowing I addressed all the cases when handling a value - Discovering with ease what function or value I can use in a given situation (a la hoogle) - Diving into a big new project and understanding what's going on very quickly - Refactoring with ease - Not having to think so god damn much I am aware of the problems of static typing. I just think the alternatives are a lot worse.
&gt; I am always surprised at how he feels the need to express his dislike or disagreement with static typing with such aggressive sniping and derision conversely, i feel like some reactions to this is are odd. do people 'identify with type systems' such that they're emotionally impacted by someone else disliking them? i mean, in some ways the 'strength' of his opinion makes me curious as to the history of how he acquired it, but it's otherwise immaterial and doesn't really bother me.
If a language makes it easier to build what you want, it can let you spend more time iterating on the design and the algorithms, and end up with something faster than a language designed for speed. This is one of the core reason why people use Haskell (or OCaml, or Scala...) instead of C++ for their projects. Rust may be nicer, safer, closer to functional languages than many other languages designed for speed, but the absolute reasoning you are giving remains a gross oversimplification that has no reason to hold in practice. I also think that you are mostly wrong about the dynamics of GHC's development -- I think that if you measured the implementation speedups obtained by code "implemented by PhD students", you would find that it is very small compared to the consistent improvements in compiler and runtime obtained by the work of the regular long-term contributors.
&gt; Knife handles don't prevent cutting yourself with the blade, therefore we shouldn't have knife handles well, in terms of analogy, i think the handle is for well, handling and operation. :P
Are you familiar with the `Control.Newtype` package? https://hackage.haskell.org/package/newtype-0.2/docs/Control-Newtype.html Your examples above could become: import Control.Newtype import Data.Monoid x :: Int x = Sum `op` mempty -- 0 y :: Int y = Product `op` memprty -- 1 And some more sophisticated variations: (under Sum.mappend) 3 4 -- 7 (under Product.mappend) 3 4 -- 12 
i think rich says a few things that are (unfortunately) fixated upon by people wanting to refute his position. however, i think he does make some larger, interesting points in his talk.
Not arguing for this specific case, but manpower and language used can be pretty related. One of the motivations Mozilla developed Rust was that C++ compiler in lacking guarantees requires more manpower to maintain. Google and Apple could afford it for Blink and Webkit, but Mozilla couldn't do it as well for Gecko. Pardon my Rust evangelism, but from Servo to Redox, Rust has shown some impressive promise on the manpower / productivity front. The guarantees from the compiler also relieve some of the fear of rookie mistakes while onboarding new developers, saving time from trivial code review. Which helps make Rust itself evolve quite fast, maybe even the fastest for now. It's still debatable whether this effort would result in a meaningful competition to the battle-tested GHC, but overall I think Rust can be a nice candidate in the roadmap of improving Haskell. 
I just realized the most hilarious thing about him railing about classes and ADTs. Clojure adopted both of those! https://clojure.org/reference/protocols
Yes, the built-in thing is basically necessary.
Yes, I'd longing to have safe and cheap ad-hoc (anonymous) records too. Similar to what Rawr and Bookkeeper libraries give us today but without the ridiculous worse-than-linear compile time slowdown and fields limited to 8 maximum. If row polymorphism gets us there the soonest, I'm all for it. 
Haha yeah. You can wrap it if you want to, pretty nicely: type PyMap = Map.Map String Dynamic (!) :: Typeable a =&gt; PyMap -&gt; String -&gt; a pyMap ! key = fromDyn (pyMap Map.! key) (?) :: Typeable a =&gt; PyMap -&gt; String -&gt; TypeRep pyMap ? key = typeRep (pyMap Map.! key) set :: Typeable a =&gt; String -&gt; a -&gt; PyMap -&gt; PyMap set key value = Map.insert key (toDyn value) And you can get all keys with `Map.keys`, check if a key exists with `Map.member`, and do all the mappy things you might want to do. You can use `?` to dispatch on type and `!` to get a value. And they fail at runtime if you ask for something that doesn't make sense, which is what you really wanted.
&gt; By restricting our past, we gain freedom in the future. \\o/
Possibly. I have an odd feeling that the projections are special cases of optimisation with particular inputs introduced as constant. It's nice that there's a name for that though
Yay! Cheap and easy with lots of runtime exceptions to go around. :D
While they are linked imo there is no unbiased way to compare productivity and when comparisons are made Haskell fares pretty well. If your primary goal is performance then rust is likely to beat Haskell. But the biggest advantage of writing the compiler in the input language is imo that you attract more people. That alone might be worth a bit of compiler performance (and might even out in the end). People familiar with Rust and interest in working on GHC are I assume a lot rarer than people familiar with Haskell and a Interest in GHC. Rewriting the runtime or parts of it in rust might be worthwhile in the future though. It's also hard to tell how much of the Rust compiler progress is due to resources and how much because of Rust. From an outsiders perspective llvm also seems to do very well and is still c++ based. 
`ApplicativeDo` was [changed in 8.2](https://phabricator.haskell.org/rGHC97aa533fd8bcbd8e42b3358e8a07423ad2a2d01f) to require `Monad` when there is a strict pattern match. Pattern matching a tuple is strict. Forcing it to be lazy with `~(a, b) &lt;-` solves the problem. https://ghc.haskell.org/trac/ghc/ticket/14252
It's not about emotional impact. It's about representing both sides of the argument fairly. I'm sure a lot of static typing people like Clojure and have written it in the past. We just wish Rich would avoid making claims about static typing that we know are _just not true_ with a lot of type systems we have today.
Didn't know about that, thanks! That works now :)
That you have to pass it explicitly.
Neither did I, it took about an hour of digging. Thanks for the puzzle.
That convinces me. Haskell is always right.
*.* I really appreciate your effort!
In my suggestion the `Int` doesnt have to be wrapped and unwrapped each time you want to use a different instance, That's all. I think most of the time wrapping and unwrapping is just a bit of a hassle. You normally don't have some profound realization that shines a new light on a type that justifies wrapping it in a `newtype` and making a bunch of instances to lots of classes. In most cases you just want to have two instances of a class. I think it's clearer to say "use multiplication" instead of treating an Int like a totally different thing, A `Sum Int`. You are mixing behavior and data that way, what is a `Sum Int`?, It's forced and awkward. Why do I have to treat it like a totally different type when I just want to multiply instead of add?
[Pinning can lead to some memory fragmentation](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/Pinned) it seems.
It would be nice to be able to do fold [True, True, False] :: All instead of fold [All True, All True, All False] like we can do with `Sum` and `Product`
I should add a note about that on the optparse-applicative Readme in the applicative do section.
I get folls who want to write non-empty types and I think that's cool, but doesn't this mean you need to write a lot of boilerplate code to get the normal instances we'd expect to have working? I'm not a Haskell saint, but I was under the impression foldable, traversable, etc are not automatically derivable, so these one-off non-empty types often push a lot of tedious work to your callers. 
you mean `fold{All} [True, True, False]`
Making it explicit seems like redundant work though. I mean I see the merit if your endpoints are Haskell and you can use that for safety. But can you give a practical advantage of specifying types for a binary protocol that's independent of a target language?
I'm a decent Haskeller, I think, feel free to PM me for questions or a quick tutorial to get started.
Is there any reason for not underscoring all of the unused parameters in the functions?
Great post, and if anyone wants to pursue this idea further, I highly recommend checking out [LiquidHaskell](https://ucsd-progsys.github.io/liquidhaskell-blog/).
Yeah, i love haskell for such hilarious phrases. "We have to destroy the past, otherwise all space would be consumed!"
I would say that lenses ([lens-aeson](http://hackage.haskell.org/package/lens-aeson-1.0.2/docs/Data-Aeson-Lens.html) and similar) are a perfectly reasonable equivalent--analogous to how purely functional data structures are fine substitutes for mutable data structures in the vast majority of situations.
[`#14252`](https://ghc.haskell.org/trac/ghc/ticket/14252) discusses adding a message explaining the situation (inspired by [`#14249`](https://ghc.haskell.org/trac/ghc/ticket/14249) a similar situation to OP)
And [#13875](https://ghc.haskell.org/trac/ghc/ticket/13875) describes why it needs to exist.
&gt;I get folls who want to write non-empty types and I think that's cool, but doesn't this mean you need to write a lot of boilerplate code to get the normal instances we'd expect to have working? What do you mean?
For example?
I filed [this ticket](https://ghc.haskell.org/trac/ghc/ticket/14252) a while back about this, as I found it the error message to be unhelpful in this situation. simonpj wanted to hear more feedback about this from users. If you use ApplicativeDo and think implementing the above ticket above would help, please give feedback saying so. If you have an alternative suggestion, filing a ticket with it would also help.
Dynamic or duck types languages have no place in mission critical settings.
Is `foldMap All [True, True, True]` close enough? The [beautiful aggregations](http://tech.frontrowed.com/2017/09/22/aggregations/), beautiful folds, etc., series are really informative for learning how to exploit newtype wrappers really nicely.
Having programmed in JavaScript and Python vs typescript he's flatly wrong. Look at the mess php is where it's loosey goosey type system mixes with it's crap libs.
There is an important duality, though. The power granted a function and the guarantees a function offers are in direct opposition with each other. I think the ever-raging argument about strong types and immutability often founders on the fact that so few of the participants understand that. Most programmers are raised to favor power but then have to deal with the consequences of writing code in languages that provide virtually no guarantees about anything. It's not even a "tradeoff", which often means in practice that perhaps if I give up a little bit of this I can have a lot of that... they're in _direct opposition_ to each other. The sum of power granted and guarantees given for a given function is a constant.
Haskell is indeed good, and that's the point. The goal of Rust is C++ performance with closer to Haskell guarantee. I said not in this case because compiler is already in Haskell. Rust runtime + Haskell compiler is like a dream :D 
`Functor`, `Foldable` and `Traversable` are all derivable with the `DeriveFunctor`, `DeriveFoldable` and `DeriveTraversable` extensions respectively.
Addressing your more general point: it *can* take a fair amount of boilerplate to define a type like `NonEmpty` and give it all the capabilities people expect of a list. The solution to that is to make `NonEmpty` as general as possible and use it throughout your entire codebase. As it happens, there's a pretty standard `Data.List.NonEmpty` type in [`semigroups`](https://hackage.haskell.org/package/semigroups-0.18.1); you can use it throughout your codebase. It does mean that creating a more restricted type for something pretty niche can carry a lot of overhead, so it may not be worth doing in some cases. But if you have an invariant that's going to be required in multiple places in your codebase, it's probably worth the overhead.
You don't need multi parameters. import Data.Ord (Down) import Data.Map ascending :: Map Int Int descending :: Map (Down Int) Int Etc.
I want to implement a GADT like this and then export it to user data MyApiHolder type c where MyApi1 :: (MyClass c) =&gt; (c -&gt; IO ()) -&gt; MyApiHolder Api1 c MyApi2 :: (!(MyClass c)) =&gt; (c -&gt; IO ()) -&gt; MyApiHolder Api2 c The idea is that the user of this should only be able to create MyApi2 if the c is not part of the MyClass... I think open type families will solve this, I will ask the user to specify type instance IsApi1 c = True for every type they want to be supported by 1, and this for everything else type instance IsApi1 c = False 
Generally speaking, you should consider the justification for using a new type on your values to be as simple as "it's there" ... even for something as simple as making sure you don't accidentally swap X and Y coordinates. I use lots but I would say I still don't use enough myself.
There shouldn't be unused parameters. What do you mean? 
To expand, newtypes literally exist precisely to provide computational context without changing representation - There is no meaning from using a newtype except to qualify the behaviors of functions in a way that is separate from constraints. If you find it awkward, that is certainly your option, but, this is what the feature is there for.
I'm guessing GP is asking if this works: newtype Order = NonEmpty Item deriving Traversable
You can do that. ``` {-# LANGUAGE AllowAmbiguousTypes, ScopedTypeVariables, TypeApplications #-} module Foo where import Data.Coerce foldMapWith :: forall a b c. (Monoid a, Coercible a b) =&gt; (c -&gt; b) -&gt; [c] -&gt; b foldMapWith = coerce (foldMap :: (c -&gt; a) -&gt; [c] -&gt; a) memptyWith :: forall a b. (Monoid a, Coercible a b) =&gt; b memptyWith = coerce (mempty :: a) mappendWith :: forall a b. (Monoid a, Coercible a b) =&gt; b -&gt; b -&gt; b mappendWith = coerce (mappend :: a -&gt; a -&gt; a) ``` Allows you do do this: ``` $ ghci Foo.hs GHCi, version 8.0.2: http://www.haskell.org/ghc/ :? for help [1 of 1] Compiling Foo ( Foo.hs, interpreted ) Ok, modules loaded: Foo. *Foo&gt; :set -XTypeApplications *Foo&gt; import Data.Monoid *Foo Data.Monoid&gt; memptyWith @(Sum Int) :: Int 0 *Foo Data.Monoid&gt; memptyWith @(Product Int) :: Int 1 *Foo Data.Monoid&gt; mappendWith @(Sum Int) 2 3 :: Int 5 *Foo Data.Monoid&gt; mappendWith @(Product Int) 2 3 :: Int 6 *Foo Data.Monoid&gt; foldMapWith @All odd [1,3,5] True *Foo Data.Monoid&gt; foldMapWith @Any odd [1,3,5] True *Foo Data.Monoid&gt; ```
I meant `head (x:xs) = x` vs `head (x:_) = x` since the `xs` is never needed. I could've written that better, sorry.
Unfortunately, the following generalization does not seem to work: with :: forall a b c. (Monoid a, Coercible b c) =&gt; (forall x. Monoid x =&gt; b) -&gt; c with f = coerce (f @a) (it type-checks, but when used I get a type error).
This blog post is a small but shiny gold nugget. It's mildly satisfying that you can use the compiler to communicate and enforce intent. It truly feels like a type system abstracts away correctness and documentation just like the maybe monad abstracts away the possibility of failure. Thanks for this great post!
Rich can be wrong about this, and I really think he is. Rich Hickey is famous and smart, but he's also very, very strongly opinionated here. Clojure is his longtime project and his livelihood as well. Which is not to say his criticisms should be dismissed out of hand, but it's probably okay to dismiss the obviously wrong ones. And sometimes he's very, very wrong about things. Besides things like Maybe SSN (a complaint that's odd because his preferred system cannot even EXPRESS the idea of an SSN, instead forcing you to validate it over and over again), he's got strange things to say about channel-style concurrency vs actor style concurrency as well. It leads him to provide the (overall, not very well-loved or understood in the Clojure community) async channel library. Transducers, too, went over largely like a lead balloon because despite his advice, they're *incredibly* hard to use without using something like core.typed or schema. And the backdrop of this is that essentially every major Clojure shop you can find talking at the conj has picked either schema or typed, and most recommend full enumeration of maps within those schemes! So even as Rich levies these complaints the community is creating enumerations around these records. With many years in Clojure and an increasing use of Purescript, I think what folks really want is the open nature of maps in our types. Purescript comes very close, and Elm's idea isn't bad. In the world of Javascript, a massive number of folks are converting to Flow and Typescript because they give some guarantees about the interfaces around these maps as well! I really think what we miss the ability to say, "Yes I can trust this open, indexed object has THESE contents in this code, but it is okay if it has more because I won't bother you." This is a common thread for nearly every successful variant of Smalltalk, a common thread for Javascript type systems, and a common thread for many data frame libraries and even data parsing libraries. 
You might be interested in this essay: http://slatestarcodex.com/2017/10/02/different-worlds/ It's useful to consider just how completely foreign different people's daily experiences can be from each other, and not in a facile way like "live in different countries" but even two people in the same workplace.
I think the question is: what *fundamentally* is the difference between the paradigm you're proposing and haskell's story for keeping the language pure? `readFile` is a pure function. Or maybe it would be easier to compare it to pre-IO-monad haskell, where the program was a pure function folding over a stream of inputs or something.
Well, to start you'd get a namespacing issue, since GHC defines two _different_ `A`s and `B`s. Trying to implement this is going against the grain of GHC's design choices. It _could_ be implemented, in principle, in various ways. The simplest of those would be the ways Agda and Idris do it, which is basically to treat constructors specially and resolve them to the type determined by context. Unfortunately, this would not help you get real subtype-y behavior like (f :: MyType -&gt; Bool) (B :: MySubtype) (that would still be a type error) It is possible to get most of the goodness of subtyping with row types. I hear that Elm and Purescript do something like this, but I'm not personally familiar. MLPolyR is a research project that had some good ideas in this regard. TL;DR: That's just not how data types work in Haskell. Every value has one and only one type, and constructors have the type of their data type.
Yeah, the most common way to do this would be to define literals as a type and then to have a `Literal` constructor in your `Prop` type. This works fine as long as you have a strict hierarchy, and you can even use type classes or pattern synonyms to make it more convenient. When this breaks down is in the rare case that you have something analogous to a diamond problem.
&gt;It really comes off as insecure and defensive Especially the part about `Maybe String`. He didn't know what he was talking about. I really wish programmers got into better habits around things like this. 
I think it actually makes sense in your domain to have separate constructors for "proposition negation" and for "negated literals", which would look like data OpF a = Or a a | And a a | Imply a a | Not a deriving Functor data LitF a = Const Bool | Var Char | NegVar Char deriving Functor type Lit = FixF LitF type Prop = FixF (LitF :+: PropF) 
&gt; do people 'identify with type systems' such that they're emotionally impacted by someone else disliking them? Maybe I just don't like listening to people bloviate about a topic they don't know anything about? I wish more programmers would stop doing this since it kind of drags down the whole discourse.
I think this is a really neat and exciting language feature, even if I don't totally understand the distinction between this and other paradigms.
&gt; His assertions about things being not structured and open ended really sound to me like assertions not about the whole world, but about the particular corner of the world that he has occupied. Plus it misses a crucial point: a programmer's job is to model the real world using mathematics or other abstractions. The more you drop the mathematical side of things, the more unwieldy your code becomes, until at a certain point it stops modeling the real world anyhow. If a `Maybe String` doesn't represent a social security number, an exception sure as hell doesn't either.
Forget inference, I'd just settle for working `~`. Asking for sorted/normalized versions to be equal doesn't work because of restrictions on what you can do with type families. 
To be fair this would alleviate 65% of stressors in my life.
"Faster" performance or "faster" development? I'm not sure I'd trade the latter for the former.
&gt; He is attempting to discredit types because they do not catch all errors, or the toughest errors; that's fine, I don't agree with people that would say types do that for you. Also, dependent types are really exciting and they're getting better. Lots of development there in Idris and elsewhere.
I haven't worked on this in a while, but it might be what you're looking for. Consider [this module](https://github.com/mrkgnao/noether/blob/master/library/Noether/Algebra/Single/Monoid.hs#L20). I'll paste it here in its entirety: -- | "Evidence"/"strategy" for a monoid structure on some type. data MonoidE = Monoid_Semigroup_Neutral SemigroupE NeutralE -- ^ it's a semigroup and has a neutral element | MonoidNamed Symbol MonoidE -- ^ monoid evidence decorated with some kind of interesting comment -- | An implementation of a monoid structure, distinguished by the [s]trategy parameter class MonoidK (op :: k) a (s :: MonoidE) -- | If a is a semigroup under the operation op with semigroup evidence zs, and similarly -- for a neutral element with evidence zn, we can always combine these to produce a -- semigroup structure with evidence tracked by the Monoid_Semigroup_Neutral constructor. instance (SemigroupK op a zs, NeutralK op a zn) =&gt; MonoidK op a (Monoid_Semigroup_Neutral zs zn) -- Adding a comment to a valid piece of evidence is always fine. instance (KnownSymbol sym, MonoidK op a s) =&gt; MonoidK op a (MonoidNamed sym s) -- The preferred/"primary" strategy. type family MonoidS (op :: k) (a :: Type) = (r :: MonoidE) -- The canonical MonoidK instance, which uses the primary strategy. type MonoidC op a = MonoidK op a (MonoidS op a) More complicated examples: https://github.com/mrkgnao/noether/blob/master/library/Noether/Algebra/Linear/Strategies.hs https://github.com/mrkgnao/noether/blob/master/library/Noether/Algebra/Actions/Strategies.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mrkgnao/noether/.../**Strategies.hs** (master → 5d4270b)](https://github.com/mrkgnao/noether/blob/5d4270bda4be46ef9a7099c5180918fced1b3952/library/Noether/Algebra/Actions/Strategies.hs) * [mrkgnao/noether/.../**Strategies.hs** (master → 5d4270b)](https://github.com/mrkgnao/noether/blob/5d4270bda4be46ef9a7099c5180918fced1b3952/library/Noether/Algebra/Linear/Strategies.hs) * [mrkgnao/noether/.../**Monoid.hs#L20** (master → 5d4270b)](https://github.com/mrkgnao/noether/blob/5d4270bda4be46ef9a7099c5180918fced1b3952/library/Noether/Algebra/Single/Monoid.hs#L20) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; at least some knowledgeable people are touting "dynamic" Everyone I respect that has said good things about dynamic languages has usually held up Erlang or the like as an example. You need to be able to introspect in a dynamically typed language, otherwise it's just begging for errors.
you have to be careful when bringing up dependent types because the use-case and best-practices for mainstream development is still experimental being figured out; they will use that confusion against you.
&gt; I really think what we miss the ability to say, "Yes I can trust this open, indexed object has THESE contents in this code, but it is okay if it has more because I won't bother you." the style of polymorphism found in java's subtyping technically provides this, i think (or something similar).
&gt; however, i think he does make some larger, interesting points in his talk that are still worth considering despite the low hanging fruit. I mean I'd have an easier time considering them if he didn't say things that were obviously wrong, like the suggestion that people model social security numbers using `Maybe String`. A lot of these questions can be subjective, so I'd rather listen to someone who's really invested in both and has a preference.
I feel really fortunate to have bought an Acer C710. It sports a genuine Intel x86 cpu, which makes fetching binaries much eaiser. It actually has memory expansion slots. I've added 8 GB to the existing 2 GB bringing it upto 10. I'll may replace the 2 GB with 8 GB later to bring it upto 16 GB. I've replaced the bios with SeaBIOS, so not it boots as an ordinary Intel laptop. I've purchased an aftermarket expanded battery. I've installed NixOS. I also copied the kernel patch to replicated the behaviour of sysreq with the f10. Everytime a kernel update happens in NIXOS my patch is automatically applied and the custom kernel is rebuilt transparently.
In as much as Type Classes also do, I suppose that's true. You absolutely could make a typeclass for each data object and explain how to extract what you want and you've done it. What you sacrifice with Clojure's approach is composable lenses. While you can compose readers (folks love to give demo's with fn-able keys and the -&gt; macro combinator), freely composing and nesting _writers_ (especially across traversals) would be deeply appreciated but is really, really hard to do safely in Clojure due to a lack of agreement and discipline around data structure libraries.
I'd much rather here someone criticize types than go on about how much easier it makes maintenance. Yeah, he misses the point, but he's also got a point. Composability, resilience, and adaptability are important. Not everything is purely mathematical, logical, or formally provable. And the world of programming, almost everything isn't.
I agree with the spirit of this, and wanted to second the sad sigh about dealing with set-like relations mentioned last. Troubles similar to that are probably my most common stumbling block with using these constructions in Haskell. It is most frustrating because it feels like the least interesting aspect of things! One day someone will write a plugin to sort such things out and the sun will shine.
A decade of Java and C++ seems to have turned him off from static typing entirely. But more importantly, it's obvious that he has a dynamic personality. He prefers situational and contextual problems, which simply don't lend well to static systems. He wants his programs to live and grow and not have to be refactored or recompiled each time they do so. You dismissing him for being subjective is a dismissal of a fundamental aspect of his personality. Clojure was born out of his personal frustrations and personal joys with programming. Not because of the latest report that correlates frequency of runtime errors with type systems. And there's no way you're going to be able to convince him to appreciate something like said report if you can't even appreciate what he advantages of dynamic typing mean to him.
Well, I was just mirroring your language, if it wasn't obvious. And if it wasn't, I don't think you "get" me.
&gt; You dismissing him for being subjective is a dismissal of a fundamental aspect of his personality I'm not dismissing him for being subjective, I'm suggesting that we value human judgment, but from people who know what they're talking about on both sides of the aisle. 
I’ve been thinking about drawing up a proposal &amp; prototype of this, after I’m done with my `InlineBindings` proposal. We could generalise the `OverloadedStrings` : `IsString` :: `OverloadedLists` : `IsList` machinery to work for any type. I guess it would look something like this: instance Is Bool All where from :: Bool -&gt; All from = All And then `{-# LANGUAGE Overloaded Bool #-}` would replace instances of `True` and `False` with `from True` and `from False`, respectively. This extension could subsume `fromString` &amp; `fromList`, and even `fromInteger` by making `{-# LANGUAGE Overloaded Num #-}` the default. In order to be usable, though, I think it would also depend on generalising the defaulting rules to allow any class, e.g.: default Num (Integer, Double) default IsString (String, Text) default Is Bool (Bool) … Extending the defaulting machinery is something that GHC has wanted for a while; I think the main issue is that no one has come up with a solid proposal or implementation for it. 
Are you implying that his preference comes from ignorance of static typing? Or that his preference is invalid until he can fully appreciate the advantages of static typing? He tried static, he tried dynamic. He pursued the one that most naturally fit him both as a programmer and as a *human*. Sure, he is sorely lacking perspective on why others take a different path, but why aren't you interested in hearing why he so eagerly pursued his?
&gt; Are you implying that his preference comes from ignorance of static typing? It's pretty obvious he has no idea what he's talking about judging from the video.
And you have no idea what I'm asking you because you keep missing the point. He doesn't have to be an expect in static typing to know why he doesn't like it, and whether he's using the best or worst in class isn't going to make a difference if it's based on principle, on *his* principle. And *every* principle he appeals to in this talk screams dynamic, whether he's talking about it explicitly or not.
Reddit and cell networks have a suspiciously large interaction.
If you have a Haskell function, like `foo :: A -&gt; B`, then what inference rule do you add to your prolog program? You could write `b :- a.`, but the `foo` part is lost. So, then I guess you write it like this: `b :- foo(a)` Let's add a few things to our database like this: * `b :- foo(a).` * `c :- bar(b).` * `a.` Now let's do a query on [SWISH](https://swish.swi-prolog.org/): ?- c. It says: procedure `bar(A)' does not exist Okay, so let's add these rules: * `foo(_).` * `bar(_).` Now it simply says `true` when we retry our query of `c.`. I don't know if there is a way to get it to print the derivation, but it seems like a different encoding is required. I'm not sure what it would be.
Okay, as soon as I posted that I realized a better encoding: app(foo,a,b). app(bar,b,c). app(baz,c,d). app(quux,d,e). app(Z,A,C) :- app(X,A,B), app(Y,B,C), Z = [X|Y]. When you discover a new function between some types, you just add another `app` rule and the last case should just stay the same. Then you can query it: ?- app(X,a,e). X = [foo, bar, baz|quux] 
ML Functors generate a new map type for each ordering used. That would be fine.
Need some more info: * The SQL-based validation rules are triggered during new data insertion, or are they for a one-time activity to detect bad old-data? * Whatever you're doing in Rspec/Rails (i.e. known good input =&gt; expected result, known bad input =&gt; expected result), can be replicated in Haskell. Any reason why you don't want to do that? (or is the post missing some other context).
What you want are polymorphic variants. Ocaml supports them, and we do something similar in PureScript at the library level using its row system.
Ah yes! The change which introduced this bug: [#14105](https://ghc.haskell.org/trac/ghc/ticket/14105). How come is `isStrictPattern` a partial function? Are `CoPat` and `QuasiQuotePat` expected to not exist at that stage of compilation? 
for it's faults, this is one thing nice about java's type system. it is weird how rich talks about some of these things. especially given how amazing some of his other talks have been.
Oh I see what you mean! That would probably make it clearer. 
https://www.youtube.com/watch?v=CRMcSAgoabw
&gt;[**Meeseeks can do ! [0:04]**](http://youtu.be/CRMcSAgoabw) &gt; [*^Hugi ^R*](https://www.youtube.com/channel/UCVROfFFhLeqNy1loLfKjDTQ) ^in ^People ^&amp; ^Blogs &gt;*^65,055 ^views ^since ^Jun ^2016* [^bot ^info](/r/youtubefactsbot/wiki/index)
Wait, what about ``` data MyNestedF k a l = MyMap k a | Leaf l type MyNested = Fix MyNestedF ``` or something along that lines? That’s every feature of an arbitrarily nested data structure, but in a principled way. See http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/
Yep, you got it! Something just like that. I imagine it as the programming equivalent to the Prolog example of airline flights going between cities: flight(AA101, houston, nyc). flight(AA991, nyc, miami). But instead of plotting courses between cities, we're routing between data types: function(foo, a, b). function(bar, b, c).
I always feel awkward when a case statement doesn't match every constructor of the given type: even when I'm really sure that adding an extra pattern would just mean unreachable code.
&gt; Not everything is purely . And in the world of programming, almost everything *isn't* [mathematical, logical, or formally provable]. This is wrong. Computer Science is a mathematical discipline. Everything in the realm of computing and software has a basis in mathematics. Do you actually believe that the process of writing code is inherently illogical? 
If you watch George's talk, he presents an example where multiple typeclasses instances can cause silly behaviour. These issues are still present in the system you're describing.
&gt; My Prolog's a little rusty, I forget what the vertical bar does. Yeah, I was being sort of lazy. If you change the last case of `app` to this: app(Z,A,C) :- app(X,A,B), app(Y,B,C), list((Y,B),Yl), Z = [X|Yl]. list(X,[X]) :- atomic(X). list(X,X). The vertical bar goes away. I think it's basically like a dotted pair in lisp. There may be better ways to avoid it that I'm unaware of.
Of course not. But it's not *purely* logical, like I already said. Not everything is a logical problem, even in programming. And a program can be both logical and totally incorrect. Also, something tells me that when you say "everything in the realm of computer science and software" you're leaving out a very crucial part. The software that interacts with humans, or even with other software. How do you type check a bad user experience? What's the mathematical foundation of variable names? I love the logic and mathematics in programming. I really do. But they are not programming. Haskellers pride themselves in being able to directly implement higher order math. And this is great. But not when it comes at the expense of understanding what it means for *programming* and not just math as programming. There's a lot more to it, and until we have math to describe the rest of it, it's wrong to equate them.
https://github.com/Marwes/haskell-compiler
It's idiomatic in Haskell to align code on things like equals signs. This cannot be done with just tabs. Spaces are certainly the standard in Haskell and you'll find every Haskell codebase (at least I have to see otherwise) uses them.
Type safety is orthogonal to functional programming. FP is about combinations of elements in formulas that keep invariants. Type safety is about making sure that the values introduced in the formulas are consistent. Both together makes programming error free, since once a combinator works, i works in every formula where that combinator is used (thanks to FP) and with all the data entered (thanks to type safety). But if you forget one or the other, normally the first, when you fail to analyse the problem and find the underlying structure that can be expressed in the appropriate combinatores, then you have nightmarish frameworks made using type safety as a brute force way, with hundreds of types. A methaphor is worth a thounsand words: FP programming with type safety is like a lego made of a few different pieces that match nicely together and there can be no wrong matches. A type safe program without FP is like a puzzle made of hundred of different pieces which matches as well, but you have to look in the mess a lot to slowly find matches and compose what you are looking for.
That would be sweet. I've thought about a tool for finding better routes in the code you've already written, by analyzing each expression as a route between points. With "better" measured by Kolmogorov complexity, or even known benchmarks. I'd also like visualizations of possible transformations, with path highlighting on a graph. The same graph could be used for visualizing call graphs, so you can actually see how data travels through it. This would be particularly useful if dynamic routing was supporting, based in input and environment.
&gt; direct opposition But is it truly a 1:1, linear, 0-sum-game? Or can it be that if we could quantify "power" and "safety" we may find that certain decisions yield some more of one than what we lose of the other? If you consider it to be a 1:1 relationship, is this conjecture on your part? Or has this, or something close to this ever been proven? Because if my first paragraph would turn out to be true, then we could find sweet spots, which give the most power and safety at the same time, and any divergence from that would decrease their sum; giving a lesser amount of one than what we lose of the other.
And also, couldn't you just have a function `fromNonEmpty :: NonEmpty a -&gt; [a]` and `fromNonZero :: NonZero -&gt; Int` (or perhaps even more general `fromNonZero :: Num a =&gt; NonZero -&gt; a`) with which you could access the functions specialized to use the non-restricted type? This way you are still safe.
If you're looking for it too, here is the repo: https://github.com/dagit/rust-prolog
Prolog because I wanted to learn it better and because the PL Zoo had a nice simple interpreter I could copy. Just this week I had some more time to work on it and I changed the search procedure to one that is sound and complete for first order logic (although I discovered the completeness is only as good as your database of facts). If you looked at it yesterday, I probably had the sound stuff pushed but not all the changes for completeness.
Sweet,I look forward to the evolution of your computational geometry library. 
This. Shortly after my last experience I decided that life was too short to write something more than once and that type-level abstractions (dependent where possible) were the best existing solution. 
Woohoo! A constructive proof of existence. :) Thanks
Something something wholesome! :)
&gt; not noticing that either having a string or not is exactly what `Maybe String` says lol
You could use a Hoogle database to get the list of functions available. Someone probably has a dump somewhere. You'd probably want to generate rules for type class polymorphic things. Like `show :: Show a =&gt; a` is a rule that for example if there is an instance of `Show Int`, then `show (n :: Int)` is a valid match. Your Data.Time example is one example. If you want to convert a `NominalDiffTime` to seconds you have to use `fromIntegral` which is a type-class polymorphic function.
There was a group at the Vrije Universiteit Brussel doing similar work about a decade and a half ago concentrating on OO languages under the banner "declarative meta-programming" They produced SOUL a system for querying and transforming Smalltalk (principal author Roel Wuyts) and Tyruba a system for querying and transforming Java (prinicipal author Kris de Volder). It was nice work, though I don't think anyone did similar for Haskell. Oege de Moor who contributed to the early development of Haskell later built a similar code querying system (maybe called CodeQuest?) which was developed to commercial scale. I think the code querying engine was more general than Tyruba or SOUL (i.e. it could target different languages) but I don't know it has been used for Haskell or other functional languages.
It sounds like the IO/pure distinction looking at what he's saying from a Haskell lens. physical/protocol = monads/effects/real world, Maybe SSN, IO Int, etc logical = pure functions, SSN, etc So what's he's complaining about is that us type weirdos have to deal with these domain clashes by using Maybes for everything, which is simply not true. If anything we have better tools for dealing with these scenarios.
How to get `"hello world!"` from `("hello", " ", "world", "!")`
If you're fine with Clojure's level of type-safety: ``` cljLookup :: k -&gt; Map k () -&gt; v cljLookup k m = unsafeCoerce $ lookup k m ```
So this is a funny one in that you're probably asking an orthogonal question. You've wrapped your strings parenthetically, which means that this is a tuple or product type. Tuples can have different types so things like (4, "four") are ok. This could have type (Int, String) neglecting the fact that literals can be overloaded. What you probably wanted instead is a list, and the function `concat` concat ["hello", " ", "world", "!"] 
intercalate from Data.List - why do you have a tuple in the first place?
/u/yogthos :P
Certainly there are cases in which this is a good solution. I would say that wrapping a `Float` in a `X` or `Y` to differentiate coordinates is actually justified, because they are totally different things not to be confused with each other. But I don'y see a justification for using `Sum` and `Product` that way. There is usually no reason to differentiate two numbers like this, since you usually perform mixed additions and multiplications, then it becomes a hassle. `newtypes` are good when you don't want to mix more than one often. In other cases I think my suggestion is nicer
Does that mean you lose pattern-matching? If you write data Foo where fooInt :: Lens' Foo Int data Bar where barFoo :: Lens' Bar Foo barInt :: Lens' Bar Int Is the compiler is free to choose the implementation, is it allowed to define `barInt = barFoo . fooInt`? Or must it give to give `Bar` its own Int field? If it must, isn't that just a more verbose way of writing `data Bar = Bar Foo Int`? What's the advantage?
Lenses are not the same as projections; the projections correspond to the getters of lenses. Lenses also have the option of replacing the values in the pair. The dual point holds for prisms. In Haskell, products are just defined based their projections and constructors, though, just like in category theory. There's no access to the low-level bit representation, only to the higher-level constructions. So it already works like this.
"Advanced" types are probably pretty mysterious to those who don't really use advanced type systems. There was a time when javaists argued against Optional/Maybe and now they've it in the standard library.
This is exactly what I thought when I heard his arguments. His arguments are kind of about statically typed languages being too rigid to deal with "data" in many real-world scenarios, because it changes and is "fluid" and so on. However, his arguments are somewhat paradoxical. If you don't know enough about the shape of your data, then you can't write a program that deals with it, dynamic or otherwise. If you do know enough, then you can also write a statically typed program that deals with it and takes into account the fluidity. A small example of what I mean: Say I have some JSON data I need to deal with. Maybe some part of the JSON data is an array of things whose shape can vary to some extent. Let's say it's an array of JSON objects that I use to create forms on a website. Dynamic languages like clojure make it easy to deal with stuff like this, since I can just iterate through all the keys and then write some code that converts these structures into forms. But obviously, how this data can vary in shape isn't "unbounded." If it was, then I couldn't take the shape variations into account in a dynamic language either. But the point is, it is possible and even easy to create a Haskell data type that contains a list of some other data type, whose `FromJSON` instance takes all the different shapes into account. You might be giving up on a certain amount of static safety. You'll have something like a `Form` data type, containing maybe `formType :: FormType`, but obviously your program can't take varying form types into account at compile time when the source of truth for these different types is an external API whose contents you only know when you run the program.
Wanted to know the inverse of `splitAt`
1. I actually watched a talk by John De Goes where he kinds of hints at this, and he says that we can do type safe pattern matching using this approach. https://youtu.be/buQNgW-voAg?t=26m10s
Understood. You can know your data, but you can never know enough because you never can predict the future. You can never know what would be added or how in the future your data will be analized. Look at this, Minute 17 https://skillsmatter.com/skillscasts/10628-data-science-in-haskell-solutions-and-challenges row names become data, and data become row names. This is not an infrequent case in modern data analysis, that is everywhere. Do you wonder why an untyped language Python is used a lot and Haskell don't? That hasn't to be the case. What is wrong in haskell is the big Hammer of type safety that is, by cultural bias, used for everything even in cases where it adds accidental complexity and makes some tasks impossible. 
I know this is a rather broad question. I've started to lear haskell a couple of weeks ago and I'm wondering how hard is it to write programs that does alot of impure stuff. I know that code tends to be much shorter and easier to reason about when its pure, but how hard (compared to a mainstream language) does it get with all those Monads/MonadTransformers and stuff, when there is alot of impure code (e.g. for an ordinary Desktop program) needed.
I don't agree that sums are defined by their injections. It's really the universal property that is important: defining a function from a sum is exactly the same as defining two functions (from either term). This is pattern matching. So Haskell already uses the categorical definition. Same for products. 
2. I was thinking you write your code just like you do now, but instead of introducing a new type into the environment, it introduces a new class. So `data Bar = Bar Foo Int` will declare a class `class (Foo f, Int i) =&gt; Bar b where _1 :: Lens b f; _2 :: Lens b i`. In essence we are replacing data types with their "classy lenses and prisms".. We get better composition for example: load :: MonadState Int m =&gt; m () save :: MonadState Boolean m =&gt; m () loadAndSave = load &gt;&gt; save -- compile error load' :: (Int i, MonadError i m) =&gt; m () save' :: (Boolean b, MonadError b m) =&gt; m loadAndSave' :: (Int a, Boolean a, MonadError a m) =&gt; m () loadAndSave' = load' &gt;&gt; save' {- somewhere in prelude: instance Pair a b c =&gt; a c where ... instance Pair a b c =&gt; b c where ... -} init :: Pair Boolean Int c =&gt; c init = (True,7) result :: Unit a =&gt; a result = runState init loadAndSave
`uncurry (++)` for `splitAt n` or `(,) &lt;$&gt; length . fst &lt;*&gt; uncurry (++)`.
Upvoted for "big hammer of type safety that is used for everything.."
Linear algebra: I need matrix inversion or preferably a numerically stable least-squares solution to an over-identified linear system. So far I have found Data.Matrix and Hmatrix. Are there others I should consider? Hmatrix has my second need, but it uses CLAPAC, which is a C library. Would this cause significant overhead per call? My matrices are fairly small, but I will need to call the routine often.
Would it be possible to force all polymorphic usage of typeclasses to take a name and default to the default instance at use sites? As in insert :: Ord{o} k =&gt; k -&gt; Map {o} k v -&gt; Map {o} k v I guess this doesn't work because one could define Map without indexing it with the instance, though.
&gt; [..] I'm wondering how hard is it to write programs that does alot of impure stuff. That depends on what you actually want to do. What's great about Haskell is that once you learn the basic stuff you can repeatedly use the same abstractions again and again. Some of those abstractions could scare you at first but in the long run they make your life much easier than you could think. &gt; [..] reason about [..] how hard does [..] it get with all those Monads/MonadTransformers and stuff, when there is alot of impure code needed. Usually, you can break down code pretty good and use abstractions which means you can reason about those pieces on their own. However, the more specific and less general you write your code, the more effects can happen. The easiest way is to encode your functionality in a type class interface, for example: class Monad m =&gt; MonadFoo m where foo :: String -&gt; m () bar :: Int -&gt; m Baz This way you can write generic code and easily test it independently of the type it gets *instantiated* to. This is common (mostly for convenience) with monad transformer libraries (e.g. `MonadReader`, `MonadWriter` or `MonadState`) and of course there are free monads and effect libraries. 
&gt; a product is a thing which has a a right and left projections, that sounds a lot like a **pair** of lenses to me That sounds a bit cyclic to me :)
Author here. For a few years now I've been working on my [dejafu](https://hackage.haskell.org/package/dejafu) library for testing concurrent Haskell and, after getting some interest, I've decided to write a tutorial series on implementing something similar yourself. This post introduces some basic ideas, implements a few thread and MVar operations, and a simple random scheduler. The idea is to build up the concurrency implementation over the next two or three posts, then dive into the testing logic: in particular, how we can do better than just random scheduling. Code is on github, with one tag per post: https://github.com/barrucadu/minifu/tree/post-01
Haha, well... it's a meta pair, the language itself has no knowledge of this pair and it can't mess around with it, it's at the level of the compiler.
I've listened twice now and I still simply don't get what he's on about "composability". After years and founding a company on Clojure, I got exhausted by the constant back and triple checking I had to do any time I wanted to compose things. The sparse nature of data is wonderful to reflect into data structures. It made a wonderful basis for a hierarchical cache at Level, for example. But when it comes to composing functions or behavior, clojure is bad at it. The way we solve this is Clojure's (underpowered and unloved) macro tooling to create a scoped DSL with a new composition primitive. It's also the case that an awful lot of reputable clonure shops use typed or schema and run tests with validation on because sufficiently large Clojure systems (particularly those using EDN or JSON as a transport format) really suffer without some notion of expected types. 
That doesn't work because `Order` has kind `*` but `Traversable` requires kind `* -&gt; *`.
&gt; He didn't know what he was talking about. Are you sure about that? :)
&gt; Plus it misses a crucial point: a programmer's job is to model the real world using mathematics or other abstractions. Nope, a programmer's job is to make stuff happen, to solve problems that matter in the real world.
How do static types help this? Do you not still have to travel around to every network call and re-jigger the types and test the interactions?
There is a saying that "Haskell is the best imperative language", and I agree. The main reason is that `IO` actions are first-class values, i.e. you can put them in data structures, define your own control flow operations, and so on. It simplifies a lot of stuff that would be boring in other languages.
Even if the *program* performs a lot of side-effects, it doesn't mean that your *code* has to be full of side-effects! For example, suppose you're writing a "guess the number" game, in which the user guesses a number between 1 and 100 and the computer tells the user whether the actual number is higher or lower than the guess. The program is full of side-effects: it uses randomness to pick a random number, it reads a guess from the console, and it prints the hint to the console. So you might write code which is full of effects, like this: import Control.Monad import System.IO import System.Random import Text.Read askGuess :: IO Int askGuess = do putStr "&gt; " hFlush stdout line &lt;- getLine case readMaybe line of Just n -&gt; return n Nothing -&gt; do putStrLn $ show line ++ " is not a number." askGuess main :: IO () main = forever playGame playGame :: IO () playGame = do number &lt;- randomRIO (1,100) putStrLn "I have picked a number between 1 and 100, can you guess which one?" keepGuessing number putStrLn "Let's play again." keepGuessing :: Int -&gt; IO () keepGuessing number = do guess &lt;- askGuess if guess == number then putStrLn "That's the right number, you win!" else do putStrLn (if guess &lt; number then "Higher." else "Lower.") keepGuessing number But that's just one way to write that program! If you find pure code easier, and most of us do, you can separate your code into a pure game logic part and an IO part. -- game logic part data Result = Correct | Incorrect String deriving (Eq, Show) data Game = Game { evaluateGuess :: Int -&gt; Result } guessingGame :: Int -&gt; Game guessingGame number = Game $ \guess -&gt; if guess == number then Correct else Incorrect (if guess &lt; number then "Higher." else "Lower.") data Decorated a = Decorated { intro :: [String] , meat :: a , outro :: [String] } decoratedGuessingGame :: Int -&gt; Decorated Game decoratedGuessingGame number = Decorated { intro = [ "I have picked a number between 1 and 100, can you guess which one?" ] , meat = guessingGame number , outro = [ "That's the right number, you win!" , "Let's play again." ] } -- IO part withDecorations :: (a -&gt; IO ()) -&gt; Decorated a -&gt; IO () withDecorations dealWithTheMeat decoratedMeat = do mapM_ putStrLn (intro decoratedMeat) dealWithTheMeat (meat decoratedMeat) mapM_ putStrLn (outro decoratedMeat) playGame :: Game -&gt; IO () playGame game = do guess &lt;- askGuess case evaluateGuess game guess of Correct -&gt; return () Incorrect msg -&gt; do putStrLn msg playGame game main :: IO () main = forever $ do number &lt;- randomRIO (1,100) withDecorations playGame (decoratedGuessingGame number) So instead of implementing the game directly, I have defined a Game datatype which could describe different guessing games, all we need is a way to determine whether the guess is correct or not, and which message to display of the guess isn't correct. I then defined my guessing game as a pure function `guessingGame`, and separately, I have defined `playGame`, an IO function which can let the user play any such game. I then realized that I wanted to give a bit more feedback to the user at the beginning and end of the game, so I came up with a new abstraction, "Decorated", which can add messages to the beginning and end of any interaction, not necessarily a game. The difficulty lies in finding the right abstractions. Once you do, you have a nice boundary between your business logic and your IO code, which allows you to write less of that error-prone IO code and more of that pure business logic code, without intertwining them like in my first implementation. Using abstractions which separate the pure parts from the IO parts has other bonus advantages, too! The pure fragment can now be tested via unit tests, e.g. we can check that `evaluateGuess (guessingGame 50) 70 == Incorrect "Lower."`. With the IO version, it would be much harder because we would have to find a way to automate a test user typing in "70", and a way to capture the message after it has been printed. Another advantage is reusability: it would now be quite easy to write a similar guessing game, e.g. guess a four digit number and the computer tells you how many of those four digits are correct. Of course, it would not be nearly as easy to make a 2D guessing game in which you pick a 2D coordinate and the computer tells you hints like `"north-east."`. There is an art to guessing what kind of changes you'll need in the future and picking the right abstraction which will make it easy to make that change when the boss reveals it.
Hello, when you are dealing with `errno`, you are usually working with very low level functions, typically system calls, or C function that directly wrap system calls and propagate `errno` out. (Also important to mention here that such wrappers are in fact usually very thin, specifically they often don't do any I/O beyond the syscall they wrap, as even a simple debug `printf()` after the syscall in question may overwrite the `errno` value.) As a result of this, you typically know exactly what the foreign call returns, in which cases it sets `errno` and to what (e.g. Linux man pages document it very precisely), so it should usually not be difficult to pick the right `throwIf...` function, and you shouldn't have to do it too often because there aren't that many functions that set `errno`. I doubt that preprocessors would do much to help you to pick the right `throwIf...` function, after all they can't know what the semantics of the function you're calling are. Though if you had some specific example in mind where they could help, we could look at that in more detail. You might already be aware of that but since it's so important I'll mention it anyway: You typically want the `...Retry` family of functions (most commonly `throwErrnoIfMinus1Retry`) because almost all syscalls return `-1` and set `errno = EINTR` when they get interrupted by a signal, in which case you want to return into Haskell (so that e.g. exceptions can be processed; if you don't, you get bugs like [this](https://ghc.haskell.org/trac/ghc/ticket/8684)) and then retry the operation. GHC's runtime sends signals all the time so code that doesn't handle EINTR will usually be wrong.
A warning: Errno is on thread local storage and can be clobbered by other Haskell threads r running concurrently with your ffi.
Nice talk, but church encoding and prisms are very different! In particular, the church encoding gives you a way to pattern-match by writing an implementation for both nil and cons, and we know that this is sufficient to cover all the cases. But if you have a `Prism' list ()` and a `Prism' list (element, list)`, you can only check whether your `list` value matches one of those two cases, you don't have enough information to know whether the concrete type chosen for `list` will have only those two cases, or if it will have extra cases. data NotList a = Nil | Cons a (NotList a) | NotCons (NotList a) (NotList a) _Nil :: Prism' (NotList a) () _Cons :: Prism' (NotList a) (a, NotList a)
Cool. Very well written. Warning: self-promotion ahead. You might be interested in the paper [Generative Geometric Kernel](http://www.cas.mcmaster.ca/~carette/publications/pepm28p-carette.pdf). Many of the same ideas are used, but carefully staged so as to specialized nicely when the dimension is known statically. We did not go full-polytope -- but now I understand that we probably should have!
&gt; 2. is just an artifact of GHC being a &gt;25 year old code base I'm not convinced. In Haskell you have to _design_ programs for reasonably efficient memory usage. Writing the same code again today without such explicit design probably would end up in the same problem. In Rust and C reasoning about memory usage is designed into the language, and easy to debug, in Haskell it is not really.
If I write f :: Bar b =&gt; b -&gt; Int f = set (_1 . fooInt) 42 &gt;&gt;&gt; set _2 43 &gt;&gt;&gt; get (_1 . fooInt) then I need to worry about whether I might accidentally return 43 instead of 42, because `_1 . fooInt` and `_2` might point to the same Int. Aliasing issues from OOP are back to haunt us! Optics make it easier to write imperative code in Haskell, and they are really good at what they do, but that doesn't mean I want to write more imperative code in Haskell. Same with the State Monad.
&gt; how hard is it to write programs that does alot of impure stuff[?] Not hard at all. Others have elaborated already. On a slight tangent, here's what I do find hard in Haskell that I imagine might be easier in other languages: **working around design mistakes without fixing them.** What do I mean by this? Let's say for example, you have a data type that you wish had an additional constructor in just one place; there's no way to temporarily add it, so you need to create a new data type (or use an `Either`). Or if a data type has a constructor you rarely want to handle, you have to deal with the warnings everywhere you don't handle it. If you fail to export something from a module, your out of luck if you want to use it; there's no magic way to get around `private`. There are small, simple examples, but I find that this property scales up; in many ways, Haskell makes you commit to your design decisions more than say Python or Java. This can bite you sometimes, because if you're not careful about designing your program, you will end up doing a lot more refactoring. The worst is if you've written a large chunk of code purely and then you find out that you need to thread a monadic effect deep into it; it's much more invasive than just changing the monad you're working with. I think, over all, this is a good thing, but it's definitely something to be on the lookout for when you start writing bigger projects.
There are certainly "sweet spots" on the trade-off, but they come about because that amount of power-vs-safety has good effects in practice, not because you've gained power or safety with respect to the baseline. To take Haskell for example, it gives you a lot of safety; it gives up a lot of power for it, but it turns out you rarely want that power and are willing to give it up. So there is not exactly a trade-off in terms of power-vs-safety, but the utility of certain powers and certain safeties is not linear, so there is a trade-off, with associated sweet spots, in "utility of power-vs-safety".
Counterexample: I run a 5 yr old clj(s) codebase. ~80k lines of code. Somebody once tried to introduce Schema. We later ripped it out because of lack of utility. *By far* the biggest problems we've encountered over the past 5 years are problems of misconception and poor (read no) design. Many times that manifests as an intractable sprawl of data and decisions. In those cases, static types would be a treatment of a symptom, not the core problem. By contrast, the parts of the codebase that are well-planned and well-executed are immediately apparent. I have a reasonable idea of what the data looks like at every point, data scrubbing happens on ingestion, and every function appears to have its place. In these cases, static types would be supplementary at best and a time sink at worst. None of that is to say that types are wholly unhelpful. But when I look back on my time with this codebase, I find it very hard to argue that they even scratch the surface of our major problems. Therefore, I focus intently on improving competency, communication, and design. Data flow issues are completely tractable by contrast.
This is not the /most/ helpful thing to argue about the details of, but only unsafe code that cheats the type system (like unsafePerformIO) is impure. All the normal things you do in a Haskell program, including IO, are pure. Now, IO is "pure" because (from the Haskell program's perspective) each IO action is shoving the entire universe along through a huge State-like Monad, and that is a slightly goofy way to think about it, but it is true as far as your program is concerned. What this means in practical terms is that all of the normal and useful concepts that you learn about other things in the Functor / Applicative / Monad typeclass tree (State, List, Either, etc) apply to any IO expression you compose as well. If you have `getLine :: IO String` and you want that String result to be split up with the `words` function, you can write it as `words &lt;$&gt; getLine`, and it'll work just like any other use of the `(&lt;$&gt;)` operator. It takes times to learn all these operators, but once you do they're as re-usable with IO as they are with the rest of the FAM types.
I'm not sure anyone is arguing that a retroactive inclusion of schema or typed will solve bad design problems. So within that context, I agree? What types give you in this scenario is a way to document and quantify the extent of the bad design, which is the first step to combatting it. There is this spectre of "types fix bad design decisions" that gets raised to be burnt down, but the argument is more like, "Code reuse is greater in the strongly typed world so using long-standing libraries that offer good structure is more feasible." We're well into the world where Applicatives, Monads and Functors are uncontroversial and solid abstractions. I think this is the biggest challenge for folks advocating for Haskell and PureScript etc: there exists very ephemeral and hairy "experimental" stuff in these worlds and they're often used to paint a picture that this wild west is not the leading edge of computer science but rather the habits of the workaday programmer struggling to even get code to compile. I know my Clojure code got a lot better as I learned Haskell because I silently adopted the idioms and found they worked well even without types. Types just make it harder to mess up, easier to refactor, and code generation safer.
Huh? Haskell does not have magically asymptotically terrible memory usage. What makes you say you have to *design* for memory usage? In my experience, it’s almost always just a matter choosing the right data structure, which is the same as in most language.
I was kind of ambivalent about the whole thing because, hey, why couldn't we just use a tag on Set/Map that decides the instant. Finally watched Type Classes vs. the World which cleared that up so thanks for the link!
Is there one? Let's look at the runtime system: We have 3 encodings of heap object memory layout. The InfoTable itself can describe data structures up to 27 words long mixing pointers and direct embeddings. This is the normal bitmap layout. Pointers-first lets you have a half-word of pointers followed by a half-word worth of non-pointers. On 32 bit platforms this would give you 256kb of each. Finally, https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#Pointers-firstlayout indicates we have a StgLargeBitmap case that can handle arbitrary size data constructors. That uses an StgWord for the size, so it can be as large (or larger) than main memory. =) So there is no bottleneck on the backend that I know of.
Interesting... though things like that ultimately make me wish for a super-compiler.
Very clean!
&gt; I'm not sure anyone is arguing that a retroactive inclusion of schema or typed will solve bad design problems. Schema's inclusion was not retroactive for the codepaths in which it was used. It was there from the start, and, from my perspective, it probably exacerbated a bad design due to the misperception that "this mess is better because it's documented/validated." The point that you seem to have missed is the fact that well-designed parts of the code are not problematic, even in the absence of static types. When I'm trying to figure out how to invest my and my team's time, I'm going to choose activities that have *proven* fruitful *in practice*. Even if I *just assume* static types are unilaterally helpful to some degree, they do cost at the very least in terms of brain cycles. And I would rather spend brain cycles on *other, more impactful things*, namely general programming competency (e.g. time/space big O evaluation, error handling, etc.), good communication, and good design. It seems like everyone on Team Static Types wants to talk about how types do solve problems and are definitely useful. That may be true, but that's doesn't address Rich's point. The point of the talk was not to say that they have zero utility. The point was to say that they do, in fact, have a cost. And that we, as professional developers, have an obligation to spend our most limited resource, our time, in a maximally impactful way.
Personally speaking I'm nothing by relieved when something I've written doesn't type check. It's immediate proof that I've made a mistake and can hopefully learn from it.
it's not a matter of identifying with my type system--It's a matter of not wanting to be around someone who enjoys belittling others he disagrees with. That is a position I hold even when the behavior is regarding subjects about which I have no opinion at all. 
This is great to know, thanks! I was thinking of the 27 word limit on `InfoTable`s
Faster development leaves more time for optimization.
It's no big mystery: You conceive a FooRequest record, a FooResponse record, add the appropriate ToJSON/FromJSON instances and you're done. The point is that any "rename" only makes is as far as those ToJSON/FromJSON instances. Everything past the interaction boundary is strongly typed. (You could technically do a similar thing in JS, but IME everybody just slurps the JSON directly into a data structure and assumes that that data structure is part of the "model" of the domain. It never works out well.)
and what about `g :: a -&gt; b -&gt; c`? It is better to have clear separation of things, with type(foo, arr(a,b)). type(x, a). type(app(F,X), B):- type(F, arr(A,B)), type(X, A). now we can have also type(foobar, arr(a, arr(b, c))). and query stuff like ?- type( app(foo, x), T ). % app(foo,x) is a term (Prolog responds:) T = b. Your scheme would force you to name every interim entity.
Wait. Are you saying, "You should validate and normalize your input"? Literally every cljs app I've written involves a "receive from server and coerce to app-state" phase. I see no leverage for involving static types in this part of an app.
Very nice. That didn't occur to me but it makes perfect sense.
I haven't thought about encoding type classes yet, but type class instances are basically horn clauses so it seems like there should be a natural encoding.
Leading questions, indeed. Are you trying to set up some sort of weird straw man here? Static type checking helps once you've transformed the JSON/XML/whatever to your internal data type. There should not be anything surprising about this.
No. I'm not. We were talking about the part that comes *before* "internal data type." The part where you receive some json, validate it, and *turn it into* the internal data type. You even said, "you could technically do [this checking] in JS." And I was confirming that this is common practice in cljs. In addition, I was noting that this process is *inherently dynamic.* You don't know what you're gonna read off the wire. You can make no guarantees about it at compile time. You *must* figure out at runtime whether it's valid and how you want to handle it.
&gt; there's no magic way to get around `private` Actually, [there is](https://hackage.haskell.org/package/true-name-0.1.0.3/docs/Unsafe-TrueName.html).
&gt; Schema's inclusion was not retroactive for the codepaths in which it was used. Sorry, I reread it and I still sort of got that impression, but evidently that's just the way I read it. &gt; When I'm trying to figure out how to invest my and my team's time, I'm going to choose activities that have proven fruitful in practice. Even if I just assume static types are unilaterally helpful to some degree, they do cost at the very least in terms of brain cycles. And I would rather spend brain cycles on other, more impactful things, namely general programming competency (e.g. time/space big O evaluation, error handling, etc.), good communication, and good design. Two things here: 1. You've really radically changed the subject here. I've got no illusions I can convince you in a reddit post that you should change your current behavior w.r.t. technology choices. It's usually not even an option unless you've got someone willing to own it and unique inflection points in your system. It's either unfair or disingenuous to expect me, suddenly, to be able to respond to this point. I've not said you should change anything here. We're discussing some of Rich's criticisms of static typing and his rather brash statement that static typing as a whole is an anti-pattern. 2. Back to the point at hand. &gt; It seems like everyone on Team Static Types wants to talk about how types do solve problems and are definitely useful. That may be true, but that's doesn't address Rich's point. The point of the talk was not to say that they have zero utility. *He has literally called then an anti-pattern.* He then gave very, very weak criticisms. Literally every clojure programmer should roll their eyes when someone criticizes the idea of "Maybe String" given that major pillars of the stdlib is written in the clojure equivalent of it, safely handling null. You're eager to place me in a "team static typing" box, but I didn't make my money in Haskell, kick off a tradition of Haskell in the US's most technically advanced national bank, or start the standard in timekeeping libraries for Haskell. I did that with and for Clojure, and that's why I care about it. It's also why I find Rich's recent missteps like this to be particularly embarrassing. 
I still don't see how you can get that value from using `splitAt`, since you can't `cons` tuples? 
They are indeed, but the premature dismissal is what's do infuriating. If it's mysterious, why not just wait? Or avoid commenting? 
http://www.vacationlabs.com/haskell-bounty-program
You can define an ADT in terms of coalgebras and algebras (see Erwig's [paper](https://web.engr.oregonstate.edu/~erwig/papers/RandomAccessADT_AMAST00.pdf)). I don't know what defining ADTs like would look like in an actual language. 
I used to think so too. But it's too hard. The compiler doesn't have enough information about intent. However, with intent, you can fairly easily write a partial evaluator that gets the job done. You can use "finally tagless" to write it cogen style too, which makes it even easier.
Worded the question in order to get a general answer, not just specific to splitAt.
It's perfectly safe to use lifted async. We use it in our work codebase frequently and it is great. If your stack is mostly reader, then you won't run into any of the surprising aspects of it. MonadBaseControl is a decent solution. MonadUnliftIO is friendlier to use,but more limited. 
Thanks. Can either of MonadBaseControl or MonadUnliftIO be used with core libraries that have their functions in IO?
How about Haskell Aeson instances for GitHub's v3 API?
I don't remember Rich being so cocky about dismissing static type systems until after Clojure "spec" came out. Before it was "I respect Haskell/static typing but there are trade-offs" sort of vibe, but now it's a "what a mistake static type systems are" vibe. Has anyone used both Haskell and Clojure spec in depth? I'd love to see some informed comparisons between what that provides vs Haskell-style static types. After all I only think that I Maybe onToSomething, and that just isn't how real world information works. ;)
I have some bounties you could hunt as well. Best to contact me via email about them.
Is that the same idea as coding to an interface rather than an implementation?
You can also use the more generic `fold`, or the differently generic `join`.
Yes
Including libraries which have functions with the following patterns: libraryFunction :: Arg1 -&gt; (Arg2 -&gt; IO ()) -&gt; IO ()
&gt; solving a absolutely needless type-jigsaw puzzle! If mtl-based monads are a reality, why aren't all the core libraries moving to MonadIO instead of IO? This problem is called “non-algebraic effects.” It’s a legitimately hard problem, so you shouldn’t call it needless or be surprised that people aren’t generally agreed on a solution. MonadIO is not a solution, as it only deals with algebraic lifting (i.e. you can’t lift `try` over a transformer stack with it). Tangent: In reality, having to call `liftIO` yourself here and there for the functions that could be using `MonadIO` is basically a non-issue that only affects brevity, so I don’t think there’s any reason to be upset that libraries on Hackage aren’t consistent when they could choose either. It’s the `MonadBaseControl`s of the world that aim to actually solve a problem. It’s trying to do for non-algebraic effects what `MonadIO` does for algebraic effects; that is, make it so it doesn’t matter if the function you call could have been implemented in terms of a class, because you can always lift it yourself with minimal overhead. This is why many don’t care to bend their libraries into `MonadBaseControl`: the user can always do that themselves at merely the cost of brevity.
[Yep](https://hackage.haskell.org/package/monad-control-1.0.2.2/docs/Control-Monad-Trans-Control.html#v:liftBaseOp) and [yep](https://hackage.haskell.org/package/unliftio-core-0.1.0.0/docs/Control-Monad-IO-Unlift.html#v:withRunInIO)
Yes. Generally speaking, the `control` function can be used for this. withTransaction' :: MonadBaseControl IO m =&gt; Connection -&gt; m a -&gt; m a withTransaction' conn action = control $ \runInIO -&gt; do -- runInIO :: m a -&gt; IO a -- the current `do` block is in IO right now withTransaction conn (runInIO action)
&gt; Like, lenses are cool, but everyone conveniently seems to forget that updates with lenses involve making multiple copies of the structure over which you traverse. Care to elaborate on that? I never knew! 
This would be: wrapped :: MonadBaseControl IO m =&gt; Arg1 -&gt; (Arg2 -&gt; m ()) -&gt; m () wrapped a1 callback = control $ \runInIO -&gt; libraryFunction a1 (\arg2 -&gt; runInIO (callback arg2))
That's how I've written my version of `withTransaction` in my `AppM`. Except, it's using `liftIO` instead of `control`, and `runAppM` instead of `runInIO`. The question is, if this is `MonadBaseControl IO` is the answer, why aren't libraries moving to it? Why do I have to write this glue code?
I like Haskell as much as the next guy, but come on now. Guy asked about Rust and Go, not functional programming!
Hi folks. I have a problem. I use the vim editor for all my coding needs, and I use the [vim-stylish-haskell](https://github.com/nbouscal/vim-stylish-haskell) plugin. Unfortunately, if I try to contribute to an existing project, vim-stylish-haskell does its job and "prettifies" the file. This means I can inadvertently end up changing a file such that it is no longer following a project's implicit style guide. I am looking into how to turn off vim-stylish-haskell temporarily, but is there a more general setup that I should be using? Some projects have nothing (except the existing code) in terms of a style guide, others have an hlint configuration file, others have a stylish haskell configuration file, and yet others still have an editorconfig file. Do I just suck it up and re-configure my editor every time I switch from a project that uses one of those 4 options to another?
An Hspec [`Expectation`](https://hackage.haskell.org/package/hspec-expectations-0.8.2/docs/Test-Hspec-Expectations.html#t:Expectation) and HUnit [`Assertion`](https://hackage.haskell.org/package/HUnit-1.5.0.0/docs/Test-HUnit-Lang.html#t:Assertion) are just `IO ()` so you can test database code with them, regardless of what database library you use. Self plug: if you'd like to use a database library with a type-safe SQL EDSL, consider trying [squeal-postgresql](https://hackage.haskell.org/package/squeal-postgresql).
So what you're saying is we need a smart tab rendering IDE that aligns tabs with interesting characters from the previous indentation level &gt;=)
&gt; The question is, if MonadBaseControl IO is the answer, why aren't libraries moving to it? It's a tradeoff of simplicity in one place or another. A value with signature `IO X` is arguably simpler than a value with signature `MonadBase IO io =&gt; io X` even though these two types are isomorphic. However, as you've noticed it makes use more complex so you gotta litter `liftBase` in places. And with `MonadBaseControl` you'll have to litter `liftBaseOp` in places where you're using callbacks. I'd agree that this complexity should be taken care of in the library instead of handed off to users.
And make Haskell source code unreadable to anyone not using said IDE? Yeah no thanks, I'm good.
&gt; whether it be as a part time job or for an open source community Are you looking for paid work only? Otherwise there're loads of open source projects that would be happy to accept contributions. For example, take a look at [this list of Cabal/`cabal-install` tasks](https://github.com/haskell/cabal/wiki/Hackathon-2017) I compiled for the HaskellX hackathon.
 liftedLibraryFunction :: MonadBaseControl IO io =&gt; Arg1 -&gt; (Arg2 -&gt; io ()) -&gt; io () liftedLibraryFunction = liftBaseOp . libraryFunction
In my experience, `Sum` and `Product` are not long lived type distinctions, that is, you'd write something like: sum = getSum . fold . coerce In fact this works nicely for any of the newtype monoid wrappers, and incurs no overhead.
Wat? So this: func x = ... where thing = ... .. is readable .. but this: func x = ... where thing = ... ... isn't? If you're talking about lining up similar things *after* the first non-whitespace character on the line, duh, use spaces, I wasn't talking about that - *that's not meaningful whitespace*.
[Simula](https://github.com/SimulaVR/Simula) is a Haskell-based Linux 3D compositor that is actively looking for new contributors. If you're looking to get your feet wet in VR + Haskell, we're willing to sponsor a free HTC Vive if it helps you get your feet wet. Look on our issues page for items tagged "new contributor" (right now there's only one such item, but we're planning on adding more). This is cutting edge stuff, so don't join unless you want to learn a lot :-)
See my recent talk on Liquid Haskell which makes a similar point: push fixes upstream instead of pushing problems downstream * [Slides](https://github.com/Gabriel439/slides/blob/master/liquidhaskell/slides.md) * [Recording](https://skillsmatter.com/skillscasts/10690-keynote-scrap-your-bounds-checks-with-liquid-haskell)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/slides/.../**slides.md** (master → 6ac8895)](https://github.com/Gabriel439/slides/blob/6ac88957d3f8e4d4442582da728af5d6749a4be5/liquidhaskell/slides.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
It's not hard at all, once you get used to do notation, then it's basically just like a normal imperative language. If you think you don't need IO but then you do, moving it can be a pain, but it's not to bad.
There are two differences I can think of that are easy to explain: First, you can import functions and types, too, which is something you can't do with `readFile`. For example, this is valid Dhall code: let replicate = https://ipfs.io/ipfs/QmQ8w5PLcsNz56dMvRtq54vbuPe9cNnCCUXAQp6xLc6Ccx/Prelude/List/replicate in let exclaim = λ(t : Text) → t ++ "!" in λ(x : Text) → replicate +3 Text (exclaim x) Second, you can resolve imports under lambda, which is also something that you cannot do with `readFile`. For example, if you interpret the above expression you get: $ dhall &lt;&lt;&lt; './example' ∀(x : Text) → List Text λ(x : Text) → [x ++ "!", x ++ "!", x ++ "!"] : List Text However, those focus on the practical differences. There's also a philosophical difference, which is to change the way that we compose code. The `readFile`/`writeFile` approach composes code by using a text-based message bus. In Dhall, you compose code directly.
The purpose of this is not to make input or output more reliable. The goal is to make it easier to compose code Right now, we can only directly compose code within a project, but we can only indirectly compose code across project boundaries by using what is essentially a weakly-typed message bus where the messages might be text that we read/write from a handle or requests/responses that we read/write from a socket. This "message bus" approach to composing code is not very functional because it interferes with equational reasoning and clutters our code with details that are irrelevant to the domain that we are trying to model. That's the problem I'm trying to solve
I tried to find the answer by compiling a dummy program involving larger and larger datatypes, but I hit upon a practical limit before I hit any hard limit: larger datatypes took a lot longer to compile. The increase looks quadratic in the number of fields: [plot](http://fooplot.com/#W3sidHlwZSI6MywiZXEiOltbIjEiLCI3LjM2MyJdLFsiMiIsIjcuMzg4Il0sWyI1IiwiNy41MzQiXSxbIjEwIiwiNy43ODEiXSxbIjIwIiwiNy43MDkiXSxbIjUwIiwiNy44OTkiXSxbIjEwMCIsIjguMjI4Il0sWyIyMDAiLCIxMS42ODAiXSxbIjUwMCIsIjkzLjEyNyJdLFsiNjAwIiwiMTY4LjY3OSJdLFsiNzAwIiwiMjk3LjQzNCJdLFsiODAwIiwiNDkyLjM1MiJdLFsiOTAwIiwiNzYyLjM4NCJdLFsiMTAwMCIsIjEzMzYuNzk5Il1dLCJjb2xvciI6IiMwMDAwMDAifSx7InR5cGUiOjEwMDAsIndpbmRvdyI6WyIwIiwiMTAxMCIsIjAiLCIxNTAwIl19XQ--) The horizontal axis is the number of fields and the vertical axis is the compilation time in seconds, so compiling a datatype with 200 fields is about as fast as compiling a datatype with only 1 field, but compiling a datatype with 1000 fields takes 20 minutes. Whether the fields were strict and/or unpacked didn't seem to make a difference. I think 200 fields is already way past reasonable record sizes, so I don't this this is a bug worth fixing, but it does impose a practical limit on how many fields a constructor can have.
It is easy to trip over the most benign things when it comes to memory usage. Take for example `for [1..1000000000] $ \i -&gt; do ...`. That is idiomatic Haskell code to write an iteration. You find that code a lot. But if you're [unlucky](https://ghc.haskell.org/trac/ghc/ticket/8763), it'll be allocated; if you use the expression twice, it can stay allocated, blowing up your computer. You have to [carefully](https://github.com/nh2/loop#loop) write your programs so that it doesn't happen. Just picking the right data structure isn't enough either. The _same_ data structure can have totally different behaviour based on how you construct and evaluate it. And it's obvious why Haskell leaves more room for mistakes here: Strict programming languages have only one possible way how e.g. a tree can exist in memory, and at any point in time you have a hard guarantee on this. In Haskell, the same tree can have many lots of possible memory layouts, as each node can either be evaluated or not. No hard guarantees, unless you put in extra effort to obtain them.
Well if you want genuine subtyping within Haskell you are going to want to use typeclasses. Here is one possible approach, unfortunately there is a lot of boilerplate, luckily it is all one-time boilerplate and once you have it everything else is very concise. {-# LANGUAGE PatternSynonyms, TemplateHaskell, ViewPatterns #-} import Control.Lens (Prism', makePrisms, preview, review) class Lit_ a where _Const :: Prism' a Bool _Var :: Prism' a Char _NotVar :: Prism' a Char pattern Const :: Lit_ l =&gt; Bool -&gt; l pattern Const b &lt;- (preview _Const -&gt; Just b) where Const = review _Const pattern Var :: Lit_ l =&gt; Char -&gt; l pattern Var c &lt;- (preview _Var -&gt; Just c) where Var = review _Var pattern NotVar :: Lit_ l =&gt; Char -&gt; l pattern NotVar c &lt;- (preview _NotVar -&gt; Just c) where NotVar = review _NotVar class Lit_ a =&gt; Prop_ a where _Not :: Prism' a a _Or :: Prism' a (a, a) _And :: Prism' a (a, a) _Imply :: Prism' a (a, a) pattern Not :: Prop_ p =&gt; p -&gt; p pattern Not p &lt;- (preview _Not -&gt; Just p) where Not = review _Not pattern Or :: Prop_ p =&gt; p -&gt; p -&gt; p pattern Or a b &lt;- (preview _Or -&gt; Just (a, b)) where Or a b = review _Or (a, b) pattern And :: Prop_ p =&gt; p -&gt; p -&gt; p pattern And a b &lt;- (preview _And -&gt; Just (a, b)) where And a b = review _And (a, b) pattern Imply :: Prop_ p =&gt; p -&gt; p -&gt; p pattern Imply a b &lt;- (preview _Imply -&gt; Just (a, b)) where Imply a b = review _Imply (a, b) data Lit = ConstLit Bool | VarLit Char | NotVarLit Char makePrisms ''Lit instance Lit_ Lit where _Const = _ConstLit _Var = _VarLit _NotVar = _NotVarLit data Prop = LitProp Lit | NotProp Prop | OrProp Prop Prop | AndProp Prop Prop | ImplyProp Prop Prop makePrisms ''Prop instance Lit_ Prop where _Const = _LitProp . _ConstLit _Var = _LitProp . _VarLit _NotVar = _LitProp . _NotVarLit instance Prop_ Prop where _Not = _NotProp _Or = _OrProp _And = _AndProp _Imply = _ImplyProp You can now freely define functions that take in and return literals and propositions without any syntactic overhead. The only thing I will note is that you are going to probably want explicit type signatures. Both because type inference will struggle without them and because you might find that functions are a little too polymorphic without them, such as `eqLit` below allowing you to pass in a `Prop`. propVars :: Prop -&gt; String propVars (Const _) = [] propVars (Var c) = [c] propVars (NotVar c) = [c] propVars (Not p) = propVars p propVars (Or a b) = propVars a ++ propVars b propVars (And a b) = propVars a ++ propVars b propVars (Imply a b) = propVars a ++ propVars b propVars _ = [] eqLit :: Lit -&gt; Lit -&gt; Bool eqLit (Const a) (Const b) = a == b eqLit (Var a) (Var b) = a == b eqLit (NotVar a) (NotVar b) = a == b eqLit _ _ = False I think this approach is pretty reasonable and models what you want quite accurately, although I'm definitely open to debate, but the initial boilerplate is a little unpalatable.
Checkout https://github.com/cdepillabout/pretty-simple which basically does the pretty+colour stuff in one package. 
&gt;Why mix em? Just use tabs and only tabs. N tabs means you are in the Nth nested lexical scope, and everyone can make the indentation as wide or narrow as they want it to look locally. No mention of meaningful whitespace. And generally, when discussions of tabs or spaces come up, people are not talking about meaningful whitespace. I would not do the former, and I don't think that is preferred stylistically in Haskell at all. I was certainly talking about thinks like aligning on equals in let or where bindings.
Maybe we should include a pretty-printer in GHCi as a feature. Have we considered that before?
- We rely on `lifted-async` heavily in our product. It's definitely production ready (although issues with `MonadBaseContol` when spawning `StateT` values etc. apply here so beware of those). If you're just getting started `unliftio` may be a better option. - We write ad-hoc lift functions for this. Functions like `liftBaseOp` etc. from `monad-control` helps.
I'm not singling you out here, but I'm not a fan of this "Haskell is a space-leaking boat" attitude. Many, many space leaks *do not* adversely affect your program in ways you care about. The "in ways you care about" is key. Strictness can cause problems too! But no one is ratting off Rust for all the times that it's "over strict". Why? Because they rarely have a meaningfully adverse affect on your program. Being "imperfect" isn't inherently bad. Failing to achieve your goal might be.
Yeah I agree. I like Rust for most of the points they mentioned, but it doesn't seem like they actually tried to give a fair shake to Go before jumping into other languages. It's actually really naive to say that Go's best features are its garbage collection and compile times. It would have been fine if he compared the CSP Go style that it is known for with some alternatives like Haskell, Erlang/Elixir, OCaml and even Rust, but they just kind of ignore that and talk about something that most languages have nowadays, garbage collection. I'm sure there are other good talking points for Go, but I don't know the language so I can't say much more than the CSP style. I agree with them that compile times are usually meaningless, but they are equally meaningless to complain about in this manner.
Because `MonadBaseControl` isn’t *the* answer. It’s *one* answer with some highly questionable behavior, whose design is such that libraries don’t *have to* move to it for people to use those libraries with it.
Perhaps we should provide specialized control operations with different signatures for different stacks (other than ReaderT ones where the problem has been solved). Like a `concurrently` for `StateT IO` that somehow returned the list of modified states.
Can you please explain more about this issues with lifted-async and StateT? I haven't wrapped my head around d MonadIO vs MonadBaseControl, so an ELI5 would be much appreciated. 
What happens then when your app-state has to change in response to the changes in the API? You have to manually chase all the places where you manipulate your app-state, and run some kind of type inference in your head (if you are lucky enough to have a codebase that's amenable to any kind of typing.)
I guess it's insecurity and/or FUD. They *do* believe what they say but they either need more "followers" or just love the controversy.
I personally like to model it as a negative-sum game. If you design it perfectly, then you're on an axis where you can slide back-and-forth for 1:1 gain-lose, but most code doesn't reach that axis, and they're wasting a lot of guarantees without gaining any useful power due to bad design. So, in that sense, we could talk about sweet spots where we actually touch the axis.
I'm new to haskell so I'm not familiar with lenses, but once I differentiate between `Not` and `NotVar`, why would this approach be better than a coproduct ([like this](https://www.reddit.com/r/haskell/comments/74rvnx/weekly_beginner_saturday_hask_anything_7/docd9of/))?
IMHO, the `IsList` instance of `NonEmpty` really ruins it :(
What do I need to know to get started on this? 
&gt; no one is ratting off Rust for all the times that it's "over strict" Then that's their weakness and our strength. I use Python daily. It deserves to be challenged for not supporting laziness sufficiently well. I know nothing about Rust, but if it doesn't support laziness sufficiently well then it deserves challenge for that. The fact that we in the Haskell community are self-reflective is to our credit. That said, there is no parallel between laziness problems in strict languages and strictness problems in Haskell. The support for laziness in strict languages is generally very poor. There are few "bugs" that are due to programming too strictly. People know their code is strict and come up with workarounds if they need to simulate laziness. The support for strictness in Haskell is *excellent*, but people get caught out because they often write their code like it is strict when it's really not. &gt; Many, many space leaks do not adversely affect your program in ways you care about One should seek to write code that doesn't have bugs regardless of whether these bugs "adversely affect your program in ways you care about". That's actually one of the reason's Haskell's my favourite language. It makes designing out these sorts of bugs easy. We should strive to achieve the same standard regarding strictness. 
Unfortunately `pretty-simple` cannot currently print infinite lists :(
Doing this would mean that if I build a Map or Set with one instance for Ord Int and use it with your 'explicitly passed' other instance or Set, that lookup or insert will break. The Set doesn't hold the dictionary. Scala does more or less exactly what you ask for here. It doesn't work out as much of a feature in practice and now that you can care about the provenance of where your instances come from you _have_ to care about provenance. You're in fairly good company, though. Literally every other programming language other than Haskell -- and I guess Rust (?) -- does it the way you propose!
 class IsBool t where _True :: Prism t () _False :: Prism t () pattern True :: IsBool t =&gt; t pattern True = (preview _True -&gt; Just ()) where True = _True # () ... gets you both the construction and matching side, but doesn't allow for things like (a -&gt; Bool) to be an instance. Unfortunately pattern synonyms currently can't have different constraints for construction and matching. =(
Not sure if this counts as a ELI5 explanation but just think about semantics of this program: {-# LANGUAGE FlexibleContexts #-} module Main where import Control.Monad.Trans.Control import Control.Monad.State import Control.Concurrent.Async.Lifted main :: IO () main = runStateT (concurrently f f) 0 &gt;&gt;= print f :: MonadState Int m =&gt; m Int f = modify (+ 1) &gt;&gt; get You get `1` as the final state here because the state is copied into two computations and one of them is thrown away when they both finished. This `MonadBaseControl` instance thus makes no sense and should be avoided when possible. Another example is when you use a `MonadState` computation in exception handlers: {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE ScopedTypeVariables #-} module Main where import Control.Concurrent.Async.Lifted import Control.Exception.Lifted import Control.Monad.State import Control.Monad.Trans.Control main :: IO () main = execStateT (catch f (\(e :: SomeException) -&gt; modify (+1))) 0 &gt;&gt;= print f :: MonadState Int m =&gt; m () f = modify (+1) &gt;&gt; error "" Think about what this should print, and then run it. `unliftio` is solves this problem by not proviging wonky instances like these.
&gt; The breadth/depth first search algorithms are Indexed Bitraversals (Bi-indexed traversals?) over the graph. Having these full-fledged traversals gives a lot of power that I find difficult to emulate with existing libraries. Sounds cool, but I don't understand the bit about "a lot of power". Could you give an example of constructing a small graph and performing depth-first search as a traversal in your library, comparing it, say, with `dfs` from the `containers` library?
&gt;Every time you run into something like this, you have to stop solving the problem at-hand, and spend a good number of productive hours, solving a absolutely needless type-jigsaw puzzle! It's not "absolutely needless". Monads don't commute. 
I was eventually convinced that a typed approach is better, using `MutiParamTypeClasses` with a phantom type parameter differentiating the different instances. I still think though that we should integrate it into the language and provide a nice syntax instead of relying on an extension that provides a lot more than that feature.
The problem is "secondary" ones still need to get passed through somewhere, now the thing you're calling needs to know what to do. Say I give you two completely unrelated 'secondary' dictionaries at different parts of your program one for Ord [a] and one for Eq a. Later on you open them up in a context and need Eq [a]. There are now two paths. Ord [a] :- Eq [a] and Eq a :- Eq [a]. But the resolution of these paths is incoherent. Also, keep in mind, there are a ton of 'constraints' that are discharged by the typechecker all the time. `sort ["foo","bar","baz"]` secretly discharged an `Ord [Char]` obligation and, importantly for your purposes, has the gall to never tell you in its type signature! ;)
 import Control.Lens -- or Control.Newtype ala All foldMap [True, True, True]
How about [ghci-pretty](https://hackage.haskell.org/package/ghci-pretty)? Sounds like a lot of trouble would have been saved if OP had known about that.
The point of this post is that it already is a feature, and it includes detailed instructions how to enable it. So I assume you mean as the default. As a direct feature of GHCi, that would mean adding a PP library to the "boot libraries" supplied with GHC out of the box. That has a huge amount of long-term collateral costs compared to a small benefit, so it is very unlikely to ever happen. However, it might very well be possible to add something like that as a feature of the GHCi wrappers supplied by build tools such as stack and cabal. It would take some significant design and implementation work, though. Are you volunteering to take the lead and push it forward? Note that the GHC boot libraries already do include a different pretty-printer, the [HughesPJ pretty printer](https://hackage.haskell.org/package/pretty). I think GHCi already uses that though, so it's probably not what you have in mind. As a compromise, if you just want a regular library that makes it a little easier to carry out the configuration steps in this post, there is already [ghci-pretty](https://hackage.haskell.org/package/ghci-pretty). 
If you (or anyone else) has any ideas about implementing this, there is an isssue on the issue tracker: https://github.com/cdepillabout/pretty-simple/issues/9
&gt; How about ghci-pretty? Sounds like a lot of trouble would have been saved if OP had known about that. I'm the author of [pretty-simple](https://github.com/cdepillabout/pretty-simple). One of the goals for `pretty-simple` was to create a pretty-printer that works for data types that don't have a "proper" `Show` instance. It does its best pretty-printing data types like `UTCTime` and `UUID` that don't necessarily have a `Show` instance that produces legal Haskell code. For instance, the `Show` instance for `UUID` and `UTCTime` will produce output like the following: d2e82636-e5e8-44dc-b143-32a4071ca0a1 2017-10-15 13:07:06.882028951 UTC `pretty-simple` will try its best to pretty-print these types of instances. However, you may be right that packages like ghci-pretty and pretty-show would have worked for the OP.
Hey -- I would just dive right in and start by reading [this code overview](https://github.com/SimulaVR/Simula/blob/master/CONTRIBUTING.md). Simula involves the following: - the [Wayland protocol](https://wayland.freedesktop.org/architecture.html) - OpenGL - Haskell FFI - VR For VR libraries, we're working with a few to try to figure out which one can get us Simula rendering on the HTC Vive. Here's [OSVR](https://github.com/OSVR/OSVR-Core), for example. Do you have a Vive?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [SimulaVR/Simula/.../**CONTRIBUTING.md** (master → a70e862)](https://github.com/SimulaVR/Simula/blob/a70e862808a97f169e20b4a81a5c50eb78dab450/CONTRIBUTING.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply doeaz10.)^.
No, I do not have a Vive. Will that be an issue? I do not have any knowledge of VR technology as such, but I am willing to learn. I saw the open issue about Rectangle. I think I can fix that issue.
I'm a Clojure programmer that found this page through several twitter and reddit mentions, so I thought I might help explain some context behind some of the comments Rich made. Also, I speak for myself, and as a Clojure programmer, I may interpret some of his views wrong. 1) Row Polymorphism - Yes, that seems quite a bit like Clojure maps. Except perhaps that it still requires you to specify types for all your maps, it seems. 2) SSN (Maybe) comment: In Clojure getting a value from a map either returns the value or `nil` (or a default) if the entry doesn't exist. Also get performs a polymorphic dispatch on the map argument and is extended to nil. So the signature of get is something like `f Maybe&lt;m&gt; k -&gt; Maybe&lt;v&gt;` (pardon my notation, I've only programmed in F#, a bit of OCaml and it's been years). In this case, assume I have a record that looks like this: `{name: string, ssn: Maybe&lt;string&gt;}`. If records already imply a maybe on every k/v why the extra maybe the type? A get would then return something like `Maybe&lt;Maybe&lt;string&gt;&gt;`. And this just gets weird. In the end Clojure took a different path. Treat `nil` somewhat like a maybe and extend it to almost everything. 3) Why store SSN in a string: We don't do that in Clojure. Infact Clojure has quite a lot of support for extensible types. We have Data Readers, one such is `#inst "2012-02-1"` which when read will be a java Date type. Likewise `#uri "http://www.google.com"` would evaluate to a JVM URI type. Not only is this system extensible, but it's also in EDN (lisp data format), Transit (EDN for Msgpack and JSON) and Fressian (Binary data format Datomic uses). So Rich is very much aware of how bad an idea it would be to store a SSN in a string, since that was one of his main value propositions behind creating Transit in the first place. It was an example created out of thin-air on the fly in the middle of a talk. :) Anyways, thanks for reading. It was fun reading (most) of the comments on this page. --- Few personal comments: I've been programming for close to 20 years now, and I've studied many languages. The one thing that I find the most annoying in these discussions is when it devolves into name calling and ad hominem. As a programmer who prefers dynamic languages I've been called sloppy, or "immoral" because I don't type check my systems. I've been told that I don't care about correctness. But in the end I just want to write good software, get it done efficiently and in a way the allows me to have the maximum power at the keyboard. For me that's dynamic languages. Many of us are aware of all these features in static languages, we simply have different value priorities. 
There is a way to use git and not commit white-space changes. I don't do this so can't attest to how easy the workflow is, but you could try `git diff -w | git apply --cached --ignore-whitespace`.
Once you understand Functor, Applicative, and Monad, I think that you could benefit from looking at a library for a problem domain that you are comfortable with. For example, I'm a web developer by trade (not training), and my first foray into real-world Haskell code is looking at a library called twilio-haskell (https://github.com/markandrus/twilio-haskell), which is simply a lib to interact with Twilio's REST API. If you're not a web dev, maybe ask around in the #haskell channel on freenode IRC for a lib that you would know how to write in another language, just to see for yourself how things stack up. 
The key point of the difference is that in **Data.Graph** and **fgl**, *dfs* and *bfs* are functions which give the indices in traversal order (**fgl** does offer some additional traversal functions, but none that are is general as the traversals in this library in my mind). In my opinion, there are two key advantages to have these be true traversals instead: * They skip an extra level of indirection. For example, if you want to print all vertices in your graph with **fgl** in *dfs* order, you would need to perform a dfs, then look up each vertex returned and print them. Contrast that to a *Traversal* where you can print the targets directly. Obviously this isn't too much easier, but it makes the operations feel more natural * They include edges in the traversal. In my mind, this is a clear winner. Since *dfs* and *bfs* as I've implemented them are *Bitraversals*, the edges are traversed in the order they were looked at in the algorithm. I don't even know how you could replicate this behavior in **fgl** or **Data.Graph** without completely rewriting the algorithm. How about an example. Lets' say we have a graph that looks like this: https://imgur.com/a/MfZ2p Let's print the content of the graph in traversal order. g = fromLists [('a', 10), ('b', 17), ('c', 8), ('d', 3), ('e', 27)] [('a', 'b', ()), ('a', 'c', ()), ('b', 'b', ()), ('b', 'e', ())] void $ dfs return print g ==&gt; 10 this 17 so 3 fun 27 is 8 void $ bfs return print g ==&gt; 10 this 17 is 8 so 3 fun 27 Of course, it's still possible to do this in Data.Graph or fgl, but it's going to take more effort. It's also not clear to me exactly what the best way to do it would be without completely rewriting the traversal algorithm.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/DjdBB3h.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Can TH output include another TH invocation?
Sorry in advance for the following rant :) &gt; good all-inclusive .gitignore I [see](https://github.com/vrom911/hs-init/blob/bb603a068ee748a174e6a577c4a82453077d5425/hs-init.hs#L761) that by "all-inclusive" you mean both files generated by building the project, and files generated by various editors. Why include the latter in the project's `.gitignore` file? Git [supports](https://help.github.com/articles/ignoring-files/) both user-wide ignore rules (`~/.gitignore_global`), project-wide ignore rules (`./.gitignore`), and checkout-wide ignore rules (`.git/info/exclude`). To me, it makes a lot more sense for users to put their editor-specific ignore rules in their user-wire ignore rules than to duplicate those rules in every single project they touch. And yet not only is the practice common, I keep receiving PRs from people who want to add their favourite editor's ignore rules to my project's `.gitignore` files. Is there some benefit to that approach I'm not seeing? Should I just accept those PRs instead of politely pointing them to the git ignore documentation?
Proper git ignore is like properly setting your path variables and shell configuration in the right files. So few people do it right that everyone just ends up using their .$shellrc for everything; same with using the .gitignore for everything. At some point you'll probably have to make a choice between being technically correct and colloquially correct, I think :)
I doubt it. [`Dec`](http://hackage.haskell.org/package/template-haskell-2.12.0.0/docs/Language-Haskell-TH.html#g:18) would need to have some `SpliceD` constructor for holding those calls, and I don't see any.
`(MonadIO m, MonadThrow n) =&gt; ... -&gt; m (n ...)` is quite an unusual type signature. Usually in the mtl style there is only one Monad `m` and all the constraints specify what that single `m` can do. The caller can then instantiate `m` with a Monad stack which contains layers for all the required effects. Here however, you are using the word "layer" but your types are not talking about Monad transformers. Instead, `m (n a)` means you have a computation with effects in `m`, whose result is a computation with effects in `n`, whose result is a value of type `a`. So if you want the failure to occur in the `n` Monad, that means that you want the `m` computation to successfully return an `n` computation, and then if the caller chooses to call that `n` computation, it will fail. Oh, and of course, the `n` computation isn't allowed to perform `m` effects at that point, those should all have occurred during the `m` computation. If that's really what you want to do, I think all those `fmap`s are obscuring your program's behaviour. I would write something like this instead: loadFile :: (MonadIO m, MonadThrow n) =&gt; TZIdentifier -&gt; FilePath -&gt; m (n TimeZone) loadFile tzName file = do exists &lt;- liftIO . doesFileExist $ file return $ do unless exists $ throwM TimeZoneDoesNotExistException fileContents &lt;- liftIO . BS.readFile $ file -- can't do IO here! (utcM, calDateM, _) &lt;- getTransitions fileContents return $ TimeZone tzName utcM calDateM In this style, the `m` computation is clearly separated from the `n` computation, so we can see what's going on more clearly. In particular, we can see that you're trying to perform some `IO` action in the `n` computation, but you didn't give an `MonadIO` constraint for `n`, so that won't work.
Is it ever correct for git not to ignore auxiliary editor files? If not, then I don't see why they shouldn't be included in `.gitignore`.
The set of auxiliary editor files is open, there will always be more files to add to that list, in every project.
The kinds don't line up. So you'd need to write a newtype wrapper, or a toList fromList pair. 
what do you mean with the kinds don't line up?
`Functor` accepts a type with kind `* -&gt; *`. `HList` has a kind `[Type] -&gt; *`. It doesn't kind-check.
Interesting challenge! I don't think I'll be able to only give you clues, I just *gave* to solve the whole thing now :) Let's see, so the kind of `HList` is `[Type] -&gt; *`, but Functor expects a `* -&gt; *`, so the first step is to create a `newtype` indicating what we mean by an "homogenous heterogenous list". We probably mean "an HList whose [Type] entries are all `a`". And when fmapping from `a` to `b`, we'll change all those entries to `b`. The *number* of entries will stay the same though, and knowing the number of entries is important in order to recover the `[Type]` from the `a`. So: type family Replicate n a where Replicate '[] a = '[] Replicate (() ': n) a = a ': Replicate n a newtype HoHeList n a = HoHeList { unHoHeList :: HList (Replicate n a) } For simplicity, I'm using a list of `()`s to represent natural numbers. All right, let's proceed by cases: instance Functor (HoHeList '[]) where fmap _ (HoHeList HNil) = HoHeList HNil instance Functor (HoHeList n) =&gt; Functor (HoHeList (() ': n)) where fmap :: forall a b. (a -&gt; b) -&gt; HoHeList (() ': n) a -&gt; HoHeList (() ': n) b fmap f (HoHeList (x :&amp;: xs)) = HoHeList (f x :&amp;: unHoHeList ys) where ys :: HoHeList n b ys = fmap f (HoHeList xs) The tricky part was figuring out that I had to use `InstanceSigs` and `ScopedTypeVariables` in order to write the type signatures which to resolve the ambiguity errors.
ah, yeah, sorry I misunderstood you. I have tried working with newtype wrapper, but it didn't go anywhere. 
1. I'm not quite sure what you mean by "specify types for all your maps." In both Purescript and Elm, you can just write the following (notice the lack of type annotations): mymap = { x = 5, y = "hello" } and the row type will be automatically inferred for you. 2. You wouldn't store a record as a `{ name: String, ssn: Maybe String }`. Almost certainly all your data would be `{ name: String, ssn: SSN}`. What instead happens is that you would normally would provide a function `String -&gt; Maybe SSN` or (more likely) `String -&gt; Either MalformedSSNString SSN` instead of just `String -&gt; SSN`. You can think of `Either` as introducing some notion of "error scoping," where `map` (or really `fmap`) lets you step into a block of code assuming that your `String` is a valid `SSN` and then once you step back out of that block of code, you've stepped back out of the "this `String` is valid `SSN`" scope and have to deal with the fact that you might have an invalid `String` on your hands. So you'll end up seeing stuff like `processRecordFromRawData : inputdata -&gt; Either MalformedSSNString { name: Maybe, ssn: SSN }` often with type synonyms to make the signature easier to read. As for extending `nil` to almost everything, I assume you're talking about nil punning? Nil punning in Clojure is very much akin to NaN for floating, that is it is an elegant way of having errors propagate through the entirety of a calculation. It, however, misses the point of `Maybe` (for example I very much disagree with this article [http://www.lispcast.com/nil-punning](http://www.lispcast.com/nil-punning) and agree with [https://bsima.me/clog/robust-clojure-nil.html](https://bsima.me/clog/robust-clojure-nil.html)'s last paragraph that pervasive nil-punning is not the same as `Maybe`). The whole point of `Maybe` and `Either` and really types in general is precisely that not everything has this type! If everything in Haskell was wrapped in a `Maybe` then that would be useless. The value of `Maybe`, and `Either` is precisely in its absence. If I don't see that type I know I don't have to handle entire classes of errors. If I do see that type, then I know I need to insert the appropriate runtime checks. 3. That's only kind of true. While there's nothing technically stopping you from `deftype`ing an `SSN` type (as well as a `Name` type and a whole host of other types), practically speaking the Clojure community would look at you weird if you did that. Almost certainly if I were writing a data processing system in Clojure, apart from the edges of the system involving network transport or persistence, within the system the SSN would be a string. (Incidentally if you have an alternative design for problems like this in Clojure I would genuinely love to hear it, PM me.) Let me try to give my personal perspective on this. As someone who has used Clojure in production and currently spends his days mainly in Scala while doing Haskell (and a smattering of other languages) on the side, a lot of the Clojure community's talks on types seem misguided. It often feels akin to a talk about the dangers of REPL-based programming. Imagine a person gives a talk about how REPL-based programming is dangerous. He brings up Python, Haskell, Ruby, Scala and a bunch of other languages that all have REPLs as examples. "I've used REPLs," he says, "and they're a bad way of programming. Sure it's useful here and there for trying out small snippets of code, but let's be honest, it's terrible for real programming. You write these random functions here and there and try evaluating them on random data you think of and then try to piece them together from that. But the functions you're building have no real relation to the overall program you're building. It's not like you can just hook up your REPL to your program; you're hunting and pecking in the dark." He boots up `python`, `ghci`, `ruby`, $INSERT_GENERIC_REPL_HERE, and starts plugging away to illustrate his point. "And on the JVM REPLs are even worse. The startup time kills you! Every time I need to test something I need to fire up the REPL and wait 10 seconds for everything to load before I'm able to even start doing anything." I imagine most Clojurists at this point would want to scream "You're doing it wrong! You don't understand REPLs at all! You don't shut it down and start it back up every time, you leave it running! What do you mean it has no relation to the program you're building? You connect it to a running instance of a program! Your REPL is your lifeline into your program, you tweak the program on the fly! No full program recompile, no nothing; just send the new form over and voila, magic happens! _The REPL lets you engage in a dialog with your program_. If you treat it as a sterile little window in which you evaluate unrelated forms of course you're going to think it's a terrible way to program!" This is exactly how I feel when the Clojure community dismisses types or touts Spec as an alternative. A powerful type system lets you engage in a dialog with your code. When writing from scratch or performing a major refactor, I'll leave large chunks of my code unwritten. Then I'll ask my compiler what it thinks the type of a missing chunk of code should be and it'll tell me, plus it'll tell me the functions in scope that might help me achieve that type. Or I might use (Hoogle)[https://www.haskell.org/hoogle/] to look up the function types I know I need and see if any third-party libraries provide them. With even more powerful type systems (such as in Agda or Idris) it can even deconstruct values for me automatically, while omitting certain values that it knows are impossible. If I add a new kind of error the type system will tell me all the error handling places I have where I need to update my code to handle the new error. It's an amazing form of top-down coding, where I can just white-board out a design, then in code sketch the top-level data structures and transformations I'll need without implementing them, then iteratively work with the compiler to flesh out the nitty-gritty details. In the entire process I might never run the code even once (because I can't since none of the details of any of my functions have been implemented yet), but end up with perfectly functioning code at the end. I would highly recommend you try out an environment with full type inference, type directed search, and type directed semi-automatic program synthesis. It's a very different way of coding from straight-up dynamic image-based coding (such as what you might find with the Clojure REPL). It's impossible to say that it's objectively better since the whole point of programming languages is that they are ways of instructing a computer that feel subjectively easier for certain groups of human beings. And indeed it's not really a competition. An open question that I've yet to see really solved well is how to marry an expressive type system with a good image-based programming experience. I really need to give StrongTalk a go one of these days...
This looks really cool. I do have one point of feedback about your readme: When showing off lens integration, please consider having both an operator and non-operator version of your lens examples. A lot of people need to cheatsheet to remember all these operators outside of the most core set. For folks who are new to the ecosystem, a cool and useful library like this is tougher to sell when the examples are so dense. Sorry, small thing.
That is one particular aspect of the non composability of the monadic frameworks (mtl, free..) currently used and it is a consequence of the Monad/applicative classes not considering effects as first class. With a monad class that consider effects, the effects are not "transformed" but are aggregated. Why liftIO :: IO a -&gt; m a Because Bind can not aggregate effects. If that would be the case your would be capable to write: f :: String -&gt; MyMonad(IO + State) () f file= redFile file &gt;&gt;= set Then there is no need for `liftIO` What you need is adding an IO effect to the lists of effects, not to hide the IO effect as if IO has not happened.
This point was made in the context of specifying individual attributes vs composites of attributes. He’s saying there is a difference between having a person with an optional SSN and having a person who optionally has an SSN. That is, if I model a field ssn as Maybe String, then I can’t reuse that record type where the string is known to be present and doesn’t require the maybe wrapper.
Holy ****! TIL!
It was very hyped at ICFP 2015 but some of the Buzz has died down since then. I think it's more analogous to Idris, Agda or Coq than Haskell. It's a dependently typed language, and its main selling points I think are: * Tightly integrated with Z3 SMT solver to automate much of the proof process. * Has effects built into its type system, so it's great for writing verified effectful programs, like communication protocols.
I think it's a really cool project. The Everest stuff in particular is awesome, as that's resulting in verified code [making it into firefox](https://blog.mozilla.org/security/2017/09/13/verified-cryptography-firefox-57/). If I have an idea for a small project, I'd strongly consider using it to learn F*.
Yes, that's way cheaper for people from India! If you want to see what it looks like, here's [a picture of my copy](https://pbs.twimg.com/media/DLsBxDxUIAEi4f7.jpg)
IMO exception s should only be used for exceptional case, ie stuff which should not arrive or if they do, there is nothing you can do. In that case : file not present, it's probably best to use Maybe or Either. Thus makes the code simpler and easier to read. If you really think you can't handle maybe and are happy to throw an exception you can still check on the maybe and throw an exception at this point.
Here are some spares for you to copy and paste :) - ₹₹₹₹₹₹₹₹
To add to this, if you really want to throw the exception inside the `n`, you could do it like this: loadFile :: (MonadIO m, MonadThrow n) =&gt; TZIdentifier -&gt; FilePath -&gt; m (n TimeZone) loadFile tzName file = do exists &lt;- liftIO . doesFileExist $ file if exists then do fileContents &lt;- liftIO . BS.readFile $ file -- can't do IO here! return $ do (utcM, calDateM, _) &lt;- getTransitions fileContents return $ TimeZone tzName utcM calDateM else return $ throwM TimeZoneDoesNotExistException But this is a pretty weird-looking function.
Thanks for the feedback! I think I probably need a lot more examples in general. Your feedback reminds me of a debate I've been having with myself about the interface design of this library: how much should I incorporate simple synonyms in the library? So for example, I present *allVerts* as a traversal over the vertices (essentially just an alias for the *traverse* instance or *travVerts*) which I find sensible, but should there also be a function with the signature: Graph i e v -&gt; [v] This function is trivially implemented as *toListOf travVerts*. So is it more sensible to include this definition or to force the user to stay in the realm of lenses? Just a question I've been debating myself about in terms of the design.
Thanks for the thoughts. I'd just like to clear something up in #2. Accessing a field on a record does not return a maybe. `Rec { name :: String, ssn :: Maybe String }` has getters that match those types: ``` name :: Rec -&gt; String ssn :: Rec -&gt; Maybe String ```
Interesting challenge! I don't think I'll be able to only give you clues, I just *have* to solve the whole thing now :) Let's see, so the kind of `HList` is `[Type] -&gt; *`, but Functor expects a `* -&gt; *`, so the first step is to create a `newtype` indicating what we mean by an "homogenous heterogenous list". We probably mean "an HList whose [Type] entries are all `a`". And when fmapping from `a` to `b`, we'll change all those entries to `b`. The *number* of entries will stay the same though, and knowing the number of entries is important in order to recover the `[Type]` from the `a`. So: type family Replicate n a where Replicate '[] a = '[] Replicate (() ': n) a = a ': Replicate n a newtype HoHeList n a = HoHeList { unHoHeList :: HList (Replicate n a) } For simplicity, I'm using a list of `()`s to represent natural numbers. All right, let's proceed by cases: instance Functor (HoHeList '[]) where fmap _ (HoHeList HNil) = HoHeList HNil instance Functor (HoHeList n) =&gt; Functor (HoHeList (() ': n)) where fmap :: forall a b. (a -&gt; b) -&gt; HoHeList (() ': n) a -&gt; HoHeList (() ': n) b fmap f (HoHeList (x :&amp;: xs)) = HoHeList (f x :&amp;: unHoHeList ys) where ys :: HoHeList n b ys = fmap f (HoHeList xs) The tricky part was figuring out that I had to use `InstanceSigs` and `ScopedTypeVariables` in order to write the type signatures which to resolve the ambiguity errors. Other than that, it was easier than I expected! I expected I would have to prove something about the behaviour of `OnlyFunctions`, `ToDomain` and `ToCoDomain` on `Replicate`. Oh wait, that's because I didn't use `mapH`! Let's try again. -- error: Couldn't match type ‘Replicate n a’ -- with ‘ToDomain (Replicate n (a -&gt; b))’ instance Functor (HoHeList n) where fmap :: forall a b. (a -&gt; b) -&gt; HoHeList n a -&gt; HoHeList n b fmap f (HoHeList xs) = HoHeList (mapH onlyFunctions fs xs) where onlyFunctions :: OnlyFunctions (Replicate n (a -&gt; b)) onlyFunctions = undefined fs :: HList (Replicate n (a -&gt; b)) fs = undefined That's more like it! Let's write a proof. class IsNat n where withToDomainReplicate :: Proxy n -&gt; Proxy a -&gt; Proxy b -&gt; ( ToDomain (Replicate n (a -&gt; b)) ~ Replicate n a =&gt; r) -&gt; r instance IsNat '[] where withToDomainReplicate _ _ _ r = r instance IsNat n =&gt; IsNat (() ': n) where withToDomainReplicate :: forall a b r . Proxy (() ': n) -&gt; Proxy a -&gt; Proxy b -&gt; ( ToDomain (Replicate (() ': n) (a -&gt; b)) ~ Replicate (() ': n) a =&gt; r) -&gt; r withToDomainReplicate _ _ _ r = withToDomainReplicate (Proxy :: Proxy n) (Proxy :: Proxy a) (Proxy :: Proxy b) $ r -- error: Could not deduce: Replicate n b -- ~ -- ToCoDomain (Replicate n (a -&gt; b)) instance IsNat n =&gt; Functor (HoHeList n) where fmap :: forall a b. (a -&gt; b) -&gt; HoHeList n a -&gt; HoHeList n b fmap f (HoHeList xs) = withToDomainReplicate (Proxy :: Proxy n) (Proxy :: Proxy a) (Proxy :: Proxy b) $ HoHeList (mapH onlyFunctions fs xs) where onlyFunctions :: OnlyFunctions (Replicate n (a -&gt; b)) onlyFunctions = undefined fs :: HList (Replicate n (a -&gt; b)) fs = undefined Now it's complaining about `ToCoDomain` instead, that's progress! Let's write a proof for that, and while we're at it, let's also generate `onlyFunctions` and `fs`. class IsNat n where withToDomainReplicate :: ... withToCoDomainReplicate :: ... onlyFunctionsReplicate :: Proxy n -&gt; Proxy a -&gt; Proxy b -&gt; OnlyFunctions (Replicate n (a -&gt; b)) hreplicate :: Proxy n -&gt; Proxy a -&gt; a -&gt; HList (Replicate n a) instance IsNat '[] where ... instance IsNat n =&gt; IsNat (() ': n) where ... instance IsNat n =&gt; Functor (HoHeList n) where fmap :: forall a b. (a -&gt; b) -&gt; HoHeList n a -&gt; HoHeList n b fmap f (HoHeList xs) = withToDomainReplicate (Proxy :: Proxy n) (Proxy :: Proxy a) (Proxy :: Proxy b) $ withToCoDomainReplicate (Proxy :: Proxy n) (Proxy :: Proxy a) (Proxy :: Proxy b) $ HoHeList (mapH onlyFunctions fs xs) where onlyFunctions :: OnlyFunctions (Replicate n (a -&gt; b)) onlyFunctions = onlyFunctionsReplicate (Proxy :: Proxy n) (Proxy :: Proxy a) (Proxy :: Proxy b) fs :: HList (Replicate n (a -&gt; b)) fs = hreplicate (Proxy :: Proxy n) (Proxy :: Proxy (a -&gt; b)) f It compiles! Does it work? input :: HoHeList '[(), (), ()] String input = HoHeList ("bar" :&amp;: "baz" :&amp;: "quux" :&amp;: HNil) -- | -- &gt;&gt;&gt; :force output -- output = HoHeList (:&amp;: 3 (:&amp;: 3 (:&amp;: 4 HNil))) output :: HoHeList '[(), (), ()] Int output = fmap length input It does! That was fun, I hope you learned what you needed from this implementation.
The reason they shouldn't be in the per-project .gitignore is because your editor will generate those files in every project, so it should be in *your* gitignore. That said, I don't think I've seen an editor-generated name that I would ever want to use for a real file in any repository, so I don't care if someone wants to add their editor ignores to the project ignores.
I've never written any. I heard it pitched as a dependently-typed F#. I have written F#. In fact, it's the only ML I've written more than a trivial amount of code in. Since it is an ML, the syntax is very similar to OCaml and CakeML. I'm not sure about F*, but F# is not pure (referentially transparent) but definitely functional. Effects like stdin/stdout/stderr I/O can be done anywhere, without affecting the type signature. If I were to start a new CLR project, I'd probably try and use F* or F#. But, that's only because I don't know of a pure language on the CLR -- I work more on the JVM or with native code, so I often miss CLR-focused innovations.
In particular, I want exhaustiveness checking in my pattern-matching, and prisms / injections are not quite enough to get there. Though, there is a library on hackage that does allow some limited exhaustiveness testing when using prims instead of patterns, IIRC. (It provides the extra bits, under specific circumstances.)
Isn’t most of this already present in https://github.com/phadej/github?
Actually my goal was to let the user decide, hence the use of `throwM`. Then the user can choose `Maybe`, `Either` or an exception if they like. The problem is that this function does IO, which forces me into that monad as well as the `MonadThrow` monad. I'm trying to find a way to keep all my errors in the `MonadThrow` "layer" and leave IO as just a shell that happens to be around it.
Well said. &gt; these sorts of bugs My point is that a "bug" is only so-called because it adversely affects your program in ways you care about. Using more memory than is actually necessary is not, in itself, a bug. If it were, then every program would be nothing but bugs! Just using Haskell in the first place would be a bug, because it uses more memory than if you were to write in assembly directly. 
Well this approach is about as syntactically lightweight as possible (once you put in the initial big chunk of boilerplate), whereas /u/Solonarv mentioned the need to do lots of tedious newtype wrapping and unwrapping for the other approach. So unless pattern synonyms can remove such boilerplate that is going to be the main advantage. I don't think you will be able to write things as nicely as `eqLit` and `propVars` above.
&gt; (MonadIO m, MonadThrow n) =&gt; ... -&gt; m (n ...) is quite an unusual type signature. I got that from the link I showed above, toward the bottom. They said you could use this in the case where errors were common. That's not actually my intention. What I wanted was to let users decide if they want to use e.g. `Maybe`, `Either` or even an exception to deal with functions that can fail. The `getTransitions` function is pure and already works this way but `loadFile` must run in IO so that forces another "layer" (not in the transformers sense) but I actually wanted to ignore IO as much as possible and keep all my error handling under `MonadThrow`. However, having thought about this, I guess using this strategy is going to force me *and my users* into ugly nested `do` situations to actually use this code. That being the case, I suppose the most convenient thing is just to say that if IO is involved just do everything in IO. I don't suppose I actually lose anything by collapsing my pseudo layers here by doing so.
Wow! I am quite new to type-level programming, so this was a problem I just failed to solve for a few hours now. Thank you!
Thanks, that looks like what I wanted to do. Having given it more thought, I've come to the conclusion [above](https://www.reddit.com/r/haskell/comments/76iw3t/error_handling_best_practices_throwing_from_the/doeq44o/). I think that will be the easiest for users of the code.
Perhaps it is! OK, fine. A GraphQL API then.
I think their approach to getting a working version of the language was nice. By using an existing constraint solver (z3) they were able to avoid a good chunk of the work in implementing a complicated type system and side-stepped a lot of potential for bugs in the solver itself. The downside to the way they've done it is that sometimes a type error simply states the line numbers of the type assertion and the expression and says that they don't match. I've heard people say that in principle they should be able to extract more information from the smt solver to give better error messages. The type system is rather complex because it allows both refinement types and full dependent types. Again, this is fairly easy and natural to allow if you're using an smt solver as your type checker. While this is very powerful and expressive, the potential downside is that it may be very confusing in some cases for programmers to have access to both in the same type signature. As type complexity increases in a program, programmers will need to be disciplined about which features they use. Mostly I'm pretty happy with the language and I really like their lattice of effects.
One way to do it if you just wanted highlighting, rather than pretty printing, would be to simply apply a Haskell lexer rather than a full parser and highlight based solely on lexemes, and revert after the lexeme stream is illegal to just copying the source out. You might get some false highlighting at the start, but you'd be properly lazy.
You don't need a Vive to contribute to Simula or understand how it works; however, you will need one to make contributions to the rendering pipeline and to reap the full benefits of the project once we get it working on a Vive (we are so, so close...). Also: I got your pull requests. That was fast!
Thanks for the reply!
Thank you! I can see how having access to the edges in the order they are encountered by `dfs` may be useful and hard to achieve with existing libraries. However, I'm still not sold on Traversal vs list as the result of `dfs`. Printing vertices in the `dfs` order is not a compelling example of power: `mapM print` doesn't look too bad and is certainly much simpler conceptually. Do you have an example where plain old lists would be significantly more painful to use? Anyway, thanks for the library, I'll definitely give it a try!
F* is pure, necessarily so because of its use of dependent and refinement types. There are a variety of monadic effects: Tot, Dv, ST, Exn, ML. ST should be familiar to most Haskellers, ML is equivalent to IO, Tot is a pure function guaranteed to return, Dv a pure function which may “diverge” (or fail to terminate normally). It does share F#’s nice syntax, and as it transpiles to plain F# you have full access to any library ever written in F#. C# code can be imported by using F# shims around the OOP bits. There is also the option to transpile to C if you use a limited subset of the F* language. However it’s all very immature, much more so than Idris. Honestly, if you want additional compile-time type checking, use Liquid Haskell. If you want a nice ML, use F#. If you want performance, use Rust, which is something of an ML in disguise. There are no good reasons to use F* yet as anything other than a hobby or curiosity. 
Ah, I see! In that case, it makes more sense to think of the `n` as a concrete container like `Maybe` than as a second computation, so your original code is more sensible than my suggestion. I think `return . throwM $ TimeZoneDoesNotExistException` is indeed what you want, but keep in mind that since you're throwing in `n`, not in `m`, this will not abort the `loadFile` computation. So you have to rearrange the computation so it doesn't execute the later steps in that case: loadFile :: (MonadIO m, MonadThrow n) =&gt; TZIdentifier -&gt; FilePath -&gt; m (n TimeZone) loadFile tzName file = do exists &lt;- liftIO . doesFileExist $ file if exists then do mTrans &lt;- fmap getTransitions (liftIO . BS.readFile $ file) return . fmap (\(utcM, calDateM, _) -&gt; TimeZone tzName utcM calDateM) $ mTrans else return . throwM $ TimeZoneDoesNotExistException 
Ah okay! In ClojureScript: Yes, that _can_ be more difficult. BUT, if you use namespaced keys, and you're disciplined about not overloading the semantics of your keys, your IDE can find all usages of each key. That's a worst case. The best case is: you're adding a new key and nobody cares except the code you choose to _make_ care. The end result is you spend a fair amount time asking, "What is this piece of data? What does it mean? What should it be called?" which, IMO, is a decent use of time in itself. And an additional payoff is you can leverage the IDE to help make edits to those names (refactor) or concepts (track down each instance and rework).
&gt; there's a pretty standard `Data.List.NonEmpty` type in semigroups Actually, it's now in base. Yay!
This is amazing! Thank you!
My recommendation is very general but I like to just do a GitHub search for "language: Haskell" and pick a project that you have previously used or find interesting at the time. From there just go to the issues of that project and try to complete one of the issues. Usually there are a lot of low hanging fruit as well as more challenging stuff so it suits almost all levels. I mean at times I have fixed Java project issues with just adding null checks for 10 minutes. This is something that I at least find fun and rewarding
How's the type inference? (When doing non-dependent type stuff.)
I'm not sure what you mean, but you can play with the language via their website: https://www.fstar-lang.org/tutorial/
I don't believe it transpiles to F# any longer. 
Haven't checked your links but try "stack ghci"
To run the REPL wity Stack, just type `stack ghci`
I did that as well but only got the same error
Thanks. Very thorough explanation! I hadn't considered some of your points, so this is incredibly helpful.
From skimming the video without audio, it looks like that video recommends downloading and installing the Haskell Platform. I can't think of any advantages that route offers, and often leads to a broken setup. Unfortunately, due to reasons I won't go into here, it is still recommended on haskell.org. Try uninstalling HP, and installing via the instructions on https://docs.haskellstack.org/en/stable/README/#how-to-install , and hopefully it will work!
If the user can chose the type of the exception then I guess you can't pass any information to the throw. Your exception handling is then isomorphic to a Maybe so why not use a Maybe and let the user convert to the desired exception if needed ?
Please support hpack. Using .cabal format is just silly in 2017.
If you installed the platform as in the video, I think the relevant files should be under `C:\Program Files\Haskell Platform\8.2.1\bin`. Make sure that's added to your PATH environment variable. The stack binary (if you use the platform installer or the stack installer directly -- it is the same) will be under `%APPDATA%\local\bin`. Check that too?
&gt; If you consider it to be a 1:1 relationship, is this conjecture on your part? Or has this, or something close to this ever been proven? It's true-by-definition, in that I've defined power as "the set of things a function may do to your world state" and guarantees as "the set of things you know the function won't do to your world state". Note I didn't say "safety", I said "guarantees". That's because I think it's a different thing. Very related, but different. In my definition it would be conceivable for a programming language to ban adding the number 4 to an integer, thus allowing you to be guaranteed that no function will ever add 4 to an integer. (Imagine we're in a very unsafe, multithreaded language, so it means you'll never witness a single atomic jump by 4 or something. It's intrinsically a stupid example so don't go too crazy overthinking it.) That's a "guarantee", but it doesn't really improve "safety", because, again, this is pretty stupid. It is absolutely the case that we should examine the relationship between "guarantees" and the much more useful concept "safety". However, I think that from a human point of view it's important to understanding the dichotomy I'm pointing out here between power and guarantees, because, well, like I said, a lot of people clearly enter this debate without that understanding. If it, after the fact, sounds too obvious to be worth stating, well, those are some of my favorite truths to uncover. But before the fact, so to speak, a lot of people have clearly not been thinking of it this way.
Interesting. Thanks for elaborating. I'm inclined to agree. I also think it would be interesting to investigate the relationship between guarantees and safety.
Reading your edit, and thinking about it a bit further: I find that the way I think and work, I also like to have lots of power at my fingertips initially. While I'm writing code that I am not quite sure how I'd like to behave yet. You call this the lisp philosophy. And I like to work like this in any language, including Haskell. So I want most anything to be possible initially. Maybe add a few obvious constraints at the beginning, if any come to me. And as time goes on, and as I understand the domain better, I tend to want to add more guarantees, gradually decreasing the power that turned out I didn't need. Would you happen to work and think similarly? Or, as I gather from your writings, perhaps you start out from the other end: Start with (near?) 0 power, and only forfeit some guarantees as you notice you need more power to accomplish your task? And if the latter, isn't this a frustrating experience for you? I remember experiencing approaching from that end like the following: "The boots are always ever so slightly too tight.", "Gosh, will I need to refactor a great deal again just because it turns out I do need more power in this little spot?...". Maybe it's different for you?
Yeah that doesn't really help with any of this at all. Take a glance at [this comment](https://www.reddit.com/r/haskell/comments/76eygu/comment/doe682q?st=J8TGQ48W&amp;sh=2acdde3b) for further reading into the problem of async + State.
I think they make a good point about how you really shouldn't use Go. I mean "lol no generics" and `if err != nil` are good enough reasons alone to stay away from the language. 
I read the lispcast article, had to rinse my eyes with pure alcohol. SO MANY MISCONCEPTIONS. &gt; An unexpected nil can surprise a good programmer, just as much as an unexpected Nothing from a Haskell function can bewilder even the most experienced Haskeller.[5] Finding out where a nil came from is the hardest problem that nils present. &gt; [5]: Even the best Haskellers complain about not knowing where a Nothing came from. Yes, but *we actually have a way to change our code to `Either` so we know where the Nothing came from*
&gt; But I can make a claim similar to what Haskellers claim about the type system: nil-punning eliminates a certain class of errors. **No, it hides the problem**, gah
When you install Stack, I don't believe that comes with GHC installed. I think you have to do `stack setup` or something like that.
&gt; I ran the installer Which installer? If it was the Haskell Platform then **please uninstall it** and install `stack` by following the instructions on its website. If you installed it properly, then running just `stack` should give you an informative message. Then, IIRC, you need to run `stack setup` to actually install the Haskell compiler (GHC). Next, you can use `stack ghci` the get the interactive prompt. 
What are your research interests? And does the research need to be with an academic institution, or it can be with a company, as well? 
What options are there for rendering font to an image file? I'm looking at gloss at the moment but can't see any options for font
Why do you say `duplicate` is useless? My intuition is that the dual to `mappend`ing two streams is splitting a stream into two streams. Indeed, `dupplicate` is called `split` in [this stackoverflow answer](https://stackoverflow.com/a/23858109). In that answer, `Comonoid` was presented as a sort of interface for violating linearity. Wasn't that what you were trying to achieve with `S.splitAt` in your "Take and Zip" post? Currying-in `List.splitAt`'s `Int` argument, its signature seems to match `duplicate`'s signature: splitAt _ :: [a] -&gt; ([a],[a]) Maybe, `List.splitAt` (and analogously, `S.splitAt`) could be implemented like so: splitAt n = duplicate . someFunc n
I'd be open to anything really, so long as it's worth my time. 
Take a look at session types: * [Lightweight Functional Session Types](http://homepages.inf.ed.ac.uk/slindley/papers/fst-draft-february2015.pdf) * (Type-Safe Eventful Sessions in Java)[https://www.doc.ic.ac.uk/~rhu/sessionj/hkpyh-typesafe10.pdf]
I think we need a notion of 'divisible resources'. If I buy a cake, then it is not possible for me to consume the entire cake and also have it left-over for later consumption. However, in the particular case of a cake, it is possible to cut it in half and set aside one half for later. The interface might look as follows: class Cosemigroup where split :: cake -o (cake, cake) We might also need some way to handle finite divisibility, because a finite datastructure would eventually collapse into singletons. 
Although it may be overkill, you could do the arg-parsing in a more structured way. Optparse-Generic is my favorite library for this.
Yes, `comonoid` provides us with both directions that we need (but not necessarily at the same time). :) 
Oh sweet, didn't know that. Now there's absolutely *no* reason *not* to use `Data.List.NonEmpty` instead of rolling your own.
Yes, this is exactly what I am trying to achieve; the issue however is that when we allow this we also lose the guarantees that we wanted for the safe streams when we loosen the strictness of linearity. :) 
If you are trying to do this on Windows, I have an article written exactly for this: https://blog.ramdoot.in/installing-haskell-stack-in-windows-7c8fd2c79f Try if that works for you.
I see, I like your best case scenario better, because the worst case is essentially coding against an unspecified type-system. My experience with Python IDEs tell me that at one point the IDE stops being able to provide much help, and you don't notice when exactly that happens. It becomes very hard to keep pleasing the IDE unless you're essentially writing Java. When it comes to that, why wouldn't you just go write Java anyway :) I haven't written any ClojureScript myself, but I imagine the reliance on macros must kill the IDE in a heartbeat.
Thank you for highlighting this about Hspec and HUnit. I think that's probably the right path forward.
Thank you for your patience! I've updated my original post with more detail - both are great questions, and I should have done a better job clarifying!
I think the Everest project is very impressive. Such a big project and they are actually doing it and finishing it part by part. 
This seems very useful for things like distributed systems or similar complex protocols. Proofing progress is always helpful in parallel setups. I still think there is a level of complexity below which the overhead of making the types explicit is no longer worth it. 
Yeah, I'll do that when I know enough about haskell :)
 printFileWithLineNumbers = putStrLn . unlines . zipWith (\i l -&gt; show i ++ "\t" ++ l) [1..] . lines &lt;=&lt; readFile
Maybe this? https://ghc.haskell.org/trac/ghc/wiki/Internships
First, can you stop downvoting comments that you don't like? Second, sure, I will gladly take a glance at your refereed comment before answering you thoughtfully
Ok, I think that my proposal eliminates all the impedance mismatch associated with runners and lifters which I think is the subject of this discussion. In your comment, two runners are used as a pseudo-bind to chain computations with different effects due to the lack of expressivity of the bind as-we-know-it. But due to the signature of runners, they only can stack effects, they can't chain them. On the other side the current bind can chain the same effects, but can not combine them. Thus, a bind combinator that can both chain and combine effects is necessary. runners and lifters (and monadbasecontrol et al) are artificial constructs due to the lack of expressiveness of the bind operation as is defined in the monad class. They are type hackery that does add nothing to the logic of the program; To summarize: they do nothing useful. it is accidental complexity that is painfully destroying Haskell as a viable language. With the enhanced bind defined above, even a new state can be bound to the chain without problems , without using glue runners and lifters without "stop solving the problem at-hand, and spend a good number of productive hours" I would be happy to discuss with anyone the details in order to reach an agreement and a solution for this vexing problem.
But then you have to run hpack all the time when using cabal (without stack) and it adds another layer of complexity for what I would argue is only marginally better configuration. I'm not a fan of yaml, because it is so unrestrictive and therefore harder to parse and to understand, things like anchors are overkill in my opinion.
Galois recently opened up applications for its summer internships https://galois.com/careers/software-engineer-intern/. They do also take undergrads, and they are great. 
There is a program called "DAAD Rise Germany" [1] that offers research projects in Germany for undergrads from the US, Canada, Great Britain and Ireland. As a matter of fact, my department plans to offer at least one project related to Haskell/compiler. I was planning on announcing it, when the deadline for students starts ; ) [1] https://www.daad.de/rise/en/rise-germany/find-an-internship/
&gt; The sparse nature of data is wonderful to reflect into data structures. It made a wonderful basis for a hierarchical cache at Level, for example. But when it comes to composing functions or behavior, clojure is bad at it. The way we solve this is Clojure's (underpowered and unloved) macro tooling to create a scoped DSL with a new composition primitive. What exactly is the problem, and how does your new primitive solve it? For LISP languages, I would expect that with composability of data comes composability of functions. Or are you saying that data composition is also a problem? Merging maps instead of combining multiple records with yet another record type seems pretty effective. Not sure how this translates to function composition, however.
Ahh. Cary on, then. :)
Why only these countries :(
https://github.com/DAHeath/ord-graph/pull/1/files And why do you construct a list of `Action`s instead of doing the applicative stuff immediately?
Perhaps `concurrently` could make sense for `Writer`, when the accumulator is commutative.
I'm not going to debate about "using more memory than necessary" but I can't see how using a factor of O(n) more memory than necessary shouldn't always be considered a bug.
Question: how is `List.take 3` different from the following? withMailbox $ \mb0 -&gt; let !(mb1, q1) = enqueue mb0 empty; !(mb2, q2) = enqueue mb1 q1; !(mb3, q3) = enqueue mb2 q2; !() = close mb3 in return $ toList q3
run `hlint`? It'd suggest eta-reducing this, for example: mapM_ (\file -&gt; printFile file) files
Thank you for your suggestion. I'm asking you to open an issue about that. I still don't think that it is reasonable to give up .cabal, but adding the option for people who prefer hpack is optimal solution.
Aww, thanks! It makes me so happy that somebody appreciate my work. This motivates to continue my attempts to help making Haskell friendlier :)
Haha, what a coincidence. I've written exactly this guessing game and it looks like your first version, but with 2 small auxiliary pure functions. But yeah, I'll need some more time to get into the "fp"-ish way of doing things. 
I'm reading and watching Bartosz' amazing Category Theory series and I have a small question regarding natural transformations. For some pair of functors (F, G), F maps an object _a_ to _Fa_ and G maps it to _Ga_. The natural transformation from F to G is described as mapping objects (_a_) to morphisms (_Fa_ &amp;rarr; _Ga_). Is this equivalent to saying that the natural transformation maps the identity morphism _a_ &amp;rarr; _a_ to _Fa_ &amp;rarr; _Ga_? I can't see why the two wouldn't give the same result but maybe there's an underlying reason why one is the "right" mindset and the other is the "wrong" one?
Thanks /u/vrom911 for a nice initiative, I think the more dev tools out there the better. Personally, I get good mileage out of stack templates, but automatic setup of benchmarks is a really neat addition. Re. the project itself: the code is pretty tidy and easy to read but perhaps you could split it up in a few dedicated modules. Oh one very puzzling thing: `instance (a ~ Text, b ~ ()) =&gt; IsString ([a] -&gt; IO b) where` what is this? :D
I'm not sure what you are trying to show, but the problems with take should be pretty well explained in the blog post + the comments here. Anything in particular you don't understand the reasoning behind? 
Use of generics might be overkill, and I personally avoid auto-generated code if I can. I personally use `optparse-applicative` for _any_ program where I want command line arguments. The flexibility in refactoring and ease of setup really makes it worthwhile at any scale.
A couple of weeks ago I read something about stackings of monads (e.g. "IO State Result ...") and the need to use MonadTransformers. So I was worried that with a lot of UI stuff it gets messy
Findings usages of a python method call is a fundamentally different operation. All you have is the method name with no namespace. Any method named the same will look the same to the IDE. Finding usages of a namespaced keyword is a *guaranteed operation*. (Unless you're dynamically generating your keywords in your code. Which pretty much defeats the purpose of keywords as a construct, so... don't do that.) Every namespaced keyword you reference is fully qualified at every point in the code it's used at. Finding those points is a straightforward operation. Clojure(Script) heavily de-emphasizes macros. They do not get in the way at all because I basically don't use them. Perhaps part of the reason for that is because I _do_ use an IDE (IntelliJ+Cursive), and you're right, macros do make it hard to resolve anything. The author of Cursive actually did a [talk](https://www.youtube.com/watch?v=kt4haSH2xcs) on this. He seemed hopeful that things would improve IIRC.
Several things I wish I knew learning Haskell: * run weeder on the project (I don't think it has any spurious dependencies, but you might as well get in good habits). * Add the `-Wall` and `-Werror` flags into the `ghc-options` field of your `.cabal` file. That way the compiler will pick up things that we don't notice. * run `hlint` on your project. * Configure your editor to use `stylish-haskell` so your imports and whatnot are lined up. Onto more substantive stuff: * In your function `printWithLineNumbers` you adopt a thoroughly functional style. That's good! But it could stand to be separated into more than one line for sake of readability. * Don't be afraid to pull in library dependencies. Particularly `optparse-applicative` in this case. It takes some getting used to but it will teach you some higher-level abstractions, and it will make your applications a lot more user-friendly. * Avoid `if... else` when you can. There are often better ways to do it (functionally). Even when there aren't better ways to write it, thinking about it is still beneficial to the learner :) 
Wow thanks for the detailed feedback
No problem :)
Nevermind
As you said, a natural transformation maps *objects* `a` to *morphisms* `F a -&gt; G a`. So it doesn't really make sense to ask how they map the identity morphism. But there is a caveat in Haskell: in `Hask`, a morphism from `a` to `b` can *also* be thought of as a value of the type / object `a -&gt; b`. In category theory terminology, we say Hask "has exponentials". So you could ask how your natural transformation acts on the *object* `a -&gt; a`. That will give you a morphism `F (a -&gt; a)` to `G (a -&gt; a)`. If you want a concrete example, you can use `safeHead :: [a] -&gt; Maybe a`. Here, `F` is the list type constructor and `G` is `Maybe`. The "input" to the natural transformation is the choice of object (that is, type!) `a`.
I currently have my own tool (that includes appveyor) but this looks to be more flexible/better. I may switch in the near future :)
Best to put in a PR when it's an open-source project. Or just fork it yourself. 
I'm not really sure why it doesn't make sense for something to map a morphism. Don't functors do that?
Sorry, I wasn't clear. It doesn't make sense to ask specifically how a *natural transformation* maps morphisms; for functors and other gadgets, it is of course fine.
I'm not sure it's as small a benefit as you purport. Imagine how much easier newcomers might understand formatted output compared to the current flat ones. And seasoned developers might reap a similar benefit on their much more complex data structures.
Thanks for the PR and for obviously spending some time looking through all of this! You're right, my bfs implementation was wrong, I believe I've fixed it. The reason I'm deferring to a list of Actions rather than performing the applicative action directly is that the order that the actions need to occur in is different than the order the new graph needs to be constructed in. So for example, if we DFS over a graph with two vertices and an edge between them, the order the applicative actions should occur in is **v1, e, v2**. However, the order the graph needs to be constructed in is **v1, v2, e** since an invariant the library maintains is that the graph always has vertex labels at every index. Adding the actions was just the way that felt most natural to me to get this done.
It's a common request. For gloss there are no options.
I really wish we could avoid the "tuple foldable instances are evil/good" "debate" this time.
I think the way I'd describe it is that I tend to try to build restrictions from the ground up. If I have some struct that is describing some user, if I know I have constraints on the nature of the username, I can put them in right away to make sure I don't violate them. From the top-down direction I tend to favor power, which is enhanced by the fact that I've eliminated a lot of ways for that power to screw up. This isn't a conscious process; I only really realized I sort of function this way as a result of you asking. Also, my professional work tends to be in Perl and Go. In Perl, I've basically given up, both because the language doesn't really let me enforce much and because I'm working in an established code base where there just isn't much point. In Go, I tend to work as described, but since it's so much simpler it's much harder to really back yourself into a corner with the guarantees it offers because they are so much looser than Haskell's. I do recall especially very early in my Haskell career though very frequently backing myself into a corner with the type system. There is certainly an art to figuring out which guarantees are useful and which are binding you with no benefit and will eventually force you to undertake a massive refactoring.
Why the heck should the uninstall the platform to install stack? The platform _comes with_ stack and in fact _uses_ the stack installer!
I see. Thanks for sharing further details about how you work.
One of the arguments mentioned not to use Haskell is "... the fact it’s a large and complicated language". Now, this doesn't stop the many teachers who teach c++ or java as a first programming language. So it makes me wonder whether this is really a problem. Sure, there are quirks which you cannot really explain to beginners. (Why does `main` return `int`? What is a `class`?) But I don't think these are obstacles in learning the language.
I think it's just one example, but anything overloaded in any language is great for experienced people and tricky for newbies. It just comes down to understanding that a name means something and being surprised to discover it can mean many things.
OK. Round 2. (I re-read your "Take &amp; Zip" blog post) The problem that you identified with `take` is that the eliminated tail of the stream may contain resource-freeing actions, which are required to avoid resource leaks. It occurs to me that one way to deal with the problem is to forbid resource acquisition and disposal in the actions of a stream; if a stream requires a resource, that resource can be provided by `bracket`ing the stream in acquisition and disposal. I think that this is the approach taken by `Pipes`. In a sense, bracketed streams are a sort of atomic unit of streaming that cannot be split into smaller units. This unit is at a coarser resolution than the elements of the stream, so splitting at arbitrary element indices is not a reasonable operation. Suppose that we concatenate several atomic streams. Then, I think that it is entirely reasonable to split up the concatenation into the constituent atomic streams. Furthermore, since the "body" of an atomic stream cannot acquire or free resources, would it be safe to truncate the body to evaluate only the actions that are required to produce the first `n` values?
I'd agree that Haskell the language is rather good for teaching functional programming principles but its implementations are not aimed at teaching. [Duet](http://chrisdone.com/toys/duet-delta/) is my attempt to create an environment better for learning, though that link is just a basic test of the system. I'm currently working on an IDE. Anyway, if you want to teach *functional* programming, then you better have a substitution stepper like Duet demonstrates. Elm, PureScript and Haskell implementations don't have this. 
&gt; Now, this doesn't stop the many teachers who teach c++ or java as a first programming language. The fact it doesn't stop other teaches doesn't mean anything. Other teachers can be wrong, you don't emulate them blindly. 
&gt; If the user can chose the type of the exception then I guess you can't pass any information to the throw. No, the user can choose the type of the exception as long as it has a MonadThrow instance, which allows passing information to the throw. The MonadThrow instance on Maybe just destroys that information.
Thanks, this is an interesting idea. I'll add a note about it on the issue.
As an internet Haskell discussion grows longer, the probability of a comparison involving the length of a tuple approaches 1.
The opposite (*not* overloading) can be confusing to beginners. They come from different languages and *expect* things to be polymorphic.
As someone who’s been learning FP using Haskell, I have to say I find it very good and clean. I can see that it can get complex really quickly, but if the context is learning FP, you can easily avoid going into those dark corners of the language.
You didn't actually address the other comment at all. Your analysis of the problem mentioned by the comment I linked is not correct. The issue is that async and `State` do not mix well. You can't just split `State` into two separate threads. If the state of one of the two threads is changed, that change can't be reflected in the other thread, so when both threads return their states are going to be totally out of sync. The issue is non-algebraic effects, and it's a legitimately hard problem, and I just want to thoroughly emphasize this is totally orthogonal to what you are suggesting. You are not addressing the OP's question. I think you need to actually work on a codebase that uses typeclass constraints / `mtl` style effects because a lot of what you are saying is just untrue. Before you try to propose any changes to the existing `Monad` class, particularly considering there are a huge amount of uses of the `Monad` class that will not want this change, (e.g `[1, 2] &gt;&gt;= join replicate` uses two different `Monad` instances that are both great as they are), I would strongly suggest writing your own class and fully implementing your desired behavior. Remember that `RebindableSyntax` means you can use `do` notation and similar without any breaking changes to Haskell. Once you implement it, do a side by side comparison with `mtl` (and definitely make sure to be open for feedback as you may not have found the right `mtl` approach and make it look much worse than it really is), and then go from there. But just know that mentioning it in this thread is not productive and does not add to the discussion.
It's harder now, since AMP and FTP got merged, and GHC stopped being a Haskell compiler. (It no longer implements any published Haskell report.)
&gt; Why does main return int It doesn't, it returns `IO t`, where `t` can be any type. The return value of the action is discarded. To return a value you need to use `exitWith`. 
You should remove all options. That's not the job of `cat` (cf. [cat -v considered harmful](http://harmful.cat-v.org/cat-v/)).
&gt; I think a realistic expectation is to treat Haskell as a pleasant language to use that lets you focus on solving real problems (as opposed to wasting your time fixing silly self-induced problems like null pointers and "undefined is not a function"). This. I think this is a really good article with very good advice. I'd like to expand a bit on "Avoid big-design-up-front" wrt development workflow/process. One very productive workflow that works for me (which is of course not Haskell specific) is to try to get a very minimal thing working and iterate on adding features+refactor until you get the program you wanted. For my latest project, [imgs](https://github.com/soupi/imgs). I had an idea of what I wanted it to look like and then I just went and did easiest thing that I could actually see that would get me a step closer. It's much easier to keep going when you see stuff changing frequently. So basically what I did was: 1. Get a server to print a "hello world" 2. Get a path as a route and print that path to the user 3. Read the directory in the path from the route and print the names to the user 4. Add html and print the file/directory names in path as a list 5. Add links for directories 6. Split items to two lists, one of directories and one of files 7. Filter out non-image files 8. show files as `&lt;img&gt;` and so on. The thing that Haskell lets you do here is experiment, change and refactor things when they get ugly. You can start with [One big function](https://github.com/soupi/imgs/blob/b935e2637af4d573bf5ac3dd49cc2030bc32149c/src/Web/Imgs.hs) and then separate it to [smaller functions](https://github.com/soupi/imgs/blob/master/src/Web/Imgs.hs) with ease. Just go ahead and implement stuff! It doesn't have to include fancy features or be perfectly designed, because you can rewrite and refactor if you need or want. Haskell will help you do that.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [soupi/imgs/.../**Imgs.hs** (master → 9929f6a)](https://github.com/soupi/imgs/blob/9929f6a5911b358fc16119dcdf244dc09034e964/src/Web/Imgs.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dog34xx.)^.
My complaint is usually "Which Haskell?" Because most folks use specific extensions that can change the language substantially. I think that this is a much bigger problem for teaching Haskell than people realize. It's why I tend to demo with PureScript, because at least in the short term it's fixed and reproducible.
I was referring to students who get to program in c++ as their first language.
In C
That's true. My point was not to copy what they are doing. But we should also look what students find difficult in such courses. And I don't believe that "it's a big language" is a problem here.
Definitely 
When does the application open up?
Thank you! 
what else would it be? universities the world over use it for Functional Programming courses. 
Fair enough. In that case, Either should be enough.
What kind of extensions are you using that change the language substantially? This problem is so overstated. Every time I encountered an unfamiliar language extension, it was no worse than encountering an unfamiliar library or native language feature. And at least extensions in use are documented at the top of the file.
I think not knowing what a class is in C++ is more than a small obstacle. But I do agree with you main point however (no pun intended).
Since Haskell (and GHC and so on) isn't the only game in town, it's useful to reflect a bit on alternatives. University courses have many conflicting demands which only makes a more worthy topic to mull over.
That was not at all clear. And a beginner may get the impression that haskell main returns int...
that's one of the main things that made me abandon haskell.
Could you elaborate on why?
As written on the web page, "the internship database opens between November 1, 2017 and December 15, 2017".
Aren't there some Haskell Google summer of code projects?
&gt; What exactly is the problem, and how does your new primitive solve it? In our case, we started running into issues computing averages for caches which were absolutely required. We didn't want to have to keep pulling down months of financial data into the cache to recompute things, so we started carefully re-evaluating our computations and where possible avoiding using a full 3 month dataset. One place we did this involved modeling the problem as a catamorphism. We'd essentially keep the accumulator around serialized in the cache and use more sophisticated numeric techniques to allow for partial updates. The challenge was that since in some cases we expected new data to be committed to the caches in parallel, we needed commutative operations (we had facilities to serialize this but it was very precarious to do so). So we made some macros to both simplify the management of these operations (you could also pretend it's a comonad-ish thing too I guess, although we didn't model join at all). In another example we were stringing together tons of async operations that may or may not fail and wanted to run that process at scale, so I wrote a macro to let you combine and control the combination of transducers over async operations in a pipeline. It felt a bit like concatenative programming but with a lot of parenthesis. But of course every Clojure programmer is familiar with (-&gt;, -&gt;&gt;, doto) and others. These are more of the same concept in a more restricted scope. &gt; Merging maps instead of combining multiple records with yet another record type seems pretty effective. I didn't say it wasn't. I'm just saying that you often end up in situations where control flow is non-trivial around the updating and analysis of these structures. This is where a haskell's static typing approach shines. 
I can see that's a problem for teaching Haskell, but OP's question was about teaching functional programming, and that can be done in Haskell without any extensions. 
Literally what the guy above said. My complaint is usually "Which Haskell?" Because most folks use specific extensions that can change the language substantially. I judged that the good points of haskell didn't compensate for the costs of using it for my needs and my team, there are a few other reasons but ubiquitously used extensions were one of the big ones. I won't say much more because that might be considered trolling on r/haskell but you have to realize the group if self-selected for people that enjoy and use haskell, so common opinions about impediments etc. might not reflect the truth.
In the same trend, don't write abstractions first. Haskell programmers love their abstractions, and it's easy to fall into the trap of trying to shoehorn the abstraction into your program. Rather, write the program like you would naturally, then when you recognize the patterns (i.e. Monad, Applicative, ...), rewrite it using the abstractions. Use higher order functions to separate similar functionality, and before you know it you have a nice combinator library which describes your problem domain well.
Please allow me to add: learning a new discipline of programming is intimidating. Early days, reproducible examples are important. Reading source code is critical. The diversity in the haskell expression ecosystem exacerbates this. Every early class on Haskell should fix on a standard set of extensions and teach to that, being very careful to explain them in a minimal way. If I had my way you'd have to turn OverloadedStrings off, for example. ScopedTypeVariables is another one I'd just shove on, it is improves consistency. 
That sounds actually like a good argument for a `Prelude98`. Most of the other GHC vs Haskell98/2010 differences I can think of are otherwise trivial.
&gt; Literally what the guy above said. &gt; &gt; My complaint is usually "Which Haskell?" Because most folks use specific extensions that can change the language substantially. No I just wanted to her an elaboration on that particular claim. I made an argument that disagrees, and I'd like to know what about it you disagree with me on.
mine used OCaml
Great advice
Well my previous comment established that I don't agree with that statement. I was hoping you could elaborate on what it is that I said that you disagree with.
I am currently in a intro to functional programming class that uses a few languages to teach, although mostly scheme and Haskell. I can speak for all 30 of us, we really like Haskell, both for it's clean syntax and documentation.
I was hoping you could elaborate on why you think that point is a serious problem. I explained that although it's certainly true to an extent, I believe that extent is extremely minor. I'd like to know why it could be perceived as major.
I'm following Hackett closely and may try using it as a language of instruction.
I just think you're overstating how drastically language extensions modify the language, or how unreproducible they can make an example. Adding language extensions almost never breaks code that works with that extension turned off, so the problem is almost exclusively about whether it's clear that a given feature in the code comes from a given extension, and what that extension does. I've never really seen beginner material that neglected to include necessary language pragmas, and it usually comes with an explanation. So to me, enabling language extensions is just like importing libraries (sidenote: that's why it's important to put them at the top of your Haskell file, not in the cabal file). If the beginner doesn't know what the extension does or what a syntax is when they read the code, it's the exact same problem as not knowing what a library does, where a function is imported from, or what that function does. My point is, this isn't really a problem with language extensions. This is a problem with learning a new language in general. Languages will always come with libraries that the beginner has to figure out. Extensions are just the same thing.
Because extensions change the semantics of common things. And each library have differing sets of extensions. That makes it hard to read code. In the end it's just a judgement of value.
My university taught (still teaches) OCaml as its introductory language during the first 2 years of the 5 years cursus. I started at year 3 and never really looked into OCaml but that could be an idea. I have been using Erlang more or less seriously for years (self taught) and even if it wasn't my first language it was my first introduction to FP. Now Erlang may not be "real" FP given all the state that goes on, it's different type system (called success typing) and the constraints (advantages?) of the fact that modules can exist in multiple versions at runtime. I feel though that the language is small enough and most principles it embeds are simple enough for beginners. When I'm asked what language to start with I say Python3 in most cases and Erlang in the others. If the newcomer has some OOP/imperative experience I feel Erlang is a quicker start than Haskell. And I feel that matters a lot for such people as their biggest fight will be with recursion and pattern matching anyway. Yes, Erlang is not perfect in many regards but a pretty good tool in some cases.
Which commonly used extensions change the semantics of common things? `OverloadedStrings` is the only serious one I can think of. For the most part, they're usually just *adding* syntactic features, which doesn't obscure the meaning of existing code. Also, there's always a language pragma indicating what's going on; this should be read with the same weight as an `import` statement, and unfamiliar syntax comes with the same problems as encountering a new function.
I would say this has got to be the wrong question! Of course, it's a perfectly good puzzle, but practically speaking, it seems to make a lot more sense to ask "What is the right way to weaken a heterogeneous list to a homogeneous one?". And the answer seems clear: upgrade the `HList` to a fairly `vinyl`-like record type: data Rec :: (k -&gt; *) -&gt; [k] -&gt; * where RNil :: Rec f '[] (:&lt;) :: f x -&gt; Rec f xs -&gt; Rec f (x ': xs) infixr 5 :&lt; hoistRec :: (forall a. f a -&gt; g a) -&gt; Rec f xs -&gt; Rec g xs hoistRec _ RNil = RNil hoistRec p (fa :&lt; fas) = p fa :&lt; hoistRec p fas Now we can use gelisam's `Replicate`: data Nat = Z | S Nat type family Replicate (n :: Nat) (a :: k) :: [k] where Replicate 'Z _ = '[] Replicate ('S n) a = a ': Replicate n a newtype VecR (n :: Nat) a = VecR (Rec (Const a) (Replicate n '())) instance Functor (VecR n) where fmap f (VecR xs) = VecR $ hoistRec (Const . f . getConst) xs That, of course, is not the only way to do it. This `Replicate` business, and pinning types down to `()`, seems a bit unsatisfying. One alternative is to encode, in the record type, not only the list of types but also its length: data Rec2 :: (k -&gt; *) -&gt; Nat -&gt; [k] -&gt; * where R2Nil :: Rec2 f 'Z '[] (::&lt;) :: f x -&gt; Rec2 f n xs -&gt; Rec2 f ('S n) (x ': xs) hoistRec2 :: (forall a. f a -&gt; g a) -&gt; Rec2 f n xs -&gt; Rec2 g n xs hoistRec2 _ R2Nil = R2Nil hoistRec2 p (fa ::&lt; fas) = p fa ::&lt; hoistRec2 p fas data Vec2 n a where Vec2 :: Rec2 (Const a) n v -&gt; Vec2 n a instance Functor (Vec2 n) where fmap f (Vec2 xs) = Vec2 $ hoistRec2 (Const . f . getConst) xs
&gt; I just think you're overstating how drastically language extensions modify the language I'm not trying to pick a fight. I'm trying to explain my problems learning Haskell and the related problems of other folks I've run into. I always get a lot of pushback about this point, and I don't really get why. I've grown as a haskeller to where they don't scare me, even though I don't always understand what they do (maybe one day I'll understand the stuff around monomorphism restrictions, I can deal with not knowing today). I'm literally working on a proposal at an education startup for a high quality "Introduction to FP" that'd spend half the 3 month time on typed FP." Part of my tests and reviews of the material all were that adding extensions was problematic because it removed constancy from the examples. I do not disagree. &gt; This is a problem with learning a new language in general. Very few languages have as much switch-enabled syntax doodads. Heck, I just talked to someone this morning, way more experienced haskeller than I. I ended up correcting him that using an extension wasn't necessary anymore because it was standard. He just turned it on by default, that was his haskell. Lots of people have their own askell. I bet mine is more simplified than yours. I bet yours is more powerful than mine. They're similar, but the differences trip me up when reading other people's code, still.
From whatever info you've given, I don't feel that you'll gain much mileage out of using hspec. If I were in your shoes, I'd do the following instead: * write the test suite in the framework / language that is inserting the legacy (bad) data. I'd use the domain functions already present on that language to insert known values in the DB, with all their warts. * next, I'd run parts of the complicated queries and assert them against expected results. If you try to do this in Hspec, you'll realise that most of your effort will go in step 1 above, and you'll end up fixing bugs in the fixtures themselves. 
What's wrong with SICP and scheme? 
People don't teach C++ or Java because they're easy or pedagogically enlightening. These languages are taught because they are the de facto standard of the industry. Most of the programs anyone is going to write is either going to be one of those languages, or in something with a very related syntax, or in some of the dozens of other minor languages that just can't be taught in advance. Haskell has none of those benefits, neither common syntax, nor simplicity nor widespread usage. It's not even a standard among FP languages.
&gt; These languages are taught because they are the de facto standard of the industry. That's not a good reason for those languages to be taught at a university. A universitys purpose isn't to create code monkeys for the industry.
I had a fp class with Haskell a few semester back... almost all of the students hated Haskell itself and didn't see any significant value in the course. That happens when you teach Java in every other class. Embarassing.
Have you looked at its 'successor' as well?
Rank2,NTypes, Existentials, GADT, Fundeps involve stuff that spread to use points.
https://github.com/DAHeath/ord-graph/pull/2 It appears that `foldr act (foldr act empty vs) es` puts `v1` and `v2` in front of `e`.
also, i suspect these extensions will show up in error messages.
You should tell that to my local universities administration. Also, they *got* that impression from the local chamber of commerce and the incoming freshman surveys; so you might want to extend you argument to cover those groups as well. I agree with you.
That can monomorphize things again, but it can't hide the instances.
Hi everyone! As you may remember, I made Bartosz Milewski's epic blogpost series [into a PDF](https://www.reddit.com/r/haskell/comments/73e7l3/i_made_bartosz_milewskis_book_category_theory_for/)! As fun as it was, it was just a straightforward conversion from HTML to LaTeX (using pandoc). I've been working hard to convert all the math in the PDF in LaTeX! I think I've got it, but I need your help to proofread, and most importantly, report (or fix!) any issues you might find! Thank you!
&gt; Now, this doesn't stop the many teachers who teach c++ or java as a first programming language. In terms of order of complication, C++ sits one or two magnitudes above Java. It's unsurprising then, that C++ is not really taught as a first language by many. Self taught developers are more likely to pick up JS, Python etc, and developers taught by course are likely to instead be doing Java or Python. These are not hard and fast rules or something, but there aren't many people teaching C++ off the bat. 
Wow, idea with appveyor is amazing! I didn't have much experience with appveyor, but if you'd like to add support to hs-init I will really appreciate that!
I feel like I read it way back, but was there some justification for tuple only having a length of 1 and it arbitrarily only being defined on `(,)`?
I think there's value in separating evaluation and execution, and the purity of Haskell helps there in a what that Scheme doesn't make clear. Lots of Lisp / Scheme novices tend to over-quote or over-anti-quote things because they aren't exactly clear about the separation. Although, maybe that's just because they never got through the later chapters of SICP; implementing an interpreter should make that clear.
I'd like to add one more advice: don't try to understand monad by reading those thousands of "tutorial" on internet; learn it by start using it and make it type check!
Not that there is anything wrong with Java, but my school does primary teach C. Most of the professors are old school "do it the hard way" enthusiasts, and I'm sure it leaves an impression on the students.
Thank you very much for your feedback! :) I actually don't want to split code into modules and make an executable project because I would like to keep it as much simple as possible. Now you only need to copy one file and everything will work immediately. What about that puzzle, it is needed for the ability to write commands in this way: "stack" ["new", repo, "temp.hsfiles"] 
My university is the same... it's useless.
Well, there are definitely several things wrong with Java.
CentOS is very conservative in general, and 6.0 was released in 2011. If you're not going to update your OS, you probably need to remove the packaged version and install from source.
Sure thing! I'll have a look at it :)
Thanks again! Yes, it does. But at that point we have already traversed the actions in the correct order to yield their effect correctly. Again, when constructing the graph, we construct the vertices before the edges.
The university I studied at (and I know of a few other as well) start with C++ right off the bat. There are two courses, one covering the "C" part of C++ (essentially learning C with the C++ keywords like new and delete and simple constructs like string) and the second part, where objects and object-oriented programming are introduced. No one I know had any significant trouble learning C++ as an introductory language.
Well there is only one possible `Foldable` instance for `(,)`. That is besides always returning nothing at all (`[]`/`0`) or infinitely looping. class Foldable f where toList :: f a -&gt; [a] f = (,) a instance Foldable ((,) a) where toList :: (a, b) -&gt; [b] toList (x, y) = [y] Which naturally leads to `length` returning `1`.
http://felsin9.de/nnis/ghc-vis/
I thought the whole point was not to have beginners worry about the typeclass generalizations that have occurred? Why does this matter if the beginners never even see the typeclasses?
I think one justification would be to think of `(a, b)` as a container with a single element of type `b` annotated with something of type `a` (similar to `Writer`). Maybe it would have been better to use an isomorphic `data` structure with a clearer name instead. I personally havent used this instance, but on the other hand I can't remember a single newcomer stumbling over this issue either. The latter could very well be a coincidence, but maybe people are just not that likely to take the length of something of which they already know the length statically.
But Gabriel! Uncle Bob says you need to More Bigger Design Up Front because the static types make it too hard to change once you've written it. Who do I believe?
Ah - thank you for your help. We're taking daily data dumps of Salesforce data (a third party tool), so the data are what they are once they hit our database. Other teams are working on process improvements and adding constraints in Salesforce to resolve that. Our team was tasked with getting a sense of how that day's dump violates a given set of rules. Currently the only code we have is 50-ish SQL files that compare fields across multiple tables through some elaborate joins and filters. Figured it'd be good to write some specs to make sure those SQL files are doing what we expect them to, and also prevent regressions in the future. I thought this might be a good first Haskell project at work since it's not driving a main app and is just validating other scripts. But it might be overkill.
you're on a "dark path" son... a dark dark path..
My problem with articles like this are that, as a haskell beginner, I don't actually see the end of the road with haskell. With a language like python or javascript I understand how to get to the end of my project. I understand how to ship a thing. Haskell seems ok to me for very small things, but the things I do regularly in other languages are much bigger than I can manage in haskell. I've tried his "Build something useful" advice before. With python (which I knew none of before I built a useful thing) it took me less than a day. With haskell, I was still reading stack exchange and tutorials and then gave up (and then probably wrote what I was trying to do in a different language). From somebody who's tried and given up haskell a few times now, the article seems a bit condescending.
I'm in a dark place myself now thanks to you guys: &gt;The rules of the language insist that when you use a nullable variable, you must first check that variable for null. So if s is a String? then var l = s.length() won’t compile. Instead you have to say var l = s.length() ?: 0 or var l = if (s!=null) s.length() else 0. &gt;Perhaps you think this is a good thing. Perhaps you have seen enough NPEs in your lifetime. Perhaps you know, beyond a shadow of a doubt, that unchecked nulls are the cause of billions and billions of dollars of software failures. (Indeed, the Kotlin documentation calls the NPE the “Billion Dollar Bug”). And, of course, you are right. It is very risky to have nulls rampaging around the system out of control. &gt;The question is: Whose job is it to manage the nulls. The language? Or the programmer? [Source](http://blog.cleancoder.com/uncle-bob/2017/01/11/TheDarkPath.html).
Well there are two questions. The first lifted async is a particular case of the second, lifting, and I answer the second. I can say that the question that you insist, which is a very particular is legitimate but not as interesting as the second case. Despite my nick, I have many years of experience in Haskell programming monad stacks in asyncronous and distributed programs, among others, So don't worry I understand perfectly the problem. Simply I don't consider it interesting. I think that the current definitions of the current monads can be translated without problems to the new definition. I is a question of type signatures, not implementations. &gt; But just know that mentioning it in this thread is not productive and does not add to the discussion Oh, so anyone can write wathever speculative thing that may think in any thread, as is notorious in Haskell. But I can not point towards what I believe is core cause of both questions? Next time I will ask your permission. sorry. Do all of us have to ask you por permission or is only me? 
&gt; With a language like python or javascript I understand how to get to the end of my project. Didn't you start with Javascript or Python like that? Not knowing how to get to the end of the road? Not knowing how to take a problem and code it out to do what you wanted? Learning Haskell is the same. I am also a beginner Haskeller, and I can tell you, right now, that I still don't know how to get from point A to B on a big project, and thus, can't see the end of the road.
Oh, it will *definitely* help. I'm just saying it's not a perfect solution, not that it shouldn't be done.
&gt; Well there is only one possible Foldable instance for (,). Because of the form of an "instance head" can't include type-level functions (type-family applications). Otherwise you'd also have the version that folds / traverses the first element of the pair.
 This! JS: &gt; "1"+"2" '12' Ruby: irb(main):001:0&gt; "1"+"2" =&gt; "12" Haskell: Prelude&gt; "1" + "2" &lt;interactive&gt;:1:1: error: • No instance for (Num [Char]) arising from a use of ‘+’ • In the expression: "1" + "2" In an equation for ‘it’: it = "1" + "2" 
But the OP is asking basically entirely about the former, the second already has an answer: use `mtl` or perhaps free monads. Now don't get me wrong there is still more to be done in those areas (such as the n^2 instance problem), but if the OP was asking about algebraic effects that is a perfectly good answer. I believe you might have plenty of Haskell and even transformers experience, but have you tried actually avoiding monomorphizing your monad stack, and using typeclass constraints. Because I'm telling you man for the thousandth time that what you claim to be some huge problem is downright negligible with typeclasses. All your change does is wreck type inference (no information is propagated backwards nor between arguments). The specific monad stack that ends up being used when you call the function is not important, what's important is the signature of the function itself which `mtl` does a nice job of handling. Using your `&gt;&gt;=` type signature for `[]` (and every other non-stacked monad) sounds awful, I mean the kinds don't even matchup, so you'd have to wrap and unwrap `[]` every time you wanted to call `&gt;&gt;=` on it. I'm just going to tell you write off the bat that getting `&gt;&gt;=` redefined within Monad to the signature you want won't happen. But remember that if you do genuinely care about the issue and are still convinced you are right, you can always implement it yourself with your own class and use rebindablesyntax if you want do notation. Then in the process of implementing it you will be able to more holistically evaluate it and any difficulties you had implementing it (non commutativity of effects is the first thing I can think of, as well as type inference), and you will be able to try and convince others to use it. I understand some things are very hard to actually go and do yourself (e.g they require a change to the internals of GHC or are just really really hard) but what you are proposing you can implement right now. So do it man. And don't get shitty about me claiming your question isn't productive. It's not the core cause of the question, your answer does not deal with non-algebraic effects, which is the true core cause of the question. Please just implement it and post it on /r/haskell so that a proper discussion can be had, and we can either put this claim in the ground, or give people the opportunity to use it if it really turns out to be beneficial. All this postulating over the solution something that I personally do not see as close to a problem is just a waste.
Yep, and for good reason, trying to allow type-level functions would be extremely hard on the implementation side (probably devolves into undecidability), and would make reasoning about code and types harder. I should also finish my answer by saying there isn't really a great reason why there isn't an instance for `(,,)`, `(,,,)` etc. as far as I am aware.
&gt; there isn't really a great reason why there isn't an instance for (,,), (,,,) etc. as far as I am aware. Yeah, if we are going to provide instances for pairs, we should probably provide instances for tuples up to size 16 or so, similar to `Ix` instances.
I've read some John Grisham stuff before, will I like this?
Oh yeah, optparse-applicative is a solid choice and probably better for refactorability. I just love the terseness and data-driven feel of optparse-generic. To each their own!
I'll make an analogy to seatbelts. They restrict your body's freedom of movement, but now you can drive your car faster. The whole reason I'm sitting in my car is to drive, so I optimize for the latter
&gt; The question is: Whose job is it to manage the nulls. The tests? Or the programmer? 
Have you considered asking in the subreddit/IRC/Slack for help on how to get to the end of your project?
The advice I would give in your case is to read the [Haskell Programming from First Principles book](http://haskellbook.com/) The only reason you learned Python in a day is because you reused your existing programming background from another procedural language. Haskell doesn't reuse your existing experiences from other programming languages so you have to learn the language from scratch. I recommend the above book because it teaches Haskell from scratch and does not assume any programming background whatsoever
I believe you will, especially if you like stories where good triumphs over evil. ;-)
Haha oh no !!
I'm going to shamelessly plug [my earlier post](https://www.reddit.com/r/haskell/comments/6jujqn/put_the_element_typeslengths_of_lists_strings_and/) as it is a best of both worlds solution to that problem. You can still use `[a, b, c]` syntax to build nonempty lists, but you don't get any partial functions / literals.
You definitely can. I mean `fromNonEmpty` already exists: `toList :: Foldable f =&gt; f a -&gt; [a]`. 
One approach would be to use nix to get a much newer set of packages. You'll have the latest cabal and ghc available.
Well, `+` for string concatenation is truly a dubious choice. `+` is a commutative operator.
Seatbelts are unnecessarily restrictive. Good drivers don't crash, so we don't need seatbelts.
It looks like you are trying to answer a homework problem, but your questions indicate that you don't understand the question at all. Perhaps review the course material, or if that material doesn't answer those questions, point out to your teacher that the question assumes knowledge which wasn't explained?
Only if you won't miss plot holes. If you do, then I'm afraid the consistency and prevalence of logic might put you off.
GHC was not a Haskell compiler before AMP. For example, it handles sections of bottom-valued operators incorrectly, and Num has had its superclasses removed.
Can't teachers use their own prelude? Heck, can't they progressively build the prelude they need by collecting the functions they introduce in their lectures as examples of deep concepts? In math you don't start your bachelor with an already available library containing the state of the art and practionners are not limited to what a beginner can grasp. Why should it be any different with a programming language?
I can't help but feel you're externalizing your issues. I had some experience with Python and had read some of the beginning of Learn Yourself a Haskell For Greater Good. I ran into a problem that would have been be painful with Java's (sadly my main language back then) boilerplate, so I tried out solving it in Python. Well I had some trouble but as my problem was essentially very functional, I decided to try out GHCi and managed to nail it despite having to look up literally everything. I was then motivated to learn more functional programming by the experience. So was it Haskell or me and circumstances that gave me that early success? I suppose a neutral observer could explain it either way or something in between. Point is, perhaps your problems weren't the language (of which you didn't mention any specific troubles about), but rather your background, the resources you used and so on.
&gt; it handles sections of bottom-valued operators differently That's a new one on me. Is that part of the "postfix operator" changes? So that `(val +op+)` desugared to `(+op+) val` instead of `\y -&gt; val +op+ y`? Or is that something different?
&gt; I wonder if there are other people who are actually using it for good. `Foldable` is a superclass of `Traversable` which is pretty useful to have on tuples.
&gt; I wonder if there are other people who are actually using it for good. `Foldable` is a superclass of `Traversable` which is pretty useful to have on tuples.
Definitely suboptimal, but "bug" necessitates some sort of felt pain. If you never feel it, it can't really be called "pain."
This is uncomfortably close to the "guns don't kill people, people kill people" meme.
I'm honestly surprised it has a 7. I was expecting 6.something.
(1) the brackets define groups. Normally, if you say `f g x`, you mean both g and x are passed to f. `f (g x)` means one value, namely `g x`, is passed to f. Note `f (g x) == f (g (x))`, which is notation you probably know from other languages; mixing notation, `f g x == f(g,x)` * (2) You are right that we can't give these variables monomorphic types, but we can certainly say some things. For instance, `f` is not an Int. * This is not true in Haskell, where `f g x == uncurry f (g,x)`. That function (`uncurry`) is named for the same person Haskell is named for!
Is this website blocking anyone else? I can access through a proxy, but for some reason I can't through my own IP address (London)
I wonder why we don't have a Num instance for tuples and functions then.
I agree that pretty-printing in GHCi is beneficial, and we already have it. I'm talking about the small benefit of saving the 5 minutes of turning it on. I could be wrong, but I'm guessing that the hard-working and already strained GHC team are unlikely to put that at the top of their priority list. 
The abuse of Q internals is a really slick trick!
Are you interested in using Haskell Platform specifically, or are you just trying to get a working Haskell installation? If you want the Haskell Platform specifically, I would look into installing it from source so you can use the latest version. If you just want any Haskell installation, I suggest using [stack](https://www.haskell.org/downloads#stack), which handles installing GHC and any packages you need. Just curious, what is tying you to Centos 6.9? The only time I've used such an old OS was when I was making packages for Python's `conda` ecosystem, where we specifically wanted to compile any dependencies on an old OS.
Honestly, I feel like the primary difference is that in Python or JS there is probably a nearly idiot proof tutorial on how to accomplish x newbie friendly project, for nearly all sane values of x. With haskell, that may or may not exist, and if it does, there's a 50% chance the author spends 4-5 paragraphs waxing poetic about category theory in the middle somewhere. It is not that haskell is harder or easier to learn - for one, that's subjective, and two, it's not an opinion I hold anyway. It's that, for those popular languages, no matter how your mind operates, there is someone out there who has written a tutorial that you'll find simple, by pure virtue of the volume of different minds that have done so. Haskell just hasn't had that many different quality explanations of the basics. It's not even that the one's that exist are bad; it's purely that there aren't as many different takes.
In one of my current projects doing it your way was never really an option: I'm using RankNTypes and a custom monad to ensure all values of a particular other type represent self-contained directed acyclic graphs. Without `join` or `&gt;&gt;=` the graphs would be limited to single nodes.
Uncle Bob mixed up "too hard" with "a bit easier"
I'd like to elaborate on my experience with "avoid big-design-up-front" (and possibly reconcile a little bit with /u/lightandlight's [comment](https://www.reddit.com/r/haskell/comments/76r04v/advice_for_haskell_beginners/dogh085/)). Haskell forces you to write code that follows the design you chose. It's difficult and unpleasant in Haskell to try to work around a bad design; usually it involves boilerplate, either when constructing or when deconstructing. Every design decision you make has a non-zero chance of being wrong. To make matters worse, when doing your initial design, you have a less deep understanding of your domain. The more you design up front, the more design mistakes you will make all at once, and the worse your implementation will be as you fill it out. If you design incrementally, you will have to do a lot of refactoring as you iron out the design kinks, but each individual refactoring will be relatively painless.
This is one of those questions where if I answer 'yes' I get labeled a smug bastard, isn't it?
I get a fair bit of mileage out of it.
Since I've seen scheme discussed and I've not seen mention of this paper already, Wadler's [Why Calculating is Better than Scheming](https://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf) (1987) lays out the case for using Haskell in education over Scheme. (Whether or not you agree with him is up to you, of course.)
You can just download cabal and ghc binaries from the respective websites: https://www.haskell.org/ghc/ https://www.haskell.org/cabal/download.html
I wish we could too, but sadly its the entire point of the article :-/
Unfortunately, not everyone is a good driver and being a good driver doesn't protect you from collisions with bad drivers
The Num instance for functions should be there but isn't for historical reasons. It didn't exist originally because Eq and Show were superclasses of Num and when Igloo ripped them off to enable it, and other uses of Num for expression types, he didn't add it in to base. I'd personally have no particular objection to adding it rather than making people float it in as an orphan. It is one of the very few orphan instances I commonly use. On the other hand, I can also see the point that ghci&gt; let x = 100 in 2x 2 is rather surprising. In general when both sides have a point, we tend to let the status quo prevail, even when it results in a slightly inconsistent state, and focus on the things where we can build consensus.
True, it should be `*` for concatenation, since that's not typically commutative in a given ring. Much less confusing that way!
I don't have any objection from a CLC perspective to adding the missing instances up to some arbitrary bound. We typically go up to 5 or so for most things.
If you teach it right, C++ can actually be a pleasant language to learn CS concepts in. You obviously can't cover everything in it, but over the span of a few courses (or years) you can teach basic control flow, functions and looping etc, OOP, some functional stuff, generic programming with templates, multi-threading via threads, atomics, or higher level promises/futures, yadda yadda yadda.
I like that idea! Like a progressive prelude that starts out with training wheels.
It is so telling how much different this thread is than the one on HN. It’s just sad over there. So much cynicism. 
Nah you just go to HN and join the rest of the bastards
I think their point is that you might write a monomorphic version of `join` or `(&gt;&gt;=)` and only later realize that the `Monad` typeclass encapsulates the pattern. This post gives several examples of writing non-abstracted code and stepping back to see the overarching pattern: * [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html)
Good drivers don't need road signs or lane dividers either, so let's get rid of those because they just slow good drivers down. Same goes for traffic lights. Cars are built with safety features to minimise damage to the driver and the victim in the event of an accident, but everyone knows that won't happen to good drivers so let's cut the unnecessary weight so they can go as fast as possible! &lt;/sarcasm&gt;
How do I link with a static lib (libx.a) using cabal ? (or nix) right now I am doing this to create the executable ghc Main.hs csrc/my-c-interface.c libx.a Please note the libx.a unfortunately does not have a shared lib (.so)
yeah i really learned a lesson with that appeal to extreme. how foolish of me not to treat my programming language like a knife.
I help maintain [IHaskell](https://github.com/gibiansky/IHaskell) and would appreciate any contributions: refactoring, new `ihaskell-display` instances, bug fixes, etc.
I'm relatively a beginner, and I like the idea of passing in a data type to a function instead of using a typeclass. There is an old post by Gabriel (http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) that is an interested read in this regard. What I like about not using typeclasses is that you get to choose the instance for each situation. Not everyone wants to order a Maybe the same way for example. There is another article somewhere saying how you can use typeclasses instead of free monads (again in the theme of keep it simple!) to create a DSL. However if you use passing a function instead of typeclasses you can then choose from an unbounded number of implementations, not just the one you define for the instance. I think this could be very useful, to switch from a database back end to a file-based back end, for example.
The whole tuple situation with picking arbitrary cutoffs does kind of irk me. I wish there was some sort of first class way to convert type level lists/arrays to contiguous data structures
Hi there, I'm wondering whether you might use a math font that matches the text font, e.g. with `\usepackage{libertinust1math}`. I can submit an issue/pull request for this if need be.
This really wouldn't be much of a project without options.
Interesting! Sure, PRs are always welcome. I'll play with it as well, let's see how it looks. I'm using `unicode-math` as well, and it changes the way some thing look in general (like, \prime), so let's see how they play together. 
The end user?
So it's my job to manage the nulls. I agree! Now, why would I pick the language that makes my job harder?
Can we get a version that uses `f ; g = g o f` composition?
It's opensource, feel free to build one 😂 I've seen someone fork it just to remove the C++ code... 
I shall do both then.
Every decade or so world is thrust upon by a language meant to make programming easy. The main idea of these "easy" languages seems to be get rid of some of the great ideas that were discovered in the past few decades of programming languages. Needless to say they have some big-shot "enterprise" backing them and lots of programmers start getting on to the bandwagon. Racks and Racks of books appear starting with Foo for dummies to Advanced Foo. And the language becomes wildly successful. Cobol, Java and Go seem to follow the same path. Note I did not include Javascript in it not because it is not bad, but because the users are humble enough to admit that there is a problem with that language (despite nodejs of course, that is an aberration). 
&gt; I should also finish my answer by saying there isn't really a great reason why there isn't an instance for (,,), (,,,) etc. as far as I am aware. It's been proposed, but the discussion immediately got sidetracked and became yet another back and forth about the Foldable instance for (,).
You might also be interested in https://github.com/plow-technologies/servant-auth#readme
&gt; Can't teachers use their own prelude? Heck, can't they progressively build the prelude they need by collecting the functions they introduce in their lectures as examples of deep concepts? I know of at least one school where the students, in order to learn C, build their own stdlib from scratch during the first year. That sounds like a good idea for Haskell.
&gt; OverloadedStrings is the only serious one I can think of. `ScopedTypeVariables` should qualify too.
Just because you could add options doesn't mean you should. Unix philosophy. Every tool has one job. `cat` concatenates files. That's it.
Well, I like using MonadThrow here so I don't have to decide for the user. Sometimes they're not going to care why it failed (e.g. if they're building a holiday list and some dates won't exist) sometimes they will.
Oh wow, I added Libertine Math, it looks good! (needed to rearrange some packages first.) I think I'll keep it :) thank you for the suggestion! 
Ok thanks, so I didn't completely misunderstand the article. :) I set things up as you (well, actually /u/Barrucadu) said but I'm still concerned that I'd be putting a massive inconvenience on my users if I do that. It would be nice to handle the errors as you describe except I don't want to force every client to have to use nested do's to do anything with my functions. I suppose something like: instant &lt;- now tz &lt;- loadFile tzFileName let zdt = fmap (fromInstant instant) tz isn't the end of the world but it's certainly less convenient than instant &lt;- now tz &lt;- loadFile tzFileName let zdt = fromInstant instant tz And, having seen how it would need to be used, I'm no longer convinced that it provides enough value to justify. Especially considering that while the `fmap` case doesn't look too bad, it will take more effort if the user really wants to know what went wrong. What do you think? Am I overthinking this the other way now? :)
Tifu by printing it yesterday.
Come to India and you will get those roads designed for good drivers ;-)
Yup! The Report demands that sections be equivalent to their obvious Lambda counterparts, but (%) = undefined (1 %) `seq` 2 -- bottom (\x -&gt; 1 % x) `seq` 2 -- 2
it is impressive what this supposes for the future performance of Haskell in real world applications. "The pointer tricks of C used with Haskell's guarantees". Great.
Not if you explain why :)
I probably should have guessed. Since the `(,)` instance is already there and not going to be removed it seems pointless to prevent the other instances from being added.
Typeclasses should be compared to C++ concepts now, shouldn't they?
Anyone know if there will a book printing done of this? I can't read on screens for long and this looks really interesting
Hello! I know this thread is rather old, but I have to ask you: What editor do you use for development? I really want to try Reflex out, but currently I run Emacs with Intero, which is dependent on Stack, and consequently does not work with the Nix setup Reflex platform uses. I can't imagine coding without at least syntax highlighting, but would really really like to keep the flychecking as well.
I'm not saying you cannot teach fp in other languages. i started learning in javascript myself. i'm saying haskell is an established option and a very safe bet.
That's what happens with DrRacket. The interpreter can run different environments and there are specific environments for teaching material.
Is there a link to that? It seems impossible to find hacker news links via google or hacker news itself.
[NoImplicitPrelude](https://wiki.haskell.org/No_import_of_Prelude)?
Maybe OP will find it comforting that the derivative of a regular plot is its plot of one-hole contexts.
I assume the same is true for other Num instances for every applicative type that we have in base, right?
I found `selda` to be fairly simple to use, and it has a `sqlite` backend.
Thanks, that's exactly the kind of thing I was looking for, as I would prefer to access the data using an idiomatic way rather than having SQL in my code.
Yeah, exactly! I certainly did enjoy my time with C++ at university and learned a lot!
Like `+` in JavaScript, for example. 
Focus on the `do` notation. It is fully imperative. `main` is a `do` block anyway, all the brouhaha about it being represented as a "monad" values *in* Haskell only matters for language implementers anyway. Use as much explicit braces and parentheses as humanly possible, at first. Haskell is just another over-hyped [JAL](http://www.abbreviations.com/term/121703). It [over-promises and under-delivers](http://conal.net/blog/posts/can-functional-programming-be-liberated-from-the-von-neumann-paradigm), in the declarativeness and equational-reasoning department. Monads are an (ugly/beautiful) hack which was comfortable enough for the very smart people behind Haskell to stop its development from going where it should have been going. Because they are so very smart. The success was *not* avoided at all costs. All of the above statements are my vague feelings. I'm not prepared to defend them with any degree of rigor. I'm also a Haskell beginner.
Focus on the `do` notation. It is fully imperative. `main` is a `do` block anyway, all the brouhaha about it being represented as a "monad" values *in* Haskell only matters for language implementers anyway. Use as much explicit braces and parentheses as humanly possible, at first. Haskell is just another over-hyped [JAL](http://www.abbreviations.com/term/121703). It [over-promises and under-delivers](http://conal.net/blog/posts/can-functional-programming-be-liberated-from-the-von-neumann-paradigm), in the declarativeness and equational-reasoning department. Monads are an (ugly/beautiful) hack which was comfortable enough for the very smart people behind Haskell to stop its development from going where it should have been going. Because they are so very smart. The success was *not* avoided at all costs. All of the above statements are my vague feelings. I'm not prepared to defend them with any degree of rigor. I'm also a Haskell beginner.
Here's a comparison of the two by Andrew Sutton: https://stackoverflow.com/a/32147454/866915 
Oh I totally misinterpreted your heading. I thought you meant you'd taken the derivative of the intermediate representation as a *type*.
[removed]
Me too.
Wait... There's c++ code? 
Thanks for the mention -- I am very much interested in that, it could have saved me a ton of work had I known it existed! Updating the post so others don't miss it.
&gt; I can't think of any advantages that route offers Please don't forget about stack issues [2197](https://github.com/commercialhaskell/stack/issues/2197) and [2048](https://github.com/commercialhaskell/stack/issues/2048). It would be nice if those could be fixed. To build a Gtk+ app like Leksah with `stack` we currently need a [big stack.yaml](https://github.com/leksah/leksah/blob/master/stack.yaml) and [another one for macOS](https://github.com/leksah/leksah/blob/master/stack.osx.yaml). Plus the Gtk version needs to be [specified manually](https://github.com/leksah/leksah#other-os-with-gtk-316). Currently `cabal new-build` is not perfect (until `cabal new-install` is done we still need an [ugly script](https://github.com/leksah/leksah/blob/master/leksah.sh) to set up the environment), but the [cabal.project](https://github.com/leksah/leksah/blob/master/cabal.project) is much simpler and I would recommend `cabal new-build` over `stack` for anyone who intends to use Gtk+. Also `cabal new-build` works really nicely with nix in a way that I don't think `stack` can currently (correct me if I am wrong). We have [a script](https://github.com/leksah/leksah/blob/master/leksah-nix.sh) that spins up a shell with all the native packages we need plus most of the Haskell ones (they show up in the global `ghc-pkg list`). Then it just runs `cabal new-build` based script inside that shell.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [leksah/leksah/.../**leksah-nix.sh** (master → 3cd43fa)](https://github.com/leksah/leksah/blob/3cd43fa3c5538bf4104172f8345c1c9acc9f14c2/leksah-nix.sh) * [leksah/leksah/.../**stack.yaml** (master → 3cd43fa)](https://github.com/leksah/leksah/blob/3cd43fa3c5538bf4104172f8345c1c9acc9f14c2/stack.yaml) * [leksah/leksah/.../**leksah.sh** (master → 3cd43fa)](https://github.com/leksah/leksah/blob/3cd43fa3c5538bf4104172f8345c1c9acc9f14c2/leksah.sh) * [leksah/leksah/.../**cabal.project** (master → 3cd43fa)](https://github.com/leksah/leksah/blob/3cd43fa3c5538bf4104172f8345c1c9acc9f14c2/cabal.project) * [leksah/leksah/.../**stack.osx.yaml** (master → 3cd43fa)](https://github.com/leksah/leksah/blob/3cd43fa3c5538bf4104172f8345c1c9acc9f14c2/stack.osx.yaml) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dohfrzn.)^.
Hello. You don't actually have to use Nix to use `reflex{,-dom}`. I usually build with stack. It was a bit fiddly to get started with, but at the moment it builds quite well for me. Search the web for `"reflex-dom" "stack"`, and I think you'll find instructions. If not, tell me and I can try to help you. And if so, still tell me so I can celebrate with you as well. :) 
Yep, thought it's gonna be something along the lines of https://github.com/paf31/purescript-firkin, but for LLVM's general IR, which would have been pretty big news.
I've often heard people mention that Haskell designs are hard to change. However, that hasn't been my experience at all. I've had much more trouble straightening out OO designs (in c++, python and delphi mostly) than my Haskell code. What often happens is that I end up with a very large number of individual functions that I can replace one at a time and the core of my program is very small.
&gt; [...] if the user really wants to know what went wrong. What do you think? If you really want to know what I think, I have a [long video explaining my thoughts on error handling](https://www.youtube.com/watch?v=8xkG660D6bI). In particular, I don't think you should leave it up to the caller to decide whether they want to handle your error or not; as the author of this library, you are in the best position to decide whether this problem should be handled before or after your function is called, and therefore whether you should use types which encourage your users to handle that case or not. It's a thorny topic though; I disagree with most of the suggestions in the Snoyman post you linked, so I suspect he'd disagree with most of what I say in that video.
To some degree, what `djinn` or `exference` do is close to what you wish for there. Couple of related notes: - I am not sure if exference's algorithm would qualify as a rete algorithm. Either way i am mildly convinced that its algorithm is not perfect yet and there is more than a linear factor to gain still, but i am not keen on rewriting that project entirely. - wildcard-queries like your "Day -&gt; _" example are not supported in djinn/exference, although it is not a harder problem. The output will just often contain a lot of junk, which makes such queries less useful. - I have made the default dictionary of exference bigger than that of djinn, yet it is far from the whole hackage dataset. You really need a rather clever algorithm if you wish to work on the full, unfiltered dataset. - The main problem are type classes and generalized functions. If you have 1000 ways to go `a -&gt; b` given some constraints over `a` and `b`, and those constraints may be solvable, you will have a hard time, no matter how clever the algorithm prunes. And excluding generalized functions is not really an option for such a haskell query engine. - That said, exference was not built with this as the main usecase. It may very well be possible to write a tool that can cope with the hackage dataset size where exference would fail.
Note that there's a discussion about making servant-auth the standard way to do auth with servant at https://github.com/haskell-servant/servant/issues/805
Yes.
I really like the simplicity and flexibility of `acid-state`
Ah, I recognize that username -- Thanks for the continued work on the library!
With python, the beginning to the end was so short it amazed me. It's true that javascript I've been writing for 10+ years so I understand the language really well at this point. I also understand the things I can do with those languages. I was able to do something similar with clojure but it took me more than a day. Haskell mystifies me for some reason. Maybe it's that I just don't know how to find the right resources as a beginner.
Added above
Cheers buddy - looking forward to a good read. I really like the idea as well, I'm a software developer with ~5 years experience but I'm also a maths undergrad. Hopefully I can make it to the end :) The Haskell community in general is great for providing bridges and insights into some very interesting theory.
That's certainly possible. I'm kind of settling into enjoying languages that are halfway between functional and OOP. Javascript (mostly I write typescript now) and python and kotlin have just enough functional stuff in there to help me keep my code simple and static. But when I hit a problem I could google and usually find a way to get past my misunderstanding of syntax or whatever. I might just have trouble asking the right questions or asking questions the right way with a purely functional language like haskell.
Thanks. I'll check that one out. I've been reading slowly through Learn You a Haskell for a few months now but I still feel like I'm missing something. Maybe a different starting point would be good. I'm fine with letting go of my baggage. I've also been playing around with clojure and lisp and find those easier than haskell for some reason.
I'd kind of forgotten that IRC exists and I'm too much of a hermit for slack. Next time I start a new haskell project I'll have to try those channels.
&gt; acid-state I like the idea that it keeps a kind of event log from which it rebuilds the current state. I wonder if it will affect performance much?
Yes, I think that C++ classes can only be compared with typeclass + existential to get the same notion of interface and virtual dispatch.
https://www.haskell.org/platform and cabal-install is considered obsolete. The currently recommended way to install Haskell and keeping the installation up-to-date is described at https://haskell-lang.org/get-started .
Your point is unassailable. But it misses the mark by a light-year. This is not a chance to debate design choices. The OP is obviously looking to get his toes wet by making a small toy application in haskell. Removing a feature from the command reduces the project complexity to the point that it's no longer useful for that purpose. 
I definitely think comparing typeclasses to C++ classes is a really meaningless comparison. Apples v. oranges, etc.
I've often used either simple JSON file serialization or a more experimental Git-based serialization. An experimental way to do Git serialization can be found on https://github.com/lessrest/restless-git and on Hackage. I've been meaning to post about it but I haven't yet...
What would that even mean?
I can definitely relate to this experience, and actually believe this to be one of the main problems when learning Haskell: While the core language is quite small and simple, you *also* need to learn many abstractions built on top of it. `Monad` is the prototypical example, but this also applies to streaming, lenses, parser combinators, concurrency and such. Add to this the multitude of GHC extensions almost any library requires, and it takes a while to get from thinking you understand the language (because you know the basic building blocks) to actually being productive. This period can feel quite daunting precisely because one thinks that one should be productive already. Other languages have the same problem, of course, particularly with large frameworks like Rails or Angular. The difference is that one can usually make do without those and get something out of the door, which I feel is more difficult in Haskell because the abstractions are more ingrained in the library ecosystem.
I have been doing similar experiments, it is indeed very fun. I've slowly but surely been working on turning my experiments into a somewhat useful DSL. And yes, the LLVM optimiser will definitely get rid of a bunch of superfluous operations :)
The Haskell `persistent` library has a `sqlite` backend which is packaged and included directly with the library, for no external dependencies. It's nice to use.
Sorry to reply with just a link, but this article explains it quite well and reasonably intuitively imo: https://pavpanchekha.com/blog/zippers/derivative.html
You can use `createCheckpoint` and `createArchive` to circumvent this. If the size of your data is reasonable, I think it does a very good job.
What is the reason for functions not to be specialized by default?
Hi Another new haskell guy. Because the community really likes vim/emacs i decided to try spacemacs... It is a handful. Anyway I am missing 2 features in spacemacs. I don't know if i can actually get them working (or if other IDE's solve these problems) - 1. I would like to be able to stack test and stack exec without going to console (yes, even a console inside spacemacs). When i go to leader c there are haskell/cabal options that do nothing. 2. i would like to be able to go to definitions of imported packages to actually see how they are implemented. Saw Something like it in intellij when i downloaded the haskell plugin but i couldn't find it in VS code or spacemacs. Any solutions for me?
Sorry, but I'm not *that* crazy... yet.
The central concern about high-level concerns not depending on low-level concerns is just as applicable in Haskell. If you look at the dependency graph of your source code modules, that tells you quite a lot about the system's "architecture." So if inside `App.CrucialLogic` you have `import Database.MySQL` then from an architectural perspective you're saying "the crucial app rules are dependent upon a specific MySQL library." There are many ways of achieving the "dependency inversion" that Mr Bob recommends. The "free monad" craze (see also "operational monad," etc) is due to that pattern's very clear way of separating domain logic from implementation details. It can also be as simple as having the crucial functions in `App.CrucialLogic` accept some functions as arguments which it can use to interact with the database without depending on the database library.
By the way, I like to use JSON files that contain arrays of events, instead of serializing my live data structures, because of the basic advantages of event sourcing...
I see. Well, just start doing something with Haskell. I'll start a haskell project today. One great resource is the "Haskell Book" But, I think that only teaches you the syntax of Haskell. For resources on how to think like a Haskeller, why not /r/Haskell?
BDB. ;)
Not in the fork.
To seems the article to talking about inversion-of-control and dependency injection, in a rather roundabout way. A good, simple way to do this is to structure everything around ReaderT to give implicit access to some application environment. So your business code could contain something like `saveThing`, which is high level and independent of the IO. `saveThing` goes in another module that contains all the boring database access an other stuff to do with IO. To change your database, just change the implementation of `saveThing` There is a write up here: https://www.fpcomplete.com/blog/2017/06/readert-design-pattern The free monad also works and provides more guarantees, but personally it seems a bit complicated to me.
Can you post a before/after? I've always preferred euler/eulervm or Neo Euler, but I admit that typography is a very subjective matter.
Since Bartosz put this out in the open with no intention to deal with any royalties or such and there are people having it printed on-demand, I do wonder if there's a way to make Lulu have a couple bucks (3,- to 10,-) be donated to the Haskell organization/foundation? Just an idea, but I feel like people might like that, knowing they get a printed book and implicitly donate to a relevant cause.
I try to take it a step further an have neither the high-level logic nor the low-level logic depend on one another, and then have a bridge that contains no (or at little as possible) logic that depends on both. Then, configure the high-level logic to use (or use DI to inject) the bridge. And, while the article mentioned OOP and gave specific OOP examples, it's all interface / implementation separation and dependencies on interface. In Haskell, types or type classes can serve as interface, and value (including function) expressions and type class implementations serve as implementation. (In some ways, having a good architecture is analogous to have proof-irrelevant knowledge.) Session / linear types, in particular dependent, session / linear types, are required to model some interactions accurately, and we don't have access to those in Haskell (or Java or C# or C++). So, you will find your dependencies can rarely be documented fully in *just* the source code, so (at least for now) good architecture is about communicating those in prose and enforcing them during code review, too. All that said, I'm not the best architect, so take it with a grain of salt. I'd rather be coding.
The standard [cat](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/cat.html) has at least one option, so you still have to se careful with filenames starting with `-`. I think supporting -e, -t, and -v are not so bad ideas.
A Haskell2010 type class instance is roughly equivalent to the vtable for a C++ class, at least in functionality. How they are located is significantly different, though. C++ does it by storing the a pointer to the vtable as part of each object then looking things up though the first argument (single-dispatch) of a virtual call. In Haskell it's all type directed, so you can have a type class member (or even the whole classes) where the type only appears in positive positions; i.e. none of the inputs depend on the instance chosen, which mean you can't look up when instance to use based on any of them. GHC adds multi-parameter type classes, which lets you do the equivalent of multi-dispatch virtual calls. In C++ you have to make use of the Visitor pattern or similar to get multi-dispatch. GHC also adds associated types, which C++ would have you do via concepts or (like in C++98) informal (prose) contracts on class templates.
POSIX cat is already an abomination.
I am pretty sure the report allows any implementation to be *more defined* than the report. So, I don't think this particular issue is in infelicity. 
Mine shipped from lulu yesterday as well :) 
Hello! I got it working starting out from this repo https://github.com/yamafaktory/reflex-starter! It works with Stack and Intero so that's all fine and dandy. Now onto learning this :) 
Glad to hear!
404
The past line is telling you the answer: this is the monomorphism restriction. This is rule says that when you define a top level symbol without explicit parameters, the computer will give it a monomorphic type. It cannot figure out how. Solution: either disable the monomorphism restriction, or add a type signature. I favor the former.
I don't understand why the static binary approach doesn't work. How can it possibly be so large that it's expensive to distribute it to servers designed to handle the network loads of the largest social network in the world?
So, it basically tries to jam some concrete type like `Int` instead of `a`?
It looks like this might not be propagating nicely. Try this instead? http://simonmar.github.io/posts/2017-10-17-hotswapping-haskell.html?q
We're not going to do your homework, but if you show us what you have tried so far and where you're stuck, we'll be happy to get you on the right track.
The core issue is time, and size correlates with the total amount of time to rebuild, package, and distribute. The smaller the thing being deployed, the faster that entire cycle will be. 
Having done a lot of "clean" architecture, one of the primary components about Uncle Bob's writings is inversion of control and decoupling your logic from your implementation details. A sample layout, imagining a simple web-app: App.Domain: core application logic. Pure functions that perform business logic on operands, broken down into functional use-cases or actions type classes describing the dependencies of your core application logic. App.Presentation: presentation logic. Functions which take in a view state of some kind, and perform analysis on it and return a new state. Alternatively, utilizes a StateT. Delegates view-level interactions to proper use-cases or actions, and delegates use-case feedback into the state which it returns. App.Services services logic. Sort of a middle man between domain logic and data layer. Might do things like, enforce authorization checks, update some piece of environment state, etc. App.View: view layer. Perhaps Halogen (if you're in PS) or Yesod's tools (like Hamlet) App.Data: data layer. This is where your MySQL dependency would reside, or your firebase dependency or whatever you chose. You implement some of the interfaces in either the services layer or domain layer here. --- How exactly this ends up looking in Haskell, I'd need to think about. It would be app dependent, and framework dependent as all things are. However, the bigger point that Uncle Bob is trying to get here is to keep yourself decoupled. type classes, state monads, etc. are a great way to do this, and you can have a little architecture too :) Let's imagine a simple web app for bank transfers. In my domain layer, I'd have my business objects (Customer, Account, Transaction) inside a Models or Entities file. Maybe these are type classes, maybe they're data objects. It's sort of up to you. What's important is that your actions or use-cases know about them and can ask them for data. Your domain layer has a few interfaces for getting information about Customers, Accounts, etc. and we'll call them things like: CustomerRepository, AccountRepository, and TransactionRepository, since they're really only there to get and push data to an external source. Each have methods describing what they do, and likely utilize MonadIO for the sake of optionality. We then build out the rest of the app. The presentation layer operates on the domain layer and returns some state, the domain operates on the state that bubbles through your layers, etc. Ideally, you can do this in a transparent way with StateT such that you can describe all of your dependencies inside of it, provide accessor methods, and use it as a sort of dependency injection. Now that I've talked all the way through this... I should mention that I'm not a pro haskeller... some people might be able to give much better insight. Though now I've tickled my curiosity and want to go bash this out and see what it looks like.
Sure, it's just surprising to me that the binary size makes that much of a difference. I suppose you're often responding to spam already arriving in real time, though, so deployment times are more critical than in most scenarios.
Thanks, I like the idea of using git as well, I considered it, but decided against because of the overhead of having to programmatically deal with git, but restless-gut actually made me reconsider. It doesn’t seem very well documented. I’d be looking forward for the blog post.
could please check now, i have my solution, thanks
The article claims that addition of types also distributes over multiplication of types. I don't think that is true: Bool + (Bool * Bool) Either Bool (Bool, Bool) Vs. (Bool + Bool) * (Bool + Bool) (Either Bool Bool, Either Bool Bool) The former has 6 inhabitants whereas the latter has 16.
Nice to hear! Yeah, it would really need an example to make sense. Also the current implementation shells out to the `git` tool -- in a principled and likely correct way, using only the low-level plumbing commands, but still it's not very performant, and its behavior is affected by your personal `.gitconfig`, etc. So it's really more of a proof of concept, unfortunately. 
&gt; Learn You a Haskell another one bitten by Learn You a Haskell. Just switch to Haskell Programming from First Principles. 
It looks fine to me. Is your indentation off? howManyEqual :: Int -&gt; Int -&gt; Int -&gt; Int howManyEqual x y z | x==y &amp;&amp; x==z = 3 | x==y || x==z || y==z = 2 | otherwise = 0
I'm not too much of a theory person, but there is a proof starting at the bottom of page 9 on the linked paper: http://www.cs.nott.ac.uk/~psztxa/publ/jpartial.pdf
Does anyone know of a good way to implement inverse trig functions in LLVM? Haskell demands I implement them to satisfy the Floating class. I will also mention that it's annoying that I can't implement Integral for those nice integer division and modulus functions because I can't convert back to a Haskell Integer. Similarly, Bits is unavailable because I would need to be able to test the bits now. One of the nice things in Rust is that all of the operators are independent. I can implement Div without Add if I wanted.
yeah exactly. I just start with `IO` and `String`, and then re-factor it into better abstractions like `MyMonad` and `MyType` as I'm understanding the problem and needing more type safety. 
I mean I am more interested in this counterexample being addressed then seeing the proof.
Yes, it's looking for something to specialize it to. The monomorphism restriction is particularly pernicious in cases where you define things but don't use them, which is usually just in demo code. If you'd actually used `typeless`, there would have been a chance to infer a type from the use site. But since it's unused, the only info the compiler has is the type class constraint. There's no default for `Eq`, so it's stuck.
Yes. You can disable that with a feature flag, and in fact most Haskell libraries do so, but the default is to always try to resolve at compile time to a concrete type.
I'm facing this currently. 1,200 modules and 28,000 LOC. Compile times are: * -O0 : ~6m * -O1 : ~18m * -O2 : ~24m Sample the following: * Pushed something to test server * QA notices a spelling mistake in the subject-line of an email triggered by the system * Bam! Takes you 30-40mins to get such a small change out.
I just wish they had a printed version. I don't have great attention span for books on a screen.
Rust seems to be working relatively well for exploratory programming. Chalk is an independent codebase in Rust for exploring the new trait inference rules. Non-lexical lifetimes were also prototyped external to the rustc codebase. Miri is yet another exploratory prototype for `const` evaluation. Given that the actual production `rustc` compiler should stay written in Rust, it may even be less work than prototyping in Haskell, since the translation to the production compiler can be much more straightforward.
Are you getting an error or a wrong answer? What is the result of calling `howManyEqual 3 3 2`?
I don't see how `Bool + (Bool * Bool)` and `(Bool + Bool) * (Bool + Bool)` works. Given variable names, we get `a + (a * a)` and `(a + a) * (a + a)`. Or, `3 + (3 * 3)` and `(3 + 3) * (3 * 3)` The typical example of distribution is `ab + ac` can be factored to `a * (b + c)`. In types, this is Either (a, b) (a, c) ==== (a, Either b c) And this does hold, witnessed by: either fst fst and \(a, e) -&gt; bimap ((,) a) ((,) a) e
What is your issue?
I like `vcache`. It's similar to `acid-state` in some ways, but I think it's conceptually superior. To give an example: it doesn't have to keep all the application state in RAM all the time. Which can be a little inconvenient for 10-100 megs, and a lot inconvenient for 1-10+ gigs.
Is that for a fresh rebuild (not including dependencies, of course)? That seems long to me for most common changes. I sync the master branch of the Agda repository (~100k loc) pretty frequently, and it's uncommon that my build times are more than a few minutes on O2, although occasionally changes somewhere in a deep module can cause it to be longer.
If you want to learn Haskell, try reading http://haskellbook.com and stick with it. If you want to learn Clojure it'll probably be better for you to ask at /r/clojure. I don't expect you are going to find a lot of comments here saying you should go learn Clojure instead of Haskell.
I mean I of course agree that the standard distribution of multiplication over addition works. But if you reread my comment and the article you will note I was talking about the other kind of distribution, which the article claims does hold and I claim does not hold.
&gt; I get the impression though, that by learning a lisp you are only "sort of" learning FP, comparing to what you would be learning with something more like HS. I don't think that's the case. FP as a concept is a bit nebulous IMHO, Haskell has its particular emphases and Clojure/Racket have theirs, that's all. &gt; It sounds like monads, for example, are actually possible in Clojure as long as you install a library for it. Indeed. That said, I do feel that a static type system can help more than hinder the understability of monads.
Clojure and Haskell have very different mindsets towards writing software. They're both good, useful, and productive. Because the ecosystems make different choices, there are different tradeoffs that are made. So Haskell has problems Clojure does not, and vice versa. Haskell and Clojure both exploit different techniques for being productive, and have different defense mechanisms for handling their quirks. There's a small core of stuff that's common to both Clojure and Haskell. But the Clojure-specific stuff won't help with Haskell, and Haskell-specific stuff won't help with Clojure. Readability is 90% familiarity. Get familiar with one, and the other will seem hard to read.
Oh, I apologize. I should stop commenting before I read entire articles :)
Tried just now. A migration failed in test server. Changed a single file and ran `time stack build --docker`: 489.40 real 0.11 user 0.16 sys That's **~8min** to incrementally compile a single file sitting a the **top** of the module hierarchy. Another 37sec to rsync the binary to staging server.
... How is that possible? I have worked on code bases that large, and they do not take that long. But I never used --docker, so I don't know how much that affects it.
That doesn't affect it by much. Running on bare metal is just a tad faster. I will benchmark again if you want.
No I just noticed the huge number of modules you're using. I wonder if it's actually just the linker linking 1,200 object files that takes so long. Try building with Gold or LLD instead of GNU Ld.
I wouldn't be surprised if it is the linker. In my experience that ends up being slow as hell even for incremental builds.
If it's the standard linked, would C/C++ projects also be facing this slow-down?
Lots of auto-gen code. Why would 1,000+ small modules (&lt;20 LOC) cause the compilation to crawl?
Not necessarily. GHC outputs fairly large object files with a lot of symbols, and I *think* it dead strips by default, which could take a lot of time.
No worries! Admittedly when I read distribution, addition and multiplication in any order I always assume `a * b + a * c = a * (b + c)` so I probably should have made that more clear.
When I browsed over the acid-state documentation I missed that it only supports a single variable per database. While it’s still something that can be worked around with some massive data structure, it does seem less than ideal. Thanks for the tip, I’ll look into vcache as well.
Ok apparently my perception of time is just way off. I just sync'd and rebuilt now, and it's been 10 minutes and it's.... still building. So yes, my bad. I can definitely see how this could get frustrating for large projects!
Again, I’ll suggest trying a faster linker. If it is the large number of modules, it’s probably the linker that’s being slow.
GHC has some scaling issues with large numbers of modules (1k+), we fixed some problems in 8.2
Oh really? Even if you’re just recompiling Main? What causes that sort of issue?
With an order of magnitude more code than this, our incremental build times are shorter. I recommend using large -j and +RTS -A32m at least. (lots of cores and lots of memory)
Accidentally quadratic things :)