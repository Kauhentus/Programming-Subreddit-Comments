I can't give enough recommendation for Pierce's first text. The second one, though, I believe he only acted as editor. Each chapter is written by a different author, and the book very noticeably lacks in cohesion. It's still got a few gems in it, but you have to go digging to find them. If you're looking for writing on dependent types, I would say Pierce's third book, Software Foundations, (available free online) is an all-right introduction to Coq. It's Coq (so yuck), but it is pretty good. 
&gt; What I most want to see, though, is for someone else to pick up what he's done and try to build something with it. For what it's worth, I'm currently trying to connect `transient` and `hsqml`: https://github.com/marcinmrotek/hsqml-transient and it sort of started to work - I've managed to write a program with several controls that trigger a piece of code when any of them is changed, all written as one block, without the traditional callback hell, like this: data Foo = Foo Int Bool foo :: QmlObj () foo = obj $ do _ &lt;- propertySelf "self" foo'prop &lt;- propertyRW "foo" 0 bar'prop &lt;- propertyRW "bar" True pure $ (Foo &lt;$&gt; get foo'prop &lt;*&gt; get bar'prop) &gt;&gt;= liftIO.print Unfortunately, I don't really understand how does `transient` work, and as a result my library is a brittle kludgepile. For example, the automatic updates only work when every single variable is "prodded" from the QML side by assigning some value in a `Component.onComplete` handler. For example, if the code above would be connected to QML like this: SpinBox { id: fooSpinbox; value: self.foo; onValueChanged: self.foo = fooSlider.value; /* Component.onCompleted: self.foo = fooSlider.value */ } Switch { id: barSwitch; checked: self.bar; onClicked: self.bar = barSwtitch.checked; /* Component.onCompleted: self.bar = barSwitch.checked; */ } ... then without the commented out parts the value will be printed only after both the switch and the spinbox are touched; after that it's reprinted after either of them is updated. Even worse, code using different sets of properties connected applicatively behaves differently when using `do` notation and using applicative operators: -- version (a) do foo &lt;- (whatever1 :: Qml something1) bar &lt;- (whatever2 :: Qml something2) -- version (b) do (foo, bar) &lt;- (,) &lt;$&gt; whatever1 &lt;*&gt; whatever2 Version (a) waits for everything inside `whatever2` to be updated, then spams all updates at once, while version (b) behaves like it should, reacting to every update separately. I haven't been able to replicate this with `transient` alone, but my `Qml` monad is literally defined as type UnpackedQml a = ReaderT (ObjRef ()) TransientIO a newtype Qml a = Qml { unpackQml :: UnpackedQml a } deriving (Functor, Applicative, Alternative, Monad, MonadPlus, MonadIO) so I have no idea how does it manage to violate monad laws. I have a feeling that many of the kludges in my library (like maintaining `IntSet`s of handlers for each property instead of using `transient` primitives) could be avoided if I used Transient for both the object-building and runtime layer, but on the other hand, I'm not sure if is it safe to run Qt engine loop inside TransientIO.
Software Foundations is great, and includes contributions from a huge collection of authors, including our own Brent Yorgey (of diagrams fame). No matter what your specific interest in computer science (or whether you like Coq or not), I think it's worth working through. But where does it cover dependent types?
Isn't this the problem that arrowized frp is trying to solve with the many different switching functions? 
links to further reading?
That is not the explicit or initial goal. The initial idea is to allow the people working on the existing haskell support in existing IDEs to be able to share work. So someone working on a tool, say HaRe, can plug it into haskell-ide and there is then a straightforward process for the UI for it to end up in all the various IDEs that may want to provide the tool.
That semantic can be expressed with a monad where the GUI widgets are first class. More or less, elm page logic is executed fully for every event, like all haskell "pure" FRPs. That happens also for react.hs. That makes it more and more slow when the GUIs are more complex besides the inability to sequence things as you mention. That is because the app can not be split out in smaller elements that handle their events independently. the pure/applicative FRPs as well as MVC architectures are not spatially composable. So they must use workarounds to reduce the overhead. These are signal pre-processors and output post-processor. An example of the first are the signal composers and an example of the second are things like the virtual DOM, which compare renderings of two adjacent executions and update only the differences. The only way to properly isolate events and split the GUI into processing elements in sequence is trough a monad. There are other ways, using manual wiring of components. react.js does it. but that is not functional composition, but a OOP-like composition, with callbacks and methods. Monadic widgets can be excited by local events and propagate outputs to other widgets that the programmer can decide, without forcing the re execution of the whole GUI logic/rendering neither using complicated wiring. Just making widgets first class elements of a do expression and using optionally also applicative and alternative combinators. If you want that rendering and logic do not mix, it is also possible. Just create the rendering first and then in the monadic expression just do the logic. To show the results, modify some placeholders in the rendering. This `preview` widget of an example application of my [hplayground framework](https://www.airpair.com/haskell/posts/haskell-tutorial-introduction-to-web-apps). it display five user editable parameters and return the parameter that the user has modified. (Then this value is used in the application to reassign values to the rest of the parameters according with a formula, but this is not shown here). This mix layout and logic: preview= do changed &lt;- h3 "Preview" ++&gt; h4 "Recalculate the budget according with priorities" ++&gt; lb "Income" ++&gt; cell Income &lt;++ br &lt;|&gt; lb "Travel" ++&gt; cell Travel &lt;++ br &lt;|&gt; lb "Food" ++&gt; cell Food &lt;++ br &lt;|&gt; lb "Entertainment" ++&gt; cell Entertain &lt;++ br &lt;|&gt; lb "Other" ++&gt; cell Other &lt;++ br `cell` is an active text box that trigger an event with the value of the parameter when it is modified. This is the same widget, but now the layout has been separated from the logic: preview= do wraw $ do -- layout in a blaze like monad, with span placeholders h3 "Preview" h4 "Recalculate the budget according with priorities and present a chart graph" lb "Income" &lt;&gt; (span ! id "incomeholder" $ noHtml) &lt;&gt; br lb "Travel" &lt;&gt; (span ! id "travelholder" $ noHtml) &lt;&gt; br lb "Food" &lt;&gt; (span ! id "foodholder" $ noHtml ) &lt;&gt; br lb "Entertainment" &lt;&gt; (span ! id "enterholder" $ noHtml) &lt;&gt; br lb "Other" &lt;&gt; (span ! id "otherholder" $ noHtml) &lt;&gt; br -- logic below changed &lt;- at "incomeholder" Insert (cell Income) &lt;|&gt; at "travelholder" Insert (cell Travel) &lt;|&gt; at "foodholder" Insert (cell Food) &lt;|&gt; at "enterholder" Insert (cell Entertain) &lt;|&gt; at "otherholder" Insert (cell Other) rest of the widget logic.... The monad permits the complete isolation of each widget, which respond to their own events. The programmer can decide what parts of the GUI are executed. When a cell is modified, the continuation of the local branch of the widget is executed. With `at` (really a DOM update by id) it can modify any other part of the interface.
why is the licensing an issue (/ who is shut out)? the front ends are emacs, vim, et cetera. my issue with ghc-mod is just that it's fragile. 
The intention is to use ghc-mod as a service layer, and build up from it. It is still the best way I know to reliably set up the context for a random haskell project on a given machine
&gt; I meant you should be able to create a widget in one spot, and then hook it up to a model later on. I'm not so sure this is a desirable thing to do. It seems inherently impure and side-effecting to me. (Again, see the above linked presentation by Ryan for an discussion of why this might be undesirable.) To use another functional metaphor, I think that is like saying you want to call a function without giving all of its arguments...it can't be done (well, there is currying, but that just defers the call). &gt; I am just worried that as the FRP parts of your application grow, they become necessarily intertwined because of the monad. I don't think this happens (at least in the case of reflex which is what I have the most experience with). A widget with a type signature like `editInPlace` above can be thought of as having the following type: editInPlace :: Bool -&gt; String -&gt; String ...or if you prefer... editInPlace :: Behavior Bool -&gt; Dynamic String -&gt; Event String Without getting into nitty gritty implementation details I think of MonadWidget as being there for managing the FRP timeline and because Haskell has really light syntax for monads that tends to make the API more convenient to work with. &gt; It seems that with the monadic approach, you must intertwine this color picker Behavior/Event logic with everything else that happens in the window's event network at once. I think I would tweak your wording a little to say that reflex-dom intertwines the DOM layout with Behavior/Event logic. This is not something mandated by reflex though. You could create an alternative to reflex-dom that doesn't use the monad for laying out HTML elements and instead lets you define your markup in one place and bind widgets to it in another place. I don't know which approach is best. I would love to see someone build a library like this so we could compare the two approaches. But having worked with the reflex-dom approach for awhile I think I like it better because I feel that it yields widgets that are more composable with less syntactic overhead. &gt; I have very little FRP experience and I thought there would be some design patterns I could learn. I don't think anybody in the world has a lot of experience building large GUI applications with FRP yet, so we're still figuring out what the best patterns are. FRP is a very different approach, so I don't think we should try to shoehorn it into fitting the MVC, MVP, etc view of the world that was born in an OO context. But that's just my opinion, and it could turn out that your ideas might end up working better. So if you feel strongly about them, then go try them and see! The more we explore the space the better off we'll all be.
That's excellent advice, thanks! I think I am indeed too tainted by my imperative MVP GUI knowledge :) I have some work to do!
I have an Ubuntu machine with `haskell-platform` installed. If I want to start using Stack, are there going to be pitfalls? Should I purge the Haskell Platform? Or just install Stack?
I read through the thread. your inability to use it at work seems to be a property of corporate lawyers policies, not the license. it says that if you develop closed software on linux with emacs, you don't need to release that software. I'm sure FP Complete's employees can and do use ghc-mod, as well as most haskell shops.
You can install Stack alongside the platform. But if it were me I'd just purge the platform and learn the stack interface because it's easier and has fewer problems.
When you `rotateAboutPoint` or `rotateAboutOrigin`, should you be changing the gird orientation? I think every 30 degree rotation should change the orientation. Alternatively, maybe the type-level distinction between Pointy and Flat topped orientation is superflous. It only matters when the grid is visualized.
It's not really clear what this is... a new codebase started from scratch? A merge of ghc-mod and ide-backend (which was recently hinted at)? Something built on top of one of those things? Are any of the existing ghc-mod or ide-backend developers involved in the project?
What changed with 5.4.*?
Fair enough.
It actually can do web applications just fine. I've written about 20 of them with servant in the past year, it went very well. It's just that the """marketing text""" on the website is out-of-date, we've been so busy working on servant, using it and handling all the contributions that we haven't fixed that.
Just curious, what vim plugin uses curl-config?
It's not curl-config per se. That binary is just an indication that libcurl-dev has been installed. I don't remember what used it but maybe it was Vundle in order to pull down vim plugins.
Sure, I've conceded elsewhere that FRP models the low-level reality pretty accurately -- there are a series of fixed IO ports on your computer, which are nothing more than time-varying values. And there actually is a clock! As for GUI semantics, the entire reason I turned my back on FRP in the first place is because I could not figure out how to prove anything important or meaningful (from a GUI/app perspective) about it in Agda. I want proofs like "an http response can't arrive before the request sent it", "the user is on the feed screen now, which implies he went through the login screen at some point", "widgets aren't positioned on top of each other in any of the valid resolutions" etc.. You may have more success than I did, but I was entirely unable to prove *anything* which I consider semantically valuable from a GUI/app perspective using FRP. After failing, I took a step back to rethink: if I'm not able to prove any of the semantic guarantees I want to the compiler, well, maybe these semantics just didn't mean what I wanted to pretend they did. Don't misunderstand -- I'm in no way contending that one should have to prove every imaginable guarantee about a GUI app; of course not. But I absolutely am contending that if you can't prove *any* of them or even any generic useful results / properties about the way your app behaves, then maybe you are using the wrong model in the first place. This is what I mean when I say FRP semantics do not model app behavior.
Sounds good, I'll give it a look sometime in the next few days.
If it ever was a Haskell project, Kubernetes is most definitely a Go project now
You're mostly talking about math, which would keep existing and applying even if I removed type classes altogether, so your entire argument is irrelevant as phrased. Worse, since BiasedTuple is an isomorphism, it makes no difference from the category theoretical point of view. If you want to use this functor, you'll only have to call this iso sometimes. I think you're committing a fallacy when you confuse "the mathematical *functor* — × A is important" with "the Haskell Functor for — × A must be defined and available by default, while the Functor for A × — mustn't" and with "well, doing that is not supported, so let's swap the functors". &gt; just for the sake of not having to explain a Functor to someone who reaches out and explicitly uses it. That's a complete strawman. I know this functor, I just don't want GHC to use it unless I name `A × —`.
Google's styleguide for C++ is also (generally recognized as) horrible, so I wouldn't wish for a Haskell one :)
Could you recommend a reference for setting up stack?
Now I'm really curious, what's well liked and respected? I was thinking mostly of comprehensiveness here.
Thanks
You said, &gt; To every fixedpoint there exists an induction principle. I wonder what you mean by this. I know how to make an induction principle from a strictly postive functor, but it involves decomposing into shapes and positions.
I think the answer to this question depends almost entirely on how well you structured your initial reflex-dom application. If you did a "good job" and encapsulated lots of business logic in ADTs and self-contained APIs like the example I mentioned, then your UI framework portability might be pretty good. But this will probably require a very concerted effort from the outset. I personally think that UI framework portability is roughly as useful in large apps as DB portability--which is to say, not very. I think you're better off coding for one specific database and leveraging its features to the max rather than watering your app down to the least common denominator of the SQL standard. But if you have a hard requirement to work with both reflex-dom and reflex-wx from the outset, then I think you'll probably need to create an explicit layer on top that lets you describe your app in a way that can generate backends for both libraries. Ultimately it might be nice to have some kind of unified approach for this that all UI framework reflex backends would write to. But I think we still have a ways to go before we're able to start tackling that problem. We're still trying to figure out what will work the best for reflex-dom by itself, let alone start generalizing across multiple UI frameworks. Also, are you aware that reflex-dom is already written in such a way that you can build your app with GHC and get a native app using webkitgtk?
C++ itself is also "generally recognized as horrible" depending who you ask. Google's style guide is designed to make their C++ codebases more approachable to people who primarily know Java or Python. That has pros and cons.
that's great to hear. I've been using it for simple services and i want to bring its lightweight (no TH) type safety with me when I build my next web app.
You used FRP in Agda? Wow, Agda sure got a lot closer to pac-man complete since I last used it.
Clean's length (AFAIK) doesn't have an instance for tuples.
What a pity, but thanks for the update.
As someone who hates the FTP proposal, I also think this was done the right way (even, overly cautiously). No one can credibly claim that the community was ignored. Instead, the community is just wrong^1. ^1 I'm joking here. Don't burn me at the stake!
Google does have a style guide for Haskell.... I'm the main editor! I've been given the green light to publish it in the public repo of style guides... Just haven't gotten to it yet... But this thread has bumped the priority ☺
Two properties of the license of the license make it risky to use in such a corporation: * AGPL has not yet been tested in courts, meaning it's interpretation can vary greatly and hence bears high risk * AGPL is highly infectious, hence a single accidental use of it in a networked situation, can make all other codebase subject to AGPL
https://github.com/bmsherman/haskell-matlab bindings are probably your only hope. Wouldn't be a different situation anywhere else, though.
Will this also allow for everyone's tools to be made easily cabal/stack-aware?
The grid package is stellar. I didn't realize it existed by the time I had written the bulk of my library. If there was anything to gain from my library, it would be coordinate system conversions (Axial, Odd-R, Odd-Q, Even-R, and Even-Q), and in the future, coordinate-to-pixel stuff. For now, I recommend the grid package well before my own.
&gt; the problem seems particularly suited to Haskell or other functional languages. I'd argue you're a particularly bad judge of that if you are not able to do it yourself.
True, but I am baby-level beginner at Haskell (that is to say, I know more than zero), and it's seems to match up. It's just a math problem, at it's heart, dealing with lots of sets (which seems to be one of Haskell's fortes). Also, I just like the way people in this sub think, so if was to pay someone to do it, I'd prefer someone from here. There's no reason the solution has to be written in Haskell. Anyway, in the mean time, I *am* working on it myself. Just would rather pay someone, and more efficiently use my time elsewhere EDIT: you know, I'm just gonna say it, as I don't think I violating rules this way: the problem is a complex knapsack problem, or a multi-objective optimization. Pretty sure the solution involves or is similar to a pereto search. This is what Haskell is good at, right?
I think the non-portable Windows installer is supposed to automatically set the path (so I heard from Anton).
I don't know anything about json, but if it's turning (byte)strings into doubles that you want, then you should use [**bytestring-lexing**](http://hackage.haskell.org/package/bytestring-lexing). It's not at all naive, but is [hella fast](http://community.haskell.org/~wren/bytestring-lexing/bench/html/readExponential-0.5.0_ereshkigal.html)
You could add fix...
Yeah, rather unpleasant... It's an euphemism. No wonder why people use HSE, the GHC API is incredibly poorly documented. Not to mention that half of the names somehow doesn't follow Haskell conventions but C ones. It took me ages to get a sense of how things work, because it's not explained anywhere. I have yet to check out ghc-exactprint.
Have you had a look at OptaPlanner? It's Java, but it might be useful for your particular problem. One day I'd really like to have a look at a problem that I can attack with both OptaPlanner and SBV/Z3, just to compare the quality of the results and the problem sizes where Z3 taps out. 
I enjoyed your sense of humor. The fact that your type is a type synonym over a non-exported type synonym is a bit... hideous. In general I'd advise using (abstract) newtypes for cases like this, although in this particular case it probably won't play nicely with `omitNothingFields`.
This is a use case for Stack that is still up for discussion. The behavior right now is to use the "implicit global" settings when outside of a specific project, and ignore stack.yaml inside tarballs. There are many use cases where this is exactly the desired behavior, but packages which (1) aren't in Stackage yet, and (2) ship with stack.yaml files that would allow them to work, are currently a case that this behavior doesn't cater to. My recommendation in this case: `stack unpack argon &amp;&amp; cd argon-* &amp;&amp; stack install`. That should work.
You're welcome. :) Re whether or not to use a type class: I would maybe forget about it for now then - it's not a big deal.
To a first approximation nobody uses Clean. :) But the first point then goes on to suggest that maybe the Clean/Haskell ratio is 1/100, which is probably more true.
`Lousy` is a newtype, not a synonym, so it's not *quite* that bad... still, I'm not arguing that it's good, heh. And yeah, the reason `Yak` itself is a synonym is to make `deriveJSON` painless; I'm not sure how to make this all work with explicit instances. That was the first thing I tried, but I kept running into issues, and I can't spend too much time refining this because I should be working on the thing I needed it for. ;) But if anyone has a pointer there, I'd love to roll it all into a single newtype.
Both the ghc-mod and ide-backend maintainers have agreed to contribute code to this new repository and then rebase the old repos on this. The reason we're using a new repo instead of modifying one of the existing ones is so that the existing projects experience no disruption during this migration process. If this was a new set of people starting a new project without support from existing projects, I'd agree with you. But Alan's reached out to existing players already, which is an important distinction.
The package binds to the glpk library, which is pretty good. It's also used in Octave for linear programming problems, and probably in a lot of other things too.
Feel free to speak on my behalf on this as much as you want :-) 
This comment /u/snoyberg got burried due to a heavily downvoted parent in this thread. It did tell me what [this `haskell-ide` project (Github)](https://github.com/haskell/haskell-ide) is all about. Here it is: &gt; Both the ghc-mod and ide-backend maintainers have agreed to contribute code to this new repository and then rebase the old repos on this. The reason we're using a new repo instead of modifying one of the existing ones is so that the existing projects experience no disruption during this migration process. &gt; If this was a new set of people starting a new project without support from existing projects, I'd agree with you. But Alan's reached out to existing players already, which is an important distinction.
Making stuff up: * there are OpenCV bindings (https://hackage.haskell.org/package/HOpenCV), * OpenCV ships with SIFT (http://docs.opencv.org/modules/nonfree/doc/feature_detection.html?highlight=sift) * ??? * profit ! 
This. Why provide such a monolithic beast (vimrc)? With mappings for toggling mouse mode? And git mappings? How are these related to haskell? I would prefer installing 2-3 different haskell plugins than copy-pasting someone's vimrc.
`MonadFail` remains on the roadmap, just with a longer schedule to accommodate a more graceful transition. It has a very broad base of support. The fate of the `Monad(return)` proposal is very much undecided. It is at the very least very polarizing. At the very least it is clear that any lead-time before execution for it must be long. `Monad((&gt;&gt;))`'s fate is looking largely decoupled from `Monad(return)`, however. It causes more active harm. The schedule may also decouple as well. In the case of both of those latter proposal fragments, options in the design space exist to support technical tricks where we can detect redundant definitions, or even simply warn if the meanings disagree, etc. Those options come with their own maintenance tax, however, for something with broad reach like this it seems worth bending over backwards. Other options exist around the way the Haskell Report phrases the membership of standard classes, e.g. allowing implementations to add additional members to the class. Then a Haskell2017 report might omit `return` from the class, but explicitly allow implementations to move extra members into classes for efficiency or compatibility concerns, letting us grandfather the `return` in `Monad` for compatibility reasons and still comply with the standard. This scenario would net the Haskell Prime committee a clean standard, and yet not induce community wide breakage, but then requires a more complicated exposition and buy-in that this is a way in which we want the Report to proceed. That last option also doesn't really address the active problem of `(&gt;&gt;)`. Compromises between any or all of those approaches also remain on the table. e.g. removing `return` from the report but leaving it in the class, while actually moving `(&gt;&gt;)` to a top level definition and possibly checking for and/or permitting redundant definitions on both.
There's a click handler on `document` which captures link clicks into a cookie.
People's main gripe seems to be with length being defined as length :: Foldable f =&gt; f a -&gt; Int length = foldr (const (+1)) 0 and not with it being overloaded per se. The problem is that the way folds work doesn't exactly correspond with the intuition of "go over all the data in a container". At most, you can say that a fold goes over "all the data of a certain kind". That is, if you have `[a]`, fold will go over everything with type `a`. If you have `(b,a)`, fold will go over everything with type `a` as well, but the `b` will be disregarded. Now, if you really want to do something with the tuple's second part, then this is not so bad, since `foldr :: (a -&gt; accum -&gt; accum) -&gt; accum -&gt; f a -&gt; accum` tells you that the first `b` can't be used. `length`, however, doesn't look at the individual elements, but merely counts them. Here lies the conflict with people's intution - the type `length :: f a -&gt; Int` *doesn't* tell you that the `b` in `(b,a)` will be ignored (at least not immediately). A dedicated `Size` typeclass might be a viable compromise, but one can levy against it the criticism of being lawless.
You could have used your browser's history.
That actually shouldn't be too much work
I don't know the current state of Euterpea but in my experience the concepts you mentioned can be learned from general sources. What are you looking to do exactly?
Useful proprietary tools are a contradiction. Something you can't use can't be useful. 
You should ask Donya Quick: http://donyaquick.com/ I have been told she is to complete the book.
Ah cool, thanks for the explanation!
But I want to find the *comments* page, which I have not visited yet.
Please stop spreading FUD. The AGPL is no more "infectious" than the regular GPL. The requirement for code to also be under the AGPL does not spread via the network. It only applies when you create a derivative work of the AGPL licensed code, i.e. when linking against said AGPL code (or copying it).
Awesome, glad to hear it! I'm really excited about the possibilities here.
&gt; Why does the similar situation in Haskell suddenly freak out so many people? I don't think I've seen any evidence of actual beginners getting freaked out.
I want to create simple command line application that will allow me to AM/FM/filter etc my custom signals. I know all this is presented in the book but examples would be very much welcome. I wish Figures from chapter 22 and 23 where complete. Signal theory I can imrove. The haskell part is what's troublesome for me :)
Probably because it has always been that way. If we had always had FTP, I doubt there would be a viable movement to move from having one length function to having two length functions -- where the only difference is that one of the length functions has a simpler type, but does exactly the same thing. I do support the idea of there being a gentle indoctrination process. But I don't think that needs to live in base/Prelude. Happstack has a very rich set of combinators and uses type classes to abstract away from being tied to any particular monad. But that can be a bit daunting. So we have the `happstack-lite` module. It exports just the most essential subset of functions and types and ties everything to the `ServerPart` monad. http://hackage.haskell.org/package/happstack-lite-7.3.6/docs/Happstack-Lite.html This simplified library still contains all the essential pieces needed to create a web server. In fact, the subset was determined in part by seeing what functions snap exposed at the time and only exporting those. When you are ready to take off the training wheels, you can simple import `Happstack.Server` instead of `Happstack.Lite` and your code should still compile modulo any ambiguous type errors. It seems to me that for teaching purposes we simply need one or two extra packages on hackage that provide a simplified beginner Prelude. But one that smoothly upgrades to the real deal. With a small bit of hackery, we can get good old H98 Prelude in GHC 7.10.2, as demonstrated here: https://twitter.com/happstack/status/656689442955005952 Rather than undo FTP, I think a much better improvement would be to work out the kinks that make it awkward to use an alternative base. With things like stack/nix/etc, a teacher should be able to tell their students to do something like: nix-shell -p haskellLevel1 And they are in an environment where Prelude/base is the teaching Prelude/base for that level of instruction. Perhaps something with no type-clases, polymorphism, etc, at all. As the semester moves on they switch to haskellLevel2, haskellLevel3, etc, where polymorphism, classes, etc, are introduced. I think the whole `length ('a', 1) == 1` thing is a red herring. I doubt that experienced Haskell developers would write that in the first place any more than they would write, `length [1]`. Secondly, I think a lot of the potential confusion could be solved by better documentation. Experienced developers have unclear ideas of what length should do because they have been using something called length for years with out considering what it really means. In many ways the new comer has the advantage of having to read the docs first. length :: Foldable t =&gt; t a -&gt; Int For a type `t a` return the number of `a` elements. e.g. length ['a','b','c'] == 3 -- length :: (Foldable []) =&gt; [a] -&gt; Int length (Left ()) == 0 -- length :: (Foldable (Either x)) =&gt; Either x a -&gt; Int length (Right ()) == 1 -- length :: (Foldable (Either x)) =&gt; Either x a -&gt; Int length Nothing == 0 -- length :: (Foldable Maybe) =&gt; Maybe a -&gt; Int length (Just ()) == 1 -- length :: (Foldable Maybe) =&gt; Maybe a -&gt; Int length ('a', 3) == 1 -- length :: (Foldable ((,) x) =&gt; (x, a) -&gt; Int The following is always true: length x == length (toList x) I think there is something syntactically that leads us to thinking `length ('a',3) == 1` is odd at first. We are certainly not surprised by `length (Map.singleton 'a' 3) == 1`. And yet at the type level, they look pretty similar -- `(k, a)` vs `Map k a`. There is, perhaps, an innate desire to think of a tuple as a heterogenous list, and then think that `length` should return the number of elements. But tuples are not heterogenous lists. But, as I said earlier, I doubt people are going to be writing, `length ('a', 3)` in real life anyway. Whether you think it should return `1` or `2`, it does not matter. You would just write `1` or `2`. Where it does matter is if you are trying to write code that works over any `Functor` and in that case it gives the correct answer even if the programmer has never considered how it would work on tuples. There is the suggestion of having 3 length functions: length :: [a] -&gt; Int genericLength :: Num i =&gt; [a] -&gt; i lengthFold :: Foldable t =&gt; t a -&gt; Int Having more choices does not make life easier. If there is only one `length` function then it is easy to know which length function to use. With 3 length functions, I now have to make a choice. There are now 3 ways to do the same thing. I have to now decide if one is better than the others. Science tells us that having multiple good choices does not make us happier, it just increases stress: &gt; Unsurprisingly, when people were asked to decide between something like an iPod and a bag of pretzels, they didn’t feel particularly anxious: the choice was clear and life was good. When both choices were low in value, the emotions were similarly clear-cut. No one was particularly happy, but neither were they anxious. But when multiple highly positive options were available—a digital camera and a camcorder, say—anxiety skyrocketed, just as Lipowski had predicted. The choices between those objects that they valued most highly were both the most positive and the most anxiety-filled. The more choices they had—the study was repeated with up to six items per choice—the more anxious they felt. “When you have more good choices, you don’t feel better,” Shenhav says. “You just feel more anxious.”[1] In summary, I doubt that giving newbie Haskell programmers more choices and more things to remember is going to make their life easier. If we want to gradually get new developers from monomorphic length to polymorphic length, then we should do it by making it easier to use alternative base libraries. [1] http://www.newyorker.com/science/maria-konnikova/bad-good-choices
And why can't I use proprietary tools?
One of the things I want to implement is a strict mode for resolving remote references that enforces that: * remote references are statically linked (i.e. they do not contain remote references of their own), and: * remote reference are already normalized Enforcing these two constraints is linear in the size of the remote expression. Once you enforce those two constraints, you can use the types to place hard upper-bounds on the amount of work introduced by remote values. For example, if I import a remote expression of type: ∀(Bool : *) → ∀(True : Bool) → ∀(False : Bool) → Bool ... then I know that the expression must be alpha-equivalent to one of the following two expressions if it is statically linked and in normal form: λ(Bool : *) → λ(True : Bool) → λ(False : Bool) → True λ(Bool : *) → λ(True : Bool) → λ(False : Bool) → False ... and I can place a tight upper-bound on the amount of work it introduces into my program. If I introduce unrestricted recursion then I can no longer enforce the normal form check any longer.
that's great to hear!
a ".Lite" module is a great idea. also a pretty neutral name, unlike ".Beginner"
when I'm quickly evaluating a number of packages, I don't want to waste time. recently, I went through like 10 "code graph" type packages, and none had any images in the haddocks, a few in the repo. did not lens confidence, and I went with the one that looked best. I'm happy to sink hours on learning a good parser combinator library, not for a tool that I need quick.
This isn't about what's familiar to non-Haskell programmers. This is about experienced Haskell programmers who think that the combination of `Foldable` on non-container types like `Either` and tuples, and generalizing basic list functions to `Foldable`, has dug a little too deep, and hit the "oh crap" point where nonsensical types now unify by accident.
Maybe is solution to create custom vim install... For Haskell development only... Separate vimrc file and there is no problem. Now problem is that I am happy emacs user...and don't want to get back on vim and tinker it(why I read and reply on this post is mystery for me)
Yes, but the only reason to want this is so that if people try to treat a tuple like a heterogenous list and call length on it expecting a list-like answer, they will get a compile time error. But there are really only a few options here. * `Foldable ((,) a)` is so useless that even though it is perfectly valid it would never be used and therefore should not even be implemented. It seems to me that requires a big burden of proof * Expecting the wrong value for`length (1,2)` is common and horrendous mistake that it warrants having two separate length functions just to make it less likely that people will actually do that. And I find it difficult that people will actually be writing that in real life. * `Foldable` and length are so confusing to newcomers that they will give up. I doubt that this alone is what is going to make the difference between people succeeding or failing to learn Haskell. If the problem is too many new concepts at once, then I think what I said [in this thread](https://www.reddit.com/r/haskell/comments/3pv0kq/why_dont_we_hear_people_complaining_about_cleans/cwa476p) about making it easier to use alternative, beginner friendly base libraries applies. I think that having to choose between three length functions is worse for beginners and experts than the possibility that they might try to call length on a tuple and get a correct but possibly unexpected answer. 
Thanks for this. It seems to focus on notes and composing pieces of music. I am more interested in the signal part of *Haskell School of Music*. But thanks anyway ;)
I think a good solution to this problem is to use a compression algorithm while normalizing. Sharing / pattern matching for free. The same idea can be applied to enable local sharing in a code chunk database, which would alleviate the space explosion due to statically linking remote references.
Oh, that's clever! I really like that idea. I'm going to try that
My original interest was not putting an upper bound on the time it takes to normalize, but rather being able to normalize at all. I really enjoy discovering what the normal form of something looks like and I don't want to give that up. How many times have you looked at code and thought: "I wish I could just see what this code would be if I were to just inline everything"
It sounds like he is talking about tuples as the simplest representation of a product type and Either as the simplest representation of a sum type. The way the Haskell type system currently works any and all instances of type classes on types of kind `* -&gt; *` are biased to the right-most parameter of types with more than one parameter by default because we do not have a simple type-level way to do partial application in orders other than left to right. 
Cool, glad I could help! If I come up with any other tricks `morte` could use, I'll send 'em your way. Edit: In that vein, it might be a good idea to have an "open problems" section in your readme or issue tracker.
While I agree with you that the `length` in `Foldable` should be left as it is now there is actually one argument for not having a `Foldable` instance on tuples or Either that has some merit because it is backed by actual bugs that happen in dynamic languages at least sometimes. That argument claims that it can happen in a nested data structure that you occidentally get what amounts to an off by one error in the number of levels you unwrap, calling something accidentally on one layer too far out or in. In that situation it is useful to get a type error. I consider that argument valid but not very good because it could just as well happen with nested lists and if your data structure is so complex and confusing that you don't know how far in you are you likely want custom data types in Haskell anyway instead of tuples, lists and Either.
I'm just back here to tell you that your explanation was the most favorite thing among students. Thanks again!
It's like that because of the partial application, not because it is more correct mathematically. In Scala, the basic `Either` type is not a functor. You need to use a projection which is either left biased or right biased. The most common use for `Either` in Haskell is as a value with an optional error. Why specialize `Either` to being right-biased rather that using some `ValueWithError` type where it makes sense as the default fmap. Going on, having the basic tuple use the right most component as the traversed part feels to me similar to just pulling an arbitrary parameterized type and saying "I declare it a functor over the last given type". Of course this doesn't make sense in the general case, my point is that I feel it doesn't make sense in the tuple case either. What does make sense, is the `over` function from the lens library. It lets you choose which component to traverse over.
I have also wrote a [game](https://github.com/soupi/ld32) for LD (32). It is a game made in three days so the code is not so well designed, but I felt that Elm's reactive part worked great for me and that I was able to abstract the different actions and signals needed for each component :)
haskellformac is unsuitable for development. and I don't get why it should use GHC-mod, at no cost, unless they donate half the profits to some fund. or something, not sure about the licensing. 
I've implemented this, although not in code I particularly want to share (or that you would want to see). This is doable with the judicious use of `unsafeCoerce` and maybe the [ghc-typelits-natnormalise package](https://hackage.haskell.org/package/ghc-typelits-natnormalise-0.2.1). I have to admit to being a little frustrated with the state of these numeric literals. They've been in GHC for years but still aren't in anything close to a satisfactory state. I also have no idea what the plan for them is or if anyone is working on them. (Although this ignorance may be my fault for not paying attention.)
As /u/mirpa said, cabal is very likely overkill. A simple script (`for d in $(ls); do ghc --make $d/Main.hs; done`) or a small Makefile (only building Main.hs and passing `--make` to ghc) should do it.
Which one? ;-)
Well, thank you. :) Sometimes one must vent frustration... A previous attempt involved `data Lolwut a = Wut | Lol a | Zomg [a]`. To be honest I came pretty close to dumping this as `acme-yak`, but it's a pretty important and common case that I haven't seen any other public solutions for. At the very least, I hope to start the conversation that leads to a better solution.
Well, proving anything about the traditional event-driven approach is even worse. Also, note that theorem provers are not the simplest tools for proving things. Sometimes, it's extremely difficult to formalize a proof, while writing down a version understandable by humans is easy.
&gt;but the only reason to want this is so that if people try to treat a tuple like a heterogenous list and call length on it expecting a list-like answer, they will get a compile time error. No, the reason is that people don't try to do it, we do it by accident. The answer is always 1, it makes no sense to ever actually call length on a pair. It happens by accident, and now we're having ruby and PHP style errors get silently accepted. &gt;Expecting the wrong value forlength (1,2) You're ignoring what I said. It is not expecting the wrong value. Nobody is ever doing that on purpose. It only exists as an error. If they wanted the value 1, then the code would just say 1, not call a function that will only ever return 1. &gt;And I find it difficult that people will actually be writing that in real life. You can't imagine calling a function on the wrong thing by accident? Something like this is completely implausible to you? systemSize sdesc = case sdNodeDescriptions sdesc of (nd:nds) -&gt; length nd -- count only the non-root nodes _ -&gt; 2 -- defaults to two generic nodes
Oh wow. Is this because the auto-generated function for `exitcode` is something like the following? exitcode :: ExecResult -&gt; Int exitcode ExecFail e _ _ = e exitcode ExecSucc _ _ = error "No match in record selector exitcode" That does seem like very undesirable behavior. Does anyone know if this is this fixed with [OverloadedRecordFields](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields)?
This is one of a few big problems with Haskell's record system, and I don't think you'll find anyone defending it. I agree that a compiler warning is a good idea -- I learned about it before getting bitten myself, but that was just dumb luck. The only safe solution is to never have sum types with named fields, or at least never export them. Instead, export accessor functions which return `Maybe`, or go one step further and provide lenses. Or wait for /u/nikita-volkov to show up and give you much better advice than I can. He's been all over this problem for a year or so. :)
The issue here is the record system, which is known to have many many issues. In this case, you notice that the accessor functions it generates are not total, and thus can fail at run-time. If you restrict yourself to using case expressions, you'll notice that it would be impossible to write a program that assumed a value for `exitcode` when the `ExecResult` value was from an `ExecSucc` constructor. In general, if you do not want run-time errors, you should restrict yourself to using [total functions](https://wiki.haskell.org/Avoiding_partial_functions). OverloadedRecordFields won't help. What makes you think it would? In general, type systems can only be as restrictive as you let them. Your `exitcode` function cannot be called on an `Integer` for example, but the type system cannot prevent you from creating a giant sum type that can represent every value in your program, and then using Haskell as a dynamic language.
Sadly no. That only allows two *different* data types to have a record name in common, e.g.: data Dog = Dog { fur :: Color } data Coat = Coat { fur :: Bool } Then, `fur` can be used for `Dog -&gt; Color` or `Coat -&gt; Bool` by local type inference. But it doesn't help here: data Animal = Animal { fur :: Color } | Rhinoceros ...because the type is `Animal -&gt; Color` and `Rhinoceros` is still an `Animal`.
Just imagine if you used Yesod and then Elm for the presentation. In reality, jQuery is probably a better domain language, though.
I'm not sure if those bindings are up-to-date, but it shouldn't be _too_ much work to automatically generate bindings from the OpenCV source. OpenCV already does this automatically for Python and Java (I haven't looked at the Matlab bindings, but I'm guessing it's the same). They have a C++ header parser, that some crazy person wrote in Python, which they use to automatically generate a C-language interface. The other languages then bind to this interface. You could do the same for Haskell, using the Python or Java bindings as the pattern. (I created their auto-generated Java bindings, but in this case it was an almost-trivial modification of their Android bindings.)
What you say is true. &gt; Going on, having the basic tuple use the right most component as the traversed part feels to me similar to just pulling an arbitrary parameterized type and saying "I declare it a functor over the last given type". Of course this doesn't make sense in the general case, my point is that I feel it doesn't make sense in the tuple case either. In fact, without a newtype wrapper, declaring a higher kinded type a Functor _has_ to be just over the last type argument it accepts. 
 length (Map.singleton a b) -- 2 toList (a, b) -- compiler error if a and b are non-unifiable, [a,b] otherwise length (toList (a, b)) -- compiler error or 2
Thanks for your answers!
I tried this on a new installation of Arch Linux last weekend and it didn't seem to work correctly. Got a message about ghc-mod not being executable. I don't know why it went wrong--maybe because my shell is fish and this method uses bash?--but I couldn't even figure out where ghc-mod had been put.
&gt;I was surprised by this. I always assumed that this was the kind of run-time error that Haskell should be able to prevent. It doesn't prevent this particular kind of error. It comes down to product types and sum types: your canonical product type is a tuple `(a,b)`: you need both an a and a b to construct it and you can be sure that it contains both. The canonical sum type is `Either a b`: you can construct it via `Left a` or `Right b`, but it won't contain both. Because Haskell doesn't require patterns to cover all cases, you can have a function like fromLeft :: Either a b -&gt; a fromLeft (Left a) = a which will fail if you feed it a `Right`. The very fact that the compiler even accepts a function of type `Either a b -&gt; a` means that it can't possibly be a safe one - after all, you can't guarantee that you'll have a `Left` available. It's the same with `exitcode :: ExecResult -&gt; Int`: exitcode :: ExecResult -&gt; Int exitcode (ExecFail c _ _) = c Because `ExecFail` and `ExecSucc` can both be used to construct an `ExecResult`, you can't guarantee that you'll have an `ExecFail`. In a safe language, `exitcode` and `fromLeft` couldn't exist. The only way to be safe would be to always cover all cases, e.g. by supplying a default value: exitcode :: Int -&gt; ExecResult -&gt; Int exitcode _ (ExecFail c _ _) = c exitcode d _ = d
Oops, sorry! I meant the opposite: length (Map.singleton a b) == 1 My bad. Also: length [(a,b)] == 1
&gt; Because Haskell doesn't require patterns to cover all cases But it does have a warning if they don't. The issue here is not that this scenario is possible. Obviously this is avoidable. The issue is that GHC happily compiled and ran my program without any indication that my data-type is unsound. 
Okay, then perhaps I should say that it's more difficult to create an unnecessarily long-running program in a total programming language
&gt; a property of corporate lawyers policies, not the license Licensing is a legal issue; of course some decisions will be made by lawyers. "That's a lawyer problem, not a license problem" is akin to "that's a programmer problem, not a library problem." By adopting a certain license, you're restricting and liberating your users in certain ways, and those users (whether individuals or corporations) may very well decide not to accept those terms, whether or not you think their analysis is correct. I made a comment in the github thread, but to briefly note: the additional clause in the AGPL is much broader than the GPL, and I can understand why some lawyers would decide not to allow the corporation (and by extension its employees) to agree to it. I wouldn't necessarily make the same decision.
You should turn on `-fwarn-incomplete-record-updates`, which catches a closely related problem (though not this one). This seems like a very natural feature request to add to the GHC bug tracker, though! I encourage you to go for it.
If we continue to explore this I think we will find a couple things. 1. there are two potentially useful functions: `numAs :: (Foldable t) =&gt; t a -&gt; Int` `numVals :: (NumVals a) =&gt; a -&gt; Int` `numAs` (aka, length) looks at a structure and tells you how many times an `a` value appears in it. So for `((,) x)` that is 1. `numVals` tells you how many subvalues are contained in the value. So, `numVals (a,b) == 2`. `numVals [1,2,3] == 3`. `numVals` is nice because it seems intuitive. But.. it is also a bit useless. `numAs` is built on the power of `Foldable` and so we can do all the other things with `t` the `Foldable` allows. But `NumVals` just gives us the `numVals` function. Once we know how many values are in the structure.. we can't actually do anything with those values. There is no abstract way to do any computations with them. Now, perhaps there is some useful class with laws where `numVals` could live. That is certainly worth exploring. What would it be? Probably some sort of generics library? For some types `numAs` and `numVals` have the same value (for example a list). But I think it is important to distinguish that for some types they do not have the same value, and that is useful. 2. `length` is a terrible name for `(Foldable t) =&gt; t a -&gt; Int`. `length` is a fine name for the `lenth of a list`. But what is the length of a `Tree`? Do we measure it like a river -- where the length is the longest branch? Or do we count the total number of nodes? Alas, I am sure we are stuck with `length`. But, really, it is not a good name. 3. We can not directly create an instance of `Foldable` that will give the result for `length` that you want. They types simply do not allow it. But we can do it with `newtype`: `newtype Two a = Two (a,a) deriving Foldable` `GHCi&gt; length (Two (1,2))` `2` In summary, I think the term `length` is confusing and leads to different ideas about what it should do. I think that what it currently does, while perhaps surprising, is more useful because it comes as part of a more powerful abstraction. In the case where you really do want `2`, you can have that today via a `newtype`. The fact that `length ((,) a)` and `length Two` return different values is actually a good thing. I guess I've now argued that there should not be any function in Haskell named `length`. In theory, such a function could live in `Data.List` with the type `[a] -&gt; Int`. But I have argued elsewhere that I am not fond of having multiple names for the same function. That just leads to a different type of confusion. Alas, I would need a write a longer blog post to work out the details. 
I am willing to agree that we should rename `length` entirely. `length` returns a sensible value for a `Tree` but what is the length of a tree? That is an confusing idea in itself. And, there are in fact people arguing in these comments that `length (a,a)` should return 2. So some people do (think) they want that. The idea that someone might apply a function from a type class to the wrong value is true of many type class functions. I don't think the right solution is to abandon type classes and have, `lengthList`, `lengthTree`, `lengthFoo`, to avoid that problem. If I wanted that, I'd use OCaml.
We can actually do worse. {-# LANGUAGE FlexibleInstances #-} {-# OPTIONS_GHC -fno-warn-missing-methods #-} instance {-# OVERLAPPABLE #-} Num a Stick this in a package somewhere buried deep, and anything that imports it will now type check with no warnings if you accidentally try to add any two things of the same type but that aren't traditionally numbers. Worse, it's an orphan instance, there's no way to *not* import it (though if you're sufficiently paranoid you can detect it). Further, the actual error description you get is quite uncommon in my experience, nor is it super helpful. It won't affect any existing correct code at all. I also feel there are nastier choices than `Num` available to do this with. It's still fixable, and I wouldn't expect anyone capable of understanding it to actually do it, but it's *weird*. Anyway, getting back to your particular problem... Currently, the easiest solution I know of is to use pattern matching and the `RecordWildCards` extension to automatically populate the names with the right values into scope rather than actually using the record accessor functions. There are compiler warnings to keep you safe for this, and you can turn them into errors, but this is still only safety through style (as is every other solution I currently know of) rather than something more concrete. It would look something like this: main = do let failure = ExecFail 1 "fail stdout" "fail stderr" let success = ExecSucc "succ stdout" "succ stderr" putStrLn "failure:" printOut failure putStrLn "success:" printOut success putStrLn "done!" printOut ExecFail{..} = do print stdout print stderr print exitcode printOut ExecSucc{..} = do print stdout print stderr print exitcode This will error, because in the second case, exitcode has not been shadowed by the record wildcard and so is a function with type `ExecResult -&gt; Int` which you cannot print, instead of an `Int` which you can print. Unless of course, someone pulled that above trick but with `Show` instead of `Num`. C'est la vie.
I specifically remember one day being bitten by this. Somebody in the IRC channel explained it and I have never used record fields over multi-constructor records again. I'll always break things out into a different type. There's lots of little sharp edges in the language that I prefer not to deal with. I use -Wall -Werror on any project I have control over, but it sounds like that wouldn't have caught this case.
But now you can't bring your changes with you to the new branch. It seems like all you're describing is an "auto commit" every time a file changes.
&gt;But it does have a warning if they don't. Yes, but that's avoided by providing a spurious default-case, e.g. `fromLeft (Right _) = error "fromLeft: no Left!` The concern you raise is a perfectly valid one which has resulted in a lot of research, however (google "dependent types" and "total languages"). It's simply an issue of pragmatics: partial functions like `head`, `excitcode`, `fromLeft`, `tail`, etc. are "bad" because they're undefined for some values, but without them, you'd have to provide proof to the compiler that you're only passing in valid inputs wherever you use them, which can become quite tedious - not impossible (and you are encouraged to minimize the use of partial functions), but tedious nonetheless. &gt;The issue is that GHC happily compiled and ran my program without any indication that my data-type is unsound. The type isn't unsound as such; the selector functions on it are. However, you can make it safe with a technique called "separating the products and the sums": 1. Turn the products `ExecSucc` and `ExecFail` into their own types: `data ExecSucc = ExecSucc String String` `data ExecFail = ExecFail Int String String` 2. Turn the sum `ExecResult` into a synonym for a generic sum type (I used `Either`, but [anonymous-sums](https://hackage.haskell.org/package/anonymous-sums) and the more complicated [compdata](https://hackage.haskell.org/package/compdata-0.10/docs/Data-Comp-Sum.html) are also available): `type ExecResult = Either ExecSucc ExecFail` 3. Only access `ExecResult` via the safe functions from [Data.Either](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Either.html) and [Data.Either.Combinators](https://hackage.haskell.org/package/either-4.4.1/docs/Data-Either-Combinators.html). These require you to either supply default values or deal with both cases.
yeah, I mean, I get their reasoning (precedence). it's just that modern software can be interfaces with over the network, not just linked against. so I get the reasoning of that extra clause. 
You provided nothing, in terms of information of your competence, so he was working with, maybe, this guys a noob, or he's nothing. Either way, what's right, is to not fail by being overly terse. The advice is free, and you can map it over to any skill level, however YMMV. joehillen, thank you for reading this far, these types for comments rarely get the attention they deserve.
Adding this warning would be a very easy patch.
I've written such a module, but as part of a larger package. Here's the Stackage LTS: http://haddock.stackage.org/lts-3.10/clash-prelude-0.9.3/CLaSH-Sized-Vector.html And here's the Stackage Nightly: http://haddock.stackage.org/nightly-2015-10-23/clash-prelude-0.10.2/CLaSH-Sized-Vector.html Note that some of the implementations are perhaps somewhat strange, but that's because I've written that code to be consumed by my Haskell to VHDL/(System)Verilog compiler.
Ideally GHC would warn about this, but as a (hopefully) temporary fix, could [hlint](https://hackage.haskell.org/package/hlint) warn about this?
A couple decades ago PL researchers devoted a lot of cycles to make compilers produce readable assembly code so that programmers could tweak and modify the output. Apparently this was desired. A few years later PL people realised that it's extremely difficult at best, impossible at worst, and usually results in substandard performance or code quality. I recently had to gently let down a prominent systems researcher when he requested our purely functional spec language compile to idiomatic C.
The [stack](https://github.com/commercialhaskell/stack) tool supports multiple packages per repository. And allows one to build all in one go.
Well it's just a decision about how much of your code you want client side and how much you want server side, and also whether you want to learn a new language. Yesod has support for view templates and forms, and you can add JavaScript, but if I only needed it to serve JSON to a client-side app I might use a lighter framework like Spock or Servant. I think Yesod + GHCJS is also an option now.
But then you break the idea that a checkout of a hash produces code in a particular state. All your tools will have to start doing reset after checkout to maintain that. I also don't see how having an implicit stash feature that is strictly less powerful and only works with checkout is an improvement. At the very least you could just implement this yourself using aliases.
Tekmo, I'm trying to understand the encoding and why you wouldn't be able to reverse it. &gt; I use Boehm Berarducci encoding and it's not clear to me if you can reverse the encoding when you start adding higher-kinded type constructors. You mean like `Either :: * -&gt; * -&gt; *` ? Why would that be the case?
In a total language, would we have two versions of the `Fix` here (one declared with a keyword `data` and the other with `codata`, say), with one of them being interconvertible with `Mu` and the other with `Nu`, but not vice versa (and with each of these having an instance for only one out of `Foldable` or `Unfoldable`)? Is "least and greatest fixed points coincide" materially different from (or more accurate than) "we are only able to express greatest fixed points"?
Yep. What I do for that is build a custom addNat :: (KnownNat n, KnownNat m) =&gt; proxy n -&gt; proxy m -&gt; Dict (KnownNat (n + m)) using reflection-like techniques, and add the other associativity and unit axioms as I need them. This can't be done with `reflection` out of the box due to the quantification but the uniqueness of the `KnownNat` instance comes from the properties of arithmetic, so it is all good.
These are different statements. When I have a least fixed point I can `cata` it, by definition. I can use `ana` to build a greatest fixed point, but in general I can't do this to the least fixed point. I have no reason to know we will stop building layers! Here we can do both to both. They've become one in the same, but the single fixed point we have has properties of both.
&gt; I don't think the right solution is to abandon type classes and have, lengthList, lengthTree, lengthFoo, to avoid that problem We don't need to. We just need an appropriate typeclass for length instead of shoving it in Foldable.
I love it! I needed this a year ago!
Fantastic work.
Building a bunch of executables is pretty slow. The way I usually do things like this is make one big executable that has each of other ones as a command. You would execute them like this: ./euler p1 ...or something. Maybe throw out the 'p' to make parsing easier. Then put all the code in separate modules, but inside a single src directory. 
Brendan, you are so obscenely productive! Keep up the amazing work, it's been quite useful for me a number of times now!
Thank you! Last week I had to decide between pull up my own riak cluster or just using google datastore. I decided to use riak because of the existing haskell client library. Now this key argument pro riak changed, maybe I will give google datastore a try again.
The problem is this API is quite hard to use as it is. You can't call any accessors without having to pattern match beforehand which kind of defeat the point of having named accessors (or whatever they are named). You arguing your design is right (and I can understand why) but at the same time your complaining that GHC let you do it. Now, you know there is a problem and GHC should forbid it, do as if it was forbidden: find another design. The traditional solution of separating sum and product type is a pain because now you are confronted with nested structures , `lens` is the solution. If you are using `lens` then don't need to do that and can use /u/edwardkmett solution. Alternatively, don't name your record, and define your accessors manually (returning a `Maybe`). 
You sound like you need a time travel library.
Compiler can't warn you in general. Consider this: module Main where data AB = A { a :: Char } | B { b :: Char } deriving (Show) getAB :: IO AB getAB = do c &lt;- getChar return $ if c == 'a' then A c else B c main :: IO () main = do x &lt;- getAB putStrLn "case x" putStrLn $ show $ case x of A _ -&gt; a x B _ -&gt; b x putStrLn "a x" putStrLn $ show (a x) putStrLn "b x" putStrLn $ show (b x) You should use something like: main :: IO () main = do let failure = ExecFail 1 "fail stdout" "fail stderr" let success = ExecSucc "succ stdout" "succ stderr" putStrLn $ f failure putStrLn $ f success where f (ExecFail e outStr errStr) = "failure:\n" ++ show e ++ "\n" ++ outStr ++ "\n" ++ errStr f (ExecSucc outStr errStr) = "success:\n" ++ outStr ++ "\n" ++ errStr 
If you are adventurous, go with 'servant'. It is cool. It is not standard yet but might be in a year or so.
full disclosure - are you a developer on 'servant' ? 
Stack should have put the binary into `~/.local/bin` The path that vim uses internally to find binaries should include this directory due to https://github.com/begriffs/haskell-vim-now/blob/master/.vimrc#L40 I have never tried in the fish shell. Could have something to do with it. Could you open a github issue please and include the error message you see as well as the result of `ls -lh ~/.local/bin`?
Unlike the situation in Erlang with cowboy, there is no de facto standard web framework. Warp is arguably the standard web server#, but that's only based on number of frameworks that use it, not actual usage percentage (of which I have no idea). I guess the best answer for you is that Yesod and Snap seem to be the most popular, and of the two Yesod seems more actively maintained. It is often compared to Rails in relation to how much the framework tries to do for you. Snap is slightly less opinionated, and I'm sure /u/mightybyte will point out the fallacy of equating frequent commits with project health :) I stand to be corrected, but it seems that although Snap has been stuck in a pre 1.0 state for a while, many people are using it in production and finding it works for them. I should also mention Happstack which is older than either Yesod or Snap but in my limited experience used less frequently. I don't know why this is, as from what I've seen of it it seems pretty cool. It offers features comparable to Yesod I think, but I'm sure /u/stepcut251 could say much more. Of late, Servant (a relative newcomer) has been getting a lot of mindshare. It provides less in the way of framework than Snap or Yesod, and focuses on type safety, doing some pretty cool things with the type system to ensure the type safety of things like routes, the content types of API end points, what data an end point consumes, etc. it really is quite remarkable how much they've expressed at the type level. At the same time, all this type level programming probably makes it less accessible to someone coming from a dynamically typed language like Erlang. Not inaccessible, but will require some dedication to deciphering foreign looking compiler errors. All that said, I've been impressed with how much attention Servant has gotten, and how much progress the project is making. Moreover, doing everything at the type level means you can derive - from the type of your API -things like a JS client, a mock API that serves mock data, documentation, etc. If you can wrap your head around how to use it, Servant will be a very rewarding choice I think. \# this is based on the empirical observation that all the frameworks I've heard of semi-recently (Scotty, Spock, Servant, etc) all use it, plus of course Yesod where it originated. Both Snap and Happstack have their own web servers, but I'm unaware of them being used outside of their parent project. Full disclosure: I've written some fairly trivial snaplets for Snap
Well, I've tried to ape `reflection` magic to bring `KnownNat (n+m)` constraints into scope, and all I've got was segfaults. Perhaps this isn't something mere mortals should dabble in. Anyway, my approach lets me write code using type-level `Double`s (or in fact any `Num`) without having an actual type-level representation of them. Unit laws can be added by a special `Zero` type (not that I'm happy with what is, for all intents and purposes, untyped programming, by writing everything using the `*` kind, but util GHC gets kind classes I don't think I have a choice), but I'm somewhat worried about other axioms; I only hope a compiler plugin could fix that.
&gt; When I have a least fixed point I can cata it, by definition. Good point. Something about the statement "least and fixed points coincide" still bothers me though... they certainly do in terms of what the type system will allow, and I guess that's all that's meant. But they still need be separate in one's head? You still need to keep track of whether the `ana` will ever stop building layers if you want to `cata` the result. I suppose this is much like saying "boolean and function types in JavaScript coincide".
&gt; In Agda, you can convert from `Mu f -&gt; Nu f`, but not back, you can embed the last fixed point into the greatest fixed point. Oh, of course: definitely finite observations on definitely finite data is not the combination we're worried about :) &gt; In Haskell laziness and general lack of concern about induction vs. coinduction lets you go both ways, as they are both the same size. Laziness "embiggens" the least fixed point to coincide with the greatest. Laziness isn't "one thing" though, [as far as I understand it](http://blog.ezyang.com/2012/11/why-cant-i-just-be-a-little-lazy/). Haskell has lazy sums, products, recursive types, and call-by-need functions. Which of them is responsible for the embiggening? (This might tie into question #3.)
Another one that hasn't been mentioned yet is [Spock](https://www.spock.li/), which (like Snappy) is intended to be a lightweight framework. There's also [Happstack](http://www.happstack.com/), which is intended to be a full-feature framework (like Yesod), but it seems there hasn't been any development activity for a year now.
&gt; I should also mention Happstack which is older than either Yesod or Snap but in my limited experience used less frequently. I don't know why this is, as from what I've seen of it it seems pretty cool. It offers features comparable to Yesod I think, but I'm sure /u/stepcut251 could say much more. The primary reason that Happstack is not used more is a lack of great marketing on my part. It is still very much alive though. In fact, I am working on `servant-happstack` right this very minute. [The Happstack Book](http://www.happstack.com/docs/crashcourse/index.html) provides great documentation and is still up to date. Now that WAI no longer depends on conduits, I think it is time to finally finish up hyperdrive -- which is a pipes based low-level backend similar to warp. I would say that one mistake we made with Happstack was giving people too many choices. Many Yesod users believe that you have to use Hamlet (the yesod template library) and type-safe routes, etc if you use Yesod. In reality, that is not true, but having a clear, blessed choice is definitely an advantage. With Happstack we have support for template HSX, blaze, hamlet, xhtml, and more. We have both type-safe URLs (web-routes) and dynamic routers (dir/path/etc). But, most people do not have the time, desire, or expertise to determine which of these options is best for them. We should really have taken a stand as to what we believed to be the best of breed. In reality, almost everything in Yesod already existed in Happstack before Yesod came around. But Yesod did a much better job of promoting their framework and providing one clear way of doing things. 
Hmm... thanks! And now I've learned a bit more of Agda as well :) I guess the issue is that `newtype Fix f = MkFix { unFix :: f (Fix f) }` gives you _both_ `MkFix` and `unFix` - while to make it be either data or codata, you would need to restrict one or the other (which then ends up corresponding to either `Mu` or `Nu`)? What I kind of had in mind, though, was that the `data` and `codata` versions would place different restrictions on `f` (which may or may not be expressible in a particular existing language) such that the result, after substitution, would amount to a "proper" inductive or coinductive definition... I'm not sure if that makes any sense. I think I was implicitly drawing on intuitions about variance, which is a different thing (but somehow tied in nonetheless due to the check for *positivity*). *Does* it make any sense to talk about an `f` functor having "data vs. codataness"? &gt; while `Fix` only works in non-total languages, and in those languages Mu, Nu and Fix are all equivalent to each other? It is alleged that laziness dun it, not just non-totality.
Do you have suggestions for how the process could be improved?
Some user-powered subjective comparison [here](http://www.slant.co/topics/727).
Well in a lazy setting a sufficiently 'productive' `cata` can still yield an answer in the presence of an infinite structure. In something more rigorous you can't even try.
It is the laziness of functors in general here. If you had something like a strict functor e.g. one that required `fmap' _|_ = _|_` then you'd get access to a smaller fixed point. (That isn't quite right because it'd do the wrong thing on a function space, but at least gets towards the right idea.) It is the lifting of our sums and products that bites you.
Do you have a different link? The video is 774mb and I can't watch it without buffering, but the player doesn't allow the video to buffer. It seems to be hosted on vimeo, is there maybe a non hd version? 
Really excited about `servant-happstack` and `servant-isomaniac`, please keep us/me posted!
Sure, here's a standard-def link https://player.vimeo.com/external/143409955.sd.mp4?s=e037dabf7e89da8a69a11146977ebbcc Thanks for reporting this, I'll look into setting up my video player to be able to choose alternate resolutions.
That use of `unsafeCoerce` looks decidedly implementation-dependent? It's not hard to rewrite it relying only on the property that `unsafeCoerce :: a -&gt; b = id` when `a` and `b` are in fact the same type.
I don't think I'd call this a Haskell dialect at all, I think this is just Python with some superficially Haskell-y syntax...
If I understand it correctly, here's what happens with your WeirdTreeF: http://sketchtoy.com/66067351 Inspecting the root value evaluates it to WHNF, and its right branch, and _its_ right branch, down to the leaf: A piece of data. Then, we can inspect left branches, and each of them is going to generate a new subtree, the rightmost path of which is immediately explored. This might continue forever, therefore we have codata. For that word, I would guess ["duality"](https://en.wikipedia.org/wiki/Dual_(category_theory), though I wouldn't use the term without making clear what it stands for.
Well I'm sold
Phwew! I came into the comments with the same question. How could one person write a library of this size? Still an impressive feat.
Unrelated: https://hackage.haskell.org/package/bifunctors-5/docs/Data-Bifunctor-Biff.html https://hackage.haskell.org/package/bifunctors-5/docs/Data-Bifunctor-Tannen.html
Not at all, `someNatVal` will happily generate a `KnownNat` dictionary for any (non-negative) `Integer` you want. Cleanest thing is to define `unsafeSameNat :: forall proxy (a::Nat) (b::Nat). proxy a -&gt; proxy b -&gt; a :~: b` and just use that *ex falso quodlibet*.
or even `unsafeCoerceNat :: forall f (a::Nat) (b::Nat). f a -&gt; f b`
That would be better written like this: (those possibly with longer names) f (ExecFail ec out _) = (show ec) ++ out f (ExecSucc out _) = out
And I don't know much about pipes. With conduit you wouldn't even need this library for that, once you are already writing that many lines of code: pathconduit :: FilePath -&gt; Source IO (FilePath, [FilePath], [FilePath]) pathconduit path = do paths &lt;- lift $ map (path &lt;/&gt;) . filter notDots &lt;$&gt; getDirectoryContents (subdirs, files) &lt;- lift $ partitionM doesDirectoryExist paths yield (path, subdirs, files) mapM pathconduit subdirs where notDots = not . (`elem` [".", ".."]) And if you want to go down the lazy IO road and get a nice tree: pathtree :: FilePath -&gt; IO (Tree (FilePath, [FilePath])) pathtree = do paths &lt;- map (path &lt;/&gt;) . filter notDots &lt;$&gt; getDirectoryContents (subdirs, files) &lt;- partitionM doesDirectoryExist paths Node (path, files) &lt;$&gt; mapM pathtree subdirs where notDots = not . (`elem` [".", ".."]) I'm sure something analogous can be done in pipes, too. EDIT: Fixed the type of the `Tree` output.
Yes there are three of them on Hackage :) I just linked the most recently-updated one, without actually trying it out. 
Or even like this {-# LANGUAGE NamedFieldPuns #-} f ExecFail{exitcode,stdout} = show exitcode ++ stdout f ExecSucc{stdout} = stdout
I am pretty sure Rust is not a total language either so it should allow some partial functions to compile. Maybe not in sum type accessors but in other areas?
What a champ. Nice work! Brendan: If you're interested in a Haskell job in London, contact me.
Alejandro Serrano, Jurriaan Hage, Dimitrios Vytiniotis, and Simon Peyton Jones are trying to better support higher-rank polymorphism (allowing forall-quantification everywhere in a type, not just at the top) than what currently exists in Haskell. Previous research (MLF and HML) has shown that a particular kind of bounded/constrained quantification is essential to good inference of higher-ranked polymorphism. This constrainted quantification is written `forall a. (ty ~&gt; a) =&gt; ...`, where `ty` is any type expressuon and `ty ~&gt; a` means "`ty` can be instantiated into `a`". More generally, `ty1 ~&gt; ty2` is a new sort of constraint that expresses the natural notion of subtyping between polymorphic types (being more general than). The traditional example for the need of such bounded types is the type of the application `select id`, where `select : forall a . a -&gt; a -&gt; a`, and `id : forall b . b -&gt; b`. In System F, you can choose to give `select id` the types `forall b . (b -&gt; b) -&gt; (b -&gt; b)`, or `(forall b . b -&gt; b) -&gt; (forall b . b -&gt; b)`, and neither is more general than the other. With MLF-style quantification, you can write `forall a . ((forall b . b -&gt; b) ~&gt; a) =&gt; (a -&gt; a)`, and that is more general than those two types. Integrating this is Haskell's type system is Really Hard. Those systems are known to be relatively complex and involve difficult trade-offs. Furthermore, the interaction with type families (or any kind of type-level computation) is ongoing research. Another difficulty in the presented design is the choice to eliminate those bounded quantifications from the types finally presented to the user -- so, if I understand correctly, the extra power would be used for internal type-checking. This requires to make additional choices that are somewhat arbitrary, and thus get good experience to find the right heuristics.
I agree, FRP models the arithmetic example you give (and several others) well because there's a static collection of widgets. Any time your signal graph is static, you end up with simple, elegant examples which is what draws so many people into Elm. `Time -&gt; T` combinations really do model static collections of widgets really well - I've conceded that since the beginning; the problem is when you want widgets to appear and disappear. That's where the elegance fails, the proofs become at best brutal, and we get people like OP wondering what went wrong and where the elegance went. My original claim has never been that FRP is worthless or stupid; rather that as soon as you need bind, it ceases to be a good model. As for the last paragraph, I'm not sure exactly what you mean? A while back I played around with modeling various "pathing logics" (I have no idea what the proper name is), e.g., stop lights, request chains, basically anything where you have a given "starting point" and a set of ways to get from one point to another (graph-ish, but possibly not finite), and I don't remember having too much trouble proving these sorts of things. I mean it wasn't "alert('hello world')" simple, obviously, but I don't remember using anything painful like quotients.
Right, but something like this f x@(ExecFail {}) = stdout x allows you to avoid changes to your code when definition of data structure is modified. I should have used it in the first place.
Yes, as much as possible is generated in both the `gogol` and `amazonka` projects. Unfortunately due to the lack of type information in Google and Amazon's respective formats, there is a lot of 'convention' and hand wavy rules of thumb to reify type information/metadata, which along with disambiguation of types and record fields seems to be where most of the complexity lies (ie. in the code generation) - outside of the usual serialisation shenanigans when dealing with remote APIs. Besides that, it's just a bunch of data types which due to the sheer number of, and compilation issues, are required to be split out into smaller packages to make it operationally maintainable.
I'm glad you found it/them/things useful!
Is this meaningfully different from a general `ty1 &lt;: ty2` subtyping constraint / is it easier (less hard) to support / how come?
This is kind of a minefield topic. Inferring most general types for prenex polymorphism (quantifiers at the beginning only) and subtyping is known to be undecidable. Inferring most general types for higher-rank polymorphism (no restriction on quantifier placement) is also undecidable, so any solution requires adding some sort of annotations. The MLF and HML work (by Didier Rémy, Didier Le Botlan, Boris Yakobowski, and Daan Leijen) has shown that this form of instantiation subtyping makes higher-rank polymorphism inference tractable, in the sense that we have reasonable specifications of where annotations should go. I don't think we know how to preserve these good properties if we add subtyping constraints (already for prenex polymorphism, and even less so for higher-ranked polymorphism). (I'm not very familiar with the subtyping research, but the works I know for type inference in presence of implicit subtyping tends to either restrict the type system or give up on decidability and aim for semi-decidable algorithms.)
Nested patterns can produce an arbitrary number of additional variables in the output language. What names will you give those variables?
Descriptive names. I understand people don't like the idea. I'll keep it for me.
Scheme manages it, though how well is up to debate 
Julie and I put this together after about a week of mulling the script, did it yesterday, uploaded it over-night. It doesn't cover _everything_, but we hope it'll get people just getting started less likely to get stuck.
We just did that for the video so that the environment didn't already have Stack and ghci and all that installed, because, of course, Chris's machines *do* have all those installed, as do mine. 
Could someone provide a practical perspective on this? What are the things this will/would enable that you couldn't do before?
Don't get me wrong. I think it's a very worth while experiment to carry out!
Ooh, thanks!
You've used the word "undecidable" twice above. In both cases, do you mean to say that "there are most general unifiers but it is undecidable to infer them" or "there are not even necessarily most general unifiers"? My recollection, though I haven't checked, is that in the higher-rank case MGUs don't even exist?
Technically it is a back end to replace the current one in happstack. It could be used as an alternative to warp but not a drop in replacement. Among other things, it uses pipes instead of conduits.
Yeah, I think that your code would typecheck as a `Producer` just as well.
One way to sort of visualize what's going on is to note that the "genuine" least and greatest fixed points are just that: the least and greatest examples of fixed points (of pattern functors). By what way are they least or greatest? By the way of "definedness" which is a way of talking about what happens with undefined values. Laziness (Haskell's CPO semantics) has the effect of allowing us to deal with partially undefined things as though they were normal, simple values, and, therefore, makes "least" defined things be properly defined far more often then they normally have any right to be. This "growing" of the least defined objects has the net effect of making them equivalent to the greatest defined objects. We can therefore interconvert between all of Mu, Nu, and Fix. To see the least fixed point as a degenerate greatest fixed point is easy, but to see that the GFPs are also subsumed by the LFPs requires careful use of laziness. You can go ahead and explore this by writing functions `Functor f =&gt; Mu f -&gt; Nu f` and `Functor f =&gt; Nu f -&gt; Mu f`. One will probably feel *way* easier than the other.
Hmm, I don't think rust allows partial functions in any context.
We did say that you don't need a Docker container or Emacs specifically for Stack. I believe we explained in the video why we were doing them. I'm sure the video isn't perfect, and constructive feedback about things we could have done differently is welcome, but I honestly don't know what to say to someone who wants to use Stack but is discouraged by the idea of needing a text editor. Perhaps we should have installed Emacs into our container before starting the video (something we will consider for next time), but having a *tabula rasa* environment seemed the best way to show the entire installation process from zero. Using one of our machines, even if we'd tried to uninstall all the relevant things, would have probably still given a different environment, and, thus, not the same problems that someone who has never installed any of it would run into. In other words, we did spend a lot of time thinking through what we were doing and why, in the spirit of giving people who want to learn more information rather than hiding things from them, and then you just think we didn't give the right impression in the first 2 minutes. That is indeed pretty discouraging. 
I'm really confused now. How is servant-happstack going to be a replacement backend to happstack, when servant uses warp for its backend?
I've only watched 10 minutes of the video, but I disagree with this criticism completely. The start at least is extremely beginner-friendly; it is clearly trying to fill in gaps people might have in their familiarity with Linux. The use of docker is totally incidental. Arguably, they could have set up a user account and called it a "totally fresh system" without ever mentioning docker. I think it's good that they go through the installation steps so as to increase the likelihood of a viewer being able to reproduce things. My only complaint with what they do here is that it makes me very aware of how slow my home internet connection is.
Having looked at various web frameworks in Haskell, and coming from an Erlang background like yourself. I found I wasn't happy with any of the usually choices. Then I found [airship](https://github.com/helium/airship) which is a port of Webmachine/Erlang into Haskell which has all the advantages of the Erlang version. I'm using Airship at work for various REST API's and have found it to be much better than Scotty and Spock. Because it's modelled on the HTTP state machine from Webmachine, it gives the correct HTTP behaviour by default. 
For some reason I cannot resolve this link, but yes, this is a result from Joe B. Wells, in 1994 if I recall correctly.
I am not saying it is a bad tutorial, my point was more that it should be labelled to be for the target audience of complete beginners so others with more experience don't think it really takes 2 hours just to install and get started with Haskell and stack alone. Haskell already has a reputation to be hard to learn.
No, it guarantees all function arguments are irrefutable patterns.
Yeah, I think that's fair. The beginning at least is more getting started programming with Haskell using Stack than Stack for Haskell programmers. Maybe there could be a time stamp to jump to if you've already installed stack yourself.
That's a good idea. We also cover some odd cases (e.g., libraries with unusual dependencies, installing Idris and PureScript) that don't come up for just every Haskell user, so maybe adding time stamps would help people see it isn't 2 hours of installing Stack and ghci. Thank you for the constructive feedback. ETA: Well, it appears my diligent coauthor has already done that. Hopefully that will help allay this concern. 
&gt; Sure, but that wasn't the question. Sorry.. :) &gt; Can it do so using "unix wildcards or regexes to build everything without needing an explicit executable section for each problem in the [yaml] file". Yups. You could port GHC to bash as well. But the thing is that "mega repos" are a thing, and it looked a bit like you wanted to have that thing. Stack supports it. &gt; But it still needs a section for each, no? It needs one line per package in the `stack.yaml` file. 
&gt;Maybe there could be a time stamp to jump to if you've already installed stack yourself. I added timestamps, you can skip the intro if that's where you're at (second timestamp).
&gt;ETA: Well, it appears my diligent coauthor has already done that. ohai
(Author here) Feedback is welcome -- I'd be particularly open to optimization ideas, since the cross-compiled JS slows to a crawl on big systems.
Not by default, but it does it pretty well with nested lambdas. And it's scheme, so the extra parentheses are nothing new. 
The article sounds interesting. The last line of the abstract is a real teaser: "The library has been used for the design of real-world track layouts." Shame about the $45 price point.
Oops. You should definitely be confused. I was in a hurry and on my phone and answered the question 'What is hyperdrive?'. `servant-happstack` is just like `servant-server` except based on `happstack` instead of `warp`. This is particularly useful if you want to update existing `happstack` applications to use `servant`. 
[Usually chilling like a villain](http://bitemyapp.com/images/haskell-logo.png), especially now that the weather has cooled down.
This is great! I can't wait to try implementing it. How much more complex is a system like cassowary?
you can edit .ghci to import your app and connect to the database. like import qualified App connection &lt;- connect App.settings 
IIRC Yesod apps by default have functions defined so you can do stuff like: $ stack ghci -- Loading... λ: users &lt;- db (selectList ([] :: [Filter User]) []) λ: newId &lt;- db (insert (User { userName = "Foobar Johnson", userAge = 40 } )) λ: db (update newId (UserName =. "Cave Johnson"))
Depending on how well Haste optimizes the JS, this might perform a bit faster: minimize :: Ord a =&gt; Equation a -&gt; Vars a -&gt; Vars a minimize eqn = hylo (step eqn) where hylo f a = maybe a (hylo f) (f a) which can be inlined to: minimize :: Ord a =&gt; Equation a -&gt; Vars a -&gt; Vars a minimize eqn vars = maybe vars (minimize eqn) (step eqn vars) (yay recursion schemes!) And then this is tail-recursive, so.. hopefully that's better too: minimize eqn vars = go (step eqn vars) vars where go Nothing finalVars = finalVars go (Just nextVars) _ = go f (step eqn nextVars) nextVars
Yes Yesod does have this. Check out [this wiki page](https://github.com/yesodweb/yesod/wiki/ghci) covering using GHCi to run Persistent queries, `Handler` code, or your entire app.
/r/scholar
&gt; Please log in to watch this conference skillscast. *sigh*
u/edsko pointed out that contravariance may also occur in work on subsumption relation (but not all such systems), it is notably a key part of John Mitchell's F-eta, a seminal work in the study of the notion of subtyping induced by polymorphic instantiation. I also disagree with the distinction between denotational and syntactical appraoches here. Various subtyping relations (subsumption included) may be also be understood as some form of inclusions in denotational models (with the known hiccup that the naive set-theoretic model of predicative calculi [cannot be extended](https://hal.inria.fr/inria-00076261) (Reynolds, 1984) to System F), and there are syntactic approaches to correctness of usual-subtyping (semantic subtyping is only one of the techniques). So I think whether or not you use models for your correctness argument is somewhat independent of the kind of subtyping/subsumption relation you are working with.
It could let us dispense with some (most?) of the newtype wrappers whose only purpose is to hold polymorphic types. 
You could try this https://open.gl/
If you are working with numerical matrices, use [hmatrix](http://hackage.haskell.org/package/hmatrix-0.17.0.1), that uses BLAS and LAPACK under the hood. I use matrices that have similar sizes all the time and it is really fast. You can create matrices with `toDense` if you are using lists of the kind `[((Int,Int),Double)]`, or with `fromRows` and `fromColumns` if you are using lists of lists, in which case you will have to convert the internal lists to vectors with `fromList`.
Both errors doesn't seem to be reproducible. We had similar assignments on university. Things like mouse GUI without clicking. 
It's painfully verbose, and impossible to persuade the tool to keep both the details of the bit you're focusing on now *and* a global overview visible at once. Doesn't help that it likes to abbreviate labels, either, so I've got lots of nodes labelled "re", with no easy way to distinguish them. At a minimum, to make this useful, you need some way of folding the first N recursive calls to a function like map or foldr, so that I can see the global design and the local call at issue simultaneously.
Indeed, there are a lot of improvements that can be made. I've been getting good feedback. I agree that better ways to explore large and deep computations are necessary. This is a basic prototype that I'm evaluating for a minor thesis, but I have plans to develop something more practical based on this feedback in the future. Determining what *doesn't* work is just as useful as determining what *does* work!
I find your experience about how verb function names affect some kids to be really interesting. I've never felt strongly either way, but this perspective makes a lot of sense. I think I'll start trying to use it more consistently in my APIs.
At least for Oracle, recent versions cache multiple query plans for each prepared statement and use heuristics on empirically observed query execution times to choose from among the plans for each set of parameter values. That also can be either good or bad - there is some discussion [here on SO](http://stackoverflow.com/questions/12286313/do-sql-bind-parameters-affect-performance).
we've got a few violations in Haskell's base library as well, e.g. sort or reverse are examples which always struck me as oddly named but I guess this is not a good time to propose fixing that :-/
Just to double check, are you able to utilize HT with your workload? Windows tends to visualize HT cores as a second core even though it doesn't have the full capabilities of a core.
imho those educators underestimate their own teaching skills or their students' cognitive abilities
If it was to be called `foldLength` I'd rather have the pre-FTP state back where I can `import Data.Foldable as F` and then use `F.length` if I wanted a prefixed `length`. If we're going to rename I prefer `size` without any redundant prefix. Just see how popular other prefixed functions such as `genericLength` are...
I found it absolutely unusable on the first task already. Frankly, I find debugging in a tool which gives you a choice between showing almost no information or too much very painful. To me this is just another indication that visual programming goes into a completely wrong direction as the spatial relationship between the bits of code (or the colours for that matter, compared to regular syntax highlighting) adds nothing at all to my ability to solve the problem at hand, on the contrary, it gives me an unnecessary choice of reading directions, making it hard to systematically evaluate every bit of the code for errors without missing something.
FYI your videos never work for me in Firefox (Linux 32-bit). It always says "No video with supported format and MIME type found". If I switch to Chrome they work. 
It works, but is not as elegant as it should be IMO; feedback is welcome!
Does the direct vimeo link work in firefox or is it a problem specifically with my javascript video player?
Are you compiling with `ghc +rtsopts` and running with `&lt;program_name&gt; +RTS -N2`?
Will you publish your results here too, or do I have to "sign up" by emailing you?
Well I guess we can rename `Foldable`'s `length` to `lengthened` and then everyone will be happy, right?
Awesome! I haven't seen beginner/intermediate Haskell code that makes a GUI before, so you can bet I'll be looking through this later tonight! Might I ask what source you used for learning to use the Graphics package? 
+1 a priori; I'll try it out in a bit :D
I started with a simple display, and after that I started googling the net. I found out about play somewhere in a blog post, and some more googling revealed playIO. I'm currently on my cellphone so I don't have the links available, but I recall one of the articles was about a pong game.
I'm not sure it's so important in standard Haskell, to be honest. There are some places I had to make some awkward naming choices to make it work, and for experienced programmers, that might matter more than the issue it fixes. I've just been convinced that in an educational environment, this sort of thing matters a lot more.
Yeah, and for the bits that are working as I expect, I don't really need a visualization; it *might* come in useful for finding out why the code isn't doing what I expect, but with the current amount of noise in the display, I get swamped by "it's fine here" noise. Pruning away everything that meets my expectations means that the noise goes down - and for something like "foldr distance measurements", it's clear whether it's the foldr doing the wrong thing, or distance being problematic.
strict languages can also target core, right? using case, not let. or is the idea that the main benefit of core is the optimization on lazy values? If I ever make a language, not having to deal with codegen would be great.
Indeed. You could use `length.toList` if you really needed the `length :: Foldable t =&gt; t a -&gt; Int`.
1. This is not visual programming, but a visual representation of the runtime behaviour of a program. 2. The problems which you point out in this case seem to be issues with the current implementation of the idea, not necessarily the general idea itself.
as its used, a "partial function" is one that might not handle some cases, might not terminate, might throw an exception.
I haven't taken too much of a dive, just read a bit about it. Can any of this functionality be backported into haskoin and/or haskoin-wallet? And/or, can this be made to depend on these packages, where there is functionality overlap? I am developing on haskoin actively (as tphyahoo), mostly working on improving the bip32 functionality, and I am interested in multisignature escrow functionality as well. 
The compiler does not autogenerate partial accessor methods for enums, nor does it allow functions that accept refutable patterns as inputs. I'm not trying to suggest anything beyond that. (Edit: but in practice just those differences provide a substantial gain in type safety IMO).
public funds, private profits
Interesting. I agree that experienced programmers are less likely to be tripped up and using a verb as a function name will probably not make them think that a pure function should have some side effects. But it seems like a solid argument for defaulting towards nouns for pure functions when possible.
More thoughts. Multisig is a beast. Many of the open issues on haskoin wallet github repo relate to improving multisig functionality and security, and ideally we would want to beat bitpay / copay at its own game. Maybe some of these ideas will even be implemented some day. Meanwhile there is a long idea thread at https://github.com/haskoin/haskoin-wallet/issues/124 you might find interesting. If easybitcoin is going to be more than toy, you should allow viewing / signing wallet split, which allows to use cold computer or hardware wallet. Since most hardware wallets are bip32, right there that means you need to support bip39/bip32 (if you want to be hardware wallet compatible). I would suggest to support bip44 as well as that gets you compatibility with most mainstream wallets which makes life easier for testing and end user. Do you already have viewing wallet / signing wallet split capability? It sounds like you have lost some of the type safety enforced by keys in the haskoin model, which I think might be a mistake. Yes, the haskoin codebase can be overly heavy in places, but I think that can be improved without forking. I will have a bit more of a look at escrow workflow and see if there is any inspiration to take there. One thing you did right, simpler is better always :)
Ad. 4 -- so that's why they weren't present! I thought it was just broken.
&gt; You are stressing and tiring out the people who are trying to do something productive for the Haskell community. I think you may have overstated yourself here, apparently speaking for all the experienced haskellers, package authors and tool maintainers, and in this paragraph the productive haskell folk generally. I think it's unfair to lay this much on one commenter, even if you are a little jaded on the topic. Perhaps take a break next time you feel this cross. There's more than one person who has repeated their point on contentious issues.
Thanks! The board representation was a conscious choice - and could have been a grid depending on my mood -: while removing a line is harder with this code, the rendering and occlusion code is very straightforward. I've solved this kata differently in other languages in the past with the grid representation, and I must say both probably have it's advantages and disadvantages. 
FYI I've made it a little bit faster, added scoring and a restart function.
Even if you could, it would still be as useless as it is today :-P
Well you have the connection, you can run anything you can run with your connection.
Personal question, why is is `-N2` and not `-N72`?
Thanks for the feedback, much appreciated. &gt;Related to the previous point: I wouldn't call this a debugger at all, but rather a trace analyser. A debugger is a program that allows the suspension of another mid-execution, and the analyser/change of its state. The web-interface at least showed a program that took a completed run and visualized the execution paths. It's true that you aren't able to modify the execution in this prototype, but the functionality exists to do that. What you're seeing is not actually a trace - as you expand parts of a computation, that part is re-evaluated so the intermediate computations can be captured and displayed. I could easily allow the user to choose their own evaluation model and interactively change parameter values. &gt; The tree view is mentally very taxing, as others have noted. In principle, one can reconstruct the source code from it, but that seems to be pointless busywork. It would be neat to have the original source code in a side panel, with parts of it highlighted when the user mouses over the tree and vice versa. Yeah, the source code would be useful. I only excluded it because I knew that people would try to find the bug using the original source, and this would compromise the experiment. I've since thought of ways that the original source code of a program could be more tightly integrated with this kind of trace to reduce verbosity and simplify things a lot. &gt; The results of unevaluated branches of an IF weren't shown and there was no way to force that. This, however, would be very useful, especially if one is debugging some faulty condition. Absolutely, this ties back in with my response to #2. &gt; Following clear errors like pattern match failures and stack overflows seemed the be the most useful feature. Perhaps you could visually emphasize such branches, or implement a "show error source" feature that opens up the tree to show the branch where the failure originated (at least for pattern match failures)? This would be great, and was always planned, but due to implementation complexity I've not included such a thing in this initial prototype (it's in need of a complete redesign). A similar feature that may be useful would be to highlight all the computations that helped contribute to a particular output value (and vice-versa).
Sure, I will do that as well.
That's a great idea, thanks. You're right that it's not very useful to be bombarded with information about things you know are working correctly. I suppose the real challenge is to find and display the smallest amount of information which answers your questions. Query-based exploration is probably better when you have an idea of what you're looking for / what you want to see.
I keep meaning to add an implementation of vectorized root-finding to `ad` for reference purposes.
It would not only solve this problem (which is good) but may help with managing orphan instances as well (which is great).
This is great, though I'm a little sad about the decision to stop work on stack-ide. I've _never_ been able to get ghc-mod to work for more than maybe a couple days at a time (I don't understand how it stops working... but every time I get it working , it stops shortly afterwards, or the same steps don't get it functional on another machine, etc), and even though it is still quite early (and it, too, unpredictably degrades into only showing type errors, rather than giving type / info on expressions), stack-ide has been more reliable (and a lot faster).
What does it mean to import a module where an instance is forbidden?
&gt; I wouldn't call this a debugger at all, but rather a trace analyser. I can definitely see your point, however I think for a pure functional language the distinction is less useful (as long as it doesn't touch IO and RealWorld). I would say that a trace analyser examines the program at a snapshot in time, and a debugger steps through it incrementally, but without defining some particular operational semantics, what's the difference?
`projectionInOneDimension`
Why, it could be not different from importing a module where this instance is just not used.
I agree with your priorities, but honestly the syntax is very nice too. Most other languages often opt to allow syntactic cruft to make writing the parser easier.
Or it could mean that the instance is forbidden transitively, which is why I ask. Otherwise asking newbies to explicitly ban instances at the top of their module isn't exactly beginner friendly.
Yeah, but you should always ask. It's better than just speculating about things. It might be the case where you are doing something that uses cabal and is making a dev/ directory in your project which confuses ghc-mod if it is otherwise trying to use Stack… known bug, but also known workaround (move/delete that directory)…
Well, I didn't have beginners in mind here... the fact that I mistakenly write `length (someValue :: Maybe a)` shows that I'm sloppy, not necessarily that I am a beginner ;) 
The real issue here is not that these types conform to a particular type class -- that's simply mathematical reality -- but rather that Foldable is naming a function `length` for a concept that is simply far more general than what anybody means when they use this English word. Math isn't the problem here; misleading English labelling of the math is the problem. Not that I have a good solution to this. Natural language is pretty arbitrary and informal anyway, and sucks at explaining anything more advanced than Justin Bieber. Just look at the nightmare the quantum physicists have gotten themselves into trying to explain their ideas in English.
Good point. I guess /u/voxfrege/'s solution is acceptable, but then I would like to be able to forbid instances also at the package level somehow, so that I cannot forget to forbid them in a module. I would prefer that "instance blacklists" that are defined in a module would be exported with a similar mechanism as functions and types are now. That is one can write module HatesFoldableMaybe (... , forbid instance Foldable Maybe ...) where to export an "instance ban". (As for functions, modules without an export list would export all bans). As a consumer one can write module LovesFoldableMaybe where import HatesFoldableMaybe hiding (forbid instance Foldable Maybe) one = length (Just "Hello World") Would that work? 
IIRC /u/edwardkmett had some good comments about the good things that follow naturally from the foldable instances. I don't have time to look them up now (and I was dumb enough to not bookmark them!), but if you trawl through his relatively recent comment history you should find one or two comments on the subject.
Fair enough. However, I think it is sound and it does not seem to be very complicated to implement (which is not a very informed statement, I have to admit). In doubt, one could always settle for /u/voxfreges/ local solution. 
Ok, thanks for the pointer!
Without knowing your specific ghc-mod issues, here's a few points that might help: 1. Always ensure that ghc-mod is installed with the version of cabal you are currently using, i.e. if you update cabal, reinstall ghc-mod. 2. Always compile ghc-mod with the same GHC version you are using to compile your libraries and executables. 3. Run ghc-mod at the command line on your Haskell project as this may reveal error messages that your text editor environment might be masking. 4. Check to see if there's any broken packages listed with `ghc-pkg check`, and fix any that are.
I want to write a bip45 alternative, based on some of the conversations on the haskoin wallet issue tracker. No, it's not easy now; and it's not really as safe or private as it should be either. I would like to see ubiquitous multisig start to hit in 2016... I haven't figured out a solid vision for making it happen, but I'm working on it.
haskoin has gotten significantly better in the last couple months. I couldn't even get it to install when I tried it in early 2015. Now everything is stack install on first try, easy and clean, and works on mac. I'm not sure if haskoin wallet is reasonable to use as a library, or if it is more of an "app." It has quite a lot of moving parts to make it a usable wallet, including zeromq and sqlite and iirc leveldb. Might make sense to depend on haskoin-wallet, but then again you might want to depend on the bits that haskoin-wallet already depends on. (To really answer that question well would take a closer look at code. Ask me again in a few weeks :) ) haskoin wallet depends on haskoin-node (for spv) and haskoin core itself, for all the key manipulation stuff. 
Oops, misread the question, missed the bit where he had 72 cores
Great idea. Perhaps I should also try to prototype the proposal as an external tool and to see how well it works.
I guess it means when the compiler can use fusion / recycling which is, unfortunately, only done locally (AFAIK) in a function. Said otherwise, if your vector is created inside the function, modify may be in place. Else it won't.
Try out the build you a compiler tutorials. The pipes library is a really good example of a well-designed project focused on types. The lens library is the Übermensch, and it's worth a few months.
Feel free to update my original tutorial too. https://wiki.haskell.org/Numeric_Haskell:_A_Vector_Tutorial
Huh. I'd like to see a benchmark comparing that to `Data.Sequence`; i.e. comparing HAMTs to finger trees.
I'm using hedis at work. Thank you for your efforts, hedis is the most mature redis client for haskell. I agree that exception-based error handling would be better. Also I find pubsub API too restrictive, I have a number of dirty hacks to work aroung the "enforced pub/sub semantics". I'd like that to be changed. I'm not familiar with redis enough to maintain the library, though I'd be happy to help as a comainainer. 
&gt; I didn’t find a lot of information about static linking with ghc on the internet AFAIK, this is quite the euphemism. And it gets even worse when you have a `data-dir`.
Hyper thread cores tend to not be very useful unless you don't have much memory traffic. Use -N36 which will be he number of physical cores. Or write your code to have less allocation / memory traffic :)
the [matrix](https://hackage.haskell.org/package/matrix-0.3.4.4/docs/Data-Matrix.html#v:matrix) function is faster, because you don't need to generate 1000 lists. If the matrix is sparse you are better of with a sparse representation (for example from the [matrices](https://hackage.haskell.org/package/matrices) package).
Try `fmap` [`fromIntegral`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#v:fromIntegral). `fromIntegral` can convert between any Integral to any Num, and so sounds like a good candidate for converting between two integer formats. And indeed, Int32 is listed as having an [`Integral` instance](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#t:Integral) and CInt is listed as having a [`Num` instance](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#t:Num).
`CInt` is an instance of `Integral`, and `Int32` is an instance of `Num`, so you can use `fromIntegral :: (Num b, Integral a) =&gt; a -&gt; b` for this.
Based on my experience, the path to this directory seems to be absolute in the executable so you need to cabal build with a `prefix` to put it somewhere shared but then cabal might not have the right permissions to access these directories and it's a pain to escalate permission on demand. I just remember struggling a lot when trying to get travis to build a static binary for Agda and the only resources available online were outdated tutorials and answers on stackoverflow linking to defunct blogs. I haven't written anything about my process because my solution was really unsatisfactory (basically create the directories I wanted to populate in `/usr` and `chown` them before starting the build).
Thanks for the quick replies, I think `fmap fromIntegral` was what I am searching for. Didn't considered it because I was searching a Function with multiple arguments. I will test this solution in the evening. Greetings Manuel
I really appreciate the qualified and explicit imports. Chasing names when reading unfamiliar code is taking up so much of my time.
if you subscribe to that school of thought, you don't have much use for generic programming anyway, do you?
A nice thing to support would be to support a 'destdir' for the installation path, aside from the 'prefix'. This is how most autoconf packages are built on e.g. Debian: `prefix` points to where the binary will *eventually be*, `destdir` is where you want to *copy the files now*. This is how you work with the fact your package has artifacts to distribute. What you really want for example is to say your `prefix` is `/usr/local`, but the `destdir` is something like `$PWD/tmp`. Then when you build the package the file structure would look like `$PWD/tmp/usr/local/{bin,lib}` etc containing the installed results - the basic system file layout as if it was stuffed in a directory. Then you can just tar that up (everything under `$PWD/tmp`) and say "Unpack this in the root directory", which is basically what a `.deb` is, just with more metadata.
Couldn't you use constraints to simulate that to some extent? e.g. (not an induction example though) ``` append :: (k ~ (n+m)) -&gt; Something n -&gt; Something m -&gt; Something k ```
Out of curiosity, if there are any Scala folks here, what would you say is the Scala equivalent of `undefined` for the purpose of wish-driven development? My working theory is `null`.
Define the core algorithms and data structures. Specify them (e.g with quickcheck or a formal modelling tool, or just by-hand reasoning) in formal detail, then wrap them in an effectful monadic program that actually makes them talk to the outside world. I put this technique to good effect implementing my wiki system ([dixi](http://github.com/liamoc/dixi)), and the xmonad window manager is defined a similar way. Also, for libraries, I get the feeling that the pipes library was designed with similar principles. For the case of my wiki, the patch theory algorithms are encapsulated in the [patches-vector](http://hackage.haskell.org/package/patches-vector) library, and the data structure used to store each version is in the [composition-tree](http://hackage.haskell.org/package/composition-tree) library. Note the large volume of quickcheck properties (embedded in the documentation with doctest), that constitute a formal spec of the libraries. The generality of these libraries is a good sign that my design is good -- these integral components to the wiki's model are so decoupled that they can be used in many other contexts, and are completely independent libraries. Once those libraries were written and thoroughly tested, I wrote the (at the moment, rather underpowered) shell for the wiki using the servant and acid state libraries, and the whole thing worked _the first time I ran it_. Getting the core abstraction right is key. Once you do that, everything just falls out of it naturally.
Is there a way to write something along the lines of `(coerce v2) orElse (fmap fromIntegral v2)` so you can provide an alternative if the compiler determines the coerce will fail?
I expected something like a package dependency check for cabal but this isn't bad either.
[Possibly](https://www.reddit.com/r/haskell/comments/3jf2tq/using_rankntypes_and_constraintkinds_to_create_an/)
Very nice! Although I don't think the minor version is needed as ghc never adds extensions in a minor version. 
Red/green cell background colors would be good too.
Also similar to "growing" a Clojure vector is modifying one: http://clojure.org/transients https://www.reddit.com/r/haskell/comments/3gtbzx/what_are_haskellers_critiques_of_clojure/cu2fr0d?context=3
See also https://ghc.haskell.org/trac/ghc/wiki/LanguagePragmaHistory.
Wow. This is fantastic. We should probably cross-post this to the Haskell Wiki, since it's an invaluable resource for anyone who needs to make GHC code work across multiple versions. What would make this even neater is if certain extensions could be clicked on to obtain additional information. For example, one thing that commonly bites me is that even though `-XDeriveGeneric` was introduced in GHC 7.2, it couldn't derive `Generic1` until GHC 7.6.
Wait, DeriveAnyClass? WOAH!
This was already useful to me. We were trying to remember where `GHCForeignImportPrim` was introduced just this morning here at work.
Well why wouldn't you just want a list of strings to begin with.
Click the column header to sort. I may add some arrows to make that clear.
Yeah, the database states they are logfiles for individual tags, and then points to the path and filename (for each tag), so I'm assuming they have data of some sort. They're around 25-50MB each also, so there's something in there. I've so far failed to open them. None of the tools on the machine seem to like .HS files, and I'm picking through Haskell to see if I can't get them open. Text editor just shows gibberish. 
Do you have an example in mind of different conventions? I'd be really curious to see some others. &gt; Beyond that, a goal should certainly be "try to have as many modules as possible be constituted just of pure data structures and pure functions" and then "build a UI on top of this pure core." This is pretty much what I mean by "types modules". Whether you actually use the word "types" isn't really important. But the word does function as a nice signal to people of where in the codebase to find this pure core.
[HelpSet Files](https://docs.oracle.com/cd/E19253-01/819-0913/author/helpset.html) are a more likely candidate since you mention the legacy system uses an Oracle Database.
I seem to recall happstack using lazy IO. That demotivated me from using it way back when. What's the status on that front?
"just" sugar :-) data T = ... deriving (Generic,Hashable,FromJSON,ToJSON) is pretty sweet 
Why would exceptions be better than either? I thought exceptions were generally disliked, though I might be misunderstanding something.
&gt; Do you have an example in mind of different conventions? I'd be really curious to see some others. Good question! I sort of think "have a Types" module is the most widespread thing to be thought of as an actual "convention" -- but if you look at the module structure of something like XMonad it has a "core" and an "operations" module, or Agda has modules structured by ASTs and then by typechecking, etc. Or some data structures libs follow an "Internal" and "Interface" model, etc.
I guess I think it really is more a question of how granular your "Types" modules are, and how strict you are about how many functions on them (if any) are defined in those modules as opposed to other modules on top of them...
Are you on a mac? Try opening up the terminal and running `file path/to/mystery/file.hs`. You can also try `strings` to dump printable strings from a binary file.
the lazy backend will be replaced my a pipes based backend named hyperdrive over the next few months. Getting Happstack off lazy IO has been on my todo list for a very long time. Before warp and snap, there was hyena. We even had an experimental port -- but then tibbe stopped developing it :-/ That said .. the lazy IO stuff is hardly visible to the end user. I have never had an issue in 10 years that was due to lazy IO in Happstack. 
Every single time I used lazy IO outside the context of a short one-off script, I was bitten by its many aspects (timeouts/cancellation not actually working, exceptions in the wrong context, IO in the wrong thread, ...). After being burned so many times, I just don't touch lazy IO.
Most of the ideas are easily transferred; just use monads instead of unsafe effects.
I had great expectations for http://dev.stephendiehl.com/fun/, but after Stephen getting a job, everything has stopped. 
Had a similar issue recently trying to run haskell on AWS Lambda, which does not give you any control over the host OS. I ended up creating a docker image based off the official Amazon Linux AMI, building in that container (with -static and using stack) and then running ldd and copying the libs into my zip file. Then it was just a matter of running the executable with LD_LIBRARY_PATH and that did it. Thought I'd share.
ghc-ide-toolkit?
The [current users' guide documentation](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/deriving.html#derive-any-class) on `-XDeriveAnyClass` is, admittedly, pretty sparse right now. GHC 8.0 expands the documentation for it a bit—you can read it [here](http://git.haskell.org/ghc.git/blob/75492e7467ff962f2f2e29e5c8b2c588c94ae8a7:/docs/users_guide/glasgow_exts.rst#l4185), if you don't mind reading raw markup.
I try to avoid unnecessarily specifying implementation details in the library name. What if they later decide that a service or daemon is not the best approach?
Considering that it supposed to be editor agnostic and support such wide range of tools like vim, emacs, atom, Eclipse, Netbeans, web front ends and even VS written in so many different languages, i do not see how would they implement it other than the service. 
Totally agree that a name change is necessary. I stumbled into this project yelling something about GUIs only to be corrected. :) My vote is for `ghc-ide-backend` or `ghc-ide-services`. It isn't imaginative but I don't think it needs to be. Most users will hopefully interact with one of many snazzy front ends with equally snazzy names.
This is awesome! Did you collect this data by hand, OP?
ghc-ide-interface?
I had the exact same question. Couldn't it also be written to work with any MonadError?
it is not very helpful: writing a compiler in haskell, using modern libraries like uniplate and hoopl and quickcheck is a complex thing that would well deserve a book. Just saying take an ML book and use monad for side effect is like saying if you know how to write a compiler, just do it. If you know, you don't need the book, that's clear.
Clever. :)
This whole comment thread has great comedic timing.
Red/green colorblindness, as I understand, doesn't mean you can't distinguish any red from any green but certain ranges of the total visible colors. I copied that pair of colors from caniuse.com and using http://lab.leocardz.com/colorblinding/ I see enough contrast. :)
I think the argument is that exceptions are just as likely to occur whether or not you also have a particular class of blessed errors that you handle in the types, so you're really just being misleading.
A semi-follow-up tutorial he wrote explaining how to write a JIT compiler for a minimal imperative language using the Haskell LLVM bindings can be found here: http://www.stephendiehl.com/llvm/ I've been using this tutorial as a base for a compiler I'm implementing, and it's been a wonderful guide along the way. Building new Monads, structuring large programs in Haskell, and Monad Transformers are still new to me; his code provided a great basis for my understanding of how to implement the functionality that was most convenient with their use (e.g. State Monad for LLVM codegen state, how to use LLVM bindings, etc).
But writing a compiler in the classic style in haskell is basically the same as in ML, this is completely true.
&gt; Subsumption has the same covariant/contravariant behaviour as subtyping. Uh, no. If I have the open expression `a -&gt; a`, it is valid to instantiate that as `Int -&gt; Int` and we say that "`a -&gt; a` subsumes `Int -&gt; Int`". Both of the `a` instances go the same direction; we cannot say that `a -&gt; a` subsumes (nor is subsumed by) `Nat -&gt; Real`, nor can we say that `[a] -&gt; a` subsumes (or is subsumed by) `a -&gt; [a]`. At least, this is what the word "subsumption" means throughout the unification and logic programming community, where the term originated. We get co/contravariant behavior with higher-rank polymorphism because the issue of interest is a form of subtyping, not subsumption in the unification/logic sense. That Odersky and Läufer chose to coopt the word "subsumption" to mean this form of subtyping is most unfortunate and causes no end of confusion in the PL literature since it is no longer unambiguous what that word actually means.
By the way, the thinking around putting ghc in the name is that the project will be used to drive changes back in to GHC to better support tooling in general and IDE's in particular.
For multisig, what I would like to see is a zero knowledge rendezvous server that detects transactions to be signed, and the signatures can be compatible with any bip44 wallet, with the user being sure that he is signing the right thing and not being tricked into something. That was a little hand wavy there. I'll try to refine and be more precise if this conversation goes on.
any chance you can contribute / backport your lens enhancements to haskoin lib?
Great answer, thanks a lot!
This is nice, but I'd be even happier to see GHCJS and UHC in there to see what exactly I'm losing when I use something other than GHC.
Super useful! I would suggest adding links to the relevant documentation on the boxes. And on the extension name for the latest documentation.
This is really nice. However, since the information was gathered by an [automated script](https://www.reddit.com/r/haskell/comments/3qfkoa/ghcaniuse/cwf8rn5), it's sometimes not so usable in practice. For example, IIRC the `DataKinds` extension was very experimental and hardly usable in GHC 7.4, even though the GHC option for it was already available. If there were also: * A "partially supported" option, and * An easy way for a human with more knowledge than the script to click and add that information, this would be even more awesome. Thanks for this extremely helpful work! EDIT: One thing that would help would be to have `ghc --supported-languages` also report "experimental" or "partially supported" when appropriate. I was going to submit that as a feature request, but according to CloudFlare, GHC Trac seems to be having issues at the moment. :(
Might be slightly better to have data NotBuilder = NotBuilder { length :: !Word, tree :: MonoidTree ByteString } instance Monoid NotBuilder where mempty = NotBuilder 0 Empty mappend (NotBuilder la ta) (NotBuilder lb tb) = NotBuilder (la + lb) (mappend ta tb) Since growing/copying a `ByteString` for every `Append` is probably not very cheap.
It's not very much about exceptions, any exception-like mechanism (computation termination in case of error with error-propagation) might also do. It's just that in current scheme, you need to manually gather responses from each operation, return them (in something like a tuple), only then getting them in a form of either-values. As a result, most people just ignore errors.
Looks like you've missed the point. The main purpose of `MonoidTree` is already exactly to avoid "growing/copying a ByteString for every Append". From original post: &gt; this type abstracts over the Monoid operations allowing to either postpone the actual concatenation until the moment when all chunks are aggregated and we have access to info on how many bytes to allocate for the output bytestring I.e., before allocating for the final bytestring, we can traverse the tree, aggregating the sizes of the leaf bytestrings into the amount of bytes to allocate. Then we will traverse it again to fill the allocated bytes with the leaf bytestrings accordingly. That effectively eradicates the need for the `length` field or the calculation of it during the appending. It simply gets abstracted from and, in the use-case of the aggregation of a strict bytestring, gets postponed. However, in case of stream generation, eradicating the immediate length computation is also benefitial, since in that scenario it is not needed at all.
&gt;Also, since MonoidTree is a quite general abstraction, is something like that already provided by some existing package? Since `MonoidTree` only satisfies monoid laws when viewed through a concatenation function (otherwise `mappend` is not associative), I figure this could be confusing for potential users of such a package. 
Right -- my intuition is that traversing the tree twice, will be slightly slower than keeping the length as the tree is built up. Obviously only considering strict ByteString or Text here. I'll cook up a tiny microbenchmark once I've un-foobared my GHC installation :/. But this seems deep into micro-optimisation territory. edit: yeah, on my machine, this is a mean 1e-7 seconds faster: ~/code/bs-tree ./BSTRee benchmarking no length time 354.3 ns (353.8 ns .. 354.7 ns) 1.000 R² (1.000 R² .. 1.000 R²) mean 354.6 ns (353.8 ns .. 355.0 ns) std dev 1.748 ns (1.077 ns .. 2.992 ns) benchmarking length time 253.5 ns (249.1 ns .. 256.9 ns) 0.999 R² (0.998 R² .. 1.000 R²) mean 256.1 ns (253.3 ns .. 257.3 ns) std dev 5.468 ns (1.442 ns .. 9.747 ns) variance introduced by outliers: 28% (moderately inflated) Full code at http://lpaste.net/144094. Mind you, this isn't a very realistic scenario, having the trees written out statically like this. Hopefully using `nf` mitigates that. edit 2: whoops, I didn't `free` at the end of `fill`. This shouldn't skew results though.
But `(forall a. a -&gt; a) -&gt; (forall b. b -&gt; b)` subsumes `(forall a b. a -&gt; b) -&gt; (Int -&gt; Int)`. If you consider this not to be subsumption anymore then fair enough -- no point arguing about the definition of terminology. But in the Haskell world at least this kind of definition of subsumption is commonplace; for instance, see the Simon Peyton-Jones paper I cited earlier. 
I was referring to the point k-bx mentions. Currently, in Hedis, every Redis commands returns an `Either` value. This correctly represents "what Redis returns", but is inconvenient to use. Especially when return values from earlier commands are used as arguments in subsequent commands. Several people have complained about this and I now believe that it would be better (more convenient) to return "unwrapped" values for successful commands and use an "abort" mechanism such as exceptions or a MonadError or similar to cancel the entire chain of Redis commands when errors occur. hiptobecubic's point is also relevant, since we're dealing with network IO which can throw exceptions anyway.
There are a few things here: - `assoc (Sin x)` and `assoc (Cos x)` are both undefined in your definition of `assoc` - you can change `return (Num n)` to `return (Num (getNonNegative n))` to avoid generating negative numbers, thanks to one of the newtypes in Test.QuickCheck.Modifiers A few other things to think about: - do you really want `frequency` to be tied to the size of the generator in that way? If not, you could replace it with a couple of calls to `oneof`, depending on the value of `s` - you don't need to use `arbExpr` recursively in the `Var` case, as it's not being used - you may not need the `Var` case in `assoc`, since the final case covers that Edit: unless you really want to skew the frequencies, this might be useful: arbExpr :: Int -&gt; Gen Expr arbExpr 0 = oneof leaves arbExpr s = oneof (leaves ++ branches) where s' = s `div` 2 leaves = [ return Var , (Num . getNonNegative) &lt;$&gt; arbitrary ] branches = [ Add &lt;$&gt; arbExpr s' &lt;*&gt; arbExpr s' , Mul &lt;$&gt; arbExpr s' &lt;*&gt; arbExpr s' , Sin &lt;$&gt; arbExpr s' , Cos &lt;$&gt; arbExpr s' ] 
Probably the most efficient way to build relatively short strict bytestring it to to allocate a buffer and fill it in `ST`, like [that](http://lpaste.net/6988611848786411520) (not actually tested.) I'm sure it can be abstracted into a library that hides all the low level details. If you don't know the resulting size upfront, then just grow the buffer by a constant factor each time it is not big enough.
Could someone explain the difference between the array package and vector package?
Do you mean a server not under the control of any of the signing entities?
awesome, thanks, that fixed the negative numbers stuff. should mention that Im a really rook when it comes to programming in haskell but thanks for the tips!
You might be interested in this implementation: https://gist.github.com/pchiusano/f602ca777d02e2b45738
That's not newtype wrapped in any case. For existentials we need `data`, and existential class constraints take up extra non-erasable runtime fields, so it's not something we can elide. What I meant was universally quantified types in wrappers. 
Ah, that's not as cool as I thought. Thanks for the explanation!
That's what `Builder` already does!
Just checked the sources. Strictly speaking, it is a bit different. On buffer overflow `Builder` produces a chunk instead of resizing the buffer. But otherwise -- yes, it works like I described. Thank you for pointing me to that!
Thanks for the `AllocationStrategy` tip! I'll check it out. But it still feels like complicating things, which could be dealt with easier. As for the following: &gt; This is just deferring the actual problem for how to efficiently build the ByteString. You haven't done any of the real work yet because you haven't yet built the final ByteString You're missing the point. See [my response to a similar comment in another thread](https://www.reddit.com/r/haskell/comments/3qj53a/an_alternative_bytestring_builder/cwfoh63).
Elm and Purescript are compiled at a much higher level -- they don't have an assembly backend. You will have none of the more low-level issues discussed in most compiler books (register allocation, instruction selection, SSA forms, whatever), and much more of the high-level stuff that really depends on the language you are compiling (possibly some type system knowledge, a good grasp of the dynamic semantics, etc.). My recommendation would be to start hacking the compiler right away and learn on the spot. Find an issue that looks easy enough for a beginner to do (or asks the maintainers for one), jump in the part of the code that needs to be changed, change it, and in passing have a look around to get a mental model of what this part looks like. If you want to participate in discussions of the language *evolution*, you may be interested in books on programming language design -- in particular on type systems for purescript. Things like [Types and Programming Languages](https://www.cis.upenn.edu/~bcpierce/tapl/) or [Practical Foundations for Programming Languages](https://www.cs.cmu.edu/~rwh/plbook/book.pdf) or [Using, Understanding, and Unraveling the OCaml Language -- from practice to theory and vice versa](http://caml.inria.fr/pub/docs/u3-ocaml/) rather than compilation books.
&gt; That all leaves me wondering, why the authors of "bytestring" implemented Builder the way they did and what the downsides of what I am considering here might be. The issue is the `Builder` was designed to support streaming lazy bytestrings of possibly-infinite size. What it sounds like you want (and what I have wanted in the past) is a `Builder` for strict bytestrings; you build up a list of values and their serialized lengths, then when it's time to "run" it you allocate an array of the exact proper size, then write your values to it. It probably doesn't much matter how you represent `Builder` but I suspect a `DList` or if necessary a `Sequence`-like thing would be generally better than your `MonoidTree`.
Thanks. spj's books are definitely on my list :)
As far as native compilation book go, I like Appel's "Modern Compiler implementation" book. It is less advanced than the monsters of the field, but it gets to the point and lets you build a full compiler (with nothing hidden) in reasonable time. Virtual machines and JITs are another matter, though, and I'm not knowledgeable enough to know what to recommend.
&gt; `Sequence` turns out to be based on quite a similar thing, __but burdened with extra structure-specific data, which in this case we don't need.__ [emphasis mine] Can you elaborate?
If you don't materialize the tree into the traditional `ByteString` data structure then the time complexity of operations change. Now `length` is no longer `O(1)` and `head` is `O(log N)`, for example.
Is there a way to visualize (or even just dump) the module dependency graph of a package? Seems like a very nice addition to Hackage?
The `import safe` syntax is now part of the `Safe`/`Unsafe`/`Trustworthy` extensions and automatically enabled with any of them. (And it was implied by them at the time too. Since it was only useful if you're using Safe Haskell, having it be a separate extension as well was admittedly pretty pointless.)
I like to view programs as just API's that you feed stuff into and it pukes out a response. Then, you have to decide *how* you feed stuff to it and *how* you, umm,... use the puke. For instance, you could have a simple text-based CLI. Or you could hook it up to a REST API and serve up JSON, opening up the ability for web-based UIs. Or you could hook it up with protocol buffers or whatever language-agnostic encoding you want and use it as a data source for another service or application. Perhaps you can write a small projech that uses Servant to expose an API for something you'd like to have. How about... A service that you send MarkDown to, and it renders it with Pandoc, returning the HTML? Then try and deploy it on DigitalOcean or something. Or dockerize it first. Try and take a project through the entire "plan, build, run" cycle.
I'm not speaking of the GC-specific pointers (to the bounds of the heap etc.) but of live roots (pointers to the heap) that would be passed in registers. Are you saying that they are all spilled to the stack (or a variable somewhere) before the GC call? Is LLVM taking charge of this?
Yeah. But in the context of building bytestrings I could care less about those operations.
I've started [a benchmark project on GitHub](https://github.com/nikita-volkov/bytestring-builders-benchmark). Can you please PR with your corrections?
I share the same concerns as /u/Yuras regarding the pub/sub semantics. In fact I have been planing to suggest some changes to the API to fix that but haven't gotten around to it yet. Unless someone gets to work on it before, I'll be doing that soon. 
Thanks for the answer :)
I agree. It's unfortunate that this is the case, since there's no reason that `DeriveAnyClass`, `GeneralizedNewtypeDeriving`, and the other `Deriving` extensions can't play along, but we'll probably need to make things more configurable for this to work well. If you want to chime in on this, please drop by https://ghc.haskell.org/trac/ghc/ticket/10598 and leave a comment.
GHC needed a ton of support from LLVM. Adding the calling convention there and the ability to generate "tables next to code" was definitely non-trivial. It just doesn't use any of the LLVM gc hook machinery.
Duncan's work on making faster serialization also seems relevant. http://code.haskell.org/~duncan/binary-experiment/binary.pdf There he uses an explicit "Token" chain with append nodes, which follows the general spirit of the BinaryTree builder here but allows a bit more specialization: https://github.com/well-typed/binary-serialise-cbor/blob/master/Data/Binary/Serialise/CBOR/Encoding.hs#L21
Maybe look at http://hackage.haskell.org/package/buffer-builder
Wow, you really took the feedback you got in a perfectly constructive way. Kudos for that on top of a cool program which I shall now use to quantify my shame! Edit: My shame is apparently almost entirely confined to functions called `go` or `aux` where I do a bunch of pattern matches. `argon` itself built easily, and is very quick!
ghc-ide-bikeshed! In all seriousness, I'm glad you guys decided a name change would be good at this stage. ghc-ide-service makes a lot of sense, though I'd say any of the ghc-ide-* names are better than haskell-ide.
Great job!
Might be a good idea; I'll put it in my backlog
it contradicts GeneralizedNewtypeDeriving (does the user want the generic/default/builtin instance, or the base type's existing instance?). but you can use both extensions when deriving instances for a data (not a newtype). still pretty useful. 
There is already ide-backend, this name will cause confusion
Thanks! I'll look into it and see what I can do. EDIT: I have to say that that snippet of code looks very weird to me! I didn't even know it was possible to write such code. One learns new things every day!
Thank you! Yeah pattern matching is one of the most responsible for increasing complexity. It's still certainly better than a bunch of ifs that you see more frequently in imperative languages, though. BTW I think that Argon speed can be improved, I just have to figure out how! ;)
it: * uses the GHC API * has machine-readable output * exposes its modules * works like every Haskell dev tool should... thanks!! one feature request: qualify methods (like `==`) with the type name (like `== (MyConfig)`) 
Thanks for doing this! I've been trying to wrap my head around the GHC API and projects like this and `ghc-footprint` really help. Consider joining the [haskell-ide](https://github.com/haskell/haskell-ide) project. Its goals seem aligned with your interests.
yes, zero knowledge: these things would run "in the cloud" and function as data stores, but the data itself would be hidden from the service. Like tarsnap, spideroak, some others. https://github.com/haskoin/haskoin-wallet/issues/124 gives a flavor of the direction. 
It's not just length and null, there's `elem`, `sum`, `product`, `maximum`, `minimum` and `and` and `or` that behave counterintuitively. 
doesn't ghcjs support every extension? at least, unless there are extensions that switch things up at the level of Cmm/assembly.
I'm not aware of a comparable web IDE for Haskell. I found atom OK to set up on my Mac. Vim and emacs are the most mature IDEs but require setup. There's a lot of active work in the IDE area now.
See also the [WSJ article] (http://blogs.wsj.com/moneybeat/2015/10/28/bitbeat-microsoft-to-offer-ethereum-based-services-on-azure/) and the [press release] (http://blockapps.net/pdfs/blockapps-strato-microsoft-partnership.pdf) 
http://hackage.haskell.org/package/monoid-subclasses/docs/Data-Monoid-Instances-Concat.html The definition is actually a wrapper of Seq, but I've recently decided to switch to a binary tree much like you suggest. 
I was only addressing the issue of reasonably easy to commit mistakes that do not lead to type errors anymore, which IMO is the most compelling motivation behind the OP's proposal.
argon: argon: panic! (the 'impossible' happened) (GHC version 7.8.4 for x86_64-unknown-linux): mkHsTyWithBndrs:kvs This is caused by code using ScopedTypeVariables, specifically some typical catches code: Handler (\ (e :: SomeException) -&gt; f e)
http://stackoverflow.com/questions/3431225/tools-for-generating-haskell-function-dependency-control-flow-graph
I've added the "buffer-builder" subject. It seems to perform quite bad in building small bytestring, worse than even the standard `Builder`, but it comes to shine on larger inputs. As for "fast-builder", I simply couldn't make it compile.
It all depends on the size. I've extended the suite to cover larger inputs, and results are quite different there. Your implementation however still performs quite bad there as well.
I've added it to [the benchmark](https://github.com/nikita-volkov/bytestring-builders-benchmark). Roughly speaking, it's bad for small inputs and good for large ones.
Yes, but then each solution is about some form of a compromise. I just want to find the optimal compromise. So far, BinaryTree seems to perform better than Seq and DList regardless of the input size. It performs worse than other subjects depending on the size of the input though. Would be nice if you took a look at the subjects considering an optimization or adding a subject of a data-structure, which you propose.
In this benchmark I'm testing only the concatenation of strict bytestring chunks, so specialization for other types, shouldn't help here much. However it still would be informative to see how it performs in comparison to the competition.
Exactly, I reached the same conclusion. There is probably a bug in how CPP is processed.
Oh, this shouldn't have happened. For starters, it happens only with GHC 7.8, because they still used undefined values in the GHC API. I actually think this is a bug in the ghc-syb-utils library, which should exactly avoid this. Can you show me a snippet of code that's causing this?
Ah, makes sense. Thanks for the feedback!
The difference list seems to be suffering as expected. You need to concatenate, which needs the total length, but the only way to get that is to get the list of bytestrings out at the end, and run through it twice. Seq on the other hand wastes a ton of time in balancing something you don't need balanced.
The whole discussion seems to focus on a use case I can't quite understand, one where it makes sense to allow derivation of empty instances to take precedence over the more specialized extensions. There is also talk about allowing `DeriveAnyClass` to derive empty instances even when the class author specified an explicit `MINIMAL` pragma. I am not quite sure if I understand its intent at all. I always assumed it was just an extension to save you from typing lines like instance Foo Bar and nothing more but for this the whole design seems much too complicated so am I missing something here?
Holy crap, this is awesome!
I think that for example `maximum (10,3)`, or `elem 6 (6,7)`, or `or (True, False)` are as easy to accidentally indirectly do as `length (4,6)`.
Huzzah! This was #2 on my "eagerly awaiting" list, right after GHC 7.10.3. (Yea, though I walk through the valley of the shadow of Windows...) Not at all disappointed to see it skip the queue. :D
At the moment, you can find annotated slides from a presentation and a series of simple examples in the [documentation section][1]. Unfortunately, there is currently no tutorial targeted specifically at beginners. At some point, I intend to write one, but that's still way off. [1]: https://wiki.haskell.org/Reactive-banana#documentation
Congrats! Random question, but I really want to know: Did the name reactive-banana originate from one of those weird Github suggestions?
I am not aware of anything comparable either, which is why I want to put together a group to carry the project forward. There are many improvements which need to be made but we have to be able to show FPComplete that we are capable to supporting the IDE with only minimal input from them. So far Ive heard from /u/agocorona and maybe one other interested in this. Please let me know if you (or anyone else) is interested. 
&gt; Random question, but I really want to know: Did the name reactive-banana originate from one of those weird Github suggestions? It's pretty obvious. If you look at the periodic fruit table you will see that bananas are the most reactive fruits. 
I'll just leave this here: [gif](http://i.imgur.com/SYj8L.gif)
I was unable to compile it with GHC 7.10.2 on Mac OS 10.11.1. I recommend you to test the compatibility using Travis CI. Concerning your results, an interesting observation is that it performs on par with BinaryTree on both input sizes.
 {-# LANGUAGE ScopedTypeVariables #-} module Utility.Exception where import Control.Monad.Catch as X hiding (Handler) import qualified Control.Monad.Catch as M catchNonAsync :: MonadCatch m =&gt; m a -&gt; (SomeException -&gt; m a) -&gt; m a catchNonAsync a onerr = a `catches` [ M.Handler (\ (e :: SomeException) -&gt; onerr e) ] 
I'd say that the code for `minimumBy` needs to be modified to check for a `null` `Foldable` and throw it's own error. 
This has nothing to do with FTP, really. They just unnecessarily removed a case in the definition of `minimumBy`. I don't see why it couldn't go back. * http://hackage.haskell.org/package/base-4.8.1.0/docs/src/Data.Foldable.html#minimumBy * http://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-List.html#minimumBy
Yep. I'd call that a bug/regression. 
I love FRP and have spent some time with Elm. I have tried to get into Reactive Banana but I couldn't find almost any documentation for getting started. That's the only thing holding me (and I would also guess others) from using it. Is it hidden somewhere?
I found FRP to quite versatile, what exactly are you looking for? 
can I grab you here to ask what your plans are with Francium?
Note: the act of doing so isn't without a cost. There are `Foldable`s for which each pass can be quite expensive to start up, and the `null` might require an entire pass. Another answer is to not use `foldr1` and instead model `foldr1`: foldr1WithCustomError e f xs = fromMaybe (error e) (foldr mf Nothing xs) where mf x m = Just (case m of Nothing -&gt; x Just y -&gt; f x y) This now works in one pass.
It has `null`, but doing a separate `null` check may require a complete scan of the data structure, depending on how it is implemented.
Let me see if I can explain why `-XDeriveAnyClass` is useful, and why you might want to use that over other `-XDeriving` extensions (you might not have asked this specific question, but hopefully this will address your concerns). As /u/dukerutledge noted, when you derive a class with `-XDeriveAnyClass` enabled [1], the generated code is simply an empty instance. This is useful for more than just classes with no methods—it can be combined with `-XDefaultSignatures` to great effect. For example, let's say you wanted to roll your own `Eq` class. We'll call it `GEq` [2] to distinguish it from `Eq` in `Prelude`: class GEq a where geq :: a -&gt; a -&gt; Bool {-# MINIMAL geq #-} Like `Eq`, we want to be able to "derive" a `GEq` instance automatically. To do this, we will leverage `GHC.Generics` and create a default implementation of `geq`: {-# LANGUAGE DefaultSignatures, FlexibleContexts, TypeOperators #-} import GHC.Generics class GEq' f where geq' :: f a -&gt; f a -&gt; Bool instance GEq' U1 where geq' _ _ = True instance (GEq c) =&gt; GEq' (K1 i c) where geq' (K1 a) (K1 b) = geq a b instance (GEq' a) =&gt; GEq' (M1 i c a) where geq' (M1 a) (M1 b) = geq' a b instance (GEq' a, GEq' b) =&gt; GEq' (a :+: b) where geq' (L1 a) (L1 b) = geq' a b geq' (R1 a) (R1 b) = geq' a b geq' _ _ = False instance (GEq' a, GEq' b) =&gt; GEq' (a :*: b) where geq' (a1 :*: b1) (a2 :*: b2) = geq' a1 a2 &amp;&amp; geq' b1 b2 class GEq a where geq :: a -&gt; a -&gt; Bool default geq :: (Generic a, GEq' (Rep a)) =&gt; a -&gt; a -&gt; Bool geq x y = geq' (from x) (from y) Now we can generically check for equality on data types which are instances of `Generic`: {-# LANGUAGE DeriveGeneric #-} data Example = A | B deriving Generic instance GEq Example &gt; geq A A True &gt; geq A B False Or equivalently, with `DeriveAnyClass`: {-# LANGUAGE DeriveAnyClass, DeriveGeneric #-} data Example = A | B deriving (Generic, GEq) This shows why `DeriveAnyClass` can be useful. But you also asked about why it would be make sense to favor `DeriveAnyClass` over other `Deriving` extensions. For example, let's say you had a generic `Show` class called `GShow` [3]: {-# LANGUAGE DeriveAnyClass, GeneralizedNewtypeDeriving #-} newtype WrappedInt = WrapInt Int deriving (Generic, GShow) An important question arises: should a `GShow WrappedInt` instance be derived using `DeriveAnyClass` or `GeneralizedNewtypeDeriving`? Since it's a `newtype`, you might be tempted to use `GeneralizedNewtypeDeriving`, but this leads to surprising behavior: &gt; gshow (WrapInt 1) "1" Arguably, the output should be `"WrapInt 1"`, but since we used `GeneralizedNewtypeDeriving`, it simply uses the `GShow Int` instance behavior, ignoring the `WrapInt` constructor. If we had used `DeriveAnyClass`, we would have obtained the desired output. That's where Trac [#10598](https://ghc.haskell.org/trac/ghc/ticket/10598) comes into play: it's quite easy to make GHC confused if you have too many `Deriving` extensions enabled in the same module. We certainly need to come up with a better scheme so that certain extensions take precedence over others, but moreover, we need a way to tell GHC to use a specific `deriving` strategy in the presence of multiple `Deriving` extensions. In the Trac link, Richard Eisenberg [proposed](https://ghc.haskell.org/trac/ghc/ticket/10598#comment:3) this: newtype MyMaybe a = Mk (Maybe a) deriving( {-# GND #-} Functor , {-# BuiltIn #-} Show , {-# GND #-} Read , {-# Any #-} FromJSON ) You also had a question about `MINIMAL` pragmas. That comes into play since if you have something like this: class GEq a where geq :: a -&gt; a -&gt; Bool default geq :: (Generic a, GEq' (Rep a)) =&gt; a -&gt; a -&gt; Bool geq x y = geq' (from x) (from y) {-# MINIMAL geq #-} gneq :: a -&gt; a -&gt; Bool gneq x y = not (geq x y) data Example = A | B deriving (GEq, Generic) Then GHC 7.10 will complain: GEq.hs:34:32: Warning: No explicit implementation for `geq' In the instance declaration for `GEq Example' But should it? Simon Peyton-Jones [argues](https://ghc.haskell.org/trac/ghc/ticket/10598#comment:11) that since `DeriveAnyClass`/`DefaultSignatures` is "filling in" a method implementation, it should count towards a `MINIMAL` definition. On the other hand, Andres Löh [notes](https://ghc.haskell.org/trac/ghc/ticket/10598#comment:10) that `DeriveAnyClass` can generate empty definitions for methods that don't have default definitions, so if those methods are in the `MINIMAL` set, perhaps they *should* generate a warning. Hopefully that explains some of the design issues that are being debated here. The issue is far from decided right now, so if you have strong opinions about this kind of thing, I urge you to comment on the Trac page. ----- [1] Provided the class isn't one of a handful of "privileged" classes like `Eq`, `Ord`, `Read`, `Show`, etc. [2] `GEq` can also be found in the [`Generics.Deriving.Eq`](http://hackage.haskell.org/package/generic-deriving-1.8.0/docs/Generics-Deriving-Eq.html) module of the [`generic-deriving`](http://hackage.haskell.org/package/generic-deriving) library, if want it. [3] http://hackage.haskell.org/package/generic-deriving-1.8.0/docs/Generics-Deriving-Show.html
Where I can look at it, the website is hard to navigate?
I wrote about the question in a bit more detail on SO: https://stackoverflow.com/questions/33418831/mutable-data-across-the-ffi-and-laziness 
Things like your formula could stand to have ! annotations throughout: data Formula a = FF | TT | Atom a | Not !(Formula a) | And !(Formula a) !(Formula a) | Or !(Formula a) !(Formula a) | Imp !(Formula a) !(Formula a) | Iff !(Formula a) !(Formula a) | Forall String !(Formula a) | Exists String !(Formula a) Generally when porting code from a strict language they won't have any infinite cases lying around, so ! annotations like this are fine.
I guess we should get that patched then :-)
Maybe it would be useful to think about deprecating or somehow dealing with the dangerous partial functions available in Prelude. All of the teams I have worked with have had a complete ban on head, tail, fromJust, minimum, maximum, etc., and it has not caused us any trouble. Also, I also strongly recommend, and see others recommend, that beginners not use head/tail and instead use case analysis, for example. In my experience, banning these functions has been easy, and has greatly reduced the amount of debugging we need to do. We do still sometimes write partial functions, but we always use failing pattern matches, partial case statements, [$notImplemented](https://hackage.haskell.org/package/placeholders-0.1/docs/Development-Placeholders.html#v:notImplemented), and other forms that end up putting a source location in the error message. For things like minimum and maximum, we use [NonEmpty lists](https://hackage.haskell.org/package/semigroups-0.17.0.1/docs/Data-List-NonEmpty.html) - you can always take a normal list and do `minimum . (0 :|)` if you need to. I'm not sure how one would go about moving the standard libraries away from these dangerous functions, or whether it's even possible. Also, perhaps people have legitimate use cases that I'm overlooking, or workflows that eliminate the problems I've encountered. It would be great to hear about those. However, one way or the other, I think it would be very helpful for beginners and professionals alike to make progress on this issue.
why no `!` on `a` in the `Atom` constructor? And what about the `String` fields?
Yep. Note: Avoiding situations like this is a large part of why `Foldable` grew so much in 7.10. Otherwise `sum` and a bunch of other combinators would have had similar issues with random flips of direction and the like.
My approach is to have a library that exports conflicting versions of partial functions: module Utility.PartialPrelude where read :: Read a =&gt; String -&gt; a read = Prelude.read *Utility.PartialPrelude&gt; read "" :: Int &lt;interactive&gt;:2:1: Ambiguous occurrence ‘read’ It could refer to either ‘Utility.PartialPrelude.read’, or ‘Prelude.read’ Then if you really want to use it, you can Prelude.read, and such uses are also easy to grep for later when you were wrong about using it and your program blows up. I wonder if it would make sense to add this to http://hackage.haskell.org/package/safe ?
You can get a stack trace by compiling with `-prof -auto-all` and running your code with `+RTS -xc`. Then you might get superfluous exception dumps (for internally caught exceptions). So the best I know of is this: https://www.reddit.com/r/haskelltil/comments/3p9x4e/better_stack_trace_on_failure/
When will documentation be available? EDIT: documentation is on hackage
And we [kinda](https://www.reddit.com/r/haskelltil/comments/3p9x4e/better_stack_trace_on_failure/) have them! But annoyingly we need to rebuild with profiling to get them.
He actually doesn't say that the semantics are essentially the same; only that the APIs are. (I don't know enough to comment about whether the semantics are different in any important ways, though.)
What type of data structure would have a costly `null` check?
Yes, let's please do this.
I'm a little bit confused by this, the whole point of bang patterns is that you avoid growing large unevaluated expressions, right? However here you start at the root of the tree and walk down, wouldn't these strictness annotations just force the entire tree and consume more memory than the lazy case?
Sorry, it was actually the author of Sodium who wrote that (linked from the post): &gt; Recent changes in Sodium and Reactive Banana have meant they are now semantically identical.
Sitting on it now.
I generally don't use List. I encounter several times that List drops the performance. List has unexpected behaviors for who don't know how to profile Haskell. Try this instead: [containers : Data.Sequence](https://hackage.haskell.org/package/containers-0.5.6.3/docs/Data-Sequence.html) You can use language extension **OverloadedLists** so that every List syntax can be kept on Sequence without numerous modifications. You may also want to use language extension **OverloadedStrings** and change String to Text because String is just an alias of [Char].
200 seconds is a big variance. Do you know what caused that?
Maybe I'm too tired, but what about that implementation is expensive?
Also see https://ghc.haskell.org/trac/ghc/ticket/10830 and https://ghc.haskell.org/trac/ghc/ticket/3416
It is not as expensive as it looks, due to lazy evaluation: For a `foldr` that behaves as intended, this will return `True` or `False` as soon as any element is found. But yes, some data types might still want to implement `null` more directly.
I took a look at the code and came to the same conclusion. The Haskel version is using `(++)` a lot.
Great, thanks! I shall try that with my script then!
Examples can be found [here][1], but Hackage hasn't built the API docs yet, I'm afraid. [1]: https://wiki.haskell.org/Reactive-banana/Examples
I haven't found the time to write a beginner's tutorial yet. [A StackOverflow answer][1] collects all information that I can offer for the moment. [1]: http://stackoverflow.com/a/9215444/403805
The API of reactive-banana, sodium and even Reflex are very similar, the underlying concepts are essentially the same. FRPNow is a little different, in that Events in FRPNow are a one-time occurrence. This is closer to the original paper by Hudak and Elliott, and a somewhat more "primitive" building block (not lower level, just more "fundamental"). Unfortunately, it seems to preclude some optimizations, like a push-driven implementation.
This got me thinking. Foldable and Monoid are kinda tied together. Would it be possible to add Semigroup into the mix (pretending it's in base) so that functions like foldr1 or foldl1 can reject non-empty data structures at the type level during compilation? Would it require a separate NonEmptyFoldable?
I feel like techniques for doing things like this are mostly useful when debugging, not really for keeping them around in your code. Here are some other approaches to contrast with yours. In case anyone wanders in without understanding the context, the intent is that all of these examples fail to compile because we tried to take the length of the wrong type of thing. -- with safe-length, first example on README file main = print $ safeLength (Proxy :: Proxy [Char]) ('a', 'b') -- plain old type annotation with normal length function. -- notice that this is actually shorter that the safe-length solution main = print $ (length :: [Char] -&gt; Int) ('a', 'b') -- With ExplicitTypeApplication, this should be possible -- in GHC 8.0. This is even shorter main = print $ (length @[Char]) ('a', 'b') -- And if PartialTypeSignatures lands in 8.0, then we could -- also get this slightly shorter variant that differs from the others -- in that it accepts polymorphic lists and not just lists of Char main = print $ (length @[_]) ('a', 'b') In conclusion, I don't think that this package offers any improvements over the normal way of doing this.
The function being trivial and quite unpractical in practice, I assume this package is more a joke than anything serious, but I find it funny. Is this a response to those complaining about the Burning Bridge Proposal allowing this ?
World domination, of course. More realistically, I would like to push it forwards - and all the pieces are in place now. I just need life to settle down a bit so I can find time to focus on it. It's a mess of ideas at the moment, and I need to form something cohesive out of them.
This is a joke response to Erik Meijer's twitter trolling https://twitter.com/headinthebox/status/655722027685400576
Exactly the same thing happened to me. I felt a bit stupid for not realizing `maximumBy` isn't total, but that error gave me absolutely no idea where to look.
What a way to start! Will take a close look in the upcoming days -- studying manifold learning algorithms was on my todo so this is a perfect occasion.
I will PR it in a moment to make it easier for people to build for ~~7.8 and~~ (uses hmatrix 0.17 only stuff, and that isn't 7.8 compat) 7.10. (I am bitemyapp) [Dunzokopf](https://github.com/emmanueldenloye/manifoldRNC/pull/1)
Why the heck is that printing 3 type errors?
don't partial type signatures already exist?
Hard to encapsulate and hide implementation details when everything's exposed.
Wow, it looks like we've already got them in 7.10. I guess I never noticed. Thanks for pointing that out.
The "not 1, not 2, but 3 type errors!" bit got a full belly laugh out of me. Thank you. If you changed it from `Proxy (f a)` to `Proxy f`, I think people would actually like to use it. Relatedly, the imminent explicit type application would nicen it up a bit.
I have converted all lists to Sequences or Sets - still slow. Feel free to have another look.
There is an antipattern that looks like : if M.member x env then unify env ((env M.! x,t) &lt;| oth) else ... This performs 2 lookups. This probably won't matter much performance-wise. Also you can ditch the whole `Failing` part, it's `Either String`.
A slowdown of 400x can only be explained by algorithmic differences. No amount of laziness is going to give you that much relative to OCaml.
That wouldn't be a supported use: &gt; While other people are welcome to use this tool, it is not supported. 
Ahh yes. It would certainly be very interesting to somewhat "recurse" and compute the complexity behind each function used in a block of code. However, that would not be guaranteed to finish in a finite time. I just implemented the definition of cyclomatic complexity, which is the number of linearly independent paths through the code. But taking your example, I'd argue that if you do abstract the complexity in a single function, and use it in others, the complexity *is* reduced. Not only from the definition, but also from the perspective of who reads the code. Obviously one should find a balance and be careful not to take this to extreme, as you rightfully noted.
yeah. I think length @[x] would work anyway, but not sure.
Hmm... is that really the biggest remaining difference? Quite a remarkable convergence if so :)
Are you sure you aren't accidentally calling a function exponentially many times? Disabling the monomorphism restriction could cause this silently. How does the running time scale with the input? Does it scale in a different way than the OCaml program?
Unfortunately the case pattern in Haskell is very versatile. Taking your first example, in Python I would surely use a dictionary. In Haskell, though, I'm afraid that case/of is a natural choice, and it follows directly from the definition of cyclomatic complexity that in this case CC is very high. It is defined as the number of linearly independent paths through the code, and as you can see the definition applies perfectly here. So while you are totally right in arguing that those pieces of code are completely natural and easy to understand, it's also very difficult to distinguish cases like these from other examples in which one would want the complexity to show up high. If we tried to customize the cyclomatic complexity it could skew valid numbers into lower ones. And it would no longer be "true" cyclomatic complexity anyway. I'm open to suggestions, though.
Thanks, I'll look into it. GHC 7.8 is giving me headaches. The above code is analyzed without problems with GHC 7.10.
As far as I can tell the haskell performance is proportional to the ocaml performance for all inputs. It is somewhat difficult to assess, as the input is a theorem, and finding theorems that can be proved in, say 30 seconds rather than 600 is not straightforward. The Monomorphism restriction is not disabled.
How does Servant compare to Rest library by Silk? I am about to embark on a project of writing a somehow simple CRUD-like web app and both of these frameworks are interesting (if I go with a REST-only server with frontend in JS).
There are examples on the wiki. 
hmm, like a difference list?
could you elaborate on that? 
maximumBy's partiality is stupid, not us :-)
I'm not familiar with the other code complexity measures that have been studied, but naively: could it look at least one level deeper in such cases, measure the complexity, or even just the number of lines, in each sub-clause and modify the overall measure based on that ? And actually, would simple line counts be worth taking into account, or reporting separately ? For this code base I get ~2500 ratings, of which 3/4 are A(1) and only 50 are B or C, and I suspect line counts might be at least as useful in practical terms. 
I just tried and had much better performance with Seq ... will publish in a minute.
Do you mean? safeLength' :: (Foldable f) =&gt; Proxy f -&gt; f a -&gt; Int safeLength' _ f = length f Unfortunately that is less safe as it allows us to take the length of the wrong list. Perhaps we want the length of the inner list but we write: *Safe.Length&gt; safeLength' (Proxy :: Proxy []) [[1,2,3]] 1 with `safeLength` we would have: safeLength (Proxy :: Proxy [Int]) [[1,2,3]] &lt;interactive&gt;:6:36: Couldn't match expected type ‘Int’ with actual type ‘[Integer]’ In the expression: [1, 2, 3] In the second argument of ‘safeLength’, namely ‘[[1, 2, 3]]’ In the expression: safeLength (Proxy :: Proxy [Int]) [[1, 2, 3]] 
Congrats! I've played with this for a while and I really like it. I wish the docs weren't on the wiki though. I'm not sure which version of RB the various pages refer to, nor am I comfortable editing anything. Would it be hard to move the docs into a Tutorial module like Tekmo does?
Exactly.
Turns out it clashes with all your recent chances, so much for the PR :) Will try again another day ! Current status is at https://github.com/bartavelle/slow-atp . I added a few SCC annotations too, these are helpful when investigating such problems.
Yes, but strict language and lazy languages have very different optimal usage patterns for lists.
indeed, ocaml's `string` is array based and should be translated to `Text`
What would be nice is if we could tell `deriving Show` to use `GeneralizedNewtypeDeriving` instead of the default `Show` algorithm, which always injects data constructors and record syntax into the output. If we could, we'd get something like this: newtype Fix f = Fix { unFix :: f (Fix f) } deriving instance {-# GND #-} Show (f (Fix f)) =&gt; Show (Fix f) which would generate instance Show (f (Fix f)) =&gt; Show (Fix f) where showsPrec = coerce (showsPrec :: Int -&gt; f (Fix f) -&gt; ShowS) :: Int -&gt; Fix f -&gt; ShowS which would behave identically to `showsPrec p (Fix x) = showsPrec p x`. Sadly, that won't be possible until Trac [#10598](https://ghc.haskell.org/trac/ghc/ticket/10598) gets resolved.
The thing that jumped out at me was the fact that many functions like `subst` and `simplify` create entirely new copies of all the internal nodes of expressions even when they make no changes. Lines like `subst dict (And x y) = And (subst dict x) (subst dict y)` are obviously correct and straightforward, but probably not as performant as they could be.
I absolutely agree but repeated list concatenation is quite terrible in a strict language too.
 if [ "$?" -eq "0" ]; then docdir=$dist/doc/html cd $docdir doc=$1-$2-docs echo -e "Compressing documentation from \033[1;34m$docdir\033[0m for \033[1;35m$1\033[0m-\033[1;33m$2\033[1;30m" cp -r $1 $doc I'm happy I don't have to write many bash scripts
/u/tekmo describes something like this for pipes: [Model-view-controller, Haskell-style](http://www.haskellforall.com/2014/04/model-view-controller-haskell-style.html)
Lets consider something like: simplify (And FF _) = FF simplify (And _ FF) = FF simplify (And TT y) = y simplify (And x TT) = x simplify (And x y) = And (simplify x) (simplify y) ... Basically without those ! patterns there it is very easy to get to the point where you have `And (simplify something enormous) (simplify something else enormous)` lying around as a result of lazily stopping afer the first step here because we didn't demand the answers to be fully computed. If you start doing many calls to `simplify` these may build up rather than push through the tree.
&gt; a tuple shouldn't be `Foldable`. Glad to see this argument seeing light, I've been [wondering about it](https://www.reddit.com/r/haskell/comments/3qcg2d//cwf76bb). I'm not convinced it *shouldn't* be, because the people who made that decision have far more expertise on the subject than myself; but I'd really like to hear the justification, because it seems to be the root of these issues.
Thank goodness!
Oh snap, how did I miss this? Was it not public/publicized?
rotfl
On the bright side, there's [Bifunctor instances](http://hackage.haskell.org/package/bifunctors/docs/Data-Bifunctor.html) for higher tuples... e.g. `instance Bifunctor (,,,)` &gt; first (*10) (1,2,3,4) (1,2,30,4) [lol](http://replygif.net/i/1210.gif)
&gt; main = print $ (length @[Char]) ('a', 'b') I suspect it would actually be: length@[]@Char 
[lol](http://www.comicbookbrain.com/_imagery/2014-08-16/batman-1989/batman-1989-a-003.jpg)
&gt; a small part of Fur Elise Now I am thinking of furries :-) Just out of curiosity: in Germany it is common to replace umlauts with *ae, oe, ue* respectively when there is no access to umlauts. So *Für Elise* would be *Fuer Elise*. I thought that to be common elsewhere too, but apparently it isn't? 
It's a double edged sword to use those strictness annotations. What if you have `simplify (And FF something_very_slow)`? Personally, I almost never put strictness annotations on recursive data structures. Instead I use force the evaluation of the tree at strategic points.
The cure is worse than the disease.
Just a question, is there a difference in type inference for [type application](https://ghc.haskell.org/trac/ghc/wiki/TypeApplication) versus type annotation? Or is it `f @type` just syntax sugar instead of properly annotating the type of `f`?
I try to, but I only know to because I have some knowledge of German. Most English-speakers don't really pay attention to diacritics from other languages because English doesn't have them in common usage, so people remember the title as "Fur Elise" rather than "Für Elise". 
Hi Dave, thanks for sharing the talk and all the written material. Yes, the explanation rate could be a bit .. gentler to non CTs, I guess. Anyway, to other n00bs like me who might stumble on this: the material should be combined with /u/Tekmo 's ["Why free monads matter"](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) blog. In particular, the example therein about functor fixed points and how to abstract them into a "free" monad (it still feels funny when I type it) is making it click for me. Then, once that is digested, one can move onto the categorical duals (which are still in the realm of black magic for me)
Please improve the formatting (usual markdown rules); also, what game is it? Is it described anywhere? After that we'll all be able to help you
There is a difference. Suppose we have type family F a -- not injective x :: forall a . F a Without explicit type application, there is no way to pick `a` when using `x`. Even if you give a signature, like `x :: Int`, that just leaves an unsolved constraint `F a ~ Int`.
Back to the future... Fudgets took this approach in the mid-nineties: http://www.altocumulus.org/Fudgets/springschool95-intro.html
What about `x :: F Int`?
I can answer myself, partially. Bartosz' blog the other day about operads and tic-tac-toe hinted (at least in my mind) at a deep connection between dynamic programming and the inside-out control flow of comonads (as in, the backward induction to compute the "expected value of being in a state" typical of Bellman's equation); which would have ubiquitous practical implications (e.g. if this behaviour could be abstracted out and be made applicable to both continuous- and discrete-valued datastructures). In this sense, yes, these are beautiful theoretical gadgets but also very practical computational concepts. But, hey, I'm in computational science grad school so I'm a bit biased as well..
Yeah, I linked to "Why free monads matter" that from the main page associated with the talk - it's fantastic. I kind of blew past a lot of that in the talk itself, since there was a very well received talk on free monads at Yow Lambda Jam 2014 by Ken Scambler (and I gave a BFPG talk on them a few weeks before that), so I figured a good chunk of the audience would be up to speed with them.
wat.
Shell scripting is terrible…
Haskell is lazy (strictly speaking it is non-strict but since I don't know or don't remember the difference I am happy to say lazy). This means haskell does not evaluate each expression as it reaches it but remembers the expression and evaluates it when it needs to. In a strict language when evaluating fun foo(x) = {return 1 + x;} fun bar(y) = {return 2 + y;} fun baz(z) = {return foo(z) + bar(z);} a = baz(1); We say 1. I want to evaluate the expression for `a` 2. enter function`baz`, put it in the stack 3. in order to do that I need to evaluate `foo` 4. enter `foo`, put it on the stack 5. got the result of `foo`, pop it off the stack 6. now I need `bar`, enter `bar`, put on stack, evaluate, pop stack 7. evaluate baz, pop stack In Haskell you just remember the expression and say the value of a is `baz(1)`. Now I realise this is not a great example because I can't demonstrate what happens as you build up and evaluate an expression bit by bit as you need to but I need to get back to work now so I hope this is enough: The key thing is that in a strict language because you commit to evaluating each thing as you reach it, you only need to remember the list of things you are currently in the process of evaluating, in Haskell there will be a whole tree of unevaluated things (indeed not just a tree but a graph since a named or bound value may be referenced in many other expressions). Again sorry this has turned out unclear I meant to write a 30s answer and realised it needed more time than that to do it justice. 
It turns out the ocaml has two functions named meson in meson.ml, and the second one overrides the first. They do the same thing, but the second one does it way faster. 
This is a cross posted at SO: http://stackoverflow.com/questions/33427798/recreating-the-game-war-in-haskell where it's receiving some attention.
&gt; And yet, to this date, I think I can still parse and understand `someFunc1` more easily, and also believe I will more quickly spot a bug in that style I wish such questions were discussed more often (so, I'm glad that this post has been written). I agree with all 8 of Michael's points, and I can also try to expand on #6: &gt; 6\. I have a gut reaction that `someFunc2` is better When thinking about my own gut reaction to `someFunc2`, I think there are several reasons why it seems to be better: 1. Matching on `Just` and `Nothing` is a repetitive operation; `maybe` is the shortest possible way to perform this operation, and getting rid of repetition feels nice. 2. Code like `someFunc1` “belongs” to beginners, and there's a slight feeling of “I don't want my code to look like code written by a beginner” (even when it's actually better this way, yeah). \#2 is pretty irrational; however, I'm not sure about #1. On one hand, it doesn't seem *particularly* implausible that people can get used to pretty much any notation with time, and so the shortest notation will win in the end simply because it takes less time to scan. On another hand, people often use arguments like “but it seems logical for a human brain to work like so-and-so”, and given how often this just *doesn't work*, I think it's a pretty terrible argument. As much as I like logical arguments, in such cases it's better to rely on research (or even anecdata). Now, I don't know of any studies concerning the role of syntactic redundancy in programming languages, but we have a pretty good example of a language with almost no syntax: Lisp. As far as I know, in Lisp there's no special syntax for `if`, pattern-matching, etc – instead lispers use what is known as `maybe` and `bool` in Haskell-land. So, one way to find out whether extra syntax (and redundancy/idioms/etc in general) helps or hurts would be to find a Lisper and ~~torture nem until we know what we want to know~~ ask nem to share nir experience. /u/chrisdoner, would you care to comment?
You can write a data type `Arrow a b` that represents a function as a syntax tree with a function `apply : Arrow a b -&gt; a -&gt; b` that runs the code. You do need functions of course: apply is still a function. I would argue that his encoding is still nonvacuous: you only need first order functions to implement higher order functions.
That version of Leksah is 4.5 years old. Have you tried a more recent one?
`Control.Arrow` is a type class, while the `Arrow` above is used as a type constructor, so they can't be the same.
AFAIR not that good. There are some tooling that may work (look for Keera Studios Blogposts), but basically its compiling the thing as native Code and then use the JNI with a small Java-Wrapper to bootstrap your App. For simple things i would go down the GHCJS-Route (basically build a "static" webpage which gets displayed when the app is run) and use that webpage with something like http://jster.net/blog/tools-to-package-your-html5-app-for-mobile-devices or http://phonegap.com/ to display it on any device (there is something similar for iOS) so you get that for free. You should not do fancy stuff with JavaScript (like games), but for a simple app that querys some data or just displays/calculates things this is fine.
In fact, you can go further, and express disjoint union and tuples as functions too, as you would when encoding them in a lambda calculus: Left x :: Either a b ≈ (\f g -&gt; f x) :: (a -&gt; c) -&gt; (b -&gt; c) -&gt; c Right y :: Either a b ≈ (\f g -&gt; g y) :: (a -&gt; c) -&gt; (b -&gt; c) -&gt; c (x, y) :: (a, b) ≈ (\f -&gt; f x y) :: (a -&gt; b -&gt; c) -&gt; c But yeah, I can't think how you'd do without `-&gt;` as a type constructor.
That's true.
Interesting. Here in the U. S. most of us just leave out accent marks when we're too lazy to go copy paste them from somewhere else (which was the case here). 
That's what I imagined, thanks! My main reason for asking that was that I need to express some kind of relation that is not exactly a function, but that's close. That means I will have to express it lambda calculus terms at least. Time for some mind bending then. Thank you once more!
Again, the example we're looking at is too trivial for it to make a real difference. I'll give another example, which is still pathological, but it will actually demonstrate a difference: type family F a where F Int = Int F Bool = Int -- myFunc will not compile at all unless we turn on -- AllowAmbiguousTypes. The problem is that in GHC, -- we currently have no way to tell myFunc what type -- to instantiate the type variable "a" to. If we turn on -- AllowAmbiguousTypes and get myFunc to compile, -- then test1, test2, and test3 would all fail to compile. myFunc :: forall a. F a -&gt; String -- the forall isn't necessary myFunc _ = "Cool" test1 :: String test1 = myFunc (22 :: Int) -- doesn't work test2 :: String test2 = myFunc (22 :: F Int) -- doesn't work test3 :: String test3 = (myFunc :: F Int -&gt; String) 22 -- doesn't work test4 :: String test4 = myFunc @Int 22 -- This should work in 8.0 The problem is that when we use `myFunc`, GHC needs to instantiate the type variable `a` to something. But to what? - In `test1`, it's going to end up with the constraint `F a ~ Int`. But that doesn't tell us what `a` is, so it will fail. - In `test2`, it gets the constraint `F a ~ F Int`. This doesn't tell us what `a` is. It could be `Int` or `Bool`. If we get InjectiveTypeFamilies at some point, this situation could be improved (but it wouldn't help in this case because this type family is not injective). - `test3` is just like `test2` - Finally, `test4` will succeed because the explicit type application tells us that `a ~ Int`. Yeah! Hopefully, that is more clear.
Just to clear up something, does `F Int ~ F Bool` because they are both equal to `Int`? I thought that the constraint `F a ~ F Int` would be like pattern matching, and imply `a ~ Int` (I sometimes see things related to type families but never actually used it - seems very confusing..). But it appears that if `F a ~ F Int`, then `F a ~ F Bool` too, and vice-versa.
One more variant till ExplicitTypeApplication is available: type Len a = a -&gt; Int main = print ((length :: Len [Char]) ('a', 'b')) 
&gt; All I know is that reactive-banana is not build for speed. You should take a look at the code! Then compare it to the [simple implementation of the reactive-banana's primitives](http://gelisam.blogspot.ca/2014/07/homemade-frp-study-in-following-types.html) I came up with when I first started to study FRP. My version is not built for speed. Reactive-banana's code is much, much more complicated, and I believe the main reason for those complications is to improve the performance. Also, I remember that Heinrich said somewhere that he has a performance test suite for making sure that performance is not degrading from version to version, so I'd say that he's very much concerned with performance.
Here is TodoMVC done with GHCJS and Pipes, might be interesting for you https://github.com/tonyday567/mvc-todo
~~Higher-order functions also require type-level functions (i.e. generics) on top of just first-order term-level functions.~~ derp
the notion of "fundamental polygon" of topological object, [here](https://en.wikipedia.org/wiki/Fundamental_polygon) and [here](http://cosmos.ucdavis.edu/archives/2010/cluster6/Lin_Matthew.pdf) made it click for me.
Does Servant have any solution for producing type-safe links? One desiderata (or maybe more, depending on how you think about REST) would be to provide opaque URL links between resources which requires being able to target links against the API. It'd be nice to do that without manual magic strings.
Can you elaborate?
In general, I do prioritize correctness and ease-of-use over performance, but I still want reactive-banana to be fast (a constant factor &lt; 5 compared to the competition seems acceptable to me). I'm afraid I don't remember the discussion you mention, but I did some performance tuning for 0.9, which carried over to 1.0. In any case, I'm happy to hear about examples where reactive-banana is significantly too slow. John Lato has put together an FRP library benchmark some time ago, but I don't remember where.
Its can't be an arrow for the usual reason. The arr method makes the Arrow class much less useful. In this case it would mean that the type would have to embed Haskell's arrow within it.
One could make a case that what we're seeing here is that Haskell web frameworks, and probably a great deal more of the real Haskell code the community has produced in the past few years, is reaching the Pareto optimality frontier between ease-of-use and type safety guarantees. I imagine a lot of people know what that is given where I am :), but in short, Pareto-optimal means that if you are optimizing between two quantities, you can not increase one without decreasing the other. You can draw a boundary between "all of X, none of Y" and "none of X, all of Y" that has certain characteristics which you can read about elsewhere if you are interested. I bring this up because one very common cognitive mistake people in general make is to speak of a "tradeoff" between X and Y without taking the time to first establish that they are Pareto optimal, because if you're not yet Pareto optimal, that mean that you _can_ in fact have more of X without having less of Y yet, and constraining the discussion to "tradeoffs" is introducing a false premise. A real world example is "security vs. convenience"; the vast bulk of real-world situations in which we putatively have this conversation, we could in fact have more of either without affecting the other, because we should be so lucky as to be on the Pareto optimality line. The less-common second-order mistake is that when a particular X and Y have spent a very long time nowhere near the optimality frontier, people can forget that there _is_, in fact, a tradeoff to be had, if you do manage to optimize sufficiently. As a third-order consequence, it becomes easy to adopt an X-at-all-costs metric because if you never have to pay in Y, more X is always a good thing. Arguably, ease-of-use and type safety could be such a case. For a long time the Haskell community has had a lot of places to get "free" gains in both dimensions. But, it should be _expected_ that if you get enough of these gains squared away, that eventually you will hit the point where they will become tradeoffs. And not just "tradeoffs because you aren't skilled enough to use the 'hard' stuff yet", but truly tradeoffs, for everybody. Users with differing skills and needs may choose different points on the curve for their own needs, but a discussion about tradeoffs is a fundamentally different discussion than one about right vs. wrong. I'm just musing and providing food for thought. Whether Haskell is there yet anywhere is an interesting question. But if it is, I'd have to say that while in some ways it may be the sad passing of an era, it is also an incredible accomplishment, because I don't think any other language community has ever even gotten close to the optimality frontier while retaining a high degree of type-based safety. (I could probably make a case for some languages with low type-based safety and high ease of use, though if you start poking at it "ease of use" is a family of criteria rather than a unique one; for instance, "easy 10 line script" and "easy million-line system" are substantially different languages.)
You need universal quantification to encode products and sums in a typed lambda calculus, which is why the simply typed lambda calculus does not work, but System F or the untyped lambda calculus do work.
So the shortest we could do in 7.10 is main = print $ (length :: [_] -&gt; _) ('a', 'b') ? And for the `ExplicitTypeApplication` versions, are the parentheses necessary? i.e. does this work: main = print $ length @[_] ('a', 'b') ?
[Yes](https://hackage.haskell.org/package/servant-0.4.4.5/docs/Servant-Utils-Links.html).
Thanks for the link to these documents. I have seen these diagrams before but I did not know what they were called. It is great to match a name to it.
Somewhat related: if you are interested in doing audio stuff in Haskell, check out [csound-expression](http://hackage.haskell.org/package/csound-expression-4.9.1). It's a Haskell binding to Csound, which is a super powerful audio programming language that's been around since the 80s. I've been playing with it for a month or so and having lots of fun. 
Amazing! Not sure how I missed that in the past.
This is interesting, anyway it is quite a peculiar concept of model-view-controller. Usually the model represents the state, while here it represents a transformation node
`flip` is also really useful when using lambdas. for example: flip map list (\key -&gt; case lookup key mapping of Nothing -&gt; defaultValue Just x -&gt; x)
For people who haven't tried Frege yet, it's *very* similar to Haskell and they are working on making it even more Haskell-compatible. I was really surprised when I first tried it how similar it was.
 for = flip map for list (\key -&gt; ...
What about tail recursion?
I agree that that's better, and using `flip` was an extreme example on my part. But honestly, the explicit pattern matching is _still_ clearer to me at a first glance.
The tension is not between beginners and experts, it's between explicit and implicit, complex and simple, abstract and concrete, and several other dimensions. My opinion is that experts should know the trade offs better and thus end up at better points on the spectrum, not run off towards explicit/complex/abstract as fast as they can.
My pleasure! BTW, what do you use manifold learning for?
Jasper's blogs are always a treat !
&gt; Disabling the monomorphism restriction could cause this silently. As someone who usually just keeps it enabled and doesn't know much about it, why would that happen?
&gt;&gt; **If the performance is that different, they are not equivalent programs...** &gt; UPDATE 3: It looks like there is a chunk of code missing in the haskell version, and this is causing the problem. I will fix and upload. &gt; &gt; FINAL UPDATE: new code checked in, GHC version now runs in about 0.7 seconds vs. Ocaml's 2.5. We have a winner, folks! 
Without dependent types, I don't think you'll be able to enforce the constraint that every `a` and `b` should appear exactly once. This sounds similar to the more common desire to represent bijections: -- fwd . bwd = id -- bwd . fwd = id data Bij a b = Bij { fwd :: a -&gt; b , bwd :: b -&gt; a } We can't make sure that the functions are inverses at compile time, so we resort to writing in a comment which laws the functions should obey. Similarly, I'd describe your cobordisms using something like this: -- x `elem` inputs (fwd x) -- y `elem` outputs (bwd y) -- fwd x `elem` fmap bwd (outputs (fwd x)) -- bwd y `elem` fmap fwd (inputs (bwd y)) data Cobordism a b = Cobordism { fwd :: a -&gt; Trans a b , bwd :: b -&gt; Trans a b } inputs :: Trans a b -&gt; [a] inputs (Keep x _) = [x] inputs (Split x _ _) = [x] inputs (Merge x1 x2 _) = [x1, x2] inputs (Destroy x) = [x] inputs (Create _) = [] outputs :: Trans a b -&gt; [b] outputs (Keep _ y) = [y] outputs (Split _ y1 y2) = [y1, y2] outputs (Merge _ _ y) = [y] outputs (Destroy _) = [] outputs (Create y) = [y] That is, I'd use ordinary functions to map each `a` and each `b` to the `Trans a b` which contains them, writing in a comment the constraint that the returned `Trans a b` must contain the input value. And some more constraints to make sure that `fwd` and `bwd` are returning entries from the same set (this is the part which I'm the least certain how to encode as constraints). It's unfortunate that most of the structure in this representation is encoded in unenforced comments, but at least it makes sure that each `a` and each `b` appear in a `Trans a b`, so it's a tiny bit more safe that if I had started with a `Set (Trans a b)`.
aha! We've been hunting down the source of this for a while now. Still no source but at least now we have a workaround.
It makes more sense when you actually bind values to it (x, y) = ("hello", 1) binds x to hello and y to 1 [x, y, z] = [1, 2, 3] is a partial example, but a lot of people are comfortable with partial cases like this that can be statically verified. 
The main use I've found for it is to define a bunch of toplevel variables at once from a large record. For example, when using Parsec's [token parser](https://hackage.haskell.org/package/parsec/docs/Text-Parsec-Token.html), you can write: Tok.TokenParser {reserved, identifier , operator, reservedOp , lexeme , parens, whiteSpace , integer, natural, float , commaSep, semiSep } = Tok.makeTokenParser myLanguageDef Instead of the more verbose (as in the Parsec doc): lexer = Tok.makeTokenParser myLanguageDef reserved = Tok.reserved lexer identifier = Tok.identifier lexer ... Either way you get `identifier` and `operator` and so forth all in scope, but one feels like a lot less boilerplate. (Note I'm also using the [`-XNamedFieldPuns`](https://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#record-puns) extension so I don't have to write `identifier = identifier` etc in the record pattern match)
Well, just because the code is complicated doesn't mean that it is fast... :-) But yes, the main reason for all the complications is to get the performance of a push-based implementation, which means that an incoming event will update only those parts of the network that directly depend on it. Most importantly, if an event occurrences does not make it past a `filterJust` combinator, this event will be dropped completely and not propagate further down in the graph. In constrast, the version that you have written, /u/gelisam, is pull-based, which means that nodes in the graph will query updwards and ask if anything has changed. This actually works well if changes frequently affect the whole graph, but the former implementation has the potential to do strictly less work than the latter implementation. In comparison, the FRP implementation in the Elm language is not entirely push-based, it will still pass the event around for subsequent processing, even though its a `Nothing` that gets passed around. At least, that's how it was in the first versions, this may have improved. Unfortunately, the complicated code for reactive-banana is the simplest thing I could come up with that does everything right and is still "easy" to understand. There are a lot of complications: Merging events deterministically, recursion, and changes to the event graph. There is certainly some overhead (e.g. monad transformer stacks) that I haven't optimized yet, so the constant factor may be worse than strictly necessary.
Thanks! I mainly use the Haskell wiki because it is a very convenient way to write a homepage for the project. The repo has a [`doc`][1] subdirectory for all kinds of documentation stuff. If somebody wants to move the [examples overview page on the wiki][2] to the repo, I think that would be useful. The unofficial tutorial on the wiki is, well, unofficial. I didn't write it. [1]: https://github.com/HeinrichApfelmus/reactive-banana/tree/master/reactive-banana/doc [2]: https://wiki.haskell.org/Reactive-banana/Examples
Is your "final update" comparing to compiled O'Caml code? It's less crazy than 400x, but 3.5x slower is still a bit surprising - in some little tests, I've seen single-threaded code that mostly just works over strict algebraic data types running at least as fast in O'Caml as in GHC, perhaps up to 2x or 3x faster (for one, the memory layout of data in O'Caml doesn't have to account for laziness or threading).
I definitly agree that `flip` makes code much harder to understand. However, I don't think that there is much of a difference between `maybe` and pattern matching on `Maybe`: pattern matching and `maybe` are really equavilent (if you use `maybe`, you never need to pattern match on a `Maybe` value and vice-versa). I think it doesn't matter much which one you write. But those cases, where you have a function that does exactly the same as pattern matching, are rare, IMO. The real advantage comes when the function is *less* expressive than some other construct. A simple example: someFunc4 key = forM (lookup key someMap) $ \value -&gt; do ... long chunk of code ... vs someFunc4' key = case lookup key someMap of Nothing -&gt; return Nothing Just value -&gt; do ... long chunk of code ... In this case, with the `forM`, I can instantly spot that this returns `Just something` whenever I pass it a `Just` and `Nothing` otherwise. I have to read the whole function to see that if I use the second form.
Ahh, I see: data Foo = Foo Int Foo x = Foo 2 main = print x but then, why does this cause an irrefutable pattern: data Foo = Foo Int Int Foo x 4 = Foo 2 3 and my original example succeeds? It seems it only actually tries to *match* things if there are other variables?
go back to Java partial function!
I guess that's a pattern match fail, just as Foo () would be, without a next case to try.
`Foo 3 = Foo 2` doesn't really mean anything as you aren't binding any variables, it likely doesn't do anything. `Foo x 4 = Foo 2 3` means "pattern match `Foo x 4` against `Foo 2 3` and give `x` the matching value".
Yea that's true. My personal style is to try to be explicit about names in scope for things that aren't prelude (or lens). The token parser specifically, also has like 30 fields not all of which are always useful and it's just easier to bring in the names I care about explicitly.
I try not to avoid length but use type level stuff so my code doesn't crash. (like Servant, there's a recent post, check it out). "new haskell" is overwhelmingly an improvement.
 ghc-options: -fPIC
Use `bracket` for resource handling. E.g. here the `fid` may not be feed in case of exception: execGetter path t g = do fid &lt;- I.newFile path t x &lt;- runGetter g fid I.freeFile fid return x 
Further investigation confirms that I was right about reflex. See my other comment [here](https://www.reddit.com/r/haskell/comments/3qsz16/why_is_this_reactivebanana_code_slow/cwisqh1).
My end goal is to have some sort of basic GUI on top. I'm aiming for live synthesis, but low latency synthesis is not a huge priority right now for me as it'll be geared towards playing note sequences rather than working with midi data from an actual midi device in real time. In my experience, that has a much higher threshold for acceptable latency (I've been able to use FL Studio through a virtual machine for that with no problems for example). The reason I figured FRP would be useful is because I'm not sure of a better way to model what I'm trying to model. That being essentially a virtual (not sure of the technical word here) audio patch, something similar to [this](http://blog.interfacevision.com/assets/img/posts/example_visual_language_flstudiopatcher_01.jpg), where the outputs of anything can become the inputs of anything else. It seems like it'd just be function composition, but in practice it seems like the code for that becomes rather unwieldy, especially when it comes to effects that require buffering a good deal of audio before processing it (stuff like delays, reverb, etc.). I'm not really sure what the best answer is here.
But beware—it’s easy to go overboard with this extension and write code where all the identifiers seem to come out of thin air. These days, I avoid it for the sake of my future self.
for me, pattern matching is easier to read the first time, but harder to remember when reskimming, if that make sense? "remember" as in "when also keep 100 things in my head at once". I don't like flip, but in this case: (1) the two args have different types (you can't mix them up) and (2) the pointfree style makes it clear that the Just case can't make use of the key, only the value. I only use pattern matching when the action is complex enough. and I try to split things out into short helper functions (with the relevant argument order), to use eliminators as much as I can. when I see a pattern match, it's often some mess in some big do block, and a signal to me, "slow down, this gets messy". but again, that only works when you keep the one liners clean. 
To be clear: I agree with your definition of `for`, I think it's the right one. `for` from `Data.Traversable` could have been named `forA` instead (which would keep consistency with `forM`). In fact, I wonder if `traverse` would have been better as `mapA`.
I had thought frege was not very active, but it looks like it has come a long way since I last looked at it.
Yeah, good point. I definitely try to use explicit imports, or more often `import qualified … as …`. Not just for code navigation, either: with wildcard imports, your code can break if a dependency adds exports. :(
I think of that pattern as a poor man's module. I mean, that's exactly what `TokenParser` wants to be: a module parametrized by the lexing rules. You might still want to import it qualified, but my personal take is that it's fine to import unqualified if a module is sufficiently integral to the code. So I'd be perfectly content with this in a dedicated parsing module which is inextricably entwined with whatever parsing library I'm using anyhow.
Cool, thanks! (And sorry for taking so long to get back!) Beyond knowing that it "works", though, I'd be interested in *why* it works. What are the deeper relationships, or higher abstractions it follows from? Is it just a "trick", or is there something more fundamental at work? And let me try to explain why my intuition lead me to suspect there should be a "`data Fix`" and a "`codata Fix`". Datatypes in total languages (like Agda) and in non-dependently-typed languages (like Haskell) bear some resemblance to each other, and have quite a large amount of overlap. There are many structures you can define in either, except you can't do dependently typed things in the latter (or not as much, anyways). The reverse direction is more interesting. You can define recursive data structures like `List` in Haskell... and you can do it in Agda too, except you can do it *two* ways, and you must choose, up-front, which: inductive, or coinductive? Whereas Haskell doesn't apparently care as much about that distinction. And in Haskell, you can abstract out "the recursion" from these datatypes into `newtype Fix`. For symmetry, I would expect that you would be able to do the same in Agda, except you must again choose up-front between induction and coinduction. Or maybe the choice would be made at the level of the base functors, instead? If you can have `Fix ListF` in Haskell, which is potentially infinite, as well as `Fix List'`, which is finite, how come you can't do an analogous construction in a total language, where the compiler knows that one of these is inductive data, and the other is coinductive codata? (I agree that `Mu` and `Nu` are brilliant at fitting snugly into the restrictions of the language - but, I guess I'm wondering, how come the language itself can't be more smart or flexible?) (Probably I need to keep contemplating your earlier comment...)
Indeed, `apply` is enough. See [Polymorphic typed defunctionalizatoin](http://gallium.inria.fr/~fpottier/publis/fpottier-gauthier-hosc.pdf), François Pottier and Nadji Gauthier, 2006. They explain how to turn any function into a new case for such an `Arrow a b` datatype (which must be a GADT for type-checking to work). You can do that for all functions in your program (as a whole-program transformation), or only some functions if you wish.
Thanks, I'll give it a try. :)
One thing I keep in mind: **variable names have a mental cost**. Perhaps that cost is worth it, perhaps it isn't, but it's something to consider. The variable is an extra piece of the puzzle, an extra piece of complexity. If it helps you limit complexity somewhere else (like avoiding `flip`), it can be fine. If it helps you follow some tricky control flow, great. But if it doesn't, I prefer avoiding the extra detour. This is why I'm *usually* happy to eta-reduce my functions, as long as I can do it without crazy acrobatics. If I can cancel out a variable name without adding anything, that's one less thing to keep in mind. The beauty of `.` (used with a bit of restraint) is that it focuses on *the function*, not *what the function does*. To me, it's like comparing &gt; To foo, do bar then baz to &gt; To foo an x, bar the x then baz the x This is exaggerated, to be sure, but the former is significantly cleaner especially if `bar` and `baz` are composites themselves. The `x` is almost an implementation detail of `foo`. Sure, `foo` takes *something*, but since it can be expressed just by combining two other things, we don't care about that thing at all. This is a special case of a more general principle: equivalent definitions of the same thing bring parts of it to light and leave parts of it in the shadow. Choose the one that points your reader's attention in the right direction. (I've had this idea intuitively for a while, but what made it click was reading Barry Mazur's ["When is One Thing Equal to Some Other Thing"][one-thing].) [one-thing]: http://www.math.harvard.edu/~mazur/preprints/when_is_one.pdf
It is crazy, but does it cause any problems?
I agree that it's not technically a WAT and has a type-theory behind it to have this: λ length (3,4) 1 But still, it somehow bugged me a bit, that even after figuring out types, it still seems a bit weird. And I think I just had an "aha" moment. I think the real WAT about all this is not `length (3,4) = 1`. I think the real WAT is: λ length ("asd"::Text) &lt;interactive&gt;:8:17: Couldn't match expected type ‘t0 a0’ with actual type ‘Text’ In the first argument of ‘length’, namely ‘("asd" :: Text)’ In the expression: length ("asd" :: Text) In an equation for ‘it’: it = length ("asd" :: Text) I think, it is quite expected for `Text` to have length, isn't it? Yet, we cannot have one, because it expects our container type to be represented as a type `c a`, where `c` is some container-wrapper, parameterized by inner-element type `a`. And that's what is weird about it: why must it be in that form? `Text` is a container in some sense, and it has an element-type of `Char`, yet it's not in a form of `c a`. So maybe we shouldn't use `Foldable` for `length` at all, ideally? Maybe we should use something like `mono-traversable` approach, and call `Foldable`-length somehow else? (like `foldableLength` or something) Just thinking out loud :) UPDATE! ATTENTION! UPDATE! It seems that: λ import Data.MonoTraversable λ olength ("3","4") 1 And I think this is a real WAT now :) Because I think there should not exist the `type Element (a, b) = b` (I also think `type Element (HashMap k v) = v` must be `type Element (HashMap k v) = (k, v)`, e.g. you would map over key-value pairs, not just values).
I asked this question on SO a while back: http://stackoverflow.com/questions/5151858/running-a-haskell-program-on-the-android-os
(oh yikes. thanks, not gonna upgrade now) for one, that's a command that cabal is calling, not a location it's installing to. maybe modify the permissions on it? (not much of a systems hacker)
The ingroup has a good explanation for our not-a-wat. The outgroup spends too much time rationalizing their wats.
That's a good reason to prefer default instance, right. But I rarely copy printed out stuff to source, and if I did, I would create smart constructors that have the same name, but lowercase. Then it would be relatively easy to get from printed structure to syntactically valid code
Totally agree. The good news is this means we can fix the problem without going back on FTP (which would be crazy talk at this point). Making Prelude.length, Prelude.maximum and friends only work on lists again would take care of it. Calling `length` on a tuple will always be an error, it should be caught at compile time.
&gt; Calling length on a tuple will always be an error, it should be caught at compile time. Not true. The discussion kept going after the end of this video and some presented situations where calling length on a tuple is exactly what you want to do. I think it was something like having a list of Maybes, and then you augment them with some kind of tag using `fmap (tag,)`. Then later you want to find out how many non-maybe values you have.
Is it the async that's throwing you? Generally you'd just write some code that retrieves the web page and processes it as desired; whether or not you run that via `forkIO` or some other threaded mechanism is a separate concern. Or is it the HTTP request that's throwing you? I'm not sure what the "simplest" HTTP client is for Haskell.
Haha, turns out `mono-traversable` has the same instances, but this time it doesn't have an argument that "there's no other way to do that", so I think that is actually a bad design decision. Added an "UPDATE" into my comment.
I didn't understand the example from list of maybes you provided, could you give just a bit more details please on how would Foldable help you in counting non-maybe values inside `[(Tag,Maybe a)]`?
I could have missed something, but I think it was something like this. Take a list of the form that you gave above. But you don't actually have it with that type. It exists as `[f (g a)]` where `f` and `g` are one of Traversable, Foldable, or Functor. By intentionally not using concrete types like this you gain the desirable property that it is impossible to change the structure of the thing you're working with. Now you can define a function like this: countItems container = fmap length &lt;$&gt; container This thing changes all the Maybes to counts without having to know that you are working with a Maybe. Now you can do this: sum . fmap sum . countItems There may be other better ways of doing this, but I think this gives you the idea.
I think the video is missing an answer to my question: &gt; Why is it a good idea for a tuples to have a Foldable instance at all? I think that is misleading. Intuitively, I thought that `length (2,3)` is 1 because it's _one tuple_. On second thought, that would make no sense at all, but the actual explanation is so unexpected that it's easy to get misled. **UPDATE:** Even less intuitively, `length (1,2,3)` is an error. Apparently `(,)` is foldable, but `(,,)` isn't?!? **Update 2:** Found a relevant discussion: https://reddit.com/r/haskell/comments/3oq0kd/proposal_eliminate_the_monad_implementation_on/ Apparently, the answer to my question is the problem with orphan instances. Wouldn't want different people defining different instances of Foldable for the same type.
&gt; Intuitively, I thought that length (2,3) is 1 because it's one tuple. That way of thinking about it was mentioned later in the discussion after the talk. The way I like to think about it is that `length :: Foldable t =&gt; t a -&gt; Int` just counts the number of `a`s in the structure `t a`. When `t a` is `[a]`, then it's obvious. It's the same for `Maybe a`. And it's also the same for `(x,a)`. You're always counting the number of occurrences of the last type variable. Once you understand this simple rule all the behavior makes sense and is completely consistent. The same is not true of the Ruby and Javascript wat's in the original wat video.
Requires the http-conduit, text and async packages import Control.Monad import Network.HTTP.Conduit (simpleHttp) import Control.Concurrent.Async (async, wait) import Data.Text.Lazy.Encoding (decodeUtf8) import qualified Data.Text.Lazy as Text import qualified Data.Text.Lazy.IO as TextIO transformation = filter (("jabberwock" `Text.isInfixOf`) . Text.toLower) . Text.lines main :: IO () main = do a &lt;- async $ do contents &lt;- fmap decodeUtf8 $ simpleHttp "https://www.dartlang.org/f/jabberwocky.txt" forM_ (transformation contents) TextIO.putStrLn wait a 
The things is we already have infrastructure in place to make this stuff happen, like Hoogle.
Oh wow, nice :)
Thank you all for the help!
There's no such library as `wget` for Cabal to install. What command are you running? EDIT: Looks like this is related to SIP on El Capitan. You can disable it by following the instructions in this [article](http://osxdaily.com/2015/10/05/disable-rootless-system-integrity-protection-mac-os-x/).
As a follow-through to this, the end of the presentation asks: &gt; What did you expect? Well, I'm not sure "expect" is the right term, but I *wouldn't be surprised* to fail at compile time with a type error. Why do we need this instance, really? What does it enable, keeping in mind that newtypes can have independent instances? Why is it important for the *base* pair type (and not other tuples!) to have a Foldable instance?
Is it lazy?
I wasn't surprised by length of a pair. Video mentioned 'consistency'. To me, it is consistent. btw. [sum [1..] == -1/12](https://en.wikipedia.org/wiki/1_%2B_2_%2B_3_%2B_4_%2B_%E2%8B%AF)
I'm curious about how much of a difference sequences/sets/texts/strictness annotations make now that the algorithms are truly equivalent. 
&gt; Let me join the bikeshed parade, and suggest backticks, which I think are underrated. Agreed. &gt; I really like compositional chains, because they are easy to refactor, and they read like sentences. Do this, then that, then the other. No branching to follow. Also agreed.
A *wat* is in the eye of the beholder. Can you honestly go: - **A**: Look at this language quirk. It totally surprised me. - **B**: No, it didn't: you just have to remember about `Foldable` and `((,) a)` and the highly abstract type of `length` and how a tuple is *actually*... It reeks of bias, and maybe I'd even call it stubbornly defending your language. Instead of considering that, maybe, some design decision somewhere along the way made Haskell a bit more confusing for someone on Twitter, the speaker explains why their confusion is really just their fault. **Keeping Prelude simple is a good idea.** It's famously the reason `map`'s type is limited to lists! But with the FTP proposal, this really silly thing happened to `length (3, 4)`, and now beginners are puzzled about it. Calling `length` on anything but a list-like type is much better off failing to typecheck. And on that note, as others have mentioned, it's far less surprising to me that `length (3, 4)` returns `1` than that it even *returns anything at all*, and this issue was completely side-stepped by the talk. The `Foldable` instance for tuples does more harm than it does good.
Gosh. That sounds like it occurs practically never -- and specifically, it happens *vastly* less often than people will accidentally write `length x` where `x` is a tuple and not a list, and they get a confusing error from the type checker somewhere completely different (or even worse, a broken program that type checked!) Just because there is *some* possible use-case for it, doesn't mean it's a good idea... Besides, at that point, `length` is no longer a descriptive name for the solution to that problem at all! If `length` is doing all of this voodoo instead of counting how many elements are in a list, why call it `length`?
(type: no link)
re cargo test, doctest will run every comment that begins with &gt;&gt;&gt; in a ghci session. it's fragile (one typo can pollute the rest of the tests, sometimes template haskell and/or profiling causes issues, etc), but it mostly works. I always use it.
There are two very relevant open cabal tickets: - [Maintain central haddock module index #509](https://github.com/haskell/cabal/issues/509) - [Automatic hoogle database for installed packages #95](https://github.com/haskell/cabal/issues/395) As of right now, you can put these options in `.cabal/config`: library-profiling: True documentation: True doc-index-file: $datadir/doc/index.html haddock hyperlink-source: True You can build your project in a sandbox, then in your browser go to `file://$sandbox_path/share/doc/frames.html`. You should have the full haddock documentation for everything installed, and though it doesn't have hoogle search integration it's quite easy to find the module you're interested in. Unfortunately you can't serve this generated documention on a local http server, the hyperlinks are absolute filesystem paths and consequently break. This has a ticket, [Always use relative paths in Haddock documentation #1465](https://github.com/haskell/cabal/issues/1465), which is closed as it is apparently subsumed by [Support relocatable packages #462](https://github.com/haskell/cabal/issues/462). 
A key part of the answer to your question is `Foldable` being a superclass of `Traversable`. That point was elaborated a few times amidst the recent discussions here -- for instance, cf. [this comment by Edward](https://www.reddit.com/r/haskell/comments/3qw8h9/nyhug_the_notawat_in_haskell/cwj6ibw) in a parallel subthread.
Well, if the issue here is orphan instances (as it now seems to me), a badly written library that redefines Foldable for (,,) would probably also try to redefine it for (,). So, it would not compile, thus hopefully leading the author to realize his mistake. So, for this purpose at least, defining an instance for (,) seems sufficient.
Perhaps it should be called flength. (f for Foldable, not Functor [as in fmap]) 
Isn't part of the problem that resources are scattered? https://wiki.haskell.org/Haskell and https://en.wikibooks.org/wiki/Haskell for instance. Languages like Rust or Go have one web page that can get you from start to end. Central site makes it easier to ensure better overall quality of documentation.
&gt; Keeping Prelude simple is a good idea. Nonsense, keeping typeclasses out of the prelude was done for pedagogical reasons in H98 and it was the wrong decision then and it's the wrong decision now.
I'm a big fan of doctest, however I've been bitten a couple of time by the example-test not being "seen" by doctest. I think this happen if you forget to put a empty line between the doc and the example. It might be a good idea to include in the haddock the actual status of the example.
&gt; Nothing beats an example. We already have awesome type signatures. If we add examples, we complement the abstract with the concrete. As a newbie this is what I miss the most. Especially when it comes to more advanced concepts I find myself making my code type-check but still being unsure on how I can actually use it.
You mean if I purchase that particular decal, some money is getting donated to Haskell.org? It's not obvious looking at the site. That actually might look pretty nice on the back. I could cover the ugly white fruit-themed image I have on the back of the laptop :-)
It's only a *wat* if you know it's a tuple. If you are given a tuple as `Foldable` then everything is fine. Example display :: (Foldable f, Show a) =&gt; f a -&gt; IO () display f = mapM_ print f How many lines do you expect `display x` to print ? the same as `(length.toList) x` ? So `display (1,2)` displays only one line. Is it a wat ? (It's not new, this code is normal GHC **before** FTP). Nobody ever complained about it. Having `map` limited to list is indeed a good idea for type inference. But there a lots of time when you wrote your code using lists and and want to switch to vector for example. You could have written `fmap` every where instead of `map` but you wish every function you call working on list, could just work on every functor or foldable. However, to come to the *wat* maybe a warning could be generated when `length` is applied to `(,)` and it's known at compile time. Maybe HLint could do it . 
oh I agree, hence "fragile". perhaps a linter could earn above lines in comments that start with "&gt;&gt;&gt;" but don't have a newline before. I've never written an hlint rule, but it sounds doable. some official support (from haddock? from ghci?) to make this easier would be nice. my biggest issue is typechecking. I want it to typecheck my docs before running my tests, but that's probably impossible. I guess, what I really want is readable test cases in my documentation. magically inlining-into-the-docs certain expressions from a testing module would be cool. 
You get a warning about a pattern binding no variables. Isn't that enough? 
It's not a reason to oppose FTP. It's a reason to not provide a `Foldable ((,) a)` instance.
If you're surprised by that final assertion that `sum [1..] == - 1/12`, and you don't like clicking links, I'll point out that that's Ramanujan summation of divergent series, the initially surprising, but consistent and brilliant work of a mathematical genius. Ironic that it gets downvoted to - 4 in a discussion about the nature of programming wats. I think it kinda proves the point that you can be as rational and consistent as you like, but if your assertion is surprising, people are going to think it's stupid. 
It compiles to Java with light rumtime in Java.[runtime](https://github.com/Frege/frege/tree/master/frege/runtime)
I think that comes from the fact that there is no "Central Authority" for Haskell. There are "separate" teams working on hackage, haskell.org, ghc, hoogle, stackage, etc so there is no real central coordination. I believe rust has the advantage here since it was started by mozilla so since the start there was already a clear leadership. 
Thank you, I loved that post when it came out and in general I agree that's a better way to explicitly state over what you're folding.
It's hard to argue with something so abstract/generic. My only real objection is regarding the use of Pareto optimality, which has a quantitive meaning, whereas the case in point is completely cognitive and rather impossible to quantify (unless you count the number of variables in code as a monotonically increasing "cognitive complexity" to be always minimized).
I should, yes. Thank you
Re: dynamic linking: does one need to run pkgconfig on TagLib ? also, there seems to be an `extra-libraries` entry for the Cabal file that removes the need to supply linker flags. 
I buy my stickers from Redbubble. A Redbubble search for Haskell got these stickers http://www.redbubble.com/shop/haskell
&gt; You're always counting the number of occurrences of the last type variable. To be more precise, you're counting the number of times the last type variable is being folded over. This distinction is important in a `Foldable` instance like this: data Repeated a = Repeated !Word a instance Foldable Repeated where foldMap f (Repeated n a) = go n where go 0 = mempty go n = f a &lt;&gt; go (n-1) `a` only "occurs" once in `Repeated`, but calling `length` on a `Repeated` value would yield a number equal to its associated `Word` value precisely because of how its `Foldable` instance is defined.
Why should the generally applicable abstractions have to live in alternative preludes library authors are discouraged from using because of the additional dependencies while a restricted prelude is the default for pedagogical reasons? Shouldn't the restricted one be provided as the alternative implementation instead?
Please don't ask me about my opinion about Servant
No-one complained about `mapM_ print (3,4)` because you don't assume that something called mapM_ does something obvious that everyone understands. The original wat video wouldn't be funny if he was doing stuff like `structureCombine({}, {})`. It's funny precisely because he's doing things like `{} + {}`. The defence that says that no-one would deliberately do that for real applies just as well to the javascript and ruby as it does to the haskell. 
Using [async](https://hackage.haskell.org/package/async), [wreq](https://hackage.haskell.org/package/wreq), and [lens-simple](https://hackage.haskell.org/package/lens-simple-0.1.0.8) {-# LANGUAGE OverloadedStrings #-} module Main where import Control.Concurrent.Async (async, wait) import Data.ByteString.Lazy.Char8 (unpack) import Data.Char (toLower) import Data.List (isInfixOf) import Network.Wreq (get, responseBody) import Lens.Simple (view) transform = filter (\x -&gt; "jabberwock" `isInfixOf` map toLower x) . lines main = do response &lt;- async $ get "http://www.dartlang.org/f/jabberwocky.txt" mapM_ putStrLn =&lt;&lt; transform . unpack . view responseBody &lt;$&gt; wait response
It is certainly possible to define Ramanujan summation in Haskell. You don't need to look at “all” the elements; as Wikipedia states, “Ramanujan summation essentially is a property of the partial sums, rather than a property of the entire sum”.
Think of it this way: Suppose you have a value `l` of type `[()]`. Apply the following function to it: f [] = [] f (x:xs) = 1:(map (+1) $ f xs) Let `rsum` be the Ramanujan summation function. Now you can distinguish an infinite list of `()`'s from a finite list of `()`'s just by `rsum (f l) == -1/12`. Since you can't distinguish finite and infinite lists in Haskell, `rsum` can't be written.
Excellent, I've added `extra-libraries` option to the Cabal file.
The point is that mirpa wasn't downvoted because people were surprised, but because there's so many things wrong with their comment.
Thanks for this post! We're investigating FRP for a highly interactive and graphics-intensive UI that uses SDL. While we do not need 44kHz updates, it would be nice to achieve 40-60 FPS in an environment that will have _many_ (I think 10 streams that produce constantly, and many others that only incidentally produce) events fire for each frame.
No, it's not wrong, it's just a deeply esoteric piece of maths presented in terms of haskell code. The only fair criticism is that it _shouldn't_ be presented as if it were haskell code, but to be fair sigma notation is hard in reddit and wouldn't be as readable for r/haskell readers. Here's the thing, though: if someone already understood why sum [1..] is - 1/12 they wouldn't for a minute think that mirpa was _wrong_, they would just see a mathematical statement translated into haskell expression, in a similar way to if you saw `sum [9*(0.1^i)|i&lt;-[1..]] == 1`. It's surprising, and for some, counterintuitive, and it's not actually haskell, but it's definitely not wrong. I think all the hate proves my point that telling people they should have known the subtlety of what was said doesn't make them think any more kindly towards you; we won't be able to convince people unfamiliar with how typeclasses work in haskell that length (4,6) ==1 is right, we'll just convince them that we're annoying "know-it-all"s who think they're right and clever when they're not. I say this in full awareness of the irony. 
I hope recursion schemes tutorials become the new lens tutorials just as lens tutorials became the new monad tutorials.
Indeed. Edward Kmett [agrees with you](/r/haskell/comments/3qw8h9/nyhug_the_notawat_in_haskell/cwj6eza)! Hence a wat even by the definition in the not-a-wat talk! ;) 
Why is it a wat? Do you disagree with the definition given in the talk, or with the argument that it's not a wat (by his definition)?
Nice designs - but shipping to the UK is extortionate - more than twice the price of the decals.
Ah, okay! It makes *much* more sense now. :) Apparently, my naive intuitions about how these things might look *were* on the right track (I'm a little surprised, even); it's "just" that their other legal interactions cause big problems. `newtype ∞ a = ♯ { ♭ :: a }` quite resembles `data Lazy a = Thunk { force :: a }` to me, which you might have as a built-in in a strict version of Haskell as well :) &gt; More extensions could certainly be written in order to allow non-problematic combinations such as `Fix ListF` and to disallow contradiction-causing combinations such as `Fix (\A -&gt; A → ⊥)`. I wonder -- the issue seems to be that `Fix` is only "meant" to be used with `f`s which are `Functor`s, or in other words: covariant, but this is not enforced (in Haskell's case, because there's no real reason to). Perhaps not coincidentally, this seems to be the same property which the positivity checker exists to uphold. With that in mind, could we put some restrictions on `F` in Agda to make it safe again? (Can we do it so that Agda can *see* that it's safe, or do we need to tell it to trust us?) Can we require a proof that `F` only uses its argument covariantly -- and would this be sufficient? (I seem to recall reading that positivity is a stricter condition than covariance, and required for a good reason, but not the reason.)
Alright, so having moved the reactive code over to work with blocks of 64, it runs in about the same amount of time as the other version. Thanks!
Maybe folength to distinguish it from the `Functor` prefix in `fmap`?
Sure. I'll be there next weekend, Nov 6-9th, at the GSoC mentor summit and I'm going to bring a bunch of stickers for the sticker exchange they do each year anyways. Ping me privately with contact info and I should be able to hand you a bunch when I'm there. I land at SJO and will be around in the Mountain View area for the weekend.
&gt; `Foldable` singles out the last component of he tuple to fold over, and that's the surprise. How is this surprising at all. It has to pick one component, that whole idea that it could somehow work on all components of all type parameters in any multi-parameter data type but only if the parameters happen to be of the same type is a very weird one.
I think the [hs-cordova](https://github.com/mtolly/hs-cordova) approach will be the path of least resistance--compliling haskell to javascript with GHCJS and then compiling that to hybrid native mobile app with cordova.
This would be what if want as well. In fact, I would like it to be forbidden to define a Foldable instance for tuples (and Functor, as well), so that nobody can define conflicting instances. The error message could suggest anybody who really needs it (or rather, a specific one of the several possible instances) to use a newtype. This has already been suggested and discussed here: https://reddit.com/r/haskell/comments/3qcg2d/proposal_forbidden_instances/ Unfortunately, it didn't get much traction.
I didn't... are you a haskell programmer? if so, what's the GHC version you think is best? most of this new stuff just seems to be more Haskell-like: rich types, more safety. 
does it run on their server or in a client? I wish haddocks supported that, perhaps with ghcjs, so hard! 
It's just a link to play.rust-lang.org.
Yeah, my one big complaint about learning Haskell is it sure feels like you need a masters in mathematics to get much beyond basic tutorials. 
Awesome! Just before I saw your comment, I ordered two of them (one for my work laptop and one that I'm going to give away). I'll write a nice review for the product if they turn up good.
I suppose there's at least two reasons for using Haskell: 1. To do practical software engineering. 2. To play with GHC's sweet type system and try new things. This post is all about number 2 :) If you don't understand it, don't worry! It's incredibly esoteric and not at all required for number 1. That said, I really would like for it to be more accessible. I suffer from that classic problem: I understand it, so I can no longer imagine what it would be like to not understand it. I welcome suggestions.
Recursion schemes in the style here are done with regards to initial algebra semantics, which in turn is certainly described via category theory. The wikipedia page on catamorphism is pretty straightforward in this regard: https://en.wikipedia.org/wiki/Catamorphism
&gt; -- GHC is watching you. &gt; 5 = 2 + 2 &gt; And it compiles! Though typing 5 in ghci still gives me 5. That's because 5 is a constant literal in the pattern match and not a free variable and therefore doesn't introduce any new binding. The value of the pattern match is technically `_|_`, but because top-level patterns are lazy, it doesn't trigger. It's similar (not exactly the same) as `case 2 + 2 of ~5 -&gt; ...`.
Haskell lets you do more, but it doesn't make you do more. The most complicated thing you need to get over to be an effective Haskell developer are probably monad transformers. And you only need to know how to use them. Not how the work.
Why, of course. You wouldn't represent an infinite series with a mere list, though. would you?
Thanks for the tip! The changelog is great. :-)
Don't let your confidence be "completely destroyed"; the post uses _many_ language extensions and techniques, each with a separate justification. You don't expect to beat the Shaolin monks after taking your first kung fu lesson, do you? Wax on, wax off ..
so I tried this: class SetLike c where view :: forall a. c a -&gt; Maybe (a, c a) instance SetLike Set where view = Set.minView any' p s = maybe False (\(x, s') -&gt; p x || any' p s') (view s) λ time (print (any' (&gt; 5) s)) True Computation time: 2.847892s grrr.... Maybe it needs the optimizer? Yup: import Data.Set import Data.Time.Clock time :: IO t -&gt; IO t time a = do start &lt;- getCurrentTime v &lt;- a end &lt;- getCurrentTime putStrLn $ "Computation time: " ++ show (diffUTCTime end start) return v any' p s = maybe False (\(x, s') -&gt; p x || any' p s') (minView s) s = fromList [1..1e7] main = do time (print (any' (&gt; 5) s)) time (print (any' (&gt; 5) s)) time (print (any' (&gt; 5) s)) $ /tmp/test True Computation time: 2.44787s True Computation time: 0.000003s True Computation time: 0.000002s
It's building the set that's slow, not finding an element &gt; let foo = Data.Set.fromList [1..1e7] -- fast &gt; foo `seq` () () -- slow &gt; any (&gt; 5) (Data.Set.toList foo) True -- fast Sets are spine strict. It has to create the whole set of ten million elements before looking for the element.
The issue is Set construction is not lazy. I'm not sure of a way around this :/
Regarding x = fst(f 100) and y = snd(f 100), wouldn't (f 100) be memorized, or am I missing something?
Ah right, I meant `Just (f v)`. Thanks.
I use Homebrew's GHC 7.10.2 and Stack (`brew install ghc cabal-install stack`). I have NOT been able to use older 7.8.x GHC since upgrading to El Capitan, however!
This does feel like the right approach - I get the feeling that a lot of these frameworks are trying to be all things to all people. A combination of a template setup, a web server, and some database modeling.... I like that Airship only does the http side of this.
That is bad news. But there are rumors that there is some way to build GHC 7.8.4 from source with a modification to use unix-2.7.1.0 instead of unix-2.7.0.1, and that will work on El Capitan. If anyone can provide instructions how to do that and/or share existing builds, that would be awesome.
The Haskell Platform has an installer built for El Capitan that places binaries in the new locations (`/usr/local/bin` rather than `/usr/bin`) appropriate for El Capitan's rootless security. However it only works for 7.10.2.
"wat"s in the original wat presentation can *also* be explained by various reasonable rules. All that coercions and implicit casting rules are concretely defined in the language standard. Yet we call them "wat"s because the consequences of the independently reasonable rules are quite surprising. It is depressing to see some Haskellers fail to even recognize the problems of their favorite language. Even worse, some are so defensive that they end up being apologists.
Excellent, thanks! For 7.10.2, that's probably what I'll use. Now, 7.8.4...
It seems that some people in this thread actually believe that if they admit there are wats in Haskell, all value will be gone from their favorite language. No, it won't. And overly defensive arguments are indistinguishable from being an apologist.
I am absolutely and completely on the side of strong typing and explicit conversion. Other people, however, see the world differently, and I'm sure see implicit type conversion as super handy. I've met plenty of people who like the fact that their programming language has no problems with stuff like myString = myTextBox. Are you sure you couldn't use writer and runWriter to bracket those cases you refer to? 
It was simply choice of some familiar notation.
Yes, I meant just those two wiki pages. Most recently I was looking for some explanation about type family extension, because I wanted to define Unbox instance from vector package. I ended up reading ghc documentation, papers and finally I found https://wiki.haskell.org/GHC/Type_families, but ultimately best thing to do was simply look into source code of vector package to see example.
To be perfectly honest though, I don't want results for symbols cluttering my package name searches. If I want to search for symbols I can use Hoogle or Hayoo instead.
No it most likely would not. 
Just pushed them: https://github.com/cgag/slow-atp/commits/master I think you mentioned profiling somehwere in here, did you use stack and if so what commands did you run to get profiling info? 
You can also install `stack` via `brew`. The package is called `haskell-stack`.
Put -auto-all and -rtsopts in the ghc-options, then cabal-configure --enable-library-profiling --enable-executable-profiling --enable-tests cabal build dist/build/test/test +RTS -prof then look at test.prof 
You can still use stack to install GHC 7.8.4, but you'll need to edit either `~/.stack/global/stack.yaml` or your project's `stack.yaml` file to have `resolver: lts-2.22` before running `stack setup`.
Good question. I flew through there about a month ago, but didn't stay long.
I completely agree with the post. If you are looking for inspiration, i'd strongly suggest the elixir language: awesome docs for beginners, properly styled, and every single method has at least 1 example, tested using doctest. Example:http://elixir-lang.org/docs/master/elixir/Kernel.html
 &gt; {-# LANGUAGE AutoDeriveTypeable #-} Oooooooh That's kind of like discovering, after ages of using vim, that there's `gv`.
 brew install haskell-stack stack setup Works great.
Keep up the good work! :-)
Thanks. Not counting merge commits I still have less commits in haskell-mode than you do!
I've never heard anyone claim frp is non composable before. The whole point of frp is to bring composability to reactive programs.
I think this makes sense based on the definition of `length` and `Foldable`, it's not a wat, I'm quite surprised by all the discussion around it and especially a whole talk on it. *Sheesh.*
Yes, of course, but do these other recursion schemes make sense as well? The abstract in the original paper by Meijer seems to imply that these are for lazy languages only, but its not made clear.
I uninstalled what I could, then re-installed with the new app from the site. I had to rebuild cabal with stack, and then replace the cabal in the app with the newly build cabal, tho.
https://github.com/gregwebs/aeson-applicative
haskell-indentation is pretty good at this point, good enough to make it the default. Moreover the code has been cleaned up and there are plenty of unit tests so that if you [see an issue](https://github.com/haskell/haskell-mode/issues) we will be able to actually fix the problem. 
Guidelines [how to contribute](https://github.com/haskell/haskell-mode/wiki/Contributing) are here: https://github.com/haskell/haskell-mode/wiki/Contributing
I agree it sounds weird, but I wasn't sure what else to call it. I mean stuff like this: data MyRecord where MyRecord :: (Num a) =&gt; { myField :: a , ... } -&gt; MyRecord If you try to update `myField` using record update syntax, the compiler complains.
It just works. There is no need to configure. If you really want to turn some knobs use Emacs customize interface: M-x customize-group RET haskell 
interesting, i'll give it a try, thx 
Can you paste some more of your code? There are some restrictions to record updates which could be lifted but have been a bit tricky to implement so no-one has got to them yet. 
The gist of the problem should be pretty succinctly captured in my reply above to /u/gelisam. As for why I'm running into it in the first place, basically I have structures which I compile down to CSS, and I'm trying to capture the essence of what's allowed while maintaining type safety. And actually, from a theoretical perspective, it works great -- everything I want to construct, I can, and most of the stuff I think shouldn't be constructable isn't; the only problem is the *syntax* I have to employ to work with these structures. I could understand GHC's complaining if I had multiple fields dependent on the same type parameter `a`, but I don't -- even when it only occurs in a single location, I still can't perform a record syntax update. I have to write a wrapper function which does exactly the same thing, except that instead of using record update syntax, the wrapper just builds a new record with all the same fields as the old one except for 1 field. Which is exactly what I would have expected record update syntax to be doing.
Behaviors are Monads, so you can compose then monadically. Events are functors and can be snapshot against behaviors as well as dynamically switch between behaviors. So every behavior and an event is effectively an "frplet", if that helps. Here are some composability examples of earlier frp: http://conal.net/fran/tutorial.htm
&gt; typesafe API based on the conceptual model of OpenGl, but without the imperative state machine. Aims to be as close to the raw OpenGl performance as possible, without compromising type safety or functional style. Daaaamn. This was part of my issue with the post-retained mode API. This is awesome, thank you :)
This looks great! I am working on an OpenGL-based plotting and scene graph package right now. I have been trying to figure out how to enforce compile-time checking between GLSL and given vertex/color arrays in Haskell code. It looks like GPipe could be useful for that. I have also looked into something like `vinyl-gl` for this.
Thanks a lot for very detailed answer!
Likely related to [#2595](https://ghc.haskell.org/trac/ghc/ticket/2595). This [blogpost](http://breaks.for.alienz.org/blog/2011/10/21/record-update-for-insufficiently-polymorphic-field/) recommends that you just create a new object by copying all the old one's fields instead of using the update syntax on the old one. Can you try that out and report back whether it works?
Thank you :) I should have been more clear about that. I named them intentionally, to follow their general paradigm, but should have been more clear.
What version of GHC?
the "app" version that just came out, for GHC 7.10
I like the idea here, but the type signatures are so damn complicated. [5 type parameters = no bueno](https://hackage.haskell.org/package/GPipe-2.1.3/docs/Graphics-GPipe-Context.html#t:ContextT).
It's working fine so far for me. Obviously I haven't tried compiling all possible programs though. Additionally a GHC 7.8.5 is coming to address this.
See the reactive banana examples for composition of widgets.
What you have _is_ an existential type. MyRecord :: forall a. Num a =&gt; a -&gt; .. -&gt; MyRecord notice the `a` is gone from `MyRecord`. It is a bit easier to see if you tuple up all the arguments to that and slide the forall into negative position. notQuiteLegal :: (exists a. Dict (Num a), a, ..) -&gt; MyRecord However, even if the `a` occurs in the `MyRecord` type so that there isn't an actual existential around, you'd have the problem that updating `myField` is also updating the dictionary for Num, which I'm betting the current record sugar isn't smart enough to do.
Nice! Is there a story yet for dealing with multiple cases / ADTs?
Wow, looks really interesting! Thanks
I'm just learning Hughes Arrows from a cat thy perspective, are there any interesting interactions or relations between this and Control.Arrow ?
This advice is everywhere and it will take some time to remove it. I've updated the README, thanks for heads up.
Have you considered using lenses? There's a learning curve associated with them, but you'd be able to scuttle the whole, unpleasant record syntax altogether. Instead of `s{field=val}`, you could write `s &amp; field .~ val`.
Ok, you're definitely right -- it's an existential. Not sure what I was thinking, but after writing it in Agda where I was forced to state and apply the type variables myself, I can definitely see what's happening here in Haskell: &gt; However, even if the a occurs in the MyRecord type so that there isn't an actual existential around, you'd have the problem that updating myField is also updating the dictionary for Num, which I'm betting the current record sugar isn't smart enough to do. It's actually a bit worse than this. The problem exists even if you remove the `Num` and quantify over any arbitrary `a`, and moreover, exists *even if you use the same type `a` in the update*! I was able to do this in Agda, however, because Agda cheats -- having `myField1 : ∃ λ (t : Set) → t` *can* be updated with record syntax, but you *must* write the type yourself and you cannot make it implicit. Unfortunately, I don't think there's any way for me to forcefully inject a type in a record update in Haskell, and while the update syntax could certainly be a bit smarter than it is (I was fairly shocked that I couldn't even perform an update using the same type), I can definitely see why the more general cases are incredibly daunting.
This is great! I didn't know about this library until now, but I'll certainly be using GPipe for any graphics programming in Haskell from now on. I just thought I'd point out a small typo in part 2 of the tutorial: &gt; The soul purpose of this text... should be &gt; The **sole** purpose of this text...
TIL it's possible to mix record syntax and GADT syntax!
7.8 is known to be busted. i'm happy to cut an unofficial 7.8.4+patched unix package build if folks really really want me to.
Thanks! Fixed!
https://wiki.haskell.org/Phooey This specifically has w-&gt;w-&gt;w functions (liftA2 (+) is one).
This looks pretty good. One of the things I found difficult about GL is that there's an accepted "new" way of doing things, but all the resources to learn it assume you know the "old" crappier way.
I'm not sure if you mean "when programming with audio" in the specific context of reactive banana or in general. In general I don't think it's good advice to say "don't process individual samples, use blocks". It means you can't have proper feedback loops and thus can't implement filters except as primitives. I agree that compiling the code to one or more efficient loops is necessary when processing per sample, but I don't agree that it will necessarily hit a performance wall once things get complicated. Reaktor does pretty complicated per sample processing without performance problems.
I looked for a solid delayed jobs library a few months ago and couldn't find one. Glad to see there's a new project attempting to fill that piece of the ecosystem. Good luck!
As a general bit of advice, Hoogle is very useful here as the search ["ByteString -&gt; Text"](https://www.haskell.org/hoogle/?hoogle=ByteString+-%3E+Text) returns the desired function.
There is an example in the repository: https://github.com/dbp/hworker/blob/master/example/src/Main.hs There are also a lot of examples in the test suite: https://github.com/dbp/hworker/blob/master/test/Spec.hs 
Inspired by the article: what is the headache that monads solve?
Functors, Applicatives and Monads all seek to solve the problem of having a 'wrapped' value and wanting to do something to the value inside the wrapped value. Examples of wrapped values might be `Maybe a`, `[a]`, or` IO a`. They all contain a value (that `a` parameter) but you can't just apply a function `a-&gt;a` to them (because that would apply it to the whole wrapped value) Functor allows you to take a function from `a -&gt; a` and apply it to the value inside a wrapped `a`, without breaking the 'rules' of the wrapping value. Applicative allows you to take a wrapped function and apply it to another wrapped value Monad allows you to take a function which takes an `a` and produces a wrapped `a` and apply it to a wrapped `a`. Without these three you'd have to write custom unwrapping code for each wrapped value you have, or just make all your functions really tied to the wrapped value they're operating on and you'd have a lot of repetitive wrapping and unwrapping in each function.
I'm not sure how new you are, but if you're interested in working on adding examples, it's often quite easy (there's lots of low-hanging fruit for popular libraries). I have a list of libraries that could use examples in their Haddocks [here](https://github.com/MaxGabriel/Haskell-Documentation/issues), though if there's something you already have in mind to work on that's even better. Usually a PR consists of typing examples into GHCI and copying them into the Haddocks as examples (e.g. [this PR](https://github.com/haskell/vector/pull/101/files)).
That just means you can't auto-derive the lens. You can still write one by hand - it's not actually a hard task.
You are quite correct -- `Monad m` and its instances like `Maybe` and `List` are *not* the same thing: many languages have `Maybe` and `List`, but relatively few have `Monad m`. So what exactly does `Monad m` give us? The ability to write code which works on any monad: specifically, things like do-notation and sequence. You'll notice in languages like Swift, they have a wimpy sort of do-notation (`if let x = y` where `y` is of type `T?`) which works *only* with the `Maybe` type. A Haskell programmer looks at this and says "Oh, that's just hard-coded do-notation for Maybe." How inconvenient!
Sure, and I described this in my original post. From a theoretical perspective, of course it's trivial; from a practical perspective, the number of times I have to do this is on the same order of magnitude as the number of CSS attributes. It's almost shorter for me to write a program to generate the Haskell code myself.
Right. I'll try implementing this in Elixir, would help me understand.
You could also just call `stack setup 7.8` if all you care about is the installation of GHC.
Here's [another classic](http://worrydream.com/LearnableProgramming/) :)
Thanks for the offer! We do need to support our released software versions for more than just a month or two. It seems to me pretty basic that a major release of a compiler will be supported on major OSes for at least a couple of years, and I believe that's the intention of the GHC team. The question is - can we decide for certain if it's really needed? If /u/glguy is right, then it's not. But we need to know 100% for certain: * A GHC binary fully works on El Capitan when compiled against unix-2.7.0.1 (say, on Mountain Lion) and used with an El-Capitan-compatible cabal. * That GHC binary can compile programs using unix &gt;= 2.7.1.0, even though that version of unix is not the same as the one GHC was compiled against. My problem is that it's not good enough for us to say "It's probably OK, let's try it for a while and see." Because six months from now we could get a sudden emergency bug fix, and at that point we can't afford to discover a GHC showstopper bug in 7.8.4.
&gt; It's almost shorter for me to write a program to generate the Haskell code myself. You can always try to modify Lens TH code to generate lenses in a way that typechecks.
It is 36 cores with HT and Windows is showing 72 in Task manager. The problem is not in the application performance. The problem is that it could not fill all 100 percents, but half of it.
Nice explanation! It employs the headache-first approach from the blog post with a real use case. It's also very clear IMHO and suitable for Monads beginners.
Very nicely written. Now what's the *headache* lenses solve?
Navigation through nested data structures, and manipulation. If you have a data structure data Car = Car { engine :: Engine , chassis :: Chassis , tires :: [Tire] } and data Engine = Engine { oxidizer :: Oxidizer , vBelt :: VBelt } And you wanted to change the v-belt of the `Car` you had to write a function change :: Car -&gt; VBelt -&gt; Car change (Car (Engine o v) ch ts) v' = Car (Engine o v') ch ts v' So you just want to exchange `v` with `v'` but you have to match a lot of stuff to get there. 
A more esoteric example of a monad can be seen in the ';' operator in java. It takes two statements and glues them together to form another statement: x = doX() ; y = doY(x). You can imagine the ';' operator needs to check if exceptions occurred in the first statement and only executes the second statement if everything went right (considering stack unwinding and frames an implementation detail). In Haskell land ';' is roughly &gt;&gt;= and while in Java you can't program your ';' in haskell we can do exactly that. It's in some sense overloading function composition (as a simplfication at least). 
So, that's the thing. As /u/sclv pointed out, the discussion about a new 7.8 release that works on El Capitan seems to have died out. Carter has now said that he might be able to do it - which would be really great! I sure *hope* you didn't try compiling all possible programs. :) But I think it might be possible for someone who knows more than I do about the new Apple security policy on El Capitan and how that affects the unix package to say something intelligent about whether your suggested workaround could be expected to work. Or whether there is some common case that will obviously fail.
I am Australian but I have been using this time from work to travel so I am currently in Taiwan. I am quite willing to relocate for a suitable position though.
In the second part of the series, the usefulness of algebras are explained. For people who are wondering if they can use the values of an algebra locally as the input to another algebra in that same node (of from the top down again), I'd like to point them towards [Attribute Grammars](http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarSystem), which are a very nice tool for this sort of thing. [Here's the "calculate sum of integers" example](http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarManual#Example_1:_Calculate_the_sum_of_a_tree_of_integers), not very different from using a "regular" algebra. [In this example, the sum is calculated, and then is used to transform the AST](http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarManual#Example_5:_Two_45pass_traversals). It's also possible to use the calculated sum of a `Node`locally and place the result in that node. Behind the scenes, anything that requires multiple passes is rigged up automatically. The authors of the UUAG system also implemented the Kennedy-Warren algorithm to make such dependencies more lightweight.
first! fibonacci = 0 : 1 : zipWith (+) fibonacci (tail fibonacci) explanation will follow soon :)
I think there are many, many, *many* gains to still be had in the IDE department. Better refactoring, visualization of data transformations (a stack build instead of a stack trace, so to speak), etc.
do you need one?
Sorry for a silly question, but could you please explain why would this law `omap f . omap g = omap (f . g)` would not be a requirement to obey? I mean: λ let f = \(k,v) -&gt; (v,k) λ let g = \(k,v) -&gt; (k,v) λ map (f . g) [("k1", 1), ("k2", 2)] [(1,"k1"),(2,"k2")] λ map f (map g [("k1", 1), ("k2", 2)]) [(1,"k1"),(2,"k2")] Seems like this law is still a perfect fit here. 
Okay, but what is evaluated up until now is in the stack and it is linear if I'm not wrong? 
Is a step forward. And because it is an step forward and it is not on the ortodoxy, it has been completely obviated. This is, unfortunately, very common in the Haskell comunity. The bulk of the haskellers, specially the ones that make more noise, are dilettantes that just want to play with problems and stay in the wave of the moment, but don't want solutions, because a solution would end their game. They adore their models. If the model does not fit the reality, it is a problem of the reality because the model is so beautiful and it uses that mystified math jargon that is so interesting. Concerning phooey, it does not use standard Conal FRP. That is very good. It has widget composability. That is great, but as you can see, it is not w -&gt; w -&gt; w but is of the kind: Widget-&gt; Widget -&gt; UI and UIs can not be composed. So it is not possible to create complex widgets out of simpler ones and so on (if that would be the case, it would be said explicitly since this is the most important achievement necessary for the componentization of user interfaces) But it is too close to be a solution, so, as sad as it seems, it has been obviated by the haskell community. 
The Haskell community needs more material like this, particularly for those who finish learning Haskell basics.
Running a cellular automata defined as a rule of type `(g -&gt; a) -&gt; a` where `g` is a monoid representing the space on which the cellular automata runs and `a` is the type of cells. run rule = iterate $ memoize $ \ conf g -&gt; rule (conf . (g &lt;&gt;))
For the people that are too lazy to run it: Prelude&gt; Control.Monad.filterM (const [True, False]) [1, 2, 3] [[1,2,3],[1,2],[1,3],[1],[2,3],[2],[3],[]] 
From https://haskell-servant.github.io/posts/2015-08-05-content-types.html `handler = return` This is the runtime code you have to write in order to build an image conversion service using Servant. It's not a self-contained example, because it relies heavily on other libraries, but also a type annotation. However, I chose this because I think it highlights an aspect of Haskell programming that almost no other programming languages I can think of have, and that's the ability to infer actual runtime code. What's happening here is that through type classes the actual "stuff to do" is being inferred purely from the types. That to me is truly mind blowing.
Maybe a small one, but I thought it was cool: liftM2 (==) Can be used to check if two functions do the same thing on the same input. For instance: quickCheck $ liftM2 (==) sum (foldr (+) 0) Will check that `sum` and `foldr (+) 0`do the same thing on a list of numbers.
I completely agree. Try to build something on top of the GHC API, there are no examples anywhere and the library is huge. Your head will explode before you can get working code. On the other hand, haskell-pipes and related packages have a tutorial module. This is a compromise (it does not feel very good to have a module with no code in it) but it's better than nothing. I think Haddock should really support a way to add pages that are not API documentation.
This is the one I like, but it's tough because you cannot appreciate it until you have the freedom to do it. It's not much a headache as a "leaving the matrix" moment.
 readMany = unfoldr $ listToMaybe . concatMap reads . tails Example usage: Prelude&gt; readMany "This string contains the numbers 7, 11, and 42." :: [Int] [7,11,42] 
Hi Heinrich! Congrats on the release! I don't think the FRPNow interface precludes a push driven optimization, working on it :)
#LÖB UP! https://github.com/quchen/articles/blob/master/loeb-moeb.md &gt; Feeling smart? Let's change that, here's `loeb`: &gt; &gt; loeb :: Functor f =&gt; f (f a -&gt; a) -&gt; f a &gt; loeb x = go where go = fmap ($ go) x
I also remember someone doing something like starting with the fibonacci numbers and mapping them back to the natural numbers or something. Something to do with corecursion, I think? 
woah
I sure hope it's not precluded. :-) I'm just a little pessimistic because of the "one-time events" as opposed to event streams. My reasoning is that whenever an event is added to the graph, the priorities of its parents have to be recalculated. Now, if an event stream is represented as a nested structure of one-time events, then this is equivalent to adding a new event to the graph (and discarding an old one) whenever an event in the stream occurs. Hence, the priorities of all parents have to be calculate, which is the same as looking at all parents in a pull-based manner.
Not to detract from the rest of your point, but ``(compare `on`)`` is now in the libraries as `comparing`.
And has been for a while.
Thanks. Yes, absolutely. By "support" I meant that usually the GHC team does try to make sure that there are working builds for current versions of the major OSes going back two major GHC versions. I didn't mean "support" as in "giving me support". A 7.8.* build that works on El Capitan would be awesome.
Also `minimumBy` and `maximumBy`.
You're right, I forgot the `last`.
Thanks for phrasing it that way. Makes me think that "folded" would be a good clear name for FTP's "length".
Your link has an extra `:` at the end
Checking here means testing on a certain amount of arbitrary data.
It was totally a joke. I was trying to think of the least impressive thing I could.
Just try it - but my guess it that you will be a lot slower (at first) - so don't be to disappointed ... aside from this why don't your repeat some of those projects using Haskell and see where it leads you?
Yeah I'll probably end up doing that to practice haskell. I knd of just want to know if, first of all is haskell suited to web app programming (as in will I be getting any benefit fro using it or am I just adding difficulty) and is there a solid set of libraries/frameworks for dealing with web apps (or is haskell still a bit immature)? Thanks for any help btw
And you can take it even further (e.g. n-dimensional spreadsheets) if you change the `Functor` constraint to `ComonadApply`. Video: https://www.youtube.com/watch?v=kpIXiHzH-OY Code: https://github.com/kwf/GQFC
You might find [this survey of server-side programming in Haskell](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#server-side-programming) useful. I personally recommend `servant` which will definitely be a step up from programming in other languages. You can use [this `servant` template](https://github.com/Gabriel439/servant-crud) to get started.
1. Nice! Do you have a reference for that algorithm? 2. Are you saying that it's possible to give a representation of event streams in terms of one-time events that has push-driven performance characteristics (maybe coupled with an incremental top sort as above)? Or do you want to add event streams as a new primitive? In the latter case, I totally agree that it's possible, but that would be what I mean with "the interface precludes certain optimizations", in that the one-time event primitive would not be enough.
Check out yesod and servant. 
I meant in general, yes. As far as I know, blocks are common practice in CSound, SuperCollider and PureData. I think I've seen this in Reaper as well. How exactly does Reaktor do it? Do they use JIT compilation? What I meant with "performance wall" is the following: To write an efficient loop in Haskell, you have to adhere to certain style; e.g. express everything in terms of a single `foldl'`. When the audio routing becomes more involved, it may be easier to use other combinators as well, say `zip`, but then the compiler cannot put it into an efficient loop anymore. Henning Thielemann has written a synthesizer in Haskell that does not use blocks, but compiles everything to LLVM just in time. It works and is faster than lists. Whether it's worth the trouble, I don't know, most of the time, the existing primitives (oscillators, frequency filters, compressors, …) are sufficient.
Here is the simple trick I discovered to figure such things out: Prelude Control.Monad&gt; let f = (ap :: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b) zip tail &lt;interactive&gt;:38:30: Found hole `_' with type: [a] Where: `a' is a rigid type variable bound by the inferred type of f :: [a] -&gt; [(a, a)] at &lt;interactive&gt;:38:5 To use the inferred type, enable PartialTypeSignatures Relevant bindings include f :: [a] -&gt; [(a, a)] (bound at &lt;interactive&gt;:38:5) In an expression type signature: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b In the expression: ap :: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b In the expression: (ap :: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b) zip tail &lt;interactive&gt;:38:37: Found hole `_' with type: [(a, a)] Where: `a' is a rigid type variable bound by the inferred type of f :: [a] -&gt; [(a, a)] at &lt;interactive&gt;:38:5 To use the inferred type, enable PartialTypeSignatures Relevant bindings include f :: [a] -&gt; [(a, a)] (bound at &lt;interactive&gt;:38:5) In an expression type signature: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b In the expression: ap :: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b In the expression: (ap :: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b) zip tail &lt;interactive&gt;:38:44: Found hole `_' with type: (-&gt;) [a] Where: `a' is a rigid type variable bound by the inferred type of f :: [a] -&gt; [(a, a)] at &lt;interactive&gt;:38:5 To use the inferred type, enable PartialTypeSignatures Relevant bindings include f :: [a] -&gt; [(a, a)] (bound at &lt;interactive&gt;:38:5) In an expression type signature: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b In the expression: ap :: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b In the expression: (ap :: (Monad m, a ~ _, b ~ _, m ~ _) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b) zip tail
Awesome. Thank you! :)
wat
Can someone explain u_u
Looks like Allele has added international shipping. Update 1 (Nov. 02): by popular demand, shipping for print copies is now available internationally!
I'm not even sure that the `loeb` type signature corresponds to a true proposition in intuitionistic logic. In particular, I'm not sure that the implementation terminates.
This is like a tree with specific links cut away, and a level can be empty but have descendants. Not that similar to a tree after all.
All that changed is that os x started returning EPERM on some accesses to protected paths when Cabal was checking if it had access to a file. GHC doesn't typically go exploring your filesystem
I read that as `fap fap fap`
[Here you go](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md).
I prefer the former, even so.
`(.).(.) :: (b -&gt; c) -&gt; (a -&gt; a1 -&gt; b) -&gt; a -&gt; a1 -&gt; c` is quite different, it would seem .. 
Cool valuable project! You should emphasize the free/libre/open licensing, not just say "freely available".
It corresponds to `&lt;*&gt;` from Applicative if you want to get some intuition for it. Here it's specialised to the `Reader e a` (or `(-&gt;) e a` or `e -&gt; a`) monad/applicative instance. (Which I think also happens to correspond to the `S` combinator from SKI calculus?)
 &gt; (!!2)&lt;$&gt;Data.List.transpose[show$sum$scanl div(10^2^n)[1..2^n]|n&lt;-[0..]] "2718281828459045235360287471352662497757247093699959574966967627724076630353547594571382178525166427427466391932003059921817413596629043572900334295260595630738132328627943490763233829880753195251019011573834187930702154089149934884167509244761460668082264800168477411853742345442437107539077744992069551702761838606261331384583000752044933826560297606737113200709328709127443747047230696977209310141692836819025515108657463772111252389784425056953696770785449969967946864454905987931636889230098793127736178215424999229576351482208269895193668033182528869398496465105820939239829488793320362509443117301238197068416140397019837679320683282376464804295311802328782509819455815301756717361332069811250996181881593041690351598888519345807273866738589422879228499892086805825749279610484198444363463244968487560233624827041978623209002160990235304369941849146314093431738143640546253152096183690888707016768396424378140592714563549061303107208510383750510115747704171898610687396965521267154688957035035402123407849819334321068170121005627880235193033224745015853904730419957777093503660416997329725088687696640355570716226844716256079882651787134195124665201030592123667719432527867539855894489697096409754591856956380236370162112047742722836489613422516445078182442352948636372141740238893441247963574370263755294448337998016125492278509257782562092622648326277933386566481627725164019105900491644998289315056604725802778631864155195653244258698294695930801915298721172556347546396447910145904090586298496791287406870504895858671747985466775757320568128845920541334053922000113786300945560688166740016984205580403363795376452030402432256613527836951177883863874439662532249850654995886234281899707733276171783928034946501434558897071942586398772754710962953741521115136835062752602326484728703920764310059584116612054529703023647254929666938115137322753645098889031360205724817658511806303644281231496550704751025446501172721155519486685080036853228183152196003735625279... &gt; (!!3)&lt;$&gt;Data.List.transpose[show$foldr(\k a-&gt;2*10^2^n+a*k`div`(2*k+1))0[1..2^n]|n&lt;-[0..]] "314159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230664709384460955058223172535940812848111745028410270193852110555964462294895493038196442881097566593344612847564823378678316527120190914564856692346034861045432664821339360726024914127372458700660631558817488152092096282925409171536436789259036001133053054882046652138414695194151160943305727036575959195309218611738193261179310511854807446237996274956735188575272489122793818301194912983367336244065664308602139494639522473719070217986094370277053921717629317675238467481846766940513200056812714526356082778577134275778960917363717872146844090122495343014654958537105079227968925892354201995611212902196086403441815981362977477130996051870721134999999837297804995105973173281609631859502445945534690830264252230825334468503526193118817101000313783875288658753320838142061717766914730359825349042875546873115956286388235378759375195778185778053217122680661300192787661119590921642019893809525720106548586327886593615338182796823030195203530185296899577362259941389124972177528347913151557485724245415069595082953311686172785588907509838175463746493931925506040092770167113900984882401285836160356370766010471018194295559619894676783744944825537977472684710404753464620804668425906949129331367702898915210475216205696602405803815019351125338243003558764024749647326391419927260426992279678235478163600934172164121992458631503028618297455570674983850549458858692699569092721079750930295532116534498720275596023648066549911988183479775356636980742654252786255181841757467289097777279380008164706001614524919217321721477235014144197356854816136115735255213347574184946843852332390739414333454776241686251898356948556209921922218427255025425688767179049460165346680498862723279178608578438382796797668145410095388378636095068006422512520511739298489608412848862694560424196528502221066118630674427862203919494504712371378696095636437191728746776465757396241389086583264599581339047802759009946576407895126946839835259570982582262052248940...
How I hate these "thinkpieces"; and the net is swamped by them. Anyway, dear all, do NOT click on that link.
It's the monad form of (&lt;*&gt;). In this case the monad is the reader monad. 
I'm currently in Taichung but I'm heading off to Taipei tomorrow. After that we're off to Thailand for a while.
How does this work? Also, here are some spaces for you: 
It's a bit of a mind-bender, but if you take the signature for `fmap fmap fmap` and specialize both functors to `(-&gt;)`, you get the signature for `(.).(.)`. Edit: more exactly, rename `a` and `b` to `b` and `c`, specialize `f` to `(-&gt;) a` and `f1` to `(-&gt;) a1`.
are these the Knuth up arrows?
We've been golfing down the shortest sequences of code that generate e and pi for a while on #haskell. It is interesting to watch the evolution over time: http://lpaste.net/144465 The e solution started as one based on Jeremy Gibbon's short implementation at the end of http://www.cs.ox.ac.uk/jeremy.gibbons/publications/metamorphisms-scp.pdf That in turn is based on reading out decimal digits from `e` represented as a Hurwitz number. More recent versions generate digits of things on a doubling cube basis, and emit digits shared by both the current and previous step of the algorithm or with various numerical bounds.
Thanks for the information. It's a tough decision, I would prefer to program in Haskell (or similar languages) but if you are purely in it for the money, it seems languages such as Java or Javascript would be far better choices. As far as science is concerned, I'm very interested in learning functional programming since I think there is so much that can be applied to solving research problems. For example, I believe Gabriel Gonzalez has a bioinformatics background and was able to do some really interesting work. I have an idea that I've been sitting on for a while so maybe it's time to take the reins and make it happen for myself.
It's in chinese though, good way to practice if you want :D
Thanks; I still haven't wrapped my head around `(-&gt;)` .. 
Haha, hope the code isn't in Chinese too. My Chinese is pretty poor unfortunately, I can string together a few sentences here and there but that's about it. Still interested in coming along though.
Not at all. `liftM2 (==)` will not terminate if its input functions don't.
Well ap is basically &lt;*&gt; for monad rather than applicative. I'm still not sure how the types for ap zip tail actually work out though, trying to figure it out. 
In the eyes of everyone else - haskell is terribly written. I'm just begging you to have just .00001% self-realization here. Just a single microsecond realize, you're not the only spoke in the machine. Things need to *interoperate*. Code can't just purposely be obfuscated. You gotta make it readable to as many people as possible. How are you being helpful if you just criticize people who can't read your stuff. If you were so logical, why would you forsake 30 years, 10tB of permissively licensed libraries? Why not open your mind a little and relearn *imperative* programmer correctly. Correctly understand how concurrency works in these languages, instead of reinventing the whole wheel to get a concurrency that's *slower* anyway.
No, &lt;*&gt;
I was reading an old paper by Mark P. Jones (“Functional Programming with Overloading and Higher-Order Polymorphism”) in which he was demonstrating an implementation of type unification in Haskell. The unifier's job was to return a substitution function that would transform one type into another. One of the base cases of the recursion was unify TInt TInt = return return The result of unifying two mono-types was a trivial substitution , which would do nothing when applied. The trivial substitution is the function`return`. So the unifier was returning the `return` function as its result. More details: http://blog.plover.com/prog/springschool95.html 
(╯°□°）╯︵ (==) ᄅWʇɟᴉl
One headache I never realised I was having was that there can only be one functor instance per datatype and it doesn't always do what you want. Basic example would be tuples, which have a Functor instance but it's rather asymmetric. `fmap (+1) (1,2)` is `(1,3)` but you can't do the same trick to update the first tupple element. Maybe if we could just specify. `over _2 (+1) (1,2)` is `(1,3)` and `over _1 (+1) (1,2)` is `(2,2)`. The trick is seeing that `over` is basically a parametrisable `fmap`.
I use Vim, and it's amazing in there :)
One issue is the fact that map keys must be unique. Suppose we have: &gt;&gt;&gt; let g = \(_, v) -&gt; (0, v) &gt;&gt;&gt; let f = \(k, v) -&gt; (v, k) &gt;&gt;&gt; let test = fromList [(1, 2), (3, 4)] Now, were we to do... &gt;&gt;&gt; omap g test ... we would have to somehow decide whether to stick with `(0, 2)` or `(0, 4)`. Let's sidestep that problem for now and suppose the result would be: fromList [(0, 2)] That being so, we would have: &gt;&gt;&gt; omap f (omap g test) fromList [(2, 0)] As for the right-hand side of the law, `f . g` is `\(_, v) -&gt; (v, 0)`. That means there is no key collision in our test case: &gt;&gt;&gt; omap (f . g) test fromList [(2, 0), (4, 0)] And so we have a law violation.
You might like http://bir.brandeis.edu/bitstream/handle/10192/30632/FonerThesis2015.pdf?sequence=3
I think you are making two separate points: 1. Functional programmers needlessly obfuscate their code I agree with this point, but I wrote a **constructive** post on how to make Haskell code more readable to non-Haskell programmers instead of non-constructively complaining about it: http://www.haskellforall.com/2015/09/how-to-make-your-haskell-code-more.html 2. Functional programs are reinventing the wheel instead of reusing existing imperative libraries That's not true. If you look at the Haskell ecosystem, a significant number of libraries are bindings to existing imperative libraries. Over time we reimagine these libraries in a purely functional context, but not before we get something else working first. I also don't like the premise that functional programmers never bothered to learn imperative programming correctly. The vast majority of us came to Haskell from an imperative and/or object-oriented background. For example, I programmed almost exclusively in C and Java before learning Haskell. On the other hand, I think you should open your mind a little bit and try to learn functional programming more so that other people's code is more readable for you.
The function `reads` (as well as the probably more familiar [`read`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#v:read)) is polymorphic over the type it is reading: read :: Read a =&gt; String -&gt; a Indeed, calling `read` without a type signature causes a parse error: λ: read "1" *** Exception: Prelude.read: no parse However, the function [`reads`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#v:reads) returns a *list* of possible parses. Here is its type signature (and also an expanded signature by replacing `ReadS`): reads :: Read a =&gt; ReadS a type ReadS a = String -&gt; [(a, String)] reads :: Read a =&gt; String -&gt; [(a, String)] So, when `reads` fails to parse anything, it simply returns an empty list.
You misinterpret either the halting problem or this function, I'm not sure which. This says nothing about what terminates and what doesn't... This just checks over a finite list of inputs whether the two functions return the same results. In this case, the "finite list of inputs" is provided by quickCheck's typeclass on [Int] to produce a bunch of random lists.
This is awesome, thanks.
Well, &lt;*&gt; :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b and when we specialise to `Reader e a` which is equivalent to `e -&gt; a`, we get &lt;*&gt; :: Reader e (a -&gt; b) -&gt; Reader e a -&gt; Reader e b or &lt;*&gt; :: (e -&gt; (a -&gt; b)) -&gt; (e -&gt; a) -&gt; (e -&gt; b) Looking at the types for `zip` and `tail` zip :: [p] -&gt; ([q] -&gt; [(p,q)]) :: e -&gt; (a -&gt; b) tail :: [r] -&gt; [r] :: e -&gt; a where I've bracketed the types and also included the corresponding terms in `&lt;*&gt;` for clarity. Then, we can see that `[p] == e == [r] ==&gt; p == r` and `[q] == a == [r] =&gt; q == r`. Hence, `p == q == r` and so `b == [(p,p)]`. Substituting that all in gives you zip &lt;*&gt; tail :: [a] -&gt; [(a,a)]
Neat, but I'm having trouble figuring out what this might model, or how it might be useful.
 h&gt; let 2+2=5 in 2+2 5 
Who wants to find primes (really slowly)? nubBy (((&gt;1) .) . gcd) [2..]
Loved the one-line analysis. :D
So does it write this for you? Or does it simply let you insect the same information without having to write it explicitly?
Although I love haskell, mostly I agree. It is not functional programming but the superiority complex of many FP programmers what is poisonous. That is due to ignorance which is ever the source of self pride. They don´t even understand what functional programming is: Just programming. And there are rules for that. And I don't mean math rules.
I hope we can get away from rigid definitions of what constitutes a book. Someone such as the author spending three months on something more like a long pamphlet or booklet actually seems preferable to a wordy discourse on programming. Frankly, a series of blog posts that follow a planned structure is appealing to me as it offers opportunities to adjust reader targeting as it goes along.
Category Theorists from Outer Space want to Inseminate your Brain with Lambdas
yeah, for some types, even with one parameter, it's hard to tell what is done with it, and how the behavior changes with different choices. think: Free Maybe Free [] ... I guess I just misinterpreted what they said, seeing it as preferring loads of concrete/different types, which take me a long time to wrap my own brain around. "too damn messy". I guess wanted to say: don't abandon the library because of those type parameters. there's only two tricky ones (besides how obnoxious rank2 types built on it can be to work with), and they provide extensibility / code reuse! the complexity is in the problem, the type just captures that. but yeah, /u/enolan is right to bring it up. the author documents each type, but linking to a few uses or providing specializations (like lens) would be even better. 
why doesn't it also match 1 and 2?
main= dowhatIWant where dowhatIWant is a method in a library that does what I want. awesome ;)
 data Fix f = Fix { unFix :: f (Fix f) } This lets you express infinitely recursive types such as lists inside lists inside lists inside lists. What's quite interesting is that if you say something like `Fix ((,) a)`, then you simply have an infinite list with `(a, (a, (a, (a, (a, (a, (a, (a, (a, ... )))))))))`. If you throw in an `Either` into you that, you can represent finite length lists.
The Yesod book is the easiest and fastest way to get off the ground and building stuff with Haskell. Database access and migrations, authentication, templating, etc. are all built in to the framework's default scaffold, and lets you get right in to building views, routes, models, and application logic.
let's agree to call that one the "owl" operator. or anything else.
I blame you for my aneurysm!
 (\x -&gt; 2*x + 3) 5 does indeed equal 13 (no snark, I had to double check). AD is crazy. this is the future of math class. 
There are certainly cases where it doesn't terminate. loeb [(! 1), (! 0)]
because of tails I presume
haha (+) 2 2 = 5 is like redefining (+) x y = 5 but fails on every other input
 flip ((╯°□°）╯︵ (==) ᄅWʇɟᴉl) == liftM2 (==) ノ(^_^ノ)
Wrong, it just defaults to ```()``` in *ghci*: λ: read "()" () 
&gt;are propositions things that terminate? Nope. I'll unpack my comment for you! A proposition is, roughly speaking, a statement or sentence in a logic. Propositions can be true or false, and sometimes they can be many more things, but not in the situation we're concerned about. Under the Curry-Howard correspondence, types correspond to propositions, and programs of a given type correspond to proofs of the corresponding proposition in some constructive logic. In particular, function types correspond to propositions about logical implication. A function "int -&gt; int" is a proof that the existence of an int implies the existence of an int. (Obviously true, and easy to prove; `f x = x`). In constructive (or intuitionistic) logic, a proposition is true when there is "evidence" for it. For example, the proposition "there exists an integer" has evidence of the form "4 is an integer". Falsity in constructive logic is typically expressed as something like "given any propositions A and B, A implies B." This leverages the [principle of explosion](https://en.wikipedia.org/wiki/Principle_of_explosion) - from falsity, you can derive anything. But the above proposition corresponds to a program which is easy to implement: `f x = f x`. Just recurse endlessly, and never return anything. Functions which never return anything (in other words, those which do not terminate) can be used to implement literally any function type. So when we discuss the Curry-Howard correspondence, we only look at functions which do not diverge, e.g. functions which terminate. And when we write Haskell functions, we usually want our function types to correspond to intuitionistically true propositions, because otherwise we're talking about a function which may or may not return. (Very bad - in most circumstances.) I've been playing fast and loose here, and no doubt that someone will correct me. But this is roughly how the field is laid out.
You should do the second thing you suggested. Also, just to be pedantic, the following statement: data CardValue = Ace | King | Queen | Jack | Int Is invalid. You would need to write: data CardValue = Ace | King | Queen | Jack | Face Int using `Face` as a data constructor. **But you totally shouldn't define cards like that**. The second option isn't is bad as you think it is. It's actually a very good choice. If you reorder the data constructors, you can using haskell's `deriving` feature to great effect: data CardValue = Two | Three | Four | Five | Six | Seven | Eight | Nine | Ten | Jack | Queen | King | Ace deriving (Eq,Ord,Enum,Bounded,Show,Read) Woah! Look at all those free function you just got. Now we can write: ... Pretending I'm in GHCi ghci&gt; Jack &gt; Five True ghci&gt; compare Four Ace LT ghci&gt; show Nine "Nine" ghci&gt; map (compare Four) [minBound..maxBound] [GT,GT,EQ,LT,LT,LT,LT,LT,LT,LT,LT,LT,LT] ghci&gt; allCardValues = [minBound..maxBound] :: [CardValue] ghci&gt; allSuits = [minBound..maxBound] :: [Suit] ghci&gt; allCards = Card &lt;$&gt; allSuits &lt;*&gt; allCardValues My point is that haskell is extremely well-suited for solving the "cards in a deck"/"poker hand ranking" problem. Let me know if you have any questions about what I just presented.
&gt; readMany = unfoldr $ listToMaybe . concatMap reads . tails Because `listToMaybe . concatMap reads .tails` returns `Maybe (Int, String)` in which the Int is the next integer and the String is the remainder. `unfoldr` will then recursively execute over the `String` part of the result: listToMaybe . concatMap reads . tails $ "First number 42 second number 21" Just (42," second number 21") Notice I omitted the type signature on `reads` for readability, don't copy-paste the above. 
We regularly hire people as Haskell programmers if either (A) they are strong at Haskell or (B) they are strong professional software developers or devadmins and keen on learning Haskell. Scientific skills are plus, especially if that means high-powered math skills. We see a number of companies allowing non-Haskell developers to learn Haskell on company time. But the key is to have some skill that's marketable on Day One while you learn additional skills.
Are you sure that first way is invalid? ghci doesn't complain when I stick it in a file and load it (7.10.2) At any rate your method is much better. Thanks! edit: OH wait I see what you mean. It actually created a new constructor named "Int", not a constructor that takes an int value. Thanks! Getting those confused is pretty bad.
I'm using 7.10.2 which is what stack installed for me.
Add to that: - Hard realtime, where you need to fully control memory allocation and automatic GC is basically a no-no.
This one caught my eye: -- all combinations of letters (inits . repeat) ['a'..'z'] &gt;&gt;= sequence 
It's a perfectly valid piece of Haskell, it just doesn't do what you want it to do. data T = Int Constructs a new algebraic data type, with a data constructor `Int`, where `Int :: T`. This Int is not a type, and totally unrelated to the `Int` type used in numbers.
If you don't mind a bit of self-promotion, check out my blog post on [Lazy Dynamic Programming][blog] for an explanation of how this works and a nice interactive visualization. [blog]: http://jelv.is/blog/Lazy-Dynamic-Programming/
Or, put another way, `($)` is just `id` with a more restricted type signature: ($) :: (a -&gt; b) -&gt; (a -&gt; b) ($) = id
Every time I learn something new about Haskell, it feels I discover 3 more things that I don't yet understand, and this has been going on for a while. So for me it never feels like I can say I'm strong at Haskell. How can you tell?
Haha &gt; White the purest of all colors That explains a lot of his hatred :D
that helps a lot thanks! 
Can you talk about how the book will handle the different web frameworks, and which ones you're covering? For something like routing, will it be "Routing in Yesod" + "Routing in Spock" + "Routing in WAI", etc?
The obvious way to implement `butLast` where we take all of a list but the last `n` elements is to compute the length and convert it to a `take` butLast n xs = take (length xs - n) xs but this obviously requires a full pass to compute the length. Here's a cleverer way of doing it butLast n xs = map snd (zip (drop n xs) xs)
Finally someone who understands what flip should do.
My company, [Obsidian Systems](https://obsidian.systems/), builds full-stack web applications using Haskell for both [frontend](https://github.com/ryantrinkle/try-reflex) and [backend](https://snapframework.com).
Interestingly enough, the Spring 2015 homework for UPenn CIS 194 has this: [http://www.seas.upenn.edu/~cis194/extras/07-monads/Cards.hs](http://www.seas.upenn.edu/~cis194/extras/07-monads/Cards.hs)
The same as just about any type class: the headache of writing the same operations over and over again in different libraries. If you already know about monads, you'll be aware that there are lots of functions like sequence and mapM and liftM2 and such which work in an arbitrary monad. Without the concept of a monad, we'd still write many of the same libraries, but for instance, these things would have to be written separately for our parsing library and for our library defining I/O actions, and for lists, and for Maybe, and for libraries like conduit, and so on. There's no shortage of monads. By capturing this pattern which occurs over and over in the course of functional programming, we can write a bunch of things once and reuse them in any case where we can implement the two functions return and (&gt;&gt;=). Even if we conservatively only got 10 or 20 things out of the monad abstraction, that would be enough to justify it, because those 10 or 20 things are multiplied by all the libraries we write which happen to be monads. (It turns out to go much farther than that too.) Most likely in practice, if we didn't have a Monad class, you'd also end up without several of the basic operations you might want in most cases, so it also adds uniformity of expectations when using a new library. Once you know that you're working with some type of computations which is a monad, you know immediately a whole bunch of basic structural ways that you can glue computations together. You kind of need to understand five or six of the examples before you really start to feel the savings in code though. I think it really clicked for me once I understood Parsec, alongside IO and lists and Maybe and a couple others.
It does, but there's also no perfectly balanced binary tree of any size not equal to a power of two. :)
We already have an owl, that looks pretty similar, according to [this](https://wiki.haskell.org/Pointfree) and `(.).(.)` is apparently called dot.
I don't really have anything to contribute, except... {-# LANGUAGE UnicodeSyntax #-} data Suit = Y :♠: Y | Y :♣: Y | Y :♦: Y | Y :♥: Y
You an actually make a data type with `n` constructors using some DataKinds and GADTs: data Nat = Z | S n data Fin n where FZ :: Fin (S n) FS :: Fin n -&gt; Fin (S n) So `S (S Z)` would, for example, be a type that represents the number 2. type Two = S (S Z) We can imagine what sort of inhabitants `Fin Two` would have. `FZ` would be an inhabitant, because `FZ :: Fin (S n)` works, where `n ~ S Z`. `FS FZ` works too, because `FS FZ :: Fin (S (S n))` works with `n ~ Z`. But `FS (FS FZ)` is not an inhabitant, because `FS (FS FZ) :: Fin (S (S (S n)))`, which can't fit into our shape of `Two`. So, you can have type Twelve = S (S (S (S (S (S (S (S (S (S (S (S Z))))))))))) data CardValue = CV (Fin Twelve) And now you can use PatternSynonyms, for convenience, so you can construct and pattern match: pattern Ace = CV FZ pattern Two = CV (FS FZ) pattern Three = CV (FS (FS FZ)) pattern Four = CV (FS (FS (FS FZ))) -- etc. It's easy to define a `cardValue`: finToInt :: Fin n -&gt; Int finToInt FZ = 0 finToInt (FS i) = 1 + finToInt i cardValue :: CardValue -&gt; Int cardValue (CV i) = finToInt i Some libraries like [type-combinators](http://hackage.haskell.org/package/type-combinators) offer these types already :)
It does overload the `(^)` and `(*)` operators (actually just `(*)`, and `(^)` delegates to `(*)`), but it does not do traditional symbolic analysis. Wikipedia has a [page on the technique](https://en.wikipedia.org/wiki/Automatic_differentiation) that you will probably find enlightening.
Nice work. This is a great library and I'm excited to see it making progress. Could you go into any more detail about how this is 'more type safe' than the older version based on functional dependencies? I recall exchanging some emails with Björn about extending dimensional, which I did previously to add a 'currency' dimension for cost calculations (e.g. `costOfSolarPanels = 300 *~ usdollar / square meter` or something). He seemed to indicate that wouldn't be possible in the data kinds version - is that related?
This is not personal. Many people here could have answered like you, but I have to say that responses like this confirm the above said that in general FP programmers have a narrow view of what programming is . They can see the trees but can not see the forest
I didn't install ghc or cabal-install this time around, just stack. I've found I no longer need the separate installs, stack keeps things far tidier.
Ah. I was thinking of balanced Red/Black trees as being "balanced", meaning the left and right branches aren't necessarily identical in size, but differ by at most 1. I think you need dependent types to represent this. See this example in [Idris|https://github.com/idris-lang/Idris-dev/issues/970] if you want a mind-fuck.
You might also like this alternate spelling: [0..] &gt;&gt;= flip replicateM ['a'..'z'] Bump the `0` to `1` in this one to skip that pesky empty string at the beginning.
Finally, the light at the end of the window! :) p.s. those tarballs are all named 7.10.2
Guessed before I even hovered the link. Not the whole answer, but probably the best single lump of info!
Add a lens example to show where it reduces that code.
Great, thanks! We are planning to have a parser for the UCUM language of unit names, but it would be nice to also have one for a more "human" form, and possibly a quasiquoter for values to go with it. We should combine efforts.
Sorry, but I think you've misinterpreted my meaning, so I expanded that in an edit. I meant that when the general public first meet an idea, they'll judge it intuitively first. It can have a rational and consistent explanation, but if that's non-obvious you've made someone confused, giving them an opportunity to dismiss you. The thing is that it's so easy here to drop the counterintuitive by just not oversimplifying the names of these functions to "length" and "maximum". If you just clue people up that you're doing a thing called a fold, they realise that they might not understand what the fold does and accept that foldLength (4,4) could be 1 and that foldOr (True, False) could be False. In exactly the same way, ramanujanSum [1..] = - 1/12 doesn't seem like an idiotic error like sum [1..]=-1/12 does. You conclude it's weird instead of contrary or wrong. Rational and consistent wins in language design, yes, but intuition wins in making first impressions, so esoteric mustn't be labelled "simple and obvious". Spock is right on matters of science, but we don't allow him to name the spaceship. Leave that to someone like Kirk. 
Are you doing mobile apps with Haskell too? If so, how?
I have an awful lot of in uncollaborative, undocumented, incomprehensible code written by my coworkers. Very little of it is written in a functional language. In fact, the most collaborative, documented, and dare I say, comprehensible (at least for what it is) code is written by the ones who know functional programming. Though that is likely simply due to diversity of experience rather than FP per se.
You are generalizing functional programmers as a whole-- centering in on those who aren't smart enough to realize FP is a tool rather than the end all be all. We aren't all like that, and I pity those FPers that don't realize that FP is not a suitable paradigm for every single application ever written. This "article" is not at all constructive, because you come off just as arrogant as you make FPers out to be. All those that I've met LOVE the paradigm, but don't swear that it's the only thing worth anyone's time. Who made you so angry at us?
Our web apps are designed to work well on mobile, and that's our main focus right now. Our team has also worked on native mobile apps in Haskell, using ghc-iphone.
You can recover a sensible Curry-Howard translation inclusive of nontermination by paying attention to *when* things don't terminate. (Here I'm going to talk about the Curry-Howard interpretation I present, since the one for Piponi's `loeb` term is pretty different.) In particular, the modal-logic interpretation of `loeb` gives you nontermination exactly when you give it a "circular" input, which corresponds to a nonexistent fixed point in the logical end of things. Since the existence of modal fixed points is a presupposition of the `loeb` proposition, we get nontermination (falsity) out when we give `loeb` inputs that correspond to falsity. Garbage in, garbage out. Haskell's type system can't very effectively enforce the well-formedness of the input to something like `loeb`, because in full generality, that means it'd have to solve the halting problem—even if we assume that each element of the input functor is locally terminating/productive!—since we can encode Turing machines in the *structure* of the recurrence we feed into `loeb`. I talk about this stuff in more detail in section 12 of [my paper](https://github.com/kwf/GQFC).
Solve the n-queens problem by abusing list functions. let nQueens n = (\ns -&gt; filter (\xs -&gt; all ((\f -&gt; (ap (==) nub) (zipWith f xs ns))) [(+),(-)]) (permutations ns)) [1..n] Example: &gt;&gt;&gt; nQueens 4 [[2,4,1,3],[3,1,4,2]] 
I recommend [Alexey Radul's introduction](http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/) as well.
Another way to implement it would be butLast n xs = zipWith const xs $ drop n xs butLast n = zipWith const &lt;*&gt; drop n butLast = (zipWith const &lt;*&gt;) . drop
Successfully built my projects with both cabal (+ msys2-x86_64-20150916) and stack on Windows (both 32bit and 64bit). The GHC version is 7.10.2.20151030 which allows people testing with existing tools for 7.10.2. 
Hi, author of the above-linked paper here! If you use functions rather than a concrete data structure, you don't get to take implicit advantage of Haskell's lazy evaluation strategy, which means that you'll incur an exponential performance hit compared to a version using a "container-like" `Functor` such as `[]`. You can cheat this problem by transparently memoizing your `(Int, Int) -&gt; a` function, but then even then you still can't memoize the position of the focus (cursor) of your spreadsheet (what the origin (0,0) points to). That is, if you define: moveRight :: ((Int, Int) -&gt; a) -&gt; ((Int, Int) -&gt; a) moveRight sheet = \(x,y) -&gt; sheet (x + 1) y ...then repeated calls to `moveRight` will build up a linear overhead in front of your spreadsheet-function. The farther away you get from home, the longer it will take to examine where you are... And if you define other "movement" functions in the symmetric way, you can go around in circles for an indefinite amount of time, but even if you return the cursor to the original position, you can't get rid of the unary arithmetic which has congealed in front of your indices. (More specifically, `moveRight` and the symmetric `moveLeft` cancel each other semantically, but not operationally.) You can try using the `Functor` instance for a container (say, a doubly nested list) instead, which gets you memoization of results *and* cursor position, but you still pay an asymptotic tax of at least `O(n^k)` more than necessary (in a certain class of cases—see below), where `n` is the number of cells you evaluate and `k` is the number of dimensions in your spreadsheet. When you switch to a `ComonadApply`-based implementation backed by zipper structures, you can finally optimize for the (common) case where the distance to *referenced* cell from *referencing* cell is bounded by some fixed constant. This brings down your complexity to a flat `O(n)` and also enables some fun polymorphism over dimensionality.
This one doesn't just blow *my* mind; it does the same for GHC itself! {-# LANGUAGE GADTs #-} data X where X :: a -&gt; X (X a) = X ()
&gt; named 7.10.2 so that you can test with existing tools (e.g., stack works out of the box).
I think this conversation is a variant of: https://www.youtube.com/watch?v=ysxG5jFeTME To the advocates, `length` can have no more fundamental, useful semantics than`appEndo (fold (const $ Endo S)) Z` that's what you *mean* when you say "length is the count of elements in an object". So, defining `length` this way (modulo performance optimizations) does not change it's meaning in the slightest. The fact that this changes something that was a compile-time error into *the real meaning* is a problem with the *previous* implementation of length, not the current one. And the "solution", as they see it, is education--not a return to the incorrect implementation. That said, the education should be enlightening to the "student", not dismissive, and some of the defenses I've seen (and probably given) are, unfortunately, dismissive.
Data in Haskell is immutable. If you want to change something in one place and have that change be visible in other data you have *not* updated, you need to use IO. Specifically, an IORef or one of the more concurrency friendly versions, which are the type safe version of mutable references.
The key was that these had to run on lambdabot, which let us cheat a few characters as Data.List is in scope for instance, but cost us things that ran too slowly for it to reply with a line of text before the timeout.
I am only in calc 2 wasn't able to understand a lot of that post oh well I can read back over it in a year. 
&gt; The alternative would be to make changeSESalary to change the position of every single employee every time the position salary is updated. Since Haskell is lazy, it'll only make that change when you actually go to look at the value stored there. This is actually much less work than you think is happening. I think you're noticing that your data model doesn't make much sense. Does a position need a salary attribute? Not really -- individuals need salaries. Positions may want to have a base salary, from which individual salaries start from. In that case, it doesn't make much sense to set all salaries in an organization. data Employee = Employee { name :: String , position :: PositionType } data Position = Position { positionType :: PositionType , baseSalary :: Integer } data PositionType = SoftwareEngineer | HR | PointyHairedBoss data Company = Company { positions :: [Position] , employees :: [Employee] }
 setVBelt = set (engine . vBelt) . VBelt for that specific case.
&gt; Control.Monad.Cont There's always a Haskell library you hear about over and over but never have had reason to play with. It ails me.
Maybe not mind blowing, but still pretty cool: take the last n elements of a list (via [stackoverflow](http://stackoverflow.com/questions/17252851/how-do-i-take-the-last-n-elements-of-a-list)). λ&gt; let lastN n = foldl' (const . tail) &lt;*&gt; drop n λ&gt; lastN 5 [1..20] [16,17,18,19,20] λ&gt; lastN 5 [1..3] [1,2,3] λ&gt; Also, the classic split a list in halves. λ&gt; let halves = foldr (\x (l,r) -&gt; (x:r,l)) ([],[]) λ&gt; halves [1..8] ([1,3,5,7],[2,4,6,8]) λ&gt; 
Thanks, didn't realize my leksah was old. Upgraded to 0.15.1.4, but my symbols still show as "unicode boxes" in both source-files and log-output. Also changed the font to the one I use in terminal (and that displays the symbols correctly) but also to no effect. 
Just wanted to leave this here, for people who aren't familiar with Haskell but are stumbling on this thread. Haskell isn't about clever one-liners! It isn't about cute syntax! It's tempting to believe that Haskell is all about clever one-liners because everyone seems to try to "sell" it with them. Even [the haskell.org homepage](http://haskell.org) is guilty of this. Writing good Haskell is about writing expressive yet readable code, and being very clear and explicit. Day-to-day, you won't be writing cute one-liners or having code golf contests with your collaborators. The benefits of Haskell are its long-term maintainability and compiler-guaranteed safety and code correctness, just by the way the type system works and how the language works. You don't get any of this from these one-liners :) Anyways that is all! I know that nobody here said "We love Haskell because of all of this clever stuff, look!", but I'm just saying this here because I know that it's very easy to look at this and think that this is what Haskellers are proud of about Haskell :)
I'm currently using Haskell in a mostly Clojure environment. - **logging**: I use [hslogger](https://hackage.haskell.org/package/hslogger) at the moment and am quite happy with it. - **Zookeeper support**: Using a binding from a C library shouldn't be most problematic. I am not using zookeeper directly but Kafka and it works very smoothly. (I use haskakafka) - **Web or other server framework**: If you need everything battery included I would really suggest [Yesod](https://hackage.haskell.org/package/yesod) or [Happstack](https://hackage.haskell.org/package/happstack-server). If you only need API server (at it seems to be the case) and you really need speed, I'll certainly give [servant](https://hackage.haskell.org/package/servant) a try. But you also have more options: [spock](https://hackage.haskell.org/package/Spock), [scotty](https://hackage.haskell.org/package/scotty), [snap](https://hackage.haskell.org/package/snap), [wai-routes](http://ajnsit.github.io/wai-routes/), etc... You could also write low level api using directly WAI. If I were you I'll do some benchmark, and it would be great for the community to have your results :-). - **Monitoring**: I use [ekg](https://hackage.haskell.org/package/ekg), it is very straightforward but doesn't send data to Graphite. But I found a package for it [network-metrics](https://hackage.haskell.org/package/network-metrics-0.4) that I didn't knew about and I'll certainly give it a try ;-) - **Setting up dev environments**: in one word [`stack`](https://github.com/commercialhaskell/stack). I don't want to start a flame war. Just know my life changed since I use stack. Now all my coworker are eager to work more with Haskell. Using stack is simply a no brainer; it works as you expect. - **Diagnostics, exceptions and stack traces**: All exceptions are handled and logged using classic catching, I never really needed stack traces (I hate them in Java/Clojure). Know handling exceptions in a multithreaded Haskell environment can be really tricky. So you should do your best to avoid them and use the type system as often as you can. Mostly my daemons can live forever, and any exception restart the subpart that created the problem. It was hard to handle them, but now, I completely forgot them. EDIT: By continuing my searches I found [wai-routes](http://ajnsit.github.io/wai-routes/) and [apiary](https://hackage.haskell.org/package/apiary) which also contains benchmarks! How couldn't I knew about them?
Actually, there are more subtleties here. I think it is possible to implement the interface with push-based performance without including event streams as primitives, but I think including event streams as a primitive is beneficial for performance nevertheless :)
Thank you! After playing a bit, I was able to make a somewhat-nice version for Traversable/Foldable with type-families, but somehow couldn't figure out Functor. I accept other points and critique, especially thanks to other people pointing out law-breaking issues. p.s.: just in case you're interested https://gist.github.com/k-bx/12f3027f08e162da26e3
I see, thank you! So, while the law might make sense, it removes too much flexibility from your function and it is easy to break the law if you touch the key. I guess it's powerful-enough argument to not do this thing.
I think I got it. I understand the only way to turn the (+) binary function into a unary one is to pass the same argument twice because we only got that one. Is it correct to say the implementation of `join` is determined by its type? However (I don't trust my own conclusion), how's the `join` implementation: join x = x &gt;&gt;= id fit here? Put another way, what's the bind operator for the (-&gt;) monad (and, tangentially, would I even find it in code somewhere? -- note: [found it!](http://hackage.haskell.org/package/base-4.8.1.0/docs/src/GHC.Base.html#line-621))? Does this mean currying is a monad? Maybe I'll think a bit more about it after work.
Just [disable SIP](http://osxdaily.com/2015/10/05/disable-rootless-system-integrity-protection-mac-os-x/) and it all works just fine. 
Wow, this is insane. Can someone provide an explanation?
The users guide is in the release notes which I'm currently uploading to the release directory.
Feels like the Y combinator all over again
Also, developing standalone desktop applications on Windows can be nightmarish; not because of the language per se but because of the tooling. Even with fancy tools like `stack` I keep running into show-stopping bugs every now and then. (though due in no small part to that Windows in general sucks for developing with the GNU toolchain)
&gt; No, I do not disparage Phooey, I praise it. You specifically disparaged code beauty: &gt; &gt; &gt; because the model is so beautiful and it uses that mystified math jargon that is so interesting What did you mean by that if not to disparage the importance of code beauty? &gt; The merit of the composability off Phoey are entirely due to the author of phoey who managed to introduce rendering (I suppose) as part of the components and de-inversion of control. That is not part of the Conal model. The author *is* Conal. Phooey *is* the typical kind of thing you write on top of Conal's FRP. &gt; Events scoped are not confined to the widgets. It considers a single monolitic reactive domain where everything happens Events and behaviors are pure functions of time. There's no better independence and "scoping" than this, nor better composability than what pure functions give. &gt; That lack of composability of the FRP model is evident in all the libraries that follow it closely. The libraries by Conal, the guy who invented FRP and knows best what it means -- are nicely composable. If anything, other less composable libraries are less composable *because* they don't follow it closely, and if you listen to Conal about FRP libraries, you'd know this. &gt; The merit of phoey is that it depart from it No.
Perhaps with a prism that projects a view with your invariant holding ?
(It won't work in the specific example)
I think the problem is that the first monad people learn about is `IO`, and it does look like it's Haskell's valiant attempt at solving problems that don't exist in any other language. I've seen respectable books like [Concepts, Techniques, and Models of Computer Programming](https://mitpress.mit.edu/books/concepts-techniques-and-models-computer-programming) bashing Haskell for this.
Also known as Decidability and Stability. Will ETA.
/u/BoteboTsebo is refering to Alonzo Churchs version of the theorem, which states that there is no universal way to check the equality of functions.
[Image](http://imgs.xkcd.com/comics/wake_up_sheeple.png) **Title:** Wake Up Sheeple **Title-text:** You will be led to judgement like lambs to the slaughter--a simile whose existence, I might add, will not do your species any favors. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1013#Explanation) **Stats:** This comic has been referenced 1609 times, representing 1.8543% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cwmqe60)
But you can use AD with symbolic numbers. Prelude&gt; import Numeric.AD Prelude Numeric.AD&gt; import Data.Number.Symbolic Prelude Numeric.AD Data.Number.Symbolic&gt; diff (\x -&gt; x^2 + 3 * x) (var "a") 3+a+a Prelude Numeric.AD Data.Number.Symbolic&gt; diff sin (var "a") cos a 
I suggest implementing `showsPrec` and not `show` for `Quantity` so you get parenthesis in these examples... Better yet, derive `Show` and use the [`Pretty`](http://hackage.haskell.org/package/pretty-1.1.3.2/docs/Text-PrettyPrint-HughesPJClass.html#g:1) class for this kind of pretty string.
Yeah. The whole ordering thing sort of breaks down if you're modeling poker (particularly because of how a straight is compared to another straight).
All the same, I've learned a lot of interesting things from one-liners. (e.g. `fromJust`, `groupBy`, etc.) But what you said is, indeed, a very important thing to realise. :)
* `-fno-warn-unused-imports` when I need something to hyperlink in a Haddock comment but I don't use it in the code. * `-fno-warn-orphans` when I need to "patch" someone else's code with instances. I'm not sure what to do about warnings about ticking promoted data constructors, as apparently [types are going to be synonymous with kinds in GHC 8.0](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell/Phase1) and I have no idea how is it going to influence this.
It's the master branch ( https://github.com/apache/thrift ).
I use ghci-ng, it removes unused imports for me as I load the file. 
Done. Each cell in green is a link to the corresponding page in the ghc user's guide, except for some extensions that I couldn't find a link ( I've scraped https://downloads.haskell.org/~ghc/6.10.4/docs/html/users_guide/flag-reference.html).
Your point-free code won't type-check. `&gt;&gt;=` is analogous to `$`, not to `.`, so is not fit for a point-free pipeline. You can use `&gt;=&gt;` instead (which is analogous to `.`).
Take your `Traversable` and use the Identity functor to walk it. What does this give you? If that doesn't pass the `Functor` laws you don't pass the `Traversable` laws either. The `Traversable` laws subsume the `Functor` laws.
This is a big project to take on as a Haskell newb. Are you comfortable with monad transformers, understanding how to diagnose and fix space leaks, and why you'd want to use pipes/conduit rather than lazy IO? If not then you may find that it's a difficult learning curve for building a central piece of infrastructure. re dev environments: [haskell-vim-now](https://github.com/begriffs/haskell-vim-now) is a pretty great set of defaults for Haskell. I hear that Atom has a good set of tooling also, but don't have experience with it. I strongly recommend stack and getting used to a [repl based workflow](http://chrisdone.com/posts/haskell-repl). It's very different from most Java dev experiences. re web frameworks: Yesod is the most mature. Servant is *really cool* but the authentication story isn't settled yet. The performance for Haskell web servers is extremely good -- I don't have any numbers at scale, but all my personal projects measure response time in microseconds.
I admit I've always thought that this would prevent the hyperlinking, but I'll have to try, thanks.
My point was that your example could just as well be written with a specialised `&gt;&gt;=` that works only for `Maybe`. Or, if the language is Java or C, just test for `null` / `NULL`: const char * get_x_of_first_child (struct xml_node * node) { if (node-&gt;node_type == NODE_ELEM) { struct xml_elem * elem = (struct xml_elem *) node; struct xml_node * first_child = elem-&gt;child_nodes; struct xml_attr * attr; if (!first_child || first_child-&gt;node_type != NODE_ELEM) return NULL; attr = lookup_attr ((*(struct xml_elem *) first_child).attr_list, "x"); return attr != NULL ? attr-&gt;value : NULL; } return NULL; } I don't think that any C programmer would consider such code a particular cause of headache, quite on the contrary, it is business as usual. On the other hand, I cannot get rid of duplication in my code without some way to pass `&gt;&gt;=` and friends as arguments to `doXY`. Well, I could do that, but it would be very unidiomatic for JavaScript, and my co-workers would not understand me ;-) You are right that in many cases `Applicative` is enough, but I would argue that if someone knows what `Applicative` is, they already know about `Monad`, and what kind of headache it solves.
Why does the database care about the types you use in your business logic?
There are tradeoffs both ways, the main reason for the philosophy of languages like F# that actively discourage you from writing type signatures whenever you can avoid it runs along two lines of thinking. The first is that you cannot perfectly anticipate how your code will be used by other people and type inference tends to find general types more reliably. Upstream changes likewise won't break working things just because of an explicit type signature. The second is similar to one of the reasons why Haskell was developed as a lazy language. If people start writing explicit type signatures everywhere, you lose some of the feedback and discipline to make sure your types can be inferred. This changes the trajectory of your language's development. While the strict MLs of the world compromise by being more fundamentally imperative in flavor, Haskell has compromised on the type inference front likewise.
Well, given that in, let's say, `C++`, you'd have to write `id` as template&lt;class T&gt; T id(T x) { return x; } it *is* a little mindblowing.
When I think of nested routes, I have something more akin to a picture of yesod's subsites in my mind. Can this library do something like that?
I'd never suspect that some day there are going to be *too many* type-level unit libraries to choose from ;)
I have a feeling I'm starting to sound like a broken record, as I've written exactly the same sentence in a couple of other threads, but you can always make the `CardValue` type abstract, i.e. export only the type name and a couple of smart constructors like two :: CardValue two = Face 2 ... although I don't think it's any better than enumerating the faces as the data type constructors. Actually, this is worse in some respects, for example you can't pattern match on those "smart constructors". A function converting face numbers to cards would have to be partial or returning a `Maybe CardValue` anyway: face :: Int -&gt; Maybe CardValue face i = case i of 2 -&gt; Just Two 3 -&gt; Just Three (...) _ -&gt; Nothing so I don't see how else would you define it, anyway.
Thanks for the tip! I wrote this package when I was still a beginner. I'm sure there is a lot of low-hanging fruit to improve :)
Yeah warp's performance is essentially on par with nginx http://aosabook.org/en/posa/warp.html
I suggest this because; * OP seems too inexperienced in Haskell to pull this off * a solution that still runs on the JVM is surely more palpable to OP and/or peers or pointy-haired-boss * the team can leverage existing Java code in the services if necessary * Akka is built for high throughput scenarios like this * sticks with existing paradigms i.e. wouldn't move to purely functional * clustering is an option for the future 
We're working hard on recruiting people to combine forces, because that will lead to more useful interoperability for everyone. I have dimensional versions of a few libraries that are useful for navigation that I am polishing for release too. atmos now uses dimensional (thanks to Greg Horn!) and my igrf library will be updated to do so this weekend.
inb4 dynamic typing enthusiasts with their snarky remarks that statically typed languages have *finally* managed to solve problems that have never plagued Lisp, for one. 
Well, my [typeful-signal-processing](https://github.com/marcinmrotek/typeful-signal-processing) library that I intend to release as soon as I get some basic functionality working and I'm done with my current deadlines, uses the [uom-plugin](https://hackage.haskell.org/package/uom-plugin). I haven't had time to asses pros and cons of different approaches, though.
&gt;There's no stack traces in Haskell yet, though. You'll have to be prudent about clear error messages. I see people using [GHC.Stack](http://hackage.haskell.org/package/base-4.8.1.0/docs/GHC-Stack.html) in their log output.
The implementation is trivial of course. But doesn't ($) actually have a bit of extra GHC magic? I remember seeing [this](https://www.mail-archive.com/glasgow-haskell-users@haskell.org/msg18923.html) before (correct me if this is out of date)
Cool. I'd need to see an example of it, but that sounds pretty neat. Also, if you're talking about homomorphisms in an article you write, you should probably either (a) explain what a homomorphism is or (b) link to an article that explain what a homomorphism is. In either case, it would be preferable to for the explanation to be in the context of haskell (meaning, don't link to the wikipedia page). I only bring this up because it's not a concept I'm very familiar with. (I've read about it before but it hasn't stuck yet). I doubt that most of your readers know what it means either. Also, I did appreciate at the end where you gave examples of two pieces of code that were not equal without a homomorphism. That was helpful, so I would encourage you to do that again in the future when you're dealing with a concept that is unfamiliar to most people.
believe me, I'd use them if I knew how to make them useful here :p
At Facebook we have [deployed Haskell in a high-volume back-end service](https://code.facebook.com/posts/745068642270222/fighting-spam-with-haskell/) (1M requests/sec). However we have custom solutions to almost all the questions you ask, so I can't really help directly. On the subject of stack traces in production, the DWARF work that will be in 8.0.1 should be a reasonable solution.
This is true for functions with an infinite domain, which is not what we're dealing with here. That being said, as a software developer, if I'm checking if two functions do the same thing, and a test like this shows me that the results are identical for 1000s of different inputs, including the "edge cases", and I can briefly glance at the code to make sure there's no ridiculous if statements, I'll declare that they're the same, remove the duplication, and carry on.
I haven't used them before, but I can see a couple of Haskell zookeeper libraries: https://hackage.haskell.org/package/hzk-2.1.0/docs/Database-Zookeeper.html https://github.com/yesodweb/persistent/tree/master/persistent-zookeeper
That would be true for `showsA = (== "A")`, not `showsA = (== "A") . show`.
It's one of those case where haskell diverges quite a bit with other languages, in that it is really the type which decides what will happen. so when you write something you try to stay at the most general level possible (that reduces the number of possible implementation and gives you more guidance) then at the point of use, the user decide which specific type he'll plug in. it feels a bit strange, how the right part of the typing judgement is seemingly driving the computation, like going from right to left....
If you need a backend that does a lot of transactions, and plenty of those to distributed services either other programs or even on other machines you may want to look at [Haxl](http://hackage.haskell.org/package/haxl), a library written by Facebook, who use Haskell and Haxl amongst other things for spam filtering at scale. Haxl is an efficient abstraction for doing complex data fetching tasks from configurable data sources.
i am lazy
Yeah, the name "monad tutorial" for that doesn't make so much sense, but that last sentence was meant to be a joke anyway.
The version of your comment I currently see says that `(== "A") . show` has type `String -&gt; String` and returns the string `"True"` if given the string `"A"`, but that would be true for `show . (== "A")`, not `(== "A") . show`: &gt; :t show . (== "A") show . (== "A") :: [Char] -&gt; String &gt; (show . (== "A")) "A" "True" &gt; :t (== "A") . show (== "A") . show :: Show a =&gt; a -&gt; Bool &gt; ((== "A") . show) "A" False *edit*: now you've edited both comments and this thread makes no sense :)
Careful with using automatically derived ordering for Poker cards, as what-trumps-what is often defined very differently based a specific game played with playing cards.
In this case, `join` is uniquely determined by its type. This alternative definition is the same as the one I presented; all this means is that, for `(-&gt;) r`, `&gt;&gt;=` is also uniquely determined by its type. The difference is that it's a weird operation that you normally wouldn't care about except for making the monad instance. Look at the type of `&gt;&gt;=`: (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b Substitute and simplify like before: (&gt;&gt;=) :: (r -&gt; a) -&gt; (a -&gt; r -&gt; b) -&gt; (r -&gt; b) What does this have to look like? Again, we could work this out from the type using the same logic as before. It's worth trying yourself before you look the actual definition up. (Again: remember that the operation looks weird. As long as it typechecks and doesn't loop forever, it should be right.)
In `profunctors` we pass both `(a -&gt; b)` and the `Coercible a b`, but the reason for passing the function is simply that it gets you the type to `coerce`. It is passed purely as a proxy. The guarantee comes from the Coercible instance. Using `(##.)` and `(.##)` there requires making up and giving types to naked `Coercion`s all over the place. In theory we could make `Data.Profunctor.Unsafe` a bit safer by ensuring the function isn't used. e.g. by making the instances only supply the `Coercible a b =&gt; p b c -&gt; p a c` and `Coercible a b =&gt; p c a -&gt; p c b` bits and having a small wrapping combinator that used the actual functions to provide witnesses. This might even perform a teensy little bit better when working with an unknown profunctor.
If you are doing signal processing then we might be able to win you over with our forthcoming support for fixed point quantities whose multipliers are statically tracked with a type-level version of exact-pi. It's pretty sweet but I'd estimate it won't be ready for a few more weeks.
Would be even more accurate written as `(.)(.)(.)`, which is equivalent.
great answer!
I think it is possible, but I have yet to flesh it out fully, so I have no arguments yet to convince you :) I'm probably writing a paper on this in the near future, maybe you would like to be a proof-reader at some point? That would be nice :)