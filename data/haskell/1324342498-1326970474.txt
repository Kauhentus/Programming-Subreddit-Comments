I failed to get it working on OS X, Arch, and Ubuntu various times against various versions of GHC. Only once did I manage and I had to remove it because there was a huge delay when typing. Everyone else I know who's tried it has had similar experiences. I'd suggest it's probably better to not waste your time. Leksah does sort of work but it's a bit... "so what". Unfortunately, Haskell still very much lacks the sort of quality IDEs available for languages like Java and C#.
Prolog doesn't determinize the programs you give it. It just runs each of the alternatives in order and backtracks on failure. Also note that the determinization of an NFA can lead to a DFA which is exponentially large. This is one reason why, in practice, it can be more efficient to just run the NFA and deal with the non-determinism instead of trying to determinize it. However, you must be sure to do it right. The backtracking approach (which is how Perl does regexes) can take exponential time, which isn't any better than taking exponential space. Russ Cox has [a nice discussion](http://swtch.com/~rsc/regexp/regexp1.html) about how to implement NFAs directly and efficiently without exponential blowup in either the time or the space.
&gt; I agree that it's unfair in that the haskell implementation is using less efficient data structures. However I would say that it is a fair comparison overall, I disagree. This is like saying it's fair to compare a C implementation of quicksort with a Haskell implementation of bubblesort. The difference in data structures is not at all trivial, it is [essentially](http://en.wikipedia.org/wiki/Isomorphism) the same as using different algorithms. Data structures and algorithms are the same thing, just one is in space where the other is in time. When you can use `ByteString` or `Text` just as easily as you can use `String`, this is intentionally making the comparison unfair as well as invalid.
&gt; the Haskell programs on Language Shootout are almost as fast as they get, i.e. ~2.5x slower than C. No, it's not safe to assume that. Many of the egregiously illegible programs were written a long time ago, and both GHC and our libraries have gotten a lot smarter since then; some of the Shootout programs have been updated, others have not. Also, historically the Shootout has placed severe restrictions on the sorts of libraries you're allowed to use, which places undue burden on Haskell when compared to standard languages like C++ and Java because of how few packages ship with the compiler due to how easy it is to just get them from Hackage.
&gt; Secondly, the only representation that is saved should be the textual source code. Nobody wants to edit source code like its a word document. Au contraire! Programs do not come in lines, they come in ASTs, so why are we using line-based version control? All tools from version control to editing should be based on what the structure of the program actually is. In addition to the issues yairchu mentions, this would also do away with nonsense about preferred textual-renditions of code such as whether the curly bracket goes on the same line or the following line as the keyword that licenses it. All of those arguments are just a waste of time. Bob can configure his editor to render the code the way he likes, and Alice can configure her editor to render the code the way she likes. Neither of their preferences should leak out into what lives in the repository or what gets shipped to the public, because it doesn't matter.
I think the straightjacket comments in Naggum's opinion are a big reason why they fell into oblivion. Of course, what a *proper* structure editor should do is have some way of accounting for malformed substructures, so that you can work in sheds and only ensure well-formedness when and where you're ready (similar to the interactive programming in Agda or Epigram). I'd love to see proper structure editors take over the world, but they definitely have to be proper. In particular, I should be able to edit everything as if it were text, and only on the backend is that stored as proper structure. If I'm forced to use some particular IDE, with its own bloatware and inscrutable way of organizing menus and config files, then the battle is already lost.
As you enable more type features, you increase the set of programs you accept as well-typed. (Type features include ML-style polymorphism, full higher-rank polymorphism, GADTs, dependent types, type families, etc.) In the limit, we would eventually accept every program which could possibly be considered well-typed. In other words, we accept all of the *untyped* lambda calculus, except ruling out specific things we've declared to be ill-typed (e.g., non-termination). But the important difference is that we still have the types! So just imagine trying to give a type to every possible Scheme program which will ensure that it cannot be used incorrectly but also without ruling out any of the correct uses. Now imagine trying to implement the type inferencer/checker that creates and manipulates those types. That's why it's hard to try to boil everything down to Scheme. In general, type features make programs less complex because they enable the types to capture more invariants of the programs, and so we can remove lots of boilerplate and the handling for corner cases. But fundamentally that complexity has to go somewhere, and in particular it ends up going into the compiler and the compiler's type checking/inferencing algorithm. This is a win because it means the complexity can be tackled once and for all (in the compiler) and also because it means we can use types to communicate what our program is about, instead of relying on guess-and-check programming in order to figure it out. But it makes your life harder if you're trying to re-implement the compiler in Scheme. However, implementing the type checker/inferencer for a language with lots of type features does not necessarily give you any insight into what those features are about or what it means to have or lack them. All too often implementations are cluttered up with silly things like how to manage name scopes and proving lemmas like associativity of scope composition/concatenation. Those details are necessary for convincing the computer, but they provide little insight into what we actually care about.
I updated the benchmark with your suggestions :-)
Here's my criticism: &gt; bind is a bit like a semicolon, and return does 'nothing' I'm not a big fan of this statement. Why not lay out plainly what return does: it takes a value and turns it into a monadic action. &gt; ‘is return the same as the imperative return?’ and the answer is both yes and no For the sake of clarity, I'd generally emphasize the *no* here. People should *not* be using Haskell's `return` in any way remotely similar to imperative return; in Haskell it does not "pop the stack" as it does in imperative languages (in fact the Haskell stack is nothing like imperative languages). &gt; The answer is that monads are imperative programming, but in a limited form, and they are ultimately converted to functional programming. Again, I'm not a big fan. I can understand the "programmable semicolon" vibe, but saying monads are a limited form of imperative programming entirely misses the point. (imho "the point" is that you can compose existing monadic actions and monadic functions to create new monadic actions and monadic functions)
The problem is that Bytestrings aren't universally better than [Char], and it's hard for the compiler to guess properly what is the optimal string representation for a particular bit of code. If you're frequently consing single characters onto the head, then [Char] makes sense. If you're copying around large blobs of binary data or text, then Bytestring makes sense. You might want to take a look at OverloadedStrings if you aren't already familiar with it-- it takes most of the pain out of using Bytestrings, so converting from [Char] to Bytestring is just a matter of changing type signatures.
I don't think so. Alternatively, have `last' :: t -&gt; Maybe Char` and `last = maybe '\0' last'`.
...have the numbers improved btw?
*Prolog doesn't determinize the programs you give it. It just runs each of the alternatives in order and backtracks on failure.* And success, i.e. findall or when the user requests more solutions.
I'm still a bit confused... :-) What's the target audience of `shake`; who should use `shake` (instead of what?)? If I understand it right, it isn't supposed to replace Cabal as a build-system (or is it?)? Can it replace an `autotools`-based build-system (which auto-detects various build-env properties)? Is `shake` the Haskell equivalent to [scons](http://www.scons.org/)?
Shake is the Haskell equivalent to scons/make/whatever. If you are building a single language project, use the build tools that come with that language (cabal, ghc --make etc). If you have to write a multi-language build system or require complex generated files, you have to use a general purpose build system. If you're thinking of picking up make/scons, instead pick up Shake. Like scons, you could use it to replace autotools based stuff. Of course, it has a number of advantages over make/scons etc, but the most visible one is that you write the rules in Haskell :)
The biggest gain for Haskell would be compiler integration. The code is repeatedly sent to GHC while I'm working on it, and compiler errors pop up as hints at the problem code. This would cut down on the mental burden of the [save-compile-hunt down line number] cycle.
...so, would the GHC soure-tree be a good candidate for `shake`? :-)
The article says yes, a little.
If Leksah gives me a visual debugger, that's great too... I've not given it enough time to try that out.
Yes, without a doubt.
Where's dons to improve the haskell version?
Looks great! I wonder if I can sneak this in as a replacement for our teams' visual studio -&gt; Makefile system...
There is no way to analyse the code to know how i'm using the data structure? Something like the frequency a function will be used?
I'd recommend waiting a few months, until it's been well tested. If you're doing something to experiment go ahead, but if you want to release a production ready thing in Haskell, the last thing you want to do it end up with a bad rep for Haskell where you work.
Also, a String contains unicode characters. A ByteString contains bytes.
I sent patches to haskell-mode maintainer several times, but no of them were applied to repository. Svein answered, that he'll apply, but this isn't happened for almost a year :-(
Seems like there was a couple of decades worth of research and development. Is all that going to waste?
Hi Neil! *Tiny* error: a backslash needs escaping (`"*.tar" *&gt; out -&gt; do`) in the example [at the top of the docs](http://hackage.haskell.org/packages/archive/shake/0.1.2/doc/html/Development-Shake.html).
Maybe last :: t -&gt; Maybe Char last' = maybe '\0' last Because '\0' would be a lot like null in other languages, and I think we can agree that that is bad.
Thanks for spotting that, now fixed in my development version. Half my haddock code seems to read as @\\/\\/@...
Er, actually I somewhat mis-remembered that. The PDF version of the OpenCL 1.0 spec only states what the (possibly-macro) constants for enums are called, not their exact values. However, I believe that the distributed header files are a normative part of the specification too, and hence the values *are* in the spec, just not in the printable part of it. Meh, still safe. Use the Khronos headers, switch to the platform headers if you get any bug reports about them. ¬_¬
Nope. Note that `undefined` has type `a`; it can be *any* type. You can put it anywhere and when evaluated, it will just blow up, and there's nothing the type checker can do to protect you against it. The best you can do is write in the documentation that `last` must be total, or better yet, have it return a `Maybe Char`. That's what `Maybe` is for, after all. What's the "last" character of the empty string? There is no last character in an empty string; so produce `Nothing`.
I spend a lot of time in leksah. It's a bit buggy but has some very nice features. I still use emacs for small stuff.
Given the triviality of `last'`, I simply wouldn't include it. Anyone who really thinks it's a good idea can just implement it themselves, in the mean time you don't need to afford their behavior.
Haskell has a very rich type system. You should choose the type that best suits the task at hand. If you are manipulating byte sequences use [`ByteString`](http://hackage.haskell.org/package/bytestring). If you are dealing with textual data then use [`Text`](http://hackage.haskell.org/package/text). If you want to write functions that can deal with a variety of different string types, then you can use a type class (the [`ListLike`](http://hackage.haskell.org/package/ListLike) package also includes a `StringLike` type class). If you want to use string literals for other types besides `String`, then you can use `OverloadedStrings`: λ&gt; :m +Data.ByteString.Char8 λ&gt; :m +Data.Text λ&gt; :t "String" "String" :: [Char] λ&gt; :set -XOverloadedStrings λ&gt; :t "IsString" "IsString" :: (Data.String.IsString t) =&gt; t λ&gt; let foo = "foo" :: String λ&gt; let bar = "bar" :: Text λ&gt; let baz = "baz" :: ByteString λ&gt; :t (foo,bar,baz) (foo,bar,baz) :: (String, Text, ByteString) You can enable this for a source file with a language pragma: {-# LANGUAGE OverloadedStrings #-} 
Welcome to the internet, and/or reddit. =)
If an answer to this question exists, I haven't found it yet. I've read the existing documentation and I have no clue how to actually go about binding C++ to Haskell without going through C wrappers, or worse, inspecting binaries. I'll be watching this thread to see if anyone has any good resources for this.
You listed the state of the art: trivial C wrappers over the C++ code. Binding C++ directly is an incredible chore. The ABI as well as name mangling alone will make you hurt (mangling is generally stable in G++ at least, but mangler updates do happen as e.g. C++11 features come into place - they're forwards compatible at least.) C++ itself is not very amenable to foreign language FFIs in the first place because of this. It's why essentially *every* large-scale C++ project exports a C-wrapper API for foreign languages to bind: it's a lot less effort and more robust than binding C++ directly. Certain languages like python can directly 'wrap' C++ and expose the results as python objects - but that's only because the objects are implemented via CPython extensions written in C++, which *themselves* expose a C API to the Python runtime - they are dynamic libraries directly loaded into the interpreter, not FFI bindings (a la `ctypes`.) So really, the boundaries and where you need to expose C APIs are just shifted around. There is no equivalent to CPython extensions in this sense for Haskell/GHC, and that's good (it makes people dependent on internals, and erodes your ability to make significant runtime changes. See the python GIL extravaganza for an example.) Yes, it's sad. If anybody else out there knows of a better solution (code generation?) I'd like to hear it as well.
To see the problem with the 2+2 example, try this: let 2 + 2 = 5 in 1 + 1 
SWIG is used to create C++ bindings in other languages. It takes care of the tedious C wrapper generation and deals with mapping C++ types to e.g. Python types. I don't think SWIG supports Haskell yet though.
 Prelude&gt; let 2 + 2 = 5 in 1 + 1 *** Exception: &lt;interactive&gt;:1:5-13: Non-exhaustive patterns in function + Interesting.
There's 1 big manual script that uses GCCXML in lambdacube's bullet package. http://code.google.com/p/lambdacube/source/browse/trunk/bullet/bullet.py It'd be great if someone ported that to use clang or something like that (GCCXML is quite ancient).
What you've done is defined the following function of two arguments (expressed here as prefix instead of infix): (+) 2 2 = 5 It's the same basic behavior as the your ```fireOn``` example: it defines '+' only on specific input values, and anything else will produce an error. If you wanted to keep the normal addition behavior except for the 2+2 case, you could use something like this: let plus = (+) in let 2 + 2 = 5; x + y = x `plus` y in 1 + (2 + 2) Note that it's necessary to define an alias for the standard '+' to avoid an infinite loop where + is defined in terms of itself. If you were writing this as a program instead of a let expression, you might import + from the Prelude and rename or qualify it, so that you could then define your new + as a top-level function, and either use it within that module, or export it to other modules. *Edit:* as follows: import Prelude hiding ((+)) import qualified Prelude as P 2 + 2 = 5 x + y = x P.+ y main = do print $ 1 + (2 + 2) print $ 1 + 2 + 2 
Yes, although you have to be careful about bootstrapping issues.
This was interesting. Thanks for the benchmark. I am trying to figure out which parts of the HashMap.insertWith is spending all the CPU, but because ghc can only assign cost centers to functions not marked INLINE, I can't get that information. Most of the interesting functions in an optimized library will be marked INLINE. Why is there no option to assign cost centers to INLINE functions?
Name mangling in C++ should be stable. I don't know of a library that exposes it though. I don't think the mangling is the problem - the problem is that FFIs don't want to deal with the full spectrum/power of C++ types. That's why we dumb it down to C types.
Mono project was also announcing that they succeeded with simplifying access to C++ from C#. Maybe some of it could be "reused". http://tirania.org/blog/archive/2011/Dec-19.html
If there isn't such option (not sure) it'd be because it significantly changes structure of program. You can recompile unordered-containers with ghc-prof-options: -prof -auto-all in .cabal file and things will start to show up in profiling info. Quite surprisingly when I changed this code to use hashtable from hashtables package instead of HashMap it got a lot slower. Most costly area is generating pairs and that's what should be optimized first I think even though HashMap can slow whole thing down.
Good point, I wouldn't submit it to them unless it was solid - as I think I'd have a hard enough time selling it if it was completely perfect.
It might be possible for certain trivial cases, but in general you can't tell how many times a function is going to be called without actually running the program. Similarly, if a module exports several functions, there's no way of knowing which functions are going to be called most often. It's usually better to leave it up to the programmer to decide which kind of string is the right one.
This trend didn't really go into oblivion for Lisp (Naggum's baby language). In fact, [paredit-mode](http://mumble.net/~campbell/emacs/paredit.el) is the Lisp programmer's choice mode. It has enough key bindings that writing and editing is more than efficient than editing the text directly. Sure enough, it stops you doing things that would've been easier, like cutting a bit of code out like a sequence of characters rather than an AST, but that's a small price to pay for the overall support.
I didn't figue out how to use paredit-mode. It feels too constraining and even buggy at times. I can't delete empty () pairs, and the comment lines change their indentation levels constantly. What am I missing?
&gt; Of course, what a /proper/ structure editor should do is have some way of accounting for malformed substructures, so that you can work in sheds and only ensure well-formedness when and where you're ready (similar to the interactive programming in Agda or Epigram) Can you show some examples of malformed substructures that you'd want to work with in Haskell? I am collecting use-cases of things a structured editor can't deal with.
&gt; I can't delete empty () pairs Press backspace inside it. &gt; the comment lines change their indentation levels constantly. When?
Thanks for all the answers :) It's a shame that a language powerful such as C++ can't be easily binded to Haskell or other functional languages &gt;.&lt; Even worse, SWIG provides binding solution only for OCaml: I guess the lack of an Object System in Haskell biased the choice towards Ocaml. Thank you, I'll try to see if I can bind only a subset of operations with C wrappers, and I'll keep you posted :)
Same here. It became unusable. EVERY symbol typed in appears after 500ms+ delay. Very annoying. And there is blank project with no heavy src files (only "hello world"-style lines). 2G RAM / Intel Core 2 Duo Ubuntu 11.04, Eclipse 3.7.0, GHC 7.0.3 edit: I dont know the architecture of EFP, maybe some checks may be switched off and it will speed up the thing? edit2: One more thing - the spam of 'build-wrapper-json' in console tab also is not a good thing...
&gt; Name mangling in C++ should be stable. [Oh yeah?](http://en.wikipedia.org/wiki/Name_mangling#How_different_compilers_mangle_the_same_functions)
You can't use extensions the compiler doesn't know about, either.
&gt;All tools from version control to editing should be based on what the structure of the program actually is. Definitely. VCS design can make a hell of a lot more sense if you look at it from the perspective of ASTs. (Of course, my favourite VCS design has this property, so maybe I'm just biased...)
I wrote wrappers for a C++ library by hand. The C++ objects were maintained in Haskell using ForeignPtr, and I did all the haskell overloading manually.
How have you handles templates? I mean, given a function X&lt;T&gt; where T := typename T, how you binded it in Haskell? I hope to have been clear enough :)
I did not handle templates generally. For each function X I wrote many C++ wrapper functions: one for each type T. My solution is not very good, I just wanted you to know that it is possible to have C++ objects in Haskell using ForeignPtr.
Well, just consider the general case of when your halfway through writing up a function. Maybe you're halfway through an identifier name, so the current (partial) name isn't actually valid/bound. Or maybe you're still moving things around because you're deciding how to fit the pieces together in a way you like, so maybe it isn't even a well-formed AST. Heck, even if it is a well-formed (partial) AST and you only look whenever a token has ended, because of infix operators and their differing user-defined associativities and precedences you can never be really sure what the partial AST should even be, since it can change as soon as the next token shows up (e.g., when moving from "3 + 5" to "3 + 5 *"). The point is, there's always some stage in programming where everything's a mess. Trying to impose well-formed structure on the messiness of development in every place and at every time is just doomed to failure, because development isn't universally pretty and well-structured. People are messy. Creativity is messy. But more importantly, we don't care about how people keep their houses just so long as they present themselves well in public. The obvious solution is to consider an interface like Agda's sheds. In general, Agda will try to compile and typecheck everything for you, but sheds are a way of saying "hey waitaminute! don't bother checking this part yet, because I know it's a mess". You do all your mucking around and developing inside of sheds, and when you're ready to show it to the world (i.e., the compiler) then you remove the shed. This way, Agda can check everything that it knows is stable, but avoid things it knows are a mess, and so the checking isn't going to interfere with the messy process of figuring out what the heck you're trying to do.
I sure hope not. In my ideal world code would always live as ASTs (with annotations like types and comments, and with subparts marked as unstructured) and that's what we'd put into version control, what we'd distribute, what we'd edit, etc. And I'm sure a lot of that research would help in making this ideal world come true. 
Ah, yes, of course. 
great write up, thank you!
&gt; Prolog doesn't determinize the programs you give it. It just runs each of the alternatives in order and backtracks on failure. And, for what it's worth, one of the biggest ways of optimizing Prolog programs is to explicitly state where the backtracking should be "cut" (i.e., committing to a particular choice and never allowing backtracking to choose an alternative, thereby pruning the search space). Sometimes this is sound and doesn't change the semantics of the program, it just allows us to explain to Prolog when certain things are actually deterministic because of some knowledge we possess but cannot render into Horn equations; but sometimes it is unsound since it can prune out a correct answer. In both cases, cut is an impure operation and it vastly complicates the denotational semantics of what a Prolog program actually means. Thus, the fact that Prolog uses backtracking search is wired into the language in a way that makes it extremely difficult (if not impossible) to actually determinize the programs you give it.
I love the idea of using GitHub for a blog this way!
Ok, Ty :)
You shouldn't need UndecidableInstances if you implement Add this way: type family AddT a b type instance AddT D1 b = Succ b type instance AddT (Succ a) b = Succ (AddT a b) Also, why no zero-length vectors? They seem to generalize just fine, and it makes your type-level adding clearer; the base case becomes `AddT D0 b = b`.
Whenever this topic comes up, I often wish more emphasis was placed on *consistency*. Introducing new mnemonics and abbreviations is fine, but only if they follow a uniform pattern. Learning such a pattern is a lot easier than learning a whole bunch of unrelated bits of shorthand. I've come to dislike the convention of `m` for `Monad` instances because of this--specifically, how to handle `Applicative` instances. Should they be `f`? That's the convention for `Functor`s, so now the mnemonics are subtly conflating those as "not-monads" in a non-helpful way. Should they be `a`? That's the convention for generic types of kind `*`, which is even worse. I also dislike the frequent assumption (not found in this article) that longer. explicit names are inherently better. Longer identifiers put more information on the screen that anyone reading the code then has to mentally filter through when looking for the information they need. Every time I see an example of supposedly "more readable" code that has `longPhrasesForVariableNames` I have to wonder if the person who wrote the example has ever actually tried to read code like that. It's horrible, almost as bad as single-letter abbreviations, and my eyes start to glaze over just thinking about it.
Haskell just needs to get a less painful polymorphic Prelude where we can use the same functions on various different types.
You can improve on that, if you use fclabels with [SECs](http://conal.net/blog/posts/semantic-editor-combinators): modify . setL fclabel $ newValue modify . modL otherlabel $ (+1) theVal &lt;- gets (getL fclabel) 
Thanks! That's a great fix. UndecidableInstances was bothering me a lot. As I mentioned in the beginning of the post, I just started learning about type families (and the more exotic type-level shenanigans in generals), so the version of AddT in the write up was just the first one I got working. As for zero-length vectors, I had them at first, but I think they made implementing SafeGet more complicated and I didn't see any real need for them.
&gt; ITA software is a Lisp startup Calling ITA a "startup" sounds condescending to me. Quoting their [Wikipedia page](http://en.wikipedia.org/wiki/ITA_Software): &gt; Founded 1996 &gt; Employees 450+
Is there a list of hackage packages that fail to compile with this release?
These are the class projects from CS240h taught at Stanford in this last quarter by David Mazieres and Bryan O'Sullivan, with David Terei as TA. People formed groups of 1-3 people to do a project for the end of the class. Most of the project code is on GitHub, and each group also did a write-up about their project.
On the other hand, I think point-free is almost always more readable if a function can be easily written that way, and that most Haskell code I've seen is pretty close to a good balance. The cluttered, verbose style that seems to be the norm otherwise just seems tiresome and somewhat obfuscating after getting used to typical Haskell style.
The class/instance isn't the cause of the requirement for UndecidableInstances, it's the way you implemented it: type instance AddT (Succ a) b = AddT a (Succ b) in this case, the recursive call to AddT isn't at a smaller type, so the compiler doesn't know that it is guaranteed to terminate. For example, you could have another instance: type instance AddT () b = AddT b () and now `AddT (Succ ()) ()` never terminates. I think you can get SafeGet to work with 0 length vectors if you think more like a programmer: index vectors starting at 0, and use the constraint index &lt; length, instead of index &lt;= length.
Wow, such a cool class. I wish, they make it an online class, like ml-class.org someday.
You could use record syntax data Instruction = Label String | Load { mode :: AddressMode } | Store { mode :: AddressMode } | Jump { mode :: AddressMode } | Call { mode :: AddressMode } deriving (Show) instructionSize (Label{}) = 0 instructionSize inst = 1 + (addressModeSize . mode) inst 
Yeah, that's a terrible way to write it. I admit that "function that can be easily written pointfree" is much too vague, but my first rule of thumb there is "no operator sections of `(.)`". Another rule is "include either all or none for paired/associated/symmetric/&amp;c. arguments", so I wouldn't write it as `dotProduct xs = sum . zipWith (*) xs` either. A third rule is "don't use `curry` or `uncurry` purely to get around the above rules", so `dotProduct = curry $ sum . uncurry (zipWith (*))` is also out. So I agree, the pointful version is clearly the better way here. Besides, the pointfree version isn't even that clever. *This* is the clever way: dotProduct = fmap fmap fmap sum $ zipWith (*)
I don't think so. I fixed a million of them before the release.
I wrote a toy assembler in [TMR #6](http://www.haskell.org/wikiupload/1/14/TMR-Issue6.pdf).
I'm not really familiar with type families, but is there any reason to implement it using type families as opposed to just a third type using MultiParamTypeClasses? e.g. class Add a b c where (+) :: a -&gt; b -&gt; c instance Add Int Int Int where (+) = P.+ I'm guessing there is some advantage to your method or some weakness to my method that I'm not seeing.
It fails to compile network-2.3.0.8. it seems ghc can't find "wspiapi.h" file needed in "HsNet.h". This file isn't included by the mingw in ghc. c:\ghc\ghc-7.4.0.20111219\mingw\bin\gcc.exe returned ExitFailure 1 with error message: In file included from C:\Users\User\AppData\Local\Temp\4796.c:1:0: include/HsNet.h:45:23: fatal error: wspiapi.h: No such file or directory compilation terminated. ghc v7.2 gives me the same error.
Are there any debian/ubuntu packages of this yet? It would be nice if there was an official haskell platform PPA that gets updated with all new HP releases. Given the release cycle in ubuntu and the lag in HP upgrades, I won't get this HP version until it's already old :) Unless I e.g. install from source. But I like to have installed stuff managed by the package manager, and it's nice not to have to compile things...
Dan Burton pointed out to me that [one of the papers](http://www.scs.stanford.edu/11au-cs240h/projects/raja.pdf) mentions problems implementing "global variables" in Yesod. I may not understand what they're going for, but I think this can be implemented by adding the data to the foundation datatype. If the writers read this, and want more information, feel free to contact me.
Keep this up and people might not be able to complain anymore how no one's ever written anything good in Haskell.
[A lot of the class material is available online](https://github.com/bos/stanford-cs240h)
They *are* closures.
I'm sorry. It's the first time I've heard of this issue. We would love to fix it, but I couldn't reproduce it.
A list of packages which are most likely to be broken due to too restrictive library dependancies (the usual candidates are currently the boot-libs `integer-gmp`, `array`, `base`, `deepseq`, `filepath`, ...) could be maybe generated by the help of [packdeps](http://packdeps.haskellers.com/)
You'd need a functional dependency to get the same functionality: class Add a b c | a b -&gt; c where ... Functional dependencies and type families are roughly equivalent. It often depends on the use case which is more convenient, although I think type families have better prospects for the future.
Hi there, I was also a speaker at the YOW! 2011 conference where I ran a (introductory) workshop on Functional Programming using Haskell in Melbourne, Brisbane and Sydney. Having taught FP for a while now, I can typically predict the outcomes confidently, but on this occasion, my expectations were altered in that all participants were able to grasp the content far better than usual. On introspection, I wondered why this might be, and perhaps if it can be attributed to me, but I have decided that is unlikely. Instead, I think it's a consequence of the helpful nature of the Haskell community and the available literature for people to prepare themselves. So thanks guys. PS: SPJ was a blast. TLDR; thanks Haskellers!
All of them is probably a good approximation.
The network package is basically impossible to compile without tweaking. It's been the same with every release.
 Downloading tar-0.3.1.0... Configuring tar-0.3.1.0... Preprocessing library tar-0.3.1.0... Building tar-0.3.1.0... [1 of 8] Compiling Codec.Archive.Tar.Types ( Codec/Archive/Tar/Types.hs, dist/build/Codec/Archive/Tar/Types.o ) [2 of 8] Compiling Codec.Archive.Tar.Read ( Codec/Archive/Tar/Read.hs, dist/build/Codec/Archive/Tar/Read.o ) [3 of 8] Compiling Codec.Archive.Tar.Write ( Codec/Archive/Tar/Write.hs, dist/build/Codec/Archive/Tar/Write.o ) Codec/Archive/Tar/Write.hs:124:29: Could not deduce (Show a) arising from a use of `showOct' from the context (Integral a) bound by the type signature for putOct :: Integral a =&gt; FieldWidth -&gt; a -&gt; String at Codec/Archive/Tar/Write.hs:(123,1)-(127,21) Possible fix: add (Show a) to the context of the type signature for putOct :: Integral a =&gt; FieldWidth -&gt; a -&gt; String In the second argument of `($)', namely `showOct x ""' In the expression: take (n - 1) $ showOct x "" In an equation for `octStr': octStr = take (n - 1) $ showOct x "" cabal: Error: some packages failed to install: 
See also http://hackage.haskell.org/package/vector-static http://hackage.haskell.org/package/hmatrix-static Unfortunately, these seem to be afflicted with bitrot.
&gt; Maybe you're halfway through an identifier name, so the current (partial) name isn't actually valid/bound. That's not really a big deal. There should be a separation between typing something and validating it before putting it into the AST. &gt; Or maybe you're still moving things around because you're deciding how to fit the pieces together in a way you like, so maybe it isn't even a well-formed AST. I'd have to see some examples of that. You wouldn't move invalid things around so validity should be preserved for free. &gt; because of infix operators and their differing user-defined associativities and precedences you can never be really sure what the partial AST should even be, since it can change as soon as the next token shows up (e.g., when moving from "3 + 5" to "3 + 5 *"). This can be sorted out with either (1) explicit parentheses, in which case there is no precedence nonsense (which I would prefer if it were just me using this editor, precedence frankly gets on my nerves), or, (2) inspecting the fixity declarations in scope. Ideally, for me at least, it would be (1) for editing, but (2) for display. Also, it depends on how entering code works. I've been thinking of typing arbitrary text in a box and then pressing RET to "commit it to the AST", if it's valid the text will become part of the AST, if it's wrong it just stays there in the box. Another, more interesting, way, would be for it to construct the AST as you go. Like this (where | is the caret): * ⌨`3 +` → `(3 +[|])` * ⌨`3 + 2 * ` → `3 + (2 * [|]])` * ⌨`3 + 2 * 5 )` → `3 + (2 * 5) |` * ⌨`3 + 2 * 5 ) + ` → `3 + (2 * 5) + [|]` * ⌨`3 + 2 * 5 ) + BACKSPACE` → `3 + (2 * 5) |` * ⌨`3 + 2 * 5 ) + BACKSPACE UP LEFT -` → `3 |+ (2 * 5)` * ⌨`3 + 2 * 5 ) + BACKSPACE UP LEFT DEL -` → `3 -| (2 * 5)` * ⌨`3 + 2 * 5 ) + BACKSPACE UP LEFT LEFT 5 -` → `35| + (2 * 5)` * ⌨`" hello " ++` → `("hello" ++[|])` * ⌨`" hello " ++ y` → `("hello" ++ y|)` * ⌨`if` → `if|` *(at this point it's still a "box")* * ⌨`if SPC` → `if [|] then [] else []` * ⌨`if SPC BACKSPC` → `if|` * ⌨`if SPC x` → `if x| then [] else []` * ⌨`if SPC x TAB y` → `if x| then y| else []` * ⌨`if SPC x TAB y TAB z` → `if x| then y| else z|` * ⌨`case SPC` → `case [|] of [] -&gt; []` * ⌨`case SPC x TAB` → `case x of [|] -&gt; []` * ⌨`case SPC x TAB foo TAB` → `case x of foo -&gt; [|]` * ⌨`case SPC x TAB foo TAB bar` → `case x of foo -&gt; bar|` * ⌨`case SPC x TAB foo TAB bar TAB` → `case x of foo -&gt; bar; [|] -&gt; []` You get the idea. Similar to paredit. But that's trickier. &gt; The point is, there's always some stage in programming where everything's a mess. Can you provide an example where the mess is syntactical mess? &gt; Trying to impose well-formed structure on the messiness of development in every place and at every time is just doomed to failure, I disagree, clearly. That's the whole point of a structured editor. &gt; because development isn't universally pretty and well-structured. People are messy. Creativity is messy. But more importantly, we don't care about how people keep their houses just so long as they present themselves well in public. You can be messy without being syntactically invalid. &gt; The obvious solution is to consider an interface like Agda's sheds. In general, Agda will try to compile and typecheck everything for you, but sheds are a way of saying "hey waitaminute! don't bother checking this part yet, because I know it's a mess". You do all your mucking around and developing inside of sheds, and when you're ready to show it to the world (i.e., the compiler) then you remove the shed. This way, Agda can check everything that it knows is stable, but avoid things it knows are a mess, and so the checking isn't going to interfere with the messy process of figuring out what the heck you're trying to do. I think you've misinterpreted my intentions. I don't care about type safety, I care about syntactical correctness. Not because being correct all the time would save me getting compile errors, but because it enables structured editing. Because structured editing, a la paredit, is fast and safe. Sheds are a nice idea but they're a separate idea. I could also implement sheds separately, in a structured or non-structured editor.
I believe the `Show` superclass constraint has been removed from `Num`. This should be easy enough to fix: just add a `Show` constraints on the relevant functions.
How does this work when you are not the maintainer of all of them?
I would recommend using Data.Colour.SRGB.Linear.rgb in Propane.Colour.cRGBA and Propane.Colour.cHSVA since real valued data is probably best represented on a linear colour scale (and this change is made easy because you have abstracted the colour module). However keep using Data.Colour.SRGB.toSRGB24 in Propane.Raster since PNG expect regular sRGB data. Your quant function in Propane.Raster isn't quite correct. As it stands it is producing pre-multiplied colours (pre-multiplied by alpha); however most formats, including PNG, expect non-pre-multiplied colours. Therefore you need to divide each colour channel by the alpha channel while taking care not to divide by 0, and instead return dummy values in that case.
 control' f = liftWith f &gt;&gt;= restoreT . return I believe this is the same as control' f = liftWith f `liftM` restoreT
Yup! I ended up trying my version out and it requires adding the return type at the call site, which is obviously undesired. I'll have to read up more on type families and functional dependencies when I get a chance. Thanks for the info.
Oh the horror! It's like 1999 all over again: http://conal.net/Pan/Gallery/default.htm I love how they simulated even the slow page load time!
Doh! I should have thought it through more carefully, thanks again. However, using one-based indexing instead of zero was a very deliberate choice in this case. It is more in line with the usual math notation for vector and matrix elements, and since I wanted to reuse the dimension types for indexing, it would've felt just wrong to index a `Vector D2` with `D0` and `D1`. 
Part of the problem is that structural editing really needs to include support for common non-tree "structures", such as sequences and expressions with associative operators. An AST grammar would code these as unbalanced trees, which does not make for good user support. By way of example, ("hello " ++ y ++ " world.") should be treated as a single object with 'subtrees' *"hello"*, *y* and *"world."*. Same for mathematical expressions such as (3 + (2 * 5)). BTW I would allow for writing such expressions by choosing operators first and filling their arguments in as the next step, with infix input as a separate option. 
how... how does that even work? Please explain it to me
&gt; Part of the problem is that structural editing really needs to include support for common non-tree "structures", such as sequences Yep, I half acknowledged that above e.g. in the `case` example and the original demonstration video, sure. &gt; and expressions with associative operators. An AST grammar would code these as unbalanced trees, which does not make for good user support. Agreed, `x * y * z` should be a node with three slots. &gt; BTW I would allow for writing such expressions by choosing operators first and filling their arguments in as the next step, with infix input as a separate option. This was the way I initially conceived of it, operator-first is a more lispy approach, but it's a goodie. It's not clear to me if infix cannot achieve anything operator-first can't. If they are functionally equivalent it should be possible to have both with some reasonable contextual behaviour. So, e.g. * `== TAB` → `|_ == _` * `x SPC == SPC` → `x == |_` * `== TAB f SPC g` → `f g| == _` * `== TAB f SPC g TAB` → `f g == |_` * `== TAB f SPC g TAB n SPC * SPC` → `f g == (n * |_)` It could work based on precedence, or always force expressions of different operators to be fully parenthesized inside another. I trust from your constructive reply you have some confidence like I do that this is feasible. Anyway, there's another four day weekend starting tomorrow, so I'll be conducting another experiment but this time in Emacs with keyboard-only control, and this time starting at the expression level first. :-)
What, the "clever" version?
`fmap fmap fmap`.
It uses the `Functor` instance for "functions from a type", i.e. `((-&gt;) t)`. If you specialize the type of `fmap` to that functor, you get `(a -&gt; b) -&gt; ((-&gt;) t) a -&gt; ((-&gt;) t b`. Written infix, that's `(a -&gt; b) -&gt; (t -&gt; a) -&gt; (t -&gt; b)`. As the type signature suggests, it's function composition. So to start with, `fmap fmap fmap` means `fmap . fmap`, which maps a function two (possibly different) functors deep. This is then applied to `sum`, which is straightforward, giving us a function `(Functor f, Functor g, Num a) =&gt; f (g [a]) -&gt; f (g a)`. This is then applied to `zipWith (*) :: (Num a) =&gt; [a] -&gt; [a] -&gt; [a]`. To unify this with the `f (g [a])` above, both functors become `((-&gt;) [a])`, so `fmap` is again performing function composition. So the full expression is equivalent to `((.) . (.)) sum $ zipWith (*)`. If you expand the function applications here, the result is the same as the ugly pointfree form with the `(.)` operator section. Which is all horrendously complicated. It's much simpler to just look at the type signature `(.).(.) :: (c -&gt; d) -&gt; (a -&gt; b -&gt; c) -&gt; a -&gt; b -&gt; d`, which shows that it's just composing a unary function with the result of a binary function.
Not sure. I'm probably in a minority here, but I don't really find it that useful as it seems to simply be an aggregation from sources that I either a) already check or b) ignore. If it had more editorial content or summaries (for the things I normally ignore) I think I might miss it more. 
Lately, each HWN has been signed &gt; Until next time, Daniel Santa Cruz So I imagine he's the one that could answer this. Sadly, I don't actually know who Daniel Santa Cruz is, nor how to get a hold of him. I added him to my Haskell circle on Google+, but it looks like he hasn't said anything [on G+](https://plus.google.com/105107667630152149014/posts) nor [on twitter](https://twitter.com/dstcruz) since November.
Totally agree. I miss HWN by Brent Yorgey.
On vacation for the holidays--largely away from connectivity. Expect the HWN to resume in January. I'm sure he'll be glad it was missed.
The last version of EclipseFP does not longer use a forked version of Scion, but the Buildwrapper package that it's available in Hackage. Maybe you can give it a second chance. It's better if you try with the latest version of Eclipse as base platform. [PD: I'm one of the developers of EclipseFP]
I´m not dead yet! :) As mightybyte mentioned, I´m out of the country until year´s end. Glad to hear some comments about content. I´ll try to have a discussion about it with people interested when I get back.
I find it useful precisely when I've been too busy to keep up with my usual sources. Obviously more content would be even nicer, but there's plenty of value in a condensed, manually-filtered aggregation of such stuff.
I agree that a nicer mechanism to read command-line arguments is desirable, but the fact that you need an infinite number of instances here (and it can't handle a variable number of arguments) makes this solution seem quite undesirable to me. Something with GADTs or similar seems preferable here. That's really vague, though, so I don't have any *concrete* suggestions right now :) I know `cmdargs` can do argument parsing with fairly light syntax, but it's not as nice as it could be.
here's the beginning of my shot at parsing cli args: https://github.com/Paczesiowa/hsenv/blob/master/src/Util/Args.hs it should work like this: you write imperative code using things like "getSwitch "foo" "defaultVal"", you can default to things coming from IO (like current directory) and because the arrow has static part it can track all the calls to things like 'getSwitch' and still provide you with a --help/--usage message.
I wasn't planning on an infinite number of instances.. just 14 or so, just like the the number of tuple instances for Read. Also, much thanks for steering me correctly on StackOverflow.
Well, you need an infinite number of instances unless you want it to just break at some point :P It's not an important limitation for most programs, but I tend to prefer solutions that scale appropriately as complexity increases rather than having to be replaced. Of course, that doesn't get things done in the short term... You might want to handle `UnquotedString` specially so that it shows as something nicer in the usage message.
I'm fine with it breaking on tuples of that length or larger. It's good enough for the core libraries: ghci&gt; read "('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o')" :: (Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char) ('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o') ghci&gt; read "('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p')" :: (Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char,Char) &lt;interactive&gt;:1:1: No instance for (Read (Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char)) arising from a use of `read' Possible fix: add an instance declaration for (Read (Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char)) In the expression: read "('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p')" :: (Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char) In an equation for `it': it = read "('a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p')" :: (Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char, Char) I will agree that the class name for UnquotedString is too ugly. Maybe an alternative to `Typeable` would let me pretty that up, but then I run into the same OverlappingInstances problem that I contemplate with an alternative to Read.
Can you give an example of use?
I don't like that facet of the core libraries, either :( But at least that just means you have to structure your code differently, rather than restricting what your program can actually *do*. Admittedly, you could consider having to use a different argument parsing method a matter of code structure... I would just create a typeclass for argument-parseable things; it's not like there are so many instances of `Read` that make reasonable command-line arguments that you couldn't just write an instance for them.
it's just a proof-of-concept for writing imperative arg parsing code, with automatically generating usage message. I do have more code (with a simple example) at home, but I won't have internet connection there for another week (just moved to a new place)
Have a look at [cmdargs](http://hackage.haskell.org/package/cmdargs) by Neil Mitchell for your argument parsing needs. It's quite good.
http://hackage.haskell.org/packages/archive/simpleargs/0.2.1/doc/html/System-SimpleArgs.html
And then someone returns `Just undefined` :D
Knew I couldn't have been the first to come up with this.
Playing with this code has been a hell of a lot of fun over the last day or two. Acow and I both have that itch to optimise wherever we can, and it's been great seeing how much we can each to to improve this demo. It's probably noting that the OpenCL kernel is really quite unoptimised, and this was on purpose to demonstrate that even with a fairly obvious kernel, you can still get very good performance. I think OpenCL has a bright future in Haskell, and I can't wait to play with it even more in the future!
Neil Michell is a great Haskell programmer, but I rather despise cmdargs. My biggest complaint is the superfluous and (IMO) ill-advised use of `unsafePerformIO`. I think Template Haskell/QuasiQuotation would be a much better option at this stage in the game for cleaning up the syntax.
The cmd args package has an API that inly uses pure functions as well. BTW, I despise anything using TH. Including my own code.
That package is LGPL - you could still publish yours as BSD
&gt; BTW, I despise anything using TH. Including my own code. I am curious about your reasons. I mean, I tend to avoid TH myself, and anything can be used badly, but it seems to me that once in a while and done well TH can be nice.
It compiles fine in my VM and always has.
I send pull requests on github.
As long as you pretend that `System.Console.CmdArgs.Implicit` doesn't exist, I don't see the problem. Of course, you might not want to even be *associated* with such code...
TH slows down compilation quite significantly, requires fairly opaque code to power it, is pretty painful to debug, and has an awkward syntax.
I don't really see the advantage of that; the LGPL isn't "infectious" in the same way the GPL is. So unless there are hordes of people scrambling to contribute to `simpleargs` but won't because of the license...
Ok, I monkeyed about with it a bit more - now it supports: * arbitrary length tuples * optional arguments * variable-length arguments * unquoted strings and chars without a special wrapper type It's [over at github](https://github.com/rampion/ReadArgs) with a Hspec test suite and a simple example case: {-# LANGUAGE ScopedTypeVariables #-} module Main where import ReadArgs -- try running this with a couple sample inputs -- % ReadArgsEx -- usage: ReadArgsEx [Char] String [Int...] Char -- % ReadArgsEx hello x -- (Nothing, "hello", [], 'x') -- % ReadArgsEx a hello x -- (Just 'a', "hello", [], 'x') -- % ReadArgsEx hello 1 2 3 x -- (Nothing, "hello", [1,2,3], 'x') -- % ReadArgsEx a hello 1 2 3 x -- (Just 'a', "hello", [1,2,3], 'x') main = do (a :: Maybe Char, b :: String, c :: [Int], d :: Char) &lt;- readArgs print (a,b,c,d) 
If memory serves me, there are (or were?) murky issues about the LGPL and compiling with GHC, such that it wasn't clear if it was even possible to obey both the letter and spirit of the LGPL.
I [went with BSD3](https://github.com/rampion/ReadArgs/blob/master/LICENSE).
The explanation written by SPJ might be of interest to library authors who tend to make ample use of the `INLINE` annotation: &gt; I have taken the trouble to write this up so that everyone gets a clearer idea of how inlining works. 
try Google Summer of Code
Looks like someone got "Learn You a Haskell" for christmas. ;-) Anyway, I do have a project called `singing-tomato` that might fit. Description: * Goal: an easy-to-use Haskell library for all things audio: synthesis, DSP, recording * Focus: Ease-of-use trumps features. Less is more. I imagine an API in the spirit of the beloved `Prelude`. The idea is that you can fire up `ghci` and get immediate gratification in terms of sound from your speakers. * Internals: All the hard work is handed to [SuperCollider][1], which serves as backend. In other words, the main task is to implement a Haskell API in terms of SuperCollider's programming model. * Status: I've done design work on the API and have familiarized myself with SuperCollider. * Difficulty: Thanks to the fixed API, the internal implementation may be as clever or stupid as you like. Basic knowledge of the `IO` monad will need to be acquired. The main task involves learning SuperCollider. * Benefits to the learner: Design and implementation of domain specific languages is the pinnacle of Haskell development. This project teaches exactly that, while having a simple scope. [1]: http://supercollider.sourceforge.net/
What skills and inclinations do you have apart from Haskell?
Hi Apfelmus - I've been working on something similar. I've just implemented an OSC encoder / decoder (I found Rohan's a bit too complicated) and was about to look at writing a scheduler for timed communication with Supercollider (or Pd) via OSC. Are you subscribed to Haskell Art - do you fancy punting the conversation there? 
Quite nice. It's still definitely going to take work to make it really usable, though. I was first off incredibly confused by linker errors - might be good to mention right in that article that you need to pass ghc(i) the -lOpenCL flag when compiling code that uses this. Then when I tried the first example I got an error... CL_INVALID_PLATFORM, I think, I don't remember the code anymore, but it was caused by ezInit trying to pass a null pointer to clGetDeviceIDs. That apparently isn't allowed with Nvidia drivers. On my system clGetPlatformIDs returned only one platform, so passing that one platform to clGetDeviceIDs worked, but I don't know what the general solution should be. Right now I'm getting a CL_BUILD_PROGRAM_FAILURE when trying to run the quasicrystal example. The first example did work, and I think I did copypaste the OpenCL code correctly, so I think what I'm going to be after now is figuring out how to get a more descriptive build error message... e: Okay, now I actually have a basic idea of how IO exceptions work in Haskell! Turns out the problem was that there's about half a billion overloadings of clamp() and the first argument's type apparently doesn't determine those of the rest in every case. Easily solvable by giving the types explicitly: float zero = 0; float one = 1; uchar r = (uchar)(255.0f * clamp(wrap(sum), zero, one)); Now, running it in ghci, I do get a gloss window flashing on the screen for a split second, then: &lt;interactive&gt;: CL_INVALID_ARG_SIZE &lt;interactive&gt;: interrupted &lt;interactive&gt;: warning: too many hs_exit()s Let's see what's up with this now... e2: Ah, right. runKernel's integer arguments need to be CInt... I think. I'm not entirely sure of the exact rules, but changing them all to CInt made the program work as intended :)
Do you have a github or other DVCS? I'll be glad to help, I ditched Computer Science for Business Management so I could have more personal time for learning Computer Science through projects like that. Could you tell me how I can help or point me to the projects page?
[Seems you're right](http://www.reddit.com/r/haskell/comments/hexnv/gpl_lgpl_and_ghc_linking/). Nasty.
Looks cool! Funnily enough, I just wrote a "Hello World" kickstarter for Yampa: http://danielmescheder.wordpress.com/2011/12/02/hello-yampa/ I'll have a detailed look at that Reactive Banana code after Christmas. By the way: Does anyone know in how far all these FRP frameworks differ? (And what their status is in terms of maintenance and maturity)
Yep, very funny :) I'll take a look to your introduction as well. I'm a very newbie into the RP field, but it seems promising. Regarding Yampa, the only thing I know is that the last update was on 2008. His successor, Animas, seems still supported, though.
Then my brain Just explodes &gt;.&lt;
Good effort. It may not be the greatest tutorial but finding any sort of tutorial on frp isn't easy and we could definitely do with more. So thank you. I like the example you chose, a (very) simple relationship between two behaviours (so you've taught me) being held true over multiple events. It is unfortunate that the implementation was done by sharing events, not defining their values in terms of the other. But most significantly was when you first introduced that haskell code. That code drop was drastic. I think you should have started with that second segment, the contents of main, and explained each of those elements in a top down manner. Functional code lends itself quite well to a to down explanation, in my experience. Other then that the only change I feel qualified to recommend is more intuitive function and variable names. Thanks again for taking the time to write this tutorial, you've encouraged me to give reactive banana another chance after it failed to build on my mac (I suspect my linux box will have a better chance). And merry christmas from australia! (2 am)
Thanks for your feedback! Yes, this first version unfortunately suffer of the cons you have reported, but I hope to improve the tutorial as my Banana's knowledge improves as well. Maybe the entire code drop was a bit drastic, but I tried to follow a top down approach as well. For building RB on Mac OS X (which is my platform too), try to create an isolated environment with the excellent hsenv. And merry christmas to you too, despite here in Italy is still the 24th, 4:30 PM :)
&gt; you've encouraged me to give reactive banana another chance after it failed to build on my mac (I suspect my linux box will have a better chance). Author speaking. Please don't hesitate to use the [issue tracker][1]. :-) [1]: https://github.com/HeinrichApfelmus/reactive-banana/issues?sort=created&amp;direction=desc&amp;state=open
Yeah. I think the most conservative assumption is that any LGPL library in Haskell might as well be GPL in practice. 
How is the oracle replaced? For example, how would this version of shake deal with the fact that the compiler version number has changed since the last build?
I liked the bit where you gave a contrived example, SPJ said it seemed implausible because you were causing absurd amounts of inlining, then two hours later he posts a detailed explanation of exactly how the problem really *was* excessive amounts of inlining. There's probably some sort of moral to this story when it comes to troubleshooting and not overlooking obvious-yet-silly causes. Anyway, SPJ's explanation is indeed nice, so all's well that ends well I suppose.
 &gt; Ease-of-use trumps features I which more libraries on Hackage did this.
For the exact same reason, SGML/XML entities are considered harmful.
Solve the Halting Problem. I hear it is just one line of list comprehension.
Wow, great job sticking with it! I think some of the issues you encountered were already addressed in the examples on github, but I should make sure those changes make their way back to the blog post. The `-lOpenCL` isn't needed on Mac OS X (for me at least), but I'll add a note. The platform enumeration issue is annoying. I've run the demo code on a Nvidia GPU, but again on a version of OS X. I'll investigate if there's a better default to use in `ezInit`. Thanks for the feedback! EDIT: The clamp overloading and integer argument issue (which appears on 64bit GHCs) are now fixed in the code in the blog post. I also added a note about `-lOpenCL`.
 Oh! This is the first time I'm mentioning the project publicly, so I haven't even set up a github repository yet. Give me some time over Xmas? If you'd like to work on something right now, I would be very happy about someone performing the following task: Write a Haskell function playAudioFile :: FilePath -&gt; IO () that instructs SuperCollider to load a sound file (like .wav or something) and play it back on the loudspeakers. This task is somewhat challening, though, because it requires learning a bit of both [SuperCollider][2] and the [hsc3][1] bindings in Haskell. I suggest trying this in the SuperCollider language first and then adapting one of the hsc3 examples linked to above. I'd be perfectly happy with a quick &amp; dirty solution, the simpler the better. [1]: http://slavepianos.org/rd/ut/hsc3-texts/hsc3-tutorial.html [2]: http://supercollider.sourceforge.net/
Or as the marketing department calls it... "the halting opportunity."
Aside from sending patches, some maintainers are OK with other people uploading compilation-fixing point releases. I've done it a number of times (with maintainer permission) for a few packages.
In the particular case of the vector library, couldn't the CPP macro itself remove the bounds check if bounds checking is turned off (which it is by default), instead of relying on the simplifier to later remove the dead code? Here's what I do in unordered-containers: #if defined(ASSERTS) # define CHECK_BOUNDS(_func_,_len_,_k_) \ if (_k_) &lt; 0 || (_k_) &gt;= (_len_) then error ("Data.HashMap.Array." ++ (_func_) ++ ": bounds error, offset " ++ show (_k_) ++ ", length " ++ show (_len_)) else #else # define CHECK_BOUNDS(_func_,_len_,_k_) #endif if `ASSERTS` is false (which it is in a non-debug build) we never generate the bounds check code to begin with.
I learned haskell by making an account at projecteuler.net and solving the problems there.
&gt;Does anyone know in how far all these FRP frameworks differ? Well, Yampa hasn't been updated since 2008. As far as I know, reactive-banana is the only practical "traditional FRP" (`Behavior` and `Event`) framework these days; [netwire](http://hackage.haskell.org/package/netwire) seems to be gaining traction as far as arrowised FRP goes.
Here is the [first set of homework problems](http://chaoxuprime.com/files/works/other/haskell-hw/hw1.pdf) I assigned to my math major friends who want to learn Haskell. I would like someone to write solutions to it. Of course this is not useful if you are already pretty advanced in Haskell.
But what about the quotes section?
You rock.
Exciting stuff! 
I'm sure it does. But the network package seems extra fragile and sensitive to changes in the environment. 
You're right. I'm away from my mac box until the 27 th but the problem was trying to build the wx bindings package. From memory I think it was banana/wx/wx.hs, when it tried to import discrete (on line 11?) It was an "ambiguous occurrence of discrete (line 40-45?)" and it was clashing with itself. I'd never seen an error like that, our couldn't tell if it was A, a location of a module, or A, the exact same module. Very weird. This was on a mac mini running os x lion with ghc 7.2.2. I've never written a bug report in my life but I'll try to remember when I get back to where my mac is.
Oh, looks like hackage has the same problem. It's probably some [really weird bug in 7.2][1]. Everything works for me on GHC 7.0.3. [1]: https://github.com/HeinrichApfelmus/reactive-banana/pull/11
For simply reading WAV files into PCM buffers, you could use the [bindings](http://haskell.org/haskellwiki/Hsndfile) to [libsndfile](http://www.mega-nerd.com/libsndfile/), or the HCodecs library. The latter is also pretty good, and has the advantage of being a pure Haskell solution. I used HCodecs recently for a project which couldn't use libsnd for licensing reasons. Then again, the WAV format is simple enough that it would not be difficult to roll your own codec. EDIT: Added links for libsndfile.
The post says that this change fixes the immediate problem: &gt; | not (doChecks kind) || cond = x to &gt; | inline (||) (inline not (doChecks kind)) cond = x So the functions "||" and "not" are not declared INLINE, causing them to be inlined too late to eliminate lots of dead code, which then clogs up the system and GHC panics. One could argue that they should therefore be declared INLINE. People usually assume that "not False" is basically in every way the same as "True" (and similar equivalences for other basic stuff), and this breaks the assumption.
7.4 will be here soon so I'm not overly concerned. The bug was so weird I kind of assumed ghc might be the issue.
I would be happy to remove all the inline pragmas from vector-algorithms if someone would tell me how to make up the more than an order of magnitude in performance they're there for. I didn't sprinkle them in there for my health. I guess I haven't tested in a bit, but I haven't heard of any GHC breakthroughs that allow it to produce comparable code to when everything is inlined.
You could also refactor your Instruction type. Something like data Instruction = Label String | MemInst MemOp AddressMode data MemOp = Load | Store | Jump | Call instructionSize :: Instruction -&gt; Int instructionSize (Label _) = 0 instructionSize (MemInst _ mode) = 1 + addressModeSize mode I find this much clearer than using record syntax.
ah, that actually makes sense to me! thanks!
I am not a big fan of this solution to the records problem. Simon's an amazing guy, but I think he got it wrong on this one. I think his efforts would be better served implementing a superior namespace system for Haskell so that different types can share record names instead of having to create a separate module for each new datatype so they can have their own namespace. Such a solution doesn't solve his other "benefit", which is duck-typing, however I don't really see that as a benefit in a strongly typed language. I consider proper namespacing to be far superior to duck-typing because it lets you specify in a clear way the intended type of your accessor function's argument so that if you accidentally call it on the wrong object type it won't compile. Those are my two cents.
I'm not sure where you're reading “duck typing.” In the context of Haskell duck typing can mean having records with arbitrary fields, like a named tuple, but from which fields can be accessed provided the field exists, at compile time. I made [a blog about using has](http://chrisdone.com/posts/2010-11-22-duck-typing-in-haskell.html) for duck typing. HaskellDB's record system is similar, but implemented with functional dependencies rather than type families. The problem with these systems is that they don't have pattern matching support (and fields of the same name can only have one type for all records). The [TRex](http://cvs.haskell.org/Hugs/pages/hugsman/exts.html#sect7.2) record system is almost perfect to me, with the exception that it can't do some kind of subtyping like the duck typing described above, i.e., I can't have a function `f :: (a::Int,b::Char) -&gt; Double` and pass it a `(a::Int,b::Char,c::Bool)`. So for me, something has-ish with pattern matching support would be marvey. Regarding the namespace/field clash problem, still beats me. I'm still prefixingAll prefixingMy prefixingRecords, prefixingIt prefixingGets prefixingKinda prefixingVerbose, prefixingBut prefixingThe prefixingLispers prefixingLike prefixingIt.
Is there any kind of summary of the various issues people have with records on any of the wikis? I feel like records are fairly useless simply because theyre not first class values. I guess some people feel that the need to make all record names unique is an important issue, but I can't relate. It seems like a bad idea to use the dot outside of type-land to be anything other than the function composition operator unless we want code that's impossible to read and to confuse the heck out of newbies.
We could always switch to using `.` for record fields and namespaces, and instead use `(∘)` for function composition. :]
Namespacing is no worse than prefixing (which it reduces to in the worst case scenario) and it's better when you only have to bring one such name into scope. My objection with **any** implementation of duck typing (including the one you blogged about) is that it lets you do the wrong thing. Like I already noted, if you call a polymorphic accessor on an unintended data type that happens to support that field, the compiler will happily typecheck it and you would be none the wiser that you introduced a bug into your code. The entire point of a strongly typed language is to use the compiler to prevent programming errors. There are legitimate uses of polymorphic functions, but I don't think the default record implementation should be one of them **by default**. If it's just so that two datatypes can use the same accessor name, then namespaces are a more appropriate solution.
Or just "-&gt;". Like C. 
I'm not sure why the polymorphic accessor needs to be fully polymorphic in all types. Shouldn't the needed type of the slot where the polymorphic accessor would be found be known at compile time? Couldn't the polymorphic type be restricted to only those types meeting the requirements at the time?
Thanks for the links! I'd like to do everything in SuperCollider, though; loading an audio file is just a small part of the larger project.
I've updated the article: it should be far better now! Feedback are always welcome :)
I hope 2012 will be the year of Records Syntax fix. It's one of few things in Haskell that shed me a tear. Solved this issue, for me Haskell will be almost perfect :)
&gt; Namespacing is no worse than prefixing (which it reduces to in the worst case scenario) and it's better when you only have to bring one such name into scope. I find these a complete chore in a project with 76 record types across 126 modules: * Writing a separate import for all the data types I use. * Re-doing it every time I move/refactor some code across modules. * Having to interrupt my workflow because now I'm using two records, so now I *do* have to go and add two qualified imports. * Having to check whether something is imported aliased or unaliased when switching between modules and working in the REPL. * Not being able to export all the types in one module. * Writing Foo.bar instead of fooBar is just more painful for me. I would have gone crazy by now if I had kept up the import nonsense. I just write `import Project.Types` and go, it feels like a language that has decent record support. &gt; Like I already noted, if you call a polymorphic accessor on an unintended data type that happens to support that field, the compiler will happily typecheck it and you would be none the wiser that you introduced a bug into your code. I don't know what “unintended data type” means. Can you give an example where the compiler would type check a buggy access with the duck typing implementation I blogged about? &gt; There are legitimate uses of polymorphic functions, but I don't think the default record implementation should be one of them by default. If it's just so that two datatypes can use the same accessor name, then namespaces are a more appropriate solution. I agree with that, I was just mentioning this record system in reply to your duck typing comment. I don't think import mangling is the solution, though.
Let me emphasize the point I was trying to make in the title. I don't think this thread is the place for yet more discussion about what record syntax should be, or what it should do. There are threads upon threads upon wiki pages of discussions of untold variations and suggestions on the ghc users list, haskell-cafe, haskell wiki, and ghc trac wiki, and probably elsewhere. What we need now is leadership. Someone needs to wade through the mess and say: I've seen the discussions, I know what the problems and trade-offs are with each approach. Here is what we are going to do, and it's going to work. Don't get me wrong - more discussion of the details is great, too. If you think a reddit thread would be good for that, please start one. In fact, just gathering together links to all the previous discussions would be very helpful. But the most important thing we need right now is direction. Without someone getting informed and then taking the initiative, this is just never going to happen. This is your chance - be the hero!
&gt; I'm not sure where you're reading “duck typing.” In the context of Haskell duck typing can mean having records with arbitrary fields, like a named tuple, but from which fields can be accessed provided the field exists, at compile time. I made a blog about using has for duck typing. This seems to describe polymorphic records, which have been extensively studied in the literature. They're related (by duality) to polymorphic sums, as described in [[1]](http://lambda-the-ultimate.org/node/4394). These may be used to implement a variety of features, such as solutions to the expression problem, exception handling, OOP, etc. all without sacrificing type safety. I'm not sure how this relates to "duck typing", which expressly disclaims any kind of strong typing.
&gt; I'm not sure how this relates to "duck typing" As I quoted from my blog: “When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.” As I linked from my blog, the Wikipedia page says: “an object's current set of methods and properties determines the valid semantics, rather than its inheritance from a particular class or implementation of a specific interface.”
[TDNR](http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution)?
It's guarded with a disclaimer that it's not as type-safe...
Not really considered harmful - just tricky but powerful 
&gt; if you call a polymorphic accessor on an unintended data type that happens to support that field Perhaps this is a stupid question, but what does that even mean? If it supports the right name and has the appropriate type, why shouldn't that be a valid program? I'm failing to see how that's "wrong".
Duck typing is the wrong word, as that refers specifically to dynamic typing. What you're after is structural typing.
&gt; Duck typing is the wrong word, as that refers specifically to dynamic typing. What you're after is structural typing. this
Statically checked *duck typing* is what I mean and nothing else, structural typing is entirely different.
No, structural typing is exactly what you're talking about, like [OCaml's object system](http://en.wikipedia.org/wiki/Structural_type_system#Example). Duck typing is for dynamic type systems. "Statically checked duck typing" is exactly structural typing.
Would a list of desiderata help? Does one already exist? I find myself having trouble even understand what the discussion is about. I wonder if anybody has already made such a list, short bullets listing the various things people would like to have, (without solutions!), eg. * more convenient record update syntax than foo { x = f (x foo) } * ability for different records to have same fields * ... etc It's funny, I do tend to run into situations where I say "gee, I wish Haskell record syntax was more convenient" but that doesn't make it easier for me to follow the discussion because I forget what they are.
Thanks for this. I wanted to learn FRP for two years now but everytime I've tried to implement something I got stuck at a dead-end. This gives another incentive to try it again. Maybe I'll reboot my programming blog and create something in FRP. Thanks
Sorry, I've seen that before (I read that when I wrote the article last year) but forgot about it, was thinking it was only equivalences. I agree that structural typing, like late binding, allows implementation of the duck analogy (and therefore I would call both *implementations* a way to do duck typing). Were you genuinely confused by my use of “duck typing” in a static context? Or are you just trying to teach me a lesson for using a term more generally than you would like? Others have made the same analogy.^12 ^1 Sebastian Sylvan and authors of http://www.haskell.org/haskellwiki/Why_Haskell_matters ^2 Oleg http://homepages.cwi.nl/~ralf/OOHaskell/ ^3 No one complained about the terminology in this thread: http://www.haskell.org/pipermail/haskell/2008-February/thread.html#20174
I feel the same way. This seems like the proper approach: * compile in a single place the list of ALL grievances people have with records as they currently exist (without worrying about solutions) * gauge community interest in, relative severity of problems above * compile/brainstorm solutions to various subsets of these problems (also in a single place) * decide which if any solutions can address some of the important problems, benefit vs. additional complexity, how much better would Haskell be if it had this new record scheme vs. removing the "setter" aspect of current records (if the answer is only "slightly better" then I suggest trimming the fat by doing just that) Even if the parties exist who are familiar with all the discussions and issues at hand and would be capable of making a decision, the first three steps are important even if only as a way of showing that the decision made was a good one, I.e. people can see for themselves the tradeoffs involved. So would a stackoverflow community wiki be a decent place to flesh out steps 1 and 2 above? I'll probably throw such a question up there at some point and people can close it if they think it's OT.
It looks like chaos, but there are only two mangling schemes you need to care about. The titanium abi which gcc 4 uses, and the Microsoft one for windows. The others can pretty much be ignored unless you care about the last .1%
Yes, it's a standard analogy, and "statically checked duck typing" is going to be read as referring to structural subtyping by most people. My only complaint with your terminology is the suggestion that structural typing was entirely different, which confused matters precisely because you were using such a common analogy.
&gt; Were you genuinely confused by my use of “duck typing” in a static context? Or are you just trying to teach me a lesson for using a term more generally than you would like? Yes. To the extent that 'duck typing' means anything at all, it refers to run-time checking and resolution, not statically-checked structural typing. The PL literature refers to the feature we're discussing here as "polymorphic records" or "extensible records"; I can't see why we would want to use a differing terminology.
Because words have multiple meanings and types won't always suffice to disambiguate, particularly in the presence of polymorphism and type inference. Sharing a name and type is not itself a guarantee that the two fields are "the same" to the degree normally expected in Haskell, e.g. that `(&gt;&gt;=)` really is the same function in some sense on every type that supports it. Yes, static typing makes this *less* of a problem, but it's still possible. Whether it's enough of a concern to influence how the record system is designed, I'm not sure.
And one day I will fix my keymap so I can just *type* `∘` instead of having to type `u2218` while holding down Shift+Ctrl... (The next things I do will be `(∘) = (.)`, `(∘∘) = (∘) ∘ (∘)`, `(∘∘∘) = (∘) ∘ (∘∘)`, etc.)
Haha. I actually use SciTE's "abbreviation" files to get a bunch of symbols like that. So typing . followed by ctrl-B gives me ∘. It's not as convenient as typing it directly, but better than general Unicode entry.
&gt; Were you genuinely confused by my use of “duck typing” in a static context? Or are you just trying to teach me a lesson for using a term more generally than you would like? I wasn't confused by it, but it helps everyone if we all use the same terminology. People reading our comments that aren't already familiar with the terminology either get confused, or they get keywords that don't help them find informative material. For instance, if someone came across your comments re:"static duck typing" and started to google "duck typing", they wouldn't get much informative material on static typing. All the literature uses "structural typing", so I think using that term in its proper context helps everyone. In a field laden with all sorts of bizarre jargon, being specific and consistent helps. I remember how much of a struggle it was for me at first. :-) Papers on structural typing will sometimes include "duck typing" in the keywords section to help search, but it's not guaranteed.
This is interesting because the TDNR proposal is really only marginally related to record syntax, but perhaps the discussions re. records are in fact not so much about records but about general name resolution and a sort of general dissatisfaction with function application as we know it. I understand both of these, but wonder how critical they are.
&gt; Because words have multiple meanings and types won't always suffice to disambiguate, particularly in the presence of polymorphism and type inference. Sharing a name and type is not itself a guarantee that the two fields are "the same" to the degree normally expected in Haskell I had a debate on LtU recently about this issue actually. I asked there for a specific example where this was actually a problem, and nothing satisfactory was provided. The context there was a language with type inference for first-class messages, which allows quite flexible degree of substitution. The parallels ore obvious. The best "counter-example" I provided was a numeric abstraction where ordering on natural was reversed, but addition and subtraction were kept as-is, so the usually invariants of arithmetic were violated. Because these invariants are not captured by the type system, code written against a generic numeric interface will probably fail at runtime even though it's semantically sound. But it's a rather ridiculous scenario, so I'm hoping someone here can provide a better example, because I'm still not convinced that it's truly that problematic to allow this sort of extreme structural typing. Either all the operations/fields required of code are provided, and the program is semantically sound, or it's not and a compile error will be issued, or there are properties not captured by the type system that would cause a compilation to fail if we could express them.
Fair enough, structural typing it is.
Yeah. I could certainly devise an awkwardly contrived example, but I can't think of any persuasive and plausible scenarios. I expect most examples will come down to "properties not captured by the type system" one way or another, and could be avoided using standard techniques such as newtype wrappers and phantom types to represent semantic distinctions or invariants not directly expressible.
I don't know much about record types theory, but I recently tried the [opa](http://opalang.org) language and I must say that I really enjoyed very much their record system, to the point that i would encourage everyone to look at them. They are similar to algebraic datatypes, but open ended, and basically each line in a sum don't need a constructor labeling it because the record itself is the constructor. I'm not sure if i'm clear, but they are kind of cool to use…
If you mean cmdargs' default `Implicit` API, I think it's type-safe, just impure.
I'm liking this... seems to have the clarity I always expected from iteratees, but never found. Looking forward to more.
I think a good workaround is something like the IME used by Asian languages. For instance, when writing in Japanese you hit the conversion key, then type "hana" and strike the space bar, which will present you with the first completion. For me that happens to be "花" (*hana*, meaning "flower"). Typing space again, or the down-arrow, presents me with a list containing 鼻 (*hana*, "nose") and others. So you could type conversion key, then "dot", select the first completion if you mean "∘", then conversion key again to go back to normal input. It's still a lot of keystrokes but it's more convenient than holding shift-ctrl and you don't need to memorize arbitrary codepoints. Indeed it can do this out-of-the-box for heavily used symbols, like "○" when you type "maru" (meaning circle).
&gt; I expect most examples will come down to "properties not captured by the type system" one way or another, and could be avoided using standard techniques such as newtype wrappers and phantom types to represent semantic distinctions or invariants not directly expressible. It's a shame, because you then lose out on a lot of natural isomorphisms from structural typing. I'd like to see a language that takes structural typing to the extreme. With some sort of static contract language to specify what invariants are expected to restrict the linking, instead of being pessimistic and requiring overloading or lifting for every isomorphism we wish to express. It'd be interesting to see what sorts of program structure would result.
Glad that it helped :)
I've also done some Yampa FRP tutorials in the past: http://lambdor.net/?tag=frp Glad to see more! :)
I don't understand what all the discussion is about either. For me it is just the ability to have records with the same fields, including for use in updates, even if there is not a more convenient update syntax. I believe that was the point of the current initiative, but even implementing that seems very complicated.
I've taken inspiration even from your blog :) All started with your article "Why I switched from component-based game engine architecture to functional reactive programming"
&gt;I think his efforts would be better served implementing a superior namespace system for Haskell so that different types can share record names instead of having to create a separate module for each new datatype so they can have their own namespace. This is the approach taken in [Frege](http://code.google.com/p/frege) Every data type is at the same time a namespace. * the function that accesses field x of data type T is T.x * If a::T then a.x = T.x a * the function that updates field x of data type T is T.{x=} * the function that sets field x in a T to 42 is T.{x=42} * If a::T then a.{x=} and a.{x=42} are valid * the function that changes field x of a T by applying some function to it is T.{x &lt;-} * T.{x?} is a function that returns true iff the argument was constructed with a data constructor that has field x.
(Sorry if it's a duplicate, I spent some time searching and couldn't find this video linked. Also, it's rather light, more about the history of Haskell than modern Haskell, but I still enjoy it.)
\**trollomorphismface*\*
If you want to know why they're useful for programming, stackoverflow might be a better place to ask the question.
Just to clarify: I think our plan is to include the conduit switch in the 0.10 release to give it some real world testing, and then release 1.0. Though maybe there are some other ideas on this.
I dare someone to commit MO karma suicide by posting the burrito answer on there.
I don't get why they would do this in the rain...
Watching SPJ talk about type systems while the tourists are taking photos behind him is amusing.
Yeah, but that's not what the original poster is asking. OP is asking about applications of monads in other, more "concrete" areas of mathematics.
Sure, but monads doesn't make it haskell. Practical applications of monads in programming (usually) makes it haskell.
It's a joke submission.
TIL I'm retarded.
I'm really looking forward to seeing where the conduit work goes. Very much liking the first few posts in this series. Even the terminology is muh easier to follow than the enumerator business.
[I am the disappoint](https://www.google.com/search?q=trollmorphismface).
It's not an issue of probability. Every bug, no matter how improbable, eventually occurs. I would like my program to be provably correct, rather than probably correct. It may be rare, but eventually there will be cases of fields that have the same name and type, but completely different semantic meanings. In a strongly typed system the type itself should denote as much as the semantics of the field as possible. This is why many interfaces end up requiring additional "laws" that implementers of that interface must satisfy for the instances to have the correct semantics (i.e. the monad laws, category laws, etc.). With record interfaces, though, unless you want to document the "laws" or semantics of each record type (which would be as tedious as our current boilerplate situation), then that bug will eventually happen. Heck, even if you document it properly it STILL may happen since the type system can't guarantee that people implement the laws correctly (i.e. the ListT debacle where it wasn't a true instance of monad). I'd prefer a type-system guarantee over an English language guarantee.
Can we all agree that lenses are good and should be part of the solution?
I know it's a joke submission, but while we are on the subject I noticed that monads tend to be very useful for combining results (i.e. 'join'ing them) into a single result, whereas comonads tend to be very good for splitting an input (i.e. 'duplicate'ing it) into multiple inputs. Is this correct and if so is there a formal way to state my vague intuition? 
"All images and animations appearing and linked to above are © Conal Elliott, 1999-2004."
&gt; I would like my program to be provably correct, rather than probably correct. I agree in principle, but you can't deny that you give up a lot of natural isomorphisms by using nominal typing to enforce semantic distinctions. I'm merely suggesting that there must be a better way. For instance, consider a language where everything is structurally typed, and then add a [static contract language](http://research.microsoft.com/en-us/um/people/simonpj/papers/verify/index.htm) of sorts, via which you can specify the properties that must be satisfied by any type applications. You would be able to specify your semantic distinctions, without losing the natural isomorphisms inherent to structural typing where the semantics are satisfied. It'd be an interesting experiment I think, where all structures are fully visible, ie. no abstract types, and read/write access can be controlled via type-level capabilities of sorts. Sort of like starting with an assembly language, and adding a meta level where you add semantic information that is verified by the compiler.
Do join and cojoin count? :3 I think it might also have to do with the fact the type constructor for Kleisli arrows is in the target, while for coKleisli arrows, its in the source.
I just imagined they were taking pictures of displays on Haskell types. "Oh! Get a picture with me in front of the Monoid display!"
I agree that a static contract language would be a major improvement and make it a lot easier to factor common elements of similar types.
Emacs does this for Agda-mode. Type \circ and you get ∘, and can choose from a list of alternative interpretations.
Already using -&gt; for function types, lambdas and case expressions. Sounds scary.
I like Agda's records. Can we just have those? Oh wait, you'd need nested and parameterized modules for that. Can we have those too?
All these concepts remind me of FranTk.
Since you brought it up: why can't we use = in case statements and lambda expressions? AFAICT the only reason for the choice of -&gt; is as a cute nod to the lambda calc. notation. Haskell could be better with an economy of syntactic material.
What happens if you have a type with the same name as a module or qualified import, e.g. D.t, where D is a qualified import namespace and also a data type in the current module?
sounds like [infopipes](http://infopipes.cs.pdx.edu/)
Because it's raining Haskell.
TIL SPJ once walked barefoot, habitually. Let's hope I'll never become respectable and don't have to put on sandals because of such vain considerations. Admittedly, I guess I'd wear sandals in Australia, too. Stepping on a dropbear probably isn't a good idea.
There seems to be a misunderstanding. Its the dropbear that steps on you over there.
Well, yes, but I'm living on the northern hemisphere, so dropbears actually lurk below, not above.
Possibly because that's where they decided to do the interview before realising that Sydney does, in fact, sometimes rain.
What was it that SPJ said you would use to do a fast matrix inversion?
http://hackage.haskell.org/package/repa
\* troll *o* morphism
Probably even more awesome with [reactive-banana][1] (functional reactive programming) on top. (To reduce browser/server communication, I do need to implement a [certain optimization][2] for the bananas first, though.) [1]: http://www.haskell.org/haskellwiki/Reactive-banana [2]: https://github.com/HeinrichApfelmus/reactive-banana/issues/19
Then one must disambiguate when one wants to name the type by writing D.D 
See also the main page, with examples http://repa.ouroborus.net/ and the super-excellent starter tutorial http://www.haskell.org/haskellwiki/Numeric_Haskell:_A_Repa_Tutorial and maybe as a preliminary http://www.haskell.org/haskellwiki/Numeric_Haskell:_A_Vector_Tutorial
Oh man, how did I not know about reactive-banana. This is the best thing since chocolate-milk.
Hey Stephen - Yes I'm subscribed to Haskell Art, and sure, I can punt the discussion there. I don't have much code to show yet, though. Basically, the project idea grew out of discussions with Henning.
Someone started to create roughly the same thing. http://osdir.com/ml/general/2011-06/msg41431.html https://github.com/MassiveTactical/panther-ajax/blob/master/examples/example1.hs
That's hilarious. It's almost exactly the same in implementation and even in naming convention. [The JavaScript](https://github.com/MassiveTactical/panther-ajax/blob/master/panther-ajax.js) exactly matches [mine](https://github.com/chrisdone/ji/blob/master/wwwroot/js/x.js). Even [the Haskell code](https://github.com/MassiveTactical/panther-ajax/blob/master/Massive/Panther/Ajax.hs) is [similar](https://github.com/chrisdone/ji/blob/master/src/Graphics/UI/Ji.hs), except it's implemented with continuations rather than a running thread, and mine has event binding from Haskell. I had thought about doing continuations as a more sensible option. Talk about parallel evolution. I've never seen this project *ever*. So it looks like I need to get in touch with Blake Rain.
Concerning the implementation of the appropriate monad, the [WebSessionState.hs example][1] for my [operational][2] package may be helpful. The program looks like it were running in a persistent thread, but it's actually started and stopped all the time. [2]: http://www.haskell.org/haskellwiki/Operational [1]: https://github.com/HeinrichApfelmus/operational/blob/master/doc/examples/WebSessionState.lhs
Note that Hackage hasn't yet built the documentation, so a few inline links are 404s.
Actually, imitating lambda calculus notation would mean using--amusingly enough--a period, e.g. writing `λx y. x` as `\x _. x`.
If nothing else, I think I prefer the Conduit terminology over the unintuitive iteratee terminology.
It seems to me like `Discrete` is a functor. So perhaps you could implement `walletB` simply by `walletB = (+10) &lt;$&gt; walletA`? Anyway, nice article it made me more curious about FRP :-)
These look nice. I use Snap (sorry :) ), which is entirely built on the enumerator library, but these look quite a bit more sane. If I wanted to use conduits in my Snap application, it would at least be possible to write a conduit "Sink" that acts as an enumerator "Enumerator", right? I think that going the other way (an Iteratee that is a Source) can't work because iteratees can't safely allocate resources.
Looking through Part 3, I had another (maybe misguided) comment. I don't know attoparsec at all, but I usually think of a parser as being something that accepts a stream of bytes and emits events (parse tree entries) on occasion. If that's the case, wouldn't the attoparsec-conduit library want to expose a Conduit rather than just a Sink? I'm probably totally wrong on how attoparsec works, but I'm at least understanding the Conduit terms, right?
&gt;emits events (parse tree entries) on occasion With things like attoparsec, parsing is a one-shot deal, with a full result at the end. All the Haskell parser libraries I know of work like this, because it's hard/inconvenient to stuff parse errors far down the parse tree.
I think you got the terms reversed: a Source corresponds to an Enumerator, and a Sink to an Iteratee. I think it's possible to create a Source -&gt; Enumerator conversion, but it would still have to live in ResourceT I think. I'm not sure how deeply embedded enumerators are in Snap and where this would be necessary, however.
I do almost this exact activity in some of my XML parsing code. The parser really is just a Sink, but you can sequence your Sinks together into a Conduit (look for SequencedSink in the docs, I'll probably cover it in part 5).
The xml-enumerator package has a parser which can incrementally produce parse events. It's fun for scraping random statistics out of an XML document.
The opposite of *one-shot* parsing is *online* parsing which allows you to stream results so long as your result datatype supports streaming (lists obviously do). UU parsing and Malcolm Wallace's parsers have supported online parsing for quite a while, I think Edward Kmett's Trifecta does as well. 
This feels like an obvious link, but have you studied the Haskell wiki performance page at [http://www.haskell.org/haskellwiki/Performance]? There's quite a lot of information behind the links there.
I'm actually thinking of bridging, so I would write something that acts as a Sink for a Conduit chain, but that acts as an Enumerator on the Enum side, so it can translate between the two. It makes sense to think of wrapping a Source in an Enumerator, but that wrapper itself is a Sink, I think. I use Snap entirely for serving/accepting large files, so most of my program interacts with Snap through handleMultipart/runRequestBody/setResponseBody. Handling resources really was quite a challenge in this project, and I think I can see how your model would be a bit nicer. I just need some bridges so I can hook it up to Snap's API. Maybe I'll get the time this weekend to play around a bit...
Have you read the "Profiling and optimization" chapter from Real World Haskell? http://book.realworldhaskell.org/read/profiling-and-optimization.html
ah, duh. you're right.
Ah, good to know; thanks for the info. Although, I haven't used Trifecta but from what I've seen it seems to be broadly similar in usage to Parsec, so I'm not sure it has such functionality.
&gt; but you can sequence your Sinks together into a Conduit Oh, that's cool. Definitely looking forward to the rest of your series.
I would actually recommend going "bare-metal" for something like this instead of trying to stick to the higher level abstractions. Wrapping up a Source directly should be very easy, since you basically just need to repeatedly call sourcePull. This sink side is likely more complicated, however. If you have the inkling, I'd appreciate your input on the new WAI design based on conduits. It should be specifically the kind of work you're talking about easier. In fact, one of the main motivators for creating conduits was to create a better WAI. https://github.com/yesodweb/wai
If you haven't seen them already the slides from my Stanford lecture includes some material: http://blog.johantibell.com/2011/11/slides-from-my-guest-lecture-at.html
Note that the library requires GHC 7.4, which should be released any day now.
I, for one, found this quite useful in upgrading my mental model of how (GHC) Haskell actually works.
It's interesting to see how practical web development fuels the progress in such low level and fundamental directions as iteratees.
I've wanted this since at least 2005. Thanks, Santa!
Would you consider support for application-specific data providers which can hook into this (and get some scaffolded UI as well), JMX-style? Or wouldn't this be the intention? If it is, I might take a look at implementing something alike (even though I'm rather new at real-world Haskell).
As far as Strings go, being merely a boxed link list of 32-bit values they have a very high memory overhead. If you're processing one character-by-character with a fold or other list-friendly operations you'll probably be OK, but for anything more than that you should use Text (ByteString is the secondary advice; String and Text are for storing Unicode, while ByteString only stores octets).
Agggh, super awesome! Is `GHC.Stats` currently the only interface that's used for input data? One thing I've been wondering about for a while is hooking up actual heap profiling data the runtime spits out rather than just the stats interface. IOWs, it's basically taking something like `hp2any` and fitting a browser on top of it for real-time profiling data. This shouldn't be incredibly difficult since it seems pretty modular, but there aren't many users. Perhaps the stats interface somewhat subsumes this these days, it'd be interesting to look at the different kinds of data they give you. Another thing I've thought about is doing the same things with an eventlog - like a ThreadScope in the browser that does real-time updating. This actually requires incrementally parsing the eventlog. I had a small demo of this functionality a while back (it used a dirty trick w/ FIFOs to make it work without modding GHC,) but I never got it ironed out. Don had an implementation of a attoparsec parser that parsed eventlog data incrementally somewhere, I may be able to find it. Ultimately I've been thinking a lot about a 'haskell performance toolkit' that displays data like this, so I think collaboration may happen :)
Yes. User specified counters would be cool to have.
For reference, the previous reddit submissions of the "Conduits" series: * [Part 1](http://www.reddit.com/r/haskell/comments/nmgpd/the_resource_monad_transformer_or_conduits_part_1/) * [Part 2](http://www.reddit.com/r/haskell/comments/nqfuy/conduits_part_2_of_five_minute_guide_and_sources/)
`tibbe`s earlier slides made a similar impression on me, and I think cover some things in more detail. http://www.slideshare.net/tibbe/highperformance-haskell
Right now I only use `GHC.Stats`. I want to get my hands on more data to display, but I want to make sure that the monitoring doesn't affect that application it monitors, so the data needs to be cheap to gather (`-T` is essentially free in my experience). An alternative would be to be able to turn on/off heap profiling on the fly. That way you could get a heap snapshot for a running application without interfering with it for a longer period of time.
Does this require a recompilation to turn on? Strace and the like in the c world don't so I'm interested how it might compare. Or is it more like systemtap/dtrace (drool)
No, just pass `+RTS -T` on the command line.
Great! This something I will try. 
I'll definitely have a look. I'm pretty impressed with the direction snap is going right now, but a lot of your more side-projects (macid, conduits) make me want to take a deeper look at yesod. It's a fun time to be doing haskell, anyhow.
I hope there is also runtime stats of thread.
Could you be a bit more specific? What would you like to see?
That's what I get for trusting Google to correct my spelling. It never gets the really important stuff.
Beautiful!
Actually, I'm not connected to macid, that's the Happstack guys.
Besides the other excellent resources mentioned in this thread, check also ezyang's [Haskell Heap series]( http://blog.ezyang.com/2011/04/the-haskell-heap/). 
I realize it sounds like an odd thing, almost nonsensical. But when you compare startup times for complex systems including shutdown, it's often faster to do a cold restart, a crash, than to try a stable shutdown and restart. Further, the idea is that if your most tested path is also your worst case scenario, you're less likely to run into problems when the worst case actually happens. After all, if you find a way to use data structures and persistence such that recovery is efficient while still safe during a crash, you've basically safeguarded against the Worst Possible Thing(tm) your program can do. Well, ideally you're using a type system that makes sure the data is either valid or your program has crashed. Hey, that sounds like Haskell to me. Anyway, it's a fascinating technique to maintaining complex systems.
Fantastic! Just the sort of introductory material to TH that I have been passively looking for.
Thanks! I'm glad you found it useful. :)
I'll check this option with code in my hands :) I'll update the post, if necessary :)
It is interesting how the python function is called extended_gcd and requires no comments, while the haskell version is called eed (? I would have to double check) which in no way helps the reader. Also of course, the haskell version requires a comment to explain the terse name. 
Great stuff!
Protip: do not EVER use code written by a non-professional (in the field) to make data cryptographically secure.
Yeah, that was purely stylistic. In retrospect, I should have named it something better. That's one habit I should get out of: a lot of Haskell programmers use terse function names, but for me a lot of Haskell's appeal is in the transparency of its code, making undescriptive function names kinda silly.
Yeah, this is really just a toy example. I learned about cryptographically secure random number generation (something my code does *not* do, due to time constraints), as well as side-channel attacks on some implementations of exponentiation. Hopefully, I'll have time at some point in the future to clean this mess up and write it properly. [](/c11 "Learning is magic!") 
This article is not convincing. In both cases, the pseudocode was more readable than the Haskell code. In the second example this was apparently on purpose, but even on the first example, I think the use of backward declaration, `where (q,r) = ...`, hurts readability. It would be better to use `let .. in` in this case. I also don't think the guard brings much compared to a simple if/then/else. Finally, the type signature is not overly helpful and could have been left out. I small off-topic remark on the preamble, as I see the author is lurking around: &gt; Pretty much my entire Summer was spent in Germany, doing an internship at Technische Universität Ilmenau. It was fun, but after three months of gluing together OpenCV and Android using JNI, I’m quite happy to be studying again, in an environment where I can choose my own tools and languages (most of the time). Why did you not apply for an internship in a place where you could practice functional programming, if that is what you like? 
I've always found 'where' clauses to help readability, but maybe that's just how my mind works. Same with guards: I find them easier to read because they're not text, but are adjacent to it. Maybe this is a dyslexic thing (reading in pictures rather than words), I dunno. Unless there's a specific reason why one is more efficient than the other, I guess it's down to personal preference. I guess this relates to my point of there being many ways to do things in Haskell; sometimes there are advantages between different coding styles, sometimes it's just aesthetics. Regarding the internship, I wanted an internship where I could get some experience in DSP and/or image processing. Even though I had a preference for other languages, it was still a very rewarding internship, and I learned a lot (including a new-found hatred of the ant build system). Also, so far as language choice is concerned, I don't think Haskell is the be-all and end-all. It's certainly fun to program in, but there are times when it's better to use a different language, while carrying through ideas from the functional paradigm. That said, the code I was working on really didn't lend itself to doing this. To answer your question: I guess, last year, FP wasn't really at the front of my mind as something to look for in an internship. This year, it certainly is!
`where` clause help readability in cases where we can understand what the value is doing without looking at the definition of auxiliary parts first. If it can't be done easily, or if there is a strong procedural metaphor behind the algorithm ("first do that, then ...") then `let .. in` are to be preferred. There is a fuzzy frontier where both are equally acceptable, but people to one extreme or the other (always `where`, never `where`) are usually less readable that people doing a reasonable compromise. Guards are clearly a win when you have more than two conditions (as `cond` in Lisp/Scheme). When there are exactly two, `if/then/else` are equivalent and easier to understand to the novice). In that case, you could/should also have used pattern matching, to get a nice equational presentation: extended_gcd a 0 = (a, 1, 0) extended_gcd a b = let (q, r) = a `divMod` b in let (d, s, t) = extended_gcd b r in (d, t, s - q * t)
Ah, I see your point! Thanks for the explanation!
Sure the text editor can format the code to the taste of the developer, but you don't need to store anything else then source code for that! The source code is parsed and then pretty printed to taste. The same principle can be used for source control. What I would object to is not being able to easily know exactly what the source code comprises of, that it would be stored either as an AST with some annotations or in a format that is not human readable through a regular editor. Anyone who has tried to debug a word document know the shortcomings of such a scheme.
 extended_gcd a 0 = (a, 1, 0) extended_gcd a b = let (q, r) = a `divMod` b (d, s, t) = extended_gcd b r in (d, t, s - q * t) ...because having `in` at the end makes it easy to mistake the whole thing for a `do` block on a quick glance. That said, in general you're right about the forward-reference thing, but with such short functions, that doesn't matter. `where` is perfectly fine in this case. foo = go 0 where go n [] = ... ... is perfectly idiomatic, and doesn't cease to be less so if you call `go` `f`. What I *do* not like about that code is the usage of `a` and `b` instead of `x` and `y`. `a` and `b` are usually reserved for type variables.
As a MLer I prefer to control variable scoping and reserve simultaneous let for mutual recursion. It's a matter of taste and your version is equally allright. But I'm not sure why would someone see a `do` block here and what harm that confusion would do, as do-blocks are equivalent to normal blocks in absence of monadic binding (`&lt;-`). I could actually have written -- even if I agree that would be a bit strange and could introduce confusion as to what monadic effects I was thinking of using extended_gcd a b = do let (q, r) = a `divMod` b let (d, s, t) = extended_gcd b r (d, t, s - q * t) I don't think `(d, t, s - q * t)` can be compared to `go 0` as long as "short functions" goes. The `go 0` example is indeed perfectly clear because there is only one identifier whose definition is delayed. If I remember correctly, there is a syntax in Scheme following exactly this pattern, the "named let" iteration method. This is ok because, even if you don't guess what `go 0` is doing, you can keep the whole expression in your immediate memory while you go looking at the definitions below. On the contrary with `(d, t, s - q * t)`, the expression is too complex/opaque and uses too many generic variables: on reading it I don't know anymore which variable where already defined and which are new, and when I read the definition below I don't remember anymore how they are used in the final result. Three passes are therefore needed: I see the expression, then look at the definitions that follow, then look back at the expression to finally understand how everything is plugged together (if I'm not jumping back-and-forth between uses and definitions). Due to the high popularity of the `where` construct, some people have taken the habit of, when they spot an `&lt;expr&gt; where &lt;defs&gt;` construct from the high-level structure of the code, jumping directly to the definition (much as how you often implicitly ignore the type annotation upon first reading) before reading the expression body. In essence that's a way to systematically turn where-blocks into let-blocks. I still think that, if we use `let` and ` where` with taste, a top-to-bottom reading style is the most efficient/elegant, and that's the one I assume when writing programs.
I think you got the Miller-Rabin test wrong. For a false-positive probability of 2^(-100) you need to run the Miller-Rabin test 100 times with randomly chosen numbers.
There are people doing signal manipulation in interesting languages: [synchronous programming](https://en.wikipedia.org/wiki/Synchronous_programming_language) (eg. Lucid), visual programming languages derived from Max/MSP ([PureData](https://en.wikipedia.org/wiki/Pure_Data)...), or [Faust](https://en.wikipedia.org/wiki/FAUST_%28programming_language%29), etc.
I do think your code is ideomatic haskell, its just that ideomatic haskell tends to use too terse a naming convention. I think this needs to be pointed out approximately 250 times on reddit to start getting traction on better naming conventions for ideomatic haskell.
Netflix has the Chaos Monkey.
This is really fantastic. The many out of date TH tutorials out there are really frustrating for those looking to learn.
Yes, not interfering with profile results can be difficult. At the time I wrote my prototype, the RTS writing eventlogs was by far the biggest bottleneck at runtime - my trick was to place a FIFO in the place GHC would *normally* write the eventlog file, start the program (causing the GHC runtime to block early in startup,) and parse the eventlog incrementally, as the program runs - effectively limiting the speed of the program by how fast you can read off the FIFO. The funny thing is that, IIRC, eventlog-linked programs actually ran faster under my little prototype than they normally did, because parsing was faster than writing to disk. Another idea is to just extend GHC with a component that can broadcast profiling data to the network, much like the JVM does. Then you wouldn't quite need as many hacks like the one I pulled above, and it of course aligns with the need for real time profiling results.
Haskell does not _need_ monads. Monads are an abstraction, they permit us to view many different kinds of things as instances of a more general phenomenon. There are monads just for function application before you ever get to IO. Furthermore, monads are not about sequencing of operations. List sequence things wonderfully. Composition of partial functions, the Maybe monad, is no more sequenced than composition of regular functions. The obvious way to do IO in a declarative language is to embed an imperative language into it. One horrible way to do this is to represent imperative programs as strings containing the relevant program text. A better way is to embed the abstract syntax. For example, here is the syntax for Winskel's IMP language (eliding defintions for arithmetic and boolean expressions) c ::= **skip** | X := a | c ; c' | **if** b **then** c **else** c' | **while** b **do** c You can embed this into Haskell as skip : Com store : StorageLocation -&gt; ACom -&gt; Com seq : Com -&gt; Com -&gt; Com impif : BCom -&gt; Com -&gt; Com -&gt; Com while : BCom -&gt; Com -&gt; Com A value of Com _represents_ an imperative program, and the operations on these programs can be "sequenced" just as well as operations on lists can sequence their elements, without any sacrifice of purity. Now you can say that main has type Com and running a Haskell program runs the imperative program represented by the value of main. The point of this diatribe is that there is no monad in the sense of IO (instead we have a monoid) but we can run imperative programs just fine. Monads are not about ordering of operations.
I feel that part of my haskell education will involve writing a Monad Tutorial at some point. Kind of like backpacking around europe to find myself... &gt; Just know the types of your Monads and what you can and can’t do with them Thanks for the explanations :)
&gt; top-to-bottom reading style I don't think I ever actually did that with code. I criss-cross until I have the rough structure of the code modeled in my mind, then recurse into parts of that structure to get details. As to what a "short function" is... anything that doesn't do any real computation, but is plumbing, or mere collection of results. `s - q * t` should probably be let/where bound.
Agreed, I was oversimplifying here a bit. In practice we tend to skim other things using vague heuristics to infer structure (that's typically the part where indentation and keyword highlighting helps most), jump in the middle of code to find what we are looking for (when we are looking for something specific), etc. Still, in the situation where the goal is to understand the *whole* code (and not to extract some specific information out of it), eg. for code review, or locally once you narrowed down your search to some piece you must understand as a whole, people (ar at least *me*, you could to things entirely differently) tend to have a mostly top-to-bottom reading direction. That's the whole point of having `where`, actually: sometimes it's nicer to know the details of how something is defined *after* we see how it is used. If everybody had a highly-efficient out-of-order code reading technique, we wouldn't need/want binding constructs in both directions.
&gt; using the specified sampling rate How is that specified?
&gt; By applying `return` to a value, you have made a Monad. I, for one, highly discourage this use of the term `Monad`. `Monad` is a typeclass, and "a Monad" is (imho) a type that is an instance of `Monad`. So for example, `IO` is a Monad, `Maybe` is a Monad, etc. But `return 3` is *not* a Monad. It is a *Monadic value*.
Good point. It's good to keep the meta-levels straight.
&gt; Furthermore, monads are not about sequencing of operations. List sequence things wonderfully. Composition of partial functions, the Maybe monad, is no more sequenced than composition of regular functions. Not correct. If I access the last element of a list of operations, the list does not guarantee that the earlier operations happen before.
You're welcome. I'm glad it was useful.
&gt; Monads are a way to guarantee the order of operations in a purely functional, lazy language. Err, no. [The reverse state monad](http://lukepalmer.wordpress.com/2008/08/10/mindfuck-the-reverse-state-monad/). There is an ambiguity in what "Order of operations" means. In some cases people are talking about order of evaluation (eg. when you reason about strictness/lazyness and the time/memory behavior of a program), in other cases it's about something, well, different... or it means nothing at all.
Sequencing is also provided, maybe at a more fundamental level, by function composition -- and that's what the usual State monad do, they use composition to sequence state transformers. Just for fun, you could turn this into a monad by adding antiquotations -- the ability to inject any Haskell values as an atomic term of this language, so that evaluation of a program could return arbitrary Haskell values. Of course you would need the type `Com a`, where `a` is the type of Haskell value stored.
This explanation of monads is simple, easy to understand, and wrong. For instance, you cannot explain the list monad, which models nondeterminism, with the "monads are about the ordering of operations" model. Or `Cont`.
What are "earlier operations"? From a denotational point of view, every list operation denotes a sequence of elements. Imagine a total functional language if you like.
I'm unclear on whether Data.Acid is thread-safe. How does Data.Acid's performance compare to, say, Sqlite or Postgresql? (I realise that I don't have the impedance of dealing with SQL and relational data if I use Data.Acid, but how much performance am I trading for it?)
&gt; Sequencing is also provided, maybe at a more fundamental level, by function composition -- and that's what the usual State monad do, they use composition to sequence state transformers. Or more generally, "sequencing" is provided by any category. This discharges the need to explain &gt;&gt;=, return, and the monad laws. What remains is what I would consider the main "point" of monads: homsets of the form Hom(a,Mb) where M is a functor. What should we say about these? &gt; Just for fun, you could turn this into a monad by adding antiquotations Yes, and then BCom would become Com Bool, etc. You could devise an imaginary history of how we got IO :)
Well... in ((a*b)+c) the * must happen before the +. It's not exactly hard to sequence operations in a pure functional language - but monads provide a representation that's easily composable too. Do monads sequence things at all? Well, the trouble with abstracting so much detail away is that any "what it's for" is bound to be a special case. Despite that, in the expression a &gt;&gt;= \b -&gt; c &gt;&gt;= \d -&gt; return (f b d) - the source of the values for b and d can only be "earlier". The value of b can only come from a etc. The right-hand argument of bind is always a function, and it's argument can only come from the left-hand bind argument. There is an inherent sequence there. Evaluation-order sequencing isn't the only kind of sequencing, and one of the quirks of lazy evaluation is that this isn't really sequencing by evaluation order anyway even though it looks like it, but I found your explanation pretty clear. 
i don't understand why Maybe is a monad. in his explanation he implies that Maybe uses ordering. But, I can implement Maybe in Java, no monad pattern. what's he talking about? is Maybe-as-a-monad just an implementation detail? can you implement Maybe without monads in haskell? (I mean, obviously you can, right?)
Maybe is a monad because it has been defined as an instance of Monad. You can get the same simple semantics of Maybe in Java (as in having a Nothing and a Just constructor), but in order to get the Monad functionality, you'd have to implement &gt;&gt;= and return somehow in Java.
Actually, I think `[]` works better than something like `Maybe` or `Identity` does. Those just reflect the intrinsic ordering of nested function application, with `Maybe` adding failure propagation. But in the list monad, the order in which the nondeterministic operations occur is preserved by the order of possible results in the list. On the other hand, `Cont` is more about *disordering* operations.
I guess you become an attractive candidate by learning Haskell to an intermediate / advanced level and demonstrating your ability to stick to a project by writing a small open source software thing.
The point is that operations don't "happen", ever, not even in the case of the IO monad.
The monad operations for `Maybe` can be implemented easily in any language with the required features. It's pretty simple in C#, for instance. What you can't do in most other languages is generic functions that work with any monad.
The reverse state monad still guarantees an order. It's just the reverse order. Without a monad, there's nothing to reverse. &gt; There is an ambiguity in what "Order of operations" means. Sorry about the ambiguity. By order, I mean that you are defining a "before" and an "after" about your operations. a&gt;&gt;b means a happens before b. In general, you cannot control the order of actions with lazy evaluation. From the post you linked to: &gt; it used the state from the *next* computation and passed it to the *previous* one You cannot talk about "next" and "previous" without Monads. That's what I mean by order.
That sounds like a challenge.
hmm. Here's a Java implementation of Maybe: https://gist.github.com/1236562 what functionality does this implementation lack, compared to a monad-based implementation? 
&gt; What are "earlier operations"? Well, that's sort of my point. Lists define a sequence of values. But lists don't guarantee that if I have a list of operations [a, b], that a happens before b. Something else has to guarantee that order, if you want it.
Where is &gt;&gt;=? Why is there a `get` in the Optional class? As camccann says, the main point lacking is that with monads, I can write code, like mapM, that is generic in that it operates on any monad instance. I cannot do that here because there is no abstract notion of a monad which Optional instanciates.
There are two notions of "order" implicit in a monad, and neither is intrinsic to it. One notion of "order" is that of function vs. argument, or equivalently the order of function composition. `h . g . f` has a clearly defined order; when the combination is applied to a value, `f` will always be applied "first". The order in which expressions are evaluated may differ due to lazy evaluation, but that doesn't matter. The other notion of order is that of the "structure" of the monad, e.g. failure in the case of `Maybe`. In some cases there is no significance to the ordering, such as with `Maybe` where a `Nothing` at any point causes the whole expression to be `Nothing`. With `State`, the second kind of ordering is again function vs. argument, but applied to the state value. Things are murky where `IO` is involved, but it's close enough to just think of the ordering as being a list of the IO primitives that were used. In all of these cases, nothing is inherently sequential beyond the degree to which function composition or a list is sequential. In particular, lazy evaluation can *still* shuffle the order in which things are evaluated, the same way it could if you desugared the monad and inlined the definitions of the `return` and `(&gt;&gt;=)`.
It's not. This is a common explanation of monads, but one of the most harmful. This isn't intended as an attack on your understanding of Haskell or anything; many of the best Haskell programmers can write some of the least helpful monad tutorials.
A fair point. Still, it's necessarily restricted to explaining a subset of the possible monads out there, and those that it does explain, it explains misleadingly.
That's a good question. The answer is that when Maybe is defined as an instance of Monad, I can compose two actions of type Maybe a, such that if the first is Nothing, the composition returns Nothing, but if the first is Just a, then the second action is performed on a. As I described in the article, it's a way of short-circuiting a sequence of actions.
&gt; Why is there a `get` in the Optional class? Why is there `fromJust` in `Data.Maybe`? :]
Cont is about ordering operations. The whole idea of interrupting and resuming computations relies on an underlying idea of an order of the actions. The actions have an order, and I can get a value that represents the sequence of actions *after* a certain point and call it later. The "after" is what makes it ordered.
The notion of "earlier" is operational. Like I said, take a total functional language and then you can fix the reduction strategy to always do a before b (or anything you like). If I say, in IMP, that in c;c', c happens before c', then that notion is internalized in that in seq c c', I can say c happens before c', but this has nothing to do with the evaluation strategy I use for seq and does not require a monad.
The functional police have been alerted to your use of partial functions. They will be at your residence shortly. Do not resist. &gt;:)
&gt; The notion of "earlier" is operational. Like I said, take a total functional language and then you can fix the reduction strategy to always do a before b (or anything you like). Agreed. But then we are no longer talking about lazy evaluation.
Like many explanations of monads, it's just correct enough to mislead people. Whatever a monad "does", for suitably useless definitions of "does", is going to be sequential in exactly the same way a monoid is. That's what the monad laws are, after all. But that's not what most people are going to get from "ordering of operations", and the distinction is likely harder to explain than monads themselves.
&gt; What should we say about these? In a Kleisli category, arrow ends can have a fancy structure, internally. Those are *effects*.
Right.
You have exactly the same machinery of monads, IO, etc. in a total functional language, regardless of what reduction strategy is used. Do you understand my contention? IO is a denotational matter, not an operational matter. By the way, did you know Haskell is not a lazy language; laziness is a strategy for nonstrict evaluation. Any strategy which finds normal forms when they exist should be permitted.
Do you know if there's any way to do this sort of thing without hiding Prelude/Num?
The List monad and the Cont monad most definitely have to do with sequencing of actions. The List monad is used to represent computations that have zero or more possible answers. For instance, the paths in an NDFA or in a game tree. One way to see my point is that to see that the paths have a definite order. Or look at it this way (borrowed from http://en.wikibooks.org/wiki/Haskell/Advanced_monads#The_List_monad): pythags = do z &lt;- [1..] x &lt;- [1..z] y &lt;- [x..z] guard (x^2 + y^2 == z^2) return (x, y, z) The monad guarantees that the return does not happen if the guard fails. This is an explicit order. &gt; This isn't intended as an attack on your understanding of Haskell or anything Thanks. But I felt no personal attack. But I still think what you said is wrong. List monad does impose an order. As does Cont. Let me be very clear. By "order", I mean that given a&gt;&gt;b, the action b cannot be executed without a being executed before it. Cont satisfies that. List satisfies that.
See that's beautiful, I think simplicity is a feature some languages take for granted. The easier it is for the programmer to not have to worry and just get to work the better. 
Nonsense, I've written only total functions. Whether I may have used partial functions to implement them is irrelevant. The functional police only enforce extensional laws, after all. &gt;:]
&gt;Let me be very clear. By "order", I mean that given a&gt;&gt;b, the action b cannot be executed without a being executed before it. Cont satisfies that. List satisfies that. Define "execute". There's no generic notion of execution that makes sense for all monads; I concur with [what camccann said](http://www.reddit.com/r/haskell/comments/ntox2/why_monads_what_i_learned_about_monads_working_in/c3bw8ro?context=3). For instance, the reverse state monad flows its state *backwards* in time, but the results of computations still flow *forward* through the binds. What about the `Reader` monad? That's not about ordering of computations, and it's *fully* lazy — no sequencing or ordering involved — it's about providing an environment. What about `Writer`? Using lazy evaluation, you could even execute both sides of a bind *in parallel* and it'd still be `Writer`! That's not sequenced at all.
You're right. There's no more ordering for a Monad than from function application. Monads just provide a convenience on that. I suppose, in that way, Haskell doesn't *need* Monads. Thanks for clarifying!
Let me get right down to center of what my dream haskell book would be like. I want to get right into the soul of the language. A books that jumps right into the everyday important things of the language, using libraries, cabal, passing state, recursion. Something that starts with the big picture and progressively gets more detailed as knowledge is gained. Most texts seem to start with the little details and slowly get to the big picture. Maybe starting with a real, visual project. And I say visual because nothing is cooler than writing code to see things change on screen. Maybe a section with don'ts would be neat, to show the haskell is different idea, like show a runnable example of someone trying to program imperatively or OO style and why it doesn't work. 
Well, we're going to detain you without charge and hope no one forces your evaluation anyway :)
I've had the enormous luck of being able to participate in GSoC twice for haskell.org, so I felt obliged to drop in my 50 cents. You obviously need a good project proposal. [This](http://www.reddit.com/r/haskell_proposals) can be a good source, or you can ask around in the community, or come up with your own idea. However, you shouldn't worry about it too much at this point, since the deadline for proposals is still far. Supposing you come up with an interesting proposal (not too hard, not too broad), the main goal should be to convince the possible mentors that you are able to bring the project to a good end. A well-written and detailed, concrete proposal is important -- and you will be able to greatly increase its impact by referring to *past* projects: probably the best way to convince your possible mentors. So, start a project. It can be related to what you want to do, and this might even help a little. Write tests, write benchmarks (if applicable), write documentation, perhaps write an article about it on a blog. Make sure to release it on hackage, don't leave it floating around github: get your code out there.
&gt; Define "execute". There's no generic notion of execution that makes sense for all monads; I concur with what camccann said. Point taken. My definition of "execute" is merely notional. Thanks for your help in clarifying my ideas.
Which isn't to say that the ordering of function application isn't important, but yes, `Monad` provides an abstraction that happens to describe a variety of things in a convenient way. The only extra thing that might be called necessary is `IO`, but that could also be done without monads. As mentioned earlier, lists also create an ordering, and if the runtime interpreted a list of IO actions rather than relying on `(&gt;&gt;=)`, you'd get the same effects and order of evaluation that you do with the monadic interface.
The point of the IO monad is to model side-effecting actions. At compile time, that's pure-functional composition of values. When the final code is executed, what was previously modelled is now "real" - the side-effects included. IO actions are both pure and impure - two different points of view, one focussing on compile-time, one on run-time. Alternatively, GHC isn't a compiler at all. It's just an interpreter used to interpret a specification (source code) and generate a complex but pointless sequence of bytes. Only if you intend to treat that sequence of bytes as an executable program is GHC a compiler rather than an interpreter. Of course IO is only one monad, but then again... what's the point of a parser combinator that's never used to do parsing, for instance?
I like to think of a Monads boxes. I can't recall where this analogy came from, but I doubt it is my own analogy. My idea of a monad is entirely inaccurate To put it concretely. There are objects that are inside of this box. These objects can only be manipulated from within the box. The laws differ from box to box. Although the fundamental monad laws must hold true.
I'm glad you've managed to refine your understanding :)
Thanks, everyone, for an enlightening discussion!
I have been a GSoC mentor before, and this comment is exactly correct. When I looked at applications the question I asked was: "how likely is this person going to be able to complete this?" which is a question best answered by seeing what other projects the applicant had completed.
That's you in the lecture? I love that lecture, really awesome and gets to the heart of it. I was half way through it by the time I saw your comment haha. You remind me of a friend of mine, your demeanor and humor especially. 
I never found it useful thinking of Monads (or Monoids, or Eqs, or Nums) as anything other than a set of methods and laws. I think the confusion (of what little there is that isn't caused by all the blog posts) lies in having a superficial grasp of the type system and type classes (and their flexibility).
Monads don't imply sequencing; data dependencies imply sequencing. That said, many monads are such that they actually imply sequencing, but that's a property of the actual monad rather than monads in general.
Thanks! I'll certainly check those out!
The Haskell Prelude is designed by people who knew about abstract algebra, but that's not how it's structured.
It's strange to see there has to be a abs function for every Num instance. 
&gt; the haskell version requires a comment to explain the terse name. It's not so much the terse name as the uncommon name. "egcd" would be perfectly understandable without comment, unlike "eea" (for "Extended Euclidean algorithm"). The original is called "extended_gcd", too.
&gt; The reverse state monad still guarantees an order. Um, no. If you want another example where the order of evaluation of a monadic computation is driven by the consumer, not the producer, take a look at [Fun with the Lazy State Monad](http://blog.melding-monads.com/2009/12/30/fun-with-the-lazy-state-monad/), and [part 2](http://blog.melding-monads.com/2010/08/25/fun-with-the-lazy-state-monad-part-2/)
How many threads alive, how many threads runnable, how many waiting. so if my server are waiting for some resource infinitely, this can reveal that.
Transactions in Data.Acid happend in memory, and writing a log to disk. I haven't used it myself, but in theory, it should be much faster than any RDBMS, 
The urge is rising within me, but I resist. My title would be "'Monad' is an adjective, not a noun", but I'm not sure what I'd put in the body after that that wouldn't be redundant. I would suggest that people in general should start trying to use "monad" as an adjective in normal writing, though. You would not say that Maybe is a Blue, we probably shouldn't say Maybe is a Monad. Maybe has been made monadic. (Unfortunately, the nounness of the word "monad" is awfully ingrained, and "Data.Maybe.Maybe is monad" doesn't sound right, even thought "Data.Maybe.Maybe is awesome" sounds fine. Well, if you spot me the pronunciation of a datatype name...)
How do you explain `Cont`?
What do you mean by an uncommon name? I mean that extended_gcd is superior in *all possible ways* compared to egcd. I think the tendency of choosing a terse name like egcd in haskell while choosing a descriptive name like extended_gcd in python is a *problem*.
&gt;or Eqs, or Nums [...] set of methods and laws Or sometimes just methods :)
I dunno. There are still laws, I think. You know the phrase "law of the jungle", right? Yeah, `Num` is kinda like that.
I didn't say that it's the best possible design, just that the people doing were aware of abstract algebra. But there are many concerns in designing these type classes, I've yet to see one I really like. 
First off I don't know Haskell very well I'm trying to learn myself. But I think, the most important thing is never to stick to one language. to add to what krappie said, you're going to want to learn C at some stage since it's used everywhere, and it's the standard comparison language, and every language seems to be able to interface with it. python is a good stepping stone for C since it's got all the Object stuff and a C interface for writing fast code. I think a question you want to ask yourself is do you want to start with a good/fun language and then have to learn some less fun languages, or start with a something less fun and know there's something better. do you want your language to yell at you every time you try to add a string and a integer? or do you want your language to try and guess what you want to do and then scream at you later because the string and integer were in the wrong order? personally I've tried to learn other languages after first learning python but I always come back to it because it's fun to use. the most important thing to remember is your first language you learn should never be the last. you could either go pick a lang, and start learning, if you don't like it (give it time what may annoy you at first may be beneficial later on) go find out what you don't like about it and find another that doesn't have that "feature". there's also a possibility (never heard anyone doing this so don't come complain to me if it doesn't work) learn two languages at the same time, (for ex. Ruby or Python and Haskell) each program you make, make it in both languages, this should help you think algorithmically and not specific to a language. I hope I've confused you enough.
You should have a look at [the retrospective on GSoC 2011.](http://www.gwern.net/Haskell%20Summer%20of%20Code#results-1) The [Haskell Proposal subreddit](http://www.reddit.com/r/haskell_proposals/top/?t=all) may also be helpful.
Data.Acid is absolutely thread-safe.. that is half the point. There are no formal benchmarks yet on performance. But, it should be very fast. I believe it is in the range of 10,000 - 100,000 updates per second. Depends on how complex your updates are and how fast your machine is of course. 
yup, that's me! glad you like it.
That is for steady-state performance. When the log gets huge, I imagine start-up latency could become very painful for acid-state.
For reference, since no one has linked it yet in this thread: the home page for acid-state and safecopy is [here](http://acid-state.seize.it/).
To be fair though, we do say "the integers are a ring" rather than "the integers can be made ring-like". This is the same situation. And in the case of IO, the monadic structure is almost all we know about it, other than certain given functions with output type IO...
Then it would be helpful if the documentation on Hackage said so (or if it already does, if it did so more prominently).
Not if you [create checkpoints](http://hackage.haskell.org/packages/archive/acid-state/0.6.2/doc/html/Data-Acid.html#v:createCheckpoint) regularly. 
I see. That's nice. Important to know.
Great work! Is this tool available as Snaplet? That would be a great way to integrate into existing Snap servers.
Interesting. Unfortunately, the derivation derails from the point it defines `fact 0 = 0`. As it is, `fact n == 0` for all `n`. For me, the giveaway that something is amiss was using `mod 0` in the derivation.
&gt; After all, if you find a way to use data structures and persistence such that recovery is efficient while still safe during a crash, you've basically safeguarded against the Worst Possible Thing(tm) your program can do. Persistence is good, but it's not quite enough. Remember: "Insanity is doing the same thing over and over again but expecting different results". Computers don't play with a full deck of cards, so you might get this pattern: recover-crash, recover-crash, recover-crash... To deal with the worst case you need some kind of robust scheduling, so that presents with bombs in them won't be reopened too frequently (especially if your users are Greek). With non-persistent systems you don't have the same problem, because there are no retries.
There's a little select box in the top right of the UI. Note that the monitor server only asks the RTS for stats when it receives an HTTP request, so the sampling rate is a property of the client.
It will be soon. I removed the Snaplet from the API last second as I ran into some snags. I will readd it in the next release.
Nice! I was actually thinking the other day about that version of `ListT` and whether something like this would be possible. Pretty cool to see it all worked out like that. :]
Thanks, I've made the correction. For some reason, I corrected this in my original code, but it never made it into the final text.
&gt; What do you mean by an uncommon name? I mean that I've seen a few implementations of this function with names matching /gcd$/, but none matching /eea/. Even the Wikipedia page of [egcd](http://en.wikipedia.org/wiki/EGCD) has a hatnote redirecting to the right place, so it can't be just me. Usually you don't name functions based on the specific algorithm used, but based on what they compute, so it's unexpected. (it's not just Haskell either, think of e.g. DGEMM, mpz_mul, cos, erf, RFFTF or sort) &gt; I think the tendency of choosing a terse name like egcd in haskell while choosing a descriptive name like extended_gcd in python is a problem. Look at the [special functions defined in SciPy](http://docs.scipy.org/doc/scipy/reference/special.html), a Python module. There are names like kn, k0e, sph_jn, nbdtrc, kolmogi, lqn, sh_jacobi, hyp2f1, lambertw, zetac, cbrt, expm1. The preference for terse names isn't a Haskell thing, it's a math thing. &gt; I mean that extended_gcd is superior in all possible ways compared to egcd. extended_gcd takes up more screen space than egcd, cluttering up code. It becomes harder to see bugs, takes longer to read and more difficult to keep it all in your head. Perhaps by not all that much for one function, but it adds up. Pen and paper are a good way to extend your brain, kind of like swap space. Source code is like read-only swap space. If you have code that's encrypted in verbosese, then it doesn't work as a swap space nearly as effectively, because every swap-in operation takes so long that you forgot something else while doing it. And any patterns become obscured, too. Of course, you need to balance the above with the effort of committing the common names to long-term memory. In the case of something like egcd, however, you probably need to look up the function at the beginning of the session anyway, because you probably don't remember what order the return values were in, so you never gain anything from a longer name anyhow. (note also that in the OP, the different versions return different sets of values, another reason to look the function up..)
You probably mean `` `mod` ``, not `mod`, in your derivations.
lambertw is lambert's w -- what else should they have called it?
I agree. Please allow me to muse: I'd observe that in both cases the word comes from heavy math. By the time you get there via the math routes, the grammatical category isn't going to confuse you. I'd further observe that by the time you get to this point in math, verbs and nouns are no longer distinct things in mathematics. Lambda calculus makes nouns out of verbs (numbers out of functions) and computation theory makes verbs out of nouns (numbers as functions). Noun is pretty much just the default English category. Now that we're trying to talk to programmers, who may have escaped with nothing more than school-calculus and have possibly forgotten that, and given that we have and are experiencing repeated failure to explain what is ultimately a rather simple concept, maybe it's time for a bit of a nomenclature change. I mean, really, this is not a good sign; it's one that we've become inured to, but it should not be this hard to explain this concept. Something is fundamentally wrong with our approach, and I'd suggest that the way everyone uses "monad" as a noun is as likely a candidate as any. If you think about it, I think you'll see it underlies every wrong discussion of "monads" floating around out there on the net. Pursuant to my own advice, I just edited this message to decapitalize "monad". Generally we don't capitalize adjectives, except ones based on proper names.
Keep in mind that, mathematically, monad-as-a-noun refers to the entire definition, not just one part of it. In that sense, it's incorrect to say that `[]` is a monad. Rather, the type constructor `[]`, `concatMap`, and `(:[])` together form a monad. Conflating the types themselves with the structure of a monad defined using them is also part of the confusion. &gt; Generally we don't capitalize adjectives, except ones based on proper names. We do capitalize type classes, though; so write "Monad" iff you're referring to that. I try to do this consistently, since monads in category theory, monads specific to Haskell, and instances of `Monad` are not the same thing.
Nice!
I'm not sure (M/m)onads are such a simple concept. I feel it's more on of those "simple once you get it" things, like initial algebras, or the Curry-Howard isomorphism. I don't think it's so surprising that people struggle a bit to understand the concept, or that it is necessarily a problem with a simple (or a single) solution.
See also [IC-Reals-Haskell](http://www.doc.ic.ac.uk/~ae/exact-computation/#bm:implementations).
How about `extended_greatest_common_divisor`, then? Is that also superior?
Monads _are_ a simple concept. They are fully expressed in a three-line typeclass containing a limited number of simple symbols, and most implementations are themselves very, very simple. There's a bound to how complicated you can get in so little space. By comparison, flip through your Design Patterns book and take a look at the number of moving parts of all but the very, very simplest ones. This is not an unreasonable level of complication. But partially because we present them so badly, people think there's some bizarre noun underlying the system when there isn't. Python iteratables are a simple concept. Fully understanding everything you can do with them can take a while; is it immediately obvious from the description of `yield` that you can build a coroutine-based eventing system from it? (Well, maybe so to the reader, but not in general.) But you don't get people protesting that they don't understand iterators in Python because they don't know how to build such a thing, because `__iter__` is presented as a simple interface and not bizarrely presented as some sort of magic thing in and of itself. It's a one-method interface with a well-defined contract and a series of useful pre-defined implementations and the ability to add your own. A monad definition is slightly more complicated, but _only_ slightly. I'm not saying that one should be able to read a few hundred words and come away with a full, rich understanding of every last thing a monad definition can be and do. That's not the bar in question; would you apply it to anything else, like Python's `__iter__`? But it ought to be possible to come away and understand a _given_ monad definition from reading it, and be able to construct a basic one of your own, and this persist failure to get that far is a bad sign.
&gt; By comparison, flip through your Design Patterns book and take a look at the number of moving parts of all but the very, very simplest ones. This is not an unreasonable level of complication. A lot of that is accidental complexity, though. I think the comparison is actually pretty fair. Look at it this way: Imagine if you were to give all the design patterns esoteric names, explain them with a mixture of mathematical jargon and confusing analogies, and then told everyone they had to understand Patterns before they could write even a simple program in Java.
Done: http://blog.johantibell.com/2011/12/more-monitoring-goodies.html Could still use a better scaffolded UI, right now the counter just show up as a text value.
Why only allow counters and not arbitrary variables? edit: I haven't had a chance to use the package yet, but plan to soon. Thanks for sharing!
It would be possible to support any value that can be serialized, but the more general we lose the ability to e.g. graph arbitrary values. 
This, a million times. Crypto is hard. 
I feel that complexity is not just about lines of code, but also about *abstraction*. Take the "continuation monad transformer". Its definition is a single line: newtype ContT r m a = ContT { runContT :: (a -&gt; m r) -&gt; m r } Do you have a basic understanding about how this transformer works? How long did it take to get that understanding? Is there a simple way of explaining how that single line of code can be useful in modelling control in Haskell? I believe that what makes it so difficult is: * It is very abstract: it has rank 1 *and* rank 2 type variables. * It has a level 2 arrow in its constructors. These difficulties are similar to those encountered when dealing with monads, especially when coming from a non-functional background: class Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b (&gt;&gt;) :: m a -&gt; m b -&gt; m b return :: a -&gt; m a There you have it: higher-order types and type variables, and a bunch of laws these abstract constructs are supposed to verify. Now i'll happily agree that concrete examples are a great help to comprehension, but i don't believe that it takes less than 10 minutes to make a Java programmer understand what the List, Maybe and State monads have in common, or how this pattern provides a solution to side-effects in Haskell.
Awesome. I was just looking for something like this (need to use postgres via ssl, which pgsql-simple does not support). Could you include a small README/docs? You could probably base them off the mysql-simple docs pretty easily.
&gt; Chris Done’s pgsql-simple is a pure Haskell implementation of a small subset of the PostgreSQL frontend/backend protocol. I do think there is technical merit to a native Haskell implementation, and in particular I’ve identified a potentially significant advantage that cannot be achieved via libpq. Agreed, namely extending, and no segfaults. &gt; However writing a native implementation is a significant undertaking, and such efforts invariably lag behind in terms of features. This is an unfortunate problem. Probably the biggest problem with a native implementation, one needs complete Haskell implementations of security libraries. It's partly the 80% problem. That said I completely agree to use postgresql-simple, where pgsql-simple remains an experiment. &gt; I sped up a program I had been using to test pgsql-simple by over a factor of 20 simply by switching to postgresql-simple. It doesn't really matter, but for completeness sake: Without the presence of a test case, it's hard to say, but there are some very straight-forward changes to make to psql-simple that would speed it up by such a factor (for example, it requests the object ids per new connection, which could be cached once for all connections). I can't imagine the parsing of messages to be that much slower. &gt; Among other things using libpq means that postgresql-simple gets the full range of connection and authentication options out of box, including support for Unix Domain Sockets, SSL, and GSSAPI. Yep. Libraries are available for Haskell on Hackage, but it's segfault city. &gt; Also, I paid attention to ensure that application code could add support for user-defined PostgreSQL types with a minimum amount of fuss and without modifying the library, something that neither HDBC nor pgsql-simple could really do. Is that done with the `Result` class? 5. Support for Listen/Notify &gt; Listen/Notify is the perfect solution when you want to write a program that responds to changes made to a PostgreSQL database. Agreed, I added this to psql-simple (commented out). I never had occasion to use it but it can be really nice.
The way I would put it: making some "wrapper type" (a.k.a. polymorphic type/type constructor) an instance of Monad means to provide a way to combine operations affecting the values "inside the wrapper" in a way that "makes sense". Actually, this is true for any functor, not just monads, the specific thing about monads is that the operations may "wrap" the values again, but the monadic way of combinations means you won't get multiple layers of "wrapping". And yes, those quotation marks imply some hand-waving, but my experience until now is that this description is close enough to the truth to give a very good intuition about what's actually happening.
&gt; Agreed, namely extending, and no segfaults. Actually, what I had in mind was getting the number of round-trips for a prepared statement down to one, and other latency-hiding tricks. According to the manual it's legal, but libpq doesn't support sending the next request until after the first request is finished. Of course in other languages this would be a nightmarish programming interface to deal with, but in Haskell I'm pretty sure you can make it look like it's normal synchronous request-response as enforced by libpq. (for the most part, ignoring the point in the evaluation that errors get thrown.) You might be interested in my "async" branch of pgsql-simple on github. Unfortunately I couldn't get your code to stream like I wanted it to (though I didn't put my attempt at that up on github.) Also what I have is a quick hack, with my code kludged to yours, and I'm pretty sure there are bugs that I observed but didn't really investigate. As for segfaults, yeah that's a concern with say OpenSSL or random C packages, but libpq's got a good reputation, so I'm not too worried about that. (Also, hopefully the low level bindings should eliminate almost all user-induced segfaulting... but read my comment at the top of postgresql-libpq.) &gt; It doesn't really matter, but for completeness sake: Without the presence of a test case, it's hard to say, but there are some very straight-forward changes to make to psql-simple that would speed it up by such a factor (for example, it requests the object ids per new connection, which could be cached once for all connections). I can't imagine the parsing of messages to be that much slower. Yeah, I didn't investigate too carefully, as honestly the test program was (sort of) "fast enough" as is, it took about a 36 hours to work through a backlog of data but was fast enough to keep up with new data once it'd caught up. Unfortunately this program is not really something I'm willing to share, so I'd have to try to create something else that's plausible. But the profiler said that pretty much all the time was spent in Aeson, top wasn't showing any significant CPU usage, and disk bandwidth didn't seem to be a problem. It seemed to be spending most of it's time idling, AFAICT. Also your caching of OIDs isn't the problem, as it's a single long-lived connection. In my implementation I did bake in most of the common types, and then lazily fetch Oid's that aren't in the table when I run across them. Sharing an Oid cache works, as long as you have one cache associated with each separate postgresql database. Dealing with user-defined types was a matter of realizing that their Oids are not stable (and can differ across databases) and ensuring the right information is available to the Result method. That is handled by "getTypeName" and "finishQuery" in [Simple.hs](https://github.com/lpsmith/postgresql-simple/blob/master/src/Database/PostgreSQL/Simple.hs)
Monitoring and graphing values are IMHO 2 separate problems. Look at SNMP - it only focuses on how to deliver and distinguish the values. For graphing SNMP values ... yes you must be able to transform the to some Y-Axis values or find another form of graphic representation. It's a whole different field than just reporting the values. BTW, it would be great to have SNMP values for haskell applications and even better allocated standardized OIDs for builtin GHC counters/values. On a slightly different topic: the concept of "current time" is very fragile in monitoring. The response lag introduces noise into the measuring. If you attach time to the measured value at the moment it was measured, the graph should be more accurate and also smoother.
Not just Simon Peyton Jones, but also John Hughes. 
Correct. As mentors we rate projects on a combination of: * usefulness of a proposed project to the wider community * likelihood of the project being completed successfully, this is determined mainly by two factors: * How difficult and *how well-defined* is the project? (Open-ended projects or too vague projects are unlikely to be accepted.) * How capable is the student? Has the student used Haskell before? For serious projects? Did the student contribute non-trivial patches to existing projects? We also recommend that students submit multiple proposals. Sometimes we get multiple applicants for the same project and then we can only pick one.
I think Conor McBride still gives talks barefoot. 
A compromise would be to allow arbitrary Integers or Doubles. Some of the built in 'counters' are the `max` of something instead of the sum for example. Exposing a `modify` function would allow for all of these modify :: (Int -&gt; Int) -&gt; Counter -&gt; IO () -- allowing for: inc = modify (+1) accumMax = modify . max -- needs a good name Right now you can (incorrectly) try to write it yourself with `read` and `set`, but that is not thread safe.
Pattern matching on integers is pretty weird (and the reason that a Num constraint entails an Eq constraint). Pattern matching is generally done on Data constructors, so it's a bit strange that tutorials mention pattern matching in the numeric context first.
`modify` is now in HEAD.
I have been looking at code written by some math phds. Those are horrible horrible java code. For example, the code used 52 case statements to convert alphabets to int. To actually learn the DRY principle, one still have to do some real project instead of small exercises. That's why I'm going to work with my friends to produce a module on continued fractions that will eventually appear on hackage. 
Heh, if you're interested in statistics and algorithms, you might look at the topic of "data stream algorithms". These are algorithms for approximate processing of immense data sets, with substantially sublinear memory requirements and almost constant per-element processing time. I have a dream to implement a library of these algorithms, but I don't have the guts to find time. Maybe you do :) I can tell more if you're interested.
more papering-over of the mistake of laziness
&gt; Probably the biggest problem with a native implementation, one needs complete Haskell implementations of security libraries. It's partly the 80% problem. Yup. I can't imagine pg's wire protocol changing often enough to make keeping up the issue. It doesn't seem like postgresql-simple supports unnamed prepared statements (i.e. "proper" bind variables). Does pgsql-simple? -- I can't help but think that pg's simple query protocol adds a lot of overhead compared to the extended query protocol. I've never done any testing, though.
I must say that I mislike the fact that there are four operators to express application/composition when we should need only one (as in category theory), or at the very worst two (as in normal Haskell functions).
Do you realize that your comments in this subreddit from the last few months seem to have a net voting total that is significantly less than 0? Please at least *try* to contribute constructively. I know you're capable of it.
Mark Lentczner might know more. I also think people managed to cross compile using the LLVM backend.
As does Hal Daumé III. Perhaps that means it's widespread enough that I can get away with it too :)
this is a reflection of two things: 1. i like to have fun with provocation 2. the haskell community (particularly on reddit) is completely incapable of handling criticism i stand by my point...laziness is basically a mistake. it looks good on fold tutorials, but shatters completely on i/o. oleg was charitable in referring to its use as "unprofessional"...more like f%-$*$*g braindead
&gt; this is a reflection of two things: &gt; &gt; 1. i like to have fun with provocation &gt; 2. the haskell community (particularly on reddit) is completely incapable of handling criticism Yes, I can see that. Those two things indeed define "not contributing constructively". Since this is apparently intentional, I'll be removing any such comments I see from here on. &gt; i stand by my point...laziness is basically a mistake. it looks good on fold tutorials, but shatters completely on i/o Good for you. Please feel free to make any points you like by presenting actual reasoning or justification.
This is specifically what I was getting at when I talked about the Python `yield`-based event handling systems to simulate coroutines. You don't need to understand that case to say you "understand iterators", just as you don't need to understand ContT to say "I understand monad". ContT is not relevant to whether one "understands monad". The fact everyone tries to wedge it in (or its moral equivalents) as a prerequisite is precisely the wrong approach I'm talking about. If the interface discussion was properly separated from "understanding every implementation of the monad interface in the standard library", we wouldn't have this sort of pervasive confusion.
How would you handle C++ annoyances like not generating vtable symbol in .h file that contains inline virtual function? My only idea was to make C wrapper that refernces all such possible functions and compile it with g++ to get symbols generated. What you described here sounds like good idea though I'm quite sure there'll be many things like above to overcome to bind to average C++ library.
I think the evidence is plain that oleg does not share your opinion that there is nothing of value in Haskell, and that there is nothing to say about it except to repeat ad inf. the abstract assertion that it's stupid because lazy, without ever elaborating, defending or illustrating it. That in uncharitable moments he considers Haskell "f%-$*$*g braindead" doesn't fit with facts that are well known to you. Appeal to his authority is ill advised for someone in your position. 
tr0lltherapy is probably referring to lazy IO there, not Haskell in general. Oleg's opinion of lazy IO is well-known, I think.
PowerPC32/Linux has been supported in the past fairly well, but I'm almost certain it's never been tried from the cross compiler mode.
[Not good](http://hackage.haskell.org/trac/ghc/ticket/3472). But that is, as the wiki page you're linking, about cross-compiling ghc, which is more complex than cross-compiling any random app, primarily because your app's build system isn't as scary as GHC's. The main issue is that the build system just doesn't properly distinguish between target and donour, neither in terms of system headers or .o files, resulting in fun bugs like code thinking directories are files. You're probably going to have to build the rts for your target platform basically by hand, as the base libraries... but you can ignore base on the first try and just do a ffi call to your platform's puts or something. Prepare to learn a lot of make if you aren't a wizard, yet. The build system isn't for the faint of heart.
that lazy i/o is a mistake is self-evident for anyone who has written even trivial programs with it. the new stanford course on haskell can barely bring itself to be any more charitable when in the notes it concedes that lazy i/o is good for "prototyping and simple scripts". you are apparently an expert in haskell since you have an "M" beside your name...surely this is not news to you i put this complaint in the same category as strictness annotations...papering over the fact that idiomatic haskell tends not to produce worthwhile programs. haskell is the only language i know of where being an expert means knowing how to route around the idiomatic learning material 
Oleg's criticisms are rational and well argued. Iteratee, Enumerator and Conduits are the results of Oleg's criticisms. 
I was taking this as understood by all, of course. The bee in tr0lltherapy's bonnet is always laziness in general; this is true here and elsewhere. There is no evidence that oleg especially objects to Haskell's laziness rather than to the specific approach to IO called lazy IO. That he doesn't think 'lazy IO' is a necessary consequence of Haskell's laziness is a little too obvious as well, since, as everyone knows, he has proposed an alternative, presumably at some considerable expenditure of thought. His attitude has nothing in common with tr0lltherapy's who is here running the matters together, using the familiar difficulties of 'lazy io' as ground for his familiar one-note no-argument laziness trolling. 
Every link involving Haskell and C++ integration is very compelling to me, so thank you for sharing this. I hope to see in the near future a tighter integration between these two fantastic languages.
Great stuff - any improvement in DB adapters is a large win for real world applications. Any chance you can demostrate the pub/sub-like functionality with an example? 
After quick-browsing through the [SWIG documentation](http://www.swig.org/Doc2.0/index.html) I certainly think that a combined approach (using SWIG for mining, by means of a plugin to emit a description file and finally 'nm' output processing) could be the most pragmatic avenue to a 95% solution. Heck, I may even start it as soon I get some air to breathe, sounds like a useful and fun project. In my specific setting, however, this hammer is probably a too big one, as slipping GHC into the build process will probably already cause enough wounds, so that I am wary of adding another dependency.
Everyone knows this. But these are quite ordinary Haskell libraries that mostly take lazy evaluation as a matter of course; they just impose a different structure on typical IO operations than the one found in the standard libraries, denominated 'lazy io'. tr0lltherapy's complaint is exactly as deep as if he has said "the IO monad is just more papering-over of the mistake of laziness" -- in fact he probably has said that, as myriad other trolls have as well. Why not say instead that the IO monad is one of the more awesome results of laziness? 
It was just an analogy that I was using to justify the fact that some 1 line definitions can be complicated to understand. I agree that it would be silly to start a monad tutorial with continuations...
FYI, Paul Chuisano, the author of the article, uses and [authors iteratees in Scala](https://github.com/scalaz/scalaz/blob/master/core/src/main/scala/scalaz/Iteratee.scala), which is a strict language and a language which its creator, Martin Odersky, makes all kinds of silly claims about laziness and functional programming in general.
FYI, Paul Chuisano, the author of the article, uses and [authors iteratees in Scala](https://github.com/scalaz/scalaz/blob/master/core/src/main/scala/scalaz/Iteratee.scala), which is a strict language and a language which its creator, Martin Odersky, makes all kinds of silly claims about laziness and functional programming in general.
&gt; The key point is that only off-the-shelf parts are needed: &gt; 'g++' or comparable &gt; 'awk' (for mining headers, creating "foreign import ccall unsafe") &gt; 'nm' for getting hold of mangled symbols &gt; a small header file (only needed for mining symbols) &gt; various 'awk' scripts that create idiomatic bindings awk for mining headers will work only so far, if you're going to process C++. I recommend to use gccxml (http://www.gccxml.org) instead, "compiling" the modules you're interested in. Instead of a compilation unit (object file) the output is a XML representation of the AST, complete with symbol mangling information and all the other required information. gccxml works well for both C and C++. 
Is there any way to translate the printSum function (under List Functions) to a more intuitive (for me) version with just a mapM_ and a fold? Some kind of conduit-fusion? Though, after thinking a bit, the helper function I'd want isn't too hard to write.
There is no c++ abi, so that'd going to be a wall to using this. However iirc larceny scheme uses your proposed scheme. [Andy Wingo had a good post about it on his blog](http://wingolog.org/archives/2010/09/07/abusing-the-c-compiler). Edit: cleaned up original post which was written from a phone.
No, the [M] means that I'm a moderator in this subreddit. Most of the real experts in Haskell have better things to be doing than fishing links out of reddit's terrible spam filter. :] I'm really not sure what you're trying to say here. You seem to be confusing laziness-by-default with lazy I/O, maybe? Those are entirely unrelated. I'm also not sure where you get the idea that idiomatic Haskell doesn't work, it seems to do just fine for the vast majority of code.
SWIG already has support for OCaml, doesn't it? Wonder if that would be useful as a starting point.
I know the KDE folks have a tool called "smoke" that they use to generate a pure C API for Qt. Basically everything that binds to Qt (PyQt, QtRuby, Perl bindings, Qyoto (.Net), Common Lisp bindings) binds to the C library generated by this tool instead of attempting to bind to Qt directly.
These are really, really nice slides. Thanks! Sorry I missed the lecture :).
This doesn't type check: let loop n = do inc counter loop
Nice library, thanks!
Using swig for mining is outdated and it never works without adding lots of ifdefs. Gcc supports plugins now. You can [program your plugin in javascript](https://developer.mozilla.org/en/Dehydra) and emit the correct data, not the approximate swig stuff. See this link for [writing gcc plugins](http://www.codesynthesis.com/~boris/blog/2010/05/03/parsing-cxx-with-gcc-plugin-part-1/) for lower level handling. There was a set of patches to gcc in the name of [gcc-xml](http://gccxml.org/HTML/Index.html) before gcc 4.5 came with plugin support. But the general idea of gcc-xml is better than swig IMHO. 
Hehe, nice. Let [me][2] know when you're ready to roll. To get started with Haskell &amp;&amp; SuperCollider, I recommend the following [little task][1]. [1]: http://www.reddit.com/r/haskell/comments/np1o8/learning_haskell_looking_for_a_project_to_speed/c3aus8v [2]: http://stackoverflow.com/users/403805/heinrich-apfelmus
Another example: primes = nubBy ((&gt;1) .: gcd) [2..] I like it. While we're listing our favourite Haskell tweaks that we use in every project… -- | When the value is Just, run the action. whenJust :: Monad m =&gt; Maybe a -&gt; (a -&gt; m ()) -&gt; m () whenJust (Just a) m = m a whenJust Nothing _ = return () -- this wouldn't be necessary if prelude's IO functions were overloaded on MonadIO… but oh well io :: MonadIO m =&gt; IO a -&gt; m a io = liftIO (++) :: Monoid a =&gt; a -&gt; a -&gt; a (++) = mappend infixr 5 ++ 
`whenJust` is called [`forM_`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Foldable.html#v:forM_) from Data.Foldable. Took me a while to figure that out. The `++` operator is going to be [`&lt;&gt;`](http://hackage.haskell.org/trac/ghc/ticket/3339) instead. But the ticket has existed for a long time, and it doesn't look like it's going to make it into 7.4... I'd love for every IO operation to have a MonadIO constraint instead. I don't think it would have any runtime overhead most of the time, since GHC is smart enough to eliminate the dictionaries if they are known at compile time, and they almost always will be. It would lead to worse error messages, though (the usual argument against more overloading/type classes). *Edit:* corrected operator for `mappend`.
&gt;I'd love for every IO operation to have a MonadIO constraint instead. You would need something like monad-control for most things that take IO actions as arguments.
The one (minor) thing I don't like about it is that it doesn't scale to even more function arguments. Ideally, we'd use (..), and use (...) etc. for more argument functions. But it seems .. is already used for something else...
Yeah, OK. I thought I was against this because you need another operator each time to handle to three, four, five arguments, but I guess binary is a special enough case. Though I wonder if something like [semantic editor combinators](http://conal.net/blog/posts/semantic-editor-combinators) isn't the True Path here.
That's why I consider SWIG a 95% solution. To get 100% one has to employ [clanglib](http://clang.llvm.org/). All other paths via XML are too heavyweight IMO.
From what I see in the blog post (and the linked paper on larceny FFI) this is a C-only solution. A moral equivalent of the 'hsc2hs' tool. I am interested in C++, which brings in the additional complexity.
IO actions in contravariant position are a more complicated issue in general, aren't they? `liftIO` doesn't help there anyway.
That's true, but it'd work fine for most things. It would be strange to have most things in MonadIO and a few things in regular IO, though.
One thing that I found lacking in Data.Maybe v `justIf` b = if b then Just v else Nothing
Yes; that's why you can't stick them in MonadIO and be done with it :)
Yeah, and `justIf` is the name I prefer for that. I also like having an operator for `flip fromMaybe`, usually something like `(??) :: Maybe a -&gt; a -&gt; a`.
Don't use Yi, just use your favourite plain text editor (not word processor); TextEdit, for instance, can save files as plain text. There are of course more advanced editors, but TextEdit will suit you just fine until you're more experienced; better to learn a language than a language and an editor.
In my own code I use `(...)` because `(...) = (.) . (.)`, however that naming convention doesn't scale very well. Now that I've seen it, I'm thinking the `(.:)` name might work better for the aesthetics of reading a long pipeline of compositions; the `(...)` name introduces too much whitespace and causes the brain to misparse it at first. *Edit:* N.B., `infixl 8` is the best fixity/precedence I've found for this combinator. Also, the best performance is achieved by using the `(.) . (.)` definition with `INLINE`; I'm not sure why that's more efficient than the pointful definition, though I have benchmarked it.
I have often preferred `ensure`: ensure p x = guard (p x) &gt;&gt; return x I switched the implementation strategy; for comparison, here are the other choices for both functions: v `justIf` b = guard b &gt;&gt; return v ensure p x = if p x then Just x else Nothing
&gt;The ++ operator is going to be &lt;+&gt; instead Actually, it's going to be &lt;&gt;; &lt;+&gt; is for something in the pretty print library.
I thought the conclusion to that discussion was to use `(&lt;&gt;)` as the new name...
Ah, the venerable boobs combinator. (.:) = (.).(.)
I've also wanted `justIf`. That's a good one. I'll adopt that.
Hm, `ensure` as generic on monads is nice. I'll take that. This is like a handy function giveaway!
Why the use of `FunPtr`s instead of calling directly?
&gt; whenJust is called [1] forM_ from Data.Foldable. Took me a while to figure that out. Hm, that's a goodie. I used `forM_` all the time but the one from Control.Monad. Not sure if the name is too generic to make me double-take when brushing over code with it (`(.) = fmap` made me do that, I went back to `fmap`), but I'll give a try and see how it feels. &gt; The ++ operator is going to be [2] &lt;+&gt; instead. But the ticket has existed for a long time, and it doesn't look like it's going to make it into 7.4... Yeah, I'm aware of `&lt;&gt;` but that's about as “real” as `(++)`. I was in favour of generalizing `(++)` as the backwards-compatibility problem seems negligible to me (I've made this change in my own code-bases, it's about as breaky as OverloadedStrings; not much); there are far more breaking changes introduced with new GHC versions all the time. The “confusing for newbies” argument is rubbish, just as it was for `fmap`/`map` and removing monad comprehensions. But anyway… I'll take what I can get from the overlords. &gt; I'd love for every IO operation to have a MonadIO constraint instead. I don't think it would have any runtime overhead most of the time, since GHC is smart enough to eliminate the dictionaries if they are known at compile time, and they almost always will be. It would lead to worse error messages, though (the usual argument against more overloading/type classes). Yeah, I pondered if possibly GHC could, if not generally, make a special case for `MonadIO m` where `m` is `IO`. Error messages is a double edged sword, indeed.
I assume you're talking about in Foreign.ObjectiveC.Raw. I'm using LibFFI to call the Objective-C runtime functions for a couple reasons. msgSend is variadic, so the standard Haskell FFI can't handle it, and I'm using the LibFFI argument packing throughout. LibFFI requires `FunPtr`s as arguments to callFFI.
emacs has a decent haskell mode
Ah, OK; I just skimmed it and didn't realise you were using an FFI library.
then tell me strictness annotations exist, and why they tend to show up in expertly-written, high-performance code, and never in idiomatic/instructional code no other language i know of encourages you to drop idiomatic techniques in order to advance to expertise 
thanks, i didn't realize i had to make it plaintext
I just wrote this yesterday. I was surprised it wasn't in Prelude, but it made my code more concise: justIf :: Bool -&gt; a -&gt; Maybe a justIf p v = if p then Just v else Nothing
he seems to have disappeared and I don't know haskell but I would like to make use of this project.
You could take this as an opportunity to learn some Haskell. There's nothing like having a real project as motivation when learning something new.
You already have a decent code base, that used to work in the past. You don't need to know anything. Just figure out which version introduced the regression and fix it. :)
Oops, yes, that was a typo. 
You need to define 'update'. It worked fine for me (see my other comment).
True, but the typechecker will let you know. I'm also hoping that type-information will be more readily supplied to the programmer in the future / hopefully soon, so errors like this will show themselves quite directly.
I don't like it. Not used enough; only implies generalizations that aren't there; and I don't think it makes the code clearer, just shorter.
Seconding this opinion - ".:" has no meaning symbolically, its just something that wasn't already in use. 
Symbolically "..." means ellipsis (omitting things), so I don't find it a good name for a composition combinator. Until Haskell goes fully Unicode, which has its own problems, my personal feeling is that we've enough symbolic operators (caveat - unless a proposal is for a new idiom as useful and general as say, applicative functors).
I left in an extra n when modifying the example. Good catch.
I verified this too, before noticing that `ThomasDuBuisson` had done it. Is there a special problem building it on Windows, maybe?
I read from the page you have posted "Smoke is not Qt specific. However, it is commonly used to write bindings to the Qt library." Btw, this approach should work on Linux, and what about the other two platforms? Nevertheless it's a start.
Thanks for the tip, I'll take a closer look at that.
I only know bits and pieces of the theory, but naively I'd say that they are orthogonal. Laziness can be implemented by adding `thunk` and `force` forms in many languages and you can strictly evaluate things in a function without changing its type.
When you give denotational semantics to an eager functional language, a term of type τ denotes either a value of type τ or a divergent computation, ⊥. A _strict_ function between sets with bottom is a function that preserves bottom, f ⊥ = ⊥. So "strict" presupposes that there is a bottom value in the models for types. The real difference between eager and lazy languages is that lazy languages have more structure in the models for types: rather than a set with bottom (a trivial cpo), there is a rich cpo structure. So its not the types that are different, its their denotations. So I think the answer to your titular question is yes. 
I don't agree with your interesting analysis. You can define the semantics of types as *value* classifiers, in which case, in an eager language, you don't need to have a ⊥ at the type level. You then define typing on *terms* as an approximation that may lead to a value or, depending on your runtime semantics, other things : non-termination, but also "failure", getting stuck, exceptions, etc. In a strict language there is a fairly clear notion of what a "value" is, which excludes variables. In non-strict semantics, it is usually most convenient to think of values as including variables, which themselves may be delayed failures. This has the very concrete consequences that programmers usually don't *think about types* in the same way in call-by-value and call-by-{name,need} languages. When you reason about the input of a function which takes input of type "A+B" (disjoint sum), in a strict language, you really think about it as a disjoint union (because function inputs are values). In a lazy language, you have to consider lifting to get the whole picture of your function behavior. You would say that this is because lazy functions allow for a richer set of behaviors. Robert Harper argues that this is a case where more constraints really help – because they allow to capture familiar abstractions such as disjoint sum. There is a choice, a compromise to make here, just as impure languages are not necessarily better because they allow to express strictly more observationally pure functions. 
&gt; In eager languages sums can be optimized, but products can not. In a lazy language it is the opposite. I think this has to do with the concept of polarity, but I've never seen a formal binding of this hunch. Could you explain?
The [wiki page on cross-compiling](http://hackage.haskell.org/trac/ghc/wiki/CrossCompilation) has been updated by me, reflecting the nice progress Mark has made (**thanks**!). I have also linked the relevant mail thread from the status at the bottom (you can also browse the mails in that directory (April and March) looking for 'cross' in the subjects. Things do not appear very dire to me at least for the upcoming 7.4.1 release. I'll definitely find out in the next days.
What I really wanted to point out is that of the duality between eager and lazy. One has positive polarity and the other negative polarity. It turns out that choosing one over the other has positive and negative consequences in both ways. You can't get everything, unless you allow your language to operate in both polarities (Which GHC does btw through strictness annotations). For the optimization musings, see the link I posted to erereieu - which relates to the difference of what values can be bottom and why.
`thunk` is just a shorthand for () &amp;rarr; 'a (where () is the unit type, also written 1 or *void*). `force` is function application.
&gt; When you reason about the input of a function which takes input of type "A+B" (disjoint sum), in a strict language, you really think about it as a disjoint union (because function inputs are values). In a lazy language, you have to consider lifting to get the whole picture of your function behavior. Can you clarify?
No joins?
Oh. I mentioned this above, but a lazy language doesn't have products either, because if fst . h = f and snd . h = g, then for f = g = ⊥, both h = ⊥ and h = ⟨⊥,⊥⟩ are solutions. For the same reason, it won't have sums. Conversely, an eager language has no products because fst . ⟨f,g⟩ = f is not satisfied (when g = ⊥), but it also has no sums because [f,g] . inl = f is not satisfied ([−,−] is the copairing).
Can ML express a type such as: type ('a iterator) is (unit -&gt; (Done | Next of 'a * 'a iterator)) ? Edit: looks like this *is* supported, using the syntax: type 'a iterator = 'a __iterator Lazy.t and 'a __iterator = | Done | Next of 'a * 'a iterator 
stop lying to yourself. haskell is aggresively non-strict by first principles. how are strictness annotations idiomatic?
v0.3 introduces a better scaffolded UI (i.e. it includes graphs)
[This](http://www.minecraftwiki.net/wiki/Block_id#Block_IDs_.28Minecraft_Official_Release.29) one should be all of them. and you're right. After I started playing with it a little bit, I realized that the json list was just the list of materials that are created for OBJ
Umm, why not just `grep`? And pipe it to `sed` to make the TH?
This interpretation is valid denotationally (in a pure language), but not operationally -- a thunk is evaluated at most once, while your "thunks" will be evaluated every time they are forced.
Without really thinking this through - is there a neater way to do it with a GADT?
I have no deep insight, but I've thought a lot about problems in FRP, and this definitely feels to me like the right way to go.
Perhaps grep is not the best way to parse Haskell source code.
Your first paragraph states that bottom is a type in Haskell, and builds some reasoning on top of that premise. However, bottom is not a type in Haskell; it's a **value** that belongs to every type in the denotational semantics. And since ML can write non-terminating programs, this is also the case in ML: every type must have "bottom" as a value. cdsmith goes into the subtleties that non-strictness introduces. But the short version, as I would put it, is this: non-strict languages are able to make fine distinctions between different computations that, in strict languages, would all evaluate to bottom (i.e., not terminate). * In the denotational semantics of a strict language, any function applied to bottom has bottom as its result. So to use cdsmith's example, "Succ undefined" in ML is equivalent to just "undefined" (well, except for the type). You cannot write a function in ML that treats them differently; any function application with either as an argument evaluates to bottom. * This is not the case in a non-strict language. As cdsmith's comment points out, "Succ undefined" in Haskell is not the same as "undefined", because you can write functions that behave differently for each. Anyway, to answer your question: yes, type checking and laziness are orthogonal. The type checking is not very different from ML, either. Be sure you understand that type checking is **completely syntactic**—it's a constraint on what combinations of symbols are well-formed and well-typed expressions—and none of it makes reference to how the program is executed.
...on my gentoo, 274M /usr/lib64/ghc-7.2.2/ghc-7.2.2 106M /usr/lib64/ghc-7.2.2/base-4.4.1.0 all in all /usr/lib64/ghc-7.2.2 sizes 666M, and that's not even the whole platform, but only a tiny bit more than a bare ghc install (I do stuff at user-level via cabal-install). looking at the individual sizes, 40M /usr/lib64/ghc-7.2.2/ghc-7.2.2/libHSghc-7.2.2-ghc7.2.2.so 73M /usr/lib64/ghc-7.2.2/ghc-7.2.2/libHSghc-7.2.2.a 128M /usr/lib64/ghc-7.2.2/ghc-7.2.2/libHSghc-7.2.2_p.a 240M total compare the statically linked (apart from system libraries) ghc binary: 32M /usr/lib64/ghc-7.2.2/ghc ...surprisingly tiny. Keep in mind that the .a doesn't contain its dependencies, the size difference stems from an .a being a flat archive, a loose collection of .o files, while an .so already has all the internal linkage done, which allows for much stuff to be thrown out. What we need to do, size-wise, is to get rid of either the .so or .a (preferrably the .a), and use an so for the profiled version. The explosion doesn't stem so much from statically linking, but from having multiple ways enabled for everything. AFAIU there's no way to roll the profiling version into vanilla, though, as the profiling code itself gets optimised into the rest. We should probably consider the profile version to be a completely different package, and make both installable independently of each other, and decide, automagically based on size, whether a library is presented as .so or .a and support linking an application with a mixture of both (that should be straight forward). (don't forget to add flags to override any and all magic, though) Heck, .so could probably be used for statical linkage, too. ar is a fossil that ought to die. Dunno if [gold](http://en.wikipedia.org/wiki/Gold_%28linker%29) or some other non-ancient linker supports that out of the box. ghc's .hi files (vanilla, dyn, prof) net 31M, it may be prudent to .xz.tar them or something.
It's always great to see well-known companies promote Haskell :)
Yeah, you probably need Perl regexps for this.
I can't find any Repa import anywhere, so how does it use Repa?
http://code.ouroborus.net/gloss/gloss-head/gloss-examples/Ray/Main.hs imports http://code.ouroborus.net/gloss/gloss-head/gloss-field/Graphics/Gloss/Field.hs which imports repa.
Wow!
Yeah, it's implicitly, implicitly data parallel.
Nice tutorial. I wrote a tutorial about almost the same topic, at: http://www.haskell.org/haskellwiki/Monad_Transformers_Tutorial
I do. That's it.
Good question! Here is my guessing: Looking at wai, it seems to just pass around bytestrings. parseRequestBody from wai-extra seems rather innocent, too, basically splitting a large bytestring into smaller ones. As far as I understand the hashtable attack, you'd have to exploit the way the runtime system allocates objects and I really doubt this step depends on the content of the object.
Advocacy requires a bit of restraint at times. I'm not sure that's the crowd any of the Haskell frameworks are trying to attract right now.
Hi 1nine8four, It is is very similar in some key respects. As I say, people have been doing this kind of things for years -- I just would like to see it integrated into the distributions so that folks would get it as a matter of course. I think the fact that you 'get it' -- appreciate that this kind of thing is highly useful -- really backs up my thesis. Everybody serious developer could probably benefit from this kind of facility so hsenv will be useful for some time. How does the justhub CentOS distro differ from hsenv? In the first place it is a Haskell distribution! So you can say 'sudo yum install ghc-7.2.2-hub' and it will download and install that toolchain and make it available through the hub/sandbox facility. The second way it differs is that all of the toolkit utilities in the justhub distro pass through wrappers that auto-detect whether a special version of the toolkit and specific sanbox have been configured. If not you get the default platform behaving as if it were the only toolkit on the system. This means that there is no need to manually activate and deactivate a sandbox in your shell to use it. A bit like git (for example) it just works things out from the directory/environment/flags context. Thirdly it allows you to name, duplicate, swap and share sandboxes between projects. Fourthly (and I think this is new) it supports full deletion of packages, managing the directories and files that Cabal uses to build the library code and working out when it becomes dead and reclaiming it.
Is there also a quicksort attack?
The DWARF debug info won't help you with, say, templates. If a template isn't instanced it won't show up in the executable. Using the information from gccxml it would be possible to make it possible to "instanciate" those templates from Haskell. Don't ask me, how I'd map this, but I'm thinking along pattern matching here, so if a certain pattern exists, this reflects to a C++ template instance. So you'd have a 4 phase build scheme: 1. Extract AST from C++ code with gccxml 2. Instanciate from Haskell 3. Compile with instanciations from Haskell, extract with gccxml and dwarf mangled names and types 4. Compile Haskell FFI code
That sounds elitist and IMO non productive. A web developer solicits advise on a web framework to use. What better place is there for advocacy? 
Short answer: Nope/Doesn't apply/Yes. That is: 1. At least Yesod uses Map for protocol handling, so it's immune. Probably all do because Haskell doesn't like hash tables. 2. Then, the standard way to implement hashed data structures is to take an IntMap and tack a hash in front of it. Chances are that the buckets are maps, too. Again, worst-case O(log n), though with a higher constant factor. Do note that the attack relies on O(n) buckets, as well as failure to rehash on excessive collisions. 3. If you look at the Hashable class, you will find that it's a joke. Don't use the default instances for data structures, just don't. Use Binary, get a bytestring, and run a decent hash function over the result. ...in general, just don't implement hashtables if you don't know what you're doing, and don't use hashtables written by people that don't know what they're doing. They're fragile beasts with lots of tradeoffs and those should be documented in detail. [Here's a (slightly smug) presentation about the whole issue](http://www.youtube.com/watch?v=R2Cq3CLI6H8)
I think most platforms switched to mergesort, by now. At least perl, that is.
&gt; A language enthusiast solicits advise on how to earn money with Haskell. What better place is there for advocacy of web development? FTFY
&gt; At least Yesod uses Map for protocol handling, so it's immune. Probably all do because Haskell doesn't like hash tables. `snap-core` uses the `Data.HashMap` type from `unordered-containers` for something. Depending on what that something is it might be vulnerable. &gt; Then, the standard way to implement hashed data structures is to take an IntMap and tack a hash in front of it. Chances are that the buckets are maps, too. Again, worst-case O(log n), though with a higher constant factor. Do note that the attack relies on O(n) buckets, as well as failure to rehash on excessive collisions. Using `Map` for buckets would add at least 6 words (e.g. 48 or 96 bytes) of overhead for every entry. It'd also slow down operations (although I don't know how much.) `unordered-containers` and `hashmap` don't use nested maps for this reason. &gt; If you look at the Hashable class, you will find that it's a joke. Don't use the default instances for data structures, just don't. Use Binary, get a bytestring, and run a decent hash function over the result. You might not like the trade-offs made in `Hashable` but they are there because I found them to be the best trade-offs at the time. You don't mention which parts you don't like so here's a guess: * String like types use Bernstein's (DJB) hash, even though it's known to be bad from a collision perspective. The issue here is that DJB hash performs better than e.g. MurmurHash (2 &amp; 3) on real data sets. The author of memcached came to the same conclusion: http://groups.google.com/group/redis-db/browse_thread/thread/dc3c180f51b4ecc2 . I've also investigated CityHash (by Google), but it's quite difficult to use due to having a very complicated implementation (e.g. it's difficult to write an incremental version of it, which we need for lazy `ByteString`s). * `Int`s and `Word`s hash to themselves. First let me note that `id` is a perfect hash function (as in the technical term) for `Int`s and `Word`s; You only run into troubles if you use it in a context where you want two neighboring values into the domain to not be neighboring in the co-domain. If you want that you should multiply the hash with a large prime. Note that e.g. Python's hash table implementation uses `id` to hash integers, as on real life data (i.e. non-random) that tends to give better locality of reference (http://hg.python.org/cpython/file/f38182db6c7a/Objects/dictnotes.txt). This being said, I have considered changing `Hashable` to multiply integers with a big prime, by default. * Mixing function: right now the `combine` (i.e. mixing) function uses a quite simply xor mixing. We should perhaps switch to using the mixing step from MurmurHash. Note that for those types (e.g. string) that implement `hashWithSalt` the default mixing function isn't used. * Using `Binary` will give you terribly performance if you use it with hashing-based data structures due to (comparatively) slow serialization. &gt; ...in general, just don't implement hashtables if you don't know what you're doing, and don't use hashtables written by people that don't know what they're doing. They're fragile beasts with lots of tradeoffs and those should be documented in detail. Between Greg and me we've implemented quite a few hash tables in Haskell (http://hackage.haskell.org/package/hashtables) and a few persistent ones (e.g. unordered-containers using both `IntMap`s and hash-array mapped tries.) The current code reflects the fastest ones we've been able to find. 
&gt; What about e.g. cereal? blaze? Same story. Think about it this way: multiplying an integer with a large prime (one instruction) will always be faster than allocating a byte buffer, writing the integer into the buffer, and then traversing the buffer to compute a hash. The second includes several branches and touches more memory. It will always be slower, no matter the serialization library (or programming language.) &gt; In general, I think a good approach would be to feature-bomb Hashable and provide everything from a fast implementation of the "fits into n bits" minimum over proper distribution over basic cryptographic hashing... not unbreakable because those are slow, but hard enough so that an attacker would have to use significantly more CPU time than you to keep you busy, to actual cryptographic hashing, i.e. SHA and friends. Salting is orthogonal to all of those. Right now there are packages for non-cryptographic uses (hashable) and for cryptographic uses (see Hackage). I'm not sure I see the benefits from combining the two as their use cases are quite separate. Note that you can use `newtype` in case you want to use a hash function from a separate package together with hashable. &gt; Binary heaps, possibly with Van Emde Boas layout? Those should at least keep the cache misses in check. I'm not quite sure what you're getting at here. Using: newtype HashMap k v = IntMap [(k, v)] already adds too much overheadp. Do you mean that `IntMap` could be replaced with binary heaps or that the slots could be?
He wants to get into it immediately. Meaning he just wants the copy of PHP with different (cleaner ?) syntax. Haskell would take him months (if not years) to grasp. Does not fit the desire of the author. His best bets would be ruby/python frameworks. 
&gt; will always be faster than allocating a byte buffer, writing the integer into the buffer, and then traversing the buffer to compute a hash. fusion? &gt; I'm not sure I see the benefits from combining the two as their use cases are quite separate. Well, sometimes using data structures securely *does* require a cryptographic hash. Imagine a situation where the attacker can get hold of the hash, and, with knowledge about the algorithm, can infer the salt. Then imagine a big enough hash table that won't be rehashed with a different salt any time soon. ...it's more about "all in one place", though. And a good opportunity to educate people about what they actually need in the docs. &gt; Do you mean that IntMap could be replaced with binary heaps or that the slots could be? The slots/buckets. You already have to have some kind of list or array there, it's just about changing the array layout to provide for O(log n) lookup (and in the vEB case, cache obliviance at the expense of index calculation). Ordering relations are usually easy to come up with.
Is `$++` a new keyword? I'd like to see an example of code that would be substantially clearer using this notation. 
CLR's byte code is typed; this is done to allow it to compile to efficient code on the target while guaranteeing safety from certain types of crashes and security problems. In particular, CLR code is safe from buffer overruns and other types of out of bounds memory access. However, that safety comes at a cost; not all programs are representable in CLR. In order to guarantee safety, CLR has casting instructions that do a run-time check that the types are valid. In the case of Haskell, which has a stronger type system than the CLR, it's likely that the resulting program has lots of these casts which are guaranteed to succeed (by the Haskell types), but for which the CLR type system isn't strong enough to prove. So the compiled bytecode ends up doing a lot of work at runtime checking that the types are correct. There's a bit of discussion on casting in CLR [here](http://msdn.microsoft.com/en-us/library/8d3h6t9s.aspx)
After wrangling two days with the build system and reading every bit on [building GHC](http://hackage.haskell.org/trac/ghc/wiki/Building), I see now how cross compiling could work. Indeed, I got my first (simple *test.hs*) file compiled to a PowerPC *test.o* on an x86_64-unknown-linux host. I can tell you, I got pretty excited seeing the PPC mnemonics in the objdump output :-) Btw., the above ticket you refer to is not really about cross-compiling, but compiler *trans-porting* to another platform. My changes (to be submitted still) add to the basis that Mark has created, and may well make the *trans-porting* approach obsolete.
Why?
[Ubuntu Server 11.10](http://cloud.ubuntu.com/ami/)?
I get one of their biggest instances using spot pricing and build my software and get it my machine image all set up on this. The CPU is way bigger than what I have at home. Then once the machine is working the way I want and I am done interactively testing and compiling, I snapshot it and suspend it. Then I fire up the production server micro instance. Since the big machines are under a buck an hour, it's worth the speed when I'm doing things interactively and builds are very fast. As long as you stick to 64 bit architecture, you can launch your image on any instance type except small.
blaze-html, binary, cereal, ... currently all have monads that exist solely to be able to use do-notation; the return type is fixed at `()`. What they really want is to be able to use nested layout with a monoidal interface; given a nicer introduction syntax and a generalisation to any Monoid, this would meet that need perfectly.
I would recommend deploying statically linked binaries. It's the common way to deploy binaries to big clusters (in any compiled language). It's much easier to manage than trying to upgrade an AMI on N machines every time you want to add a library.
How about `moo`, for "monoidal do"? :) More seriously, here are some top-of-the-head ideas: * `app` for `mappend` * `coll` for collection * `cat` or maybe `con` for `mconcat` (`cat` is nice but seems fairly likely to clash...)
* ` be ` for not ` do `ing anything
For example blaze-html violates **all** three [monad laws](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Prelude.html#t:Monad): return a &gt;&gt;= k == k a m &gt;&gt;= return == m m &gt;&gt;= (\x -&gt; k x &gt;&gt;= h) == (m &gt;&gt;= k) &gt;&gt;= h Plus, the description of the `(&gt;&gt;)` operator clearly says: &gt; Sequentially compose two actions, **discarding any value produced by the first**, like sequencing operators (such as the semicolon) in imperative languages. For blaze, the only operator that does *anything* really **is** the `(&gt;&gt;)` operator (granted, the value `()` which is "produced" in this monad is discarded, but it shouldn't even need to exist in the first place). Even the type `Html` is not of the form `m a`, only the internal type used to exploit the do-notation is - and there is no other reason to this than the `Monad` instance.
Er. What? I admit I'm not experienced with blaze-html, but I doubt it violates all three monad laws. cereal's [PutM](http://hackage.haskell.org/packages/archive/cereal/latest/doc/html/Data-Serialize-Put.html) monad, which is another monoid-as-monad, certainly doesn't. I think you're misinterpreting "discarding any value produced by the first"; only the actual *result* is discarded. Writer is certainly a monad, even though `tell [1,2,3] &gt;&gt; return ()` isn't the same as `return ()`. IO is certainly a monad, even though `globalThermonuclearWar &gt;&gt; print 42` isn't the same as `print 42`. Edit: OK, I checked out blaze-html; yes, it violates the monad laws: h1 &gt;&gt;= f = h1 &gt;&gt; f (error "Text.Blaze.Internal.HtmlM: invalid use of monadic bind") However, the other examples I can think of don't, and blaze-html's violation isn't related to `(&gt;&gt;)`.
Take a look at its [monad definition on github](https://github.com/jaspervdj/blaze-html/blob/master/Text/Blaze/Internal.hs#L147): instance Monad HtmlM where return _ = Empty {-# INLINE return #-} (&gt;&gt;) = Append {-# INLINE (&gt;&gt;) #-} h1 &gt;&gt;= f = h1 &gt;&gt; f (error "Text.Blaze.Internal.HtmlM: invalid use of monadic bind") {-# INLINE (&gt;&gt;=) #-} 
`be` is pretty much perfect. :)
Whoa... people still ask Slashdot for advice?
I think we have different definitions of "hacky". The code itself is fine, I agree. But there's no other reason to have that `PairS a` type other than to define a monad (and fmap/applicative) instance for it. The `a` is simply not used otherwise. That's all I meant to say. :)
Way too short and liable to clash, IMO.
I would suggest `mdo` for "monoid do", but that was used for recursive do notation until recently... (it's `do rec`) now. `mcat` is okay, but the `mconcat` connection isn't really visually clear, and it's a bit long/weird-looking to litter through code nesting it heavily.
Oh, man. I have a barely started .hs file that is supposed to become a Permutation-focused extension/simplification of [Shan&amp;Thurston's WordNumbers](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers1), but readable at a level that does not require a Masters' degree in computer science and math. Well, I should still do it, and compare the result to this post. BTW, this "{ }" style of commenting is very hard to read, as well as a bit confusing since these example evaluations are not legal Haskell, but are displayed in "code" font as Haskel code: indexOfPerm 0 [] { Definition of perms } = indexOfPerm 0 (perms 0 !! 0) { By assumption } = 0 
What about [hmatrix](http://hackage.haskell.org/package/hmatrix)?
Oh, quick response. Seems about right. Looking into it...
If you'd rather roll your own, you'd want to use Array, with the indices being tuples.
How does this rely on the specific structure of `FullList`? What is the problem when embedding something else?
`FullList` can be unpacked. Any other structure you want to use must be unpackage so it cannot be a sum type.
&gt; If you can hash your data, you can certainly order it! duh, yes. &gt; And similarly, if you can hash it efficiently, you can certainly order it efficiently too! What's faster, ordering my movie collection by file hash or by file content?
Oh, sum type. That makes sense. Thanks.
It depends... if all your films are distinguishable in a sufficiently small set of initial bytes, then ordering by file content could be much cheaper. 
Well, I'm not that familiar with the details, but what does that have to do with 11.10, which I see has ghc 7.0.3 available in the repos?
Also the slogan of Conor's [Frank language](http://cs.ioc.ee/efftt/mcbride-slides.pdf).
One shouldn't have to look at implementations to understand semantics, it just feels wrong. Better yet, their names should give some hint to their semantics, but in this case, they are just plainly confusing: iterator vs. iteratee vs. enumerator vs. enumeratee. 
This was really just an excuse to play with the PolyKinds extension, but the datatype invariant could probably be captured using a nested datatype instead.
&gt; * Iteratees can be forked and run on different threads while preserving isolation of local state, and &gt; * Old copies of the iteratee state can be kept around, and resumed later as a form of backtracking (swapping a bad input for a newer one). What's interesting about these two points is that they hint at two of the weaknesses we've run into regularly with enumerators: * Since the `Enumerator` completely controls the flow of control, it's impossible to redirect new input elsewhere, such as a different thread. * Once an `Enumerator` has been started, it can't be paused and resumed again later. I don't think it's inherent that providing flexibility on the receiving end (the `Iteratee`) will lead to more constraints on the sending on (the `Enumerator`). In conduit, we have a lot of flexibility with `Source`s (e.g., they're resumable), and although we don't expose any resumability with `Sink`s, I don't think there's anything in the types that prevents it, just the current implementation of `$$` and the fact that it automatically calls `sinkClose`.
So instead of using do notation for something that doesn't obey the monad laws we'd have a new monoid notation to use for things that don't obey the monoid laws. I'll stick to do.
Hmm, why does the last example fail to satisfy? Say I define [f,g] = match x with | inL a =&gt; f a | inR b =&gt; g b end Now, irregardless of the termination of g, I have that (f x) is the same as my ([f, g] . inL) x. What am I doing wrong here? *update*: Ah got it. If g needs to compute we may have g bottom.
&gt; Would it be possible to extend Hashable to support anything deriving Generic? Then one wouldn't have to go through Binary, but one can still hash any type. Yes, I think so. &gt; Similarly for supporting a generic binary tree for the buckets. It should be possible to generate a generic ordering for anything deriving Generic, no? It would be possible to generate an ordering, but that's not the issue. The issue is the memory and speed cost of using a binary tree for buckets.
I believe Jasper implemented it this way for efficiency. IIRC, having `Html` obey the monad laws slowed it down a bit.
Yeah, and Haskell would be so much faster if `head` didn't bother to check for `[]` before blithely dereferencing the field :) I appreciate that the hack may be the best solution in the short-term, but it's still pretty awful.
Iteratee is the new Monad. Expect to see lots more tutorials and explanations. 
I've almost finished deploying a simple Yesod app using NixOS (http://nixos.org/). Once you learn Nix it's awesome for virtualization. By almost I mean: I just need to hook up nginx to the Yesod/WAI app then I'm done. Which is actually just learning nginx and nothing really to do with Haskell or EC2. All that I've already got working. I intend to blog about the whole process but I hate blogging. Messaging me in about a week is a better idea if you want to get details.
From the source below, it seems like `&gt;&gt;` for `HtmlM` is just the `Append` constructor -- looks monoidal to me!
Did you read the [comparison to enumerators](https://github.com/snoyberg/conduit) that I sent you and was also linked to from the conduits series? Michael spent a lot of time reaching out to those implementing and using enumerators. Ultimately we did conclude that it is at least theoretically possible to solve all the issues with enumerators that conduits solves with better enumerator implementations. But even then we are skeptical that the code will be anywhere near as nice to write as conduits.
That would be `in (:)`. You just have to end it with a list. Note that the generalization to monoids also requires wrapping each line in a singleton. What you really want is a monoid and a singleton constructor. (Pretty sure there's a name for this, but I forget it.) Maybe have: in f a b c desugar to `mconcat [f a, f b, f c]` or equivalent, with some way to default `f` to `id`. Then you can construct lists with `in (:[])`, sets with `in Set.singleton`, sums with `in Sum`, etc. and then just bare elements.
I don't see why. I mean, yes, ``(a `Append` b) `Append` c`` is not technically identical to ``a `Append` (b `Append` c)``, but if the constructors are considered internal-use, then it should be fine; you can do all sorts of nasty things with internal modules anyway.
I think this proposal is pretty much spot on.
When I first started working on conduits, I wrote up a [README explaining why](https://github.com/snoyberg/conduit/blob/master/README.md). Since then, those of us who have actually used conduit as a replacement for enumerator have found a number of unexpected benefits, such as Felipe's Google+ post. The biggest example that's come up is likely the drastic simplification to the Yesod codebase by getting rid of the `liftIOHandler` function to work around enumerator's inability to deal with exceptions. We also ditched some home-made functions like `catchIter` which- while seemingly correct- always made me nervous, and were specifically not included in enumerator itself since John wasn't sure it really worked correctly.
Thanks for the comment! * If I use OpenAL I'll have to make a binding. I couldn't find source for one that works already. For something like LD48, jittering is probably "okay". * Hiero sounds nice. * A sprite packer does sound handy for 2D games. Thanks for the idea! 
The only NixOs AMI is instance-backed which is a little painful.
And analogies. :|
The code examples are cut off for me.
Is conduits solving the same problems as enumerators, i.e. is it just an alternative? Or do they have fundamental differences?
&gt; I said "It's a thing for connecting a thing to another thing." Iris objected that this was a completely unhelpful explanation, but I disagreed. I think I agree with Iris. Because what the hell is the type of that green part? Does it connect 2 parts of a certain type? gp :: Thing -&gt; Thing -&gt; Thing -- or parts of different, but still fixed types? gp' :: OnePart -&gt; OtherPart -&gt; YetAnotherPart -- or maybe it's a super duper connector like this? gp'' :: (ModelPart t) =&gt; t -&gt; t -&gt; t -- or this? dog :: t -&gt; t -&gt; t 
Came here to warn about this as well. Was trying to figure out how this function could possibly work, realized it was missing the right half.
Whoops, fixed. ^ is a pain on my keyboard (and I was not thinking carefully). 
Spot pricing works like ebay bidding. You pledge how much you are willing to spend for the system not to go down, and you get charged the minimum amount that wins you the bid as long as that amount is less than your maximum. You can see what the current winning bids are and get a graph of their history. For the biggest machines the bids that win them are typically around 90 cents or so, or you can get one that won't go down at all for 2 bucks or so. So, when you want to be a cheapskate like me, you bid 50 bucks on that server and generally save a buck an hour. There is the possibility that a bidding war breaks out and you end up paying more than the 2 bucks it would have cost you to just pay for the thing, but those are pretty rare. I've only seen that happen near Christmas and on micro instances (Amazon probably repurposing the micro hosts to bigger boxes to deal with website scaling and depleting the available pool). If having your machine disappear out from under you is a big concern, I would pay the standard rate. It's not a lot of money and the peace of mind might be worth it to you. The real money savings in spot instances is when you need massive amounts of CPU but are not constrained on when you need that CPU. In those cases you can fire up hundreds or thousands of servers when the spot price drops below a threshold and shut them down as soon as the price spikes back up.
Can this principle be used to extend the capabilities of any parser? For example can I recognise a context free grammar with my regular expression parser?
As you admit, it's not a monoid, even if it can be made one. Requiring the monoid laws is a mistake. First, there is no syntax for the unit, so why require one, so we're down to a semigroup instead. And why require associativity? Just require it to be a magma. Next, I don't like the special syntax. Why not just reused the do notation and change the typing rule so that when only &gt;&gt; has been used in desugaring it's only required to be a member of the magma class instead of the monad class? 
[Yes](http://sebfisch.github.com/haskell-regexp/regexp-play.pdf). (See Act 3).
[Yes](http://sneezy.cs.nott.ac.uk/fplunch/weblog/?p=271). You only need those combinators to recognize any language (with a finitizable alphabet) that the ambient language can recognize, using an infinite grammar. And those are all operations allowed by regular languages (empty language, empty string, single letter, catenation and alternation). So infinite 'regular' grammars are enough for everything you could reasonably want.
So what would be the Iteratee equivalent of Warm Fuzzy Things?
Thanks.
Okay, but this leads to a fairly obvious question: *Why* do this? This is a straightforward encoding of basic logic in Haskell, I don't see what benefit this post serves or why the author chose to translate his logic work into Haskell.
Indeed. If you limit the size of the POST request, or even the number of parameters, you don't need cryptographic hash. I'm surprised that people are talking as if the mere of existence of non-cryptographic hash tables were a security hole.
I compiled it with -N as an RTS option. If it wasn't running in parallel, then why did I see my CPU usage jump way above 100%? Making it more robust is always nice, I'm more concerned that there's a cleaner way to express the algorithm, the helper functions seem rather kludgy, but I don't have a good feel for what works and doesn't work in situations yet in Haskell.
&gt; If it wasn't running in parallel, then why did I see my CPU usage jump way above 100%? Parallel GC?
It's very similar. You can see the [relevant blog post](http://www.yesodweb.com/blog/2011/12/resourcet) for more information, but the basic idea is "allow me to release early, but guarantee it *does* get released."
For the casual Haskeller, what's the status of emuerators/iteratees and conduits? It seems that around 2008 a few people started talking about iteratees; enumerators got wide exposure in 2010 year due to Yesod and http-enumerator and their being featured at Strange Loop; and in 2012 conduits replace (upgrade?) enumerators. As I (weakly) understand from the Yesod/Snoyberg blog posts, enumerators are fundamentally broken with respect to the problem of "timely resource deallocation", and conduits is a fix for that. So, are enumerators still important? Only if the content being enumerated doesn't need any resource deallocation? I see at least two aspects of conduits: * Bug fix for resource deallocation * renaming away from the uncomfortable enumerator/interatee vocabulary Are these the main points? Are they orthogonal? As a web-service programmer (to whatever extent one is), one is happy that Yesod has a bug fixed and a slightly shorter package name in conduits. As a general programmer, should one new to enumeration bother to learn enumerators, or skip right to conduits? More generally (maybe for another thread), how does one learn "intermediate" Haskell these days? (In fact, I'll go post another thread.)
That's really clever, thanks! When I have some time later today I'll do this justice and dig in, I haven't used an IntSet before so that'll be interesting. I'm wondering how this performs when compared to the helper function approach I considered above...
Small typo: res2 &lt;- http req2 manager responseBody res $$ sinkFile "post-foo.txt" On second line, `res` should be `res2`
The problem definition isn't clear to me, but you seem to have a couple of bugs in *betterApproach*. If a number appears only once in the list, can it appear more than once in the answer? If the input list is [1] and the input sum is 3, is (1, 1, 1) a valid answer? Your program fails with an exception. But if we give [1, 2, 3] and 8 as arguments, it returns (2, 3, 3). So in any case you have a bug. The other bug is that your program fails with an exception when the problem has no solution, because you're applying *head* to an empty list. PS: I checked the naive implementation, assuming it's correct then it's clear (1, 1, 1) should be considered a valid answer.
He was asking if people would be interested in a second edition [a few months ago](http://www.reddit.com/r/haskell/comments/jtdrz/interested_in_a_2nd_edition_of_real_world_haskell/).
Casual haskeller does not need to know or use neither iteratees nor conduits. These are specific technical solutions for processing resources in constant space. Most of the time this is not an issue and lazy IO is perfectly adequate for everyday tasks. I'd say iteratees/conduits are of interest to library/framework developers. But those are of course not casual haskellers. 
I believe a new edition of RWH is being written.
A few months ago I was working on a Haskell binding for the new Dropbox API but flamed out trying to get OpenAuth working properly. Glad to see someone put in the hard work and got a binding out there.
&gt; Casual haskeller does not need to know or use neither iteratees nor conduits. Casual or beginner no, but as soon as you get to low intermediate level and try to do real work, you will sooner or later have a problem with iteratee/enumerator/conduit is the best solution. 
I would go straight to conduits. Apart from the improved resource deallocation, it is my understanding that conduits make some kinds composition of existing conduits easier than they could ever be with enumerators.
There is a lot of dust being kicked up in this area right now, and I don't suggest "learning APIs" unless you also plan on developing a closer understanding of what is going on under the hood until this dust settles.
http://hpaste.org/56169 - Here's what I got, but on rereading your question I'm not entirely sure it does what you're interested in. It uses the lifted fork (and a few other functions) from lifted-base to deal with the thing I couldn't quite wrap my head around either (though I suppose the answer involves the monad-control package), but I am not sure using the same manager actually buys me anything here.
Oh! [There is even a survey for that]( http://www.reddit.com/r/haskell/comments/jtdrz/interested_in_a_2nd_edition_of_real_world_haskell/)
 i've been looking at http://www.scs.stanford.edu/11au-cs240h/notes/ http://books.google.com/books?id=ZQJnYoAmw6gC&amp;printsec=frontcover&amp;dq=bird+functional+pearls&amp;hl=en&amp;sa=X&amp;ei=B98HT8m2BO7TiAKdtsimCQ&amp;ved=0CEMQ6AEwAA#v=onepage&amp;q=bird%20functional%20pearls&amp;f=false
If people could live with a terminator couldn't this one be done with (white space sensitive) quasiquotation? As GHC has quasiquotation already (doesn't it?) an implementation wouldn't have to add anything new. 
Thanks, that example of yours is exactly what I was searching for!
With respect to resource allocation, say I have something in the style of -- req1, req2 and req3 refer to urls on the same host res1 &lt;- http req1 manager if (someConditionOnRespHeader res1) then do -- decided we don't want to download the body from res1 -- download req2 instead res2 &lt;- http req2 manager responseBody res2 $$ sinkFile "post-foo.txt" else responseBody res $$ sinkFile "post-foo.txt" res3 &lt;- http req3 manager responseBody res2 $$ sinkFile "post-bar.txt" liftIO $ threadDelay 1000000 -- the connection from 'res1' might still be open until the `runResourceT` block scope's ends in case req2 was performed How do I signal that I'm not interested in the body of `req1` anymore, and that the http tcp/ip connection shall be closed straightaway w/o consuming (i.e. downloading) the remaining response body?
Is something missing from the definition of the all-important `effectfulFold`? I couldn't get it to typecheck. http://hpaste.org/raw/56174 shows the arbitrary nonsense I had at some point. The monad instance for `Reader a m` is using a type synonym in a way the compiler rejects, but maybe I'm missing an incantation. 
Already looking forward to it!
`bsourceClose $ responseBody res`. Thanks for asking the question, I've added a section to the end of the blog post for other similar questions.
It looks to me to be much closer in spirit to conduit than enumerator, since your `Stream` type simply returns new data instead of transforming a `Reader`. This same is true of `Source`s and `Sink`s in conduit. I think this approach is vital, as defining the `Enumerator` as an `Iteratee` transformer is the main cause of the "inversion of control" issues that have been coming up. (Compare the API for http-conduit and http-enumerator.) One major difference with conduit is that you handle state via closures and returning a new `Stream`/`Reader` after each call, whereas conduit handles state via mutable variables and keeps the same `Source`/`Sink`. Obviously mutable variables are a downside in general, but I'm not convinced there are any real tangible benefits to one approach over the other, while (it seems at least) the mutable variable approach is easier to grasp. How do you handle the issue of resource allocation? If you have a `Stream` reading data from a file and a `Sink` writing data, and one side throws an exception, how do you ensure that the file handles are closed? 
I second your praise of the good examples, and I agree with you that we could use more of those in Haskell documentation. That said, Haskell type signatures are very expressive. Often they really are enough to understand completely what a library is doing and how to use it. But it requires more mental effort. Library authors who want to be considerate to their users should consider including lots of examples.
That was in fact the original plan for Hackage. And Hackage 2, implementing many of those ideas, has been in development in fits and starts for quite a while now. 
I see, so I don't go wrong if I take advantage of the symmetry of `D` and `Step` and extract `loop`, making it a parallel fold-ish thing for `Step`? Something like: effectfulFold :: (Functor f, EMAlgebra m a) =&gt; (f a -&gt; a) -&gt; D m f -&gt; a helperFold :: (Functor f, EMAlgebra m a) =&gt; (f a -&gt; a) -&gt; Step m f -&gt; a effectfulFold f_algebra (D mstep) = algebra $ liftM (helperFold f_algebra) mstep helperFold f_algebra (Step fdmf) = f_algebra $ fmap (effectfulFold f_algebra) fdmf (Pardon hideous nomenclature.) The use of the separate Haskell `Functor` vs `Monad` jargons was sort of helping me keep the 'pure' and 'effectful' functors apart thinking about the equational rules that come next, the first of which becomes pretty evident, I think, with these definitions. Or maybe this is a muddle.
Well, yes, but I don't think `liftM2 id` is quite what the OP was after here... and desugaring the definition of `liftM2` results in something rather more verbose than necessary, with a spurious bind and return that could be replaced with `fmap` (well, could be if the `Monad` type class wasn't defined incorrectly, at any rate).
That doesn't seem to work as expected, e.g. consider the following: main = do req1 &lt;- parseUrl "http://ftp.heanet.ie/pub/ubuntu-cdimage/releases/11.10/release/ubuntu-11.10-dvd-amd64.iso" req2 &lt;- parseUrl "http://ftp.heanet.ie/pub/ubuntu-cdimage/releases/11.10/release/ubuntu-11.10-dvd-i386.iso" withManager $ \manager -&gt; do res1 &lt;- http req1 manager liftIO $ print $ statusCode res1 bsourceClose $ responseBody res1 liftIO $ putStrLn "closed" res2 &lt;- http req2 manager liftIO $ print $ statusCode res2 bsourceClose $ responseBody res2 liftIO $ putStrLn "closed" The code above still downloads ~1.5GiB although I signaled I didn't care about the first response body content. It does this after the first printed "close" message.
Sorry, I misread your question. There's no exposed API for that right now. You can use a hack of starting a separate ResourceT and then throwing an exception, though I'd like something a bit more robust. Do you have any ideas? http://hpaste.org/56219
&gt; It looks to me to be much closer in spirit to conduit than enumerator I think so too. It simply makes more sense to me to define `Stream`s as a standalone concept, rather than in terms of the things that read them. I think it also makes clearer that effectful streams are really just lists interleaved with effects. One difference between my `Reader` and your `Sink` is that, as far as I can see, `Sink`s are more push-based. My basic pattern is that the `Reader` is asked what it wants to do next (Read or Emit), rather than just having data pushed into it (although I see you have a special constructor for `Sink`s that are unwilling to accept new data). &gt; One major difference with conduit is that you handle state via closures The main part of my post covers handling the state using closures, but at the end I wrote about an alternative representation using an explicit state representation and an evolution function. This can be defined as follows (using GADT syntax to get existential types): data D m f where D :: s -&gt; (s -&gt; m (f s)) -&gt; D m f So there is an initial state of type `s` and one "method" that turns this state into a monadic action and then some pure data containing potential new states. From here, it is easy to see that we could have multiple "methods" instead of just asking it "what would you like to do next?". So another method could be postulated that allows for data to be pushed in, or to request deallocation of resources. It is pretty easy to define translations between the explicit state representation and the closure-based representation. Also, from this representation the link to conduit's mutable state representation is clear: one can stick the explicit state of type `s` in an `IORef` or `STRef` to generate a `Source`/`Sink`/`Conduit`. Making the state explicit has the disadvantage that it is not possible to treat streams as mutable variables outside the scope of the definition of a `Reader`, but OTOH it has the advantage that it is easier to reason about them, and (I hope) easier to get GHC to optimise chains of stream processors using fusion. &gt; How do you handle the issue of resource allocation? If you have a Stream reading data from a file and a Sink writing data, and one side throws an exception, how do you ensure that the file handles are closed? There is no explicit handling of resource deallocation in what I've looked at. I think this splits into two parts: * Guaranteed eventual resource deallocation, even in the presence of exceptions. This, I think, could be accomplished by using your `ResourceT` setup in tandem with the definitions I gave. * Exploiting the possibility of early resource deallocation. Obviously, a `Stream` can close any resources it is using when it gets to EOF, and a `Reader` could close a file when it has written everything it wants to. But if a `Reader` does not read everything from a `Stream` then there is no way for the `Stream` to be told that it is no longer needed. Your `Source`s, `Sink`s and `Conduit`s come equipped with what is effectively a `close` method. This could be added to my setup, at the cost of going outside the theory we've worked out. I've been mainly investigating the practical use of this stuff in the context of a [library](https://github.com/bobatkey/Forvie) for writing programming language parsers, so resource deallocation hasn't come up very much. You can see some more definitions of `Stream`s, `Reader`s and `Processor`s in the file [MonadicStream.hs](https://github.com/bobatkey/Forvie/blob/master/src/Data/MonadicStream.hs). Its all written using explicit recursion rather than the `effectfulFold` combinator, but the idea's the same.
There already is a stream processor package: http://hackage.haskell.org/package/streamproc though the functor and applicative instances are not provided.
Yes, I'm planning on putting this to Hackage eventually. I'll just have to think about the terminology and API a bit more.
I've been thinking about this stuff, and I have a few ideas how to make this stuff more useful. There are still some issues to solve, but I hope to write about this soon.
Hmmm ... Yampa. It's nice to see Yampa SF (signal function) dissected separately from FRP concept.
How does this approach compare to the (famously confusing) request-response client-server example in section 4.4 of Gentle Introduction to Haskell? Same concept but with more expressive types and less syntactic hacking? http://www.haskell.org/tutorial/patterns.html
I didn't find the author's rationale compelling. If the title was "When forced to code on an iPad, Haskell is a good idea", then it might have made sense. But having a single-tasking platform with shitty keyboard support (not only awful to type on, but without simple access to commonly use operators), and then needing to connect to a server on which you have a shell account in order to code seems far from being a good good idea, really. 
&gt; In the console there is a Screen session running Hugs in one window... Hugs? Really? Is there some secret, compelling reason to use Hugs instead of Ghc(i) that I am unaware of?
How about it actually runs on iPad?
No need for a server: The author's shell account is on the iPad. screen, vim, and hugs are all running locally on the iPad. Interaction with them is via the iSSH terminal emulator. I expect using the iPad keyboard will be more of a pain as the author learns more Haskell. It seems like the more advanced Haskell code gets, the more line noise there is. All the concise operators were grabbed by Prelude, so I keep running into punctuation trigraphs. This might be solvable: Since the author already presumes a jailbroken iPad, installing a custom programmer's keyboard could fix this problem system-wide. I don't know whether anyone's created this keyboard. (Brief rant: That multi-character punctuation operators aren't annotated with conventional readings really hurts readability. `+` as "plus" and `==` as "equal to" is fine, but `~?=` is a mouthful. How should you read that? "Wiggle-huh-whiskers"?)
Thanks for the clarification on the server thing.
It's not dead. Plenty of signs of life on github (https://github.com/leksah). Sadly, my installation is broken at the moment too... I love the ability to track down values imported from all over.
For some, the decision between laptop and iPad is the decision between work and play, the same way some people want a physical separation between office and home. If the laptop is your work machine and the iPad is your fun machine, and hacking Haskell is what you're doing for fun, you might go to some lengths to hack Haskell on your fun machine. Using a Bluetooth keyboard would fix the iPad keyboard problems, but jailbreak + setup + maintenance still sounds like more trouble than it's worth. (And you can't even run ghc?)
We will release 0.12 soon. Please try out the latest dev binary as described here... http://groups.google.com/group/leksah/browse_thread/thread/c5fe4c8328908950 OSX Binary Installer http://leksah.org/packages/leksah-0.11.0.0-ghc-7.0.4.dmg Windows Binary Installer http://leksah.org/packages/leksah-0.11.0.0-ghc-7.0.4.exe
Actually, `ZipList`s of length n *are* a valid monad, but not in a very useful way. `join` takes the diagonal, which works out to mean that any expression using `(&gt;&gt;=)` can be rewritten to use `(&lt;*&gt;)` instead and that `f &lt;$&gt; x &lt;*&gt; y` = `flip f &lt;$&gt; y &lt;*&gt; x`. Lists of a fixed length are also equivalent to their element-by-index function, so if we have a type `N` with n possible values, `ZipList`s of length n with elements of type `a` are equivalent to functions `N -&gt; a`, and their `Monad` instance is likewise equivalent to the instance for `((-&gt;) N)`. `ZipList` in general fails to be a valid monad because the monad laws don't hold if the lists are of different lengths. Infinite streams do not have this problem, however, and neither should your coroutines. If we think of `Coroutine i o` not as a stateful transducer between streams of `i` and `o` values, but rather as a stateless function `Time -&gt; i -&gt; o` taking a discrete time step as an argument, then the only difference between `(&gt;&gt;=)` and `(&lt;*&gt;)` is the order of arguments: `(a -&gt; Time -&gt; i -&gt; b)` vs. `(Time -&gt; i -&gt; a -&gt; b)`. Which would all be well and good, except that just like `ZipList`, the implementation gives us no way to directly compute the element we want. Where `Applicative` gives us a stream of functions, `Monad` gives us a function producing a stream of values. To take the diagonal with `ZipList` we have to discard progressively longer prefixes, which is bad enough. With coroutines, we have to retain the *entire input stream* and fast forward each newly-minted coroutine from the beginning of time to the current step, then throw away everything but the output value. Essentially, the `Applicative` version assumes that all coroutines proceed together in lock step, discarding past inputs, while the `Monad` version assumes coroutines may react differently to past inputs based on the current output of another coroutine. The equivalence remains whether the `Monad` instance exists or not, but using `Applicative` at least means you have to *intentionally* implement the pathologically inefficient version yourself.
I am also interested in the same problem. but I found that it was nontrivial at all for building a nexus for an arbitrary graph without knowing any predefined structure. (Bird's example is using rather special classes of graphs. ) I would like to hear about other's intuition on this. 
ghci doesn't?
Not on arm chips, yet (unless I have the wrong end of the stick).
Thanks for suggestion, but updating (to latest haskell platform) didn't solve the problem. Do you think it might be caused by using Windows? Well, maybe you mean even more recent version of ghc?
To be fair, you kind of need to type up the code and run it to really understand what is going on. This is not a light read. A pearl is hard and dense. 
Did you look at the [hogre](http://hackage.haskell.org/package/hogre-0.1.3) package? I notice that one doesn't have any modules exposed though &amp;#3232;\_&amp;#3232;
Is there a video of the talk anywhere?
&gt; Using Map for buckets would add at least 6 words (e.g. 48 or 96 bytes) &gt; of overhead for every entry. It'd also slow down operations (although I &gt; don't know how much.) unordered-containers and hashmap don't use &gt; nested maps for this reason. Interesting. IIRC, Walter Bright (designer of the D language) claimed that using (unbalanced) binary trees for collision handling in hashtables was the primary reason his Dscript outperformed some other ECMAScript implementations - though it was some years back that I read that. Maybe a custom binary tree, rather than using the Data.Map library? A simple binary tree should only need two pointers worth of overhead per node, at least using C - I don't know how using Haskell impacts on that. 
This is a minor variant of the reason why ternary trees are often faster for string lookups than hashtables - most of the string comparisons complete in the first few bytes, while the hash calculations need to read the whole string. Searches for keys that aren't present can fail quickly. Usually, quite a lot of searches are for keys that don't exist. Of course caching the hash values can mess with that reasoning. Ordering by file hash would probably defeat the purpose - if you're getting collisions, maybe that's because the complete hashes are identical - if all the keys are equal, tree search for collision handling won't help.
took me a second...
I could've sworn it was on the sidebar a while ago (maybe a year? maybe two?) because I definitely recall reading it...
It's the last link in the sidebar: "Haskell blog aggregator"
Not following. Is it 4/3 as in total less than the number of patches, or is there some inside joke here ? 
Also if you have a blog and write about Haskelly stuff regularly, be sure to submit it.
Weekly rollups should resume.. sometime (good candidate for /r/ sidebar) http://contemplatecode.blogspot.com/2011/12/haskell-weekly-news-issue-210.html
The right way to read darcs progress messages like that is "4 items done, 3 items queued". It's a bit misleading to have written it as a fraction, that's not what it means. Recent versions of darcs are clearer on this, and will write something very similar to the above.
Thanks. I'll give that a read.
You lie! There is no planet.lojban.org and no planet.intercal.org.
Doesn't the separate `ResourceT` scope inhibit keeping alive the connection, i.e. doesn't it force closing the connection even for the case where I wanted to consume the body and the http-session-state could be reused for further http requests? Alas I don't have any good ideas yet, as I'm still trying to fully grok the conduit-concept; also I've got that other goal to be able to somehow be able to maintain a configurable http-connection pool, where connections are opened and closed while honoring some parameters such as max-connections per target-host/port E.g. somehow combine functionality from [pool](http://hackage.haskell.org/package/pool) or [resource-pool](http://hackage.haskell.org/package/resource-pool) with `http-conduit`s `Manager`... is it planned to be able to supply a custom `Manager` in `http-conduit`?
&gt; This makes certain kinds of usage incredibly. You accidentally the.
Fab.
Is it open only for UK/EU students as before or «any nationality» means worldwide? (I'm from Russia)
You're at least half wrong: http://www.lojban.org/planet/
Very happy to see well-typed go from strength to strength.
Were you unable to find any Haskell programmers interested in the job? Have you used [haskellers.com](http://www.haskellers.com/)?
Sounds like you should get rid of him before he can do any damage. Not knowing Haskell is fine, but people who aren't intellectually curious aren't going to change.
Even if you couldn't find a Haskell programmer, it sounds weird you hired someone that you describe the way you do; you could a least hire someone who has the right mindset to learn Haskell and have an idea of what it is.
I tried haskellers.com, haskell cafe, even here on reddit. We got 2-3 interested candidates. But it did not work out (mostly relocation concerns)
There are many factors at play here. So it was not under my control really. BTW we had to let go the guy we hired before him. That one just stonewalled us. 
BTW we had to let go the guy we hired before him. That one just stonewalled us. But this new hire is definitely better. I have a hope for him. 
I was surprised at this too. This sounds like an ok salary for a salaried employee, but a consultant should get a lot more indeed.
And unfortunately people like me, who'd like to work in Haskell do not have the opportunity.
On an annual basis it's not a whole lot less than what the partners make, and the partners carry the cashflow risks. The rate is currently not so high because currently we have quite a few long term projects with a lower daily rate. We do other shorter term things with higher rates but there's an obvious cashflow variability danger with relying too much on short-term projects. We call it a consultant job because that's what we do, we are a Haskell consulting company. This particular job will likely initially be mainly design and programming, not meeting clients so much, but that could change as projects change. We offer it as contracting not salaried employment because we're still too small to offer the reliability guarantees of employment. Finally, yes it's true, all of us could get paid a lot more if we went and did something else entirely, like Java, but where would be the fun in that? What we do now is really interesting, there's a lot of variety, we get to help lots of Haskell early adopters and we have control over how and when we work. I wouldn't swap it for anything.
We've had an interesting last few months, lots of new interesting clients and enquiries.
You asked for a Haskell **expert**. Maybe it scared people off. It would've scared me off if I had seen this offer on Haskell Café: I've been using Haskell for 2~3 years, I don't consider myself as an expert. But I'm sure a lot of people who don't think they're experts now think they're at least better than the guy you hired, given the description you make of him.
I often say that C-like languages have a logarithmic learning function. Haskell's is exponential.
Just why should they deserve this much more?
You are replying to the wrong guy then. I am not OP and describe a case in DIFFERENT company :)
But Haskell isn't the only choice. If you want to get paid to program in a functional language there are (fortunately) plenty of other functional languages being used commercially these days. Just a gentle reminder.
USD 80k for a haskell programmer LOL edit: no, 55k !!!!! *brain asplodes* me and all my buddies got hired straight out of an average private university, knowing nothing let alone understanding monads, for more than that.
You are exactly right. And "Haskellish" is exactly the problem for folks who are trying not cross the bridge from imperative programming to Haskell.
Well-Typed are great guys. I am sure it would be a great experience working for them.
It's quite low even for a salaried employee. A decent Java developer should be able to get twice that.
I've been trying to hire competent F# programmers in NYC and it's been hard. I even run the local F# User Group so I figure I have an edge. I can't afford to pay like the hedge funds though, that's the hard part about finding talent in NYC.
There are also plenty of straight up Haskell jobs for qualified individuals.
Well, I guess I could always say your bonus will be based on a 1d6 x 10K, if you roll a 1 you're fired. That way it would be like a hedge fund :). It's hard though, we're a small company with 3 people right now. We're exploding right now and need more talent, but it's hard to find and none of us have time for marketing. We are paying well into the 6-figure range, just not Hedge Fund 200K+ well. 
Well, given that Well-Typed offers telecommuting, we can't really ignore the difference in living cost between Seattle/SF and wherever their hypothetical hire happens to live. I have a simpler theory, though: the job market is ridiculously inneficient. It's not that people are "willing to pay $250/day", it's that they don't know their options, or the options aren't really available to everyone (due to relocation/visa issues/etc).
Yeah, it definitely is inefficient. In an ideal world, you wouldn't have to choose between Haskell and $400/hr, too :)
Yeah, I'm in one of them (scala)! But Haskell has more than just the "functional" aspect to it. People drawn to Haskell also tend to have similar interests and an academic sort of curiosity. I'm extremely lucky to work with a team of "enlightened Haskellers" where we're given the freedom to put all the stuff Haskellers usually get mocked for, to work in scala, but the broader scala community doesn't appeal to me much at all. Same with clojure or other functional languages. I'd definitely pick scala or clojure over a java job, but I think many Haskellers are looking for more than just "functional".
&gt;silverlight
Glad to hear you say that. I drafted a comment to that effect but I don't have hard evidence behind it. Considering what a Google PhD earns, it is hard to imagine Haskell monks performing their rare and powerful skill for peanut wages. Sure, if you are at a university working for the public good and only 8 months a year. But fulltime for bankers and engineers? BTW, did anyone ever write a blog post about how Standard Chartered swept in and hired everyone from dons and younger? From nowhere they became the largest employer of famous Haskellers. 
Are there? Where are they hiding? I need to know this for about 3 years time when I might be a qualified individual :-)
Again, I mean marketing in a lose way. You have to let people find about you. It could be a blog, or it could be buying "f# / haskell / scala jobs" on AdWords. 
Plenty? All I'm currently aware of is [a position at an investment bank in Singapore](http://cufp.org/jobs/language/31), and perhaps some positions at Galois. I have an always-on filter on [Jobserve](http://jobserve.com) for positions mentioning Haskell. 99% of the time, the job is for a Java or C# developer, where an interest in Haskell would be a plus. Keyword-stuffing is rampant. [Haskellers](http://www.haskellers.com/jobs) shows no job listings. [Functional Jobs](http://functionaljobs.com/) shows no job listings. Where should I look?
I completely agree, and I think this is an all-too-common problem, sadly.
Might as well ask here: I tried to install yesod (via cabal-install) a couple of days ago, but there was a monad-something-something package that wouldn't build. Is this a known issue?
Great walk-through. Some typos: * "The ~~once~~ **one** exception you'll see below will be i18n messages." * "We have four entries: a homepage, an entry list page (BlogR), an individual entry ~~pge~~ **page** (EntryR) and our authentication subsite."
Are stuff like ``Unauthorized "Not an admin"`` i18n-ready?
I can attest to that
Try again, I messed up by releasing a new version of failure that gave an overlapping instance. The problem only existed for a few hours, but you may have been an unfortunate person in that window. To be safe, try running: `cabal update &amp;&amp; ghc-pkg unregister failure -f &amp;&amp; cabal install yesod` __Edit__: s/-f/--force/
Thanks, typos corrected.
Okay, sorry but your original post was misleading. Looked like it was in reference to the link's offer.
Hm, I'm still seeing them even after force-refresh.
Thanks for the reply. I was thinking something along the lines of the Hamlet-style templates except for Markdown, but that seems kinda weird :).
Yes, thank you Michael. This make perfect sense now. But I also forgotten it was a litterate Haskell and it took me some time to understand "where" to put this deriving line. edit: Personnaly I certainly found it easier to understand if you explained just after a complete code block containing "deriving".
There are inotify bindings on Hackage which you can use, but personally i just use [inotifywait](http://linux.die.net/man/1/inotifywait) plus some shell scripting: while inotifywait -qq -r -e modify .; do cabal build &amp;&amp; ./dist/build/tests/tests; done
Actually, we have discussed the possibility of a Markdown Shakespeare language (called Markus of course... sometimes I think the presence of a good name is the determining factor in whether we do anything). Everything's very sketchy right now, but in my mind it wouldn't have nearly the power of Hamlet; it would basically be a light-weight format for writing large blocks of text in a type-safe way.
Are you alluding to his attitude toward scalaz? I think it's largely driven by Tony Morris behaviour on the mailing list. Plus, there's a difference between functional programming (immutable, higher-order abstractions) and (simple) category theory applied to functional programming. I wouldn't say his work isn't related to functional programming, that would be bad faith.
Great stuff! I would love to see tests for everything in the example app so I understand how I could review something like this as a commit to a large code base.
Excellent! It's always nice to see more pure Haskell image codecs. Would it be possible to support Storable arrays directly as well? 
 {-# INLINE (!!!) #-} [That's](https://github.com/Twinside/Juicy.Pixels/blob/master/Codec/Picture/Png.hs#L86) a very emphatic inline pragma, shouting at poor GHC...
I'm working on an FRP library called `reactive-banana` and I have [working GUi examples to show][1]. [1]: http://www.haskell.org/haskellwiki/Reactive-banana/Examples
I hope that hxournal may be counted as one of such apps. See http://ianwookim.org/hxournal Original first announcement : http://www.haskell.org/pipermail/haskell-cafe/2011-December/097484.html
Any idea if (and when) it will go on Hackage as well?
darcs is a command-line versioning tool IIRC - it might have some user interaction, but not really what's meant by "interactive user-facing app". There are a number of GUI libraries (GTK based, wxWidgets based, Win32 based and more) and games libraries (OpenGL based, SDL based IIRC) for Haskell. While it's not the worlds most popular language, it's certainly used for a few interactive user-facing apps.
Maybe this is obvious, but is this more or less a full-time position? I assume so, since the salary given is a daily figure.
I was going off this [page](http://hackage.haskell.org/trac/haskell-prime/wiki/UnicodeInHaskellSource). Also I'm getting lex errors when I attempt to use certain Unicode characters so Haskell's support for Unicode is still lacking.
Yes, but I would like more general support.
`:-)`
Constructing IO actions is itself entirely pure, and I think everyone will agree on that. It's only the semantics of the IO actions--particularly when chunks of pure code only interact by way of IO--that things are muddy. Personally I have no problems describing an application as being pure if all the key logic happens in pure code and IO is only used incidentally, such as for literal I/O, and could conceivably be replaced with something else that provided certain primitives. But my thoughts aren't really the point here, because I'm not the one asking, and I still don't know what the question is looking for.
There are tons of apps built on x11, gtk2hs, wxWidgets, qtHaskell and others. Check Hackage for hundreds of examples
Hmm
&gt; To most people, if it runs in a terminal window, it's purely for the beardiest and wierdiest of geeks. I think this conversation started with XMonad, a tiling window manager. Now I don't know about you but I know plenty of programmers that couldn't handle XMonad...
The next 'big' thing will be Jpeg writing, more bitmap loading I think and some API enhancement. The rest will depend of the user feedback, if you want some specific format, don't hesitate to post a github ticket
He is probably looking for something more like a non-trivial application built on top of reactive banana or some other functional reactive framework. I've noticed that a lot of Haskell code that talks to the user just jams everything into the IO monad, where most of that code ends up looking just like regular procedural code. I don't have a problem with that, but it doesn't seem to be a purely functional approach, at least not in the way that reactive banana is based entirely on combinating stream functions.
Well, it is still a program working mostly in batch mode, getting input on the commandline before it starts and returning output to the user non-interactively (with a few exceptions) before it terminates.
In other news, Don Stewart also brushes his teeth, and does other non-Haskell things on occasion. :)
I rarely run a darcs command that requires no interaction. The most common things (~~darcs init,~~ darcs record, darcs rollback, darcs send) all have interaction. What is it you do with darcs that happens in batch mode?
I heard he even takes photographs on occasion!
So users don't interact with the command-line?
That's so depressing.
Hmm, I was mighty confused about your answer here until I saw your other thread!
&gt; He is probably looking for something more like a non-trivial application built on top of reactive banana or some other functional reactive framework. Did you just.. reply by talking about yourself in third person?
fair enough.
By definition, a pure function always evaluates to the same result when given the same value. Similarly, any "pure" program will always evaluate to the same value. But why? A purely functional program contains only *immutable* values. Every value (numbers, lists, dictionaries, function, etc.) is instantiated once and never changes. For example, reverse does not destroy or modify the given list; it creates a completely new list. As a result, you cannot provide new values during execution (such as new mouse coordinates or key presses). How would you refer to the mouse coordinates when you only have immutable values in your source code? When you *only* have immutable values, you cannot even talk about user input. Does that clarify?
I find hybrid functional-OO programming in Scala extremely convenient. I know it's not exactly new and can be done in other languages, but after Haskell, the ability to quickly bang out some imperative update code, and mix and match it with functional code, and not have to worry about monads or monad transformers, is quite liberating. All I have to worry about is the actual updates themselves, not the monads that I would have used to represent them in Haskell.
makes it confusing for others...not worth it
Agreed!
You could take a look at my [base-unicode-symbols](http://hackage.haskell.org/package/base-unicode-symbols) package. I would like to have more general support for unary prefix operators. Then we could define the ¬ operator: (¬) ∷ Bool → Bool ¬ True = False ¬ False = True prefix ¬ 
How can it be a function if you can't supply arguments?
Inclusion in the Haskell platform is my long term goal, I have put some donation system at the end of the readme file.
That may be one way to look at it, but... The IO monad doesn't do any world passing. It's a common analogy to explain what the hidden innards of the IO monad might do, as if it were a variant of the State monad, but Haskell simply defines the IO monad to be a black box. This is essential, otherwise it would be possible to pass the-world-yesterday and the-world-today as arguments to the same function - this is not possible for the IO monad, though the equivalent is both possible and useful for State monads. The behaviour of IO actions and how they bind is primitive - not defined in terms of functions hidden inside the IO actions. Consider this expression... getAnIntFromTheUser &gt;&gt;= \x -&gt; (return x+1) Perhaps x at some point gets a specific value, and x+1 is evaluated using that value. In which case this expression is not referentially transparent. Or, perhaps this expression denotes code rather than a value, in which case x is just a way to express data flow. There are other arguments in favor of purity which I don't understand (I tried and failed). To me, there's some interesting philosophy following the "it denotes code" logic, but I lean towards this not being referentially transparent. Even if the code is in translated form, at run-time, x does get a value which came from the user.
Editing on the command-line is handled by the terminal emulator, not be the program that command-line calls. Parsing command-line arguments is not interactive. Some programs have some degree of interactivity within a terminal window. In the DOS days, text-mode interfaces with windows, menus etc were quite common. These days, curses-based interactive interfaces do exist, but are relatively rare - and mostly not considered "user facing". Points to note - (1) I did say "it might have some user interaction", (2) I already confessed that I probably underestimated that, and (3) irrespective of that, darcs is still a programming tool, and not really what people mean by "user-facing interactive app".
Xmonad is for Grandma Tilie.
I am not all that familiar with darcs but the darcs init I just tried ran without any output and didn't require any input. The same is true for all the commands for repository inspection I have seen so far.
A pure program could evaluate to a function. But in a pure setting, that would be the end of it. You'd just have a function. You could supply arguments in the pure source code, but now that function is evaluated to something else. Do you mean command line arguments? This leaves the realm of pure programs. How do you refer to them? One idea is to have a pure function and feed in different values as events come in (a more robust form of command line args). This is the basic insight behind FRP user interfaces! No matter what you do, you must introduce something that is not immutable to talk about the world!
You're right; I was thinking of "cabal init". I just tested the others to make sure because of this error, though, and they all required interaction.
Sorry, you are not with Well-Typed right, but have a company in NYC? (I was asking about the position in the submission)
Thanks for the details. Asked below, possibly dumb question, but is this basically a full-time position, i.e. 150 GBP per 8-hour day type situation?
I only mentioned something other than FRP because I am sufficiently ignorant to wonder whether other approaches exist.
No. IORefs do not violate referential transparency of Haskell expressions.
I have it and it's awesome. I'm going to patch my fork of GHC with support for the prefix operators and once I get it working I will see if anyone is interested in having it in the main branch. Don't know how long it will take me as I'm new to the internals of GHC.
I don't see how it is different from ordinary definitions of operators. I agree that many will be irritated to have to use cumbersome text input methods but I would probably provide two versions of my libraries, one using plain ascii and another providing unicode sugar for the plain ascii library.
I was just messing around when I said 'why?!?' :) I happen to really like FRP (which is why I am using it in Elm), but the message-passing approach is really cool too! CML gives you much more flexibility at the cost of being lower level. If you really want performance, the CML approach is probably better. John Reppy's [book on CML](http://www.amazon.com/Concurrent-Programming-ML-John-Reppy/dp/0521480892) is a great resource and is extremely well written if you'd like to learn more about this approach. His relevant [papers](http://people.cs.uchicago.edu/~jhr/papers/cml.html) are really good too (in addition to being free online). It is amazing to me that CML came out in 1991-3 and most of its cleverness has yet to make it into popular programming. The documentation for eXene is not great in my experience, but it will at least give you an idea of what GUI programming in CML is like. Erlang is similar to CML, but it is not nearly as elegant (in my experience). It uses asynchronous message passing, instead of synchronous as in CML. I would recommend looking at CML though; it will explain the principles behind Erlang way better than any Erlang resource I have found.
I just tried to write some code to prove you wrong, and failed, gaining a better understanding of your claim along the way. I now agree - providing you're focussing on the "meaning" of an expression, IO monadic code (including using IORefs) is referentially transparent. There still remains the issue of other definitions of "referentially transparent", though, such as http://foldoc.org/referential+transparency - this clearly states that the *value* of the expression E must be the same. If you think of an IO action as being a value, this seems compelling - but it's not the only possible interpretation. Considering the following... main = someIOActionYieldingInt &gt;&gt;= \v -&gt; return (v+1) One interpretation is that the bind operator (at run time) executes its left argument, extracts out it's "return" value, and evaluates its right argument passing in that value as a parameter (v in the lambda). The value (IO action) returned by the bind operator, when it's evaluated, is the value returned by that right-argument function. The left argument has been executed. Other than it's result and effects, the left argument is no longer relevant. IOW, in this view, the left argument to a bind operator isn't really "composed into" the return action. Rather the expression can only be partially evaluated at compile time. The IO action that the above main returns at run-time is "return 1" or "return 5" or "return 12345" or whatever, depending on run-time conditions, but someIOActionYieldingInt had to be executed in the course of completing the evaluation of that main expression. However, this view is more relevant to writing interpreters and compilers (thinking it terms of graph reductions) than it is to normal use of Haskell. Thinking of bind as returning a composed action as its value is probably more intuitive for most purposes, and (as I discovered while failing to prove you wrong) Haskell does a much better job of leak-proofing that abstraction than I thought.
&gt; you have to roll the user back to a *previous state* in order to have the same program That sounds downright stateful! Consider the following function where Mouse.x is the lazy list you describe (it's first value - the initial mouse location - being available immediately): f :: Int -&gt; Int f x = x + head Mouse.x This function can have different results given the same argument. Sure, if you give it the same value for Mouse.x, it will result in the same value. But this is exactly *not* how you are suggesting we use it. Also, see Conal Elliott's argument that [the C language is purely functional](http://conal.net/blog/posts/the-c-language-is-purely-functional). A loose definition of pure is not particularly useful.
scion-browser fails to build Loading package terminfo-0.3.2.3 ... &lt;command line&gt;: can't load .so/.DLL for: ncursesw (/usr/lib/libncursesw.so: file too short) is there a workaround?
 $ cabal install reactive-banana-wx Resolving dependencies... [1 of 1] Compiling Main ( /tmp/reactive-banana-wx-0.4.3.13734/reactive-banana-wx-0.4.3.1/Setup.hs, /tmp/reactive-banana-wx-0.4.3.13734/reactive-banana-wx-0.4.3.1/dist/setup/Main.o ) Linking /tmp/reactive-banana-wx-0.4.3.13734/reactive-banana-wx-0.4.3.1/dist/setup/setup ... Configuring reactive-banana-wx-0.4.3.1... Building reactive-banana-wx-0.4.3.1... Preprocessing library reactive-banana-wx-0.4.3.1... [1 of 1] Compiling Reactive.Banana.WX ( src/Reactive/Banana/WX.hs, dist/build/Reactive/Banana/WX.o ) src/Reactive/Banana/WX.hs:41:44: Ambiguous occurrence `Discrete' It could refer to either `Reactive.Banana.Discrete', imported from Reactive.Banana at src/Reactive/Banana/WX.hs:10:1-22 or `Reactive.Banana.Discrete', imported from Reactive.Banana at src/Reactive/Banana/WX.hs:10:1-22 cabal: Error: some packages failed to install: reactive-banana-wx-0.4.3.1 failed during the building phase. The exception was: ExitFailure 1 reactive-banana installs fine, but reactive-banana-wx gives me this error :(. (using GHC 7.2.2)
If the user is modeled as a generating function of event values, then the user is not stateful data, rather the user *is* the function. The user moving forward in time is executing the function. In order to run the function again, it is not sufficient to restart the Haskell runtime, you have to restart the whole machine and the user is part of the machine. Of course, this is infeasible until we can use time travel as part of our unit tests. In your example, the mouse is now part of the function. The program is pure as long as you can guarantee that on a second run, the state of the universe will be the same as what it was in the first run (assuming of course that the universe itself is pure, which it probably isn't.) If that guarantee cannot be made, then your function is not type correct. The type of your function is f :: Universe -&gt; Int -&gt; Int, or we can wrap the whole universe in a state monad and call it f :: Int -&gt; IO Int
Referential transparency means precisely what I said it means. I don't really think the value definition is a particularly good one, but the two probably imply each other. For instance, you can replace the expression: let x = 3 ; y = 3 in x + y with its value: 6 And you can replace each sub-expression "`x`" and "`y`" with their values `3` respectively, which is essentially what my definition would say. But in some cases where referential transparency doesn't hold, I don't even know what the value definition means. If we have a language that impurely includes mutable references, say one named `r`, my definition looks like (pulling in some C++): (let v = r++ in f v v) = f (r++) (r++) which is wrong, because one bumps `r` once and passes `f` the same number twice, and the other bumps `r` twice and passes `f` two different numbers. This will not in general be the same. But I don't even know what the 'value' of these expressions is, because it depends on what was stored in `r` when it was run. The "value" thing isn't particularly good for `IO` either, and it's causing you confusion, I think. The value of `(putStrLn "hello")` is not `()`; the best we can say is that its value is a representation of the fact that the string `"hello"` should be printed. `IO` is an abstract type, and we don't really have access to any internals to know what its normal forms are, so the "value" definition is kind of useless, if you take it to mean some normalized version of an expression can be substituted in. But the definition about factoring and giving names remains useful. It says we can do things like going from: putStrLn "hello" &gt;&gt; putStrLn "hello" to: let m = putStrLn "hello" in m &gt;&gt; m and back. And this is true. It doesn't say we can go from: writeIORef r 5 &gt;&gt; readIORef r to: return 5 because that certainly doesn't look like just an inlining or factoring of the Haskell expressions. And the value version of the definition doesn't mean that either, because the "value" its talking about is not the value that gets stuck in `x` in: (writeIORef r 5 &gt;&gt; readIORef r) &gt;&gt;= \x -&gt; ... it's talking about some normalized expression for the part before the `&gt;&gt;=`, and the best we can do is already there with the information we have. I suppose in either case you'd have to know the low-level details to know whether one is a rewrite of the other, but considering the above writes to a reference that may be accessed elsewhere, and `return 5` does not, you can guess that they are not rewrites of one another by the evaluation rules of Haskell.
I get what you are saying, but my initial point still stands. Pure programs are not interactive. &gt; If the user is modeled as a generating function of event values, then the user is not stateful data, rather the user is the function. Ok, the user is a function now, not a lazy list. Fine, but it still doesn't work. A pure function always gives the same result given the same value. This 'generating function' takes what? Unit? A time step? It doesn't matter. For a given value, a pure function can only give one result, ever. This user is deterministic and cannot model user interaction. At every time-step, the generating function's result is always the same! &gt; The program is pure as long as you can guarantee that on a second run, the state of the universe will be the same as what it was in the first run Yes, but why would you *ever* make this guarantee? If the universe doesn't change, it models a static world! It is immutable! With this guarantee you get a picture or an animation! A pure user interface always turns out the same. That's the whole point I am trying to make. Maybe you want a weaker definition of pure. Such a definition is not useful. If you'd like to introduce 'Universe' or IO, the functions can be pure, but the resulting programs are not. This is *literally* Conal Elliott's point in the link I provided above. If Haskell+IO counts as pure, so does C. Ahhh! Why must everything sound like a flame war on the internet?!? I swear I could have this discussion in real life and not sound like such an ass. I am mainly upset that you give me a sassy one-liner that wasn't actually valid! I guess forums encourage this though. Sorry to vent like this. I feel much better now though :P
Minimalist wallpaper, 700MB compiler :P
Yes (well, part-time is also possible) 
BTW - sorry for the accidental deletion of that post (got distracted and deleted the wrong one). Hope it doesn't make the rest of this too confusing.
Throw a subtle texture on that background
FWIW, I think you've made a very convincing argument.
I prefer this one: http://imageshack.us/f/156/swahili.png/
I actually think I understand this a little bit, which is pretty cool because I usually don't when people start talking about iteratees. Nice work.
For the case of an error state, the simple answer would be to use exception handling the way you normally would address this problem: lift $ action `catch` handler and throw an exception in another Pipe if you are in an error state. However, the other case you mentioned of "when I'm already done" is more complicated and worth mentioning. I spent a lot of time thinking of how to address this, including trying out various implementations of "catch" and "finally"-like functions. I can summarize pretty cleanly the nature of the problem: you have a function like: do x &lt;- lift readSomething yield x y &lt;- lift readSomething yield y lift someFinalizationRoutine ... and you want to be able to specify on the fly that you want only the first readSomething to execute and the someFinalizationRoutine to execute, but not the intermediate readSomething routine. All three lifted calls are indistinguishable from the point of view of the composition implementation (they are all just a bunch of monad effects as far as it cares) and once you construct a Pipe there is no way to selectively skip monadic actions within it while evaluating it. I HAVE spent a lot of time considering varying ways to distinguish certain monadic effects or blocks as finalizers so that they could be automatically called when terminated under Lazy composition, but I have not yet succeeded, although I haven't by any means tried everything. BUT, you don't have to do it like in the tutorial where you specify how many lines you want to read up front. It's a monad, so you can read input and choose whether to read more based on the current input. It's just not compositional, so this behavior has to be integrated within a single Pipe, because Pipes cannot communicate back upstream. So to summarize, for exception handling, use normal exception handling to call finalizers, but for laziness the library has no way to distinguish finalizer monadic code from ordinary monadic code and I am definitely considering implementing such a functionality. Edit: Also, if you don't like Control.Exception, just add ErrorT or EitherT to your monad stack and use those to manage error states. They both work flawlessly with Pipes since Pipe is just another monad transformer.
Simple API. Elegant design. Excellent documentation. Motivates me to learn Category to see how it helped your design. How does it contrast to conduits? They seem straightforward, but this seems a bit simpler. (Maybe just be because your doc is pipe-user-focused and doesn't emphasize the open/close resource handlers like the conduits blog posts did. Or because Pipe allows resource management to be implemented completely orthogonally using lift in the Producer, instead of needing special open and close functions? ) When would you ever call runPipe with lazy &lt;+&lt; pipes but not use discard? It seems that this would leak the input handle (as your example shows). Why not add a builtin strict pipe into runPipe, so it is impossible to leave a dangling input? Or is the reason that you want to allow this? runPipe $ takeAndPrint 1 &lt;+&lt; pipe1 doOtherStuff -- read more from open input, then close, runPipe $ (takeAndPrint 1 &gt;&gt; discard) &lt;+&lt; pipe1 
Yeah, perhaps I should have better explained my reasoning behind runPipe's insistence on a closed output end in the documentation. The only reason I do not do this is so that you don't have a Pipe generating output that you forget to handle. A runPipe that discarded output silently might cause you to carelessly drop output you intended to use. That's the most important reason I designed it that way. Regarding leaked handles, lazy composition will ALWAYS leak handles because it's designed for infinite non-terminating inputs. Strict composition forces the input pipe to completion so it can finalize handles. Building a discard statement into runPipe would not necessarily finalize the input pipe for the same reason that seq is not the same as deepSeq. Here's a simple example of why: discard &lt;+&lt; return () &lt;+&lt; lift $ finalizeSomething discard drives the middle pipe to completion but then finalizeSomething still never gets run. To absolutely guarantee that input gets finalized you need to make everything composed with the finalization statement strict, and not just the most downstream pipe.
Okay. I'm concerned because your pipes are all about individual items rather than chunks thereof, and you have to be manipulating unboxed byte arrays for decent IO performance. So perhaps you'd have pipes of bytestring chunks, but we'll have to see how that goes - I don't currently see a way to pull a "part" of an item from a pipe.
Unfortunately, that's a [bug in GHC 7.2][1]. Everything works fine with GHC 7.0.4 from the latest Haskell platform. [1]: https://github.com/HeinrichApfelmus/reactive-banana/pull/11
...and you could import it from the 'void' package to avoid duplication.
This is really nice! Have you tried to make an instance of Arrow? The problem usually is arr, but you have pipe for that, so it's probably going to work.
Why GPL? This is a show-stopper for many projects. All the other iteratee-inspired libraries (Iteratee, Enumerator, IterIO and Conduit) are either MIT or BSD3. 
Not a site, but [Pearls of Functional Algorithm Design](http://www.bookdepository.co.uk/Pearls-Functional-Algorithm-Design-Richard-Bird/9780521513388) is a really good (although not easy) book on the subject.
This is an awesome book! I read some of the things in the google books preview and I'm learning already :)
[Why Functional Programming Matters](http://www.cse.chalmers.se/~rjmh/Papers/whyfp.html) contains several such examples (no drawings though). Also, besides "Pearls" mentioned by jaspervdj, it might be worth looking into *Introduction to Functional Programming using Haskell* (2nd ed), also by Bird. I don't own either but they're on my to-buy list. 
That's how Erik Meijer would like it!
Is that any different here than in the rest of Haskell? Lazy allows evaluations to terminate when using idiomatic Haskell, and strict tends to give constant factors of speedup, often significant factors relative to real machine hardware limits. It seems to me we need both, but I am a novice, so I would like to understand your advice. Maybe strictnesss doesn't actually add much in this case? 
That yields a result down the pipeline; what's required is an `[a]` field in `Pure` or similar.
Wouldn't a better choice for zero be forall a. a? The only member of that type is ⊥.
Yeah, I know the Haskell community likes the BSD license. I wouldn't like my library being used to make closed software. I've been the overwhelming beneficiary of free and open software and more generally the beneficiary of a culture of software freedom that promotes the free exchange of ideas, and releasing things under the GPL is my way of trying to help promote that culture. I'm not saying that my iteratee library is so much better than other libraries that you will feel compelled to release open source software in order to use it, but I strongly believe in open source and I feel like if I release things under BSD then I'm not doing my part to make things better, even if it is at the expense of my library being used and getting recognition. That's how much I care about software freedom. The trend of culture here in the United States has been towards less and less freedom and I have trouble conscioning that, even if it comes at my own expense. So yeah, maybe there will be a lot of projects that will skip over my library because it is GPL, but releasing it under GPL is my way of encouraging you to think twice about the cultural cost of closing down your source and not sharing it with others. Also, I won't just say "It's my library and I'll do what I want", because I do actually value people's input on the license and I'm not 100% convinced yet about using the GPL. If you can put forth a persuasive argument for another license I will definitely listen to it. Edit: Good arguments for the BSD license. It's a month before the next release and I might change it to BSD upon the next release. I wait a month between updates to avoid excessive version numbering. Edit #2: Fine, it will be a BSD license in the next release. You guys make many valid points so I will include a license change in the next patch, probably a month from now unless it's urgent for any of you guys.
I'm going to interpret what you asked as: "I have a Pipe and I want to pass data to it in 8192 byte chunks and I want it to consume the last 10 bytes of each chunk and then pass the remaining 8182 byte prefix downstream to be handled further". The following code works for bytestring onlyUses10Bytes :: Pipe ByteString ByteString m () -- m will depend on what monad "onlyUse" runs in onlyUses10Bytes = do x &lt;- await -- the 8192 bytes from our upstream pipe let (prefix, suffix) = splitAt 8182 x lift $ onlyUse suffix yield prefix
&gt;I'm going to interpret what you asked as: "I have a Pipe and I want to pass data to it in 8192 byte chunks and I want it to consume the last 10 bytes of each chunk and then pass the remaining 8182 byte prefix downstream to be handled further". That would be a misinterpretation :) Say you have a pipe that parses an HTTP header, and want to run that pipe, giving a parsed HTTP header, and then switch over to a new pipe that wants to be fed the rest of the request. Think [WAI](http://hackage.haskell.org/package/wai). The HTTP header parser receives 8192-byte ByteStrings from a socket. What does it do when it sees the CR-LF-CR-LF that terminates the header, but there's still another 4 kilobytes of data in the chunk? It can't just finish off, because it'll throw away data that the application pipe has to receive, but it has no way to indicate that it *hasn't processed* some of the data it's given (so that the code that sequences the two pipes can pass it on as the first input to the second pipe).
This is an excellent question. I answered this a bit in another comment, but here's perhaps a more clear version of my other answer: The simplest example would be the following producer: produce :: Producer Data IO () produce = do replicateM_ 10 $ lift readSomeData &gt;&gt;= yield lift finalizeData Now let's say it's downstream consumer is done after 5 await statements. The library has no trouble at all detecting when "produce" is no longer needed. The problem is deciding what to do with the remaining monadic actions in the "producer" code. The issue is that the library can't tell which of those IO calls are finalization routines and which are ordinary data. From its point of view, you just have 6 unresolved monad calls left to perform in the "producer" routine when its downstream pipe shuts down. So there are two options: either do none of the monad calls (the 'Lazy' approach) or do all of them (the 'Strict' approach). There IS a solution, namely to distinguish certain monadic calls as finalizers so that the library can tell which ones to run selectively. This is something I'm actively working on because it would make Lazy evaluation really desirable: You could just terminate the Pipe and it would selectively evaluate only the finalization monad routines. I wouldn't release such a solution, though, until I was sure it didn't break the category laws. This is a feature I'm aiming to include in the next release of the library. It would be some sort of "finally" call that would distinguish a certain action as mandatory to execute before terminating. Edit: Oh, and to answer your question about prioritizing downstream effects, it means that if you run: runPipe $ lift a &lt;+&lt; lift b ... then a will run before b. This is actually a requirement from the category laws. Actually, to be more correct, it's a requirement if your id pipe is: id = forever $ await &gt;&gt;= yield That id definition, combined with the identity laws and associativity laws, implies that pipes must evaluate monads from downstream to upstream. Interestingly, I was able to prove those two categories are the only two solutions to the category laws (for the given id pipe).
Your intuition that there's a better way is probably correct. Automatic finalization is something that matters to me because it would make the Lazy version of the library extremely powerful. See [this](http://www.reddit.com/r/haskell/comments/ohjg7/a_new_approach_to_iteratees/c3hfja8) comment for my explanation of the issue.
Thanks. I did in fact mean take'. I'll pull your fix. There are also some other typos in the library I only just now caught and I plan to make monthly updates of the library to Hackage avoid version number overload. Also, how do you typeset code within a line of a reddit comment? I only know how to typeset code in blocks using the 4-space indentation.
I just figured out how to make them Arrows. The Arrow instance will be in the next release of the library (roughly a month from now).
I did consider that, actually, and at one point I did have: type Producer b m r = forall a . Pipe a b m r The only reason I didn't do it is because it was my first time using the Rank2Types extension before and my first few forays failed when I got to the runPipe type, since I wasn't sure exactly how to write it, so I postponed it for a later release of the library so I have time to work it out. I think the forall method is actually better and I think the Zero method is ugly.
&gt; It stops them using it in MIT and BSD-licensed software, too. No, it doesn't. Please stop spreading misinformation about the GPL.
I just wanted to get feedback on my explaining and writing skills before I start a tutorial blog post series on Functional Programming and Haskell.
My experience with `reactive-banana` is that people are happy to give feedback, suggestions and patches, whether they are commerical users or not. Concerning sharing, the most important factor is actually the ease with which users can submit feedback. These days, that's mainly a UI problem, using sites like github helps a lot with that. Also note that GPL is a means, not an end: people who don't like your license won't contribute back, so you are actually making a trade-off: using GPL has a cultural cost, too. Your intention to promote software freedom is great; but unfortunately, choosing GPL doesn't necessarily guarantee the intended outcome. "Intent and outcome are so rarely coincident". Concerning commercial use, I was actually offered a consulting opportunity, so I could have profited from my library, even though it was BSD licensed. The thing is simply that the author knows his library best, it is usually more costly to "steal from the author" than to simply pay the author.
What about making `yield` throw an exception when the consumer is done reading data? If I remember correctly, that's how UNIX pipes handle this problem. Otherwise, you end up reading and discarding all the data from, say, a file instead, which is probably not what you want anyway.
Personally I think release-spamming is OK if you follow [PVP](http://www.haskell.org/haskellwiki/Package_versioning_policy).
&gt; It stops them using it in MIT and BSD-licensed software, too. Afaik, only when using the original BSD license (w/ the advertising clause) there was a legal hindrance. But the currently popular BSD3 (aka "modified BSD") license is compatible with the GPL. See also the list of [licenses compatible with the GPL](http://www.gnu.org/licenses/license-list.html#GPLCompatibleLicenses)
Thank you
&gt; we release all our Haskell libraries and tools BSD who are you referring to as "we"?
Thanks for the clarification. What you are describing is a Parser (consumes some input, returns the parsed value and unconsumed input to be used for the next parsing stage), and its an incremental one that uses chunked input. My first attempt would be to implement it exactly as the parser monad would, except using Pipes to replace functions. So let's say our two parsing primitives were: pipeThatParsesHTTPHeader :: Pipe ByteString [(Header, ByteString)] IO () pipeThatParsesHTTPHeader = do chunk &lt;- await let (header, unconsumed) = parseHeader chunk yield [(header, unconsumed)] pipeThatParsesHTTPBody :: Pipe ByteString [(Body, ByteString)] IO () pipeThatParsesHTTPBody= do chunk &lt;- await let (body, unconsumed) = parseBody chunk yield [(body, unconsumed)] Obviously these may consume more than one chunk to assemble a completely parseable input, but you get the idea. I'm just keeping this example simple. Then you newtype Pipes so that you can make a parse monad based on pipes ala Hutton and Meijer. newtype ParserPipe a = PP { unPP: Pipe ByteString [(a, ByteString)] IO () } split :: (Monad m) =&gt; Pipe [a] a m r split = forever $ await &gt;&gt;= mapM_ yield instance Monad ParserPipe where (&gt;&gt;=) :: ParserPipe a -&gt; (a -&gt; ParserPipe b) -&gt; ParserPipe b (PP m) &gt;&gt;= f = PP $ proc cs -&gt; do (a, cs') &lt;- (split &lt;+&lt; m) -&lt; cs unPP (f a) -&lt; cs' This uses the Arrow instance for Pipes, which I just came up with in [this](http://www.reddit.com/r/haskell/comments/ohjg7/a_new_approach_to_iteratees/c3hfpnh) comment. Then you use ordinary do notation to get streaming parsing: parseHTTP = do header &lt;- PP pipeThatParsesHTTPHeader body &lt;- PP pipeThatParseHTTPBody return (header, body) I haven't actually tried the above yet because I'm busy trying to answer other comments, but I'll come back later and make sure the above code type-checks.
The Functional Programming Group at KU. http://ittc.ku.edu/csdl/fpg/Tools
That is the whole point of the GPL license.
Thank you. Looking back, it's pretty clear that I overreacted, so I'm glad it was convincing anyway :)
Have you considered the LGPL? I believe this would allow BSD libraries to depend on your library. Con for you: LGPL doesn't enforce open code for a whole application (since someone could write a closed source application that uses Pipes), Pro for you: but it still protects you from a closed-source competitor than expands Pipes and then sells it. Also, in the Haskell world in particular, are there actually any proprietary software products created by people who are not huge code contributors? Galois, Well-typed, Standard Chartered, etc, are all staffed by major contributors of open source code. And I get the impression that most Haskell code in commercial use is for "in house" work built in-house or on-contract, not "software for sale" or "public facing website", which is where the GPL vs BSD distinction is relevany. So the extra "aggressiveness" of the GPL (which I generally support, personally) might not be more helpful than BSD to your goals in this case of a Haskell library. 
Shouldn't it be type Producer b m r = Pipe () b m r with type Consumer a m r = Pipe a (forall b. b) m r because `forall a. a` is a pseudo-terminal object and `()` is a non-pseudo initial object. The pipeline could be type Pipeline m r = Pipe () (forall b. b) m r and then you can keep the current type of runPipe.
&gt; `pipeThatParsesHTTPBody :: Pipe ByteString [(Body, ByteString)] IO ()` The problem is that we want to be able to use a `Header -&gt; Pipe ByteString Response IO ()` or similar here. It's unfortunate that a concern common to a large amount (perhaps even the majority) of situations is punted to the consumer of the library.
I agree. Even "withR" would be an improvement.
http://www.reddit.com/r/haskell/comments/hexnv/gpl_lgpl_and_ghc_linking/
I was looking through other libraries to try to figure out what kind of function you had in mind. I think you are referring to something like iterIO's: inumHttpBody :: Monad m =&gt; HttpReq s -&gt; Inum ByteString ByteString m a In short, you want to use the results from the header to affect how you parse the body (i.e. content-length, transfer-encoding, etc.). However, you can trivially modify the example I gave you to do just that: pipeThatParsesHTTPBody :: Header -&gt; Pipe ByteString [(Body, ByteString)] IO () pipeThatParsesHTTPBody header = do chunk &lt;- await -- parseBody needs the parsed header to know how to parse the body let (body, unconsumed) = parseBody header chunk yield [(body, unconsumed)] Then our final parser is: do header &lt;- pipeThatParsesHTTPHeader body &lt;- pipeThatParseHTTPBody header return (header, body) Does that answer your question? I mean I could define a special function that translates parsers that consume all their input and only return one possible parse interpretation, like the function you just gave me, into parsers that fit my general type: convert :: Pipe ByteString a -&gt; Pipe ByteString [(a, ByteString) convert = do x &lt;- await yield [(x, empty)] do header &lt;- pipeThatParsesHTTPHeader body &lt;- convert (yourFunction header) return (header, body)
&gt;I was looking through other libraries to try to figure out what kind of function you had in mind. Well, what's missing is the equivalent of the `Stream a` field of enumerator's [Step](http://hackage.haskell.org/packages/archive/enumerator/0.4.17/doc/html/Data-Enumerator-Internal.html#t:Step) type. My HTTP example was based on how WAI currently works; see the [Application](http://hackage.haskell.org/packages/archive/wai/0.4.3/doc/html/Network-Wai.html#t:Application) type. &gt;In short, you want to use the results from the header to affect how you parse the body (i.e. content-length, transfer-encoding, etc.). Well, it's just that the application should have control over processing the body; it defeats the entire point of streaming IO to parse it all in one go — what about uploading multi-gigabyte files? I understand that you can structure your code specially to make this work with pipes, but it just seems to lose quite a bit of compositionality, as you have to handle it specially, despite the large number of uses of streaming IO that work like parsers in this sense. Working entirely with ParserPipes would work, but for that to be elegant, it seems like the whole library would have to be structured around them.
Also, the fact that the theoretical possibility of people/corporations 'stealing' your open source software and passing it off as their closed-source own is so rare in practice wasn't an insignificant consideration. It's so uncommon that it counts as a big story in the open-source media whenever someone actually tries to do it. So I don't feel like it's worth it to cause complications for well-intentioned people just to rule out people doing something which they weren't inclined to have done anyways (it turns out that people are on the whole fairly decent).
This is a good point. I'm starting to seriously consider the BSD license. Thanks for your input.
Good work, you write well. Get yourself a [flattr](https://flattr.com/)-account so I can send you some monetary love :-)
Oh, ok, I misunderstood you. Yeah, I am trying to implement something similar to exceptions so that you could write: do lift openResource catch (lift closeResource) $ do lift useResource lift closeResource Then when the downstream Pipe terminates, `closeResource` is called automatically before terminating our resource handler. Then you could make shortcut functions for common patterns like `finally` or `using` equivalents.
You might also want to consider this: With a BSD-style license, a commercial user can use your code in their proprietary project and, while not being obligated to release *their* project as free software, they might become interested in contributing bsd-licensed code back to *your* project as they make improvements, fine-tune the performance or maybe even discover bugs. Or maybe they won't, or maybe you don't expect that kind of contribution because of the nature of your project or any other reason. But if you use the GPL for your project, there's a whole class of users who are potentially interested in contributing back to the community but who are not in a position to do so, because they never had a chance to work with your code in the first place. I am not trying to convince you (and am personally a fan of the GPL for a range of projects), but there's many perspectives to consider.
Yeah, this is a good point. I'm starting to learn towards BSD.
I'm sure that there would be lots of interest if you posted about this on University of Reddit.
Fixed: https://sourceforge.net/projects/eclipsefp/forums/forum/371922/topic/4948926
It is no more stateful than the state monad, which is quite pure.
A++ for documentation effort! This is the best documentation of any haskell library i've seen so far.
And that's why I avoid GPL. I want as many people as possible to use what I make (well, not at work), so I want to release it with a license that allows as much use as possible. I don't care if people "steal" it. I don't get poorer if it is "stolen".
It's best to [ask this sort of question on Stack Overflow](http://stackoverflow.com/questions/ask) (and tag it haskell). &gt; Naturally, it doesn't work as I thought. It's easier for people to help you if we don't have to c+p your source code into a file and compile/run it ourselves. So if your code does not compile, please tell us what the error messages are; if your code runs but generates incorrect output, tell us what that incorrect output is, and tell us what output you were expecting instead.
Let me guess, you get identical rows or columns? To understand randomness it helps to understand how state works in Haskell. All stateful functions have the type: s -&gt; (a, s) where the first `s` is the initial state, the second `s` is the final state, and the `a` is the result. Random number generators are stateful functions and their type is: StdGen -&gt; (a, StdGen) In this case, `StdGen` is the state. If you like at the type of `random` from `System.Random`, you can see that it's type is: g -&gt; (a, g) This means that if I have a certain seed value of type `g` and call `random g`, you get out some random value of type `a` and a new seed value of type `g`. That new seed value is what you have to feed into the next call to random. `random` is a pure function, meaning that if you feed it the original seed again, you are going to get the same result again. Haskell simulates impure functions by using pure functions of the above type and just passing in the new state to the next function to make it seem like the state changed when it really didn't. So back to your code. Your problem is this: repeat $ randomList gen sizeX randomList basically automates the process of feeding the new seed values into subsequent random calls to generate the list for you, BUT it doesn't give you the final seed value back, meaning that if you call it again back on the same original seed, you're going to get back the same original list. That's why every row (or column) of your matrix is identical. Each one is starting from the same original seed. Note that there is no magical hidden place where the standard number generator is stored. That seed that `random` returns is your new seed and if you throw it away then you just threw away your updated state. The two simplest ways to fix this are: * use randomList to generate enough values for the whole matrix and reshape them * use Control.Monad.Random (which wraps `random` in the `State` monad): `randomMatrix x y g = evalRand (replicateM x $ replicateM y getRandom) g` I highly recommend you learn about the `State` monad since it is the core of how all side-effectful code is written in Haskell, including `IO`.
I don't take kindly to sassy one-line arguments, so sorry in advance. The state monad is 'quite' pure, but this is a false analogy. If this was true of the lazy list example, user interaction would be impossible. I have already made my case. See the rest of this discussion. If you want to argue for the lazy list model, *actually make the argument*: examples, arguments, resources, etc. For example, why is my example function 'f' pure? If you don't accept the example, provide me with a better one and tell me why it generalizes to *all* uses of lazy lists of events. Maybe try to model user interaction with the state monad, if it is actually the same as lazy lists of events.
&gt; I didn't say that — I said it stops people from using it in MIT/BSD-licensed software. I don't see any difference between "the GPL stops people from using it in MIT/BSD software" and "GPL'd libraries can't be used in MIT/BSD software". &gt; Fair enough, it's probably slightly misleading, but since there's basically no point in licensing such software under MIT/BSD (since it can only be used under GPL terms), it hardly seems like it would be a popular choice; people pick those licenses for a reason. Huh? Of course there is. When you license your code under the MIT or BSD license, it remains your code, and under those licenses. Anybody who wants to is free to copy it into their own software, under your terms. That's the whole point of using a weak license like MIT or BSD -- it allows people to copy your code into proprietary software. Having your code depend on a GPL'd library is irrelevant to this common use case. &gt; It's true that source-based distribution is by far the most common with Haskell, but I rather think that most people do not want to have to traverse the entire dependency graph of their project to find out whether there's any GPL-licensed libraries depended on by a BSD-licensed library that a library they use uses... before making binary distributions. They need to do this anyway. **If you're distributing binaries made from other people's code, you *need* to know what code is in it, and who it belongs to.** That means when you download the dependencies, you record what they are and what licenses they have. The only sane way to implement this is automation. Checking dependency manually is highly impractical. For example, say some package somewhere deep in your software's dependency hierarchy adds a dependency on a GPL'd package. Now your compiled binary contains GPL'd code, and you need to distribute the source to it. It gets worse. Say another dependency, somewhere completely different, adds a dependency on OpenSSL (which uses BSD-4). Now, through no action of your own, your compiled binaries are illegal to distribute (because the GPL and BSD-4 are incompatible). &gt; True, they technically should anyway, but I think that the BSD license is so popular in the Haskell community because it makes answering questions of use so simple (no references to ambiguous terms like linking, etc.) and reduces friction. So I doubt that authors of BSD-licensed libraries will want to depend on a GPL-licensed library. The BSD license is popular in the Haskell community because many people believe it is more important to increase (quantity of Haskell code) than (quantity of open-source code). I do not think there is any significant fear of the GPL among Haskell users -- indeed, it's often said that developing Haskell on anything but a Linux-based system is difficult exactly because most Haskell developers use Linux and other Free software. &gt; I did provide a link to a rather famous conversation. I have heard vague murmurs that the FSF's lawyers have changed their opinion since then, and also that they might have been wrong in the first place, but as I've never seen anything official about it since and interpreting the GPL in a way that differs from the position of the FSF seems foolish, that's my current understanding of the matter. Please read that link again. It has nothing to do with APIs. That person wants to statically link their binary against readline, but not distribute the source to his binary. His claim is that because it's the user doing the final conversion from object code to executable, his code does not have to be open-sourced. In other words, he's attempting to avoid his responsibilities to his users with a silly technical trick. It is unlikely that a judge would agree with his position. In contrast, APIs have been widely seen as "safe" to depend on, precisely because they do not cause the code to become a derived work. Consider this: I build a Haskell program that dynamically links against libeditline (BSD-licensed). A user downloads a binary, and runs it on a system that has readline installed. Although the binary uses the readline API, no infringement has occured because no portion of readline was distributed, and the program is not dependent on readline itself. It can run with any library implementing the published API. Another case: Linux is GPL'd, but proprietary applications may freely depend on it, as long as they use the public APIs. The official position of the Linux development community is that as long as a program uses the public API, and doesn't use the internal code, it does not have to be open-source just because it runs on Linux.
I rescind my comment as glib.
I played around with the same type a bit last year; here's the [hpaste](http://hpaste.org/45587/higherkinded_quantification) and [irc log](http://tunes.org/~nef/logs/haskell/11.04.11). While I don't recall exactly why I didn't pursue this idea further, I kind of think that I believed this didn't have the same resource properties as Iteratees. And, I think I was dissasified with certain kinds of composition. And, I probably got busy with other things as well. I'm not sure that you can really avoid dealing with error handling in the library itself, for a variety of reasons. For example, you can't catch exceptions in your type unless you are using an IO base monad, and even then I'm not entirely sure what you can and can't implement from Control.Exception.
You're welcome. After listening to others I ended up changing because it seemed like it's better to use the same license as the rest of the community if only to prevent license friction in large Haskell projects. Also, I really want to see Haskell succeed as a language and if my library can help towards that end then I'd be pretty happy. I'm also a big fan of Happstack. Keep up the good work.
Yeah, that's the exact same type, except with the additional error handling. Actually, error handling works well because I already use it with the library without any special integration. I just use ErrorT or EitherT as part of my monad transformer stack and they work with anything, although Control.Exception is just fine, too. The part I think I may need to integrate into the library is a way to distinguish finalizers for automatic finalization when input is no longer needed. This is the only thing really holding the library back from production use. One idea I've been toying around with is some sort of `Catch` constructor like the following: Catch ((Pipe a b m r, Pipe a b m r), Pipe a b m r) ... where the first two Pipes are the code block within the catch statement and its associated handler. The last Pipe, like all other constructors is just downstream code after the block. It has various problems so I can't get it to work yet.
Thanks, fixed it now. Btw, I designed my blog based on your blog. I really liked the minimalism in there.
Excellent! I understand. I'll try both approaches since my goal is to get better at Haskell.
Thanks! Things are really clear now.
Pipes does one thing very well: teach. By studying a simple and elegant solution, it becomes easier to understand the possibly less elegant and more complex practical solution, and the justification for that complexity. Thus, Pipes and Conduit complement each other nicely.
Also, if everything else fails, have a look at the ST monad. It lets you write an imperative algorithm in Haskell in much the same way as you would in an imperative language, but the mutation in kept locked away from the rest of your programme. You might want to compare this to the concept of "weakly pure" from the D programming language as the ST monad and D's concept of weakly pure seem to do the same thing. http://www.haskell.org/haskellwiki/Monad/ST
No prob, I was just a little confused.
One thing that sort of weirded me out about Conduits was buffering. Conduits seemed like a simple enough concept, but then it felt like the buffered data types were hacked on top, and started weakening promises about how Conduits behaved, giving me the sense that Conduits are still very young and that the concepts haven't quite settled yet. I'm hardly qualified to complain, as I do not have the opportunity to write production code in Haskell, let alone Conduits. But did anyone else feel the same? Am I way off base here?
Excellent. I suppose it's the languages I have come from, but this terminology is much more familiar and intuitive for me.
No worries. Interesting that this is your senior thesis. Are you doing much in the way of FRP? I think there is some amazing work that can be done in that space.
Looks neat, except this: ... let li path | path `B.isPrefixOf` (currentURI `B.append` "/") = X.Element "li" [("class", "active")] | otherwise = X.Element "li" [] return $ map (\(path, title) -&gt; li path [buildLink path title]) links Just looks a little cluttered. It could probably be more clearly written (in my opinion) as: ... return $ map buildLi links where buildLi (path, title) = X.Element "li" $ activeClass path currentURI activeClass path uri = if path `B.isPrefixOf` (currentURI `B.append` "/") then [("class", "active")] else [] Part of that is personal aesthetics (putting the less relevant details after, in where clauses, so that you can more easily figure out the overall idea), but it's also to avoid repeating the X.Element "li" part, and that the let with a guard is really just a more complicated way of writing an if statement.
Big fail on me for not noticing that I was breaking associativity. I found a way to fix that, but since my solution requires type-level numbers, it's not that pretty: {-# LANGUAGE TypeOperators, TypeFamilies, MultiParamTypeClasses, FlexibleInstances #-} module Temp where -- type level addition data Unit data Succ n class Summable n m where type Sum n m :: * instance Summable Unit m where type Sum Unit m = Succ m instance Summable n m =&gt; Summable (Succ n) m where type Sum (Succ n) m = Succ (Sum n m) unsucc :: Succ n -&gt; n unsucc _ = undefined -- variable length tuple, left-to-right data a :+ b = a :+ Maybe b infixr 5 :+ class Prependable t r s where type Prepend t r s :: * prepend :: t -&gt; r -&gt; Maybe s -&gt; Prepend t r s instance Prependable Unit x y where type Prepend Unit x y = x :+ y prepend _ = (:+) instance Prependable n x y =&gt; Prependable (Succ n) (w :+ x) y where type Prepend (Succ n) (w :+ x) y = w :+ Prepend n x y prepend _ (w :+ Nothing) _ = w :+ Nothing prepend t (w :+ Just x) y = w :+ Just (prepend (unsucc t) x y) -- variable length tuple, right-to-left data a :- b = Maybe a :- b infixl 5 :- class Appendable t r s where type Append t r s :: * append :: t -&gt; Maybe r -&gt; s -&gt; Append t r s instance Appendable Unit x y where type Append Unit x y = x :- y append _ = (:-) instance Appendable n x y =&gt; Appendable (Succ n) x (y :- z) where type Append (Succ n) x (y :- z) = Append n x y :- z append _ _ (Nothing :- z) = Nothing :- z append t x (Just y :- z) = Just (append (unsucc t) x y) :- z -- pipe type data Pipe a b t m r = Pipe (a -&gt; m (r, b)) return :: Monad m =&gt; r -&gt; Pipe a b Unit m r return = undefined (&gt;&gt;=) :: Monad m =&gt; Pipe a b t m r -&gt; (r -&gt; Pipe a b t m s) -&gt; Pipe a b t m s (&gt;&gt;=) = undefined (&lt;+&lt;) :: (Prependable t r s, Monad m, Summable t t') =&gt; Pipe b c t m r -&gt; Pipe a b t' m s -&gt; Pipe a c (Sum t t') m (Prepend t r s) (&lt;+&lt;) = undefined (&lt;-&lt;) :: (Appendable t' r s, Monad m, Summable t t') =&gt; Pipe b c t m r -&gt; Pipe a b t' m s -&gt; Pipe a c (Sum t t') m (Append t' r s) (&lt;-&lt;) = undefined 
Yes, my thesis actually focuses on FRP. I put up an interactive demo of my implementation work so far [here](http://elm-lang.org/). If you want to try it out, you can write and compile code in the example section. My original goal was to bring *real* functional programming to the web. After creating a largely static system, I found FRP, and pretty soon I was writing a thesis about concurrency and FRP. The thesis has so far been a great way to stay focused and motivated :)
So I've seen Small-step and Big-step formalizations of toy imperative languages in Coq; what's the learning curve for something like K semantics?
But at least it’s safe.
Worth watching just for the guy behind the camera saying "maths did that first".
I solved the same problem three times, using pure-functional programming, in each of Haskell, C#, and Python. It is heavy with commentary as a kind of walkthrough. http://sites.google.com/site/simplehaskell It is a public thought experiment. 
Mathematical logic, more specifically. The earliest comprehension schema I know of comes from Frege's work, where he called it the course of values of a predicate; for any predicate P, the course of values of the predicate is ἐP(ε) (for any Greek symbol you like as the bound variable, to avoid mixing variables). In set builder notation this is of course the same as { ε | P(ε) }.
Thanks a lot! Reading through it now :)
That's a really cool implementation! Good job!
Right. Its inappropiate to assign space or time costs to lambdas in a non-strict setting so talking about the O(f) of a lambda doesn't make sense. But short of reasoning with thunks explicitly... what objects should we assign cost to, what is used to measure cost, and how does the cost of an expression depend on the cost of its subexpressions?
Appending to the end of a list is O(n), the difficulty comes in that the cost is paid not all at once when the call to (++ [x]) is made, but rather incrementally as the result is consumed. So as you point out, appending to the end of a list is O(1) if you never look at the result. This allows you to append to the end of an infinite list, because you'll never look at the entire thing, though (++ [x]) will slow down the production of each element of an infinite result by a constant term. And this is what makes reasoning about resource consumption tricky: you can't really say unless you know how the result is used. In other words, resource consumption is a non-local property under lazy evaluation whereas most programmers are accustomed to eager programming where resource consumption is a local property.
Singly linked lists are great for certain purposes -- especially when used, in essence, as a control structure. Sure, they're not the right structure for all purposes, but we have all those other structures in Haskell too. So it's not even that Haskell uses singly linked lists. Rather, it's that lots of simple idiomatic Haskell does, because for many simple purposes they're the right thing. I actually tend to think most Haskell libraries do a very good job at documenting their performance and complexity.
That's completely fair. I agree (and don't think I ever implied otherwise) that we should not stop the search for a better solution. I've been looking for input on what people feel are the problem points. The fact that buffering is underdeveloped is hardly a surprise: conduits is the first (and only) package to provide a feature anything like it. __Edit__: Just to clarify a bit. My request was to hear what others dislike so I know where to look to for improving the package. Concrete suggestions are of course even better, but just knowing the problem points from an outside perspective is invaluable.
&gt; This all goes along with my conjecture that the real world isn't an elegant place, and thus elegant solutions aren't always enough. I've seen you say this elsewhere, and I can't disagree more. Not with the first part of your statement. The real world isn't an elegant place. But to me, Haskell is about finding elegant, obviously correct abstractions for dealing with that inelegance. Unicode text processing, for example, isn't elegant. But the `text` library, while internally tricky, has a simple API that is very easy to understand. `Functor`, `Applicative` and `Monad` are other elegant concepts that can deal with lots of inelegant concepts in the real world: containers, parsers, IO. Your statement seems like you've given up, and said: the world is messy, so our code will be messy. To me, this is the opposite of what the Haskell community should strive for. That said, an actual working library to solve a real problem is also worth a lot, especially if you need that library for real work. But let's not give up on finding elegant solutions.
At run time. I don't know what the downsides of converting at compile time would be, assuming the non-String type asked for it.
Oh, right, of course. I had noticed that too but forgot. You could add support for Control.Newtype as well (from the 'newtype' package) as another way. (I assume Alternative+Monad implies MonadPlus?)
Runtime, unfortunately.
I think you're reading a lot of stuff that I didn't say. Let me give you an example to demonstrate my point. I think having exceptions greatly complicates the IO monad. I would *love* to be able to ignore them. Especially getting rid of async exceptions, or simply masking them all, would greatly simplify my code, and make it much more elegant. But that's not the world we live in. In the real world, there are real reasons async exceptions exist (e.g., timeouts). So we need a more complicated, less elegant, solution. The same thing with buffering here. I think the package is much more elegant if you get rid of buffering entirely. But in the real world, there's a real need to have it. So we've complicated things a bit, removed a bit of elegance, and produced an approach that solves problems better than what we had before. This in no way means I've "given up." I have no idea where you got that idea. If someone comes up with a more elegant approach that solves the problem, I'll take it. But for now, this is the best solution to the problem I know of, so I'll use it.
Is there a (draft) schedule available? Is it possible to attend all courses, or will the student need to make a selection?
FWIW the problem still exists for doubly linked lists. Haskell still needs to copy the list to append and thus you still have O(n) appending.
Okasaki's [debit method][1] is a great way to reason about lazy evaluation. Basically, you assign a debit to each constructor of an algebraic data type. Whenever you pattern match on a constructor, you have to pay the associated debit. Also, you are allowed to redistribute debits from deeply nested constructors to those that can be reached earlier. This way, you can discharge debits early. Any upper bound on the run-time of lazy evaluation can be proven with this method (in a first-order setting). Also, you are free to make the method as granular as you like, you don't have to keep track of debits in detail if you're happy with the bounds given by eager evaluation. [1]: http://apfelmus.nfshost.com/articles/debit-method.html
Whether we call it `empty` or `[]` is rather unimportant. The key difference between lists and abstract data types is that the latter don't support *pattern matching*. That's where the various proposals for [view patterns][1] come in. [1]: http://hackage.haskell.org/trac/ghc/wiki/ViewPatterns
I think you identified the crux of our disagreement. I think the right approach is 'make it work', 'make it work right', 'make it work fast', and then use it until you decide either (a) it's a good solution, or (b) there are problems. I don't think it's possible to truly know that a solution is elegant or not until you have experience with it. Enumerators are a great example of this. When I started using them, I thought they were the right solution to the problem, and that any discomfort using them was simply due to inexperience. Only after a lot of effort spent on them did I begin to see that there were flaws. I'm assuming the same will be true of conduits. Perhaps buffering as we have it really is a problem, or perhaps once we use it for a while, we'll realize it actually is a perfectly suited solution to the problem, whereas some other aspect of conduits is really ugly. We'll be in a much better position at that point to start working on a better conduits than just guessing today that buffering looks inelegant. That isn't to say we should ignore initial reactions, they're important as well. But nothing compares to actual experience.
I guess it's also a difference between the academic and the professional viewpoint. I don't know about your background, but you are currently a professional Haskell user. I am too, but before that, I was a university master student, and that still has its effects. I try to stay somewhere in between, but I'm not sure if I always succeed. One last point: I think elegance is often a good indicator of a 'right solution' (provided it does the right thing, of course). For example, take Conal Elliott's type class morphisms. That is what I meant with 'obviously correct'. Elegant code is less code, clearer code, and thus less bugs and better abstractions.
Note though that even Haskell's native String gets converted at run-time. In the binary everything has still the form of null-terminated C strings. Custom strings might actually profit from that by having a RULE match "fromString . unpackCString#", then implement a more efficient conversion.
Holy crap, one of my undergrad lecturers is teaching (Georg Struth). Super tempted to go now (though I don't think I'm *advanced* enough yet).
Uh, yeah, that's why I'd like to use Data.Sequence, for example.
It's up to the compiler if it happens at runtime or compile time. It's a runtime expression, but the compiler may decide to constant fold it.
Note further that Frege's ἐf(ε) worked for any functional expression, not just predicates. He interpreted predicates as denoting functions that map things to the two truth values, calling such functions 'concepts'. In that special case ἐP(ε) would be more or less the set-theoretic { x | P(x) } but in general the only way to look at ἐf(ε) is as `\x -&gt; f x`. Application is expressed by a sort of arch symbol we might write ^ (cp.. `$` in Haskell notation) so that ἐ(ε + ε) ^ 2 = 2 + 2 = 4 and ἐ(ε = 2) ^ 2 = (2 = 2) = True. In the latter case ^ could be interpreted as flipped ∈ . I guess you could say he also invented currying, with the so-called double value range, where ἀἐ(ε + α) ^ 2 = ἐ(ε + 2) -- a 'partial application' -- and so (ἀἐ(ε + α) ^ 2 ) ^ 3 = ἐ(ε + 2) ^ 3 = 3 + 2 = 5.
This would go nicely with overloaded pattern matching!
Why Haskell 98 and not Haskell 2010?
It's pretty straightforward to pick up, especially if you are familiar with smallstep kind of definitions. K semantics are very operational, and so often look like interpreters.
Idris is available [on hackage](http://hackage.haskell.org/package/idris). Be sure to read [the tutorial [PDF]](http://www.cs.st-andrews.ac.uk/~eb/writings/idris-tutorial.pdf).
Does the reference implementation have any efficiency issues? It seems like if you try to build a coroutine which has multiple pieces depending on a common piece, the work of the common piece will be done multiple times and not shared. Not sure though.
I was thinking the same thing. Imperative programmers without deep knowledge can founder on low-level issues the same way functional programmers can in their domain.
Idris has dependent types.
Good question. At least when using arrow notation, multiple dependencies convert to the equivalent of `arr $ \x -&gt; (x, x)` so values should be shared. But now that I think about it, I'm not sure about the default implementations for `&amp;&amp;&amp;` and `***`. As for general performance, I'll try to include some meaningful benchmark numbers to the next blog post.
Yes it's not good for documenting the API, haddock is great for that. Literate programming still has its place however, but in a much less common niche: documenting how the code works / writing a report describing a program as a solution to a problem. I did this last month for a client. It was my report on a demo of a system we'd been asked to make and it was also about 750 lines of code. That was actually the first time I'd done a literate programming in a serious context, so yes a rather uncommon niche, but given the context it was just the right solution. I also used lhs2TeX (which is excellent).
On [the example page](http://idris-lang.org/example) is given a partial function: app : Vect a n -&gt; Vect a m -&gt; Vect a (n + m) app VNil VNil = VNil app (x :: xs) ys = x :: app xs ys Is there some reason the first case wasn't this instead? app VNil ys = ys
This is now [merged in](https://github.com/Twinside/Juicy.Pixels/commit/7fa557cead27561b0679e8c0316e0952e51524d0) and there is a [repa conversion wrapper](https://github.com/TomMD/JuicyPixels-repa) available too.
Does FRP normally make it hard to have a fixed time step? That seems to be at odds with using FRP for physics-based animations.
A quick look at ATS and it looks like C with horrible syntax, and seems to be an impure language. I'm not quite sure why I would get excited over it - is there a compelling example showing off it's dependant typedness?
I agree. Literate Haskell works better for things like blog articles or tutorials, where the writing is more important than the code.
I didn't mean to imply that existing FRP solutions were unsuitable for fixed time step, but that's not how they are usually used (and at a glance, I couldn't find any examples using fixed steps instead of continuous time). However, constraining the implementation to fixed steps makes the implementation really simple (and a natural fit for coroutines, which was my original inspiration). I also suspect that a real implementation of an FRP library specialized for fixed time step would be easier to optimize.
The features of ATS I tend to use are the linear types and low level C access. I have a post on [dependent types with ATS](http://www.bluishcoder.co.nz/2010/09/01/dependent-types-in-ats.html) which goes through the dependent type-ness of it, plus [other posts](http://www.bluishcoder.co.nz/tags/ats/) which more cover my "use it as a better C" usage.
Cheers for the write-up! We're still hacking a way a bit in a separate branch[1], hoping to solve a few more issues. Most of it has already been merged to trunk though. In the mean time, check out[2] as well. It is a fork of a web app which I had originally written with a CoffeeScript front-end. Alessandro Vermeulen is now rewriting the front-end in 99.9% pure Haskell ;). You can find the HS code in the resources/static/hjs directory. There's a makefile there as well. EDIT: P.S. There's a UHC wiki page[3] as well, which includes build instructions and tons of other information. * [1] https://subversion.cs.uu.nl/repos/project.UHC.pub/branches/jurrien-xp/ * [2] https://github.com/spockz/JCU * [3] http://www.cs.uu.nl/wiki/bin/view/UHC/GettingStarted
I hope to `finish' the port today. That means, it will have all the old functionality but the HS code will still look a bit jucky. 
Looks great - seems very approachable.
&gt; I'm not quite sure why I would get excited over it - is there a compelling example showing off it's dependant typedness? Perhaps because it allows one to use (a restricted form of) dependent types in practical programming. Also, I do not see why anyone would get excited over a language with beautiful syntax and purity, either. EDIT: spelling
&gt; ATS has two separate languages; a static language, and a dynamic language with types indexed by the static language. It is a bit more complicated than that, even. Some dynamic terms are actually erased (e.g. proof terms, i.e. terms of sort "prop" or "view" are not present in the compiled program). &gt; Generally this means you have to write data definitions (at least) twice: once at the static level, and once at the dynamic level, indexed by the static level. Yes, this is a hindrance sometimes.
Is there a plan to implement Idris in Idris? Is it possible, at least in principle?
I've read the tutorial. I have two questions: 1. What is the evaluation order? (see the `App` vs `app` in the example interpreter) 2. Is there any kind of termination checking like in e.g. Agda?
1. Eager by default but you can add laziness annotations. 2. I haven't figured that part out yet for certain, but here are my thoughts. You can turn off proof checking which may be similar. As far as I can tell, it's a total language and therefore I think there must be some form of termination checking.
I'm very excited about Idris and I want to play with it. I'm having some trouble getting it to build on OSX 10.6. I installed Haskell via the Haskell Platform, which means it doesn't play well with macports (iconv link errors). Do you know which of these are easier: * a) Building my own GHC that is linked to macports iconv (not too hard, I've done it before) * b) Building gmp and boehm without macports so I can use them with the Haskell Platform I don't like (a) because I don't really like macports and I'm always worried it will cripple my ability to use Haskell with standard OSX libraries. So then I'd have to build a sandboxed GHC just for Idris. Not ideal and would I need to always put that in the environment ahead of the system GHC when I use Idris? So then (b) sounds ideal but it might be a lot of extra work. Note: I've looked at homebrew in the past and I already know I don't want to use it, so I didn't list it as an option. Thanks!
&gt; Is there any kind of termination checking like in e.g. Agda? Was thinking about the same thing. In the example interpreter, a program encoded in the term "fact" is a factorial function that will make use of general recursion when evaluated (the term has a loop in it, created with the lazy application). This is also a plus. If there is no termination checking, then one will be able to get any program type-checked. :)
Pardon, it was a bad example. Try this one on then: let cnt x = [x] ++ cnt (x+1) dcnt x = cnt x ++ cnt x in dcnt 1 The prefix of the append is of infinite length, yet it completes in constant time (since it just creates a thunk for the append call). Furthermore, accessing the `i`th element. If append was O(n) on the lefthand side, this wouldn't be possible.
Exactly. Any operation that observes the costs of the append would already observe the costs of traversing, so long as each list contains at least one element. Likewise, if empty, the costs could skyrocket. E.g. an infinite sequence of []++[]++...++[1] would never reach the first element: let bad x = [] ++ bad (x+1) ++ [1] 
&gt; However, because unfoldM is terminated with a Nothing, a fatal &gt; parse error to the consumer is indistinguishable from hitting the &gt; last page, it terminates the stream and says nothing more. If &gt; the resource you are consuming is more error prone than most, &gt; it may be a good idea to roll your own unfoldM which uses &gt; Either to distinguish between normal termination and &gt; termination due to error. I think this could be solved by making `GooglePlusM` an instance of `MonadError`, and then calling `throwError` inside `simpleDepaginationStep`. Alternatively, you could change `simpleDepaginationStep` to return `[Either YourErrorType a]`. This would allow you to discover errors on a per-item basis.
The tutorial doesn't mention structural recursion, or termination checks. Is totality not a goal in Idris? It would be nice if things like: instance Eq MyType where {} would throw a compilation error, rather than runtime partiality.
(I'm the author of [reactive-banana][1].) In FRP, continuous time usually denotes the ability of behaviors ("time-varying values") to vary continuously instead of being pinned to integer multiples of a fixed time step. For instance, it means that you can express behaviors like (\t -&gt; 3 + 5*t) :: Behavior Double [1]: http://www.haskell.org/haskellwiki/Reactive-banana
HWN is back!
I develop on a Mac, and I use the Haskell Platform and compiled boehm and gmp from source. Fortunately, it's painless. A colleague also recently installed in on a Mac using just boehm and gmp from Macports (I think...) which doesn't require iconv and all the problems that entails...
No, there won't be any distinction. It's just a question of what's allowed to reduce during type checking and what isn't. I suppose you could look at it another way: everything needs to be total, except the things that are runtime only, for which the typechecker will just let you get on with it.
Ah, great. That empty instance is a problem in Haskell and a pet peeve of mine :)