Hi Alex, I'm Jed.
seems like a pretty artificial bench mark.
Also, the C++ one appears to be cheating according to the guidelines. It's supposed to be a test of the general memory allocator, but the C++ one doesn't use the default allocator, it uses a pool allocator from boost. I guess the wording in the problem description says "don't implement your own custom memory pool" which is ambiguous. Does boost::object_pool not count because it's not implemented by the submitter? Shall I put up a memory pool library on hackage and then use that in the Haskell example?
Aren't they all?
Like me, you probably groaned, but loved it.
Yes but some more than others. Especially when you have to go against most of the core paradigms of the language like enforcing complete strictness in a lazy language.
Yeah, it was the generic binary I had to install - but still running ./configure and 'make install' gives the impression of compiling from source :-)
Everyone. You can make these *better* (i.e. faster and/or smaller). Some background and Q&amp;A: &gt; Why are shootout programs often long or ugly? Because they in many cases date back to 2004, before we had a sufficiently smart GHC (or any libraries) to do much of the work. &gt; knucleotide is hideously ugly Yes. It rolls its own pointer-based hashtable. If you can roll a faster/smaller/cleaner one, please submit it! &gt; Is vector allowed? Yes. Maybe. Well, you have to ask Isaac very nicely to "cabal install vector". And you better check your code is actually faster before you do so. &gt; Why is the Haskell version of binary-trees so slow Mainly because GHC doesn't grow the heap fast enough, so it spends to much time growing the heap, and not enough collecting the heap in parallell. If you use the (illegal) -A400M setting to preallocate the heap size, GHC is very, very fast. There's not much we can do here, short of a -server flag that would increase all the limits for such situations. &gt; reverse-complement is twice the code of Java It's also twice as fast as Java :-) &gt; Some flags should be tweaked for 7.x series. Try using -fllvm for example.
How does well written typical haskell (not ugly speed optimized haskell) tend to perform compared to other languages?
I'm guessing that detecting the heap growth trend quicker still is possible, but this is pure speculation on my part. What is the growth algorithm? 
One thing not clear from that web site... What is Detexify? Edit: Google cache says: What is this? Anyone who works with LaTeX knows how time-consuming it can be to find a symbol in symbols-a4.pdf that you just can't memorize. Detexify is an attempt to simplify this search. How does it work? Just draw the symbol you are looking for into the square area above and look what happens! 
 F# Mono #3: 57 seconds C# Mono: 7 minutes F# Mono #2: 14 minutes
Additionally, having used it a few times: it is really stunningly accurate.
I believe the folklore number is about 2-3x slower than C (but significant size, debugging time, and cognitive overhead wins).
It's possible to write slow code in any language/VM
I'm embarrassed to say that it took me 10 minutes to understand why this worked. It seems a bit miraculous at first glance. Anybody know of possible concrete application of this technique?
I have to say I agree with you completely, learning Haskell has completely changed my thought processes when coding in other languages. Also kudos must go to the "learn you a Haskell" writer - I think it's a brilliant introduction, reflecting the clarity and logic of the language
You can exploit Hinze style generics to get implementations of lyingSearch for recursive polynomial data types for free, as long as you have the instance. Second, you can search for (some) functions using this mechanism by tabulating them and searching for appropriate tabulations (by exploiting side-effects you can limit the amount of data you need to tabulate). Since every search predicate has to terminate for all inputs, it can only inspect the result of applying the function to a finite number of inputs. {-# LANGUAGE Generics, TypeOperators, FlexibleContexts #-} import Data.Generics import Data.List (find) import Data.Set (Set) import qualified Data.Set as Set import Data.Map (Map) import qualified Data.Map as Map import Data.Maybe (fromMaybe) import System.IO import Data.IORef import System.IO.Unsafe lyingSearchWith :: Search a =&gt; (a -&gt; b) -&gt; (b -&gt; Bool) -&gt; b lyingSearchWith f g = f $ lyingSearch (g . f) class Search a where lyingSearch :: (a -&gt; Bool) -&gt; a lyingSearch {| Unit |} _ = Unit lyingSearch {| a :+: b |} f = if f a then a else lyingSearchWith Inr f where a = lyingSearchWith Inl f lyingSearch {| a :*: b |} f = lyingSearchWith (lyingSearch (\a -&gt; f (lyingSearchWith (a :*:) f)) :*:) f search :: (a -&gt; Bool) -&gt; Maybe a search f = if f a then Just a else Nothing where a = lyingSearch f every :: Eq a =&gt; (a -&gt; Bool) -&gt; [a] every = every' [] where every' xs f = case search (\x -&gt; f x &amp;&amp; notElem x xs) of Nothing -&gt; [] Just x -&gt; x : every' (x:xs) f everyOrd :: (Search a, Ord a) =&gt; (a -&gt; Bool) -&gt; [a] everyOrd = everyOrd' Set.empty where everyOrd' xs f = case search (\x -&gt; f x &amp;&amp; Set.notMember x xs) of Nothing -&gt; [] Just x -&gt; x : everyOrd' (Set.insert x xs) f finiteLyingSearch :: (Enum a, Bounded a) =&gt; (a -&gt; Bool) -&gt; a finiteLyingSearch f = maybe maxBound id $ find f [minBound .. maxBound] instance Search () instance (Search a, Search b) =&gt; Search (a, b) instance (Search a, Search b) =&gt; Search (Either a b) instance Search Bool instance Search a =&gt; Search [a] instance Search a =&gt; Search (Maybe a) instance Search Char where lyingSearch = finiteLyingSearch; every = everyOrd instance Search Int where lyingSearch = finiteLyingSearch; every = everyOrd data Stream a = a :&gt; Stream a deriving (Eq, Ord, Show) instance Search a =&gt; Search (Stream a) stream :: [a] -&gt; a -&gt; Stream a stream (a:as) b = a :&gt; stream as b stream [] b = bs where bs = (b :&gt; bs) -- a very slow search for a function {- instance (Eq a, Search a, Search b) =&gt; Search (a -&gt; b) where lyingSearch = lyingSearchWith (\(b,abs) a -&gt; fromMaybe b (lookup a abs)) -} -- a faster search for a function by exploiting side-effects instance (Ord a, Search b) =&gt; Search (a -&gt; b) where lyingSearch f = f r `seq` r where r = lyingSearchWith fun f fun (xs,x) = unsafePerformIO $ do mbs &lt;- newIORef (stream xs x) mas &lt;- newIORef Map.empty return $ \a -&gt; unsafePerformIO $ do r &lt;- Map.lookup a `fmap` readIORef mas case r of Nothing -&gt; do b &lt;- atomicModifyIORef mbs (\(a :&gt; as) -&gt; (as, a)) modifyIORef mas (Map.insert a b) return b Just b -&gt; return b -- alternately you could use a list instead of a Map and obtain a slightly more permissive type: -- instance (Eq a, Search b) =&gt; Search (a -&gt; b) where 
 $ time ./chameneosredux.ghc-4.ghc_run +RTS -N4 -RTS 6000000 real 1m12.632s user 1m2.980s sys 2m8.630s (Just as a quick check that this isn't a problem with the measurement scripts.)
Indeed.
Learn you a Haskell is probably the best intro/tutorial website I have ever seen written that I could also share with my daughter and wife and they can learn a little from too. Both are math geeks, neither a developer. Now they are learning them a Haskell.
I'm not embarrassed to say that I still don't get it. Anyone care to enlighten me as to why the search' example terminates?
As someone who doesn't know much about this stuff, why would their libev backend get "significantly better" performance when GHC and libev are now both using epoll backends? 
Because your search predicate is required to be able to terminate for all inputs (even infinity in the case of the naturals) and in the case of the naturals the process of evaluating it, gradually builds up a result that IS infinity. Since the predicate can only do a finite amount of work, eventually we'll find something that gives the right assignment to everything that was inspected.
The new age has begun: devs re-write code in Haskell to improve its performance.
&gt; Everyone. You can make these better (i.e. faster and/or smaller). Someday I will be good enough to do just that... someday...
I think the shootout is doing a fine job of showing what different languages are good at, and it goes both ways: Haskell sucks at this kind of stuff, but C++ (relatively) sucks at thread-ring because it doesn't have lightweight threads. There are many algorithms which simply work best with a C-style binary tree and a pool allocator. If it's slow, it's a weakness of Haskell, not of the algorithm for being "against the paradigm".
Hey, given that it was apparently implemented in Ruby before, it's not a shock.
There are basically two choices with regard to web frameworks. 1. Yesod is a very complete framework that Michael Snoyman has done some very impressive things with. It's sort of its own country though: if you use Yesod, then there's a Yesod way to do web serving, persistence, templating... the works. And there's a very strong flavor to it: lots of special purpose languages (not combinator-based embedded DSLs as you'd expect in Haskell), and a rather extreme use of quasiquoting. If you don't mind feeling like you're programming in Yesod as much as Haskell, though, then Yesod is very full-featured and impressive in many ways. 2. The rest of Haskell web applications is more or less similar: you grab a library for request routing, and a library for templating, and a library for persistence... and you just use them. In the Happstack case, at least, there's a bit of glue written for you, and the persistence library and the request routing library have the same library name attached to them... but ultimately, the pieces stand alone and you choose. This isn't actually too bad, since one of the strengths of Haskell is composability: it's not an odious task to make this stuff work together. If you choose option 2, you have a secondary choice of: Snap, Happstack, or even something more primitive like the CGI package. Happstack and Snap seem like the most reasonable choices in *most* situations, and I think it comes down to a matter of: (a) taste, and (b) your short-term performance needs. (If you need high performance and heavy load in the next 6 months, Snap is the way to go; I expect the two to be on a par with each other in the long term.) It's almost trivial to port back and forth between Snap and happstack-server, too, so don't worry too much before you pick one and get started. It's also entirely possible to use Snap with happstack-state (the persistence library that shares the Happstack projects), or various other combinations.
comparing Snap to RoR on server performance is imprecise. RoR is an application framework independent of any server backend. Mongrel, Unicorn and mod-rails would be better candidates for sampling Ruby web server performance. If RoR is insisted to be used in testing code, then the server backend needs to be specified precisely. Simply stating RoR is too general, benchmarking webbrick is also technically useless. The test would be more meaningful if it benchmarks a dynamic request, as no one in production environment needs to pass a static request to the application framework. 
 $ time ~/cham +RTS -N2 -RTS 6000000 real 0m4.843s user 0m7.760s sys 0m0.400s $ time ~/cham +RTS -N3 -RTS 6000000 real 0m11.307s user 0m15.230s sys 0m4.040s $ time ~/cham +RTS -N4 -RTS 6000000 real 0m40.326s user 0m40.960s sys 0m15.630s I only have two cores, so this slowdown isn't entirely unexpected. However my older computer does significantly better even with four threads. Have you tried bumping it down to -N3 or -N2?
So the *tl;dr* version should be: Yesod is like Django or Rails, the rest are like Pylons or Sinatra or others?
The horror!
Thats not entirely true. It is true that Yesod's various monad stacks will include instances for Persistent's PersistBackend that is more of a convenience and doesn't lock you into using them at all. One could use happstack-state to store persistent MACID state in Yesod just as easily as in happstack (not sure about the mtl/transformers issue, but that is getting resolved soon). Also, I believe that Yesod can easily use snap as its server, which I think snoyberg does for haskellers.com. I personally like Yesod, there is a lot of compile time checking of errors that most frameworks wouldn't catch until run time (bad routes for instance). However, cdsmith is right that there is a lot of QQ usage and often you will feel like you're programming in Yesod.
Happstack from darcs and hackage both work with mtl 1 and mtl 2 now. Also, the web-routes library supports Happstack as well. (In fact, it was originally developed for Happstack). Though Yesod has some extensions to it which I have not had the chance to fully understand.
How do I fix packages that require dph-* to build (repa, for example)?
If you choose Happstack, (which I think is a great choice), I recommend using the darcs version. The happstack crash course documents the darcs version, http://www.happstack.com/docs/crashcourse/index.html There are no more API changes planned between now and the next release, so it should be easy to upgrade to the new release in a few weeks. For new applications, there is really no reason not to use the darcs version. (For existing applications, the primary reason not to switch right now is that I have not written the migration guide). 
...And the Reddit load took down the app.
So, as of now there are basically three people with three favorites. Thanks guys! How about we try an informal poll? Vote for the one you like best:
Yesod
Happstack
Snap
Other (please mention)
&gt; so this slowdown isn't entirely unexpected Add more cores causes slowdown isn't unexpected? :-) $ time ./chameneosredux.ghc-4.ghc_run +RTS -N2 -RTS 6000000 real 0m8.072s user 0m15.800s sys 0m0.140s $ time ./chameneosredux.ghc-4.ghc_run +RTS -N3 -RTS 6000000 real 0m38.988s user 0m37.120s sys 0m58.630s $ time ./chameneosredux.ghc-4.ghc_run +RTS -N4 -RTS 6000000 real 1m11.019s user 1m0.200s sys 2m7.650s
I love the first entry in their blog: &gt; Really sorry for the inconvenience but reddit found us. Again. Appears to be working again...
*I was expecting to see a survey of some of the many advanced NLP tools available in Haskell, and some hands-on how-to-use information for experienced programmers.* To be honest, that is currently a very small market. Currently, I'd rather try to bring in more NLPers and people with an interest in NLP to Haskell.
Maybe it's a result of the better inliner in the new GHC. EDIT: Ok, I'm wrong.
[Haskell on a Horse!](http://haskell.on-a-horse.org/)
I agree that it can be uncomfortable to use QQ so much. Basically, there are three places it's used: * Persistent module declarations. The QQ here adds nothing but a convenient syntax, and those truly bothered by it lose nothing by dropping down to the underlying datatypes. * Routing. Here, the QQ is actually very important: it warns you if you have overlapping routes. However, it's still possible to use underlying datatypes. * Hamlet. Though nowadays, external template files are preferred. I should probably write a blog post one day explaining how to get around the QQ usage.
I'm the author of Yesod, so I'll give you some very biased reasons why I think you should use it: * It is incredibly type safe. It uses the type system to help you avoid 404s, XSS attacks, breadcrumbs, security issues and all the regular things Haskell helps you out with. * I think of the three it provides you with the most tools out of the box to just get started. * It deals well with SQL databases, while still providing a type-safe interface. * It runs on different backends. You can use Snap as a backend, FastCGI, or even CGI. There's also a development backend that automatically reinterprets your application. * It's fairly well documented (though I'm still working on it). Check out http://docs.yesodweb.com/. That said, Yesod is not for everyone. I think Happstack is a better second choice just because it will give you more support out of the box, but Snap will give you more raw speed.
GHC guys are looking into this, but the most likely reason is that for libev we spawn N event loops for N cores and use `forkOnIO` to bind Haskell threads to specific capabilities over their lifetimes, but GHC only spawns one event loop for the entire program.
GC improvements factor in here also.
&gt; The test would be more meaningful if it benchmarks a dynamic request, as no one in production environment needs to pass a static request to the application framework. ...Unless your application framework is fast enough so that you don't need the superfluous extra machinery. Re: optimal RoR setups, if some RoR expert wants to provide us with a working setup we will definitely benchmark that on the same machine, but setting up Rails like this is such a Pain In The Ass (w/ capital letters) that we honestly can't be bothered. Besides, Ruby's so slow compared to Haskell that no matter how it's set up we're probably going to whip them.
GHC does not use an event loop as such, when using the threaded RTS. But it does use a single IO manager. When bos and tibbe were working on the new IO manager they had a go at using per-capability IO managers but were not able to obtain any speedup. They were not able to spend much time on that aspect and it remains to investiage why it made little difference.
No, this is runtime system and IO system stuff, not code generation.
&gt; Routing. Here, the QQ is actually very important: it warns you if you have overlapping routes. However, it's still possible to use underlying datatypes. That can be done with a combinator library, too? (Not entirely statically, though, but "almost" statically.)
I always quite liked WASH, although I'm not sure how actively supported it is. It's a pretty neat way to do sessions over CGI, and generally leads to a very natural style of programming. Not sure how the others deal with sessions, but I quite liked being able to e.g. just grab the value from a text field and pass as a parameter to the next page. Caveat: not a web programmer. 
&gt;&gt;so this slowdown isn't entirely unexpected &gt;Add more cores causes slowdown isn't unexpected? :-) I meant the slowdown from using more than two threads on my 2-core machine. =) My hunch was correct, however. This behavior is indeed rather unexpected, and something is definitely broke.
Not really true. Most of the servers used for production Rails apps are written in C.
Yesod uses [web-routes-quasi](http://hackage.haskell.org/package/web-routes-quasi) for routing. Check out the [Web.Routes.Quasi.Parse](http://hackage.haskell.org/packages/archive/web-routes-quasi/0.6.1.1/doc/html/Web-Routes-Quasi-Parse.html) module. The key thing to realize is that the [parseRoutes](http://hackage.haskell.org/packages/archive/web-routes-quasi/0.6.1.1/doc/html/Web-Routes-Quasi-Parse.html#v:parseRoutes) quasi-quoter just generates a list of [Resource](http://hackage.haskell.org/packages/archive/web-routes-quasi/0.6.1.1/doc/html/Web-Routes-Quasi-Parse.html#t:Resource)s. You can completely bypass the quasi-quoter if you like.
Do you have an idea of when 0.3 will be released? Also, what did you decide regarding iteratee vs enumerator?
The PHP benchmark is misleading because: 1. you're using `printf()`, which is more than 10x less efficient than `echo` for printing a single string (based on my contrived benchmark); 2. you're using an outdated version of PHP; 3. you're using Apache (and probably the inefficient prefork model); 4. and you're probably not using APC or a similar opcode cache. Anyway, you should probably also specify your OS and benchmarking utility's settings.
"...we are benchmarking Snap 0.3 which is still in development and scheduled for release in January."
Just a comment on one bit of that: it's true that Yesod can use Snap as its web server... but in my mind, irrelevant. That's an implementation detail, and makes no difference to someone writing web applications. What's worse, it is misleading. Snap provides a perfectly good interface on top of which one can build web applications... one comparable in functionality to happstack-server, for example, but with a somewhat cleaner API; but it feels like there was an early effort to paint Snap as being only suitable as a back-end for a "real" web framework like Yesod. For example, see this summary of web frameworks that keeps getting posted, where inexplicably Happstack is presented as an option in it's own right, but Snap is only mentioned with a meaningless scare-clause "and custom code and libraries..." or as a back end for Yesod... although Snap isn't really less complete than Happstack except for the state stuff in Happstack (which isn't a web thing at all) 
I wouldn't classify myself as an RoR expert but their documentation steers people to Apache + Passenger as the default way of doing production deployment. With a typical rails app page you are making a big overhead payment to have access to a lot of helpful support. I'm guessing Snap doesn't have much support available. (Edit: by support I mean lots of utilities for persistence, templating, routing, etc.) There's a lot of ways to deploy Rails. I suspect that "Thin" plus a Rails "Metal" response would be comparing apples to apples with Snap. Or maybe a raw rack response with Thin would perform well. Also, can we consider switching from requests per second to plain old average/stddev of reponse times? RPS tends to amplify differences in a way that I don't think reflects user experience well. 
Snap is still lacking features as a web development framework. It looks like the developers are focusing on performance over features for now (with very good results so far). For instance there is no authentification library yet. PS: The authentification library is on GitHub, but at version 0.0.1 and not published on Hackage yet.
But no word on iteratee vs enumerator?
Yep. Which makes it possible to check for overlapping routes after parsing, by inspecting the `[Resource]` on server initialization. Alternatively, the routing tree could use smart constructors. It's just that I'm a bit wary of quasi quotation if it can be replaced by an ordinary combinator library approach.
Weird, it seems entirely the opposite to me. Happstack is this "we'll throw away your perfectly good RDBMS and re-implement it ourselves for no reason and force you to do things our way" kind of deal. Where yesod is more like "hey you've done web development before right? Well keep doing it but use haskell now".
I think in general, most real world Happstack doesn't use happstack-state. I don't really think of it as a part of Happstack. It's a separate persistence library that happens to live in the same project.
Heh, I don't know if anyone has actually tried to use Haskell on a Horse (besides myself). If the OP (or anyone else) wants to give it a shot, feel free to email or PM me with any questions. I'd love to help more people use it. It's an exciting piece of technology. That said, it's experimental: use it if you want to challenge yourself and learn some new ideas, not if you want a working website in the near future. HoH will probably not reach a stable release for months.
As far as I can tell, there's basically one thing missing from Snap that I'd like to see included, and that's a decent way to handle stateful tasks. I ended up building such a thing on my own, and it was nontrivial to handle rules for expiring sessions, security checks to prevent stealing session keys, and so on. And it's basically web app plumbing; it should ultimately be part of the library used to write web apps. Outside of that, I don't really *want* anything else to become a part of the base Snap. I'm developing a web application (for now, concurrently in Snap and Happstack) that does authentication just fine... I'm not sure what part of it I'd want to have a library do for me... everything is pretty closely connected to who my application accepts as users, which includes some rather involved things like potentially talking to corporate LDAP servers and such based on how someone set up the configuration. I'll repeat what I said earlier: one of the strengths of Haskell is composability. We don't need to have a framework that provides a new way of doing *everything*.
I feel almost the opposite -- quasiquotation should *only* be used as perhaps vastly better sugar that produces something resembling ordinary combinators in the end. After all, the original motivation is, e.g., creating C syntax trees with concrete C syntax.
I'm with you on the "vastly better syntactic sugar". :-) C source code is a good example. Another very good example is the specification of [TCP headers with ASCII graphics](http://www.reddit.com/r/haskell/comments/cl8d3/quasiquoting_ascii_art_to_define_data_structures/). For Yesod routing tables, I don't see how the syntax is so compelling that it merits dropping the benefits of Haskell as a meta-language. 
Thanks for your comment. I experminetnted a little bit with Yesod a few weeks ago and was pretty pleased. Also, thanks for the haskellers.com site! Though, a bug report: in the top right, my name floats over "your profile". FF 3.6.8 OSX 10.6.4 The site mentions that you support MongoDB. I've never worked with these new fangled NoSQL dbs, and I was wondering if I could get your take on MongoDB vs. CouchDB.
Agreed. After many years of web developing and dealing with "rapid development" frameworks, I have found one basic rule: If it looks easy to implement a simple blog or wiki, don't use it, because it will only give you *tons* of headaches later on. People seem to think that Snap is only focused on performance but I think that's unfair. Its basic request handling and the template system is the best I've seen in any framework. 
Well, Yesod (Persistent) doesn't exatly support MongoDB, and I certainly have little to do with it. Greg Weber wrote an experimental MongoDB persistent backend that I believe has not yet been released. I've never even installed MongoDB or CouchDB. All I know about those two is what I've read on Reddit (ie, they're web scale, they're the future, they suck, etc). I was also working on a Redis backend of Persistent, and I think that's a backend worth following up on. Redis is interesting because it has such a simple data model (key-value store), yet a few very useful features on top of that (incrementing, lists, hashes). I think at some point after a 1.0 release for Yesod I'd follow up on that backend.
Snap has a very good reputation from some of the engineers at work. Strongly recommended.
Do you know of anyone using Yesod on top of Snap? Do people prefer it because it's a fast or because it's easier/more scalable? 
Hey, no downvoting here..
The comments I've heard at work is that snap is both fast, but, more importantly, easier, than pretty much anything out there.
One problem of relying so much on doing things statically and with top-level features (typeclasses, data declarations) is that you are restricted to Haskell's pretty basic module system. In Snap and Happstack, "parts" of a website are first-class values, and you are free to parameterize them using regular Haskell functions, and mutually recursive references are well supported. Both of these are essential in large-scale web development. 
That may be the case, but that is certainly not how the framework is presented. The second paragraph on the front page of the website is "buzzwords, silly nonsense, blah blah, we threw out your RDBMS and did a half assed job of making a mostly useless replacement for it because clouds". Then it mentions "oh, yeah you can use a DB if you really insist". Despite the potentially inflammatory wording, I am not actually suggesting one is better than the other. Just saying it seems crazy to suggest that yesod is "out in its own world" when it does everything pretty much exactly as I would expect. While happstack seems entirely in its own world, even going so far as to suggest using a database is wrong.
You actually get more correct results if you encourage people to downvote the items they didn't upvote, unless you can guarantee that nobody ever downvotes. 
To be fair, the team developing Snap definitely *does* seem to be spending an awful lot of their time working on performance of the server implementation. That's great; but *not* because a lot of performance work needs to be done. Rather, what is needed is to wait for a lot of different people to try Snap, solve their own unique problems in a way that's appropriate for their needs, and then talk to each other about what they did and why, and let the best ideas float to the top. I'm happy to see the Snap team avoiding premature standardization. I agree that the request routing and templating systems so far are very impressive. The request routing, in particular, is as nice as it is mainly because I think we had already reached a good spot in terms of what works out there, and Snap had the benefit of starting from scratch and building it cleanly. My hope is that the intention of the team is to let other problems get solved in a similar manner.
No, I agree with you about the presentation; it's very confusing. I delayed me taking a serious look at HAppS (before it was Happstack) for probably a year or more. I'm just clarifying that when I put Happstack in the same category as Snap, I mean Happstack as a web framework, not as a persistence framework. I can't honestly recommend that anyone seriously consider using happstack-state; it's an interesting idea, but in practice, it insists on enough annotation and structural control of your source code that it doesn't feel very much like programming with native Haskell data structures anyway; and then you'd lost most of the benefits of using a more mature persistence system. I'm also not trying to say any one option is "better" than the other; but I do think there's a fairly deep distinction between the Yesod world and the Snap/Happstack world, right now, in web app development. To me, the main benefit of Yesod is that Michael has done a lot more with it; and the disadvantage is that you have to like how Michael did all that stuff, which as I said, has a strong flavor to it.
I've used Snap at work, and I was very impressed both by the code and by the team behind it. I'd be hard pressed to consider using anything else.
Well, it's hard to disagree when you two are voting Snap. I think I'll give it a look this weekend. Do you have any opinion on the whole NoSQL thing? Which has the best Haskell support?
I updated the post to address your points 1 and 2. It didn't make much difference. Your third point is irrelevant given that more than 50% of the web uses Apache and it's reasonable for us to benchmark against the market leader. The OS is Arch Linux (2.6.35 kernel). You can find the benchmark utility's settings in our previous benchmarks post.
Can anyone provide an idea of where a fast Java web server (whatever that is) would fall in the ranking?
In the article I linked to a Hacker News thread (http://news.ycombinator.com/item?id=1380405) that has some benchmark numbers for the Play Java framework suggesting that it is quite fast. Our newer benchmarks close the gap quite a bit, but since I couldn't get Play working I can't give you a precise comparison. Our goal with the Snap framework is to demonstrate that with Haskell you can get performance that's in the same ballpark as Java with the expressiveness and ease of development of Ruby on Rails. We don't yet have the high-level functionality to rival RoR, but that's coming.
Can you give a concrete example of what you're talking about? It *almost* sounds like you're advocating against a static type system because it's too restrictive, which would seem like a strange argument to make to Haskellers.
I don't think anyone was trying to do such a painting of Snap. I think you're referring to a page on the Haskell wiki. I certainly didn't write that page, and frankly think it should be rewritten (I just haven't had a chance). If it really bothers you that much, please update it.
Firstly, I take absolutely no offense at anything you've said here, my question is coming truly out of interest. In what way do you see Yesod as being so distinct? It's certainly not the Handler monad, right? Both Snap and Happstack (and CGI for that matter) define their own monads that you write your handler code in. Is it just the route declaration? If so, I think it's a shame to really get hung up on it, since it really takes up a tiny fraction of any real website. Are you referring to Hamlet/Persistent? That would just be surprising, given that you were referring to happstack-state as "not part of the package" so to say. I'd just like to know what sends off the red flags about Yesod being different. I'd really appreciate the input.
I *still* don't get it, but I'm a bit thick. How can there be such a thing as "gradually builds up a result that IS infinity"? If n is finite, so is Succ n. Somehow, search' knew when to give up, for this simple predicate. How about coding the Collatz conjecture as a predicate. Can search' now solve *that* conjecture? 
By the way, I think you will get much better advice if you give everyone more of an indication of the types of web apps you're working on. There are definitely strong and weak points for all of choices. For example, if you have a simple service that won't benefit from all the static safety Yesod gives and needs to handle huge scale, you should go with Snap. If you need something that supports FastCGI, Yesod and Happstack would be better choices. And so on and so forth.
While that is true... the fact that Ruby is literally at the _bottom_ of the Shootout while Haskell is hanging around at the top is going to have an effect. The implication of that in the real world is a lot fuzzier, but running on a runtime ~30-40x slower isn't going to be something Ruby is going to get away from scot free.
&gt; Shall I put up a memory pool library on hackage and then use that in the Haskell example? please do!
Here's what *I'm* talking about that feels unusual, but which I accept as a (temporary ?) cost of the unique level of static checking yesod+hamlet provides: - hamlet needs a hard-coded file path for templates etc. It can be relative, but it must be fully known at compile time; ~ is not allowed. Also the files need to actually be there to compile the app. - the same file path will be used at run time. If the files are not there, the app gives an error each time they are accessed but otherwise keeps running. This may be no problem for most web apps. I have an atypical "just-works" personal web app (hledger's web ui), and here's my current workaround: use a path relative to the current directory, embed the template files in the executable, write them to the same path (relative to the current directory) at startup, and then restart the app (hamlet won't see them in that first invocation.) I was actually pretty happy to find this compromise.
PS and the embedding happened via the magical file-embed, for which I thank.. you. :)
Each predicate is only allowed to do a finite amount of work, given any input, so for instance, == infinity would be an invalid check to perform, since it would fail to terminate when given infinity. So we apply the predicate to zero, and it returns eventually, because of the constraint we have on predicates that they are only allowed to do a finite amount of work, no matter what the input is. so we return Succ (lyingSearch (f . Succ)) -- we have started giving back a result! If the predicate never succeeds, We productively return Succ (Succ (Succ ... which is infinity. Your predicate isn't able to actually check for infinity though, since it is only allowed to do a finite amount of work. You can apply your predicate (which can only do a finite amount of work) to the conatural/"one point compactified natural" that was returned.
Some of the things I like most about yesod: - WAI makes it easy to plug in different web-serving handlers, which gives me valuable flexibility and future-proofness. - Completeness and practicality. It's actively developed by a guy using it professionally. - Fresh well-maintained documentation and frequent informative blog posts. - Relative ease of cross-platform installation via cabal install. 
Well, I don't quite know yet :-). I'm just looking for a side project. One of my ideas is more social in that there is user editable content and lots of interaction between users. So, a very dynamic site that could probably benefit from some of the type safety that comes from Yesod. How does Yesod scale? I'm not expecting my side project to be any kind of hit, but do you have any idea of how it would perform in the face of a good volume of traffic? With a moderately complicated site that has lots of AJAXy features, you're going to get many HTTP requests per user per page view, each possibly causing some form of RPC call to a database.
"If the predicate *never* succeeds..." How is it inferred, that it *never* succeeds? In the general case, we only can know that it hasn't succeeded *yet*. f(847) has just failed, but why give up now? f(848) might just succeed. Does some limit, on the oracular powers of search', prevent it from solving [the Collatz conjecture] (http://en.wikipedia.org/wiki/Collatz_conjecture)? 
I haven't done extensive performance profiling, and the benchmarks I *have* performed are certainly too old to mean anything now. But from back then, I showed very minimal overhead from Yesod. Basically, the performance of an app is going to depend on two things: * Persistence layer * Backend (FastCGI, Snap, etc) And, of course, your coding ability. But the thing to keep in mind is that architecturally, Yesod has made a number of performant decisions: lazy I/O is avoided completely in favor of enumerators, Hamlet performs a lot of the encoding and concatenation work at compile time, clientsession offloads storage of session data and allows for horizontal scaling. If you need the absolute fastest web app, you'll probably want to stick with Snap. Or better yet, write it in C. But I have yet to see any performance bottlenecks on any of the sites I serve, and no one has reported any such behavior to me. I do intend to do thorough performance profiling, but I don't believe the time is right yet. I am much more interested in developing a well thought out API and getting a solid set of features than having the fastest framework on the block.
Yes, we need to look into it more. The biggest obstacle right now is that I have no good way to find the bottlenecks. I'm in need of support for getting syscall information into GHC's event log and thus ThreadScope.
Just to clarify: I actually use FastCGI for serving haskellers.com, not Snap. It's entirely possible to set up a reverse HTTP proxy and have an almost identical setup, complete with vhost and HTTPS support. However, I haven't done this for a few reasons: * I get to used named sockets when I use FastCGI. Not a very good reason, but I do prefer them. * In my initial tests, FastCGI outperformed Snap by a significant margin. These tests were in the very early days of Snap, and my guess is that results would be different today. But I'm waiting for 0.3 before doing any further analysis. * I get smaller executable sizes by using FastCGI instead of Snap, which is important on small VPSes hosting multiple Yesod sites. Not that it's entirely relevant here, but the Yesod wiki is hosted using Gitit/Happstack/reverse HTTP proxy.
Are any of the projects they've done public facing? And are they targeting a certain style of web app (AJAX, web service, etc)?
I guess I'd suggest that the Snap benchmark authors drop the Rails numbers from their reports in the future, or spend a bunch of time futzing with Rails config for production. I know which one I'd pick if I was them, and my choice would result in me having more free time. The authors already dropped, what was it, grails because they found they didn't have the expertise to configure it for production and measure it properly. Even though we all "know" Rails is slow, I think it deserves the same respect. Besides, for a Rails project I deployed this year we went with Passenger + Apache as our deployment strategy since that seemed to be the clear default for our low traffic site. So we're strictly slower than Apache out of the gate. Beat Apache -&gt; Beat Default Production Rails | Yay for us! 
&gt;and the disadvantage is that you have to like how Michael did all that stuff, which as I said, has a strong flavor to it. Maybe I just don't notice the flavor because it is pretty much exactly as I would have expected it to be. I haven't looked at snap at all so I can't comment there, but yesod doesn't seem to be particularly unusual in any way.
Since you can't hear me, I will say I am sympathetic to the issues you raise. However, do you have any reason for me to believe that you're going to convince rails to serve, say, ten times that number of requests with minimal tuning? Because at ten times faster, that still leaves it at one-quarter the performance of the next slowest thing in the benchmark. You have a point in the abstract, but concretely, I don't think it really matters unless you can show that they've done something not merely "not quite right" but "hideously wrong and unfair".
The mongoDB library is actually pretty decent now. Yesod's Persistent is essentially a front-end NoSQL interface to storage backends, but ironically the only backends working right now are SQL.
I don't know why people get so turned off by quasi-quoting. It is just used for the routes and to declare the persistent schema (you can use it for inline html also if you like). I find it very helpful to succincltly be able to declare the routes in the schema. You don't have to use the Persistent back-end if you don't want to. I am working on an app now that has no short-term plans for persistent.
Having haskellers.com as a reference has been really helpful for me- all frameworks should come with an example working app. The book I learned Rails from went through creating an app, chapter by chpater, showing all the code.
So I'm not an expert on IEEE floating-point arithmetic, but as far as I can tell this isn't an issue for programs that don't read or write the floating point state (via the FFI). By default, the floating point "exception" which occurs when you divide by 0.0 just causes a bit in the floating point state register to get set, which you can't observe from pure Haskell. It doesn't raise a Haskell exception, or send your process a SIGFPE. If your program does write to the floating point rounding mode register or read from the exception register, I'd think you have much larger problems than the one in the linked issue, due to the fact that Haskell pretends that floating-point operations are pure, when in reality they read and write the floating point state. You'd have to either manage the exact evaluation time of all the floating point operations in your code somehow (including disabling constant folding), or abandon the built-in floating point arithmetic altogether and implement your own operations living in the IO monad. Either way you are obviously working against the grain of the language, and it shouldn't be too surprising that there are issues with things like forkIO.
Hmm, thanks for the good questions. First off, I think I've been careful to fairly point out that Yesod is the most full-featured, capable, and complete option out there for web programming in Haskell... because it's true. Yesod remains rather distasteful to me, though. Partly, I think, it's the overall feeling of the whole space of libraries; Yesod essentially reinvented *everything*, from the underlying iteratee API, to the routing system, to the template system, to the persistence layer, even up to exception handling. You have to admit this is a bit unusual, even in most other languages, and even more so in Haskell. I still have a lot of skepticism left over from times when, for example, someone announced an interesting library for web programming, and immediately the community response was pressure about "WAI-compliance", before we even really understood the design space very well. This isn't as independent as one might hope either. I did seriously consider using persistent once, hoping to avoid the quasi-quoting by writing on my own the code it generates... but it turns out that persistent depends on hamlet and web-routes-quasi. I don't know how essential those dependencies are, but it was disconcerting enough for me to look for other options. And then, yes, there is the quasi quoting stuff. It does look from where I'm standing like every major feature of Yesod depends on a quasi-quoter in some way or another. Okay, so I hate quasiquoting. I'd even rather see some custom build tool reading a separate file at compile time, or something like that, where there's at least a remote chance of having some editor support for the new language in the future. But even more than that, it feels like a lost opportunity to do something with combinators. It's hard to design a nice set of combinators to capture an idea, but when done, you've got something that composes nicely with things around it, in ways you *didn't* expect. Let me give an example: In my current project, the routing rules look something like this: (a) if the Host header is found in some database table, then look up the organization and process the rest of the routes as if we already know the organization; (b) else, if the host name matches "xxx.org.example.com" (which is a wildcard DNS rule), then use "xxx" as the domain name, and proceed knowing the domain; and (c) otherwise, treat the first path component of the request URI as the organization name, and put the rest of the path after that. I don't know whether I can use web-routes-quasi to handle that or not, honestly. It's not obvious from any documentation I could find, but I stopped short of reading the source code. It is immediately obvious that I can invent a new combinator in Snap or Happstack to do it. Why? Because they use Haskell for this stuff, and Yesod doesn't. As for the question of happstack-state... I don't know how different this is, honestly. As I said, it took a year, and the realization that most people use Happstack *without* state, for me to warm to the idea of using Happstack even though the persistence bit is probably not a good idea. But that's not so bad, because there is *no* tie-in between happstack-server and happstack-state; the same certainly can't be said of Yesod and Hamlet, for example. But on top of that, the community matters, and it never pays to be that one guy that insists on doing everything differently, especially when there's interaction going on between the pieces.
I dropped Grails because httperf wasn't able to calculate a replies/sec number once the JVM warmed up. I included Rails (Webrick) because httperf is able to calculate numbers and they are consistent.
Thats fine, because if f(848) succeeds then its done, and it returns Succ (... 848 times ... Zero). The Collatz test for a given integer is a predicate you aren't allowed to pass to lyingSearch. It does a _potentially unbounded amount of work_ on an integer, and so this technique won't be able to provide an answer in the event of a counter-example. That is why you can't tackle the halting problem, etc. You can only ask lyingSearch to apply predicates that do a finite amount of work given any value in the ADT, even if that value is infinitely large. You can ask for a conatural that is &lt; 49 or if it is &gt; 1000 because you can reply to both of those queries in a finite amount of work, even if the conat you are comparing with is infinity! but you can't ask if it is even, because that requires finding the Zero, and so doesn't work for infinity. The whole trick here is that any predicate that eventually returns true or false in a finite amount of time, can only inspect a finite portion of a tree structure in that time. Therefore I can find an assignment to the portion fo the tree structure that it inspects that passes the test eventually, or have exhaustively enumerated all such candidates, even if I don't know anything about the predicate than the fact that it does finite work.
Maybe there's a point here, but I thought that this was well known. I tend to figure that if you understand the need to control rounding modes and floating point exceptions, then you understand that it will take special work to control these things in the presence of green threads. There are ways to do IEEE properly in Haskell, but it requires extra work. Frankly, I think that's fine. It would be better to explicitly document *all* the ways that floats don't act "properly" by Haskell standards though -- which includes more straightforward issues like equality (especially but not only with NaN) and soforth.
&gt; Yesod essentially reinvented everything, from the underlying iteratee API Now that's not exactly fair; the Enumerator datatype in WAI is an almost direct copy from Johan's WAI proposal, updated to avoid dead language extensions. &gt; to the routing system And this one is also a little strange. web-routes is shared between Happstack and Yesod. I'm also not sure about the error handling (failure is basically used internally only, users needn't interact with it). But you're right about templating and persistence, but I don't really think it's unusual for a framework to do that. &gt; I did seriously consider using persistent once, hoping to avoid the quasi-quoting by writing on my own the code it generates... but it turns out that persistent depends on hamlet and web-routes-quasi. That's too bad; hamlet and web-routes-quasi were only included so I didn't need to create orphan instances elsewhere. They aren't actually used in persistent. And as I mentioned, you can do all of persistent without QQ. But IMO, the QQ is less of a problem than the Template Haskell code. But that's just my opinion. &gt; Okay, so I hate quasiquoting. I'd even rather see some custom build tool reading a separate file at compile time, or something like that, where there's at least a remote chance of having some editor support for the new language in the future Do you mean the way that Hamlet files can be external and then read by TH? That sounds like a reasonable request, *except* that it screws up the automatic recompilation when the routes change. If there's demand for something like that, however, I have no problem adding it to packages. But at that point, I'd recommend just using the combinator approach instead. &gt; Let me give an example: In my current project, the routing rules look something like this: ... So firstly, this is entirely possible using MultiPiece. But more importantly, I agree with your decision not to use the default Yesod routing techniques: they simply don't apply here. I believe in using the right tool for the right job. Yesod routing is one of the most succinct forms of declaring routes *for the routing rules it expects*. In your situation, I probably would have written the application directly against WAI to keep flexibility in backends. Overall, it seems to me like you're going to be happier avoiding Yesod, just because you're looking to figure out your own way to solve each problem. Yesod is trying to be opinionated, but with choice: Hamlet is the default templating system, but you can plug in a difference one if you really want. Persistent is the default persistence layer, but you don't need to use it (I think only half my Yesod sites actually use it).
I'm working on an experimental enumerator-based branch, when it's finished we'll evaluate and decide.
Well, we're spending ~30% of our time here parsing http requests, and inliner improvements should have an effect there.
I think the real difficulty is that it may be 'reasonable' to write a library routine which, under the hood, uses floating point rounding modes to efficiently calculate a certified result with interval arithmetic (that is referentially pure and correct, unless another thread messes with the rounding mode along the way) without the library writer even considering that threads could break it. A user might then erroneously use this in a forkIO context, not realizing it's unsafe --- isn't this the sort of thing that the type system is supposed to help with?
It's not exactly extensions, it's just a very specialized Application type to allow subsites. At some point in the past I wasn't using the standard web-routes datatypes, but I've moved back entirely to the original Site datatype.
It's much worse than forkIO threads. Lazy evaluation means that unless you're incredibly careful then you can leak changes in the FP state between unrelated functions. Basically you can't just go playing with the FP state. An IEEE FP binding that covers the full API would be a big undertaking and would almost certainly need code generator support, as well as some cunning to model the IEEE FP API in a pure way.
&gt; isn't this the sort of thing that the type system is supposed to help with? And it does. Setting the FP state is not pure. So you cannot do it without cheating (unsafePerformIO) and if you cheat then it's your fault.
This was an old effort of mine, that didn't work quite right: http://hackage.haskell.org/packages/archive/ieee-utils/0.4.0/doc/html/src/Numeric-IEEE-Monad.html It didn't take green threads into account. Someone pointed this out at the time, and there was a solution, but I never followed up -- basically just force evaluation to take place inside a bound thread. Also, the IEEE type is exported, so it's fairly easy to cheat, but that's easy to clean up. An explicit monad is of course much better for controlling evaluation than working directly even in a fairly low-level imperative language :-)
Also not an expert, but i believe that the point being made is that writing on the floating point state can be done implicitly (for instance when rounding using a foreign function call). In that case the problem is more serious, as the user does not intentionally create any effects.
Well sure, but the obligation is on those wrapping the call to make sure that the calls are not outrageous with regard to the rest of the runtime. There are similar issues with signal handlers and lots else once you start really thinking about FFI.
So, it looks to me like it sort of flattens out after 12 cores or so. That seems... underwhelming. Any idea why it doesn't continue to linearly increase? It would be interesting to see a wider variety of programs.
It happens in all programs. See [Amdahl's law](http://en.wikipedia.org/wiki/Amdahl%27s_law).
Seems like the tension here is between purity and performance. Abstractly, in purity terms, this bug is a no-brainer: Correctly maintain the state. Concretely, in performance terms, not only will this slow things down, it will slow things down for code that 99%+ of the time is not using floating point in a way that cares. If only one choice can be made, the only acceptable choice is to do nothing and leave the system working the way now. The other alternative that is at least abstractly sensible is to give users the ability to tap into the context switching process at compile time. By default, if the user does nothing, they get exactly the same behavior they do now, but if they need to save things, they can. This interface need not even be pretty, IMHO; literally do it in C if you like. It won't be used often enough to polish it, just to make sure it works, and I would suggest doesn't _quite_ require you to compile your own GHC to get it. Then those that need this exotic functionality can make it work. I could see this potentially being useful in other ways too, for similar sorts of interfacing to external libraries and state that is at such a low level you need a more sophisticated interface into Haskell-land.
Or downvote if they think one option is especially bad, upvote if they think one is especially good, and not vote for ones they think are just ok.
ahemhem: http://www.haskell.org/ghc/docs/6.12.1/html/libraries/base/Control-Concurrent.html#v%3AforkOS
Although the possibilities for an `onContextSwitch` that takes a callback do sound fun :-)
Are you suggesting that that be made the default and the current behavior be what you have to reach for? Because there are a lot of issues raised in the bug report that doesn't really handle directly. The linked page did mention that, after all, but I still find the matter of accidentally using something in an FFI that sets the flags to be a potential problem. Yes, the vast bulk of the time that's not a problem. I don't anticipate any problems in my life as I rarely actually use floats. But anybody interested in Haskell at this point ought to know just how hard these problems can be to actually factually _fix_ the problem of your base primitives being broken. Isn't that a big part of why a lot of us are here?
I find the "using something in an FFI" to be a red herring. How about using something in an FFI that messes with your signal handlers? Or using something in an FFI that sets the computer on fire? Or that etc.... And the FFI issue has nothing to do with the threads issue really -- if the function messes with rounding modes, then that messes with everything that comes afterwards, even with only a single thread. The responsibility should be on the person wrapping the FFI call, and to a certain degree on good library support for wrapping tricky FFI calls in a straightforward fashion.
Thanks, hadn't read about that before.
Thanks, for patiently explaining! Even though each individual f(n) is resolved in some finite number B(n) steps, there's no *overall* bound B, which is enough steps for *any* f(n). Is that correct? If so, I'm still mystified as to how search' knows when to give up the search. Of course, what I have to do is study the damn code. There goes my weekend! 
This is nice :) Seems like you'd want it to be polymorphic over the vector type though. Do people use boxed vectors very much?
Ah this is a real curio. I first saw it Jerzy Karczmarczuk's Lazy Time Reversal, and Automatic Diﬀerentiation (he credits it to Wadler's The Essence of Functional Programming). I've a possible use-case come up for it in my own code in the last two days, but I'm somewhat afraid to use it. As a general principle, I do like to be able to understand the code I write.
Ah. I just need to figure out what that really means. The normal web-routes stuff is supposed to already support 'subsites'. So I have not figured out why it is insufficient. If there is something more general I could do in web-routes that would be nice. But maybe your subsite stuff is required glue code for Yesod. Or maybe our ideas about what sub-site means are different. I'll look into after I wrap up this Happstack release.
Title is a misnomer, and I don't want to come across as a dick, but... The distribution itself is neither purely functional, nor transactional, nor lazy. Package manager != Distribution Though this is still a really cool package manager and idea, it is, by no means, a distribution with the aforementioned features. Edit: From the NixOS site: &gt;In NixOS, the entire operating system — the kernel, applications, system packages, configuration files, and so on — is built by the Nix package manager from a description in a purely functional build language. Thats the extent of the purely functional aspect. Again, nothing to scoff at, but not what the title suggests.
A lot of what makes a Linux distribution a distribution is the packaging system and the packages themselves. Otherwise you've got a linux kernel and a blob of difficult to manage goo. Even distributions that build from source have a management system that differentiates them from the distribution. So, not to be a dick, but I think perhaps you're being overly pedantic about the title :-) The title didn't say that Linux was purely functional, transactional, or lazy.
What you just described is exactly what I assumed from the title...
No, but it did say that the distribution is purely functional, and the distribution encompasses more than just the package manager. As you said yourself, the packages are also a part of the distribution, and it is quite possible that someone would build a purely functional distribution (packages, et al) on top of Linux. Maybe I am being pedantic, or maybe I have a different connotation of what a distribution is (I am in systems research, particularly OS stuffs so I could just be overly sensitive). Again, wasn't trying to be a dick, but the title did not describe what the link was to IMHO. Edit: Clarity Edit: As an example of how the title could have been better worded, "NixOS: a Linux Distribution Based Nix--a Purely Functional Package Manager" or something similar.
Are you using APC caching for the PHP opcodes? Most people that uses PHP in prod uses APC which can increase the performance a lot.
&gt; As a general principle, I do like to be able to understand the code I write. I applaud your principles! :-)
Read the second paragraph of the article: &gt; These benchmarks were run as a quick test of how much speed-up is possible for our application. We were just trying to pick the low hanging fruit, here. The project under consideration, consists of ca 9000 lines of Haskell code and it can easily make up something in the order of one working year or more... On the other hand, extending our program for multi-threading took less than three days. They took a *nine thousand line program* which was developed over *an entire year*, with apparently no eye towards parallelism or concurrency. They modified it in *three days* to run *five times faster*. (Edit: Had the wrong number; seems I can't read graphs.) Using a tool which *guarantees* the new version produces the same results. You call that underwhelming? Most programmers and IT managers will laugh in your face if you say a 9000-line sequential program can be parallelized in three days. "You'll have race conditions everywhere; you'll have to gut and redesign the code for new locking protocols, new data structures..." Even though it's just an anecdote, this is the kind of obviously impossible feat that should make anyone stop and give Haskell a closer look.
&gt;Does Nix "garbage collect" old and unused packages Yes; it's a command.
Here's the [context](http://stackoverflow.com/questions/4186902/how-to-write-parallel-code-with-haskell-vectors).
I've been running NixOS for a year now on my laptop. AMA.
Well it's not purely functional otherwise it wouldn't actually do anything. I don't think just because you treat your objects as immutable you can call yourself "pure" - not in the sense of pure functions. The package manager still has side effects, it has to actually download and install shit. So you can be immutable, but not pure - this is fine, and a good thing - but I don't think mis-marketing is a great idea.
Generally, what do you think? Where are you on the bleeding edge/stable spectrum, and does NixOS meet that need?
Neat!
I'm very happy with it. However, since it is such a young distribution you have to package some software yourself. Fortunately this isn't so hard, and kinda fun after a while. Nixpkgs currently runs pretty close to the bleeding edge, almost too close for my taste. However Nix's ability to support incompatible libraries mitigates problems with this.
If it's so "functional" why is their website broken?
Will this actually work in practice? Sun and now Oracle have been working on IPS for a few years, and on production servers the theory breaks down. Packages don't exist in isolation -- they work on data. Unless all packages guarantee indefinite and lossless forward- and backward-compatibility, problems arise.
&gt; ability to support incompatible libraries mitigates problems with this. Of all the things you could have said, I'm not sure you could have appealed to me more directly. I'm currently using Gentoo and cursing exactly this problem; it is becoming increasingly difficult to assemble a system and keep it running without fundamental conflicts that portage can't simultaneously resolve. I've got a 20GB partition sitting empty on my hard drive for just such an occasion, perhaps I'll try it out sometime.
I would like to point out that you can use the Nix package manager without using NixOS. I've used Nix on Ubuntu with good results. Everything is kept nicely separated: the Nix stuff all ends up in /nix without otherwise polluting the host OS. In other words: very low barrier to trying Nix out. And if you find you don't rely at all on the host OS's packages you can take it one step further to NixOS.
Thank you. I can't say I always stick to it as I do have code that's incomprehensible when I go back to it in a years time, but I try at least. 
looking forward to a tutorial and/or paper. (glad you liked the names!)
Generational GC is tuned for programs that allocate a lot of short-lived data, whereas this one allocates a lot of long-lived data. The default GC settings try to make best use of the cache, but in this case it's better to blow the cache and avoid the extra copying. Maybe there's a way to get the GC to self-tune itself for this kind of situation, I'm not sure.
iirc in nixos you get convenient set of symlinks (e.g. /usr/bin/emacs) for all the stuff in /nix, does work the same way if you use nix under different distro?
&gt; Multiplate does not require GADTs and does not require multi-parameter type classes. It only requires rank 3 polymorphism. This made me laugh. :-) Even though I know it's meant in all seriousness. 
I believe the symlinks end up in something like ~/.nix-profile/bin which you would add to your PATH, before /usr/bin if desired. There is probably a "global" /nix/bin as well, don't remeber the details there but the docs should tell. But the short answer is: yes, you get convenient symlinks if you set your PATH appropriately.
Here are some examples: http://haskell.org/haskellwiki/Multiplate
I'm trying to get Snap running with GHC 7.0.1... am I the only one having problems to build *snap-server* with GHC 7.0.1? Specifially, one of *snap-server*'s direct dependancies, *vector-algorithms-0.3.4* fails to build: Data/Vector/Algorithms/Combinators.hs:36:23: Couldn't match type `forall s1. ST s1 (Mutable v s1 e)' with `ST s (mv s e)' In the second argument of `(.)', namely `N.New' In the expression: new . N.New In the expression: (new . N.New) (M.unstream (stream v) &gt;&gt;= \ arr -&gt; algo arr &gt;&gt; return arr)
&gt; to run twelve times faster. Where by twelve you mean five, I presume?
&gt; Using a tool which *guarantees* the new version produces the same results. Oh?
It is also possible to use plates with for each constructor a function. Here are the examples from the wiki written this way: https://gist.github.com/707890
Warning: this information has not been reviewed properly, but I *think* it's correct. The main extension that I needed was the concept of a subsite argument, which I now refer to in Yesod documentation as a "foundation." It's basically a value that gets associated for each site and subsite. I needed some way of passing that information around, so put together a [YesodApp](http://hackage.haskell.org/packages/archive/yesod/0.6.1.2/doc/html/Yesod-Handler.html#v:YesodApp) datatype. We can definitely discuss this later, a better web-routes can only help everyone. I really hope it gets wider adoption (cough, snap, cough).
If it was hard to write, it should be hard to read!
It won't work for unboxed vectors if that's what you mean (I've used unboxed a lot but would have switched to boxed it it ment getting parallelization). Are you saying the majority of your uses are mutable or storable vectors?
Oh, I didn't mean to say it's not impressive. It's just a bit disappointing that it doesn't continue to get better the more cores you add. I mean, with companies like Intel suggesting that we'll be using 100-core computers in the next 20 years, seeing improvements drop off after 12 cores shows that these CPUs will be even harder to program for than one might have otherwise imagined..
We don't officially support GHC 7.0.1 yet, since it is still so new. I made some manual changes to get it working for these benchmarks. Specifically I think I changed the vector-algorithms dependency to &gt; 4. I don't think there was anything else, but I could be forgetting something.
search' doesn't "know" *per se* when to give up the search. It tries to solve **f(infinity)**, since infinity is the result produced by lyingSearch (the result of infinity\*infinity in fact). This computation takes only finite time to finish (15 "steps" to be precise). The tricky part is this: if the result fails for infinity, then in fact it fails for every finite natural number. This fact is related to the shape of the request "x\*x==15".
I liked the brief examples. Often with generics library, I find the necessary boilerplate incomprehensible, but this looks like something I could see myself writing. Or maybe experience in Haskell has given me new experience.
Huh, `mfix` is magic..
@cdsmith: «The last time disk space on a personal computer has been a scarce resource was, oh, 1995? These days I simply can't buy a laptop or desktop computer without an unreasonably large amount of disk space?» Hm... Did you ever shoot in RAW? If you did, you wouldn't consider modern disks to be unreasonably large. There are many other uses which require plenty of disk space. I speak from my experience. On my _laptop_ two years ago I didn't readily have extra 100+ MB on / just to entertain me with a new and cool window manager. Of course, if I really needed it, I'd find some packages to uninstall or even could repartition the disk or maybe could have installed GHC and the rest to /home... Anyway, I just installed `awesome` instead. And on my wife's eeePC 901, there is only 4 GB for / partition.
NB: I'm the one in the email responsible for porting the changes Max did to GHC HEAD. I've since updated his plugins a bit and I now have two of them working with the latest HEAD: https://github.com/thoughtpolice/cse-ghc-plugin does CSE https://github.com/thoughtpolice/strict-ghc-plugin can turn haskell into a strict language (as simple as annotating functions with `{-# ANN f Strictify #-}`, although it would still be nice to strictify entire modules by saying it at the module level, too.) I still haven't ported the loop unrolling plugin Max wrote but I'm working on it.
Oops, seems I can't read graphs.
Yeah, they're using "semi-implicit" parallelism, not explicit concurrency. `par x y` is guaranteed to have the same semantics as `y`. I mean, any time you touch the codebase you run some chance of introducing errors. So it's not a guarantee in the strictest sense. But it avoids the main problem with this sort of project, which is incorrectness due to race conditions and other concurrency errors. It avoids that by avoiding concurrency entirely.
Oh, right; it's just that lately I've been dealing with MVars and atomicUpdateIORef and the like; I kind of forgot about par. Have an upvote. Cheers!
Minor error: newVars n = mapM (const newVar) [1..i] should be: newVars n = mapM (const newVar) [1..n] Very nice work. Haskell is something I'm still wrapping my head around completely.
no, I'm not "jelly". I've already got unsafePerformIO. Thanks anyways though.
I like 'unsafePerformIO' more because it's more middle-fingery. :-)
Seems like `replicateM n newVar` would be even better.
I'm new to Haskell, but this doesn't work for me: &lt;interactive&gt;:1:0: parse error on input `import' &gt; :m GHC.Types &gt; let removeIO :: IO a -&gt; a; removeIO (IO a) = a; &lt;interactive&gt;:1:27: Occurs check: cannot construct the infinite type: a = GHC.Prim.State# GHC.Prim.RealWorld -&gt; (# GHC.Prim.State# GHC.Prim.RealWorld, a #) When generalising the type(s) for `removeIO'
I have been schooled. I had no idea unsafePerformIO existed--or that such a thing would be allowed to exist.
It would work if IO is defined like this data IO a = IO a but it's actually a lot more than that. And in case you haven't realized already, you just got trolled :)
Also: st = Dense [ [ 2, 1, 1 ] :&lt;=: 5 should be st = Dense [ [ 2, 3, 1 ] :&lt;=: 5
Thanks! Fixed that typo. :)
IO is a wrapper for a function over RealWorld, not a value, so to get result of a computation we must pass it RealWorld. newtype IO a = GHC.Types.IO (GHC.Prim.State# GHC.Prim.RealWorld -&gt; (# GHC.Prim.State# GHC.Prim.RealWorld, a #)) The function can be written in this way(taken from Data.ByteString.Internal), which is quite similar to the original removeIO. removeIO (IO m) = case m realWorld# of (# _, r #) -&gt; r BTW, why is not inlinePerformIO in System.IO.Unsafe yet?
IO is not a wrapper around the RealWorld. The IO type is abstract and you don't know how it's implemented. Ghc happens to implement it the way you suggest, but other implementations of Haskell have done it differently. 
It's not really allowed to exist since it breaks everything. 
It is true, the Haskell specification does not state how IO should be implemented. But there is no generic way to write such function, so we have to choose a specific IO implementation. I thought that GHC in type names clearly pointed that this was GHC-only code. It would be interesting to see comparison of IO monad implementations.
Not to my knowledge.
Yesod is looking better &amp; better. Also http://docs.yesodweb.com/blog/please-break-yesod/ Keep going!
I only/always use it when I need a function to appear functional without the taint of IO. For example, if I need to pull a bunch of data from a relatively immutable data source[1], I'll use it inside a function such as getUrlTitle :: String -&gt; Maybe String. The type checker can't guarantee purity, but I get close enough from knowing the context. [1] For example, run-once programs that run through raw historical or factual data in a database, on the web, etc.
I am pretty sure there are some somewhat similar front ends for some of the other linear programming tools. The normal hmatrix-glpik Numeric.LinearProgramming was the only one I could seem to get working properly though.
Some implementations might not allow you to implement unsafePerformIO at all.
&gt; * It is incredibly type safe. It uses the type system to help you avoid 404s, XSS attacks, breadcrumbs, security issues and all the regular things Haskell helps you out with. Is there any writeup of this? I had a quick look at the docs and didn't notice anything specific. One thing I've been thinking about is being able to tie together database information with authentication, so that it's statically impossible to show one user the wrong user's data (for example). Is there any work along these lines that you know of?
I don't actually consider that to be trolled. Actually, this just instructs me to look further into why such a function can't exist -- I would like to think I understand why it shouldn't exist. I find this to be an insightful and attempt to shed light on something I don't yet understand fully. Thanks! If you think I just don't get, that's ok -- but; it's got me thinking. My logical next question would be how is `IO a` defined differently as to prelude such a function as a language semantic. 
The introduction chapter of the Yesod book touches on the type safety giving some of those benefits, and the rest of the book tries to spell out different things. But I think above is the only place where all of those individual features are put in a single sentence. Now, it sounds like you're trying to implement an authorization scheme. My preference is to do it at the controller (ie, handler) level, *not* the model/database level. You can try the latter if you like, but Yesod has built-in support for the former: [isAuthorized](http://hackage.haskell.org/packages/archive/yesod/0.6.3/doc/html/Yesod-Yesod.html#v:isAuthorized). All of this kind of information will go into the auth chapter, which sadly I have not yet had a chance to write.
I always saw them as justifications for truth. False is the absense of justification.
Rails (Webrick) isn't used in production anywhere. If I say, "Look!, nginx and raw c is 1000x faster than Rails using webrick," I haven't said anything meaningful. I'd be willing to help you come up with a reasonable number. However, I have a day job, extra contracts, and my kids have been throwing up all night. So I'm too busy. Maybe someone with more time and fewer kids will volunteer? 
The Nix package manager is purely functional, which is the really compelling part: &gt; Nix is a purely functional package manager. This means that it can ensure that an upgrade to one package cannot break others, that you can always roll back to previous version, that multiple versions of a package can coexist on the same system, and much more. So the title seems apt to me, since it's referring to package management, the only property that differentiates Linux distributions.
Thanks for sharing this link : that gives inspiration :) Haskell is really great for DSLs.
I would disagree with you there--the packages also differentiate distributions, and it is quite possible that someone was trying to create a fully functional distribution (functional applications as well). The package manager is just one part of a distribution, otherwise we'd call Debian and Ubuntu (and every other distribution that uses apt) the same thing.
The system configuration in NixOS is more or less purely functional as well. Not that I disagree with anything you have said.
Yes, but they would not be compliant Haskell 2010 implementations, since H2010 includes FFI which specifies `unsafePerformIO`. Of course, the implementation could provide `unsafePerformIO` as a primitive.
It's allowed to exist by the [FFI Addendum](http://www.cse.unsw.edu.au/~chak/haskell/ffi/). The idea is that you import some pointerific C function, and implement marshalling on the Haskell side, and then assert that the overall procedure is a pure function. In practice it's used for a lot more than that.
What I'd like to see on `/r/haskell`: Interesting articles about Haskell. What I don't want to see: Regurgitated Internet memes and funny pictures which are slightly computer-related. There's [another Reddit](http://www.reddit.com/r/programming) for those. Thanks. 
the λ in the prompt is totally dope
Nom, nom, nom. Gimme more, please.
[Oleg already did it](http://homepages.cwi.nl/~ralf/OOHaskell/)
&gt; Oleg already did it This is usually true. ;)
Can't we use type classes for this?
I was wondering that too.
&gt; This library is inspired by HList[2], and interfaces are stealed from data-accessors[3]. And lenses[4], fclabels[5], and records[6] devote themselves to similar purposes. &gt; &gt; [2]: http://hackage.haskell.org/package/HList &gt; [3]: http://hackage.haskell.org/package/data-accessor &gt; [4]: http://hackage.haskell.org/package/lenses &gt; [5]: http://hackage.haskell.org/package/fclabels &gt; [6]: http://hackage.haskell.org/package/records
It does.
You can kinda achieve it with vanilla type classes, but it's less pleasant. * You have to have a fresh class for every interface. * You need to implement instances for types that will contain that field. We can try do that with tuples: class Flap a where flap :: a -&gt; IO () data Flap_ = Flap (IO ()) instance Flap Flap_ where flap (Flap a) = a instance Flap a =&gt; Flap (a,b) where flap = flap . fst fly :: Flap duck =&gt; duck -&gt; IO () fly duck = do go; go; go where go = flap duck λ&gt; fly (Flap $ putStrLn "Hi!") Hi! Hi! Hi! λ&gt; fly (Flap $ putStrLn "Hi!",1) Hi! Hi! Hi! How do I implement `Flap b =&gt; Flap (a,b)` with the first already defined? That side, a fresh class for every record kind of sucks too because classes can't be controlled by import/export rules. Try it, though, I would be interested! Has uses indexed type families, i.e. the instance is indexed upon the field's type. HList solves it with multi parameter type classes and functional dependencies. I might go through all the available Haskell record libraries and evaluate how they work and how nice they are to use some time... it's a good way to learn how to push what you can do with Haskell. If someone did that for me, I wouldn't complain either. ;-) I [had a go at](http://hpaste.org/41564/policy_classes) implementing [policy classes](http://en.wikipedia.org/wiki/Policy-based_design) after discussing the concept with a friend who mainly speaks C++. I don't know whether I'd use that "pattern" in Haskell, but it's fun to try!
I think they play games with hashes to make it work. Consider: downloadSource :: URL -&gt; Error String PathToSource That obviously has side effects. downloadSource :: URL -&gt; SHA1 -&gt; Error String PathToSource I think now you can argue that this fits into a pure build system. And I think this has worked well enough in practice that they reproduce bit perfect builds on different machines. You just have to capture all the inputs to your build tools and create a way of assuring that your functions actually are pure. In practice I think it takes malice to write an impure function in Nix which works well enough for what they are doing. I think it's a nice design. I'm glad to see it getting attention here. 
You wanted purely functional office apps with easy to configure purely functional printing too? Or a purely functional filesystem with transactions in your Linux? I think they're pushing it with "lazy" (make is arguably lazy) but purity in OS packaging and configuration is a massive accomplishment and the primary focus. Probably a better description than purely functional is admin-proof.
Why would you want to subvert what is potentially the best part about Haskell? *hides*
Funny, Wikipedia's MD5 was my first stop too (back when I started pureMD5). It's actually not bad for a general wiki description of MD5.
Minor nitpick: "Is composed of" vs. "Consists of" vs. "Comprises" But nice blog post, otherwise :)
My categorical journey was very similar.
Awwwwwwww yeaaaaaahhh!!!!
My similar journey was very categorical.
Fixed :)
It's a pity transatlantic flights are so expensive for students. Have fun there!
If I'm really lucky I might be able to make it.
That TheCatsters link has made me spend my whole day watching the lectures. Great resource! Thanks!
I am still in the beginning of my journey, but I would like to recommend a book "An introduction to Category Theory in four easy movements" http://www.cs.man.ac.uk/~hsimmons/BOOKS/CatTheory.pdf It is relatively easy, free and has exercises.
A few years ago I wrote the [category theory chapter in the Haskell wikibook]](http://en.wikibooks.org/wiki/Haskell/Category_theory). It covers just enough category theory to allow you to understand what a monad is from a categorical point of view, and does so without introducing natural transformations. It won't teach you adjunctions, limits and other categorical ideas that have fewer applications in computer science (or at least in Haskell), but: * Is written from a very Haskelly point of view (at the time I wrote it I hadn't studied any category theory in my maths degree -- I since have, and it still holds its ground). * It's shorter than a textbook and could be read inside an hour or so.
FYI, the hackage: http://hackage.haskell.org/package/manatee The documentation: http://haskell.org/haskellwiki/Manatee
Isn't one more drawback that all patches from the various local repositories of the contributors must be included in the main repo before conversion to hashed ? Maybe it is related to conversion from the Hashed to Darcs 2.0 repo. It means that there are three kind of the repositories - OF, Hashed, Hashed-2.0. Can you please clarify it? Thanks ;) [Update]: Thanks now I understood it! Sorry for inconvenience I have too been lazy to investage it myself. 
OF repos do not use caching at all, so they always contain all patches and there is nothing special you need to do before converting to Hashed. What we call Hashed in this post is the repositry format. Inside of a repository, you can have either all patches in the darcs-1 patch semantics or all in the darcs-2 patch semantics. However, Darcs does not enable to create OF repositories that contain darcs-2 semantics patches. Thus there are 3 mutually exclusive flags for ``darcs init``: --old-fashioned , --hashed, and --darcs-2 (the default). They respectively correspond to the combination: OF+darcs-1, Hashed+darcs-1 and Hashed+darcs-2. Only the first kind of repository is to be deprecated in Darcs 2.8 so if you have a Hashed+darcs-1 repository and have no merging problems, you should probably stay with that. If you want to convert from darcs-1 patch semantics to darcs-2 patch semantics, then yes you need to have all patches of the repository. But the conversion depends on the order of patches, so you should better run ``darcs convert`` on the main repository of your project, which should in theory have all the patches. If not, you can force the repository to get all patches by running ``darcs changes -v`` (this displays the contents of all patches of the history). This page may help: http://wiki.darcs.net/DarcsTwo
Anyone else think this decision was made a little too quickly? &lt;/sarcasm&gt;
Each new version of the Haskell Platform comes with a free copy of Duke Nukem Forever.
You meant to say, your co-similar journey was very co-categorical.
The HP follows prescribed, 6-monthly, time-based releases; each package will have at most 6 months consideration. 3 months will be typical, I expect.
Conversion of hashed can be done at any time without having all contributions in one repo. You're thinking of the conversion to the darcs2 repository format. Darcs2 repository format always implies hashed, but the darcs1 repository format can be hashed or unhashed. I hope that makes sense.
What is the way forward? Is Text to be normally preferred over String (at least for anything where better performance is desired)? What does String offer now (besides being more built-in)? Is there a way to overload strings to use Text?
What is the benefit of using this instead of letting the OS + Windomanager (Linux + Xmonad) handle things.
I also wonder, how would I use text together with Text.JSON efficiently for handling utf8 en/decoding?
Be it a troll attempt or an aim at humor, I don't look upon this post positively.
No he's right. This is simply outrageous. One word payload with three words of overhead, this is horrible.
Obviously there's "getting all the ladies," but I, too, am curious about any *secondary* benefits.
This is all part of an integrated livecoding environment. Think lisp or smalltalk os (though of course this is very far on top of the os, but still..)
The discussion was arduous, but as dons notes, even if the discussion was perfunctory, it wouldn't have made it into the platform any sooner. :-) I don't think that sarcasm, even sarcasm tagged with w3-approved semantic markup, will help the process along for the next set of proposals, some of which I'm already quite looking forward to.
[`{-# LANGUAGE OverloadedStrings #-}`](http://www.haskell.org/ghc/docs/6.12.2/html/users_guide/type-class-extensions.html#overloaded-strings)
I'm just have some fun, no need to worry. It's not so much about the *speed* of again in, but about the relative speed: seeing all those discussions on text made it *feel* like years.
I'd love to add my name to the list to show interest, but I'm either being dumb, or there isn't a way to do that without a membership. I'm sure you do good work, but I'm not paying $100 to register for a hackathon.
I think someone needs to write a JSON library that uses text and is focused on good performance (and maybe fixes up some of the silliness in the Text.JSON API). The text library provides all the necessary parts to make that happen.
Both parts of the hackathon will be free and open to the public. Ah, sorry: You used to be able to sign up for a wiki account w/o signing up for the dojo, but no longer. The page had my e-mail on it, for people w/o access. I rewrote the page to make more clear that both a) the event is free, and b) you can e-mail to let me know of your interest. 
Could you give a few hints how one would go about implementing a replacement? (And which silliness in the Text.JSON library are you referring to exactly?)
This might be a more up-to-date version: http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf
&gt; What does String offer now Laziness. Conceptual simplicity. Direct compatibility with list functions. &gt; preferred... where better performance is desired Yes, use `Text` if you care about performance.
&gt; Laziness. Data.Text.Lazy
what happened to 2.6-2.7?
Some explanation please?
[http://snapframework.com/](http://snapframework.com/)
Haskell vs. Emacs Lisp?
It's a troll, which becomes obvious if you read the comments. Don't be bothered by it.
Hmm, the Sony Snap page has prominent TM marks on the name. This may become a legal problem for our own Snap team. 
For the next release we switch to an odd-even alternation of version numbers to represent unstable-stable releases. So we are going to release alpha versions (2.7.x) before 2.8 goes out.
His claim that sum updates a variable is false, if I understand correctly, because sum is implemented on top of fold and fold is a recursive higher order function.
I appreciate it looks slow, but if you're following the thread in detail you'll see that it is actually far from a bikeshed argument, there was a genuine disagreement over which of two principles of API design should take precedence. This stuff is important because it affects how the code you write "feels" and how easy it is to learn, remember and use these APIs.
Doanld, don't be a troll. You may be right, but why be a dick about it?
It's possible, but their TM is probably not a *registered* trademark (otherwise they'd be using ®) and as I understand it, without registration they have no right to prevent "infringement". We also might have a case of prior use, although Sony has lawyers and we don't. Most likely, we'll leave them alone and they'll leave us alone and everyone will stay happy.
At least in the U.S., trademarks do not require registration to be enforceable. Trademark infringement is based on a) demonstrated actual use in trade, and b) likelihood of confusion among consumers. For example, "Snap" brand tires probably wouldn't be considered infringing on "Snap" brand pajamas. Whether an "on hold", only in beta, never released, but advertised application framework named "Snap" infringes on a released, but known to only a smaller community web framework named "Snap" is a anybody's call.
I'm not sure if this is a troll, but if it's not then he's an absolute idiot.
The point at which I took exception was when Bryan said (paraphrase), "Look, I designed the API the way I did for a reason. I've heard the arguments, I'm unconvinced." That should have been the cue to go and vote on whether to take the library as-is. I'm not trying to pick a fight about this: I'm glad that there are quality controls in place, and I'd rather err on the side of being too conservative in the process. But you have to admin that the discussion appeared comically drawn out.
Curried functions represented as branching stripy lines, with colours representing types. Compatible types automatically connect, closest first. 
If you're going to put time into spoofing someone, you could at least try to be a bit funnier. 
Quite cool, nice stuff.
Looking at Text.JSON, its clear there are multiple parser backends already. Adding a new backend for, e.g. parsec/Text or any other Text-capable parser should be very straightforward. This could even be implemented as an entirely separate library. The type of JSON strings is kept abstract in the library, so they can be changed to Text as well, although that would be a genuine patch to Text.JSON.
&gt; Mainly because GHC doesn't grow the heap fast enough, so it spends to much time growing the heap, and not enough collecting the heap in parallell. If you use the (illegal) -A400M setting to preallocate the heap size, GHC is very, very fast. &gt; There's not much we can do here, short of a -server flag that would increase all the limits for such situations. Try +RTS -H (no argument) with GHC 7.0.1. I can rename it to -server if that would help :-) But you'd have to wait for another release cycle. 
To sum up my thoughts I quote the author: &gt; Opinions are a dime a dozen, including yours. You're still a fool, from my vantage point.
No, still not funny.
Superb work... a little hard to follow at first, but after reading your first comment I get it a lot more. The crash made me smile too :)
Probably makes most sense if you focus on the definition of every: every :: Int -&gt; (Pattern a -&gt; Pattern a) -&gt; Pattern a -&gt; Pattern a every nth cycle, apply the given function to the pattern and &lt;~ &lt;~ :: Int -&gt; Pattern a -&gt; Pattern a so (&lt;~ 1) is passed as a function to (every 3), along with a pattern specified as a sequence within [ ] 
Goading people for being profoundly interested in their work, by implying that they should instead be interested in sex, only works if your target is also a teenager who isn't getting any. 
where can I get it to play with? :D
Man, you just threw a lamb into the lion's den. Can you expect it to survive? Shame on you. But thanks for the traffic. :-D PS. Mathematicians have shot computing in the foot. Programming is not about function calculation. It's about stimuli and responses, about sensors and effectors, about timing and communication. In the end, it's all about behavior. Problem is, timing is not an inherent and fundamental part of functional programming or the Turing computing model, for that matter.
Have you considered that, maybe, some people like to make up their own mind without a little know-it-all chaperon on their shoulders?
The OP was asking what people thought.
At some point there certainly are values loaded into registers or pushed onto stacks.
Right. But a "variable" is a semantic construct at the language level which is far above the level of registers. A single "variable" in C or whatever could at times be stored in a stack, a register, and maybe even the heap. But it only has a single identifier.
Wow. This entire blog is grade a crackpottery.
I haven't released it yet, will do soonish though.
Thus, the low level manipulation of state is hidden by the language. And that's exactly what the blog post said. Now, what he concludes from that is a bit over the top. But in principle he is right in that it is hard to predict memory and cpu usage in purely functional programming languages.
Yeah, but I'm sure he/she was not asking for chicken shit ad hominems from people who assume that their mere opinions are worth more than a dime a dozen. He was asking for arguments for or against the ideas expressed in the linked article. And that's the problem with social sites like Reddit. It's all about chicken shit opinions and giving assholes and ass kissers the right to vote.
Thanks for the review, i've been interested in what Haskell is, so i'll give it a shot. I admit, so far, anytime i've looked at Haskell i ended up closing the browser pretty fast. So i don't even have a grasp as to what makes it so amazing. Though, i am a bit of a Python fanatic, so i may not end up switching (plus, i am a web dev.. so i'll miss my python web frameworks)
I would really recommend proactively changing your name now. Snap (Haskell) is still young and you can still change. If it becomes a problem later it'll be a lot harder. And if they do decide to sue, you _will_ lose. Having it first is not a defense, and if it isn't fully registered it probably will be soon. And there's no way they are not going to be considered "confusable" by a court. I'm not saying this because I'm on Sony's "side". I think it was rather tasteless of them. I'm saying this because this is a chance to dodge some real problems in the future and I am hoping I can prod you to take it while it's still (relatively) easy.
Is less lazy than `String`.
That looks very useful. 
Manatee is MVC design, that's mean you can use split window (M-t / M-T) split current window, then mix different extension to work together. And classic WM can't do that, you can try Manatee, open many different module (F5 - browser, F4 - filemanager), then do M-t and M-0 Session manager in developing, after then, you can switch different layout smart. Example, when you're coding, at this time, someone send mail to you, you can press key to switch *Mail Layout* to reply, after that, you can switch back to *Development layout* to continue coding.
Now stilling in early stage, many ideas lying on my TODO list. :) Next step is "Customize System", "IDE features", "Mail-Client", and "Terminal emulator", after that, i will develop more fun extension, such as Twitter-Client, BT-Client, Org-Mode (orgmode.org) ... etc. I believe you guys can build better extension than me. Any contribution are welcome! :)
Yes, i like "Integrated Live Environment", build everything you need, but not just for development. :)
is very easy to setup, just install the PHP APC extension and set it to on. Of course you could fine tune it, but I believe that is not the goal of your benchmark
Yes. The low level manipulation of state is hidden by the language. But that's not the same thing as saying that there is a "variable" which is "updated". One can argue that since in typical C you don't explicitly load values into registers, that equally hides low level manipulation of state.
What about web socket support? Is any of the haskell web framework support them? There is a [library](http://hackage.haskell.org/package/websockets) that works well on Hackage, but it'd be nice to have it integrated with the frameworks.
&gt; Having it first is not a defense Um, yes it is? BTW I fibbed about not having access to lawyers. Maybe I should ping them and see what they have to say.
Very cool!
Frankly, I think it's a bit too early to start incorporating websocket support because: * it's not well-supported in browsers yet * we haven't really figured out the proper techniques of providing support I'd be interested in any thoughts people have for support websockets in Yesod (or Haskell in general), but I personally don't have any designs in mind for implementing something.
The function itself is the variable. Think about it.
Functions are constants. They don't vary.
Haskell is on its way to high performance computing! I see a pattern on the joint of Haskell and Accelerate code that many arguments are lifted into Accelerate with use: dotp xs ys = let xs' = use xs ys' = use ys in fold (+) 0 (zipWith (*) xs' ys') Perhaps a family of functions liftAccN will be more convenient. Then we could write dotp xs ys = fold (+) 0 (liftAcc2 (zipWith (*)) xs ys)
That sounds wise to me.
I like it a lot, good work.
Looks like fun! Will definitely try it out once it becomes readier.
Personally I prefer Haskell syntax over Lisp syntax, but then syntax is just a matter of taste. One thing confused me while reading the article though. In the Macros section, the author shows a simple problem he says he could not solve with Template Haskell. Now maybe I'm misunderstanding his problem, but the following seems to do exactly the same thing, without any need for Template Haskell: import System.Directory if' t f c = if c then t else f function1 = doesFileExist "lalala" &gt;&gt;= if' (print "It does") (print "It doesn't") mif c t f = c &gt;&gt;= if' t f function2 = mif (doesFileExist "lalala") (print "It does") (print "It doesn't") In fact, for the first version he pretty much provides the solution himself. The second one seems virtually identical to his Lisp version. Perhaps his example is too simple, but I don't understand why he claims that this isn't a 30-second fix in Haskell as well. Maybe it's just me, but I only very rarely find myself needing macros in Haskell. Most everything can be done with plain old functions.
Yep, this was discussed some time ago when the compiler patch he's talking about was released. If doesn't necessarily do a lot of good for 'if', you're right that the solution given in Lisp syntax isn't any better than the solution you provided. The patch mentioned *does* do a lot of good for 'case', where there's pattern matching involved and you can't pass patterns as arguments to a function. I haven't thought about whether having a LISP macro would help there, but I greatly look forward to seeing something like that patch in GHC for the *case* bit, not the *if* bit.
Interesting, though the comments that imply those of us that prefer Haskell syntax to Lisp were a bit off-putting. &gt; I love Lisp syntax! Who doesn't, right? I feel like raising my hand and saying "me, me!" Especially when the article is mainly a lot of personal taste: don't like whitespace sensitivity; don't like uppercase letters, etc. I especially didn't understand the whitespace bit: is the point that if Haskell didn't use indentation for syntax, programmers wouldn't bother indenting their code consistently? If that's the idea, then I'd be glad, as a collaborator on the same project, if they are using a compiler that complains about that before they check it in and let other people have to deal with it! In any case this is clearly a matter of taste. If Chris dislike Haskell syntax enough to write the code for a preprocessor, then I suppose he ought to do it! :)
I like it. But I basically think syntax is just a rabbit hole which sinks a lot of time and achieves little more than aesthetic pleasantness: a lot of code and conventions to change just for the sake of prettiness. Semantic improvements are typically much more worthwhile (and harder to discover).
&gt; Direct compatibility with list functions. Although one has be really careful when treating Unicode sequences on an element-by-element basis (which is what the standard list functions do). For example, this is wrong: stringToUpper = map toUpper as uppercasing a character can yield more than one characters. 
Syntax. Why do we still fight this battle? He hates upper case because it's "hard to type". Personally, I find upper case easy to type, I do it without even realizing I'm doing it (I'm a very good touch typer). '_' and '-', however, are more difficult and require a pause, because they are on the wrong keyboard row. I hate that. However, why does any of this matter? Why isn't code stored in an AST and every individual coder can be shown their own syntactical representation of the AST? One can write code with a C-style syntax, another with Lisp. One can have all the types written out, like in Java, another can see just the types one would see in an inferred language, and another can skip all the types if they want. Why can't we do that? Is it because computers aren't fast enough to make this translation from code to AST and back again transparent? Is there some fundamental reason a given AST cannot be shown in such various syntaxes? Would it be too difficult to keep the AST stable when generating from different code syntaxes that are essentially the same? 
I have been wondering about this for some time. Anyone interested in writing an editor/plugin with me as proof of concept?
tell that to yesod devs
I couldn't even finish reading the article, so maybe there's a disclaimer at the end telling me the whole thing is a joke. Lisp syntax solves very few of these points (lowercase vs. uppercase, really? and if you want to fully parenthesize all your expressions, you can do that in Haskell syntax too!) and the proposed syntax of multiargument functions is just bizarre.
I remember when I started writing Haskell programs, it was really kind of difficult to get through the syntax, but after I overcame it , I find haskell the most beautiful and readable programming language to code in.
&gt; Syntax. Why do we still fight this battle? He hates upper case because it's "hard to type". Personally, I find upper case easy to type, I do it without even realizing I'm doing it (I'm a very good touch typer). '_' and '-', however, are more difficult and require a pause, because they are on the wrong keyboard row. I hate that. Which battle? I can touch type just fine on my Das Keyboard, but you're essentially just pointing out that people have different tolerances and tastes. &gt; However, why does any of this matter? Why isn't code stored in an AST and every individual coder can be shown their own syntactical representation of the AST? I agree, and that's what I want. Lisk is parsed into Haskell's AST. Haskell is parsed into Haskell's AST. There is a pretty printer to print that AST to Haskell. If I define a pretty printer to print that AST to Lisk then there will be bidirectionality between the syntaxes. &gt; Why can't we do that? Is it because computers aren't fast enough to make this translation from code to AST and back again transparent? Is there some fundamental reason a given AST cannot be shown in such various syntaxes? Would it be too difficult to keep the AST stable when generating from different code syntaxes that are essentially the same? It's sort of an ideal situation for this to be the case. I certainly do not want a segregated community due to syntax differences. In fact it's an opportunity to make the community bigger by attracting more people if there is more choice of syntax. You know this, but it's worth mentioning that although the .NET and JVM runtimes provide half a solution by abstracting away from even the AST, when you want to hack on the actual code you can't unless you know the language. I think we *can* do it provided that there is a *proper* bijection. There are some issues though: things like backticks for making functions infix in Haskell has no equivalent in Lisk, so that means it is not a proper bijection. Comments might be another issue, one would need to make sure that the converter keeps track of comment positions and inserts them at appropriate places. I don't think speed is a concern at all. Mostly it seems like a social issue. If we stored the *AST* itself in SCMs, instead of code, then maybe that could work. However, until that's the case, for now, we can at least, for example, take a Haskell project written in vanilla Haskell, open it up in our editor and hit "convert to Lisk", hack hack hack, then when done, "convert to Haskell", push the patch. There will definitely be differences between what the pretty printer outputted and what someone wrote originally, mostly to do with spacing, but it's probably not a big issue. Right now I am happy with a basically one-to-one correspondence. There will be more technical issues once Lisk has a richer syntax that extends beyond what Haskell has. For example, if I add syntax for map literals, then in Haskell that would be converted to a `fromList` call. When I read that back from Lisk, I would have to be clever and realise that I should translate this into a map literal when pretty printing. There are many other issues once you start thinking of new syntax for Lisk, but I think you get the idea. I won't bother mentioning the difficulties of dealing with macros.
I think about syntax more than the average programmer. maybe because I am not a programmer. there is no syntax battle here. I think Chris Done is just having fun so let's leave him at that. 
As cdsmith mentions, `if` is probably a bad example. Case is a better example. Although your definition of `if` does not match mine. In mine, I can write `(if cond this that)` or `(if this that)`. This is the lambda-if provided by the patches [cdsmith](http://www.reddit.com/r/haskell/comments/ec0kp/lisk_lisp_and_haskell/c16y33a) describes. The same was done for `case`. You can't do this in plain Haskell. With Template Haskell, such a macro would have to be surrounded in `$( .. )`.
It's only partly about prettiness, that's just an added bonus I considered after trying to solve the real problem: editing. [Paredit-mode](http://www.google.co.uk/search?hl=en&amp;source=hp&amp;q=paredit+mode&amp;btnG=Google+Search&amp;aq=f&amp;aqi=&amp;aql=&amp;oq=&amp;gs_rfai=) is truly a fantastic way to edit code. Lisp lends itself to being very easy to work with. Haskell is complex and tiresome to work with. I don't think you should undersell syntactic improvements. The fact that I can write `1` instead of `fromInteger 1` is a good syntactic decision, same for `"foo"` meaning `fromString "foo"`. I think generalising this to all datastructures with [foo] would be awesome. Haskell's semantic richness is what attracts me to it. This is the first time in two years I've allowed myself to consider the syntax and how much I dislike it, I'd like to think I've had enough experience and consideration to decide that I need to do something about it.
I think there's a bit of precedent in doing automatic conversion of some "canonical" form to and from various user- or system-specific forms on checkout and checkin from SCM. Namely, line break conversions. All that remains is to get to the point that conversion to and from your alternative syntax for Haskell is as canonical as converting from \n to \r\n, and we're golden! I suspect you'll run into more difficulty than this, though. Programmers use the flexibility in their syntax to great advantage: lining things up to express the idea that they are comparable or emphasize that they follow a pattern, for example. I doubt many programmers would be happy with a system that normalized all the whitespace in their programs upon checkin. But perhaps they could be taught to live with it, if there were other advantages.
Yea, layout sucks sometimes. Good thing it's optional. let { a = 1; b = 2 } in a * b
With a good AST editor we'd need a good AST-oriented SCM too, not the existing line-oriented ones.
I'll raise my hand too! I don't like Lisp syntax, and I don't like macros that much. I avoid template Haskell as much as possible.
&gt; is the point that if Haskell didn't use indentation for syntax, programmers wouldn't bother indenting their code consistently? No no no. Although there are indentation mismatches between developers (hence tibbe's excellent style guide), that's not a big deal for me. It's the editing that is tiresome to me. I even enjoy writing JavaScript more than Haskell, syntactically, *not* semantically. I don't know if you have tried to write Haskell code very quickly, because, granted, you tend to think more than code in Haskell, but I find it slows me down and tires out my hands. &gt; I love Lisp syntax! Who doesn't, right? That was only joking! Very few people who I've spoken to in the Haskell community like Lisp syntax. They make a wise crack about lots of irritating parentheses (amusingly, to a Lisper, Haskell and Perl look the same), but go figure, taste matters.
Let me be more clear: It is not a _sufficient_ defense. It is not the case that you can just show the judge proof that you have it first and get it summarily dismissed at no great expense to you. Remember "[Go](http://code.google.com/p/go/issues/detail?id=9)"? A preview of coming attractions.
&gt; Lisp syntax solves very few of these points Which points are not solved? &gt; the proposed syntax of multiargument functions is just bizarre. Why? &gt; and if you want to fully parenthesize all your expressions, you can do that in Haskell syntax too! You can. Kinda. For expressions. But it doesn't gain you anything. In Lisp, it does. The syntax for everything is `(operator argument1 argument2 ...)` The space is important, meaning that what variables can contain is very flexible. I can define `1-` as a function, or `call/cc` instead of `call-with-cc`, we can turn operators into multi-argument functions for free. Spaces are similar in Agda, and so operators you define can be very arbitrary and very natural. Speaking of which, in Lisp I can define ℕ as a variable. Anyway, I expect people who've written software with Lisp before will be more interested than than those who haven't.
 getSomething &gt;&gt;= \x -&gt; case x of OneThing -&gt; AnotherThing -&gt; ...
His problems with the indentation seems to stem from his use of Emacs, or any automagically indenting editor. I had this problem too, but realised that it wasn't Haskell's fault at all. And if you're going to indent your code anyway, why not assign semantic meaning to it? 
&gt; I can write (if cond this that) or (if this that). I'm curious: what effect does this have on type inference, particularly when using partial application? Having both means that the function \a b -&gt; if a b can be either Bool -&gt; a -&gt; a -&gt; a or a -&gt; a -&gt; Bool -&gt; a. The only way I know of in Haskell to solve name conflicts is to use qualified names. How does Lisk do it? Of course the same thing goes for polyvariadic functions. Do you use the same solution Haskell does for polyvariadic functions (such as printf) and assume functions are always fully applied, preventing point-free style? After all, (+ 1 2 3) could technically take anywhere between zero and infinity arguments.
The problem, as was discussed when the patch for lambda-case was suggested, is that you have to come up with an intermediate variable name, in this case `x`.
&gt; I'm curious: what effect does this have on type inference, particularly when using partial application? There isn't partial application for `if` because it is not a function but a special form, so you have to write `((if :true id id) 1)`. This is the same in vanilla Haskell: `(if True then id else id) x` &gt; Of course the same thing goes for polyvariadic functions. Do you use the same solution Haskell does for polyvariadic functions (such as printf) and assume functions are always fully applied, preventing point-free style? After all, (+ 1 2 3) could technically take anywhere between zero and infinity arguments. The polyvariadic functions only applies to operators. I certainly do not want to prevent point-free style or partial application. Haskell defines certain symbols as operators, like `+`, so it's easy from Lisk to detect that and say "oh, if this is used with more than one argument like `(+ 1 2 3)` then lets rewrite that to 1 + 2 + 3", for arbitrary functions it would be more difficult and most likely wouldn't work out.
XMonad basically forces split window views. Right now I have my e-mail in the first tab of my browser. So if I am coding without my browser showing, I switch to the browser desktop (M-1) and switch to the first tab if necessary using vimperator (1gt) Is it correct to say this is kind of like a Desktop Environment (like Gnome or KDE) but it is integrated in a user space program instead of with low level OS integration? It seems like the benefit of Manatee is that the extensions can more easily inter-operate than normal OS programs- but I think you need to describe (show video) of some compelling workflows that are difficult to do otherwise.
Wow. Nothing varies in functional programming. It's a block universe where nothing changes. What utter crap.
Check out the ["subtext" videos](http://subtextual.org/). The guy's point is that it's ridiculous that we still work in text files like cavemen programmers, and that having smart editors for ASTs could be very cool.
&gt; I don't know if you have tried to write Haskell code very quickly, because, granted, you tend to think more than code in Haskell, but I find it slows me down and tires out my hands. I've noticed this as well. Thinking of defining some Vim scripts/macros to help. E.g. when inputting a function composition: func = foo . bar . baz I write the functions in reverse order which is clumsy to type. Maybe I'll look into that.
&gt;However, until that's the case, for now, we can at least, for example, take a Haskell project written in vanilla Haskell, open it up in our editor and hit "convert to Lisk", hack hack hack, then when done, "convert to Haskell", push the patch But here's my concern with that: if you did that, would all the Haskell that you didn't touch be pretty printed back into exactly the same Haskell that you originally downloaded after converting to AST, then to Lisk, then back to AST, and then finally back to Haskell again? I doubt it. Because of that, I would think that a culture change is required wherein people understand that what is saved is the AST, and that the code they start working with every morning is always "normalized" into a very standard format. At that point, they may start writing non-standardly-formatted code, but in the morning, it will be changed (or in fact, after any commit and refresh cycle). &gt;but you're essentially just pointing out that people have different tolerances and tastes. Yes, it's like you read my words and understood my point. 
I would, I know ANTLR and StringTemplate enough to assist, but I don't know Haskell or Lisp, so my help would be fairly targeted toward templating and parsing issues. Though I'd love to learn Haskell :-)
I think I would prefer writing it in Python. It would depend on the availability of parsers and the ease of integrating it with an existing editor.
&gt; But here's my concern with that: if you did that, would all the Haskell that you didn't touch be pretty printed back into exactly the same Haskell that you originally downloaded after converting to AST, then to Lisk, then back to AST, and then finally back to Haskell again? I doubt it. I doubt it too. The problem lies specifically in that `astToHaskell . haskellToAST /= id`. &gt; Because of that, I would think that a culture change is required wherein people understand that what is saved is the AST, and that the code they start working with every morning is always "normalized" into a very standard format. At that point, they may start writing non-standardly-formatted code, but in the morning, it will be changed (or in fact, after any commit and refresh cycle). Which could be considered a good thing. People in the #haskell channel were saying that they'd rather follow a style that was machine-enforceable. Tricky culture / social thing. &gt; &gt; but you're essentially just pointing out that people have different tolerances and tastes. &gt; &gt; Yes, it's like you read my words and understood my point. [Alright, gay!](http://www.youtube.com/watch?v=NQkA9iqCGSY)
Good point. I do that too. It's really weird! baz... backwards, dot, bar... backwards, dot, foo. For what it's worth some people use (&gt;&gt;&gt;) but I find that way too noisy by comparison. The order of (.) seems counter to how people think and write code. But then it also matches up with (foo (bar (baz))), maybe the analogy it's not worth it.
ANTLR supports generating python lexers and parsers, so that's not a problem. We may not be able to use StringTemplate, but that is not a big deal, as I suspect python has zillions of templating engines :-)
Another issue is that we have to recognize this would only be for pure syntax differences. For instance, one could not use an AST to translate java to scala and back again, because scala code has more information in it than java code does. I suppose the AST could include all info, but code written in Java might underdetermine scala code, thus making the scala template printer difficult to write.
Thanks, I was trying to remember the name of this project. I saw it a few years ago and thought it was brilliantly innovative. I look forward to seeing some real stuff written like this.
How do you read indented code with more than 10 lines ? It's really hard to jump up and down just to see the scope of particular line at the bottom of the big block in multi-layered code. In fact it is so hard to read that in Delphi (pascal like language and IDE) i am using an IDE plugin that draws lines between indented blocks so it is easier for me to see the scope. And this is in language that does not require indenting. In Lisp this is solved (again) by IDE (emacs) that highlights matching parens. I guess something like that can be done for indented code as well. Maybe highlight the first-last line of the block you are standing in. 
Another person that has not read Typeclassopedia.
You could do this for if..then..else: {-# LANGUAGE RebindableSyntax, MultiParamTypeClasses, FlexibleInstances #-} import System.Environment import System.Directory class IfThenElse a b where ifThenElse :: a -&gt; b -&gt; b -&gt; b instance IfThenElse Bool a where ifThenElse True t e = t ifThenElse False t e = e instance Monad m =&gt; IfThenElse (m Bool) (m a) where ifThenElse m t e = do b &lt;- m if b then t else e main = do [d] &lt;- getArgs if doesDirectoryExist d then putStrLn $ if True then "ok" else "nope" else putStrLn "Moooep. Wrong" But yeah, it's a pretty new extension and you talked about it already. :)
[Here](http://community.haskell.org/~yitz/typeclassopedia.pdf) is the paper, for all who have not read it.
I recently had to use ANTLR and completely hated it. Just a side remark.
If you can't tell the difference between the levels, then increase your tabsize. I hardly ever have this problem though, so maybe it wouldn't help.
Interesting. That seems like a bug in `Data.Char`, then; shouldn't we have toUpper :: Char -&gt; [Char]
I don't see the point. The author rehashes an example that appears in every Haskell tutorial. Then, he mentions `Functor` without saying anything about it or relating it to the previous topic. Seems like a reasonable post for a personal blog, but why bring it to our attention?
That's not what I said. But take some function, e.g., sin. It doesn't vary; every time you call it it will return the sin of the argument. 
I don't know what there is to hate about ANTLR. I think it's fantastic. Far better than javacc or yacc.
&gt;I doubt many programmers would be happy with a system that normalized all the whitespace in their programs upon checkin These days, I can't live without my IDE normalizing whitespace immediately upon save. Along with inserting brackets and parentheses where necessary. I just type a whole bunch of crap on one line, hit ctrl-s and bingo, everything goes where it should. I'll type something like if(x&gt;0)return a;else if(b==0)for(int i =0;i&lt;10;i++)b+=i; And then ctrl-s it becomes: if(x &gt; 0) { return a; } else if(b == 0) { for(int i = 0;i &lt; 10;i++) { b += i; } } I can't but think programmers could get used to the computer rewriting their code for them and think of it as a great service. 
This is where functional programming fanatics deceive themselves big time. The variables are on the argument and return stack. The stack is the variable. Even if you believe you are passing new unchanging constants every time you call a function and getting constants in return, all you're doing is changing the argument stack. The variables are there. You just ignore them and choose to live in a fantasy world.
Usually, you would actually define dotp as dotp :: Acc (Vector Float) -&gt; Acc (Vector Float) -&gt; Acc Vector Float) dotp xs ys = fold (+) 0 (zipWith (*) xs ys) and leave it to the caller to apply 'use' where necessary. That definition is more compositional as you will often want to use the dot product on results of other Accelerate computations as well. In the paper, we needed to introduce the 'use' function and that's why we chose that more verbose (and actually somewhat awkward) definition.
Oh i can tell the difference between levels. I just can't tell which level exactly it is when it's header is off the screen. 
&gt;&gt;Lisp syntax solves very few of these points &gt;Which points are not solved? Well, I didn't understand "Lisp syntax" as talking to the *lexer*. To me, "Lisp syntax" means writing everything in prefix notation, even the special forms. If you just want to put - and : into the same lexical category as letters, that's not really a big deal (IIRC Agda is quite permissive about what characters can appear in identifiers [edit: oh, you even mention this yourself], and surely its syntax is much more Haskell than Lisp). So to me writing ":true" instead of "True" has nothing to do with Lisp syntax (I also don't understand how ":true" can be easier to type than "True", but to each his own). And as I mentioned, saying that Lisp solves the problem of not knowing about relative operator precedence is a bit silly when you can just write your Haskell expressions in exactly the Lisp notation (with a few extra parentheses around operators made prefix), or just write parenthesized infix expressions. I will grant that it is a reasonable position to dislike layout and prefer explicit grouping; I guess the braces style supposedly addresses it, but I never write code in that style, so I don't know what it's like in practice. &gt;&gt;the proposed syntax of multiargument functions is just bizarre. &gt;Why? It's irregular; the semantics has a weird shift of behavior at n=2 arguments. If you have (++ foo bar baz) or whatever, and you end up deleting all but one of the arguments, suddenly you have a type error. It's not what you expect from Lisp. Also, I guess it must only apply to, uh, "infix operators"? How do you decide what those are, with the usual Haskell punctuation-vs.-letter rule? How does that make sense when you've just made your constructors start with colons?
Can I just point out that you said you disliked the whole capitals thing and yet your post here contains 78 capital letters that you didn't have to think about when typing? Can I point out that you used extremely well formatted written English, nary a missed capital, punctuation mark, etc.? You obviously have no problem dealing intuitively with arbitrary orthographic conventions in English. Perhaps the trouble you have with Haskell is that you've not given Haskell the same care and dedication that you've given Lisp or English. Before decrying a language for aspects that you just don't like on aesthetic grounds or supposed "usability" grounds, you should try to become fluent in it in the way that passionate users of the language are fluent.
&gt;In mine, I can write (if cond this that) or (if this that). I'd think this overloading would get annoying as soon as you have an if form with long arguments, and you have to read all the way to the end of the form to know whether the first argument is the condition to the if or one of the branches. Surely it's better to spend one extra character and call the second form if', or distinguish it in some other way.
&gt;I spend a lot of time hitting tab to go through the tab cycle until the editor finally gets to the one I meant. Sometimes there can be five or more possible indentations. It’s often easier to go there myself, which sucks. Have you tried the kuribas indentation mode? It's enabled (confusingly) by `turn-on-haskell-indentation`. I find it much preferable to the tab cycle system: in the kuribas system, you press tab to move to the next tab stop to the right, and backspace to move to the one to the left. (Or maybe you're using a non-emacs editor; I don't know how Haskell indentation works in other editors...)
Special forms are half of what's wrong with Lisp. A function should be a function; that Haskell gets this right is one of the many things putting it far ahead of any mainstream language, and most of the other languages too.
He mentions that it's cumbersome to write: a -&gt; b -&gt; c -&gt; d and that he prefers: (-&gt; a b c d) This, however, is very artificial. Most code doesn't look like that. Compare: F x y -&gt; G A -&gt; H (B x) y with: (-&gt; (F x y) (G A) (H (B x) y)) No so pretty now, is it? Guess which kind of type signature is more common in real-world code?
&gt; translation from code to AST and back again Have a look at [gf](http://www.grammaticalframework.org/) and just have a shot at writing a sufficiently sensible resource grammar for functionalish languages.
If you can't tell the difference between the levels, then use less levels. Top level, unnested let/where and an additional one for multi-line expressions is all you need, that's two (2) levels of indentation. If that gets confusing, use more top level definitions.
&gt; I can define 1- as a function and why would you do that? because it looks like the correct, infix form instead of the ugly, prefix one? call/cc vs. call_cc isn't really worth arguing about, is it? the ℕ thing is not necessary, you can't input it from the keyboard key anyway, you need some mechanism to do that. why not input the word 'nat' and make your editor display it as ℕ? variadic operators are very rare, for every case when you can (+ x y z), you get many more cases where you have to read (+ x y). the only really used examples would be that type thing (-&gt; a b c d), which I think looks terrible. I think the only nice example is cons, which is the only thing that looks good in lisp: (list a b c d) and it's not very surprising. lisp people usually argue for parens because of that whole code-is-data deal, which would work the same way with haskell syntax - you'd have application (f x) and binary operators (x op y). sure, there is also all the other stuff like case expressions, but they exist in lisp too, and if you do care about them in your macros, it gets wors to manipulate them as nested lists, where you have to remember which sublist means what.
Cute!
Indeed, this new version of dotp more consistent. But on the joint between Accelerate computations and ordinary Haskell, one will have to use use for each argument anyway. They will just be on the caller side. Lifting functions would allow cleaner interaction. dotProduct :: Vector Float -&gt; Vector Float -&gt; Acc (Vector Float) dotProduct = liftAcc2 dotp Actually, it is just a matter of taste.
&gt; And as I mentioned, saying that Lisp solves the problem of not knowing about relative operator precedence is a bit silly when you can just write your Haskell expressions in exactly the Lisp notation **(with a few extra parentheses around operators made prefix)** Yes, let's try it: (+ 3 (* k n) (/ 5 (+ l (^ p 2)))) ((+) 3 ((*) k n) ((/) 5 ((+) l ((^) p 2)))) I don't find it readable or reasonable to type. Maybe you do. &gt; It's irregular; the semantics has a weird shift of behavior at n=2 arguments. If you have (++ foo bar baz) or whatever, and you end up deleting all but one of the arguments, suddenly you have a type error. It's not what you expect from Lisp. I don't think it's what a Lisper or a Haskeller would expect. A Schemer coming across Common Lisp would not expect this: * (let ((f (lambda (x) (+ x 1)))) (f 1)) The function COMMON-LISP-USER::F is undefined. No one expects the Haskell semantics! Amongst our weaponry are such diverse elements as laziness, partial application, ruthless static type checking, an almost fanatical devotion to referential transparency, and monads--oh damn! Lisk is a Lisp with these elements, your hypothetical "you" won't expect those either. Maybe you can call it a Lisp-3 if that helps? &gt; Also, I guess it must only apply to, uh, "infix operators"? How do you decide what those are, with the usual Haskell punctuation-vs.-letter rule? How does that make sense when you've just made your constructors start with colons? Yes, as it says in the article... Haskell operators are clearly defined. No problem is presented with using colon for constructors. Again, the space is important. `(:abc)` is different to `(: abc)`.
I think I made it clear enough that I've been using it for nearly three years to indicate that I've given Haskell dedication and care. I've released enough packages on Hackage and use it on full size projects at my workplace, I'm not some newbie who came to Haskell and had a bad reaction to the syntax. english has rules and as arbitrary and stupid as they are i follow them so that people can understand me and take my points seriously if i dont then people will find it hard to follow and or think im an illiterate idiot from yahoo questions its hard for me to invent a new grammar and vocabulary and get people to understand it as much as id like to whereas its quite easy to take a familiar programming language syntax and write a parser for it. Also, 78 out of 2021 characters is 3%. This is English. We write uppercase at the start of sentences and sometimes for names and acronyms. Something more analogous is: It'sSortOf anIdealSituationFor thisToBeTheCase. ICertainlyDo notWantA segregatedCommunityDue toSyntaxDifferences. InFactIt'sAn opportunityTo makeTheCommunityBigger byAttractingMore peopleIfThere is more choiceOfSyntax. YouKnowThis, butIt'sWorth mentioningThatAlthough the .NETAndJVMRuntimesProvideHalfA solution byAbstractingAwayFromEven the AST,WhenYou want toHackOnThe actual codeYouCan't unlessYouKnow theLanguage. I thinkWeCan doItProvidedThat there is aProperBijection. ThereAreSome issuesThough:Things likeBackticksForMakingFunctions infix inHaskellHasNo equivalent inLisk,So thatMeansItIs notA proper bijection. CommentsMightBeAnother issue, oneWouldNeed toMakeSureThatTheConverter keeps trackOfComment positions andInsertsThemAt appropriate places. IDon'tThink speedIsA concern at all.MostlyItSeemsLike a socialIssue. If weStoredThe ASTItself in SCMs, insteadOfCode,ThenMaybeThatCould work.However,Until that'sThe case, forNow, weCanAt least, forExample, takeAHaskell projectWrittenIn vanillaHaskell,Open itUpInOur editorAndHit "convertToLisk", hackHackHack, ThenWhenDone, "convert toHaskell",Push thePatch.There willDefinitelyBe differencesBetweenWhat thePrettyPrinter outputtedAndWhat someoneWroteOriginally, mostlyToDo withSpacing,But it'sProbablyNotA big issue. RightNowI am happy withABasically one-to-oneCorrespondence.ThereWillBe moreTechnicalIssues onceLiskHasA richer syntaxThatExtendsBeyond whatHaskellHas. For example,IfI add syntaxForMap literals, thenInHaskellThat would beConvertedTo aFromListCall. When IReadThat backFrom Lisk, I wouldHaveToBe clever andRealiseThat IShouldTranslate thisIntoA map literalWhenPretty printing.ThereAre manyOtherIssues onceYouStart thinkingOfNew syntaxForLisk, butIThink youGetTheIdea. I won'tBotherMentioning theDifficultiesOf dealingWithMacros. I also don't type English all day. If I did, I'd wish I could change it more. I wouldn't be able to, like I can do with a programming language.
Haskell doesn't get it right, though, does it? if ... then ... else ... But yes, lazy evaluation removes the need for lots of things that are macros or wrapped in thunks in other languages.
&gt; call/cc vs. call_cc isn't really worth arguing about, is it? Well, in this case, yes. `/` means 'with'. It means more than `-` or `_`. Read any Lisp code, if you see `/`, you read it as "with". &gt; the ℕ thing is not necessary, you can't input it from the keyboard key anyway Who can't? Many Haskellers complain about this. You can do this in Agda just fine. &gt; variadic operators are very rare Any evidence for this? &gt; for every case when you can (+ x y z), you get many more cases where you have to read (+ x y) (. a b c d) (&gt;&gt;= a b c d) (&gt;&gt; a b c d) (&gt;&gt;&gt; a b c d) (-&gt; a b c) (, a b c) (&lt;|&gt; a b c d) (++ a b c d) (&amp;&amp; a b c d) Do you genuinely believe that these regularly occur with only two arguments?
Just a minor point, but I don't think you really see lots of Haskell programmers complaining about not being able to use ℕ in their code. Rather, instead, you see lots of them thinking it be awesome if they could. But back to reality, the instant they are suddenly allowed to, it will become a style guideline to never do it except when showing off to your friends that you can. In reality, it's nice if people can actually write code without leaving a character map or unicode table open in another window. In fact, that's way nicer than being able to write code in blackboard bold.
&gt;Speaking of which, in Lisp I can define ℕ as a variable. ℕ is a valid identifier in Haskell, too.
&gt; Just a minor point, but I don't think you really see lots of Haskell programmers complaining about not being able to use ℕ in their code Especially because the people who really want to do this tend to actually try it instead of complaining and then discover that it already works!
I don't think my example is any more artificial than yours is. I'm basing this off of my own development experience. Why would I make up trivial examples if I didn't think they'd actually help me? In my own code I know that I use concrete types all the time. Maybe is really the only type function that appears in my code a lot. We can judge from Hoogle's database, though: * [Hoogle with vanilla Haskell notation](http://hpaste.org/raw/41852/hoogle_vanilla2) * [Hoogle with Lisk annotation*](http://hpaste.org/raw/41853/hoogle_lisk2) They are clearly the same in length, looks like because I've added `:` and `-` to the var names. I think the sexpr one is easier to edit, whether or not it's more beautiful. * If some things look broken it's because my sed skills suck: `hoogle 'a' | grep '\-&gt;' | sed -r 's/^.+ :: //' | sed -r 's/(.*)/(-&gt; (\1))/' | sed -r 's/(.)([A-Z])/\1:\2/g' | sed -r 's/([a-zA-Z]):/\1-/g' | sed 's/\.:/./g' | tr '[:upper:]' '[:lower:]' | sed 's/ -&gt; /) (/g' | sed -r 's/\(([^) ]+)\)/\1/g' &gt; hoogle-lisk.txt` I didn't fix the ones with class constraints, can't be bothered coming up with more regex.
As oft noted, it's trivial to remove `if_then_else_` thanks to laziness. There are only two exceptions I can think of to Haskell's avoidance of special forms: (1) the fact that patterns (and consequently individual branches of a case expression) aren't first-class. There are approaches to making first class patterns, but case analysis is pretty well baked into the language. And (2) record updates, but then Haskell's record system is generally considered a wart overall.
It looks like there in fact far more concrete types than there are type variables. I suspected this but now it's confirmed. A coworker suggested that I could swap it, so that all types are assumed concrete and variables be annotated by a prime or something. Interesting thought: [AddrInfoFlag] -&gt; Family -&gt; SocketType -&gt; ProtocolNumber -&gt; SockAddr -&gt; Maybe String -&gt; AddrInfo becomes (-&gt; [addr-info-flag] family socket-type protocol-number sock-addr (maybe string) addr-info) whereas for type variables: GenTokenParser s u m -&gt; ParsecT s u m a -&gt; ParsecT s u m a becomes (-&gt; (gen-token-parser 's 'u 'm) (parsec-t 's 'u 'm 'a) (parsec-t 's 'u 'm 'a)) I dunno, is that too noisy? (-&gt; (gen-token-parser :s :u :m) (parsec-t :s :u :m :a) (parsec-t :s :u :m :a)) Hmm. Eh, I'll think about it.
 λ&gt; let ℕ = "Nat" in ℕ &lt;interactive&gt;:1:4: Not in scope: data constructor `ℕ' &lt;interactive&gt;:1:17: Not in scope: data constructor `ℕ'
Yeah, I hope to be able to solve the record problem in a nice way. I'll probably take the great work done by the Hugs guys and implement TRex, or something similar (the Has and HList libraries are essentially the same as TRex, so I would just provide a syntactic sugar for them). We really need extensible records that can both have accessors *and* be accessed via pattern matching... which is impossible in any of the systems right that, AFAIK.
Please don't call me ignorant when it was your awareness that was at fault.
[Grothendieck group](http://en.wikipedia.org/wiki/Grothendieck_group)
That's because 'ℕ' is considered an uppercase character (isUpper 'ℕ' = True). It's still a valid identifier.
No. I know it's an uppercase letter, that's kind of the point. I said variable. Not constructor. **Edit:** Sorry, I didn't mean to be rude. I've had enough for reddit for now. People treat you like you're ignorant because they disagree with you. Back to being productive.
I'd recommend posting it to haskell-café if you want a bit more feedback than you'll get here.
Well, fortunately Chris Done is the exception and just about everybody else hates the Lisp syntax and loves the Haskell syntax\*. And for good reasons, because it is the Right Thing (tm). Yes, of course Lisp is very elegant. Also very unreadable. And yes, of course the Haskell syntax is far from perfect. It's just better than anything else we have at the moment. Learning some mathematics will teach you to love infix operators. Converting between prefix and infix notation in Haskell is in my opinion a genius idea. The finite predefined set of precedence levels in Haskell is indeed an issue, but in practice the only precedence problem I regularly bump into is `&lt;$&gt;` vs. `$`. Indentation-based syntax is again a genius, and much more important, idea (tab *characters* though is the single worst idea in the history of computers, if you let me be explicit). Also, as others already noted, the forward looking thought is to have different representations of the same underlying AST, which solves many problems (but also introduces some: communication will be harder). (\* or at least I really really hope this is true)
Thanks for the link, I didn't know the construction had a name!
Personally, I don't quite understand how hitting the shift key for data constructors is a bore while all those extra shift 9s and 0s are not. In fact, come to think of it, the author's preferred :just requires as many invocations of the dreaded shift as Just. But, there really is no accounting for taste. Much as I appreciate s-expressions to serialise data, when that data is actually code, I don't find it pleasant to read.
Postfix notation doesn't have this problem. 
&gt; Do you genuinely believe that these regularly occur with only two arguments? yes. if it was uncomfortable for people, someone would come up with variadic version (parsec has choice for (&lt;|&gt;), and people don't do that. anyway both (+ 2 3) and (+ 1 2 3) look absolutely unreadable, there's too much infix stuff in math and cs to look at that with immediate understadning. someone once suggested (here on reddit) that if + was called sum, people wouldn't hate lisp, because (sum 2 3) doesn't look so bad. but if I was living in such a world, I'd want a special case for situations with exactly two arguments. &gt; Who can't? Many Haskellers complain about this. You can do this in Agda just fine. you can't input it from your keyboard (at least there's no 'ℕ' key on mine), programming language doesn't have anything to do with it. anyway, wtf is wrong with your accounts? why do you keep deleting them?
Thanks! I'll give that a try.
Agree with all of that. One thing I always wish is that writers of example code wouldn't use variables like A, X, or V. Choose good names for your variables in your example code just as you would in production code.
Personally I am upset by this writing: Suppose I want to search a list of data Player = Player { playerNumber :: Int, playerName :: String} The better way I found is: f name = find (\Player {playerName = pn} -&gt; pn == name) I really dislike this, I'm feeling I'm writing four time the same thing on the right hand side. You got a better idea? Corentin
&gt; Do you genuinely believe that these regularly occur with only two arguments? For most of those, absolutely! The only operators I've used or seen used regularly in chains are (-&gt;) and the true monoids/categories: (.), (&lt;=&lt;), (&lt;|&gt;), (++),...; and a number of the true monoids are still only seen in binary usage the vast majority of the time: (+), (*), (&amp;&amp;), (||),... Chaining seems to be an explicitly categorical thing which only includes certain of the monoids considered as categories (typically only monoids that do not participate in a larger structure like semirings, rings, fields, etc). People typically don't use (&gt;&gt;=) nor (&gt;&gt;) as a chain because it's far nicer to freely switch back and forth between them via do-notation. Otherwise you end up needing to be explicit about all those lambdas associated with `_ &lt;-`. And since (&gt;&gt;=) is syntactically non-associative, that's further reason to avoid variadic notation for it--- if you wanted variadic notation you should be using (&gt;=&gt;)/(&lt;=&lt;) anyways. As for (,) I'm firmly of the opinion that n-tuples for n&gt;2 should only be used as throwaway constructs. If you need to regularly bundle more than two things together then you need to give the bundle and its projections some names, a la records. Not to mention that (,) is non-associative thanks to bottoms. So the standard isomorphisms `(a,(b,c)) ≅ (a,b,c) ≅ ((a,b),c)` fail, which gives further reason to avoid fancy syntax that obfuscates the differences.
&gt; I also don't understand how ":true" can be easier to type than "True", but to each his own Let's see: shift-; t r u e vs shift-t r u e Hmm...
&gt; I don't find it readable or reasonable to type. How is it any different from Lisp? You have two extra characters per function name, but Lispers don't count parens as real characters anyways. *Neither* of those options is readable. Just say what you mean: `3 + k*n + 5 / (1 + p^2)`
You mean besides [The Haskell Platform](http://hackage.haskell.org/platform/) ?
New hackage will eventually have lots of awesome features to help folks sort though all the libraries by various metrics. In the meantime, you can look at the reverse dependencies hackage (http://bifunctor.homelinux.net/~roel/hackage/packages/hackage.html) and that'll tell you whether one or another, e.g., xml library, is more commonly used. Dons has also compiled some very useful stats about packaged popularity: http://donsbot.wordpress.com/2010/06/30/popular-haskell-packages-q2-2010-report/ If there's a particular question on a recommendation that you have, it's also worthwhile to either email -cafe, ask on irc, or just post a question right here to haskit.
&gt; what you suggestions would be? What are you trying to do?
Just thought I'd share.
f name = find ((==name) . playerName)
In my opinion, these are some of the most well-written libraries: bytestring, text, binary, vector.
Nice! In the first semantics, you may want to replace g(x) by g(v).
Good catch, thank you. :)
What is `(~&gt;)`? It is used extensively, but not defined, in `Data.Thrist`, nor is it defined in any of its imports.
I am constantly amazed how smart people who get Haskell find Lisp unreadable. For me, several years ago the "lots of parens problem" disappeared after 2 hours of toying in REPL. Since then, I instantly see even far away from display which functions are recursive, which do a lot of stateful juggling, just by looking at one (unamiguous!) shape of automatically indented code. Now, looking at Haskell or Perl, or even Java hurts my eyes. I can't say where the expression ends, how it nests. In case of Haskell, often looks like ugly spagetti of characters. I believe Haskell is very nice and powerful language, but why do I need to cognitively employ this function preference table again and do a reasoning what expression means by the indentation? The "lots of parens problem" is really, really superficial. And please don't bring up the argument about "math is written that way in books for centuries", that's quite dumb.
It is a syntactic trick to let a type variable (of kind * -&gt; * -&gt; *) appear between two other type variables. These are equivalent: m a b === a `m` b (~&gt;) a b === a ~&gt; b As you see for alphabetic variables the prefix notation is more terse, while for operator variables the infix notation is more succinct (and natural).
&gt; And if anyone else has any more ideas on usefull instances, please let me know!
Oh I see, it's just a type variable. The TypeOperators extension allows you to use it instead of things like `a` and `b`. Got it.
Interesting... I personally would prefer a caseM over "if" for handling an IO (Maybe a) value...
what do you mean by parsec/Text? The way I understood [this blog post](http://www.serpentine.com/blog/2010/03/03/whats-in-a-parser-attoparsec-rewired-2/), using parsec would be rather slow... do we need something like attoparsec for text instead of bytestrings?
I wonder if allowing full rebinding of the if..then..else construct is the right level of abstraction. As shown, it is possible for one to make instances of IfThenElse that defy common sense, or intuition. In Smalltalk, which faced this problem, rather than allowing overriding of the main conditional constructs (ifTrue:, ifFalse:, ifTrue:ifFalse:, and ifFalse:ifTrue:), those constructs compile into something which calls mustBeBoolean: on the condition. That is allowed to be polymorphic. This has two benefits: 1) Most desires to apply a conditional to some new type involve just deciding what is true vs. what is false. 2) The user doesn't have to override a family of four methods. In Haskell's case, taking this approach would remove the ability to have different types for the then and else branches, as you do have in some of your examples. On the other hand, like @Peaker, I think I prefer a cond-like expression in those examples.
Video (Select 720p HD) at : http://www.youtube.com/watch?v=weS6zys3U8k 
Video (Select 720p HD) at : http://www.youtube.com/watch?v=weS6zys3U8k
Having a rebindable case..of would be pretty cool indeed. But I don't think it's possible with current GHC. It would make up for a cool feature request though. :) Limiting if expressions to something like &gt; ifThenElse :: IsTestable t a =&gt; t -&gt; a -&gt; a -&gt; a might be reasonable, and I highly agree that Maybe might not be the correct type for an if expression. My blog post is more about showing what can be done with RebindableSyntax. I'm currently thinking about more modules which could override Monad instances etc. to avoid boilerplate like &gt; foo :: ReaderT IO (); foo = liftIO $ putStrLn "bar" instead of just &gt; foo :: ReaderT IO (); foo = putStrLn "bar" But that's just an idea for now... I'm happy about any input on this topic. :) Update: I changed the class a bit and updated the example on the [haddock page](http://n-sch.de/hdocs/rebindable-if/).
I meant parsec with a Text backend. Sure. Parsec would be rather slow, probably, compared to a parser specialized only to an underlying array-addressable type (like attoparsec is). So no doubt an attoparsec type library that parsed Text would be a good thing. But my broader point was that Text.JSON has pluggable parser backends, since it represents everything as an intermediate JSON type. So you could really use whatever parsing technology you wanted, and it wouldn't require changing anything existing in Text.JSON at all.
1. What parser generator reqires you to go over your grammar twice? I am talking about the AST walker. You basically have to copy all your grammar rules. Every other parser generator I know of doesn't require you to do that. It sucks big time in my opinion. 2. ANTLRWorks is a buggy mess.
hoogle seems broken at the moment, is that related?
I think haddock does some odd things with custom type operators which adds to confusion.
hoogle is being moved to the new machine. 
I am just new to Haskell. It isn't that I am trying to do anything at the moment, except get a good concept of what is good in general so when a use case comes up I can say "Ah! The package I want to use is...". Also it helps in that I can go look at how those "good" modules are coded to get a good feel for proper Haskell. I am just starting the "Learn You..." Haskell tutorial and the question popped into my head
Yes, definitely speak to a lawyer if you can. I was once working for a company that had its name taken away by a much larger company. Our lawyers told us that the law was clearly on our side and we would definitely win - after being forced to spend enough money on legal expenses to put us out of business. Or, we could just take their offer to cover getting a new domain name and printing new stationery. We took the offer.
I am a bit surprised no one mentioned (the prior art) [Liskell](http://www.liskell.org) before. It seems the code lives at http://code.haskell.org/liskell/ nowadays.
costate comonad
state-in-context comonad
selector comonad
aculector comonad
oxylector comonad
elector comonad
eclector comonad
optilector comonad
pointed function comonad
context comonad
position comonad
Pro: Technically correct and precise. Con: doesn't convey much intuition about what the structure is. 
Con: Confusing; has little to do with state.
se- means "apart" -lector means "to gather" or "to collect"
acu- means "pointed" or "sharp" -lector means "to gather" or "to collect"
oxy- means "pointed" or "sharp" -lector means "to gather" or "to collect" 
e- / ex- / ef- means "from" or "out of" -lector means "to gather" or "to collect"
ec- means "out" -lector means "to gather" or "to collect" Cons: eclectic has a sense of multiple choices that isn't present in a comonad structure.
opt- means "choose" -lector means "to gather" or "to collect" Pro: opt- also means "eye", and the costate comand happens to be the codomain of a lens (aka functional reference).
Con: pointed function typically means a homomorphism between pointed sets.
Con: Vague. Every comonad is a value in some sort of context.
Con: Vague. Almost every comonad is a position in some sort of context.
I've kvetched about that for hours and hours on the chat room. I'd trade do-syntax for comprehension syntax any time -- it's just a more expressive and more natural way to express monadic operations, as the list special case shows off clearly. I understand that there's a whole process and probably GHC will have them as a nonstandard extension before Haskell Prime II rolls around and so on. But that's the beauty of having a reference implementation; while Lisps (and Schemes, RxRS notwithstanding) have splintered enough that Allegro and LispWorks actually have a business model being very, very expensive reference implementations. 
branching state comonad
The Haskell implementation is in the ZIP file linked from here: http://www.ucombinator.org/projects/parsing/ It isn't what I would call idiomatic Haskell, with the requirement for linking references together to create (mutually) recursive parsers. The idea of extending the concept of derivatives to context-free grammars sounds like an interesting enough topic, but I don't see any further work on the topic on the author's home-page, and it looks like the paper didn't make it past review. I hope it's an interesting read.
My motivation for this is simple: one can still get and set states. Just like monads, comonads are best thought of not only with their fundamental operations, but also with the additional operations available above and beyond those. Otherwise, for the most part, they're variations of Identity. Costate allows one to branch state, via `duplicate`. `extend` lets you fold an operation which potentially alters the state before extracting a value back into an operation in the original state. 
It was rejected http://twitter.com/mattmight/status/28895672923 On the whole it feels very much like an earley parser. It is top down filtered It builds the tree bottom up from the left corner of the tree. It builds a parse forest It builds the state sets in a lazy manner Oh and it doesn't build a push down parser, it is pretty easy to implement and it is reasonably fast, although there are various ways to make it go faster (like linear time on lr-regular grammars, and general constant lowering) It is interesting enough as an approach but I didn't see any work on time bounds, nor efficiency on certain types of grammars. It would be nice to see a comparison with the work pypy did on a glushkov automata for matching.
pointed environment
I'm not sure how helpful this will be, but what you really need is a non-language restricted project to work on. You will have to come up with something fairly simple that would actually be useful - so that you have the motivation to work on it. The first serious Haskell project I worked on was an IRC bot that monitored log files and spat out changes to a channel.
Doing something will a well documented spec would be the best. This guy did a [bit torrent client](http://jlouisramblings.blogspot.com/2010/04/haskell-vs-erlang-for-bittorent-clients.html). You could do an IM client using the Jabber protocol or a phone using the SIP protocol.
Try the [/r/haskell_proposals](http://reddit.com/r/haskell_proposals) subreddit!
pointed context comonad
I liked this. But then changed my mind because it doesn't capture that you can change what is being selected from. This implies that the comonad carries around a selector, but really, it carries around what is being selected *from* -- the selector is the monadic action itself.
You can always do a lot of Yak shaving and solve your problem in Haskell. Need javascript? How about getting the [LLVM-javascript compiler](http://code.google.com/p/emscripten/) to compile your haskell down to javascript. There is also a recent project to compile from [GHC to javascript](https://github.com/sviperll/ghcjs).
help out others on stackoverflow
I'm looking forward to this feature because it simplifies code that uses non-determinism monads. Hence, I'm glad that someone is working on this. I think there is a small mistake in the translation rules: [ ... | pat &lt;- rhs ] =&gt; rhs &gt;&gt;= \pat -&gt; is inconsistent with how patterns are currently handled in list comprehensions and do-notation. With the above rule, the list comprehension [ () | True &lt;- [False] ] would yield a pattern match failure whereas currently it yields the empty list, also if written with do-notation like this: do True &lt;- [False]; return () I think the rule should be similar to the rule for translating patterns in do-notation and use the fail function. Alternatively (and I prefer this), monad comprehensions could be translated into do-notation instead of calling &gt;&gt;= directly to ensure that possible later changes to the translation of do-notation automatically affect monad comprehensions.
Well one tactic would be to consider what it would be called categorically. The second component of the pair, the `b-&gt;a`, is an object of the slice category over `b` (aka the comma category `b`↓**Hask**, aka C\_`b` ↓ id\_**Hask**), which is also **Alg**(C\_`b`)--- where C\_`b` is the constant functor to `b`. And then the first component of the pair is there because it's pointed. So if you're theory-minded I might suggest **pAlg**(`b`) for the category of pointed C\_`b`-algebras. Though that wouldn't help any non-theorists on the suggestiveness front.
In agda, ℕ is inputted by \nat, emacs/vim rewrite it for you in agda mode.
Mentioned in the article.
An alternative would be to consider what it would be called set-theoretically. Each value of `Costate b a` denotes a pointed set of the image some function, where `extract` projects the point, and `duplicate` takes a pointed image of `f` into the pointed set of `f`-image pointed sets (pointing out the original `f`-image pointed set) essentially erasing or unspecifying the point of the original image set. So "pointed-image comonad" captures the semantics of how you're picturing it.
How about writing a web testing framework? That's something that would be very useful to the community.
Rebuild something you already undertand, or want to understand. Write 'wc'; make it fast.
The pypy work was inspired by a Haskell program for regular expression matching which (unlike the Python version) can handle context free grammars via infinite regular expressions. There are [a paper, slides, a project page](http://www-ps.informatik.uni-kiel.de/~sebf/pub/regexp-play.html) and a [Hackage package](http://hackage.haskell.org/package/weighted-regexp). I agree that a comparison would be interesting, also with [total parser combinators](http://www.cs.nott.ac.uk/~nad/publications/danielsson-parser-combinators.html).
It seems that the primary use of the costate monad is to make a function invertible, allowing us to pass around an `a = f b` value with an annotation that lets us get the `b` back from the `a`. So `extract` removes the annotation and yields the `a`, whereas (the poorly named) `get` does the inversion/undoing and yields `b`. So, perhaps the "(single) undo comonad". Which also suggests the idea of a multiple undo comonad, aka history or trace comonad.
Speaking of which, what is the latest news with the new Hackage?
You can try solving some [Project Euler](http://projecteuler.net/) problems, but that may not be the best way to really learn how to use the language, and what it's good for.
I've had classes that take this approach. Very cool idea. I like it. Thanks!
&gt; The intended audience include students for whom the basic curriculum is not feeding their hunger to learn Very cool. Thanks for the link, I'll keep it handy.
Very interesting idea. I like it. Thanks!
That's awesome, I had no idea those tools existed. Thanks for the great ideas.
That's really great. Along with Project Euler, this seems to be exactly the kind of thing I was looking for. Thanks!
I like your idea of writing an IRC bot in Haskell. You definitely make a good point -- there are problems I come across every day that I'd like to solve. I guess I just need to pay more attention to them and start thinking, "Why don't I solve this in Haskell?" Good stuff. Thanks alot!
Are you sure your example is correct? For me, both versions return the empty list, the do-expression and the monad/list comprehension, regardless whether or not MonadComprehensions is turned on. As far as I can tell, this is the way "fail" is defined for lists. And my translation rule should be correct, as the "fail" rule is handled separatly. And reusing do-notation is indeed a tempting idea, and early versions of this extension actually did that when I didn't have my own desugarer for monad comprehensions yet. But it requires modifying the AST in the typechecker, and obviously this isn't the typecheckers job. And since we already need different rules for monad comprehensions in the renamer (which is basicly looking up function names, especially "&gt;&gt;", "&gt;&gt;=", "fail" and only for monad comprehensions "guard") and the typechecker it would require an additional intermediate step between typechecking and desugaring. This however, would be way more complicated and more likely to break in the future than just using a new monad comprehension desugarer. Of course I try to reuse (aka "copy &amp; paste") as much code as possible, but there are limitations. :)
I don't think that names derived from obscure Greek morphemes will help intuition much. A name can be correct and reflect the nature of this comonad, but if we need a dictionary to grok what it means, it is bad. Can you post more code using "Costate"? It is easier to find patterns when there is more material. Carrier comonad?
I think you have just described functors not monads specifically (although monads are functors too so it's right in that sense). I'm still learning myself so take what I say with a grain or two of salt. Monads I kinda get: they are sets of rules for composing functions. The monad threads the functions through the control structure it implements. The monadic functions (the lines in your "do" statement) can communicate with that control structure through monadic values. That's my best three line explanation right now. Can any more experienced haskellers verify?
I fear that what you get are Functors, not Monads yet. The "create a copy of itself that has the same structure as the original, but each element in the copy is the result of a mapping function applied to the corresponding element of the original container" is exactly what Functors provide. See, e.g.: http://stackoverflow.com/questions/2030863/in-functional-programming-what-is-a-functor Every Monad is a Functor, but not every Functor is a Monad. 
According to your blog post my example translates into [False] &gt;&gt;= \True -&gt; return () which does not yield the empty list but fails with a pattern match error. Apparently, the implementation does the right thing and only your description in the blog is incomplete.
Btw, if your Python oriented mind knows Twisted, you might have already encountered monads in the shape of Twisted's Deferreds
As others have said, what you describe is Functors. For a functor to be monad you also need a function called join and function called return. join :: m (m a) -&gt; m a return :: a -&gt; m a Where return puts a value into a monad in the simples way possble, for example return x == [x], return x == Just x and so on. Join, as you can see from the type, is a way to "flatten data". For lists it's the concat function, for Maybe we have join $ Just (Just x) == Just x join $ Just Nothing == Nothing join Nothing == Nothing. For IO we have join io = do io2 &lt;- io io2 The join function has to satisfy these laws, for the functor to be a monad: join $ return a == a join $ fmap return m == m join $ join m == join $ fmap join m The rules all makes sense if you think about them for a while. However, the way people usually define monads is not via the join function but using the bind operator (&gt;&gt;=) which can be defined as (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b m &gt;&gt;= f = join $ fmap f m You can also express join using (&gt;&gt;=). join m = m &gt;&gt;= id You can think of the bind operator as a generalized way of sequencing functions. I think the join function is easier to understand than the bind operator when you are just getting started though. I hope this helps and that I didn't make any silly mistakes since I don't have a Haskell system at hands and can't double check the code. :)
As others said, you described Functors. There are as many explanations of a Monad as there are Haskellers, but in my mind they are simply an interface, or (excuse the dirty word) a design pattern, for how to represent computation and how they are combined. A computation is represented by some "action", that when executed will return some value. A monad provides you with a general interface for constructing differend *kinds* of computations. All you have to do is specify how the result of one computation can be used as an input to another (i.e. how they are combined) and how to return a final value. In the case of IO, the combination is a simple causal depency, i.e. for a combination to work you *must* run the first one first to collect its side-effects, and then the second one. Returning is just.. well, returning. For the non-determinism (or list) monad, the combination says you take every result from the list of results of the first component and run the second one on each of them, and then concatenate the resulting lists, and returning them returns them in a list. The Maybe monad represents computations that can possibly fail, the combination operation says that if the first one succeeds, we try the second one, otherwise the whole thing fails. Returning simply indicates success. I think the only bullet-proof way of understanding monads is to try out many different ones.
So it should be a "selection comonad". The first thing I think of when I see this data type is lenses: type Lens a b = a -&gt; Costate b a You can then see `Costate b a` as a `b` that is selected from an `a`. So I would vote for "Selection b a".
Those are a specific instance of a monad, the continuation monad, which is important, but still just a specific instance.
Monads are captured by a library so are not a "pattern". Design patterns are characterized by not being capturable as libraries in the language they're used in.
I think you should do something you're really interested in, what better than to enjoy doing something you're interested in!
You're talking about a specific instance of a monad. I recommend first learning about a bunch of different instances of monads and only then try to pick up on the generalization. I think the reason monads are perceived as so difficult is because they're truly more general than the stuff you've seen in other languages. They also use higher kinded polymorphism along with return type polymorphism, both of which are features that don't exist in most languages.
general abstract nonsense
Coming from an imperative mindset, think of monads as something which allows you to inject code between each line you write. For C/C++ people, this means something like being able to overload ";"
Cont is the mother of all monads, if you disregard evaluation strictness
You're welcome. If you're not familiar with Haskell doing something so IO bound will probably be fairly difficult since it'll force you to become at least familiar with a lot of concepts. Don't get too hung up on making it pretty, because it probably won't be. :)
Deferreds don't implement Cont's methods (e.g: callCC) so they're not mothering any monads... I guess I was wrong to say they're even the Cont monad, and they just boil down to explicit IO binding, and nothing generally monadic.
Ah, that makes sense. I apologize.
What about the observer pattern? Can be wrapped up in a library easily in many languages, but is still a design pattern.
When I started learning Haskell (not finished yet :) ) I tried to quickly get a grasp on Monad's understanding and it proved harmful to the learning process. I understood monads when I started writing some code without thinking so much about them and realizing that I kept repeating the same pattern of composing functions. Then it occurred to me: Monads are just a way to write concise code while dealing with repetitions in code.
Write a version control system. Watch it fail. Learn. Move to something else.
Con: Vague. Almost every categorical construct is general abstract nonsense.
&gt; I think the join function is easier to understand than the bind operator when you are just getting started though. Agreed. Monads really are about fmap, join and return. &gt;&gt;= is just defined using fmap, join and return to capture a common use case. Since &gt;&gt;= tends to be more useful in practice than join, monad implementations usually define &gt;&gt;= directly in order to optimise it. 
By the way, you can express the lift operation for functors using bind: lift_M f = \m -&gt; x &gt;&gt;= return.f As in "unbox m, then apply the pure function f and box".
Design patterns are tied to a set of languages. Observer is a pattern in C++ and similar languages.
[Programming Praxis](http://programmingpraxis.com) provides a collection of etudes, updated weekly, for the education and enjoyment of the savvy programmer.
This is what makes monads so difficult to explain: if you completely understood them, in full generality, then your proposal for "here's what I understand monads to be: am I right?" would just be the definition of a monad, which is in turn known to be incomprehensible to most beginners! Monads are an abstraction which describes a huge class of very different things with some structure in common. As soon as you say something more specific, you've excluded a huge number of possible things that actually are monads. That's okay, though. If you have a good understanding of a selection of specific monads, and you understand that other things in the future may or may not happen to have that same API, then you're doing fine for programming in Haskell. Understanding specific monads is usually not hard, and recognizing them when they occur is... well, okay, it's a bit harder, but in many cases, they will occur because you want to combine or modify things that you already know to be monads, and then you've got a pretty decent idea that you might expect it. (I'll also concur that what you described is really more about functors than monads. A functor F associates, to each possible type t, a new type F t, such that for each possible function f from t -&gt; s, you can get a function which Haskell calls "fmap f" from F t -&gt; F s. There's one more thing that has to be true: namely, that if you do "fmap (f . g)" (where f . g is the composition of two functions: (f . g) x = f (g x)), then you have to get the same thing as "fmap f . fmap g". That's pretty much as you'd expect: if you do two things to each element, that's the same as doing one thing to each element and then another thing to each element of that result. Of course, there are functors that aren't containers, too, so you haven't really described functors in full generality, but you'd got the idea. And you're on the right track: all monads are functors. However, there are some functors that are not monads.
No love for &gt;=&gt;?
That gives you a `put`, of sorts, I suppose, but I don't see how it gives you anything resembling a `get`? edit: nevermind, spl's comment explains it to me.
You may have an unusually general notion of "container", but it sounds like you are thinking only of a subset of the monads. Are you comfortable with Reader r a ~= (r -&gt; a), and State s a ~= s -&gt; (a, s)? 
In the realm of failed computations, the functions composition law is that if a function fail, its successor won't be run. In the realm of list computations, the functions composition law is that a functions is applied on each item of the result of the previous functions. Then the list of lists is flattened to be send to the next. In the realm of logged computations... The realms are monads. They have specific composition law. Most functions (because of their type, a -&gt;mb ) are functions of one of the realms. If you need to use a normal functions, just lift it, aka you disguise it to cross the border and you make it behaves like a good citizen (whic respect the realms law). 
Kleisli composition is IMHO the easiest way to understand monads. Monads are functors for which composition of functions of type a -&gt; m b "works". If you composed them naively, you'd get fmap g . f :: a -&gt; m (m c). Join allows you to reduce this double m to a single m so that the resulting function is of the same type, and so can be composed with yet more functions. join . fmap g . f :: a -&gt; m c. The monad laws specify that Kleisli composition is associative and that return is an identity of it.
I had a similar thought. That's the same as the type newtype XFun a b = XFun { runXFun :: a -&gt; (b, b -&gt; a) } from Takeichi's [Conﬁguring Bidirectional Programs with Functions](http://www.ipl.t.u-tokyo.ac.jp/~takeichi/attachments/METR2009-342.pdf).
[Context Comonad](http://hackage.haskell.org/packages/archive/category-extras/0.53.5/doc/html/Control-Comonad-Context.html)
Monad comprehensions were an official part of the Haskell 1.4 language standard. They were implemented in GHC, and a couple of other compilers. But they caused such an upset that removing them was one of the prime motivating factors that prompted the creation of Haskell'98. Almost everyone in the community agreed that monad comprehensions led to very poor compile-time error messages, especially for beginners. If anyone wants to agitate for the feature to be brought back, they will need to demonstrate a very much improved story for how to explain errors.
Once it is implemented as a library, its implementation ceases to be repeated, so it is no longer a "pattern". I've seen various libraries that implement adaptations of the observer pattern, but except perhaps FRP, none of those were general-purpose.
To clarify why people might be interested in `&gt;=&gt;`, consider how natural the monad laws appear when written this way: return &gt;=&gt; g = g f &gt;=&gt; return = f (f &gt;=&gt; g) &gt;=&gt; h = f &gt;=&gt; (g &gt;=&gt; h) P.S. If these remind you of the Monoid laws, pat yourself on the back! You can think of monads as being "typed" monoids.
I love you Kleisli Arrow! 
memory comonad
Is that a darcs joke, or am I reading too much into it? :-)
That looks fantastic. Thanks for the link!
That makes sense. I would imagine that IO would be a bit different from everything else, seeing as it requires side-effects. You're right that it might be best to make code clarity secondary while I'm learning. Thanks for the tips.
Well, you're right of course. But as I said, the fail rule is handled separately. There is a test in the typechecker whether or not a pattern can fail and if it does the "fail" operation is typechecked and applied. However, the basic translation rule is correct, and I really don't know how I would write that "can it fail?" test for a lambda expression outside of GHC source... :)
I think kamatsu might be referring to this blog entry: http://blog.sigfpe.com/2008/12/mother-of-all-monads.html
noice
The comonad is a bunch of labeled cells, which is analogous to what memory is. The fact that the memory is pointed is implied by being a comonad. Memory is a nice dual to State.
BTW, [one user reported](http://twitter.com/ozataman/status/9314146210287616): "FYI, when buying ebook+print, the discount is $24/$55 = ~44%."
I have yet to delve into Haskell. Is programming in Haskell faster than programming in C? And isn't it easier to multicore using functional languages than with imperative ones?
I think U&amp;V call it the "array comonad" at some point. Still not a very appetizing name though :-)
Don't get trolled :-)
You can probably produce working solutions to problems faster, but will have to suffer when you start optimizing. Yes, it is relatively simply to do multicore in Haskell compared to other languages. Mark my words though, you will regret it. You either stay away from it and curse the smug sense of superiority emanating from functional programmers or you join in on the madness and curse yourself for now having to waste time on different things than before. Also you will probably start to hate dons, who really is like one of those guys who give out a little Heroin for free only to make you pay when you're addicted. It will steal all your time.
So if you plan to primarily multicore, it's a good idea? 
Absolutely. You should give it a go, but don't come back complaining to me when you get anger management problems because of it. I would like to add that if you're going to go into Haskell, please make something useful with it. I have wasted so much time on the mental masturbation these Haskellers like so much, only to find out that there is no resulting ejaculation.
I think its only fair to point out this discount applies to all O'Reilly books.
hahaaha. I love haskell, and you seem to like it a lot too. But the way you express your love is really funny xD
obvious troll xD. I like darcs, but i laughted.
Con: Vague. Every comonad is pointed context of some kind.
Try some problems on the [Sphere Online Judge](http://www.spoj.pl). It's one of the few (only?) challenge sites that will accept answers in languages other than C/C++/Java, and even the more trivial problems will help you understand how to optimize Haskell's IO operations. 
No, it's not. I wrote a MUD when I first learned how to program. I watched it fail. I learned a lot from it. I moved on. Writing a MUD was a monumental step in my development as a program. I simply find VCSs to be more interesting than MUDs, so I think they're worth working on.
Hah sorry, this wasn't a reference to Darcs. This was a reference to my own development as a programmer.
Pointed Reader
GHC has a very consistent way to report errors. Adding the correct messages for monad comprehensions was basicly just one single line since I was able to reuse a lot of the list comprehension code. Functions used for desugaring and typechecking aren't shown in those messages. Yes, there's this "scary" word "monad" in "monad comprehension" - but I guess monads are widely used and accepted these days.
People do not need yet another application. But people desperately need LIBRARIES. Port something, or at least create FFI bindings to some library. I personally would vote for a pdf library.
Audio signal processing is fun. You get a good feel for lists, folds, maps, scans, zips, recursion, state, higher order functions, arrays, etc. It's like the hello-world factorial on steroids. The following paper should get you started. http://dafx04.na.infn.it/WebProc/Proc/P_201.pdf
You're doing it wrong.
Some suggestive functions/names: -- | Undoes the function, loosing the capability to undo. undo :: Undo b a -&gt; b undo (Undo b _) = b -- | Undoes the function, keeping the capability to undo -- to the same point. revert :: Undo b a -&gt; Undo b b revert (Undo b _) = Undo b id -- | Commits to doing the function, with the capability -- of future functions reverting to this result (as opposed -- to 'extract' which looses the capability). commit :: Undo b a -&gt; Undo a a commit (Undo b f) = Undo (f b) id -- | Redo the function with a different input. redoWith :: Undo b a -&gt; b -&gt; Undo b a redoWith (Undo _ f) b = Undo b f -- | Despite the suggestive type, this isn't quite 'return'. doWith :: a -&gt; Undo a a doWith a = Undo a id -- | Despite the suggestive type, this isn't quite 'bind'. doing :: (a -&gt; Undo c b) -&gt; Undo c a -&gt; Undo c b doing f (Undo c g) = Undo c (extract . f . g) thenDo :: Undo c a -&gt; (a -&gt; Undo c b) -&gt; Undo c b thenDo = flip doing infixl 1 `thenDo`
True, but you're only remembering the source and the path from that source. Which is why I suggested the undo or revert/reversion comonad.
Along the same theme as [my previous suggestion](http://www.reddit.com/r/haskell/comments/ed184/survey_new_name_for_the_costate_comonad/c177uif): the reversion comonad (with type constructor `Revert`).
Well, yes and no. In many, the comonadic operations themselves alter the context. in this case, fmap &amp; co. don't touch the context at all. They do, however, alter the point.
I believe that the correct mathematical term is 'fish'.
You could mimic the translation of do-notation described in the [Haskell 2010 Report](http://www.haskell.org/definition/haskell2010.pdf) (on page 25). An adapted translation rule for monad comprehensions would introduce a potential call for fail regardless: [ ... | pat &lt;- rhs, ... ] ==&gt; let ok pat = ... ok _ = fail "pattern match error" in rhs &gt;&gt;= ok That GHC is smarter than that and investigates the pattern in order to decide whether the more complicated translation is necessary is an implementation detail that does not change the meaning of the transformation described above. edit: refered to Haskell 2010 report instead of Haskell'98 and added link.
Currently monad comprehensions are only brought back as an optional language extension of GHC. Users who don't enable them explicitly are not confused by more complicated error messages. Before bringing them back into a Haskell standard, error handling needs to be addressed indeed. Users who don't know that they use monad comprehensions should not be confused by error messages about monads when they just use list comprehensions.
fmap, duplicate and hence extend alter the context. Arguably the only interesting thing that duplicate does is alter the context.
doing f = fmap (extract . f)
Can someone explain the selection/selector difference and why one word is preferable to the other. I'm a bit confused.
This book desparately needs a second edition. The first edition is a good attempt at making a followable Haskell book, but after a few very clear starting chapters it becomes deeply flawed. They keep accidentally introducing functions that haven't discussed before in examples, and they don't really seem to have an empathetic grasp of what people do and don't find intuitive. I left quite a bit of feedback on their site, hope they take some of it into account...
I think I'm missing something. I think of the context as the `b` part of `Costate b (b -&gt; a)`. Duplicate alters the `b -&gt; a` part and preserves the `b`. So does `fmap` and hence `extend`.
Do you have a link to the PyPy work, too? Is it [this one](http://morepypy.blogspot.com/2010/05/efficient-and-elegant-regular.html)?
I consider the context to include all the `a`'s that are not the one returned by extra, plus all the other label stuff. Basically I think of the context of a comonad as everything other than the value produced by extract.
Ok - this really helped. At least I think I have a better understanding of what features a monad provides - basically it's a functor that also provides `join` that can flatten nested 'containers' and `return` that can construct a 'container'. I call it a container but I understand it could really be something more than a typical container like a list. But it helps to first understand the concept using just containers. I think the 'generalized way of sequencing functions' is what I don't really grasp yet. That is also what other threads in here allude to. Once I get a better handle on what features a monad provides, I can probably start to understand how it might be used.
Yes it is (Your reference and a [follow-up post](http://morepypy.blogspot.com/2010/06/jit-for-regular-expression-matching.html) are linked from the project page of the Haskell package.)
I see - this sort of makes sense. Does this mean that other languages (e.g. Python) provide only one or two ways to combine computations. For e.g. you can take the result of one function and pass it to another: result = f(g(input)) Lets assume both `f` and `g` take an integer and return another integer. If `input` it happens to be None, an exception will be raised. I'm assuming in Haskell, instead of passing an Integer to `g`, I could pass in a `Maybe Integer` and I would get a result that is a `Maybe Integer`. So basically if I can apply a function `g` to a value of type `T` and get a value of type `R`, I can also apply it to a type `Monad T` and will get back a value of type `Monad R`. Is this an example of something that monads let me do? Or I don't need to use monads for this and I can just use functors?
It's worth underlining that the idea of the `b` as a context of `a`s doesn't mean that context can't be infinite, continuous, and in some sense much smaller than the selector portion of the Costate object. Consider, for example, `Costate 1.5 (\x -&gt; round $ x * 17)`.
Yeah, I think so too, but it was based on my mistake in the first place that Deferreds are Conts. They're just explicit I/O binding, not really generic monads.
dons et al were working on a second edition last year. I wonder what happened to that.
That's just functors.
Hmm ok. I guess I should follow everybody's advice and try to use various monads as I learn about them, rather than trying to understand the concept up front. At least I understand functors and their various uses :)
(Note: I am coauthor of the paper) I have since developed a much more elegant Haskell solution that also optimizes LL(k) grammars to be linear parse time. This latest implementation is also just a really interesting read; I compute the recursive properties of the cyclic parser data structure without references, pointers, or monads. (Although, I do use Data.Reify). The whole thing is heavily commented in Literate Haskell. Check it out if you like (git repo): http://david.darais.com/git/research/der-parser-3
Hi, you don't seem to have put it on the [/r/haskell_proposals](http://www.reddit.com/r/haskell_proposals/) subreddit. Would you mind doing so? 
And it's because of lightning fixes like this that I'm glad text is in the HP now. I sent Bryan an email, went to bed, and there was a blog post and new text package when I woke up.
Simply start coding and the skills will come on demand, just don't give up (seriously, this works). A good metric of progress is a lot of refactoring as you understand new concepts.
I don't see anything inconsistent with what you said and my understanding of context. Every comonad `w a` is some structure of some sort. That structure contains at least one `a`, the value produced by extract. Typically the structure will contain many `a`'s for `fmap` and `extend` to do interesting things. Those many different `a`'s may be `a`'s that have the same value as the `a` produced by extract but are at different locations in the structure. In that sense I am considering them different `a`'s (similarly I consider the electrons in my hands and the electrons in your hands as different electrons, even if they are indistinguishable as particles). The number of other `a`'s could be infinite our continuous. The structure relating these `a`'s together could be arbitrarily complicated. Consider some examples The stream comonad: The value is the data at the head of the stream. The context is the tail of the stream The zipper comonad: The value is the data held at the cursor, the context is the rest of the ADT around the cursor. In the case of an editor buffer the value is the character at the cursor location, and the context is the rest of the buffer. The identity comonad: The value is the only data. The context is empty. In this case there is only one `a` value in the comonad. The environment comonad: The value is the component of type `a`. The context is other component. Again this comonad happens to have only one `a` (in general). But this one has a non-trivial context. Finally, the costate comonad: The value is the `a` occurring at the selected cell. The context is all the other `a` and the labels for the cells.
They remind me more of the Category laws than the Monoid laws.
&gt; I think the 'generalized way of sequencing functions' is what I don't really grasp yet. The "sequencing" aspect is implied by the `join`s or, equivalently, by the nesting. Using lists as an example again, if you have a triply-nested list `[[[a]]]` and flatten it down to a single list `[a]` you get the same result whether you flatten the outer layers or inner layers first. This property is required by the monad laws. So if you have functions `f :: a -&gt; [b]` and `g :: b -&gt; [c]`, and a list `x :: [a]`, there's only one way (up to equivalence) to get a value of type `c`: map with both functions `map (map g . f) x` and select an element from one of the innermost lists. Each layer of list corresponds to a function that introduced that layer, and the order in which they're nested provides some sense of "sequencing" for those functions. The significance of the `Monad` type class and the `join` function is that by compressing multiple layers into a single layer, all monadic expressions have the same type regardless of how deep the nesting goes. You can't write a function that takes an argument of type `[a]` and produces a result of type `[[...[b]...]]`, for some variable depth of nested lists depending on the contents of the input list, but using lists as a monad you can use `join`--which is just `concat` in this case--to flatten each layer as it's introduced.
The difference in this case would be if instead both `f` and `g` take an integer and can return either an integer *or None*. In Python, you'd either check at each step whether the value is None, or wrap the whole thing in a `try` block and catch any exceptions that arise. In Haskell, you could do `input &gt;&gt;= g &gt;&gt;= f` which would evaluate to `Nothing` if *any* intermediate step did. This is actually pretty similar to a feature found in some languages where calling a method on a "null" object just returns null instead of causing an error, or the way NULL values in SQL and NaN values in floating point computations work, except that in Haskell you can decide where you want that behavior to occur and have access to a bunch of useful functions that work on *any* monad instead of being special-purpose.
&gt; Con: Vague. Almost every categorical construct is general abstract nonsense. As far as I can tell "general" and "abstract" both follow directly from the definition of "categorical", so I take it that you are asserting here that there exists some categorical construct X such that X is not nonsense? Or perhaps that, of all possible categorical constructs, all but finitely many are nonsense.
There are some nice papers linked in there: http://homepages.inf.ed.ac.uk/wadler/topics/monads.html Reading the monad ones now; thanks for the link.
This is insanely freaking awesome! After seeing asvm's work on [mirage](https://github.com/avsm/mirage) I was seriously thinking about how possible the intersection between Xen and GHC is. I knew about halvm since a few years ago, but had given up on it ever being released. Thanks for giving back awesome code! One quick question that I couldn't answer myself from a quick glance: does this use the GHC you currently have installed, e.g. if I'm using GHC 7, will executables be compiled using it? Or does halvm currently ship with its own special version of GHC? It would be nice to have this clarified, since that has ramifications in terms of what GHC bugs, features and language extensions are present, as well as performance expectations.
It ships with its own version of GHC, which is a slight variant of 6.12.3. GHC 7 is on our TODO list.
Yay! I've been waiting for sooo looong!
Thanks for the fast reply! It would probably be nice to put that on the main wiki page so people are aware. 6.12 was a mighty good release, and I look forward to when it's available for GHC 7. :) One more question: Is there any hope of getting this functionality merged back into mainline GHC? As I understand it, GHC HQ would probably be quite reluctant to do so unless Galois swore to maintain it and put forth people to do so - otherwise it becomes a large maintenance burden on behalf of GHC HQ, who naturally don't really use such environments like Xen. It's not really a 'win' for them to have such functionality, is what I mean I guess, unless someone else can maintain it. You guys (Galois) are built on Haskell and GHC, so I very much doubt you lack the people or expertise to maintain it, but it would probably alleviate your maintenance burden a bit to have it integrated mainline (and let's face it - that giant list of patches in `patches/ghc` is pretty ugly to keep around and update I imagine!) Just wondering if you guys think it'll happen or if it's ever a possibility. I'm going to be installing this and using it soon I think. Expect bug reports, general annoying complaints and possibly patches. :)
We're under very heavy traffic now. Here's [a mirror](http://corp.galois.com/blog/2010/11/30/galois-releases-the-haskell-lightweight-virtual-machine-halv.html).
Quick second-reply since it's not related to my last post: you should also probably change on the 'download and install page' that you'll need to generate `configure` from `configure.ac` via autoconf, since it doesn't look like the git repo has the generated `configure` script. It also doesn't look like there's been a git TAG for 'release-1.0', so you may want to add that to the latest HEAD commit, but maybe I'm just being too quick on the uptake and annoying by association. :)
There's always hope. :) There are basically two problems, here: pushing the patches to GHC into the mainline, and allowing for the weird cross-compiler-but-not build we use. It seems more likely that the first might be integrated into the mainline, but the second would be tricky. I suspect all this would be predicated on us getting up to speed with GHC's HEAD, though, and I'm not sure what the timeline on that is. Depends on how many interested hackers we get to help, I'd guess. :)
fixed and fixed. thanks!
I'm curious, is there any relationship between this and the [House](http://programatica.cs.pdx.edu/House/) and [Lighthouse](http://web.cecs.pdx.edu/~kennyg/house/) projects developed at Portland State University?
Very cool. I was just playing with Oberon today (well a version of it) and started thinking about House and Haskell. I guess it's Wirth a shot! Sorry... 
We share information, and Andrew Tolmach at PSU (who spent some time at Galois) did some of the initial work on the HaLVM. I'm not sure if there's any shared code, though, and we've drifted a lot from Andrew's initial implementation.
Haskell just got hardcore 
Can someone comment on similarities/differences to Mirage: http://www.openmirage.org/
So I guess this wouldn't let one throw a haskell program onto Amazon EC2 (since they restrict the available kernels, and probably a lot of other reasons I don't understand), but are there any other hosting services that might allow something like that?
If/when GHC gets genuine cross-compilation support, does that knock out the second issue?
(Light)House uses some old HALVM patches for GHC 6.8 in order to get a functional Haskell to x86 bare metal compiler pipeline. I used the same patches when doing the Linux modules work. The patches are a pain but it seems HaLVM has been polished to the point it's simple-stupid to build (great work Adam).
Certain categorical constructs may be quite abstract and general, but exceedingly specific. To take an arbitrary example from the current lead article at the n-category cafe (which I don't claim to understand): "a localisation of the 2-category of proper etale Lie groupoids at the class of ‘weak equivalences’". In fact, the more specific, the more nonsensical it tends to seem :-)
These days, you can boot almost whatever you want on EC2, provided it works with pvgrub.
Probably? It depends on how they do it. :) The trick is that the compiler can't bootstrap itself, as a lot of the notions that GHC has don't work on a barebones HaLVM. ("What is this file system thing of which you speak?") On the other hand, Template Haskell support is really handy, and we'd like to maintain our ability to include that in the HaLVM; currently this requires either a stage2/bootstrapped compiler, or some weird hacky things like we currently do.
Rather, everything general abstract is nonsense. According to the disparaging coining of the term. 
True nuff. 
Meaning it wasn't before you heard about HaLVM? ;-)
wow, that's awesome.
It's disgusting how much talent there is at galois.
&gt; This is what makes monads so difficult to explain: if you completely understood them, in full generality, then your proposal for "here's what I understand monads to be: am I right?" would just be the definition of a monad, which is in turn known to be incomprehensible to most beginners! You're right, and I did believe I was missing something, because functors are not that complicated.
Take a look at [CrossCompilation](http://hackage.haskell.org/trac/ghc/wiki/CrossCompilation), I think we should be able to fit your scenario into the plan there. Specifically you want a build with host=i386-unknown-linux and target=i386-unknown-halvm. In this configuration you need to build the libraries twice (once to link with stage2, and once for the target platform), which the build system doesn't currently support, but it wouldn't be too hard to add.
Hmm, I think I like Johan's solution better in the long run... 
Hmm, yes, you can roll your own AMI on EC2 from scratch, so I would suppose this might potentially be possible. I wouldn't pretend to be nearly enough of an AWS expert to say for sure, though. However, it occurs to me that cloud applications might well be the very *point* of a system like HaLVM.
Here's a dirty little script to wrap a kernel binary into an AMI. https://github.com/avsm/mirage/blob/master/scripts/ec2.sh Unfortunately, I can't figure out the AWS magic to build EBS images directly, which means that you can't use the free m1.micro instances with this script (the free ones don't support AMI, it seems). Help from anyone who knows welcome...
Tom Harper (author of the first prototype of the text package) has been working on this for several months. The principle works fine, it gets pretty tricky when it comes to the constant factors, especially efficient access to the beginning part of the string. Also, traversals across a finger tree have to rebuild the finger tree which slows things down. Trying to do fusion with these more complicated tree structures is even more fun. Still, my hope is that we can get a really good implementation.
Except for being written in/for OCaml? :)
That's an excellent difference :-) Some more comments [here](http://www.reddit.com/r/programming/comments/ee4zx/halvm_run_haskell_directly_under_xen_without_an_os/c17hrxj). too many dup threads on this topic! 
One could even combine multiple encodings within one rope this way. E.g. a huge string that is almost but not quite entirely ASCII could be encoded as a tree consisting of some large O(1)-indexable ASCII-arrays with a few UTF-8/16/32 pieces.)
store comonad
Is there any reason that the standard libraries of HLLs don't just give people ropes when they ask for strings?
Because Strings are not really ADTs, they're mere type aliases. And lists are convenient.
People often say this but it's not clear how useful it is. If indexing is sometimes cheap and sometimes expensive then should you use it? If you do then on some inputs you may get terrible slowdowns, if you don't use it much, then is the extra storage, conditional branches and code complexity worth it?
It seems the included `stdint.h` is broken (empty) and all your std ints are in `types.h`. This breaks `cryptohash` among other packages. EDIT: Patch submitted
I want an UTF-8 based Unicode type with no extra fields for offset and length so that I can store millions of them in a HashMap. GHC 7 ByteArray# should be perfect as it already stores the length in bytes. Pretty please with sugar on top. :) 
I'm not saying you're not right, but aren't those the ones you've been working on with bos?
wow. just wow.
Here's a sketch: http://code.haskell.org/~aslatter/code/smallstring/Data/SmallString.hs It's built on top of this: http://code.haskell.org/~aslatter/code/bytearray/ It offers no high-level string operations, only to/from string, Eq and Ord. We still store a length prefix because the GHC primitive length operations return what was allocated, not what was requested for allocation. I think this was fixed in GHC 7 but I haven't tested it in a while.
Why do people want cheap indexing? I'm having a hard time coming up with actual use cases that isn't better served with other abstractions.
I'm now proposing the "store comonad". I like the state/store duality. It sounds nice.
Right, I think that's a good API. Don't provide lots of operations, have it just as a compact format. For operations people can convert to Text/ByteString. Eq/Ord is just enough for the main use case, which is keys.
It seems to me the useful place would be when you are dealing with something where Unicode isn't enough. Unicode doesn't contain everything, after all. Though I imagine anyone with the need for this use case gave up and went back to paper a long time ago. This definitely strikes me as the sort of thing where if you're sophisticated enough to need it, you're probably one of the few people who can usefully implement it anyhow.
I think that this article is another attempt to lure out the lurkers but fine. I like lurking alot and I give a shit about Haskell. There, I said it.
Yep. Industry programmers, by and large, don't give a shit about Haskell. That's because industry programmers don't give a shit about the correctness of their programs, any CS theory at all, or any language feature invented post-Watergate. If they gave a shit about any of these, they'd give a shit about Haskell, but they don't. This, of course, is Haskell's fault.
I'm an industry programmer and I care about it a great deal. My employer though, different story. 
Shouldn't this have been posted 8 months ago (or 4 from now)?
&gt; "We do have an experimental humor monad," added MacDougal. "But it doesn't seem to be getting much adoption. Haskell fans just don't see the need for it." A humor monad? So basically that's a model where jokes about something are equivalent to jokes about jokes about something. In other words, meta-humor in-jokes. No wonder it's not working!
&gt; any language feature invented post-Watergate Which is why stuff from the early 70s is commonplace, such as the features found in Lisp and ML. Seriously, a lot of the "advanced" features in Haskell are older than the programmers not using them.
Steve is possibly the worst writer on the internet. That was so unfunny I felt embarrassed for him while reading it.
&gt;"People see words like monads and category theory," Briars continued, swatting invisible flies around his head for emphasis, "and their Giving a Shit gene shuts down faster than a teabagger with a grade-school arithmetic book. I'm really disappointed that more programmers don't get actively involved in reading endless threads about how to subvert Haskell's type system to accomplish basic shit you can do in other languages. But I guess that's the lazy, ignorant, careless world we live in: the so-called 'real' world." This sounds like he's actually deprecating Haskell.
Umm, thanks for sharing?
[The move is complete](http://twitter.com/haskellorg/status/9854129107111936). Thanks for your patience.
Allston Trading, a Chicago area company, uses Haskell for their high-frequency trading critical sections, and they look VERY favorably on students who show interest in functional programming.
If he were a Haskell programmer, he'd have known that it should probably an applicative functor, or perhaps an arrow. And quite likely an instance of Traversable and Alternative as well, and a half dozen other type classes for good measure!
C'mon man find your funny bone, he wouldn't have been able to write what he did if he *himself* didn't at least give a little bit of a shit :-) Industry programmer learning Haskell mostly because I'm sick of null
Looks great!
But how do you decide what chunk size your leaves should have? I suppose it's a time vs. space efficiency thing. I wonder if it's possible to adjust them optimally based on usage, instead of having to decide apriori.
Congratz!
He says you're welcome!
Some pieces appear to have gone missing, such as the Haskell Tutorial for C Programmers at http://www.haskell.org/~pairwise/intro/intro.html.
But we're at 41 up votes already!
But the images are still broken almost everywhere. I'd hardly call such a move "complete" :(
It would be funny from a Haskell guy; but from a lisp guy it sounds like sour grapes.
You beat me to it. Sour grapes big time.
Nah, it's not even a monoid. If you apply it repeatedly, it's not funny anymore.
After I logged in, all of the new styling disappeared and the page went back to the default WikiMedia style.
It seems like every time there is a change to the Haskell front page or the Communities page, the link to the Wiki disappears. I put back the link on those pages. Does someone have something against the Wiki? I think it is still one of the most important sources of information about Haskell. Furthermore, without a link it is hard to find. The main page of the Wiki is taken up by the haskell.org home page, so you have to know how to get to the Category:Haskell page.
&gt; If you apply it repeatedly cf. "Internet memes"
You don't say...
do you mind elaborating a bit on why haskell is commonly used in computational linguistics? Im a haskell beginner studying comp.ling.+math in my first year but we only learn Perl, C and Prolog.
very classy, i like it
Sony has put it's Snap framework on hold (probably because of Google TV (Android)), so I expect no difficulties
I am not quite able to figure out his actual stance on Haskell. Anyway, it will be amusing to see him ask "Can somebody explain to me what a monad is?" after Haskell's world domination in a not so distant future...
+ infinity ;-) Looks very very nice
Contact the author. They were supposed to pipe up when the move was announced a couple months ago. Probably the thing to do is to put the material, either on the wiki, or on the community server (or elsewhere) and ask the haskell.org people to set up a permanent redirect.
Article debunked. And thanks for the karma. Vote this comment up if you're mad at Steve Yegge! (just kidding, don't) I thought the article was funny. I mean, yeah, it's mean to Haskell people, and it definitely pokes at me, I mostly just lurk and only do a little with Haskell myself, but I can take a dig. The article mostly made me smile. The main thing I've worried out is articles like this are indicators that we may be achieving the wrong kind of success. "Avoid success at all costs." Wasn't that one of Haskell's guiding principles? Didn't that help the community continually _improve_ Haskell to get it where it is today? Once you start trying to compete in less forward thinking organizations you give something up. For a community like Haskell, we might start winning the numbers game, and then find we sold our soul to get the trophy. The Haskell community is a happy happy place. The thing we built this community around is a language and runtime that works really well. I don't think it's just quality people. It's also building what we do around something we can depend on. That makes life pleasant and it shows here. The world of corporate programming that chooses Java or .NET because everyone else does, that's not a happy place. Those are not happy people. You bring in a couple thousand people who only know how to gripe about how nothing works well (because it really doesn't) and a lot of the fun of Haskell could be gone. We'll have to start all over again. Maybe with DDC... 
He doesn't like it (he had a falling-out with statically-typed languages some time ago).
Ah, thanks. A pity.
http://book.realworldhaskell.org/read/ You can also read it online for free. The problem I had with this book was that when I wanted to learn something new, the example given would be too big, and would refer to a previous example in the book. I'm very lazy and there's no way I will read some massive example just to find out a concise definition of a tiny language/library feature, so I ended up just using google and not bothering to read any of it.
Haskell has a robust polymorphic algebraic type system, which is good for all sorts of things, but which is perhaps most widely used in Haskell to enable insane amounts of data-directed programming, which allows you to streamline your programs by eliminating unnecessary redundancies (as in what the Reader monad does) across differently typed phenomena. Thus in a linguistic context, it lets you do a lot of complex things without cluttering up your theory with lambdas everywhere just to pass around some data that otherwise is hardly relevant to the majority of things that have to carry it around. Your model is cleaner, basically. Also, Haskell nicely employs the lambda calculus as straight forwardly as possible, as do most linguistic theories, and it's lazy, enabling you to do all sorts of crazy things without the need for complex definitions. Perl and C are suited more for NLP, and Prolog is more suited for proof-oriented techniques, whereas Haskell is more for the traditional computational linguist who's interested in theoretical issues of computation in language. Also, Haskell is a great resource of ideas and prior work. The Reader monad and it's essentially identical sibling, the (-&gt;) r monad, are a relatively easy way to bridge the conceptual gap between variable-based semantics and variable-free semantics, and the step from the one to the other, if unintentionally discovered while just reading Haskell blog posts, is a wonderful epiphany.
Also, if memory serves me he has a thing for Lisps and languages that you can (to some extent) treat like a Lisp. So probably the reason you found it hard to figure out his angle is that he's taking cheap shots at Haskell (which I expect he considers cute, but impractical and overly restrictive) as a means of making fun of "industry" languages like C++ and Java (which I'm pretty sure he hates with a burning passion).
Finally! Love it.
I find it funny personally. I see nothing wrong with a language that lets you push and pull pure values in and out of imperative contexts when needed. Sure it would have helped if someone would have explained Haskell in that light to me on day one, but I think most of the documents around Haskell are a bit too academic. Articles like this blog post seem to be the fallout of that, or perhaps he was just using his humor monad.
It sounds more like he's actually not understanding Haskell. Those endless threads aren't about how to "subvert Haskell's type system to accomplish basic shit you can do in other languages", they're about how to leverage Haskell's type system to *check properties* that you *can't* check in any other languages (at least, no other languages that wouldn't garner even more scorn from these people).
Based on the comments here, I feel like I know all I need to know about the article. It mystifies me why folks like Steve seem to believe that other people spending time doing work he doesn't understand (most likely because he's afraid to put in the time required to understand it, because he might fail to) is *harmful* to himself. Functional languages are becoming more popular because industry programmers are finding they can't function with the status quo.
Just clicking around randomly, it seems that for example [this page](http://haskell.org/haskellwiki/Pronunciation) on the wiki has some issues with the wiki software.
I upvoted this in /r/programming, but downvoted it in /r/haskell. Hoping to get the /r/haskell number down to 38.
That is the spirit. Put up practical examples that rely on Haskell and we'll show those unbelievers. 
my preferences -&gt; skin -&gt; hawiki
The side bar mentions languages he likes, which includes Haskell.
nice, thank you!
Good catch!
I think part of the point of the exising layout is to fit within the existing filesystem heirarchy standard: non-executable data is stored in $prefix/share so that it can be mounted from a central server to multiple machines on multiple architectures, $prefix/bin is arch-specific etc. Making a new set of dirs under $prefix/haskell would mean that any existing shared environment would have to be modified just for haskell installs. Perhaps better would be $prefix/bin/haskell/... , $prefix/share/haskell/... etc.
A minor suggestion: Confronted with some wiki page, it is not immediately apparent (except from the logo and the URL) that this is actually a page of the Haskell wiki. Maybe the text of the “Home” link could be replaced by "The Haskell Wiki" or something along these lines?
Some reasons not to do this: * The vast majority of installs are on local disk. * Things in share are generated and can actually vary by architecture. * Things in share are generally very small and not worth the cost of shared storage. * Anyone with such a shared set up can still install the way you wish or the current way because it is all user configurable. I'm just proposing changing the default.
Done: http://hackage.haskell.org/package/smallstring Let me know if it is small enough.
Perhaps you can explain why I'd want executable files from multiple different compilers to be installed simultaneously. Unless I am doing compiler development and suspect one of my compilers of being buggy (in which case I shouldn't be installing its binaries in a default global location anyway), executable files built with different Haskell compilers should be interchangable. On the other hand, of course moving bin from /usr/local/bin to /usr/local/haskell/ghc/bin would result in plenty of people installing programs (which they need have no idea were written in Haskell) and having to change their PATH before being able to run the program. That's certainly unpleasant. I don't want people to recognize programs written in Haskell because they are a pain in the ass to install and run. Essentially the same issue was raised for the share directory. In all of these cases, things that have nothing at all to do with Haskell expect the bin, share, lib, etc. directories to be there in essentially that form, and Haskell packages should respect that unless we want to be bad citizens in the operating system. The one thing that may have merit here is moving documentation under compiler-specific directories... but I rather hope not, actually. If there are differences in the documentation of packages based on what compiler is used to build them, then perhaps things need to be separated out into compiler-specific packages.
maybe this is the perfect name for state-in-context comonad.
Some binaries, such as haddock,, ghc-pkg,, and c2hs, are compiler-specific; if you want to have versions for multiple compilers installed (such as for automated builds), one must currently play annoying games with prefixes and manually copy stuff around. Perl, Python, and Ruby have all solved this problem by keeping everything under `/usr/local/lib/language/version` or similar, and simply symlinking a binary to /usr/local/bin when it's installed. That allows the common case (overwriting with a new install) and the more specialized (multiple versions).
&gt; It mystifies me why folks like Steve seem to believe that other people spending time doing work he doesn't understand (most likely because he's afraid to put in the time required to understand it, because he might fail to) is harmful to himself. &gt; &gt; Functional languages are becoming more popular because industry programmers are finding they can't function with the status quo. You realize that Yegge is pretty much a smug Lisp fanboy, right? That kind of "folks like Steve" have been sneering at languages used in industry since before Haskell even existed.
And cabal already has built-in support for creating symlinks for binaries!
Actually, this package layout proposal is for covering haskell packages that create haskell libraries and binaries for use during development. If you are packaging an executable built in Haskell for end-user installation and use, the layout will be determined by how you package it, not by how cabal installs it during development. That said, cabal already has built-in support for symlinking binaries into a directory on the user's PATH.
I don't really know who Yegge is except that he's known for being kind of douchy. I guess that inserts another piece into the puzzle, though. So he's one of the my-time-now-is-more-valuable-than-my-time-later anti-static typing folks, rather than just a generic anti-intellectual?
Well, I finally have an excuse to stop trying to follow his 10,000 word rambling essays.
The only thing I would add to this is utilities to maintain links (or copies) in `$prefix/bin`, `$prefix/share/doc` and other directories, in deference to the FHS. The utilities need to be able to do two things: * Select the "most recent" filesystem object to place in the system directories. * Scan for and remove stale objects in the system directories. 
Not really. If you insist on being snide about it, smug Lisp fans are the ones who think writing clever metaprogramming hacks is more important than the actual programming itself, a preference which demands a level of flexibility that excludes any static type system short of full dependent typing.
looks great. But where do I find a link to "compilers and interpreters"?
shitty graph colors
How would this proposal work out for distro packagers, eg the Debian Haskell team ?
I read this, and some part of me thought "man, people really like to nitpick these days". Then I saw the graph... OMG YOU ARE SO RIGHT.
Same here. Someone please buy that blogger a copy of [Tufte's book](http://www.edwardtufte.com/tufte/books_vdqi)!
&gt; a preference which demands a level of flexibility that excludes any static type system short of full dependent typing. Or at least, they assume it demands this level of flexibility, while not actually understanding what advanced type systems can do (as far as I can deduce from the other comment on this post that illustrated that Yegge didn't really understand what it is that people like Oleg spend their time tricking the type system into doing).
I call BS on the graphs. There isn't enough data to reach such detailed conclusions. In any case, the correlation between language and programmers above each rank seems fairly small, as you can see if compare the real data vs. a linear guess of what the number should be based on the total pool. Here are the real numbers vs. the predicted numbers. Pool of 4620: [1634,1232,948,485,80,55,51,42,33,19,18,12,6,4,1] 200 real: [70,64,34,17,3,2,3,0,1,1,4,1,0,0,0] 200 pred: [71,53,41,21,3,2,2,2,1,1,1,1,0,0,0] 100 real: [33,32,20,9,0,0,1,0,1,0,3,1,0,0,0] 100 pred: [35,27,21,10,2,1,1,1,1,0,0,0,0,0,0] 10 real: [4,3,2,0,0,0,0,0,1,0,0,0,0,0,0] 10 pred: [4,3,2,1,0,0,0,0,0,0,0,0,0,0,0]
That is officially the worst graph I have ever seen. I have no clue what it says.
No, bring runtime values into the mix enough and at some point it really *does* demand dependent types. And, honestly? A lot of GHC's advanced features and Oleg's crazy type hackery *is* jumping through hoops to do stuff that would be trivial in, say, Agda. Here's an exercise for you: write a function `apply` that takes two arguments, the first of which is a function of arbitrary arity and the other is a tuple of arbitrary size holding arguments for the function, that applies the function to said arguments. All types (including the function's arity) will be determined based on values known only at runtime. For extra fun, make sure that it also works correctly combined with higher-rank polymorphism for argument types. And no using `Data.Typeable` or such, since that's basically just creating a dynamic-typed DSL inside Haskell. Usefulness aside, can you seriously say that doing that wouldn't be a huge pain in the ass in Haskell?
A link to your example is now on [the multiplate page](http://haskell.org/haskellwiki/Multiplate#Alternative_Plates)
I mean... Maaaybe if the colors were at *least* indistinguishable from each other...and... Nah, nevermind. That graph is just straight up horrible in every way Edit: Later in the post the, some of the languages are separated. But not all of them.
Wow - 19 people entered the challenge using Javascript?
Dang. :(
But unless I'm writing a compiler or interpreter, why would I want to do that? (It's not like compilers and interpreters aren't important, but the people complaining about ivory-tower language implementors are presumably not language implementors themselves.) That's what I don't get about a lot of criticisms of static typing: they say "But how would I do FOO?", without making it clear whether they love doing FOO in and of itself, or whether FOO is actually a solution to some other problem BAR that actually has a much more elegant solution QUUX in a typed language, but because we don't know what BAR is, we can't reply with what QUUX is.
C: http://1.bp.blogspot.com/_FsLa1cMTCWU/TPgyKSc-rvI/AAAAAAAAAjk/5Si6vyFPEvQ/s400/c_density_plot.png Haskell: http://1.bp.blogspot.com/_FsLa1cMTCWU/TPg0CBK3W5I/AAAAAAAAAj4/q3iEbQvyMhc/s400/haskell_density_plot.png Lisp: http://3.bp.blogspot.com/_FsLa1cMTCWU/TPgyBXF3PhI/AAAAAAAAAjg/M6v-8WEvv98/s400/lisp_density_plot.png
Well, I'm pretty sure the Lisp fans aren't the ones complaining about ivory-tower academics. I mean... seriously. But yeah, can't really help you on that part. Obviously I think accepting those sorts of limitations is more than worth the benefits offered by Haskell's type system. On the off chance I really want to do that stuff, it's not as if I'm obligated to use Haskell for *everything*. Personally what I really don't understand, though, are people who use horrible languages with more type annotations than type safety and then still defend that sort of static typing as being useful. What?
You'll notice I was never saying that Haskell was particularly adequate, only that I didn't understand the belief that a complete absence of static typechecking is necessary to solve some problems elegantly. I think we're going to end up programming with some flavor of dependent types eventually; there's no way around it. It's just a question of whether industry people go through 5 more years of pain, or 500, before they figure it out.
But why do you *want* to check them statically?
[Haskell](http://1.bp.blogspot.com/_FsLa1cMTCWU/TPg0CBK3W5I/AAAAAAAAAj4/q3iEbQvyMhc/s400/haskell_density_plot.png) &lt;troll_mode_on&gt;May be because masturbation to categories, monads, and types doesn't actually require brain?&lt;/troll_mode_on&gt;
Because you take pride in your work and don't want to release software that has bugs that you could have found statically? (Similarly, because you're interested in trying out methods that the next generation of developers could use to keep bugs in safety-critical software from ever appearing in released code? That means saving lives.) Because you've learned that being able to say more about your programs in a format that the computer can check for you is useful for documentation and for quality assurance? Because when you can make your specification part of your program (because the specification is expressed in the type system), you eliminate bugs resulting from a specification that unintentionally diverges from an implementation, or vice versa? Because you can think more clearly about code, and thus write more and more reliable code in less time, when you can express your knowledge about it in a concise, machine-checkable form? Because it's fun?
How can this be a reminder if I didn't hear about it before?
Cofree pointed-b-container comonad.
Are Arrows still useful if we can relatively easily express them in terms of Category + Applicative (and vice-versa)?
Is there a torrent of the packages anywhere?
Why why why does any of this map to A machine? There are SO MANY virtualization services, that are CHEAP and DISTRIBUTED. $100 bucks a month buys a LOT of not-dealing-with-this-kind-of-shit. Give me a paypal link, I'll subscribe at least $20 bucks a month.
How long will it be down?
&gt; looks like the regexdna program fails to compile due to the lack of `Text.Regex.PCRE.ByteString' The regex-dna program [now fails to compile](http://shootout.alioth.debian.org/u64q/program.php?test=regexdna&amp;lang=ghc&amp;id=2#log) due to - regexdna.ghc-2.hs:75:3: Precedence parsing error cannot mix `$' [infixr 0] and `using' [infixl 0] in the same infix expression 
The problems are socal, not technical. In the past, there was no authority for "haskell.org" that could authorize any infrastructure investments. As of Nov, 2010, however, we [have a formal committee](http://haskellorg.wordpress.com/2010/11/15/the-haskell-org-committee-has-formed/) that can authorize spending of money. Things still to do: * a legal entity that can collect paypal donations * further infrastructure spending to solve problems in distribution and failover.
Not interested in content-free image links. Take this shit to [Proggit](http://www.reddit.com/r/programming).
Why is that so surprising? Javascript is a dynamic, object-oriented, functional language. In other words, it's a lot like Python. Except the interpreters are a *lot* better.
The graphs for individual languages near the bottom are more readable. And the author is offering the rest for download.
Not a surprise. The world is divided into: * Languages used for industrial shovelware * Languages people learn for fun People who will learn a language for fun tend to be smarter people. That's it, really. That's why you see two different shapes. At this level of broad correlation, the technical properties of each language don't matter. C, Haskell, and Lisp don't really have much in common. Sure, poorly-informed taxonomists will lump Lisp and Haskell together into some functional languages ghetto — because they both have lambda? So does every other half-decent language. It's weird to think of C as a "language you learn for fun", but it really has become optional in most CS schools and most industrial programming jobs.
&gt; Because they in many cases date back to 2004 Haskell programs for 8 of the 13 were contributed during 2010. Haskell programs for 4 of the remaining 5 were contributed during 2009. &gt; short of a -server flag Even without an explicit -server flag on the command line, Java defaults to the -server settings on a quad-core machine with 4GB. (The explicit flag is used so everyone can see that it's the -server settings.) &gt; &gt; reverse-complement is twice the code of Java &gt; It's also twice as fast as Java :-) Until someone contributed [a faster Java program](http://shootout.alioth.debian.org/u32q/performance.php?test=revcomp). (And the runtime for reverse-complement is so short that JVM startup cost should be considered.)
I think we need a 501(c)(3) org for Haskell.
We're working on a plan to have a non-profit to cover haskell.org, and hopefully the IHG as well, in place in the new year. Stay tuned for news.
["... will create only one TreeNode and 2 thunks ..."](http://shootout.alioth.debian.org/u64/program.php?test=binarytrees&amp;lang=clean&amp;id=1#about)
&gt; Also, the C++ one appears to be cheating... *Cheating! Not fair!* Wouldn't it be more interesting to consider what makes [this Erlang program](http://shootout.alioth.debian.org/u64q/program.php?test=binarytrees&amp;lang=hipe&amp;id=2) faster than [this Haskell program](http://shootout.alioth.debian.org/u64q/program.php?test=binarytrees&amp;lang=ghc&amp;id=1)?
The regex-dna program [now fails to compile](http://shootout.alioth.debian.org/u64q/program.php?test=regexdna&amp;lang=ghc&amp;id=2#log) due to - regexdna.ghc-2.hs:75:3: Precedence parsing error cannot mix `$' [infixr 0] and `using' [infixl 0] in the same infix expression 
Does this help? http://www.cs.nott.ac.uk/~gmh/solutions.pdf
I'm happy to help and I'm pretty sure you can go onto the #haskell IRC channel any time and find someone to "grade" your solutions. The reason why Haskellers love helping newbies is that they fondly remember the time their own mind was blown.
Honestly this is absolutely my favorite part about Haskell. Even if I never use it for any "real" work I think I will always appreciate how many times my mind has been blown (even just this far) in learning it. Lisp is the only other language that has ever really done that to me at all, and it was much less frequent. Also would you mind pm'ing me your email and I can send you what I have done? I guess I am a bit awkward about just going on #haskell, and posting my solution after solution, though I know that they are very helpful and it is one of the best channels I have ever been in.
It does, thank you. I guess for some of these though I might benefit from human feedback, or a pointer of where I went wrong or a critique of how what I wrote could be optimized. But thank you; I (stupidly) was unaware of that links existence.
Just put your source to GitHub and I'll comment there.
But the database seems really outdated...
It's not a live mirror. This is a snapshot from August.
Any update about time (I know it was written 'around')?
It's back up now.
hlint is a good start, it won't grade your algorithms, but can give you suggestions on improving your code.
This is a way to write it using the [enumerator](http://hackage.haskell.org/package/enumerator) package. import Control.Monad.IO.Class import Control.Applicative import Control.Monad import System.FilePath import System.Directory import System.IO import System.Environment import Data.Enumerator hiding (map) throughFiles :: (MonadIO m) =&gt; FilePath -&gt; Enumerator FilePath m b throughFiles fp = step [fp] where step (d:ds) (Continue k) = liftIO (get d doesFileExist) &gt;&gt;= k . Chunks &gt;&gt;== \s -&gt; (liftIO (get d doesDirectoryExist) &gt;&gt;= \ds' -&gt; step (ds' ++ ds) s) step [] (Continue k) = k EOF step _ s = returnI s get f filterFunc = map (f &lt;/&gt;) . filter flt &lt;$&gt; getDirectoryContents f &gt;&gt;= filterM filterFunc flt "." = False flt ".." = False flt _ = True printSizeAndName :: (MonadIO m) =&gt; Iteratee FilePath m () printSizeAndName = continue step where step (Chunks []) = continue step step (Chunks xs) = liftIO (psize xs) &gt;&gt; continue step step EOF = yield () EOF psize xs = mapM_ (\x -&gt; withFile x ReadMode hFileSize &gt;&gt;= \s -&gt; putStrLn (show s ++ " " ++ x)) xs main = do (x:_) &lt;- getArgs run_ $ throughFiles x ==&lt;&lt; printSizeAndName It feeds the files one dir at a time. **UPDATE:** a fix
I've got to second this, please hop in IRC and join #haskell, it's pretty much the best resource for learning haskell.
I won't be in a position to test this for a little while, but if you're interested in the problem at all, I'd recommend checking this for space leaks. All the Haskell-ish solutions people proposed, and every variation of them I could reasonably derive, has ended up using something like 1.5 GB of memory in the course of walking my laptop's filesystem. "du" or "find + perl" approaches use much less, and a very short C program using the fts_open() family of calls naturally uses next to nothing (even when I abuse the fts calls somewhat by interleaving them with a recursive function call per directory in order to keep track of path names and other data on the call stack, something like this: struct fsstats handle_directory(FTS *traversal, FTSENT *dir) { ... while (entry = fts_read(traversal)) { switch (entry-&gt;fts_info) { case FTS_D: chdir(entry-&gt;fts_name); merge_stats(accum, handle_directory(traversal, entry); break; case FTS_DP: chdir(..); return(accum); ... } } ). 
Yes you were right, it does have a pretty big leak. I'll try to pinpoint it 
But even with a formal committee, in 2010 it is a strange decision to host so much on a singular, known computer. Fixed computers have fixed networks and fixed power; they fate share with other computers on the same rack, in the same facility, and on the same uplink. When a fixed computer dies, everyone must wait for it to be fixed or replaced. It just isn't the right answer for serving something as small as hackage and haskell.org, the reliability / maintenance probabilities are all wrong.
I think you're missing my point: we've inherited donated infrastructure, sometimes more than 10 years old. The team is now working on making the kinds of decisions that ensure long term stability. Redundancy for Hackage is on that list; however, more pressing problems are getting tackled first.
Fair enough. But seriously, give me a way to subscribe, and I'll chip in.
Encorporating a legal entity from a community of interest is non-trivial, but we're trying.
[This](http://i.imgur.com/zHiZD.jpg) is the memory profile output for a big tree. It goes through the symlinks though, so if you have a symlink that points to '.' it'll cause a space leak. 
Shouldn't this headline be tagged with [monadic spoiler alert]?
I'll do that when you start calling the list monad the free monoid monad. :D
Is there a "Haskell for the Computer Science-impaired Mathematician"? :3
If it isn't a downloadable executable for windows with a graphical installer, I won't use it
1) a monad is any data type that has an instance in the monad class 2) 'do' notation is just syntactic sugar and is translated to other equivalent haskell functions by the compiler (bind, lets, lambdas etc) 3) all monad analogies are stupid
I've had some success by building a fingertree of bytestrings, and requiring that leaves have no utf8 tail bytes or if they do have any, that they be limited to a given size. Then indexing is O(log n) worst case and O(1) best case -- and then you can index by bytes or characters, or even find the 1st non-ascii character quickly with fingertree splits. The cost is a second number in the monoid of the fingertree counting utf-8 tail bytes. You already typically have a case when consing onto a rope as fingertree of bytestrings that has to check to see if the left finger points to a 'small' fragment anyways. checking if tails == 0 and length &lt; 32 isn't an appreciably more complicated process, and replaces a linear time search with an O(1) or O(log n) one.
For an arbitrary mathematician who wants to learn some programming, incidentally in Haskell? Or for a category theorist specifically interested in how it's used in Haskell? I don't think either one exists, but the ideal approach would likely be different for each, and the former probably much closer to standard introductions to the language.
wooow So many people in #haskell! i just got around to using IRC and i'm hella impressed how did i live without all these channels?
done: https://github.com/NaLaurethSulfate/Programming_In_Haskell. Thanks for the great suggestion.
Well, I'd really like to have the denotational semantics fully laid out. Definition of a monad, etc. is no big deal for me, which is what everyone lingers on.
You install it via the Eclipse built-in plugin installer (Menu Help -&gt; Install New Software...). So it's downloadable and graphical.
I use only haskell-mode. Perhaps it's only me, but it just doesn't bother me to click back and forth between emacs and a separate ghci window. Also, I have enjoyed auto-completion in IDE-style environments for other languages, and I suppose it would be nice for Haskell also. But it just doesn't seem worth the trouble. I *like* the regular old simple emacs environment, and I don't like cluttering it up with all kinds of add-on machinery.
Do you feel comfortable developing medium to large sized applications this way? I'm asking, because at least for me it's a time-saver to have "IDE-style" features like for instance jumping to definition sites of a symbols under point, especially the larger the code base gets, my brain behaves more and more like a FIFO buffer ;-)
You can get the jump-to-definition functionality with etags (for top-level functions). I put: hasktags `find . -iname "*.hs"` into my build script, then bind a key to find-tag, and pressing the key jumps me to the function definition (at least, where it was when I last hit build, but that's fine for me). The only problem I have is that it doesn't properly understand apostrophes in names, so if you define foo and foo' and attempt to use find-tag on foo' it will take you to foo. Usually they're defined right next to each other, though. I use dabbrev-expand to provide auto-complete. It's not the cleverest solution and only uses completion from open buffers but I find it works well enough. It has the downside that it will use words from your comments to make the expansion when you're writing code, but it has the upside that it uses words from your code to make the expansion when you're writing comments :-) (Seriously: very few IDEs provide that, even though you usually use a lot of function names or parameter names in the comments.) To answer the original question: I use these two features with haskell-mode, with customised auto-indentation. I use compilation-mode with cabal for building, and don't use GHCi.
fascinating, nowadays you have highlight that it's not in haskell if you post FP-relating links on reddit
Er, mentioning that something isn't written in Haskell when posting it to /r/haskell? You're right, that is quite peculiar.
You're welcome to use my branch of scion for emacs too. What happened is that I made changes and extensions to use in EclipseFP, and I didn't sense a lot of interest to push these back to the main scion, so they just stayed in my branch that gets bundled with EclipseFP. I would love a collaborative effort on improving and enhancing scion so that not only EclipseFP but other IDE could use the same version, but I had the feeling nominolo didn't have the bandwidth to really push it.
Well, it's posted in /r/haskell
I use emacs-mode and [ghc-mod](http://www.mew.org/~kazu/proj/ghc-mod/en/) which provides (among other things) on-the-fly error highlighting (either GHC or HLint warnings and errors).
So, more like content [along these lines](http://en.wikibooks.org/wiki/Haskell/Denotational_semantics) but assuming that the reader already knows what domain theory and such is all about? Something with more/less detail? That sort of thing intermixed with "how to actually write programs in Haskell"? Or something else entirely?
My favorite quote: "Character A doesn’t attack Character B — it’s just that as part of the evolution of the world, Character B’s connections to Character A factor into the determination of a new state in which Character B has fewer HP." I would add, and this makes it easier both to reason about the code and to test the code.
How do Scion and ghc-mod compare to each other? (in case you had the chance to try both) The way I understand ghc-mod, it's a one-shot executable, wheras Scion is a long-running process. So I'd expect ghc-mod to be slower for larger source-trees, or does it cache its results from one execution to the next?
Yes, pretty much exactly what you've said (assume Scott's work, more detail, more thorough, practical examples, etc.). The wikibook is okay, but its far from covering everything you'd want to know about Haskell.
Is the profile from the original you posted, or did you revise it to resolve the pretty big leak you mentioned?
I'm currently doing a rather large restructuring of the internal architecture. The result will hopefully be more robust and flexible, but it's quite a lot of work to get it up and running. Once that is working I will integrate your changes, though probably with some modifications/cleanups. My advice to anyone planning to run Scion+emacs is to wait for the next release. Meanwhile, if you want to try it out now, though, you may want to use [Bryan's branch](https://github.com/bos/scion) because he made some improvements of the emacs mode.
I revised it. The spikes are folders with many files. Check it, my tree might not be big enough for it to use a lot of memory. 
Interesting. I think the hardest part might be finding someone capable of writing it at all; would need to be reasonably familiar with Haskell as well as know how to write for a mathematical audience. While I certainly spend enough time explaining Haskell to people, someone who knows abstract math well but whose only programming experience is hacking together stuff for their university web site or using some horrible scripting language in a computer algebra system or whatever would be a strange reversal and I have no idea where I'd even begin.
ghc-mod (if you use auto-complete.el for completion) is great for completion of library functions (it only loads names from imported modules), but has issues (== doesn't work) with qualified imports, or modules from the current project. things like jumping to hackage docs work, but local doc also have issues, it thinks that Data.List comes from base-3, and local docs only exist for base-4. templates (module X where) don't really work, because usually you use hierarchical modules, so you get the wrong module name. inserting dummy definitions didn't work reliably for me and I'm not a fan of flymake-mode. ghc-mod has the advantage of being fast and really small (easy to understand codebase). I wanted to contribute, but I realised that the right way is scion. scion. I tried nominolo's version from git-hub and didn't work at all (it looked like all that elisp code was in unusable state - I couldn't even make emacs accept a symbol to act on, &lt;RET&gt; was broken in minibuffer). I decided to have a look at scion's haskell (I hate reading elisp) sources, expecting total crap, I was really surprised, the code looks great, and the only glitches seem to be the result of apis of ghc and cabal. as soon as I have some free time I plan to contribute to scion.
I use haskell-mode (..I would), with pabbrev-mode for tab completion. Seems to work out pretty well, and I've got hotkeys in xmonad for switching to other windows, i.e. the GHCi one.
Didn't work for me.
This is not surprising as a recruiter's view. The unsaid part is the real news: many programmers hold the same views. A recurring theme in /r/programming, for instance, goes something like this... &gt; I've done all the right things and am proficient in Java and C#, but I can't get any of the cool jobs and always end up making boring corporate web stuff. ... I'm interested in learning &lt;cool language&gt; but on monster there weren't many jobs. Help me get out of my boring corporate rut! The largest number is in the middle of the bell curve. If you're happy there then great.
I am skeptical. That said, I don't have hard data to back it up, but I don't think this is so clear cut. Say you want to hire some Java programmers: you'll find plenty of them for sure, but you'll also have to sieve through the large number of the ones who bought a 'teach yourself Java in 2 hours for dummies' book and claim to be programmers as a result. Might prove costly for a small startup with limited resources. Try to hire someone who knows something less popular (and a tiny bit harder to grasp) like Haskell, and I am willing to bet that the signal to noise ratio is going to be a heck of a lot better (and it might be easier to spot fakers as well). But what do I know, I am not a recruiter.
This week I am interviewing some candidates. One I picked because (among other reasons) he put Haskell as something he'd learned. No claims of work experience with Haskell, mind. Mind you, we're not using Haskell (yet), but just putting something interesting down was a huge bonus.
&gt; The basic stuff like moving is easy. But all of that refers to ways in which a single character changes based on his own actions. What happens when his actions don’t determine a new version of himself, they determine a new version of someone else? At that level of granularity, the model breaks down. I'd say his actions don't determine a new version of himself, but rather his actions plus his surrounding environment determine a new version of himself. I hear the ringing of a comonad somewhere.
My interview/resume count is still only 50ish, after basic filtering from other levels of the company, but I'm yet to see somebody who has Haskell or something similar on their resume who _isn't_ kickass, and the only ones we've failed to hire are the ones who find better opportunities elsewhere, not because we decided against extending them an offer. Even just seeing Python (barring getting Python taught in a class) is damn near an instant "hire". Not that I would actually do that, of course, still have to talk to them and stuff, I'm just observing the outcome.
What language is that? I'm guessing OCaml, but I can't be sure.
I presume it is F#.
I'm equally unsurprised, but here's what I think the unsaid part is: anything that makes placing candidates easier (more homogeneous skill sets, larger candidate pools) is preferable to the recruiter.
Using only Indeed.com job listings as a metric, C++ is a better choice than everything else on that list except Java. Using his LinkedIn logic, C++ is the #1 choice out of all of them. As a full-time C++ programmer for more than 5 years, let me tell you: I would never recommend it for a new project unless the conditions of the project made it absolutely necessary. The majority of people putting it on their LinkedIn profile under proficiency or expertise are probably full of shit, and just because there are a lot of job openings in some language doesn't make it a good choice for future projects. There are tons of job openings for MUMPS and COBOL too. Guess how many people are starting new COBOL projects? **TL;DR** Using arbitrary page rankings to decide what language to use for a team project instead of which language is best-suited for the problem is flawed logic and puts the cart before the horse. Subsequently filtering out the most popular of those page rankings to fit your conclusion is also extremely dishonest.
Paul Graham calls that [the Python Paradox](http://www.paulgraham.com/pypar.html). &gt; if a company chooses to write its software in a comparatively esoteric language, they'll be able to hire better programmers, because they'll attract only those who cared enough to learn it. And for programmers the paradox is even more pronounced: the language to learn, if you want to get a good job, is a language that people don't learn merely to get a job. Personally, my feeling is that the time I have invested in Haskell has made me more kickass than I would otherwise be (which still leaves me a long way away from being *actually* kickass). 
Seems like a good way to filter out recruiters with silly ideas would be a quiz on apostrophe use. Choosing a language should probably be done in collaboration with people who know how productive each of the languages will be for solving the problem at hand. "# of developers available" isn't the only variable here.
This is consistent with my experience.
This is great advice if you want to optimize for mediocrity, and I don't think I've ever seen a better example of the differences in goals between recruiters and employers. The numbers he uses do not represent disjoint sets, but rather overlapping interests. If you are an employer looking for top talent then you should seeking out points of differentiation, aiming at the largest group is counterproductive unless you are in an incredible hurry (in which case you probably have other bigger problems.) 
I was looking for something like this not long ago.
I like how many of the Haskells in the job search seemed to not be for Haskell the language. For instance a job from Monsanto in Haskell, Texas.
Some feedback added. Another suggestion, set up a [gitit](http://gitit.net) wiki where literate haskell is supported so much better than GitHub.
Never trust a salesman's opinion on quality. The entire concept of recruiters is fucked up.
thanks for the suggestion I had no idea this existed, and it will make life much easier!
Ocaml values cannot start with a capital letter.
Unfortunately, while many programmers would be interested in learning new languages and related skills, that does not make them a good candidate for a job that requires programmers who already *have* those skills. I think this is the point the recruiter was trying to make: being the company that takes on unskilled people, pays to train them, and then sees them leave for better money/higher profile/whatever just makes you the only party in the system who loses.
But this is also a classic difference between developers (particularly the geeky kind that follow specialist programming forums on sites like Reddit) and good industrial managers: the managers understand that as much as they would like to hire only top-notch people, the reality is that there aren't many such people in the job market. Unless they are a manager at the kind of company whose name we all know, they are unlikely to be in the position of snapping up the best people at will. They run their business accordingly. (See also: office environment; software development process; tool selection; strong interest in job applicants' non-technical skills.)
&gt; being the company that takes on unskilled people, pays to train them, and then sees them leave for better money/higher profile/whatever just makes you the only party in the system who loses. So be a company that doesn't lose those people. Be the destination they want to arrive at instead of a way for them to get there. There's no easy, 100% solution. But if you ask yourself how you get better people, do more and better work, etc., the answer almost certainly will *not* be to use the most popular languages, tools and methods, and to hire people who know little aside from those. Stop trying to figure out how not to lose. Instead figure out how to win.
I wonder how many of the Javas are for positions at Starbucks.
Also the case: if you're a recruiter, you don't make money by finding the very best candidates for positions. You make money by convincing your clients to settle for the people you have. If you inspire your clients to insist on the best programmers they can find, you end up with an unemployable candidate pool, while the people your customers really want don't have to look for jobs through recruiters.
The CYA method, in other words.
Not about Haskell. Barely mentions Haskell. I see signs lately that `/r/haskell` is becoming `/r/programming` and I'm not happy about it.
Yay!
you mad bro?
Stupidest thing I've read in a while. Employees choose jobs with mainstream languages to have transferrable skills? I must be really stupid for learning new languages and frameworks, then. That Scheme and Haskell sure made me a worse PHP programmer. 
Yep, there's definitely comonads involved in this type of game. Ages ago I started writing a roguelike in haskell, wherein the world was represented by a 'Dungeon' comonad. Moves were determined using a BiKleisli arrow 'Dungeon Actor -&gt; IO Move', then a function took a Dungeon containing Moves and sequentially applied them outwards from the players location. It all worked rather nicely, but I never got around to finishing it.
Is it possible with these combinators to run multiple IO actions in parallel if the IO actions have different types? Let's say I want to launch these hypothetical actions in parallel and collect the results: updateIssueTracker :: String -&gt; IO Either String URL testPatchBuild :: FilePath -&gt; IO Either String Bool Can I use this package for running the two actions in parallel, and collect their results? If parallel-io can support this, great. Otherwise, I could use a package like [Orc](http://hackage.haskell.org/package/orc).
it is.
I have not used scion. ghc-mod has never been too slow for me. I think it only compiles the current module when you change it so I don't think project size matters.
If he wants the answer I think he does then this highlights the difference in thinking between recruiters and programmers: &gt; Would you rather take a programming job paying $X.year on a project you love or a $X+30K/year on a project you merely like? Would you spouse's opinion matter? Me? The first one, hands down. My spouse would rather I'm happy, and so would I. As long as X is enough to get by on, that sounds amazing to me.
Do you mind elaborating on that a little? I think I understand that comonads tend to be useful representing the idea of a context and a pointer within it. But I have been wrong about stuff like that before. Do you mean one comonad that represents a world and all the things in it? Or are there several discreet comonads running around?
What would you expect the type of "collected results" to look like?
it should *depend* on the types of input actions. hint hint.
Very loosely speaking, this game is like a cellular automaton. In a cellular automaton, a cell's next state is the based on it's current state and it's neighbouring states. One can use the [`Costate Integer` comonad](http://www.reddit.com/r/haskell/comments/ed184/survey_new_name_for_the_costate_comonad/) to represent a 1-D world for cellular automota. If each cell can be either on or off, then the world with selected cell would be a value of type `Costate Integer Bool`. One the can write a local update function of type `f :: Costate Integer Bool -&gt; Bool` which says whether this selected cell will be on or off in the next state. Then this local update function can be `extend`ed to a global update function of type `extend f :: Costate Integer Bool -&gt; Costate Integer Bool`. So ya, there is one comonad that represents the world and all things in it, with one particular thing selected. Edit: replaced `Costate Nat` with `Costate Integer` in my example.
Are you hinting at dependent types? 
&gt; So be a company that doesn't lose those people. That's easy to say, but very difficult to do. In our industry the workforce is highly mobile, and more so the higher you go up the talent scale. There is something of a catch-22 situation here, I think. Given that there is only a relatively small number of developers who have certain niche skills, it seems most likely that relatively small, flexible companies will be the ones to try innovative technologies first. However, by their nature, those same companies also tend to lack both resources and diversity, and those are the things you tend to need to keep good developers interested over the long term. And of course, people often leave jobs for reasons that are unrelated to the job itself anyway.
One way to model it would be using tuples and different variants for each arity, e.g.: parallel2 :: IO a -&gt; IO b -&gt; IO (a, b) parallel3 :: IO a -&gt; IO b -&gt; IO c -&gt; IO (a, b, c) That wouldn't be elegant, but would achieve what I need. What I really want (as a Haskell newbie) is a solution for the general problem of sequencing heterogenous operations. I'd encounter the same problem using Monad.sequence, but if I was in a monad I could use do notation and bind each result individually. Maybe I need a monad for sequencing concurrent operations, which sounds like Orc.
depending on types doesn't need dependent types (those are for depending on values). I'm hinting at fundeps
Could you give more details about what failed?
Where's [nothaskellnazi](http://www.reddit.com/user/nothaskellnazi) when you need him? He's a lazy bum, that one...
That's very clear. I saw your original post but I didn't see how to apply until now. So a start on a the costate game might take the simple Costate Nat Bool and go to a 2d state with tiles. data Tile = Wall | Walkable Character | Door Locked data Character = InterestingStuff newtype World = World (Costate (Int,Int) Tile) That's really elegant. Thanks for taking the time to apply that for me. My intuition (I think wrong) so far is that the Costate type is a very useful variation of the State monad. However I know that's wrong right away because State is a monad and Costate is a comonad, making it a Different Thing(tm). I wonder how long it would take for my mental model of Costate as monad to break down if I tried to actually write a program with it.
The World isn't quite `Costate (Int,Int) Tile`. `Costate (Int,Int) Tile` is a world with a selected `Tile`. To get the world you need to forget which tile is selected. But, as you can see, having a World with a selected Tile is a useful construct.
Cool, I hadn't come across functional dependencies before. I can't see how they could help with sequencing and collecting the results of heterogenous operations though. Are you suggesting that the package author could use them in the API, or that the package user could use them in their consuming code?
I don't know what he does with the list of requests of type IO X (for some type X), but I believe that he doesn't do anything really specific with those Xs, so he could parametrize that code over the specific type, and make the api acccept a heterogenous list (tuple really), and return similar tuple/hlist of results. if you want to know more about fundeps, please read HList paper http://homepages.cwi.nl/~ralf/HList/
Sweet - thanks for the tip!
&gt; That's easy to say, but very difficult to do. Indeed. I'm afraid we're getting a little off-track. I'd like to restate a little bit: The points made by the recruiter have enough truth in them. But those points do not invalidate the flip side of learning uncommon languages. There are choices to be made by both companies and developers, and one size does not fit all. A developer interested in different programming models and esoteric languages may choose a non-mainstream career path, and probably should. A developer interested in working for large companies implementing big projects should probably focus on mainstream tech. Companies doing large projects should pick tech that they know will be feasible, both for implementation and personnel reasons. Companies doing small, exploratory projects can afford to choose from a wider selection of tech that best fits the problem domain, team dynamics, developer interests or whatever. Perhaps we can agree on the above? A problem I see often, and brought up before, is some corporate developer using j2ee or whatever *wants* to be using cool languages and working on some cutting edge project, but talks themselves out of it because it's not *marketable*.
&gt; Perhaps we can agree on the above? Yes, we can. &gt; A problem I see often, and brought up before, is some corporate developer using j2ee or whatever wants to be using cool languages and working on some cutting edge project, but talks themselves out of it because it's not marketable. That is indeed regrettable. As you say, one size does not fit all.
There's also: parallel2 :: (a -&gt; b -&gt; c) -&gt; IO a -&gt; IO b -&gt; IO c
According to your definition, `f &lt;= g` holds iff the domain of `g` includes the domain of `f` and both functions agree on the domain of `f`. First I thought it is equivalent to f &lt;= g := for all x f(x) &lt;= g(x) but it is not. For example \_ -&gt; [bot] &lt;= \_ -&gt; [()] according to this definition but not according to yours. Is this intended?
Sweep algorithms are also one of the reasonable solutions for Klee's measure problem (in higher dimensions)...
Your definition is correct, mine is wrong. I’ll amend it accordingly.
I have similar experiences. In difficult interview questions, the only candidates making it through it all in time usually write in python, ml, or even erlang. Great candidates even switch languages from one part to the other, instantly recognizing the inherent strength of the various languages. For example, a problem might seem to require a recursive solution. The candidate starts writing in ML where this can be expressed succinctly. Then there appears a better solution that is better expressed using mutable state in python. Languages embodies a way of thinking, and it is the efficiency of the thought process that defines a good candidate. Actually *writing* code is trivial if the thought process is clear.
It seems to have magically started working now. Thanks!
My experience on the #haskell channel would probably qualify as one of the most rewarding experiences I've ever had on irc. No attitudes, and a lot of really helpful people, especially to newbies. 
I've... I've got a baby panda that I'm very fond of and don't want to see it coming to any harm.
Yes, I was thinking about building a program like this myself and now I don't have to. Thankyou Michael.
It seems to be ignoring "Enable most language extensions" (-fglasgow-exts) in Window -&gt; Preferences -&gt; Haskell -&gt; Haskell Implementations -&gt; GHC Options. I have a file that I can run without errors in GHCI with -fglasgow-exts put in to the run configuration under Arguments, but an error still shows up under "Problems" (I am using multi parameter type classes). Is this a problem with Scion or is there something else I need to do to get it to work? I have tried flipping the option multiple times and re-starting eclipse multiple times. Also, typing in characters is very slow (as in a character takes 1 or 2 seconds to appear after I type it) - is there something I can disable to remove this problem? Version: Helios Service Release 1 Build id: 20100917-0705 (under Windows 7 (32 bit))
And the [announcement](http://www.haskell.org/pipermail/haskell/2010-December/022436.html). This is the first public output of the [real world parallel Haskell project](http://www.well-typed.com/blog/48) that MSR is funding. 
 import Control.Arrow curry $ g *** h &gt;&gt;&gt; uncurry f
Interesting that we always end up back at MPI somehow :-). I used to implement parts of a commercial version in another life. Cool stuff.
and now it seems so obvious. Thanks!
I love haskell more everyday
How does this compare with Yesod's functor forms?
Indeed a good question. This piqued my interest for a number of reasons: * I liked the formlets library, but couldn't use it because of minor issues like inline error messages. digestive-functors seems to have addressed this. * Jasper writes good code. * I've been contemplating factoring out the form code from the main yesod package for a bit now, due to growing complexity. If it's possible for Yesod to jump on the bandwagon on this one, I'm all for it. Jasper, if you want to provide any insight for us, I'd appreciate it, but I will definitely be looking into your package in the next few days.
A frequent special case is `a=b` and `x=y`. [Data.Function.on](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Function.html#v:on) is made for it: compare `on` length :: [a] -&gt; [a] -&gt; Ordering This combination of `compare` with `on` is predefined as [Data.Ord.comparing](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Ord.html#v:comparing).
TIL that blogspot.com can't deal with a trailing period, which is in fact the correct fully qualified domain name. Poor form.
These are the types of Applicative-related things, so if you define a "parallel &lt;*&gt;", (|*|) :: IO (a -&gt; b) -&gt; IO a -&gt; IO b infixl 4 |*| f |*| x = do { v &lt;- newEmptyMVar; forkIO (f &gt;&gt;= putMVar v); xVal &lt;- x; fVal &lt;- takeMVar v; return (fVal xVal) } then you can define `parallel2 x y = (,) &lt;$&gt; x |*| y`, `parallel3 x y z = (,,) &lt;$&gt; x |*| y |*| z`, etc. Not sure whether you can implement this with a thread pool, though.
I hate comparing, how could you resist any chance to use `on`?
I think you might also be interested in reading [Pointless fun](http://matt.immute.net/content/pointless-fun) by Matt Hellige.
It's not that we want MPI as a programming style, but since MPI is basically the driver interface for high speed interconnects then we're forced to deal with it.
For those of us who don't know Ruby, what is the argument to Hash.new?
It's very similar to Yesod's functor forms, though more abstract -- I wanted a library I could use with Happstack, Snap, Yesod, loli... To integrate this with Yesod, I think you would just have to write a `digestive-functors-yesod` backend, and perhaps a frontend for something like Hamlet. If you come across any API limitations, let me know!
So far the only things I've noticed are: * It seems that the Happstack and Snap backends don't support GET forms, though that does not seem to be a limitation of digestive-functors itself. * I've been moving Yesod's forms to allow for a monadic interface in addition to the Applicative one to make it easier to create free-style forms. It's also a crutch for newer Haskellers who are more comfortable with do-notation than Applicatives. I'm not sure if that would fit in with digestive-functors. I think it's safe to say that there *will* be a Yesod backend and Hamlet frontend at some point, and a good guess that this will start supplanting the standard Yesod forms library.
This does not seem to define an applicative functor. I don't know about `Hash.new` either but with `pure _ = empty`, the identity law for applicative functors is broken : pure id &lt;*&gt; x = empty &lt;*&gt; x = intersectionWith ($) empty x = empty ≠ x
&gt; I've been moving Yesod's forms to allow for a monadic interface in addition to the Applicative one to make it easier to create free-style forms. It's also a crutch for newer Haskellers who are more comfortable with do-notation than Applicatives. I'm not sure if that would fit in with digestive-functors. A monadic interface is also possible in digestive-functors. I haven't implemented it yet because I think it could allow users to create "invalid" forms (duplicate field ID's), however I'm not sure, I need to do some more exploration here.
Hi, I'd just like to explain why the Wikibooks chapter was structured that way. Mathematics isn't all about formulas. It's about ideas. When definitions are made, they're made with a motivation in mind. The derivative of a function is the function whose value is the slope of the original function. We take this idea and ask how to find the derivative. That's where the formula comes into place. The actual formula doesn't matter as much, but what really matters is how it is derived and what it means. Rarely anybody uses the definition of the derivative (as a computer scientist, you could call the definition being abstracted away by our already known derivatives of common functions.) If you'd like to explore math, I'd recommend you take a look at Abstract Algebra. It sounds much harder than it is. Unlike calculus, which sometimes hands you formulas and theorems without any understanding of why they work (they also assume you know what functions are, which many people mistakenly think they know), abstract algebra starts with nothing but the notion of a set and builds everything straight up from the ground. Personally, I feel more comfortable when I know how something is derived when I am learning it. If asked, I could re-derive many key concepts in a first-year algebra course - I think this shows true understanding and is much more fulfilling than knowing the superficial knowledge of carrying out algorithms. A computer science analogy to this would be understanding everything required to run a program: the actual program, the virtual machine that is running the program, the operating system, and the low-level machine code versus simply knowing how to use the program. I hope this helps. Good luck discovering math! Keep in mind that math is about beauty in ideas. Calling trivial things mathematics (such as calculating bills, finding tips, or even solving circuit equations) is like painting a room or a fence and calling it art.
Conal has a very nice paper: [Denotational Design through typeclass morphisms](http://conal.net/papers/type-class-morphisms/). In it, Conal describes a "TMap", which is a total-map, a simpler variant of Map, which is total rather than partial, and is semantically equivalent to a total function. Since it is semantically equivalent to a total function, its Applicative instance is also clear: It is the same one that functions have. The way I view TMaps is as being a combination of a "background value" and an actual map. If you want something like Map, you can use a TMap with a background value of "Nothing", and all other keys are wrapped with just. Then you have: instance (... k) =&gt; Applicative (TMap k) where pure = TMap.const (&lt;*&gt;) = -- something like intersectionWith ($) between the TMaps. See the paper for the actual instance... Other type-classes (e.g: Monoid) work really well and simply too, especially when combined with newtypes like First and Last. Another nice thing is that we know all of these type-classes are correct/follow the laws, because they behave exactly like the semantic model of TMap, that is: A simple total function! This is the main point of the paper as I see it...
So it's worth giving the correct URL for convenience: http://learnmeahaskell.blogspot.com/
Why not feed the arguments before giving the actions to the parallel framework? That way you have: `IO (Either String URL)` and `IO (Either String Bool)` I guess I'd approach it with something like: parIO :: IO a -&gt; ParIO a And then if ParIO has an applicative instance, you can just use: liftA2 (,) :: ParIO a -&gt; ParIO b -&gt; ParIO (a, b) Or any other combinator to combine a bunch of ParIO computations into a single one with a type product or whatever kind of combination in it. And eventually: execute :: ParIO a -&gt; IO a So to tackle your example: do (trackerRes, otherRes) &lt;- execute $ liftA2 (,) (parIO (updateIssueTracker "tracker")) (parIO (testPatchBuild "SomePath")) -- If you want to combine the Eithers: (url, bool) &lt;- liftA2 (,) trackerRes otherRes 
So, it's like a M.Map with a default value ?
Actually I also don't like `comparing` and prefer to use `on` especially in cases like groupBy ((==) `on` length) . sortBy (compare `on` length) where using `comparing` would hide the similarities between the calls to `groupBy` and `sortBy`.
What you want in this case is simply the Applicative Functor. You precisely do NOT want monadic-ness here, because you want `IO a` and `IO b` to execute in parallel, so a signature like `(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b` does not make sense because it implies you have the `a` before giving it to the latter action, i.e: an execution order. With Applicative, parallel2 is just `liftA2 (,)` and parallel3 is `liftA3 (,,)`. Look at my proposed solution above: http://www.reddit.com/r/haskell/comments/ehsqo/package_of_the_day_parallelio_combinators_for/c18jfwe
Why not newtype around it and use the real Applicative methods? A new type (maybe "data") could give the implementation more freedom too (e.g: an action that creates a thread-pool/mvars separate from another action to actually execute the code, or what not).
Conal's [Semantic Editor Combinators](http://conal.net/blog/posts/semantic-editor-combinators/) address this very nicely. abc :: a -&gt; b -&gt; c xa :: x -&gt; a yb :: y -&gt; b result :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c result = (.) argument :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; c argument = flip (.) xyc :: x -&gt; y -&gt; c xyc = argument xa . (result . argument) xb $ abc -- Read the above as: To the first argument, apply `xa`. -- To the argument of the resulting function -- (or simply: the second argument, due to currying), apply `xb`. -- All this is applied to 'edit' abc from `a -&gt; b -&gt; c` to `x -&gt; y -&gt; c`. A small extension of those, mentioned by RayNBow: [Pointless fun](http://matt.immute.net/content/pointless-fun) by Matt Hellige is ~&gt; which makes SECs much nicer here: infixr 2 ~&gt; f ~&gt; g = argument f . result g xyc :: x -&gt; y -&gt; c xyc = xa ~&gt; xb ~&gt; id $ abc -- Read as: -- apply xa, xb, and id to each of a, b, c in a -&gt; b -&gt; c, respectively. -- Do this on the value abc. alternatively, going Arrow (actually Category) style: result = (&lt;&lt;&lt;) argument = (&gt;&gt;&gt;) (f ~&gt; g) x = f &gt;&gt;&gt; x &gt;&gt;&gt; g 
Hahaha. I'm such a useless idiot. How did I not think to do that? Cheers.
Yeah, but that has some (perhaps surprising) consequences: * It means it has the semantics of a simple function * It means it can inherit the meaning of all type-class instances from those of functions * It simplifies the documentation makes the code more composable And probably more...
The Dual numbers is actually the same idea as in the Dif numbers, but the Dif implementation has a lazy infinite tower of derivatives. Also, you can do symbolic derivation with Dif if you want to $ ghci Prelude&gt; :m +Data.Number.Dif Data.Number.Symbolic Prelude Data.Number.Dif Data.Number.Symbolic&gt; let f x = x*x Prelude Data.Number.Dif Data.Number.Symbolic&gt; deriv f 5 10 Prelude Data.Number.Dif Data.Number.Symbolic&gt; deriv f (var "x") x+x 
very cool, good luck!
I disagree. In anything but a calculus book, the motivation for the definition typically comes after the definition -- with a few exceptions. There are the things that we don't really understand the reason behind the definition. For instance, if you look at old expositions of Hadamard's finite part (or [Wikipedia's account](http://en.wikipedia.org/wiki/Hadamard_finite_part_integral), it seems to be ancient enough...), they usually have some motivation, "assigning values to integrals of functions with hypersingularities," and then the definition. After we discovered the reason for the Hadamard finite part, that it is the meromorphic continuation of the functional determined by the integral Hadamard was trying to evaluate, the order of things changed. (I learned this first from [this short note](http://www.math.umn.edu/~garrett/m/v/snake_lemma_gamma.pdf)). Notice that here, the definition comes first, and the motivation later. **TL;DR**: The kind of exposition you advocate is appropriate when the reason behind the definition isn't yet known. After the theory falls into place, definitions ought to come first. In the case of the derivative, we've had centuries to figure it out.
Quick question: is there any way to get HWN back up on planet haskell? I miss it there. Getting it on haskell reddit would suffice as a workaround, if not permanent enhancement. Seriously, I think your excellent curating are not being valued enough simple because of its low visibility.
It is also worth checking out edwardk's http://hackage.haskell.org/package/ad
the default value to return when looking up an unrecognized key. So `Hash.new(3)[ :whatever ] == 3`. There's also a block form, which lets you pass an anonymous function which is called with the hash table and key when lookiing up an unrecognized key. ht = Hash.new do |ht,key| puts "looking up #{key} in hashtable #{ht.inspect}" 3 end ht[:whatever] #=&gt; prints "looking up :whatever in hashtable {}" and returns 3 ht[:whatever] #=&gt; prints "looking up :whatever in hashtable {}" and returns 3 The block form also allows you to modify the hashtable, which is handy for caching: ht = Hash.new do |ht,key| puts "looking up #{key} in hashtable #{ht.inspect}" ht[key] = 3 # cache the value end ht[:whatever] #=&gt; prints "looking up :whatever in hashtable {}" and returns 3 ht[:stuff] #=&gt; prints "looking up :stuff in hashtable { :whatever =&gt; 3 }" and returns 3 ht[:whatever] #=&gt; just returns 3 
&gt;...but if it is true, there will be an endless variety of proofs, not just one! &gt;Mathematical facts are not isolated, they are woven into a vast spider’s web of interconnections. Would be nice if that were true under all circumstances -- unfortunately, it is not. There are logical and mathematical truths which are unreachable (and can be proven to be so).
It create's a default value. For a ZipList we have instance Applicative ZipList where pure x = ZipList (repeat x) ZipList fs &lt;*&gt; ZipList xs = ZipList (zipWith id fs xs) A default value for a map is similar to (repeat x) in the ziplist. intersectionWith for maps is like zipWith for lists. EDIT: some motivation (|+|) = M.intersectionWith (+) (|&lt;-|) = flip M.union -- Takes advantage of left biased union updateMovers :: World -&gt; World updateMovers w = w { position = position w |&lt;-| velocity w |+| position w } If we have maps from some EntityID to their components of velocity and position, we can use the intersection of all objects that have both to compute new positions with intersectionWith (+). An applicative functor would generalize this but unfortunately there is no way to write pure for a standard map.
Just posted a comment on Jasper's blog. Repeating here for discussion: -------------- Thank you for the really interesting package and a great post! I've been thinking about real world usage for digestive-functors since you made the post yesterday and a couple of potential open points have occurred to me: - In production usage, web app developers will likely want to design/customize the markup of each form individually. They might want to use tables or floating divs instead of label/input pairs. They might want to add buttons, images, etc. Any idea how this can be accomplished with digestive-functors? - How would you handle nested collections / sub-forms? An example case would be a person's contact form that can have any number of phone number fields. Perhaps there is a JavaScript-based add button on the form that adds additional empty phone number fields. Could this be incorporated into digestive-functors? That's all the feedback I have for now. Thanks again for the great contribution!
There's a lot of code that's been written directly to the MPI specification at the C level. So being compatible with that isn't really all that bad. That said, I know a lot of people have really been looking to create something a bit higher level for some time. MPI has some in-built problems with fault tolerance for example. A failing communicator with thousands of nodes is broken if one node should die. Unfortunately the MTBF goes down as the number of nodes goes up, so as scalable as MPI is, it's also fragile. There's been a few proposals over the years to fix this, using job snapshot and restart, or even fault tolerant communicators with redundancy etc, but I'm not sure a "sweet spot" was ever really hit, and it's a real pain in the butt to restart processes and rebuild TCP connections, but the company I worked for did all those things. It was really fun! Different infiniband implementations used to conform to the Mellanox set of verbs, but Myrinet used an API called GM, so I understand why one would think that MPI is the lowest portable level for high speed networks. It's not necessarily so, or does not have to be :-). In fact there's even a light weight layer designed by some very bright folks at Sandia National Labs in New Mexico called Portals (http://sandiaportals.sourceforge.net/), that was used as the base API for the Lustre parallel filesystem (in an adapted form) to work across many networks. I'd almost want to start somewhere near the Portals layer if I had the time to do this myself. It might be more work overall, but it is a bit lighter weight than MPI, and might provide new opportunities to deal with some drawbacks of the MPI standard. But as I said before, being compatible with MPI codes is not a bad thing, and you'll get some mindshare around how that API works, but for higher level Haskell abstractions. I find the whole thing moving into the Haskell space to be very cool! :-)
I think this ordering comes up because of how calculus is usually taught: as a required course for high school students or undergraduates who aren't particularly interested in mathematics. From my own experience teaching, students have varying ability to memorize definitions and formulas, but never really gain any facility at problem solving unless they understand the motivations and ideas. If you present the formula first, a significant number will completely ignore any motivation, and just memorize.
Read about [the source of that quote](http://en.wikipedia.org/wiki/Gregory_Chaitin), and tell me whether you think that might have slipped his mind. Not to mention the book it came from.
I was also thinking about the same thing. While this looks great for people who want to do web design inside their source code, the blaze front-end is unusable for people like me, whose web pages all changed from gray to white only when Netscape changed their default background color. It's unreasonable to expect web designers with no interest in Haskell to pick up blaze for a project. It looks like the "form" concept in digestive-functors has some dependence on the idea that you are building HTML programmatically. Likely what's needed is a lower level, and front-ends for stuff like HStringTemplate (resp., Heist) could be implemented to just give a set of attributes (resp., splices) from which it would be easy to place the individual components of the form. Perhaps that can be teased out of the existing form support, so concerns that only apply to generating HTML from Haskell (positioning various subforms to the left and right, what CSS styles and classes are applied to elements, etc.) could be added cleanly on top of this base.
Haskell itself would do fine for pseudocode imho. Most FP itself is quite declarative don't you think?
Well, it's still wrong, so, apparently, yes? (To clarify, his statement is true in many circumstances, but provably not in all of them.)
I disagree. Definitions are opaque, and boring. Theorems are opaque. Proofs are boring (with a few exceptions), and often opaque. Motivation should come first, always, because it allows a little bit of transparency into the deep stuff; and only after that can the dry and boring definition/theorem/proof/goto10 stuff come, which (almost) everybody hates.
I just write regular Haskell, but leave the boring stuff out.
write the program on paper with a pen and say it's the psuedocode
That's exactly it. The blogger in the post, though, they are certainly interested in mathematics. There ought to be some text geared to that sort of student. I suppose there are some, but I can't think of any off the top of my head.
I absentmindedly looked through the slides and was confused that it apparently didn't have anything to do with galois-theory.
It's wrong by omission, and probably so for poetic reasons. "Mathematical facts (except those that cannot be proven) ... " doesn't have quite the same ring to it. Since the author is admittedly an expert, and the book deals explicitly with such pathologies, I think you're being a bit hypercritical.
Your assignment has a false presupposition. Haskell has no pseudocode, therefore the pseudocode for the entirety of your program is and is not 'Elephant'
&gt; It looks like the "form" concept in digestive-functors has some dependence on the idea that you are building HTML programmatically. This is not really the case. For example, there is a [CLI](http://github.com/jaspervdj/digestive-functors/blob/master/examples/Cli.lhs) which uses an entirely different approach. I believe a sensible backend is very possible, and I'd like to brainstorm about it on the Snap IRC channel in freenode some time.
My point exactly. Thanks.
I don't see any hypocritical about it. I just wanted to point out that this quote does not really convey the truth very well. &gt; If a mathematical statement is false, there will be no proofs, but if it is true, there will be an endless variety of proofs, not just one! To an unsuspecting reader this would seem like a pretty definitive statement, but interpreting it like that would be very wrong.
The purpose of psuedocode is to clearly show the algorithm being used behind the cruft of the programming language. Historically, the cruft of a computer language has come from a combination of dumb compilers (due to limits of resources available to run smart ones; try running GHC on your 8086), languages that required you to do a lot of low-level work, and cultures that did not value separating the low-level work from the algorithm. (It's usually possible in those languages, but not always necessarily _worth_ it.) These are all actually intertwined and related but that would be beyond the scope of this point for me to get into. When you can write clear code that shows the algorithm being used and it actually executes as is, the need for psuedocode as a separate entity disappears. Look at all the Literate Haskell posts. What you'll typically find in them is some clear specification of the core algorithm, *and* a set of "misc" code stuffed in either the beginning or the end that supports the clear specification. When that's all you need, you don't need much psuedocode. Python is pretty good with this as well, with its tradition of "executable psuedocode". I'd submit that functional languages don't have a psuedocode standard, because the functional language psuedocode standard is what those Haskell posts do; a bit of supporting code if it is necessary to help make the algorithm more clear, and then, just the algorithm. Rather than a special "psuedocode" standard, it's just a particular didactic coding standard, and there's nothing "pseudo-" about it.
Pseudo-code is great if you are working top-down. It was the rage when everything was procedural programming languages (pascal etc) Often functional programming evolves bottom up. But, pseudo-code is any abstraction that makes sense to the reader, so input --&gt; parsing function --&gt; semantic tree is viable pseudocode. Make something so that the reader understands the basic flow of your program.
**Hypercritical**, not *hypo*critical. "Excessively and unreasonably critical, esp. of small faults."
My experience with pseudocode is that writing it differently to how the professor writes theirs usually results in lost marks. In general though, when you see 'functional pseudocode' it's either mathematics or some less formal notation closely resembling the author's favourite functional language.
Ah, sorry. I didn't realize that was a word, and assumed you misspelled it. Well, I think it's an important point to make, and I don't really see any other obvious interpretation of the quote that does not include this error.
What I sometimes do--and I expect I'm not alone in this--is sketch out a Haskell program using a sort of "pseudocode" consisting of naive versions of data type definitions, a bunch of type signatures to give the general shape of the top-level functions, and a few of the key functions written out fully in a straightforward fashion, with the tedious parts represented by named-but-not-yet-written functions. The result isn't quite pseudocode in that it usually compiles, but isn't quite "real" code either because it mostly consists of things that look like this: descriptiveName :: (Applicative f) =&gt; Foo -&gt; (f Bar -&gt; Baz) -&gt; Baz descriptiveName _ _ = undefined 
I did notice the CLI example. On the other hand, I see a lot of stuff in the digestive-functors package (not in the blaze front end) that has to do with CSS and relative positions. Perhaps I'm misunderstanding, or perhaps those bits are just there as a convenience for the (eventual) several back-ends that may share them. As I see it, a decent implementation for Heist would be that a view type would be something like 'TemplateState m -&gt; TemplateState m', along with a bit of documentation that says, e.g., "if you want are writing a form for inputs of these types, here's what the corresponding HTML form tags ought to be, and what you should name them." A bit messy, yes, but that's likely to be inherent in the desire to design the forms in HTML separately from the web application; we're stuck with the kind of composition that HTML gives you. But as messy as that would be, I see it as being really important to make digestive functors work with external templating systems as well as HTML generation. For better or worse, 99% of web applications are written with a separate templating system for a reason. If that's a bad idea (though I don't think so), the path from there to a better idea would be to first get people to use a tool for form processing that will transition well to the supposed "better way". And digestive functors look like they would be excellent, even without the tight binding to HTML generation. The ability to compose the handling of form input in this way: with composable validation, error detection, and composition into higher-level types, this would make good form handling with Heist much easier, as well.
Any thoughts about client-side validation? What I'm thinking would be something like... a new variety of validator, which would implement only a limited set of kinds of validation (say, min and max length, regular expression, ...). It would act exactly like the existing server-side validations, except for the possibility that front-ends could specialize to also validate on the client. So, for example, the blaze front-end could write a chunk of JavaScript to do the validation, and arrange to have it called on form submit. To the CLI front-end, these would act just like the existing validators that run on the server. That would be completely awesome.
I'd agree with other respondents that Haskell itself can be used as a form of pseudo-code. Indeed, 'undefined' is very helpful for this purpose. If you don't want to think about something yet, define it as 'undefined' and write a brief one-line comment explaining what it means. To convert your pseudocode to real code, find and implement the undefined stuff. I'd also comment that Haskell types sometimes play a role similar to what pseudocode does: in idomatic Haskell, types often dictate the structure of your code, so in a sense the process of defining the more meaningful types for your project is akin to writing pseudocode. Indeed, a lot of Haskell projects start my someone opening a text editor and writing some type definitions, thinking as they do so, and revising along the way. Once the types are right, the code can be written. Even later changes often come down to changing a relevant type, and then letting the compiler point out all the places that the code needs to be changed to match. Will either of these ideas satisfy your college class's requirements? No idea. But these are things that play roles similar to "pseudocode" in the Haskell world. (To be fair, a lot of people don't really write pseudocode in the imperative world any more, either. It's in large part a relic from back when your program didn't really say what you meant very succinctly. If you're wondering what I mean, find some examples of doing something non-trivial in C and the Win32 API.)
Alternate tongue-in-cheek solution: Write program in nice, clean Haskell using descriptive names. Call that the pseudocode. Now abbreviate all the names, run the whole thing though [pointfree](http://hackage.haskell.org/package/pointfree), and say that the resulting mess is your actual code. The pseudocode being a working Haskell program as well, of course, would be entirely a coincidence.
What's the galois connection for gcd?
I think he's proposing `pure x = Map.newMapThatIsCompletelyFullAndAlwaysReturns x`.
So it's definitely a sort of TMap, as a completely full M.Map does not make sense (except for Finite keys).
 -- this function is left as an exercise for the reader
Why not the obvious interpretation that he is, as conover suggested, taking some poetic license and implicitly using "true" to mean "true things that are provable in a sensible manner"? I mean, it's not terribly uncommon in mathematics to concern oneself only with objects having certain well-behaved, tractable properties even when those are but countable dust in an uncountable sea of uncomputable, pathological bullshit. You might object that non-mathematicians aren't necessarily aware of how "common" such intractable objects can be in mathematics, which is why conover mentioned that the quote comes from *a book about noncomputability aimed at non-mathematicians*.
ok so this was a big PITA, but is now actually happening. Check out https://patch-tag.com/r/NaLaurethSulfate/NaLaurethSulfate-testproject/wiki/ to get to it. As I said above please only edit the "discuss" pages. Thank you again for the great suggestion!
But it comes at a cost. Totality means you can't really union them as nicely.
This is awesome! I'm so tired of writing everything in Fortran.
When mathematicians concern themselves with particular objects, this is usually expressively stated somewhere in the writing. His writing, on the contrary (and it might be taken out-of-context, I do not know that, of course -- in that case the quoter would be to blame :) it seems like a generalization about all proofs. Thus I do not find the suggestion you offered to be that obvious at all. In any case, I can now rest assured that the issue is sufficiently clear to those who find the interpretation obvious, and hopefully now also to those who were confused by it (like me), after reading the comments.
I agree that reviving the "WAI compliance" specter is not going to take things closer to cooperation, but I honestly think this is not the intent, and it's a tiny part of the article. Michael did an excellent job here of summarizing the state of the Haskell web application space right now. To me, the question is, does Haskell let us combine independent tools well enough that we don't need something like Rails? I think it does, and I think that when it doesn't, we should work through that as a community, and develop options for solving the problem. And yes, I think we need to reject the forces of premature standardization. Just say no to community-enforced standardization when less than 500 people have ever seriously used any of the options we have. That's especially true for WAI, whose actual direct "users" number in the range of 3 to 5.
Quite a good showing I would say!
Pretty interesting. I chanced to see Io with a high SO rank, so I went to SO and checked it out. I'm afraid the `io` tag is really I/O much of the time. :(
I actually [posted this earlier today](http://www.reddit.com/r/haskell/comments/ejozu/in_popularity_haskell_scala_actionscript/) but with the much less catchy title: "Haskell == Scala == Actionscript". I meant it as a compliment, but I think some people might have taken it negatively. 
g -&gt; (g, g), with the product partial order on N x N. In general, meets and joins are a special case of products and coproducts, or more generally limits and colimits, which are adjoint to diagonal functors, i.e., functors which take an object to the constant diagram on that object.
Looks like it got caught in the spam filter.
This post has `?ftw` added to the URL so both posts show up. &gt; An enthusiastic emphasis to the end of a comment, message, or post. &gt; Sometimes genuine, but often sarcastic. &gt; &gt; (Urban Dictionary)
&gt; In general though, when you see 'functional pseudocode' it's either mathematics or some less formal notation closely resembling the author's favourite functional language. Which is funny, given how often mathematicians use shorthands for not spelling out exactly what they mean, whereas programmers have to convince an unthinking computer what they mean :)
My guess is that you need to write "detailed" pseudocode to justify that you thought through the problem prior to writing it. So however you think through the problem just write it down. Pseudocode is the product (and luxury) of being in a field where no one wants you to write 10 page paper about your feelings on your design. So for haskell, I might have something like this: type Name = string type Age = Int data Cake = Vanilla | Chocolate | Uranium deriving (Read, Show) Main = do name &lt;= getNameFunction age &lt;- getAgeFunction kindOfCake &lt;- getCakeTypeFunction printOutBirthdayInvitationsFunction(name, age, kindOfCake) The above code wont work and is missing alot of parts but its makes the plan pretty clear and you know what each of the functions does :) ... its pseudocode
Why not? If you have: union :: Map k a -&gt; Map k a -&gt; Map k a You can replace it with: mappend :: TMap k (First a) -&gt; TMap k (First a) -&gt; TMap k (First a) If you just want to union and get the first of each, or normal Maybe if you want a recursive mappend...
Very useful if you have packages that are not released to hackage. For example, at our company we can now use this on our private packages.
I'd love to say this isn't I produce code myself but it pretty much is.
I'd just like to clarify history on Yesod and WAI a little bit: WAI is a proposal that existed long before Yesod came around, a proposal which was simply never put into practice. Before WAI, Yesod was using Hack. However, there were three main problems with continuing to use Hack: * Jinjing was not interested in following the PVP. * It had no support for enumerators. * It used Strings in many places where it should have used ByteStrings. I would be interested if you could point out a single design decision in WAI which favors Yesod: I asked for community input, and have designed it to the best of my ability to be a completely generic web layer. &gt; By contrast, Yesod appears to be mainly a one-man project (at least based on its web site), so of course being able to snap in components from other projects to build the framework would be a huge benefit to Yesod. I don't think that's really fair: besides the fact that there *are* other contributors to Yesod, and that people *have* written add-on packages for Yesod, I think it would be **incredibly** difficult to claim that Yesod is trying to piggy-back on the work of others. In fact, if you look at the discussion I link in the post, people seem to be arguing the opposite: Yesod is trying to create its own world by doing everything from scratch. Let's try this from the other perspective: what is the *downside* for Happstack and Snap adopting some version of a WAI, assuming it meets their standards? If there are problems with the current implementation, let's discuss them. But just saying "it's too early for standardization" seems foolish to me.
Firstly, I agree that Haskell does not require an all-in-one solution to the same extent that Ruby does. But my main reason for commenting: I think you are describing a chicken-and-egg scenario here: let's not standardize on WAI, because no one is using it. How exactly do we get *past* that?
It works with Main&gt; (+1).(*2) $ 34 69 I'm not sure, but I think regular old function application has higher priority than `.`, which which would mean haskell sees `(*2) 34` before `(+1).(*2)`.
 (+1).(*2) 34 == (+1).((*2) 34) == (.) (+1) ((*2) 34) == (+1) . 68
This is unrelated to lazy evaluation. It's happening during typecheckinh, not at runtime (evaluation). The issue is that an infix precedence only is applicable when you use a function infix. Writing `(*2)` makes it into a regular function and applying it that way (prefix) means you're using the precedence of function application, which is higher than any infix operator (and can't be changed). So the error is telling you, in a somewhat roundabout and way, that it parsed your code by applying first to 34, then composing. The result is `2 * 34` which is still a polymorphic `Num` value, and then it tries to apply the dot to that, and complains that the dot wants a function, but it can't find any instances of `Num` that are functions. Hope this helps!
I'm not sure, but I don't think that (\*2) uses the priority of *, since you're using it as a prefix function. And normal function calls bind really hard.
It becomes a regular function when you put parenthesis around it, doesn't it? 
Well, I think the way we get past that is by a bunch of people writing the same code over and over again, and letting things converge on a common design. I've not much of a believer in not repeating ourselves as a community; after all, that's how we learn. My statement wasn't so much that people aren't using WAI. It's that very, very few people have written the kind of code in Haskell that would even have the possibility of using WAI. If there were 15 to 20 people that had written web servers for hosting different web apps in Haskell, and they all talked to each other about what it is that they all did in common, or that they wish they'd done like someone else... then that could be the beginnings of a common architecture. The problem is, right now, those 15 to 20 people just don't exist. So necessarily, any effort to standardize is going to be done without exploring the design space as a community. And pushing standardization now makes it even more unlikely that will happen.
I respectfully disagree. The following efforts are already duplicated: * GZIP compression for responses * Cookie handling * Multipart form parsing * GET parameter parsing * Probably a dozen others These are duplicated in wai-extra, Happstack, Snap, CGI, and probably Kibro, loli, and a few others. But even more importantly than this, look at the things that we *could* write: * A single HTTP unit test framework for all WAI apps * Backends like mod_wai, similar to mod_python, mod_ruby or mod_perl. * Development servers that automatically recompile applications * Virtual hosting for any WAI application, all running off of a single, fast Haskell HTTP server (probably Snap) without needing Nginx or anything else. Edit: I've forgotten how to write proper English apparently.
Nice summary. Thanks.
Exactly. `mappend` gives left-biased, right-biased, or symmetric/recursive merge/union as desired, simply by choosing the `TMap` element type to be a `First`, `Last`, or `Maybe`, respectively.
I wish the standard libraries (e.g: Data.Map) were re-designed this way, so their API would have a much smaller vocabulary... Also, the beauty in the TC morphisms suggests, IMO, that we should strive that more and more of our vocabularies/API's should come from standard type-classes, rather than be concrete functions. But something tells me ListLike isn't the right direction :-)
So where are we? 1. WAI has a number of good back ends (fastcgi, snap-server, etc) 2. There are at least two front-ends (Yesod and the experimental Happstack one). The experimental Happstack front-end for WAI is still experimental not because Jeremy is standing in the way, but rather because no one has offered to make a commitment to make it stable code; I'm sure Jeremy would be thrilled if someone did. 3. Looks like snap-core is abstract enough to provide different back-ends for it, since in fact Snap does so... hence I suspect it would be feasible for someone to get a Snap-based application running on top of WAI. So it appears that development on both sides of the WAI interface is possible, and happening. I just hate that it so often seems like people are waiting for others to *stop* writing their own code. I've had a bad taste in my mouth for WAI since this thread (http://www.haskell.org/pipermail/web-devel/2010/000153.html) on the web-devel mailing list, in which someone wrote and shared a library for writing web applications in Haskell, and the ensuing discussion all came down to whether that person should have used WAI... a decision which is, of course, irrelevant to any of the user-visible pieces of when the author was sharing. The more often that happens, the more I think people need to start standing up for each others' right to try different ideas without being beaten down my a misguided community enforcement of a "standard".
I don't think anyone is recommending that Hackage ban any web-related package that doesn't use WAI. On the other hand, I think a simple question of "does this use WAI" is *not* uncalled for. I do remember that the thread ended up focusing a bit much on WAI instead of Salvia, but that really is the nature of the Haskell community. I think the attitude that "we aren't ready for standardization" is wrong, but I'm also not asking for a standardization where everyone **must** use the same thing. Now, I would appreciate if you- or anyone else- has thoughts on the WAI itself, instead of whether it should exist or how it is being proselytized.
`(+1).(*2) 34` is interpreted as `(+1).((*2) 34)`.
&gt; And normal function calls bind really hard. This is an amusing way to express higher precedence, and it actually makes more sense to me.
Also a cool idea. Porting libraries is something I've been thinking about recently. Thanks!
Sometimes it causes trouble with indentation though.
Huh? How?
I'm guessing at what aekeru meant... but a lot of the time, I notice that people accustomed to using 'let' as a statement in a do block are unaware that it introduces a layout context. Then it just happens to be three letters long, so that the layout context starts 4 characters in from the do block's indent. And many people use 4 character tab stops... I can't count how many beginning Haskell programmers I've helped solve a syntax error because of indenting the wrapped part of a long let statement... it's embarrassingly common.
In a perfect world it would maybe be nice if we could all hold hands and agree on exactly what the web app interface should look like. Kumbaya etc. Right now I don't see the space for agreement on standardization: at the stages we're all at now with our respective toolkits I think we're getting more out of a "marketplace of ideas"/"healthy competition" approach. Last time this discussion came up we agreed that we would cooperate where appropriate and so far I'm pretty happy with how this is turning out. For us though, there's absolutely no win for switching our stuff to WAI at the moment: we have a web-app interface, it's called snap-core. If the whole of snap-core got pulled out and *became* the new WAI, then hey, now we're talking! But I don't think you have any more of an appetite for doing that than I do for switching to use your stuff.
Well, if you really don't see the benefits, there's nothing I can do to convince you. But I think I've laid out the arguments pretty clearly. I also don't know what the *downside* is for adopting the WAI, besides the initial work to get it working. As far as snap-core = WAI: snap-core is **massive** compared to WAI, and very highly specialized for your needs (AFAICT). The idea of a WAI is something lightweight that can easily be implemented for both handlers and middlewares, while providing enough generality to not limit applications at all. The point is *not* to provide a lot of details to the applications, since that can easily be provided by an add-on package. That's the exact case currently with wai-extra. I am interested though as to why you think WAI provides you "absolutely no win." Do you mean the benefits that I bring up are worth nothing to you, they aren't really existent, or you simply mean that the costs outweigh the benefits?
&gt; I also don't know what the downside is for adopting the WAI, besides the initial work to get it working. Aside from the fact that I don't particular care for its design, especially in the IO part, I suppose there aren't that many downsides. &gt; I am interested though as to why you think WAI provides you "absolutely no win." Do you mean the benefits that I bring up are worth nothing to you, they aren't really existent, or you simply mean that the costs outweigh the benefits? Because the main benefit I can see is the ability to interoperate with applications which don't really exist?
I would love to get it back in HTML format somewhere. Currently, it seems that the site that used to host the web version of the HWN (http://sequence.complete.org/hwn) is having some issues. If we can get an admin to fix that, I'd like to post things there again. I've thought about posting them to my blog, but I think that would be very bad form. As far as linking to reddit goes, I just hate linking the text version. Got any thoughts?
OK, this is probably me just beating a dead horse, but I'll say it anyway: did you notice that I'm bringing up proposals now to change the way IO is done in WAI? Also, doesn't WAI currently have better multipart and GZIP support than Snap? Snap can't currently run on FastCGI. These all seem like things that exist right now.
Multipart yes (until we finish it shortly, at which point I'm confident ours will be as good or better), gzip: not really, and we don't care about fastcgi. Moving to a web programming interface which we don't particularly care for is not worth getting a handful of useful features a couple of months quicker. It also would involve chaining ourselves to an API we don't have maintainership for, which would mean working to build consensus to make radical or breaking changes. At this point in our development we're not willing to give up that flexibility, as we're still in the high-temperature phase of our annealing process. We have a couple more I/O changes (like integrating blaze-builder into the I/O core, which is something you've been thinking about also) in the pipeline for snap which could be a difficult sell for other WAI users. Why are you pushing for this so hard, anyways? It makes sense to worry about wasted effort in our small community, but you're operating under the presupposition that I'm going to love everything you produce and vice-versa. Like anything else on hackage, when you put out something I like (like http-enumerator), I use it, and when you put out something I don't like (like WAI), I don't. The fact that you're going through a proposal process to make changes to how WAI works -- when besides your own stuff you might have two or three users (just like us, of course) -- is a perfect example of why I'm not jumping out of my shorts to get on board. You've been at this "community" angle for WAI since you started with it, and honestly I struggle to understand why. The impression I had at the time was "who is this guy I've never heard of and why's he trying to run a standards committee of size 1"? You solicited community feedback, sure, but used your de facto chairmanship of the project you yourself created to pick and choose which feedback you wanted to listen to and which feedback you wanted to ignore. And the fact of the matter is, you personally wrote close to 100% of it as far as I can see. I don't really have a problem with this -- you made a useful thing and some people have chosen to use it, which is great! But please don't front like it's anything close to a true community standard. The only difference between snap-core and WAI is that we're not pretending our project isn't run by fiat.
On the one hand, you accuse me of having a fiat leadership on WAI, and therefore it's not a real community standard. And on the other hand, you don't like the fact that I'm asking for input. So it seems you want a package that doesn't take input and is run by someone *else's* fiat leadership... which is why you keep bringing up standardizing on snap-core I presume. I never claimed WAI is a "community standard": its my package, and I make the calls. If someone thinks I'm taking it in completely the wrong direction, then they should write their own, and let the community decide what is best. I personally would have been much happier if Hack had made some of the fixes that were really necessary to fix its architecture: follow PVP, use ByteStrings and adopt enumerators. My problem is *not* that you don't like the current WAI package: reasonable people can disagree. It's the entire approach the Snap community is taking to this: I had an incredibly bad taste in my mouth (to borrow the phrase above) when we started discussing a possible WAI on the cafe, and you came in explaining how stupid this whole thing is. And oh yeah, you guys are creating a framework which will be the de facto single Haskell web framework. Basically, it seems like you guys are trying to create a walled garden with Snap, while everyone else is trying to play nice. I don't care if you don't particularly like my approach, but instead of simply saying, "This is stupid, I don't like it, drop everything and use our stuff instead," you would have come back with some *serious* proposals. And no, "adopt snap-core" is not a serious proposal: that package has too many dependencies and is completely specialized far beyond any claims you can make against the WAI. Trying to compare those two is IMO completely ludicrous.
Sorry, missed that post. If you're still there, the options in the preferences only apply to new projects. They're only options that are used when creating the Cabal file the first time. So you change them all you like, it's not going to impact your projects. You need to go and edit the cabal file directly (there is a special editor for that) For the speed issue, I don't know, it's pretty fast when you use the standard streams to communicate with scion. Are you using the network maybe?
In Van Jacobson's [Tech Talk](http://www.youtube.com/watch?v=gqGEMQveoqg), I remembered he called the phone networks "Path-centric", the internet "Endpoint-centric", and says the logical next stage for networking is "Data-centric" or "Content-centric". Can you expand a bit about what's going on in this project?
Standardizing on snap-core wasn't a serious proposal, relax, Michael :). What I was trying to get across when I made that joke is that pushing us to base our stuff on WAI is like us asking you to base your stuff on snap-core. I don't think you would seriously entertain that idea, so I don't know why you're surprised that the same is also true for us. On the one hand you say things like 'I never claimed WAI is a "community standard": its my package, and I make the calls' and on the other gripe at us in blog posts for not adopting it. You can't have it both ways. Re: us being the "de facto Haskell web framework" -- this is definitely an aspirational goal for us, no doubt. It's also pretty tongue-in-cheek, we don't expect people to stop working on their stuff. In fact, quite the opposite: we hoped that us laying down the gauntlet like we did would inspire some healthy competition in the Haskell web space. To a certain extent I think we've succeeded at this, the happstack website is definitely prettier today than it was in June, and I don't think that's a coincidence. And if competing with us has made you work harder on Yesod so that you could win points by claiming to have features we don't have, well that's a win for Haskell users. I want to make it clear that what I care about more than anything here is having viable solutions in the Haskell web space. We're all pushing towards the same goal. We do want our stuff to be so good that there'd be no reason for anyone to use anything else, but we're a long ways off from reaching that bar. If we're going to get traction, it's going to be through market mechanisms, not through committee processes/politicking/etc. In a sense what I'm trying to get across is that Snap *is* our "serious proposal" for a Haskell web application interface. Re: your "walled garden" thing, that's silly, Michael. We just have a philosophical difference of opinion about where we want to go: an integrated, consistently coded and documented, well-tested, high-performance, single-source, newbie-friendly toolkit vs. a collection of interoperable libraries scattered all over hackage, of varying quality, from a diverse group of suppliers. We feel our approach will serve users better in the long run.
It builds up to be a cheap lame joke, but ends surprisingly well :-)
 Oh, and also: &gt; On the one hand, you accuse me of having a fiat leadership on WAI, and therefore it's not a real community standard. And on the other hand, you don't like the fact that I'm asking for input. So it seems you want a package that doesn't take input and is run by someone else's fiat leadership... I don't think anything I've said on this has been inconsistent. The point is: I *don't* want such a package, and I never have, from the very beginning. I think the whole idea is a waste of time actually, and it would be a lot more sensible and honest of you to just rename that thing as "yesod-core" and manage it as you see fit. I just checked the dep graph for wai, here's the full list: Direct dependencies: web-encodings, wai-extra, web-routes-quasi, yesod, wai-frontend-monadcgi, wai-handler-devel, wai-handler-fastcgi, wai-handler-scgi, wai-handler-snap, wai-handler-webkit, web-routes-wai, and yesod-auth. Indirect dependencies: persistent, hamlet, hack-handler-simpleserver, persistent-sqlite, hack-middleware-cleanpath, hack-middleware-clientsession, hack-middleware-jsonp, happstack-hamlet, hledger, hledger-web, maid, on-a-horse, persistent-postgresql, yesod-continuations, yesod-examples, yesod-markdown There's like one or two things on there you didn't write yourself!!! I wouldn't mention this at all btw if you weren't pushing so hard to get happstack and Snap to sign up. The point is: people will use it if it solves problems for them. We're not interested because it doesn't really buy us much, and we don't like the downsides. Full stop.
Ah, I see, thanks. It is set up to use the standard streams. I have an Intel 1.8ghz Core Duo from 2007 in my macbook under Windows 7 32-bit, with 2gb of RAM, and the latest version of Eclipse 3.6.1. I only have one haskell project with one source file, and no other projects. When typing new code or deleting it, the characters appear very slowly, and my CPU usage goes up to about 60%, and then when I stop it goes back to a more normal 10% or so, but otherwise eclipse is running ok. The haskell file is about 600 lines though - do you think this could be the issue? Also, every time I type/delete something, or hover the mouse over some code, the output console gives out loads of blue messages from scion - do you think the act of rendering all this text could be the problem? Can I disable it somehow? Thanks for your help. Edit: Scion output in the console seems to actually display (on one line) the entire structure of my source file every time I type a character. I've got a feeling that rendering this much text could be the issue, I get stuff like (but longer) this every time I type something, and it includes every identifier in the source: &gt;&gt; {"id":7,"result":[{"location":{"region":[79,16,79,25],"file":""},"name":"Applyable","block":{"region":[79,0,80,27],"file":""},"type":"class"},{"location":{"region":[80,4,80,15],"file":""},"name":"defaultRule","parent":{"name":"Applyable","type":"class"},"block":{"region":[79,0,80,27],"file":""},"type":"function"},{"location":{"region":[66,5,66,17],"file":""},"name":"AbstractNote","block":{"region":[66,0,66,35],"file":""},"type":"syn"},{"location":{"region":[64,5,64,14],"file":""},"name":"DeltaTime","block":{"region":[64,0,64,23],"file":""},"type":"syn"},{"location":{"region":[63,5,63,17],"file":""},"name":"AbsoluteTime","block": ...............
Let me make something clear then: I put on my blog that I wished you would consider using WAI. It's not a gripe, or an attack on Snap: it's simply the way it is. I want you to use it or something similar, and I've laid out the arguments of why I think it's beneficial. You don't want to do so. That's not going to change the fact that I wish we as a community had such a standard. Now walled garden I think is accurate here: you're saying you are not interested in any kind of standard that would let Snap applications run on other backends, let applications not written for Snap run on the Snap server, share middlewares between Snap and other frameworks, or share libraries such as request parsing. I think that *is* a walled garden versus everything else. For everyone else's information, since Gregory clearly knows this already: I've had plenty of cooperation and help in writing wai-handler-snap, so I'm *not* trying to claim that Snap is *avoiding* interoperating with everyone else, just that it's not actively pursuing it as I think it should. Maybe "walled garden" has a stronger connotation than I had realized, I've been outside of English-speaking countries for a few years now.
You mean like hamlet, persistent, authenticate... Personally, for Yesod, my naming scheme is if it can be used outside the scope of Yesod, it doesn't get a yesod- prefix. If it requires yesod, it does get the prefix. But getting hung up on naming conventions seems silly to me.
Of course I've considered it: you're one of the half-dozen or so people actually hacking on stuff in the Haskell web space, and so I pay attention to what you're working on and listen to what you have to say. Concretely, in a year or so we *might* be in a place where we could revisit this again, but as it stands everyone is using a variety of incompatible or pseudo-compatible I/O models, and if we can't even agree on that there's little hope for a unified standard. I suspect what will happen over the next few months is that we will both independently come to the conclusion that builder enumerators (i.e. `Enumerator Builder IO a`) are the "right"/most efficient model for doing network I/O in Haskell. If that turns out to be the case, then there's an argument to be made for unification. Until then, more experimentation, research, and benchmarking are required. &gt; you're saying you are not interested in any kind of standard that would let Snap applications run on other backends, let applications not written for Snap run on the Snap server, share middlewares between Snap and other frameworks, or share libraries such as request parsing. I think that is a walled garden versus everything else. You're right: I have little interest in spending time trying to interoperate with hypothetical applications or middleware which don't exist. There are a handful of things you have in WAI which we haven't yet implemented in Snap (and we're catching up), and I'm fairly certain that if/when we implement those, we're going to want to make different design decisions. Embrace heterogeneity a little bit, dude. :) Anyways we're talking in circles, if you want to discuss this further let's take it to email.