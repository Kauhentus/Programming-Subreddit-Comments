&gt; xml — there's a lot to choose from I like hxt, but it's big and complex, uses arrow when applicative might have been better and string when text might have been better... How about hexpat for something fast and simple that doesn't incur dependencies on anything like conduit? How portable is hexpat?
What is his book worth, though? Someone tried? It looks interesting but it's a bit of a mouthful, and I don't have a lot of time currently.
What? Why would placing the balancing function at a destructive update point later down, rather than at an earlier point, before spreading the tree, simpler algorithm-wise in any way? After all, the semantics of the balancing function are `id` (at least denotationally). Placing an `id` call anywhere in the algorithm couldn't complicate it, surely?
For me, the GUI department is the most welcome, but also the most tricky addition. After all, the platform would have to include a copy of wx/gtk for OS where they are not installed by default.
This post is about a continuation of some work to formally validate parsers, done as part of the [Compcert](http://compcert.inria.fr/) project (a formally verified compiler for C) and published at ESOP last year ([paper](http://gallium.inria.fr/~xleroy/publi/validated-parser.pdf)). The point of interest here is a contrast between the usual view of a classic lexer/parser toolchain, where a lexer basically is a pure, lazy stream processor whose output get consumed by the parser to build an AST, and what actually needs to happen when parsing a C program. Because of a syntactic ambiguity `int f(a (b))`, the parser needs to know if some identifiers are variable names or may be type names. This is traditionally done using side-effects that rely on the interleaving of the lexing and parsing actions. As you can imagine, this is hell to formally specify and prove correct. The solution used here use a two-step parsing process that first constructs the overall static structure, analyze it for naming information, and use it to re-parse the ambiguous part correctly. This program also demonstrates usage of monads (and monadic notation) in a real-world (if a bit dull) Coq program. I think arguments for purity are even more convincing in a total language with the additional pressure to write actual proofs about the programs. This is a problem domain where choosing the right design to strongly restrict side-effects, and not only hide it under some monadic semantic sugar, can be extremely important. (On a totally unrelated note, you can do extremely cool things with monads when you have dependent types.)
&gt; It also makes me think a little bit of Clean's linear typing. I think the link is stronger with Common Lisp's nconc/nreverse-like functions. Clojure transients are a direct implementation of this "destructive functional paradigm".
Ok, cool. I will keep playing wih this.
Sorry I wasn't explicit. The problem isn't as related to balancing as it is to *amortization*. When you have an amortized data structure you have some expensive operations from time to time but its OK since you have lots of cheap operations in between to compensate. The problem is that if you have a persistent datastructure you can "fork" multiple copies just before the expensive operation. For example, consider how you can implement a purely functional queue with a pair stacks (lists). You have an in stack and an out stack. When you add things to the queue they go in the in stack and when they are taken out they are taken to the out stack. If the out stack is empty you empty the in stack into the out stack. Now consider what happens when you "fork" a queue with a big in stack: queue1 = add_list_to_queue empty [1..1000] queue2 = add_to_queue queue1 1001 queue3 = add_to_queue queue1 1002 x = pop_queue queue2 y = pop_queue queue3 If you have a naive implementation and can't change the internal structures behind the scenes then you end up flipping that big 1000 element list twice. This possibility for common expensive opetarions makes it so that queue operations may end up being O(n) amortized instead of O(1) amortized like you want them to be. If you want to guarantee O(1) performance then you need to use a more complex data structure or you need to allow for hidden updates behind the scenes. 
Instead of this rigid separation (a package belongs to platform or not), maybe we could consider multiple layers ? For example, * { GHC, base } would form layer 1 * { array, bytestring, ...} would form layer 2 * { aeson, ui ...} would form layer 3 * ... and so on The rules would be : * a package can only depend upon packages of same or lower layers * a higher layer can be updated without touching lower layer * a lower layer can NOT be updated without updating higher layers A layer could just be a cabal meta-package, with dependence to its packages, and to a specific (rigid version number) lower layer. That way, the constraint of the top layer is sufficient to constrain all lower layers versions. Because the lower layers have less dependencies, and more at the core, they are updated less frequently. Also, their policy must be much more conservative than the policy of the higher layers. The advantage of this system would be the scalability with respect to the number of "good packages" that we want to bring to the end user. What do you think ?
I can hardly imagine anyone feeling insulted merely by being called a motherfucker.
There was a comment in there about list comprehension not being fundamental to haskell because it's syntax sugar which disappears at the level of Core. First of all, Core is only one way to implement haskell. Take the C language. What also disappears at the level of GCC's RTL? Can we say those things that disappear aren't fundamental to C? List comprehension is utterly fundamental to haskell and not merely in the sense that it's defined in the language spec. It's that it's modeled after set comprehension in math, and the best model of purity we've got is the behavior of functions in math-land. 
You might be able to achieve something similar using meta-packages, one for each layer.
Clearly just a monoid in the category of endofunctors. What's the problem?
List comprehensions are totally incidental to Haskell. They could be removed with very little effect on existing Haskell code. Yes, you'd have to rewrite them, but that's entirely mechanical (and not really much expansion if you use do-notation in the list monad).
No worries. It happens a lot.
http://news.ycombinator.com ?
I noticed that i rarely (if ever) use list comprehensions. I prefer composition of map, filter, fold instead. 
But if the balancing is part of a mutation, then you lose sharing at that point anyway? Or do you want to reap the benefits of the rotations in the shared parts?
Is lens a successor of [data-lens](http://hackage.haskell.org/package/data-lens) package ? 
data-lens doesn't seem to support polymorphic updates. That makes it *much* less useful (imagine using a lens to apply a function that isn't Endo on a tuple element). ~~data-lens also doesn't seem to support applicative/monadic updates. e.g: You want to apply an `a -&gt; f b` function on a field of a `Rec a` to get an `f (Rec b)`~~. data-lens also doesn't support "traversals" (multiple-field accessors that can all be updated, or folded into a Monoid, etc). data-lens also doesn't support composing these various kinds of different lens using the simple Prelude function composition `(.)` operator. Which is pretty awesome. lens is much much more comprehensive (and thus also larger). It also supports "projections" (colens, first-class representations of data constructors in type sums), isomorphisms, and much much more. Also, lens comes pre-baked with the TH needed to auto-generate the interesting lens you need. tl;dr saying lens is a successor implies it is something much smaller and less ambitious than it actually is.
My understanding is that Swing is fairly self-contained: it's implemented in Java. So I haven't had any problems installing Swing. What I'm really curious about is how Python and Ruby deal with having Tk as a separate library, outside of their respective ecosystems? Do they have the user install it themselves? For me, the biggest difficulty with installing wxHaskell was making sure all the non-Haskell dependencies were present and working. 
You bring up a good point, but at once, why not have both? Don't advertise the tiered approach, and direct people towards the unified install. Those that care can use the more fine-grained layers.
data-lens was the first implementation of lenses by Edward Kmett. That's the only reason really i'm asking. I'm not familiar with both packages. 
Oh, I didn't actually notice that :-) I guess it could officially be a successor, but seeing it as such still downplays the awesomeness of "lens"...
Any slides? Edit: I see I have to login, but I've got them now!
I posted this for those how are interested in new Haskell communities but are not currently active on google+. There is also: Haskell: The Haskell-Beginners Community https://plus.google.com/u/0/communities/101629034466170191725/ Biohaskell: https://plus.google.com/u/0/communities/103973218999479996671 Xmonad: https://plus.google.com/u/0/communities/116189152436684878348 Edit: adopted GiantMarshmallow's correction.
What is it about?
Or (OTOH) get the same using Data/Typeable through Data.Aeson.Generic, without TH.
Overarching theme: There are two modes of operation while working. There's the intuitive mode, where you make rapid intuitive decisions, and there's the attentive mode, where you make calculated thought out decisions. While more natural and desirable, the intuitive mode can easily result in undetected mistakes. Haskell provides tools to make it easier to operate in the intuitive mode while avoiding many of those kinds of mistakes. He goes into details about specific examples (aeson and attoparsec) about how the type system assists with conveying meaning and error avoidance. He also talks about unit tests and how QuickCheck helps with correctness verification. It was a fun talk to me because bos is good speaker talking about something I enjoy. But if you've been reading the pro-haskell literature, there's not a whole lot new here.
Could you take a heap profile and upload it to http://heap.ezyang.com (and link to it?) We only need the first 15 seconds of execution or so. Use of Parsec is a code smell; Parsec is not known for being fast. Other code smell is not using the strict update functions for HashMap; insertWith in particular.
Bresenham's algorithm is a linear interpolation between two variables; for 3D you have to do it twice, using whichever delta is largest as the "source". The extra condition is to sort that largest axis into `run`. You won't care about the order of the other two axes, nor if one of them is equal to the largest. zip [0..run] . map fst $ iterate step (0, run `div` 2) So `run` is that largest axis. We take that many steps to go from min to max and start the error at half of this to prevent the steps from being biased towards either end. (the results will be "correct" with any starting error between `0` and `run`) where error' = error - rise `rise` is the target axis you're interpolating to. You'll want to parameterize `step` over this value so you can use it twice. We subtract `rise` here because in one step of the source axis, you move up the target axis by target/source. So I think the final `otherwise` case will be something like: zip3 [0..dx] (map fst $ iterate (step dy) (0, dx `div` 2)) (map fst $ iterate (step dz) (0, dx `div` 2)) after sorting the largest axis into/out of `dx` and adding a `rise` parameter to `step`. BTW, you have a typo in `( z, y, -z)`.
 1. ~~How do I take a heap profile?~~**Edit:** [Found it](http://www.haskell.org/ghc/docs/7.0.1/html/users_guide/prof-heap.html#rts-options-heap-prof). Which kind of heap profile would be best? **Further Edit:** [done, here's the profile](http://heap.ezyang.com/view/b87a4c9ffa9af1baeef01a5569617a41a4fe27c3#form). 2. What are the strict update functions for HashMap? I don't [see any listed](http://hackage.haskell.org/packages/archive/hashmap/1.3.0.1/doc/html/Data-HashMap.html).
I disagree on Parsec. If you're doing anything interesting with your data, typically Parsec will *not* be the bottleneck. Attoparsec is great, but Parsec is still great too, especially if you don' misuse try and the like. Parsec might not be the right tool for all cases (and certainly isn't for binary formats), but it is often, if not absolutely "right" at least more right than most alternatives, and certainly extremely simple and more than adequate. If there are heap overflows, odds are solving those will do way more for performance than swapping out any libraries or the like.
He probably thought you were using the [unordered-containers](http://hackage.haskell.org/package/unordered-containers) package which provides [Data.HashMap.Strict](http://hackage.haskell.org/packages/archive/unordered-containers/0.2.2.1/doc/html/Data-HashMap-Strict.html). Confusing that hashmap uses the same name.
I'm guessing you have a space leak and if you solve the space leak then everything should speed up dramatically. My intuition is that your program is spending all its time trapped in garbage collection because the garbage collector keeps traversing the heap but can't free anything. Do the heap profiling that `ezyang` asked for and you should figure out where the space leak is. Just from a cursory glance at your program, I suspect that your problem is a combination of your `rankedUrl` parser combined with `many`. You are demanding a list of functions, and there is no way to force the functions so they become a huge space leak liability. You should also try translating your ruby code directly into Haskell. You can translate it almost verbatim.
Thanks for posting the profile. Normally I'm pretty okay at reading them but yours is...is weird. All that pinned stuff is definitely telling us something, but I'm not sure what. Here is some speculation: Pinned must be the pointer type that Data.Text uses to store chunks of text. You must have tons and tons of Data.Text values, most likely living in separate strings. I'm not sure why that would be the case, perhaps your program churns though a good chunk of the input file. For small strings, [Char] may be more efficient than Data.Text. Initially I came here to complain about using lazy IO. But really, when I read the problem description and glance at the code I think that a fold over a lazy input stream should work really well. As long as the hashtable is built strictly and the (key,value) pairs are processed fully one at a time. Basically, strict in the pairs and the table construction but lazy in the spine of the input lust. I think having a retainer profile (-hr) and a type profile (-hy) would help at this point. You'll have to collect them as separate runs of the program. The retainer profile should give us clues about places in the program that are creating lots of thunks, if that's happening. The type based profile should help us figure out what's up with all those pinned values.
For those that would like to read an entire book on the topic I highly recommend [Thinking, Fast and Slow](http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow#Two_systems). I've not finished it yet but it's very helpful for identifying common mental mistakes. [Amazon link](http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637).
I am confused at your use of the state monad transformer. Why don't you just combine parse results?
Maybe if it gets very popular I'd consider it, but otherwise, Google+ seems *really* awkward and obnoxious to me. What's wrong with just posting things on Reddit?
One quick comment: the following line defeats the desired streaming/lazy reading of the file, since the whole file needs to be read in order to count the number of lines. let numLines = fromIntegral $ count '\n' contents If you can leave out the number of lines from the progress output, then you might want to remove that feature. If including the number of lines in the progress output is required, I wonder if opening and processing the file twice would result in better performance (by processing the file as a stream twice instead of trying to keep things in memory). I do not know how much this contributes to your issue, but it is something that I would definitely look into.
Since the whole file is read upfront you might as well start with +RTS -H512m. I'd expect it to run considerably faster. Tweaking allocation area size (e.g. -A4m) might also work miracles.
I'd say category theoretic thinking (as well as algebraic and type theoretic) counts much more than set theoretic thinking in Haskell. Though, any kind of mathematical thinking helps a lot when learning Haskell (or anything else, really).
Yeah, I'll try translating the ruby to haskell, but I'd like to figure out how this is going wrong first, so I can avoid these mistakes in the future. I updated it to remove the function building in `rankedUrl`: -- the urls for each category are given as "RANK. URL\n" rankedUrl :: Stream s m Char =&gt; ParsecT s u m Url rankedUrl = do many1 digit string ". " url &lt;- many1 $ noneOf "\n" char '\n' return $ (Url $ pack url) -- a full listing including the category and its ranked list of urls categoryListing :: Stream s m Char =&gt; ParsecT s u m (Category, [Url]) categoryListing = do cat &lt;- category urls &lt;- many rankedUrl return $ (cat, urls) transposeCategoryListing :: Stream s m Char =&gt; ParsecT s (Map Url [(Category,RelativeRank)]) m () transposeCategoryListing = do (cat, urls) &lt;- categoryListing let tot = length urls let include !m (url,index) = let pair = (cat,RelativeRank index tot) in insertWith (const (pair:)) url [pair] m modifyState $ foldl' include `flip` zip urls [0..] But it still stalled out about 20% of the way through, after about an hour or so.
And does it fix the problem?
Any thoughts on aeson-lens[1]? [1]: http://hackage.haskell.org/packages/archive/aeson-lens/0.4.1.0/doc/html/Data-Aeson-Lens.html
I think it's more dramatic in Ruby-land than elsewhere. Ruby programmers *love* Ruby, and there's a shared philosophy of programming that people absorb with the language (dynamic typing is good, test-driven design, and so on). That lends an air of heresy to using anything else. Sort of like us, except we're right :)
Also worth noting that Bendyworks is a Ruby web design shop. "Ruby is the awesome" is their business model. So props to Chris for branching out none the less.
I think Text values can be if they are big enough. But yeah, ByteStrings are always pinned.
Reddit's no box of kittens and sunshine either. What was wrong with just posting things on mailing lists and talking on IRC? Oh that's right, Reddit is popular, so we put up with it. All else equal I'm happy to have people staking out Haskell groups on any social medium growing in popularity--at least that way there'll be something ready and waiting if the social medium gets so popular the rest of us can't keep ignoring it.
Oh, oh, I've seen something like this before. Is it the thing where substrings of text (which are being stored in the HashMap) are pointing to larger text strings which were originally parsed, which are then never getting freed? It was discussed here: http://www.haskell.org/pipermail/haskell-cafe/2011-June/092596.html (Edit: nope, looks like bytestrings) If you use a later version of GHC, you'll get better information about pinned; Simon Marlow improved the code here in d56cbac1a875ea9 But I agree that you'll probably find that this pinned code corresponds to text.
Yes. Note that the "oh well, it sucks but everyone uses it" argument applies to mailing lists (and IRC) just as much as to reddit and G+ and basically everything else. :]
The data is [in alexa_scrape.txt in this tarfile](https://docs.google.com/open?id=0B6-U1uFbN8CkWHN1UmV0T19KTTg). The format's pretty simple: /slash/delimited/category 0. first.ranked.url 1. note.not.just/domain/some/have/paths 2. http://some.may.have/protocols/2 /not/all/categories/have/urls /thats/pretty/much/it
Can't help pointing out that the Ruby program doesn't do the same thing as the Haskell program. Either make Ruby read the whole thing in lazily converting between various string types and use a parser combinator library (in a pretty non-trivial way), or make Haskell read one text type in line by line and use a C implementation of regex to (trivially) match on it. Seems kinda weird to write completely different programs and say "why they perform so differently? ¯\\(°_o)/¯" I realise the "X vs Y" is supposed to be an incentive to get the performance gurus to help you, but at least start with something the same or your comparisons will be misleading, to you and others! If it's a revelation to you that "GHC doesn't optimize everything away for me" then that's a good start, now you know. I'd normally be motivated to write what I feel is a more realistic version to work with, but I'm heading to bed. :-) Anyway, looks like you're getting help on it so good luck.
If lenses are the coalgebras of the costate comonad, are prisims the algebras of the state monad? If so, how?
If you're willing to come to Paris, there is pretty much everything you could wish for there, with for example: - [PiR2](http://www.pps.univ-paris-diderot.fr/pi.r2/) the principal team developping the [Coq](http://en.wikipedia.org/wiki/Coq) proof assistant, also dealing in (even) more theoretical topics like various refoundations of logic or computational paradigms; in this area we also have the [Parsifal](http://www.lix.polytechnique.fr/parsifal/dokuwiki/doku.php?id=people) team working on fundamental logic calculus, often inspired by a logic programming approach. - The [Tocatta](http://toccata.lri.fr/tools.en.html) team that works on [Why3](http://why3.lri.fr/), a programming language with integrated specification facilities that can use various automated provers or proof assistants to verify programs; they also develop SMT solvers and participate in the [Frama-C](http://frama-c.com/) project, a (Paris-centered) static analyser for C program - functional programming research around [OCaml](http://en.wikipedia.org/wiki/OCaml) at [Gallium](http://gallium.inria.fr/~scherer/gagallium/), that also hosts a a large-scale project of formalizing a compiler for C ([Compcert](http://compcert.inria.fr/)) (disclaimer: I'm doing my PhD there) - The [Mathematical Components](http://www.msr-inria.inria.fr/Projects/math-components) that does heavy formalized proving of mathematics and recently finished the formalization of a hard theorem of finite group theory - The [Abstraction](http://www.di.ens.fr/~cousot/Equipeabsint-eg.shtml) that does abstract interpretation. They have a large-scale project, [Astrée](http://www.astree.ens.fr/), a static analyser software that is used to check for absence of various categories of software faults by [Airbus](http://en.wikipedia.org/wiki/Airbus). Teaching-wise, most of those teams are present in the [MPRI](https://wikimpri.dptinfo.ens-cachan.fr/doku.php) research-oriented master. You are expected to take classes there (or in another master) at the beginning of your graduate program, and then contact the teams that interest you for an internship, and later a PhD.
This tutorial might help: http://logitext.mit.edu/logitext.fcgi/tutorial
Heh, over in `/r/programming` it's proving [much more controversial](http://www.reddit.com/r/programming/comments/14ibgc/multicore_with_less_pain_deterministic_parallel/).
Oxford seem pretty hot.
Well, your argument isn't convincing; after all, there's nothing special about arrays compared to linked lists with respect to length. Arrays only have a fast length because they store it; linked lists could, too: data ListWithFastLength a = Nil { length :: Int } | Cons { length :: Int -- or perhaps length :: !Int , head :: a , tail :: ListWithFastLength a } nil = Nil 0 cons x xs = Cons (length xs + 1) x xs
I am skeptical that you can even _know_ the length of an array if you don't store it in some form. C tried this with strings, and that turned out to be one of the largest sources of bugs and security vulnerabilities ever.
Yes, having GHC refuse to override the input-file (or at the very least yield a warning) is probably wise.
gasche is giving good advice as well, but it all depends on what sort of Formal Verification you're looking into. I work on Model Checking and the like, so the UK is more fitting for me. If you're into proof assistants such as Coq, then Paris is the right place for you.
At Oxford, formal verification is what a huge number of people are doing and this department has been doing it for decades. 
For example, [Luke Ong](http://www.cs.ox.ac.uk/luke.ong/) and its students have been developing nice fundamental results on model-checking of functional programming languages.
Perry Alexander at University of Kansas does lot of formal verification work as well and is great mentor.
I love QuickCheck, I've begun porting it to different programming languages. http://www.yellosoft.us/quickcheck
The quadruple 'N' makes this event especially awesome!
Fortunately, people will solve the fragmentation problem by inventing aggregators that, over time, accumulate enough imperfections and quirks that they become merely another distinct source that must be checked! And progress marches on. Maybe I'm just too negative about social media in general...
However, tools like hlint could recommend that substitution to you. I don't know if hlint does this but it seems like something it certainly could do.
&gt; or am I not in the target audience? You're probably not. All the funny symbols are from logic (though they are sometimes used for talking about programming languages). The "division lines" are inference rules. If you have proofs for everything above the line, you have proven the thing below the line. It makes for a very clean way to visually represent proofs. However, the proofs end up becoming very large, so you rarely actually see how they fit together after the introductory examples in your textbook. As ezyang put it above, http://logitext.mit.edu/logitext.fcgi/tutorial is a good introduction to them. The Γ (by convention) is a context. In a programmer's terms, it's the list of each variable in scope, together with its type. For instance, `x :: Int, y :: Char` is a context. There are two logics everyone learns about when they learn logic: natural deduction and sequent calculus. In natural deduction, you can extend contexts (introduce new variables through `let` bindings, lambda abstraction, etc), but that's all you can do with them. In sequent calculus, you can do more interesting things with the context you have. For instance, if you have a context we gave above `x :: Int, y :: `Char`, you can turn it into a context: `(x, y) :: (Int, Char`. Now, your contexts are term-type pairs, rather than variable-type. You can also manipulate your contexts in many more ways (Our example showed us you can go from a context with `x` and `y` to a context with just one entry, `(x, y)`). Now, the main point of the article was showing how typeclasses can be modeled in the sequent calculus. If you extend your notion of what a context is one more time, we can allow a context to contain typeclass instances. When you have a predicated type class like `(Eq a, Eq b) =&gt; Eq (a, b)`, you can treat that `=&gt;` sign as an inference rule that changes the *context* you're working with. That is really the only point of the blog post. The `=&gt;` symbol in type classes makes sense as a logical implication in the sequent calculus. If you were to try and model typeclasses using natural deduction, it would be really awkward. That's it.
This series is great. Cheers.
The title of this post is intriguing; I'd love to see what's on that page, with Bryan at Facebook and now Simon joining. Unfortunately, it requires a Facebook login. Any way to find out what's there for non-Facebookers?
I love lenses, but the `lens` package in particular is just too huge and has complexities I don't understand, which is why I stick to the more straightforward `data-lens`. It's a good concept to cover, though.
I'm like you, but the more I see of `lens`, the more revolutionary it seems. My problem is that I'm too dense to have understood the basic 'van Laarhoven lenses' concept, and the generalization of it to four type parameters (lens families). I guess if I had that, the rest wouldn't be so impenetrable. I'll keep trying...
Instead of using `length x &gt; 1` as a precondition I recommend using the `NonEmptyList` data type defined in QuickCheck. quickCheck2 = quickCheck $ \ (NonEmptyList x) -&gt; absAverage x &gt;= 0 Correct, more efficient, and easy on the eye. QuickCheck defines several of these data types which can be used to generate data satisfying some condition, such as positive integers.
Part of the issue is that the philosophy of the `mtl` is to avoid the user needing to explicitly use `lift`, which is why we define all those classes for you.
This never bothered me. We speak English without understanding all the formal rules. Lens is like a spoken language that can express sophisticated ideas, but unlike a spoken language lens is type-checked.
&gt; they have different semantics with respect to termination. Indeed. They can also have different semantics with respect to space usage. Besides, you can do this [efficiently for any length you want](http://hackage.haskell.org/packages/archive/list-extras/0.4.1.1/doc/html/Data-List-Extras-LazyLength.html); there's nothing special about zero, so why bake the zero case into the compiler?
It's actually really easy, and the "complexities" are all auxiliary things meant for advanced uses. The intuition came for me when I looked at Scalaz's implementation of Lenses, which has a `modF` function of (Haskell) type `(Functor f) =&gt; SimpleLens a b -&gt; (b -&gt; f b) -&gt; a -&gt; f a`. In the same way that Church encoding represents a list as its `foldr` function, *van Laarhoven lenses represent a lens as its `modF` function*. The subsequent generalization to lens families simply *allows the source and target types to change in tandem during modification*. That's it (aside from `Traversal`s, etc.).
&gt; data-lens also doesn't seem to support applicative/monadic updates. e.g: You want to apply an a -&gt; f b function on a field of a Rec a to get an f (Rec b). `(^%%=)` does this.
Right, but I don't use `mtl`, I use `transformers` :)
...what's the URL for that?
Hashmap is lazy. Category and RelativeRank are not going to be evaluated until you print them out. That's going to be a large part of your memory problems. Also, the bang on the include doesn't help you. Size for hashmap is O(n). You'll probably want to remove that. That won't solve all your problems, but it'll solve a lot of them.
I feel uncomfortable using a library I cannot fit into my head.
Adam Chlipala has recently setup shop at MIT; they are working on very ambitious formal verification problems and Chlipala is a world expert in Coq.
I think Edward got it exactly right. I think the only mistake he made was not first distilling the main concepts into a core library with very few dependencies and some simple example operators for each type of lens. As you know, we both went to the same lens talk Edward gave. All throughout the talk I could not hold back a huge grin because I realized that lenses were basically a huge leap in moving Haskell closer to a natural language. The analogy I like to give is that if functions are verbs then lenses are adverbs. In fact, that intuition very closely follows how Edward formulates a lot of these lenses. He takes some common function (the verb), and parametrizes it on what was originally a hard-coded intrinsic behavior. This new parameter becomes "how" to do the function (the adverb), and it miraculously always has the general form of one of these lenses. So many things about the library are incredibly tasteful and elegant. Edward has already pointed some of these out, but I feel they bear repeating: * Lens chaining exactly matches accessor notation in objected oriented language. This wasn't something Edward contrived but rather a fortunate coincidence. * You don't need to make `lens` a dependency to define lenses. This is a really really big deal, because it means you can add language support for lenses (for "records done right") without making the Haskell language dependent on the `lens` library. Additionally, there is zero buy-in to his particular library. You just define lenses oblivious of what lens libraries are actually out there and any library can build their own lens operations to take advantage of your lenses. * Many of the features that `lens` implements are simply NOT POSSIBLE with other libraries, polymorphic update being the most significant one. The lack of this feature is what killed many other record proposals.
This isn't shifting from compile to run time, as I understand it, but shifting from "do every time you run the template" to "do once on loading the template" -- conceptually this is introducing a "compilation" vs. "interpretation" step for the template language. The target of "compilation" isn't assembly, in this case, but an in-memory function. So perhaps "optimized rather than direct interpretation." In any case, it's quite straightforward in Haskell in general. But for any given control structure, interface, etc., doing it can pose a set of tricky questions to answer. The problem isn't about expressiveness of language or expressiveness of macro systems -- it's just about thoughtful design, and the tradeoffs in terms of expressivity of the *interpreted* language (i.e. if it is sufficiently dynamic and powerful, then you can't do anything statically, but if you can do everything statically on the other hand then you might as well have a text file, etc.)
Makes sense.
Let's walk through the construction of a van Laarhoven lens from the basic idea of a lens as a getter and a setter. Start with a lens a a 'getter' `(s -&gt; a)` and a 'setter' `(s -&gt; a -&gt; s)` You could then tuple them up into a data type for lenses: data Lens s a = Lens (s -&gt; a) (s -&gt; a -&gt; s) or maybe even fuse the fact that they both start with (s -&gt;) to get data Lens s a = Lens (s -&gt; (a -&gt; s, a)) The former is the version I used in my stackoverflow response on lenses, the latter is what I used in data-lens. But this required us to make up a data type. Twan van Laarhoven came up with a way to CPS this form of a lens in a blog post a few years back. What he did was use type Lens s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; s -&gt; f s which looks complicated, but lets implement one _2 f (a,b) = (,) a &lt;$&gt; f b the fact that we retrieved the 'b' out of (a,b) is playing the role of the getter. the fact that we put back the parts we didn't use after fmapping is playing the role of the setter. This is just the `data-lens` lens in disguise! The tricky part is figuring out how to read from or write to it. We can read from or write to the lens by choosing the Functor. If we choose `Const` view l s = getConst $ l Const s then view _2 (x,y) = getConst $ (\ f (a,b) = (,) a &lt;$&gt; f b) Const (x,y) = getConst $ (,) x &lt;$&gt; Const y = getConst $ Const y = y if we choose `Identity`: over l g s = runIdentity $ l (Identity . g) s then over _2 (+1) (x,y) = runIdentity $ (\ f (a,b) = (,) a &lt;$&gt; f b) (Identity . (+1)) (x,y) = runIdentity $ (,) x &lt;$&gt; (Identity . (+1)) y = runIdentity $ (,) x &lt;$&gt; Identity (y + 1) = runIdentity $ Identity (x, y + 1) = (x, y + 1) This lets us update. The four parameter version of things follow when you go to write `view` and `over`. If you just type in those definitions without any signatures the type system will tell you what has to stay the same and what can vary. Similarly for the signature of `_2`. This lets us generalize the notion of a lens, by saying that the type of the contents you put back may not be the same as the type of the part you took out. type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t That may look scary but in 'getter/setter' form: * You have a getter: `(s -&gt; a)` that says you can get an `a` out of an `s`. * You have a setter: `(s -&gt; b -&gt; t)` that says if you replace that `a` with a `b` you get a `t`. Whenever you are in doubt in the four value form you can always instantiate a = b, s = t, or otherwise it isn't a legal lens. ;) Finally, we can see how the notion of lens starts to generalize: If you restrict the signature of `Lens` to only allow `view` by limiting the Functor to stuff that works like `Const` you get a `Getter` If you restrict it to only allow `over` by limiting the Functor to stuff that works like `Identity`, you have a `Setter. If you use &lt;*&gt; in the "lens" construction, you get a traversal, and the definition looks a lot like you were writing an appropriate instance of `Traversable`! both f (a,b) = (,) &lt;$&gt; f a &lt;*&gt; f b gives you the traversal of both halves of a pair.
sclv is exactly right here. It's not shifting from run time to compile time. It's shifting from template run time to template load time. And in this case it's tricky because you can't completely shift to load time because in a lot of cases the data that you want to substitute into the template isn't available at load time. So you have to create a clear distinction between load and run time. This distinction is trivial to make and statically enforce in Haskell's type system, but impossible in dynamic languages (or impure languages either for that matter).
No, I'm afraid not. I think Facebook got rid of their RSS feeds some time ago, and this is a closed group, so it wouldn't have a public RSS anyway.
I feel the Haskell code is a bit of... overshooting it? I wasn't even able to read it the first try, and granted I have a limited attention span, but... It seems over-engineered something awful. I assume you're doing this to learn Haskell (keep in mind that this is the context I'm typing in, in case I come off as patronizing). If that's not the case then all is moot here and you can ignore me. You say you wrote the Ruby version *afterwards* to convince yourself that it could be done. I think that what you should have done, was to take the Ruby version, which is simple and clear, and translate it to Haskell. type IdxURL = (Int,Text) -- #N, "url" type CatIdx = (Text,Int,Int) -- "cat", #N, #Tot -- for conversion: -- "99. Text" =&gt; Right (99, "Text") -- "somethingelse" =&gt; Left "somethingelse" urlOrCat :: Text -&gt; Either Text IdxURL urlOrCat t = (const t +++ second (drop 2)) $ decimal t -- collects a list of cat/urls into a map of url -&gt; cats aggUrls :: [Either Text IdxURL] -&gt; M.Map Text [CatIdx] aggUrls = ... this is where I am going to be a pain in the ass and say it's an exercise, as you seem to be doing this to learn anyway ... -- start with collecting to M.Map Text [Text] (url to just categories) first, if it's unclear how to do it printUrl :: Text -&gt; [CatIdx] -&gt; IO () printUrl = ... main = do ... inp &lt;- readFile inputFile mapM_ (uncurry printUrl) $ M.toList $ aggUrls $ fmap urlOrCat $ lines inp This was roughly the template I used. (Typing this from memory, since I'm on a separate computer than the one I tested on earlier when I saw your post, so I don't have the code in front of me.) Using Data.Text everywhere (no Strings), hide all the Prelude stuff. Data.Map to collect values with (++) seems fine. All very simple, straight-forward -- and I got a run time of about 1-1.5 minutes on the full file. Always get something that works first. Then think about rewriting it to use Parsec, HashMap &amp; stuff like that. If you want to force it to be "imperative" and print progress while it's collecting, stuff like that, you could just do some printing in between rounds of collecting N list elements? Or better yet: use Data.Concurrent for stuff like that, so you can keep the pure pure. 
Is the version bump to Heist the reason all other Snap packages are bumped too ? I thought it was christmas when I saw snap 0.10 on Hackage.
Very nice explanation. If you have the time then please consider contributing an article to [The Monad Reader](http://themonadreader.wordpress.com/), I would love to read it. PS: I think there is a typo in your demonstration of `view` acting as a getter: view _2 (x,y) = getConst $ (\ f (a,b) = (,) a &lt;$&gt; f b) Const (x,y) = getConst $ (,) a &lt;$&gt; Const y ^ Should be an 'x'
Fixed.
I share this sentiment. Lately I see a lot of people talking about the `lens` package and everyone seems to be really liking it. As the author of `fclabels` I wondered if I should maybe give up maintenance and let `lens` win. But after digging into the `lens` package a bit more I decided very strongly not to do this. Maybe (=definitely) I'm biased because I'm the author, but the `fclabels` API is so much more appealing to me. One main module, two optional modules for partial labels. We deliberately try to keep the main API simple, while still having a very extensible abstract core. I like polymorphic updates and I like composing without `Category`, but the increase in complexity is not worth it for me.
Thanks! Do you have a suggestion for a non-lazy map type I can use?
I just recently look at what's with the all the racket about lens, and one of my first thought when i read the examples was that, some of them look some like a bad perl joke. I think the main problem to my eyes is probably the large definition of operators for everything. For the lens defense, the pong example was alright. Overall I think i'll prefer stiking to normal haskell tuple/record manipulation, even though they are not always nice to manipulate.
A detailed Monad.Reader type article would be fantastic. I think your talk(s) contains everything that would go in the article, but having something written down, perhaps with a few side exercises for the reader would go a long way to furthering understanding in this area.
This is, I think, why my `mustache2hs` ended up having such good (comparative) performance, without even trying.
The snap release includes a bit more stuff. See [the announcement](http://snapframework.com/blog/2012/12/10/snap-0.10-released).
I am really getting interested in using this. Good job.
Yes, they are heavily static. But in the "heavily dynamic" applications I've seen a lot of the markup is still static. Benchmarking heavily dynamic applications is also going to be more involved and end-to-end performance will likely be determined primarily by other factors such as DB performance. I think these are the main reasons why we never got any complaints about Heist's performance. That said, I would also like to see what the speedups are for a larger dynamic application. I don't know if I'll get around to doing it myself though, since it's a pretty big job.
I used to have [exactly the same opinion](http://softwaresimply.blogspot.com/2008/03/to-template-or-not-to-template.html). Then I built a real website that way. Heist originated as a direct result of the frustrations I experienced in doing that. Heist gets you several things. First of all, it's a language that designers know. When you hire a designer, you're frequently hiring them because of their graphic design skills, not their programming skills. They're not going to want to learn your obscure Haskell DSL for generating markup. Second, it lets you modify your markup without recompiling your code. The designers benefit here too because your backend coders can work independently of the designers. And if the backend is done, the designers can play all they want without the painful compile cycle.
Quoting [myself](http://www.reddit.com/r/haskell/comments/148ete/sirea_simply_reactive/c7byr07) from when RDP was last posted: &gt; It seems like the author has some interesting ideas, but IMO so far he has not really communicated them in a way that the rest of the FP community can understand what he is talking about and engage with them. I would love to see a series of complete examples, with type signatures, discussion, and working code.
&gt; I am pretty sure a designer used to typing &lt;h1&gt;Blah&lt;/h1&gt; can be taught to write (h1 "Blah") instead. They probably *can*, but that's not the point. The point is that this is just one less thing they would have to worry about. And typically I imagine that experts in design would prefer to not have to worry about yet another template syntax. &gt; Did you ever consider just building something for that on top of the GHC API? Snap has a dynamic code reloader that does this. It does overlap a little bit with Heist's reloading support, but it's still useful. For example, at my job we have a production website with 125 modules. At this scale the dynamic code reloader is unusable, but the Heist reloader still works fine. The biggest problem for me with using a Haskell DSL to generate markup was that it provides TOO MUCH abstractive power. The abstractions I built ended up being very domain-specific and as complexity grew, I forgot things and it became very hard to find the appropriate place to make a markup change. Yes, with great discipline you might be able to overcome this problem, but then you'll have to go through the entire process again for each project. Providing a more complete explanation of this argument would be a much longer discussion. So for now suffice it to say that I have built several websites with Heist and I vastly prefer this approach. The level of abstraction provided by Heist is just right in my experience. You could write this off as author's bias, but a number of other people have said the same thing (see for instance the second paragraph of [this article](http://cdsmith.wordpress.com/2011/02/05/html-5-in-haskell/)).
&gt;I'm sure designers would ultimately appreciate the vastly better tooling you can build around templates that are actual programs in a real programming language The vastly inferior tooling you mean. There's already tools to check if your html is valid or not. So that isn't providing better tooling. But you also remove all the huge ecosystem of html tools that already exist if you switch to a different syntax like haskell's.
&gt; I am pretty sure a designer used to typing &lt;h1&gt;Blah&lt;/h1&gt; can be taught to write (h1 "Blah") instead That's what every person who made a new template syntax has said. That is why there are hundreds of template syntaxes, and virtually no tools supporting any of them.
A lot of this comes down to some very personal attributes, I suppose... it seems that for some people, the `lens` package just fits nicely with their though processes. For me, that's very much not true. I'm far from impresses at the massive number of new names defined in the package, as this writeup seems to be. I'm even more unhappy when many of them are infix operators that have little relationship to existing notation. And on top of that, many examples use identifier names that I was surprised to learn Haskell even allows, like `_2`, which certainly didn't help. This is a HUGE obstacle to becoming familiar with the package for me... but I fully understand that this is a personal thing, and will be different for different audiences. I will say, though, that you should be more skeptical about the claim that lenses "solves" polymorphic updates. The issue with polymorphic updates is that fundamentally do not compose well, and are at a basic level incompatible with meaningful modularity. While lens (at great cost in complexity) adds support for polymorphic updates, it remains difficult to do horizontal composition with them, for instance. Consider: data Foo a = Foo { one :: a, two :: a, three :: Bool } What are types for lenses for `one` and `two`? Can you compose them together and use the result to perform a polymorphic update?
Well, the point is that you don't have to use the lens library specifically. The concept of a lens is something more general than his library. The point that most `lens` advocates try to make is that even if you don't like Edward's specific operators or naming conventions, the core lens formulation is spot on and very elegant and general. Like I said before, I can safely define lenses knowing that I'm not buying into his particular library. When I define Van Laarhoven lenses, I'm basically tapping into sort of a universal API that anybody can consume, with or without Edward's `lens` library.
You usually don't need to do explicit recursion. There are a lot of higher order functions that will express the intent of most types of looping.
Oooo. This is nice. &gt; fmtI = lit "The value of " ^ char ^ lit " is " ^ int Hm. &gt; (^) :: ((String -&gt; t1) -&gt; t) -&gt; ((String -&gt; t2) -&gt; t1) -&gt; (String -&gt; t2) -&gt; t Can it be `(&lt;&lt;)`? Not sure. Looks arrow-ish, too. Someone with a type checker and more time on their hands, investigate!
We have just under 500 examples/doctests inside of the lens documentation. Plus a whole bunch of extras strewn through the wiki, readme, tutorial, etc. We're definitely still looking at add more though. There are parts of the library where the examples are still fairly sparse.
We have, at last count, 99 operators in lens. Something like 80 of them follow a pretty common pattern, based on existing infix operators. * `operator~` -- function update to the target of the lens using (operator) and a supplied value. * `operator=` -- update the target of the lens in your state using the operator and value. * `&lt;operator~` or `&lt;operator=` -- same as `operator~` or `operator=`, but also return the changed value, or a monoidal summary if multiple targets were involved because of a `Traversal`. Is this vocabulary larger than would be strictly minimal? But it is convenient and easy to learn and you don't have to use it. lens +~ 2
I don't think so; I think the plan is to move Snap to [io-streams](https://github.com/snapframework/io-streams), when that's ready. It'll be interesting to see whether that gives Snap more speed, and how much.
Amen to that! And you know, one of the first projects I attempted in Haskell was an emacs `org-mode` parser, with the plan that I could have a command line search tool. That never happened, but bits of the parser still kick around - https://github.com/ocharles/Orgdex/blob/master/Orgdex/Parsing.hs#L67. This code still makes me smile when I read it. As an aside, what I also find interesting is that code I wrote 2 years ago still mostly looks how I write code today... I find that a testament to maintainability of Haskell!
I had a difficult time when I was working through some of the documentation. But in-person chats with edwardk and hands-on experience had made things clear for me (at least for the subset that I use: lenses and traversals) and I also understood what prisms and isomorphisms are all about (though I barely used them). I don't think any single one of the previous lens libraries were usable in practice (Because they lacked polymorphic updates, monadic updates, traversals, etc). I really think "lens" deserves this name as it *finally* solves the problem in a comprehensive way, without a large compromise (except difficult to read types, perhaps). "lens" finally allowed us to throw away our ad-hoc TH to make polymorphic record updaters in [lamdu](https://github.com/Peaker/lamdu), yay!
There are two type parameters, so (&lt;&lt;) wouldn't work. It could be composition in a category but I can't see an identity function at a glance. Maybe one is obvious from the post? I'm just looking at this type signature.
I happily use the Lens library, and also dislike a proliferation of operators. I basically import it qualified, and mostly use just these combinators: `Lens.over`, `Lens.set`, and `Lens.view` (also tend to import `(^.)` unqualified). These are basically the same combinators of the various other lens libs. Less frequently, I also use: `Lens.mapMOf`, `Lens.sequenceOf`, and `Lens.traverseOf` which are really great. For example, if I have a tuple: `(Int, (String, Maybe String))` and I want to convert it to: `Maybe (Int, (String, String))` I can use: `Lens.sequenceOf (Lens._2 . Lens._2)`. I think if one is accustomed to `Data.Traversable`, `sequenceA` and `traverse`, that kind of code is very satisfying and natural. The wonderful thing is, it is so easy to "scale" this approach to larger, more complex composite types and do these kinds of transformations very easily. I have an AST, and I want to apply a monadic fold over each field recursively in the entire AST? `lens` makes that kind of thing elegant and easy. If you dislike the cartesian multiplication of line-noise operators in Lens (so do I!), just avoid them. The elegance of the model behind lens is great and the power just makes all other lens libraries seem completely inadequate.
Just ignore the line-noise. Take a look at how we use `Lens`, for example in: https://github.com/Peaker/lamdu/blob/master/src/Lamdu/CodeEdit/Sugar.hs We import `(^.)` and `Lens` qualified, virtually no line noise :)
I wonder if you can give as `IsString ((String -&gt; w) -&gt; w)` instance and be able to write it as: fmtI = "The value of " ^ char ^ " is " ^ int 
If it used the Categorical (.) it would have a problem with polymorphic updates. The fact it works as it does in C is a (fortunate? unfortunate?) coincidence, but allowing `Prelude.(.)` for composition and allowing polymorphic updates at the same time is a *huge* win.
I'll be honest: I have turned on -O0 because GHC was getting stuck in infinite optimization loops that I did not understand :[
I only use operators in my `State` monads, because everyone knows C-like languages so: a.b += 3 is pretty clear! Simply because the world of imperative languages had spread this operator vocabulary. But I still avoid the `~` functions because they're not yet an established vocabulary (I'm not helping, I know) and I'm not yet sure the price of extending the vocabulary is worth the slight brevity. Also, as an early user of `lens` I had to fix my operator use as they were still in flux :) Now that you've explained the operator scheme, it seems like less of a burden to learn.
Getting a little off-topic, but what is with the obsession with performance from people making sites that could easily be handled by a shell script getting forked per-request from inetd? Are you actually hitting the limit of what a single cheap server can handle with snap-server?
&gt; They probably can, but that's not the point. The point is that this is just one less thing they would have to worry about. And typically I imagine that experts in design would prefer to not have to worry about yet another template syntax. Let me make an analogy. Suppose you were leading a team writing some piece of software in Haskell, and a new developer, Frank, joins your team. Frank is an expert PHP hacker, and insists he'd really prefer to write large parts of your system in PHP, and connect it via some elaborate runtime bridge to Haskell. Don't worry, he says, this runtime bridge will be uber-optimized, at least 3000x faster than the last runtime bridge he wrote at his last company. :) Would you let this developer write in PHP, because he felt more comfortable and productive in PHP, at least initially? Probably not. Why is that? Perhaps it's because as project lead, you need to keep in mind the long term maintainability and sustainability of the project, and having large swaths of your system in a separate language not under the purview of the Haskell typechecker is probably not good for that, especially when you want to make changes later and be assured nothing broke. Not to mention all that complexity of the runtime bridge and the slowness of having to marshal data back and forth across it. Heist is PHP in this scenario. :) Frank is the designer who would 'feel more comfortable' using HTML-like syntax for writing templates, who doesn't want 'one more thing to worry about'. My point here is not even to say that doing templates with a Haskell EDSL is better, or that we should ignore what tools designers are most comfortable using. Those are certainly things to factor in, but they cannot be the sole basis for a decision. (Maybe if Frank was _really_ productive with PHP, you would decide it compensated for the added complexity, slowness, and loss in type safety) I am just saying, as developers, we have a professional responsibility to advocate for whatever technology we think is best for the long term health and productivity of the software system as a whole. We should not cease this advocacy at arbitrary boundaries of system functionality. I think developers are uniquely poised to see the big picture here--a designer is not trained at keeping a huge amount of complex functionality organized, comprehensible, extensible, and maintainable.
&gt; but what is with the obsession with performance from people making sites that could easily be handled by a shell script getting forked per-request from inetd How do you know that?
I don't *know* that, which is why I asked if he was actually hitting the limit. Given that 99% of the web falls into the shell script category, it isn't unreasonable to ask.
It strikes me that such unrestricted use of exception propagation will allow you to, for example, compute the intensional modulus of continutity of a functional. Since such a value is not extensional in general, you can no longer replace functionals with extensionally equivalent functionals without possibly changing the behaviour of your program. i.e. You can no longer safely optimize your program.
 newtype Foo o a b = Foo ((o -&gt; a) -&gt; b) instance Monoid o =&gt; Category (Foo o) where id = Foo $ \k -&gt; k mempty Foo a . Foo b = Foo $ \k -&gt; a $ \sx -&gt; b $ \sy -&gt; k $ sx &lt;&gt; sy I haven't checked the laws.
Tried it with a newtype and `FlexibleInstances`, but type inference doesn't seem to work very well with it.
This is a great series, thanks for writing this up!
Not sure if Oleg's code is on hackage, but the idea seems to be the same as http://hackage.haskell.org/package/xformat I don't believe that xformat allows specifications like "%f4.2", but perhaps that could be addressed by a `data FloatF = FloatF (Maybe Int) (Maybe Int)', and then defining a typeclass which has methods to deal with the cases where one or both of the format specifiers are Nothing. But it might be too much to ask for a: &lt;x&gt;%"float Dot 5"%&lt;y&gt; when with the usual unsafe printf it would have been a shorter &lt;x&gt;%f.5&lt;y&gt; 
so there is no actual implementation of yield available?
Termination (not being bottom) is part of official Haskell semantics, space usage is not.
The catch is that not many people know Haskell, and it's not just a reskinning of a language that everyone already knows, so requires actual effort to learn.
Changing completely how you think about programming is seen by many as too much effort, so surely the rewards aren't worth it. It's a shame they're wrong.
&gt; But, I've found that dealing with and io that branches in Haskell is still difficult. Have you tried using the [errors](http://hackage.haskell.org/package/errors) and [monad-loops](http://hackage.haskell.org/package/monad-loops) packages?
Upvoted for the comments about cabal. I use http-conduit for a project I've got and I can't even install yesod now without breaking it. I've already blown it to crap 5 times this month. Have mostly given up.
&gt; Those are certainly things to factor in, but they cannot be the sole basis for a decision. They're not. I already gave other bases. I get your point, but I don't think it completely applies here because PHP is much less of a hegemon than HTML. &gt; I am just saying, as developers, we have a professional responsibility to advocate for whatever technology we think is best for the long term health and productivity of the software system as a whole. This argument doesn't hold much water to me...and I am one of those who tends to be more of a perfectionist looking for the Right solution. If what you say is true, then you should be axing javascript altogether and building your own browser with a better bytecode language that is suitable as a target language for Haskell code generation (or something similarly grandiose). And while you're at that, how about completely reinventing the X86 architecture to something more sane. But you're not going to do that because in the world we live in you do have to cease advocacy at arbitrary boundaries as dictated by necessity.
Cabal is exceptionally good package manager. The packages are the problem.
Paraphrasing from a comment that I read in this subreddit some time ago (don't have time to look it up now), but in Python (e.g.), there is approximately one way to do anything (file I/O, GUI programming, lexing, parsing, whatever), and usually exactly one correct way. By contrast, in Haskell, there are many ways to do the same thing. For instance, with file reading/writing, it took the community years to figure out monadic I/O; now, we're beginning to realize that lazy I/O is often not the best idea, so we've got various solutions like iteratee I/O and conduits, which often aren't the most intuitive for beginners to use (though, in their defense, they do a good job of what they're designed to do). This tendency of Haskell to support many ways of doing the same thing makes it difficult for beginners who are used to learning new programming languages in a very structured, linear, cut-and-dried way. Also, GUI programming in Haskell is a bit of a nightmare from a pure functional programming standpoint. Projects like reactive-banana are making it more tractable, but as it stands, you might be better off doing your GUI stuff in C/C++ and using the FFI to call into a Haskell backend. Edit: Also, cabal as a distribution tool isn't fully mature (to put it mildly), and of course, there's the Catch-22 that the Haskell ecosystem isn't as full-fledged as that of other programming languages because it isn't widely used, and it isn't widely used because the ecosystem isn't as full-fledged.
If someone wants to see a large application, [Pandoc](http://johnmacfarlane.net/pandoc/) makes extensive use of Parsec to parse many lightweight text markup formats. This is the type of data where a Parsec parser shines, because the syntax is often irregular and context-sensitive.
I mostly agree. Haskell holds the promise of being as fast as C, as concise and fun as Ruby, highly error-resistant ("if it compiles, it probably works"), better at abstraction than any mainstream language, and effortlessly parallelizable. It's not quite there yet; right now you probably have to choose some subset of those for any given code. But it's improving, and you get to write all of the code with those various properties in the same language and interface them together, which is quite something already. And some of them, apart from being fast, are things that mainstream languages just cannot do period. The catch: it's got quite a learning curve (this is more of a dark lining to the silver cloud, though, because for the most part it's very useful and important ideas that other languages simply don't or can't express, and will make your brain explode in very good ways, but you do have to learn them), and the execution model / performance characteristics can be highly unintuitive. It's possible that it doesn't provide enough direct control over the low-level details of performance, period. But in any case there's probably a separate learning curve for the language itself and then for understanding performance and reliably being able to write code that has it. Edit: And yes, as others have mentioned it's got teething pains, the language itself is evolving (LANGUAGE pragma), package management and infrastructure aren't where they need to be yet, some problems (streaming IO) are still being solved, and there isn't yet a library for every single possible thing. To some degree this is implied by "not a mainstream language". Another language you might be interested in, with some of the same features and ideas as Haskell but a different emphasis (less high-level abstraction and type system wizardry, more low-level control, less purely functional, more of a mixed-paradigm thing) is [Rust][1]. [1]: http://rust-lang.org/
It's a good thing they're wrong. It's a shame they don't see it differently.
I am new to Haskell as well and as far as I can see the points pointed out by several people here with unintuitive I/O handling is what has made me lose some of the enthusiasm I felt to begin with reading about the cool features of the language. I think it at least slowed down my learning progress. I also think the language is slightly more tilted towards the academic side of the user scale, correct me if I'm wrong, which I believe make the language a bit more inaccessible and less used for "regular" users. I've also had problems with the build system but I figured It was mostly because of my misunderstanding and bad configuration under Gentoo linux. Apparently not. 
&gt; There is also no good package manager. So there is a package manager for C++? Java? C#? Don't get me wrong, there is a problem with package manager in haskell, and those languages works differently than haskell, but IMO they are not comparable / this is not what hinders haskell going mainstream Edit: Putting in another way, those languages went mainstream despite (before) having good package managers.
&gt; By contrast, in Haskell, there are many ways to do the same thing. *While most people would think this is good, you don't want to get stuck deciding which way is the correct way.
Presumably, the fact that it is not pure, given that the parent said &gt; Also, GUI programming in Haskell is a bit of a nightmare *from a pure functional programming standpoint*. Of course, your point is also valid that there are perfectly good ways of programming GUI's that use callbacks just like imperative languages do, so I strongly disagree with the parent's conclusion that &gt; you might be better off doing your GUI stuff in C/C++ and using the FFI to call into a Haskell backend.
Hm, that's clever. Simple but effective. I like this a lot. **EDIT:** THIS IS AWESOME. Oh, the possibilities!
Oh, you beauty! This is a really sexy library. I'm playing with it right now.
Ack, I meant to mention that at the end. I've made a note to add that when I get home. Thanks for the reminder!
The official semantics (such as there are any), sure. But I hold myself to a higher standard when reasoning about programs. As does GHC when manipulating them. I loves me some denotational semantics, but that's no reason to ignore the reality of operational semantics.
&gt; Also, GUI programming in Haskell is a bit of a nightmare from a pure functional programming standpoint. Projects like reactive-banana are making it more tractable, but as it stands, you might be better off doing your GUI stuff in C/C++ and using the FFI to call into a Haskell backend. Even with all its flaws, I think that wxHaskell is one of the easiest GUI frameworks to program in, in *any* language. Haskell is my favorite imperative programming language.
Maybe your global package metadata is messed up somehow? If you haven't read [Albert Lai's writeup](http://www.vex.net/~trebla/haskell/sicp.xhtml), do. It's really helped me to have at least a vague inkling of what cabal's actually doing.
It's not easily portable. Try getting a GUI and OpenGl code working nicely on Windows... I guarantee much cursing and dirty hacks. And forget about using the VS compiler.
You obviously don't know about NuGet.
I was under the impression that a large part of the issue was the aggressive trans-packages optimisations performed by GHC, which to my knowledge you don't really have in other languages.
Parsec is great. For my [project](http://hackage.haskell.org/package/penny-lib) I have tried Parsec, attoparsec, and Happy/Alex. Parsec and Happy/Alex were roughly equivalent in speed. (I remember that one was slightly faster and the other used slightly more memory, but I can't remember which was which.) Attoparsec was slightly faster. The issue with attoparsec is that its error reporting features are not nearly as good as Parsec, and it does not track the source location. Of course maybe this is part of why attoparsec was faster. attoparsec for me was not so much faster that it was worth it to maintain both an attoparsec parser and a Parsec one (I would have been willing to maintain the attoparsec one for speed and then fall back to the Parsec parser if the attoparsec one failed, in order to get better error messages.) YMMV; attoparsec is sometimes reported to be much faster, but that just wasn't the case with my grammar.
&gt; If what you say is true, then you should be axing javascript altogether and building your own browser with a better bytecode language that is suitable as a target language for Haskell code generation (or something similarly grandiose). I think we generally agree here. I am saying we should do a cost/benefit analysis, not implement The Right Solution (tm) at all costs. This cost/benefit analysis should factor in designers productivity, developer productivity, network effects from existing tools and ecosystems, future maintainability, etc. My point was just that we should not _a priori_ accept some arbitrary constraint like 'Thou shalt have thy templates written in an HTML-like syntax'. Instead, I view the fact that designers prefer HTML-like syntax for templates, etc, as things to factor into an overall equation. It sounds like you agree about that, but feel like the this equation works out in favor of HTML-like templates. Which I totally get. Incidentally, if you were operating at a large enough scale, it _could_ pay for itself to invest time in building a better javascript a better web browser, etc. This is why huge companies like Google actually have people employed full time doing exactly this. For them, this investment pays for itself. For a small company, not so much. I would not call this an 'arbitrary' boundary at all, I would simply say that the investment doesn't pay for itself in that context. Getting back to the original subject, I don't really know what the 'right' answer is. This discussion was prompted because I see large advantages to templates just being ordinary Haskell and I wanted to understand why people think that's a bad idea or it doesn't work out so well. I take seriously your report that HTML-like templates work out better in your experience. Thanks for an interesting discussion!
Well, it's not a surprise that exceptions allow you to observe more about your programs, since this is another classic Felleisen theorem.
I understood it was a package manager. I got my question wrong: How does it compares to *hackage? 
I've found that a lot of people think cabal is "broken" when dependencies break, but cabal always gives you a list of these about-to-break packages, and you can just include them on the install command line to rebuild them while the new stuff installs.
&gt;Erm, no. :) Haskell can do one better - it can check that the template is well-typed, in addition to checking for well-formedness!! What do you mean well-typed? Html doesn't have types. You seem to have gotten very confused somewhere along the line. We're talking about html. &gt;Serious question: what is the 'huge ecosystem' of HTML tools that exist, and what do these tools do that could not just be handled by the existing tooling around Haskell itself? Have you never seen or heard of a web designer before? http://www.adobe.com/products/dreamweaver.html http://www.microsoft.com/expression/products/Web_Overview.aspx I assumed your question was genuine, but that does not appear to be the case. It was answered, and you just want to argue about it. You don't have to like html, you don't have to use html. Blaze already exists, if you want it, go use it. Don't ask "why do people like html templates?" if you really mean "please let me argue about how much better haskell templates are".
First there are installation and distribution issues. It is not a light-weight install, not pre-installed on most platforms, and compiling things for Windows is a pain. Also, it is not easy to define a Haskell compiler of your own. Next, technical issues. Space leaks are the single greatest flaw you will encounter. Space leaks are like Haskell's version of segmentation faults: common for beginners and difficult to debug. The next biggest issue is learning which libraries and idioms are high performance. API issues: Haskell's type system is fantastic and never really gets in your way. However, typeclasses are very fragile. When they work they are amazing, but when they don't work they require all sorts of extensions and hacks to work around. The library situation is great, with the exception of GUI and networking utilities, which are notorious for not compiling on all platforms and breaking on compiler upgrades. The job outlook is okay but not terrific. If you learn Haskell you more likely find a job as a Scala, F#, or Ocaml programmer or as a web developer (or, you start your own business).
I believe Haskell's IO model is the best there is. The main thing that trips up newcomers is that it syntactically punishes you for using global mutable variables, for a good reason. However, once you get over that you will discover that Haskell has first class support for IO because of its elegant model, even better than mainstream languages.
Yeah, i'm trying out this new Tesla Model S. And it is a dream come true compared to my Toyota Corolla. But there are millions of people driving corollas and civics and only few hundreds that drive Tesla Model S. It obviously not very popular. What's the catch ? Oh yeah, i did not look at the price sticker yet. NewHasker. The catch is in the price sticker. Things cost money (time, effort). There are cars that are cheap and driven by millions and cars that are expensive. There's coca cola that is very cheap and hundreds of millions of people drink it and there's Dom Pérignon. There are millions of nurses (2-3 years of vocational school) and thousands times less doctors (10-15 years of very expensive education). The more something cost, the less people can afford it. Haskell is VERY expensive (time and effort wise) compared to such languages as python and ruby. Not many people can afford it. If you can, you are lucky. You are the rich guy. 
Maven is much more than a mere package manager, but a way the bring order to the chaos to the hellfire that was Java. Two years ago, I started to develop in Java after a 12-year hiatus. It is amazing how far Java progressed, because Java development in the 90's was soul-sucking. Haskell will mature. If Java can do it, so can Haskell. 
&gt; but when they don't work they require all sorts of extensions and hacks to work around. Or just don't use typeclasses to solve problems when they don't work to solve that problem :)
Is real climb to learn the language
Also `Foo o` = `Cokleisli ((-&gt;) o)`, which is a proper `Category` instance (and more).
I'll clarify: templates, expressed using some HTML-like syntax ala Heist, have types. You can think of a template as a function of several variables, which will be substituted into the body of the template. Whether the template is expressed as an actual Haskell function or with some sort of HTML-like syntax like Heist template doesn't change that. I know about tools like dreamweaver, etc. I guess I'm more asking what is the workflow around these tools that is so productive. On some level, you are making changes to HTML, seeing the results, making more changes, etc. Oh, and building up a library of reusable components that you draw on when building pages and so forth. This feels like programming to me, and the tools that programming languages have for organizing this sort of thing are quite good. Is it the WYSIWYG editors that these tools support that is so important? Something else? I am genuinely asking.
Cool. I'll warn you that the code is still pretty ugly inside and the documentation is pretty much nil. Those are top priorities for clckwrks 1.0 though (plus unit tests, and some other stuff). The goal for clckwrks 1.0 is 'not embarrassed to show it to other Haskell developers'. ;) When I split the current clckwrks package into two packages, the code will get a lot nicer (because I will do a bunch of cleanup at the same time).
&gt; Have you actually read the code on the language shootout. On just about any problem, you will find idiomatic Fortran beating fairly specialized Haskell that makes liberal use of Foreign. This is silly. The shootout doesn't allow libraries that don't ship with the compiler; most of the ugly stuff in, say, [the Haskell solution to the n-bodies problem](http://shootout.alioth.debian.org/u32/program.php?test=nbody&amp;lang=ghc&amp;id=1) is implementing and manipulating mutable vectors. If you were solving the problem in a real world context, you'd find that [someone has already done the hard work for you](http://hackage.haskell.org/package/vector). Meanwhile speed has never been idiomatic Fortran's problem.
&gt; Haskell is VERY expensive (time and effort wise) compared to such languages as python and ruby. Not many people can afford it. I've found Haskell *easier* to learn than Ruby. Likewise in college, I found Scheme easier than Java. Yes, I suppose I'm weird. But I exist!
&gt; I believe Haskell's IO model is the best there is. Really? I think I/O (not the monad but the philosophy), especially lazy I/O is one of Haskell's biggest warts. [Streams](http://okmij.org/ftp/Streams.html) are elegant but complicated. I think this [old Reddit thread](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/) showcases some of the difficulties in writing working code.
Sometimes it seems to be getting steeper.
Thanks, this was very informative.
It's not in the browser.
Do you have any (non-anecdotal) evidence that learning Haskell is worth the effort?
&gt; Haskell holds the promise of being as fast as C Is it as fast as C? Is it even as fast as Java?
I pretty much never use type classes these days, save the built-in ones that I can usually `derive` or easily define. I feel much happier for it.
Free monads are basically control abstraction. Data abstraction is kind of annoying in Haskell. In ML, you can have a module export a type T and operations on that type. In the implementation, you could instantiate T to `Int` and it would remain abstract in the interface. In Haskell, having an abstract type like that requires you to define a `newtype`, which is a whole load of syntactic baggage.
Cabal isn't and never has been a package manager. Hell, it's right there in the name --- Common Architecture for Building Applications and Libraries. No mention of Tracking, Upgrading and Removing Packages. We need, uh, TURP for that.
The problem is trying to do I/O in Haskell using standard Haskell idioms and a Haskell-ish notion of elegance, which many people try to do with limited success. The reddit thread you link to is an example of this. In contrast, accepting `IO` as an imperative EDSL--something like C, only nicer--and simply writing imperative, strict I/O in procedural style is very pleasant. But only if you take it on its own terms.
checkout elm
&gt; * Debugging is harder (due both tools and nature of language). On the flip side, debugging is easier when you don't have to mock an entire system to run a test.
I know, but does chrome debug in Javascript or Elm? Is it json or some variant of S-expressions? The other point I would have made is that there isn't a Haskell based operating system yet, which can be a lens through which to see a modern browser, an os in an os. Edit: I meant to conclude, that is why C still dominates the proper os and how JS accelerated to dominate the web os, because there was just no other competition. HTML now is the goto tech for GUIs even for the desktop. If Haskell wants "to win" it needs an OS optimised for itself, or it needs to replace the browser paradigm whilst solving teh problem of security and identity management (UNIX had that sorted a number of decades ago). The latter I think is totally possible because I view it as a turd of a paradigm that has been polished for a decade now, and I'm sure we could do better.
Ah. Thank you!
Can a plugin depend on functions provided by another plugin? How does that work? =)
I didn't know that either - and I'm the person who admins the benchmarks game ;-)
No, but it's within a factor of two [1]. That's close enough that the difference simply doesn't matter 99% of the time. [1] - http://shootout.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=all
Why would you expect people to comprehend the website text when they seem incapable of reading the project name that's shown at the top of every page? :-)
That's would be ideal. Any collected data that has statistical significance would be good - I'm thinking statistics like development time/bug count etc. Here's a relevant talk by Greg Wilson about beliefs and software development http://vimeo.com/9270320.
I work in DevOps. This is the kind of thing we would kill for!
Haskell *is* easily portable. The thing is, Windows developers hardly ever step up and actually contribute code that will fix their specific problems with particular libraries. I would generalize the Haskell-on-Windows community as okay at complaining, poor at providing details, and very weak when it comes to actually fixing stuff. (This mirrors my experience of Windows-based developers in general, who seem to come from a low-competence mirror world that is much like our own except for everyone having half the IQ and drive.)
The reason regular expression libraries are poorly maintained is because we all use parser combinator libraries like `attoparsec`.
Suppose you have a Heist template with an `&lt;apply-content/&gt;` tag. That is like a function of a single argument, is it not? Trying to apply that template without supplying an argument is a type error... you either fail at runtime or do something semiarbitrary like strip out the 'free' variable. Likewise, if the template uses tags `&lt;arg1/&gt;` and `&lt;arg2/&gt;` that it expects to be bound by whoever calls `&lt;apply&gt;` on the template. That is a two argument function, where the parameters must be passed by name. This is just a simple example of types in templates. Beyond just arity checking, other forms of typing might be useful as well, like maybe you want to enforce that an input to a template is a sequence, etc. The input to a splice is just a DOM element, right? That seems like dynamic typing to me. You can't pass an Int to a Splice, you have to pass a DOM, and at runtime the Splice parses that into an Int (and if it was expecting a String, that will only be detected at runtime). Btw, even though it seems like I am hating on Heist, from what I have seen of it, it seems like a nice point in the design space. :)
Do you have some examples of Haskell code written in this style?
So, what is the `experiment` function that's referenced?
Go for what?
I'm wrong, then. Sorry for the FUD!
there's a huge number of good tutorials and books (I think not too many people know about the books by Hudak ("*School of Music*"), Thompson (*Craft of FP, 3rd ed*), Hutton *"Programming in*" and Bird (*Pearls of Functional Algorithms*). Also many people here and on SO and beginners mailing list to field Q's http://haskell.cs.yale.edu/euterpea-2/haskell-school-of-music/ http://www.haskell.org/haskellwiki/Meta-tutorial http://www.haskell.org/haskellwiki/Tutorials http://www.haskell.org/haskellwiki/Books_and_tutorials 
I think you should try a users group and talk to devs who have used Haskell and other languages. effortsto define metrics for productivity and code quality (ABC, Kemmerer, Halstead maintainability, cyclomatic complexity), haven't really borne fruit; every method has vocal critics, even comparing LOC's is non-trivial. I've seen ruby coders that are vertical compression maniacs end;end;end;end;end;end;end http://wadler.blogspot.com/2011/04/empirical-comparison-of-seven.html http://wadler.blogspot.com/2011/09/experiment-about-static-and-dynamic.html https://news.ycombinator.com/item?id=4136752 haskell vs python http://www.scala-lang.org/node/3069 (eye tracking trial says scala is preferable to java
&gt; You can't pass an Int to a Splice Sure you can. intSplice :: Int -&gt; Splice m You can't do that from data coming from the template, but that's fine. If it's coming from the template, it's Text. Why try to jury rig Ints into templates when they aren't part of the HTML spec, don't belong there, and can be passed in as above perfectly easily?
To be fair, you're linking to a post that was exploring the categorical connections of such an algorithm. If you view the full thread, there are plenty of less abstract solutions.
Data abstraction. Doing it on a module level requires you to define a new type, which means if the type you're abstracting is just a synonym, you'll need a newtype to hide the implementation, which is very clunky. You could also do it with an existential type bundled with a dictionary of its operations, but this seems to be uncommon in Haskell code.
Very pretty!
For logging, I use hslogger in a number of my projects. I find it easy to use, and think it would be useful to have in the platform
I spoke with tibbe yesterday and he's interested in splitting this package up so that there is a minimal dependency that doesn't bring in all of Snap with it. If you're interested, you should contact him and maybe help get the ball rolling :)
Consider that a C++ callback needs to manually capture all the free variables. Haskell makes callbacks *much* easier than C++. There isn't really much of anything that is easier in C++ than in Haskell, except controlling the specific operational semantics you want. With GUI applications, this is rarely very important.
A Haskell API can contain IO actions...
Thanks for the info! I'll definitely look into it during the holidays if it remains undone then.
Say you wanted an abstract Stack type in Haskell, secretly represented as [Frame]. In ML, you could declare a module with a signature like (in haskell like syntax): signature Stack where type T push :: Frame -&gt; T -&gt; T pop :: T -&gt; Maybe T peek :: T -&gt; Maybe Frame And then instantiate that T in your implementation to [Frame], and write functions `push :: Frame -&gt; [Frame] -&gt; [Frame] = (:)`, etc. In Haskell, to achieve similar data abstraction, you have to do this: module Stack ( Stack, push, pop, peek) where newtype Stack = Stack [Frame] push v (Stack x) = Stack $ v:x -- etc Which is much uglier because you're creating a `newtype` just to hide the `[Frame]` implementation. Not to mention you can't instantiate the same signature multiple ways. One solution is to use type-classes and associated types, but they are unfortunately global, and are irritatingly brittle. One option I have only recently considered is using records plus existentials, but it's still uglier than ML or Agda. 
the closest things (which really aren't close) are "+RTS -s" and GHCi tools like :show, :steplocal and stepmodule, and (many) tools for memory/GC monitoring, visualizing AST http://www.haskell.org/ghc/docs/latest/html/users_guide/ghci-debugger.html#single-stepping http://felsin9.de/nnis/ghc-vis/ http://www.well-typed.com/Chalmers/Presentation.pdf threadscope http://www.mgsloan.com/wordpress/?p=36 (using src-exts 
It's more that clockworks.com, clockwerks.com, klockworks.com, klockwerks.com, etc, were all taken. The problem seems to be land grab nature of DNS. :-/ Anyway, I think it looks funny to see clockworks with all the oo in there now.. :)
I added a link. experiment :: (ComonadStore s w, Functor f) =&gt; (s -&gt; f s) -&gt; w a -&gt; f a If `w a` is a store of `a`'s indexed by `s`, then you pass `experiment` a function `h : s -&gt; f s` for some container `f`. The `h` function is passed the current index of the store and produces a whole container full of new indexes. Then, for each new index, the corresponding item from the store is retrieved and a container full of store items is returned. PS. this also works for non-container functors, such as the continuation monad, but I haven't thought about what happens.
I find working with so called "Structured Graphs" as defined here: https://www.cs.utexas.edu/~wcook/Drafts/2012/graphs.pdf to be a real pain. 
I'd personally want type-directed record field disambiguation even if the language had nested modules. I've been using it in a branch of OCaml recently and it's been convenient so far, although you need to add a type annotation occasionally. OCaml has nested modules, but many people have been wanting this feature despite that.
I've noticed for some people Haskell is an easier to learn first language.
on p. 18, right side, why do you assume &gt; h . g == idB And also (similarly) &gt; g . f == idA EDIT: corrected composition order, added other question
There's nothing stopping you from doing both. Clean up a specific document on the wiki and export it to an appropriate format for Monad.Reader. Then in the Monad.Reader just make a note that a more up-to-date version of the document is available on the wiki.
f : A -&gt; B and g : B -&gt; A By definition of composition g . f : A -&gt; A and idA is the _only_ existing morphism in hom(A,A). Therefore: g . f == idA. Using the same argument, h . g == idB.
&gt; cheater ... there are a few others that make use of native bindings (php comes to mind) Please be specific -- which PHP programs? *"To be generous, guiltless and of free disposition, is to take those things for bird-bolts that you deem cannon-bullets: there is no slander in an allowed fool, though he do nothing but rail; nor no railing in a known discreet man, though he do nothing but reprove."*
&gt; Shootout is a nice concept Hasn't been called that for at least 5 years because - [A shootout is a gun battle between armed groups](http://en.wikipedia.org/wiki/Shootout) - a shitty reality, not a nice concept.
Aaaahhh. The only existing morphism in that (almost-) category, right. So if I define a similar category with more arrows A -&gt; A and B -&gt; B I could make g . f /= idA, correct? Thanks a lot explaining, I've been tripped by this in other tutorials too.
&gt; clckwrks 0.13 is built on an entirely new plugins platform currently named web-plugins. (Please submit your ideas for a better name). If you want to run with puns on "clock", then there are a few obvious suggestions: * cogs * gears * sprockets
Presumably not. igouy is, as I understand it, intimately involved in the running of the shootout. I can't find a concise description on the website of what is and is not allowed WRT external libraries, but if he says it goes then it probably does.
[bragging rights](http://www.reddit.com/r/haskell/comments/uyr0y/ann_clckwrks_a_haskell_blogging_and_cms_framework/c4zv0bt), (c) drb226 2012
The hardest part of getting haskell FFI bindings to work on windows reliably is getting the libraries they bind to to work on windows reliably.
Somebody just informed my first example does not type check. It should end with a `()` instead of `r`. How embarrassing! That's what I get for making last minute changes to my tutorial at 1:00 in the morning.
Yes exactly. If you add enough morphisms you can "repair" the graph and make it a real category.
 instance Functor Maybe where fmap :: (a -&gt; b) -&gt; (Maybe a -&gt; Maybe b) fmap f (Just a) = Just (f a) fmap f Nothing = Nothing instance Functor ([]) where fmap :: (a -&gt; b) -&gt; [a] -&gt; [b] fmap = map Seeing this made me think of something. Just a quick shout out to ghc 7.6.1 &gt; There is a new extension InstanceSigs, which allows type signatures to be specified in instance declarations. ~ http://www.haskell.org/ghc/docs/latest/html/users_guide/release-7-6-1.html
 data Tree a = Empty | Node a [Tree a] toTree :: [a] -&gt; Tree a toTree [] = Empty toTree (x:xs) = Node x [toTree xs] toList :: Tree a -&gt; [a] toList Empty = [] toList (Node x l) = [x] ++ concat (map toList l) The assertion is made that `toTree . toList === id`, but this is not the case: toTree (toList (Node 1 [Node 2 [], Node 3 [])) toTree [1, 2, 3] Node 1 [Node 2 [Node 3 []]]
&gt; this solution is entirely Haskell98 and no longer requires FlexibleContexts &gt; I can safely say that you don't need to use any other functions to write fully featured pipes libraries, which dramatically simplifies the API. &gt; However, these utilities don't just work with both base proxy implementations; they also now work as proxy transformers, too! This means you can now use these utilities seamlessly within any feature set without any lifting. Next you'll be telling me that money grows on trees and that all the raindrops are lemon drops and gumdrops! Way to go, Temko!
True. Consider me interested but slightly overloaded at the moment. ;)
You're welcome! I'm glad that you enjoy it!
My current plan is something like this: * Keep the ekg package as is, but have it depend on a lower-level package or two (e.g. a ekg-counters package). * The lower-level package will just contain the various counter types and a way to register them in a map that maps names to counters. It's all a bit vague now but the idea is to let lower-level libraries export counters and to let people use this both as a standalone server, a snaplet, or a yesod subsite. 
Fixed. Thanks.
Concerning the fact, type of different kind than * aren't Hask object, I might be completely wrong. But it feels natural to me that when people say: Hask objects are Haskell types, it means the type of all possible kinds, not only _closed_ types. Concerning the "Non Haskell" functor: data Id a = Id a instance Functor F where fmap f = f for example: f=(+1), a=Int: fmap (+1) (Int 1) -&gt; ERROR with fmap (Id x) = Id (f x): fmap (+1) (Int 1) = Int 2 But even if _isomorph_, Int 1 is not _equal_ to 1. Concerning, the second example, anything with the topology of only one object and one morphism is a category. And you can call the morphism a category, even if this morphism isn't an id function. You can "choose" the rules you want. If you want to define (\_-&gt;0) as the id_{Int}, you are free to do it. Edit: concerning the first point. What would be the status of a function of type [a] -&gt; [a]? If we only consider type of kind *, you have to consider that such a function is many different morphisms. 
Argh... It is a complete mistake. The example is completely wrong. I wonder why I didn't thought about (toList . toTree), I just checked the over way :(. I should have token: [] isomorph to data List a = Nil | Cons a (List a) I'll fix it ASAP. Thanks to remark this. I am a ashamed :/. **EDIT: Fixed. Thanks again.**
So for those of us who never got what the whole pipe, conduit, hose, aqueduct, etc wars were about: is it over? Is there a clear best option now? Does this release solve whatever category theory related problem you had before?
A winner, among these chump libraries?! Oh ho, no, wait until I release Collapsible-Fabric-Pet-Tunnel 1.0 to hackage next week.
My mistake. Those are missing the initial `()` from the type signature: -- Wrong version downstream :: (Proxy p) =&gt; Consumer p () IO () -- Correct version downstream :: (Proxy p) =&gt; () -&gt; Consumer p () IO () Same thing for `upstream`: -- Before upstream :: (Proxy p) =&gt; Producer p () IO () -- After upstream :: (Proxy p) =&gt; () -&gt; Producer p () IO () I checked and it compiles with that fix now.
I found the category theory related problem I was referring to: "at the expense of breaking the monad transformer laws". The 2.5 release said that was needed for performance. You figured out a way around that to get the performance without breaking the law? I guess I figured that would be a big deal and get mentioned.
What's the difference between the fast and correct implementations?
I'm surprised you're not recommending OCaml instead of Rust as a lower level Haskell. Especially considering that Rust is not in a usable state yet.
Yes, I understand how the `IO` monad works.
Yes. That has been solved and I did mention it, but it was buried in the middle of one section and not given its own section. The entire API is type-classed now, which means I can offer both proxy implementations. The library provides the fast implementation in `Control.Proxy.Core.Fast`, which gives the nice performance, and the correct implementation in `Control.Proxy.Core.Correct`. These two modules only export a single function (basically), `runProxy`, which selects which implementation to use. `Control.Proxy` imports the fast one by default, but if you prefer the correct one you instead import: import Control.Proxy.Core import Control.Proxy.Core.Correct That then gives the exact same imports, except with the alternative `runProxy` function that selects the correct implementation. This means that the user may now freely select whether they value speed or correctness, which I believe is an acceptable compromise between the two main groups of users: the performance-oriented and the correctness-oriented. Because all proxies are now type-classed and work with both implementations, these two groups of users can peacefully coexist.
You may want to go through those few slides again, there are still 'Tree' left around, which should be 'List'. 
The fast implementation does not always obey the monad transformer laws if you have access to the constructors (or use a function that has access to the constructors and violates certain invariants). Some people have made a very cogent point that pipes do not even need to be monad transformers and that the fast implementation is in fact also the correct one. However, other people value the extra structure the monad transformer laws guarantee and take advantage of these laws to prove extra things about their code. In practice the fast implementation is completely safe as long as you don't import the constructors from `Control.Proxy.Core.Fast` directly, which you really shouldn't do anyway and you should always stick to the `Proxy` type class instead. That's the only internal module in the entire library and as long as you don't break that rule you can't violate the monad transformer laws. So there is really nothing to worry about if you use the default fast implementation that `Control.Proxy` exports. I provide the correct implementation not because it adds extra safety but simply to demonstrate how the correct-by-construction implementation should work in principle and also if people want to tinker with the library themselves without worrying about violating laws. However, library writers trying to build upon `pipes` should not import the internal module either. All functionality is implementable entirely in terms of the `Proxy` type class, which cannot break the laws, so there is really no reason to be worried about breaking them. Your only risk of ever violating them is if you use some library which ignored me and tried to imported that internal module and program concretely to the `ProxyFast` type and didn't use it correctly. Also, if you don't give a hoot about the monad transformer laws, then the library is also completely safe. :)
It looks like the answer is 'weaken the monad transformer laws slightly and don't easily expose ways to actually break them'.
This seems like a great split for development. Build the thing with Core.Correct, then switch to the fast version for release.
I noticed that all the operations in `Control.Proxy.Prelude.IO` are duplicated for Producers/Consumers and CoProducers/CoConsumers. Would it be possible to define a `DualP` newtype wrapper / proxy transformer that flips a proxy upside down? -- | Flip a pipe, exchanging upstream and downstream. Requests become responses and vice versa. newtype DualP p a' a b' b m r = DualP { getDualP :: p b' b a' a m r } instance Proxy p =&gt; Proxy (DualP p) where request = Dual . respond respond = Dual . request p &gt;-&gt; q = Dual . ((getDual . p) &gt;~&gt; (getDual . q)) -- etc. Then you should only provide the natural orientation of the prelude operations, so only the server `getLineS`, not the client `getLineC`. If someone wants to use it, they can just define getLineC = getDualP getLineS And also coidT = getDualP idT Unrelated remarks: * The documentation for `(&lt;-&lt;)` should say "This is (&gt;-&gt;) with the arguments reversed", instead of copying the documentation. That makes it much clearer what is going on. * Similarly, why does only `(&lt;~&lt;)` say "You don't need to use this. I include it only for symmetry." ? Isn't the same true for `(&lt;-&lt;)`? * Why is `printD` a generic `Client`, while `getLineS` is a `Producer`. I.e. why doesn't the latter just ignore arbitrary upstream values?
I have. In principle you can use things like monad transformer laws and functor laws to fuse away layers of abstraction and rewrite to the theoretically optimal code. In practice that is very difficult to implement. I spent a great deal of time trying to get the elegant way to go fast using rewrite rules, but it did not pan out. It wasn't really a failure of the theory so much as the inadequacy of the rewrite rule system for optimization. I could not get it to take advantage of the extra structure to optimize the code.
Alternatively, just add a type signature for `myPrint`. [`pshow` is `Show a =&gt; a -&gt; String`](http://hackage.haskell.org/packages/archive/ipprint/latest/doc/html/IPPrint.html#v:pshow), so this should work: :{ let myPrint :: Show a =&gt; a -&gt; IO (); myPrint = putStrLn . HsColour.hscolour (HsColour.TTYg HsColour.XTerm256Compatible) myColourPrefs False False "" False . IPPrint.pshow :}
If you're deprecating Pipe anyway why not reuse the name for proxies... I get the backwards-compatibility issue, but, this is the time to do it, not later. :)
This Google+ community corresponds to the Haskell-Beginners Mailing List on Google+.
After messing with conduit and pipes, I came to the conclusion that I like pipes a lot better. Being able to have bidirectional proxies is pretty amazing. And the code tends to be a lot shorter due to the way request and respond work. Also the ability to not just say, give me a chunk, but rather give me a chunk with these specifications, is too cool. But since it is late to the game, conduit has far better libraries at the moment. I'm hoping some people will step up and port conduit-http and a few others to pipes. It wouldn't be very hard.
I would second the ops suggestion of something more mnemonic than a' a b' b. Even something as simple as u' u d' d would be better. I've never been able to remember which is which for long.
I'm actually pretty happy with the type synonym names. I think reappropriating the `Pipe` name would cause several problems, the main ones being: * `Pipe` conveys unidirectionality for me. I'd have to replace `Pipe` with another unidirectional name. Since the `Pipe` synonym is likely going to be the most highly used type synonym, I would rather keep the original name for it. * The `Proxy` name evokes its behavior very clearly (in my mind). I can't think of another name that clearly conveys how the underlying abstraction works. * The `Proxy` type synonym matches the `Client` and `Server` pattern of using networking terms for bidirectional type nomenclature. I think I had a really good metaphor going there and switching from `Proxy` to something else would break that nice pattern. Also, `Proxy` is not so bad of a name, when you consider that most people have accepted even less sexy names like enumerator and iteratee. I think if they can put up with those they can certainly accept the name `Proxy`. Yes, it's not as nice as `Pipe`, but it's not that bad. However, as far as library naming goes, I will probably prefix libraries with `pipes` still, for several reasons: * Paolo has offered to hand back the `pipes-*` libraries he has created back to me so I can recycle them. By recycling these libraries I can avoid a lot of confusion for people new to the `pipes` ecosystem. * `pipes` is a sexier name that I've started building a brand around and I would like the ecosystem to still be referred to as the `pipes` ecosystem. So my long-winded answer is that I'll probably keep the type synonyms and still stick with the `pipes-*` prefix for libraries.
So, technically the module's documentation prominently notes that it is an internal module. Also, I highly doubt people will ever use it directly anyway. If they do I will just yell at them, or something. :)
Yeah, I also noticed the `Control.Proxy.Tutorial` module was inferred unsafe. Weird. I will fix that for the next release since there is no reason that it should be unsafe. Also, I'm not quite sure whether unsafe Haskell just means that it is referentially transparent (which the library definitely is) or includes additional variants like the validity of monad transformer laws that the library author must marks as unsafe for certain modules. Either way I will add `Safe` pragmas to the relevant user facing modules in the next release so that it catches that weird behavior and I will try it on different compilers. The `D` naming convention means that it interacts with values flowing downstream (sort of like tapping into the stream). `S` is more like the original source of the values. Also, like you noticed, `Control.Proxy` is safe because it hides the constructors.
&gt; bytestring, text, vector, attoparsec, and hmatrix and repa.
A model of the fast version is trivial: you don't deal with the "fast type" but rather with the quotient you get by "dividing out" by the relevant equivalence relation. The problem is that Haskell does not have quotient types (modular boundaries work pretty well to fake them though). Actually, this problem shows up even in the correct case: their are almost no instances of Monad unless you treat "\x -&gt; undefined" as being the same as "undefined" (but seq means they are not!). A bunch of the thinking about equality in dependent type theory is motivated by the desire to get quotient types. For example, Homotopy Type Theory lets you have quotients by interpreting each type as an (infinity) groupoid, and enforcing functoriality on functions and pi types. My guess is that with the "Fast" implementation, every function exported by the module (and hopefully every function definable in pure Haskell without breaking the module boundray) is "functorial" with respect to the "native equivalence realtion" of each type (equality for most types, but the quotient relation for proxies) and this would establish a measure of safety. 
I've been trying to see if I can come up with a name that might compete with Proxy, here is what I can suggest Road Wire Self-Sealing-Stem Bolt My only concern with Proxy is that data Proxy p = Proxy is becoming a standard type in the kind polymorphic world. Like in the definition of Typeable class Typeable t where typeOf :: Proxy p -&gt; TypeRep which is fully kind polymorphic. Otherwise I think Proxy is a great name, but this overloading could become a problem, since I would like to see a world where both the kind polymorphic Typeable and Pipes are considered to be widely used core packages.
I was just playing really, using Leksah. I vaguely remember not being able to debug either via GHCi, so that's a fair point.
Sorry, didn't know about the namechange and was just using it as a pointer to the site (google finds it like that still for me); I have been using the site for many years and just now read about the ""The Virginia Tech shooting in April 2007" you posted at the Mercury blog. Will not use that name again. 
&gt; Data constructor are not "type". When I talk about the type [a] I mean the "type constructor". And thus it is an "object" and not a morphism. Really, when you see a type "[a]", that is, more properly, forall a. [a], which is a _type function_ (written in System F as /\a. [a]). The type constructor [] (is that what you're referring to?) is not an object in Hask. There is no identity morphism [] -&gt; []. I must say I'm having difficulty understanding what you're saying. A type function _f_, is an endofunctor on Hask, so long as there also exists a lifting transformation `map :: (a -&gt; b) -&gt; f a -&gt; f b`. This is what the `Functor` class is trying to capture, but it falls short of capturing _all_ Hask endofunctors, because type classes can only be instantiated on _type constructors_, not arbitrary _type functions_ (in turn because type synonyms and type synonym families must be fully saturated). Therefore, we have a hack to solve this problem. Any type function can be reified into a type constructor through use of `newtype`, however if your notion of equality is too strict, then that is not exactly the same type function as the type function you originally defined -- Id a /= a. In my view, so that we have a greater symmetry between endofunctors on Hask and the `Functor` type class, it's a better idea to consider _equality_ on Hask to be _modulo_ `newtype`. You can reason about it via isomorphism, as you have, but that makes explicit the disappointing reality that Haskell lacks first-class type functions. You have to have extensionality for equality to make sense in Hask anyway, or it's not a category. Seeing as this already divorces us from the notion of definitional equality used in Haskell (*), I don't see the harm in identifying newtypes with their contents. (*) - if you add extensionality then `undefined :: a -&gt; b` and `const undefined :: a -&gt; b` are treated as the same morphism in Hask.
&gt; But even if isomorph, Int 1 is not equal to 1. I'm only interested in Hask modulo type synonyms (in fact even considering an idealized *pure*, including nontermination-free, language, rather than just Haskell), as kamatsu argues for. I think that if your example was meant to introduce the idea of studying isomorphism, maybe it could have been formulated more explicitly. &gt; You can "choose" the rules you want. If you want to define (_-&gt;0) as the id_{Int}, you are free to do it. I was assuming that you were describing an endofunctor from Hask to Hask, not a functor to somethine where you get to decide the notion of equality. In (what people usually talk about when they say) Hask, equality of morphisms is program equivalence and it is not true that for any `f : Int -&gt; Int`, you have `f . (\_ -&gt; 0) = f`.
In SafeHaskell terms Unsafe means that it breaks the type system - i.e. you can use it to make pure code segfault or break referential transparency (unsafePerformIO, unsafeCoerce, and so forth). Trustworthy means that it uses Unsafe things internally, but the exported interface is safe (i.e. the composition of impure things can be pure, which is the whole point of unsafePerformIO). Trustworthy is set by the module author, but it's up to the user to affirm that they are actually providing that trust (IIRC with ghc-pkg?). Safe is anything that uses only Trustworthy modules and other Safe ones (it's Safe by default if there's nothing in the transitive closure of imports that's Unsafe), but of course if the user has revoked their trust from a Trustworthy module/package, using it won't be allowed. So: - Unsafe means "definitely not safe" - Safe means "safe as long as the imports are trusted to be safe" - Trustworthy means "I'm claiming that this is safe, but it's up to you to decide whether you believe me" So, no, this is not the right mechanism for specifying whether type class laws are obeyed, unless those laws are also necessary for ensuring type safety (which could be said about something like Typeable, or maybe Storable if it were in ST instead of IO...).
When I tried the debugger for single stepping, it's… good but wasn't so good when using it programmatically. I made a simple thing in Emacs to, given some code, evaluate ten steps of that piece of code, just like in Inventing on Principle. You would type on the left and on the right you saw the result. In the background I was making a breakpoint and telling GHCi to run the steps. But it was (1) rather slow, (2) started to gobble up insane amounts of memory and was unusable after a few updates. Can't remember which GHC version I tested with, I might try it again, though, with the GHC API, sometime, as it would be rather useful.
For 90% percent of the applications one of the types is enough. Having extra type parameters and writing `force` everywhere makes the code a lot less clean. As a user of the library I want `Text` to be a drop in replacement of `String`.
You should release your ampq wrapper as well, when/if it's ready. I like pipes quite a bit, but the lack of libraries means I'm going to be on conduit for a long while (sadly).
I agree with twanvl. The default of separate types is just fine. If you really need something polymorphic that could be either of the two, that's easy enough to define. When twanvl said that `Text` should be a "drop-in replacement" for `String`, I don't think he was claiming that the two are exactly equivalent. Obviously there are differences, and you have to be aware of them when you port from `String` to `Text`. But strict/lazy polymorphism is a very non-trivial extra dimension to add to a program. In Repa there are specific reasons why that kind of polymorphism is sometimes useful. But normally you know which kind of Text you are using and you want that to be clear in your code.
&gt; there's a lot in haskell that appears ready to be thrown overboard but is kept around for compatibility Half of the stuff in the Prelude, for instance.
Well, with something like Agda's instance arguments, type-directed disambiguation is quite straightforward ;)
&gt; more accurately reflected real-world work [Whose](http://shootout.alioth.debian.org/dont-jump-to-conclusions.php#app)[ "real world work"?](http://shootout.alioth.debian.org/dont-jump-to-conclusions.php#jump) &gt; i wish they added... "If you're interested in something not shown then please take the program source code and the measurement scripts and [publish your own measurements](http://shootout.alioth.debian.org/more.php#languagex)."
Very nice!
homepage: http://ianwookim.org/hoodle/
From the video, it looks fantastic. Will try it out later. Checking the mail-list I notice that it was almost called Grimpad instead! 
&gt; There is a downside: the types and error messages become more complicated. Error messages won't be any more complicated than they are now. "Got Text, expected Data.Text.Lazy.Internal.Text" isn't a very nice error message. At least this way it'll be "Got Text Strict, expected Text Lazy" or at worst, "Cannot unify Text a with Text Strict.., a is skolem in blah..". &gt; It might also hurt the ability of the compiler to do type inference. That's also not true. It's true that the compiler has no trouble doing type inference now, because you're essentially specifying the type every time you make a function call. But _even if the exact text type can't be resolved_ it will just make your function polymorphic over both. There's no situation like type-classes where you end up with it not knowing which instance to pick, at least, if you use a GADT. &gt; Also, sometimes you do want to use the strict operations for combining strings. For example when working with small strings that represent, say, variable names. What? When would you want to combine several variable names into one variable name? &gt; And in this case it is much simpler if you don't have to deal with lazy Text at all. Well, those cases are easily covered by `force . concat`, and I can guarantee that cases with Lazy text being preferable are far more likely to occur.
Thanks for the clarification. Now I just need to figure out why the tutorial is not referentially transparent. :)
Yeah, it didn't. The other reason is that monad transformer provide a really useful structure. I've defined some other code (not in any library right now) that made use of the monad transformer laws and would not have benefited from monad coproducts.
I don't know. I kind of liked Collapsible Fabric Pet Tunnel. Yeah, I've been aware of the clash with the other `Proxy` and that the standard libraries should avoid such clashes. One thing I can do is perturb the name of the type class ever so slightly to something like `Proxies` or `ProxyC` (artificial namespacing) to avoid the issue.
Awesome. I love xournal, this is great to have a Haskell-hackable version. I want to add SVG export because PDF export is mostly useless.
&gt; Error messages won't be any more complicated than they are now. "Got Text, expected Data.Text.Lazy.Internal.Text" isn't a very nice error message. That's true, but this only applies to the '10%' cases. In the case where you use just one type of Text, I still expect the error messages to get slightly worse. Just tried it; and I think you are right that the difference is hardly noticeable. "Couldn't match expected type Int with actual type Text" vs "Couldn't match expected type Int with actual type Text a". There might also be some issues with the evil known as the monomorphism restriction. &gt; &gt; It might also hurt the ability of the compiler to do type inference. &gt; That's also not true. When you use GADTs, doesn't ghc require that you write a type signature for all your functions? Or is that only when pattern matching? In addition, when Text becomes a GADT, I assume it means that it will have more than one constructor, which will hurt the compiler's ability to optimize. &gt; What? When would you want to combine several variable names into one variable name? Okay, maybe not the best example. I just did a grep for `appendFS` (which is essentially `Text.Strict.(++)`) in the ghc source, and it is not as common as I would have thought. But there are a few occurrences. &gt; and I can guarantee that cases with Lazy text being preferable are far more likely to occur. Probably true, but it depends on the code base. And for generating output, is Lazy Text really the right type? Shouldn't you use some kind of Builder instead?
thx for the link!
Thanks!
I am actually thinking of it using GtkMathView, which can support both LaTeX and MathML. But of course, text rendering first. 
It's the non-container `Functor`s that give `experiment` it's name, actually (I think). It is to `ComonadStore` what `traverse` is to `Traversable`: you get to (possibly, and possibly multiple times) access the `Store` while doing side effects (e.g., you can let `f ~ Maybe` and then you have an optional access, or you can let `f ~ IO` and then you can let your retrieval be guided by external forces).
&gt; Lazy text, however, is much more expensive to consume or access randomly. That's not right. It's not much more expensive to consume. It's very slightly more expensive to consume. If your chunk sizes are big enough it should be unnoticable. As for random access, that's true, but we don't typically do that with text, and the text api strongly discourages it by not giving you operations in terms of indexes.
Awesome! I'm using Xournal right now on my ThinkPad X230 Tablet, but this looks very promising :)
The idea is to split out a lower-level library with almost no dependencies. Also, we should be able to support other export formats than JSON so we can use standard infrastructure like statsd. Just haven't had time yet.
Question: where can I find some good examples of the different semantics of the "fast" vs "correct" formulations?
Can you give a default implementation for `(&gt;~&gt;)` in terms of `(&gt;-&gt;)`, `request` and `respond`?
ok, this answers my question, awesome!
That's a really good question. I don't think so, but I'm not sure. The closest you can get is given by the following proxy law (and its symmetric equation, too): (liftK g &gt;=&gt; request &gt;=&gt; p1) &gt;-&gt; (liftK f &gt;=&gt; request &gt;=&gt; p2) = liftK (f &gt;=&gt; g) &gt;=&gt; request &gt;=&gt; (p1 &gt;~&gt; p2) However, I don't see how you can work backwards from that law to derive the definition of `(&gt;~&gt;)` in terms of `(&gt;-&gt;)`.
Hmmm, this seems aimed at avoiding attacks. In my use of hashing I have zero worries about attacks, and a lot of worries about performance. I guess I should use the old library. :(
&gt; When you use GADTs, doesn't ghc require that you write a type signature for all your functions? Or is that only when pattern matching? Neither are true, however type signatures can cause the type inference engine to succeed where it otherwise may fail, but usually that's only if you do weird shit. &gt; I assume it means that it will have more than one constructor, which will hurt the compiler's ability to optimize. case-of-constructor expansion and inlining (which GHC already does, and can be encouraged to do more with hints) and some well placed RULES will solve that problem.
I made the mistake of skimming this talk and it made me sad. His Javascript audience will still have only a vague idea why monads matter and no clue what they are or how to use them. His example use of the Maybe monad: propagating a null value through a program instead of raising a null pointer exception, just as a NaN propagates through later computations. Aside from the fact that Javascript *already does that* with *undefined* ... what?
He also refuses github pull requests *but* rewrites them himself. Also, JSLint's license includes the clause "The Software shall be used for Good, not Evil" which prevents it from being hosted on Google Code or included in either Debian or Fedora package repositories.
Thanks for letting me know the incompatibility. I haven't installed ghc-7.6 on my system until today, and now I am installing it. Hopefully, I will be able to fix the version number incompatibility with ghc-7.6 and up-to-date packages until this weekend. 
I think there's two distinct (and mostly non-overlapping) use cases for `printf`. 1. I'm doing some hardwired output, I know the format string at compile time. 2. I'm doing some user-determined output, I don't know the format string until run time. For (2), since the format string isn't known until runtime, it *can't* be type-checked without runtime dependent typing. So let's table it. The more common case for me has usually been (1), where Oleg's example , or the HoleyMonoid would be useful. But if I'm at compile time already, why not have a TemplateHaskell macro or some other compile-time magic to transform a c-standard format string like `"%2$s: %1$*4$f"` into a value of type `Format (Float :&gt; String :&gt; a :&gt; Int)`?
That was a pretty bad monad tutorial
I made hoodle compatible with ghc-7.6.1. I updated all the packages in the hackage by loosening or removing the upper bound of dependency. please check this with cabal update and cabal install hoodle. I have checked from a clean install of ghc-7.6. It should work. For poppler (pdf rendering library), ghc-7.6 and the most recent version is not compatible yet, please wait for the new version of poppler. In fact, I became a maintainer of poppler library from two days ago. 
Has anyone ever said that you need to learn category theory before you can learn monads? I've never heard that from anyone, ever. Just got to the part where he introduces "context coloring" and I suspect it's the most useful idea I'll get from this talk.
so basically as long as i actually respect the abstraction even though the guts are sorta visible, the fast one is as correct as the correct one, BUT theres no abstraction barrier *guaranteeing* that I do that. So per se, as long as I informally enforce respecting the constructor abstraction, these laws *essentially* hold, right? cool (am i understanding your remarks correctly?)
[This was my reaction](http://www.reactiongifs.com/wp-content/uploads/2011/09/mind_blown.gif)
ARGHH! The function hash is severly broken. It returns different values for each program run. Is purity and ease of testing of no concern? And I can't just change all my calls to hash, since those are in packages I use, like hashmap.
When reading through other libraries, I often stumble upon the term “leftover”. I never quite understand, what they are, for what they are needed and especially why not in this library. Could you help me clarifying this?
Here are some benchmarking figures for different configurations. The benchmark is a (Haskell) compiler front end doing lexing, parsing, module chasing, renaming, and type checking of about 3800 module. So while there is a lot of table lookups it's by no means the majority of the time (that honor goes to parsing at 25% of the time). ghc-7.2.2, hashmap-1.2.0.1, hashable-1.1.2.3: 95.4s ghc-7.6.1, hashmap-1.2.0.1, hashable-1.1.2.5: 97.5s ghc-7.6.1, unordered-containers-0.2.2.1, hashable-1.1.2.5: 109.4s ghc-7.6.1, hashmap-1.2.0.1, hashable-1.2.0.0: 102.4s So the new hashing slows it down by about 5%, not as bad as using unordered-containers over hashmap, but unacceptable. 
I think that this should be a separate package for those who are worried about DOS. Many of us would prefer speed and purity instead.
Here's an argument against. Haskell's purity makes parallel programming easier. This slightly tighter notion of purity also makes distributed programming easier.
I can give other arguments against. :)
Please look at the API: If you need to rely on repeatedly having the same the hashed results use the 'WithSalt' variants.
How does that help me when I'm using the hashmap package which calls hash? Am I supposed to rewrite all packages using hashable?
Remember that there are other data structures that depend on hashing too, such as bloom filters. The old hashable library was completely unusable for bloom filtering.
Look at the .cabal file to see how to get it to not use a different salt for each run. (And I'm afraid I am not planning to change that to be the default.)
No worries, I'll make an alternate hashable package. And then figure out how to make hashmap use that. You've made a package for the use case you have at hand, I will make one for mine. 
The new numeric hashing is what kills my performance. 
I'm having trouble parsing: &gt; The trick is that for every function you want a rewrite rule to match on you define a top-level function with the most specialized type possible that the rewrite rule uses. &gt; &gt; The important guidelines are that you don't: &gt; &gt; - Specialize the type of the function to match on as much as possible So it should be as specialized as possible, but I shouldn't specialize it? And if it were specialized it would match more things? Huh? Anyway, what I was actually curious about was what things you're rewriting to what other things and why. :) And whether that's interesting enough to be a topic for a blog post, or if it's just "your code will be faster, yay". WRT name, this concept of "someone or something which takes requests and then makes other requests of others in order to be able to fulfill the original request" is something like ridiculously common throughout human society and its creations, so I'm very surprised that I can't find any halfway non-awful word which describes it. But so far, I can't. (I mean, 'intermediaries' or 'brokers' wouldn't be much of an improvement on 'proxies'.)
bad-practice alert: constructResponse :: String -&gt; String constructResponse url = unlines ["HTTP/1.1 301 Moved Permanently" ,"Location: " ++ url ,"Content-Length: 0"] even though the above might be "fast enough" for the task at hand, the use of `ByteString` or `Text` should be preferred over the rather expensive `String` type to avoid surprises should the toy application grow to handle more heavy string operations...
You did [look at the .cabal file](https://github.com/bos/hashable/blob/master/hashable.cabal#L30) when I suggested it, right? 
Alas, there's not much I can do about that. I have the fastest integer hashing functions that do a plausibly good job, but they're simply not going to compete with the previous behaviour of doing nothing :-)
And doing nothing will also give a denser key set for the IntMap in the hash map, which is faster. So doing nothing for Int is just right for me. :)
My understanding of bloom filters is that to tune them properly and actually use them in a solid way, one should create hashing functions specific to your data and use case? This is to say that I would not feel comfortable taking any given off-the-shelf hashing function and throwing it at a bloom filter (or a sketch algo, or etc). edit: i.e., would you feel comfortable ripping out the hash functions provided here: http://hackage.haskell.org/packages/archive/bloomfilter/1.2.6.10/doc/html/Data-BloomFilter-Hash.html and replacing them with a dependency on the new Hashable?
I have s suspicion that unordered-containers is too strict. I'm going to put this to the test. 
I might be able to avoid your fancy hashing of ints by some extra instances. It's worth a shot. 
Yeah, I meant that you SHOULD specialize it as much as possible. The actual optimization just inlines the monad's bind and hits the compiler over the head with the correct optimization. To see what it inlines to, just look at the utility code in version 2.5. However, INLINE pragmas do not work as well, for some strange reason. Also, remember that if you rename Proxy you need to rename Client and Server, too, to keep a consistent metaphor between them.
I just installed and started it (don't have time right now to test it more fully but I will get to that soon hopefully). Seems to work. Thanks for the quick help :-) Great work.
Fixed in 1.2.0.1.
Do this: data Base a = A | C | G | N a data RNA = U data DNA = T Then you use these like so: -- A function that only accepts DNA bases f :: Base DNA -&gt; ... f A = ... f C = ... f G = ... f (N T) = ... -- A function that accepts RNA bases g :: Base RNA -&gt; ... g A = ... g C = ... g G = ... g (N U) = ... Parametric polymorphism is always the solution whenever you want to make a datatype flexible or extensible.
&gt;So I'm just saying that for me this is the wrong trade off. No, if that is what you said then I wouldn't have replied. What you said was "doing it right should be a separate package, so people can keep doing it wrong". That is not a statement of your personal preference, it is a generalization to "many of us". I suspect the many you refer to is much smaller than you believe it is, and also much smaller than the group of people who would choose the dangerous 5% faster way even when the danger matters.
You're welcome!
I agree - having non-determinism is really nasty. Bug repeatability is *really* important.
I've already found one optimizer issue. A conversion from `Word` to `Int` is resulting in allocation and going via `Integer`. Probably due to a missing RULE in GHC.
Do those distributed algorithms also rely on hashing giving the same answer on different machines?[*] Because if so, this is now not suitable. [*] retorical question, obviously many do, like hashing keys to work out which shard and hence which machine holds the data.
Context coloring is not as useful in Haskell because Haskell has sensible scoping rules, unlike Javascript. Also, purity reduces the importance of context, too.
I've never understood why the default list implementation isn't a difference list. I imagine the linked list implementation must be a large constant factor faster for normal list cons operations, but that must be one heck of a constant to justify getting rid of an O(1) optimization.
Lack of pattern matching, probably.
I would love to see more comparisons here. I largely love Haskell, but for hiring purposes have been considering Scala.
This was posted a couple of months ago [here](http://www.reddit.com/r/haskell/comments/11gg80/video_lennart_augustsson_on_making_edsls_fly_at/). It seems like the LLVM results weren't competitive partly because he was making many small FFI calls, with correspondingly large overhead.
Yeah, given the title of the talk and that it was Crockford, I couldn't muster up the interest to watch more than the first minute or two. I find his opening statements deeply ironic, because many monads aren't all that practical without proper tail calls, which Crockford is legendary for being deadset against.
You can always just prepend and then reverse at the end of your algorithm to get similar performance characteristics. It is well known that linked lists have bad performance for appending so prepending is standard in many functional algorithms. It would be very confusing to have everything the other way around from any other functional language.
Especially since it is a synonym, of sorts, for "father."
dlists are only really useful if you intend to consume the list exactly once. If you need to look at it multiple times you'll duplicate work with the dlist representation. This holds in general for lots of monads: compare working with free monads to working with the codensity monad of the free monad. The former can be easily inspected a constructor at a time, and its easy to convert from the latter to the former, but the latter supports more efficient (&gt;&gt;=). Consuming the latter directly on the other hand multiple times will duplicate work.
The only problem with a GADT is that you can only extend the data type at the point of definition of the data type. With the polymorphic type I can extend it even if you lock down the original type. The GADT suffices for this particular problem, but I think it's better practice to teach beginners polymorphism, which is more generally useful and idiomatic Haskell.
Shouldn't the `Poly` example use `Generic1`?
Can you be concrete about what operations reveal what internals? This seems like an abstract concern. Do you really have code who's correctness or performance is actually dependent on consistent repeatable internal hashing sequence inside Data.Map from run to run? It's an academic argument that if you run the code twice, and the hashing changes, you'll be able to notice variations in timing. Such variations will be in the noise compared to other variations in timing from run to run on modern systems.
So much noise in the syntax...
Thanks , I didn't catch this previous post 
It's a shame the slides were so succint (are they should be for a talk, though). Basically, all I got is that they wrote a parsing library, that Tokyo and Singapore are beautiful cities and that working for Tsuru is pretty damn cool :P
Nice and clean. I like it. 
What GUI library does Tsuru use?
Or if you want to annoy Tekmo with even more extensions: {-# LANGUAGE DataKinds, KindSignatures, GADTs #-} data WhichNA = RNA | DNA -- what's the name for these? data Base :: WhichNA -&gt; * where A :: Base t C :: Base t G :: Base t T :: Base DNA U :: Base RNA Which prevents you from writing `A :: Base (Bool, Char)`, or that sort of thing.
At what point do we specify the character encoding to use?
gtk + html/canvas
&gt; data WhichNA = RNA | DNA -- what's the name for these? Nucleic Acids?
E.g., the result of the toList function depends on the hashing. I don't have anything that relies on the actual result of toList, but I do rely on having repeatable tests for debugging.
You could also have a HashMap monad that is run with a seed. The interface would feel very similar to the hashtables package, which does mutable hashtables in the ST monad.
I think `sendMail` could be even simpler if it took a `Host` or something to connect to. `Host` would be a sum of `Insecure` and `Secure` or something - one constructor that just needs a host/port, and one that needs a host/port/username/password. Then `sendMail` can just pattern match, and have one less API function to learn about.
2^63 = 9223372036854775808. Looks too expensive...
I guess difference lists would be perfect for Writer, then.
I was waiting for something like this since a long time ago! Thank you very much!
I don't really think these slides are very good on their own... there were a lot of things that I talked about in addition to the slides. The code examples are also simplified in a brutal way in order to convey the ideas more easily.
i would suggest using the connection package instead of using network directly if you do this pull request.
I don't buy this argument. In my experience, there aren't realistic situations where debugging a test relies on getting a repeatable result out of a function defined like toList. I think to hobble the whole implementation for this rare at best concern is wrong.
The Network.Mail.Mime package by Michael Snoyman which this depends on includes some functions for sending mail via the local sendmail. http://hackage.haskell.org/packages/archive/mime-mail/0.4.1.2/doc/html/Network-Mail-Mime.html I suppose the easiest thing would be to reexport those.
I have a completely unjustified distrust for haskelldb and hdbc. I know I'm wrong, but they feel a bit antiquated to me. I think I would feel better about haskelldb if there were a version based off of postgresql-libpq and had postgresql specific features. It's the only database it performs well on anyways. 
Thanks, it doesn't look so bad, but I was thinking about a small tool which , given a file to profile, invoke the profiling, grab the .prof, run a small local server and public details and beautiful graphs of the profiling there.. who knows, maybe I'll write it myself :)
OK, that's done: https://github.com/jhickner/smtp-mail
The encoding for the message is always UTF-8. If you need to specify special encoding for a part of a multipart message, you need to import Network.Mime.Mail and build a Mail record manually. Docs are here: http://hackage.haskell.org/packages/archive/mime-mail/0.4.1.2/doc/html/Network-Mail-Mime.html
&gt; The beauty of this interface is that haskelldb is free to rewrite your query in a more optimal form &gt; ... &gt; Coupled with PostgreSQL's stunning query planer, the end result is query with the same speed as one I'd write by hand! I rather not to allow libraries to do such thing for me. Doesn't PostgreSQL know better? If in the current example there's no speed difference then can I expect performance degradations in some other cases?
See http://heap.ezyang.com/
Still no love for commercial databases. I'm stuck with ms SQL server. My only option for now is hdbc-odbc. I can't even use yesods persistent lib. I know some companies are working on providing commercial Haskell support. But if you guys continue to ignore ms SQL server and oracle, you won't make any inroads in enterprise.
It's a possible solution.
I think there have been some attempts, but it's a very very ambitious project.
Who are these "you", and why should they care?
It works. :/ Each project maintains its own set of packages, there is no global state to mess up or munge. It usually Just Works.
But Ints are signed, so 2^63-1 is the maximum value.
I *believe* it is to enable exporting of type level functions. It was part of and used for the new type level literals/naturals. For an example, see the module export list in the [GHC.TypeLits source](http://www.haskell.org/ghc/docs/latest/html/libraries/base/src/GHC-TypeLits.html) and notice the `type` keyword. You won't see `ExplicitNamespaces` explicitly listed as a language pragma because it's implied by `TypeOperators` and `TypeFamilies`, as noted in the [commit](http://www.haskell.org/pipermail/cvs-ghc/2012-March/071650.html) introducing it. Unfortunately, I don't think it has anything to do with the proposal you've listed.
Having deterministic pseudo-randomness is *really useful* for debugging and testing even in languages that *aren't* pure, for that matter.
Oh, I fail. You're right.
FWIW, I posted this to haskell-cafe a few months ago: http://www.haskell.org/pipermail/haskell-cafe/2012-June/101652.html
Yes, I believe it was originally to allow things like module Foo (C (type F)) where ... where C is a class and F is its associated type, whereas `C (F)` would be parsed as a data declaration and its constructor. And now also TypeOperators apparently.
Of course, just harder to ensure. In Haskell I can rely on purity to get it. Until someone sneaks in something impure.
&gt; If you really need something polymorphic that could be either of the two, that's easy enough to define. How? With something like `ListLike`? &gt; But normally you know which kind of Text you are using and you want that to be clear in your code. I disagree. The majority of the time I really don't care.
like http://jnordenberg.blogspot.co.at/2012/05/my-take-on-haskell-vs-scala.html ?
We can't give up on purity, that's just too much of a cost...
As a refinement on this idea, why not always have the salt set to zero, and have an API call setDefaultHashSalt :: Int -&gt; IO (), and setDefaultHashSaltRandom :: IO (), which can be used to turn on salting. I imagine the web frameworks would turn these on, but people writing safe code would probably not. You solve the framework problem (people needing to modify downstream libraries) without breaking purity for other end users.
Sadly, still broken for me in 1.2.0.2. I need to try tracking this down myself, since it no longer occurs for bos, but occurs easily for me.
That being said, it was still interesting to read through the slides, at least for me. In the context of having seen a few other talks/papers on financial uses of Haskell and having worked in the language a lot, even those brief words on the slides provided information. Not surprising to read that a lot of parsing is being done, interesting to see that Tsuru's chosen to use template Haskell to write specialised ones, for example. Ditto with choices for FRP. Sometimes just little snippets like that make it worth the 5 or 10 minutes of reading, so thanks (to you and Tsuru) for allowing them to be posted.
I've been using Haskell for about 7 years now, so I definitely hope you're right. I'm sort of in a weird situation though in that I might be hiring in the midatlantic (Baltimore, DC, maybe Philly) so it feels like a less exposed hiring pool—if there are a lot of Haskellers out here I haven't seen significant evidence. Which... might just mean that I should start up a FP meetup around these parts.
Thanks for the offer—I'm definitely excited by the chance to spread Haskell-in-industry opportunities! Unfortunately, I'm not looking for a job myself but instead am struggling with choosing a Haskell stack for a new phase of my startup project for fear that I won't be able to find great people in my area who have experience with/interest in Haskell.
It also could be that you are not allowed to send from the address you're using as a "from" address. In which case you'd get a relaying denied error. I'm going to make some changes to display what the server is responding with.
This appears to be the issue. I added some additional output on errors, and I get this response from your test code: *** Exception: user error (Unexpected reply to: RCPT "shakleton42 [at] gmx.de", Expected reply code: 250, Got this instead: 554 "5.7.1 &lt;shakleton42 [at] gmx.de&gt;: Relay access denied\r\n") (I changed the @'s to [at] in the output)
It's still nice to have something simple, just for cool demos or if I feel the need to send an e-mail via `ghci`. Not everything has to be about production servers.
The equatorial reasoning, strong immutability, a powerful type system with good inference, and a community of people writing many clean libraries. It's not that you cannot write clean code in other languages, it's more of the "O'Caml as a refactoring tool" power which continually compounds the clarity of your systems over time. I haven't looked at F# which purports to some of the same benefits. Scala's mutability/complex type system/intermingling with Java worry me in the brief looks I've had at it so far. Further, I've come personally to feel much more comfortable and fluent in a restricted functional environment over an object-oriented one. Objective-C and C++ aren't nearly high level enough in my eye. And honestly a huge driving force is just that I'm the lead developer and the Haskell stack is what I know by far the best.
The server I was using is the one of my faculty at my university. It appears that it will only accept Mails from inside the network for some reason. Sadly, I will have to talk to our admins first before I can get your package to run on a university computer, but at least now I know the cause of the error. Thank you very much for your help! :-) (Oh, and why not push those additional error outputs? They appear to be quite useful!)
GHC runs fine on a 256MB Raspberry Pi which has similar architecture to (low spec) smartphone with an off-the-self Debian OS. I compiled my TZAAR Haskell game with X/GTK+ GUI and it uses around 15MB to run (same as on an x86, actually). Compiling did take most of the Pi's memory, but the Haskell executable is quite light to run.
I'm genuinely curious to understand this. Can you point me at the code the breaks? 
Woah, I'm not trying to fight or make universal claims here. I'm explaining my feelings and thus my personal quandary. I'm sure you can write elegant code in whatever language you please. I quite like the JVM having used it extensively via Clojure. Nulls are a concern, and so is "nativeness" where I found in Clojure that often to write things for Java libraries you end up with such a thin API to the java library that you end up very far from Clojure-like code. I can and have written many other stacks. This time I get to choose one and I'm considering Haskell. That is all. (And Ocaml as a refactoring tool is an essay by a device at Jane Street talking about how he can refactor entire code bases in Ocaml confident that type-checking implies correct code. It's not to say one shouldn't test, but instead that you have a very powerful guiding hand/safety net in a sufficiently powerful type system with good interactivity and inference.
Healthcare data analysis, experimental design, and language design. I'm on my phone so I won't elaborate, but reach out for more details if you like. I may post here soon with my specific needs.
Yeah. I successfully built happstack on a 256MB RPi. I had to tweak the environment a bit to add a little more swap, etc. I have a 512MB RPi now, and I expect it to just work 'out of the box'. Phones like the Samsung Gallery S3 have 1-2GB of internal RAM and 1.5GHz+ dual core processors -- making them much more powerful than the RPi.
If you're doing game of life, you should try doing it with Repa and a stencil convolution. Then you get free parallelism.
neither has a business model that accomodates adding support for those enterprise DBs unless they have a customer paying for that support being added. I say this as someone building a haskell biz. If no one sees a clear way to recoup the cost of the engineering time to add support for those DBs, its not going to happen. Theres so many other high value ways that engineering time can be used in terms of biz value and current haskell community value. 
Ridiculously warty and ugly type system -- [here's how to implement a free monad in Scala](http://www.reddit.com/r/haskell/comments/zxcks/haskell_vs_f_vs_scala_a_highlevel_language/c68ybn1). No mutual tail recursion -- thanks JVM. Explicit trampolines are ugly and annoying. Escape hatches galore -- null, side effects, subtyping causing random stuff to unify when it should be a type error etc. a terribly slow compiler which performs next to no useful optimisations for functional programs (and in many cases can't because it doesn't have the purity information GHC does). Implicit conversions - these fly in the face of good type system design, horrible things that should die a slow and painful death. That's all I can remember for now, but I'm sure there were other reasons I disliked it, but those are the obvious ones. Edit: Also, you can pry equational reasoning from my cold dead hands. Scala completely throws that out the window. Even in SML, which has unrestricted side effects, I can manage most of the time to think denotationally not operationally.
&gt; Woah, I'm not trying to fight or make universal claims here. I'm explaining my feelings and thus my personal quandary. I'm not trying to instigate anything either. I was just asking you what makes Haskell a significant advantage over writing simple and clean code but all I got are things that don't really relate to that. I'm genuinely curious. But... &gt; I'm sure you can write elegant code in whatever language you please. It doesn't seem like it from what I'm getting with your thoughts regarding Scala if I'm reading in between the lines right. As with any language, why not take the time to learn it fully in order to make an informed decision? :-) Side note: I'm not talking about you specifically but I frequent /r/haskell every now and then and every time I do, it feels like the majority of Haskell community on reddit are rather... snobbish/elitist. Especially with comments like: &gt; Google makes you log in, and then gives you a 404? No thanks, I guess I'm sticking with Haskell. and some comment I read awhile ago saying that Windows programmers were lazy and unproductive programmers. I mean, really? I consider myself an okay programmer and I love learning about any programming language. I definitely wouldn't slam a programming language without even looking at it. Looking at /r/scala or /r/java or whatever, I don't see the same type of comments. It's actually rather striking to see the differences in the Haskell community vs others.
It's obviously your opinion but... attempting to program another programming language like another programming language isn't generally a good idea and doesn't really give Scala or any other programming languages a fair shake. While monads in Haskell are very important to know because it's Haskell's bread and butter, knowledge of monads in other **multiparadigm** languages are definitely not needed. I could complain that Haskell is terrible with non existent memory management or things that require mutating compared to other languages like matrix manipulations but I don't because I know how to program in that particular language. The same goes with your other points. Scala isn't good at those things because of the 'sacrifices' they made with their design decision along with the fact that it's on the JVM. In return, they get access to things like interop with Java which bring things like maven to the table and deployment to anything that supports the JVM including Android even if the support is a bit shaky, which Haskell can't claim to do. Your points on slow compiler and implicit conversions: It's supposedly faster in 2.10 but I haven't really moved to it yet. I'm also not a big fan of implicit conversions/parameters/classes but I like it in very limited cases. I can't exactly say that I'm enamored with Haskell's global type classes either but eh.
I see. I guess that's a pretty good start. 
As far as I know, GHC already can compile for ARM (as in iTrash). And the kernel is Linux anyway. So the question is who implements (FFI?) adapters to the relevant Android APIs …
Yes, so while my point is that principle Haskell runs on a lower-spec architecture than modern smartphones, there are still some annoyances: 1) GHC is not a cross-compiler, which means you have to run it on the RPi; often GHC itself is much more resource-hungry than my Haskell executable. 2) The RPi runs a standard GNU/Linux OS with X windows stack, etc.; to develop apps for Android (or iOS for that matter) you'd need some bindings for native OS and GUIs which is likely to be a lot of work. 
That's OK, a day or two here and there won't matter. Please don't get discouraged if you miss a day. Thanks again for a great series.
Nonsense. You only have to not be a *bad* Haskell programmer who lets stacks blow up like crazy.
I actually meant 2^63 - 1, the -1 unintentionally became part of the superscript.
The whole point of declarative programming is to trust the runtime/compiler to optimise your code. SQL and Haskell are both good at letting you express intent instead of procedure. Of course, good systems will provide debugging tools and a way to override the automagic when necessary, but it need not be default behaviour.
HaskellDB doesn't really do any interesting transformations on your queries for optimization. If you're familiar with PostgreSQL's query planner you will know that it doesn't distinguish between whether you make a join or a bunch of subselects, it [converts the latter into the former](http://chrisdone.com/posts/2011-11-06-haskelldb-tutorial.html#speed-and-optimisation). HaskellDB can only generate specific kinds of queries, i.e. joins and subselects. Having used this combination for years, if you can come up with a query in HaskellDB that performs poorly compared to hand-written SQL with PostgreSQL, I will be surprised. Your real worries with query performance is to make sure that you have indexes for things you join on and query, but that's a problem you have regardless of HaskellDB.
Perhaps start by posting what code you have, and where in particular you are having problems. The [Haskell Cafe](http://www.haskell.org/mailman/listinfo/haskell-cafe) is also a good place to ask.
I have no code yet, I'm just looking for information on how best to implement the algorithm. I'm new to Haskell so just anything that could help me out really! I'm not looking to have the assignment done for me, I want to do it but just looking for information on how best to approach it. Thanks for the link I'll check it out!
deepSeq
Well, one thing I would recommend is the use of ByteString (if 8bit suffices) or Text (if you need unicode) if it is supposed to be fast. String is not the best Haskell type to use for anything performance-critical.
Then I would encourage you to approach the problem by getting a reasonable understanding of how it works in an imperative context (you're not likely to find an example of it in a functional form, and the process of translating from an imperative understanding to a functional implementation will help you better understand functional programming). Once you understand the algorithm in an imperative form, think about how you would translate the instructions of the imperative approach to the equations and transformations typical of functional approaches. This won't be easy, but changing how you think about problems from an instruction-oriented to an equational and transformative approach is what learning functional programming is all about. Once you've done the above, you'll be able to post some more concrete questions that we can help you with.
Well, pretty much everything else to be said about the implementation is in the algorithms or at the very least requires a naive implementation to be improved on first.
Thank you for that. It is an interesting piece of code. It fails because a) It uses the keys in order, as returned by toList to represent the map. (In this particular case, the values don't matter!) b) It writes those representations to disk, and reads them back. c) Somewhere else in the code (presumably), it compares those representations from different runs. On the face of it, I wouldn't have thought to code this sequence using toList as I wouldn't even have presumed it gave me the guarantee of same order for the same data values even within a single run: Order of operations could easily influence the sequence of keys in toList, even with the old hashing!
It was probably an interesting talk, but unfortunately hardly any information comes from the slides themselves. :(
&gt; While monads in Haskell are very important to know because it's Haskell's bread and butter, knowledge of monads in other multiparadigm languages are definitely not needed. That's completely false. Monads are an extremely useful control abstraction, which is why monadic combinators exist in Scala for all sorts of types, including Seq and Option. They also exist in a variety of other languages including C# and JavaScript. &gt; could complain that Haskell is terrible with non existent memory management Haskell has non existent memory management? What? &gt; or things that require mutating compared to other languages like matrix manipulations Repa's got you covered. You hardly need mutating for matrix manipulations. Even if you do need mutations, there's always ST. &gt; Scala isn't good at those things because of the 'sacrifices' they made with their design decision along with the fact that it's on the JVM. Not all of these things are due to sacrifices made for Java interop. Nullability could've been handled much better (and is in Kotlin, also on the JVM), Side effects could still be tracked with effect types (and it just marks all Java functions with unknown effects). &gt; In return, they get access to things like interop with Java which bring things like maven to the table and deployment to anything that supports the JVM including Android even if the support is a bit shaky, which Haskell can't claim to do. This is irrelevant. I'm comparing each language based on my personal taste for programming in the language -- _not_ on extrinsic benefits like platform or tool support. Haskell does well in both departments, but Scala has sacrificed one for the other. There's nothing stopping a purely functional, statically typed language from being implemented on the JVM with far fewer sacrifices. Frege is an example of an attempt that is quite usable, although not well optimised at present. &gt; I can't exactly say that I'm enamored with Haskell's global type classes either but eh. I agree there, but I don't like Scala's approach here either, where the compiler will go through all sorts of contortions trying to find an instance.
&gt; what makes Haskell a significant advantage over writing simple and clean code but all I got are things that don't really relate to that. Equational reasoning doesn't relate to writing simple and clean code? I think you'll find that it does. Also, what's with the overreaction? &gt; Google makes you log in, and then gives you a 404? No thanks, I guess I'm sticking with Haskell. This is clearly a joke.
You still get spooky action at a distance. It's equivalent to Java's `final` -- functions that mutate variables are not marked by the type system in any way.
Yeah I have been looking at it in Java to see how I would change it over. One of the main things I will have to do will do all the hashing stuff. I barley have an understanding of this in Java, let alone Haskell. Is hashing where you use numbers to represent strings or something?
But the order in which you do things is deterministic. It doesn't vary between two runs of the program with the same input. Relying on toList returning the same value each time it is applied to a hashmap constructed in the same way is the norm in Haskell. Violating this is an impurity.
&gt; the type system is as complex as you want it to be. Except, trying to implement any sort of higher-kinded polymorphism usually leads to horrendous type signatures with ugly projections out of structural types where in haskell it's a natural extension that you barely even notice. 
I fixed the compilation error of poppler library and upgrade it to poppler-0.12.2.2. Please check this new version and try hoodle with pdf support within ghc-7.6 environment.
I believe you're referring to this comment by bos: &gt; Haskell is easily portable. The thing is, Windows developers hardly ever step up and actually contribute code that will fix their specific problems with particular libraries. I would generalize the Haskell-on-Windows community as okay at complaining, poor at providing details, and very weak when it comes to actually fixing stuff. (This mirrors my experience of Windows-based developers in general, who seem to come from a low-competence mirror world that is much like our own except for everyone having half the IQ and drive.) I quote this in full because, in context, it is obvious he's directing his comment at members of the _Haskell_ community who use windows. Seeing as he's a co-author of Real World Haskell, and made several contributions to numerous Haskell libraries, as well as other tools such as netplug and Mercurial, I'm sure he's talking from extensive experience and frustration with windows. &gt; No offense but I see negative comments towards other platforms in /r/haskell in much higher frequency than any other programming subreddits. Anecdotal, but nonetheless I must question your observation. My experience is the exact opposite.
It definitely sounds like you need to start by understanding the algorithms involved. Implementing hashing is its own task, though, which most languages provide libraries to do for you. If you double-check and the assignment doesn't ask you to implement your own hash function, then you can just use http://hackage.haskell.org/package/hashable instead.
&gt; Honestly give me a real world example that isn't Haskell where higher kinded types are an absolute necessity. I've been writing production code in teams for over 7 years and never ever had a situation where that is necessary. Honestly, give me a real world example that isn't Java where objects are an absolute necessity. I've been writing production code in teams for over 7 years and never ever had a situation where that is necessary. The C family of languages have been doing just fine without it along with many other languages. Just because it's possible to do fine without doesn't make a feature useless or invalid. Seriously though, if you have a bunch of types Int, Bool, Char, and you want to generalise them, you use a type variable of kind *. Java supports this. Haskell supports this. Scala supports this. If you have collections of stuff, which are parameterised by one of these types, you can write List a or 'a list or List&lt;A&gt; or List[A]. Now, what if you want to generalise over a bunch of collection types, each of which take an argument in this fashion? In Haskell, you just use a type variable of kind * -&gt; *, just as easy and simple-kinded data types. In Scala, you can do it with a lot of noise and horrible type signatures. In Java you can't do it. 
I don't get your hostility. We're done here.
Well the lecturer provided literally no detail to the assignment haha! The actual description on the assignment website was: "Implement the Rabin-Karp and Boyer-Moore string searching algorithms in Haskell." So I assume we can just use hashable! Thanks for that should help. So I just convert each string into a hash number or something?
Well, again, you should start by understanding the algorithm you choose. You won't need hashing at all for Boyer-Moore, and for Rabin-Karp, you'll end up hashing various substrings as described in the algorithm itself.
Again, where the NDK is commonly used isn't really relevant here. What the NDK offers is an environment where native code can be run outside of the managed bytecode environment, albeit without direct access to Android APIs. It turns out that except for the bit about access to Android APIs, that's precisely what we want! We don't WANT to use Dalvik's garbage collector, for instance, since GHC generates code for an RTS with its own garbage collector, which will operate on top of chunks of memory allocated from the underlying system. The problem with Android APIs will require a more creative solution... but in the interim, just being able to call Haskell code via the NDK would be a huge step forward. But as I said, it's not too difficult to imagine a code generation answer there, where a generated Dalvik stub interacts with the NDK Haskell code. This is likely to require that the top-level Haskell bits be written in continuation style... but then again, we have the one programming language in the world where things like that aren't particularly painful.
Yeah I have been reading about Rabin-Karp. I'm going to try that one first. No idea why I'm getting so many down votes for trying to get some help, oh well. I understand how Rabin Karp works, just hard trying to figure out how to get it in Haskell. 
then patch GHC for Dalvik codegen.
&gt; That's completely false. Monads are an extremely useful control abstraction, which is why monadic combinators exist in Scala for all sorts of types, including Seq and Option. They also exist in a variety of other languages including C# and JavaScript. Sorry, I should have clarified to say explicit knowledge of monads aren't necessary. There aren't going to be many programmers in the wild that know what exactly monads are. Programmers on my team use LINQ all the time without knowing so and it's not necessary. But with Haskell, it's an in-your-face type of deal. Just read any tutorial or books. &gt; Haskell has non existent memory management? What? Does it? I've been using Haskell for a year now and have never done anything related with memory. With Objective-C (or any language with support for this), I can allocate native Objective-C objects on the heap and free it whenever I'd like. Haskell has no such support. &gt; Even if you do need mutations, there's always ST. Which is incredibly shitty compared to other languages that doesn't force immutability. If you disagree, you're obviously donning the fanboy gloves. &gt; Not all of these things are due to sacrifices made for Java interop. Nullability could've been handled much better (and is in Kotlin, also on the JVM), Side effects could still be tracked with effect types (and it just marks all Java functions with unknown effects). Nulls definitely could have been handled better. I never claimed it was perfect. &gt; This is irrelevant. I'm comparing each language based on my personal taste for programming in the language -- not on extrinsic benefits like platform or tool support. Haskell does well in both departments, but Scala has sacrificed one for the other. Durr. I prefaced my comment acknowledging that it's your personal opinion. But expecting features from a fully functional and pure language from a hybrid one is rather ludicrous, don't you? Also, what planet are you on? Haskell is rather poor in the tool department. It's there but I wouldn't say it does well either as it pales to a lot of tools in the JVM/.NET world. Nor does it do well on the platform support as you have Windows programmers out there somewhere complaining about libraries not building and Haskell has zero presence on ARM devices whereas the .NET/JVM/C family of languages have a decent foothold. &gt; There's nothing stopping a purely functional, statically typed language from being implemented on the JVM with far fewer sacrifices. Frege is an example of an attempt that is quite usable, although not well optimised at present. No one mentioned Scala being purely functional nor does it claim to be. Obviously your bias is fully towards pure functional languages and while there is nothing wrong with that (well, I think so but whatever) and I mentioned it already, you shouldn't expect a pure functional language out of a hybrid one.
&gt; But it's an extremely simple to implement change. Wat. What does this even mean? &gt; After all, _it's already possible to do in Scala_, just extremely ugly for no reason. Then don't do it. Simple as that. There is a reason why casting in Scala is hideous and it's because it's also discouraged. &gt; Like I said, Scala already does, just with a bunch of horrible hacks that pull in _even more complicated_ type system features, specifically structural subtyping and type projection. The problem I have with languages that start out with academia is that many developers shoehorn their favorite or experimental feature. I feel the same is happening with Rust and it's already happened with Scala and Haskell. &gt; When I'm asked to compare my preferences of two languages, I'm going to bitch about the parts of the language that make my life difficult. When I can't easily generalise over containers, that makes it harder to write generic programs in the language. Doesn't matter what paradigm it adheres to, this indicates the type system is poorly designed. No. It indicates that the type system isn't what you want it to be. Bringing up higher kinded types isn't going to help you in this regard because quite frankly, it doesn't make life difficult **period** in Scala or any other nonfunctional or hybrid language. If it makes life difficult to you then it tells me that 1) you're not a very good programmer because you can't adjust to different programming styles or 2) you haven't written code for any client whatsoever or you've only written libraries or for academia or 3) you just want to rain hate for the sake of complaining that X programming language can't do Y in Z programming language, which is of zero surprise. Wake me up when someone says "I developed a distaste for Haskell because it doesn't have any features from MY programming language" in other subreddits. 
The ST monad in hashtables is different. It exists because the hashtables are mutable. Even if we made a monad that carried around the seed, every library that used HashMaps would have to expose it in its interface.
&gt; So your objection is to the use of the term "monad" then? I'm afraid you've lost me. I mean the specific knowledge of what monads are and implementing them and as I've said, many programmers in the .NET/JVM/C world knows diddly squat about monads, let alone implementing one. It's not prevalent in those worlds because it's not needed as much as it's needed on the Haskell side. Try using Haskell without monads. Oh wait, you can't since you start out through the IO monad. For C on the other hand... &gt; Oh, so that's what you meant. Well, you can't do that in Scala either. Pretty obvious, no? What is this supposed to address? &gt; I do disagree, but before you dismiss me as donning the fanboy gloves... Doesn't even address what I said and it does doesn't change the fact that mutating in Haskell is terrible compared to other languages that supports mutability. &gt; Haskell supports imperative programming. It also supports functional programming. It's obviously targeted more to functional programming than imperative programming, and I don't begrudge languages that go the other way, but Scala handles the mixing of the two styles very poorly, in my opinion. Clojure, while dynamically typed (a _big_ downside to me), does much better in this respect. If by support imperative programming, you mean in a hackish way then yes. In other programming languages, I don't have to worry what monad I'm in when performing statements. But sure, your opinion that Scala mixes the two terribly is perfectly valid. My take on it is that it's something the JVM world desperately needed since C# on the .NET side is progressing rapidly and if you're deploying on the JVM, then it's a godsend compared to anything else out there at the moment. It's also easy to hire developers that are familiar with Java or C as they share similarities and easy to target many areas including the mobile arena. &gt; I disagree. We have one of the most advanced runtime systems and garbage collectors in the GHC RTS, one of the most sophisticated compilers in GHC, some of the best test frameworks (QC etc.)... And that's great. &gt; ...some of the best tools for profiling and benchmarking (particularly for parallel code -- see criterion, threadscope, ghc event logs etc.), an interactive optimiser (hermit), a type-based code search (hoogle), as well as all the basics: build system, documentation system, coverage checking etc. Eh. The JVM and CLR have these too with regards to profiling and benchmarking along with very nice ones for OS X. Look at Visual Studio's built in profiler which isn't bad and Shark. Where Haskell falls down is an IDE and a decent build system, which is damn important because that's what is done the majority of the time. &gt; In fact, GHC beats the scala compiler hands down in terms of compilers And your reasoning is? &gt; Haskell does compile to ARM, but we do have more work to be done there too. As for .NET/JVM/C gaining foothold on those platforms, I think that's mostly because they're supported by the companies that make those platforms, rather than any specific issue about the language itself. Yeah. ARM support is terrible and it was really painful last time I tried using it. In all fairness, it could be better now. Regardless, Windows users are also apparently boned on some libraries. 
This is what the `operational` package does. Edward says (but I haven't verified) that this `operational` is isomorphic to `Free`, so you are in fact using free monads in principle.
Reminds me a bit of http://hackage.haskell.org/package/IOSpec
Very cool! I guess I'll use `operational` then! I suppose I should send the author a patch for my tracing and testing functions for it, if he'll like them.
Igraph is a really good graph library, and if [this example code](https://github.com/giorgidze/igraph/blob/master/examples/Example01.hs) is any indication, these bindings are admirably straightforward. I've been wanting this for a while.
Exactly! I should be able to rely on it, and I did. I personally consider it slightly bad form to rely on things like toList between runs which are undocumented as to their output order, so am happy to fix it, but breaking purity does break programs. I like Haskell because it lets me be dumb, as we lose purity I spend more time working round administrative coding issues, and less time doing clever stuff.
&gt; It absolutely does address what you said. Why is it terrible? I find it quite useful, particularly for static reasoning. In what way? You just gave an opinion on why Haskell have to go through extra steps for simple mutability, which missed my point. My point is that Haskell has to go through extra steps just for mutability and it's not as simple as other languages. &gt; Actually, one of my fellow researchers has done a huge amount of work on parallel Haskell as well as parallelism in other systems. He says for parallel profiling, GHC is far better than any competition. To each their own, I guess. &gt; Mostly this is a problem of Haskell including a lot of bindings to C libraries, which are always a PITA to set up in windows. This problem exists in pretty much any language when you're binding to C. But for some reason, python's pip and ruby's gem largely don't have that problem.
http://hackage.haskell.org/package/operational
I've used haskelldb a good bit. I'm not aware of any migration support like you're probably thinking about. The way I always did it was to modify the database manually and then re-generate the code for the table representation. With persistent, migrations are certainly easier.
&gt; How am I wrong? I haven't even made any arguments (in this part of the thread). I am just stating facts. Sorry, I may have been too forward in this case where I skimmed your comment. Regardless, I was directing my comment towards this when I was talking about the general elitism and negativity towards other platforms on /r/haskell: &gt; Anecdotal, but nonetheless I must question your observation. My experience is the exact opposite. and &gt; Also, what's with the overreaction? &gt; Google makes you log in, and then gives you a 404? No thanks, I guess I'm sticking with Haskell. &gt; This is clearly a joke. I'd rather not hunt down the comments I've read the last time I read though /r/haskell but from just reading your comments regarding Scala, vocabulary like "substantial distaste" and "ridiculously warty and ugly type system" doesn't convey positive feelings. Nor does comments like: &gt; I would *not* be give up haskell for hiring purposes. &gt; I believe strongly in writing systems which are simple and clear and in my limited experience using Scala I feel that there is a significant advantage in Haskell here. and &gt; This mirrors my experience of Windows-based developers in general, who seem to come from a low-competence mirror world that is much like **our own** except for everyone having half the IQ and drive. (Emphasize mine) I just get this elitist vibe from /r/haskell but it could just be me and hopefully I'm wrong. 
Haskell has no variables because nothing ever varies. All values are immutable.
I don't mind the extensions so much (`EmptyDataDecls` is Haskell2010; MPTCs really should be official Haskell already; `FlexibleContexts` and `FlexibleInstances` are no-brainers, especially once you have MPTCs). Though I do think it's much better to use a `{-# LANGUAGE #-}` pragma rather than hiding this away in the cabal file. The pragma ensures that the code can be used by tools other than cabal ---I'm not just talking build systems here, what about syntax highlighters? `haskell-src-exts`? refactoring tools?...---; it makes it a lot easier (for you) to develop/debug the code in GHCi; and, frankly, it makes it easier (for us) to see what all is involved when browsing the source code.
good work tekmo!
&gt; I feel the theory behind it is extremely compelling, and the provided API is something that doesn't require hours of academic reading to get results from. There's a certain irony here, given that (to my knowledge) compared to its general competitors, `pipes` makes more extensive and enthusiastic use of exactly the sort of math-influenced concepts that are often accused of being too academic. Score one for Tekmo's general philosophy, it seems.
Ah I see. Man I hate this language
I would like to see all possible documentation, including all source code, to be the default across all platforms. If the install footprint were 10 GB I'd still feel this way; Haskell is a reason I buy computers. So, no, I don't share your concern over the install footprint.
I've been programming for 3 years, not very good at it though, doing it in College. Mainly been using Java. I have always wondered about the main uses of Haskell and why people would choose to use it, and that post explained it quite well, so thanks! I wouldn't mind being good at Haskell, it would be a pretty cool thing to say you are proficient in. This assignment is hell for me though.
Included libraries? Plus documentation? Also: Are those values for the same version? Here on Gentoo, GHC 7.6.1 (self-compiled) is **693.57 MB** in **5630 files**. Most of which (in number of files) being documentation an libraries (like base, Cabal, etc). And considering that OSX doesn’t have a Linux-style package management system (or does it?), I bet the package there contains a ton of libraries.
My understanding is that part of the reasons is that GHC install multiple versions of a bunch of libraries (like having both native and byte code version), although I could be wrong about this. I have never hacked GHC and really don't know that much about the distribution internals. From my perspective the large size is not an issue, since a. Haskell is a necessity b. 1gb of HD space costs order of magnitude 10 cents USD. On the other hand, I could see this being an issue if I were trying to run GHC on a mobile device (something I could imagine doing within a few years). 
Tekmo for president!
Awesome! I've updated the hsx-jmacro and happstack-jmacro packages to work with jmacro 0.6.
&gt; vocabulary like "substantial distaste" and "ridiculously warty and ugly type system" doesn't convey positive feelings They're not meant to. I'm criticising Scala because I was asked about my opinion. I'm not trying to praise Scala here. Excuse me for using rhetoric. Also, I don't see what's wrong with this comment: &gt; Anecdotal, but nonetheless I must question your observation. My experience is the exact opposite. In what way is that elitist, or negative? &gt; I would not be give up haskell for hiring purposes. I believe strongly in writing systems which are simple and clear and in my limited experience using Scala I feel that there is a significant advantage in Haskell here. In what way is saying that _he feels_ (therefore, _his opinion_) Haskell has an advantage in a particular area elitist or negative? Languages are not equal, there is legitimate comparisons to be made between them, and some languages will be better than others in various metrics. If anyone feels that Scala is better than Haskell, they absolutely should be allowed to explain their reasoning without being criticised. If anyone feels the opposite, as I do, I don't see why the situation should be different. Next, as for this comment: &gt; Also, what's with the overreaction? Google makes you log in, and then gives you a 404? No thanks, I guess I'm sticking with Haskell. This is clearly a joke. Once again I fail to see what is elitist about this. I'm just stating that you appear to have interpreted a joke literally. Happens to all of us (this is the internet after all), but it's not intended as a form of condescension or elitism. &gt; This mirrors my experience of Windows-based developers in general, who seem to come from a low-competence mirror world that is much like our own except for everyone having half the IQ and drive. (Emphasize mine) "Our own" here refers to "non-windows developers" (bos writes plenty of Python too), not to "haskell developers". 
&gt; But for some reason, python's pip and ruby's gem largely don't have that problem I take it you've never tried to use pip or gem on windows for C-bound libraries. I have (well, not pip, but I have tried gem), it's not pretty. No worse or better than cabal manages things. &gt; In what way? You just gave an opinion on why Haskell have to go through extra steps for simple mutability, which missed my point. My point is that Haskell has to go through extra steps just for mutability and it's not as simple as other languages. See, in my view, ST is more simple than other languages. By separating the notion of execution (of side-effects) from evaluation, you can reason about the composition of state transformers and the transformations they make entirely separately. This is a very powerful tool for static reasoning, which makes understanding side-effects (in a formal sense) much, much easier. Not only that, but this separation allows for standard equational reasoning to be carried out for all programs, not just those that stick to the purely functional paradigm (as in SML). To me, this makes writing correct programs _simpler_ not harder.
&gt; If its so simple to implement then why isn't it done yet? It is, but it's a compiler plugin that is difficult to get approved by corporate bigwigs. https://github.com/non/kind-projector &gt; Oh I'm sorry. All of our apps for the iPhone and Android was so difficult since we didn't have higher kinded types! No seriously. You still never gave me a real world example. I don't want theoretical examples which could be gotten anywhere. Easy. Suppose you have an effectful program, and you want to test it. You want those side effects to do something different now for testing purposes. You'll want a generic interface for those effectful programs, and then you'll want to ascribe semantics to those separately. This needs monads (specifically free monads), and it's very difficult to do in Scala. I can tell you that this is the exact use case I ran up against while I was working on this big Scala project, but obviously the problem is not insurmountable, just annoying. I never claimed Scala made things impossible or fail, I just claimed it made things annoying. &gt; And these so called kludgy work arounds are what? In the example above, switching to mock objects that rely on mutable state, or using the kind-projection trick in the type system to implement the free monad. Either way, quite kludgy. 
Also, since you already have a functioning solution, I thought I might give you a taste of a more elegant solution. The real idiomatic Haskell way to do this would be to define two functions. The first function would split your string into windows that match the length of the pattern: import Data.List (tails) windows :: Int -&gt; String -&gt; [String] windows n = filter (\s -&gt; length s == n) . map (take n) . tails -- You could even skip the filter, to get the same behavior as your current algorithm &gt;&gt;&gt; windows 2 "ABC" ["AB", "BC"] The next function does the actual hash comparison: import Data.Function (on) hash :: String -&gt; Int hash = ... hashEq :: String -&gt; String -&gt; Bool hashEq = (==) `on` hash Then the final solution is just: rabinKarp :: String -&gt; String -&gt; Bool rabinKarp str1 str2 = any (hashEq str2) (windows (length str2) str1) It literally says "Check if any of these windows of `str1` are equal by hash to `str2`". A functional style is all about breaking problems into smaller solvable pieces and then combining those correct pieces into the final solution. Half the challenge is just learning that everything you've always wanted to do is already somewhere in a standard library. The other half of the challenge is just assembling these pieces into a solution.
&gt; In what way is that elitist, or negative? I'm pretty new at reddit and I couldn't figure out multi level quoting. Didn't want it screw it up. I wasn't referring to you saying "Anecdotal, but nonetheless I must question your observation. My experience is the exact opposite." as elitist but rather my response to you being wrong referred to this and bos's comment on Windows developers. &gt; They're not meant to. I'm criticising Scala because I was asked about my opinion. I'm not trying to praise Scala here. Excuse me for using rhetoric. Whatever. It's the way you said it that gives negative connotations as you supposedly got from some of my comments when no vitriol was intended. &gt; In what way is saying that _he feels_ (therefore, _his opinion_) Haskell has an advantage in a particular area elitist or negative? Taking a brief time to glance at a language and immediately claiming that the advantage of clear and concise code goes to the language he is most familiar with is utter bollocks. How is that fair to any language? &gt; If anyone feels that Scala is better than Haskell, they absolutely should be allowed to explain their reasoning without being criticised. If anyone feels the opposite, as I do, I don't see why the situation should be different. Absolutely. But at least take the time to fully learn the language. &gt; Once again I fail to see what is elitist about this. I'm just stating that you appear to have interpreted a joke literally. Happens to all of us (this is the internet after all), but it's not intended as a form of condescension or elitism. That's your opinion and to me, it isn't a joke. If it is, it's a rather poor joke made in jest. &gt; "Our own" here refers to "non-windows developers" (bos writes plenty of Python too), not to "haskell developers". No, I don't see it that way. But even if he did, it's still a form of elitism and it's so obvious, you didn't even say anything about that. Anyway, clearly you haven't read his comment history and seen his dislike for Windows. Read the hash table release thread. 
&gt; It is, but it's a compiler plugin that is difficult to get approved by corporate bigwigs. https://github.com/non/kind-projector Gotcha. &gt; Easy. Suppose you have an effectful program, and you want to test it. You want those side effects to do something different now for testing purposes... It's a rather poor example and isn't concrete at all. It doesn't show me what you gain in real world projects. And again, there are plenty of successful apps and frameworks that don't make use of higher kinded types. Mind actually mocking something up that shows a substantial benefit to users of languages without higher kinded types? &gt; I never claimed Scala made things impossible or fail, I just claimed it made things annoying. You claimed the complex implementation of higher kinded types in Scala made life very difficult. Which now it seems like it's merely "annoying". Makes me not what to know what your opinions are on languages without an exotic type system. 
&gt; I take it you've never tried to use pip or gem on windows for C-bound libraries. I have (well, not pip, but I have tried gem), it's not pretty. No worse or better than cabal manages things. I have and it's fine for the small things I've used it for with devkit. What gems did you have problems with so I can try it out. :-) &gt; See, in my view, ST is more simple than other languages... We're not going to see eye to eye on this so let's just leave it at that. Let me say this though because you keep on repeating it over and over again. "Equational reasoning" isn't the end all be all. If it was, droves of developers would be flocking to Haskell, which last time I checked, isn't quite popular yet. Not everyone can think in abstract terms or have trouble dealing with side effects in code. But of course, it's your opinion. 
&gt; Let me say this though because you keep on repeating it over and over again. "Equational reasoning" isn't the end all be all. It certainly is extremely useful though, if you use formal reasoning when writing programs. Most programmers do not. I do. &gt; If it was, droves of developers would be flocking to Haskell, which last time I checked, isn't quite popular yet. Sure, I don't expect most programmers to use the same methodology I do, but I will loudly proclaim equational reasoning as the reason that _I find it_ easier to produce correct, clear, and succinct code in Haskell than most other languages. There are, of course, many other languages that also have this property, and it's the one thing that makes the biggest difference to ensuring correctness in my opinion. All those languages which allow for equational reasoning also enable me to write correct programs much faster and more easily than I otherwise would. Edit: as for my gems experience, this was some time ago (i.e DevKit barely worked, the old slow MRI was the standard), but I couldn't get even the most basic C extensions to build. I see it has improved now, though. Haskell on Windows isn't as bad as you're making it out to be. All it requires is installing a C compiler (mingw or cygwin), setting some environment variables and GHC will just work, for most libraries. The issue is that most Haskell developers don't use windows when maintaining their libraries so it's easy for windows-breaking changes not to get noticed. http://www.haskell.org/haskellwiki/Windows
Thank you so much for the in depth answer really helps man. My current soltution now looks like this: http://hpaste.org/79354 But I still get errors on compile. It's saying "parse error on input let" What could be wrong with it? EDIT Acutally I got it working :) I was missing an = on line 9 I think. All good now man thanks a lot for the help. Any information on Boyer Moore algorithm? Should this be easier or more difficult than Rabin Karp?
Ah, this is for the 'bidirectional' pipes stuff, which is possible. The unit argument is the 'metadata' that is sent when a downstream pipe requests more data. For example, you could put more complex data in there, such as the time of day, to indicate to the Producer that it should only send information about things that happened before/after that time.
And it'll still break your "web server that uses shake" (or any other example you could care to think of where one bit of the code wants determinism and another bit has to think about network attacks). It's just not a modular solution. The modular solution is to pass in salts to the bits that need it.
Great Article. But now, as a beginner, i'm lost on what to choose: pipes or conduit? 
a) I'll let Neil answer that. I don't know the details of how the table is constructed. But I do know that the table is sometimes constructed in exactly the same way. b) In Haskell memory allocation, thread scheduling, RTS settings, etc. cannot affect the final result (if there is one) for a pure computation. If I want an impure language, I know where to find them. Haskell should be kept pure as far as practically possible.
I actually agree with you. I do provide a [tutorial](http://hackage.haskell.org/packages/archive/pipes/3.0.0/doc/html/Control-Proxy-Tutorial.html) that covers these issues in depth, but I did make some design tradeoffs compared to, say, `conduit`. For example, if you want to fold a source with `pipes`, you have to add `WriterT` to your base monad. With `conduit`, you don't. So you might wonder why I do it this way instead of copying conduit. The first reason is the simplest: when you fold things like `conduit` does you lose the category laws. A foldable pipe has no upstream identity and doesn't form a true category. You might think "So what? How does that practically impact me?" The answer is "a lot!" You can think of theoretical laws like the category laws as the "canary in the coal mine" and when they break it is the early warning sign that you are making poor design tradeoffs. In fact, for any theoretical law violation there is usually a plain English explanation of the problem with that law violation. In the case of the category laws, the lack of an upstream identity is the category theory way of saying "your implementation is biased towards upstream and gives the most upstream component special treatment". We can build an intuition for why this plain English interpretation is true by imagining we wanted to write some function that took as an argument a chain of composed components and returned the most upstream component: f (p1 &gt;-&gt; p2 &gt;-&gt; ... &gt;-&gt; pn) = p1 If our API formed a true category, this function would be impossible to write, because the upstream identity law would continually foil it: f (p1 &gt;-&gt; ...) /= f (idT &gt;-&gt; p1 &gt;-&gt; ...) So, conceptually, the identity laws ensure that the two ends of the composition chain don't get special treatment, which helps prevent all sorts of corner cases that might arise at the boundaries. Ok, so back to `conduit`: how does the lack of an upstream identity translate into the `conduit` user experience? Well, if you've ever used `conduit`, you know that requesting information from upstream is much more difficult and non-compositional than with `pipes`. With every request you must check to see if the incoming value is `Nothing`. Equally frustrating, there is no generally correct default behavior to do if you just want to ignore the `Nothing`. `conduit` tried to give special treatment to the upstream end by ensuring that the upstream pipe doesn't bring down the session when it terminates, but as a result, that extra corner case ends up infecting the entire API and you can't write any utilities that are not aware of this corner case and you can't write anything that abstracts over the `Nothing` check. Also, in `conduit` only the most downstream component can return a value from a fold. Again, this is a symptom of `conduit` giving the upstream element special treatment. This mean you cannot: * Fold upstream stages * Fold more than one thing at a time * Fold information flowing upstream (Of course, `conduit` doesn't even provide upstream communication, but if it did you can imagine that folds would not work the other direction). Also, the problem is not just about folding. The fact that the return value always comes from the most downstream component also complicates other things like: * The ability to implement native error handling like `pipes` does * The ability to embed composition chains in do notation All of these are victims of the one upstream identity law violation. The only reason people haven't been aware of these deficiencies up until now is because they've never worked with a streaming library that got it correct and therefore didn't realize all the features they were missing out on as a result of that deficiency. The `pipes` approach to folding is slightly more verbose for the one particular case that `conduit` optimized its API for, but unlike `conduit` it handles so many other cases that `conduit` cannot even do. More importantly, any utilities you write never have to be aware of any other folding that might be going on. Contrast this with `conduit`, where if one conduit in the chain does not propagate the folded value correctly then the entire fold is broken. `pipes` completely decouples the folding concern from the rest of the API so that you can program entire standard libraries blissfully unaware that there is a folding mechanism that you need to be mindful of. So yeah, for toy examples `pipes` is slightly more verbose, but for more sophisticated examples `conduit` is brittle or outright breaks.
You're correct. His version shadows the existing variable with a new one. In fact, if you compile his with `-Wall`, it will warn about this shadowing. So, yes, your version makes it a lot clearer that those are two separate values.
It supports bools… Depends how you define complex. Some things like PostgreSQL's arrays aren't supported. Here's some normal code demonstrating most of the features of HaskellDB: getEntryConferences :: Bool -&gt; Maybe Int -&gt; Model [SimpleConference] getEntryConferences requireComplete limit = fmap (mapMaybe makeSimpleConference) $ query $ do event &lt;- table T.simpleEvent meta &lt;- table T.event_data restrict $ isNull (event!F.parent) .&amp;&amp;. notNull (event!F.startDate) .&amp;&amp;. notNull (event!F.macronym) .&amp;&amp;. (val (not requireComplete) .||. notNull (event!F.logo)) .&amp;&amp;. meta!F.event .==. event!F.id .&amp;&amp;. meta!F.label .==. val "description" .&amp;&amp;. (val (not requireComplete) .||. (notNull (meta!F.value) .&amp;&amp;. meta!F.value .&lt;&gt;. val (Just ""))) .&amp;&amp;. (event!F.startDate) .&gt;. now .&amp;&amp;. event!F.appearInList order [asc event F.startDate] whenJust limit $ \limit -&gt; do top limit project $ F.acronym &lt;&lt; coerce (event!F.macronym) # F.start &lt;&lt; coerce (event!F.startDate) # F.id &lt;&lt; event!F.id # F.description &lt;&lt; coerce (meta!F.value) # F.period &lt;&lt; event!F.period # F.website &lt;&lt; event!F.website [Here's an example](http://hpaste.org/79365) of composing queries together with the monad, and using the text search API of PostgreSQL. Hardly complex, but not trivial either.
The other problem with your hashing function is that you never use the rest of the string (i.e. the `xs`), so it only hashes the first character. Probably the easiest way to get the index is to first zip the string with a list of natural numbers and then use the zipped result: &gt;&gt;&gt; zip "ABC" [1..] [('A', 1), ('B', 2), ('C', 3)] This is the idiomatic way to label elements with their position. You can then fold this resulting zipped list using the integers as the index value you pass into your hash.
I suspect most everything in the docs is out of date! CalendarTime works. I haven't used anything else.
Setup.hs was going to be the EDSL. Turned out having a declarative, non-turing-complete dependency specification format was much better for static analysis, than executable code embedded in Haskell. Also made bootstrapping a lot simpler. See for historical reference, ["Why is there a cabal file at all?"](http://www.haskell.org/pipermail/cabal-devel/2007-January/000336.html) Better question for the G+ Haskell group, or haskel-cafe@ , than reddit, though, no?
I understand the boostrapping issue but I'm not sure it's as important now as it once was. As for static analysis, how do these differ? cabal :: Cabal Package and parse :: FilePath -&gt; IO (Cabal Package)
I believe some `vi` or `emacs` extensions can do this automatically for you, although I don't use them so I can't give any specific names. Perhaps somebody else can chime in.
With a tiny change, you get the more general [Free Applicative](http://hackage.haskell.org/packages/archive/free/3.2/doc/html/Control-Applicative-Free.html) which only requires `f` to be a `Functor` to get an `Applicative` out the other side.
&gt; Now, consider how the implicit search interacts with subtyping. Of course it's going to be that way for any language that is to handle variance along with trying to shoehorn in other complex type system features. With Haskell, you don't have to worry about it and trying to force Haskell's type system onto Scala's type system is ridiculous as they are both set out for different goals. In respect, Scala gains some powerful OO constructs but "loses" some on the type system. For some the drawback is well worth it while for others, it's not. No big deal. For me, it's not such a bad gig since I can mix functional and imperative programming along with some OO and I can handle the syntax just fine. I'm not aware of any language which successfully blends variance along with having a type system like Haskell. Please let me know if you do as I'd like to know as well. 
example (haskell-mode): https://news.ycombinator.com/item?id=4932868 
The step that usually follows parsing/validating a .cabal file has the same problem, no?
&gt; I suppose that [...] is true if you restrict yourself to the intersection of java and scala, but the intersection of scala and haskell already starts out complex, because scala is OO first and functional second (i.e. the OO parts of scala are more fundamental than its function features; you can ignore the functional bits and wholly use OO, but not vice-versa). Definitely true. Your point is? &gt; Are you just trying to say that you don't think that embedding algebraic/categorical structures (monoids, functors, etc.) into the Scala type system is a good idea? Or that trying to capture something with similar semantics to typeclasses is a bad idea? I'm saying that you shouldn't expect Scala's type system to be able to emulate Haskell's type system exactly as it's more complex due to it's underpinnings and design decision as you were with your comment. Nor should you expect the same with Scala -&gt; Haskell. It's obvious in that situation that Scala can't guess which supertype you're attempting to use due to variance. Were you really expecting magic at this point?
Do you consider the llvm, z3, smt-lib, etc bindings a downgrade from their respective languages?
I don't see the analogy.
I could, but I thought it would be more constructive to have a discussion about it than to send a patch that is essentially formatting. Perhaps I was wrong. Also, I was hardly complaining. More like observing.
In fact, Haskell is half a decade older than Java.
&gt; What's the point of such a sentence; Haskell too is from the 90s. Right, and Haskell's predecessors are "1970s" or "1980s" era software technology (I mean, Miranda isn't *so* different from Haskell, at least in the basics of syntax.) 
They say it like it's a bad thing. It takes a long time to develop a high-quality implementation and a good collection of libraries. The only widely used language that I'm aware of that is more recent is C#, from 2001, and that took huge amounts of resources from Microsoft to get where it is today. Almost anything more recent that has even marginal success piggybacks on some other language for libraries, like Clojure or Scala with Java, or F# with .NET.
By looking at the title I thought this is about the Mayan apocalypse.
&gt; There is a global shortage of skilled software developers &gt; skilled &gt; haskell if there's a shortage of c skills, applicative comonoidal semigroups arent the answer. the haskell lang is notoriously difficult
It's possible to do very difficult things (or to intimidate beginners) in C++ or in almost any language. Having interviewed a lot of Haskell users, I'd say most people ultimately don't find it particularly difficult -- just different. It helps if we don't always start lesson 1 with advanced theory; a lot of people like to learn from applied examples. To be clear, we believe the most effective way to address the developer shortage is to use tools like Haskell that are dramatically more productive. The alternative, training more people to use dramatically less-productive tools, is ultimately much less efficient.
indeed. In general, marketing that using lowest common denominator emotional appeal is not a way to draw interest from sophisticated people or sophisticated businesses. That said, there are a lot of cool problems people are attacking in industry using haskell tools, and people sharing those stories is always a good thing! :) -- Someone building a haskell biz
I want to make sure that this type of thing is OK here. I'm a long-time subscriber to /r/haskell and I tend to see a lot more advanced blog posts far more often than relatively basic things like this. But, I wrote this and I thought that someone here could definitely benefit. If there's a better place to post this type of stuff, let me know! Otherwise, enjoy it, and feel free to give me suggestions!
Awesome - I wasn't aware of this one.
I'm not sure I buy it, unless you're referring to the type-system goodness that is "modern Haskell" and I'm misreading you. The genesis of what we now call "functional languages" are possibly as old as computers themselves; [Plankalkül](https://en.wikipedia.org/wiki/Plankalk%C3%BCl) is from 1948 or there about. Even if you stuck only to languages that were *actually* implemented, the first Lisp was from 1958, and there were several languages that could be classes "proto-functional" (I mean, Algol68's condensed dialect, &amp; other such expression oriented languages come to mind. BCPL, &amp;c &amp;c.). Even "new-ish" areas of types are pretty old, and have foundations that run deep. Haskell is most definitely "state of the art" when it comes to the application &amp; research of many areas, but I think you're stretching it when you references "generational difference[s]".
&gt; I would argue that Java perpetuates a much older model of programming. As in, 1975 vs 1985?
+1 for applied examples. Also examples that relate to real world problems encountered before. I didn't find learning Monads via list or Maybe intuitive at all. STM on the other hand, that gave me a reason to think it was important.
The prospect of being able to express all of my computations as lines of swimming fish is deeply tickling to me. (An aside, you refer to (&gt;-&gt;) as horizontal composition and (&gt;=&gt;) as vertical: have you considered actually encouraging horizontal composition to be arranged horizontally and vertical composition vertically in the source code itself, [like diagrams does][1]?) [1]: http://hackage.haskell.org/packages/archive/diagrams-lib/0.6/doc/html/Diagrams-TwoD-Combinators.html
Well Haskell does need a little marketing.
This remembered me the joke: "You have a problem and you decide to use Haskell. Now you a have monoid in the category of problems".
Hopefully it doesn't rely on such extremism in advertising to be effective.
These are on-site classes designed for companies who want to get training for a group of people. For those who don't want to read through the whole page, here are the key links: [One-day intro for developers &amp; managers](http://fpcomplete.com/services/course-intro-fp-1/) [Two-day intro for developers](http://fpcomplete.com/services/course-haskell-2day/) [Two-day advanced for developers](http://fpcomplete.com/services/course-haskell-adv2day/) 
I was referring to features like [conditional imports](http://msdn.microsoft.com/en-us/library/92x05xfs.aspx). Don't underestimate the idle mind of a build engineer :-) &gt; While conditional import statements work in command-line MSBuilds, they do not work with MSBuild in the Visual Studio integrated development environment (IDE).
That is really cool. I think I will add a small note about that in the tutorial.
I still feel like stepping from Python into Haskell is like climbing from a car into a space shuttle. It can be a bit daunting.
&gt; It's obvious in that situation that Scala can't guess which supertype you're attempting to use due to variance. It seems like it could be reasonable to do a linear search of your supertypes. For example, look for an Applicative[Some], then an Applicative[Option] then an Applicative[Product] then an Applicative[Equals]. Ordering the search when you extend multiple traits is problematic, but it could work for extending classes.
I'd like to use regular pure Haskell code to define things like project defaults, warn when someone uses a bad package on our bad list, add compiler plugins to every project and call out to hlint as part of a dev build. I can do this with Makefiles today but that complicates managing projects that are also uploaded to Hackage. Think about the larger families of packages (snap, yesod, etc) and there is duplication and occasionally version conflicts with subprojects. Upgrading dependencies shouldn't require manually updating the version bounds in 20 cabal files. So let's move them to a common location. Does that mean inventing a new macro language, or perhaps relying on some other preprocessing tools to do the work instead? Or just use Applicatives and GHC, getting all the functionality documented by Haddock at the same time. And it's easy to automatically flatten the package descriptions when uploading to Hackage: evaluate the package description and save it into the sdist tarball.
I've been switching to optparse-applicative from cmdargs over the last couple months, it's great. Deleted so much code in the process... wish I would have found out about this earlier.
&gt; It seems like it could be reasonable to do a linear search of your supertypes. For example, look for an Applicative[Some], then an Applicative[Option] then an Applicative[Product] then an Applicative[Equals]. Ordering the search when you extend multiple traits is problematic, but it could work for extending classes. Completely unacceptable and you know it. Not only is this a land mine for bugs galore, how on earth is the compiler going to guess which super type I wanted to use for custom traits? You really think the designers didn't think this one through?
We are in the process of revamping our web site and moving it to Yesod (which is written in Haskell). No more grey on white, promise!
"applicative comonoidal semigroups arent the answer." Amen to that! You don't need to learn higher math in order to be an excellent Haskell programmer. I've been a C++ programmer for a good portion of my life before I discovered Haskell. Let me tell you, template metaprogramming in C++ is hellishly difficult, but the same thing translated to Haskell is simple and clear. 
We need MORE beginners posts here.
I like the post -- seems clear in both description and code and beginner posts always seem to go pretty well. I do think, in cases like this where the explanations are from first principles, it's worth mentioning somewhere that it's not a *requirement* to go through all this work because the language provides existing modules. It doesn't invalidate anything you've written, but heads off the objection forming in some readers' minds that it all seems like a lot of work. So just a sentence or two that Data.Graph and Data.Graph.Inductive (in the fgl package) exist might be useful.
What does the little dash with the circle at the end mean?
"In extensive interviews we’ve conducted at FP Complete, companies say Haskell brings them: ..." I'd love to see some data or survey backing up this claim. 
It could have been named Commercial Haskell Training classes for developers and managers.
Sweet, I'll post any subsequent ones that I do that I find particularly interesting! Glad to hear that this stuff is welcome.
"Over the next three months we will be publishing several detailed case studies to share what we’ve learned from specific companies using Haskell." Looks like you'll be able to.
Yes indeed, but stick with it, it's so worth it (at least until you start wishing you could do x in python)! Coming from python also seems to be a pretty common path. As an aside; I heard from a recruiter the other day that Haskell has in some ways taken over from python as a competence "flag" language that employers look for in a CV.
In all the software marketing I have seen this would hardly be considered extreme. Haskell has a value proposition. Sales when done correctly will be articulating what it is and making people care. If there is no value proposition then we are all just pissing in the wind.
I've never used the package, but it looks pretty clean and powerful. Nice work. Using terms as 'applicative' and such in your package name feels a bit like a code smell, though. Is using Applicative such an essential choice for your implementation to be put in de name of your package? Especially for something so concrete as command line option parsing?
Python is good enougth in most of the cases. Imagine if you should climb from Fortran to Haskell. I see Fortran too much nowadays in my work.
&gt; "You'll find that equational reasoning is required for clean and concise code" when I don't. I never said that. I said you'll find that equational reasoning relates to writing clean and concise code, because you can absolutely use it to write clean and concise code. I am not saying it's required by any means.
There's a huge difference between Haskell's principled design and Java's "practical" (some even may say "dumbded down") approach. The fact that Haskell started is earlier does not mean it is less advanced. I would prefer to avoid this marketing fallacy of "newer is better" altogether.
Anyone who claims that a programming language alone is going to solve the software crisis is a fool, a salesman, or both. The problems that face many organizations are much deeper than the language they use. The whole process has to be changed. A lot of software developers have trouble extracting the requirements. As much as 25% of the requirements can change during the course of development. Developer skill can vary by as much as a factor of 10. If the team isn't all equally skilled in Haskell, there will be issues. Tight schedules may force developers to drop unit tests, documentation, and code inspections. Refactoring may be delayed and the architecture becomes spaghetti. Team communication may fall apart and work is duplicated or not completed. The language can certainly make a difference, but the software development process is what really counts. A lot of Avionics software is written in C and they still manage to produce high quality software because they have a solid process. A higher level language may assist in these goals, but the article in question reads too much like it's proposing Haskell to be a silver bullet.
It is a datatype with an infix name and 2 type parameters. It is declared by the line `data a ⊸ b where ...`
I figured it would be something like that :) I would hope that there was a Free Applicative+Monad too (a bit like my expanded implementation), but since Monad doesn't imply Applicative yet, it isn't there :/
No it doesn't. By extrapolation, it then dates back to untyped lambda calculus, which should be compared to the Turing machine. But they were published in the same year, so he must've meant something else. Lisp and Haskell are obviously very different, so let's assume we're talking about a much specific class of languages. Pure non-strict functional languages, as in Miranda, which would have to be compared to class-based OO languages with generics, as in Ada. But then again, both concepts were implemented around 1980, the OO ones, I think, arriving a few years earlier to the party. So what models of programming are being compared here, exactly? Well, I don't know, it looks like a load of bollocks. That's what you are expected to end up with when trying to judge large complex concepts as black and white. The same goes to your “better” and “worse” dichotomy; it's just as emotional and nonsensical.
Would this not work for getting the rest of the stuff: hash :: String -&gt; Int hash [] = 0 hash (x:xs) = (ord x + (hash xs)) When I check the hashs of the words they have different hashs
The symbol is used for linear implication in [linear logic](http://en.wikipedia.org/wiki/Linear_logic). It is sometimes called lollipop, but in this case I would pronounce it "transformed to". So you can pronounce `apply ∷ (a ⊸ b) → (a → b)` as "`apply` has type (`a` transformed to `b`) to (`a` to `b`)".
Your definition is narrow. Two groups with differing skill levels may optimally make opposite choices about the right tool for them. We believe a significant part of Haskell's future is in large projects where more than one language is in use -- and where Haskell's attributes are important in many parts but not necessarily all. Better cross-language support is among the very top requests from commercial Haskell users we have interviewed. We will work on getting out some case studies that do show Haskell is more productive in real-world environments where it has been used. This is, frankly, a key reason why FP Complete exists: to increase global adoption of an incredibly effective, productive tool. 
Good point. Commercial users are definitely requesting better cross-language interop from Haskell, as well as requesting some more powerful native Haskell libraries for common scenarios.
Go may have 1-4, but it certainly doesn't boost productivity of 90% of coders. I don't see how it offers any boost in productivity over current Java or Python.
I wouldn't expect anything more scientific than measured anecdotes. I think programming languages are a bit too complicated to gather honest results about, even in the trivial case and within a single language.
This comment agrees with all its siblings that critique your "old = bad" marketing strategy. You could fool a lot of people with it, no doubt, but the cognoscenti who knows their CS history would stay clear away from you. At the end of the day, who do you want as your customers? Much less controversial is Java = imperative = a pernicious, widespread bad. For Haskell as hero, you'd need to further dissect for a lay audience, the pure vs unmanaged effects and the compositional / FP-style modularity vs OO-style modularity nexuses. One [1] is not enough. [1] http://fpcomplete.com/the-downfall-of-imperative-programming/ 
&gt; See, Control.Applicative _helps_ for reasoning, because you get the functor/applicative laws which give you several important guarantees about the behaviour of the &lt;*&gt; and pure combinators. I think you're just approaching programming from a fundamentally different way than most Haskell users do. Totally off topic. I didn't say anything about how Control.Applicative affects reasoning. I addressed "clear and concise" which to users where Control.Applicative or any libraries with DSLs is unbeknownst to, is confusing. Even I have to look it up from time to time. I would pull an example from my code which involved using old versions of Dispatch and have you try and decipher it but it seems like you would rather be blinded by a love for a language. 
&gt; I addressed "clear and concise" which to users where Control.Applicative or any libraries with DSLs is unbeknownst to, is confusing. Control.Applicative is in base, standard Haskell libraries. If you don't know about it, you don't really know Haskell. Are you suggesting I shouldn't use parts of the standard library -- parts which, I might add, most Haskellers understand, and use regularly because of the compositional and reasoning benefits they provide -- on the off-chance that people who aren't well-versed in the language will not understand it?
what site locations are available?
&gt; Control.Applicative is in base, standard Haskell libraries. If you don't know about it, you don't really know Haskell. Oh, really? You know absolutely every function in the standard Haskell library? I call bullshit. Just because a developer doesn't know of a function off hand, doesn't mean that the user doesn't know the language. &gt; Are you suggesting I shouldn't use parts of the standard library -- parts which, I might add, most Haskellers understand, and use regularly because of the compositional and reasoning benefits they provide -- on the off-chance that people who aren't well-versed in the language will not understand it? As a "Haskeller" myself, yes. From your comment, you would trade equational reasoning for simple and clear code. Therefore, equational reasoning does not help with producing such. 
That is because your hash function can still collide for different words. It is only a quick filter and you still need to compare for true equality if it passes hash equality.
I don't think "generational" is even correct. I'm not sure either for an "older" programming model. McCarthy's LISP (without mentioning lambda-calculus) is pretty old. Now maybe you're talking about mainstream, widely-known/used programming model. That is pretty different. I'm not a marketing guy but I think Haskell has so many good/better things on its own you can simply use them, no need to confuse people with half-truth.
Right. It doesn't make code simple and clean. Why don't you understand this? No one is arguing against the benefits it brings to the table. I'm arguing equational reasoning can obfuscate code for the user that doesn't know either 1) the DSL if used, 2) advanced obscure syntax in the language, or 3) concepts that require a PhD to understand. Okay, a bit of an exaggeration there but using complex concepts doesn't equate to simple code. Applicative functors were just a dirt simple example I had off the top of my head. Obviously there are even worse examples and stackoverflow and the Haskell wiki is full of it. Also, if you're arguing that DSLs are fantastic then great, kudos for you for thinking so and learning all of it. I on the other hand fully disagree and I have code that is testament to that with every newcomer cursing over it. It's also why Dispatch is being rewritten to have a saner API. Edit: you didn't ask for this but here is a rather mild example: Http(relationshipUriById(id) / "properties" / propertyName &amp;lt;:&amp;lt; postJsonHeaderMap &amp;lt;&amp;lt; value + "" &gt;|) Can you honestly tell what the hell is going on at first glance? Sure, it's very concise and simple for the person who wrote it but for the person who didn't write it? Luckily, my variable naming skills are great so you can somewhat figure it out but not really due to the brain dead API. :-) What if I wrote code using the conduit libraries with function names such as '$$' and '$$='? Would a person be able to understand that at first glance without looking anything up? Edit 2: Grammar
FP Complete definitely seems to do some hard work pushing Haskell into a commercial setting, and I like the idea. But I would like them to be a bit less excessive. For instance, having (as a very last topic!) "Introduction to Yesod" for a class titled "Advanced Programming in Haskell" sounds plain wrong.
Actually I got it sorted man thanks for all the help!
How does a misleading statement that doesn't even hint at any sort of "value proposition" elicit your response?
Have you ever used java before? It is a huge, convoluted, incredibly complex language. Your characterization of it as easy, and with only one way to do things is truly bizarre.
&gt;+1 for applied examples &gt;I didn't find learning Monads via ... Maybe intuitive at all. That is an applied, real world problem example. A very simple one, which easily maps to a genuine concern in typical imperative languages.
I like optparse because there is no template haskell
Interesting, I'd never heard of -XImplicitParams. More info here: http://www.haskell.org/ghc/docs/7.6-latest/html/users_guide/other-type-extensions.html#implicit-parameters
I dislike the `Monad` superclass constraint. It would be nice to compose these classes with ordinary `Applicative`s. It is really independent of the `Monad`icness.
langauges can easily produce a factor of 10 in productivity difference. The difference between C and ADA are enough to be statistically significant (even experienced C programmers are more productive in and generate fewer errors in ADA). I know of no empirical studies comparing , say, Haskell and Java, but given my own experience a factor of 10 sounds like an underestimate. "unit tests" are great, but having a language with a high quality type system and something like QuickCheck makes quality software MUCH easier. Tools mater, and languages are the most important tool in software. 
I don't claim to be a Go expert, but I thought the "const" keyword was not a type modifier, but just was a replacement for the use of DEFINE with the C preprocessor. Like, you can't have const fields in structs. Toplevel consts is relatively unimportant--type enforced immutable data structures are a must.
Out of curiosity, in which circumstances is optparse-applicative shorter than cmdargs? From what I can tell, they'd normally work out about the same length, but I don't have that much familiarity with optparse-applicative.
Oops. I think I'm confusing it with the other command line package. Sorry about that! Keep competing! Competition is good!
The other thing I didn't find easy to do in cmdargs was changing the behavior for dealing with multiple args. If I wanted an option that could either be comma separated and/or specified with multiple --option flags I usually had to run a function over the parsed result and split the fields I cared about. I'm doing this now instead: splitOption :: Char -&gt; Mod OptionFields String -&gt; Parser [String] splitOption x flags = concatMap (split x) &lt;$&gt; many (strOption flags)
As bos mentioned, it's hardly used. The only reason I've pulled it in is to temporarily use it as a workaround for holes (Refs [Holes Wiki](http://hackage.haskell.org/trac/ghc/wiki/Holes), [TypeHoles Wiki](http://www.haskell.org/haskellwiki/GHC/TypeHoles), [Mail List](http://www.haskell.org/pipermail/glasgow-haskell-users/2012-January/021691.html)) - which, as I understand, is being worked into GHC 7.8.1
It makes large codebases difficult to manage - the scope of variables is hard to track down as binding sites have no connection to use.
&gt; What it made me wonder, though, is why can't "pure" code just implicitly be in the Identity monad anyway. How would that help things?
you guys need a buzzword list (and a link to Readability ;-} https://news.ycombinator.com/item?id=4939390 https://addons.mozilla.org/en-us/firefox/addon/readability/
Well, I may have inartfully phrased that, but the idea is whether we could change a piece of "pure" code into applicative or monadic just by changing the type signature. Imagine that something like this: foo x y = bar x y was, behind the scenes, translated something more like this: foo x y = run $ pure bar &lt;*&gt; pure x &lt;*&gt; pure y If `run` is `runIdentity`, then this is the same as the conventional pure definition, and you get something like `foo :: T1 -&gt; T2 -&gt; T3`. If OTOH we pick `run = id`, we get `foo :: Applicative f =&gt; T1 -&gt; T2 -&gt; f T3`. So the general idea is if whether by that sort of translation and some sort of defaulting + override mechanism, you could get the same "pure" code to be inferred pure as the default, but still allow you to override that and get the Applicative version.
So you would make all code depend on a function "run" that sets the global monad in which all code is run? With the consequence that the type signature of every function by default depends on the type signature of "run"?
Go has a keyword const, but it's more like #define in C. It's not a type modifier. So no, it doesn't have "const". The atomic mutexes are just locks. There's nothing nice or helpful about that. 
Where are you located?
Conor McBride's language, Frank, effectively does this.
 fact :: Integ -&gt; Integer 
Not strictly Haskell, but reddit-ing it here anyway because some Haskell employers are already pinning Agda and Coq as resume-filtration keywords.
If you're based in the NYC area I'm happy to chat. (I'm involved in 1+ efforts building products using Haskell) Either way, best of luck on building a biz! Building tech + customer relationships + etc is hard work!
Based on what I recall from previous threads about hiring Haskell programmers, one major hurdle is going to be location--the people who'd like to work for you aren't necessarily in one place or near you, but if you're flexible about people working remotely most of the time it should be fine. I note the comments already here seem to support my point, too... The other major hurdle is that the people *already* working with Haskell professionally are likely to be doing so in very attractive careers already--you'd have a hard time luring many of them away, I suspect. You'd be better off looking for people active in the community but not using the language in a professional setting.
I have read all of "Programming in Scala" and been using it occasionally for years. I use Haskell as my primary language, but I learned Scala (as a better Java) before I had written a line of Haskell code. That Haskell has lighter weight syntax seems to me an objective fact, at least for functional programming. The value judgment of if this is a good thing is more complicated. As to if Scala is "ugly" or "wart covered" with regards to syntax seems like a value judgment you could make quickly. I know many people who think Haskell has too many operators. A friend told me that it is "the perl of functional programming," and that therefore he did not want to learn it. This makes sense. Investing in a language has opportunity cost, and quick aesthetic decisions are a programmers best indicator as to if they will enjoy working in a language. You may or may not be right about r/haskell having a high number of derisive comments about other languages. I criticizes languages a lot (including Haskell on occasion). If all languages were created equal, we wouldn't have so many languages. Making arguments comparing languages is a good thing. There are way more skilled python engineers than Haskellers. Same goes with Java, and with Fortran, with C++ and with &lt;insert almost any language here&gt;. That the Haskell community has some elitism is not a sign that they are not nice people, or that they don't like other language communities. It is a sign that people in this community have strong (and honestly usually informed) views in language design. Look, I don't have a game in this fight. Scala does some nice things (like subtyping!) and it seems to me pretty cool to have a language "closed over modules." On the other hand, Haskell's typed segregation of effects and near universal type inference is pretty awesome to. I'm only posting because this discussion seems to me less friendly (subjective opinion here) than what I am used to reading on r/haskell. Saying "hand in the sand" and "zero respect from me" does not exactly help your credibility when you accuse a programming language community of being elitist and condescending. Here are some other things you have said in this discussion: "what the hell", "Oh, really? I call bullshit", "Which is incredibly shitty compared to other languages...", "what planet are you on?" Now, kamatsu and I believe in having opinions--in articulating them, in arguing for them, even in using strong rhetoric to defend them. Just, you know, try to play nice. 
http://www.haskellers.com/ may be worth checking out.
&gt; The "product" symbol enclosed within &lt;&gt; somehow means generalized product Sure, what's wrong with that? &gt; The dollar symbol enclosed within &lt;&gt; means generalized... money? The _application_ symbol `$` enclosed within `&lt;&gt;` means generalised (or "effectful") application. &gt; Just for kicks, how do you view &amp;&amp;&amp;? `f &amp;&amp;&amp; g` is a morphism product, usually written `&lt;f,g&gt;` in categorical literature, but because we don't have that sort of syntax in Haskell, we have to use something else. Out of curiosity, what symbol would you view as more appropriate, here? Full disclaimer: I'm not much a fan of the arrows library, I virtually never use it, mostly because of the unreadable arrow `proc` notation extension that is often associated with it. I do use `first` and `second` and `***` occasionally though.
Yep, that's the same approach I prefer ([see also][1]). A neat thing is that if you make a datatype representing the various operations (free monad or `operational` style), you can also make *that* an instance of your classes, so that if you instantiate your polymorphic functions at that type, you'll get a syntax tree of the code you wrote, which you can do whatever with (such as re-interpret it back into a polymorphic function...). (BTW, isn't this class-based approach basically the same thing as, or at least related to, "finally tagless"? It feels similar, at least.) [1]: http://stackoverflow.com/questions/2383778
Just wanted to say I love your posts. Meta: I can't find a list of the articles. Tried searching your homepage, but couldn't find it there also. Is there such a list?
I actually use an abstraction like that in the pasted example. 
I'm so glad I chose Haskell as my first language. All this packages presented on this blog are friggin dark magic
Using that trick, though, requires enabling the UndecidableInstances extension.
Yep, that seems perfectly reasonable for smaller things, but for bigger command lines does not look like it scales so well.. I may add an applicative interface to cmdargs to capture that. 
Neat, I should add that feature as an attribute - I have a version that does that with tuples so I should just generalise it. 
Well, your original comment made me take pause, since I thought exactly what you're saying here: if Haskell had gone, say, the Clean or Mercury direction with state &amp; IO, it would probably have a very different feeling. Still, I just felt as if the original discussion was Haskell's genesis, rather than what *made* the language. What made Java was Sun throwing money &amp; time at it, whereas I'd agree with what you say &amp; go with Monads as the "made it point" for Haskell. One other comment: Java *the language* probably doesn't offer much new to people who had seen, say, C++, Modula-3, &amp;c. I think what Java offered were interesting applications of previous work on Self &amp; Smalltalk VMs: a decent base VM, decently performant GC, JITing, and non-host-specific VMs, with browser integration &amp; cross-platform GUI creation probably tossed in there as well. It was the platform that was interesting rather than the language, at least from my PoV.
Can't one just use `System.IO.Strict(hGetContents)`?
Those both seem very likely. Right now we're Baltimore based to take advantage of business and scientific mentors at John's Hopkins and the NIH, but it's not out of the question at all to set up a more distributed tech team or to move to a center where hiring would be easier.
That's correct, but not necessarily binding. We're open to a distributed team and may move offices away from our medical mentors if it's needed for building. So we've been in a lot of prototyping stages and most of the code is internal, but I have a strong interest in expanding our open source commitments.
I'll PM you! I'm not in NYC but I'm more than happy to chat and I'd love to hear more about your experiences.
I've looked on there a few times and will probably reach out to some people directly from there. The site doesn't paint a pretty picture for my immediate location... but it's still a useful list of people who have at some point been looking for a Haskell job.
I'd be really interested in your experiences building products with haskell too!
&gt; Sure, what's wrong with that? How is one supposed to know that a symbol surrounded by &lt;&gt; means "generalized"? It's not a universal symbol anywhere in any language. It sure doesn't strike to me as "generalized". &gt; The _application_ symbol `$` enclosed within `&lt;&gt;` means generalised (or "effectful") application. So now you changed generalized to "effectful". &lt;*&gt; means "effectful product"? Anyway, to any newcomer, &lt;$&gt; doesn't exactly convey "generalized application", which doesn't exactly mean map either. &gt; Out of curiosity, what symbol would you view as more appropriate, here? None because you already know my stance on using arbitrary symbols in an API. How am I supposed know that |@| is an applicative functor in Scalaz or even what it does? Nor does any of the functions with crazy symbols in Control.Arrow/Applicative make any sense whatsoever and I don't try to pretend that it *means* something. Which even further enforces my point that it does not produce simple and clear code along with the Haskell wiki which agreed with me. If I have to look it something up and take more than 5 minutes (exaggeration but you get my point) to decipher line of code, it's not simple nor clear even if it only has 2 functions with symbols in their name. It's even worse when you have a huge module with different symbols strewn all over the place due of the use of multiple libraries that use their own DSL. Clever code? Maybe. Readable? I think not. I'm a fan of producing code where others can easily read it no matter what their level is and not because I want to be a dickwad to everyone else just because I feel that using complex concepts is genius or I have the assumption that everyone is a proficient "Haskeller". My code is usually touched upon by many people and I feel that it's a disservice to them if I force them to sludge through a field of unreadable code just to add or refactor something. It just doesn't *feel* good and it demoralizes many people, especially when under *a time limit* where they have to sit and look up many of these functions and piece them together. For the record, that's how I feel when I sludge through legacy asp apps written terribly. You may disagree. Sure, I'm okay with that. To me, you don't care about anyone else that reads your code or you have wild assumptions about the field you're working in with or whatever. So lets just leave it at that?
Is the hacky solution presented in the post the one that the IRC channel came up with? Sounded like that to me after the first reading. If not, the post lacks the clean solution. Also, in what cases would `!y &lt;- hGetContents` not work?
Presumably the hacky solution is from IRC. You can see from Github that he has somewhat refactored it[1] and then it's clear that it's essentially just System.IO.Strict.hGetContents[2]. Using `!` would not work as that only forces the list to weak head normal form, i.e. enough to tell whether it is empty or a cons. [1] https://github.com/bartavelle/language-puppet/commit/1eb0296f22f3c45bd8bc70ddfc7ca4efdde7bf69 [2] http://hackage.haskell.org/packages/archive/strict/0.3.2/doc/html/src/System-IO-Strict.html#hGetContents 
&gt; That the Haskell community has some elitism is not a sign that they are not nice people, or that they don't like other language communities. It is a sign that people in this community have strong (and honestly usually informed) views in language design. You misunderstand my point with this. Think of it from the view of a "programming enthusiast" and not a "Haskeller". Perusing /r/Haskell gives me the sense that people that use Haskell primarily think that other languages or platforms are crap or they don't give them a chance like many comments on this thread. Many of them are not informed about other languages either. Like I recall someone saying that C++ has to tediously capture all of its variables in closures which is absolutely not true or that ruby's gem has a problem with windows when it doesn't for most gems with C extensions. I could point out bos's hatred for Windows or comments in the android thread (iTrash? Really?) or comments in the hash table thread or even some trying to give some weak justification on Haskell's weak points like it doesn't exist. I don't do this for the many languages I know and I fully acknowledge their weaknesses. Have you seen me parade around a language at all? It's just alarming to me since I like Haskell but the community on reddit isn't exactly inviting to others *to me*. &gt; I'm only posting because this discussion seems to me less friendly (subjective opinion here) than what I am used to reading on r/haskell. Saying "hand in the sand" and "zero respect from me" does not exactly help your credibility when you accuse a programming language community of being elitist and condescending. Except that I'm not being elitist. Condescending maybe. I'm just rather frustrated with posters attempting to use another programming language like another programming language when it's rather ridiculous (e.g. super type inference for traits, come on!) I'm also frustrated at some of seemly close minded comments ("I don't wanna learn about Scala because the link doesn't work"). The world isn't only just Haskell! Need I point out to numerous comments that do these things? Why not just enjoy and use the language as a programming language enthusiast and not from a Haskeller sense of view? I realize that I'm on /r/Haskell, but it doesn't mean you can't enjoy another language. &gt; Here are some other things you have said in this discussion: "what the hell", "Oh, really? I call bullshit", "Which is incredibly shitty compared to other languages...", "what planet are you on?" I get rather derisive if I get the runaround or the comment does the same and maybe that's a terrible thing. Nevertheless, you're right. I shouldn't stoop down to other levels. Edit: autocorrect didn't work too well 
That was basically cut out of a very large command line. I'm actually using the arrow interface to have named captures instead of applicative since it's so large, works great. Adding something similar to cmdargs seems like a good idea.
Thanks. What could be the reason that the IRC channel did not direct him to `System.IO.Strict` or to the `deepseq` function? The latter even features this exact problem as its first example [here](http://hackage.haskell.org/packages/archive/deepseq/1.3.0.1/doc/html/Control-DeepSeq.html).
Yeah, don't use lazy IO in a non-lazy way. In fact, if you're using lazy IO and doing finalization, you're almost always doing it wrong.
&gt; So now you changed generalized to "effectful". Well, it's still generalised, but Applicative papers tend to use the term "effectful" to refer to the specific type of generalisation that's happening here. &gt; How is one supposed to know that a symbol surrounded by &lt;&gt; means "generalized"? It's not a universal symbol anywhere in any language. It sure doesn't strike to me as "generalized". Languages have syntactic conventions. In Haskell, which lets you define operators in libraries, some libraries have syntactic conventions too. If you accept that using &lt;S&gt; means a generalized/effectful version of S, then pretty much everything is clear in terms of understanding Applicatives. What sort of notation would you prefer here? Do you object to the _notation_ of applicatives? or the idea of _using applicatives at all_? I'm sure some Haskell users might dislike the notation, but I'm pretty sure any serious Haskell user would raise an eyebrow if you suggest avoiding using them at all. They're pretty foundational to a lot of libraries. 
No idea. If you look at 10:23:22 on 12.12.17 [1] he asked the question and no one knew about or bothered to mention System.IO.Strict. Using a strict ByteString *is* the correct way, nonetheless. [1]http://tunes.org/~nef/logs/haskell/12.12.17 
We offer these classes virtually anywhere a company wants to hire us to come and teach their people. If the location is remote we ask them to help pay for the excess travel expense, but that's about our only serious restriction. A typical location would be a company's headquarters or a major engineering or IT office location. A few people have also asked us to offer the classes in public sites with open enrollment, but this hasn't happened yet.
It doesn't excuse you from having to include the constraint in future type declarations, but it does restrict the types that the data constructor can be used at.
If you can make it up for a NY Haskell Users Group meeting (www.meetup.com/NY-Haskell/) , you'll meet a large community of people from in and around the area who are interested in haskell. There's also a great community in the Philadelphia area who I've met at the fantastic Hackathons hosted at Penn. Typically if you post an announcement to -cafe for positions you'll get a first cut of resumes. It also helps if you maybe have one senior person locked in, because that's harder to come by, but odds are there will be a fair crop of fresh-out-of-college types with FP experience (Penn, but also/moreso CMU, brown, some of the ivies, etc.) who would be open to relocating, especially for a job that involves nifty technology. I've met a bunch of people who seem like they have decent chops and are interested in Haskell employment, but I'm in NY, so...
&gt;there would be no incentive to learn java if the benefits of its network effects hadn't been soundly established Those effects were already established before the language was created. Sun was the enterprise, you bought what they told you to buy. They were selling java, so people bought it. &gt;in 1996 java could have failed That wouldn't have retroactively disincentivized learning it.
&gt; if you're arguing that package-scoped consts don't do anything but provide package-scoped consts...i'm not sure why you are puzzled. once again, Go's creators aren't hiding its design principles I'm not saying they are. I'm saying that Go offers no way to protect against clobbering shared memory. You seemed to argue that it does have such a way, but the "const" keyword you presented is completely different.
That was pretty much what I had in mind, yes. One other possibility would be having a great deal of independence (freelance or contracting work, for instance) where they're using Haskell because they can choose to use whatever they want. It tends to be hard to convince someone like that to go back to being an employee.
Okay. Sorry if i unfairly picked on you.
My impression is that there really aren't many locations with especially large concentrations of Haskellers, and most that do exist are because of specific employers of the sort I doubt you'd be able to reliably lure people away from. Honestly, if you're willing and able to commit to the idea of building a distributed team I suspect you'll have a much easier time finding people. You probably don't want people who live in out-of-the-way locations getting a first impression of "oh well, guess that's not relevant to me" and then ignoring any other job ads from your company. By the way, I apologize if I'm sounding overly negative here. I'm kind of assuming that if you're here asking about this at all, you don't need me selling you on the upsides. :]
In other words, you still have to add the constraint to every function that would have needed it if the data type had no constraint at all, plus a few extra constraints that serve little purpose. For example, if `Data.Set` had an `Ord` constraint baked in, it would force anything creating an empty set to have the constraint as well as all the usual `insert`, `delete`, &amp;c. functions. If `Data.Set` used GADT-style declarations for the `Ord` constraint (like the latter solution in the post), only functions like `fromList` or `singleton` would need a constraint. Incidentally, though, I don't believe the old misfeature style actually restricted what you could apply the *type* constructor do. Given `data Integral a =&gt; Bar a = ...` you can still have `Bar String`, as the type of `undefined` or the elements of an empty list, &amp;c.
&gt; I've thought about this too (probably lots of people have). I always assumed there must be some kind of serious problem with it (like maybe the mapping isn't 1:1?), otherwise it would have been done. Yeah, we're on the same page...
No worries about the negativity. I'm mostly interested in trying to figure out the feasibility of pushing Haskell through as a core language. I believe there are serious upsides... but need to find all options to mitigate the downsides. Needing a distributed team can be a major downside, though without changing our current location that might be in the cards anyway.
Doesn't Agda do this? (I don't code Agda but I've heard from others it does this)
The discussion of type safety that Randal has at the end reveals his ignorance. The exact example he has (dates) can easily be enforced with advanced type systems. Of course tests are good. And testing is one of the best argument *for* Haskell as a language. But, types are really the only way to *prove* properties of your programs.
I can't answer your question, but this was the final push for me to finally sit down and really give happstack a fair shake. I'm kind of excited about it - I already adore stepcut for the few times I've heard from him. If I had to speculate about why happstack isn't very popular today, I think happstack just needs a little more PR. It has some outdated or untrue reputation that it needs to shake off. My personal list of unverified and likely untrue inklings regarding happstack: * Somewhat old and cryptic architecture due to years of cruft * Not a lot of resources / maintainers and moving at a snails pace * Slower performance than the rest, possibly even significantly if old benchmarks are to be believed * Possibly unpredictable performance profile because it uses older methods for (streaming?) IO * Hard to get into due to choice paralysis, all the options and libraries can be a bit overwhelming * Hard to know if you're doing things "right" / using best practices / in a way you aren't going to regret later * Hard to understand code due to heavy usage of monad transformers It's about time I sat down and find out for myself whether any of these silly things are actually true. @stepcut should you come across this: I'm not sure if these are misconceptions held by just me, or if others share the same views. If others did hold it, I think it would help if ... * There were more frequent blog posts (showing there's movement) and/or documentation * Maybe a scaffolding tool to show you an obvious prototypical application * Maybe some posts showing specifically how clckwrks.com is good for happstack; right now it doesn't bolster my confidence in happstack all that much - even less so now that clkcwrks.com is giving 503s - and this may be lost opportunity * Maybe a mirror on github - not to use because I know stepcut prefers darcs - but I think it would help with backing away from the image of being antiquated and stuck using only "old" technologies. It's also more familiar territory for those looking to evaluate how active a project has been. I could just be unique and crazy. So feel free to ignore me and my unfounded / unprompted thoughts.
Out of legitimate curiosity, how do his example? I can't figure out how you can catch incorrect dates at compile time. For example, the following should have a compile error: lastDayOfFebruary = MonthDay February 31 
Wat?
You don't find: join :: Maybe (Maybe a) (&gt;&gt;=) :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b sequence :: [Maybe a] -&gt; Maybe [a] So on and so forth to be very useful functions? Or did you just not think it was important that these functions are shared across all these types, and may as well be duplicated like in other languages?
Marketing-wise, I think the word "pure" is really bad. "*Purely functional*" makes virtually everyone I know not familiar with Haskell think: "No effects at all". It really makes them think that if effects are involved, Haskell is probably not a good fit, and therefore, Haskell couldn't be a good fit for their current program, and the next one, and the next... I think the same feature should have been called "typed effects" which gives a much better idea of what it is about to an outsider. Haskell has *great* model for doing effects, and the "purity" thing is [subjective](http://conal.net/blog/posts/the-c-language-is-purely-functional) and unconvincing.
Is 1958 Lisp really a functional language? It didn't even have correct scoping... Haskell makes old Lisp and all the other languages you mentioned seem very imperative.
Not the only way. Very possibly the nicest way, though.
I don't know if Happstack.Lite existed when you were looking, but I've found it to be a very friendly introduction to Happstack: http://happstack.com/clck/view-page-slug/9/happstack-lite-tutorial
&gt; Take mapM :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]. Is this a pure or impure function? You are right, I don't know for sure, but what I do know for sure is that map :: (a-&gt;b) -&gt; [a] -&gt; [b] is *never* an impure function, which is something that I would no longer know under your scheme. It is true that Monads don't *always* come with any kind of side-effect, but on the other hand pure code *never* comes with side-effects (except of course when using something like unsafePerformIO, or when considering time or space) and if you turn all pure code into Applicatives and Monads then you lose this property. Edit: Made some tweaks.
I think I used the word "really" as an attempt to hedge a little, which is admittedly an odd use of english. The relevant question is this: have you ever formally proved anything about a program without using types? Pencil and paper proofs don't really count (they are not formal), and how often does anyone do those? The answer might be yes, you may have proven an imperative program correct using a set theory based formal reasoning tool (for example). But, this is exceedingly rare. You are right though. My claim was too strong. 
There are specific programming tasks which may have that factor. But overall project development, includes the less interesting tasks, etc. 
I disagree. Snap and Happstack are very similar, both in interface and in philosophy. Yesod, is quiet different from both.
I also used to be very excited by Snap in the early days, but never really appreciated the huge focus on the "pong per second" metric. Also I did never really get the point of the "snapplet" thing, which seems to bring too much convention into what used to be a simple and clean Haskell library. The "snap init" stuff recommended in the doc finished to decide me to look elsewhere (hello happstack). In the end, I still like snap-core, and I hope that it will continue to evolve and offer a good API for handling HTTP requests, routing, consuming content, building response and so on, all in the form of plain old good haskell combinators. This is the strength of our language. 
Although his date example may technically be able to be solved with types, I would suggest that unit testing the dependent relationship would probably be a much more pleasant experience. What irked me was that there was an implicit conclusion drawn from his premise of "type are not necessary and not sufficient" to "it is better not to have types". Of course types are not necessary, and an insufficiently powerful type-system will not be able to prove all the invariants you want to mandate, but they're damn helpful regardless!
Laziness + error + IO is also killer. For example: main = do db &lt;- fromMaybe (error "couldn't read db") . decode &lt;$&gt; readFile "db.json" let newDB = .. do something with db writeFile "db.json" $ encode newDB Guess what happens to the db.json file when there is an error.. 
This can be done in a very simple way without dependent types, but it might not be exactly what you expect. Just make the datatype opaque (don't export constructors from your module) and create a smart constructor that validates the date and returns a maybe: mkMonthDay :: Month -&gt; Day -&gt; Maybe MonthDay Now the type checker won't complain about invalid constructions, but your program is guaranteed to never have an invalid version of `MonthDay` floating around. No magic going on here, but a very powerful idiom.
You have a point. "Pure" is great for irony though, although I fear Wadler has got most of them already. Decades ago, to boot. In addition to typed effects, there is also *managed effects* which would appeal to the MSFT wonks out there. You know who you are ;)
Like kstt said, yesod and happstack have very little in common. Yesod is a "full-fledged framework". Happstack is quite explicitly a toolkit to let you pick and choose what you want and put them together. Snap is very similar to happstack, but has a standard for "putting stuff together": snaplets.
Thanks - good suggestion
Impatient people such as myself can download the video from youtube and use the noise removal filter in [Audacity](http://audacity.sourceforge.net/) to clear up the audio quite substantially. Save the cleaned up audio and replace the old audio stream in the video using ffmpeg, avconv, mencoder or something similar. For example: $ avconv -i cleaned-audio.mp4 -i downloaded-video.mp4 -acodec copy -vcodec copy -map 0:0 -map 1:0 output-video.mp4
Seriously. Every time I have to do some performance work in most other languages, I start swearing because of the lack of criterion.
So, with massive success of lens, will you add documentation to your older packages now? ;-)
Also, I was trying to use this with headphones and you've only got audio in one channel. If I'm reading rycee's command line correctly (never used avconv), it takes care of that in the map commands, but I wanted to make sure you noticed.
Thanks, everyone, for making the lens talk such a great success. And thanks to the denizens of #haskell-lens (ion especially) who chipped in to help us cleanup the audio. Slides, by the way, are available here: http://comonad.com/haskell/Lenses-Folds-and-Traversals-NYC.pdf The next Haskell NY talk will be Thursday, January 17. Ryan Trinkle and Doug Beardsley will speak on "Coding and Reasoning with Purity, Strong Types, and Monads" with special attention to the recent redesign of the Heist template system. RSVP now, so you don't forget! http://www.meetup.com/NY-Haskell/events/96038872/
I don't think I've ever written a project that didn't use this package. Once you learn how transformers work for the first time you find them in disguise everywhere in your previous code. That to me is the litmus test of a truly good abstraction: it naturally arises in people's code even when they are unaware of it.
The -map is only saying what tracks to put the audio/video in in the resulting mp4 file, not duplicating the audio between left and right. In fact, the fact that one audio channel had just the ground loop hum was quite useful in cleaning up the hum. ;) I wish youtube let me upload a new version and keep the old link, comments and tracking information.
I used to use a similar setup to run my personal website, but I always had trouble building the app on my server. Although Haskell applications can have a small memory footprint, GHC itself does not. I'm curious about how the author dealt with compilation.
Happstack is a bit of both now I think. All of the various happstack libraries form a toolkit. ~~happstack-framework~~ `happstack-foundation` combines selected pieces of that into a more structured (though still low-level) framework. We also have clckwrks, which is developing into a fairly high level framework. The internal clckwrks code is very close to having the CMS related functions split into a separate package -- leaving the core clckwrks code which is just a framework for doing some things like authentication plus loading plugins and themes. I will be blogging more about that after the split is complete.
GHC can use a ton of ram during compilation, it's true. One thing you can do in a low ram situation is to separate compilation from linking. By default GHC forks the linker from the compiler process. You can run them separately instead by doing this: # JUST compile cabal build --ghc-options="-c" # Now since compilation is done, this will just link: cabal build
We use upstart as well, but it has given us a lot of grief. I don't really know a good alternative, though. 
&gt;But we also offer a very yesod-like framework named happstack-foundation. Don't be so hard on yourself, happstack-foundation isn't so bad. HSP is certainly evil though. &gt;and then allow them to swap out the pieces later Largely because of this.
Am I really the only one who doesn't have any need for convoluted deployment systems? The only time I've had a need for anything more complex than "scp binary host:bin/ &amp;&amp; ssh host restart_app" is when there's DB migrations involved, so I need to do it for real anyways (pre-update sql changes, update app, post-update changes).
Yeah. Having a compiler installed in production seems like the wrong thing. Having source in production only makes sense to anyone because of the whole ruby/python/perl/lua thing.
Oh cool, a package I already know and understand, surely nothing new here. Learned something new anyways (Functor composition).
Doing this with systemd instead is just about identical. How is logging handled, though? Doesn't it just end up in syslog?
I don't really feel like it is better or worse, pretty much the same. I guess I would rather use HSP than blaze-html directly just because at least I get some editor support that way (syntax highlighting, jump to open/close tag, etc).
A version of it with nearly flawless audio is now available: http://youtu.be/cefnmjtAolY?hd=1 Thanks go to Johan Kiviniemi for his hard work cleaning up the audio stream!
Well, now I know how I'm spending my winter holiday.
* Routing: string based pattern matching, string-named parameters * Configuration: dynamically typed, loaded at runtime * Templating: interpreted/compiled at runtime, splices added by string name at runtime * Forms: field names as strings, disconnect between form and template * Databases: string-based SQL queries * Sessions: text key-value store
 newtype Merge e a = Merge (Compose ((-&gt;) (MergeScope e)) Maybe a) deriving (Functor, Applicative) Can someone explain this to me? Or link me to someone else's explanation/documentation :) Specifically, what does the `(-&gt;)` represent? Doing `:t (-&gt;)` in ghci (quite predictably) results in a parse error. Is this some Haskell extension I've never heard of? Also, looking at the documentation for Compose, it's type is `f (g a) -&gt; Compose f g a`, yet it takes 3 arguments in the example. Is this a case of forgetting parentheses, or is something else happening here?
Not having source on the server only makes sense to anyone because of the whole C/C++/PHP security hole thing.
First the Compose *type* takes three arguments, the type of the value that it contains is a *function* of these three arguments, and that happens to be (leaving out the field name): &gt; newtype Compose f g a = Compose (f(g a)) So in other worse, when Compose is being used in the context of *types* it takes three arguments, and when it is being used as a constructor in the context of *values* it takes a single argument with the type given above. Also, (-&gt;) is the type of functions; you presumably can't to :t (-&gt;) because it is built-in and so has special behavior.
FiOS...
Right. You can layer more expressive static stuff on top of snap -- just as you can strip it away from Happstack, and use Snap's more dynamic approach. In fact, outside of the core server engines, just about *everything* is interchangeable between the two frameworks. Anything it isn't currently so could pretty easily be made so. Both teams are very good about promoting reusable components. I think the issue is more which approach is favored by default, and which components each team devotes energy to developing.
`(-&gt;)` is a type constructor, not a value constructor. It doesn't exist at the term level, and so can't _have_ a type -- it already is a component of a type. On the other hand, we _can_ get its kind. &gt; :k Maybe Maybe :: * -&gt; * &gt; :k (-&gt;) (-&gt;) :: * -&gt; * -&gt; * 
One nice note: with ConstraintKinds, you don't need the class and instances for MonadApp, instead you can just write: type MonadApp m = (MonadConfig m, MonadLog m)
This is meant to reflect the broad trend of the directions the frameworks are going in. And Happstack seems much more focused on web-routes than dir/path routing combinators. I've gone out of my way in [other places](http://stackoverflow.com/questions/5645168/comparing-haskells-snap-and-yesod-web-frameworks/5650715#5650715) to emphasize that all the frameworks can be mixed and matched. But those kinds of comparisons aren't as helpful to newcomers. Sometimes broader generalizations are useful...hence this post.
&gt; Same with happstack. Happstack seems more focused on strongly typed sessions with web-routes. As I mentioned above and sclv also point out, this matrix is about the broad trend of the frameworks, not a statement of what is or isn't possible with them.
My list would be just routing and templating, with routing being a bit of a maybe because there's a chance we may look into more strongly typed routing in the future. We're agnostic to ~~templating~~ forms and databases and I don't consider configuration and sessions to be features that we hang our hat on.
If you go :i Functor you'll see ((-&gt;) a) is a functor. That means that for a function (a-&gt;b) you can fmap something and change b to whatever you want. So for example fmap show (+1) will change Num a =&gt; a -&gt; a to Num a =&gt; a -&gt; String To make it clearer, if you look at the definition of fmap: class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b And substitute in ((-&gt;) r) aka (r -&gt; ?) for f, you get class Functor ((-&gt;) r) where fmap :: (a -&gt; b) -&gt; (r -&gt; a) -&gt; (r -&gt; b)
It's rather a stretch to call the data difference "non-existent". I'd wager the vast majority of Snap users are talking to databases through SQL build with strings and substitution. By contrast, it's a rather recent development that acid-state is no longer called happstack-state and branded as a core component of the happstack project. Similarly, I developed and released typed sessions for Snap (http://hackage.haskell.org/package/snaplet-typed-sessions is a quick but untested translation to snaplets) before the core project even had sessions at all; but the core project chose to support untyped string-based sessions instead, and it turned into a single-person proof of concept, which is now somewhere in the low-priority section of my to-do list to get building with recent Snap versions, because I'm not sure it would ever be used anyway. If you take into account not just the existence of code, but the focus, history, and direction of the various projects, I think Doug's assessment is right on.
&gt; transformers ... in disguise Carry on. 
There's a final revision of the video up here along side of our last user group video: https://vimeo.com/56063074 It's been edited so that you can read the slides without full screen.
Sure! Did you mean agnostic to forms rather than templating, though? While you can certainly use other templating solutions with Snap, there's a strong preference for Heist. Similarly for databases, while there are many snaplets and you could make your own, the preference among snap *users* seems to be postgresql-simple. Same with forms and digestive-functors. As for configuration, I think it bears mentioning anyway. With configurator and snap, you get the benefits of dynamic: you can reload configuration at runtime without restarting the app, and you get the cost: you can make mistakes in your handling of the configuration and it isn't caught at compile-time. I think this point might apply to Yesod too, who seems to favor YAML based configuration. In Happstack there isn't really an obvious preference for anything beyond the haskell records of our libraries, but I personally like to use "dyre" which gives you some type safety without a full application recompilation. You still have to restart the app to load new configuration though, but I imagine we could do something with the "plugins" packages to make that more dynamic? I think stepcut is working on doing that with his new web-plugins package, unless I misremember. Regarding sessions, as the original author of happstack-clientsession I must first admit that it is sorely incomplete, compared to the other frameworks. For example, we don't have any code yet to make sure session lifetime is honored - it just sets a lifetime on the cookie and hopes the client abides. However, one interesting thing we did with it was to use the safecopy package (as used with acid-state) instead of a key/value map, meaning you can use arbitrary Haskell types and get all the type checking that go with them. By comparison, with a key-value store you can make typos in the keys you look up, for example.
http://www.reddit.com/r/haskell/comments/156wj6/haskell_web_framework_comparison_matrix/c7k1ygr
All those papers should have been uploaded on arXiv. We have access a good quality, open access by design, well-known repository for articles, we should use it. In some communities, people just subscribe to the relevant arXiv section, get a daily email with the abstracts of the article submitted, and that is enough to follow what's happening in their domain. Programming language research is far from that goal. If mathematicians can efficiently coordinate to push stuff on arXiv, there is no reason computer scientists couldn't. (Of course having the preprint on your webpage as well is good policy, I'm not asking for a single centralized point of distribution.)
Hard to compare them without expressing opinion :) For me, DSL per se is not a problem. DSL through quasi-quoting is a serious problem. Really, who wants to type thousands of lines of some non-composable code with an unpecified syntax ? Template Haskell when used to express application logic is also problematic in my opinion, see here for a discussion on the matter : http://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell
&gt; you can make mistakes in your handling of the configuration and it isn't caught at compile-time I'm not sure "compile-time" should be treated as a binary attribute. What's really important is finding errors before they can get out to the user. .hs compile time is a great time to do that, but for something like a web app where there is a clearly defined startup process before it can serve users, any errors that occur at that point and cause the service to refuse to start are almost as good as compile errors. In fact it's not hard to say that's a second compile time, provided by the program compiling its configuration into its internal representation, with a straight face. Contrast this with a command-line app like a Haskell implementation of "ls" or something, where there isn't a time to clearly error before "real use"; real use is upon you now.
&gt;I'd wager the vast majority of Snap users are talking to databases through SQL build with strings and substitution Just as I am doing with happstack. I don't see anything at all that favors snaplet-hdbc or snaplet-postgresql-simple vs snaplet-acid-state: http://snapframework.com/snaplets None of them suggest any sort of endorsement or official status, just like happstack. &gt;If you take into account not just the existence of code, but the focus, history, and direction of the various projects, I think Doug's assessment is right on. Which I agreed with, twice. As I said, the issue is simply with the particular examples donri gave that aren't accurate.
One example is that it is very possible to get into a state where your application isn't running, but upstart thinks it is. To recover from this state, you have to either reboot, or run a [ruby script](https://raw.github.com/ion1/workaround-upstart-snafu/master/workaround-upstart-snafu) to start a process with a given process id, just so upstart can kill it. Another thing is missing features, like logging output to a file by default. Debugging in general is really tricky. The whole way it detects if your process was started and what it's process id is, is really weird. Now, this is a hard problem, but solving it is the whole purpose of upstart. As said I don't know of a good alternative, but upstart didn't solve all my daemonizing problems like I had hoped.
Yes, we use Ubuntu. But we're not married to it, so perhaps we should experiment with something else. Another option would be to install systemd on Ubuntu, but googling suggests it's all very experimental. And I don't think it can run without being pid 1, can it?
I think that obscures something true about the Haskell community, though. Almost everybody in any language community will at least say they agree about errors being caught as early as possible. (There are a couple of exceptions where people believe in trying to convince their programs to stumble on regardless of errors, but they don't build very large, successful communities or platforms.) Haskellers really want it to be at the point where the program is compiled, and put a lot of effort into that that other communities do not.
The common reason cited for not having compilers on production systems is that it would make an attacker's life easier once they are able to execute arbitrary commands on a shell. The languages I cited were those responsible for the most public services with security holes due to bad language/library API design that leads to people making the same security relevant mistakes over and over again.
Unfortunately the facts of the matter are that Haskell does not currently have production worthy support for dynamic linking new code while running, so your suggestion is simply not practical given today's Haskell.
I think the point here is that it provides a very powerful low-level abstraction suitable for building a wide range of higher level abstractions. I don't actually use this style much myself. Snaplets have effectively standardized on the route function which in my experience mostly eliminates this problem.
&gt; Did you mean agnostic to forms rather than templating, though? Yep, sorry. I fixed that. I agree that we have a strong preference for Heist. I do not agree that we have a strong preference for databases or forms. &gt; With configurator and snap, you get the benefits of dynamic: you can reload configuration at runtime without restarting the app, and you get the cost: you can make mistakes in your handling of the configuration and it isn't caught at compile-time. Here's where I think type safety just doesn't make any sense to me. Configuration is by definition a runtime thing. Compile-time configuration == your source code. Of course I understand the need to validate your configuration, but we're providing a lower-level abstraction. If you want validation, then build it on top. It's [not very hard](https://github.com/mightybyte/snaplet-postgresql-simple/blob/master/src/Snap/Snaplet/PostgresqlSimple.hs#L192). &gt; Regarding sessions As Chris mentioned below, there's a [typed sessions snaplet](http://hackage.haskell.org/package/snaplet-typed-sessions). And since it's a snaplet you can easily use it instead of the sessions snaplet that ships with Snap. &gt; By comparison, with a key-value store you can make typos in the keys you look up, for example. Or you can just be DRY and avoid this problem that way.
See my comment below... http://www.reddit.com/r/haskell/comments/156wj6/haskell_web_framework_comparison_matrix/c7k86tl
Love this series!
I don't actually believe that configurations need to be statically checked. I was just taking his idea to the logical extreme.
Uploading to arXiv [may conflict](http://www.r6.ca/blog/20110930T012533Z.html) with ACM publishing policy.
http://www.theregister.co.uk/2012/12/21/financial_software_disasters/page2.html As mentioned above, C++, despite its superficial similarities to Java, is infinitely easier than Java to write impenetrable code in. And one language I’ve been warned about, though I’ve never had the opportunity to use it, is Haskell, an offshoot of ML. According to a friend in academia who’s studied it, it’s “the Taliban version of ML,” in which it’s all but impossible to write readable code. My answer: 1.)financial services or Wall Street, USA is a very strange place 2.)Haskell has disadvantages. BUT IT IS PRACTICAL. 3.)obviously, python can be written in 3 or NINETY NINE approaches: a.)functional programming b.)'black magci' c.)glue code aka 'semi black magic' d.)imperative programming style e.)mixtures f.)different versions of python - version 3 versus version 2.7 etc. Please ADD YOUR ANSWER HERE. Please feel free to FLAME THE AUTHOR Dave Mandl, - What Compsi textbooks.... IMHO - in my opinion Haskell has helped me a.)better system integrate to parallel computing - hardware b.)better UNDERSTAND PYTHON and RUBY c.)better REALISTIC AND REAL WORLD PROOFS and TESTS. not the 'fake theoretical math' - and I studied electrical engineering. d.)even better understand ENGINEERING in general e.)REDUCE THE AMAZING CODE BLOAT. f.)avoid the obscure memory torture tests of Java - read 3000 pages and still not be able to productive in writing structured classes??? g.)run code that is MUCH MORE READABLE THAN MY C code and benchmarks somewhat close to C code. THAT IS POWERFUL C code. h.)LLVM and Clang toolsets (sometimes used on BSD) will have problems or warnings or alerts or GLITCHES on Python, Ruby, C++ and forget about JAVA. LLVM toolset with the Haskell compilation and various compiler flags SEEMS TO BE PRETTY ROBUST. i.)have not yet tried WEB or YESOD, but it appears much safer and RELIABLE THAN even the Mozilla stuff....Talk about Obfuscated Mozilla code. PLEASE TALK ABOUT THE REGISTER. I like them. I read it. I think THEY HAVE MADE A MAJOR MISTAKE. WHY is this important???? WHY? 1.)CEO reads it. Tells the CIO to FIRE ALL HASKELL PROGRAMMERS 2.)YOU, the reader are now possibly TALIBAN and possibly MUSLIM in religion?? - see the quote 3.)Some of the TALIBAN can be considered INTERNATIONAL PARIAHS violating INTERNATIONAL LAWS 4.)do a basic statistics survey on Rosetta Code. Haskell is readable and actually LEARNABLE - unlike some parts of JAVA CODE BASE. 5.)language featuresets include type safety, laziness, etc. JAVA includes other features that sometimes lead to MEMORY LEAKS and intermittent faults/hangs/freezes or BAD THINGS ON PRODUCTION SYSTEMS. 6.)Universities and Colleges dominated the Compsci textbooks market. Some or maybe a lot of what COLLEGES TEACH IS WORTHLESS. for example. HOW MANY of YOU HAVE TAKEN CHINESE PHILOSOPHY? Read the Tao Te Ching? Learned Tai Chi or Chi Gung? Thousands of years, including active experiments. insert Tao quote here. movie: Star Wars - May the FORCE be with you. May the WAY be with you? May the FUNCTIONAL be with you? MAY the IMMORTAL WAY or IMMORTAL Statelessness be with you?? IT IS UP TO YOU TO EXPAND THE COMMUNITY and encourage the way of Haskell or the TAO OF HASKELL. NO, this is NOT the Zen of Motorcycle Maintenance, nor practical CAGE FIGHTING mixed martial arts. HASKELL IS PRACTICAL, Robust. But it is not common and often LOOKED DOWN UPON
&gt; "the Taliban version of ML," in which it’s all but impossible to write readable code. Wow, no words. Kind of ironic, given that if any language has a chance of being a partial solution to the "crappy software" mentioned in the rest of this poorly-written article then it's Haskell. 
"The Taliban version of ML." God, I'm crying from laughing so hard. I will say one thing: I have sometimes written functions in Haskell that I do not expect I would be able read afterward, but rationalized it by saying "meh, it does what it's type says it does." [This is hardly an original idea of mine](http://r6.ca/blog/20120708T122219Z.html), of course...
&gt;Getting angry about troll bait.
Looks like an error in the article. It should be `flip runReaderT c $ do ...` 
Yeah, yeah. Whatever you think of this, it is not relevant here, as we are discussing preprints, not final versions.
Much of the article makes complete sense. It isn't just leaving academia for commerce. It's leaving your undergraduate degree for anything that's doing something new, including academic research. In the former the syllabus has been honed for decades or centuries. In the latter no one is sure what is going on, and everything's a mess. Even (or perhaps *especially*) academic code sucks. Regarding the author's comment about Haskell, I've actually found it very hard to write *unreadable* code.
He was just pushing his definition of the "too" smart developer to the extreme. He is mixing the idea of high-quality code that requires some learning curve, with code that unnecessarily uses difficult-to-understand construct (I'm talking about his C++ example). His paragraph about Haskell is so absurd I guess the author is simply trolling.
Thanks a lot. That is really informative.
Reading attributes is done in IO. That should mean it does *not* violate referential transparency, it just means you now have IO at every place in your code you read a configuration value (rather than one IO for an initial load), which is the real trade-off of configurator's support for automatic reloading.
What does this article have to do with Haskell specifically? Also: &gt; in more than 20 years I’ve done work for maybe a dozen companies, *almost all of them in the banking industry* Well, there's your problem.
I am confused. Some example code would be greatly appreciated.
import statements, maybe?
The meaning of containers here is, I assume, derived from the stuff with strictly positive types: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.120.9567
Sounds like Lambda the Ultimate in wiki form. I like it.
Can you give an example of something not built entirely out of sums and products?
Functions are not built out of sums or products.
roconnor is, i believe, working on a theory of traversables as finitary containers (but I don't think that e.g. generics need to be finite, and for that matter, properly defined serialization may not need to be finite either).
But those are usually not finite, except for finite domains.
Traveling today so no example, but I just mean to use "subscribe" to reconfigure some subsystem on the fly. Read your Config as soon as possible in a program so that you'll catch configuration errors early and keep the majority of your program pure.
That technically depends on whether you're external or internal.
His source for the example of unreadable Haskell is Uncyclopedia.
With all due respect... To be honest, if we compare the risks, I'm much less worried about prejudices being spread by that article, and much more worried about representing our community poorly by trying mobilize against the article's "FUD". Even people who hate Haskell as a language grudgingly accept that we've built an extraordinarily helpful language community, characterized by a willingness to be helpful and accepting. To even let people struggle and get frustrated and make incorrect statements at times, and still welcome them to keep trying. While I don't think this author is (yet?) actually even trying to learn Haskell, I do think that overdoing the public rants against his "FUD" can be mistaken for being unwelcoming to those who *are* legitimately struggling with the language. Having a welcoming and friendly community takes effort, it takes restraint... and tossing around the word "FUD" is usually a sign we're getting it wrong.
This library has proven to be very useful in real projects. It helped to get basic HTML quickly ready, and allowed to build high level interface abstraction in the long term. Like dave, I wonder why a writer-like monad has not been used instead of the current implementation. Also, a generalized do notation in GHC would be appreciable here (something like "doWith mappend $ ...").
I know they are all minor items individually, but they all add up to a pretty incomplete API. I'd rather have those little functions as part of the framework I use rather than carting around my own little TheRestOfSnap module into all my projects.
Kewl. I had a very [dumb](https://github.com/chrisdone/css) CSS DSL like this, and Dag was working on a type-safe version. It seems you've done it. Sure he will chime in soon. I like the web site and docs. Seems even non-Haskellers could use it without much trouble! Regarding compiling with Fay for the client side, looking at the code, not yet. Too much type-classes and other stuff Fay doesn't grok. GHCJS on the other hand, could work.
In case anybody else was wondering, the do notation uses the Writer monad, so it is a real monad.
Also look at McBride's "exploring the regular tree types"
* performance, there seems to be no edit on the iPhone app
I guess that's what happens when you take an excerpt from Uncyclopedia as the best example of a programming language.
Hmm, well the patch function assumes the old structure matches the structure of the patch, so I thought it was an equality. At least my initial tests made me think it was true but I haven't proved it.
It looks very nice, but to be honest I think these days I prefer DSLs implemented in QuasiQuoters/TH because they don't have to pollute your namespace and you don't need to import them qualified or hiding a lot to not pollute your namespace. But if ordinary EDSLs are your thing, this seems great.
Just a heads up: the page layout needs to be fixed; the last menu item ("Source") trails over to the next line. [Screenshot](http://i.imgur.com/UFRmX.png).
Are you serious ? Do you really consider TH / QQ as a way to manage namespace issues ?
Couldn't we encode this just using this new feature? data SameType a b where Refl :: SameType a a data NotSameType a b where NotSame :: (Equals a b) ~ False -&gt; NotSameType a b or equivalently newtype NotSameType a b = NotSame (SameType (Equals a b) False) we could wrap this up in a typeclass class (Equals a b ~ False) =&gt; a /= b where notSame :: NotSameType a b instance (Equals a b ~ False) =&gt; a /= b where notSame = NotSame letting you define potentially useful functions like const' :: (a /= b) =&gt; a -&gt; b -&gt; a const' = const I have not checked anyof this with this new branch of head to see if it works, but I don't understand why it would not. Would this do what you want? EDIT: as I think more about this now I wonder: is it possible to define the following function? ineqSym :: NotSameType a b -&gt; NotSameType b a since it is possible to define the function eqSym :: SameType a b -&gt; SameType b a we have the perhaps even more interesting question, is it possible to define univSym :: SameType (Equals a b) t -&gt; SameType (Equals b a) t my speculation is the answer to these are all "no", with a possible solution if you abuse the typeclass system (essentially use Oleg's old type eq hack, only have the instance method return evidence of the equality/inequality).
Thanks, I'll try to fix this. It seems the web font from typekit didn't load and the fallback is a bit bigger so it overflows. edit: should be fixed
This is so cool. Are you planning to write (or have already written) a paper laying out the details? This making it into GHC 7.8 would be amazingly cool. BTW, I still use functional dependencies for another reason beyond the equality stuff--I can encode functions as being isomorphisms, and use this to lead the type inferrer. I still think you need fundeps if you want to do something like implement usable session types. Not having overlapping instance with TFs is a big problem though...I have been recently looking at encoding module systems/object systems in Haskell using new features (like first class identifiers, aka promoted strings), but have been restricted to using fundeps for anything interesting. 
In my experience there is always an important difference between definitional equality, as understood by the type system (that is the one used to actually make sense of `Refl a a` or `Equals a a = True`), and the encoding you use afterwards to reflect it into the world of terms or type-classes, that are more flexible to manipulate on the user-side. You generally end up in a situation where you want "the system" to understand that some (in)equality holds, but it uses its definitional/hardcoded notion of equality reasoning and entirely ignores your hard-earned (in)equality witness, which was a lot of a pain for little use. So I think those things are best understood as a change to the type system directly, and later, when we have more experience, eventually understood as expressible or dispensable, not the other way around. One user-level encoding of equality that does not seem to make hidden uses of the definitional equality is the Leibniz principle: data Equals (a b : k) = Conv (forall (f : k -&gt; *). f a -&gt; f b) Unfortunately, it is not expressive enough because of Haskell's restriction to type constructor polymorphism at higher kinds, instead of general type-level functions (eg. `Equals (a, b) (c, d) -&gt; Equals (a, c)` is not derivable). I do not expect any of Richard's present issues (which are precisely about understanding the status of type-level functions in Haskell) to be solvable by such considerations. This suggests, however, than "let's encode this at the user level" approaches *might* work in a very robust and well-structured type theory, but there are a lot of practical or historic reasons why GHC surface type system (not talking about System FC here) is not quite ready for this.
I mostly agree with you, and think some sort of "definitional" inequality may need to be included in the language. But, I'm not sure. `Equals (a, b) (c, d) -&gt; Equals (a, c)` is derivable in the presence of GADTs (since Leibniz equality implies definitional equality). GHC's surface type system has gotten pretty powerful. 
`mconcat [...]` or `&lt;&gt;` isn't good enough? What alternative would you suggest?
I'm coming late to the party, but why would that be "unacceptable"? In a language with implicit subtyping I expect implicit resolution to be done modulo subtyping. This means that you have to work harder to ensure that resolution is coherent (but there already are restrictions to Haskell instance declarations mechanism to avoid dangerous overlappings), but that doesn't seem unthinkable to me, and in fact it seems The Right Way to do it. &gt; how on earth is the compiler going to guess which super type I wanted to use for custom traits You can either: - request all the possible solutions to have a coherent semantics - specify a notion of "most precise" type and specify that you pick the most precise existing instances; in the general case you can have several incomparable "most precise" choices, and then you should impose coherence between those choices, or fail saying that the implicit resolution would be ambiguous 
&gt; mconcat [...] or &lt;&gt; isn't good enough? Yes.
Why would you want this quasi-quoter if I may ask? I don't directly see the use for it, but might consider adding one. Combining with existing stylesheets maybe?
Just having the odd browse through the source code of this - is Clay intentionally trying to have light dependencies? I ask, because there is stuff like `mempty = error "Selector is a semigroup"`, while the `semigroups` package provides a `Semigroup` class. Also, that same file defines `newtype Fix f = In { out :: f (Fix f) }`, which is in `recursion-schemes`. It's a minor pain point for me when people reinvent the wheel, as they usually have wheels that always come out looking ever so slightly different.
This approach is more or less what I optimized for. You might even have to hide some stuff from the `Prelude` sometimes (`div`, `(**)` and `span`). But there are always alternatives available to do the same, like `"div"`, `deep` and `"span"`. Mixing Clay stylesheets with a lot of other Haskell code in one module might not be the best of ideas.
Yes, I understand this sentiment, but it is intentionally trying to have light dependencies. I'm not planning on including extra dependencies just to avoid defining the `Fix` combinator (used internally only) and one partial instance. I do consider making `Selector` into being a real `Monoid` if possible, or ditching the entire instance and provide my own appending operator, though.
It's amazing how close we can get to real dependent types without actually having them.
This is a very exciting project, and the code for the compiler is meticulously clean (compared to GHC it's downright _beautiful_). If you're looking for a compiler project to hack on, I'm sure Ben would appreciate the help!
Sure, but we are going to get a lot of overlap.
It'd be cool to see an example on how to use Clay with Hakyll.
In your install instructions you should link to the Haskell Platform in case your audience isn't "in the know".
I vote for a special list notation, that would allow to write lists using the layout, with one element per line. Let's pretend this notation is `:~`: mdo = foldr1 mappend main = mdo $ :~ foo bar It has the advantage, in my opinion, of being general enough to be useful for various purposes, yet limiting the magic to the bare minimum (just a special list notation). EDIT : Peaker courteousely remarked that this can also be written more compactly using mconcat main = mconcat $ :~ foo bar 
About as “unnecessary” as the &lt;&gt; notation? =)
It's okay. The community needs to learn to use name spaces properly anyway. Just use the names that make sense for your library.
`(&lt;&gt;)` doesn't give you more power than you need. `Monad` does.
The point is really that do notation is just a lot better to read, write and edit. If you can't see that as plain as day, there's not much left to discuss. I don't have to maintain your code, you don't have to maintain mine.
It does not require the list element type to be a Monad.
Because using a monad is abusing a monad.
 mconcat [ one , two , three ] vs: do one two three The `do` block is lighter, and this encourages people to make superfluous `Monad`s when they just want a `Monoid`. I think either of a special syntax for multi-line lists or `Monoid`s is OK. Though perhaps the former would be slightly more generally useful.
Ahem, Clean pretty much meets all your bullet points... Although I do like Clean I don't expect it will conquer the world any time soon.
This library is probably a great solution for Haskell programmers, but likely a horrible idea for pure designers that you are now asking to learn Haskell or at least a whole bunch of operators that map to the operators they already know. There is already a CSS quasi-quoter (lucius) http://www.yesodweb.com/book/shakespearean-templates
I should have said "real System F+extensions" (System F doesn't have type classes and type operators either) Aren't type families just open functions with pattern matching and equality, though?
Type level equality! Finally! This is great news.
The problem discussed is a specific case of implicit resolution where you have some term `t` (in the original example `Some(1)`), and you are trying to resolve an implicit `Applicative` for that term. This means deciding a type `T[X]` that is a correct type for `t`, such that you can produce an implicit `Applicative[T]`. (In the general case the resolution may not be driven by a term in such a manner, but by the return type of the expression, or something else entirely. But let's discuss this simpler case to get the idea through.) In some languages (generally core formal languages) there is a unique type `A` such that `t` has type `A`. Either `A` is of the form `T[X]` and resolution just looks for `Applicative[T]`, or it fails. In most languages there is a set of different types `A` that may be used to type `t`. In languages using the ML type system (Haskell included), there is a notion of "principal type" saying that there is a most general type for `t`, a polymorphic type of the form `forall a b c . A(a,b,c)`, and all other possible types for `t` are of the form `forall ... A(foo,bar,baz)`, where `foo,bar,baz` are specialized instances of `a`,`b` and `c`. This is the "good case" in a language with implicit instantiation of polymorphic types: at best there is a most general case and you can use it for resolution -- or look for which of its instances can be resolved, and fail if different instances are resolved in incompatible ways. In practice ML languages or Haskell have overloaded the original ML type system which various stuff that makes this harder. If there is a form of subtyping (for example in OCaml), it is generally explicit (mixing implicit polymorphism and implicit subtyping is too hard), so it does not change the set of possible types for a term `t`; if `t` has type `Nat` with `Nat : Int`, then you need an explicit cast `(t :&gt; Int)` to see it as an `Int`, so the term is different. In most object-oriented programming languages, subtyping is implicit but polymorphism is explicit (at least the generalization, not the application, but this is enough to make it tractable). This means that you do not have to "guess" how much generalized or specialized the term is, the set of possible types for `t` has less different results due to polymorphism, but there is a number of different choices due to subtyping. For example if `1 : Nat` holds and `Nat : Int`, then `Some(1)` may be either `Some[Nat]`, `Some[Int]`, `Some[Any]`, `Option[Nat]`, `Option[Int]`, `Option[Any]` or `Any` (simplifying the class hierarchy here). There is clearly a notion of "most precise type" in this case, `Some[Nat]` (the one that required no implicit casts). However, if you cannot resolve an implicit at this type, you have to look in rest of the set of possible types for resolution to succeed. You may find one for `Some[Int]` and one for `Option[Nat]`, and then you're in trouble because they're incomparable (none more precise than the other); you could set rules to say that you give higher priority to the type operator (prefer `Some[Int]`), or (more reasonable) check that the two resolutions actually give an implicit that behaves in the same way. The reasoning behind this idea is the following: the purpose of subtyping is that when `B` extends `A`, then objects at type `B` can be used *just as if* they were of type `A`. If in some place of the code resolution succeeds if you put an `A`, but fails if you use a term `t` of type `B`, you have broken the illusion of substituability that is so dear to Barbara Liskov. This tells you that, after maybe some phase of "more precise" research at type `B`, you have to consider `t` as if it was an `A`. I know that Scala already does that to some extent (when search for some implicits, it also looks in some superclasses), but apparently it does not always -- or cannot -- which is the stem of this discussion. &gt; Er... Not sure what you even mean here or how the compiler would decide on "coherent semantics". That's a lot of hand waving there. If you have several possibilities, either you have priority rules to tell you which one to select, or you should check (somehow) that all possibilities have the same semantics, or reject with an ambiguity. For example in Haskell, you may resolve the type class `Eq Int` in two different ways: 1. Use the `Eq` instance of the `Int` type 2. Use the `Ord` instance of the `Int` type, and project its subclass `Eq` The type-checker knows that both solutions will return a dictionary with the exact same semantics: coherence is guaranteed.
Very nice. A few weeks ago I started thinking about a "typecase" extension which would handle similar needs, and here it is.
Silly question, but what do people use `Control.Applicative.Lift` for? I guess the whole transformers package could do with a metric ton of examples..
It got committed to GHC HEAD a day or two ago. So yes, it will be in 7.8.
We have type-dependent types, which is a funny way to say type function (a value function would be a value-dependent value), and is in the limit of what System F_omega can do and does. A weak form of dependent typing would be having type-dependent kinds, kind-dependent "sorts", (Set n)-dependent (Set n+1)s, but no value-dependent types. I wouldn't call classes in Java "a weak form of higher order functions" because they can be used to replace HOFs in some places. &gt;First, type families aren't "just" something, Aren't type families *not just* open functions with pattern matching and equality, though? &gt;since they break some nice properties of the types (GADTs do that too), namely that any given type constructor is a functor (not necessarily covariant, though). I think you also lose parametricity on kinds.
DataKinds lets you define datakinds (well, lift datatypes to datakinds actually), TypeFamilies lets you define open type functions with pattern matching and equality. Nothing of this lets you define a kind indexed by a type, "dependent typing" is just the wrong term for it. No one is saying that it sucks because of that, either.
`Bool :: BOX` here, not `Bool :: *`. Not a dependent type. EDIT: to expand, what gives you more power is being able to pattern match on types (which is also what breaks parametricity), not dependence, because there is none (in the usual sense)
This depends on technicalities of definitions, but for practical purposes, `Test` is dependent on values of type `Bool`.
Well, you wouldn't call this function plus :: Nat -&gt; Nat -&gt; Nat plus Z n = n plus (S m) n = S (plus m n) a dependent function, but it depends on values of type `Nat`. Or it might be a dependent value of type `Nat` that depends on a dependent value of type `Nat` which also depends on values of type `Nat`. But that's being pedant, dependence usually means having a `Set n` that depends on `Set n`-*small* values. &gt;`Test` is dependent on values of ~~type~~kind `Bool`. It *is* important that `Bool` here is a kind and not a type.
Agreed. Besides the silly jab at Haskell, the article makes some good points. Which is unusual for this publication, as cmccann points out. So we might as well cut them some slack in this case.
The point of my original post is that we are close to dependent types *without* actually having real dependent types.
But it has nothing to do with dependent types! Pulling out one or two tricks that seem to use dependent types, with syntax similar to languages with real dependent types, is not being close to dependent types. (and it's not "a weak form of dependent types" by any means) These extensions certainly have their own awesome applications, the mere fact of them possibly breaking parametricity on kinds should tell you that. You can probably do even more awesome things if you stop thinking of them as just a way to fake dependent types.
Yeah, I simply forgot about this one, I don't know the APIs by heart unfortunately. I'll edit for clarification.
Yes, but we all have to maintain and learn to read any additions to the language syntax. I think the bar should be a little bit higher for any proposed syntax additions.
That gets into operator precedence problems though, depending on the actual expressions used.
I agree 100%. This is an amazing contribution and I hope it will be archived somewhere and perhaps linked from the various pages in the community listing tutorials and introduction material.
"I like the idea that there's no such thing as a good programmer, just a programmer who follows good practices. " That thought must be very reassuring for someone who isn't good at programming, too bad it isn't so.
Great article. I'd like to add some more examples of best practices you learn from Haskell: * The unforgiving type system teaches you to fix bugs early and often * Haskell promotes better concurrency idioms than most languages (the exception being message passing and Erlang, which I've heard people argue is better than STM). * The library ecosystem discourages the Pythonic culture of stringly typed programming and opaque dictionaries * Purity and referential transparency teach you to think about what code *is* as opposed to what it *does*
Then just ask! `#haskell` on `irc` is probably the most casual and welcoming environment to ask these kinds of questions in. `Stack Overflow` is okay, although less helpful and sometimes anal about what questions are a "good fit". Here is fine, too, and I always up-vote beginner questions.
&gt;I like the idea that there's no such thing as a good programmer, just a programmer who follows good practices. If you expand on this idea then you will uncover much more interesting things. Like for example following good practices requires a constant push. In most cases that push is gone after initial burst of enthusiasm. There's only one way to ensure programmers perform on the top of their abilities. It's to put them in the environment that constantly pushes them. I found haskell to be such an environment. GHC compiler is like a military Sergeant that barks at you every time you do something wrong. It forces you to follow all the best practices, because otherwise programming in haskell becomes increasingly hard and painful. Sometimes i wish haskell would force me to do other good things in my life. Like for example start doing yoga or aikido again :) 
I think I prefer the original, even if it is a typo
Well, there's a difference between a good programmer and a good software engineer, just like there's a difference between a good bricklayer and a good architect, I guess. Not a perfect analogy, but being good at using the tool is just a matter of lots of practice and following the right procedures. Being good at designing a whole project definitely takes more than best practices.
&gt; it's really just getting closer to real System **Fomega** FTFY
* TypeFamilies (minus associated types and the openness aspect) are just type level functions. * DataKinds are just type level data types. Both of these are things belonging to Fomega. No dependent types anywhere to be found in these two. Seriously, you should take a look at [Omega](http://code.google.com/p/omega/) and the papers by [Tim Sheard](http://web.cecs.pdx.edu/~sheard/). Indeed, Tim explicitly states on the GoogleCode page that "proper dependent types (will never happen)".
The mistake you're making here is that you're looking at some implementation of the calculus of constructions (or something more powerful), such as Coq and Agda, and then saying "hey, we can do some of that". The reason this is a mistake is because CoC extends Fomega by adding dependent types. Whereas the examples you're pulling out are things that belong properly to Fomega. That's like saying "hey, we can do something like the real numbers" when all you have are the integers. Sure, the integers encompass more than the natural numbers, but that doesn't mean they encompass as much as the reals do. The calculus of constructions contains three major extensions over the simply typed lambda calculus: polymorphism, operators, and dependency. Haskell98 is already halfway along the track towards having full polymorphism. H98 is restricted to rank-1 polymorphism, but with RankNTypes we get all the way to System F. H98 doesn't have much in the way of operators (except what can be encoded by H98 type classes), but TypeFamilies and DataKinds gives that to us. Using all three of these we're getting pretty close to System Fomega--- though we still lack proper lambdas at the type level (we need to name all our functions), the syntax is especially baroque, and we have a lot of weird extra stuff (type classes, the fact that we can't define data kinds without having a related data type,...). But still, no dependent types. We start heading the direction of dependent types once we enable GADTs. Have you ever noticed how GADTs cause all sorts of mayhem for type inference? (e.g., [let should not be generalized](http://research.microsoft.com/en-us/um/people/simonpj/papers/constraints/let-gen.pdf)) Yeah, that's dependent types. Certainly, modern GHC Haskell can get pretty darn close to full dependent types as demonstrated by the links I've mentioned in my other replies on this thread. But allowing ordered overlapping type families isn't a part of that. *Edit:* If you want to see real dependent types ---that is, dependent types without any of the other stuff in CoC--- then take a look at LF or Twelf. Twelf gives you a bit extra because it has some metatheory for checking completeness of pattern matching, etc. After you've played around with LF for a while, then come back and think about all this again.
i'm the "maintainer" for the portaudio bindings. unfortunately, the kinda suck. there's some on-and-off-again work being done by myself and a few others on some new bindings found here: https://github.com/sw17ch/portaudio
While you paint that as a bad thing, it's actually a fantastic design choice for Erlang. In Erlang you expect every process to be able to die and thus to implement rebuildable caching and fault tolerant messaging. It's an utter pain in the ass, but it makes you build things that are utterly unstoppable. Launch a core service with Erlang under daemontools with good error logging and you never have to think about it again.
I'm not trying to paint it as a bad thing! There are certainly downsides to the approach, and I was being a bit tongue in cheek, but it has clear benefits. The Adams quote has some truth to it applied to Haskell--when code that should never go wrong goes wrong anyway (say, faulty RAM mangles a few bytes inside a thunk somewhere) it tends to fail completely (like GHC did under high memory use conditions before I replaced that RAM :]). Erlang has almost the polar opposite of the preferred approach in Haskell, taken to a similar extreme and reaping benefits for doing so. The common ground is what both languages discourage--merely sweeping possible error conditions under the rug.
&gt; Yes there is. You could check, for example, that they have been defined in exactly the same way.... Except this isn't Haskell. This is Scala. Scala doesn't have global type classes and Scala inherits Java's type system by default. Not hard to understand. Sure you can say, "well, this is how it would be done in Haskell using Haskell concepts" but don't you see how ridiculous it is to ask of Scala to do the same when the type systems are set out to do different things? To fix that, I already mentioned that you have to have an implicit conversion for the same type that you would have to import or explicitly set the type, which doesn't take any effort for both in most cases. Which leads us to... &gt; As I tried to explain, having to annotate `t : B` at type `A` explicitly breaks the idea that `B` is implicitly substituable with `A`. No, it doesn't. See C# and the Foo/Bar/Baz example on why you should explicitly annotate in corner cases. In my examples, subtypes are implicitly substitutable but there are multiple types in which there is no way for the compiler to correctly choose. But yes, to make everything clearer, maybe you can roll some examples in idiomatic Scala in what you think Scala should do? That would be awesome :-) Also, as I have mentioned in another comment, if you know of a language that combines variance and a type system that correctly infers 100% of the time then it would be fantastic if you can mention those also because I can't seem to find any, let alone any with Scala's complex type system. 
The point is, we are *close* to having dependent types. A dependently typed function (pseudo dependently typed functional language here) g : \Pi (x : T), f(x) can be constructed using lifted types and singletons by breaking it into two pieces. A single dependently typed function toSing: \Pi (x : T), Sing {t} x and a non dependently typed function, but one that uses operators and equality proofs (what is universally called "faking it") g' : Sing T x -&gt; f (x) Haskell now allows you to encode the second one (and the language for type level computation just got a whole lot better with this extension). Obviously, you can not write down the first one...but that is necessary to preserve the phase distinction! We get modularity, type erasure at compilation, and parametricity all from **not having* the `toSing` function. We can write essentially toSingE : t -&gt; Exists x, Sing {t} x toSingE = MkExists . toSing of-course, we can't write this implementation (since we can't write toSing) but a function that does the same thing as this is basically possible in Haskell *now*. So, in a sort of precise sense, we are *close* to dependent types with Haskell. Improved handling for type pattern matching is important to making this usable. So, I think this is an appropriate place to say this. Adding dependent types to Haskell has to happen in a way that preserves the phase distinction. There are a couple of features I see missing that would imo make Haskell dependently typed * Path dependency. That is, the ability to project the type out of an existential. Bob Harper says this is the "right way" to do Sigma types. But, having path dependency does not imply or require full dependent types. Scala can do this. SPJ has proposed adding it to a future Haskell like language as a way of allowing the writing of modules as just data. What is more, since Haskell is pure, we could easily due this so they work like Ocaml style "applicative functors" by treating two expressions as equal if they are syntactically the same. Having a full blown beta reduction support for path dependency is also possible (I think) but more difficult. * Pi kinds. We should have real dependency from types to kinds. My impression is everyone is in agreement about this, so I think it will come some day. I don't think tailcalled deserves the negative responses. Haskell fakes it darn well already.
are not open type functions sufficient to encode equality? Both in the way this extension makes type level equality computable (akin to what Oleg did with fundeps) but also in the Leibniz equality becomes sufficient to prove everything you can show with definitional equality (at least I think it does). 
If you can get a university assignment (or something small but real world that you **have** to do) that is allowed to be done in haskell that might be a fun way to get into it.
&gt; Not hard to understand. I'm tired of your rudeness. You've been obnoxious during all the thread (not especially with me, in fact you've been on average more pleasing with me than you were during the rest of the discussion). You complain about a certain elitism on `/r/haskell`, on which I can't particularly comment, but one thing I'm quite sure is that the conversation standards, and the way people are polite with each other (even when disagreeing, or when discussing with beginners, which isn't the case here), are much better than what you're doing here. Please tone down the aggressiveness. &gt; Scala doesn't have global type classes Of course, Scala implicits can be locally defined, and that makes Haskell's argument for coherence unapplicable. But that doesn't mean coherence is unreachable and shouldn't be thought about. &gt; Sure you can say, "well, this is how it would be done in Haskell using Haskell concepts" but don't you see how ridiculous it is to ask of Scala to do the same when the type systems are set out to do different things? Implicits and subtyping are concepts in the Scala language, and I want them to work beautifully together. It may not be the case in Scala today (apparently you seem to think this is unthinkable), then I'm interested in knowing how they can be made to work in the next object-oriented language with a powerful type system and implicits. In any case, I ask this question because I'm interested in the situation of implicits in languages with implicit subtyping. I haven't commented on that previously because that was not the topic I was interested in, but as you're mentioning it, let me say that I don't think "you're talking about Haskell, Scala is a different language" holds as an argument here. In type systems, there are certainly some things that are specific to one language and don't make sense in another, but there are also hard absolutes that are simply more powerful when they exist than when they don't. Parametric polymorphism is not a question of "what you're set out to do", it should be present in any typed language because it's general and powerful and there is no way around it. Sadly for your argument, I believe a lot of the feature Haskell has in its type system, and *in particular* the handling of higher-kinded types and kind polymorphism that were mentioned in this thread, are also in this category: every typed language should have them because they're meaningful in a universal sense in any type system. I also think Scala's type system has interesting things that Haskell doesn't (I'm thinking of path dependency, that I haven't seen mentioned anywhere here, to my surprise), and that may turn out to be fundamental as well. If you discussed them in this thread or elsewhere I don't think you would see people saying "oh but this is ridiculous, you shouldn't ask Haskell to do the same as Scala". &gt; But yes, to make everything clearer, maybe you can roll some examples in idiomatic Scala in what you think Scala should do? Well the example that started the discussion seems reasonable. abstract class Default[A] { def default : A } def get_default[A](x : A)(implicit d : Default[A]) : A = d.default implicit object OptionDefault[A] : Default[Option[A]] = new Default[Option[A]] { def default = None } scala&gt; get_default(Some(1)) error: could not find implicit value for parameter d: Default[Some[Int]] get_default(Some(1)) In this example, given that `Option[Int]` is a valid type for `Some(1)` (without any form of annotation), I would like the polymorphic implicit at type `[A] =&gt; Option[A]` to be found. If polymorphism makes the problem harder, I can craft the same example with a monomorphic implicit object `Default[Option[Int]]`. &gt; In my examples, subtypes are implicitly substitutable but there are multiple types in which there is no way for the compiler to correctly choose. Then the compiler should fail with an error.
Ideally each of these would be right there in the package's documentation.
Vaguely related, but interesting: [Crash-Only Software](http://radlab.cs.berkeley.edu/people/fox/static/pubs/pdf/c22.pdf) (PDF): &gt; Crash-only programs crash safely and recover quickly. There is only one way to stop such software—by crashing it—and only one way to bring it up—by initiating recovery. Crash-only systems are built from crash-only components, and the use of transparent component-level retries hides intra-system component crashes from end users. In this paper we advocate a crash-only design for Internet systems, showing that it can lead to more reliable, predictable code and faster, more effective recovery. We present ideas on how to build such crash-only Internet services, taking successful techniques to their logical extreme.
Agreed, many thanks to the author for writing this series.
Well, Cloud Haskell seems to provide just the primitives monitorProcess :: ProcessId → ProcessId → MonitorAction → ProcessM () linkProcess :: ProcessId → ProcessM () But doesn't Erlang provide a little bit more? PS: The blog-series you linked doesn't seem to address fault tolerance
Well, that would solve part of the problem but I also like about this series that it highlights some of the more useful and mature libraries people might not know about.
Is there really a problem with this, though? I don't understand why this is problematic.
 data TwoWayCat a b = TwoWayCat (a -&gt; b) (b -&gt; a) An arrow in each direction doesn't imply isomorphism.
get a nagging wife.
Don't forget to learn how to use software transactional memory (i.e. STM, provided by the `stm` package). It's really simple and easy to use.
Chan/TChan are all I use for concurrency in Haskell, too :)
Depends on whether the library being used is thread-safe. I don't really care, as long as I can play two sounds at once.
I think adding an actual Pi-type would be easier and cleaner than automating the singleton construction. There's no point adding Pi the way SHE does if you're actually in a position to upgrade the type system. The phase distinction is important. I propose to make phase explicit, then abstract the core language rules over it. Quantification-over-static is what's currently forall; quantification-over-dynamic is what's currently -&gt;. By combining the structure that's common to all phases, we get fewer rules, but we still make clear which phase we're working at. It's then not such a stretch to add a new "shared" phase for stuff that can be used both statically and dynamically: quantification-over-shared is exactly Pi. (A full spectrum dependently typed language is one for which everything dynamic is shared, so Pi and -&gt; coincide.) Pi is not parametric, but forall is. So, the phase distinction can live on without disrupting the structural similarity between the phases, and we get parametricity wherever phase interplay enforces information-hiding.
Those are the core primitives that Erlang's fault tolerance are built upon. From there you build supervision trees to get the next layer of functionality. In Erlang this is all built in to the OTP library, which I don't think exists in any form on Cloud Haskell yet. There's also the Communicating Haskell Processes library which gets some amount of this fault tolerance going, I believe.
&gt; Sometimes i wish haskell would force me to do other good things in my life. Like for example start doing yoga or aikido again :) Incidentally, military style fitness programmes can be very good for just that (the barking, but not when you're doing something wrong, just to keep encouraging you). Your mileage may vary.
Oops, you're right. Thanks!
You're welcome!
When there is no value to bind, the essence of the monad is useless, and the interface looks ugly. Just like a function in the IO monad without performing IO. It works, but it is annoying.
This is how I got into it. For a graduate course on functional programming (well, mostly just Haskell), I had to enhance a parser combinator library to have lookahead. I knew about parsing, and I knew a bit about Haskell, but not enough of it to build the library from scratch. But even trying to clean up the code helped me to understand both the library and the language it was written in.
Are you aware of http://hackage.haskell.org/package/MonadRandom ?
&gt; values of each I see what you did there.
It's not only GHC's persistence, either! There always seems to be another crazy concept out there that I have absolutely no idea how to use. Examples: * Comonads * Free Monads * Lenses * Tying the Knot * Tardis * Monad Transformers ...etc. There's always more to learn! Once I'm familiar with a concept, there's always another to learn about, which makes Haskell an amazing programming language to gain experience from.
Yes, I'm aware of that package. I thought my library as an improvement. First of all, my package provides better complexity in the case of fromFreqs / fromList. Also, I would think mine is more intuitive, separating creation and evaluation of random variables, but that's very subjective!
Yeah, I've used Daemontools for that. Launching an OTP process into Daemontools is frighteningly bombproof. Literally, if you launch a buggy process it can be hard to stop it!
This paper from PADL on the intensional transformation also looks really interesting: http://www.softlab.ntua.gr/~gfour/dftoic/dftoic.pdf
It makes the types and code less descriptive. It's nice to see monoid concat when no monad power is involved. 
Can I make a suggestion? It would be awesome if you can update that post with links to all of the others. I think this will be a good thing to refer people to, and having something like a table of contents will be invaluable once http://ocharles.org.uk/blog/ has a lot of newer posts on it. I considered putting the list on the wiki, but it feels like directly linking to your blog makes authorship clearer and is a better experience.
Thank you! I really enjoyed these!
I like how straightforward this is. Definitely post it to hackage! It's easy to do. First get an account (instructions here: http://hackage.haskell.org/packages/accounts.html) then run "cabal sdist" to create a gzip of your package and upload it here: http://hackage.haskell.org/packages/upload.html 
As I understand it, the main issue being either one process can grab all your resources if you're not careful, or on the other hand, one there can be a ffi binding to some library that just stomps all over your memory with wild abandon.
There's no explicit saving graces for those, but the OTP failsafe would be to have the failures of the resource starved processes kill their immediate supervisors which might subsequently kill the resource hog before restarting that entire subsystem. More likely the whole thing will go down first though and then you'll (maybe) get a daemontools restart.
No, thank you! You must have put a lot of work into this and we all appreciate it!
Even better, use `cabal upload mypackage-x.y.z.tar.gz`.
Which means GHC's surface type system isn't full System FComega+whatever. But it's still not a robot goose, it's a robot duck. 
I pondered whether this could be achieved with the type-level strings. I don't have a GHC 6.7+ installed anywhere to try this out presently. (Downloading it on my VM presently.) I've never played with these type-level `Nat` and `Symbol` so I may have some syntax or semantics messed up. Think I would would perhaps need to use the singleton [types described here.](http://hackage.haskell.org/trac/ghc/wiki/TypeNats/Basics) But e.g. type symbol :~: a = a (~) :: (symbol :: Symbol) -&gt; a -&gt; symbol :~: a numGoblinsToSpawn :: "MonsterLevel" :~: Int -&gt; "NumHeroes" :~: Int -&gt; Int then numGoblinsToSpawn ("MonsterLevel" ~ 23) ("NumHeroes" ~ 55) Could it work? Whether it would be a good idea is another question.
No, implicit parameters are parameters that are passed around without you having to say so explicitly. This, however, is about labelled parameters, so that instead of only knowing `Int` or `Float`, you have some notion of the parameters' meanings without having to create a new type just for it.
Quick, start thinking about an extension that would fix the module system. :)
Correct. I will fix it.
This feature is COOL. I didn't know OCaml was hiding such awesome things. What are the downsides, if any?
&gt; consider the only downside to labelled arguments to be that if you define a function that uses them then you must use them at the call site unless you are okay with the resulting warning, and this is obviously a pretty minor downside, and definitely not fundamental to the idea. Doesn't [this docs here](http://caml.inria.fr/pub/docs/manual-ocaml-4.00/manual006.html) say that you can just write: # f 3 2;; - : int = 1 Or otherwise I don't know what you mean by "must use them at the call site"? Or does this docs mean I can't write e.g. `f 3` because it's partial? That would suck a bit. &gt; OCaml also has optional arguments, and these come with significant downsides. What are the downsides? &gt; OCaml is also hiding a lot of other awesome things that I want in the Haskell world, especially a decent module system. Polymophic variants are definitely up there on my list, too, but I think they can complicate the type system a bit. Yeah, I know the module system is supposed to be equal to or better than type-classes (also ML's). I guess Haskell doesn't really have a proper module system, more like a namespacing system with some frills.
Yes, you can write that, but if you have warnings enabled you will get a warning for that, which I think is reasonable since the labels imply that the argument ordering alone might not be enough to make it clear what you mean. Like I said, it's not like a major downside; it's just that labelled arguments can give you a false sense of security about argument ordering, like if you have a function that takes labelled arguments then you might think you should be able to reorder them without affecting call sites, but that's just not true. &gt; What are the downsides? Optional arguments can be terrible. They cause all sorts of surprising things. For example, you are allowed to define a function that accepts optional arguments and no non-optional ones, but you will have a lot of trouble using it (it is hard to tell when the function is fully applied, since all possible combinations could be valid). If you have optional arguments, you must use them before the last non-optional one, otherwise it will think all arguments are filled. If you want to partially apply a function with optional arguments, it will sometimes work they way you intend and sometimes not. If you give it a default value, it will be inserted by the compiler verbatim at the call site, which means that the work will be repeated and, in OCaml, side effects will be repeated, too; Haskell could change this behavior without consequence, at least, but there are downsides to the alternative, as well, such as not wanting to hold on to that default in memory since it could be very large. &gt; Yeah, I know the module system is supposed to be equal to or better than type-classes (also ML's). I would not trade type classes for a better module system, to be clear. While they have a lot of overlapping abilities, I think they serve different purposes. It's just that lacking one or the other you tend to misuse what you have. For example, [ListLike](http://hackage.haskell.org/packages/archive/ListLike/3.1.7.1/doc/html/Data-ListLike.html#g:36) should probably be a OCaml-style functor, and [monad.ml](https://bitbucket.org/yminsky/ocaml-core/src/8808e3a2571f4c5566957767c1049dba47a71dc9/base/core/lib/monad.ml?at=default) should really be providing a type class.
Right, those type aliases serve as labels to make the code more informative. The special part here is that you don't have to declare the labels up front. E.g. like this: {-# LANGUAGE TypeOperators #-} module Labelled where type typ ? label = typ numGoblinsToSpawn :: Int ? monsterLevel -&gt; Int ? numHeroes -&gt; Int numGoblinsToSpawn x y = x * y demo = numGoblinsToSpawn 25 25 Which would actually be quite nice, I think. You could use it as a general "documentation" operator. But people would have to agree on it, I think. But the OCaml syntax seems to have [has far more neat advantages](http://www.reddit.com/r/haskell/comments/15fn2u/neat_labels_in_ocaml_thoughts/c7m1ea2)!
&gt; you must use them at the call site How's that work with `Applicative`, or is there nothing like it in OCaml? What about lenses? Actually, what about any higher-order use of such functions? Does passing such a function to `map` produce warnings?
Expanding on my previous approach, why not generalize it to a de facto standard documentation operator that Haddock could interpret? With the type-level lits you could write arbitrary strings in there, or some types about what it might do. Not only could Haddock interpret it but also testing utilities or whatever. {-# LANGUAGE TypeOperators, DataKinds, KindSignatures #-} module Labelled where import GHC.TypeLits import Control.Exception type a ? (sym :: Symbol) = a numGoblinsToSpawnOld :: Int ^ -- level of monsters -&gt; Int ^ -- number of heroes -&gt; Int numGoblinsToSpawnOld x y = x * y numGoblinsToSpawn :: Int ? "level of monsters" -&gt; Int ? "number of heroes" -&gt; Int numGoblinsToSpawn x y = x * y foo :: String -&gt; IO () ? "still experimental -- use with care" ? throws SomeException foo = print 
Isn't that what the `-- ^` Haddock annotation does already? E.g. numGoblinsToSpawn :: Int -- ^ level of monsters -&gt; Int -- ^ number of heroes -&gt; Int 
Except only Haddock can see that, it's just whitespace as far as Haskell is concerned. Whereas compare: foo :: String -&gt; IO () ? "still experimental -- use with care" ? throws SomeException foo = print I can inspect that from GHCi's `:t`. Maybe we don't need Haddock's comment syntax anymore with this? The comments could also be compiler checked. E.g. if `SomeException` doesn't exist anymore, GHC will tell me. In a comment, it's waiting to become out of date. What do you think?
I'm not sure yet what to make of substituting the existing Haddock comment syntax by the grammar/type-level annotation above... But as to the exception annotation I'm actually wondering if the compiler couldn't infer the (potentially - depending on how reliable dead code can be detected) thrown exceptions anyway so that there's no need to specify them in the first place.
I don't think replacing checked newtypes with labelled parameters would be a good idea. That way you could again make mistakes confusing parameter order without getting compiler errors. Or did I misread the OCaml code and it actually does check them in some way? Named parameters might be handy for partial application situations where we now need to use flip or a lambda.
The labels are part of the function's type, so any higher order function must also include the labels of any function arguments in its type. If it does not, you must use a lambda to fill in the arguments.
&gt; We can write essentially [...] but a function that does the same thing as this is basically possible in Haskell *now*. Indeed. Did you not see my link to the [implicit configurations](http://www.cs.rutgers.edu/~ccshan/prepose/prepose.pdf) paper? This sort of reflection has been around for a while. But I still would not call it proper dependent typing (or "full-spectrum", in Connor's words). This sort of "dependent type" is the same thing that was done in Dependent ML, and Omega, and the same thing that we've been doing for ages with type-level Peano encoded numbers. Explicitly duplicating the value-level at the type-level is not dependency; it's the same old hackery Fomega has always had. And it is this need for explicit duplication that I am principally interested in doing away with. The important difference between operators and dependency shows up when you look at the type of your `toSingE`. There isn't any sort of connection between `t` and `x`--- at least, not any sort of connection captured in the type. You can believe that the function is returning a type-level encoding of the value of the argument, but you must have faith, because there's nothing to guarantee that that's what's actually going on. So far as I'm aware, there's no way around this without actual dependent types (of which GADTs are a fine subset). It's great that we can reify values at the type level, but you should be wary of the fact that you need to hide your actions from the compiler by using an existential to erase the "dependency".
Why don't we just make a GHCI :t command that tries to print haddock docs if available?
Nice, but see if you can use the excellent [mwc-random](http://hackage.haskell.org/package/mwc-random)-package instead of system.random. MWC-random is (or at least used to be) some 50x faster than system.random according to [this blog post here](http://www.serpentine.com/blog/2011/03/18/a-little-care-and-feeding-can-go-a-long-way/). I think I've heard that some people claim that it is even faster than Mersenne-Random package. The API of the package is little less nice though, and it would be great to have something like this built on top of it.
Or newtypes: newtype Celsius = Celsius Double
I only watched until I heard silly misinformation about Haskell, then I lost interest..
Thank you!
This might be a disadvantage too, though. The documentation does not necessarily compose correctly as the functions do.
Hm, example?
ghci also has funny behaviour when you do not label your source file with a module declaration. If the file contains no "main" function then ghci allows you to run all the functions that are declared, whereas if there *is* a "main" function then you can only run "main"! I find this very irritating.
Hmm. I've never heard of this, and I don't observe it with GHC 7.6.1. % cat test.hs main = return () foo = 42 % ghci test.hs GHCi, version 7.6.1: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. [1 of 1] Compiling Main ( test.hs, interpreted ) Ok, modules loaded: Main. *Main&gt; foo 42 
Don't use tabs. Seriously, you can't hope to share your code if you do, so just don't do it.
This is one of the things that I plan to cover in the subsequent articles ;)
Ah, the behaviour is different between "ghci test" and "ghci test.hs". Thanks!
Tabs for indentation, spaces for alignment. dmwit has even demonstrated how you can switch from tabs to spaces and back within a single line, completely coherently: http://hpaste.org/74331
I use a solution based on http://vim.wikia.com/wiki/Highlight_unwanted_spaces
Tabs are eight spaces in Haskell. Pretending otherwise can lead to "invisible" bugs.
Not exactly. A tab is not always eight spaces, it aligns the column to the next multiple of 8.
As I discovered from dmwit's comment, if you have already compiled test.hs to test, and then invoke ghci with "ghci test" it will not see "foo".
whitespace-mode in Emacs has a way to do this.
To distinguish indentation from alignment and thus provide more semantic content.
Tabs are *interpreted* as something with 8 spaces (see g__'s answer) by the compiler. Designing your code so that it is invariant under a change of the tab size automatically leads to correct interpretation.
&gt; they should just standardize on tabs and make space tabbing a syntax error. Of course, the opposite is true: tabs should be a syntax error. Having undefined-width characters mixed in an otherwise fixed-width layout is simply a bad idea. You end up having to use whatever tab width the guy with shortest tab width committing to the code base uses, or your code won't fit in 80 columns (or any other number if you don't like that one) anymore, so you might as well be that guy and set your tab width to 1. Then tabs are spaces anyway, and having both is redundant.
If it is stored as '\t', then doesn't that free the viewer to choose how to render tabs? (2 spaces, 4 spaces, or [elastic](http://en.wikipedia.org/wiki/Elastic_tabstop)). I guess your starting point is that fixed-character-width layout matters (80 chars or whatever). Does that matter anymore? 
Which part do they care about? It seems that font, font-size, and syntax coloring make a bigger difference than tabbing depth, and those rendering parameters are accepted as varying between people.
&gt; I guess your starting point is that fixed-character-width layout matters (80 chars or whatever). Does that matter anymore? It does. Code is still displayed inside rectangular windows, and looks bad when it runs outside the edge. The narrower you can make the window, the more windows you can fit on the screen. Wide windows use the available screen space inefficiently, as the area near the right edge is mostly blank. If you use variable width characters, then code that fits inside a window of right size no longer does when those widths change, and then you end up reindenting/line-wrapping the stuff (or writing a tool that always reindents/line-wraps files for you). That's unnecessary busywork and produces pure noise in source control systems.
It can be somewhat difficult to reason about space usage, and garbage collection delays can kill some embedded applications. I use it for just about everything else, and even for the latter scenario, I can write a DSL in Haskell that generates the code to run on the resulting system, and get many of the reasoning benefits.
Numerical computing. Haskell's great for toy numerical examples, but if I were to use it for the kind of computing I do at my day job, it would be a disaster. Destructive updating is a must when you're working with 4GB arrays of floating point numbers. Fortran 2008 isn't half bad for numerics, especially with coarrays, and it allows for pure functions, which the Intel and PGI compilers do amazing things with. I/O is a huge pain, though.
- A significant language barrier between Haskell and OOP. Possibly even worse than the one between OOP and SQL, which was bad enough to spawn an endless horde of tools to generate leaky abstractions to not-quite-bridge the gap. If you need to interact with existing systems that assume OO, expect awkwardness. - While using Haskell's FFI to interop with C is relatively painless, for many other languages the easiest route is to use the *other* language's C interop and bridge the gap through a C wrapper. If you need interop with something that doesn't have a C API, good luck. - For problem domains that lend themselves to OOP, Haskell can be cumbersome for the same reasons it doesn't communicate well with OO systems. That said, OOP is a poor fit for most problems it's applied to these days. - It's harder to write quick-and-dirty code for many one-off tasks in Haskell compared to languages designed to support those tasks, due to both the demands of the type system and lack of extensive standard library support for tasks that need quick-and-dirty coding. Note that this is a constant factor, while any advantages of the languages themselves scale with the size of the codebase. - It's not Lisp. Many people consider this a drawback in and of itself. Those people should try Clojure, it seems nice. - It's easy to write simple, idiomatic Haskell code that performs "good enough" for most tasks. If you have strict performance requirements (for overall speed, memory use, latency, throughput, whatever), you may be faced with either murky optimization voodoo or calling out via the FFI. If you need FORTRAN, Haskell isn't going to be a replacement (but there are FFI bindings for all the same hand-tuned FORTRAN packages everyone else uses). - All the usual it's not very popular and nobody else knows it blah blah. I don't really consider this a serious drawback but some people do. I've used C# at work for years, and for almost any project, if I needed to assemble a team and build something from scratch I'd rather have two good Haskell programmers than a dozen good C# programmers and be stuck using C#.
If garbage collection doesn't fit what ever you are doing (writing a video game, an OS, or anything else that needs time guarantees) then don't use haskell. If you need to do a lot of operations that are unsafe, like an OS don't use haskell. If you are writing a GUI application you should probably stick to another language. FRP is making good strides and there will likely be good GUI libraries in haskell in a few years but right now they are immature and limited. Write your GUI in C++ or something and call Haskell from it or vice-versa. As a matter of personal preference I wouldn't use haskell for most scripting tasks the type system for most scripts I want to write becomes more burdensome than useful. But libraries like parsec mean that string parsing is a hell of a lot easier. So there are good arguments either way on that one.
Reasoning about laziness is one of the greatest impediments I encounter while working in Haskell. Until you have internalized considering evaluation explicitly while writing code (and from what Edward has just said, perhaps even after this point), it is quite easy to construct programs with extremely poor space and time characteristics. This behavior is frustrating as code which apparently works on small inputs will unexpectedly fail with large inputs. While GHC's profiler can be helpful in tracking down these issues, it can still be non-trivial for one with limited experience to isolate and fix the problem.
You can definitely write C in Haskell. Unfortunately, C is a horrible language for numerics. Here's a simplified version of some code to calculate diffusion in Fortran. pure function diffusion(x0) result(x) real, intent(in), dimension(:,:) :: x0 real, intent(out),dimension(:,:) :: x x = x0(2:n+1,1:n ) &amp; + x0(0:n-1,1:n ) &amp; + x0(1:n ,2:n+1) &amp; + x0(1:n ,0:n-1) &amp; -a*x0(1:n ,1:n ) end function There's vector notation, the compiler can use SIMD instructions in a very efficient way, since the loop is implicitly vectorized, and it's easy to add OpenMP instructions to take advantage of multiple cores. If this was running on a cluster, coarrays would add just a few more lines of communication. C/Haskell have none of these features, which are a godsend for this kind of computing. Once Cray's new language Chapel has better performance, I'll ditch Fortran, but for now, Fortran's the easiest way to get high performance numerics.
I'm not sure where the notion among many programmers that C is the gold standard for "fast" code came from. Lots of widely-used numerics libraries are written in Fortran and no sensibly-written C code is going to match them. If memory serves me, the `hmatrix` package in Haskell and SciPy in Python are both full of stuff written in Fortran. I seem to recall that OCaml can outperform C in some cases as well, for somewhat similar reasons (the compiler can be smarter and has more to work with, &amp;c.).
That feels to me like a solution in search of a problem. So I follow this idea, and now I must pay attention to multiple kinds of whitespace, and I lose the ability to use the "tab" key to insert multiple spaces when spaces are in fact what I want. So it's `C-u 8 spc` or something, or `spc spc spc spc spc spc spc spc` … eh. And what do I get? The warm fuzzy feeling that I've enabled the sick desires of some perverse individual who wishes to indent eight spaces?
I actually took more issue with the fact that you can't prove the monad laws in a language like Javascript. Haskell lets you equationally reason about code, so you can prove these kinds of laws very easily. The `IO` monad is the only monad that we assume works as a matter of faith. Contrast that with Javascript, where there is no straightforward way to prove anything without building some model of the Javascript interpreter, what variables might be in scope, assumptions about the global state, etc.
&gt; I don't think there's anything about Haskell-the-language that makes it unsuitable for HPC use. Laziness works against you here, for one thing. Also, many of the features that make Haskell easier to optimize are lost or obfuscated when you start using mutable data. A Haskell-like language that was strict by default with something in the type system to allow reasoning about mutability in pure code, given even a fraction of the effort put into the Intel Fortran compiler, could potentially be fantastic (and yes, I am aware that there are at least a couple candidates for such a language, given resources to develop them).
&gt; ... though if you can use GPUs, the accelerate library looked nice last i checked. Incidentally, for some kinds of numerics, convenient libraries are more important than raw performance. Python has [Theano](http://deeplearning.net/software/theano/), which not only compiles code for GPU, but also automatically computes gradients and Hessian-vector-products for you (you *really* don't want to write these manually, especially when you're just exploring the model space). It's the kind of library I would've expected to appear for Haskell first, not Python :/
I agree, also something like numpy, scipy and matplotlib would be a must for me to consider using it.
Off memory from many hours ago: "Loophole in the function contract", "IO Monad solves a problem you will never have", and more, but I'd need to replay the video to remember them now.
I am writing a GUI in Haskell, and it's great. We did implement our own GUI framework in Haskell though, and it isn't yet comprehensive, but it's already much nicer than traditional GUIs in many ways.
Hard realtime programming, i.e. very strict constraints about order of evaluation and the timing in which they happen. Lazy evaluation makes it practically impossible to determine the exact order of operations and the timely fashion in which they happen. You'd have to wrap all of this into heaps of Monads of the IO type or something very similar which effectively makes you writing a procedural, imperative, impure program.
Ideally, the '\x09' character that nobody agrees on the meaning of would just *not exist*. This would make all our lives simpler, and we wouldn't have to spend so much time fixing messed up whitespace and fixing merge conflicts resulting from *invisible whitespace differences*.
On the other hand it's a neat way to track where things are coming from and how they travel around.
The simpler solution is to ban tabs. Choosing the desired indentation width is just not important. Messed up whitespace is important. Merge conflicts are important.
&gt; and I lose the ability to use the "tab" key to insert multiple spaces when spaces are in fact what I want I've never had that ability. It doesn't even make sense to me. &gt; And what do I get? The ability to have the computer change layouts and show layouts instead of having to do everything by hand. The ability to easily unindent an entire block. Etc.
Really? I wish it were so! From the code I've read, haskell has *less* uniformity than other languages. I've seen code with leading comas, trailing commas, with explicit braces and semicolons (and both leading and trailing variants) and some without. Some line up vertically, some indent. Some dedent 'where', some line up under the 'w', some line up after the space. The "align vertically" camp is all over the map with how much they align and where. Then some people stick to 80 columns, while others have infinite columns and no wrapping at all. Then for imports, some import unqualified (with and without import lists), some import qualified with single letter aliases (and different ones), some import qualified with the module name. I'm resigned to the idea that any hackage code I edit is going to look messy to me and if I make any significant changes I'll have to figure out what flavor of mess this particular author favors. And.... just to weigh in on tabs vs. spaces: spaces only. The "adjustable tab stop width" thing doesn't work unless you are also willing to resize your windows for every different project and remember all those widths. That's too much work. Use spaces, and use either 4 or 8. 2 and 3 are problematic because they're shorter than "let ", so you'll have trouble in 'do' blocks. I don't mind so much how many spaces the author chose to use to indent, but I do mind whether or not they use a consistent width and whether or not that width is &gt;80. It's easy to get used to 2 or 4 or 8, it's never easy to read wrapped lines.
&gt; While using Haskell's FFI to interop with C is relatively painless, for many other languages the easiest route is to use the other language's C interop and bridge the gap through a C wrapper. If you need interop with something that doesn't have a C API, good luck. Well, to be fair in many cases that is more the other language's fault than Haskell's, particularly when the other language offers no sane, stable calling convention like e.g. C++ with its name mangling issues, template and inheritance issues (meaning it is hard to instantiate a template or inherit from a class via a FFI),...
Why do you state your personal preference as if it were some religious matter of faith? Hundreds of thousands of people have managed to write code without problems using indentation characters to indicate indentation. Clearly the character is not the problem. This problem only exists in languages that make whitespace part of the syntax. In sane languages, indentation is purely a presentation issue, and simple tools can trivially set the presentation to any desired format.
A procedural, impure program... that uses garbage collection, so is probably still disqualified for hard realtime.
And you can use the exact same justification for banning spaces and only using indentation characters for indentation, like go does. Neither of these are making a logical argument, simply insisting that someone's personal preference be regarded as objectively correct.
The whole "make other people's code look like mine" thing has negligible benefit, but the required discipline to get spaces right in a team has enormous costs in practice. Doing a conflict resolution because someone got spaces incorrectly a single time negates more than the entire benefit you ever got from "forcing the indent of your liking onto others".
How is that misinformation? Maybe I'm misinterpreting, but from [this paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf) I understand that monads were introduced into Haskell exactly to solve a problem JS users never had.
I work in a C shop. The amount of time we waste due to tabs in code is insane. The bugs that get in because of incorrect conflict resolutions, where the conflict results from invisible whitespace changes is terrible. The fact you cannot open any single file that has tabs in it and expect it to have sane indentation due to tab messups. It is such a huge price to pay, and for such a miniscule benefit, that there might as well be no benefit at all. And this is in C, not a whitespace-significant language.
Haskell is good for writing compilers, even for languages targeting real-time environments. But that does not make Haskell a good language itself for such environments :)
&gt;It definitely requires less discipline to get things correct with only one kind of whitespace. So these two positions are not symmetrical. Are you seriously suggesting people put tabs inbetween tokens randomly, and this is a problem?
What discipline does using spaces require? If it works anywhere, it works everywhere. There's no hidden type of whitespace that looks good here, but will wreck havoc elsewhere. Only one kind of space ' ' is much simpler than having both ' ' and '\x09' with (by default) no visual indication of which is which.
I agree that haskell is about the least uniform stylistically of any language around. As for your justication for spaces, it doesn't really work. You already have code with varying line lengths anyways, as you pointed out.
Yes, and that involves a whole pile of reasons that have essentially nothing to do with the quality of the language. :]
No, I'm suggesting that having only one kind of whitespace negates the issue of invisible whitespace differences that people have to maintain, and takes a huge burden off collaboration.
&gt;Unless you adopt a (slightly odd) set of formatting conventions that indents everything that needs to be aligned This is what we do. We're not haskell programmers, we use haskell and a number of other languages. Rather than the terrible situation of having to have seperate tools, editors, style, etc just for haskell vs every other language, we just treat haskell like a normal language and indent as we would anywhere else.
Another thought about purity: the fact that the JavaScript version of `State s` is a monad should be equivalent to the fact that `StateT s IO` is a monad in Haskell.
&gt;What discipline does using spaces require? What discipline does using tabs require?
It's not absurd -- because it means your editor is going to support both kinds of whitespace insertion, and it's *not* typically going to visually discern them. This means you could easily insert spaces rather than tabs accidentally for indentation -- and you'll only find out when it comes to bite you in the ass in a merge conflict after someone fixed the indentation so he can read the code again. Having *only one kind of whitespace* is the solution, and that can only be spaces. If you think Linux has none of these troubles, you've not read a lot of Linux drivers (or at least, not third party ones).
Whitespace being cosmetic is nice, but not required. Python for example manages to be sane while giving meaning to whitespace. It just defines it in terms of indentation rather than aligning things to arbitrary positions.
I think a consistent policy is the solution to the problems you mention (either marking indentation via a tab character, or some fixed number of spaces). If I understand correctly, your comment seems to indicate that even if the standard was to represent indentation levels via a single tab character, there would still be collaboration problems. Can you elaborate? Thanks. 
But when one of those 2 or 4 width people starts editing your file, they will happily make lines that won't fit on your terminal. Perhaps you can configure editors or a presubmit to warn you if a line exceeds 80 if tabs were 8 chars, even though tabs are set to something else, but .... I'm gonna stick with "that's too complicated" and go with all spaces and a hard width limit. :)
I assume that would be the reason Fortran has the longevity that it does.
You're sitting on a big pile of proprietary Haskell code that you won't open source, because you use hard tabs..? Do tell.
&gt; If you are writing a GUI application you should probably stick to another language. FRP is making good strides and there will likely be good GUI libraries in haskell in a few years but right now they are immature and limited. Write your GUI in C++ or something and call Haskell from it or vice-versa. Or use a library like wxHaskell; you don't *need* FRP to have a GUI.
please elaborate
Python has, from what I've seen, by far the best collection of high-level, high-performance numerics and number crunching libraries around. If I was going to start a serious project in the data analysis space, much as I don't particularly like python, I'd have a hard time justifying using anything else. (under the covers, of course, the optimized parts of these libraries are fortran or c).
You can use -fwarn-tabs to enforce spaces. As far as I know, there is no automatic way to enforce proper use of tabs. During development of GHC some lines contained a very ugly mixture of tabs and spaces. http://urchin.earth.li/~ian/style/haskell.html Those two facts are enough to convince me to use spaces.
Yeah, even if the standard was to represent indentation levels via a single tab character, there would be problems. The reason is that most collaborators will not configure their editors to actually show the different kinds of whitespace differently. That means whitespace messups are invisible and thus rarely caught in time. Instead, they are caught much too late, when they've already caused spurious changes and conflicts. Various collaborators will not have the same editor, same editor configuration, and same discipline as the guy setting the standard. Therefore, there *will* be deviations from the standard (messups). For many collaborators, following any whitespace standard besides "Just use one kind of whitespace" is going to be hard. A hard standard to follow is a standard people don't follow.
&gt; It is absurd, because nobody on earth hits their tab key between words That's missing the point. The point is that even if you try to standardize on tabs for indentation-only, spaces everywhere else, you *might* easily get spaces for indentation, due to misconfigured editors. Enforcing no tabs at all is much harder than enforcing "tabs for indentation only". Ditto regarding editor configurations. Also, the relationship between the tab key and '\x09' is not necessarily existent. When I press tab I don't actually get '\x09's in my file (thank goodness). &gt; What do third party drivers have to do with it? Linux does have none of your troubles The Linux source code itself might be enforcing the discipline required to avoid a total mess (at the cost of casual contributors having a harder time submitting patches). Trying to work with 3rd party Linux code (forks of Linux code and/or drivers written with the Linux style guide) is a mess. They use the Linux tab style with less discipline and look like a random indenter added arbitrary whitespace to every other line.
https://github.com/Peaker/lamdu/tree/master/bottlelib/Graphics/UI/Bottle
Not mixing it with other kinds of whitespace.
This one is easy — you just make `fmap` behave like identity when the argument is not a list. ~~I imagine more problems with `return x &gt;&gt;= f = f x` — `f x` may be computable even though the structure of `x` may not match a particular bind function.~~ (that was a midnight nonsense, sorry)
I totally agree that a hard standard to follow won't be followed. My thinking for a new language is to enforce the standard at the syntax level, but that certainly isn't an option for a C shop (unless you run some kind of Lint program before accepting changes into revision control). 
Once again, you could replace tabs with spaces and that statement is equally true (and equally meaningless).
No, it isn't. But also I am not advocating that. I am advocating using 1 indent character to mean 1 level of indentation, and not doing random alignment to create noisy diffs for no reason.
&gt; The ability to easily unindent an entire block. All my editors can do that regardless of whether the indentation was performed using tabs or spaces...
I don't understand - "nobody wants pull requests of code wriiten using indentation"? Do you mean indentation using tabs? Your in-house Haskell style uses hard tabs? Or some funky custom layout? I still don't believe you are blocked on sharing code due to indentation .. Unless your code is really, really weird.
I think that C++ with a C like interface should be fine, even if it uses fancy inheritance and templates.
Why do you think that significant whitespace is a bad idea?
I hadn't noticed anything about 'axiomatic semantics' in `Tekmo` s remark.
Presumably the same troll?
Well, it is two seperate things, but yes to both. Indentation, and using tabs. The standard haskell style is not to indent anything, but to align stuff. We don't do that, as no other language we use pushes that kind of style.
&gt;My point is to try and convince you that there are other people in the world who have perfectly legitimate opinions, and that some of these opinions are going inevitably to disagree with your opinions. Why do you think I need convinced of this? I am simply suggesting that if a language is going to try to enforce a style, it needs to be done automatically by a standard tool, like the compiler. And that if you are designing a language and you aren't enforcing a style, then choose a syntax that is able to be manipulated by tools. That way all those differing opinions can use the style they like, and still share code without messing with each other's formatting.
writing SOAP clients and servers.
There is nothing inherently trollish about asking what the weak spots of a language are.
How do you know when you're done if some tasks can run forever? Also, if a task doesn't return a number but forks two more tasks instead, you'll want their result (if a number) added to the sum. Correct?
No one said it was; but asking twice in the space of two weeks would put a different complexion on the matter, no? Here we have two intensionally distinct users, neither of whom seems to have done anything but pose questions intensionally identical to, What's wrong with Haskell?
&gt; If I get a pull request that includes them I don't take it. #1 rule of pull requests: use the formatting of the source. I always use spaces when contributing to a project that always uses spaces because it's just plain polite.
It approaches the answer, but it never stops. And yes, if a task is forked, both children are added back to the pool.
I don't think they intended to be distinct users. It just sorta happened that way. :)
Yes, that's really helpful. I didn't expect you to go through so much trouble - thanks!
Data abstraction is quite annoying. The only way to export an abstract type is to _create_ a type to export. You can't export an abstract synonym, which leads to unnecessary newtypes.
Yes please!
Wow! this is wonderful. It'd be nice to also print the type of the function with relevant haddock docs for each argument, but even as-is this is quite useful.
I believe the DSL to which Axman6 is referring is [Copilot](http://leepike.github.com/Copilot/). Internally it uses the [Atom DSL](http://tomahawkins.org/) (search for Atom on that page) to generate C code for hard realtime systems. Googling should turn up many useful results. Oh, I see a [Copilot whitepaper](http://corp.galois.com/storage/files/downloads/Copilot_Whitepaper.pdf) at Galois. Might be worth a look...
Oh yeah, one last piece of advice: Strictness is really important! You'll notice I make sure the value I put in the TVars is fully evaluated before I stick it in there via using `modifyTVar'` (the strict version of `modifyTVar`) and: let !newtasks = tasks - 1 writeTVar taskstv newtasks (This would also work) writeTVar taskstv $! tasks - 1 Otherwise it would just build up a thunk each time and never actually evaluate it until it was read in the main thread - and then the computation would actually happen in the main thread which is undesirable. It also could run out of stack space if there were enough thunks because what would be in tasksmv would essentially be: `tasks - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 -` ... rather than an `Int`. That's one of the biggest problems I had when I started doing Haskell concurrency, especially in long running programs that need to do something like keep statistics. It is quite easy to run into space leaks if you don't understand strictness. Unless you specifically want something in a concurrency variable to be lazy, you generally want to use strictness annotations or records with strict fields: data Blah = Blah { somefield :: !Int , someOtherField :: !Double }
Some time ago, I was missing something like `haskell-docs`, but then I learned that `hoogle` can do that already (see http://www.haskell.org/pipermail/glasgow-haskell-users/2012-August/022657.html) ...or does `haskell-docs` provide something that `hoogle` doesn't?
Sorry, I don't follow — could you show some examples?
I think Tekmo is referring to equational reasoning a la "Fast and loose reasoning is morally correct", not so much any rigid axiomatic semantics.
I am not well versed in the technicalities of purity and I do not understand what you are trying to show with your stated equivalence or the exact nature of the equivalence you are trying to point out. Would you mind elaborating?
[Fast and loose reasoning is morally correct](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.8232&amp;rep=rep1&amp;type=pdf) only refers to ignoring partiality. It doesn't tell which rules/laws you can actually use to reason about programs. Axiomatic semantics is just a fancy word for what is usually called "equational reasoning", but it does assume that you fix your rules in advance and check that they are sound w.r.t. the "main" semantics (operational or denotational). So, my question is, where are those rules written out and where is the proof of soundness?
&gt; ease of equationally reasoning in Haskell I wonder whether such reasoning has any justification. See my reply to kamatsu.
Tekmo points out that we have to account for global state and other effects in JavaScript while checking the monad laws. That's true, but (more or less) all of JavaScript side effects are also possible in the IO monad. Thus, if we manage to formally prove that `StateT s IO` is a monad, then the same argument would give us that `State s` is a monad in JavaScript.
&gt; One nice thing about the approach we'll be taking is that it uses a "final" encoding of applicatives, rather than building up and then later interpreting a structure. You could also use the finally tagless approach. It gives you both generality and absence of an intermediate structure.
Let's see if it really is just as easy. Getting rid of tabs: sed -e 's/\t/ /g' enforces the space style and "fixes" any messups due to misconfigured editors inserting \t's. The result may not be indented correctly due to the messup, but it will not have any "wrong" kind of whitespace anywhere. What is the equivalent for enforcing tabs-for-indentation-only? Whatever script you'll use, it's going to have to figure out whether we're talking about indentation whitespace, or about alignment whitespace. See if you can make a script as simple that also, like this script, handles indentation and alignment, etc.
I'm not sure what you mean by the signature of a module. Are you referring to the type signatures of the functions exported in that module? As for switching between modules with similar interfaces (Data.Vector comes to mind), I always solved that by just importing qualified. What else is gained?
Real world coding in an environment where 95% of the coders are incapable of dealing with rather drastic change. Writing programs where a majority of the work is done in IO. Solving problems where the importance of code quality is not that high. And, worst of all, Haskell is bad for writing readable code. *(edit: better put, Haskell practically encourages writing unreadable code)*
I hate it when I read an excellent post, have no interesting substantive comment to make, and can only point out typos, misspellings and the like. ---- &gt; This is to say that the composition of two applicatives in cleanly and naturally always also an applicative. ("in" should be "is"?) ---- Many of the code samples are truncated on the right, and it is not possible to scroll horizontally. Both on my phone (Safari/iOs) and laptop (Firefox, maximised, with wide empty gutters on both sides of the window).
Okay, checked out hoogle. Two differences: 1. It doesn't display the documentation of arguments. E.g. try `finally`. 2. It doesn't know that `Data.List.Split` exists. However, (1) is merely a formatting issue, I think. I can patch hoogle to output this, too, I imagine. And (2) is probably a question of how to populate hoogle's database. For mine, you just need to enable `documentation: True` in your Cabal and then it generates the haddock when you install any package. So how do you make this automatic? I see that haddock can output for hoogle with `--hoogle`, but nothing in `~/.cabal/config`. How do you set this up?
The key here is that the target binary (the real-time one) is *not* produced at *compile-time* by GHC (or an other *Haskell* compiler), but at *run-time* by the Haskell program. Haskell just happens to be the language hosting the Atom library.
Agreed, so it looks like the haddock docs. So it seems that [hoogle may be able to do this](http://www.reddit.com/r/haskell/comments/15hrtz/haskellcafe_announce_haskelldocs_given_a_module/c7mphlk) with some tweaking of its output.
I don't quite see how this is specific to concurrency. I ran into similar problems with my single-threaded fold (well, mysql-simple's version) too. If you don't fully evaluate the state of the fold it can become a really big thunk really fast.
To be fair, you can't expect that the eta rule for functions will hold for numbers: p =/= (\x -&gt; p x) (with p :: (a,b)) You need the correct eta rule for JS' `U` type (which will be huge and probably not that useful): p == let (x,y) = p in (x,y) (modulo bottoms) That being said, in JS, `f =/= function(x) { return f(x); }` in general even for functions (even ignoring multiple arguments), because of `arguments.callee` and `arguments.caller`.
I just realized that I said ListLike might better as a functor when I actually meant as a signature. A signature is a type description of a module. It's not just the types of the functions, but of anything you might define inside a module, which also includes types, other modules, other signatures (which can be defined and named separately from modules, something like type synonyms), or, as would be the case in Haskell, type classes. They serve completely different purposes from qualified imports. A qualified import simply allows you to identify things that might otherwise have the same name. Parameterized modules allow you to completely abstract over your imports, deferring the choice of which modules to depend on for client code to determine. Signatures in this context are just a way to describe what you expect from the modules you abstract over. Think of this module abstraction as similar to higher order functions and signatures as the types of the arguments.
Yes, if you squint, the JS semicolon can be interpreted as a monadic bind. But that just prooves Crockford's point: JS users never had the problem, it was already solved. Citing the paper, &gt; Lacking side effects, Haskell’s input/output was initially painfully clumsy, which was a source of considerable embarrassment. Necessity being the mother of invention, this embarrassment ultimately led to the invention of monadic I/O, which we now regard as one of Haskell’s main contributions to the world, as we discuss in more detail in Section 7. When reading though section 7 carefully (esp. 7.4), I see that IO was the first monad instance added to Haskell, and the primary reason monads were considered to be added to the language. The fact that they proved to be useful for managing state, exception, and other things like non-determinism is, first of all — further development of the idea, and second — irrelevant to Crockford's statement. He was talking about IO, correct? But IO aside, what about other monad instances? Thing is, JS already has effectful ways to allow exceptions, parsing (I would also argue that applicatives are almost always a better choice here), IO, and most other things monad instances are used for in Haskell. Finally, “loophole in the function contract” is a “solution to allow effects around referential transparency” repharased for the audience.
Ah, I see. So then you want your interface to be "portable" across modules so that it can be swapped out by someone *using* the library? Although, now that I say that, I again don't see what is gained. When using a qualified import, the library user can also easily swap it out for a different implementation. I suppose you gain the ability to pass entire modules as parameters. So if they provide a function named bar that has the right type you'll be able to use it without recompiling. Is that correct?
How do I add the command to my ghci? I have the `haskell-docs` CLI tool, which is cool, but having it right in ghci also seemed cool.
Could you provide any references for what 'finally tagless' is? Is there a go-to paper on this topic?
&gt; they should just standardize on tabs and make space tabbing a syntax error That is what makefiles do and it is a pain in the ass.
[Seems related](https://github.com/chrisdone/haskell-docs/issues/1#issuecomment-11707496). I don't know how to deal with this version difference. What would you do? I don't mind writing #ifdef's, but on what?
I only looked at it cursorily; it's just that the lack of efficient vector and dense matrix types in the signatures worries me. You don't really want to do elementwise differentiation on something like matrix products, because then you have to implement the matrix product as a loop instead of calling a highly optimized BLAS. Numerics is funny like that; you can use a very slow language like Python with reasonable performance, as long as those basic linear algebra operations are efficient. So an automatic differentiation system should really operate on matrices. That requires a bit more care than the usual scalar differentiation rules (multiplication does not commute for instance). Also, in reverse-mode differentiation you write a log of the operations that occurred and then use that to compute the gradients; if there is an operation for each scalar multiplication in a matrix multiplication algorithm instead of one operation for that matrix multiplication, that's a lot of overhead.
 indent
HTML isn't usually discussed in this context because XML-style syntax, especially when pretty-printed is about as bad as it gets with regards to whitespace in version control. Wrap something in an extra tag and suddenly you have a diff the size of half your file due to whitespace.
I'm not saying "code" in html, but a separation of concerns *like* html. No tabs no double spaces.
Hmm, until I can get hold of a GHC 7.6.1. version locally I can't do much for that. 
This begs to be integrated with hdevtools..
&gt; If you are writing a GUI application you should probably stick to another language. FRP ... It is a serious error to think that UI development in Haskell means using experimental techniques, like FRP. Just use the UI or IO monads, and existing frameworks, and your job is done. We write commerical software in Haskell, using highly interactive GUIs, without issue. Higher-order functions and types, as well as precise control over laziness, allow us to get exactly the interactivity and latency we need. See e.g. http://ianwookim.org/hoodle/ for how successful this can be. UI development in Haskell is a non-issue.