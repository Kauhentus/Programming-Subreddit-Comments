In my theoretical language I might eventually implement, I'm differentiating between structs and ADTs. Struct values can be accessed with traditional dot notation, and I'm using a different symbol for the function composition operator.
&gt; Extensions become de facto standard not only because they are good, but also because they solve important issue (even poorly) and there is no alternative. TemplateHaskell is an example. This is a point in favor of extensions IMO, at least as an application developer trying to be productive. Template Haskell is fantastic for making stuff work quickly when there isn't a more principled way to express that in Haskell's type system and for reducing error-prone boilerplate. I know it's a bit of an ugly spot, but it's one of the nicer metaprogramming facilities I've used. I'd much rather *have* metaprogramming than not. I think we are about due for another standard, and I would like to have a bit more of a democratic process around what gets added to the language (I'm still bitter that `ArgumentDo` won't even be an extension despite ~50% of people liking it, but `TypeRep` is coming despite everyone agreeing it's ugly...). Given the choice of language extensions or not, I'd have to say extensions are preferable.
Nothing constructive to add -- thanks for mentioning the paper! I enjoyed reading it, and the implementation is really clever: dpll :: Int → [[Literal]] → Bool dpll n = evalState s . initialState where s :: State DPLL Bool s = runContT (sequence $ replicate n ϕ) q ϕ :: ContT Bool (State DPLL) Bool ϕ = ContT $ λp → p True &gt;&gt;= p makes [my own implementation of DPLL](https://github.com/parsonsmatt/dpll/blob/master/src/DPLL.hs) seem rather silly in comparison.
Previous languages I used on my own and at work: C++, C#, Java, Python, Ruby. Hardest things to learn: how to decompose the problem into modules, particularly, avoiding import cycles. Also knowing when/how to abstract various things. Becoming OK with laziness. Monad transformers. Things that were easier than I thought: monads themselves. Ensuring totality. Using `IORef`s.
I didn't mention it, but part of what I meant by that was that even when I'm not using Haskell directly, I am a better programmer. For instance, at work I write Python almost exclusively, but all of my side projects use Haskell. As a result, I tend to shudder any time I see state mutated unnecessarily, redundant conditional blocks, or unnecessary for loops, for instance. In other words, that quote shouldn't be interpreted as "I only write Haskell because I'm a better programmer for it", but rather "Knowing Haskell makes me a better programmer, in spite of the language I'm required to use to perform the task.".
The Heroku buildpack for Haskell doesn't work, I already tried it. I think Someone will have to write it ;-)
I'm also mostly a web developer. I've done backends with Rails, Yesod, Servant, a tiny bit of Node/Express, and a class project with Java servlets. I've done front end work with jQuery, React, Elm, and PureScript. I've been learning Haskell for around a year and a half, and it's my most productive language right now. In order to deliver a product or feature with roughly the same level completeness/correctness in these frameworks, I find that I need to spend significantly more time looking at documentation, writing tests, and debugging code with the weaker type system languages. Ruby *feels* faster because the initial prototype is often done quicker, but ensuring that is correct and maintaining it takes much more time. I've only had one runtime error that required debugging while developing one of my applications. One! You don't even know how much time you're spending on this stuff until you're not, and you go back to Ruby for a bit and are mired in it. It's taken me a good bit of time to learn how to put more information in the type system so my code is safer, but it pays for itself over and over again. There is **nothing** in any other programming language that is as nice, quick, and productive for developing a JSON API than Servant/Aeson/Persistent. It is simply unparalleled. It's the same story with front end -- Elm and PureScript both catch so many errors at compile time that you almost never debug runtime problems. There are so few "gotchas" with these FP languages that you don't have to have a bunch of incidental complexity in your head all the time. They're not perfect, of course, but it's _so much nicer_ on this side of the fence that I have no desire to cross back over. Haskell/PureScript/Elm don't have as many libraries as Ruby/JS, but the ones that are there tend to be excellent. It's generally easier to learn new libraries, since they usually provide a `Monoid`, `Functor`, `Applicative`, or `Monad` instance that you already know how to use. The type signatures tell you most of what to expect with stuff. I know this is pretty ranty and not entirely specific. I'd recommend continuing to learn Haskell. At the 2/3 month point, I had no idea how to really use it, and was still thinking imperatively. I think it took me about a year before I was comfy writing idiomatic Haskell and really leveraging its strengths.
Yes: * I make small updates all year round as I see new libraries reach a sufficiently high level of polish or I noticed old ones begin to go stale or abandoned * I also accept pull requests from volunteers. See the newly added sections on ARM and computer vision, for example * I also plan to review it more carefully every 6 months, followed by a blog post That means that I will release an updated blog post this month. These update posts will not contain the full text like the original announcement post. Instead these posts will mostly highlight what were the major changes to the ecosystem over the previous 6 month period and link to the Github post as the authoritative full text. These would be things like changes to rankings of various domains or additions of new domains. The main goal is to keep the post as "fresh" as possible and to actively remote outdated libraries or resources, unlike other resources which never prune stale entries (such as the Haskell wiki).
Do you have any experience with React/Redux, which tries to be more functional than most javascript (It copies Om and Elm)? Also if I were to make a product today I'd like to use Falcor or GraphQL, which I don't think Haskell/Elm/Purescript have good support for. Would you default to JS in this case?
Any insight as to why it didn't come through?
Aren't SPECIALIZE pragmas for that?
In the long term, their eventual fate, as is the fate of all things, will be to gradually degrade and dissolve into a steady soup of nothingness as the heat-death of the universe continues its inexorable advance. Well before then, the drives on which GHC code resides will probably melt away as the sun swells to a red dwarf and melts our oceans, rendering the earth an inhospitable wasteland. Most likely, that too will perish, as the earth is caught in the outer layers of the dying star's final expansion, spirals inwards and finally vaporizes into plasma. Happy Haskell Hacking!
Polymorphic recursion and existential types prevent pervasive monomorphisation. Sometimes things *have to* remain polymorphic. It seems intuitive to me, however, that when monomorphization (or I'd rather call it specialization given that the pragma is called `SPECIALIZE`) is possible, it should happen. I don't know why GHC doesn't aggressively specialize everything it can. Perhaps someone more knowledgeable can answer this. It might have to do with being CPU-cache friendly.
&gt; GHC devs is a relatively closed group of people. I think you'll find that if you start developing code in GHC, they will welcome you. I suspect that many people get the feeling GHC developers don't care about their thoughts or ideas, when the truth is that GHC developers are just very busy and don't have time. They are necessarily not open to doing work for other people, when they already have a lot on their plate. Keep in mind that a lot of GHC development is done by people who, while they care about Haskell pretty deeply, also have a job. That job is often academic, and certain kinds of work on GHC are part of that job (the type theory parts, for instance), but other work really isn't. Complaining that GHC developers spend too much time on type theory aspects is sort of like complaining that you spend too much time doing your paid job, rather than hacking on GHC for the greater good. So the answer would be to jump in and do it. If you were to implement most reasonable things, and do it cleanly at a standard consistent with the code base, I doubt the contribution would be rejected. I know getting started with compiler development is hard, though. There are ways to mitigate that, but there aren't really ways to fix it.
The problem is that pretty much any extension, given enough time, becomes a de facto standard because there will always be *some* widely used library that will pick it up somehow. If we let user adoption drive the ratification process then the language will continue growing until it collapses under its own complexity and gets replaced by leaner languages. I think a reasonable compromise would be requiring that all language extensions have a formal specification **before** they are added to the compiler because we can never predict in advance when they will become widely adopted in the wild, so we have to be prepared to ratify them at a moment's notice. Each extension's specification must also specify how it interacts with previously ratified extensions, so the specification requirement will naturally slow down the addition of extensions over time and stabilize the language.
It's not just about runtime performance but also speed of compilation
The high level reason people hope that type families will be the answer in the end is that they keep a functional programming style where programming at the type level doesn't look too much different from the value level. On the other hand, type-level programming with functional dependencies looks a bit like grafting on an odd sort of logic programming language at the type level. On the practical level, type families can help avoid situations where classes have a half-dozen parameters. But the cost is that they are a lot more verbose.
Also, I believe `INLINE` and `INLINABLE` can have the same effect since they ensure that the original source code is exported, which in turn means that `ghc` can specialize the code at the call site. I might be mistaken, though, so correct me if I'm wrong.
Maybe, but I found in my machine learning class compared to everyone's Python and Java implementations my Haskell code performed about the same and was more expressive and easier to understand for someone looking at the code which I think matters more on a maintainability aspect. You're right though, it really comes down to using the right tool for the right job
**tl;dr:** `makeClassy` is the lens version of overloaded record fields
If anyone happens to have questions about aspects of the .FieldTH or .PrismTH modules feel free to reply to this comment. (I did the rewrite of the TH code in lens a couple years ago)
That is really not at all an accurate assessment of contributing to GHC. There are indeed some great folks involved, but offering them code does not result in those offerings being accepted.
 fix :: (a -&gt; a) -&gt; a fix f = let x = f x in x This is the definition of `fix` in the standard library. I think even with -O0 using `fix` and recursive `let` will compile down to something essentially equivalent, because actually the latter is the primitive and the former just a bare wrapper around it. that said, i always get chastised by my coworkers if i use `fix` or `mfix` where i think it's more appropriate.
Okay, thanks, now I see how it's useful! I added a section about `makeClassy` to the post.
There's this package https://hackage.haskell.org/package/OrPatterns It's kind of a shame Haskell doesn't support these by default.
The main thing missing in the `makeClassy` approach is that I'd often like to be able to ensure that the class generated for say, HasBar has a HasFoo superclass constraint. class HasFoo t =&gt; HasBar t where bar :: Lens' t Bar But getting TH to generate such a constraint has a bit of a chicken and an egg problem, due to the way the splices layer into the source file. data Foo = ... makeClassy ''Foo data Bar = ... makeClassy ''Bar instance HasFoo Bar ... has the problem that each of those statements gets type checked in sequence at a separate time, so the makeClassy call for `''Bar` can't know about the HasFoo constraint, nor can I really enumerate them easily. So I'm left with some sort of quasi-quoted mess. There is probably an elegant solution for it, but it escapes me. So, while I can use makeClassy for 'leaf level' record types like: https://github.com/ekmett/quine/blob/master/src/Quine/Input.hs#L31 https://github.com/ekmett/quine/blob/master/src/Quine/Options.hs I wind up rolling instances for stuff up the food chain by hand: https://github.com/ekmett/quine/blob/master/src/Quine/System.hs#L34 
That allows all of 'm; I need more fine-grained control!
Hmmm. I don't know of any extension frozen in some random state. Yes, there are bugs being fixed constantly. If an extension is "bad" it will be discouraged and bugs won't be fixed. After some time it will be removed. If there's a good replacement, a new extension will be implemented and maintained. How else can we move forward? Just warn people about the new and unstable ones, like `TypeInType`. Extensions are like API - if it's experimental, you use at your own risk and assume that they will change and your code might break. But many extensions have been stable for years and classified as "benign" (see [Stephen Diehl's](http://dev.stephendiehl.com/hask/#language-extensions) page).
And DoBlocks or whatever Andrew's extension would be called. And Ollie's local imports, which I also implemented. These came from long-time members of the community, were all thought-out *and* implemented; not drive-by code dumps. Encouraging people to just chip in is really disrespectful of everyone's time. If someone wants to contribute, find something Simon has already signed off on. If there's something you want, talk to someone who can get on the phone with Simon to get him to sign off on it. That's it.
I don't want to suggest that folding things into a revision of the language is easy, but it's a versioning mechanism that is currently going unused.
Maybe something like this? data Entity = Tree | Car | Animal data Example = Known Entity Int Int | Other Int f (Known _ x y) = x + y f (Other x) = x 
I mainly use Python and Java at work, but learning FP made me a better programmer. I shun away from stateful constructs as much as reasonable for the language. For example, less for loops when creating data structures. Many times I will stick the per element code in an inner function and create the structure with a list/dict/set comprehension, ensuring the creation is well controlled.
It's an inadvertent consequence of labor recruitment efforts. We have more recruiters than people willing to champion outside contributions. After the flare up of contributions, things have settled back down to the usual moaning about changes thrown over the wall from GHCHQ, which at least means that GHCHQ is able to keep moving forward!
I haven't really used the extension, but every time I did type inference and type checking basically did not work for me... Can someone with a little better Understanding explain what this extension is expected to do?
You can use pattern synonyms {-# LANGUAGE ViewPatterns, PatternSynonyms #-} data Example = Tree Int Int | Car Int Int | Animal Int Int | Other Int two :: Example -&gt; Maybe (Int, Int) two (Tree x y) = Just (x, y) two (Car x y) = Just (x, y) two (Animal x y) = Just (x, y) two Other{} = Nothing pattern Two :: Int -&gt; Int -&gt; Example pattern Two x y &lt;- (two -&gt; Just (x, y)) f :: Example -&gt; Int f (Two x y) = x + y f (Other x) = x 
Oh, handy! I didn't realize that stack was capable of doing that. Given that it's convenient for managing newer versions of the compiler, I like this solution better than just installing under /opt. This is exactly what I was looking for. Thank you!
I believe a more useful question is "is doing it extensively a good or bad idea?". The advantages are clear and great, but disadvantages are pretty serious too. No separate compilation, code bloat and crazy compile times. Try MLton to get a taste. As usual, when there are this type of tradeoffs involved sometimes it's best to leave the decision to the user. I believe we have a lot of room for improvements in area. GHC's `SPECIALIZE` pragma helps, but it's not enough for some things (for example, what happens if I want to specialize a `Data.Map` function to a type I defined in my module? GHC doesn't allow this, and you'd need the source of that function). I'm hoping to write a blog post about this in a few weeks but in the meantime there's an old but great write-up about implementing polymorphism: (mentions monomorphization too) http://www.eecs.harvard.edu/~greg/cs256sp2005/lec15.txt
&gt; The only place where GHC can monomorphize less than Rust/C++ is polymorphic data definitions. That's not entirely true, see my other comment.
&gt; Polymorphic recursion and existential types prevent pervasive monomorphisation. also, GADTs.
If I get your meaning, you put quickcheck properties outside of your normal test suite and into your library/executable code? And you would like those to be run? I think it would be possible ... I could add some logic based on a option passed via the preprocessor line. I could imagine something like (basically, something that hspec-discover already does but with different logic): ``` {-# OPTIONS_GHC -F -pgmF tasty-discover -optF --module-name=XYZ #-} ``` And then you could create a new test suite in your cabal file, add a `Tasty.hs` file to the top level directory and it would find and run all your `prop_` tests inside modules with the XYZ suffix. Perhaps even a regex would be nice ... Please feel free to open an issue with more info - https://github.com/lwm/tasty-discover/issues
On the one hand, I'll repeat the mantra that general recursion is the goto of functional programming. On the same hand, "fix" doesn't really do much to alleviate that, but you might like [Y overriding self-application](http://okmij.org/ftp/Computation/overriding-selfapplication.html). I haven't give too much thought to how to better write `sliceGroups`, but one interesting solution might rely on the fact that you are essentially working with ["compositions of integers"](https://en.wikipedia.org/wiki/Composition_%28combinatorics%29) here. Maybe someone has a neat solution involving Pascal's triangle or somesuch?
Couldn't you opt in to the runtime polymorphism when it's needed, rather than have it be the default? Rust's trait objects are an example of such a scheme, where you can say whether you want compile-time or runtime dispatch (obviously some code only works with the latter).
[removed]
I was using the excellent `users` library that had a nested call to an `IO` action using a `ConnectionPool`, which when used with the `postgres` backend to the `persistent` backend, caused the whole program to hang. IO ¯\\\_(ツ)_/¯ It wasn't terribly difficult to debug -- the GHCi debugger is totally reasonable to use along with some `Debug.Trace`, and it was really easy to return an `IO (IO a)` rather than executing the `IO a` to fix the nesting.
At this point, I don't think there's a reason to recommend the platform. I would recommend Stack instead. I also would not recommend LYAH. It's shallow and it lacks exercises. Your list of "real world" Haskell uses is missing [Facebook's](http://cufp.org/2015/fighting-spam-with-haskell-at-facebook.html).
[Discussion on Hacker News](https://news.ycombinator.com/item?id=11138818)
Totally agree. LYAH is fun and engaging, like a magician showing a bunch of tricks and then saying "behold!" is fun and engaging. You might be entertained, but you won't know how to perform any magic yourself. In my experience, the majority of people who use LYAH as their sole learning resource come away with nothing more than "that's cool, but now what?"
This doesn't seem to be discoverable from the front page. I could only find it when I tried to submit it. Very strange. Anyway, nice article. &gt; In practice any non-trivial business logic module can very easily have 100+ lines just of imports, and frankly it gets tiring. True, but you don't need a custom *Prelude* for that, just reexport modules for common cases. &gt; `"password": "hunter2"` &lt;3 &gt; Records of hundreds of fields are somewhat pathological but in practice they show up in a lot of business logic that needs to interact with large database rows. If your database library doesn't allow representing large database rows as *nested* records, file a ticket. &gt; Documentation is abysmal. ... What this means for industrial use is to always budget extra hours of lost productivity needed to reverse engineering libraries from their test suites just to get an minimal example running. A valid pain point. However, if everyone who complained about some library's documentation contributed some documentation back (once they understand how that library works) then the problem would be less by a significant factor. 
Reposting since the [original post](https://www.reddit.com/r/haskell/comments/46oona/the_joy_and_agony_of_haskell_in_production/) seems to be invisible.
What is a better alternative to LYAH?
Haskell's a high level turing complete language. It can be used for basically anything. Performance-wise it probably beats python by a lot. Library support is just a matter of how much work one's willing to invest oneself. While I agree that it'd be difficult to convince other people to use it, I don't see a need to convince them beyond learning the basics of the language - since I strongly believe that the lack of most of Haskell's cool stuff in other languages will drive people back to Haskell naturally. And if not? Well, then perhaps the language isn't the right one for them, so no point trying to talk them into it. If you want to use Haskell, it's at a point where there's lib support for a good amount of stuff, where the tools are somewhat mature and if you want to do something in a less explored area, well, there's the C FFI or simply learning things more in depth and writing ones own library.
For my use case it is typed javascript, so either Flow or TypeScript.
[removed]
&gt; That said, the language actively encourage thoughtful consideration of abstractions and a “brutal” (as John Carmack noted) level of discipline that high quality code in other languages would require, but are enforced in Haskell. Hmm, I have a hard time parsing the second part of that sentence. Is it saying that in other languages, writing high quality code requires a brutal level of discipline, whereas in Haskell the compiler enforces high-quality code so you don't need as much discipline? Or is it saying that writing high-quality code requires discipline in any language, even in Haskell, and that Haskell enforces a minimum amount of discipline?
My advice would be not to pick too many new technologies at the same time. And probably replace your existing site incrementally too. I'd say your choice of web framework is really down to your personal preference. But maybe Scotty for something simple, Yesod for a full opinionated framework generating HTML or Servant for something more API oriented.
It's hard to compare LYAH with real world Haskell, for example. I think that LYAH Haskell is a gentle introduction and don't see alternatives that fit that need.
&gt; LYAH is shallow and lacks exercises. Knowledge always appears trivial in hindsight, but as even a look at /r/haskellquestions shows, switching to a functional style of programming is a difficult and shocking transition for people, with even things on which we hardly even waste a thought (pattern-matching, data vs type, HOF) being challenging, simply by virtue of being unfamiliar. LYAH is, in my opinion, a good guide that eases the reader into the languages and provides enough tools to be reasonably productive. When evaluating it, you have to consider the mindset of the beginner and his willingness to invest time into an unfamiliar technology. They might appear shallow to the experienced, but there really is value to such gentle introductions to topics. Yes: at the mountaintop, the Haskell-programmer will be able to whip up HTTP servers and EDSLs, but one has to get to the mountaintop first - and that won't be accomplished by positing free monads and Template Haskell as the first steps.
I wish he had some example of abysmal documentation in a library that is actually worth using. What library has abysmal documentation yet has sufficient test suites that the library can be reverse engineered from the test suites? I haven't seen libraries with abysmal documentation but awesome test suites. Typically if a library has abysmal documentation it just isn't worth using.
&gt; https://haskelliseasy.com/ Do you mean http://haskelliseasy.readthedocs.org/en/latest/ ...?
I know that I'm bad in explaining things, especially in English (no irony). But I see a number of people already got the idea. You may find the issue not important, it is totally OK, we all have different priorities. But I don't understand how you don't accept its existence. But I'm happy to explain it again if you are really interested. Yes, the state is not truly random. I meant that it is out of control of extension author or (hypothetical) language committee. Extensions just freeze at some point. People decide that an extension is good enough (or just better then nothing) and adopt it. At that point most benefits of extension mechanism disappear -- its author is not free to experiment with it any more. And benefits of bureaucratic procedure are not available either! A lot of extensions don't even have a formal spec. I closely follow ghc trac, here is how things often work: * user: I want recursive superclasses * dev: why do you need such strange thing? * user: I want this wild trick to be possible: &lt;...&gt; * dev: ok, here is implementation, please try it * user: amazing, ship it! * simonpj (after a week or two): hey, please mention it in user guide! &gt; Yes, there are bugs being fixed constantly. If an extension is "bad" it will be discouraged and bugs won't be fixed. After some time it will be removed. If there's a good replacement, a new extension will be implemented and maintained. The same occurs in other languages. Sometimes faster, sometimes slower. It has little connection for extension mechanism, it depends mostly on how community is tolerant to breaking changes. &gt; How else can we move forward? I'm not ready to answer. But I believe that accepting an issue is half of its solution :) 
GADTs are existential types with additional equality constraints.
&gt; If your database library doesn't allow representing large database rows as nested records Can you give an example of what you mean by this?
The platform isn't broken. It just doesn't have stack.
And gives unfriendly dependency errors...
oh man, not going to lie, this sounds *awful* -- Rails is tricky enough to test/deal with as-is without worrying about which version of a Haskell command line utility your application is shelling out to. How is your deploy/build process working with all of that? Also, [this should probably make an appearance](https://github.com/nh2/call-haskell-from-anything)! A company I was working at previously was considering making the transition, and the plan we put together was: 1. Identify services/features that would make great candidates for Haskell 2. Wrap those in service classes that are totally decoupled from the rest of the application 3. Write a new instance of the service class that, instead of doing it locally, made an HTTP request to a locally running web service that uses the same Ruby code. 4. Finally reimplement the functionality in Haskell of course, this is more of a SOA approach, which we had Reasons for wanting to do. A more monolithic transition would be more difficult to do.
&gt; and that won't be accomplished by positing free monads and Template Haskell as the first steps. No one suggests that. LYAH is bad not because the material starts too slow, but because the lack of exercises makes it easy to have a false sense of understanding. I remember reading the chapter on monads, thinking 'oh i get monads now!', and then being totally confused when it came to actually learn them. As a whirlwind tour of neat things Haskell does and what Haskell code looks like, it's fine. I wouldn't recommend it to someone who wants to actually learn Haskell though.
LYAH and practical exercises. TBH, most of RWH went right over my head when I started out. But LYAH gets you started on simple stuff rather quickly. Of course, it doesn't have any exercises. But that's no big deal if you use Project Euler or other common CS problems to tinker around with what you've learned. And once you get to the part about functors or so, you're already capable of starting your own little projects.
This is the first I've heard of hpack. It looks really cool! https://github.com/sol/hpack
&gt; shelling out to small Haskell command line programs Do you use the standard process module by itself, or some helper library on top of it?
&gt; Wouldnt it be nice to have some applicative record sugar? Yes! Something like let c = Config.require config . ("database." ++) -- ^^ Please let this be a typeclass polymorphic function Mrs Typechecker return &lt;some special magicky magic&gt; ConnectInfo { connectHost = c "hostname" , connectUser = c "username" , connectDatabase = c "database" , connectPort = pure 5432 , connectPassword = fmap (fromMaybe "") (c "password") } This is the kind of thing I was trying to achieve with fully polymorphic product types in Opaleye, but nobody liked it :(
You can notify [the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fhaskell) if a non-SPAM entry/comment gets blocked by a the new-account/low-karma filter (see [this post about the SPAM filter announcement](https://www.reddit.com/r/haskell/comments/40acwr/recent_spam/) for more information).
I'm a bit confused about what you mean by dynamic behavior. I assume you're talking about the HTML-canvas backend for Diagrams. How would adding CSS classes allow the diagram to be dynamic?
It is obvious in this case, but I imagine that you would still want to use field names in the general case.
Why not?
I really liked the paper as well :)
Ha, perfect response!
Why didn't they like it? Fully polymorphic product types are "not idiomatic Haskell", apparently. Admittedly they are syntactically awkward, though.
The problem is that we don't know if that's what's happened. I'm not even sure the original submitter knew.
[removed]
out, err, st = Open3.capture3("&lt;haskell-cmd&gt; #{current_user.id} &lt;other args&gt;") in ruby 
I'm working on it! I'm trying to learn Idris's theorem proving mechanisms at the same time I'm learning more generally how I might encode proofs in types. It's a lot to take in.
hpack generates cabal files from a package.yaml file, which is less redundant and easier to maintain. If a package contains a package.yaml file, stack will automatically use hpack to regenerate the cabal file before building, and likewise during stack init presumably. There are [some](https://github.com/commercialhaskell/stack/issues/1813) [issues](https://github.com/commercialhaskell/stack/issues/1814) being worked out. hpack *is* cool, and I'm happy to see support for flags and conditionals is coming, which will make it viable for a much wider range of packages.
Got it! I wonder if we could do this using Diagrams' ability to name subdiagrams using [Diagrams.Names](http://projects.haskell.org/diagrams/haddock/Diagrams-Names.html). [This example](http://projects.haskell.org/diagrams/doc/quickstart.html#a-worked-example) uses this feature. Perhaps we could add functionality to the canvas backend so that the user can provide a function that maps names to HTML ids/classes, and the backend can apply these ids/classes for all named subdiagrams.
I'm planning to do a (almost) traditional website, with authentification, etc ... so I'm at the moment leanind towards Yesod. I read the tutorials and it seems to be ok. I tried Spock in the past and find the monad transformer thingy a bit complicated. If I understood well, Yesod seems to use typeclass to add "static funcion" instead of monad reader, which simplifies thing lots (even though you can also use monad reader with Yesod).
The latter.
I think Diagrams could do with some "hooks" to allow users to customize the output a bit, whilst at the same, staying compatible with the different backends.
Just added it, thanks for the advice!
`ApplicativeDo` would properly desugar the above code to use the `Applicative` class exclusively
I've actually had reasonable success with gloss, at least for my purposes: [this](http://i.imgur.com/Ac95UHO.png) was made with gloss. The source code is [here](https://github.com/vektordev/GP/blob/master/src/Frontend.hs), at least for the plot generation. While the font rendering of gloss isn't perfect, and the performance in terms of FPS tanks when having several 10s of thousand of data points, it's good enough for my purposes, and quite frankly: plotFeatures "0 - fitness - 1" "0 - compilation rate - 1" fitness (fromRational . compilationRate) features or similar is already what plotting should look like imo. Give it a filename and it pops out a.svg or so, that would be ideal. Feel free to use that code if it helps you. You might have to modify it a bit and pull in dependencies from GRPMath.hs To actually answer your questions: I think nothing stands in the way of actually making a plotting lib. My code above shows it probably wouldn't even be all too hard to get the basics running.
1) I think the reason that Haskell doesn't have a great plotting library is that not many people are using Haskel for data analysis. Baked into ggplot2 is support for data.frames, or the high speed data.table, and an overall better ecosystem for doing things that result in plots. 2) I think Chart seems like a viable option today, but to compete against ggplot2, the best option is to implement Wilkinson's grammer of graphics in Haskell using Diagrams to render the actual image. 
High discipline for both other languages and haskell but haskell enforces the discipline.
What didn't work? Can you open an issue with a trace that didn't work? Thanks!
I think the weird thing about the quote is that it's saying that a *perspective* and a *kind of programmer* in incommensurable, which is either trivial or some sort of "type error." (Of course, what's meant is that two *perspectives* are inconmensurable.)
Do people not generally go wherever is needed for an internship? I wasn't in co-op in university but I knew lots of people who would fly across the continent or further for an internship.
&gt; I'm specifically looking for internships in New York Are you open to using non-Haskell languages? Jane street uses OCaml, and has an [internship](https://www.janestreet.com/join-jane-street/internships/) program. The ClariFI team at S&amp;P Capital IQ uses an in-house language called [ermine](https://launchpad.net/ermine-user-guide). /u/edwardkmett [gave a talk at CUFP a few years ago about the use of FP at ClariFI](https://www.youtube.com/watch?v=o3m2NkusI9k) (he works at the Boston office). The NY office has a summer internship program and I know that the ClariFI team has had interns in the past. If you're interested, /u/sclv is probably the right person to get in touch with.
Well, in English we have many forms of coercion via rhetorical devices. In this case, "industrial programmers" is used to mean "the perspective of industrial programmers", which is a form of synecdoche.
Actually, probably more people will use haskell for data analysis, if haskell add a tool similar to ggplot2. Put it in another way, lots of people are using R because of gglot2 (and plyr)
I've seen your post the other but didn't have time to try (mostly because I don't have anything to graph at the moment) but also because the overhead of connecting to a web server and using D3 (is it needed ?) has put me off a bit.
It takes a fraction of a second to start the server. After that it will run in the background until the ghci session is over (it will survive the `:reload`). What do you mean by using D3? The library can use D3, but you only have to understand JSON. You'll never have to touch anything but Haskell and JSON (you write both in your `.hs` file)
It would look awful due to the demoralizing experience of developing in those languages in the large, which gets more pronounced in more seasoned developers who actually understand what they're missing
Not going to lie, I'm having flashbacks to using open3 with some really ugly command line apps. Do not miss that at all. Good luck!
Isn't it a bit like asking how a famous painter would do if you gave them a coloring book and a sharpie? There's a reason paints are so popular. 
Do you have a real example of when it's useful to use a significantly different haskell datatype from your table?
Updating large records is slow, according to the article. Updating nested records is probably quicker, depending on precisely how they're nested.
I think Chart is a good foundation. I used ggplot for all my plotting needs for a few years, but I was entirely dependent on cookbooks and StackOverflow so I think that the main thing missing is more examples of creating plots. Perhaps an expanded section of demo plots for Chart would be a straightforward path: present an album with dozens of plot thumbnails that each link to their source. Data frames is another side to this. I'm biased, but I'd love it if those Chart examples nonchalantly used Frames to pull in CSV data and do whatever minor massaging necessary to feed them to the plotter.
Not sure what you mean by "fully polymorphic product types," but this is exactly the polymorphism provided by `vinyl`'s `Rec` type, which is parametric over the functor applied to each field. So you can have a `Rec IO ConnectFields`, `Rec Maybe ConnectFields`, `Rec Identity ConnectFields` etc. and use morphisms or traversals to move from functor to functor. See [my reply to the same parent](https://www.reddit.com/r/haskell/comments/46pjmm/the_joy_and_agony_of_haskell_in_production/d076sxi). Syntax is always a sticking point, of course.
Unless I'm much mistaken, *that* you can do with records parametrized by an applicative data ConnectInfo f = ConnectInfo { connectHost = f String , connectUser = f String , connectDatabase = f String , connectPort = f Int , connectString = f String } You can even get generics to deduce Applicative f =&gt; ConnectInfo f -&gt; f (ConnectInfo Identity) for you. Opaleye's polymorphic product stuff is a bit more general, and doesn't require you to wrap pure values in `Identity`.
Internships in the USA have a Specific Meaning which most people ignore. They're intended to be learning experiences where the business doesn't get more value from the intern than the intern gets from learning on the job. Usually they're full time over the summer (when college students aren't in class all the time) or part time during the fall/spring semesters.
Chart Maintainer here: I wasn't aware of R ggplot etc back when I first wrote Chart in 2006 - did they even exist? Nor was I aware of the grammar of graphics book, though no doubt it would have influenced me had I read it. The Chart library gets occasional care and attention from me now, but it generally meets my needs. I think the main reason haskell doesn't have a great plotting library is the lack of commitment in making it happen. For most haskell developers, plotting is a small addendum to their regular work. They want to get a plot quickly, and then move on. Generation of plots is not a key focus for many people. Whilst the Chart library is far from perfect, I think it could become a useful base for higher level visualisation functions based upon some sort of data frame type.
Yes
Worse, from my own personal experience working with C/Java/Python. The problem with other languages is that it's too difficult to change your first solution to the problem so you usually end up being stuck with a sub-standard solution. With Haskell I can change direction pretty quickly even for large projects so I don't have to get things right the first time and the quality of the project ends up much higher.
Hmm, come to think of it that Generics code wouldn't be too hard and could be easily packaged into a library. Maybe I'll throw that together.
I understand that there's good scientific evidence that generalizing what you say is wrong. Personally I'm happy that people are providing github links. I regularly file issues about missing examples in libraries I use. It takes a few seconds and helps highlight the cost of this to those that still believe that types is an efficient way to learn an unknown library.
Given so much Haskell docs are written from a type theoretic math domain the problem is telling programmers in the trenches what corecursive endofunctors are actually good for. That and operator abuse. 
Hey thanks a lot for this! ermine looks really interesting, I've never heard of it before.
Thanks that's really interesting, and useful... what do you mean by avoiding import cycles? Do you mean cyclic dependencies? Yeah, Monad Transformers can be pretty intense to understand...
Yeah, that's a good point. In some cases I think I've realized this in the back of my mind and used softer terms for those three points. For example: 1. Higher order functions (which allows you to abstract control flow more easily) 2. Static type system 3. Controlled side effects YMMV
You might not want to delete all of .stack, as you might have customization in .stack/config.yaml and .stack/global-project/stack.yaml. You can be sure that there will be no package sharing between snapshots that have different GHC versions, so one approach is to delete all the snapshots that have the same GHC version. But yeah, being able to clear out unused packages is tracked by https://github.com/commercialhaskell/stack/issues/133 . We'll definitely need some form of snapshot garbage collection once https://github.com/commercialhaskell/stack/issues/1265 is implemented (hopefully soonish).
Others have answered this well. One thing to note is that the support for hpack is experimental. We might change how it works in the future (call an hpack executable instead of using it as a library), and the format itself is still a work in progress.
I think Chart is a great start. One thing notably missing from Chart is the ability to render TeX equations in the plot labels. This is crucial for having publication-ready charts. It also means that often Chart can only be used to the exploratory phase of a project at present. In addition, the drawing backends aren't as polished as I'd like for producing vector graphics. This can be fixed in Diagrams. I would like to see Chart fully embrace a Grammar of Graphics as I think its a great fit for how we compose things in Haskell already. I do want to stress that I think Chart already is a fairly capable tool. I used it quite productively in http://indiana.edu/~ppaml/HakaruTutorial.html.
+1 for TeX equations in the plot labels. I'd love to see this in Chart. Maybe I'll try to contribute this feature to Chart.
http://happylearnhaskelltutorial.com (self-plug) ... though we don't cover things in the same way (it's not language-feature driven, it's broad example-driven, then deepening, when we get the time to do more).
If it's bad, it's because it's just a demo... and yet it doesn't show you a lot, it just walks you through *some* of it, which means as you get into the latter chapters you kind of don't have a frame of reference for why you're learning those things.
HaskellR works very well on the plotting examples i've thrown at it. 
No, in the case of `HashMap` the programmer would have to write O(n^2) instances. Specialization needs to be call site driven (i.e. automatically based on actually used combinations of keys and values).
I can personally recommend parsec and happy/alex, as i have used them painlessly and to good effect in my projects. However, i, like you, would be quite interested in a comprehensive compare-and-contrast. CFG (for happy) writing can be a bitch to write and debug, but it pleases me to have my grammar defined in an understandable and declarative way. It makes sense in the use case I use it for (interpreter for a bash-like language), but it is definitely overkill for simple parsing. Parsec is the only haskell parser besides happy i am really familiar with, which is why i use it, and as such i can't really tell you why it's better than the others. I have heard good things about trifecta, but have so far failed to find the time to research it properly. Thanks for posting this, and I'll look out for a more helpful answer in the other comments.
[Earley](https://hackage.haskell.org/package/Earley) is a little underdeveloped, but the algorithm is awesome.
I would suggest checking out clckwrks: http://www.clckwrks.com/ Out of the box it provides user accounts and an admin console. It can be extended via plugins. One of the standard plugins is the `page` plugin which provides pages and blog posts which can be edited by the admin. The look and feel of the site can be modified by using different `theme` packages. Any custom functionality you need for your site can be added by implementing a new plugin.
There's also [megaparsec](http://hackage.haskell.org/package/megaparsec) which is a fork of parsec claiming to be better and appears to be more actively maintained. https://github.com/mrkkrp/megaparsec#megaparsec-and-parsec
Oops, I totally meant to put that on my list, but it looks like I forgot. Thanks for pointing it out.
Saving this for when I'm teaching a Haskell class.
I agree that the *performance* issue is not a bug in the database library. But the non-composability is. It just so happens that composability allows you to work around the performance issue, it is indeed not a direct solution.
I disagree with the strawman premise that explicit imports are bad. They are good. They make the meaning of every symbol clear to anyone who reads your code, including yourself. It's worth investing some effort. And worth using automation where that can help, e.g., emacs auto import sorting. If you find maintaining imports painful, don't throw out the baby with the bath water; invest in better tool support for explicit imports.
&gt; I disagree with the strawman premise that explicit imports are bad. That's not something I intended to imply at all. I always use explicit imports and find it very painful to browse unfamiliar code which doesn't. I just though the claim you need a custom *Prelude* to handle that is a bit overblown. [EDIT: Looking back at my post neither the word "implicit" nor the word "explicit" occurred!]
To be honest, it's one of the worse options for practicality's sake. As I said, it's underdeveloped. The reason I mentioned it is largely just because it's an implementation of a really strong algorithm. It is capable of parsing any context free grammar, which makes it more capable than other parser combinator libraries. It is not monadic (because context free grammars can't be monadic), it is applicative. It's severely lacking in builtin combinators and utilities.
When running a parser uu-parsinglib tries all possibilities in parallel so it has no need for a parsec-style `try` function. Instead, if you have an ambiguous grammar you can either add an extra cost to the dispreferred branch(es), or get a list of all possible parses. (For example, you may add a small cost to an identifier parser so that reserved words take priority over them.) It can also use `[Token]`s (for some type `Token` you've defined), `Text`s, etc., as well as strings. I don't like the `pWhatever` naming convention, but apart from that I like the library a lot. [This tutorial](http://www.cs.uu.nl/research/techreps/repo/CS-2008/2008-044.pdf) is quite readable IMO. If you already understand `parsec`'s internals then you probably want to skip ahead to section 4 (p18) or so. edit: also, it prefers writing the parsers applicatively rather than monadically if possible; if you need to use monadic sequencing the implementation needs you to specify the minimal length that that parser can match.
I'd mention using old and proven way of making lexing step before parsing. E.g. `alex` + `parsec` combination is not unheard. If you don't like `alex` you could use [lexer-applicative](https://hackage.haskell.org/package/lexer-applicative) or even the same or other parsing library. Yet the benefit of purposely made lexer is that it runs productively as the result is a list of tokens. (e.g. returning `Error` token if it cannot lex anymore, and not running in `Maybe` or `Either` monad). One could argue that, with separate lexer and parser, the error reporting is much better as well. The downside (or upside?) is that you limit which languages you can parse.
Will be finishing a project in about a month for log collection (currently ~ 10kLOC). Hopefully I will soon use stack when it supports aeson 0.11, I quite often need to use latest versions of libraries that are not on hackage yet. The experience has been quite pleasant; most libraries I'm using are very good, I have found and fixed errors in some of them. I have dutifully filed a Pull Request, it sometimes takes months for the PR to be accepted... (and sometimes it's immediate). I'm doing quite a lot of parsing, network protocols etc. - using conduits, attoparsec, cereal, aeson-0.11 for encoding, json-stream for decoding, binary-serialize-cbor for serializing/deserializing internal structures (currently probably the fastest in haskell, it doesn't have 32-bit support though and it is not released to hackage yet). I has never been so easy to write code for network protocols; all the parsers support incremental parsing and are usually integrated with conduits. I am sure the code would be faster in other languages, but I would never be able to write such code in the other languages. In another project (that ultimately isn't in production) I needed to process and modify XML - xml-lens turned out to be a surprisingly effective means to do that. Servant seems to be a nice way to build web services. Most performance problems were problems in my code. I cannot quite compare it to performance in C/C++ as implementing anything of this in C/C++ is hell. Been there, done that, no more. (I even rewrote one haskell project to C++ - it was an 'openvpn' like software and we needed to run it on embedded platforms and have as much performance as possible. The C++ code is quite OK, but I'm much more confident in correctness of my Haskell code).
The main point of Earley is that it can parse *all* context-free grammars, including ambiguous and left-recursive ones (which combinator parsers struggle with), and is still reasonably efficient for many commonly-used subsets. It is significantly more high-level than combinator-based approaches because you don't have to worry about backtracking semantics or similar details, and it is easier to verify that the grammar you typed in actually corresponds to the grammar you wanted. In particular, you don't have to left-factor grammars for infix operators and such by hand, which I've found quite nice, but I've only used it on toy examples. Downsides include a loss of performance in most cases (can't quantify, sorry) and the inability to parse anything that's not context-free (whereas combinator parsers allow context-sensitive constructs via the monadic interface). Can't speak to the quality of its error messages unfortunately.
`uu-parsinglib` seems to be almost unmaintained and untested (no issue tracker, no repo alive). For instance, there was a nasty regression between 2.8.1.1 and 2.9.1: following snippet is freezing import Text.ParserCombinators.UU.BasicInstances import Text.ParserCombinators.UU.Derived import Text.ParserCombinators.UU.Utils main = print $ execParser (pSym 'z' *&gt; pList_ng (pSym '_') &lt;* pSym 'y') "" We switched from `uu-parsinglib` to `attoparsec` and `alex`+`happy`.
I'm writing in haskell for food for the last few years. I'm a free contractor from Belarus, and I have a customer in US. I'm writing and maintaining a BaaS solution for him and few smaller projects. I'm very productive in haskell, I think it was right decision to use it. Though there are drawbacks. Haskellers tend to over-engineer code. It is common to design unnecessary complicated API just for "purity" or "type safety". Often it is easier to reinvent a wheel then to use an existing library. But I hope it is just a growing pains.
Actually, you're right. When it was updated for 7.10 (when that breakage was presumably also introduced), I hoped that was a sign that it'd pick up again. Apparently not. :(
&gt; In addition, the drawing backends aren't as polished as I'd like for producing vector graphics. This can be fixed in Diagrams. As it stands, the diagrams library is somewhat awkward for generating quality plots due to it's lack of support for text metrics.
Nuking .stack is pretty simple :) Thanks for the tip! Worked beautifully.
We use Haskell in production, we have 10-15 people writing Haskell full time. It certainly client facing (but the clients are within the company). It's working out quite well so far.
Our Haskell compiler does complete monomorphization. As others have pointed out, this precludes certain features, e.g., higher ranked polymorphism and polymorphic recursion. Monomorphization produces a bit of code bloat, but we recover most of that by using a more coarse type system for the monomorphization, e.g., all types represented by a pointer can share the same id function.
I'm sorry, I don't know what happened. We (the Haskell committee) decided on structs, existential types, and higher ranked universals (inside a struct) at La Jolla in the summer of -95. Next time I saw anything it was instead the current Haskell records, designed by Mark Jones and Paul Hudak. Paul Hudak was the editor (and czar) at that time and I guess he got cold feet. I'll ask Mark Jones what happened next time I see him. 
I'm confused about this. I've been thinking I should start lexing before I parse, to simplify my parser. In fact, why stop at two stages ? Wouldn't more and simpler parsing stages be easier to understand (analogous to GHC's many intermediate representations) ? Then, how to connect these lexers/parsers - would you just use lazy evaluation, or enforce some kind of streaming ? And, doesn't multiple stages mean I'll lose the source positions which parsec nicely keeps track of, which allow accurate error messages ? If I have to save those positions along with every lexeme, isn't that likely to increase allocation and hurt performance ?
I'm quite fond of Attoparsec simply because of the fact that it's Parsec except everything backtracks. In general, I've found that the parser labeling it offers is usually enough to figure out why it's not matching, but it's not as easy as it could be. Whether you want to use it or another parser library would probably depend on how orderly your data is. For hand edited text files, it will work but probably isn't the least painful option.
I have. It's a simple (~3 kloc of Haskell for backend, plus another ~3kloc of JavaScript for frontend) web app with a small number of users (~2k). I was the sole developer. A few things: * stack is excellent. Developing a web app means almost all of your dependencies are in stackage LTS, and that means you don't ever have to deal with cabal hell. * Compile times are occasionally an issue. Most of the time I rely on flycheck (emacs) to point out errors in my code without a full recompilation. This takes less than a second and is often very helpful. When you do need to compile and run the stuff, compilation time could be anywhere between a few seconds to a full minute, depending on which module needs to be recompiled. Granted, the code contains a lot of TH so it's understandable. * Documentation is indeed bad as Stephen Diehl has mentioned, but that often is not an issue. You'll quickly get used to seeing incomplete documentation and read the source code of dependencies. As long as the package doesn't use TH, source code is often easy to understand. Referential transparency helps a lot: if you don't understand how a function works, just copy and paste the definitions of other functions it uses. In practice, starting to use a new library might be initially slower due to bad documentation than to use a similar library in say, Python, but very quickly thereafter you can be just as productive. Overall, using Haskell at this scale isn't a problem at all. Instead you'll be delighted at the ease of refactoring and the "easy" correctness of the code you write.
I think it boils down to reducing the friction to adopting iHaskell. A browser as an I/O system is a brilliant idea
Photos and sounds are already stored as blobs you cannot really diff/merge. That part would be the same. Text data as JSON would probably be saved structurally (in the spirit of Lamdu) (e.g: as an `Aeson.Object` rather than `String`).
Megaparsec's docs are great, especially the [tutorials](https://mrkkrp.github.io/megaparsec/tutorials.html). They talk a lot about lexing and make it sound really easy. No separate stages are used there - it's just one parser, but a well organised one, with the levels of meaning nicely abstracted within it. I'm still curious.
&gt; Text data as JSON would probably be saved structurally (in the spirit of Lamdu) (e.g: as an Aeson.Object rather than String). The problem is that usually existing software and other tools need to be able to use it, and I'm probably not going to be able to teach my proprietary 3d scene editor software, Excel, Vagrant or even Bash to use the Lamdu AST blobs instead of YAML, CSV/XML, Ruby or Bash, respectively. Or teach Lamdu to use a proprietary binary merge tool for certain types of files. This is undoubtedly a solvable problem, but it's a big one because real world projects involve many parts and often (in web, game etc development) also many people who are not programmers, and being able to get a single master reference to the entire project at some revision is really important.
attoparsec is the "fast parsec", as im sure you know.
Yes, but I needed to interleave IO with my parsing to do 1 pass parsing supporting includes. Also I heard the difference between them is no longer very emphasized.
We have one Haskell service in production at [OrgSync](http://www.orgsync.com). It's small (less than 1,000 lines of code). It collects feedback from our mobile users. Previously the biggest pain point was tooling, but Stack fixed that. Now the biggest pain point is records. I eagerly await overloaded record fields. 
As always: it depends. More stages don't necessarily make the parsing pipeline simpler. I wouldn't call "different intermediate representation transformations" "parsing". Vague definition would be: parsing is transformation from textual format (string) to structured "semantic" format (ast). After that we can check that ast is well-named (all names used are in scope), well-typed: populate ast with type information (type-check) or transform ast into different ast (e.g. to Core in GHC case). Using monadic parsers like parsec we could do name-checks already in the parsing stage, for some languages. Yet in Haskell the values can be defined in arbitrary order. So to do the check, you'll need to traverse the whole module. So you need to parse everything first, and then do check if all names are in scope. &gt; How to connect these lexers/parsers? &gt; And, doesn't multiple stages mean I'll lose the source positions which parsec nicely keeps track of, which allow accurate error messages? That's easily answered with two type signatures: lex :: ByteString -&gt; [Token] parse :: [Token] -&gt; Either ParseError AST lexparse = parse . lex Two have accurate error messages you have to keep track of source positions, in all intermediate steps / respresentations. You have to keep source positions around in AST as well, to e.g. give accurate "name not in scope" errors. The memory pressure of source positions is pretty small, they are just two `Int`s (line, column) (or four for range). In fact, making `parsec` (or say `Earley`) input smaller (from `n` chars to maybe 1/10th tokens), you'll increase the parsing performance. Returning to the multiple stages -question. In some complicated cases, it might make sense to have some additional steps, e.g. `[Token] -&gt; [Token]` if there's something complicated happening in between. E.g. with stateful, yet productive scan over tokens, you can transform the stream of tokens, so that parser definition simplifies enormously (say from requiring monadic parsing to context-free). One example on top of my head: attach leading/trailing documentation comments to following/previous tokens. Also one way to solve indentation based layouts between lexing and parsing, i.e. transform "whitespace from the beginning of the line" into `INDENT` / `UNINDENT` tokens. 
Looks like a great project! One thing I miss though is the benefit of seeing the result live - I understand the importance of grasping the underlying control flow and structure when writing a piece of program, but I'm not sure adding the live results means anything better than just looking at that piece of text. If there is a video demo of how a program is actually written it would be nice.
Probably get better answers in /r/purescript. Also, your search efforts may go better if you search for electron instead of atom. Note the number of [relevant google results](https://www.google.com/search?q=electron+and+purescript&amp;ie=utf-8&amp;oe=utf-8) when searching "purescript and electron". 
great I'll do just that!
doesn't it lack the features for building a blog and other things I need to?
There's also [picoparsec](http://hackage.haskell.org/package/picoparsec-0.1.2.3/docs/Data-Picoparsec.html), which works over any instance of **FactorialMonoid** from [monoid-subclasses](http://hackage.haskell.org/package/monoid-subclasses).
There is an up-to-date tutorial. It's here: https://github.com/VinylRecords/Vinyl/blob/master/tests/Intro.lhs But now that you bring it up, that is a pretty confusing place for it to be. I've opened an [issue for this](https://github.com/VinylRecords/Vinyl/issues/84).
As far as I know, Earley is the only parsing library that: * Fully backtracks (no need to left-factor the grammar) * Produces good error messages * Is first-class (i.e. embedded in Haskell, unlike happy)
I'm living now basically as a Haskell programmer, but I'm a 3D technical director. We have a lot of javascript and python code for internal tools that our artists use. I'm the sole developer using Haskell exclusively.
I have no idea. But if you randomly mash keys there is a good chance perl will interpret part of it as a program. If you randomly string together type and category theory words there is likely a library exists for it.
That's actually really interesting. I had heard that Haskell wasn't really suited for computer graphics. Is that true or have you had a different experience?
Well, I know a little bit about corecursion, and a little bit about endofunctors, and "corecursive endofunctors" seems like gibberish to me.
I'm using Haskell in production. I've written a phone dialing system where in the Haskell part is the central database of sorts sharing information about calls in flight. I used Haskell for the great concurrency support and the high performance. It's been really great. 
I don't use Haskell directly for computer graphics, so I cannot comment, in fact, I admit - shamefully - that I've never used any of its graphics libraries. I use Haskell to create tooling mostly, and do non-glamourous backend chores. I'm trying to move our tools frontend to something Haskell-like as well. We use javascript for that and it has become clear that it is sucking our resources.
[Link for the lazy.](http://unisonweb.org)
Hire me! :)
I'm not sure I understand. If I write `nonlinear x = (x,x)`, what's preventing me from writing the expression `nonlinear myGlContext`?
Yes I did consider Erlang. I went to the Erlang Factory too. The are two reasons I went with Haskell instead. For starters Haskell is also great at concurrency and the model fit better. I have effectively a relational database model but expressed in rich Haskell types rather than db tables. STM allowed me to transactionally update whichever interlocking pieces of that I wanted with many concurrent threads. The start is wide with many agents updating. Chances are they don't overlap so I rarely roll back. This gives me overall speed. In Erlang this would be an non shared isolated state with actors lining up to give their update. That would be more efficient in a model where the data is not wide and most updated are likely to collide and be rolled back wasting effort. Secondly this project was done at my company were I introduced Haskell for the project and no one else knew Haskell. I wanted as much safety as I could in a new language I was going to introduce. I chose Haskell because I could set up my own types for the IO that's allow. For example logging and reading the config but nothing else. I could make new types for every int in my codebase making it illegal to accidentally pass a call ID where a patient ID was supposed to go. All of this leaves developers who follow me well guided even if I'm not around. Wide case they can't get it to compile and it's because what they were about to patch out would have broken everything. Lastly I'll say that though this was a phone dialing project Erlang had no advantage there. Free switch was how we actually did our dialing. My Haskell code was really a database. A custom one that is much faster than the Oracle it replaced but has no durability which was not needed for us. 
Why is that? Does the type system allow easy refactoring or does the structure of Haskell just lend itself to rapid changes?
You don't honestly believe that, do you?
&gt; In c, I'd probably just mmap the file and that's it, it's deserialized. I'm working on a Haskell implementation of [Cap'n Proto](https://capnproto.org/), so we'll have that in Haskell (hopefully) soon enough :).
&gt; Right now I'm looking for just one really polished widget toolkit before I rate this area of the Haskell ecosystem "Mature". How does `gtk` not count? It's got practically 100% coverage of all user interfaces widgets and runs on Windows, Linux and OSX. It also has an established interface builder, meaning that you don't need to do UI layout in code. Are the packaging problems the reason you don't consider this? The API is pretty nice to work with, too - close to the C interface, but with just enough Haskell sugar to stop making me want to scream at the monitor every 5 seconds.
The type system is the main reason Haskell is easy to refactor
I've spent a fair amount of time trying to make the API to FLTKHS as nice as possible. Is there something I could be doing better?
Sounds like a cool project! &gt; I will soon use stack when it supports aeson 0.11 You should be able to just add `aeson-0.11` to your extra-deps list. Might need a couple other extra-deps if it requires some other new versions, but I bet it'll just work out of the box.
I'm working on a project that required a custom PL with all sorts of fancy gadgetry, so to figure out the structure of how to build a modern functional PL, I wrote a number of implementations of increasing complexity. The repo has lots and lots of explanatory docs, which include the type theory in more or less complete form.
Thanks for the FLTKHS shout-out (http://github.com/deech/fltkhs)! The install process has been completely updated to use 'stack' on Linux/Mac and Windows. I had some Mac OS X issues on Yosemite and El Capitan earlier this month. I think I have them worked out but if a Mac user wants to help test the install, please ping me. Unfortunately it's a little rougher on Windows and I don't think it will ever be out-of-the-box easy but I'm working on making it good as I know how. I should have a release in the next week or two where installing on Linux and Mac is pretty much a `stack build`.
Just saw [this](http://www.luna-lang.org) on HN. Seems related. I'm glad people think about getting us out of that pit (or local optimum..) that is the current unix inherited ecosystem and the even less well designed web technology.
Do you think a better course of action is to contribute to Chat directly?
That was not meant as a criticism at all of FLTK - my comment was orthogonal. I'm sure the API to FLTK is good, but I haven't really played with it
/u/tekmo I disagree with the rating "Best in class" for parsing (though perhaps it's only because I wish for something better). What we have are a bunch of solid libraries that only solve about 80% of the problem: * Parsec and friends: All the parser combinator libraries I know of are recursive-descent and lack unordered choice. I don't want to do the manual transform for eliminating left-recursion; I don't want to wait for a bug report to realize that my precedence was off in some corner case. I don't want to ever type `try` again. * Earley: This will probably be my library of choice next time I need a parser. I was so happy when it was announced, but I'm still left wanting more. I'd like warnings in cases where the grammar is ambiguous (or even, cannot be proved to be unambiguous); I know this is undecidable in general, but there are approaches with some practical results ([Analyzing ambiguity of context-free grammars](http://www.sciencedirect.com/science/article/pii/S0167642309001506), [pdf](http://www.sciencedirect.com/science/article/pii/S0167642309001506/pdf?md5=3abd4d7d0e8041457c874e265dd3cf3d&amp;pid=1-s2.0-S0167642309001506-main.pdf)). There also doesn't seem to be a good way in the library of dealing with ambiguity for the end-user. I wish there were a way to point to part of the string and say "There! There's the source of your ambiguity." * Happy: As far as parser generators go, Happy fully functional but nothing special; it provides some help in debugging your grammar, but not too much. * All of the above: I have yet to find a way of dealing with source locations (i.e. keeping them around in the parse tree for use in higher-level error reporting) that doesn't make me uncomfortable at the amount of boilerplate; at least with OO languages, you can pretend that the source locations don't exist when you don't need them. (This is my most tentative point; it's not really any of the parsing libraries' fault, and I may very well be ignorant of some approach that addresses all my concerns; cofree comonads, maybe?) I'm also hopeful to see if [meerkat](https://meerkat-parser.github.io/index.html), a new Scala library, gives the benefits of Parsec and Earley in one package (for more information, see ([Practical, general parser combinators](https://dl.acm.org/citation.cfm?doid=2847538.2847539) [pdf](https://github.com/meerkat-parser/papers/raw/master/pepm16.pdf))), but I can't read Scala and I worry that it might depend on implicits or mutation in a way that makes it unsuitable for Haskell. That disagreement aside, **thank you** for the wonderful writeup and for keeping it updated as the ecosystem changes.
That's a real shame because if you are working in a domain when you only care about Linux then it makes a lot of sense. Having something cross platform doesn't really matter to me so much as I have the energy to provide support more than one (in terms of all the other issues that could arise).
I hope people don't mind if I plug a commercial product here, especially given that I work for the company that develops it. It's a possible alternative to HDBC-odbc, for cases when odbc access doesn't work or is not available. There is a commercial "data virtualization" solution called Denodo Virtual DataPort, with connectors for many types of sources. A free "express" version exists. As it happens, DataPort can speak the postgresql wire protocol, so some time ago I tried to connect to it using postgresql-simple and had some success. I wrote about it [here](http://productivedetour.blogspot.com.es/2014/12/connecting-to-denodo-virtual-dataport.html).
We are using Haskell to deploy a significant part of the customer-facing web presence of some of the world's largest enterprises, with more in the pipeline. We are now in or approaching the go-live phase for a number of them. So the world will soon wake up to the fact that Haskell is now one of the major platforms for enterprise web applications. We have done all this with a small team of Haskell developers. I have experience on other major platforms, and I seriously doubt this could have been done on any of them. Our VP Dev is duly impressed. But in our current situation we need new hires, and he has not been very successful at finding them in Haskell. For that reason, there are now long-term plans to migrate our technologies gradually to a more "mainstream" platform. There is a strong preference for on-site employees at our Jerusalem and Tel Aviv dev centers. But we recently hired an off-shore Haskell developer who is working out great. So perhaps they will be more open to that now. If you are interested in joining us, please be in touch! Apply to jobs@suite-sol.com, but also please PM me here on reddit.
I've used `gtk` in a Haskell project on Windows for some time and recently switched to a C# GUI that simply calls into the Haskell executable after having some problems with bugs and general maintainability. It's quite a heavyweight package with many dependencies. And I couldn't get it to use a different theme. Also there were bugs with file paths that contain special characters and I couldn't find a way to work around them. It's a huge package with a very rich API and it seems to work really well on Linux, but on Windows I wouldn't recommend it at the moment.
&gt; However, if Earley gets a little more polish then I'd probably switch to Earley as my default parsing library recommendation for new users. Early parsers are way less easy to read/write than the parsec family, I don't know why you'd suggest it to a new user as a general parsing library when it appears to be mostly intended for grammars that the others are bad at.
Yep. I replied to your post because of your quote of OP to focus in on that point. I agree with what you wrote.
You can constrain it using the `DataKinds` extension.
So it seems like the main issue is the lack of fltk binaries on windows, so one must build it themselves? One solution going forward might be to just make those binaries available, as tied to the msys distributed by stack and/or the platform...?
I'm not familiar with this extension. How would you do it ? 
Sounds very cool, I’ll definitely try it out. Thanks for the summary; that’s precisely the sort of thing I’m looking for. It looks like it can handle non-character tokens, too, which I really like.
Even though I like the idea of iHaskell, the problem is it doesn't integrate well with external editor. I much prefer the orgmode-babel approach, which allows easily to mix language, display graph within the editor and all of that using the editor of your choice (nearly, I'm a vim guy but I'm switching slowly to Spacemacs, maily for orgmode).
Yes; while it may not have been explicit in my original post, my focus on which parser combinator libs can consume non-character tokens is due to my wish to at least *be able* to perform some sort of lexing step first. I mostly want to avoid dealing with whitespace in my parser—littering space-consuming parsers throughout my code is annoying and tends to make error messages worse without careful annotation, in my experience.
There are currently a few issues on Windows that make the install process rougher than it needs to be: * Unlike Linux/Mac, Windows does not have the development toolchain installed so the user has to install MSYS2, CMake, `tar`, `gzip` etc. Yes, GHC 7.10.3 does ship with MSYS2 but I've addressed that below. * `fltk` binaries are available through MSYS2 via `pacman`, but I can't get Cmake to see the header files and libraries. I get weird errors at link time about types being defined in two places. The issue magically disappears when installing from scratch and it's pretty easy to do. It's not ideal but since the fix is pretty simple I'm concentrating on the bigger problem of ... * Currently it doesn't work with 7.10.3 on Windows. There appears to be a [bug](https://github.com/haskell/c2hs/issues/157) in either `language-c` or the MinGW headers that ship with that version. It works fine with 7.10.2. In the `stack`world this is at least manageable by having a separate `stack-windows.yaml`but it does mean that until this is sorted out Windows users get pinned to 7.10.2 for any project that depends on FLTKHS and have to take that extra step of `export STACK_YAML=stack-windows.yaml`.
gnome.org dropped the ball since gtk3 to be honest. There's no stable theme engine or api and there are at least a few easy to encounter drawing regressions. While use of CSS for theming can be nice a feature, albeit a slow one so far, the pendulum swung too far into slow and bloated during the process. It doesn't help that their focus is GNOME the desktop and everything else is ignored.
Why no mention of Leksah?
Thanks so much for these posts and the state of the ecosystem repository. It's not only useful for newcomers but a reliable reference resource when I want to get something done.
How are they less easy to write? The syntax is different, and the documentation could use improving (i.e., a worked example so folks don't have to figure it out themselves), but you're still just writing out a CFG... Or is it actually the specification of semantic actions you're complaining about? I can see how giving up the monadic interface for semantic actions takes some getting used to, but Applicatives are now part of the assumed knowledge base so it shouldn't be too outré.
The gtk package might have full coverage, but Gtk itself is only mature on Linux.
Re memory pressure of source positions, I had in mind the thought of storing file paths as well (I have included files, and need to know which one had the error) but I guess one can arrange it so it's not necessary to store the file path in every single token. Thanks very much for your thoughtful reply, I'll re-read and ponder. I find it a tricky subject to get a handle on. Probably taking a few courses in it would fix that.
I have a few additions to your excellent summary :) &gt; Downsides include a loss of performance in most cases (can't quantify, sorry) To quote from the `earley` Haskell implementation's [github page](https://github.com/ollef/Earley#the-parsing-algorithm): &gt; The worst-case run time performance of the Earley parsing algorithm is cubic in the length of the input, but for large classes of grammars it is linear. It should however be noted that this library will likely be slower than most parser generators and parser combinator libraries. I've used the `earley` Haskell implementation for the parser of an Agda-like toy language. It comes with a module for mixfix parsing, which made it relatively easy to parse user defined mixfix operators. The restriction to context-free grammars hit me, when I tried to parse blocks of semantic-whitespace, as used in Haskell's `do`-notation. Fortunately, I could simply customize the lexer to make blocks explicit in the token stream, by inserting `BlockBegin`, `BlockNext`, and `BlockEnd` tokens at the right positions. This basically isolates the non-context-free part of the grammar into a custom pre-processing of the token stream, and leaves a context-free grammar, which can be parsed generically with `earley`.
Ah right, understood. Cheers.
So.. Edit it and add it in :) You have currently the most complete/modern list. Maybe you could also update the Haskell wiki after this thread sinks down. Good question: I did not need a parser lately, but I also felt that the Parsec dynasty is over, not knowing what's next :) This thread provides a great overview.
&gt; Early parsers are way less easy to read/write than the parsec family Quite the opposite. With an Earley parser all you need to do is write a context-free grammar and the algorithm parses it for you. With the other recursive descent libraries, you have to examine the grammar and write a parser by hand. In many ways, the recursive descent parsers like Parsec and Attoparsec are a step backward from parser generators because you have to get mired in the minutiae of the grammar. Earley liberates you from this--give it the grammar and off it goes. I wrote a library, [Pinchot](https://www.stackage.org/package/pinchot), that takes a description of the context-free grammar and generates both a parser and all of the corresponding data types. This sort of thing would be much more difficult to write (if it's possible at all) with the hand-written recursive descent libraries because all of them parse only a small subset of the context-free grammars. Earley handles any context free grammar.
Yeah, I also considered it a pretty strange categorization. I think Atom is more in the category of things that include Emacs, Sublime Text, etc. It's just a highly configurable text editor with lots and lots of packages (and probably the best package system that I've used for an editor)
thx dude :)
You're welcome!
`--allow-newer` might be helpful, worth a shot. It's got issues unfortunately https://github.com/commercialhaskell/stack/issues/1579 - will probably be resolved soon, I have some changes that are nearly ready to be committed which fix it.
The library did feature the monadic API when I tried it, but my impression was that the monadic API did not actually make things simpler, or at least not in a way that merited the introduction of a monad. I have dug out the old code just now, and uploaded the [relevant snippets in a gist][1]. Both examples are significantly longer than a single line. Granted, they do more than can fit in a single line, in particular with the titles, but I still found this too verbose, in particular the gradient stuff. Since the monadic API did not meet my verbosity goals, I tried out the pure API (second example). Event though it was a lot more verbose, I was happier with it, because it was easier to understand. If I remember correctly, the documentation for the monadic API was much harder to understand. In the pure case, I was at least dealing with pure values. In the end, I think it's essentially just a matter of compressing verbs and nouns. Complex plots will always be complex, but I think that agressive word compression makes them easier. I don't think that other plotting libraries (MATLAB) do complex plots well, but the simple ones have very good compression, which somewhat extends to the complex ones. Here some thoughts: * The monadic interface does not really do a good job at compression. It essentially eliminates the `$` and the `$ def` of the second example, and removes the `tvar` which is need to avoid type ambiguities. However, the cost for this modest compression is a duplication of the interface, with more complex types. I think the same compression can be achieved with a pure interface. * `layout_title` should be compressed to just `title`. The layout part can be inferred from the types if necessary. * `plot_lines_values` should be compressed to an empty string, instead using the values as an argument somewhere, because the thing to be plotting is almost never a default argument. * The camel case for `setCase` clashes sorely with the underscores used in the rest of the API. It should be `set_colors` if anything, but I would absorb it into something else entirely. [1]: https://gist.github.com/HeinrichApfelmus/3e5caee296e3543d876e
It doesn't look quite like that now though. Looks better, and I am not sure the records work like that.
&gt; I have yet to find a way of dealing with source locations (i.e. keeping them around in the parse tree for use in higher-level error reporting) that doesn't make me uncomfortable at the amount of boilerplate I use Earley and I traverse the entire parse tree in a State monad, keeping track of the location as I go. It's tedious and boilerplate ridden. However an easier way to deal with this would be to pair each input character with its location; if you need to know where a production started, just dive through the syntax tree until you get to the first character. I haven't tried this yet though.
There are many dataflow programming languages, I am not sure they are very related to this.
Wow, lots of info there. Thanks!
I'm working on getting an arm port of ghc 7.10.3 to alpine linux. Finally managed to start building stage2 binaries that run this weekend! (well "run" is a relative term here but still) So there's that at least. Should help out some in the future with my crazy science experiments. https://github.com/mitchty/alpine-linux-ghc-bootstrap/issues/8 If you want to follow progress (which is slow, cause building on arm is slow, like frozen molasses glacially slow).
I'm not very convinced by the "Avoid Template Haskell" argument. I don't get why Haskellers tend to be so allergic to meta-programming.
`polyparse` is nice in that it doesn't need `try`, but I've never used it for anything bigger than exploratory. As for recommendations, if you're designing a grammar, use `happy`; it's the only one of those tools which will help you debug your grammar, detect ambiguity, etc. If you already have a grammar (possibly an implicit one), I'd stick to `parsec` until something definitively better comes along, though using the `parsers` interface is also a good idea, since it makes it easier to swap out backends.
There's a better version now (just merged into master on github). But it still might not make any sense. 1. The example is in app/Main.hs. It demonstrates making a "builder" from scratch. This builder is not useful: it just builds types from user input of each field. But it demonstrates the basic idea, that a complex type (like TestNested) can be assembled from builders for the primitive types and some code to handle sum types. 2. If you run main, you can see that values get built and that the information you might need for parsing (expected type, field name if present) is available to each builder. 3. The builder type (BuilderEx) and the way primitive types are built (simpleBuilder) is completely flexible. So BuilderEx could be Parser and simpleBuilder could just be a one-field reader (liftOpt? I don't know optparse-applicative that well). 4. If I get a chance in the next couple of days, I will look at optparse-applicative and see if I can get a POC working. I needed to do the work simplifying the types first. So look again--if you would--things are somewhat clearer now. And I'll post again if I have a useful optparse-applicative example. Adam 
Wouldn't it be better to extend cabal with the hpack functionality (globs for example), rather than creating a completely new file format? 
If I want to implement fp language like [this](https://github.com/dotneter-/aeiar). Would docs in this repo be enough, or it would be better to read something before it?
I couldn't understand bound. :)
It out to be totally doable, but you should know some type theory first. A note: you describe an "empty" type but what you're actually describing is a unit type -- a type with one element (but no data contained inside it, like a length-0 tuple). The name "empty type" is usually reserved for a type with zero elements, which in Haskell is usually called `Void`, defined w/o constructors, as data Void
Heh. Well, obviously I have a lot more invested personally in Haskell than my boss does. But the reality is that we need to build up a much bigger team.
Absolutely. If you are one of those, or if you have some you can send our way, please do!
We should rather concentrate the focus energy on Stack instead and replace the archaic `.cabal` format by the more modern YAML format ASAP. By improving cabal we would only delay the inevitable switch-over to Stack and needlessly keep the community fragmented till this happens
[removed]
Filepath can be shared by all lexemes in the same file. It will add a one pointer to each lexeme. There is an overhead especially for short tokens (like variable name `x`), but still manageable.
Note that the video is old and that the visual design and color scheme of Lamdu has improved a lot since then :)
Is there any higher-order unification? That's the part that would interest me most.
The server is cool, but maybe you could demonstrate how to render to SVG using some Node library or whatever, to show that it can be used noninteractively. 
But the whole hackage pretty much relies on cabal at this time. Do you plan to teach hackage the new format? Or is the plan to phase out hackage and replace it with stackage?
It can't be used noninteractively. The browser does the rendering and the browser is not allowed to do things without the users actively requesting it. I'd have to either use a Haskell library to render it or install Node.js (I'd like to avoid that dependency).
I don't do any higher-order stuff in SimpleFP, but one could imagine such a thing!
If you want to fork and make a version that uses Bound, that's fine, but I don't want Bound in SimpleFP. I don't find it to be easily understandable, so replacing the ABT representation I used with Bound would make SimpleFP less useful for learning, in my opinion.
Growing the number of people and the code base. :)
I'm be interested to see how concise an equivalent plot would be in matlab (or something else). &gt; layout_title should be compressed to just title How would you get this overloading to work, given that the plot itself, the axes, and other elements all have titles? Can you point me to some example code that acheives this sort of overloading with lenses? &gt; The camel case for setCase clashes sorely I used to have a convention that record fields are snake case, and values and types are camelcase. I liked the way that accessor stand out, but I guess it's not to everyones taste. 
Referential transparency and laziness help a lot. 
Oh, I've always wished it was possible in cabal.
Why not? [Hackage's makover is overdue](https://www.fpcomplete.com/blog/2015/03/composable-community-infrastructure). Just as we ditched `cabal` and started from scratch implementing Stack, why not do the same with Hackage? Then we could add warnings to Hackage that it's being phased out in favor of Stackage. As well as add warnings to `cabal` urging the user to upgrade to Stack(age) as soon as possibole.
I was wondering if there is an IRC channel where people who want to have conversations about Haskell in production could have discussions. If there isn't, someone should start one :)
I've never tried to install it using stack, which is what Gabriel recommends here. You tried with stack and still had problems?
I agree, I had the impression it was based on a survey or similar, not that it was the sole opinion of the author.
Anyone have an experience using `glue`? It's something I've been wanting in Haskell for a while but I've not really tried digging in yet. I'd love an experience report!
He is correct though. the hdbc-odbc driver is alsmot useless, has major bugs and performance issues. I'm talking on linux of course. I do not have windows to test it. 
We're very much sold on the yesod ecosystem: it's well designed, more documented than anything else out there and caters pretty well to our needs. Interestingly enough, we haven't had any kind of scaling issues with Haskell in our application tier. Up until a couple of mil users we were happy running on a single 8 core EC2 instance that was barely going beyond 20% CPU utilization. Eventually we added a couple more for resiliency. We're not especially compute-heavy, more IO-bound than anything else, but that's the case with most web applications. Wrote about that before: http://tech.frontrowed.com/post/137761195996/scaling-beyond-65k-reverse-proxy-connections-on
I think the easier way to show this is to link to how `morte` uses `Earley` here: * [Morte parser](https://github.com/Gabriel439/Haskell-Morte-Library/blob/master/src/Morte/Parser.hs) I attach source locations to each lexed token and then use that plus Earley's set of expected tokens to create the error message.
Does anybody know what happens to Matthew Mights' [Parsing with Derivatives](http://matt.might.net/articles/parsing-with-derivatives/)? Their [ICFP 2011 functional pearl](http://matt.might.net/papers/might2011derivatives.pdf ) was a very interesting read, partly due to the buzz generated by the draft title (of the same paper) **Yacc is Dead**. They also have a Hackage package called [derp](http://hackage.haskell.org/package/derp) filed under "Parsing" category, which didn't seem to have gained much traction after that paper. But still, it should support **Context Free** and **left-recursion** and **ambiguity** out-of-box, with reasonable efficiency. 
&gt; http://tech.frontrowed.com/post/137761195996/scaling-beyond-65k-reverse-proxy-connections-on This is exactly what I was interested in! Nice to see that. 
It worked for my use case for many years. I don't think its great, no. But I was only expanding on the point that it _exists at all_ which many people don't realize, because they go looking for oracle-specific connectivity or the like and don't go looking for odbc drivers. The claim I was correcting wasn't that the bindings were immature, it was that database support as a whole was immature because _no_ bindings existed... And in another sense they're very mature bindings, as in very old and battle-tested ones. Just ones with a bunch of stupid bugs. (On pt 3. I think the difference between the bindings and using e.g. sqlcmd is that the latter just has a lot more retry logic wrapped around everything to begin with, so i don't know how much to chalk that up to a bindings issue, and how much to a 'yuck, odbc is a pain' issue.)
There are plenty of examples in their repo, the parsers just end up with more line noise.
Okay. I added a builder for optparse-applicative. If you look on github now, you'll see it in the "optionParser" directory. If you look in Main, you'll see how simple it is to make the parser from the type (basically your example). The builder also supports making sum-types in fields into optparse-applicative commands. If you want to see how it works, the builder details are in "OptPABuilder.hs" Let me know if that is what you had in mind/makes more sense! Adam 
Have you considered training people in Haskell on the job?
I write Haskell at work for client-facing projects, with scale in mind, thought I don't have enough users yet to put that scaling to the test :) I love it and wouldn't do web development any other way.
First is choosing Haskell itself, studying the multithreading story and profiling. Then little things like keeping the performance rules of thumb in mind (avoiding strings, spotting laziness issues, using the right data structures for my usec case, thinking about which loops will be inner loops and how to optimize them; and the flip side of keeping non-inner-loop code readable and refactorable), and considering which parts of state should be stored in the session or behind a database lookup. There's nothing here that's particularly scale-conscious - I mentioned scale more to qualify my answer (since your question mentioned scale). The main benefits I'm feeling aren't necessarily about performance - it's the other things that really make me miss Haskell in the ~10% of my time spent writing in other languages: - Being able to learn new domains by looking at a libraries' types and documentation (this is huge since I'm kind of a web-dev newbie) - Fearless refactoring, few run-time errors - Fearless concurrency - Really fun dev toys edit: Almost forgot another huge Haskell factor - being able to put code down for several months, then come back to it without much time wasted relearning it. Totally invaluable for a single-dev, several-project setup like mine!
I'd race you if I didn't have a Lambda Jam proposal to prepare :)
Interesting, why can't you? Something to do with the structure of your languages?
Heh, well I know what STM is, but assuming that a random reader does is not what I'd consider an entirely safe assumption. I'd suggest that you perhaps link to the [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/index.html) web page, where folks can read all about it if they're not familiar with the subject. The haddock for stm is... not quite helpful on its own. As to roguelikes, and ones in Haskell, I've got a big interest in them. At the moment all I've got kicking around on that front is a package I've decided to call [ludolib](https://github.com/Lokathor/ludolib), which has some RNG helpers to roll dice in different ways, a PRNG of its own, and an FOV calculator. I'm also in the "you have to build it yourself" zone, which is why I've taken a detour to work on some [curses binding](https://github.com/Lokathor/hexes) as well. Once my curses package is more complete I'll be able to go back and add more features to ludolib, and eventually end up with a whole game thing perhaps. I'll star your project and try to keep it in mind. You can PM me on freenode most of the time when you've got a draft you want me to read. I'm logged in most of the time, even if I'm away from the computer.
I'm surprised this works so well! This is really neat.
It's not a recognized bug. I'm just declaring it a bug. In a language where records are immutable, there should be an optimization pass for very large records, so that large records are not bound to use a large continuous memory region.
You are mistaken. I have never used Scala. And yes, haskell even with dicy odbc drivers is miles ahead of my dreaded experience with java. 
Enforced scope works really well when you know the number of bound variables you need at any given moment. So its fine for, eg, lambda's pi types, etc. But for case expressions or record types, the number of bound variables is arbitrary, and determined at runtime, so you need full dependent types to index by runtime-computed data.
Interesting, must have been another haskell redditor taking the SQL Server &gt; ODBC route. I know they were using Scala but jumped ship from JVM for Haskell, a move that seemed questionable at the time given that they were entering uncharted waters migrating a Scala production app to Haskell with a SQL Server backend.
I must be 12, I read this as fornication, then did a double take and read it the same way again.
&gt; A computation is given a name using a fixed point (not fixpoint, dammit) operator Damnit, not dammit, damnit! Also, what a waste of a post. Of course programming is taught like shit to thousands of students every year. Every subject is. Sturgeon's law is universal, and computer science education is not exempt. 
right? even better, cutting :: ((a, b) -&gt; c) -&gt; (a -&gt; b -&gt; c) cutting = curry cuttingRe :: (a -&gt; b -&gt; c) -&gt; ((a, b) -&gt; c) cuttingRe = uncurry The connections between type theory, programming languages, and category theory are absolutely amazing.
The URL stopped working for me, I think the post was moved to here http://www.parsonsmatt.org/2016/02/22/proving_with_types.html.
&gt; If it's going to be a gripe about education, then there are bigger issues than misconceptions about recursion. I think Prof Harper ran across this one most recently, one of his classes his discussing recursion at the moment. As for his writing style, he tends to redefine terms as he goes so that the definitions he's using are internally consistent but outwardly confusing. This leads to some statements like the above which just seem absurd. Also frankly I think he likes some of the statements on his blog to be.. shocking. Sometimes this is not so fun to read.. The core point of this post is just that recursion is about self-reference and if you implement this in some ways complications arise but all of those complications don't *define what recursion is*. Fairly reasonable in my opinion.
There are quite a few examples at https://hackage.haskell.org/package/fltkhs-demos. Feel free to email or DM you have any questions.
The move in the post from P =&gt; Q to ~P v Q is no good; this is a distinctively classical move. (Since P =&gt; P is straightforward, this would give ~P v P.)
oh shoot, you're totally right. That equivalence is precisely the banned De Morgan that I was talking about. Intuitionistic gets us: P =&gt; Q === ~(P ^ ~Q) which is not quite as pleasant. As it happens, the `(P v A) =&gt; A` is sufficient in a constructive sense, I think, to determine that the proposition isn't true. I've fixed the post now. Thanks for the correction!
Why is classical logic no good here?
Keep it simple. If you're comfortable with emacs, use `haskell-mode` w/ `flycheck` (using `haskell-stack-ghc` checker). Expand out your tooling as you learn more haskell.
ReadtheDocs doesn't support HTTPS properly, sorry :\
Lol autocorrect corrected currying to cutting. I ment to say currying. I think we should write the type signature as forall a b c. ((a,b) -&gt; c) -&gt; a -&gt; b -&gt; c Because the real power of the proposition in the types stems from the universal qualifier. I know that the only possible implementation (ignoring undefined) is curry. Sometimes I wish that all type variables had to be universally qualified.
Well, I can't exactly reject `A v ~A` because I'm sticking to intuitionistic logic only to have used double-negation! [Apparently,](https://en.wikipedia.org/wiki/Material_conditional) you can go from `A =&gt; B === ~(A ^ ~B)`, but to move the outer negation in requires the Banned De Morgan.
(oh lol i assumed that was another `cutting` i've heard about in logics and is actually totally unrelated -- damn parametricity) The required explicit forall is how PureScript works
This reads a lot like "I am angry that people shorten the phrase Recursive Function Call to Recursion"
I use Spacemacs for Haskell development on Mac and I really like it. I don't have it pimped out with all the available haskell-mode features yet (although I hear that it's certainly doable), but it is still quite good. 
I think for someone coming from a PHP background Yesod is a great choice. The best thing is the **documentation** - IMO it's the best documented framework at the moment (there is even the [book online](http://www.yesodweb.com/book) with quite a few example projects) and you can get a working start-point with something like `stack new MySite yesod-postgres` out of the box. Authentication and authorization is quite easy and once you understand the way yesod handles settings with type classes it a really consistent framework. Strangely the only thing I don't enjoy as much (I'm doing lot's of ASP.NET stuff) are the Hamlet templates - if you are used to the way you can write HTML with tooling support this *half* HTML (literally) can feel quite stupid (I don't know you but I like to test/write the plain HTML/CSS first - or when I am lucky have some designer provide this for me - and then extent it and fill the holes - with Hamlet that basically means heavy editing the HTML Also it's really easy to setup [`keter`](http://www.yesodweb.com/blog/2012/05/keter-app-deployment) with it which you might enjoy if you want to publish to your web-server with a simple `yesod keter` ;)
The original Grammar of graphics book has been around since 1999 I think.
&gt; Well, interspersed with the grumblings about why this shouldn't need to be said is a pretty good explanation The problem is that the only readers who benefit from that explanation are ones who don't understand recursion. And he's so busy insulting and yelling at those same people that there's little chance they'd read this.
By "runtime" you mean the runtime of the compiler?
Someone can correct me if I'm wrong, but I think Hindley-Milner corresponds to some kind of intuitionistic logic (a quick search led me to [this comment thread](http://scienceblogs.com/goodmath/2009/11/17/types-in-haskell-types-are-pro/).) It makes sense intuitively; I know what sum types are and I know what disjunction is; I know what product types are and I know what conjunction is; I know what function types are, and I know what implication is. But I have no idea what negation in CL would correspond to in HM. I guess it also makes sense to stick to constructive proofs (on the logical side). Again, I have no idea what kind of program a nonconstructive existence proof would correspond to.
To expand on that, [here](http://spacemacs.org/layers/+lang/haskell/README.html) is a guide on how to setup your environment on spacemacs. Make sure to use stack and not cabal for everything you need to install.
Do we have a Docker image for this?
&gt; `parsec` is still a "free" dependency since it comes with ghc by default /u/Tekmo, are you sure `parsec` still comes with GHC? Afaik, the last release to bundle `parsec` was GHC 6.8 :-)
It seems trivial to be able to parse a vanilla HTML file and remove the closing tag ? You'll still have to do the indentation, of course.
elm?
I might have kept at it longer, but the build times got to me and I gave up. It was taking minutes and minutes to build some of the packages each time I tried a different thing, and that just saps the effort out of you fast.
I also tried with stack but "new enough node" was the only issue I had. I just downloaded the latest version of node and put it in the path (no ppa needed, no compilation required).
Seems reasonable, I'm looking forward to trying this out :)
A while back I wrote a tool that would do some basic reasoning on the data types and function signatures in your program, and warn about the ones that were necessarily partial. Not terribly useful, since you can just grep for `error` or `throw` or whatever, but it was a fun exercise. I should clean it up and publish it someday…
Negation is always A -&gt; Void, even in HM. But the problem is in HM you can't go from (A -&gt; Void) -&gt; Void to just A, although that type might look familiar if you introduce a little parametricity: forall c. (a -&gt; c) -&gt; c Classical logic and continuations are related.
No, are there any good docs or tutorials for it? When I look at the Haddocks for it I have no idea how to use it. 
[removed]
Are you saying that `() \/ A =&gt; A` is a valid proposition? `()` in some sense represents truth so if anything we have `() \/ A =&gt; ()` but you can't just eliminate the disjunction like that.
If you don't have too much experience programming, I would consider [Programming in Haskell](http://www.cs.nott.ac.uk/~pszgmh/book.html). It won't be the only book you will need, but it's the book that made me finally "get" some concepts. But we're all different, I don't know if it will be right for you. The ones I have are Programming in Haskell, Learn you a Haskell and Real World Haskell. They're all very different, and I'm glad that I have all of them. I started off with RWH, but in hindsight that was a bad idea. It's a good book, but it isn't very good at introducing the fundamentals of the language.
it is trivial but after you cannot but you loose most of the help editors would give you with HTML - Lucius is much "nicer" in this way - but that's just my first impression after a few days using it to do more than a test or an example
Haskell Programming from First Principles is definitely the one you want: http://haskellbook.com/index.html
+1 for Haskell for Mac. Also, I can report that Atom with haskell plugins (ide-haskell, all required command line tools installed, etc.) provides a really nice experience.
Because we have different aims, and because I've read HFFP quite a few times. Granted, he didn't know I'd read it. He also recommended I read "On Writing Well", one of whose core proposals is for writers to be as brief as possible. HFFP is not brief. Irony! :) Perhaps it's not irony?
I guess advertising the fact that you need more Haskell people more widely and opening for remote would both get you more programmers than you'll ever need.
I second the vote for Programming in Haskell. It's like the K&amp;R of Haskell (and I've been using Haskell for over 20 years now...)
FWIW, you can read LYAH online for free over at http://learnyouahaskell.com/chapters if it's any help deciding. Two other great books you may be interested in, and which are freely available online as well: - [Real World Haskell](http://book.realworldhaskell.org/read/) - [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/index.html) 
I don't understand why he pretends to be surprised, but I don't understand how pretending to be surprised is pretentious either. Wouldn't someone pretentious pretend to know more than others, instead of pretending he didn't know how compilers tend implement recursion? *edit*: okay, I got downvoted, so the explanation is probably so obvious that downvoters think I'm also pretending. Care to explain the obvious anyway?
I found Elm very restrictive. Last year I tried converting one of our applications to Elm, Purescript and GHCJS/Reflex. Even though I did not finish the conversion, the result in each case was better than what we had before. The one I had more trouble was Purescript, because at the time there wasn't even a package manager, the tooling was entirely dependent on JS. Reflex was my favourite, but setting up GHCJS was nightmarish, particularly because people here are running Windows. In the end the project was postponed, and I'm trying to get people motivated to try it again, now that the ecosystem has improved.
I would definitely recommend Haskell from first principles. The book is targeted for nonprogrammers to be able to learn Haskell, and does a pretty great job of it. It's a pretty huge book, covering twice as much as LYAH in more depth with exercises.
[removed]
&gt;Because we have different aims, and because I've read HFFP quite a few times. I don't think that makes any difference to the content of what you were proposing he add. &gt;Granted, he didn't know I'd read it. Again, I don't think knowing this changes anything. Listening to Bach when I was younger didn't make me a composer, but it did spur a brief dalliance with composition software and MIDI. &gt;He also recommended I read "On Writing Well", one of whose core proposals is for writers to be as brief as possible. HFFP is not brief. This is uncharitable. HPFFP is not brief in terms of page count because it: 1. Covers a lot 2. Has a lot of code and examples. Strip the latter out and match coverage and HPFFP is quite a bit shorter than RWH. There is not much more prose in HPFFP than is necessary to cover the topic at hand. The chapters start shorter when we initially write them and it's because we _test_ our material that content explaining things more clearly gets added or existing material gets rewritten. &gt;Perhaps it's not irony? I don't think so, no. Please do not use us or mention us when promoting your webpage.
[removed]
Oh, nice, thanks for pointing this out! When I wrote the operational monad tutorial, I was aiming to make the technique more popular, hopefully answering questions such as the ones by the OP. The technique itself did not seem new to me.
Did you try to make a production-grade GUI in Haskell (not HTML) ? It has been an almost endless source of frustration for me.
I second this. This is the best book for learning Haskell. 
I'm not so sure that named arguments are that great sure they look good when you write Rect(width: 100, height: 5, xPosition: 20, yPosition: 50) BUT you're not supposed to use 'magic numbers' in your code so if you follow this rule, you now have Rect(width: my_rect_width, height: my_rect_height, xPosition: my_rect_x, yPosition: my_rect_y) now named parameters doesn't look like a good idea anymore! IMO strong typed interface are better: Rect(100, 5, 20, 50) would be a compilation error, you'd have to feed the proper types Rect(width(100), height(5), xPosition(20), yPosition(50)) this way, this is clear with numbers AND with variables: Rect(my_rect_width, my_rect_height, my_rect_x, my_rect_y). 
Ah, I see. Thanks for the explanation.
I disagree. Other than data-dependence and whitespace sensitivity, I can't think of a single common use-case of grammars that are not context free. Much of the time, parsers are written for simple configurations or as a replacement for complicated regexps, both of which tend to be context-free; most of the rest of the time, it's to parse a complex file format, but those tend to be specified using a CFG, usually in some form of BNF.
I wouldn't call any book poor just because it doesn't have practice problems. I think LYAH covers all the basics you absolutely need to understad so that you can to dive into Haskell and it's well written (my opinion). I don't think that Haskell can be learned by reading a book, if Haskell Programming from First Principles is 10% better than LYAH or vice-versa is irrelevant. You learn the language by reading code other people written, experimenting in ghci and writing your own code. It doesn't matter which you pick, I picked LYAH as mentioned above and was happy with it.
For some people that's an issue but I found it easier for me to go through and read it, reread it, and then work on some toy projects. It really just depends on what's your learning style. If you need practice problems then lyah isn't for you.
Whoops, you found a typo. I'll fix it.
I don't think this answers my question. Can you elaborate?
I was confused by the use of "and-elimination" as well. I asked about this above, and your post answers my question!
I haven't heard of this book until now. Does it really have to be 1,165 pages with an estimated 1,300 pages when it's completed? Seems a little ling-winded. The longest book on my shelf right now is C++ Primer at 938 pages. 
If you have some time and find this stuff interesting, Pierce's [Software Foundations](http://www.cis.upenn.edu/~bcpierce/sf/current/) is an excellent, free, pleasingly hands-on intro to theorem proving using dependent types. Beyond the concrete subject matter (Coq), it has taught me more than all university courses combined about proof techniques (and a bit of type theory on the side).
Has to be? Depends what you need it for. But. .is not like it's long enough to cover everything even at 13 hundred pages, skip sections you aren't interested in.
Maybe? I really wouldn't recommend it. It's looking at the basics of the language from a very advanced place, so it's kind of mismatched from a learning point of view.
If `parsers` had `megaparsec` instances, would you still use it?
Oops! I don't know where I got that impression then because the earliest version of `ghc` that I used was `ghc-6.12`. I'll correct that. Thanks for letting me know! :)
I totally agree for this example. My point is more like... "it's not too hard" and "proof sketch" are what we're trying to avoid when moving from pencil and paper proofs to proof assistants. That's all! Of course this example we can say is trivial.
Unless you're going to say two words, "dammit" is correct. "Damn it all to hell!" "Listen, dammit!"
I explain the main reasons [in this issue](https://github.com/Gabriel439/post-rfc/issues/64) but I can summarize them as "it needs more polish".
Nice to see more Haskell jobs coming up. Cool to see you're using Opaleye (author here). How are you finding it?
He is speaking correctly but unhelpfully: I doubt a novice learning C as his first language would come away from reading that article with new understanding even of a fact Harper rails against (the teaching of function calling as always using the stack, where it doesn't have to) that is highly relevant to him. He takes his points to be so obvious that material arguments that might convince me are missing, for example: what are the tradeoffs of implementing functions without using the stack as a default? In an academic supposedly attempting to spread truth, this looks like buggy behavior. Makes me wonder if he is trying to spread truth, or do something else I do not understand. Willfully ignoring reasonable empirical claims like "explaining it through a stack helps some students" is also a bug, IMHO. 
I am sure something like this does have a future. But currently the "new programming experience" (at least for me) is such that I tried half an hour to enter the equivalent to: fibs = 1:1: zipWith (+) fibs (tail fibs) but couldn't figure out how to construct a list. Neither `:` nor `cons` or something I could think of worked. From a philosophical point of view I'm of the opinion that all the attempts to avoid text are a failure. This doesn't mean that a clever presentation / AST editing could not be valuable. Just because you have an AST editor doesn't mean you can not or should not store the edited artifact in the best representation we have: human readable text, rather than computer readable text (this is what an AST is, in the end). Therefore, I guess that languages that allow to switch between text and AST editing will win in the end. (Luna lang claims to do that, but I haven't seen it yet.) 
It looks like the [named-argument interface](https://metacpan.org/pod/distribution/Marpa-R2/pod/NAIF.pod) does let you use Perl to generate the parser.
It's not necessarily a gripe about education but perhaps the muddled concepts surrounding recursion.
In case of Lambdacube-gl build succeeded, but documentation [failed](http://hackage.haskell.org/package/lambdacube-gl-0.4.0.2/reports/). According to [logs](http://hackage.haskell.org/package/lambdacube-gl-0.4.0.0/reports/1) haddock failed to parse a comment: src/LambdaCube/GL/Data.hs:73:69: parse error on input ‘-- $ Image w h $ SV.fromList $ pixelFoldMap (\p -&gt; let PixelRGB8 r g b = convertPixel p in [PixelRGBA8 r g b maxBound]) i’ So you probably should notify package maintainer. Also I believe haddock should not fail here, so probably it makes sense to notify haddock maintainer too. Often docs are missing because hackage builder is broken or just slow (it may take days for it to process new package).
Sorry, my mistake, I overlooked something.
If somebody gives you the type `a -&gt; a` don't you still know what it's implementation must be?
The `parsers` API is a *lot* better, especially the `Text.Parser.Token` module. Unfortunately, as we need custom whitespacing in `language-puppet`, everything needs to be newtyped, which is annoying (as it's typeclass-based). An example of better API is the [reserve](https://hackage.haskell.org/package/parsers-0.12.3/docs/src/Text-Parser-Token.html#reserve) function.
`hpack` (and `stack`) has luxury to be opinionated! and depend on non-trivial dependencies. E.g. you [cannot (yet?) specify license-file](https://github.com/sol/hpack/issues/73). Also for globbing `hpack` uses `Glob`. Even `Glob` doesn’t have extensive dependencies (only `dlist`), it’s still a burden for bootstrapping GHC (as GHC needs Cabal to build core libraries). BTW, Cabal (recent ones) support some kind of globbing (not deep, pattern must have trailing extension IIRC). Anyone can help it: https://github.com/haskell/cabal/issues/3178!
I am aware, that's necessary for any Turing Complete type system. That doesn't make banning that type useless. It it would take some research to see if it tended to catch errors usefully or got in peoples way more.
That seems to line up with what /u/Yuras said about builds taking up to several days. It was uploaded just a few days back, so that probably explains it. However... if you just want the old docs, if you click on 0.5.1.0, you'll get the last docs. If those are close enough to the latest API for your purposes that is.
I have a bunch of Haskell books. It is difficult to say which is best - all of them are helpfull. If I had to choose one though it would be "Programming in Haskell" by Graham Hutton.
My choice also.
Page count hasn't featured in my assessment criteria. It is easy to read and easy to understand. The material covered is fresh. The presentation of the material uniquely manages to not presuppose much while simultaneously being deep enough to be useful. I think it's well worth the fifty bucks and am looking forward to more chapters even more than Game of Thrones and Star Wars VIII.
&gt;No, are there any good docs or tutorials for it? Well...our [haskell book](http://haskellbook.com/progress.html#parsers), but otherwise, nothing thorough. Learning to muck about with `parsers` and `trifecta` is about it. There's not much example code and the way Idris et al. use it is idiosyncratic and hard to understand whe you're starting out. /me checks his notes This thread lists about all you're going to find: https://www.reddit.com/r/haskell/comments/2uc6kp/are_there_any_tutorials_or_introductions_to_the/
It's a big project, and won't be fully general: CSS classes added dynamically won't be included. If you want a general solution, you'll want to check Firefox [1], or Chrome [2], or automate an actual browser instance [3] (with Haskell! [4]), or use an existing tool [5]. 1: https://addons.mozilla.org/en-US/firefox/addon/dust-me-selectors/ 2: http://techglimpse.com/chrome-developer-tools-improve-site-speed/ 3: http://docs.seleniumhq.org/ 4: https://hackage.haskell.org/package/webdriver 5: https://davidwalsh.name/uncss 
`ghc-mod` may do it nicely. Other than that, a simple script that looks for file changes and runs `ghc` would probably do it.
``` stack build --fast --file-watch --exec build-stuff ```
You'd better take hamlet and co instead. import Text.Blaze.Renderer.Utf8 (renderMarkup) render :: Markup -&gt; ActionM () render = raw . renderMarkup ... get "/" $ do beam &lt;- param "beam" render [hamlet| &lt;h1 #siteMain .header&gt;Scotty, #{beam} me up! &lt;p&gt;Or else... |] (or something like that, typing from memory) 
It's chugging along nicely, you can track its progress in the [`nix-local-build` branch](https://github.com/haskell/cabal/commits/nix-local-build). This will be available in cabal-install 1.24 as a tech-preview. Currently, patches are being extracted from the `nix-local-build` branch into pull-requests against `master` for code-review. But given the large amount of new code and features which Duncan et al. contributed, it takes time to cleanup the patches and review those thoroughly before merging to `master`. I expect there will be a blog post describing how to opt into the new tech-preview features. The UI needs some polishing, and we'll need user feedback before we can make this one cabal's new default for everyone. I'm already using this new mode of operation of `cabal` almost exclusively myself, it works well enough for that. And it's particularly useful for buildbots with multiple jobs where you want to share non-local packages from Hackage, while at the same time avoiding broken packages in your shared package database.
May I ask for at least one Screenshot on the Github page?
To be honest I didn't expect this thread to stay vacant for so long so have some praise from me :) I haven't tried *FLTKHS* yet, but as Haskells GUI story is really lacking compared to other languages, every effort to improve it is highly appreciated. Keep up the work on *FLTKHS* and I'll be sure to try it when I get some spare time :) 
`-- $ Image` is lexed as a haddock comment, but the parser doesn't expect a haddock comment in that position. From GHC's [Lexer.x](http://git.haskell.org/ghc.git/blob/a3e0e93:/compiler/parser/Lexer.x#l351): -- Haddock comments &lt;0,option_prags&gt; { "-- " $docsym / { ifExtension haddockEnabled } { multiline_doc_comment } "{-" \ ? $docsym / { ifExtension haddockEnabled } { nested_doc_comment } } Where `$docsym = [\| \^ \* \$]`. `haddockEnabled` is `True` when you run `ghc` with the `-haddock` flag.
I'm unfortunately not particularly familiar with Agda, so can't recommend any. However, the basics of Coq/Agda/Idris should all be relatively similar, with major differences to be found primarily in syntactic conveniences, libraries and the use or non-use of a separate tactics language for constructing proofs. (Someone correct me if I'm wrong.)
That's high praise. I'll consider purchasing it then. 
&gt; (1) Is parsing the right way to approach the problem? Yes, I would approach it as followed: 1. Parse the CSS source string into an AST (abstract syntax tree). 2. Analyze the AST to find out which selectors are used. 3. Prune the AST by removing unused selectors. 4. Pretty print the AST back to a CSS source string. On hackage there is a library called [`css-text`](https://hackage.haskell.org/package/css-text) providing a CSS-parser, CSS-AST data structures, and a pretty printer for the AST. I haven't tried it yet, but it should solve point 1 and 4, leaving you with the static analysis and the subsequent AST-modification. &gt; (2) If it is, which parsing library would be most appropriate? There was a recent discussion about general Haskell parser libraries [here](https://www.reddit.com/r/haskell/comments/46u45o/what_is_the_current_state_of_parser_libraries_in/). But in this case, you could also use an existing CSS specific parser as provided in `css-text`. &gt; (3) Any other sage advice. If you haven't written a parser before, I'd suggest either using something like `css-text`, or writing a few simple parsers beforehand. Debugging a big parser can be quite a pain, especially if one is not used to the parser-library's quirks.
Sorry for taking a while to reply, I've been busy dealing with the deluge of emails I've received! It's early days in terms of our Opaleye usage, so you should probably ask me this in a few months :) I've used it personally though, and have been very happy with it, which is why I've been happy to choose it.
Is that your exact code? Where are you running this from? Try adding a `hFlush stdout` after the first print. Does that work? 
Often flushing happens implicitly on a "\n", so you could try `putStrLn` instead.
What /u/pdexter said should work. You can also try hSetbuffering stdout NoBuffering (before this piece of code).
There is a quasiquoter that turns any quasiquoter (like `hamlet`) into one that reads from a file instead. Would that do?
I'm not deech, but this may be insightful: https://groups.google.com/forum/#!topic/fltkcoredev/i3uskllEp8Y In otherwords, it doesn't seem as though there is, but there's a help view widget for rudimentary HTML. Disclaimer: I haven't tried it.
Um, that's not how `unzipWith` should work. `unzipWith` should take a function from `a` to a tuple, and a list of `a`s, and produce a tuple of lists, which is most certainly not the same operation as `map (uncurry (||))` - which would be more like `uncurry (zipWith (||)) . unzip`. I suspect that `unzipWith` is just considered unusual enough to not be in `Prelude`.
You don't agree on what `unzipWith` is supposed to be. unzipWith :: (f : a -&gt; b -&gt; c) -&gt; [(a, b)] -&gt; [c] vs. unzipWith :: (f : c -&gt; (a, b)) -&gt; [c] -&gt; ([a], [b]) The second one is actually compatible with [`unzip`](http://hackage.haskell.org/package/base-4.8.2.0/docs/Prelude.html#v:unzip) in the sense that `unzipWith id == unzip`.
Good grief this thing must be amazing if people trudge through all this just to figure out how to use `trifecta`. (Hopefully your book will help but that's now how people have learned it.) There's no way I could have known that I was supposed to look at `parsers`. I haven't looked at your Haskell book before because I know most of what's in it, but now I will probably take a look. The Performance chapter might also have some good stuff.
~~Ah, so it's not whether the unzipWith which I've seen in the wild does what I want it to do, but whether that definition is the reverse of unzip in base. That incompatibility seems like a decent reason for it at least to not be in base.~~ Except that: Prelude&gt; :t zip zip :: [a] -&gt; [b] -&gt; [(a, b)] Prelude&gt; :t zipWith id zipWith id :: [b -&gt; c] -&gt; [b] -&gt; [c] In other words, it doesn't appear that zip and zipWith are compatible in the way you suggest unzip and unzipWith should be, yet they both exist in Prelude. Thanks for that /u/gallais.
Yes.
This is related to your buffer mode. Often, the default is `LineBuffering` in your terminal, which means it flushs after a newline. `putStr`isn't writing a newline so its not flushed. Set the buffer mode for `stdout` to `NoBuffering` as suggested by Ferdie4 should work. You need to `import System.IO` and put `hSetBuffering stdout NoBuffering` in your IO. 
Thank you! That was extremely useful. Actually was exactly what I wanted to see. &gt; If you haven't written a parser before...I'd suggest...writing a few simple parsers beforehand Do you have any suggestions for simple parsers I could practice writing?
Clearly 'tis the Year of the Haskell in Production.
There's more information here: http://haskellbook.com/training.html
What is fragmenting the community are ungrateful trolls like you. For years, Cabal, with all its problems, was the workhorse of a lot of Haskell programmers, myself included. I'm very grateful for the people that developed it, for without it, I would never been able to become a Haskell developer. I complained about its shortcomings, sure, but that is very different from the infantile hostility you display at every chance.
While I don't disagree with the notion that teaching recursion as an esoteric concept of no practical relevance is bad, I don't find the article particularly helpful in explaining why it's bad. Also, not being familiar with his work I found his notations really difficult to parse and they didn't really seem to add anything relevant.
I feel like Scala is a better middle ground
[removed]
[removed]
I use Nix http://nixos.org/nix/ on the Mac and it's great for Haskell development, using emacs, ghc-mod, hoogle. However you must set aside a few days to learn it :)
Because it's unwanted romantic attention rather than discussing technical merits of her presentation. Sexism basically.
Here's a long explanation by a someone who sent a similar message and later regretted it: http://innuendostudios.com/2012/07/22/an-apology/
This is one of my favourite talks ever. It was an interesting (albeit light) insight into the architecture of a system at Facebook written in Haskell, as well as a great overview of Haskell's strengths. What I really enjoyed though was how it was presented in a really clear and approachable way. Kelly is obviously very smart, knowledgeable and I wish I had an ounce of her talent!
Saying that you *would like* to marry her would be perfectly fine. I personally think you went too far by saying that you are "gonna" and even how you will name your son. It might be intended as a joke, but please consider what would be her opinion about it before you make it. Edit: By the way, since you **are going** to marry her, have you made any attempt at contacting her, or are you just a regular keyboard Dom Juan?
Sweet, downloading it right now! It has closures and lambdas, right? I don't want to use it if it doesn't have closures and lambdas. I can't believe I never saw this on Hacker News. Are they YC grads?
&gt; Saying that you would like to marry her would be perfectly fine. No, it most definitely would not.
Yes, it was the first my thought. But I remember an announcement that haddock now never fails on bad docs. So I still believe it is a bug, probably not it haddock but in ghc, but anyway.
Why? People can't say they *would like* to marry someone anymore? This is silly, women are not made of glass. Your attempt at defending women is more patronizing than his comment. The fact that he bothered to comment here about the talk is enough to say that he admires her, while your comment is so cryptic that implies we should devolve into some paranoid darkness and pretend that everyone is genderless.
Yes, agreed. It was a great link between the two. I've often thought that the academic side might mean it'll not pick up in industry but it's reassuring to know it had the aims quoted from the start.
/u/thomie Oh, never mind. I just checked my emails and found your comment here: https://github.com/haskell/haddock/issues/485 (Sometimes I think that you remember each ticket on ghc tack, open or closed :) )
Yeah, Go and Swift are pretty much destined to overtake Haskell given their current trajectory. Haskell's been pretty consistent in its position (popularity wise, not rank), barely moving from its current spot for years.
I read the slides a while ago. very glad to see there's a video of the presentation! great talk! it was very clear and fun.
&gt; I personally think you went too far by saying that you are "gonna" and even how you will name your son. The point about the son was supposed to make it clear that it was a joke based on the exhaggeration of my reaction to the subject. &gt; please consider what would be her opinion about it before you make it. If some woman said she'd like to marry me in reaction to something I did, I couldn't consider that in any other way than appreciation mixed with a cute joke. Seriously, what could the other way be?
For the record, stack doesn't have anything like this feature, which enables a single db where the same version of the same package may be built against two _different_ versions of one of its dependencies. This provides a superset of the functionality of the "snapshot" approach.
Yes, the lack of a pleasant `do` notation is the problem. You can almost get away with overloading the comma operator but things break down as soon as you want to assign a new variable on some line...
Don't get your hopes up, it's just an academic language.
[removed]
It would be just as inappropriate for a woman or gay man to make the same sort of comment in a thread about a talk given by, say, Simon PJ. Gender really doesn't have anything to do with it.
I admit the son joke made me chuckle. I still think it was out of place, though. But I agree with your second point wholeheartedly, if someone made a comment like yours after watching a presentation I did, I would be very happy. After all, what reason could you have for posting that in a forum like this, in this very thread, if not admiration? If you were being sexist you would be commenting on a pornography forum or something. I rather have your murky humour than the brainwashed pretentiousness that I saw here any day.
[removed]
A good extra benchmark would be to try using `unsafeIOtoST`, `unsafeSTToIO` to transform the `IO`-centric calls to `throwIO`, `catch` into ST calculations. These are a "little" unsound in that the particular exception you catch can be non-deterministic if you catch something you didn't throw inside of the ST calculation, but should dodge all of the `CatchT` overhead completely.
[removed]
What exactly does Nix do?
[removed]
&gt; Consider the degree to which your comment ~~resembles~~ **differs from** a wolf-whistle, or yelling “hey baby!” at a passing woman: not much, and claims to be making an “appreciation” are a common response to criticism of that kind of behaviour. Edited to reflect what I think you meant to say. No factual disagreement here. However... &gt; Consider what it says about what you think is most important about this person: how nubile you think she is. How do you come to this conclusion? I see exactly the opposite; he's saying that the qualities most important to him are the ones on display: intelligence, expertise, good communication. &gt; Consider what it says about your assumed privilege (you're gonna marry her) and what it says about your idea of her right to an opinion in the matter (none: you're just gonna). And come on, this is just being pedantic over idiom. People say "I'm gonna" to indicate hopeful aspiration all the time, it's not a statement of predetermined prophecy. Not to mention /u/nikita-volkov lives in Moscow and has a very Russian name; I don't know for a fact that English isn't his first language, but it's a reasonable assumption. To be clear, I'm not saying his statement was circumstantially appropriate. But people are definitely reading more into it than is there, and this is the last place I want to see a witch hunt brewing.
`filepath` can do stuff like that because the format is similar, but that would be a hack.
Keep in mind that this page count includes the ton of exercises this book includes. While it's probably not a good idea, what I ended up doing was skipping the exercises for material I had already understood and coming back to them when I needed to reinforce my learning. I ended up getting through the book in about a week of vacation (while simultaneously reading other things). I'll probably go back through and start a github repo dedicated to the exercises for extra practice. TL;DR - it can feel long-winded for topics you already understand, but for the bits you are still fuzzy on, the extra detail is very much appreciated, and almost seems necessary. 
[removed]
Yea, we're all ridiculous and you're smarter than us. Can you leave now?
Dude, communication is a two way relationship. The extreme conclusion of what you find most laudable would be complete self absorption and narcissism. Just take the advice and roll with it instead of digging in your heels for an ideal.
This might sound odd, but I'm beginning to think Haskell small and stable popularity is a good thing. There is something inherently suspicious about something that the majority sports, or that it takes by enthusiasm.
Nix is a package manager. It's like homebrew, or MacPorts. But it goes a little further in how it works, which is closer to a lazy, functional language. It also supports the Haskell ecosystem well, and provides binaries of (at least) stackage, so you don't have to wait tens of minutes for lens to install, for instance. If you try it out, it's a simple addition to `.bashrc` to configure your PATH, and after that anything nix does goes into `/nix/store`, so it's also very easy to remove (it won't touch any other part of the system). If you try it out feel free to PM me with questions.
Great talk. Now it's time to try and write some projects in Haskell. Again. I like FP but Haskell can be so frustrating.
Yeah, it doesn't have side effects and mutable variables, right? Who would use such a thing?
Yeah, I more or less agree with that sentiment. I'm cool with Haskell being as popular as it's possible to be while still retaining the ability to make meaningful evolutionary changes to the language itself that might break code. Haskell shouldn't be the language that you can write code in once and never have to worry about it breaking for 10+ years, it should be the language that gets dependent types 26 years into its life. And I think its critical that it maintain that experimental nature even in the event of greater industry adoption.
What sources of friction are you encountering? Any specific pain points compared to what other languages are offering?
Yes, every social space should accommodate every bad faith troll with an axe to grind.
[removed]
I'd imagine this is not the default case because of performance reasons. Imagine calling getLine in a loop, as if you're expecting a file to be piped in to stdin. Now imagine if the output buffer was flushed everytime a line was read. 
Isn't the `ApplicativeDo` example actually one where bind is necessary?
As well as a mob of self-entitled politically correct priests, by the way
[removed]
[url-generic](http://hackage.haskell.org/package/url-generic)
I don't think it's actually possible, because `Generic1` isn't kind-polymorphic. It works for things in `(* -&gt; *)` but our record would be in `((* -&gt; *) -&gt; *)`. I don't think the instances can be written using regular `Generic`, but I'd love to be wrong about that. Darn. It sounded like such a great idea too.
Many of the puzzles in the [Advent of Code](http://adventofcode.com/) require a trivial parser to read the input. Skip on the regex and dive in with parsing. The most fundamental way to get into parsing is to write your own, which will centre around a type somewhat like: newtype Parser i a = Parser { runParser :: i -&gt; (Maybe a, i) } Which says a parser is a thing which takes input (type `i`) and might be able to extract some `a` from it, and will also give back all unconsumed input. Make it a `Functor`, `Applicative`, and `Monad` and write a combinator or two like: many :: Parser i a -&gt; Parser i [a] Which takes some parser and returns a new parser which repeats the given parser until it can repeat no more. Sooner or later you'll also want to make it `Alternative` and `MonadPlus`. A handful of goes around like that and you'll be ready to use the built-in `Text.ParserCombinators.ReadP`, which incidentally has a type like this: type ReadS a = String -&gt; [(a,String)] Which is `String` input producing a list of possible parsed outputs, a more powerful option than the `Maybe` approach above. But you pretty much just use it the same way.
&gt; putStr "Please enter your name: " hFlush stdout name &lt;- getLine putStrLn ("Hello " ++ name) That works. Thank you all.
`a -&gt; a` implicitly is `forall a. a -&gt; a` already. Requiring the `forall a.` is just requiring boilerplate, IMO.
For all the packages which require a tool, ensure you've provided a full path. `haskell-ghc-mod` should have a full path such as `/Users/tmpz/.local/bin/ghc-mod`, `ide-haskell` should have a similar path for `stylish-haskell`. Actually, I think that's it, you don't need `hlint` if you have `haskell-ghc-mod` as your `ide-haskell` backend. 
I used to share the same opinion before Stack came around. Cabal was a nightmare. Try learning a language when you can't even compile example code because some really standard dependencies don't compile. Now the tools are excellent. 
How did you search? Did you search [hackage] (http://hackage.haskell.org) for 'URL'? Did you Google 'Haskell URL'?
Thanks for understanding.
Wardraft is telling you that you can replace the [hamlet |...|] bit with a read from a file containing the relevant Hamlet template. Your question is fair clear, and that was an answer to it. :)
I am a Haskell skeptic, so maybe I should not be posting here, but I watched this talk in good faith, waiting to be convinced, and that did not happen, so I thought someone may be interested to hear why. The main reason is this: like Marco Rubio, the talk was all about dispelling notions that actually have little to do with actual decisions. I don't know how people work in Silicon Valley, but in the rest of the industry we don't sit around asking, "hmm, what cool new tools should we use next?", and then someone says, "well, how about Haskell?" and the rest of the group says, "nah, it's too academic/hard/elitist." In fact, reasons *not* to use Haskell hardly matter; what matters is the reasons why we *should* use Haskell in the first place, and they are utterly unconvincing (see the last 5 minutes of the talk). Shops that don’t consider using non-mainstream languages a virtue in and of itself don’t go out looking for new languages at all. They would use a new language (or any other tool) if they happened to find one which they believed would provide value, i.e. reduce development cost, increase performance, etc. Because new languages have a very high adoption cost (learning not just the language but libraries, getting familiar with their idiosyncrasies and bugs, integrating with legacy code, learning new tools) and high risk (especially if the language has such low adoption), the rewards need to be especially significant to offset cost and risk. Haskell simply does not seem to meet that requirement. Now, don’t get me wrong: I’m not saying Haskell doesn’t have some advantages. I am saying that those real advantages — at least so far — don’t seem even close to offsetting the disadvantages/risk. Until we see that switching to a new paradigm and betting on an immature ecosystem reduces development costs by, say 50% (which was about the industry average cost reduction Java had over C++), there just aren't compelling enough reasons to rationally choose Haskell. And BTW, when I say that Haskell is academic, I don't mean that it's not fit for production. I mean that its advantages are mostly academic (exploring a new paradigm). 
[removed]
[removed]
[removed]
I don't think it's political correctness that elicited the negative response here. I've worked with women who received unwanted attention and who wanted to be appreciated for their technical contributions instead of their appearance. These people were friends of mine. I appreciated their contributions to our projects and I wanted them to feel comfortable continuing to contribute because their insight was valuable. The unwanted attentions they received were embarrassing and cringeworthy and it's the same here. It's a good way to make someone feel like they should no longer contribute, which is a tragedy.
Oh, like the entire type is internal? Ok that makes sense. Somehow I interpreted what you wrote as the constructors were hidden (even though that's not at all what you said).
I mean as long as you can download and use old versions of packages / GHC I totally agree. Sometimes you do need code that works for a long time however so making it not too difficult to keep old code running is pretty important IMO.
Plus, Stack is built on top of Cabal so it gets it for free
 (\(x,y) -&gt; (length y &gt;= length x `div` 2) &lt;$&gt; (do friends &lt;- getFriends; pyFriends &lt;- filterM likesPython friends; return (friends,pyFriends)) On the right of the &lt;$&gt; there is still a do expression: do friends &lt;- getFriends; pyFriends &lt;- filterM likesPython friends; return (friends,pyFriends)
AMA.
If finally desugars to something equivalent to: (\x y -&gt; length y &gt;= length x `div` 2) &lt;$&gt; getFriends &lt;*&gt; filterM likesPython friends
I suspect that *most* of the time is spent somewhere else, than in "business logic", i.e. making the integration requests.
in that case they should stop reinventing Stack features in `cabal-install` and focus on helping Stack by improving the cabal library. Isn't the long-term plan to replace the cabal tool by Stack anyway?
[Link to that moment in the video.](https://www.youtube.com/watch?v=mlTO510zO78&amp;t=19m53s) Some of the time could be spent waiting for responses from other systems, so 3x faster could be a reasonably large improvement. The performance increase could also be due to making requests to other systems more efficiently.
How many lines of Haskell are there in the codebase?
&gt; Until we see that switching to a new paradigm and betting on an immature ecosystem reduces development costs by, say 50% (which was about the industry average cost reduction Java had over C++), there just aren't compelling enough reasons to rationally choose Haskell. The arguments for statically typed FP (I'm not singling out Haskell here) are already sufficiently compelling for some businesses (e.g Jane's Street and Standard Chartered and now Facebook's Sigma). Katie's slogan of "moving fast and not breaking things" is a good synthesis: domains that require fast development and that, while not "safety-critical" in the traditional sense, nonetheless have large economic costs to failure. 
Applying can't hurt. This is a primarily software development and design role though, not operations so much.
[removed]
&gt; people are definitely reading more into it than is there The thing is, natural language is not a serialization format by which an idea is streamed out of one person's head and into others'. I'm happy to believe that /u/nikita-volkov did not intend to make a demeaning, belittling comment, a comment far too likely to reinforce the sense that women are not respected for their ability in technical communities. But…he hid. And if, as you suggest, English isn't his first language then this is what we can call a ”learning opportunity”.
You mention desktop GUI with Haskell. What kind of tools do you use for that ?
Out of interest, what's your explanation for haskell having a lower (much lower) TIOBE index than python, ruby, ECMAScript/JavaScript, perl, php, scheme and lua, to mention only those languages that are almost the *exact* opposite of Haskell in every way? If industry adoption is a figure of merit in a language, industry seems strongly to favour dynamic, dynamically typed, and often interpreted languages. What does that tell us?
I'm not involved with SC in any way, so this might be wrong/outdated/incomplete/... but AFAIIK: Excel! See https://www.youtube.com/watch?v=hgOzYZDrXL0
Do you provide relocation help to London/Singapore?
It's legal but the single bind between the two effects remains a bind, so ApplicativeDo seems to have no noticeable effect.
I've really no idea. I used to try to understand why the world doesn't see things exactly as I see them but I didn't make much progress so I've put that project on ice for now.
Stackage makes it even easier. You specify a snapshot and your code will always build.
There is very little Excel use today, instead there is a UI toolkit with multiple backends. 
&gt; how can you not see a joke in that? I see the joke. “I was joking” is a very, very poor defence. “I was just joking”, “can't you take a compliment?” these are the stock phrases of a certain kind of man. Whether you are one or not, you've made yourself look like that kind of man. &gt; you realise that you've basically just associated "marriage" with "demeaning" and "belittling"? Yes, I do. And: I do. I suggest that you read up on some feminist critiques of marriage…
[removed]
Shortened summary of the proposal (with some format editing and elisions): &gt; […] Foldable has several laws, but I will focus on two. First, for `foldr` and `foldMap`, &gt; foldr f z t = appEndo (foldMap (Endo . f) t) z &gt; Second, optionally, for Functors, &gt; foldMap f = fold . fmap f &gt; […] It is generally acknowledged that these laws are relatively weak: it is difficult to conceive an instance which violates them that is not an egregious violation. &gt; With that, I offer the following simple proposal […] &gt; &gt; ### SAFE - Stronger Alternative Foldable Enhancement / Elucidation &gt; Foldable will gain a superclass, Alternative: &gt; class Alternative t =&gt; Foldable t where { ... } &gt; The members of the class will not change. The class will gain the following laws to supercede the existing laws: &gt; foldMap f empty = mempty &gt; foldMap f (xs &lt;|&gt; ys) = foldMap f xs &lt;&gt; foldMap f ys &gt; foldr f z empty = z &gt; foldr f z (xs &lt;|&gt; ys) = foldr f (foldr f z ys) xs However, I concur with [Gershom](https://mail.haskell.org/pipermail/libraries/2016-February/026795.html). The `Alternative` constraint is too much.
Agreed. Being popular gets you PHP or worse, Donald Trump. I get the feeling that there's lots of inhouse software written by large corporations who hire experienced haskell hackers. This would also explain a static position on github-SO.
This rules out too many useful instances, and doesn't even work for many Alternative instances. Also, not a fan of the verbose and flowery language used in this proposal. Proposals should be brief and motivation should be clear and concrete.
It is not open source and probably never will be - there's too much custom integration with our internal stuff. As a user, Strings are abstract types and there are no infinite data structures, but otherwise it feels just like Haskell. 
I disagree. Responsible organizations don't make such big decisions based solely on a few people's positive or negative experience (let alone immediate perception), although there's certainly some of that. If a tool is shown to boost your productivity by 100%, then you *will* adopt it or lose business to your competitors who will. At the end of the day, that was the winning argument for Java, and the one that caused its near overnight adoption (combined with the low adoption costs relative to Haskell). I know because that was what swayed me (as a decision maker in the early aughts) to switch away from C++, in spite of not being especially impressed at first sight. The evidence was so overwhelming that you couldn't responsibly ignore it. You had case studies from company after company, sector after sector, reporting a 1.5-3x boost in productivity, and then canary projects within the (large) organization similarly reported such an overwhelming boost, that it was silly for the organization *not* to adopt Java across the board, and the sooner the better. Of course, this doesn't apply to early adopters who usually have other reasons, and why I'm happy that some companies are giving Haskell a try. Their experience can be a good positive argument, and one that can carry serious weight.
Yes, but those bad decisions don't usually spread across the industry (although that happens too). I think that in most cases the reason why things that seem like good ideas to some fail to gain significant traction is because they aren't as good as they think. This can happen quite often, as few people can see the big picture well, and understand the true needs of a large market. I don't think Haskell would gain wide adoption (compared to other non-mainstream languages) until it shows some significant business advantages.
Probably because the industry was largely focused on OO for awhile, for some reason. Many people argue that OO was a step backwards, even Uncle Bob himself. Our industry is shifting towards FP, and away from OO. Things like the TIOBE index are lagging indicators. You're not going to see that shift toward FP languages until the development in them is increased dramatically, but it's already been moving in that direction. Languages like scala (which tries to encourage FP) are growing fast. Likely because it lets you transition from OO to FP pretty easily. From there, many people wind up moving to Haskell. The joke was recently made by the creator of scala that, scala should be renamed "Haskelator", because it's the gateway drug to Haskell. I dunno. Maybe it's just me, but it seems clear that the industry has no problem moving to a new (old) paradigm. Especially when you consider how easy it is to parallelize FP code vs traditional OO. 
The "proposal" author seem totally OK with that for some reason : &gt;&gt; (..) Map is not something that can be made an Alternative either (..) &gt; (..) For the same reason as Set, though, I do not think it should be Foldable (..) Not sure why he would like such a class ...
What we need is a cross-query - Haskell repos on GitHub that are not on Hackage.
[removed]
I think the flowery language was to indicate that this proposal was not meant to be completely serious, in the style of ["A Modest Proposal" by Jonathan Swift](https://en.wikipedia.org/wiki/A_Modest_Proposal).
There are at least 3 viable monoid instances for Maybe. The one we have will suck a lot less once Semigroup finally becomes a superclass of Monoid and it becomes instance Semigroup a =&gt; Monoid (Maybe a) Then it accurately models adjoining a unit to a semigroup to make a monoid. This subsumes the current definition and is quite close the Conal-style lifting `mappend = liftA2 mappend, mempty = pure mempty`, except `mempty = Nothing` instead. It doesn't match the `mappend = (&lt;|&gt;)` story that Conor would prefer, but as there is no migration plan that could viably get us to that alternative without randomly changing the semantics of existing code that isn't going to happen, and we'd have to come up with yet another option type with the other semantics, anyways. No matter what was picked 20 years ago, someone was going to be disappointed.
Well, now that you point it out: &gt; Before, we saw as through a glass, darkly. To drive back the night, we stoked the fires of our passion. Now the heat burns us! The smoke stings our eyes! &gt; I intend to magnify what was hidden so we may see clearly by only a small flame. […] &gt; SAFE - Stronger Alternative Foldable Enhancement / ***Elucidation*** That being said, it was posted on `libraries@`, not `haskell-cafe@`. I would expect non-serious proposals on the latter than on the former.
A simple thing to start with, would be a simple arithmetic expression language, containing programs like `"3+(2*3)"`. The corresponding AST may look like: data Expr = EAdd Expr Expr | EMul Expr Expr | EInt Int Parsing the above example program should yield the AST `EAdd (EInt 3) (EMul (EInt 2) (EInt 3))`. This is captured more generally by the following grammar (in BNF notation) describing a simple version of our expression language: expr ::= '(' expr '+' expr ')' | '(' expr '*' expr ')' | int This reads as: *An expression is one of 3 things: Either it is a `(` character, followed by another expression, followed by a '+' character, followed by another expression, followed by a ')' character, or it is…* (I guess you get the idea) Each operator expression has to be surrounded by parentheses, so you don't have to worry about whether `3+2*3` is parsed as `(3+2)*3` or `3+(2*3)`. `3+2*3` would be simply rejected. You can write a parser function, e.g. `parse :: String → Maybe Expr` and for fun also an interpreter `eval :: Expr → Int`, evaluating the arithmetic expression. Then you could extend your grammar to optionally use operator precedence to disambiguate `3+2*3` uniquely to `3+(2*3)`, and make `+` and `*` right-associative (meaning that `3+4+5` can be parsed uniquely as `3+(4+5)`). A corresponding grammar would be: expr ::= summand '+' expr | summand summand ::= factor '*' summand | factor factor ::= int | '(' expr ')' Beware of left-recursion: most parser-combinator libraries are restricted to grammars having a certain structure. Especially, most of them don't like left-recursion, so we could not just use `summand ::= summand '*' expr`, because reading a summand would start by reading a summand, never advancing the input stream and hence looping forever. In the previous example this was not an issue, because the terminal symbol `'('` was expected before the recursion: `expr ::= '(' expr …` 
This is an excellent question. The job description makes it sound like an intense position. It's hard not to wonder implicitly if people can keep up with it.
Author here. PR's welcomed, especially on getting this into a more releasable state! Also, I think this is the might be the largest type in the haskell ecosystem: https://github.com/soundcloud/haskell-kubernetes/blob/master/lib/Kubernetes/Api/ApivApi.hs#L292
I am an Economics major but doing a minor in Computer Science. I am learning Haskell in my Advanced Programming course and seemed to have developed passion for it. Am I eligible to apply given that I only have bachelor's degree? I have taken considerable number of courses in computer science.
[removed]
&gt; The uri-bytestring library referenced by /u/mightybyte looks like it is based on the ideas of network-uri, but it uses ByteString instead of String, and it provides a more modern and faster parser. Yes, that was the motivation for uri-bytestring. Soostone needed a fast and memory efficient URI parser. `network-uri` seemed to be the only game in town and it was not acceptable because of it's use of String. So `uri-bytestring` is definitely trying to be a modern replacement for network-uri.
[removed]
Why are you using an in-house Haskell compiler? Is this for some kind of competitive advantages?
[removed]
Found this in the code: &gt; Copyright (c) 2016-present, SoundCloud Ltd. Did not know SoundCloud was using Haskell :)
Once you got hask you never got back.
I'm sorry, I think the behaviour you consider acceptable is nauseating, and I will leave it at that. Anyway, he didn't heckle at her at the conference. This is reddit FFS.
Would you say it to a male programmer? Why not?
&gt; And if, as you suggest, English isn't his first language then this is what we can call a ”learning opportunity”. Yes, except reading his other comments reveals that he hasn't learned anything from this.
I'd agree with Bob that the kind of OO we have these days is not great. But then I've written Smalltalk for money, and that's a very different proposition to Java or C++ (which I have also written for money). Was OO a step back? It's difficult to say, since the kind of OO we have today is compromised and also mixed in with a bunch of other stuff. &gt; Our industry is shifting towards FP, and away from OO. Well, certainly towards FP. I don't see OO going away any time soon. And the two are not mutually exclusive. There's no reason for an OO program to have uncontrolled side-effects all over the place, in fact many of them do not. And there's no reason for a functional program not to be organised around classes of values, many of them are. Scala may well win this round exactly because it allows people to move towards FP *without* abandoning OO. &gt; scala should be renamed "Haskelator", because it's the gateway drug to Haskell. I don't think so. Haskell's tooling is far, far too immature for widespread industry use and I don't see that being fixed quickly enough because too many in the haskell community thinks that “use EMACS” is a reasonable answer to “what's the best IDE for haskell”. Other languages will adopt the good ideas out of haskell sooner than haskell has an IDE that even beings to come close to, say, IntelliJ, because the current haskell community just doesn't value tools like that. But industry does. Similarly, although haskell itself is a fairly small, fairly clean language, the insistence of the haskell community on building ever higher, ever more abstruse towers of abstraction upon abstraction, solving the same basic problems over, and over again in more obscure ways, makes haskell *and its ecosystem* actually less and less appealing to mainstream developers and their managers. Haskell, or rather its community, blew it. Which I think is a shame, but there you go. So, yes, a few small teams in a very few finance houses are making headway with haskell—but the same is true of K and I don't see that taking over the world any time soon. &gt; Especially when you consider how easy it is to parallelize FP code vs traditional OO I think that's a huge red herring. Mainstream programmers in industry don't think up algorithms and then try to fit them onto multi-processor machines. If they have a problem that needs to recruit mass processing power to solve economically they use frameworks and services that hide the parallelism, just as they don't—or shouldn't, anyway—think up algorithms and then work out how to make them multi-threased. In the vast majority of cases, if a mainstream developer in industry is wrangling with low-level threading constructs then they've made a deign or architecture error, and the same will soon be true of parallelism. 
Yes, because clearly more than one person disapproves.
One wonders if the acronym itself is part of the satire. Many mathematicians find their inspiration from applied math. Nevertheless, there's one telltale indicator that sends pure mathematicians scurrying away from an applied paper: acronyms. It's like finding a beach strewn with litter; one moves on, looking for a more pristine beach. The feeling is that inventors of acronyms are playing house, not serious, one has left the realm of reason. Call a web framework SFFYWBI, and where's room for an even faster package? EFFTTLF ? This is of course merely a cultural difference (like complaining about the Warriors celebrating wins, why shouldn't people enjoy their work?) but the existence of the prejudice I describe interferes with a free exchange of ideas.
As if we needed more trolling / passive-aggressive comments on the topic...
&gt; There's no reason for an OO program to have uncontrolled side-effects all over the place Is that really so? If most objects are immutable then surely that doesn't count as OOP?
Oh come on, you are just backtracking now. You did say "we do all", and that is universal quantification. And in the following sentence you suppose I should ignore that, and pretend that "we" means "we, who disapprove?" I just think you are pilling on a valuable member of our community, using nothing but cryptic "we disapprove" *newspeak*. Nothing but an unreflective mob preaching the consensus.
If you don't know the difference, I think you have never been heckled by a man.
One is a form of abuse, it presents a physical threat, it makes you **scared**. The other is impersonal, digital, and in a friendly community. If you think you are making a service to women by equating the two, stop it.
Some problem domains are very well served by solutions that work by streaming immutable data through a graph of stateless calculations. Functional languages are a good fit for implementing such solutions. Haskell, as a functional language, will therefore be a good fit for building such solutions, all other things being equal^†. Does haskell have a market-dominating USP over Scala or F# for doing that? I don't see one, and at seems that neither does the market. I stand by to be (pleasantly) surprised should it turn out otherwise. ---- ^† they aren't.
No, but they are not nearly **as bad**, like you just proposed in your comment about a homosexual conference. I hope you realise the original comment was not made at the conference.
I also felt disappointed by fp101x and believe that I was able to pass it with nearly no effort only because I already knew enough haskell. I clearly remember many friends confused and annoyed by the parsing and continuation exercises in particular because they needed to use language constructs that were not explained in the lectures at all. Also the discussion on FUN's forum where much more interesting and various than on edx. But this can be fixed easily
[removed]
Since you've conceded that there is minimal benefit to using Haskell in industry, how do you explain that those "who tend to be among the better developers" want to work with it? 
API as a type does have that drawback. 
&gt; &gt; If most objects are immutable then surely that doesn't count as OOP? &gt; Why ever not? Well, does it? Isn't OOP about encapsulating state in objects and state, by its nature, changes? But perhaps you're saying most objects are *mutable* but one can be careful how one mutates them. Well sure one can, but that's a much weaker claim than I thought you were making.
If that is a strawman why you just used it then? Why did you ask how would he feel if he was at a conference, as *if it was the same thing?* You are the one that presented the strawman, and now is backtracking from it.
I'd be interested to know what it is, to the best of your understanding, about the nature of finance that makes Haskell (somewhat) suitable for it, despite being unsuitable elsewhere.
If you can demonstrate strong functional programming ability you can absolutely apply regardless of academic background.
&gt; The one we have will suck a lot less once Semigroup finally becomes a superclass of Monoid and it becomes Any background on this? I think I've missed it. Is Semigroup going into base? 
They tend to have lots of this going on: &gt; solutions that work by streaming immutable data through a graph of stateless calculations Note that we're talking here mainly about boutique-y funds and small, specialist teams in the trading arms of what used to be called merchant banks. Those six (count them! six! world-wide!) seats that dons is promoting at Standard Chartered, for example, are described thus: *[t]he focus of these role[sic] will be on market pricing feed automation and algorithmic pricing*. And, by the way, these roles are on the trading floor—I've seen dev teams working alongside traders in much bigger banks than SC blow *millions* of dollars on whimsical technology adventures that they then suddenly abandon because, hey, it's not as if anyone is going to notice. To be clear, I neither expect nor predict that this will happen at SC, where I am sure everyone is much smarter than that. 
I don't doubt that for one second, but simply listing some pros and cons doesn't cut it. We need to know whether on the whole the benefit exceeds the cost or not. Maybe the answer varies by domain or some other variable, but that is the kind of answer that can be too convincing to be ignored. 
What made a strict Haskell more appealing than something like OCaml? Just more Haskellers around at the time?
&gt; So it seems Scheme was almost a leading language. :) That is a very interesting story, but JavaScript's success -- whatever the language looks like -- has little to do with its merits and everything to do with the success of the browser. It could have looked like Ada, Haskell or VB (it actually was VB for a while on IE) and it would have achieved the same success. If you are the only/leading language for some platform and that platform succeeds, then the language will succeed (Objective-C is another example of that). In any case, at the time JavaScript was introduced (pretty much at the same time Java was), Java hadn't yet been taught at universities, so that certainly wasn't the reason why Netscape wanted JS to look like Java ...
Incredibly vacuous note: Something something, facebook made an unlinker to do something about integrating new rules in haxl/sigma. What this does, I don't know, what it can do, maybe related.
For a start, all the ones where human interaction and decision making is a critical part of processing—which is still many. There's a lot more to “enterprise” software development than straight-through and analytic type operations. 
&gt; the ones where human interaction and decision making is a critical part of processing Which describea a significant chunk of the Haskell work at Standard Chartered. Are they using the wrong tool for that job then? 
It's not a fork.
It is not a fork. It does rely on the haskell-src-exts parser, as an external library, but all the rest is custom written. The path of history actually runs from the backend, not the frontend. We started with an existing internally developed runtime system and wrote the compiler to target it.
It targets a preexisiting strict runtime. That's literally the only reason.
How about this one: https://www.reddit.com/r/haskell/comments/47685k/remote_haskell_developer_role/
Apparently this is an expansion. All the current members are also the original members.
Wow, that's impressive for the city
I don't think that's literally true (someone more knowledgeable correct me if I'm wrong) but it is true that there is very little turnover of staff in the Strats team.
Sure, but JS and Objective-C are very unique. You could succeed the same way they have, but it's much harder than any alternative path to success.
&gt; there's no evidence that ever happened, and java became popular entirely through heavy marketing, not a demonstrated value. I was there, the evidence was overwhelming (case-study after case-study, in trade magazines and research-firm reports), and we felt the difference immediately in every project we switched from C++ to Java (and, being a large defense shop, we collected lots of metrics). Also, people forget that there were other languages/environments heavily marketed at the time (like Delphi and Visual C++), and they didn't do as well. But hey, I was a Java skeptic for a long time (at least until 2003, IIRC) right until I could no longer afford to ignore the evidence. Maybe the same could happen with Haskell.
Also a fair point.
From /u/dons: &gt; Team is growing. No one has left yet :)
[removed]
LIke I said, I don't think that's literally true.
[removed]
One of the main barriers I see in my day to day job is differentiating between when a feature is first shipped, and when the same feature has stopped causing issues that require large amounts of dev attention. That is, the difference between when a feature is shipped and when it is complete/correct. I see lots of estimations go out where people say "we think we can have this done in x weeks", but once the feature ends up in production, it keeps eating away at dev time until it is finally correct. Even more so, the constant randomization is a big production hit for the next feature, and it can cause a big spiral of doom and gloom. One of the main tools that we use for catching bugs is TDD. TDD in my experience (and in research literature) has been shown to reduce defect rate substantially. When paired with a strong type system, it becomes stronger: * Strong typing acts as a test - aka, I don't need to test what types on inputs a function takes * Negative testing - it is really hard to unit test a negative, whereas if I say a function produces an Int, the type system will ensure it does * Property Testing - Once I heavily constrain possible inputs to a function, I can start talking about property testing (this function holds true for all Doubles, etc.) Now, this is not all free, but it is reasonably cheap (once you've adjusted to using strong types). I would go as far as to say a lot of unit tests can be removed in favor of asserting that a particular implementation has a particular type, so you are getting a lot of "unit tests" for cheap (not quite free). I use Scala at my day job, and we see a lot of benefit from using functional programming, but I consistently see things where having (say) IO as a type or not having to deal with subtyping would have alleviated issues. This is in addition to many other reasons I would like to use Haskell, such as easy control flow, simple and robust parallelism mechanisms, etc. At work, when we interview, we ask people what quality they think is most important in their code. Most often, that quality is "correctness" - aka, code is less useful if it is not correct. We have seen success in using a type system paired with TDD, and I believe we would see more success towards with with Haskell.
[removed]
I finally realized what a "Monadic data builder" is :-). That's pretty cool. Of course the optparse-applicative builder will need some improvement (to handle default value using type literal for example) but that's a really good start. Many thanks. You said you will try Generics instead of TH, did you have time to have a got at it yet ? 
This is the one quality that is most interesting to me personally (I'm interested in software verification, but I can't say that it is objectively more important than other considerations; most companies today deliver software that is as correct as required or nearly so), but the extent to which inferrable type systems help write correct software requires empirical data that we lack. The strength of Haskell's type system (at least as much of it as I know) is that of a finite state machine, namely the body of any Haskell function could be replaced with a FSM[1] while preserving the type signature. As most programs deviate a lot from that signature-preserving FSM, the question of how much those types actually help with correctness can only be answered empirically. I don't think we have evidence that Haskell's type system helps create correct programs significantly more cheaply than simpler type systems (like Java). Especially that we now have tools that are both more targeted and more powerful than HM type systems (like Java 8's advanced pluggable type systems, various verification tools employing JML -- the Java modeling language that lets you attach a formal spec to each method to be verified formally or semi-formally by various tools, or static analysis tools employing sophisticated abstract interpretation), I'm not sure HM type systems are a net positive at all compared to the alternative. To me they feel too crude and too restrictive. [1]: In Haskell this basically means no recursion of non-constant depth. So, for example, the length function could be replaced with one that returns 0, and the map function could be replaced with one that returns an empty list (or an empty or single-element list, depending on whether the input is empty or not). In fact, we can be even more precise: any type system can fully specify the behavior of a system with number of states equal to the number of concrete types.
I'm super excited about this!
[removed]
Padon me. I wasn't disparaging your work. It was just a comment on the auto-generated type. It's cool that you're integrating it with pipes-http.
Purity is not an optional extra, it's absolutely key to finance - you want to know what is pure, what inputs it takes, and be able to reproduce it months later. Things that subtly take the current date are a nightmare.
In general we do hire such people, but not currently as far as I know, and we'd probably be assessing them more on quant stuff than Haskell stuff.
That's very impressive. And the inline-java thing seems pretty exciting too.
We mention this in the README, but this boils down to Maven using a home made script to pack the Haskell executable and all the libs it needs. The command for the "hello" app is: mvn -f sparkle -Dsparkle.app=sparkle-example-hello package
[removed]
`First`
This is incredibly exciting. Beyond the near-term practical implications (which are many), I love seeing data analysis in Haskell being expanded in general. There is so much untapped potential in that space. Thank you for your work!
This law has been proposed before and also shot down. Doesn't work for `Map k`, `Set`, `Maybe`...
I and I believe much of the industry demand just as much evidence from Haskell or any other new technology as we did from Java, but let's leave it at that as this is turning from a discussion to flamewar. As to industry incompetence, in the general case, the computational complexity of verifying (or constructing) a correct program is at least exponential in the size of the code, regardless of representation or abstractions used, even for finite state machines (for Turing complete languages it is much worse: linear in the number of program states). Ensuring program correctness is among the hardest known theoretical computational problems, and no language, approach or algorithm (or super-intelligence) can solve it in the general case. So the existence of bugs cannot be a sign of incompetence, as producing true AI is an easier computational problem than verifying the correctness of a large program. Concluding that the industry is incompetent because we don't have true AI yet makes more sense than your conclusion, because that's a much easier problem than ensuring all programs produced have no bugs. So the evidence you refer to is simply an empirical demonstration of a well known theoretical result rather than a statement about someone's IQ. Also, that line of reasoning would lead you to conclude that American (or European) literature is worthless and has always been worthless, judging by the number of terrible novels written, which always vastly outnumber good ones.
&gt; Discussions were 100% focused around two subjects: fear, and what everyone else is doing. Just like 20 years earlier when the FUD subject of the day was linux. Nobody ever moved to linux because of "evidence that it was superior". They moved because "IBM did so its ok". That's asking for evidence by proxy. Most companies don't have the resources or the knowledge to evaluate risky technology on their own, so they rely on larger companies that do. Sometimes this trust is misplaced, but it is not unreasonable. Also, switching an OS requires a much smaller investment than switching a language. I think that saying that we're not seeing greater Haskell adoption due to FUD is ridiculous. Haskell is too small to threaten anyone enough to justify the effort of FUDding it (other than maybe fans of other non-mainstream languages that see similar levels of adoption). It is certainly interesting, so there are far more people with opinions about it than people using it in production, but I wouldn't call the negative opinions FUD. &gt; And if 99% of books contained hundreds of basic spelling and grammar errors, we absolutely would say that the publishing industry is incompetent. Finding basic spelling and grammar errors is computationally trivial. I am not at all sure that this is the case for most bugs, so I'm not sure the comparison is fair. Also, the work you ascribe to the publishing industry is mechanical, while the work done by software industry is creative, and much more similar to the work done by writers.
Of course! And this could easily be made to have linear time complexity by using a `Vector` or `Array` instead of the list. I really like this typeclass now -- you get `Foldable` or `Traversable` out of it depending on what extra constraints you allow `t` to take. Are there any other typeclasses that are generalized by it? Also, you can define this function, which lets you traverse so long as you can provide a `map` that works for the specific `b` you're mapping into: generalTraverse :: (Applicative f, Yotraversable t) =&gt; (forall x. (x -&gt; b) -&gt; t x -&gt; t b) -&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) generalTraverse map' f xs = let unwrap (Coyoneda g ys) = map' g ys in unwrap &lt;$&gt; yotraverse f (Coyoneda id xs) ... which can be used to easily define a `traverse` for `Set`s: setTraverse :: (Applicative f, Ord b) =&gt; (a -&gt; f b) -&gt; Set a -&gt; f (Set b) setTraverse = generalTraverse Set.map
Sure. In fact most of the code in the demos we showed were implemented in Scala or in Java. It's just the top-level part that was implemented in Haskell. We use Maven to package the app in a form suitable for consumption by Spark, so in principle you could include whatever extra Java classes you like in the resulting .jar.
I think we'll just be doing beginner and intermediate tracks. (There's two of us anyway) Intermediate track starts with monad transformers and works forward from there. What is it you'd want to learn?
I don't know much about Spark but if there is no native dependency, it should also be possible to compile Haskell to JavaScript via GHCJS and then directly run it on the JVM via JVM's Nashorn script engine with the advantage of easy Java interop. Here is an example that I recently tried (Haskell code that converts between Java's ArrayList and Haskell List): https://gist.github.com/mmhelloworld/240ec2c13310eef14a51 
Oh there are no topics in particular. Intermediate would be enough for me at this point. I was purely curious to see what topics would be categorized as advanced just so I can have a list of topics to eventually tackle.
Personally, I think OCaml looks ugly. ;)
&gt; I don't think so. Haskell's tooling is far, far too immature for widespread industry use and I don't see that being fixed quickly enough because too many in the haskell community thinks that “use EMACS” is a reasonable answer to “what's the best IDE for haskell”. Other languages will adopt the good ideas out of haskell sooner than haskell has an IDE that even beings to come close to, say, IntelliJ, because the current haskell community just doesn't value tools like that. But industry does. It's very hard to reconcile this with the growth of Scala. The tooling available for Scala is pretty bad, but there are still sound reasons for adopting the language. I don't see any significant difference in tooling between Haskell and Scala.
I like it a lot too, but /r/Haskell didn't seem very enthusiastic about it [last time I proposed it](https://www.reddit.com/r/haskell/comments/32wjoz/lawful_foldable/). &gt; Are there any other typeclasses that are generalized by it? Not directly, but using `Coyoneda` probably makes it possible to generalize quite a few typeclasses in such a way that they work on non-`Functor`s too. (For example, `Set` is a `Yomonad`, even though it isn't a `Monad`.) I don't know how much value there is in that, though.
What happened is that email can't express feelings very well (and you see part of that in this thread). So he didn't have the advantages of verbal and physical communications. On top of that, with no feedback he had no idea about what the recipient felt. So his situation is different from yours. He became confused about whether he managed to convey the message. That's all natural. You are confident that you conveyed the message correctly. That's also fine.
&gt; That there was evidence of java's supposed superiority. So you mean that the fact that I have not dug out and reproduced 15-year-old-reports that I read while working in the defense industry simply because you demanded that I do is failure to support my position? You are perfectly within your rights not to believe me when I said that I was a holdout until the evidence was too overwhelming to be ignored. But you have not supported your position that Java's massive adoption was only due to marketing and industry incompetence. I guess we both remember things differently, or worked in different kinds of organizations, which is likely because you've seen incompetence everywhere, while I witnessed it relatively rarely. &gt; there's been plenty of time to switch. Switch to what? Do you know all hardware C libraries are required to run on? Do you know all the software compatibility requirements? Do you know how much money rewriting all this code would cost, let alone the cost in missed opportunities? I would think that a competent person would calculate the rewrite cost vs. the bug costs and choose the less costly alternative. Have you done that so that you can so confidently call millions of engineers incompetent (I assume you exclude yourself from this judgment)? &gt; does not mean it is difficult to create a safe API. I don't know what you mean by "difficult". Is it hard to write one? Certainly not. Is it hard to write one while still conforming to the requirements of all possible clients? Yes. For example, you'd need to make pretty much every function -- even sort -- able to signal a failure, which isn't easy in C without breaking many clients. Sure, this is a shortcoming of the language, but there are still plenty of good reasons for a project to choose to use C under some requirements (even if those requirements are dictated by budget alone).
I don't think it's quite as intense as Don's ad made it sound. 
I was a bit rough and I apologise. Stack may very well be the better tool, but don't forget the hard work that was put on Cabal, and the importance it has for the community as a whole. At every thread that mention Cabal, you post with pure hostility, without showing any appreciation, you simply belittle it with absolute disregard for the work of several people who offered it to you, *to us*, for free. Please don't treat it as a joke. My answer to your ending question is: because we can. Why waste effort on Functional Programming? Why waste effort on Haskell? Why waste effort on plan B? I will just say I'm very glad there are people who "relentlessly pursue plan B" 
Now if only you could install it with any existing released compiler! It's a package ahead of its time.
How long out of academia is too long?
Having got the examples running on OSX with deech's help and browsed around a little, this looks quite promising. Even without native look and feel, a no-fuss uncomplicated reliable cross-platform deploy-friendly GUI toolkit with rich widgets, GUI builder, smooth OpenGL integration, and a large existing community would be a real step forward for Haskell. I especially appreciate your careful documentation of goals, relation to other toolkits, and issues likely to be encountered on each platform and with GHCI (at http://hackage.haskell.org/package/fltkhs-0.4.0.5/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html). Thank you very much! For the curious, here's how some apps using FLTK look: Haskell: http://pasteboard.co/1PnrKYM1.png http://pasteboard.co/1PnsEE1a.png C: https://en.wikipedia.org/wiki/ZynAddSubFX#/media/File:Zynaddsubfx.png http://prodatum.sourceforge.net/screenshots/current.png https://www.youtube.com/watch?v=z1hDTMXQArY http://openvsp.org/vid_tutorial.shtml 
&gt; Imagine you're giving a technical talk to a group of homosexual men. One of them shouts "marry me!". &gt;Would that make you uncomfortable? Where do you go after this when the answer is "no"?
The definition the author gives for `transpose []` begs the question for the definition of `transpose (xs:xss)`, and thus bakes in the assumption that there's enough lists from the recursive call to `transpose` to avoid dropping columns. This is just a special case of the fact that `[[a]]` doesn't enforce that all sublists are the same length, for the case that there are no sublists. A stronger type would encode the dimensions in the type, and thus be able to represent effectively a 0xn matrix. But, ignoring that, making a _reasonable_ base case first then ensuring the definition of the inductive case handles it is an approach which works just fine here: transpose :: [[a]] -&gt; [[a]] transpose [] = [] transpose (xs:xss) = zipWith (:) xs (transpose xss ++ repeat []) For malformed input with different length rows, the original will truncate all rows to the length of the shortest row, and this variant will "left shift" columns in the transpose. Given it's a violation of precondition, both implementations are "correct" to the strength of the type, but the author's implementation is incorrect for any 0x0 matrix, and mine is incorrect for any 0xn matrix where n &gt; 0, as there's just no way to distinguish the 0xn matrices from `[]`.
Thanks very much for the kind words!
Thank you for all your kind words and upvotes. I'd like to take this opportunity to specifically ask for assistance with the [GHCi situation](http://hackage.haskell.org/package/fltkhs-0.4.0.5/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:3). In a nutshell: * users are restricted to GHC 7.8.4 on Linux and Yosemite. This seems to be due to a [bug](https://ghc.haskell.org/trac/ghc/ticket/10568) that was closed as fixed in 7.10.3 but it's still occurring. I will report it tomorrow but I don't have a reduced case and any insight would help. * GHCi even with 7.8.4 does not work on Windows. It keeps wanting some DLL called "uuid.dll" and I don't have the first clue what that is. * windows launched via the GHCi never fully go away until the session is ended. Probably some misunderstanding on my part about how the REPL and FFI work. Appreciate the help! 
No, it's lower-level than OpenGL. The idea is to move a lot of code that previously would have been in the driver to the application, which enables more control.
Nope. I know quite certain there is no such guide. But as a GL-noob you can follow one of two paths: 1) Learn OpenGL with example's in C and write your own code in very imperative looking Haskell. You probably want to use the `OpenGL` package for this; [read here why you might (not) want to use the `gl` package](https://www.reddit.com/r/haskell/comments/43ck6x/unsure_what_library_to_use_for_opengl/). 2) Use any of the OpenGL-to-more-idiomatic-Haskell wrappers around. All are in flux and not well documented. So you will end up at (1) to learn more OpenGL first in order to deal with the leaky abstractions. That said: my favorites are [LambdaCube](http://lambdacube3d.com/editor.html) and [Gelatin](https://github.com/schell/gelatin). 
Inline java would solve my odbc issues. Java has great Ms Sql Server jdbc library. Inlining it into haskell would solve all our database connectivity problems. 
has anyone built it locally? I run into the following error on two different OS X, when I run stack --nix build /sparkle/sparkle/cbits/bootstrap.c:11:1: error: unknown type name 'pthread_spinlock_t'; did you mean 'pthread_rwlock_t'? pthread_spinlock_t sparkle_init_lock; ^~~~~~~~~~~~~~~~~~ pthread_rwlock_t /nix/store/w090w1675pg26kcwj7ka9f42c0a87wg9-Libsystem-osx-10.9.5/include/pthread.h:99:35: note: 'pthread_rwlock_t' declared here typedef __darwin_pthread_rwlock_t pthread_rwlock_t; ^ /sparkle/sparkle/cbits/bootstrap.c:16:2: error: implicit declaration of function 'pthread_spin_init' is invalid in C99 [-Werror,-Wimplicit-function-declaration] pthread_spin_init(&amp;sparkle_init_lock, 0); ^ /sparkle/sparkle/cbits/bootstrap.c:26:2: error: implicit declaration of function 'pthread_spin_lock' is invalid in C99 [-Werror,-Wimplicit-function-declaration] pthread_spin_lock(&amp;sparkle_init_lock); ^ /sparkle/sparkle/cbits/bootstrap.c:26:2: note: did you mean 'pthread_spin_init'? /sparkle/sparkle/cbits/bootstrap.c:16:2: note: 'pthread_spin_init' declared here pthread_spin_init(&amp;sparkle_init_lock, 0); ^ /sparkle/sparkle/cbits/bootstrap.c:35:2: error: implicit declaration of function 'pthread_spin_unlock' is invalid in C99 [-Werror,-Wimplicit-function-declaration] pthread_spin_unlock(&amp;sparkle_init_lock); ^ /sparkle/sparkle/cbits/bootstrap.c:35:2: note: did you mean 'pthread_spin_lock'? /sparkle/sparkle/cbits/bootstrap.c:26:2: note: 'pthread_spin_lock' declared here pthread_spin_lock(&amp;sparkle_init_lock); ^ 4 errors generated. 
Its nothing like OpenGL
Only 9 years ago. :) Jeffrey Yaskin's [original suggestion](https://mail.haskell.org/pipermail/libraries/2007-March/006950.html) was to just have the `First` and `Last` newtypes. I [suggested](https://mail.haskell.org/pipermail/libraries/2007-March/006957.html) that the `First` instance be used for `Maybe` itself, Yaskin was [concerned](https://mail.haskell.org/pipermail/libraries/2007-March/006965.html) that the choice was arbitrary and potentially confusing, Ross Patterson [suggested](https://mail.haskell.org/pipermail/libraries/2007-March/006969.html) the current, more "obvious" instance, and that appears to have won out. Sadly, Conor McBride did not [object](https://mail.haskell.org/pipermail/libraries/2007-November/008653.html), until eight months later, and no changes were made. See also [ticket 1189](https://ghc.haskell.org/trac/ghc/ticket/1189).
&gt; The thing is, natural language is not a serialization format by which an idea is streamed out of one person's head and into others'. Which is why it's so overwhelmingly important, *especially* in a mixed-culture environment like this, to avoid projecting your own prejudice onto the words of others. Here's what happens otherwise: &gt; I'm happy to believe that /u/nikita-volkov did not intend to make a demeaning, belittling comment, a comment far too likely to reinforce the sense that women are not respected for their ability in technical communities. But…he hid. I'm happy to believe that you did not intend to present reactive social judgements specific to your self and immediate clique as a-priori facts, informing a conclusion otherwise backed only by the specter of a misogynistic straw army. But...you did. This is not just a pot shot; shoddy rhetoric is a major peeve of mine. I find it offensive and genuinely disturbing. But I think you mean well, and though I find your zealotry unappealing, I actually agree with the core ideal you're espousing. You hate bigotry, so do I, and in this polycultural melting pot that's precious common ground. Extrapolate advice from that if you care to.
The answer would have been a "No". I just didn't see any point in continuing a conversation with a man, who'll use any argument, no matter how ridiculous, just to have a chance of branding me a sexist.
&gt; Well over 90% of Facebook PHP code is now statically typed via Hack. &gt; &gt; * **Much of our code was always statically typeable!** &gt; &gt; Some code is still too tricky to statically type. I'm interested in examples of the 10% code :)
what else does the intermediate track cover besides monad transformers?
Of sorts. The class will be in base in in 8.0, warnings to switch will happen in 8.4, the superclass will switch over in 8.6. So from that point on you can rely on it as a superclass of Monoid from 8.6+ but if you need a full 3 release window for whatever you are developing and don't want to use CPP to fiddle with type signatures, then yes, it'll be quite some time.
I see. But if warnings are turned on in 8.4 but the class isn't made superclass yet until 8.6, how are we supposed to write CPP-free code in 8.4? Shouldn't we add the superclass constraint in 8.4 then?
I do not.
How useful is Vulkan for general compute tasks? Does it supersede OpenCL, too?
Honestly, it's kinda rare that good code written in a weakly typed language doesn't follow static typing rules. Most functions in these languages are only gonna accept a relatively strict set of types for its parameters.
I'd guess there are(relatively) simple exercises of dependent typing, e.g. functions like `zipN` or "value indexed types" with more-or-less simple index relations, like operating on arrays of the same length, which is known only at runtime (something we use functions like [`reifyNat`](http://hackage.haskell.org/package/reflection-2.1.2/docs/src/Data-Reflection.html#reifyNat) for)
Thinking about it, I'm not sure using type literals to add metadata (like default value, help etc ...) is such a good idea as using `Tag`ged type instead of plain harder to access. When using TH, this is not really needed, because you can pass all the metadata as a parameter to the TH function. For Generics, as you are not calling anything metadata will have to be passed through a typeclass or equivalent. For the default value, the easiest is probably to pass it to the parser or use the `Def` typeclass. I'll play with you code and let you know. Thanks
Yeah, you can't even program if you can't change variables. How foolish. 
But modern GHC is HM plus many extensions (like type classes) plus type checker plugins. And the Maybe type, the Monad class, and units of measure all obviously complement and benefit each other. You can easily waive proofs in dependently typed languages too. 
It's the Haskell compiler we use at Standard Chartered. 
The proving power of Haskell's type system is still, I believe, only that of an FSM. Maybe and monads don't add anything (Haskell's type system is too weak to verify that a monad instance is really a monad); typeclasses make it possible to write more general code, but they don't add to the proving power, I think. Dependent types are indeed complete (i.e. they can accurately specify any Turing complete program), but you basically need to prove everything manually (or with a model checker, which is how liquid types do it). This is so hard that only relatively trivial programs have ever been verified in this way, and then at great effort (usually years even for a small program). The extent to which "FSM verification" prevents bugs in Turing-complete programs is an empirical question, as mathematically "virtually all" bugs still slip through in the general case. But that's not my point. Anything possible with inferrable types is possible with inferrable static analysis (static analysis is even more powerful), and anything provable with dependent types is provable with non-type-based proofs (like Java's JML provers). Whatever advantage type systems have, and they certainly have advantages, does not stem from their proving power -- anything provable with types is provable by other means, which are often easier -- but from an interaction with how humans think (e.g. they provide always up-to-date documentation, and they help writing more structured functions). This effect must be measured empirically, however, as there is no mathematical justification that says that a stronger type system is necessarily "better" (i.e. makes it computationally easier to prove a program correct). These are all fascinating issues, but there's a lot of empirical study required. It's much easier for someone who comes up with a new abstract interpretation technique (static analysis) to run it on a large real-world program and see which bugs they can find and how many false-positive they have, as opposed to doing the same after re-writing the whole thing in a different language. However, I believe that there's a lot of room for cooperation. As type systems [have been shown to be special cases of abstract interpretation](http://www.di.ens.fr/~cousot/COUSOTpapers/publications.www/Cousot-POPL97-p316-331-1997.pdf), empirical results from abstract interpretation can help direct us in designing type systems that have better impact on program correctness (i.e. help prevent more important bugs). 
Thanks for the report. Created an issue [here](https://github.com/tweag/sparkle/issues/12).
This is my package! This is being worked on at the moment quite a bit and is in some state of flux. I'd be delighted for some feedback on what's good and what's no so good about the (haskell) API; I was planning on making a post about this soon, but /u/galapag0 seems to have beaten me to it :/
Yeah, sorry about that, but I really wanted to use DuplicateRecordFields to prevent having a really confusing namespace situation. Strict is nice to have too for this (although it doesn't matter a huge amount as the code is generated automatically) Hopefully this will be a non-issue in a couple of months. 
Thanks for making a binding available so quickly! What OS and which drivers/hardware have you been using for testing this?
The difference between free monads and freer monads/operational is rather subtle — I suspect Oleg's explanation (in his webpage/paper) could be worth even your time: http://okmij.org/ftp/Computation/free-monad.html If that's helpful, the freer monad of a type constructor f is just the free monad applied to the left Kan extension of f, what's the problem ;-)?
You're welcome! It was fun to do! For software I'm currently using: - Ubuntu 15.10 x86_64 - Nvidia Vulkan driver 355.00.26 - Linux kernel 4.2.0-29-generic (I had problems installing the driver on a couple of others)] - LunarG Vulkan SDK 1.0.3.1 - GHC 8.0.1 (from Nix) - Nix HEAD (for having ghc 7.10 to compile the generator program and ghc 8.0.1 for compiling the package itself) For hardware I have: - i7-4770 - 24GB some kind of ram (This seems to be necessary for compiling without -O0!!!) - Nvidia GTX 650 Ti
Vulkan won't be faster, just specialized to graphics stuff. It is actually closer to OpenCL, than GL ever was. For example Vulkan and OpenCL 2.1 share the same bytecode.
&gt; Hard to say. I was trying to get him to empathise with his victim, but if he can't then it's probably a lost cause. I think you've misunderstood. Of course, anyone can say "no" to a question whether it's true or not. But what I'm asking you to think about is how you respond to the possibility of "no" being the true answer.
&gt; You aren't, as they say, even wrong. And I very much doubt that your invitation to be corrected is sincere. I think there must be some kind of miscommunication here. There really does exist an ideology that says that women have access to knowledge that is fundamentally unavailable to men (and the same ideology says the same thing about other "axes of oppression," as a general principle). Here is proof that such ideology exists, by way of an example: * https://sindeloke.wordpress.com/2010/01/13/37/ I don't know, however -- since I don't have enough information about you -- whether *you* subscribe to that ideology. Maybe you don't. I've replied under the assumption that you do. But I know I could be wrong. That is why I say, "correct me if I'm wrong." If you tell me -- "no, I don't agree with that" -- then certainly I will not insist that you do! I acknowledge (as I did upfront) that I am merely making a guess about what your position is. (The reason I have made such a guess is that it allows me to reply without an extra round-trip in which I ask you what you believe. It is like in the game of chess, played via correspondence, where I send move A, and also add, "assuming you respond B to A, I respond C to B." Of course the opportunity *not* to respond B is granted *sincerely* -- the inclusion of C merely saves time -- the assumption of B is not a deeply held conviction but merely a hypothetical possibility which enables the inclusion of C.)
Heh, I didn't take your comment as disparaging, just an opportunity to clarify :) All good!
[removed]
&gt;And that makes you a presuming jackass. Given that I assumed it provisionally, and made that very clear, I don't see how it would "make me a jackass." This kind of assumption is actually just good communication skills. You're really straining for any reason to dismiss what I'm saying and attack me personally. (Perhaps your goal is merely to avoid answering the question about whether you actually believe in the ideology or not.)
&gt; It is clearly something like OpenGL, Nope &gt; the question is just how much like. Nothing like it. In OpenGL there is a global state and a whole state based object model. Command queueing is not a thing in OpenGL, there's just some asynchronous execution pipeline that may or may not synchronize at certain points. --- Vulkan on the other hand has *no global state* (yay) and defines no object modell whatsoever. Instead everything in Vulkan is about user managed buffers and command queues. In Haskell terms think of an command queue as a I/O monad on the GPU; you can have as many queues as desired; they can be populated in parallel. Evaluation happens upon submission of the queue to the GPU. Although Vulkan is very low level its design caters much nicer to the properties of (purely) functional languages. I expect higher level abstraction for Vulkan to appear soon (I'd write them, were I not piled up in other things). Also SPIR-V adds the possibility to create shaders and compute kernels through DSL or even through a runtime just-in-time SPIR-V generator. All in all Vulkan is much more FP friendly than OpenGL.
OK. Bye now.
&gt; What's an example of a real world case that's easy to prove with JML, but hard in Haskell? Well, the canonical example is proving that the value returned by `length` is equal to the length of the given list, but it's true for any recursive function. You cannot prove anything about the depth of recursion (FSMs can only do constant-depth recursion or infinite recursion -- think regular expressions), and this has big consequences. For example, integer division (or modulo) is really recursive, so you can't have a Haskell type for even/odd numbers that can be passed to a function doing integer division/modulo. Similarly, you can't prove that you don't overflow/underflow arrays. JML (or dependent types, which are equivalent) doesn't make those properties easy to prove, though. It only makes it possible for them to be *stated*. Proof is done by some other tools -- usually some form of abstract interpretation/model checking -- which can be built into the type system (in the case of dependent types) or simply applied to the program (otherwise). In the general case, these things cannot always be proven, so languages like Idris defer to the programmer, and hope they're able to prove it manually. This is not directly related to type theory (which I know little about). These are properties of the logic by which you specify programs (it is more related to software verification). Types are simply one way of writing that logical specification (and JML is another). [Type theory](https://en.wikipedia.org/wiki/Type_theory) is a whole other, and rather deep concept, which is not directly related to program verification (it is related to program verification just like logic is); it is an alternative to set theory as the foundations of mathematics. But these are all the consequences of the halting theorem and its finite variants. The interesting questions are mostly about which properties that we *can* prove are actually useful in reducing bugs in real-world programs written by humans. These questions can only be answered empirically. An example of a relevant study would be a taxonomy of bugs and their relative cost. [This is an example](https://www.usenix.org/sites/default/files/conference/protected-files/osdi14_slides_yuan-ding.pdf) of one such study, which discovered that most costly bugs in distributed systems are due to a psychological effect in humans (not planning for the worst, if it seems unlikely) which makes us not handle exceptional situations correctly (this isn't about ignoring exceptions/`Either`, but about not putting enough thought into handling the outcome we consider less likely), and that those bugs could be detected with a simple linter. This is not something that math alone (or type systems) can tell us. We need to classify bugs, rate them by cost, and then design type systems or other verification methods targeted at those costly bugs. As that paper demonstrates, sometimes the worst bugs are actually computationally trivial to prevent.
Thanks. It's good to know that there still are people that get me.
Is it just me, or does it seem like they made it too complicated for what they are trying to solve? You could just have a request queue or something, right? When certain conditions are met (such as a thread going to sleep), you can batch the requests or whatever.
Could you elaborate a little on how is all this autogenerated? (possibly in web-layman terms ) Thank you!
He has a new paper with improved implementation coming up at PLDI16 https://github.com/emeryberger/PLDI-2016/raw/master/preprints/pldi16-paper317-preprint.pdf
Well I'll ask that question again in some months then :) Leaves me some time to improve in Haskell
very nice! I use Spark with scala at work and the combination works pretty well. It is good to see more functional language bindings to Spark.
The [implementation](https://hackage.haskell.org/package/pipes-bzip-0.1.0.0/docs/src/Pipes-BZip.html) looks almost too easy... is this really all that needs to be done to implement a pipes codec? EDIT: I'm confused that nobody seems worried about `L.toStrict . BZip.decompress . L.fromStrict`. Doesn't this destroy any kind of constant-memory streaming? Isn't this even worse than using lazy bytestrings?
Yes, that's all! Pipes have a very concise and compact API :)
I see it's raw bindings. That's the right place to start! Do you have any ideas what your slightly-less-raw bindings might look like?
Well... It's as simple as it gets for a fully strict and nonstreaming wrapper around a lazy implementation of BZip. It should be fine for small chunks of data, but if you need to decompress large ones (over 1 MB), it might be suboptimal in terms of performance. Anyhow, it's a nice contribution :)
Tutorial by David Raymond Christiansen which juxtaposes syntax directed typing with bidirectional typing. I found it very enlightening.
Well... isn't streaming the whole point of using `pipes`? ;-)
The implementation is tiny, which is great. But much of it is translating to and from lazy Bytestrings. That takes time, doesn't it? That makes me suspect that this might go faster using [Data.ByteString.Streaming](https://github.com/michaelt/streaming-bytestring/blob/master/Data/ByteString/Streaming.hs), because that streams strict ByteStrings. Given how small the code is, I should really try porting it. (Or I could could just get on with completing my HR paperwork at my day job.) Is anybody else up for porting it? Is it likely to help performance? 
Yes, it's a little ironic :D
It isn't really a codec, it is decompress = P.map (L.toStrict . BZip.decompress . L.fromStrict) which is to say, what is coming in from the left are complete compressed bytestrings and what is going out on the right are complete decompressed bytestrings. It's a perfectly reasonable function but not what one might expect. 
Well, now it's a library, too. :)
How do you receive feedback and how much time (%) do you spend on it? I'm wondering about the book's quality to quantity ratio. 
I would find [io-streams](https://hackage.haskell.org/package/io-streams-1.3.5.0/docs/System-IO-Streams.html) useful here. It's quite popular and already supports Zlib compression.
That's correct. The Spark applications written in Haskell you create are only as portable as GHC makes them. They should be mostly oblivious to small changes like different distro versions and the like (because the .jar's we create include most system libraries), but you won't be able to compile on OS X and deploy on a Linux cluster.
Hmm, let's see. If it was: decompress :: L.ByteString -&gt; L.ByteString decompress = L.fromStrict . BZip.decompress . L.toStrict Then yes, it would introduce a choke point in your lazy ByteString pipeline, because the entirety of your data gets accumulated into one giant strict ByteString before getting decompressed. However, the code is instead: decompress :: S.ByteString -&gt; S.ByteString decompress = L.toStrict . BZip.decompress . L.fromStrict This time we aren't introducing a choke point in any pipeline, because there is no pipeline: the data comes to us as a single chunk, we convert it to a lazy ByteString because `BZip.decompress` supports streaming, but that ability isn't really used here because the data isn't coming to us as a stream. Okay, so why is this code used then? Isn't pipes a streaming library? Well, enough simplifications, let's look at the real code now: decompress :: Monad m =&gt; Pipe S.ByteString S.ByteString m () decompress = forever $ do bs &lt;- await let d = BZip.decompress (L.fromStrict bs) yield $ L.toStrict d A `Pipe S.ByteString S.ByteString m ()` is indeed a stream, but it's not at all the same kind of stream as a lazy ByteString! A lazy ByteString is a stream of *bytes* (or rather chunks), while a `Pipe S.ByteString S.ByteString m ()` is a stream of strict ByteStrings. That is, the streaming comes from the fact that we're processing one strict ByteString at a time, not from processing the bytes of those ByteStrings one at a time. So `decompress` is *not* decompressing a bzip-encoded stream of bytes, it's decompressing a bunch of individual bzip-encoded ByteStrings, and since it's doing so one ByteString at a time, it's streaming as much as it can. All right, good, now we understand what `pipes-bzip` is for, but I bet what you really wanted was a way to decompress a bzip-encoded stream of bytes, right? How could we do that? Well, `BZip.decompress` is already streaming, so we could just use that: decompress :: L.ByteString -&gt; L.ByteString decompress = BZip.decompress And how could we incorporate this in a Pipes-style pipeline? Looking at the documentation, I spot [`fromLazy`](http://haddock.stackage.org/lts-5.4/pipes-bytestring-2.1.1/Pipes-ByteString.html#v:fromLazy) and [`toLazy`](http://haddock.stackage.org/lts-5.4/pipes-bytestring-2.1.1/Pipes-ByteString.html#v:toLazy), which would probably allow us to construct something: -- those ByteStrings are chunks from a byte stream, not individual elements like before decompress :: Producer ByteString Identity () -&gt; Producer ByteString Identity () decompress = fromLazy . BZip.decompress . toLazy The type is not quite the `Pipe Word8 Word8 m ()` I was expecting, but that wouldn't be [the first time I expected the Pipes library's API to be simpler than it is](https://www.reddit.com/r/haskell/comments/287jx3/understanding_the_pipes_library/). So I think I'll stop here.
Right, /r/michaelt. Do you have a view on the performance and lazy byte string thing?
The only place where this property is used, is on mobile phones. Nobody cares about this property on the server side.
thanks
Honestly, I like the magic-number-version better. Sometimes it makes just sense to have a magic number at place (thats why so many functions f in haskell have a f' and even f'' version).
Yes. You can do this in language that have subtyping and there are multiple ways of doing this. If you want to see a practical example of this that you can use in Haskell you can try out [Liquid Haskell](https://ucsd-progsys.github.io/liquidhaskell-tutorial/01-intro.html) which extends Haskell with refinement types. Then you can give a value a type like this: {-@ one :: { v : Int | v == 1 } @-} one :: Int one = 1 ... which is basically a way of saying at the type-level that you statically know that this `Int` will always be `1`, but you can always have that type-check as a weaker constraint if necessary, like this: {-@ one' :: { v : Int | 0 &lt;= v } @-} one' :: Int one' = one Liquid Haskell accepts this because even though it knows that `one` is exactly `1`, it's also fine weakening the constraint to "integers greater than 0" if necessary.
How are you being suppressed? &gt;Master suppression techniques are defined as strategies of social manipulation by which a dominant group maintains such a position in a (established or unexposed) hierarchy This seems like a very high-powered response. Are you and I in a hierarchy? How did that happen? Am I in the dominant group? How did that happen? I'd say that making comments in a subreddit is a great leveller. Let's see, there's a shopping list of stuff on that wiki page: * Making invisible? I have literally no way of stopping you from commenting here. The best I could do is appeal to the mods after the fact, who would probably do nothing, so why would I bother even if I wanted to? Which I don't. Our conversation here is strictly turn-based, I can't talk over you even if I want to. Which I don't. And it's strongly asynchronous, so of course I'm doing other things while you're typing. * Ridicule? I have made no statement about any of your personal characteristics, such as accent, appearance, or anything else. I found your *comment* risible, and I hold *it* up to ridicule, yes. That does invite the inference that I don't agree with or respect your position on this point very much. But that's not tricksy ad-hominem rhetoric to indirectly attack your position through you. If you interpret ridicule of your statements as ridicule of you personally then I suggest you…chill. I get upset when it seems like someone does that to me. And then I remember that the opinion of random strangers on the internet is nothing to me, have a cup of tea and cuddle the cat and move along. Your pulling in of Norwegian feminist theory is interesting and got my attention for a few minutes before I get the train home, but I really don't think we have that much to talk about. * Withhold Information? N/A * Double Bind? N/A * Blame &amp; shame? N/A * Objectifying? Not even a hint. * Force/threat of? Not even a hint. I suppose that you are applying the “intellectualise” counter strategy. But why? What do you hope to gain? I'm some random commentator on the net who thinks your statement there isn't really worthy of sustained engagement. OK. File me under “idiot” and move on with your life. 
I was more poking fun. It certainly should be a non-issue pretty soon.
Also http://docs.scala-lang.org/sips/pending/42.type.html I believe it's already implemented in Dotty.
To note the distinction, things like strings and numbers frequently share a type but each value can be treated as separate when dependent types are involved. So for instance, excluding universes, 1 has type Nat. But I could have a type P(1) in some universe somewhere. So most values won't inhabit unique types but types that talk about specific values can be inhabited. Refinement type is a bit more spot on. The smallest set any value is contained in is the singleton set containing it. This could be viewed as "the" type of a value and all other inhabitations are simply a result of subtyping.
As I understand it, there is no way to do this in the language itself, but you can write type checker plugins that could.
Well, you're right about that—where I come from, ridiculing statements that we find ridiculous is a popular passtime. Also, we don't as general policy prohibit classes of rhetorical figure everywhere always because they are sometimes used badly by some people. And, “please chill” isn't an excuse, it's my recommendation. Go find a cat, give it a hug. Seriously. You'll feel better. You have have stated a very high minded purpose for your reply. Another habit that we cultivate where I come from is a profound suspicion of high-minded purposes. If what you mean is that you personally found my comment offensively flippant, you could try saying so and see what happens.
I'm not sure that this is possible, even with type checker plugins. In the initial example, it seems like the OP wants the following unification: MyType [FlagA, FlagB] ~ MyType [FlagA] Presumably we would also need: MyType [FlagA, FlagB] ~ MyType [FlagB] Which implies: MyType [FlagA] ~ MyType [FlagB] This seems unsound.
I think the right thing to look up is "refinement typing" where each value has a _range_ (spectrum even) of type predicates applicable to it, and the most specific is the actual thing it is equal to.
https://www.reddit.com/r/haskell/comments/47p0ae/pipesbzip/d0eznkn
Why would it be any better with `streaming-bytestring`? It seems the upstream library (`bzip`) has already committed to lazy ByteStrings, which are incompatible with the concept of an effectful streaming ByteString. That is, if you want to write "streaming" versions of already-streaming algorithms such as bzip, the proper place to do the streaming is in the `bzip` library itself. Otherwise, as you point out, you'll have to jump back and forth between strict and lazy bytestrings, just to re-stream the already streaming lazy list of strict chunks.
Right, here &gt;&gt;&gt; fmap (length . BL.toChunks) randomData 31 so `P.fromLazy compressedData` yields 31 chunks, and fails at then end of the first chunk, since it isn't a complete bzipped item.
It sounds like you're looking for some way to extend the subsumption relation, rather than the equality relation. Unfortunately the best that can be done with current GHC is something like your `Subset` class (albeit that type-checker plugins can sometimes be used to give a bit more flexibility). There's no way to override the built-in notion of subsumption, because it is not made explicit in the type system. There has been some [discussion about introducing explicit subsumption constraints](https://ghc.haskell.org/trac/ghc/wiki/ImpredicativePolymorphism/Impredicative-2015), mainly in order to improve support for "impredicative" polymorphism. It's possible that such a system, combined with type-checker plugins, could allow what you're describing. But that's quite a way off.
[removed]
Does anyone know of a fix for this? I tried upgrading to the latest GHC but the problem persists. Apparently it's an issue with the bytestring library - I think?
Yes, they're both graphics API's, but that is about it. Hence "very different". Vulkan was built up from zero. Saying Vulkan is similar to OpenGL is like saying it's like DX11. Heck, it's like *any graphics API*. In particular, Vulkan is probably the least *graphics* API like and the most *GPU* api like of all, or at least similar to Mantle and DX12. Edit: I feel like I'm talking to a wall. In spite of all concrete examples you just keep saying "but they're very similar".
Here's one part of a possible solution: type family Intersection (xs :: [k]) (ys :: [k]) :: [k] where Intersection '[] ys = '[] Intersection (x ': xs) ys = IntersectionHelp (Elem x ys) xs ys type IntersectionHelp (b :: Bool) (x :: k) (xs :: [k]) (ys :: [k]) :: [k] where IntersectionHelp 'True x xs ys = x ': Intersection xs ys IntersectionHelp 'False x xs ys = Intersection xs ys type family Elem (x :: k) (xs :: [k]) :: Bool where Elem x '[] = 'False Elem x (x ': xs) = 'True Elem x (y ': xs) = Elem x xs GHC should successfully reduce `Intersection '[ 'A, 'B, 'D] '[ 'D, 'B]` to `'[ 'B, 'D]`.
I have several times run a workshop at my company to run people through some basics: http://www.boltmade.com/presentations/intro-to-functional-programming I have also (once) at the local hackerspace run an introduction to computer science, which contains quite a bit of functional. Curriculum: https://github.com/singpolyma/cs-top-and-bottom
You are right.
Good point, maybe antithetical was the wrong word. Rather, I meant to say that this function doesn't really *do* anything on its own; it doesn't provide any additional benefits of streaming libraries (constant memory usage, strict IO) because the lazy bytestring can already be consumed with constant memory, *and there are no side effects* in bzip compression/decompression. If you want to plug this bzip transformation into the `pipes` ecosystem, `pipes-bytestring` is sufficient.
Lambda calculus. If you cover this well, it will serve as a good foundation for anything else you'll teach afterwards.
You definitely won't be able to get the kind of type inference you want with `case`. But you could build a similar mechanism. Here's a [gist illustrating the idea](https://gist.github.com/andrewthad/0e82fd8b3cbfff6a516e). Look at `attrC` at the bottom.
At the end of the day, Rust is still a memory-managed language. It still requires you to think about the lifetimes of values. It gives you the way to do that in a static, safe way, but you still need to worry about it. This introduces numerous complications. For example, closures and higher-order functions are possible, but are much more complex because you need to please the borrow-checker, as well as the typechecker. Haskell, on the other hand, is garbage collected. You get the odd space leak here and there, but for the most part you can just write programs as you intend, without worrying about memory deallocations. I think there's a lot of interest in Rust from communities like Python because it's closer to an imperative style (still (kind-of) has mutable variables, uses loops for control-flow, etc.) The monad-based Haskell code, relying on recursion, higher-order functions, and immutability isn't necessarily harder, but it's probably less what people are used to, so Rust seems like a more gradual step. Moreover, Haskell has *many* more features for ensuring safety and proving correctness at compile time, through GADTs, TypeFamilies, DataKinds, and soon to be, Dependent Types. I would say, if you are opposed to "advanced" type system features, or you *need* memory management and blazing speed, use Rust. If you prefer to write declarative code and ignore lower-level details, and want to use an advanced type-system, Haskell is for you.
You might want to look at the Utrecht Class [Advanced Functional Programming](http://foswiki.cs.uu.nl/foswiki/Afp/WebHome). It covers a lot more than what you can do in a 1-credit class, but it should give you a nice idea of topics for advanced undergraduates or early graduates.
I tend to use Rust when I need its low-level features, or it provides something that Haskell doesn't. For example, it looks like Rust would be easier to [run on Android](https://ghotiphud.github.io/rust/android/cross-compiling/2016/01/06/compiling-rust-to-android.html), and easier to call from C (no GC set-up required). Rust FFI: https://doc.rust-lang.org/book/ffi.html Haskell FFI: https://wiki.haskell.org/Calling_Haskell_from_C
&gt; This thread chain is from someone giving a completely worthless non-answer As I fundamentally disagreed it was a worthless answer, which should be obvious since I backed it up. You didn't get an answer, nobody here told you anything which should make you think they're similar. If anything, you looked it up yourself and with a novice eye determined all more experienced people in this thread are idiots.
[removed]
Rust is better suited for lower-level (systems) programming, where Haskell is better suited for higher-level (application) programming. In between these two levels is a grey area where both languages can be defended. Let's not forget to look at the similarities of the two. Both languages want to eliminate whole categories of bugs at compile time, and (thus) make it harder to write incorrect code. Both are general purpose languages. What really sets Rust apart is it's design principle "non of the features should impact runtime speed", and it's primary purpose of being fit for writing a browser (engine). Where Haskell has more of a research-y origin where laziness and strong typing are it's main ingredients. *edit:* touch-ups
[removed]
Yes, like this: ![Drawing with FLTKHS](/images/arc-windows.png)
Here's the relevant bug against base: https://ghc.haskell.org/trac/ghc/ticket/11009#ticket Depending on the exact thing you want to accomplish, you'll probably need to find some workaround. Switching from lazy to strict bytestrings if possible is one proposed solution.
Pretty sure OP said that in the original post. And TypeScript *is* a language. 
[shapeless allows you to use types of compile-time constants](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#singleton-typed-literals), and I believe in the future (Dotty may already?), you will be able to do things like `"foo".type` or `13.type`. I haven't tried it, but I believe that if you had `val x = "foo"` there *should* be proof available that `x.type &lt;: "foo".type &lt;: String`
These compile times seem ridiculously huge. I can't stand 2 minute compile times let alone multiple days. How do you develop anything? Or am I misunderstanding what you mean?
Not in the presence of lazy IO and closures, I don't think.
I'm not using bytestrings directly, though. I guess the Conduit network module is using them then, but does that mean I can't use it in my project? 
No its completely absurd. Compiling a small project with few dependencies is ok, but stack and yesod are behemoths that are almost uncompilable on a typical arm board. Once you've completed your 12+ hour compile of all the dependencies (perhaps enabling a swapfile for more memory) things are much better, incremental code changes are ok, ie only taking 5-20 minutes. But if anything goes wrong and you have to recompile dependencies because of a version problem or whatever, woe betide you. Issues like this crushed my productivity on arm, and basically I've switched to rust for now. Ed: to be fair yesod is only multiday if you make mistakes or you have to fiddle with versions of packages and recompile. If you get it right on the first try everything takes beween 10 and 20 hours. 
Rust is a low-level language. When you need it, you'll know. (I'm writing my OS kernel in Rust. It would *not* have worked out to write an OS in Haskell.)
I've written an [OS kernel](http://github.com/tathougies/hos) in Haskell (using JHC which outputs some nice ANSI C code). Other than write a memory allocator (which I would have had to do anyway), I think it made things easier. Granted, I've done no benchmarking. Currently, working on getting it to work with the GHC generator.
See [here](https://github.com/crust-os/crust-os). Still very much WIP. I'm in the middle of an enormous refactoring, it should land by Monday.
Lol I love the name. How does it compare as far as progress to Redox?
We have some reviewers who read chapters before they're released. We receive their feedback mostly via email. After chapters are released, we receive feedback via our support email, sometimes our personal email addresses, IRC, and occasionally Twitter (and, once in a while, Reddit). We spend quite a bit of time on reading and responding to feedback, but I'm not sure how to quantify that as a percentage of....of what? Of our overall time spent on the book? I'm not sure what the connection between that and the book's "quality to quantity ratio" is precisely. Most of the feedback we get is very positive about the book's quality; many of the support tickets we get are people finding typos or similar minor errors. 
I'm glad you found it useful!
Any particular example? Link to github for a big project not super informative.
It is way easy to break out of bracket and end up operating on a closed file handle or some such.
Ha, I wrote the web app first on my laptop, then tried to get it to run on the arm. At that point it was sunk costs fallacy all the way. If I work on that stuff again I may rewrite in something else, or maybe at some point cross compiling will start working.. 
I just flipped through a bunch of files and it all seemed logical and clear to me. That's not always the case with projects (in any language). Certainly not the case with the compiler I'm working on. :) https://github.com/cruxlang/crux/tree/master/Crux My first exposure to Haskell was a raytracer ( http://www.nobugs.org/developer/htrace/htrace.hs ) which, to me, epitomized how beautiful Haskell could be, and inspired me to learn it. Of course, it wasn't until a coworker at IMVU showed me how great Haskell could be for production web servers that I actually had a reason to get paid to write it. :) I don't think there's a particular file that is worth calling out -- I just scanned the project and wanted to share that I thought it was clean and elegant. 
Why did you want to get it to compile on ARM?
What about applying the same trick as with ST handles?
Can we see some screenshots?
I found the first chapter of Haskell programming from first principles serves as a good introduction to the subject. I also found the [wyah chapter](http://dev.stephendiehl.com/fun/003_lambda_calculus.html) on the subject useful.
"Representation" (i.e. haskell algebraic datatypes) can take you very far. e.g. Instead of proving that a function expects a list with refinement type `length &gt; 0`, expect a `NonEmpty` [1]. e.g. Instead of storing a low-dimensional `Point` as a `List&lt;Integer&gt;` in Java where `1 &lt;= length &lt;= 3`, define `data Point = Point1D Integer | Point2D Integer Integer | Point3D Integer Integer Integer`, which is easy in Haskell because it has sum types. (These examples aren't hypothetical, I write Java at work, and they're verbatim. No one I know uses JML, but now I wish they did.) Also, LiquidHaskell [2] seems similar in expressiveness to JML. Static length: {-@ length :: xs:[a] -&gt; {v: Int | v = (len xs)} @-} length :: [a] -&gt; Int length [] = 0 length (x:xs) = 1 + length xs Safe head: {-@ head :: {v:[a] | len v &gt; 0} -&gt; a @-} head (x:_) = x head [] = liquidError "impossible" [1] where `data NonEmpty a = a :| [a]`, in https://hackage.haskell.org/package/semigroups-0.18.1/docs/Data-List-NonEmpty.html [2] http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/
Now that I think about it, I may have met you at the orgsync clojure meetup.
Why do you want to use `Cofree`? If you want the most flexible approach why not just use [`Fix`](http://haddock.stackage.org/lts-5.4/unification-fd-0.10.0.1/Data-Functor-Fixedpoint.html#t:Fix) and [`Compose`](http://haddock.stackage.org/lts-5.4/transformers-0.4.2.0/Data-Functor-Compose.html) instead? [EDIT: This seems to correspond to the "Two-level types" approach from EZ Yang's article]. data TermF a r = Var a | Apply r r | ... data AnnotationF ann r = Annotation ann r type Term a = Fix (TermF a) type AnnTerm ann = Fix (Compose (AnnotationF ann) (TermF a)) By the way, `Lambda a (Scope () a bv)` is probably not what you want. You probably want `Lambda (Scope () a bv)`. You may find this haskell-cafe message and linked code useful: https://mail.haskell.org/pipermail/haskell-cafe/2015-July/120583.html http://lpaste.net/136836
&gt; can take you very far ... as far as an FSM can take you :) Note that in your examples, the depth of recursion you can support is constant (0 vs. non-zero or 1..3), which is precisely what I said. When your specification power is an FSM and your language is Turing, mathematically "virtually all" bugs still slip through (as the difference in behavior between the FSM and the Turing machine cannot be specified and cannot therefore be caught). The question of exactly how far an FSM specification actually helps you *in practice* to catch errors in a Turing-complete language is an empirical one (as in theory the answer is "virtually none"), which is my point. Another interesting question is precisely what kind of inferrable type system is most effective in practice, and as all type systems are abstract interpretation, the research applying to abstract interpretation (which is easier to test empirically, and therefore real-world results are relatively abundant) should direct our type-system design. Personally, I'm very skeptical that HM type systems would prove particularly effective vs. other possible type systems, simply because their design was not directed by empirical findings (i.e. it was a result of the question "what a simple inferrable type system can we apply to lambda calculus" rather than "what inferrable type systems would be most effective in preventing most expensive real-world bugs"). &gt; Also, LiquidHaskell seems similar in expressiveness to JML. Of course. Liquid types are a subset of dependent types that can be verified with a model checker (I think SMT solvers, usually). JML is still more expressive (as I think it is complete, like full dependent types), but the question of how the verification is done in practice (manual proof, model checking, abstract interpretation) remains, regardless of whether you specify with types or by other means (such as direct logic a la JML/TLA+/Isabelle). The choice of representing logic as types or other means is also not a mathematical question, as the expressiveness and the chosen verification algorithms are essentially identical, but an empirical one, which has to do more with human cognition than with any mathematical argument. I believe that types have significant advantages, but the answer to which type system we should use is likely not "as strong as we can make it while still being automatically inferrable", because abstract interpretation could always take you further than your types, and so a type system that is too strong may make programming unnecessarily more difficult. For example (although it's probably not a good one), the question of whether a given `int`-typed argument is positive could often be answered by abstract interpretation without burdening the developer with maintaining different types for positive and negative integers. Liquid types seem like an appealing solution to explore, provided that they handle subtyping and flow well. Personally I'm not fond of LiquidHaskell (because it's still Haskell and I'm skeptical of PFP), but I really like the concept of liquid types in general.
This is interesting, but I'd rather go for a refinement types approach, like LiquidHaskell. Wonder if it could ever get integrated into GHC as a compiler plugin?
my font rendering in leksah was so terrible that it gave me an headache. Anyone know how to fix that?
Here's what it looks like for me on linux (gnome 3.16/arc theme): http://imgur.com/tCWENaC
&gt; They allow you rule out things at compile time that Haskell just can't. I believe you *can* do this sort of thing in Haskell: the corresponding abstraction is indexed monads. It's just a lot more painful to work with.
Though they share some dependencies, why not use something like Scotty? If it's an IoT ARM thingy, sure it wouldn't need all the power of yesod?
Its part of a gadget that can be set up outside. The web server is just to control some parameters and whatnot - yesod is massive overkill for this mission. 
Could get to the simplest recursion schemes.. I think 1/2 hour would be generous for each.. (since it's an advanced optional class of interested students) Basic data types. Basic function syntax and `.`. Basic data declarations. Type classes in general. Higher kinded types. Functor. The generic traversal scheme (maybe more like a whole hour). Guessing at 4 hours, and most of that isn't specific to recursion schemes.
That sounds likely! I'm usually the one rattling on about Haskell or PureScript and trying to get everyone to use Schema or core.typed. 
&gt; it's primary purpose of being fit for writing a browser (engine) This isn't exactly true. Whilst one of the initial impetuses behind Rust was to be used for Servo, the situation is more general now and the needs of all kinds of users are factored in to the design. We also have people doing application dev in Rust. Rust tries to do away with the "low-level capable languages should be for low level programming only" false dichotomy and makes it pretty easy to design higher level abstractions and whatnot. Of course, Haskell might still be better suited for this purpose than Rust.
I haven't kept up to speed! Can you givne me a pointer to what dependent types are going to look like in Haskell?
&gt; Was bound but now I'm free Worth it for the pun alone.
IMHO it's looking good because it's consistent in style. And that's kind of expected, it's a very new project with only 4 contributors.
I didn't realize that I've wanted/needed this until now, especially in combination with `diagrams`. Awesome.
It would be an enormous overhaul and huge reversal of design for general types to show up at runtime, so while I have not actually investigated, I very sincerely doubt there even could be any overhead.
Let's go with that ;), when I originally created my account diff. geom. was not a concern of mine.
I installed it from a binary. I did switch from a retina to a non-retina display so that could have been the issue. Also, is there anyway to change the theme? I really like the dark theme shown on leksah.org.
A quick look at STG output of a program that uses Proxy reveals that it's actually passed as an ordinary argument. Is it possible to optimize it away in a lower level? This I don't know..
Good point about liftedness of Proxy. But still, at least in theory, it may be possible for GHC to optimize it. Demand analysis is already collecting information about how arguments are used by the function, so in some cases we can guarantee in compile time that a function can't distinguish `⊥` Proxy from a `Proxy` Proxy.
It's due to the use of Data.Vector.Fixed with large (256) sized vectors. There's a PR in the works to switch to a Nat sized vector instead.
The short answer is... no (*). The long answer is that there is currently an overhaul of the compiler internals ongoing: a new intermediate representation "MIR" is being introduced; and it is expected that once this overhaul is completed then: - rustc will be able to emit a much leaner LLVM IR (reducing LLVM codegen time) - it should pave the way for incremental compilation I believe the initial support has landed, but doubt it'll be finished in the coming month. (*) You might be interested in the `cargo check` command: it just validates the correctness of your program (no codegen) so is much faster.
I think you made a very important point here: it is perfectly possible to use BOTH Haskell and Rust in a single project, because Rust's lack of GC and slim runtime makes it easy to integrate with other languages. As a result, it means that you can write an application with both Haskell and Rust switching from one to the other depending on which you find more ergonomic for a specific task.
There is a special zero width `Proxy#` in `GHC.Prim`: https://hackage.haskell.org/package/ghc-prim-0.4.0.0/docs/GHC-Prim.html#g:26
GHC could do that hypothetically, but I guess there isn't a great demand for such feature. Monomorphic `()` fields are pretty much useless. People sometimes put `Proxy`-s with existential parameters in GADTs for convenience, but they aren't essential there and can be replaced by a helper function. It would make more sense if it were just a consequence of a more general erasure mechanism (one that removes actual runtime indices instead of just `*`-s and lifted kinds). 
The fix was in there! Thanks so much nmkolev.
Point taken! Initial impetuses is much more accurate then primary. :) Sure Rust is usable for higher level stuff, but he is asking /r/haskell :)
vty doesn't let you write a character directly to a location on the screen, so that's really no good for what I'm after. The others don't have a "direct" access layer; they only provided the abstracted interface. I want to have both.
Perfect! Thank you
[removed]
I understood from your post that it's not easy to replace C by Rust in Haskell ffi while you meant that it's hard to replace C by Haskell in Python ffi.
`Proxy#` can be used in the "provably non-\_|\_" cases when you aren't worried about taking an argument of kind *`. When you can bring yourself to use `Proxy#` then it provably will be erased as there is no runtime representation. Anything else would require delicate compiler magic.
What would be the downsides to using it? Also, if it's erased entirely, why do you have to prove its not bottom?
Please add that to the README to increase the chances of someone picking this up.
`Proxy#` simply can't be bottom, it isn't that you have to prove it isn't. It is unlifted; it doesn't inhabit kind *. The downsides are somewhat complicated to explain. `Proxy` has many instances, it is a `Functor`, `Monad`, `Traversable`, etc. all in a nice way that means it has a lot of interesting usecases in its own right. e.g. `Tagged` has an instance in corepresentable profunctor with `Proxy` as its representation, because `Tagged s a` is isomorphic to `Proxy s -&gt; a` (modulo an extra bottom) and `Proxy` has the same shape as `U1` in GHC.Generics. On the other hand `Proxy#` has basically no instances. You have to work with it very much manually. Also, many of the combinators that accept a proxy don't accept `Proxy a`, but rather accept `proxy a`. This lets them take things like `[a]`, but doesn't allow `Proxy# a` since you can't abstract over stuff in kind #. 
Done.
I'm new to Haskell and have zero knowledge of Ogg, but would happily help out bug fixing and the like.
Maybe what GADT are and what the algebra behind them is? When doing flow of control you might want to implement your a lazy and a strict lisp? When I was in such a class I was very disapointed "bleeding edge" and state of the art features where not covered. I haven't learned anything about depended types and that was disppointing. Also you might cover some type theory.
I gave you a concrete example of a grammar where pure parsing is useful despite allowing includes in any *reasonable* grammatical context. The C preprocessor's blind `#include` mechanism doesn't require any significant parsing, other than parsing the `#include` directives themselves. For that trivial case, the first step in my two step process, the pure step, would produce a stream of text blocks interspersed with parsed `#include` directives. The stream can be a simple list, a conduit or pipe, a CPS enumerator, or whatever, depending on your use case. The second step, the IO step, does the actual includes. A more useful preprocessor for C - but then less useful as a general preprocessor - would be more aware of C syntax. So, for example, it might reject something like if (foo) { #include if-ending.txt where `if-ending.txt` provides the closing bracket. EDIT: Fixed the confusing reversed order of the two steps.
Sounds good. House it works out.
Can you provide any references or further information on how Haskell could be implemented without garbage collection? It seems reasonable to me that it's possible, but I can't find any information on how it could be done.
There isn't a single comment
Heh, OK. Well good on you for honestly answering. But of course that completely invalidates the question as meaningful. It ruins falsifiability.
You might want this ? http://dev.stephendiehl.com/hask/#profiling
I just stumbled on this gold nugget from Stephen Diehl: http://dev.stephendiehl.com/hask/#other-languages &gt; Rust &gt; Rust is a general-purpose, multi-paradigm, compiled programming language developed by Mozilla Research. It incorporates many of the foundational ideas of Haskell's type system but uses a more traditional imperative evaluation model. Rust includes type inference, ad-hoc polymorphism, sum types, and option chaining as safe exception handling. Notably Rust lacks higher-kinded types which does not allow many modern functional abstractions to be encoded in the language. Rust does not enforce purity or track effects, but has a system for statically analyzing lifetimes of references informing the efficient compilation of many language constructs to occur without heap allocation. &gt; Main difference: Rust is a modern imperative typed language, Haskell is a modern functional typed language with recent type system. &gt; Rust's main implementation is rustc. &gt; Rust is a statically typed language. &gt; Rust is a general purpose language. &gt; Rust allows polymorphism by means of parametric polymorphism and ad-hoc polymorphism. &gt; Rust is not garbage collected by default, instead uses static semantics to the analyze lifetimes. Optionally supports garbage collection. &gt; Rust is impure by default and does not statically track effects. It does however have static tracking of memory allocations and lifetimes.
Thanks, looks useful Hopefully it doesn't require me to recompile GHC **edit:** Oh man, looks like I will... along with editing the source and the build
Just to check, are you profiling the output of GHC, or the performance of the compiler itself?
It looks like you'll need to use a smaller chunk size than what `conduit` is using by default. You can inline [the definition](http://haddock.stackage.org/lts-5.4/conduit-extra-1.1.10.1/src/Data-Conduit-Binary.html#sourceHandle) of `sourceHandle`, change the chunk size, and even apply it automatically to stdin. That would look something like: smallStdinC :: MonadIO m =&gt; Producer m S.ByteString smallStdinC = loop where loop = do bs &lt;- liftIO (S.hGetSome stdin 4096) if S.null bs then return () else yield bs &gt;&gt; loop
Some of the generics libraries are really slow! I always use uniplate, which is a lot faster and easier to use than most. If that's still not fast enough try geniplate which is a very close API but generates template Haskell code so should have no overhead at all. 
Is there any hope of this somehow allowing documentation to be specified for individual parameters so the --help text can be more helpful? I think that's an important feature for a general purpose command line parsing library.
Perhaps all the field can be wrapped inside a `Doc` type which adds a phantom type level String... But it will force unwrapping. Or an aside type with the same shape but with String as subtypes ?
If Proxy were defined as data Proxy (a :: k) = P# (Proxy# a) then wouldn't `{-# UNPACK #-}` be guaranteed to elide it from structures?
You should have mentioned Alternative. That's one of the major features of a parsing library, the ability to try again from a failure. (Also, it gives you `some` and `many` for free, which can be useful in parsers.)
Or using the GHC API to access the haddock comments
&gt; Rust allows polymorphism by means of parametric polymorphism and ad-hoc polymorphism. As I understand the term 'ad-hoc polymorphism,' Rust does not support it. I think this quote must be referring to trait objects, but I think of 'ad-hoc polymorphism' as function overloading and possibly inheritance, neither of which are features of Rust.
&gt; Or an aside type with the same shape but with String as subtypes ? Perhaps we could parameterize the main type with a functor and wrap each field in the functor: data Example f = Example { foo :: f Int, bar :: f Double } deriving (Generic, Show) The documentation could given as a **Example (Const String)** value, the parsed parameters returned as a **Example Identity** value. But it would complicate the type and still require unwrapping.
The wrapper constructor accepts `Proxy` arguments, but then once that gets inlined into some kind of call site and case analysis gets done most if not all of this should vanish in practice. This relies on the wrapper constructor call getting inlined. For that you probably have to eta-expand `foo`, and then here the fact that those are called with explicit `Proxy` arguments will let known-case optimization finish simplifying everything. If I eta expand `foo` to get `bar` you get the following core in -O2: Main.bar :: GHC.Types.Int -&gt; Main.Foo [GblId, Arity=1, Caf=NoCafRefs, Str=DmdType &lt;S,1*U(U)&gt;m, Unf=OtherCon []] = \r srt:SRT:[] [a_s2XY] case a_s2XY of _ [Occ=Dead] { GHC.Types.I# dt1_s2Y0 [Occ=Once] -&gt; Main.Foo [dt1_s2Y0]; }; Notice how `bar` never even mentions `Proxy` and does precisely the right amount of work. The naïve `foo` above, however yields: Main.foo :: GHC.Types.Int -&gt; Main.Foo [GblId, Arity=1, Caf=NoCafRefs, Str=DmdType, Unf=OtherCon []] = \r srt:SRT:[] [eta_B1] Main.$WFoo Data.Proxy.Proxy Data.Proxy.Proxy Data.Proxy.Proxy Data.Proxy.Proxy eta_B1; While `ghc` creates the eta expansion variable for the constructor, it doesn't try to optimize the definition afterwards. If your data constructor has any fields like this that erase in whole or in part through a wrapper, then it is usually worth it to eta-expand smart constructors like this.
The code is a bit tangled but I do have a GADT of actions. The GADT is parametrized with a couple of type level lists and there are combinators that combine actions into a bigger action if they are compatible according to some rule. Then I use rebindable syntax and pretend it's a monad. It's great up until case expressions.
`vinyl ` records abstract that pattern: type Person f = Rec f [String, Nat] Or: type Person for = Rec f ["name" ::: String, "age" ::: Nat] (Where `:::` is only in `frames` I think.) But the advantage of the generic instance is its compatibility with existing data types. 
Oh. That stuff looks very useful. If so, can someone please update the wiki to include it?
Yes, I'm really looking forward to using this feature!
This is being added to my haskell toolbox. So handy! Thanks
Wow. Very nice for many of the most common use cases. Huh. For years there was only `System.Console.GetOpt` and no one batted an eyelash. Then /u/ndm opened the floodgate with [cmdargs](http://hackage.haskell.org/package/cmdargs), and since then the flow of command line arg parser libraries hasn't stopped. Yet this library still managed to add something really new and useful.
Outstanding!
I think that your problem is that this is not tail recursion. anyChar *&gt; anyText &lt;|&gt; pure () anyChar *&gt; (anyChar *&gt; anyText &lt;|&gt; pure ()) &lt;|&gt; pure () anyChar *&gt; (anyChar *&gt; (anyChar *&gt; anyText &lt;|&gt; pure ()) &lt;|&gt; pure ()) &lt;|&gt; pure () ... What about ...? skipWhile (const true)
A solution to allows meta information (like short version, help etc ...) would be to had a new Typeclass (or maybe reuse the ParseRecord one) and a function `String -&gt; MetaData` (string being the name of the field, or `Either String Int` to also handle positional argument.
Without seeing more code I can't really give any advice that hasn't already been given :)
&gt; You omitted your implementation of anyText. ...`anyText` is right there!
I approve of this naming scheme.
Recursion is a difficult-to-understand concept for FP/Haskell programmers :)
Indeed. Presumably what is happening is that it attempts to parse using the recursive `anyChar`s. This fails so it goes all the way back to the beginning, just to return `pure ()`. In order to go back to the beginning it must retain the whole 1 GB stream in memory.
Could you get around unwrapping with type families? {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE TypeFamilyDependencies #-} module Test where type family Identity a = result | result -&gt; a where Identity a = a type family Const a b = result | result -&gt; a where Const a b = a 
Could also trivially use template-haskell + type aliases for this: λ&gt; :set -XDataKinds -XKindSignatures λ&gt; import GHC.TypeLits λ&gt; type Doc a (docs :: Symbol) = a λ&gt; data X = X { foo :: Doc Int "Hello!" } λ&gt; $(reify ''X &gt;&gt;= stringE . show) "TyConI (DataD [] Ghci8.X [] [RecC Ghci8.X [(Ghci8.foo,NotStrict,AppT (AppT (ConT Ghci7.Doc) (ConT GHC.Types.Int)) (LitT (StrTyLit \"Hello!\")))]] [])" That would mean there's no unwrapping. Technically the doc string has no semantic information so it shouldn't ideally affect the shape of the actual data structure. It seems like that's exactly the purpose of `type`, just for readability anyway.
Yeah, one of the things I thought about doing with [ReadArgs](https://hackage.haskell.org/package/ReadArgs) if I ever got the time was to add annotations with Symbols and PartialTypeSignatures: (name, T count) &lt;- readArgs :: (_, Tagged "number of times" _) replicateM count $ putStrLn name 
Great PR! There were a lot of great suggestions in there, highlighted and not. I have updated the code with improvements. I also asked a number of questions on the PR. I think the biggest things in there that I learned were Text.Read.readMaybe and your alternative data pattern in the code using it. I also proposed a third variation of `Board.nextTo`. You spent good time on this, and I appreciate the feedback.
My `parseargs` has been out there for a lot longer than dumb ol' `cmdargs`. [Sobs quietly]
You're welcome!
You aren't prevented. It is just that it won't give you the average of anything, since the second appearance of `stream` requires you to run the stream a second time. So given a stream like ints :: Stream Int IO () ints = Stream go () where go () = do m &lt;- fmap readMaybe getLine case m of Nothing -&gt; return (Done ()) Just n -&gt; return (Yield () n) which reads Ints from stdin until the user types a non-Int, and given lengthS = foldlS (\n _ -&gt; n+1) 0 avg stream = div &lt;$&gt; sumS stream &lt;*&gt; lengthS stream I will see, e.g. &gt;&gt;&gt; avg ints 1 2 3 q &lt;- unparsable element ends first stream 1 q &lt;- unparsable element ends second stream 6 &lt;- sum of the first stream / length of the second which is not what is desired. 
At the moment It's just tedious, as I'm more so profiling multiple versions and comparing the compilation times, so I have download a bunch of different versions of the source and then make the appropriate changes to all of them then build them all.
That works great. Thank you for the help!
I'm not sure. This is probably the only implementation to read/write ogg files in pure Haskell and it is sad to saw it abandoned. We are using it for discovering bugs in OGG implementations with [our fuzzer] (http://quickfuzz.org/) so any improvement will allow to cover more features in the specification and to discover more interesting bugs. 
Forked and got it building with something more recent 7.8 and 7.10 on travis [here](https://github.com/tmcgilchrist/hogg) Has Conrad said he isn't maintaining this anymore? Submitted a PR [here](https://github.com/kfish/hogg/pull/2). I know Conrad from the functional programming group in Sydney, can ask him in person if there's no response. Also the maintainer is still listed as Conrad on hackage. https://hackage.haskell.org/package/hogg
I see, thank you.
I think that's correct, with the caveat that I know very little about transducers (I read up on them a while ago, but don't remember main of the details). However, I think you're referring to the `Vegito` module's implementation, not `Gotenks`. `Gotenks` is just a reformulation of conduit, which is coroutine based and therefore has no bias towards either the producer or consumer of data. Interestingly, the iteratee/enumerator approach is also very much focused on the consumer of the data, making the `Iteratee` the first-class citizen of the library, providing a convenient `Monad` instance for it, and expressing a data producer as a function that transformers a consumer. I've theorized for a while that making the producer first class (like the approach in my blog post does) works out better.
&gt; redective doesn't seem to work for me at the moment, so I can't see its full functionality at the moment. I find [Redective](http://www.redective.com/) more informative than [Reddit Investigator](http://www.redditinvestigator.com/). I feel that redective's report could be improved. &gt; However, there is a reddit package on hackage. Thanks. I was led to the [ANN post for `reddit`](https://www.reddit.com/r/haskell/comments/3e4g6s/ann_the_reddit_library_a_package_for_interfacing/) The ANN link points to two consumers: /u/intolerable-bot (cf. https://github.com/intolerable by /u/Intolerable) and /u/whatarethebest (by /u/tejon) Now that I have located some consumers for the reddit package, I have decent starting points. The comment suggests that [Heroku](https://www.heroku.com/) and [Halcyon](https://halcyon.sh/#halcyon) are a good starting points for *hosting* a haskell-based bot or an app. ----- Btw, I am not so much interested in running a bot but in *investigating* (or *summarizing*) activities of a user (or a subreddit) much like what `redective` does (and hopefully more). I hope the `reddit` library has the right set of functions to achieve what I want to.
That's correct. Note that you can also just check the Stackage page for a package, which contains reverse dependencies (as well as dependencies themselves), e.g.: * https://www.stackage.org/package/aeson#reverse-dependencies * https://www.stackage.org/package/reddit#reverse-dependencies
A few years ago, Oleg Kiselyov proposed its own producer-centered streaming abstraction, "generators": https://www.cs.indiana.edu/~sabry/papers/yield-pp.pdf but I'm not sure how they compare. Interestingly, in this comment he says that his generators cannot be zipped, while iteratees can: http://lambda-the-ultimate.org/node/4690#comment-74402 
&gt; https://www.stackage.org/package/aeson#reverse-dependencies &gt; https://www.stackage.org/package/reddit#reverse-dependencies Thanks. The `used by` section of `aeson` presents me with a wall-of-text. This is overwhelming. I wish it presented me with a table that contains both the name of the libary and a quick one-line summary of what the library is. Such a presentation (much along the lines of http://elpa.gnu.org/packages/) exploration or discovery of the haskell eco-system. Btw, the table in http://packdeps.haskellers.com/reverse gives me a use-count for a package. That tells me how popular or stable a package is. Very informative. Thanks once again. 
Really? For small programs maybe, but would you like to have to maintain a program full of magic numbers that you didn't write? Recently in a C program I maintain, I noticed a mistake: sockets were initialized to 0: bad idea , 0 is a valid socket value so there were random errors, but the change of the default socket value wasn't easy as the code was full of these magic numbers..
I use https://github.com/commercialhaskell/stack-templates to generate my projects. They generate a directory structure from an `hsfile` (I don't know what their formatting engine is, but I think it has conditionals at least). What features do you want in a templating engine? I tend to prefer as much work done in the programming language as possible, versus the format. e.g. you could use https://hackage.haskell.org/package/interpolatedstring-perl6-1.0.0/docs/Text-InterpolatedString-Perl6.html and interpolate a sum type for conditionals, like so: {-# LANGUAGE QuasiQuotes, ExtendedDefaultRules, LambdaCase #-} import Text.InterpolatedString.Perl6 (qq) init :: CabalConfig -&gt; String init CabalConfig{..} = [qq| name: {cabalName} {stanza cabalStanza} |] stanza :: StanzaConfig -&gt; String stanza = \case LibraryConfig dependencies -&gt; [qq| library build-depends: {dependencies} |] ExecutableConfig name dependencies -&gt; [qq| executable {name} build-depends: {dependencies} |] data CabalConfig = CabalConfig { cabalName :: String, cabalStanza :: StanzaConfig } data StanzaConfig = LibraryConfig [String] | ExecutablesConfig String [String] main = do s &lt;- getCabalConfigFromCmdlnOrFIle &gt;&gt;= init writeFile “project.config” s 
This is the problem which I also faced. Maybe you can have a look at https://github.com/dbushenko/trurl -- I'm developing this project for creating general templates for projects as well as files. I'm already using it for my hobby projects and even gave a talk on it on fby(by)'2015, see the tool working here https://www.youtube.com/watch?v=Sc3ci8j7Wok
Oh it's recursive. I see, thanks. Well, in any case it turns out that attoparsec will always hold the entire input in memory, as pointed out by /u/Yuras.
Gabriel this is really nice! Can I make a request? I see from the source code that the parts depending on GTK can very easily be separated out into a separate module. Could you please do that? In fact, you could quite easily even separate this into two different packages: typed-spreadsheet-core (containing also the examples), and typed-spreadsheet-gtk. You can guess what would almost certainly happen immediately after that...
GHC has `RecordWildCards` too. And IIRC even before Facebook did hack, it had to ban e.g. `eval` from their `HipHop` PHP variant.
That's very helpful, thanks!
That makes sense: in iteratees, the consumer is the first class citizen. But another way of saying it is that it's _underpowered_. It can request more data, but cannot control the flow of execution at all. This is why in enumerator you can write an exception safe `enumFile` (which produces a stream of bytes) but not an `iterFile` to consume that stream (since it can't control the full flow of control). In conduit and pipes, none of the individual components fully control the flow of execution. You cannot write an exception-safe `sourceFile` _or_ `sinkFile` in conduit without using something like resourcet. But by taking that power away from the primary abstraction, you get a lot more flexibility in what you can build on top, such as zippable Sources and Sinks. What vegito here is adding is a new dimension: not only does the conduit/pipes approach make some things impossible (e.g. `enumFile`) while making other things possible (e.g. `ZipSink`), but it also makes some things less efficient without rewrite rules (because of what GHC is good at optimizing).
Curious about a particular design decision: why wrap `GotenksF` with `Gotenks`? Why not just write using the `GotenksF` datatype? What does `forall b. (r -&gt; GotenksF i o m b) -&gt; GotenksF i o m b` give you that `GotenksF` doesn't? Thanks! Great post!
This is not possible currently, I'm pretty sure. Debugging Haskell can be done with GHCi.
I thought the purpose for avoiding accumulation inside the pipeline was that there's no way to detect upstream termination. In other words, in pipes `await` returns `i` and in conduit it returns `Maybe i`. The efficiency issue is an interesting one, I hadn't heard it mentioned before.
Sorry ... I was about to make a major revision in the text I just deleted, which was: `GotenksF` / `Gotenks` are `Pipe` / `ConduitM` from `conduit` [with slight modification]( https://github.com/snoyberg/conduit/blob/master/conduit/Data/Conduit/Internal/Conduit.hs#L119) The type that the user principally faces in conduit is `Codensity (Pipe a b m)` (simplifying to leave considerations of leftovers out) [^ 1] The practical purpose of applying the codensity transformation is to make accumulation within a `conduit` associate correctly. Without it &gt;&gt;&gt; mapM_ C.yield [1..5] $$ replicateM 5 C.await [Just 1,Just 2,Just 3,Just 4,Just 5] would involve repeated traversal of the constructed list. Interestingly `pipes` doesn't bother with this, preferring *functions* like `P.toListM`, `L.purely P.fold L.vector`, etc. for cases where accumulation is desired, but making systematic war on all forms of accumulation inside the pipeline. For the rest it advises explicit use of `Codensity`. [^ 1]: i.e. again leaving aside leftovers etc, `Codensity (FreeT (Sum ((,) a) ((-&gt;) b) m)`
I expect it is true that the pressure for a codensity transformation with conduit is greater, because there are more uses for accumulation inside a pipeline. The basic difference is that pipes has a separate discipline for folding over, breaking and managing producers/sources which is equal in status to piping. Fwiw because I separated bits of this out so people could think about it separately http://hackage.haskell.org/package/streaming-0.1.4.0/docs/Streaming-Prelude.html https://hackage.haskell.org/package/streaming-bytestring-0.1.3.0 All of this is contained in pipes and is a crucial part of the discipline of using them; in one sense the packages I link are a pipes-tutorial. 
You have a point indeed. 
Scala has working plug-ins for IntelliJ and Eclipse and that's all that's needed. Haskell has…well, it has EMACS and Vim modes, and it has impenetrable thickets of self-contradictory instructions for getting it to kind-of play along with things like Atom, and it does have Leksah, which is slow and ugly and…mainly isn't IntelliJ nor Eclipse, which is what industry mostly wants—like it or not.
See my response to the parent re Smalltalk. What a missed opportunity…
LOL. I'm good, but thanks. :-)
[Scrap your boilerplate](http://research.microsoft.com/en-us/um/people/simonpj/papers/hmap/) -papers are good starting point.
Stack uses mustache.
http://stackoverflow.com/questions/5770168/templating-packages-for-haskell lists some more options.
Snoyman eliminated operators =$=, $$, &gt;-&gt;, &lt;-&lt; but instead used standard function application. Would this be applicable in the Streaming/Pipes eco-system?
Very helpful, but it would've been also useful to add some of the classes hierarchy and their associated operations (Functor, Applicative, Monad, Monoid, State, MonadTrans, etc.).
I would like to think that [protobuf](https://github.com/alphaHeavy/protobuf) is a nice example.
My favourite has to be [`generics-eot`](https://hackage.haskell.org/package/generics-eot) because it uses generics to make using generics easier.
&gt; We now get to trivially prove the category laws, since we're just using function composition! I'm afraid this isn't what you think it is. `Stream i m r -&gt; Stream o m r` is a significantly bigger type than `Proxy () i () o m r` (or the Conduit equivalent) and therefore the category law is correspondingly less powerful. This claim about trivial category laws is a bit like saying "If we use functions `m a -&gt; m b` then we get to trivially prove the monad laws".
Not related to your job offer directly but from the description Unison sounds like something that is in danger to dying of being too much of a walled garden, similar to Smalltalk. Might have to watch out for that.
What's your reason? My reasons for using images as code is: 1. Forces the learner to manually input the code (which I think is useful for learning). 2. Looks a certain way.
Mainly copy&amp;paste. I do not think people will enter the code manually themselves. I think they are more likely to experiment with it if they could copy and paste it and then modify it, especially with the longer examples. I know this kind of attitude from teachers and university professors and quite frankly even if stuff is typed manually it is perfectly possible to do that without engaging any higher brain functions. It is even possible to do so with whiteboard to handwriting so it should be even easier to mentally automate it with this format. They also do look quite bad. If you do want images with this background colour maybe you could switch the page itself to a similar background at least.
From a quick read a lot of your post seems to be about learning curves of individual technologies. I was more thinking about the problems arising when you want to replace more than one technology at the same time. You can replace the user's text editor, version control tool, programming language,... individually a lot more easily than you can replace all of them at the same time.
Can you please make an email list for Unison in case somebody wants to get infrequent updates?
That also makes the page much heavier, which is annoying on a phone or any slow connection.
[System.Envy](http://hackage.haskell.org/package/envy-1.1.0.0/docs/System-Envy.html) uses Generics to map environment variables onto Haskell ADTs
To expand on the sum example, there are two more possible implementations that you should look at: Using a strict left fold (faster than the foldr version and equally elegant ): sum = foldl' (+) 0 In tail recursive style (by far the fastest although not as pretty): sum = sumTR 0 where sumTR ac [] = ac sumTR ac (x:xs) = sumTR (ac + x) xs
Hi Paul! I was actually planning to start contributing to unisonweb after my current project finishes. I didn't realize you'd actually pay for such a thing.
&gt; I was more thinking about the problems arising when you want to replace more than one technology at the same time. Gotcha. &gt; You can replace the user's text editor, version control tool, programming language,... individually a lot more easily than you can replace all of them at the same time. That is very true. Unfortunately, Unison does need to somewhat replace several things at once since the big benefits come from a cohesive rethink, not just an incremental improvement to only one aspect of the overall programming experience. There is still a lot of room for rolling things out incrementally, though, even given the constraints of the project. And I've also found that going back to the beginning ends up making a lot of things vastly simpler to implement, so you can still deliver something useful pretty quickly. With 2-3 developers on board, I could see releasing the first product built on Unison within a year or so, even though the platform itself won't be fully baked yet in all areas. That would be something to strategize on with whoever comes on board. :)
line by line debugging (more or less, AST directed debugging anyway) is possible in GHCi.
Don't refer to your product as revolutionary. It is off-putting. Every shitty company in Silicon Valley has a "revolutionary" product.
Well for starters you should probably read the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia).
It would be great to use annotations but GHC.Generics does not give access to them either. 
I think maybe the reason for the feedback to your flow library is that it seems to just give some different names to operators. Even if the selection of symbols and fixity is better, unless they offer large benefits, it's just painting the bikeshed and potentially harming communication. Still could be a good idea in the long long term, but that is the source of the feedback.
That's a good point. I made a conscious effort to make Neon "weirder". There's no sense in it existing if it's basically the same as what's already available. That's why I flipped function argument order (`"he" :add "llo"`), split type classes (`(Bottom a, Top a) =&gt; ...`), *and* renamed the type classes (`type Add a -- Semigroup`). 
Yeah, I think the author would do well to re-read "No Silver Bullet". That said, I think it's interesting!
Yeah I actually think the product is pretty cool. "Revolutionary" not so sure. Either way it was just an honest piece of advise. I know a lot of people are growing very tired of the Silicon Valley culture and using words like revolutionary are part of the reason why.
"They also look quite bad" &lt;- this is your opinion. Personally I like how they look. But yeah, I think the point is made. I'm convinced. I'll make the adjustments. It will involve rebuilding it which isn't particularly trivial, and I'm not sure if it'd improve it *that* much comparatively, but it's probably a good idea anyway as we're going to make more of these over time. Will also have to adjust the way the (PDF) book is generated.
With the exception that the original scrap-your-boilerplate approach involves radically different code. I'm pretty conversant in GHC.Generics but still find `syb`-based code incomprehensible, and I'm sure a lot of people are the other way around.
Is there anything an interested student can do to help?
At this point we're mostly looking to see how many slots we'll be able to fund off community donations and what sponsors we'll be able to find. We should post up a brainstorming thread for project ideas and to help mentors and students connect soon, but it'd probably be better to have a sense of scale first.
Can you motivate a little your approaches to type classes? (Specifically lawlessness, no hierarchy, functions in type classes.) I agree with you about naming (e.g. https://www.reddit.com/r/haskell/comments/3pc5p9/question_is_there_a_good_reason_why_fmap_map/cw5ps9r ).
https://www.reddit.com/r/haskell/comments/48eurt/haskell_summer_of_code/ Please follow up there with questions. That way we can consolidate things.
Possibly&amp;mdash;check here: https://www.reddit.com/r/haskell/comments/48eurt/haskell_summer_of_code/
&gt; define what "bigger type" means There's a non-surjective (structure preserving) injective function from `Proxy () i () o m r` to `Stream i m r -&gt; Stream o m r`, just like there's a non-surjective (structure preserving) injective function from `a -&gt; IO b` to `IO a -&gt; IO b`. I'll have to explain in more detail when I have more time ...
I have dealt with funding for open source projects in the past, and my experience is that it can help significantly to have a nice easy link to a paypal or what-have-you payment system on the front page. I realize that the popular payment systems (paypal, google, stripe) can take a significant chunk of the money donated, but you will probably get more if you use them. That ClickAndPledge thing, though, is... really funky and dodgy looking. Especially because I clicked on a link to it from a _wiki_! I strongly suggest a link from the front page of haskell.org, ideally with a little text area to enter how much you'd like to donate.
That clickandpledge thing is what we get from SPI. Now that Haskell.org is incorporated as a 501(c)(3) proper, there's a plan to add a system more like what you describe using stripe...
Good point! (That wiki page is locked, but it would help give users a better sense of security if it wasn't a wiki link.) In addition to the rather rinky-dink looking clickandpledge donation link from the wiki above, checks payable to Haskell.org, Inc. can be mailed to: Haskell.org, Inc. c/o Ryan Trinkle 434 E 72nd St #4B New York, NY 10021
I think what industry needs is something like Haskell but with a good records system built upon structural typing. Nominal approaches (especially popular in OOP) are not satisfactory when trying to type e.g. the result of a query or data transformation. Most of what we do in industry is data transformation. Structural typing also does not suffer from the "anti-modular" compliant of nominal typing. 
Also, patreon is working really well for YouTube people. I'm not sure how well it'll work for FOSS projects though. 
Well, to be honest, while encouraging new contributors is very important, I would rather spend the funds on infrastructure improvements by established contributors. As a hypothetical example, if Simon Peyton Jones were to apply for a small project whose goal is to improve the implementation of weak pointers in GHCJS, I would wholeheartedly endorse that -- he is clearly knowledgeable about both weak pointers and the gory details of implementing them in a Haskell-RTS (but probably has other interesting things to do at the moment. ;-)) Google Summer of Code projects tend to be hit or miss, and depend very much on the quality of the student. Some projects work out better than expected, while some just don't pan out. I don't think that the output of previous Summers of Code is unequivocally successful. Which is fine if Google takes a dollar hose and points it all over our small garden, but with limited resources, I think a more precise approach to watering flowers (projects) is more appropriate. There is plenty of room for failure anyway. One thing I noticed with Google Summer of Code is that long-term continuity does not seem to work very well. I think a format where individual Haskell projects submit proposals to a "bazaar of things that would be nice to have done" would be preferable. 
&gt; "They also look quite bad" &lt;- this is your opinion. Personally I like how they look. To clarify. The images themselves don't look bad. The way the dark background integrates with the otherwise white page does.
We have started exploring ideas along these lines recently, actually. E.g. We recently paid for some contract work on perf.haskell.org. Partially this was a trial balloon to figure out the vagaries of the process, partially it was work that needed to happen and wouldn't otherwise get done. We've also been talking to the IHG and Well-Typed about helping out with organizing work on that front. One of the ideas that has been put forth on this front was just such a "bazaar of things". Before we were hit with the unaccepted-for-GSoC curveball, we were working towards the idea of picking a successful GSoC project and paying to help carry it forward. That is one way that we could help focus our efforts. Regardless, we're going to be looking at a _lot_ fewer projects than the usual summer of code. In years past, while the summer of code projects as a whole may have been somewhat hit or miss, the top rated GSoC projects we accepted have been remarkably solid. We're simply going to be forced to be much more selective, given available funding. Putting up a GSoC slot worth of funding plus whatever we can raise from the community won't appreciably compromise our ability to do any of these other things.
I kind of agree with this. However, the internet has numerous places where this is the case. I quite like it. :) Take a look at this: http://elm-lang.org/docs/syntax Conversely, I suppose, you could say "well github looks much easier to read"... which is white on white... I guess you have to make *some* choices... ;) However, there's also things like this, which using the "lights" button, you can choose to view it in dark or light styles: http://elm-lang.org/examples/unordered-list There it's obviously not such a problem, because the whole left section is all source code. I'll take what you're saying into consideration, though. You're not the first to point out that preference... it's probably much "easier on the eyes" to not have such high contrast source code.
~~gittip~~ [gratipay](https://gratipay.com/) exists in the same design space and is definitely oriented towards FOSS projects
If you have not played with http://lambdacube3d.com/editor.html, you are severely missing out.
Nice! I had to see how far I could go with this idea in C++14. Turns out quite far in only a couple of hours. It clearly showed me the benefit of monadic bind (in the do notation) for separating argument parsing, printing and early returning. Thanks for this neat idea! Here it is in case anyone is interested: https://github.com/daniel-j-h/argparse-generic
&gt; The first is that there will be a rather sharp dip in income for the next year for haskell.org. Last year's GSoC accounted for $9500 worth of income towards managing servers and the like, but we will not receive such a booster shot this year. I remember there being a thread on /r/haskell about Microsoft looking for open-source projects to support with server hosting. What happened with that?
These are great to see, one common thing I wonder while learning Haskell is what using it as a backend for other web front ends would look like. Near as I can tell (I haven't researched this much yet), there is GHCJS, Purescript, Haste, and Elm, all of which have a similarity to Haskell. -I'm getting on a tangent. Thank you for posting this series of blog posts, it will definitely come in handy for me in a week or two (I'm currently blitzing through HaskellBook with the hopes are starting a webapp in couple weeks and will have to choose a front end).
Sure! I give brief (one-sentence) explanations in [the readme](https://github.com/tfausak/purescript-neon/blob/v0.4.1/README.md). I'll expand on those. - Lawlessness: The `Eq` type class has laws, but you don't have to look far to find scofflaw instances (`Float`, for instance). Part of the problem is the word "law". Laws are really suggestions about how to implement your instances and how to use the type class. Which is a problem that every other language solves with documentation. For instance, let's say you want to implement `Ord` such that `x &lt; y` does not imply that `x &lt;= y`. Then you're writing an instance that doesn't obey the *laws*, which sounds scary. This is probably a bad idea, but it'd be a bad idea in any language, with or without laws. - No hierarchy: This is [composition over inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance) applied to type classes. When you remove the hierarchy, your type signatures start to reveal more about your functions. This is similar to splitting `IO` into various `Eff` types. For example, splitting `zero` from `add` allowed me to write [`absoluteValue`](https://pursuit.purescript.org/packages/purescript-neon/0.4.1/docs/Neon.Helper#v:absoluteValue) without requiring `add`. You can probably guess the implementation based on the type signature alone. - Functions in type classes: I get frustrated every time I have to import a module qualified to use some function that isn't overloaded. Type classes are the only way to avoid this that I know of. The downside is that the errors message get a lot worse. And you sometimes have to explicitly give type signatures when it feels like you shouldn't have to. The example I keep using is [`singleton`](https://pursuit.purescript.org/search?q=singleton). Look how many times it's defined! Granted, Neon doesn't define it yet, but when it does it'll be in a typeclass. - Naming: I know we're already in agreement on this one, but I wanted to remark on it anyway. I think using type class names that suggest their functions names is the way to go for two reasons. First, the error messages are better. Would two rather read "No instance for Map (YourContainer a)" or "No instance for Functor (YourContainer a)"? Second, you can tap into that 70 years of mathematical literature through documentation. Since `Add` is synonymous with `Semigroup`, I note that in [the documentation](https://pursuit.purescript.org/packages/purescript-neon/0.2.1/docs/Neon.Class.Add).
Is `Stream i m r -&gt; Stream o m r ` ever used? The main operation translates a `Gotenks` into the usual ListT operation: toTransform :: Applicative m =&gt; Gotenks i o m r -&gt; Stream i m () -&gt; Stream o m r we never envisage something like mapS :: Functor m =&gt; (o -&gt; u) -&gt; Stream o m r -&gt; Stream u m r being used, except at r = (). In general conduit `map` only works on a stream that has obliterated its return value, which is the condition of `$=` and `$$` and co. Which, again, is why I cannot apply one conduit to the first 100 elements of a stream, and another to the second 1000, and cannot write a streaming `lines` and so on. 
/u/dcoutts do you know why these benchmarks are not showing that `binary-serialize-cbor` is any faster than `binary`/`cereal`?
The fact that this product only works in a browser is a no-starter for me.
[Here](http://www.haskellforall.com/2015/10/basic-haskell-examples.html) are a few simple Haskell programs you might be able to learn from. I will self plug [the source for my own chip-8 emulator](https://github.com/soupi/chip-8), hoping it will help you as well.
While I agree that describing a product as 'revolutionary' while it's still in development is perhaps a bit questionable, I don't think this is comparable to how shitty companies in Silicon Valley use the word. Seeing companies describe their products as "revolutionary" is only irritating because it's transparently obvious that people have just slapped the word on without thinking about it too much, and in most cases, fail to present any reasonable argument that it is actually revolutionary. By contrast, the author's blog has plenty of well-argued, thoughtful pieces about what's wrong with programming environments, how those problems might be fixed, and how much more effective programming environments could be if we managed to fix them.
Why should your tail-recursive implementation be any faster than foldl'?
Use [hlint](https://github.com/ndmitchell/hlint#readme)! This will catch a ton of these kinds of things and warn you about it.
Thanks! That is super encouraging. :) I hope that you'll consider getting involved in the open source project at some point, or if you have more tolerance for risk, consider applying for one of those open roles at [unisonweb.org/jobs](http://unisonweb.org/jobs).
I THOUGHT it was my wording, as I was thinking of a name better than "generic programming", and I was very proud of it, but a simple Google search revealed that people had come up with the phrase as far back as 98. The fact that it comes naturally to people probably indicates that it is the best term for it.
I know that but it's still work to move stuff over and learn new APIs/dashboard/etc. Depending on balance of labor and cash available, may or may not be worth it. I don't know anything so I couldn't say myself.
Getting a minimum viable product is exactly what you should do. However, I will have some constraints for any editor I use: * It must work without any active internet/network connection (ever). * It must work as intended even with my mouse unplugged. I.e. I NEVER want to have to use my mouse. * Any sensible slowdown makes me throw away the product: &gt; 200ms.
Even an Appveyor-like build system for Windows systems to build Hackage packages on would be amazing.
What editor do they use? It is convenient how quickly it displays expressions type.
The source code for it is [here][editor-source]. Cursorially looking at the source code, it seems like it just runs a websocket server (the `compiler-service`) type-infers and compiles LC code input and responds with a giant annotated JSON syntax tree. On the client side, they have some PureScript that talks to the backend and handles displaying the types etc. [editor-source]: https://github.com/lambdacube3d/lambdacube-editor
Great series! The links at the end of the first post don't work. Don't know about others.
Even structural typing where you have to explicitly downcast would be useful (in fact I'd prefer it). Or are you suggesting that's not called "structural typing"?
Out of interest. What do you think a better alternative would be? (By that I mean in addition to the Ad-hoc / parametric stuff Haskell already has)
We use generics in [unbound-generics](https://hackage.haskell.org/package/unbound-generics) to implement alpha-renaming, alpha-comparison and (optionally) structural substitution. For example if you write a datatype like type Var = Name Expr -- variables stand for expressions data Expr = V Var | I Int | Arith ArithOp Expr Expr | Lam Fun | App Expr Expr | If0 Expr Expr Expr | Letrec (Bind (Rec (Var, Embed Fun)) Expr) deriving (Typeable, Generic, Show) newtype Fun = Fun (Bind Var Expr) deriving (Typeable, Generic, Show) (Where `Bind`,`Rec`, and `Embed` are combinators from the library:`Bind p t` means that *p* specifies a pattern of variables that are bound in *t*, `Rec p` means that all the variables of *p* are scope over *p* itself and `Embed` embeds a term inside a pattern) Then you just say: instance Alpha Expr instance Alpha Fun And you get a Scheme-like lambda calculus with variables, integers, arithmetic, lambdas and application, if-expressions and *letrec* where all the usual renaming, comparison and substitution functions are constructed for you based on the scoping combinators! Disclaimer: All the [clever ideas (PDF)](http://ozark.hendrix.edu/~yorgey/pub/unbound.pdf) are from the [unbound](https://hackage.haskell.org/package/unbound), all the bugs in porting to GHC generics are mine.
You might want to use the [the random monad](https://hackage.haskell.org/package/MonadRandom-0.4.2.2/docs/Control-Monad-Random-Class.html#t:MonadRandom) instead, which will thread the seed for you.
I think you mean explicitly upcast? Downcasting is usually explicit. Note that a larger record type is a subtype of a smaller one—the empty record is a supertype of all records. And yeah, it might help keep things sane. Type systems with subtyping usually have implicit upcasting, so I dunno if it counts as subtyping if it’s explicit, but I don’t see why not. 
I've read all three of them now and I love it. I used to dismiss the rank-2 trick as magic but now it makes sense why it makes things work. You also have a great writing style.
&gt; I think you mean explicitly upcast? Maybe. I mean whatever direction is converting `{| x :: a, ... |}` to `{| x :: a |}`, for example. I'm suggesting this not be done implicitly (altough it could be done with a polymorphic function).
No problem! It was nice to hear someone was interested in finding and reading them again.
Why can't there be more than one default implementation for a class method (with different assumption constraints)? I always believed I could have more than one :-( 
Thanks for reference, I will have a look.
Your Neon examples don't have an explicit forall on them. Is that something special or does the newest version of purescript not require explicit forall?
Right, this was my point though. I'm probably missing something still. The type of `toTransform` is toTransform :: Gotenks a b m r -&gt; Stream a m () -&gt; Stream b m r thus the type of `toTransform (mapG succ)` is toTransform (mapG succ) :: (Enum o, Applicative m) =&gt; Stream o m () -&gt; Stream o m () whereas the type of `mapS succ` is mapS succ :: (Enum o, Functor m) =&gt; Stream o m r -&gt; Stream o m r If I fit two Gotenks together I will annihilate the return value. Here it can be said that of course I can program with `Stream` directly, but this is obvious. All of the types Pipes.Producer a m r Conduit.ConduitM () a m r IOStreams.Generator a r -- (for m = IO) Streaming.Stream (Of a) m r forall x . (r -&gt; m x) -&gt; (a -&gt; m x -&gt; m x) -&gt; m x basically come to the same, and can be associated with the same prelude of functions, namely [this one](http://hackage.haskell.org/package/streaming-0.1.4.0/docs/Streaming-Prelude.html). It was always clear that adding a return value to the Done constructor of `Data.Vector.Fusion.Monadic.Stream` would yield another (practical) equivalent. Even as it stands the `vector` `Stream` type is equivalent to any of the above those with `()` as a return type, or to `IOStreams.InputStream a`, or `ListT m a` and so on. If the point is, right, you should mostly program directly with `Stream`, and only use `Gotenk` for special problems, I agree: I use other concepts equivalent to `Stream` ... but `Conduit` for a certain range of quasi-parsing problems and where it has some binding or the like. The question, whether to use the present `Stream` rather than e.g. `IOStreams.Generator` or a wrapped Fold ( forall x . (r -&gt; m x) -&gt; (a -&gt; m x -&gt; m x) -&gt; m x) or `Pipes.Producer a m r` or whatever you please, can only be decided by questions of optimization, which always turn crucially on the api that is being supported. That the `Vetigo.Stream` implementation is as fast vector for the problem in the benchmarks is not too surprising, since `Data.List` is just as fast as unboxed vector for that particular problem, as you can easily verify. benchmarking sum $ map (+ 1) $ map (* 2) $ enumFromTo 1 9001/vector unboxed time 7.779 μs (7.674 μs .. 7.894 μs) ... benchmarking sum $ map (+ 1) $ map (* 2) $ enumFromTo 1 9001/list time 7.636 μs (7.592 μs .. 7.687 μs) ... Note that the reason for the `Skip` constructor totally turns on the api vector means to support - it makes it convenient to write non-recursive filter and drop and on and on. The `Skip` constructor by itself doesn't make it faster!! And it isn't I think, used in that benchmark. One should notice before proceeding a priori in this matter that there are reasons why the list optimization in [the `stream fusion` package](http://hackage.haskell.org/package/stream-fusion)` wasn't adopted by ghc, which uses a church encoding like the one I wrapped above instead. 
The IDE situation has improved so much in the last year since that post and I see the momentum continuing. The Atom integration is really maturing rapidly. With stack support and ghc-mod fixed, I finally have a stable environment. Things like IHaskell are amazing as well. * It would be great to have interactivity like in [IDRIS pretty printer](https://www.youtube.com/watch?v=m7BBCcIDXSg). * Integration of the refactoring tool that could automatically apply hlint suggestions. The tool already exists (sorry, forgot the name, it was a GSoC project), just not integrated. * `organize imports` feature mentioned by /u/mightybyte. Very useful indeed. * access to haddocks directly from IDE Most important of all - GHC speed. The consensus is that for the Haskell 98, the speed remained stable; however, it tanked for some of the new language features, which are now seeing pervasive use. And the slowest of all is Template Haskell.... On the GHC side, `-fdefer-type-errors` is great but I'd like to be able to defer other errors, e.g., name resolution errors. This would allow for rapid prototyping like in a dynamically typed language.
You might be interested in *Extensible Records with Scoped Labels* by Daan Leijen. I believe PureScript’s records are based on this, and I used it as the basis of the “permission” (coeffect) system in my pet programming language.
That's an excellent summary of the options, in exactly the right order. We should have this answer somewhere canonical.
This is a perennial question on this subreddit. I'd urge you to outline your problem in greater detail. Your design space does not likely require heterogenous lists and your future self will thank you if you choose not to endure their complexity.
I have not tried Leksah because I'm a longtime vim user and recently switched to spacemacs. I also strongly prefer things that work in console mode so I can work on remote computers via SSH. The Ctrl+R feature you describe sounds nice, although I would like to have more control over the output. I especially like separating imports that come from the current project from external imports like I do [here](https://github.com/mightybyte/hsnippet/blob/master/backend/src/HSnippet/Site.hs#L11). I also like to have them alphabetized. I suppose the productivity increase would be big enough that I could do without these more subjective features, but it seems like they're a relatively minor thing compared to the rest. Maybe I should try Leksah, but trying to get people to switch IDEs completely is a lot to ask and usually involves them sacrificing something. This is why I think these problems should be approached in a generic way so that all IDEs can benefit.
Thanks, and yeah. We actually had considered applying to be one of the Haskell projects ourselves but decided we didn't want to focus on GSoC at this time. For reference, we reviewed *all* the platforms and options at all related to FLO funding, so here's the *entire* summary for anyone interested, including summaries of the issues with Patreon and Gratipay and any others: https://snowdrift.coop/p/snowdrift/w/en/othercrowdfunding
The existential solution doesn't need a class. useFluent1 :: Fluent t -&gt; Int useFluent2 :: Fluent t -&gt; (Int, Int) data EFluent = forall t. EF (Fluent t) type Obj = [EFluent] objs = [EF $ Fluent 5, EF $ Fluent True, EF $ Fluent [1,2,3]] applyMethod1 = sum $ map (\(EF ft) -&gt; useFluent1 ft) obj applyMethod2 = map (uncurry (*) . (\(EF ft) -&gt; useFluent2 ft)) objs
Agree with this completely. It's worthwhile to know about potential solutions with tuples, HList, and existentials. But 9 times out of 10, they're completely unnecessary. OP's problem is likely a matter of design decisions.
&gt; I also strongly prefer things that work in console mode so I can work on remote computers via SSH. Leksah can work over ssh + sshfs using [vado](https://github.com/hamishmack/vado) (performance might suck a bit if your network latency is high). Once you have vado setup there is a checkbox in the Leksah preferences dialog to enable vado support.
I believe it's the latter.
Nope, it's still required, although you can use a unicode forall symbol as of the latest release.
Ok turns out it's reasonably well documented (about python)... here: sounds fine. Thanks for your response! https://www.reddit.com/r/learnprogramming/comments/2mpuvw/learning_programming_while_blind/
Reliable refactoring capabilities, import optimization, lint, a "find usages" feature, stack support, extensibility, a debugger.
An IDE needs to be a **platform** for coding. It should be **extensible;** it should allow users the freedom to program it for whatever purpose they need, even playing games if they feel compelled to do so. But of course it needs to have, installed by default, all the features that people expect of IDEs: package management, revision control, refactoring, debugging, and of course code editing. I would prefer something that has a good set of default features, but is otherwise fully customizable, right down to every last detail -- like Emacs and Vim. Yi and Leksa are both good starts, but I have not found them to be too extensible without actually modifying the code for the IDE itself. They need to be cleaned up and re-purposed as a Haskell IDE. They can take advantage of GHCI to load widgets at runtime, but need to make this as easy as searching a "widget store" and pressing the "install" button. I envision a Haskell IDE that functions much like a web browser except it executes Haskell instead of JavaScript. In fact, modern web browsers like Chrome and Firefox both have extensive JavaScript development tools built-in, nowadays, and the Atom IDE is built on JavaScript, NodeJS and uses the web browser as the GUI. A Haskell IDE should be similar, but of course should focus more on rendering and navigating Haskell code, rather than HTML documents. A web browser lets you write a JavaScript program and run it immediately in the browser, a Haskell IDE should provide a library which you can import, like `HaskellIDE.Prelude`, then you can write a widget in a few lines of code and immediately see it run in the IDE. Users should be able to easily write new syntax highlighting engines with `attoparsec`. Users should be able to write their own Vim clone, with all the modal keybindings, or to import a Vim clone build on the Haskell IDE platform from GitHub or BitBucket or Stackage. Users should be able to clone from GitHub/BitBucket/Stackage a HaskellIDE GUI editor extension for GTK+ Glade, draw a new push button, write the Haskell code snippet to be executed when the push button is pressed, build, run, and see the GUI window with the button popup and run their code snippet when the button is pressed. Users should be able to clone from GitHub/BitBucket/Stackage a WebKit bindings extension, use this extension to write a Haskell web program, compile to HTML5+JavaScript using GHCJS. then launch it in a split-screen WebKit pane side-by-side with their code, all from within the IDE, because the IDE is a platform that allows web designers to program their own web-design tools in Haskell for the Haskell IDE. Anyway, that is what I would like to see.
I'm not getting it. I get what you mean by "bigger" now, but I don't see how that leads to the need to compose `Gotenks`, at least not from your comments here. I can certainly picture times you'd want to do that, but simply because it's "bigger" doesn't tell me that. I'm not even sure what you're trying to prove about `IO a -&gt; IO b`. After all: foo :: (IO a -&gt; IO b) -&gt; (a -&gt; IO b) foo f a = f (return a) bar :: (a -&gt; IO b) -&gt; (IO a -&gt; IO b) bar f ma = ma &gt;&gt;= f
Thanks. Chris Allen told me the same thing on the Yesod mailing list, and I'm inclined to agree
&gt; A Haskell IDE should be a stand-alone program of its own, not built upon Vim or Emacs or Atom or other fancy editor. I'm leaning more towards the `haskell-ide-engine` approach. Build a tool that provides a high-level backend that any editor can hook into. Mainly because getting everyone on a new editor is not going to happen. That said these editors that hook into this backend could be written in Haskell. They could also be named Leksah and Yi. :) 
Might also be useful to eliminate space leaks? Perhaps run the pure function in the background with large inputs (we can use QuickCheck's classes to get coverage for a lot of mockable data structures) and see how the memory use is?
Actually, there is an IDEA plugin for Haskell.
I can't believe I've been wasting all that extra energy with the damn caps key!
&gt; Most of the success stories in that area (e.g. diagrams, pandoc, etc.) are labors of love. That is a big part, yes. But many greenfield projects also contain a large amount of "tedious labor", in particular when they require significant polish. In some domains, this polishing is the most important part of the work, for instance in GUI libraries, IDEs, plotting, numerics. It's true that money doesn't buy love, but money can buy polish, and unfortunately, love doesn't buy polish. I think that's the dilemma we are facing in these domains -- the amount of love needed is also disproportionate. The traditional open source solution is that a large community also buys polish, but that solution is not large enough for us, yet. Ah, and something I also wanted to mention is that some greenfield projects also require a very specialized skills. For instance, in the GUI domain, using an external C++ library will invariably lead to linker problems at some point. Being able to make this play nicely with, say, GHCi, requires a special skill set that few people have or wish to acquire.
It isn't new, and is widely known (and great!). I am not sure this should be here.
for me, the most important feature bar none is an interactive debugger. When your programs go beyond a certain size, the ability to step through instructions one-by-one really becomes a boon. True: GHCi has a debugger built in, but its console-based nature imposes a significant mental strain on the user. Instead of breakpoints being visible in the code and current expressions being highlighted, one has to manually match up debugging information and code. I have found that, after a while, this becomes quite bothersome, and, in fact, bothersome to the point that one just switches to printf-style debugging. EclipseFP, being built on RCP, had this feature, but the whole program was prohibitively slow ("one keystroke taking three seconds to register"-slow).
Call me crazy but I don't want an IDE - what I want is to improve on the emacs integration - for example: - smarter completion (based on the context) - improved goto-decleration and or show info - some kind of visual debugger support (for those cases that never happens in pure FP ^^) - maybe some nice things like (name) refactor, move to module, etc. but I would prefer a faster GHC here (I am fine with doing it manually but the long compile cycles can be tedious) That's basically it - most of these things where or are there with GHC-mod (which I basically dropped for now as it misbehaved more often than it helped me) so maybe the first step should be to stabilize what we have?
This seems like an [XY-problem](http://xyproblem.info/) - that kind of heterogeneous lists is usually not a good idea in Haskell. What do you want to do with it?
Hakyll uses [pandoc](https://hackage.haskell.org/package/pandoc) to generate HTML from Markdown. Pandoc also has LaTeX readers. A quick Google gave me this: http://travis.athougies.net/posts/2013-08-13-using-math-on-your-hakyll-blog.html So it certainly seems possible :)
For CSS, there is a [Hakyll CSS Garden](http://katychuang.com/hakyll-cssgarden/gallery/) available.
Is it on Hackage? EDIT: it is http://hackage.haskell.org/package/tls
I don't know how much TeX is understood by MathJax, but I'm pretty sure Tikz requires importing additional LaTeX packages, on top of having its own syntax. Luckily, some generous folks have thought about all this already: https://hackage.haskell.org/packages/search?terms=tikz
&gt; A Haskell IDE should be a stand-alone program of its own, not built upon Vim or Emacs or Atom or other fancy editor. Can you elaborate why it can't be done in an editor agnostic way? We all have our favorite editors for which we want IDE-like features. Why can't we have Haskell IDE features for Atom, IntelliJ, Emacs, Vim, etc? 
To be fair, your username is /u/_Vitriol, so you're sort of asking for it ;)
Definitely goes towards making Haskell look even friendlier, too! ;)
&gt; technically safe I'm pretty sure this is a trigger phrase for engineers
Has this always been available or is this a new feature?
Right, I think I can state this better in terms of `Vegito.Stream`. Suppose I want to fold over a segmented stream, as in the maximally simple import qualified Data.ByteString.Lazy.Char8 as B main = do str &lt;- B.readFile "/usr/share/dict/words" print $ sum $ filter (/= 0) $ map (B.count 'X') (B.lines str) which tells me how many lines have more than one 'X' on them. This streams fine as written above, but uses lazy io, (The segments resulting from `lines` are themselves unaccumulated streams, though in this case there is no point in that). If I am to use a Gotenks to express `count 'X'`, I would need a function that preserves the return value of the streams it applies to, since in a segmented stream, represented as, say FreeT (Vegito.Stream a m) m r FreeT Identity (Vegito.Stream a m) r CoroutineT (Vegito.Stream a m) m r or indefinitely many other ways, I need to map over the segments, or fold separately over the segments. But I won't be able to use a Gotenks to define the map or fold since toSink :: Monad m =&gt; Gotenks i Void m r -&gt; Stream i m () -&gt; m r obliterates the return value of the stream. Where there is a stream of streams, the return value is the rest of the stream of streams. If the system is to compose and reject accumulation everywhere, I would need something like toSink' :: Monad m =&gt; Gotenks i Void m r -&gt; Stream i m x -&gt; m (r,x) (There are a million ways of going about this.) Then I can recover `r` - the fold result - and turn the succession of streams into a stream of `r`s. Of course this is not a practical problem with folds over streams, I can easily write an equivalent of pipes `folds` for `Vegito.Stream`, or indeed `ConduitM () a` and then I can e.g. apply pre-cut folds from any "beautiful folding" library. `Gotenks`s were never going to allow the forms of composition they allow anyway. But there are many contexts where a `Gotenks` seems to be exactly the ticket, but if they are to be usable for mapping over some kind of stream of streams, then I would need maybe something like toTransform' :: Applicative m =&gt; Gotenks i o m () -&gt; Stream i m r -&gt; Stream o m r Again, in the wider streaming case `r` is _the rest of the succession_ and can't be touched. But judging from the way it is defined, I really am stuck with: toTransform :: Applicative m =&gt; Gotenks i o m r -&gt; Stream i m () -&gt; Stream o m r which can't be mapped over a succession of streams. But maybe I'm missing something. 
Yes, I was trying to write `toTransform'` starting from `toTransform` but was defeated. I will try again. I think `map (count c) . lines` and `let (xs,ys) = splitAt 3 zs, ... ` and so on are only "uncommon use cases" with conduit because they are not provided for. On github there are 5,568 uses of Haskell `groupBy`s, 14,671 uses of Haskell `splitAt`s and on and on; it is very strange to pretend they are esoteric. The application of `map f`, where `f` is a list function just isn't an uncommon use case, except in frameworks that don't support it.
Used by warp-tls and http-conduit among other things. http://packdeps.haskellers.com/reverse/tls Hmm, it defaults to allowing sslv2, that seems an increasingly bad idea.
yes, it is provided by many editors in combination with `hasktags` or `codex`. But ideally, should be provided by `haskell-ide-engine` as a standard function.
it came 2 days ago as a personal question by email, so it's a good idea to clarify it here: tls doesn't support the SSLv2 protocol at all (the SSLv2 code is just not there, so you can't enable it by mistake ..). The flag is somewhat misnamed, as it *only* allow SSLv23 Client Hello message (the first message of the handshake), which is/was commonly sent by openssl for maximal compatibility at some point. It is expected that the client will negotiate a higher version of the protocol, otherwise tls will return an error anyway.
Putting on my "I'm being opinionated" hat, and echoing /u/tailcalled's and /u/dukerutledge's responses: &gt; What's the most elegant way to fix this code? Throw it out. You are asking the wrong question, and you should change your approach. It's like asking "How do I manually allocate and deallocate system memory in Java?" It's doable; there may even, on occasion, be good reasons for it; but if you're asking the question, your first, second, and third instincts should be "Don't!"
Wasn't sure how to apply `unsafeIOtoST` to this, but I've added case with `unsafeSTToIO` and updated the post. Interesting to see now that `runSTthrowIO` can still outperform but only in the `catch`-less case.
I was basically suggesting you could write an evil catch that works directly in `ST s`. Doing so requires turning the arguments from `ST s` to IO, but also the final result from IO back to `ST s`
If you don't want to use ghcjs, you might consider PureScript.
Was there a website?
I did the `toSink'` which was a picnic http://lpaste.net/153881 So using a goofy `($$$)` and `mapsG` to apply a Gotenk I can do, say &gt;&gt;&gt; (n S.:&gt; rest) &lt;- productG $$$ S.splitAt 2 $ S.read $ S.takeWhile (/= "q") S.stdinLn 33 33 &gt;&gt;&gt; print n 1089 &gt;&gt;&gt; productG $$$ rest 33 33 q 1089 :&gt; () &gt;&gt;&gt; S.print $ mapsG productG $ S.chunksOf 2 $ S.read $ S.takeWhile (/= "q") S.stdinLn 33 33 1089 33 33 1089 q I think it is only because of the way this developed historically that `groupsBy` and company are viewed as esoteric. They fit with a streaming library exactly as simply as they fit with Data.List. There is really no difference at all, unless you are starting from Iteratees and the mass of things Oleg was fretting about all at once. 
Your github link isn't working for me. As for `cconsume`, I have something similar in my old unfinished library [Polymorph](http://hub.darcs.net/mjm/polymorph/browse/src/Data/Polymorph.hs). I've modeled the `handles`/`keeps` approach on [mvc](https://hackage.haskell.org/package/mvc). Basically, you can always "widen" such a type, and sometimes also contract it. Thus, monadic actions for monads in `MonadPlus` (`Alternative` would be okay too, I think. But then `a -&gt; m b` functions are rarer.) can be combined "laterally". The package is rather stale, and it is compatible with the ancient version of `vinyl` that had `Rec`s parametrized with type families. I thought I'd need it for messages in a GUI event loop, but I couldn't bring myself to finish it, it's just too abstruse. EDIT: I think if you represented `CoRec` internally as a `Dynamic`, then `cconsume` would be trivial; just linear search at the type level.
My experience is that haste (and by extension fay) was too far from Haskell to get good code reuse. When rewriting a several thousand line haste project in ghcjs we achieved a 40% code reduction. Some of that was due to getting a proper FRP system (reflex), but some of it was definitely due to improved code sharing. Fay is even further from Haskell than haste is, so while I haven't actually used it I'm quite sure it would be worse.
Ryan, I meant to talk to you more about this in NYC, but can we get a Stripe account going that I could get access to? We could have a donations page directly on Haskell.org (with ~20 second donations) within a week if so. There is a (functional, demo) implementation of the donation page [here](https://haskell.org/donate) that uses my personal test keys, but I never deployed it really, and the backend needs to be reworked a bit (it re-uses an external Ruby implementation I wrote many moons ago and deployed on Heroku, but should be integrated into the Haskell.org server code itself). It can actually do charges, though.
Can't you distinguish "has {| x :: Double, y :: Double |}" from "is {| x :: Double, y :: Double |}"?
The red cross donation (via bitpay) looks like this: https://bitpay.com/520663/donate
As /u/paf31 mentioned, the explicit `forall` is still required. Which examples are you referring to, though? 
AFAIK, Haste has everything that GHC supports too, but generates much less Javascript code than ghcjs. And it's often faster too! 
Did leksah + yi ever materialize? That was my first complaint with leksah - how weak the text editing is.
I'd be really interested in seeing a conduit-groups, at the very least to help me understand this better and hopefully get over any preconceived notions I have about its complexity. I'd be happy to code review the leftovers bits.
I know people who reuse their Purescript types in Haskell, because the syntax is identical. Purescript is great and i would say the most mature alternative for production
Slight correction: Haste does not currently support Template Haskell - support is planned for the upcoming 0.6 release - and will not support weak references until it can be supported efficiently. Support for packages with the "custom" build type is also currently a bit lacking. Other than that, your claim is correct.
I would be very interested in evidence backing up your claim that Haste is not Haskell. By which definition of Haskell? I would, however, be even more interested in knowing what concrete problems you encountered when using Haste (aside from not having access to reflex) that made code sharing difficult, since that is something I can actually do something about.
Your last post mentions an epilogue; that never got written, did it?
Have you looked at elm? I have just done toy things but it seems cool. http://elm-lang.org/ EDIT: /r/elm
Speaking of issues with Haste, I'm happy to report that most of the problems I described in my [Ludum Dare 34 post](https://www.reddit.com/r/haskellgamedev/comments/3x1jqr/growing_up_a_short_browser_game_written_in/) turned out to have an easy solution. The libraries I couldn't compile all turned out to have a common dependency, hashable, and once I followed the [workaround](https://github.com/valderman/haste-compiler/issues/51), there were no other issues compiling the other libraries. The one remaining issue is that even though the documentation [recommends using the random package](https://hackage.haskell.org/package/haste-compiler-0.5.2/docs/Haste.html#g:3), doing so still gives me the error "ghczuwrapperZC0ZCbaseZCSystemziCPUTimeZCgetrusage is not defined" at runtime.
My understanding in the past had been that the concurrency model differed from GHC more than that of GHCjs as well?
Could you file a bug report for that? It shouldn't happen as of 0.5.4, so it would be great if you could also include the version, platform and OS you're using.
Right, that's true: lifting concurrent GHC code right into Haste won't work, but lifting concurrent Haste code into GHC does. It's a good point however, thanks for pointing it out!
Are these useful for sharing code between Haskell and the browser?
Is there a good reason to support SSLv3?
Yeah, you should use Elm.
I've been using Docker since 2013 and it has never _not_ made me incomprehensibly angry. I've liked Ansible + LXC (Vagrant in dev) better.
Ah! I was using 0.5.3, and the issue with Random indeed disappears when I switch to 0.5.4. Thanks!
GHCjs has filled in lots more of the missing bits in terms of standard libraries that use cbits, such as Text and ByteString iirc.
I've been using Fay for some internal stuff, and in the last 1-2 years it didn't break anything at least :) But for a new project I would say try purescript instead, looks much much better. My only issue is their fetish with the javascript ecosystem, which makes it hard to start, but the language itself looks great. Fay isn't really Haskell anyway at the level of actually sharing code, there are no type classes for example.
This is not quite true. PureScript used to have the `[a]` syntax for array types, but it does not any more; it's now `Array a`. `[a]` in a type is now a syntax error. There are other solutions, to this, though. For example: https://github.com/typed-wire/typed-wire
Personally, I think only TLS 1.2 should be supported in 2016, but sadly many many things do not upgrade their libraries or program, and you still have lots of old implementation in the wild. Probably a bit less in the last 2 years, since they were so many important TLS issue and people *had* to upgrade. I think the TLS protocol has passed its expiration date too, it's time to start again from scratch: leaner (no asn1,x509, mandatory-for-security *extensions*) and with only modern crypto supported by default (ed25519, ...).
 flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c ($) :: (a -&gt; b) -&gt; a -&gt; b So what you want is flip ($) :: a -&gt; (a -&gt; b) -&gt; b though for your example all you need is map ($ 3) [(* 2), (+ 5), (+ 10)]
You can use ($) as a section. map ($ 3) [(* 2), (+ 5), (+ 10)] 
I don't know all that much. You might find these guidelines useful: https://www.w3.org/TR/WCAG20/ I don't use any assistive tech, and it's an area I'm still learning about so I can't give you much more information than that.
Thanks!
`flip id` also works! Try it.
Oh, I can't believe I've been wasting all that extra energy reading that ugly uppercase pragmas!
This is absolutely true. In fact, the main goal for the upcoming 0.6 release is to compatibility-wise get up to par with GHCJS for such code. A lot of the work on Haste is going into more research oriented activities (that's what I'm getting paid for, after all), so there is some polish that's still left to apply.
Is that because the only information you can get out of a heterogeneous list (excluding discrete length lists with specific types for each element) is the information they all share. At which point you may as well just convert the list to only the information that they share.
Well I meant the examples right on the page linked to. It was my mistake in reading the type signature for main though. I thought that it was extensible without a forall, but I just read it wrong.
IIRC Stack just recently improved support for GHCJS. http://docs.haskellstack.org/en/stable/ghcjs/
Does this mean that things like the lens package (and template haskell which it depends on I think) works with haste now? I gave haste a quite serious try about a year ago. I really liked the idea and I felt that it almost hit the target but in the end I gave up because of dependency compilation and ghc version issues. Edit: ah, github page says "except Template Haskell"
Oh Ok. So, is it reasonable to to say Arrow is the "superclass" of Monad but "subclass" of Applicative? 
Sure, and that sounds like it'd get close enough, but when asked for a *wishlist* you gotta go big or go home.
Because I view the "Integrated" in "Integrated Development Environment" as being the biggest factor. I mean, I guess the haskell-ide-engine approach will probably get close enough, but when asked for a wishlist I'm going to go all the way. I use vim and emacs, and I'm never actually happy with either. I want a program that's not designed with the limitations of a terminal in mind. I want a program with a GUI and panes that I can move around and resize. I've attempted to use Atom twice and both times I was deeply unhappy about the inability to configure something or other that I wanted to reconfigure and so I just uninstalled the whole thing and went back to emacs. IntelliJ is pretty solid from what I've seen of it. I usually use Eclipse for Java, so I don't have much IntelliJ experience, and when I did use it I was using a "secondary" mode designed for erlang. Still, things worked roughly how I expected, and I could probably get used to the differences. If IntelliJ got an aces level module based on haskell-ide-engine that allowed remote file editing (and I'm sure IntelliJ can do remote file editing somehow) then I'd probably use that.
[Data.Function](http://haddock.stackage.org/lts-5.5/base-4.8.2.0/Data-Function.html#v:-38-) includes this: (&amp;) :: a -&gt; (a -&gt; b) -&gt; b You might recognize `&amp;` if you've ever used the lens package. For example, from the [turtle example](https://github.com/ekmett/lens/blob/master/examples%2FTurtle.hs): forward :: Double -&gt; Turtle -&gt; Turtle forward d t = t &amp; _y +~ d * cos (t^.heading) &amp; _x +~ d * sin (t^.heading) 
Oh, and you can always hoogle such things: https://www.stackage.org/lts-5.5/hoogle?q=a+-%3E+%28a+-%3E+b%29+-%3E+b
Insertion of inferred type annotations (C-u C-c C-t in haskell-mode) is a killer feature.
Well it was a hack from the start, and it still is. (To add to your story, string serialization didn't actually work when I first tried to use Fay back in 2013...) It was a useful hack at that time, now we have better alternatives.
Not yet, unfortunately. Template Haskell is currently in the works, but will most likely not land before the ICFP and Haskell Symposium paper deadlines are out of the way.
Many languages, such as F# and Elm, have an operator `|&gt;`, where `x |&gt; f = f x`. In Haskell, you could define this by `(|&gt;) = flip ($)`.
Is 14 pages really **a** cheat sheet?
For many users of such a `Data.Map.Strict.Really` data type, having instances that just violated the laws would be the lesser of evils because they aren't really thinking in terms of the `Functor` abstraction, and refactoring using the laws, but rather `fmap` is just the name of a thing you do. I do think if you are going to fiddle with multiple map types, then it'd be handy to have access to internals and zero-cost conversions between these things, and the current `containers` API is not well kitted out for that sort of thing.
No. We need someone with the will to make it happen. The code we have is a good proof of concept, but we need someone with the will and the time to make it usable.
 &gt; Is the library intended to be used entirely within a `Fresh`M monad? Or are their ways to do things like lambda application without being in `Fresh`? In most situations you will want to operate within a `Fresh` or an `LFresh` monad. &gt; What kinds of values can I give to `freshen`, in order to avoid the errors they list? So I think the reason you're getting the error is because you're passing a *term* to `freshen` rather than a *pattern*. In Unbound, patterns are like a generalization of names: a single `Name E` is a pattern consisting of a single variable which stands for `E`s, but also `(p1, p2)` or `[p]` are patterns comprised of a pair of patterns `p1` and `p2` or a list of patterns `p`, respectively. This lets you define terms that bind two variables at the same time, for example. Other more exotic type constructors include `Embed t` and `Rebind p1 p2` former makes a pattern that embeds a term inside of a pattern, while the latter is similar to `(p1,p2)` except that the names within `p1` scope over `p2` (for example if `p2` has `Embed`ed terms in it, `p1` will be scope over those terms). This is really powerful because it lets you define things like Scheme's `let*` form, or *telescopes* like in dependently typed languages. (See the paper for details). Now finally the type constructor`Bind p t` is what brings a term and a type together: A term `Bind p t` means that the names in `p` are bound in `Bind p t` and scope over `t`. So an (untyped) lambda term might be constructed with `data Expr = Lam (Bind Var Expr) | App Expr Expr | V Var` where `type Var = Name Expr`. So back to `freshen`. You should only call `freshen` on *patterns* so calling it on something of type `Bind p t` is incorrect (and I suspect the source of the error message you're seeing) - you should call it on just the `p` and then apply the resulting permutation to the term `t` to apply the renaming that `freshen` constructs. &gt; If I end up using `unsafeUnbind, under what conditions is it safe to use? The place where I've used it is if I need to temporarily sneak under a binder and do some operation that I know for sure does not do anything to the names. An example might be collecting some source position annotations from a term, or replacing some global constant by a closed term. Also if you can guarantee that the term you're working with already has been renamed so any names that you `unsafeUnbind` are going to be unique already. Hope this helps. PS: I maintain [unbound-generics](https://hackage.haskell.org/package/unbound-generics) which is a clone of Unbound, but using GHC.Generics instead of RepLib.
Ahh thank you this will be a big help.
&gt; A lot of the work on Haste is going into more research oriented activities May I ask what kind of things are being researched? Just curious.
&gt; I read on Typeclassopedia that Arrow is a more general form of Monad. Where? I was wondering in which sense, but I couldn't find such a statement in the page you linked. What I did find was: A [diagram](https://wiki.haskell.org/File:Typeclassopedia-diagram.png) showing the relationship between many of the type classes, in which we see no arrow between Monad and Arrow. That diagram also includes Functor, Apply, Semigroup and Category, so that should answer the second part of your question. A [passage](https://wiki.haskell.org/Typeclassopedia#Arrow) saying that "Arrows generalize functions". The closest I could find was [a reference to a paper](https://wiki.haskell.org/Typeclassopedia#Further_reading_10) named "Generalising monads to arrows", immediately followed by an explanation clarifying that arrows do not actually generalize monads: &gt; Although Hughes’ goal in defining the Arrow class was to generalize Monads, and it has been said that Arrow lies “between Applicative and Monad” in power, they are not directly comparable.