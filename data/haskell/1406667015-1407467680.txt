I'm actually trying to figure this out presently. Last night I went on the snap IRC channel to get some guidance, but there was no conversation activity in the hour or so after I asked my question, so I left. I'll need to try again. There's also a similar question on the Silk engineering mailing list, and last night I also posted a follow up asking if there had been any headway, just like you have. So far no responses. I'm pretty new to Snap and rest, so I'm still figuring things out. What I want to do is send a Snaplet down the pipe in the apiToHandler' call like is done with the BlogApi in the rest example application. The problem is that I don't know how to bind a plain snaplet to a variable. It looks like the snaplet types are set up so that once you nest your snaplets within the app state, you end up with a SnapInit for the whole program state. But what can you do with this? serveSnaplet and runSnaplet are the only things that consume SnapInit, but neither of those helps me since I need to run apiToHandler'. So in short, sorry, no conclusions yet :(
Why can't we bake the very few pragmas used by core libs into a `Haskell2014`?
pragmas are not a language feature, they are a compiler feature this isn't just a pointlessly pedantic distinction...it makes learning haskell difficult, and basically makes it impossible for anything but ghc to exist
You might want to consider https://www.youtube.com/watch?v=MoxqBGaTmWQ
There are some of each. DataKinds is a language feature, as are OverlappingInstances and such. I suppose it could be argued that something like the proposed strictness annotations would be compiler hints, but many are definitely language extensions.
oh come on, once a pragma is out in the wild and people are using it in libraries, it most certainly is "baked in". try removing overloadedStrings support from ghc and see how many of your codebases compile. the degree to which people will apologize for haskell's warts is mind-numbing. 
Why would you remove that particular one? It's essentially some opt-in syntax sugar. Cherry picking examples to suit your argument doesn't mean it reflects the system as a whole. And haskell has plenty of actual warts, the naming of fmap, applicative not being a superclass of monad(getting fixed), return in Monad etc. You've just picked something that isn't really a wart to moan about.
However, the way the package description is written, people with GHC 7.0 will probably run into a compile error rather than a cabal rejection. Why don't you simply add a general `ghc&gt;=7.4` lower bound (and also retroactively to the already uploaded packages) so Cabal fails early for older GHCs?
But that's exactly what deprecation warnings are for. If we know right now that in *n* releases the extension will be removed, why not tell (warn) about it? Just because people don't like warnings?
You might want to re-check it for `StateT` (well, unless you count it even though the map isn't a monad morphism). Edit: also, it's not the case for `WriterT` either, unless the monoid is commutative, or, again, unless you count it even though the map isn't a monad morphism.
These warnings are too noisy. It would be fine, perhaps, if you could have a pragma to disable deprecation warnings on a case-by-case basis.
These pragmas are, in a sense, overlapping beta periods. It takes longer to suss out the value of a change/extension than a single compiler release cycle, so we want a mechanism to make that uncertainty explicit. The current method lets certain pragmas fall *out* of fashion after the shine has worn off, which provides for a much nicer evolution than arguing hypotheticals on mailing lists. I do wish that there was a bit more eagerness to bake some of the common extensions into the language, but I really don't know the details of why no committee has chosen to do so.
&gt; I do wish that there was a bit more eagerness to bake some of the common extensions into the language, but I really don't know the details of why no committee has chosen to do so. As far as I can tell the reasons are mostly practical. In short, there isn't a group of people who have enough spare cycles divert to the language specification process. After all, language specification is [hard](http://www.reddit.com/r/haskell/comments/1dhbjr/haskell_2014_committee_has_now_been_formed/c9qnibu). If there were more people who volunteered to serve on the Haskell' committee the process would be more likely to move along (although you'll have to wait until the next iteration; the [Haskell 2014](https://ghc.haskell.org/trac/haskell-prime/wiki/Committee) committee has already been formed). Frankly, I don't find the slow pace of the Haskell' process to be problematic and weakly prefer that resources be put into exploring new language features, especially given that the system we have for using existing features (e.g. pragmas) works fairly well.
Can't say it's not good to have scruples.
The equivalence between classic arrows and the arrow calculus is one of the reasons I'm surprised there doesn't yet seem to be an implementation because it demonstrates that the classic arrow implementation is unnecessarily convoluted. The clarity and expressiveness of the arrow calculus notation and the reformulation of the nine obtuse classic arrow laws down to five more intuitive laws greatly enhances understanding and, I presume, the practical usefulness of arrows not merely to the arrow library implementers but to end users as well. 
&gt; Cherry picking examples to suit your argument doesn't mean it reflects the system as a whole. why are you moving the goalposts? i provided you with an obvious example of a pragma that is indeed "baked in" and impossible to remove. and now you're assailing me for picking a too-good example? 
You know `Vector (Vector a)` is a type and that it's pretty easy to write the pair-based indexing function if that's what you want: !@ :: Vector (Vector a) -&gt; (Int, Int) -&gt; a v !@ (x, y) = (v ! x) ! y You might even be able to unbox the inner-most layer, depending on what `a` is. I could be reading things wrong (empty type classes, and the overlap between type names and type class names makes vector a little hard to read from just the source and the haddocks), but I'm pretty sure nested vectors are allowed.
I agree with everything you said, except that I think you're giving giving the Haskell' process a free pass. We have https://ghc.haskell.org/trac/haskell-prime/ to indicate progress. Good news: the 2011 committee was appointed! Way back when, I had hoped that Haskell2010 would go a bit further than it ended up doing, but, at the time, there was an expectation of a Haskell2012, say, so nobody was really bothered. This isn't a whinge, but rather just to say that I think there would be some benefit to continued, slow adoption of non-controversial extensions into language standards.
I guess I could do that, but I don't understand why I would want to / how that relates to the loop question? Performance is important, so it really should be a plain chunk of memory. Turning this into basically an array of pointers to arrays can't possibly help. Also, if you pass a 2D image to OpenGL or any API like that, it wants a single chunk of memory. Also, how does this change anything about the loop itself? I mean I would still have to write it exactly as before? I guess I could use some kind of nested mapWithIndex function or so now?
As I said, Overloaded Strings is essentially just syntax sugar, aiui having a program transform code using overloaded strings to explicitly using IsString instead would be simple. I disregarded it because its an utterly useless example to use if you're trying to show the ecosystem is dependant on things provided by pragmas and would somehow break if they were removed. there are much better examples if you wanted to show extensions whose removal would break the semantics of packages.
I guess it just hasn't been something I have encountered that much which is why I was asking for some examples of libraries. I mean I have used quite a few libraries and the only library which required pragmas beyond overloadedstrings (which even that just saves some typing, it's not critical) was probably repa. Which makes sense when you see what repa is trying to accomplish.
Nor is it a monad morphism for `Either`. Right (Left 1) &gt;&gt; Left 2 = Left 2 but f (Right (Left 1)) &gt;&gt; f (Left 2) = Left 1 &gt;&gt; Left 2 = Left 1 I don't think that matters though. The laws given by /u/Tekmo are stated here https://hackage.haskell.org/package/mmorph-1.0.0/docs/Control-Monad-Morph.html#t:MMonad and I don't think they require the result to be a monad morphism.
Wouldn't these be the first semantics-changing pragmas? This seems like a pretty big deal in terms of precedent if so. It means I can't rely on semantics being defined by declared LANGUAGE extensions, but also by other pragmas throughout the code.
This is exactly what I do
&gt; basically makes it impossible for anything but ghc to exist I think this is a valid concern. But, again, I don't see an alternative that I'd rather see happen. It's not really the pragmas that make alternative to GHC challenging. It's the pace at which the language is changing. And by "the language", I mean Haskell as it is commonly used in practice. Pragmas make that pace of change tolerable, so that the pain is mainly felt by those in niches like this. Without pragmas, but with a similar pace of change, it would be equally difficult to build a usable alternative to GHC; but also much harder to use the language, too! So we can put a stop to the language changing... or, we can accept that GHC is going to keep a monopoly on the Haskell compiler market. Is that such a bad thing? At least it's an open project that's eager to accept contributions from anyone with the inclination to make a change. We're just moving the innovation from competition between compilers to cooperation in the same compiler. This is, in a way, and extension of Haskell's beginnings; it has always stood as a counter-example to warnings about design by committee.
AIUI (disclaimer: this is from reading the paper, not from looking at the source) the Haskell code is only used as a formal specification; it's not actually run. The C code is a hand-translation of the Haskell code (and hence pretty bizarre by C standards), and is what is actually run.
I don't think OverloadedStrings is baked in for all users of Haskell. It's certainly very widely used, and rightly so, among "commercial" or "industrial" Haskell users. It's relatively more rare among educational users. (That said, my main work on Haskell now is on an educational system, and I use OverloadedStrings there; so I think that's changing, and it's likely safe to include that one in the next Haskell Report. Obviously, the situation with OverlappingInstances is somewhat different.)
The killer for an AD system is to be able to combine this stuff with big matrix/tensor libraries (and then execute the code on a GPU), like python's theano - though as far as I know it's nowhere near as flexible as AD. I've heard this talked about before, being able to use AD (Vector Double) as opposed to Vector (AD Double)
If you're doing 2d array stuff and want high performance, then you definitely need to worry about cache performance. [This lwn article](http://lwn.net/Articles/255364/) is a classic about how to tune a c program for performance. Most of the advice can be applied to haskell in a straightforward manner. For example, I've gotten code to run 20x faster in haskell by refactoring 2d loops to be more cache friendly. Eventually I'll write a blog post about this...
True, and there are flags to disable warnings. I think 2 releases before removal would be nice though (so I can avoid ifdefs)
You want something that has fast access (iterating over) and update (writing to), so you want a vector, so use a vector. IIRC, if the function is inlinable V.map generates a pretty tight assembly loop. If you want it as a single chunk, allocate it as a single (unboxed) Vector. You will, of course, have to convert from tuple-based indexing to a single index, but that's a one-line function. Depending on what you are doing a you have want column-major or row-major indexing, and you might want to bypass that type of indexing or at least refine your loop to iterate across the structure in a [Morton Ordering](https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-1) or along a [Hilbert Curve](http://en.wikipedia.org/wiki/Hilbert_curve) for cache locality. For the simple cases you do get back the "simple V.map" that you were apparently "missing" before.
... and not all monad transformers are `MMonad`s (monads over monads). The counterexample is `StateT`.
I'm very excited to see this. I've been contemplating RocksDB at work (because it seems like an ideal data storage solution for the system I'm building), but wasn't thrilled about trying to see how much of the LevelDB package on Hackage I could use to try and make a library. I haven't had to deal with FFI in Haskell yet, and I'm not looking forward to it just yet (I'm still trying to get my sea legs). The source looks very well laid out, although I wish there was actual documentation (or at least Haddock docs for the modules). I think I can make pretty good sense of it from the source though; so thanks for this.
I was really excited seeing this too! Though there really ought to be docs on hackage. There was a recent comparison article that found RocksDB to be better than LevelDB for their use case. (I don't remember much enough to find it though.)
Use the `vector` package, especially unboxed or storable vectors if you can (since they are cache friendly). It supports both mutable and pure vectors, and both are fast.
A hand-translation of Haskell to C seems like a gigantic gap in the whole "end-to-end verified" thing; how is it accounted for?
http://mathb.in/18807
Boxed vectors (the default implementation in Data.Vector) store pointers to the elements. Unboxed and storable vectors store the elements directly and contiguously without any pointers.
&gt; Though there really ought to be docs on hackage. Haskell uses "documentation"? That's new.
Too late, the cat is out of the bag! Are you going to remove "1.0" and "1.1" from Hackage later?
Any way I could help with this migration? Perhaps write scripts to assist with moving data over from trac? 
&gt; Though there really ought to be docs on hackage. I guess the docs just didn't build yet - or the haddock build fails. The code seems to be documented for haddock.
&gt; For example, I've gotten code to run 20x faster in haskell by refactoring 2d loops to be more cache friendly. Eventually I'll write a blog post about this... Please do!
Cool stuff, thanks! I wrote an initial LevelDB binding long ago, but never got around to releasing it. Extremely minor complaint that isn't worthy of much attention: I find the name 'rocksdb-haskell' quite annoying and redundant as a package name. Repository name on GitHub? Sure. But the name 'rocksdb' alone as a package name would be perfectly adequate on Hackage, and would surely show up in Google for example. Perhaps I'm alone in this opinion, however. Anyway, good stuff.
Language pragmas serve as a middle point between your two extremes. They allow the continued change of the language, but at the same time, allow for the community to distinguish the core language from the extensions. I am gathering that you don't see much of it, but there is resistance to much reliance on unnecessary language extensions among much of the Haskell community, and extensions leave the problem space open for new solutions. For example, had Haskell not used language extensions, functional dependencies would have been added to the language a long time ago. After all, functional dependencies were a critical part of "real world" Haskell for a long time, with no alternatives in sight. But once they were added and considered part of the main language, it would have been unthinkable for type families to be proposed, and widely implemented in libraries. Type families now solve many of the same problems in a manner much more consistent with the rest of the language, and it's no longer clear whether one, the other, or both of type families or functional dependencies will be widely used in Haskell in five years time. It would have been a mistake to jump into enshrining functional dependencies into the Haskell Report back in 2007. See the big hoopla about removing something as trivial as n+k patterns, and do you really think fundeps would ever be gone even if the community decided they were a bad idea? I think the comment that described language pragmas as a kind of "extended beta" for new language features got it right. As a beta rather than an alpha, the new features do have non-trivial users, who will be impacted if they suddenly go away. But at the same time, they are kept apart from the core language because we're not sure it's the right thing to do. I don't disagree that with more active management, we could reduce the number of language extensions, by incorporating the obvious ones (like FlexibleContexts, ScopedTypeVariables, BangPatterns, and OverloadedStrings) into the language. But I wouldn't want to do away with the culture of trying things out in non-trivial ways, sometimes over 5 years or so, before making a big commitment.
I'm sorry, I think you just completely misread my question. I have absolutely no problem or concern with allocating an unboxed 1D vector or indexing it with 'x + y * w'. That's all fine, I'm asking how to write the loop to iterate over it. Why would I want to use a space filling curve to index into a row or column ordered 2D array? That would not increase cache locality at all, quite the opposite.
I'm sorry, it seems like I must've expressed myself very poorly but all the answers so far don't even seem to address my question at all. I'm asking about how to write the loop. I know about cache performance and it's completely obvious in what order to iterate over a 2D array for best cache performance (linear in memory). All the loops in my post already do that.
I know, and that's what I'm already using if that wasn't clear. I was asking about how to write the loop, like in the four examples I gave.
The M-word.
Like suggested in [#9037](https://ghc.haskell.org/trac/ghc/ticket/9037) ?
I had issues with the forM_ and mapM_, which worked but allocated many list cells, adding un-needed pressure to the GC. The usual solution is to write a direct recursion using a strict index, the loop package is doing exactly that for [numLoop](https://hackage.haskell.org/package/loop-0.2.0/docs/src/Control-Loop.html#numLoop). The pattern also appear in [Juicy.Pixels](https://github.com/Twinside/Juicy.Pixels/blob/395f9cd18ac936f769fc63781f67c076637cf7aa/src/Codec/Picture/Jpg/Common.hs#L179), to iterate quickly over the various arrays. I would even say that the JP version doesn't use a bang pattern because everything is specialized to Int, and a check is performed at every iteration, making the index strict in practice. As we are working on similar subject, to get performance in JP, I had to: * drop every "important" call to mapM_ and forM_, benchmarking told to avoid them. * Avoid monad transformers, I gained 5 to 10% of performance by ditching StateT (ST s) and carrying my bit reader by hand for the jpeg decoding. * Use unsafeWriteAt (15% just there) 
Seems surprising that there are instances for `WriterT` and `ErrorT` then.
That might be a mistake. Perhaps /u/Tekmo can weigh in?
It seems you have a mistake in your MMonad instances. Look at my response to OP for details.
&gt; However, the way the package description is written, people with GHC &gt; 7.0 will probably run into a compile error rather than a cabal &gt; rejection. As I said: &gt; Although next time I upload to hackage I'll change it to &lt;7.4.
Thanks, I added your rasterMap to the comparison! I'm actually surprised by how well the forM_ performs here, actually beating forLoop for the transformer stack. Seems like the list is fused away here and the bounds check from succ doesn't matter much. I generally avoid doing any real work in a transformer stack, especially after that ~40x slowdown I recently had with ConduitM over IO ;-) For the plain IO case, forLoop seems to win. Without relying on 'loop', two nested forM_ worked best. Did you see how poor numLoop performs with the transformer stack? Just seems really surprising, I think I'll file a bug for the loop package. unsafeWrite etc., for sure.
Perhaps your problem is related to [this GHC issue](https://ghc.haskell.org/trac/ghc/ticket/8763).
Every recent GHC release has been an exciting one, and the pace of new features doesn't seem to be slowing.
Hmm this is interesting, maybe switching back to a forLoop would be beneficial. I wonder if in the monad transformer stack, the forLoop wouldn't be inlined and specialized for Int, which would be a killer for performance, did you look at the generated core?
Honest question: can someone help me understand what is the important point here? from what I see, there are three aspect discussed: - a haskell to js framework, which contains useful combinators for event manipulation. This looks nice, but I don't see the advantage of this over some MVVM js lib such as angular/knockout/react. Here the fullname display is easily done with a databinded computed value - a HTML templating DSL. Why not? There are also tons of alternatives here, such as jade or hamlet if you like typesafety. - a css templating library. Once again, I don't get why this better than what already exist. I can see the benefit of using the same language for everything. That's why Google has GWT and Microsoft has Webforms. Both projects are in maintenance only modes because they are too big abstractions for web applications. I think GHCJS has to demonstrate its usefulness vs using js libraries with typescript for medium/big web client projects, because the abstraction cost must be justified. The simplicity vs terseness speech is interesting, but I don't see how the tools presented are better than other solutions. 
Maybe he just isn't aware you can now edit the cabal meta-info on Hackage post-facto...
sorry, misunderstood that one
Yes, it is! The author of the loop package used in my examples filed this bug, it gave the original motivation for him to write it.
I just had a look at the core of the bad numLoop case, and I have to admit that I can't make much sense of the relevant (and lengthy) bit of core code. I'll have to simplify things first for me to grok it. There sure is a lot of Control.Monad.Trans.* in there, though ;-)
The proofs are written in Isabelle/HOL, see e.g. [this theory file](https://github.com/seL4/l4v/blob/master/proof/crefine/CLevityCatch.thy) for an example.
I have a question that nobody locally here was able to answer, and doesn't seem to be addressed in the original paper by [Klein et al](http://ssrg.nicta.com.au/publications/papers/Klein_EHACDEEKNSTW_09.pdf) or on the project's website: what was the original motivation for verifying seL4? Was it a commercial project by General Dynamics where they needed NICTA's expertise, or was it an academic project by NICTA that needed General Dynamics' funding, neither, both, or something else? If it was a commercial project, does the GPL'd release of the source code imply that the commercial side of the project is now dead, or was this planned all along?
 bracket (forkIO myComputation) killThread $ \_ -&gt; do I made that exact mistake, took me quite a while to figure out why my threads could not be interrupted. The solution was a simple asyncWithUnmask vs async, but yeah, what a beginner's trap! ;-)
Thank you for solving my exact problem (https://github.com/blitzcode/conduit-chunked) ;-) I also figured out that it isn't possible to do this efficiently inside ConduitM, but I was unhappy with having no way to automatically yield the finished chunk. I just gave up at that point and put it off to think about later. It was probably too myopic to insist on having the exact same yield/await semantics. Your design of just queuing up the chunks and offering a forceFlush is probably sufficient. Good stuff!
That's too coarse. Need to say: "I realize this particular use is deprecated, but I have to keep it that way until I no longer support the old compiler" yet still get other deprecation errors that you haven't dealt with.
Yeah, it is problematic, but how does the dominance poset look like?
Why not use `withAsync`?
I do not know of any but if you feel the need (and nobody will give you a better answer) than maybe start something. I would be interested in something like this too and would happily contribute to any open-source project ;)
Besides Clckwrks I know this one: https://github.com/Tehnix/HsCMS &gt; Anyone working on anything? We are planning to; but we need a few months more to have something to show. &gt; It's a shame that specialized server-side web packages aren't getting that much love. What do you mean by that? I think server-side packages that are useful for web development are actually getting a lot of love lately.
Otherwise, you can use some SAAS platform specialised in managing content like prismic.io and use haskell to handle it on your side. Someone already started a haskell kit for that purpose (https://github.com/Herzult/haskell-kit)
If ensuring the cancellation of the thread in case of an exception is all that's required, I agree that withAsync would be the way to go.
Little bit offtopic, but they (being the originally rocksdb authors) claim &gt; that mmap-ing a file into the OS cache introduced performance bottlenecks for reads. Is there any obvious reason why this should be the case? I always thought that mmap-ing is one of the fastest ways to read from disc, as long as the files are sufficiently large.
Why is it "Voldemort's theorem"? Is there Harry Potter reference that I'm missing? 
Isn't this kind of problem the whole reason for the existence of [resourcet](http://hackage.haskell.org/package/resourcet)? I'm actually finding it a bit difficult to find a good, clear overview of `resourcet`. It was clearly split off from `conduit` for a reason. But the package description refers to the [Yesod book](http://www.yesodweb.com/book/conduits), the Yesod book says that the documentation was moved to an [FP Complete page](https://www.fpcomplete.com/user/snoyberg/library-documentation/conduit-overview) which is really about conduits, the FP Complete page has only a small example of `ResourceT` and then says "For more information on ResourceT, please see Control.Monad.Trans.Resource," which sends us back to the Yesod book. Around and around in circles...
I have often felt that haskell's habit of dying when the main thread dies is a poor choice. I have used http://hackage.haskell.org/package/threadmanager in order to deal with this, however it doesn't work for libraries that manage their own threads. 
You're right, I did not read the article as none of this applies to my question and I already know about the uses of space filling curves for improving cache locality in higher dimensional arrays. In the original post I mention: "having the 2D coordinate is a requirement, so no simple V.map etc." I appreciate you trying to help me out, but until you actually read the original question I'm afraid this won't be of much use ;-(
I always use explicit recurssion: let go (-1) ret = ret go i ret = go (i-1) $ dosomething go (length vector-1)
I did show you how you have both have 2D indexing (via a custom !@ operator) AND still have the ability to use the "built in" recursion schemes.
&gt; with Text (or something ByteString), or I think you mean "sometimes" instead of "something", maybe.
That's cool! The API seems so similar to `mtl` that I missed the benefit when defining instances on a cursory glance at the docs. I'm sorry I guessed incorrectly what you were getting at.
It sounds like you want everything in one bag but you don't want the bag to be heavy.
Yeah, maybe I should have named the class something else (i.e. `FMonad`), because those two instances are only monads over functors. I'll probably keep the instances, though, and change the documentation to note that not all instances are monad morphisms, despite the class name.
Java uses the opposite default. Threads have to be explicitly set as daemon threads (using `java.land.Thread.setDaemon`). It is annoying to have a process go "brain-dead" due to an uncaught exception in the main thread and one random should-have-be-daemon thread. But, all solutions have their problems. Of course, calling `java.lang.System.exit` gets rid of all threads. In Java, if a thread does need to be daemonized, that can be a local change. In Haskell, if main needs to wait on some threads, that's a wide-reaching change; a `MVar` or `TMVar` or something similar has to be communicated back up through the call stack to main. IME, yes, the choice of daemonizing every thread by default is poor.
`clckwrks` is still actively developed. In fact, the last upload to take was on June 26, 2014. http://hackage.haskell.org/package/clckwrks Where did you get this February 2013 date? Current work is focused on overhauling the authentication library so that it is (1) better designed (2) fully supports I18N (3) friendly for building SPA (single page applications), using libraries like AngularJS. The next step will be to make similar improvements to the rest of clckwrks. Also, if the related GSoC project for plugins-ng goes well, we will be able to support one-click installs of plugins into running apps. 
`zipWith (\i e -&gt; stuff) [0..]` is a mapWithIndex for lists. Similarly `\v -&gt; zipWithM (\i e -&gt; doStuff) (generate (length v) id) v` is a mapMWithIndex (monadic mapping with indexes) for vectors.
Yes I did, thank you!
Interesting. How are routes from different plugins collected? Can users define their own "nodes" (to borrow Drupal-speak) via the web UI? Any form building plugins? 
What's the open-source driver situation for seL4? They say this release supports the BeagleBoard, but how easy or difficult would it be to for a hobbyist with no real budget use the various peripherals provided by the board?
`clckwrks` builds on top of the more general `web-plugins` library. A plugin is defined by creating a `Plugins` value: data Plugin url theme n hook config st = Plugin { pluginName :: PluginName , pluginInit :: Plugins theme n hook config st -&gt; IO (Maybe Text) , pluginDepends :: [PluginName] -- ^ plugins which much be initialized before this one can be , pluginToPathInfo :: url -&gt; Text , pluginPostHook :: hook } Each plugin is required to have a unique `pluginName` which can be used as the initial path component. So, for example, the bug tracking plugin has the uninventive name "bugs" and all the URLs are something similar to `/bugs/submit-bug`. `web-plugins` also expects that plugins to use type-safe URLs. The `pluginToPathInfo` hook will generate the portion of the url that comes after the `/bugs/` prefix. So, in the case of bugs there is a type which defines all the routes: data BugsURL = ViewBug BugId | SubmitBug | SearchBugs | BugsAdmin BugsAdminURL | BugsData FilePath | Timeline | BugList deriving (Eq, Ord, Read, Show, Data, Typeable) These routes can then be used in the compile-time, type-checked templates like so: &lt;td&gt;&lt;a href=(ViewBug bugId)&gt;&lt;% bugId %&gt;&lt;/a&gt;&lt;/td&gt; The routes are added to the routing table when the plugin is initialized via a call to `initPlugin`: initPlugin p "" bugsPlugin You may have noticed that the route is a combination of a `Text` value and a type-safe value. That is because we want to be able to add new plugins while the server is running. Since we do not know what those plugins are -- it is clear that we can't do any 'compile time' checks for something that won't happen until runtime. So, `Text` is really the best we can do for figuring out which plugin should handle routing the remainder of the URL. The core of clckwrks pretty much just supports login and loading plugins. So, even the basic CMS functions are provided by a separate plugin `clckwrks-plugin-page`. Users can create new pages and posts via the web-ui -- but it seems like the nodes stuff is a little more flexible. There is currently not any form building plugins. It would be neat to see a `clckwrks-plugin-nodes` though. Or perhaps the exist `page` plugin could be more generalized? 
That's a good point. For now, I'll stick with just forLoop from the loop package. I did some more measurements and it seems to consistently provide the best performance for plain IO loops.
Heh, I spent most of yesterday learning about these very things. Finally discovered the 'async' package at exactly midnight.
Wasn't there another potential problem when the releasing action blocks?
Glad you got something that works well.
Very interesting, thanks for the detailed explanation! I was actually thinking of doing exactly this and building from there. I'll have a look at the plugin system and see if I can make a plugin for defining fields (=data types) and nodes (=collection of fields). Will probably require generation of some Haskell modules that then ought to be [automatically (re-)loaded](http://www.reddit.com/r/haskell/comments/229e6k/how_does_automatic_code_reloading_in_haskell_web/). You could then write type-safe custom plugins for a certain node while leaving the more "boilerplate" stuff (access to certain pages, node rendering, etc) to other plugins (or define some things in the UI once more). The main idea here is to not save data types in the database (like Drupal does), but generate code to allow for high levels of type safety. Might also be interesting to look at database value marshalling so you can do stuff like loading node content from the database, do some calculations, and then send it all to the theming plugin.
The leveldb package is called leveldb-haskell, and as this is a fork of it I went with rocksdb-haskell. But maybe you're right...
In any language, printing tasks are almost always io bound, not cpu bound. I don't know if this is the case for you exactly, but it's a strong possibility. Outputting to a tty via stdout tends to be especially slow for some reason. Are you trying to use output redirection to send this to a file? If so I would benchmark that specific task. I would also probably prefer using `hPutStr` instead of `putStr` to always send the output to a file.
You're going to need to provide a few more details. What do you ultimately want to produce? At very least a type signature on `pr` is in order.
Lots of maybes can be removed if you set default values during parsing: v1 &lt;- o .:? "optional" .!= default_value 
I was also wondering this when I read their claim. I really can't imagine what could be going on there. TLB pollution maybe?
Thanks for the comments. I just need write the output to stdout, with C, I can use binary IO, using fwrite every 65536 bytes of data, which is quite efficient I think: full source code is here (sorting 1M ints in range of (1, 1000000) ): {-# LANGUAGE BangPatterns #-} import qualified Data.ByteString.Lazy.Char8 as L import qualified Data.ByteString.Char8 as C import Data.Array.Unboxed import Data.Array.IO import Data.List(foldl') import Data.Maybe(fromJust) bucksort iou [] = return () bucksort iou (x:xs) = do c &lt;- readArray iou x writeArray iou x (1+c) bucksort iou xs -- readInt = fst . fromJust . L.readInt getinputs contents = let (nr:inputs) = map readInt $ L.lines contents in (nr, inputs) pr uarr = go begin end uarr where (begin, end) = bounds uarr go k n u | k &gt; n = [] | otherwise = foldl' C.append C.empty (replicate (u ! k) (C.pack (show k ++ "\n")) ) : go (1+k) n u tsort (nr, ints) = do array &lt;- newArray (1, nr) 0 :: IO (IOUArray Int Int) bucksort array ints iarr &lt;- unsafeFreeze array :: IO (UArray Int Int) return iarr main = L.getContents &gt;&gt;= tsort . getinputs &gt;&gt;= mapM_ C.putStr . pr Sorry for missing essential details, the profiling data provided before remains valid.
As an aside, it's generally good practice to provide type signatures for top-level definitions. Indeed [`Data.ByteString.Builder`](http://hackage.haskell.org/package/bytestring-0.10.4.0/docs/Data-ByteString-Builder.html) is likely going to the best tool for this job. `Builder` constitutes a `Monoid`, so you can simply smash them together with `&lt;&gt;`. When you have your final product, you can either get a lazy bytestring out of it with `toLazyByteString` or write it directly to a handle with `hPutBuilder`.
It might actually be a good choice, if the community would choose to embrace [Crash-only software](http://en.wikipedia.org/wiki/Crash-only_software) fully. Are you sure you want to mess with the filesystem directly if you could also use [`acid-state`](http://hackage.haskell.org/package/acid-state)? As for *actually* messing with the filesystem for messing with the filesystem's sake... well, I bet there's a way to ACID that, too.
Make sure to check out Parallel and Concurrent Programming in Haskell (http://chimera.labs.oreilly.com/books/1230000000929/index.html), that's the text which finally made all these concepts click for me.
Thanks for suggesting ByteString.Builder, never thought there is such a cool library, modified source: {-# LANGUAGE BangPatterns #-} import qualified Data.ByteString.Lazy.Char8 as L import qualified Data.ByteString.Char8 as C import Data.ByteString.Lazy.Builder import Data.ByteString.Lazy.Builder.ASCII import Data.Array.Unboxed import Data.Array.IO import Data.List(foldl') import Data.Maybe(fromJust) import System.IO import Data.Monoid bucksort iou [] = return () bucksort iou (x:xs) = do c &lt;- readArray iou x writeArray iou x (1+c) bucksort iou xs -- readInt = fst . fromJust . L.readInt getinputs contents = let (nr:inputs) = map readInt $ L.lines contents in (nr, inputs) pr uarr = go begin end uarr where (begin, end) = bounds uarr go k n u | k &gt; n = mempty | otherwise = foldl' (&lt;&gt;) mempty (replicate (u ! k) (intDec k &lt;&gt; string8 "\n" )) &lt;&gt; go (1+k) n u tsort (nr, ints) = do array &lt;- newArray (1, nr) 0 :: IO (IOUArray Int Int) bucksort array ints iarr &lt;- unsafeFreeze array :: IO (UArray Int Int) return iarr main = L.getContents &gt;&gt;= tsort . getinputs &gt;&gt;= hPutBuilder stdout . pr It shows there some performance hit, but mainly because the elimination of mapM_ @main: (Old version): Wed Jul 30 11:51 2014 Time and Allocation Profiling Report (Final) bucksort1 +RTS -sstderr -p -RTS total time = 1.04 secs (1043 ticks @ 1000 us, 1 processor) total alloc = 1,433,767,360 bytes (excludes profiling overheads) COST CENTRE MODULE %time %alloc pr.go Main 48.0 53.6 main Main 21.9 22.4 getinputs.(...) Main 10.6 15.6 readInt Main 9.4 3.9 bucksort Main 8.9 3.9 tsort Main 1.2 0.6 individual inherited COST CENTRE MODULE no. entries %time %alloc %time %alloc MAIN MAIN 44 0 0.0 0.0 100.0 100.0 main Main 89 0 21.9 22.4 100.0 100.0 pr Main 98 1 0.0 0.0 48.0 53.6 pr.go Main 102 1000001 48.0 53.6 48.0 53.6 pr.end Main 101 1 0.0 0.0 0.0 0.0 pr.begin Main 100 1 0.0 0.0 0.0 0.0 pr.(...) Main 99 1 0.0 0.0 0.0 0.0 tsort Main 95 1 1.2 0.6 10.1 4.5 bucksort Main 96 1000001 8.9 3.9 8.9 3.9 getinputs Main 90 1 0.0 0.0 20.0 19.5 getinputs.inputs Main 97 1 0.0 0.0 0.0 0.0 getinputs.(...) Main 92 1 10.6 15.6 20.0 19.5 readInt Main 94 0 9.4 3.9 9.4 3.9 getinputs.nr Main 91 1 0.0 0.0 0.0 0.0 CAF Main 87 0 0.0 0.0 0.0 0.0 readInt Main 93 1 0.0 0.0 0.0 0.0 main Main 88 1 0.0 0.0 0.0 0.0 CAF GHC.IO.Handle.FD 82 0 0.0 0.0 0.0 0.0 CAF GHC.Conc.Signal 70 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Encoding 66 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Encoding.Iconv 62 0 0.0 0.0 0.0 0.0 (New version, using ByteString.Builder): Wed Jul 30 11:52 2014 Time and Allocation Profiling Report (Final) bucksort2 +RTS -sstderr -p -RTS total time = 0.78 secs (780 ticks @ 1000 us, 1 processor) total alloc = 734,138,040 bytes (excludes profiling overheads) COST CENTRE MODULE %time %alloc pr.go Main 50.1 52.1 readInt Main 15.6 7.6 bucksort Main 13.3 7.6 getinputs.(...) Main 12.9 30.5 main Main 4.7 1.0 pr Main 1.7 0.0 tsort Main 1.5 1.1 individual inherited COST CENTRE MODULE no. entries %time %alloc %time %alloc MAIN MAIN 45 0 0.0 0.0 100.0 100.0 main Main 91 0 4.7 1.0 100.0 100.0 pr Main 100 1 1.7 0.0 51.8 52.1 pr.go Main 104 1000001 50.1 52.1 50.1 52.1 pr.end Main 103 1 0.0 0.0 0.0 0.0 pr.begin Main 102 1 0.0 0.0 0.0 0.0 pr.(...) Main 101 1 0.0 0.0 0.0 0.0 tsort Main 97 1 1.5 1.1 14.9 8.7 bucksort Main 98 1000001 13.3 7.6 13.3 7.6 getinputs Main 92 1 0.0 0.0 28.6 38.1 getinputs.inputs Main 99 1 0.0 0.0 0.0 0.0 getinputs.(...) Main 94 1 12.9 30.5 28.6 38.1 readInt Main 96 0 15.6 7.6 15.6 7.6 getinputs.nr Main 93 1 0.0 0.0 0.0 0.0 CAF Main 89 0 0.0 0.0 0.0 0.0 readInt Main 95 1 0.0 0.0 0.0 0.0 main Main 90 1 0.0 0.0 0.0 0.0 CAF GHC.IO.Handle.FD 82 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Handle.Internals 78 0 0.0 0.0 0.0 0.0 CAF GHC.Conc.Signal 71 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Encoding 67 0 0.0 0.0 0.0 0.0 CAF GHC.IO.Encoding.Iconv 63 0 0.0 0.0 0.0 0.0 I guess ByteString.Builder is relatively a new stuff, and the API isn't stabilized yet cause API changed between 0.10.0.2( used by GHC 7.6.3) and 0.10.4.0 (latest). Any way thanks for suggesting such a cool library.
Even I replace the old pr function with pr' = L.fromChunks . pr then main changed from mapM_ C.putStr . pr to L.putStr . pr' It won't have the slightest performance gain even mapM_ is eliminiated, so the ByteString.Builder indeed improved the performance, as you can see from the profiling data (1.04 -&gt; 0.78). 
IMO clearer: https://gist.github.com/jtobin/5bcdb652baaff927d286
Strange, I just wrote a simple repro case: module Main (main) where import qualified Data.Vector.Unboxed.Mutable as VUM import Control.Monad.State import Control.Monad.Reader import Control.Loop main :: IO () main = do let w = 1024 h = 1024 v &lt;- VUM.new $ w * h :: IO (VUM.IOVector Int) forM_ [0..1000] $ \_ -&gt; do {- forM_ [0..h - 1] $ \py -&gt; forM_ [0..w - 1] $ \px -&gt; VUM.unsafeWrite v (px + py * w) (px + py) -} {- forLoop 0 (&lt; h) (+1) $ \py -&gt; forLoop 0 (&lt; w) (+1) $ \px -&gt; VUM.unsafeWrite v (px + py * w) (px + py) -} {- numLoop 0 (h - 1) $ \py -&gt; numLoop 0 (w - 1) $ \px -&gt; VUM.unsafeWrite v (px + py * w) (px + py) -} return () void $ flip runReaderT 0 . flip runStateT 0 . forM_ [0..100] $ \_ -&gt; do {- forM_ [0..h - 1] $ \py -&gt; forM_ [0..w - 1] $ \px -&gt; liftIO $ VUM.unsafeWrite v (px + py * w) (px + py) -} {- forLoop 0 (&lt; h) (+1) $ \py -&gt; forLoop 0 (&lt; w) (+1) $ \px -&gt; liftIO $ VUM.unsafeWrite v (px + py * w) (px + py) -} {- numLoop 0 (h - 1) $ \py -&gt; numLoop 0 (w - 1) $ \px -&gt; liftIO $ VUM.unsafeWrite v (px + py * w) (px + py) -} return () Here the issue with numLoop does not show up.
Tty's are limited in some way thays hard coded in. If I remember correctly it has something to do with the fact that they're emulating an actual tty which was a terminal that connected to a server and so it had to have limiting factors built in so you didn't accidentally crash a server. I could be wrong on that, though. It's been a while since I read up on it. In any rate, a tty very slow by design. It's actually recommended to quiet almost all the output of a tty when booting up linux on a computer with a modern hard drive (especially for ssd's) as having to output everything significantly increases startup time. 
Kind of a derail, but does tmux or screen help with this? Like, do they decouple the writing from the displaying so that the writer can go very fast, even if you're "watching" with a real TTY?
So I asked on the snap framework channel how to bind a snaplet to a variable outside of an initializer and then pass the snaplet around the application. Mightybyte himself said this was not possible. Another user (imalsogreg) suggested that a possible way would be to use the functions in Snap.Test, which might allow you to get the snaplet itself, but to me this seems like a hack. So after a lot of searching, it looks like Snaplets are not compatible with rest, at least not in any obvious way. It makes sense since rest itself is kind of a framework built on top of snap-core or wai or happstack, depending on what driver you use. Snaplets are designed to pretty much do things the snap way.
I intentionally didn't want to set default values.
This looks really promising! Thanks. I'm going to do a refactor and see how it shakes out.
Automated: #!/bin/bash set -euo pipefail IFS=$'\n\t' die() { printf "$@" &gt;&amp;2 exit 2 } ensure-single() { case "${#@}" in 1) die 'No %s found\n' "$1";; 2) return 0;; *) die 'More than one %s found: %s\n' "$1" "${*:2}";; esac } f_cabal=(*.cabal) ensure-single '*.cabal' "${f_cabal[@]}" n_package="$(grep -iE '^name:' "$f_cabal" | sed -e 's/ */\t/' | cut -f 2)" if [[ -z "$n_package" ]] ; then die "Can't find cabal project name\n" ; fi cabal2nix --sha256=deleteme "$f_cabal" | perl -pe 's&lt;\bsha256\s*=.*;&gt;&lt;src = ./.;&gt;' &gt;default.nix cat &lt;&lt;EOF &gt;shell.nix let pkgs = import &lt;nixpkgs&gt; {}; haskellPackages = pkgs.haskellPackages.override { extension = self: super: { $n_package = self.callPackage ./. {}; }; }; in pkgs.lib.overrideDerivation haskellPackages.$n_package (attrs: { buildInputs = [ haskellPackages.cabalInstall_1_18_0_3 ] ++ attrs.buildInputs; }) EOF Possibly quite flaky. (Actually, I wrote it a couple of days ago, before seeing your reply; perhaps there is some detail your post mentions that my script fails to handle.) ---- (My original problem, it transpired, was that my `~/.bashrc` was messing with `PATH` in a way nix doesn't expect. I suspect that `nix-shell` sources `~/.bashrc` before modifying `PATH`, the other way round from what I'd expect.)
What does it mean to be a research language? I've seen many in the community hoping Haskell will become more popular and laud it's virtues for real world projects. Then I hear it's a research language. I guess it comes down to what you mean by a research language? A research language can still be useful for real world applications I suppose.
`web-plugins` should support that option. If you look at the serve function: http://hub.darcs.net/stepcut/web-plugins/browse/web-plugins/Web/Plugins/Core.hs#520 You'll see that it gets passed in the name of the plugin and the unconsumed path segments. Currently in `clckwrks` we get the plugin name by extracting it from the head of the path: http://hub.darcs.net/stepcut/clckwrks/browse/clckwrks/Clckwrks/Server.hs#105 But it could just as easily pull the name from the sub-domain. I would gladly accept a patch.
I’ve toyed a bit with that last type and it was surprisingly fun. It’s absolutely the sort of thing I might use in the future. If I have one thing to say about it though, is that the name `Managed` isn’t helpful at all. I got an example running which involved file IO, and composing it with itself obviously led to bad things since it re-opened the same file without closing/flushing first. Even more simply, you can do `resource &lt;- flip with return $ withResource foo bar baz` and now you’ve got a hand on an expired resource. Now to me the value in that transformer is that is allows convenient composition of fragments which come with ‘acquire before’/‘release after’ pairs. So why not a name like `Delimited a` (and `DelimitedT m a` I suppose)? Esp. since I suspect I would use it as subcomputations rather than as, say, part of the monad stack of an application: myComputation inPath outPath = do delimited $ do hIn &lt;- withFile inPath ReadMode hOut &lt;- withFile outPath WriteMode -- dumb copy and hGetContents is icky but -- this is for the sake of an example liftIO $ hPutStrLn hOut =&lt;&lt; hGetContents hIn -- reopen, but that's fine now! firstLine &lt;- delimited $ do handle &lt;- withFile outPath ReadMode liftIO $ putStrLn =&lt;&lt; hGetLine handle -- still wrong, but perhaps more obvious now -- that the resource escapes its limits -- at least we're straightforward that there is no -- 'management' other than what the user chooses -- to do badHandle &lt;- delimited $ withFile outPath ReadMode edit: on the other hand, since this involves continuations I probably made an association without realising it at first—since this isn’t about delimited continuations though this may be misleading as well
Spiffy. Sorry about the current state of documentation. Everything will be fully documented by the 1.0 release. The plugin system in clckwrks itself is very general -- so you should be able to just about anything. Also, I just remembered that a coworker was once working on plugin that allowed you to create plugins entirely via the web-ui. You filled out some forms, pressed a bunch, and it generated the new plugin. I'll find out what happened to that.
Yeah, I don't mind a different name. `Delimited` is pretty good.
Oh, I see! Thanks.
`foldl' C.append C.empty thelist` is an inefficient way to build a big string. You'd be better off with `C.concat thelist` or `C.intercalate "\n" thelist`.
Mark: http://www.haskell.org/platform/ still mentions 2013.4.0.0 as the next release and following that link RC2 is missing (RC1 is present).
Yes but they still runs faster than doing IO directly, and they are not called too much times because (u ! k) is a small value. pr uarr = go begin end uarr where (begin, end) = bounds uarr go k n u | k &gt; n = return () | otherwise = (unflattern (u ! k) k) &gt;&gt; (go (1+k) n u) unflattern k v | k == 0 = return () | otherwise = replicateM_ k (print v) Above code runs even slower.
what's the time complexity of Data.ByteString.Lazy (&lt;&gt;), sounds like the same as append but feel like it's faster?
I think its O(n). But you should be using the Builder monoid instance which is O(1).
Bummer, there are a lot of attractive features in rest.
Does this work for non-generic JSON generation? I'm using manual instances and I can only see this being used with generic functions. I can only see it for: genericToJSON :: (Generic a, GToJSON (Rep a)) =&gt; Options -&gt; a -&gt; Value
&gt; With the "let it crash" model of erlang, you actually don't want the main thread to die. You want the child threads to die and the supervisor threads to restart them. See http://learnyousomeerlang.com/supervisors. Under the crash-only paradigm (which Erlang doesn't enforce fully) you *do* want your main thread to crash, because crashing is the only way to terminate the program. If you don't expect the whole program to crash, how are you ever going to survive a power failure? The whole paradigm relies on the notion that consistency is guaranteed even if *not a single* finaliser is ever run during shutdown. &gt; Acid-state can lead to something even worse: version lock in. Future acid-state versions will be compatible with previous ones, yes, that's what it says. And I'd be interested in which ways the on-disk format of acid-state isn't thought out, as it's, considering it provides full ACID and migration, dead-simple. I guess it might upgrade from serialise to Duncan's new CBOR thing soonish™, but I'm confident that upgrades are going to be seamless, as promised.
Looks like Sean wrote a blog post at least related to this here: http://lambdalog.seanseefried.com/posts/2011-06-27-generic-dot-products.html
Hey again, I see that you asked a question about generated documentation in the Silk mailing list. I was able to get the documentation to generate, but hrefs were pretty broken (both the js imports and the navigation links) and required manual editing. What arguments did you use for everything to work without manual editing? This is what I did: ./result/bin/luscene -dfile:///home/dan/Code/my-rest-project/docs/ --source=/home/dan/Code/rest/rest-gen/files/Docs BTW, I too am curious about ways to manually elaborate the docs. I hope we get an answer on that soon. 
It is only used for Generic and TH generated instances. 
indeed. (thanks :) ) i'm trying to do pseudoradio silence while i finish preparing an alpha preview, but thanks. :) 
&gt; Some typical expressions from common imperative languages include: s/expression/statement/ Two of those would be expressions absent the semicolon, but "statement" would be more correct and clearer. Don't mean to put down the article though - on the whole, fantastic!
Thanks, that's indeed very interesting.
Can't use it then, but I'll keep it in mind for other things.
I'm not sure how this would fit the type signature of a `FoldM`. Did you have something in mind? As usual, I'm happy to extract reusable components of this outside of conduit, I'm just not sure how to in this case.
Does adding a `SPECIALIZE` pragma for `conduitVector` close the performance gap with `conduitVectorIO`? One thing to be careful of in this analysis is to see if rewrite rules are fusing away all of the intermediate steps. I actually had to search hard for a simple case that stumped the standard conduit rewrite rules (I also found no speedup in simpler cases). But more complicated cases were *definitely* helped by `vectorBuilder`.
Usually if the library depends from an external C library (as most FFI are) the docs fail to build, because such library can't be found by the docbuilder bot. Or at least this is my experience. Try to upload them manually via the script, or you can even use the new Hackage feature to upload 'em directly on the website via a file upload box where you can upload a compressed tarball (though I'm not sure if this last feature is already live)
Some minor things: Have you considered `newtype Triple a = Triple {{ unTriple :: (a,a,a) }}`? `join` (from line 39/40) exists in `Control.Monad` `printSlot` from line 56 could be more simply defined as `maybe "E" (head . show)` The part of `getPlayerInput` that decides whether a spot is taken, and what the message should be if it is, could be a pure function. Doing `line !! 0` and `line !! 2` is a bit dangerous; won't that crash the program on invalid input? Maybe define another function `parseInput :: String -&gt; Maybe (Int, Int)` and handle that situation by asking the user to enter again?
Sorry, I've just realised I made a mistake in `printSlot`. You could define `printSlot` as `maybe "E" show` and then you'd have to add a `concat` somewhere (because `printSlot` would then be producing a String rather than a Char).
Sure, I just added a benchmark based on vectorBuilder to my test repository: 1M file size/Speed-of-light: 21.33466 ms 1M file size/Chunking with raw memory buffer: 24.95672 ms 1M file size/Chunking with vectorBuilder: 43.72639 ms and the code processAndChunkOutputVectorBuilder :: (MonadBase IO m, MonadIO m) =&gt; Conduit ByteString m (VS.Vector Word8) processAndChunkOutputVectorBuilder = CC.vectorBuilder chunkSize $ \yieldByte -&gt; do awaitForever $ \bs -&gt; liftIO . forM_ [0..B.length bs - 1] $ \i -&gt; yieldByte $ unsafeIndex bs i vectorToByteString :: MonadIO m =&gt; Conduit (VS.Vector Word8) m ByteString vectorToByteString = awaitForever $ \v -&gt; let (fptr, offs, len) = VS.unsafeToForeignPtr v in yield $ PS fptr offs len and , bench "Chunking with vectorBuilder" . nfIO . runResourceT $ CB.sourceFile oneMBFileName =$ processAndChunkOutputVectorBuilder =$ vectorToByteString $$ CB.sinkFile outputFileName I didn't spend any time tweaking / optimizing, so I don't know if the performance difference to the raw memory chunking is because of forM_ doing poorly or something silly like that. In any case, it's performing pretty great already.
Enums in Java can have values attached to them
While it's true that Java enums can have constructors that take parameters, it doesn't make them carry information like a value constructor in an algebraic data type. The constructor of an enum is private, so you only get the fixed set of values that the enum defines. In contrast, a value constructor that takes a byte, say, would define 256 extra values
Very interesting for me at least. I wonder what is the consensus of things like instance Monoid (Event t a) where mempty = Event (const ([], mempty)) e1 `mappend` e2 = Event go where go t = (xs1 ++ xs2, e1' &lt;&gt; e2') where (xs1, e1') = runEvent e1 t (xs2, e2') = runEvent e2 t vs instance Monoid (Event t a) where mempty = Event mempty Event e1 `mappend` Event e2 = Event (e1 `mappend` e2) Could the latter version be considered as harder to read? I understand that many haskellers are queen on abstracting everything away, and in the case of simple things like monoïd I completely agree. But for more complex patterns, I sometime think if the "indirection" of standard instance (lists, pairs) makes it harder to understand what is going on. Maybe I don't write enough idiomatic haskell. I sometime wonder how things would fare if we used haskell in my team. The code reviews would be a place of many hot debates.
I actually [sent a pull request to bytestring](https://github.com/haskell/bytestring/pull/9) a while ago that might explain the slowness. I'd be interested if using `mapM_CE` or mono-traversable's `mapM_` has any impact. Also, `mapM_C` would be a bit more convenient than `awaitForever` + `liftIO`, and *might* be faster due to rewrite rules.
I didn't mean that you would have incompatibility between versions of acid state, I meant that you would have incompatibility between different versions of your program: """ Keep in mind that acid-state does not provide schema migrations. If you plan on changing the definition of your data-type during the lifetime if your application (you most likely do), you can either use a fixed schema such as XML or JSON, or you can use safecopy. As of version 0.4, safecopy is the default serialization path but using XML or JSON is still a possibility."""
Migration support is included via `safecopy`, which is a very capable versioning wrapper around `serialise`. It's a different package because it's useful on it's own, too. `acid-state` actually requires it for everything, dunno what's up with those mentions of XML and JSON. Probably just out of date, 0.4 is ancient.
The Codensity monad is pretty similar, but subtly different. Maybe that?
In Java, the values are "attached" once and for all in the enum declaration, via the (private) constructor. All the tags have the same *type* of data attached, and the same tag always has the same value. Different constructors of the same ADT, on the other hand, can have different types of value attached (as in `Some of 'a | None`), and you can use that constructor with any value of that type (`Some 1`, `Some 2`, `Some "stuff"`) to produce a value of the ADT.
Right, `Codensity` is just a `ContT r` universally quantified over the `r` so I imagine it will also not be an `MFunctor`.
I understand the choice of the Identity Applicative, but why was the Const Applicative chosen for the foldMapDefault implementation? How do it work exactly?
I dig your use of holes when working through the article. Very cool way to get people involved with actually trying to figure out the implementation themselves.
It's just how `Const`'s `Applicative` instance works instance Monoid e =&gt; Applicative (Const e) where pure _ = Const mempty Const e1 &lt;*&gt; Const e2 = Const (e1 &lt;&gt; e2) We're injecting values into the left side of `Const` when we traverse. The mapping function from `foldMap` lets us transform things into monoids, then the `Applicative` instance for `Const` mashes those monoidal values together.
Traversals require you to return an applicative functor "containing" a value of updated container `t`. Specifically the result of a traversal is of type `f (t b)`. But in the case of Foldable, you don't actually want to return an updated container, you want to return something entirely different, a monoidal value. The trick is to use an functor of two arguments, `f x1 x2` such that `x1` holds the type that you really want, i.e. a monoid type `o`, and the `x2` argument is totally ignored. If our functor if applicative in the second argument, the result of a traversal with this two argument functor is `f o (t b)`, and the `(t b)` argument is totally ignored. The `Const` functor is exactly of the required form. The second argument is completely ignored (in other words it is a phantom argument) and the `Const` functor is applicative in its second argument when the first argument is a monoid.
Never mind, I just realized that the `pipes`/`conduit`-independent portion of this is already encapsulated in the `vector` fold from `foldl` (minus the explicit buffer sizing).
Why not? You could `hoist f = lift . f . lower` couldn't you? At least in theory. Looks like `MFunctor` doesn't constrain the target `n` to be a `Monad`... but if `f` is a Monad morphism then it needs to be. But `Codensity m ~ m` when `m` is a `Monad` so this really shouldn't be a problem. You need the quantification of `r` for that property, so `ContT` forsakes it.
`LogicT` is a continuation-based monad, and as such is not an MFunctor. (At least I couldn't figure out how to make it one.) Because of this I cannot translate `Series m` into `Series n` in smallcheck (unless m and n are isomorphic).
Presumably `lower` is `Codensity m a -&gt; m a`. I never realised `Codensity m ~ m` for `Monad`s `m`. I find that rather suprising, but I'm not sure if I should... Anyway, that means `Codensity` is isomorphic to `IdentityT` so is an instance of `MFunctor`. Thanks! Still searching for any odd ones out ...
Thank you, I really like getting these juicy performance-sensitive cases. Please keep me aprised.
Good explanation, thank you.
Have you seen [probable](http://hackage.haskell.org/package/probable), [random-fu](http://hackage.haskell.org/package/random-fu) and friends? I admit the plotting is what distinguishes it from the packages we already have around for these tasks, at a first glance. I'll look more closely one of these days.
You should consider [GStreamer](http://en.wikipedia.org/wiki/Gstreamer). It's platform independent, and there are [Haskell bindings](http://hackage.haskell.org/package/gstreamer), although it looks like they're pretty low-level.
Neat. I'd strongly prefer a separate package for plotting - there are a lot of applications of probability distributions that doesn't require plotting, and graphical packages are usually heavy and hard to install. Perhaps make a haskell-distribution-core without the graphical dependencies?
Hi! Thanks for your reply. Yes I had a look at those packages. I made this particular package because I wanted to precisely compute the probability of random outcomes in a tabletop game, and display the different outcome probabilities as a chart. None of the packages I found seemed to perfectly fit my needs, and so I hacked together this one. The sampling feature came later as it was a very simple addition. 
I'll take low-level if the documentation is there. Thanks for the tip! I'll be checking it out tonight. 
fair enough, I thought I should point it for other readers.
Thanks for your reply! Indeed the dependencies for plotting can be quite hard to install. I will put plotting in a different package as you suggest.
Look at the types. &gt;We can describe a fold as taking a structure and reducing it to a single result. That's from the blog. That's a quote. That's all a fold does. No one is going to bother to spoon feed someone with a poor attitude. If you want a step by step guide with concrete examples feel free to read some of the more beginner materials available until you are comfortable enough to deal with the style of type-focussed presentation given in the blog post. EDIT: deleted brain fart conflating sequenceA with traverse.
So what does this example do that can't be done just matching on the string "dance" in the normal way?
Recent historical trends aren't reliable indicators of future performance. There's no magic in technical indicators. In my experience, people usually only use technical indicators to find the *very* specific entry or exit point for a security they already want to buy/sell for independent reasons.
This is an extremely well-presented explanation. Kudos, and please continue! Adding presentations such as this for more of lens will be a very valuable contribution.
I guess the point is that you can take something out of existential box and retain some type information that would be typically lost. Obviously you can write very simple programs to do the "dance" thing, but the article is about types.
&gt; Compare this to the definition of over and you can see how it looks and feels almost exactly the same. fmapDefault :: Traversable t =&gt; (a -&gt; b) -&gt; t a -&gt; t b fmapDefault f = runIdentity . traverse (Identity . f) over :: Lens s a -&gt; (a -&gt; a) -&gt; s -&gt; s over ln f = runIdentity . ln (Identity . f) An excellent observation, but "looks and feels almost exactly the same" is more handwavy than necessary. `over` is nearly *identical* to `fmapDefault`, with the only exception being that it is parameterized over the `traverse` function. Said another way, the above definition for `fmapDefault` is just the inlined version of the following: fmapDefault = over traverse 
Very nice. I was implementing something similar in Java recently called [jhist](https://github.com/carabolic/jhist/). But looks really good. Some minor nitpicks though. I would make the [Data/Distribution/Domain](https://github.com/redelmann/haskell-distribution/tree/master/Data/Distribution/Domain) folder a top-level folder called `examples` because this is what it basically is. Furthermore *Haskel-Distribution* is not a good name and quite misleading. And as the others said: no need to include plotting.
&gt; There exist purists in the Haskell community who are adamant that types are all you need, which is largely true. Sorry, but that is just total garbage. The behavior of functions is not determined by their types. For example, some function returns Maybe. OK, **under which conditions does it return Nothing?** That needs to be *documented.* For another example, suppose a library does something (say, it returns Nothing under some condition), and that I *know* it does that, because I read the source. Does the author *intend* it to do that? Is there a guarantee that future versions of the library will always do that? What promises is the author making about this interface? *That needs to be documented.* &gt; The issue with documentation is that people coming from dynamic languages expect a different type of documentation, focused mainly on extensive example code and tutorials. The #1 thing I want out of a library documentation is to convince me, within 30 seconds of looking at the very top of it, that it will do what I need it to do -- or that it won't. Documentation isn't just about providing a specification (although it needs to be that). It's also about selling the library to users. What are the most important features of this library? What are the most important limitations? What's the coolest thing you can do with it? *Why should I invest the effort into reading the type signatures?*
All good!
But presumably working at the type level allows you to do things you couldn't do at the value level, right?
typo? &gt; DataKinds are relatively simple. You turn on the DataTypes extension and all of a sudden some of your data types have unique kinds! &gt; &gt; λ :set -XDataKinds s/DataTypes/DataKinds/ ?
You can be more precise as to what programs are valid and what are not, but it's difficult to see on such a simple example.
Nice writeup. Do we now have enough machinery to do a typesafe printf without using Template Haskell? 
Sort of, you need dependent types to make the type of printf depend on the value of the format string, but you can fake it Haskell by depending on the type of the format string instead. See http://research.microsoft.com/en-us/um/people/simonpj/papers/assoc-types/fun-with-type-funs/typefun.pdf
Fair enough :-)
Like dependant typing?
Exactly.
Small update to the Haskell Platfrom 2014.2.0.0 release: We have new Release Candidate 3 versions of the source tarball... ...and a new generic-linux bindist of the platform! Details in the mailing list annoucement: http://www.haskell.org/pipermail/haskell/2014-July/024280.html 
It's an effectful list. They've been reinvented a few times for slightly different purposes, but I don't know that they were ever given an "official" name.
Yay! The cost was small, but I still couldn't justify the cost of a personal license for myself because I just don't do Haskell very much. Now I can start using it for my occasional Haskell/github open source activity, sweet! Thanks, FPComplete!
Is there a non-Church-encoded way of writing the datatype?
Is there any reason why the bot environment monad needs to be parameterized over a monad? or even why it has to do with IO in the first place? great idea btw
This is a valid question, I don't think it deserves downvotes. I don't think concrete examples are what is being asked for, but an idea about what the significance is of using a fold to take a structure and reduce it to a single result, vs the way we have always obtained single results from structures.
&gt; so as long as the type system allows us to express the intent It doesn't.
I thought there was already a free version. I remember trying it out a while ago.
&gt; This is why there is no filter or concatMap, since Traversable only defines a way to move through the data structure, but not a way to change it. I'm aware you can write filter as filter p = foldMap (\x -&gt; if p x then pure x else mempty) but this makes structures with no obvious applicative instance like Map unfilterable. Wouldn't it be easier to make filter a member of Foldable or is there something i'm missing?
At first glance it looks well written, with well-chosen function names. Like others, I'm not convinced that the plotting functionality needs to be part of the package, but it's nice to have. I wrote a probability distribution package a year or so back. As far as I know, it's the only package to (a) not be subject to combinatorial explosion (b) allow bayes-type calculations and (c) have a monad instance, giving you a nice embedded language for doing probabilistic calculations. This is achieved by having two constructors for the distribution: one which knows that the elements are ordered (so you get to use an efficient internal representation based on Map) and one which forgets that (so you can create Monad, Functor etc instances). I hereby give you permission to steal any ideas from it that you like.
That was great. Don't know how I missed it before. Would love to see the slides for this talk!
`((,) a)` is `Foldable`. How would you write `filter` for it? filter p (x, y) | p y = (x, y) | otherwise = ???
If you're interested in the subject of monad morphisms and transformers, this is a really good overview: https://hackage.haskell.org/package/layers-0.1/docs/Documentation-Layers-Overview.html
good point did think about that.
Looks like the free version didnt have all the features of the paid version &gt; As of October 1, users of our free Community or “Open Publish” edition will have access to features previously offered only to FPHC Commercial customers. This includes complete remote git support, complete git command line integration for Emacs clients, as well as git “mega repos,” and inter-project dependencies.
Wow, that's an in depth summary! Thanks for the link.
Update the copyright on the web page?
I disagree about the weirdness of the systems. Autoconf was written at a time where there were no standards for basically anything. The weirdness it supports has little value today but the cost of the crappy system design is getting higher and higher as the years pass. 
Thanks.
This example only really maintains the invariant that each command is only defined once. We're working on a project now in which we plan to use this invariant for default implementations of serialization/extraction from a work queue and magically building API endpoints from a single type the user provides. It's basically making the user's API less error prone and simpler, at the expense of some complexity for the library writer.
We should put the slides and video online some time this month. I'll reply here and create a new thread when it's done :)
Which MSYS should I install? I always used to install one using the official [MinGW Installer](http://sourceforge.net/projects/mingw/files/). When using the MinGW installer, one must be very carefull **not to install MinGW but only MSYS** components due to version conflicts with the platform MinGW. However, recently I noticed that another MSYS binary is recommended on [Haskell Wiki Windows Page](http://www.haskell.org/haskellwiki/Windows). Yet another MSYS is being pointed to from [GHC Building/Preparation on Windows](https://ghc.haskell.org/trac/ghc/wiki/Building/Preparation/Windows/MSYS2) page. Finally, Johan posted yet another instructions on his [setting up Windows Haskell development](http://blog.johantibell.com/2011/01/setting-up-haskell-development.html) page. 
Also, what's the recommended way of installing the latest `cabal-install`? HP comes with &gt; cabal --version cabal-install version 1.18.0.5 using version 1.18.1.3 of the Cabal library doing `cabal update` recommends updating `cabal-install` &gt; cabal update Downloading the latest package list from hackage.haskell.org Note: there is a new version of cabal-install available. To upgrade, run: cabal install cabal-install I heard discussion that people should never install it, or they should never install it with `--global` only with `--user`. I want my Haskell tools (cabal-install, hdevtools, ghc-mod, etc.) installed once for all my projects. What's the best "official" practice? Also, the platform installer could recommend adding the C:\Users\&lt;username&gt;\AppData\Roaming\cabal\bin to the PATH as all new executables, including `cabal-install` will be there. Otherwise, just running `cabal install cabal-install` will not help. All new users will be confused.
Woo hoo! Love me some free FP Haskell!
This makes complete sense if you take a lesson from Github's playbook of quickly eating the whole market. The false start they've made unfortunately must be still damaging to the image however.
GHC does retain information about modules it has built or external interface files it has read during a given compilation, in case it needs that information later. So that would explain the behavior you are seeing. There could be unintentional memory leaks too, I suppose. I suspect the answer to your last question is "no" but I don't know for sure. How much memory do you have on this system?
If symbols were actually type-level lists of type-level characters, you could move the format string to the type level with the existing tools, creating a type-safe `printf`. Unfortunately, that's not the way symbols are implemented.
My gut tells me anything with a `Data.Functor.Foldable.Foldable` instance that is of kind (* -&gt; *) has a `Data.Foldable.Foldable` instance (cata) and a `Data.Traversable.Traversable` instance (gcata). Some things might have a `Data.Functor.Foldable.Foldable` instance but be of the wrong kind. I don't think you can generate a `Data.Functor.Foldable.Foldable` instance from a `Data.Foldable.Foldable` instance because of the non-associated type family, though you probably CAN do a (existential) wrapped version. `Data.Functor.Foldable.Unfoldable` may also be related to `Data.Traversable.Traversable`. I've not gone through the equational reasoning to confirm this.
The commercial arm of the project was a company called Open Kernel Labs, but they were never able to successfully commercialise seL4, unlike the previous (unverified) kernel OKL4 which saw widespread use as a radio OS for mobile phones. General Dynamics purchased OKL a couple of years ago and last year they dissolved the company completely and laid off all the employees. The open sourcing was, as I understand it, because GD didn't know what to do with the seL4 IP and having it just sitting around collecting dust was quite useless. And the people that actually worked on it (NICTA) wanted it to be made available as that fits in well with their research aims.
Seeing as GD fired all the OKL staff and closed their Australian operation, I'm pretty sure open sourcing seL4 was indeed a sign of the commercial side's death. I think GD are hoping that sufficient community interest from the open-sourcing will lead to them being hired for lucrative consulting contracts, but apart from that they have no real idea what to do with the seL4 IP. NICTA want the open-sourcing because it makes research work way easier. In short, I don't think we (NICTA) are going to push towards commercialisation any time soon, and I don't think GD have any idea how to commercialise seL4. 
seL4 has no real drivers except the couple it needs to actually work, although it's possible to host linux underneath it and make all the hardware available to the guest linux kernel. We're not really interested in _writing_ drivers for the kernel, rather we're more interested in [generating the device drivers automatically from hardware specs](http://ertos.nicta.com.au/research/drivers/synthesis/).
As always when this topic comes up: I work in the Trustworthy Systems group at SSRG, NICTA. So feel free to ask me any questions and I'll do my best to answer (although I'm not an expert in everything that we've released, I do have a pretty good idea of how the verification side of things works)
&gt;I never realised Codensity m ~ m for Monads m. Is not true. I wrote [this](http://stackoverflow.com/a/12591749/683453) a couple of years ago explaining why. Technically, the relation between Codensity m and m is a retraction not an isomorphism.
Awesome, I'll be submitting something for sure.
Not quite like that. We're working on the UI detail, but there will be some approval-to-publish action required. This is covered in the FAQ, but let me know if the FAQ needs clarification, since that's what an FAQ is for. 
You're welcome. Just doing our job over here. Let us know how else we can help.
This is really a continuation of what we started more than a year ago with School of Haskell, which is free and quite popular. I expect that as our business continues to develop, we'll be able to keep putting out more and more free Haskell goodies. Our priority is to release things that help *you* put out more great work. That's a nice multiplier effect.
Existing type systems do to a degree; and theoretically, I don't see why they couldn't express much more. Look at the crude syntax awareness built into things like doxygen or javadoc: comments, there, are partially understood by the toolchain and aid static analysis. Now extrapolate that idea, and imagine a comment syntax that completely merges with the language itself, allowing you to reference all of the regular language constructs, and express their relationships, conventions, prerequisites, etc. (Oh, and, just to avoid confusion: a suite of unit tests is pretty much a poor man's type system - 100% unit test coverage and "perfect" expressiveness in a type system converge towards being the same thing). Many parts of such a system already exist, things are moving in that direction anyway. I doubt comments will become obsolete entirely though, and I believe the majority of the industry will simply fail to reach the point of enlightenment and just prefer free-form comments over rigid assertions because "it's so much easier". That's why I said "In a perfect world".
&gt; traverse :: (Traversable t, Functor f) =&gt; t (f a) -&gt; f (t a) That isn't the type of `traverse`. 
That's what I get for typing without engaging my brain at all.
Can't upvote this enough.
The HaskellWiki Windows page you reference is the most recently updated of the three (27 June 2014). MSYS2 seems to be the recommended project to use now. A link at the bottom of that wiki page is to the older/original MSYS, so perhaps that is causing some confusion? The GHC wiki page consistently references MSYS2. The version of MSYS (original or MSYS2) is probably at your convenience. That is, unix build tools of all flavors are used across the various other platforms, so the particular MSYS version should not be that critical. A GHC release (e.g., 7.8.3) on Windows comes with a particular version and subset of the mingw32 toolset, specifically for the gcc compiler. The version of gcc to use with that GHC is more critical and should be the one which came packaged with GHC. Note that since the Haskell Platform on Windows comes complete with one of these full Windows-specific GHC releases, it includes the needed mingw32 subset. There would certainly be concerns about installing other mingw versions (not MSYS, of course, but mingw) if those installs altered the PATH variable (e.g., putting themselves earlier on the PATH) and caused GHC or Cabal to be unable to get the matching gcc toolchain.
I'm not sure about the advice about never installing a newer cabal-install. Using the sandbox mechanism to install into your project directories with just the dependencies needed for a given project, is very convenient (and the version of the cabal tool in this HP has the sandbox support). The HP currently adds the PATH you suggest during installation (where --user binaries will be installed). However, I just realized it is not adding the location where --global installed binaries will be placed.
Indeed, I've found that Haskell as a whole is full of wonderful insights. This is a great example[.](http://hackage.haskell.org/package/base-4.6.0.1/docs/Control-Monad-ST-Safe.html#t:RealWorld)
&gt; In functional programming, our proofs are not by contradiction, but by construction. 
Such values are sometimes considered "nullary operators" in Algebra, but this doesn't seem like a useful way to think about values in Haskell generally. (Or even a correct way, depending on your definition of function.)
I'm incredibly glad this was written. I recently tried to gently call someone out on behaviour that I thought was a bit poor - in this instance, dogmatic advocacy to the point of being quite arrogant. The response was short, sharp abuse that was hurtful enough that I've found myself dialing back my activities in quite a few avenues of the online Haskell community that I was previously involved with. I really value those communities, and feel that I've got a lot to offer them, but I can't quite make myself get back in there since I really don't want a second helping. Hopefully articles like (and possibly the associated discussion) will mean there are less instances of these kinds of interactions. (On the plus side, I've been doubling down on my involvement in my local FP community, and have been ramping up my work on blog posts and libraries, so it's not all bad)
Being a dogmatic advocate of something is easier (and for some people, more fulfilling) than actually doing something with that something. 
No, because it's a duplicate http://www.reddit.com/r/haskell/comments/2c9u0g/fp_haskell_center_now_free_for_open_projects/
So, all homework done by students using it will be available to their friends automatically. Unfortunate.
I like [the export comment](https://github.com/haskell/bytestring/blob/2530b1c28f15d0f320a84701bf507d5650de6098/Data/ByteString/Internal.hs#L78): &gt; Deprecated and unmentionable Anyway, I looked at this function and it's not so bad, they're just ėxplìcitlẏ ųnpacking thė IO action, Ì dōn't séȩ—the geòmétry of the dreäm-place I sāw wãş abnòrmal, non-Euclidean, and loathsömely redolent of spheres and dimensions apart from ours—a̮ͅn̠̠̲̣d̪̩̹͍̻͇̘ ͓̻̱͖͙y͍o̤̻͔̳̪ͅͅṵ̰̫̟͉̠̖ ̹i͚̻̦̠ṉ̲̩̰̖̯̺e͖͎͉̫̤̲̟̱̖v̪̺̟͎͍̯̻̟i͈͓t̬̬͚a͎̗͙̘̬b̗̜̤̰̞̼͉̪l̬̥͔͚̻ͅy̮͕̭͉̳͇̺ ͕ͅn͉̼̭̜̫e̮̤̳̳e͔̪̼͚d͙̪̟̭͕̞̠ ̬̹̱̲̮̲̘u̳̬͓̱̟̝n̟̜̜̘ṣ̥̮̪̖͈a̯̻̤̬f̪̟̫̱͔̩̠͖e͔̹P̥̗e̥̦̱r̞̮f̙͙̹͔̘͍̯o̮̪͔̹̩̤r͔͔̝̣͓̤̹̦m͕͖͓̗̬̣Ḭ̼O̳̼̫ ̤̪̟͇t͕̩͉̞̣o̞͕̙̦̳ ̖̦͈͉̟̰g̝͚͖̠̘͔e͎t͎͓̗ͅ ̪͙̠̜̘̯͕̭a͚͎n͓̳̟͕͕͓̻͔y͉̲͍̼t̬̜̥̟͓h͉͍ị̜̟̺ͅn̠͕̬̖̮̼̗g̻ ̫̫̼ͅs̭͕̳̖̺͚͔ẹ̲̙r̖̭͚̣̘ͅi͙̤o̜͉̬̺̤͎̫͓u̺̭̥̱̙̥̻͍s̙͇ ͚̤̮̭d̮͈o͎̼̭͔̙̻͍ͅͅn̠̲e͇̦̪.͙̬̣͓ ̻̪̼̜̼̝ ̗͙̯̻I̬̮͖͚͔̺n͔̜̫̼̬̻͙ ̖͓̤̜̟̝͈p̭̝̬̘͇̮͍̘r͖͙̬͈̼͇̜ͅa͍̮̜̟̘c̗̼̘̜͚̬̣̗̺t͕̣̹͙̖͉͕i̯͓c͍̱̣̩̜̰a̹̖̗̻l͍̖̳̘̜̳͍̳̥ ͉̻t̰̫̥͎͈̭e̯͈̯r͈̞̖͇ͅm̟s̙,̳͉͕̫ͅͅ ̰̩̯̩͕ͅy̤͓̙̼̺̗o̳̭̗u̩͇̯͖̱̘̟͚ ̺͓̘̙a̤͈͈͍̱͓r̪̮͔̩e̻̫̩̖̜̭ͅ ̜̱̣̰͉̺d̙͖͚̞̯̤͚̳̼e͚̩̣̪̥͓p̗̼̦̲͍̠͙̣r̤̦̳͙̬̼i͕̭̹̼̥͈̯̙̰v͔͈̥͙͕̝͓e̜̬̜d͈̮̬̹̝ ̩͔̫o̹͔̟͈̱f̺͖̠̮̳̖̹ͅ ̩̼̼t͖͔̝̙̣h̤̤̬e̹̥̯͇̗̦̗ ̣͙̲̱u̻̱͙̖͉s͖̣̹̼̺͙̤̳e̲̣͍͖̲͉͍̰f̜̘̰̘u̝̟̠̩̯̱ḽ͎̻̭̳ ͉̘̟͉̳͍͚c̰̭̰̪̦̯̹͇o̮̤̮̯̣ͅn̘̦͖̰̮͇̼̜̼c̩͓͕̻̣̹̫̲e̬͉̬͇̲p͈̠t̘̖̣̖̺ ͉͉͖̞̜͓͉̼o͉̞̠̟̫̠f̪͖̺̻͓ ̥̳̘a̹͎̖̩̻ ̭̪͉̪``benign effect''—̼̖̭t̗̭̱̗h͇͎͉̫ͅe̟ͅ ͚̜̪̱͍̰̗N͓a̖̠̟͇̩̦̙̰̞v̹̟̼̪̞ị̗͎̳d̖̗̞̪̯s̟̮̲͙̫o̬̱͍n̬̱͎̞̮̜s̫̣̺͕̤̞͙̻ͅ'̺͓ ̟̩̩̠h͖͍o̤̺̞͚m̱̤̣̼̠ͅe̜̠ ͚̝̙͚͕̻̘̯h̠̱̩͙̰͙̙̹̹a̰̣̰ͅḓ̣̜̳̘̱͇ ̮̜̳͚̞̠̱b̥̹͍͓̬ͅe̩̖̲̮̬̞̱̤̟c̺͎̭̫̞͓͚͓o̘̻̩m̥͓̮̤̠̝e̥ ̰͇͉̩̻̳͈̫s̺̞͈̼̖̠̱̼o͕͓m͖̝̥̯̙̰̝ͅe̗̟̜̝̪̤t̪̭ͅh͖͈i͓̱͉n̲̘̜͈̳̹̯̭̻g̻̝͓ ͔͈̘̯ḛ͚̤̬̣͉͙ͅl̼͎̬̤̥͓̦̳s̼͉͓͙̼̦̹̟̙e̗̺̲̬,͕͔͎͙̫͍͇͈ͅ ̭̰̪̻̰̝a̠̭͍n̼̺͚d̳̘̻͓̦̣ ̤͓̦ͅw͈̥̲h̙̙̟̫̬i̹͙̣ḷ͍e̞̬͙͚̳͕̤ ̙̺͉̰̲ͅn̯̲o͔̗̜̣͖ͅt͙̖̦̤ ̭̣͎e̜͈̥͎̺̻ͅx͉̲͓a̖̠̘͎ͅͅc̟̥͕͎̖͕̺̘t̼̦̦͔̟͚ͅͅl̠̹̜̬̰y̺͓͖̩̫ͅ ̙̼s͕̻̘̠̘͎i̝͈̼n̼i͉̹̖̘̜̖s̰̼̘̤̰̠̜t͎̳e͇̟͙̗̜r̥̯̲̜̜̘͖̮̻ ̘͎̻̫̗ͅo̟͍͕̠̲̱̻͓r̞̮̪ ̰̳̝͈̦̬͔e͕v͖̺̭̭̟͉e̼͚̪ṉ̣͉ ̟̘̲͕͓̰̟t̫̺̣̯͔h̙̭̲̫̼̞̪͉r̺e̙̙̱̦̻͈͔a̲͖t͕̠e̱̯̬̤̻̬n͈̺̫͙̲̖͉͖ͅi͓ṋ͈͉̭ͅg͚̲̩͔̯͉̜,̝͓̮̳ ̩̟͕͙̦ͅt͖̭̜̰̮͙̠h̗͈̜ͅe̟͎̬͉͉ ̘͇̮̩̭̣̘c͈h̲͚͖̩̠͔̘̜a̱̺̗̬͎͙n͔g̦e̟ ̦̝̗͇̣s͓͙t̼̪̦̩͔̝̙i̬̜̜̜ḻ̻̬̝ḻ͈̺̞̩ ̰͈̻̭d̖͍̭e̯s̰̬͙̻͖̤̜t̖̦̯̟̗̮̺ͅṟ̤̥͎̮̞o̜̮̩̞̠͍y͈̜̮̟̼̞e͚̩͙͍̫͕̻d͕͖̮̪ ̠͉̰͉͈̭̮̳̫a̘̟̬n̟̯͈͓̣͎̫̱y̖ ͇̲̠ͅs̹͈̘͔͚̠e̫̤̟̖̰̮n̥̼s̘̯̮e͍̳͕̱͙ͅ ̝̹̬̫̥͎̮͍̱o͖̜̹̹̙f̗̮ ̠̝̺̥̤͓̥ͅs͔̣̝̼͈e̜̜̩̳̹c̩̞u̦̙r͔̯̠̻i̩̗̦͙̫̝t͖̣̝̝y͉̟̠̳̖̭̼̫ ͎̯̹̰͓͍̞ọ͙͙̦̰͉̰ͅṛ͖͉̭͈ ̜̺w͇͓̙̜͙̳͈̖̟e͎͍̜̝̮̝l̳̫̠̲̲l̻̳̦͈̣-̜̮̦b̭̰͈͓̤̬ͅe̦i̠͇̰̲͕n̦̹͖̞̬̭̜̰̞g̝̱̠̙̻̭̞̠—̞̥̭̣̤̗͈̞—̞̳͚̩͡In G̘̤̯̀ͅerman̵ ̻̟̝͠t̷͚̝͉̤̜h̝̜͎́è̤̲ ̶̮w̸̩̙ơ͉̩̣̖̣̠ͅr̜̬͉͔͇̘̻ḏ̸̘͕̙ ̙͎͝f͚̙͈o̻̲̙̲̠̥͓r͍̮̜̳̀ ̙̭̹̜ͅ‘uncanny’ i͉s̛ ̨̩̙̫͓͖̤̗'̢͕̠͈̜̝ͅu̸͙̤̭͉̗͙̩n̟̦̥̙̻he̩͙̫͚im̥l̩͍̯͕į̤̤c͏̜̼̗͖ḥ'͖͍̱͚ ̩͉̤̝̝̮͜w̖̲͉̘͎̬͝h̘̬̕i̡͚c̪h̷̹̼͎̣̮̺ͅ ̣͢H͖e̺̕i̙̘̼̤̫̕d̦͎̭͙͖̜e̮̱̼g͎g̦̟e̝̲̯͓ṛ͔͓̱̬ ͎͉͡i̟n̶ ̠͈̗h͇͎͖͟i͓̪̣̪s̼͈̩ b͈̜̘͠ͅo͈͔͉̝o̭͝k͓̲̤̥̞͠ ̜̖͔̩͚"̪̙̣̳̠͘S͏̰ẹ̬̲̘ͅͅi̳̰͖̮̲̪͔͠n̰͇̘̱͍ͅ ̹u̸̖̙n̞̜̬͚d̟̥̻͙͚̱ ̵͖̘̥̙Z̙e͕̖͓i̡̻̼̤̟t̢̝̞̘" ̻̼̻̯t̨͔̳̹ͅho̞̞͍̩̞u̩͟g̞͍̦̼̗͟h͔͜ͅt̨̯̹ͅ ͚̗w̷̝͔̦̞̻̺͓or̲t̲̠̭̳̭ͅḫ̨̞y҉̰ ̱͖̤͖̩̹͚o͖̘͎̕f͎̩͔̩ ̴͚s̢̝̻͖͖̠̦o҉̝̳̖͙͖me ̷̯̳͚͙c͕o̪n͏͕s̴̫̩̰i̳͖̫̫̖̙d̲̦̝ͅe͍̼͔̼͕r̫̭̘̻͎̪a̮̟̤̣̗̪ṱ͖i͙̗̰̪͚̱̹o̱̦̰̺̞̤̟ṇ̭̳̤̻̼:̲͇̤̗̫̖̭́ ̪͔̼̻̪̜[҉͉̲…͉͙]̶ ̶̯͎̥͈̱̘I̷̩͇̹̝̥̖ņ̖͕̲̟ ̡̰̩a̖̘̻͙͍̼n̶̼x̣̩̫̥̀į̭̻e͇̪̣̗̣̭ț̨̘͕̼̘̩̝y͚͝ ͈̮̭͔̼͉̹o̬͕̞͎͕͔̳͝n̩͕̥̠͍͍͔e̘̺̪ ͏̟̩̪̻f͙̲ḛ̖̣̗e̜͓̻͍̠̗̗l͉͔͖s͔ ̣̹͘ú̙͚n͍̼͚̜c̪͇̩̘a̬̰̥n͔̟n̮̭̦̱̣̭̺͝ỳ.̵̣̼̟̞͎ ̯͓͇̥̫͙H͟e҉̮r̘̺̰͈̲e͚̘̭̠̩ ͕̤̦͟t̼̼̟͍̥̙͟ḫ̶͚̭̜̺e͏̙͙̲̮ ̫̤͉̜͖̬̮pe̡̟̤̦̯̘͎c̯̞̼̱͈͞u̧l͏̺͇̘̩͍ͅi̮̠̯̪̝ͅa̟͉̪r͚̤̳̞ ̜̲̩͇̗i͏n̢ḍ̸̝͓̪͓̱e̴̮̲̜̩̥̠͙f̭͡i̩n̴̯͖̱̞i͖̰̜t̸̙͎̠̖̰̝e̛̝̭n͖͍̯̟͔e͕͝s͎s̱ ̖͞o͉̭̗̖͜f̹̯͓͓̠̣ ͖̯ͅt͚̬͖̯͡h̙̘̦͢a͖̟͚̦̗t̫̲̰̫̗͘ w̡̫͍̭̤͇͚h͕̜̭͚͍̫i̷̺͕̠c̻̠͕͕ͅh͖͚̗̀ ̤͢Da͇̫s̮̻̬̦̯̣̼͜e͎i̝̰͚̬ņ̭͓̞̖͈̖̼ ̪̼͝f͇͟i̢̳͕̻̫͎n͡d͇̲̠s̷͕͔͚͕̥ ̯̲̗͙͘i͚̠͙̱ͅt̳̱̣̙͠ś̬̙̯͙͇̫el̖̲͉̗͎̘f̤̳̲̝̪ ͏̻̥̮̤a̱̹̖ͅl̨̮̗̺͖o̫̞̯̪ņ̝g̤̪̤̞s͏̘̰̳̜̬̯̖i̛̳d̶̙̭͓̺ͅͅe͕͔ ̀ị̦̻̘̱̼͜n̥͚̟̮̝͠ ̘̱ạ͉̘n҉̖̰̬̣x̹̲̣i̼̩͍e͉͉̦ṭ̤͙̻͟y̵̺͉̦,̗͖͖̫ ̫̝͈̳̖̘͎c̪͉͚o̡̫͔̺̟̫̱m̴̳͇̯̠e̘͡s̢͇͚͕̫̝ p̦̹̤̝̜͚r̩͔̯̘̻̲̲o͞x͘i̟͔̱͡m̵̗͇̥͓͚̖̥a͏̥̰͎̘͈̰̙l̺̝̖̳̻ly̱͈͇̤̬̳ ̫͚̲̮̤̺t̰͉̞̘̩͇̟o̯͓͚͟ ̭̖̥̬̺͙t̳h̶͇̩̪e͕̠̬͚̕ ͉̠̱ḛ̶̥x͇̥̞̘̺̞p̩͠r̟̞͉̦̻͎ẹ͉̫̯s̡̳͉̟s̸i͖͎̥on:̮͓̬͈ ̛̠̪ṭ̤͉̻̟ͅh̼̫͡e ̺̤̮́'͎̳̱̗͈NOTHING ̨̱̗a͠n̷̙͕̙̜d҉̱̩̖͓̱ ̱͎̜̬͈̩NOWHERE'̠̙.̲ͅ ̯̪̯B̵̯͔̳̞̹͔̙u̵̝͈͍̳̪̠t̹̖ ͙̟͉͕̭͙̞h̶̗̦̗e̬̬͓̕r̢͉̯̹͈̗ͅe̡͕̞͙͔̱̱ ̙͖̦͚͙'̧̮͔̟̼͙u͖̥̞̳n͏̲̞̦̦c̨̰̭a͔n̤̲̜̗n̴͙͉̤̠͔i̸n̰̠̻̻̳̘ęs̪̦̦̮̗̥̮s̸'҉̝͕̤̲̰̺͙ ̯̹̫̞̜a̤̲̬͔̹̞ls̸͎͕͍̻̳̪ò̫͖͖̟͔ ̵̬m̬̮͎̰̝̪͠ͅe̴á̬̘n̳͝s̴̪̪͎̳ ̖̺͇͓̝'̮͕̜̺̝̖not-being-at-home.͝'͔ ̰͚̩͉̬
One useful trick in these situations is to build and run with profiling, then you can just use the CCS to get the full stack from a crash site, plaintext in call order instead of the usual evaluation order.
Yes, if the crash is exclusively in Haskell land, then that can indeed be useful. However, the GHC API is not available in profiling mode, and moreover, the crash in this case was deep in the bowels of RTS functions, in which case Haskell cost centers would not be helpful. But yes, in the Haskell-only example that I gave that would be helpful. I probably should say something about that in the blog post.
&gt; imagine a comment syntax Sure, you can put documentation in comments. That's not anywhere near the same thing as putting intent into the type system, though. Let's be concrete. Suppose that an HTML parser claims to support all of the HTML5 draft, v4, except for feature X. How is that going to be "expressed in the type system"? Does there need to be a type for feature X? 
I also run into problems sometimes compiling pandoc from source and wonder the same.
FFI invoked C/C++ and RTS code can of course just be traced back with the normal system stack traversal till we hit Haskell's STG stack where the CCS would be available. But yeah, I never used the GHC API and never did anything like what you were debugging for your client, so that might indeed be very different ;-)
Alright, so you replace the documentation with links to an HTML document, that are "in the type system" because you've put HTML links into the type system somehow. But the HTML document is still textual documentation. And as far as Haskell's popularity, the only issue is the lack of the textual documentation; certainly, nobody is feeling for a lack of ability to create types for links to documentation.
OK, you'll need a bit of imagination there. Imagine the spec is not a free-form HTML document, but something structured - XML, JSON, whatever. And the URLs I put in my annotations are not just URLs, they point to specific nodes in the spec; further, the spec would allow the compiler to extract a complete list of must-have features. This means that the compiler can do a coverage check against the spec, and fail if I don't meet the coverage requirements, forcing me to somehow tell the compiler that the omission is intentional. And frankly, I don't even think Haskell lacks textual documentation at all. The better packages (the ones you should be using) are documented quite thoroughly, and they have seldom left me wondering. I believe the real show stopper for Haskell is that the overwhelming majority of all easily-accessible code is in the imperative / object-oriented realm; practically all the languages out there subscribe to the mutable state / sequential execution / strict evaluation paradigm, and Haskell is thoroughly different from that. This means that reaching the level of proficiency where you can make use of the documentation takes longer than it does in, say, Python; in fact, I would argue that most of us are already very familiar with a procedural language, namely the subset of our native languages that covers user manuals, cooking recipes, and other step-by-step instructions. Much of that translates to imperative programming very easily, while functional programming is more declarative in nature, more similar to math than to a practical hands-on how-to. But once you are familiar with the idiom and the language, the type system actually *is* a very powerful documentation system.
Also, of course, if your program _really_ crashes (segfaults), running with profiling won't help either -- you will segfault and that will be the end of that. No nice stack trace :)
An excellent article that *many* technical communities could learn from. There's one more syndrome that I think it could usefully cover, however: if someone has differing views from yours on a technical topic, and doesn't immediately embrace yours, don't assume that this is because they are an ignoramus (best case) or idiot (worst) who needs schooling the right path (i.e. yours). Rather, assume that their personal and professional history has lead them to a different set of problems and priorities to yours, and therefore to a different set of preferred solutions. Understand the problems they deal with and the aspects of them that they think are important before demanding that they accept the superiority of your solution.
I actually meant picking up the CCS from the stack yourself with the debugger or an automated tool, not the +RTS -xc feature.
I once did a thought experiment about what is harder - add laziness to strict program or strictness to lazy program. It turned out that it is easier to add strictness to lazy program than vice versa. If you have mostly strict program and want to add a lazy list map, then you have to write at least two versions of map: element-lazy and spine-lazy. For other list processing functions you have to also write special lazy versions. After that you have to be careful combining them - one strict map will ruin everything. When you add strictness to lazy list program, you write two combinators - one adding element-wise strictness and another adding spine-wise strictness. And apply them to the computation as a whole, letting the compiler to sort it out. That's why I am big fan of bang annotations and in huge opposition to that proposal.
Oh yeah, +RTS -xc is a bit of a mess, I filed a bug over it. Till 7.10 with DWARF3 goodness (hopefully) hits there's no system stack debugger backtrace, but you certainly can manually locate and walk the STG stack or just pick up the CCS from the top closure and walk that (very simple memory layout). Even manually with gdb/lldb, it's not that bad.
Ah, yes, that is something that I did think about. It would certainly be worthwhile trying to write some Python script that could do that.
Nitpicking: To increase code readibily I would use a tag prefix instead of longuest record field names such as simpleQueryStringQuery (https://github.com/bitemyapp/bloodhound/blob/master/Database/Bloodhound/Types.hs#L529) 
I think Scala uses something like that for its monads, except it's [implicit arguments](https://docs.google.com/presentation/d/1Jg-GRxRmMuG1Oks_-OrBT8qWGmuDqblqWoAhicFmswo/present#slide=id.i76) instead of (or perhaps in addition to?) implicit functions. I wonder how it's working out for them? Without static restrictions on the allowed implicit stuff, like Haskell puts on type classes, it seems to me that the compiler should often have issues with ambiguity and possibly termination, especially if the implicit stuff supports chains like `instance Monoid a =&gt; Monoid (Just a)`.
I think a general example of the ambiguity they face is the inability for them to run good abstract Set algorithms because you can't know your order will be the same between construction and lookup times.
Is this FAQ published somewhere? I can't seem to find it. Does this mean that I won't be able to make a commit to a private repository without paying for a commercial license?
Students would need to actually create a commit of their code, "working tree" changes won't be publicly available. So a student can still work on his/her homework as much as he/she wants without sharing the answers with the rest of the class.
Is there a Raspberry PI bindist for GHC? If so, building the platfrom for it should be pretty easy... modulo the bootstrap needs (You need a working haskell install to build the platform!)
It's not *that* hard to do, but certainly messy! I wrote an article explaining it and a C++ tool that automates it for one particular platform (https://github.com/blitzcode/ghc-stack). It is very much compiler/RTS/OS/CPUArch dependent, and I did not invest the time to make it widely portable. I was also thinking of maybe just scripting it on top of gdb's machine interface, but decided to do it fully manually with Mach kernel and POSIX APIs. It's a tradeoff, as always. If you manage to write a script driving lldb to do this, be sure to also blog about it!
The web page might not have been changed since 2013, in which case I'm pretty sure it's illegal to put any number other than 2013.
Hmmm, actually it didn't add the Roaming one to my PATH, I added it manually. I'll check again tonight. 
How did you arrive at this? Does this take into account the fact that bots in this version of the game are allowed to inspect each others decisions? (And as such, a completely deterministic bot like tit-for-tat will be obliterated.)
https://www.fpcomplete.com/business/Open-publish-frequently-asked-questions/
House of Leaves is great.
i've had a similar issue installing stuff on a vm w/ limited memory, and the easiest workaround i could find was to simply add a bunch of swap space
It's just like my opinion man, but there is a difference between asking for some clarification vs &gt; over and over and &gt; bother to explain I think the tone is crap. That's my opinion. I've been wrong before.
This might even be suitable for /r/all ...
Hmm, that's another good reason to prefer type classes to implicits! The kind of ambiguity I was thinking about was compile-time ambiguity: if there are two values of the same type in scope, I imagine that the Scala compiler would not know which one to use as an implicit argument. Now that I think about it further, however, the symptoms of this problem might be closer to Haskell's "The type variable m is ambiguous" errors than to the overlapping instances issues I was worried about. Hmm, now I'm starting to think that implicits might not be such a bad idea after all! I should try Scala and see for myself.
So having read the FAQ, it seems to say that all projects currently on the FPHC will become published and indexable. Maybe it's just me, but I found it really convenient to be able to have private repositories, and despite the tiny amount of Haskell development I do, I found it worth paying $10 a month to have be able to use the IDE for occasional projects that I might not want to share. So I hope you consider brining this back. All in all though, I'm super grateful for everything FPComplete is doing to advance Haskell, and I don't want to sound ungrateful for this release. Thanks
It's like trying to learn driving by randomly moving handles. Funny but somebody will die. What's the semantics? Is it the same to traverse a list and traverse an Either? Does that semantics depend on applicative used? Are traversing list with IO and Maybe do the same? Am I supposed to run all possible combinations and edge cases before using it in production code?
Thank you for writing this. I feel this is as much about a philosophy for (and approach to) life as it is about creating and sustaining a good Haskell community.
It's obviously dependent on the workload. RocksDB's original constraints include things like databases 5x the amount of RAM, where LevelDB plummeted. Baseless speculation: with a DB that large, when you have to issue lots of *reads* against it, the kernel is going to be very prone to evicting pages as necessary to reduce pressure on the memory subsystem. It can evict read-only pages with impunity from the page cache to reduce pressure as it sees fit (while dirty written pages must stick around in the page cache, until they're written back to disk), and every time this happens, you're introducing page faults and multiple context swaps per page you touch, providing it was evicted. I'd speculate in heavy read-only workloads, the cost of paging in these pieces of data can get high and turn it into your bottleneck. `mmap` also doesn't do any kind of read-ahead on pages to mitigate this scenario, where something like a traditional `read` on a file will typically page in a lot of data (and `mmap` shouldn't do this anyway, because many times people specifically want `mmap` for random access on the resulting pages; it's possible notifying the subsystem using `madvise` isn't an option here). Finally `mmap` performance is also quite dependent on the underlying storage device and driver to deal with paging (especially in the case you're doing random access). Not all hardware (or drivers!) are created equal, after all. Anyway, that's my baseless assumption.
I for one have given up on implicits as something that seem superficially like a good idea, but which rule out too many good things. You gain the ability to have multiple instances in scope, but you lose the only power we have to know two value level things are the same: coherence of instance resolution. Losing _all_ ability to reason about value level equality except through some vague notion of a contract with a user that they wouldn't dare pass you the wrong instance is a very very big thing. In Haskell pretty much all of our code is built around the notion of making fairly dumb data and moving the instances to the use sites. data Compose f g a = Compose { getCompose :: f (g a) } is Functor, Foldable, Traversable precisely when f and g are. You get this automatic structural subtyping of features based on the features of your inputs. In a world where anyone can hand you any implicit this becomes more difficult. You need to not only reason about the types you have, but you need to reason about what happens when someone uses a _different instance_ and comes back and manipulates your structure. What invariants are lost? Well, consider `Set`. We need the fact that the instance that was used to build it is the one you use to read from it. `Set` doesn't hold the `Ord`. So you say, well, maybe we'll capture the `Ord` constraint inside `Set`, then we can know that `insert`, etc. will have to respect it. That'll teach it. Now given two sets how do I work with the pair? In scala we can double down and say, "Well, I'll require equality between Ord instances! Apparently every type in mathematics comes equipped with a decidable notion of equality and a convenient hashCode, so we'll use that." But now what happens when you need to work with Mu and have recursive cases? There are cases where the decidability of value equality will not-terminate, nothing prevents them from being infinitely complicated and recursive. Once you can distinguish between two instances with different provenance it now becomes important to have vocabulary in your language for distinguishing between these different provenances. Constructions that were once canonical cease to be. You often no longer can rely on the fact that you can build a structure in one place and use it in another unrelated place and that you'll be manipulated by your own API. You can't hide pieces of information and expose it to your own plumbing. Invariants become much harder to maintain. Most damning to me is that the beautiful space of stark abstractions we have in Haskell where we have these very simple data structures that we elaborate with common instances breaks up very quickly. You get very much first order code for working with particular things that carry around all the instances they need to work defensively. I do not want to live in that world or write in that language. Ultimately, _why do we want these_? The ability to construct a dictionary out of whole cloth and control the plumbing is a powerful tool. There is a good usecase. I want to make an instance that depends on values I happen to have in scope. e.g. make a monoid based on tabulating the state space of where an automata goes given a given input. You may want the monoid to be tied to the particular value of a graph that you started with. But we have that. This is what the `reflection` package is used to do! We have it today, and we don't have to give up our ability to reason about coherence to get it. 
&gt; a completely deterministic bot like tit-for-tat will be obliterated. I don't see this. If your opponent (tit-for-tat) deterministically chooses to defect (state A), you can do no better than defecting yourself. If you cooperate, you lose 5 points, but transition to the following state. If you defect, you gain nothing and are in the same situation next round. If your opponent (tit-for-tat) deterministically chooses to cooperate (state B), you can do no better than defecting yourself. If you cooperate, you net gain nothing, and stay in this case. If you defect, you gain 5 points, but find yourself in the previous situation. You start in this case. Playing against tit-for-tat lets you get a 5 point advantage over tit-for-tat, but if you want to go back to cooperating (for more total points for both of you), you must sacrifice 5 points.
But can they submit their work without a commit? Is it possible to export the working tree to the student's computer? Obviously, there is always "copy every file manually", which may be good enough for the simpler assignments...
I just highlighted the fact that there is a lot of extremely similar tutorials. Authors implement some space-high abstraction without leaking any light on what that abstraction mean or what that particular implementation do in terms of the datatype. I'm trying hard to see any point in them, but fail, again.
How?
We have an export functionality (under the publish menu currently) which exports to the format used by [project-template](http://hackage.haskell.org/package/project-template), which would probably work very well for this purpose. Perhaps a fun intermediate exercise in the course would be to write a program that takes that export file format and converts it into files ;).
You can't really know that in GHC either. [[source](http://blog.ezyang.com/2014/07/type-classes-confluence-coherence-global-uniqueness/)] I find I like the Scala approach better (their type class equivalents are first-class without introducing a Constraint kind, e.g.), but I would like to have fast TreeMap / HashMap implementations -- perhaps orphan instances should be upgraded to an error and only allowed with an appropriate pragma.
IIRC, sometimes Scala doesn't warn about the ambiguity. ISTR, that if you have two suitable value for your implicit argument in scope, it will use the one with tigher scope -- it only errors if there are two suitable values with the same scope. Still, it's interesting to play with. Agda and Idris also have implicit parameters, although they generally limit it to types that can be inferred and will eventually be erased. Scala is rather more free with it's implicits -- if you do too much with the collections library you'll find them hiding behind all the flatMap calls.
If you have implicit arguments, partial function application can be bothersome.
I've been involved with many different open source and tech communities over the years. Forums, IRC, mailing lists, etc and have experienced some pretty terrible treatment of newcomers. I've almost never experienced this with Haskell. In fact, this style of thinking that Gershom is talking about was a big factor in drawing me in to Haskell. The Haskell community does a great job of basically being a big collection of people that like to think and teach in the way the article explains. I love going to HacPhi every year because the awesomeness of the community comes through even more in person. Thanks to everyone that's helped me learn over the years!
Did i detect some Bob Harper in there?
In this version of the game your bot can run the code of your opponent but cannot inspect it. I wonder if it's possible to pass the AST (algebraic syntax tree) of you opponent instead of its compiled function, so that you would be able to analyse it. I think also that the language in which you describe your bot should be a subset of Haskell (a small DSL such as a GADT): this way it would be easier to analyse it automatically. How would that look like? I'm sure it has been studied before, any pointers?
Once is enough because it is all you can give.
&gt; If they tell you they don’t need types to catch errors, tell them that they must be much smarter than you, because you sure do. Be careful: this might be what you mean, but a lot of attempts to say this end up sounding like "my software works and yours doesn't, because I do it right and you do it wrong". Too much Haskell (and more generally FP) advocacy seems to be based on the idea that it's just *impossibly hard* to write correct software any other way. Which leaves people who are pretty sure that they write correct software for a living feeling patronized or confused (or both).
Oh I found a nice implementation http://lpaste.net/90890 linked in a completely demoralizing discussion on the libraries list http://www.haskell.org/pipermail/libraries/2013-July/020378.html 
Is this really an isomorphism? If so it is very surprising that `forall r. (a -&gt; m r) -&gt; m r` is not isomorphic to `m a` (as is under discussion in another thread).
Do you agree with this argument for why `Codensity m` is not isomorphic to `m`: Consider `Condensity m ()` which is isomorphic to `forall r. (m r -&gt; m r)`. This is clearly not isomorphic to `m ()` because it contains `\x -&gt; x &gt;&gt; x`, for example. This doesn't correspond to any element of `m ()`.
Yes I've been looking at that library since you mentioned it in another discussion. There's a nice idea in there.
Here's a hypothesis that I haven't looked into yet: * Every free monad is an `MFunctor` * Every quotient of an `MFunctor` is an `MFunctor` This would give us a large amount of `MFunctor`s for free.
yah. I think that is pretty good and gives a good intuition.
Ah, yes, I had seen that before actually. Very nice article! It would be great to do that stuff in lldb. Might have to give that a go next time :) (Although it wouldn't have helped much in this particular case..) 
https://github.com/alphaHeavy/ghc-lldb/blob/master/ghc.py#L453 Note: was hacked together on an early (undocumented) lldb. Probably doesn't work anymore but ymmv.
Free/Operational Monad? Short answer because I'm not experienced enough to build it in a reddit comment. IIRC, both are ways to basically turn control-flow and execution into data structures and evaluation.
The intention isn't to allow tasks to create other tasks. It's really to compose tasks together and keep track of which tasks will run when the fully composed task is executed. An attempt at an example: firstTask :: Task String firstTask = createTask "first" (\ctx -&gt; createTempFile) secondTask :: String -&gt; Task [String] secondTask filePath = createTask "second" (\ctx -&gt; readLines filePath) myFinalTask :: Task () myFinalTask = do tempFilePath &lt;- firstTask lines &lt;- secondTask tempFilePath return () getListOfStepsInTask :: Task a -&gt; [String] getListOfStepsInTask = undefined main :: IO () main = print $ getListOfStepsInTask myFinalTask -- should print ["first", "second"] main2 :: IO () main2 = runTask myFinalTask As far as restricting the tasks.. I'd rather not if at all possible. 
Sweet, from a quick glance over it you got all the relevant RTS objects handled for stack and heap inspection! I guess GHC's RTS and such changed a bit since then, but like with my code, that shouldn't be too much work to fixup.
One reason why I didn't implement the actual stack capture on top of a debugger was performance, I had the profiling use case in mind as well. For a pure crash analyzer / debugger it would probably make life easier to just script lldb, yes ;-)
With a monad, you generally could make a task whose name depends on the result of another task, our even run or not run depending on the results of another task. With applicatives, you can get static information about the tasks that will be run, but can't use the information from a task in another task at all. I may be mistaken, maybe there's some creative way to make a monad or applicative that will work. But I think maybe arrows will work. I don't understand them too well, but just thinking about the syntax sugar for them, you could have name and fact of each task static, while other data incoming into the task based off results of tasks.
yeah, basically if you get a weird bug and i see ANY mention of this operation, i assume the bug is in the author not the compiler. [this committ](https://github.com/haskell/bytestring/commit/71c4b438c675aa360c79d79acc9a491e7bbc26e7) [and this commit](https://github.com/haskell/bytestring/commit/210c656390ae617d9ee3b8bcff5c88dd17cef8da} [and this ticket](https://ghc.haskell.org/trac/ghc/ticket/3486) [and this one](https://ghc.haskell.org/trac/ghc/ticket/3487) [and this one!](https://ghc.haskell.org/trac/ghc/ticket/7270) (links from the source code for the lazy)
Is the Church encoding of a type generally 'isomorphic' to it? These are not completely orthodox Church encodings. It is easiest to write an orthodox Church encoding for data ListT m a = Nil | List a (ListT m a) | Monadic (m (ListT m a)) which would be: type CListT m a = forall x . x -&gt; (a -&gt; x -&gt; x) -&gt; (m x -&gt; x) -&gt; x then we have mkListT clist = clist Nil List Monadic for example, no?
This one is kinda of tricky. What we're trying to convey when we say something like that is the fact that we use types as a tool to help us, not as a bondage discipline to which we subject ourselves. It can be difficult to get that point across without sounding preachy. To unpack that slighly snarky statement a bit, let's take lens. I really could not have written lens in a language without types. Why? I make use of types in three major ways. First, I would have no idea what I was plumbing where. Everything is at a level of abstraction not really supportable in a world where you have to rely on everyone perfectly using the combinators in just the right way. Now, sometimes these types are an encumbrance to someone like Tarjan who can come up with crazy deque structures in lisp that we still have trouble duplicating in the typed world, so there is an exception that proves the rule. On the other hand there are times when the types I have available to me in Haskell are just not strong enough to say what I want to say, and to let me collapse the space of things I know are the same. But ultimately, I spend most of my day in dialogue with my compiler. I write half of a line of code, and ask what the types are around the hole I'm working at and I turn the amount of stuff I have to hold in my head into a very small sub-problem. I have no effects to concern myself with, no global state, just a handful of types I need to get to line up and whatever I put in that fits that goal works. Programming becomes like solving sudoku puzzles. I much prefer this process to one where I can write the code down, but then I have to run it and treat it like a black box that I can only really probe pointwise for correctness or try to reason about at a meta level. The REPL is then used as a tool to try to cut the round trip time on that probing process to a manageable level. I can do this in ML dialects just as well, I just give up some of the infinite cases I automatically extend to in Haskell. Second, which is really just a refinement of the first, parametricity collapses the state space of possible functions I need to consider down in ridiculous ways. (a -&gt; a) collapses from being every possible function that does anything ever to 'the identity function or something that doesn't terminate' and since "Fast and Loose Reasoning is Morally Correct" we can even discount considering the second one when we want to know what the code does. Third, in Haskell we have typeclasses. Typeclasses write code for us. They secretly plumb constraints right where they need to go at just the right time. The extra constraint language we have can do a lot of heavy lifting for us invisibly. The type system goes from being a tool for just checking correctness to being an infallible anonymous co-author. The beauty of this approach comes not in the first time you write down the code, but when you go to refactor. If you have to explicitly plumb the dictionaries around the you have to explicitly plumb them around the second time and recapitulate all the boilerplate that entails. If you can talk about things you have to talk about things, which means juggling more names. These things keep me programming in a language with types (and typeclasses). The power of these abstractions is very hard to convey to someone who has never had the privilege of learning them. They really do reduce the amount of stuff I have to think about to a manageable level, even when the invariants become very complicated. When I'm forced to work without types, I'm forced into writing very direct code. Now, there is definitely a beauty to that simplicity as well. The code does exactly what you want, and does so usually in the only obvious way to do it. You can write that kind of code in Haskell too, though, and as one who likes to explore design spaces, it is occasionally useful to be able to write the code that isn't obvious and still retain the ability to reason about it and most importantly, the ability to come back and fiddle with and fix it 6 months later when you've paged out how everything works. But ultimately to come back to your point. My approach is a bit different than Gershom's. I try to go out into the exotic areas of math and computer science and find interesting baubles to bring back to dazzle and amaze and hopefully show that this stuff is useful. Unfortunately this tends to reinforce the stereotype that you have to be really smart to be good at functional programming, because people tend to conflate knowledge of esosterica with intelligence. My way gets across the utility message, but loses the "anyone can do it" populism that I think is very important as well. There is a real balancing act here. You want to get across the idea that types are a powerful tool that let you focus on other things, but you need to strike a balance between sufficient detail to get their attention and too much detail, to where they overload and either shut out the idea or fall back on thinking "Wow, this person must be really smart. You have to be smart to use Haskell." Thus there are two horns to the dilemma of how you present this topic and I'm not sure there is a fully ethically consistent way to present the idea due to these meta-level concerns.
I'm very new to all this- what does this mean? Thanks, Louis
I would emphasise your second point; I have had discussions with colleagues where we have gone in to a friendly discussion disagreeing on a technical topic, with both of us expecting to convince the other person that they're wrong and I'm right. As we put forth evidence for our own positions, it becomes clear that actually, we're both wrong - we've each fixated on one part of the problem, solved it, and not spotted the other part that invalidates our solution. The friendly discussion means that we can spot this together, and come up with a third solution that covers both the edge cases.
Especially if you have a tendency to condescend towards women...
To expand on this, you'll need to define your own (free) monad in which task actions run, which may be superficially similar to IO. Unfortunately this means you'll have to define up-front exactly which IO actions tasks can perform, so that you can interpret each action as data. Otherwise, there's no way to perform IO without... performing IO. Here's one simple version: type TaskContext = String data TaskF next = Log String next | GetContext (TaskContext -&gt; next) -- 'IO' actions | ReadFile FilePath (String -&gt; next) | LaunchMissiles next deriving Functor type Task = Free TaskF log :: String -&gt; Task () log msg = liftF (Log msg ()) getContext :: Task TaskContext getContext = liftF (GetContext id) readFile :: FilePath -&gt; Task String readFile path = liftF (ReadFile path id) launchMissiles :: Task () launchMissiles = liftF (LaunchMissiles ()) -- Interpret task in IO. runTaskIO :: TaskContext -&gt; Task a -&gt; IO a runTaskIO _ (Pure a) = return a runTaskIO ctx (Free (Log str next)) = putStrLn str &gt;&gt; runTaskIO ctx next runTaskIO ctx (Free (GetContext f)) = runTaskIO ctx (f ctx) runTaskIO ctx (Free (ReadFile path f)) = Prelude.readFile path &gt;&gt;= runTaskIO ctx . f runTaskIO ctx (Free (LaunchMissiles next)) = putStrLn "missiles launched" &gt;&gt; runTaskIO ctx next -- Collect log messages. logTask :: TaskContext -&gt; String -&gt; Task a -&gt; [String] logTask _ _ (Pure a) = [] logTask ctx str (Free (Log msg next)) = msg : logTask ctx str next logTask ctx str (Free (GetContext f)) = logTask ctx str (f ctx) logTask ctx str (Free (ReadFile path f)) = logTask ctx str (f str) logTask ctx str (Free (LaunchMissiles next)) = logTask ctx str next Apologies if this doesn't typecheck
Replying to &gt; First, I am running some experiments with typed finally-tagless where I have `rep :: Nat -&gt; Maybe Nat -&gt; *` and it works nicely. This blog post is about the (stripped-down) non-typed case, that's why the parentheses. &gt; Second, So far, yes, I have no example for the non-default constraint. Thinking about it, what you propose is PHOAS *already*, because `rep` is an unknown type, so `lam`'s argument function cannot pattern match on (inhabitants of) it. Also, `var` is utterly useless in your simplification, because the identity can simply be inlined. Thanks for the review, I'll let this ripen a bit and maybe come back with a follow-on post how it worked out. 
I usually mention types as an antidote for my own idiocy and short-sightedness when it comes to programming. Being able to start at a concrete implementation and generalize it by essentially changing a type signature is neat, too.
What do you mean?
Even to the argument "types allow me to be stupid", there are counterpoints and caveats that exist outside of the average haskeller's worldview, but not outside of the audience's worldview, so the audience still rejects the argument (and is correct to do so).
So can anyone give an explanation of accursedUnutterablePerformIO that doesn't involve Zalgo? I see that accursedUnutterablePerformIO is: {-# INLINE accursedUnutterablePerformIO #-} accursedUnutterablePerformIO :: IO a -&gt; a #if defined(__GLASGOW_HASKELL__) accursedUnutterablePerformIO (IO m) = case m realWorld# of (# _, r #) -&gt; r #else accursedUnutterablePerformIO = unsafePerformIO #endif and unsafeDupablePerformIO is: {-# NOINLINE unsafeDupablePerformIO #-} unsafeDupablePerformIO :: IO a -&gt; a unsafeDupablePerformIO (IO m) = lazy (case m realWorld# of (# _, r #) -&gt; r) The comment for unsafeDupablePerformIO explains a little bit but I want to understand better: &gt; &gt; -- Why do we NOINLINE unsafeDupablePerformIO? See the comment with &gt; -- GHC.ST.runST. Essentially the issue is that the IO computation &gt; -- inside unsafePerformIO must be atomic: it must either all run, or &gt; -- not at all. If we let the compiler see the application of the IO &gt; -- to realWorld#, it might float out part of the IO. &gt; &gt; -- Why is there a call to 'lazy' in unsafeDupablePerformIO? &gt; -- If we don't have it, the demand analyser discovers the following strictness &gt; -- for unsafeDupablePerformIO: C(U(AV)) &gt; -- But then consider &gt; -- unsafeDupablePerformIO (\s -&gt; let r = f x in &gt; -- case writeIORef v r s of (# s1, _ #) -&gt; &gt; -- (# s1, r #) &gt; -- The strictness analyser will find that the binding for r is strict, &gt; -- (becuase of uPIO's strictness sig), and so it'll evaluate it before &gt; -- doing the writeIORef. This actually makes tests/lib/should_run/memo002 &gt; -- get a deadlock! &gt; -- &gt; -- Solution: don't expose the strictness of unsafeDupablePerformIO, &gt; -- by hiding it with 'lazy' That really doesn't seem that difficult to program around. accursedUnutterablePerformIO must be used with IO that is referentially transparent and doesn't need to be atomic. I don't understand the strictness part though. 
&gt; Now given two sets how do I work with the pair? Do you think the language should have no way to construct two sets that are type-compatible but use different comparison functions internally? If you allow such sets, the problem happens in any case, no matter if the language uses typeclasses or implicits. Or you could disallow such sets altogether, but I feel that's a little extreme.
Thank you for taking the time to write this. Haskell really does have a good community!
Having committed almost all of the sins enumerated in the article, I agree that this is a) great advice for *any* programming community, and 2) something I'll need to go back and read every couple of months. Thanks for the post. :)
RIP darcshub. You will be missed.
Shouldn't that last line exclude the "in contrast" part?
I am happy that I don't see the same level of "adversarial pedantry" in the Haskell community that I see in some other circles. Nothing is more boring than reading a thread that spirals off on some pedantic tangent that only serves to allow one person to prove they are smart or right about some tiny nitpick they've found in a somone else's post. It is not fun to need to spend 15 minutes making your casual programming post be "disertation level correct" so that people don't jump on the nitpick bandwagon over some tiny aspect of what you said. Sometimes I end up writing waaaay more than I want to, just to try avoid people nitpicking.
Very nice. I urge you to write a shorter version. That will expand the article's readership.
hub.darcs.net FTW!
&gt; The rest of the code is not provided because I get a syntactic error in the last line I'm new to Haskell, but AFAIK `if` **requires** an `else`. You do not have that above, therefore I do not know if that is causing the error (because it's not there) or if the `else` is there, but you hadn't included it; there error somewhere else. 
You probably want to use `when` from Control.Monad instead of `if`
That's interesting! I wrote a bunch of stuff last month on this very theme. I wasn't ready to open source it yet, but I will if there is enough interest!
To just answer your question regarding syntax, your problem is twofold. First, in Haskell, the `if` construct is an expression as opposed to a statement. As a result you must include an `else` expression, for a value must be returned. For example, in the following, what would `bar` equal if I were to omit the `else` clause? foo x = if x &lt; 5 then x * x bar = foo 10 Second, you are using "do notation" (ie the `bind` operator: (&lt;-)) outside of a "do block". With this in mind, the following should work... yascc :: [String] -&gt; IO () yascc (x:xs) = do isDir &lt;- doesDirectoryExist x if isDir then do content &lt;- getDirectoryContent x -- etc... else undefined 
No problem, glad I could help!
This is because it haskell "if/then/else" isn't a statement. Instead it is a well-typed expression. Expressions always have a value -- what they e*valu*ate to. So for when the condition is false, you'll need a sub-expression for the else part. If you are used to C and haven't suffered under PHP, think of it as a ?: construct instead of an if(){}else{} construct.
Yup. When I first heard this line of argument, it came across as "humble-bragging". I realize now that its meant literally - imperative programming in (for example) dynamically typed languages requires keeping too many things in your head at once - and I'm just not smart enough to do that. But that doesn't change the fact that the argument "i'm not smart enough to program without types" doesn't translate well to the audience its aimed at. 
Some versions in D language: http://leonardo-m.livejournal.com/111831.html
Well observed. :-)
This solved it. I thought that below the first "do" everything could be done in do notation, even inside "if then else" and didn't remember to try what you said above. Thank you.
&gt; The spec is a free-form HTML document. It's only one of many real-world examples that are just like that. It's not entirely free-form though. There are patterns and conventions in there, and many of them could be formalized, or maybe they already are. It's a technical spec, after all - it *has* to use exact language, and the step from this kind of exact language to a fully formalized spec is rather small, if any. &gt; You are imagining not just something being added to haskell I was actually beyond arguing the Haskell case here. Haskell is not perfect, and in its current form *does* require free-form documentation. Just not as much as most other languages do. Put simply, this: foo :: (Monad m, MonadDB m) =&gt; UserID -&gt; m Duration ...tells me a lot more about the `foo` function than this: def foo(user_id): &gt; I have another theory, by the way. Haskell culture is CS culture, academic culture. Academics tend to create research projects that are not intended to be used or maintained long-term. I think your argument is a bit short-sighted there. First; Haskell culture is heavily rooted in academia, but the community has expanded beyond that and matured. Take me, for example: self-taught programmer, majored in jazz music, no formal CS education whatsoever. I use Haskell because it allows me to get shit done. Second; the Haskell ecosystem has produced a few impressive things that *are* being used on a daily basis, such as Pandoc and XMonad, and numerous big players in finance, insurance, and similar industries are using Haskell for some of their mission-critical systems. Those are not "research projects that are not intended to be used", those are real-world, get-your-hands-dirty, living codebases, and the people who maintain them generally tend to praise Haskell code for being very maintainable, not in the last place because the static type checker has their back. (From my own experience, I can tell you that refactoring Haskell code is almost a no-brainer.) Third; the fine folks at Bell labs, I would argue, were a rare blend of CS brainiacs and tinkerers, and what they were doing could easily have gotten them fired in Bell's engineering culture. Part of the innovation was in how they *departed* from the stiff engineering culture of the time, and just went ahead and changed things as needed - the documentation culture they developed along the way was born out of necessity just as much as the engineering culture they departed from. &gt; I would also say (I hope you don't take offense) that your posts in this thread exemplify just what I was talking about. No offense taken. I believe that apparently I wasn't being clear. Let's try again: - Haskell is not a perfect language; writing good Haskell means writing good amounts of documentation. - However, Haskell is better at self-documenting than many other languages, and what you'd have to write in free-form comments in some languages can be encoded in Haskell's type system. - And then again, extracting useful information from type signatures requires more than superficial knowledge of Haskell, so in order to acknowledge the full expressiveness of it, one has to first tackle a somewhat challenging learning curve. But explaining the programming language itself in the comments, or the conventions and idioms that commonly go with it, is frowned upon in pretty much every language, and Haskell is no exception. - Finally, the better Haskell libraries *[all](http://hackage.haskell.org/package/text-1.1.1.2/docs/Data-Text.html#t:Text) [come](http://hackage.haskell.org/package/scotty-0.4.6) [with](http://ekmett.github.io/lens/) [excellent](http://hackage.haskell.org/package/base-4.7.0.1/docs/Prelude.html) [documentation](http://hackage.haskell.org/package/parsec-3.1.5)*. There are lots of packages that are lacking in documentation, but most of them are also lacking in other regards, so it's actually not that different a situation from other languages. Just takes some time to find the good libraries and learn which ones to avoid. So far the Haskell part of the argument; the theoretical part, where I use a Haskell-like language for illustration purposes, is not about Haskell at all. It is about the theoretical possibility of making documentation so much part of the syntax that it is possible for a compiler to verify almost all of it; when that point is reached, the documentation metalanguage and the language itself become one. First steps in that direction exist - attributes in C#, decorators in Python (or at least some use cases), formal documentation comment syntax in javadoc/phpdoc/doxygen etc., and I've seen people injecting special function calls into call chains in ruby for the purpose of integrating documentation with the language itself.
Don't know if I should be using the same topic but, I have another problem similar to before(I think). parse_files :: [FilePath] -&gt; [FilePath] parse_files [] = [] parse_files (x:xs) = do isDir &lt;- doesDirectoryExist x if isDir then do content &lt;- getDirectoryContents x parse_files (content ++ xs) else x : parse_files xs and I have the following error messages: yascc.hs:39:12: Couldn't match type `IO' with `[]' Expected type: [Bool] Actual type: IO Bool In the return type of a call of `doesDirectoryExist' In a stmt of a 'do' block: isDir &lt;- doesDirectoryExist x In the expression: do { isDir &lt;- doesDirectoryExist x; if isDir then do { content &lt;- getDirectoryContents x; .... } else x : parse_files xs } yascc.hs:41:16: Couldn't match type `IO' with `[]' Expected type: [[FilePath]] Actual type: IO [FilePath] In the return type of a call of `getDirectoryContents' In a stmt of a 'do' block: content &lt;- getDirectoryContents x In the expression: do { content &lt;- getDirectoryContents x; parse_files (content ++ xs) } Why in this case it's complaining about the return type of "doesDirectoryExist" ?
What if you were to define your final Task monad as a transform of `WriterT` applied on top of your base Task monad? Then for each composed task you could write some data (like a tuple) describing the task, and later retrieve the collected results by running the task's `Writer` context?
Thank you, it worked. Though I'm not new to Haskell, I am, to working with monads, so lots of "stupid" questions. Again thanks for the help.
No, no. I mean coding it in Ada instead, using the SPARK subset. You can then specify contracts that are statically enforced, though the tools need manual help to provide some proofs. I know there are probably solutions to do this in C++, but I have no knowledge of them. For C there is Frama-C.
In my experience, many of the things Haskell is good at tends to be things that imperative languages are bad at (which is fair, considering it's an entirely different paradigm). So I think that "in contrast" works, since the things Haskell tends to excel at are often things that more common languages tend to be poor at.
&gt; the Haskell ecosystem has produced a few impressive things that are being used on a daily basis, such as Pandoc and XMonad, and numerous big players in finance, insurance, and similar industries are using Haskell for some of their mission-critical systems. Yeah, I wasn't saying haskell isn't practically useful. I was saying that there is a certain culture, a certain structure of meme propagation, which derives from academic CS rather than engineering. Unix on the other hand has a culture that derives from engineering. To give an example of a relevant meme, most Unix documentation contains some or all of the following sections: NAME SYNOPSIS DESCRIPTION RETURN VALUE ERRORS CONFORMING TO BUGS SEE ALSO AUTHOR (Notably, almost none of those could possibly be generated from a type system.) Those section headings may have originated with Unix, or they may even go back earlier. But the people who use them today are not just the people who work at Bell Labs. The meme has propagated to most of the Unix free software world. But it has *not* propagated to the Haskell world! So Haskell's documentation culture must come from somewhere else. I propose that it comes from CS. That does not mean that each individual Haskell programmer is a CS graduate student or anything like that. It is just to say where the memes come from.
Somebody produced a condensed version here: https://gist.github.com/jessitron/7d951b31d61bcd1de07a
That's very nice, thanks. However, I find myself constantly flipping between the code and the template matching names. I wonder whether it would be possible to show the splices on a hoover pop up when looking at the template? What I'm missing is some nice visualization of the relationship between the code and the template. Also, what's the "reading order"? Clearly, it's not code first, then templates. Nor is it templates first, then the code. The presentation should, IMO, reflect the logical structure of the application. Maybe a third tab with a logical view which has the code / template mixed or some kind of graphical view would be helpful. 
Can we start getting some new quality frameworks/libraries instead of constant re-chewed tutorials and promotions for, to be completely honest, quite terrible (compared to frameworks and libraries for other languages) software in this forum?
Why is this code here? Why not use unsafePerformIO?
I think you might regret splitting code into multiple repos (makes refactoring work harder to do). It is **much** easier to split git repos than put them back together, if you decide in the future it was a mistake to split them.
As someone who is struggling to learn Snap (and I am trying to use it in one project), I would appreciate some nice explanations of the basic concepts, minimal and isolated from each other. For example, just a static template, just having a few constant splices of different types and showing them in a template. In general, examples are most useful when they explain some abstract rule. What are the rules these examples explain? Just phrase it in one sentence for people to remember. Like, "Templates can be composed of other templates using the `apply` tag", then show the example of how to do it. Also, adding "negative" examples, which show common errors, is a good approach.
Hi, I'm the community manager of ZeroVM. Good question. Background info on NaCl (which is what ZeroVM relies on) briefly mentions GHC https://developer.chrome.com/native-client Some issues ppl have raised re porting GHC to NaCl: 2013: https://ghc.haskell.org/trac/ghc/ticket/8206 2012: http://www.reddit.com/r/haskell/comments/ppdgq/gsoc_2012_project_proposal_ghci_in_the_web_browser/ Our team are happy to help you out with more details. All of the core engineers hang out on #zerovm (freenode) and http://groups.google.com/forum/#!forum/zerovm so either is a good place. You're also welcome to ping me at carina.zona@rackspace.com
git subtree to the rescue. Lovely little plugin for git that makes it very easy to both stitch repos together and split them apart.
What's a better framework in some other language? I am a committer to Snap, but don't mistake cause and effect. I'm a committer to Snap because every framework I've used in every other language is total garbage, and Snap at least does less wrong than them.
You can actually turn off the issue trackers (and the wikis, for that matter). It's in the repo settings under "features". You can just mention where to find the actual issue tracker in the READMEs of the other projects.
From snap home page: &gt; snap is &gt; A fast HTTP server library &gt; A sensible and clean monad for web programming &gt; An HTML-based templating system for generating pages Claim one is untrue, not even fast relative to other haskell frameworks like yesod, not to mention other languages. Claim 2 is subjective, wouldn't say it is any better than yesod for example. Claim 3 is also fairly irrelevant because of frontend frameworks like AngularJS and ReactJS. Writing clean, safe REST API is quite painful with snap and this is more important than templates. Better frameworks: Yesod better overall for haskell. Akka + spay for scala. ActionHero, hapi, koajs for nodejs. The fact is Haskell is (currently) not a good platform for a web application. Modern web applications today are typically fairly thin 'proxies' between the data and heavy logic - Haskell is not ideal for this due to the ghastly interfaces to persistence available, purity, performance. What happens on a GET request for some REST API endpoint in a haskell web application? Haskell cheats on purity for read -&gt; cheats on types for json to haskell -&gt; cheats again on types for haskell to persistence -&gt; cheats again on purity for read from persistence -&gt; does the same thing again backwards. Point? You end up ignoring most of what makes Haskell awesome anyway.
Go for it. =) I'm far less worried about stealing my ideas than I am that they might not be listening at all. ;) Translate and downshift away. I'll happily answer questions as well.
`unsafePerformIO` incurs the overhead of a lock and a function call. `inlinePerformIO` removes both, at the expense of a few safety guarantees. I'm guessing as a core library, `bytestring` would need something like this for performance.
In Haskell you can build a type yourself that carries its dictionaries. It might suck but it'll be in your own little universe. I've build things like that before when I needed them. You can borrow all the work from the existing set by making a type that takes its dictionary from context, just like an implicit! newtype My a s = My { runMy :: a } instance Reifies s (a -&gt; a -&gt; Ordering) =&gt; Eq (My a s) where a == b = on compare runMy a b == EQ instance Reifies s (a -&gt; a -&gt; Ordering) =&gt; Ord (My a s) where a == b = reflect a (runMy a) (runMy b) Now pick. That can be distinguished so you can see the 's' and be safe. We've made up a dictionary from context, but we can still see it in the type. reify :: a -&gt; (forall s. Reifies s a =&gt; Proxy s -&gt; r) -&gt; r This function guards the selection by a quantifier that keeps you from using `MySet` in bad ways. `MySet` will internally make use of the dictionary we defined above and tied to `My a s`. We can continue to work in the open like we've done, or you can head off to the wild west : data MySet a where MySet :: Reifies s (a -&gt; a -&gt; Ordering) =&gt; Set (My a s) -&gt; MySet a but even so you can steal all the heavy lifting from Daan Leijen's classic Data.Set. When you need to crib an ordering from the environment just make one. reifySet :: forall a. (a -&gt; a -&gt; Ordering) -&gt; MySet a reifySet f = reify f (\(Proxy :: s) -&gt; MySet (empty :: My a s)) Now you've got a little universe of sets made by whatever ordering you pulled out of nowhere, and the rest of us don't have to pay for this feature. You have 3 points in the design space available to you, in a way that we haven't ruled out anyone's preferences. You've lost access to the `union`/`intersection` operators from Data.Set. Why? because when you open up `MySet` you have no idea which choice of `Ordering` you have, which is a factually correct statement. Given two of these sets you can take all the elements of one and insert it into the other respecting the chosen ordering. If you unilaterally impose implicits on everyone in your world you only have one point and you are constantly wasting your time reasoning about your code in terms of social contracts and other made up things. Or you can pretend that implicits are typeclasses and do the Haskell-style code transform. We do that in scalaz. Many of the worst bugs in scala that i've had have come from someone sneakily introducing local instances to be clever or work around scala bugs or to work around the limitations of implicit dispatch. Here we've written a handful of code, and given you the thing you asked for without giving up the things we have that make Haskell a place where we can reason about code. You are welcome to embrace Scala for its many features for interoperating with the JVM. It is a lovely syntax for Java, but I'm hard pressed to love implicits over typeclasses. I literally have no usecase where they can do something I can't say with typeclasses, but I have plenty of things I can't say in the other direction. Now there are plenty of things that are _harder_ to say with typeclasses than implicits, but in the other direction things are just flat out self-delusions, not difficulties ;)
I usually phrase specific practices as being a way for me to automate something so that I don't have to think about it quite so hard, allowing me to focus more on other aspects.
Thanks, I have started working on How to prove it. Does the pattern have to be followed like "How to prove it" -&gt; "Software Foundations" -&gt; "TAPL" or "How to prove it" -&gt; "TAPL" ? I'm interested to know, how does Software Foundation help in understanding the contents in TAPL ?
When I looked over the 7.6.3 -&gt; 7.8.3 changes, the RTS etc. seemed pretty stable, so you're probably right. Stack chunks might've been introduced since you wrote your wrapper. Let's see how the DWARF3 support in 7.10 turns out, it was supposed to allow stack walking by native debuggers. In any case, the STG stack is fairly tough to interpret. Compiling with profiling and dumping the CCS is still going to be easier to look at.
Yes, I suppose you can sometimes feel like that (at least, I still do). If "randomly" is another phrase for "I do not know what some particular instance do / how is implemented". It would be nice if there is "short" summary list where one could look up just details (i.e. how is traversal defined for tuple). Function traverse has side effects. So for list, traverse can't *replace* map (yet), but rather mapM. And, yes, those side effects depend on the applicative used. So list, Either and IO behave very differently. It depends on the implementation details of the instance. There is an [SO answer] (http://stackoverflow.com/questions/7460809/can-someone-explain-the-traverse-function-in-haskell/7461194#7461194) that shows interesting effects for list. You're supposed to know and understand the implementation details of each instance (and run them while learning). Haskell code is very condense, and so is the knowledge one must have to read/write/understand it. But, luckily, it very much based on logic and similarity, so there is not that much to learn as it might sound. It might help if you think of traverse (and many, many others) more as a concept, and less as a function. For function you expect to always behave exactly the same (that's what Haskell is all about), but you expect the concept to behave differently depending on the use case. So, you should test all "concept use cases", not all "edge cases". But, again luckily, type system does most of the work for us. I understood from your questions that you're a beginner in Haskell. If you're not, please ignore last two paragraphs. You already know that.
I use GitHub quite a lot too. Hopefully one day the incremental darcs bridge will become so convenient to use you no longer have to make these sorts of choices. Good luck!
&gt; At the end of the round-robin round, the lower-scoring half of the tournament pool will be eliminated. What score? The number of games won? Or the number of points acquired in all games of this round?
This looks similar in purpose to [Yesod DSL](https://github.com/tlaitinen/yesod-dsl). It's a DSL for describing web services typical backed by a database accessed through Persistent, and it all compiles to a Yesod sub site so it's easy to include in other Yesod projects, or use separately. 
I have opened your first issue! I was one of those people that promised some contrib, and I intend to keep that promise. I would have loved to have gotten into darcs, but I have never even managed to get it to build :/
The overhead of grabbing the lock is huge relative to the scale of the bytestring operations. Even `unsafeDupablePerformIO` has too much overhead for `bytestring`. So someone figured out they could strip down even that in horribly unsound ways and get things to run faster so long as the method involved didn't do any allocation, etc.
All my github repo are actually in darcs. But they are not, ahem, quite popular, so it's more like write-only from darcs. I wonder if anyone tried that with the actual two-way work. 
It's a sad day when network effects and social inertia effects decide what tools we use rather than technical merits. I for one, will continue using darcs because I like its unique technical and usability advantages.
Yeah. I continue to happily use darcs for things where popularity is not a requirement. But, when comparing github and hub.darcs.net, it is clear that github is better at just about everything except using my favorite RCS :) 
what about non-portable NaCl first? Go currently supports NaCl, but not PNaCl
http://ro-che.info/ccc/25
At first blush, it seems that static analysis tools could check whether a use of accursedUnutterablePerformIO is safe.
darcs-to-git has been working [for me] for a long time without any trouble. I've seen problems with mark tables with darcs' its own exporter. But without them it seems to works well (incrementally too, but without the mark tables it needs to read the entire history every time). 
Right, so why is vector faster than something like UArray?
So, there is an accepted way, but it's broken, and you and the `loop` author knew that, so it would have saved a lot of confusion and misdirected conversation if you mentioned that issue in your post and if the `loop` haddock mentioned the bug.
The section on Execution isn't correct. As ghci shows, Binary isn't why IO happens. Forcing the wrapped value is why IO action happens. Evaluating main at runtime (or ghci prompt) is what forces the wrapped value, which forces the action. Also, &gt; Something just happens when you see an assignment. is incorrect. Side effects happen at (strictly evaluated!) function call time, regardless of assignment. Laziness is a key factor in IO, which the article leaves unmentioned and mysterious. Besides that, good discussion.
You are hinting at the sentiment behind the original series of "Lambda, the Ultimate X" papers by Guy Steele
IO is just lazy. It's not abstract. Operational/Free Free monads, or type class "m a" is abstract.
&gt; It's like trying to learn driving by **randomly** moving handles. [Emphasis mine] Good type classes come with laws about what you can expect form their behavior independent from the implementation. Good instances follow those laws. Those laws restrict your motion so that you can no longer drive randomly, and instead travel a path that is not only agreed safe, but agreed useful. A little more advanced is writing your own instances; driving randomly is like inventing your own type class or going without them. I understand your hesitation though. When I learn a base class / interface in Java, I'm always a bit suspicious calling methods on objects created outside of my code flow; who knows what crazy subclass they cooked up? Is it the same to get an iterator from a HAMT and get an iterator from an ArrayList? Does it depend on the element type? Are iterators over things containing Strings and BigDecimal the same? Great type classes (or just types) use parametricity to generate (and prove!) the laws for free; if there is an instance it follows the laws, guaranteeing some denotation. `travserse` takes some advantage of parametricity. Since it has to work for *any* `a` and `b`, it can't do anything that depends on the particular choice of `a` or `b`; it can't construct new values or even inspect existing values of either of those types. Since it has to work for *any* Applicative, it can only use the Applicative methods; it will call the given function on all the data of type a and pure on all the data constructors and other data in the traversable in some order, then it will "rebuild" the structure by applying the lifted data constructors to the lifted data. It can permute the data structure and it can permute the "ordering of applicative effects", but those are basically the only choices the author of a particular `traverse` gets.
As you point out, telling people that something they do on daily basis is impossible without Haskell makes it obvious that you're overselling the benefits. The other part of the problem is that nobody ever mentions the cost of using the Haskell type system. After all, you do have to be smart enough to figure out how to encode what you're trying to say using it. As soon as you get past trivial examples, things can get pretty tricky. Once you go through this exercise, you'll have a high guarantee of correctness, but it has to be weighed against the effort required to get there. By contrast, working in a dynamic functional language, like Clojure, is similar to writing a mathematical proof or solving an equation. You generally don't need to keep all the steps of the equation in your head, and often the steps are simply too numerous to do that. However, you can walk through the steps and be confident that each step follows logically from the previous. Thanks to the REPL, I know exactly what the code is doing at each step and I have complete confidence in it. Since the data is immutable, all the states are local and I never have to consider more than a few steps at any given time.
Oh, I hadn't heard about yesod-dsl. Thanks for the pointer. (But from a first glance I can definitely say the approaches are quite different.)
Why not? My experience with VS is that it's a pretty powerful editor that can support a large number of languages, and a lot of people use it already. If you have a professional license, it's also somewhat that you'd want to get your money's worth.
Because I like Visual Studio more than Eclipse.
I love to see random references to it whenever something uncanny/"otherworldly" comes up :)
Maybe it was just me. I tried it intermittently over the course of the last year or so, and it always failed to have the constraints satisfied and I didn't care enough to fix it.
You mean like a type system that tags dangerous actions with tags like `IO` and prevents one from using them in unsafe situations?
&gt; Which leaves people who are pretty sure that they write correct software for a living feeling patronized or confused (or both). All of my experience indicates that people with that mentality are either wrong or aren't writing very complex software. The more software I write the more convinced I am that I am incapable of writing correct software of meaningful complexity. I don't think it's unreasonable to extrapolate this to all humans because the smartest developers I know agree with me. If you accept that we're right here, then your argument goes away. Even if you don't accept that we're right, I think you have to accept that the number of people who can write correct software all the time is very small. And in that case, I would argue that you don't want to write software that requires these infallible people because eventually fallible people are going to be maintaining / modifying it and you're back to having bugs. Haskell is uniquely well positioned here because it gives the safety that the fallible people need while also providing enough power, abstraction, and depth to keep the infallible people stimulated.
Yes, halvm only targets xen. If you are still interested and can be more specific about your problem then I'll try to help.
Good job, you've just made it obvious that you are simply an inexperienced developer.
 &gt; That is absurd nonsense. 90% of the web is content driven and piecing together a document using javascript is worse in every way than being given the document in the first place. In every one of the ways that you mentioned? Which is none? I don't think you understand how templating works if you think the document is not 'pieced together'. The web is the presentation layer for your data, the persistence, transfer, and presentation of data should be decoupled, that is just good design. &gt; Doing more, but doing it wrong is hardly better. Yesod doesn't do more than snap, it does a *lot* less. You have no clue what you're talking about do you? &gt; Have you tried it? It can be hard to mentally separate the horror of using scala from the horror of the libraries, but I'd sooner go back to PHP than try that buggy mess again. Your inability to grasp straightforward concepts isn't really a good arguing point. You clearly have no idea what you're talking about, maybe you should actually learn the things you're talking about before trying to argue about them.
Great! I'm not a VS user myself, but I know a lot of beginners will like this. 
I'm thinking that perhaps I should merge the three most important libraries, `clckwrks`, `clckwrks-plugin-page`, and `clckwrks-cli` into a single repo since pretty much everyone will need those. But all the themes and other plugins will remain separate. On the other hand, `clckwrks-cli` and `clckwrks` are supposed to be relatively stable. So, having to update them all the time shouldn't be an issue. I guess I'll stick with things as they are for now, and if it becomes a problem remerge a few of them. There were definitely too many things in the mega-repo before. 
Yes I think they are. Servant appears to take the approach of making it easy to write very high-level code in Haskell, whereas yesod-dsl takes a more yesod appropriate route of code-generation from an entirely separate DSL. I might have to try writing a toy service in each to compare/contrast.
I've heard some notable Haskellers say that they view any project still in darcs and not in github as dead. Which is obviously not correct -- but what people think is more important than the truth :) I think that being on github lowers the bar for simple contributions. For example, if a build-dependency needs to be bumped, or there is a typo in the documentation. That can become a gateway towards large contributions later. Also, these days many newcomers to Haskell have never even heard about darcs or know anything about its history. When they see some weird RCS, it can make them thing the whole project must be weird. A very easy mistake to make is to think that if people are open minded enough to use Haskell, that they are also going to be open minded about RCS, etc. But, ultimately, technical people have to make lots of hard decisions and choices all the time. At some point you just can't evaluate things anymore. Either your decider is worn out, or you just don't have the time. And so you have to revert to the 'easy' thing and go with the things that are popular and have pretty web pages. There is also the issue that there is only so much room for improvement left. When there was just CVS -- it was clear that CVS was not good enough and lots of people created alternatives to deal with it. Mercurial, TLA, darcs, svn, and many more. Due largely to the popularity of Linus, git became the dominate RCS. These days git is 'good enough' for just about any project and so I think it is very difficult to get people to adopt an alternative -- even if it is better, because it is hard to be 'better enough' to be worth the cost of switching. 
15 minutes seems like a long build. Is Heroku building everything from scratch every time?
It (or at least the version on hackage) hasn't been updated for GHC 7.8: it claims dependencies `base &lt;4.7`, `array &lt;0.5` etc. Perhaps that was your issue?
I'll get on it when I have time.
 And this is just incomparable with how easily you can do it in darcs (especially with the new goodies like rebasing and hunk editing). And after it won't be [all fucked up](http://r6.ca/blog/20110416T204742Z.html). 
It took me a moment to understand how it worked but this is really interesting. While we have this on our minds, does anybody know what the best library for writing full-screen terminal apps is? I'd seen ansi-terminal and some ncurses stuff but it seemed relatively low level. I was looking for something analogous to lanterna.
There are two easy and related tasks I would love to see. 1. switch `clckwrks-plugin-page` from markdown+hscolour to pandoc. This should be pretty trivial. While pandoc is available as a library and an executable -- I think we want to stick with the executable version because using it as a library has GPL problems. 2. if `clckwrks-plugin-page` tries to run pandoc, and the executable isn't found -- it should report it in a sensible and understandable way. Currently the page just renders blank and the log says: fd:25: hClose: resource vanished (Broken pipe) After that, it would be great to add caching support. The main difficulty with that is determining when the cached version is out of date. Obviously, if the page source has changed, then the cache is out of date. But, the page source can contain 'dynamic' values as well. For example, if you embedded a {clock|current-time} in your page, then that page cache would expire every second. Another task is updating the code base from bootstrap 2.x to bootstrap 3.x. While theme plugins are not required to use bootstrap -- the admin console and other internals that are not themed do require bootstrap. If anyone is good graphic designer, it would be nice to have some better themes. The media gallery plugin is pretty anemic at the moment. It would be nice to have more plugins in general. That's just a few ideas off the top of my head. There will soon be a whole bunch of other possibilities once the new I18N and AngularJS stuff goes in. 
Builds are largely cached, but it has to succeed at least once for that to work. Getting the first build in under the limit was the only challenge, not subsequent deploys or day to day development.
Thanks for the comment. &gt; I'd recommend doing the proof exercises by hand Proof exercises of TAPL or SF?
I love it's [user manual](http://jtdaugherty.github.io/vty-ui/manuals/vty-ui-users-manual-1.7.pdf). I wish every Haskell package had something like this!
That is some impressive documentation, I'll have to check this out! Thanks.
I had similar issues with the 15 minute limit using vanilla Yesod (so it's not just Pandoc causing the problem), including the cryptic error messages from Anvil. I ended up setting up a $5 digital ocean instance and using Keter to deploy. This wasn't a cakewalk either, but was much easier than trying to hack around heroku's time limit. 
Really nice, but the light colored text on white background code is pretty hard to read IMO.
Charts is pretty straightforward.
[Easy Plot](http://hackage.haskell.org/package/easyplot-1.0/docs/Graphics-EasyPlot.html) is pretty easy to use. I'm assuming you want to plot points in the plane here, maybe I'm wrong about what you're looking for. So if you have a = [1,2,3,4,5] and b=[2,4,6,8,10], you could plot ordered pairs (a_i,b_i) easily to a PNG file by the command `plot (PNG "filename.png") [(n,2n)|n&lt;-[1..5]]` In general it will plot any list of ordered pairs easily, just do `plot (PNG "filename.png") LIST`
thanks! that's exactly what i wanted... only it seems not to be working. it just says false in the terminal.
Does the info [here](https://hackage.haskell.org/package/easyplot) help?
What is that cool looking terminal coloring in the screenshots?
One vote for 'cabal hell'.
Make strict evaluation the default.
Does this add Haskell to the project types in VS? Does it support debugging with breakpoints etc? I'm a total noob at Haskell.
No, thanks. 
Here's my fantasy wishlist for a fictional "perfect" Haskell. Toolchain: Full-featured Visual Studio integration, at least as good as F#. * Editor: I want red squiggles on type errors, type-aware autocomplete, * Debugger: Set breakpoints on any expression. Force thunks to evaluate (either 'for real' or temporarily in the debugger). Inspect closures in code just by clicking on unevaluated thunks in the watch window. * Project: Output a cabal package automatically. * A more minimal / brought-in-as-needed runtime. HelloWorld shouldn't be 1MB. Language: * A subset of the language that allows deterministic memory management a la Rust. * More 'magic' in typechecking. Ad-hoc overloading (why do I have to qualified import everything? There's only one version of my program that typechecks), implicit monadic &amp; applicative operations, types that overload the function application operation. In general, the compiler should be searching for a program that does what I want, instead of the 'guided constraint solving' that happens during typeclass resolution now. I have no idea how all these features work together, but I don't want to have to know--that's why we have PhD's designing the language :) * Better EDSL support. Better binding in do-notation with observable sharing. More intuitive syntax for arrows in general. Arrows without `arr`, that instead use a minimal set of combinators to apply the transformations required by arrow notation. * Prelude typeclassified, at least in 'advanced haskell'. `fmap` -&gt; `map`, `mappend` -&gt; `++`, etc. * String shouldn't be a list of Char. Libraries: * These are pretty good already, but there's always room for improvement! * Easier fetching of libraries. Why am I building everything? Just grab a working, tested binary for me. * On that note, less unix-centrism in library build processes. * Bindings to popular graphics, physics, sound, GUI, and game engines. I want to write Haskell to script Unity3d! * "Solved" FRP--intuitive *and* efficient. * Efficient symbolic math libraries for deriving more optimal math code from a working but inefficient base. * EDSLs for DSP-like code, including HLSL and audio.
Over my cold dead body.
Thanks guys, have added references to both of these in the blog post.
Nope, better *control* of strictness would be good, but it should not be the default.
Oo. Controversial :)
Projects might get added but are not included at the moment. As for debugging, I don't plan on messing with that. The VS API is horrendous as it is and I'm not that versed in debugging Haskell like that in the first place.
Make seq a type class, remove lifting from functions and products. This kills the need for newtype, as its main purpose is being unlifted. Break ADTs into records and sums as separate declaration types. This avoids the problem with partial record accessors. Perhaps remove positional products, just like we have no positional sums built in. Can use tuples instead. Tuples can be composed from 2-tuples only because of the unlifting, rather than an infinite set of tuple types. Allow anonymous records and sums in subexpressions, that can share their data constructors and fields with row polymorphism as well. This would allow much more precise subexpression types more conveniently. Possibly: put laziness/strictness in the types. Allow polymorphism over laziness.
I second that. Although I'm a Haskell newbie, having Emacs configured with ghc-mod continually amazes me. That plus the ability to "C-c C-l" to reload current buffer in ghci REPL running in another Emacs window and I'd say it's as good as REPL-driven development gets.
This presumes the person you're replying to hasn't read it, implying they must just be [ignorant of the linked argument, rather than simply having a different opinion to you.](http://www.reddit.com/r/haskell/comments/2cbbgf/the_comonadreader_letter_to_a_young_haskell/cjdvaps)
Sorry, I really didn't mean to go on teaching mode. I will be more cautious further on.
Also their mtl is unfortunately pre 2.2.1, so no ExceptT. Damn. Ah well, it's 27C and the weather's nice outside! ;-)
* I would love a better module system ala "backpack" * Even If I know and use emacs + ghc-mod I still second the need of a better toolchain. * Haskell by nature gives you a lot of freedom. The drawback of this is a lack of convention. There are so many way to express/do/achieve the same goal that reading haskell isn't that easy in general (IMH0) * Last but not least a consensus around monad transformers. 
&gt; strictness cannot be expressed as a type in a lazy language. Why? How do Haskell's types of kind `#` measure up?
One can design a strict language with a thunk datatype `T` and `force :: T a -&gt; a`. I don't believe there is any similar way of "adding" strictness to a lazy language that is reflected in the type and equally composable. "Composable" here means it should be possible to write code that is polymorphic in strictness. Unless I am much mistaken, types of kind `#` don't have the composability properties I'm looking for.
Do you already have the package installed?
[Stackage](http://www.stackage.org/) is up. Not a mirror, but everything in the repo builds together and passes tests. Choose `exclusive` if you want _only_ packages that build together, choose `inclusive` to include unvetted packages from Hackage that might not build.
In case it's helpful, here's a much newer version with many more packages (including lens and pipes): http://haddocks.fpcomplete.com/fp/7.8/20140719-74/ The one linked in the original was from a subset of packages that built on a pre-release version of GHC 7.8 last year, so very incomplete. BTW, there is no manual "approval" process for packages on fpcomplete.com. Just add the package to [Stackage](http://www.stackage.org) and we'll pick it up with the next refresh.
Thank you!
I would like compile errors to stop the build; then I would like test failures to stop the build; and finally I would like compile warnings to stop the build, even if the module they originated from was not compiled this time around. Currently, if compile warnings are to stop the build, it must be at the same time as compile errors. Making `Monad` a subclass of `Applicative` is great, but the math-inspired typeclass hierarchy could do with more refinement. There should be a typeclass containing `&lt;*&gt;` but not `pure` (for e.g. `Map`). `Semigroup` should be a superclass of `Monoid`. To make this sort of thing easier, we really need to be able to implicitly declare instances of superclasses when declaring an instance of a subclass: hopefully the community will accept a proposal soon. It irks me that the `Monoid` instance for `Map` has `mappend = unionWith const` rather than `mappend = unionWith (&lt;&gt;)`, reusing the elements' `Monoid` (`Semigroup`, really) instance. This is a little thing, it is easy to get around with a newtype, and it is not even as if I need it that often. And changing it now would be a needless pain. But still it grates.
Get rid of "head" and "tail". Partial functions are the devil.
Great! Thank you
wouldn't that taint your existing Cabal cache/installation with unofficially patched packages? 
It's true that polymorphic functions can't manipulate unboxed values in GHC. `a` against `Int#` is a kind mismatch (`*` against `#`), except in the case of `(-&gt;)` which will accept either `#` or `*`, of course (which is a kind (hur hur!) of kind polymorphism). Reference for those interested: [Unboxed values as first-class citizens in a non-strict functional language](http://research.microsoft.com/en-us/um/people/simonpj/Papers/unboxed-values.ps.Z). From an implementation side that makes sense. Polymorphic functions expect pointers to values instead of the unpacked values themselves. What isn't clear to me is whether these restrictions come only from an implementation convenience in GHC or that it semantically would never make sense to allow polymorphism over strictness. Is it _undoable_ or not-been-done?
&gt; There should be a typeclass containing &lt;*&gt; but not pure (for e.g. Map). You mean Data.Functor.Apply from the "semigroupoids" package?
I'd recommend just wiping your existing cache from Hackage. It's a matter of publication. Hackage will publish any **package**, whether it builds or not. Stackage only publishes **package sets** that build together and pass tests.
I recently used `head` and `tail` in some code; could you show me (an article/post that describes) how to accomplish the same without them?
Anyone have any idea how to set up the mirror for nix? Do I just need to change the nix expressions, where ever that lives?
Hayoo has the complete Hackage documentation: http://hayoo.fh-wedel.de/?query=package%3A%21lens%20type%3A%21module Although, not in Haddock style
Admins are aware and are on it. Apparently a machine rebooted and some drives need to be fscked by hand.
Admins are aware and are on it. Apparently a machine rebooted and some drives need to be fscked by hand.
Admins are aware and are on it. Apparently a machine rebooted and some drives need to be fscked by hand.
I'm actually in a park right now :p Outside coding is best coding.
Cool, thanks. Guess I just picked the wrong time to try (again) to get up to speed with Haskell and some of its tools!
On another note, why does http://status.haskell.org/ not say hackage is down/hasn't been updated in 2 months?
&gt;All of my experience indicates that people with that mentality are either wrong or aren't writing very complex software. See, this is what I mean. With this rhetoric all non-Haskell programmers are at least one of liars, deluded or working on trivial stuff. That's not he way to win friends and influence people—supposing that this is what you want to achieve. And, by extension, you are inviting us to infer that all the software written by any non-Haskell programmer in any other language either doesn't work or doesn't do anything non-trivial (or both). Can that really be true? What I infer from this rhetoric is that Haskell programmers have an idea of what “correct” means which is so wildly divergent from what the rest of the (wildly growing, profitable, multi—bajillion dollar) industry thinks “correct” means that it's probably not worth paying attention to. Is that what you're aiming for?
I would prefer to replace head :: [a] -&gt; a tail :: [a] -&gt; [a] with their safe equivalents head :: [a] -&gt; Maybe a tail :: [a] -&gt; Maybe [a] Sometimes, partial functions are useful for really quick hacks where you know it's not truly partial. For those cases, I would use these with `fromJust`, which I think is okay because it's *obviously* partial and dangerous, unlike `head` and `tail`.
Is there are a mirror for GHC?
Just don't use them: &gt; totalHead = listToMaybe &gt; &gt; totalTail = drop 1
[See this comment](http://www.reddit.com/r/haskell/comments/2chz8g/wwwdownforeveryoneorjustmecomhaskellorg/cjfohps), which explains why the link OP provided has so few packages in it.
Pretty sure this was posted already, but upvote because Nix and NixOS are too cool to miss
Your definition of `totalTail` matches the semantics of [`last`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Prelude.html#v:last), not `tail`. You were looking for totalHead l = case l of [] -&gt; Nothing (x:xs) -&gt; Just xs Probably there's a more elegant way of defining it, but `reverse` should *not* be in the definition of `tail`!
&gt; With this rhetoric all non-Haskell programmers are at least one of liars, deluded or working on trivial stuff. How can you possibly conclude this from my statement? My statement only refers to people who "are pretty sure that they write correct [read bug-free] software". Incorrectly expanding this to "all non-Haskell programmers" is a misrepresentation of my argument. &gt; That's not he way to win friends and influence people—supposing that this is what you want to achieve. I'm keenly aware that it is not easy to say what I was trying to say without coming across as inflammatory. I thought long and hard about the wording, which is why I ended up opening with the big caveat of "All of my experience indicates". &gt; And, by extension, you are inviting us to infer that all the software written by any non-Haskell programmer in any other language either doesn't work or doesn't do anything non-trivial (or both). Can that really be true? No, I'm not saying that. I'm saying that **almost** all software everywhere (non-Haskell and Haskell alike) either has bugs or is not very complex. To me this is obviously true.
&gt; String shouldn't be a list of Char Why not? The fact that string is a special type in OCaml that has its own set of operators is one of my least favorite things about that language. You have to manually explode all strings anyway to do anything useful with them. Hard-coded UTF8 strings might be more efficient, but I've found that being able to do standard list manipulations to them for free makes my code MUCH more intuitive.
- Split all memory-unsafe operations from `IO` (including `Storable`, etc.) into a separate `Unsafe` monad, with `unsafe :: MonadIO m =&gt; Unsafe a -&gt; m a` which is `{-# LANGUAGE Unsafe #-}` to run an `Unsafe` operation. There should be `instance MonadIO Unsafe`. (There should *not* be a `class MonadUnsafe`, which would defeat the purpose.) - Ban orphan instances. - Lose the need for `CPP`. Grow some actual language features to fill the necessary holes. (Perhaps make TH suck less.) - Mutually recursive modules without `.hs-boot`. - Submodules. - Rename individual entities when importing them, e.g. `import Module (foo as bar, baz as quux)`. - Anonymous sum types, e.g. `(Bool|Char|Int)`. - Disjunctive patterns, e.g. `foo (Left a \/ Right a) = a`. - Explicit type application. - Ways to reduce too-many-type-variables boilerplate, noise, incomprehensibility etc., such as scoping type variables over multiple declarations, specializing a whole module at once, ... at the limit this probably means ML-style modules and module-functors. - Higher-order constraints, e.g. `type Alternative f = (Applicative f, forall a. Monoid (f a))`. (Not necessarily a good idea w.r.t. laws, just an illustration!) (Can be faked with [`constraints`][constraints] up to a point.) - Entailment constraints, e.g. `instance (forall a. c a =&gt; Show a) =&gt; Show (Exists c)`, where `data Exists c where Exists :: c a =&gt; a -&gt; Exists c`. - Real first-class polymorphism and impredicativity a la MLF. :) - (First-class existentials too!) - Supercompilation. - `pin#` and `unpin#` primops, or something similar (`withPinned#`, `PinningPtr#`, ...); make `ByteString`s and so forth unpinned by default. EDIT: - `type ByteString = Vector Word8`. ([`vector-bytestring`][vector-bytestring]) - `newtype Text = Text ByteString`. - `data Lazy a` implements chunking generically for vector-like things. `Data.ByteString.Lazy` is replaced with `Lazy ByteString`, `Data.Text.Lazy` with `Lazy Text`. [constraints]: http://hackage.haskell.org/package/constraints-0.3.5/docs/Data-Constraint-Forall.html [vector-bytestring]: https://hackage.haskell.org/package/vector-bytestring
Hurray! It's back on! Let's celebrate! )
It looks fixed to me!
**`.haskell.org` servers are back!** ^(short story: servers rebooted due to yet unknown reason, but some got stuck during `fsck`)
**`.haskell.org` servers are back!** ^(short story: servers rebooted due to yet unknown reason, but some got stuck during `fsck`)
Unfortunately, it's going to stay that way for the entire GHC 7.8 series. For more information, see [the GHC 7.8 and transformers 0.3 blog post](https://www.fpcomplete.com/blog/2014/05/lenient-lower-bounds). Fortunately, in this case, you can use [transformers-compat](http://haddocks.fpcomplete.com/fp/7.8/20140719-74/transformers-compat/index.html), which includes ExceptT.
I prefer fromMaybe (error "specific message about what happened") rather than `fromJust`. It makes it even more obvious.
&gt; Haskell by nature gives you a lot of freedom. The drawback of this is a lack of convention. There are so many way to express/do/achieve the same goal that reading haskell isn't that easy in general (IMH0) I agree with this. It's one of the biggest stumbling blocks I've noticed among beginners – they've seen so many ways to do something they don't know the front from the back anymore! On the other hand, I thnk it's a consequence of how powerful Haskell is. I don't want to limit myself to writing code the way I did two years ago. In two years' time, I will probably not want to limit myself to writing code the way I do now. Haskell is good at DSLs and combinator functions, and the more of them you learn, the more of them you'll want to use. In the back of my head, I'm afraid we simply have to accept that a persons Haskell code will evolve with the person.
Oops. I don't use that unsafe stuff much (;
I didn't at all interpret it as an implication of ignorance. I personally love when people meet my opinions by recommending interesting articles with more reading on the subject like that. Maybe /u/aicubierre hasn't read that post, in which case they might enjoy doing so. Maybe they have, in which case they are free to say so or just ignore it entirely. There's nothing wrong with giving people ideas on things to read, especially not when they are related to opinions/interests they clearly possess!
Why does that make me imagine a magnetised needle and a steady hand?
Would [this NFData-based proof-of-concept](https://gist.github.com/gelisam/58495f7b996f77e09d80) qualify as code which is "polymorphic in strictness", or were you thinking of something else?
You got really unlucky then! The hackage servers are usually pretty stable, and most of the outages I've seen were fixed by the time I had heard about it. 
Or just use pattern maching
You do know that Data.Text sports a list-like interface and is an instance of Monoid and such, right?
I think it's easier if you show how you used `head` and `tail` and then we show the safe version. Generally the safe version involves: * pattern matching * using `Maybe` to handle the case where your list is empty
Replace String with Text. The only reason to use String is to avoid needing to work around the fact that it's the default. A more modular typeclass mechanism. Considering the issues with Functor-Applicative-Monad, it's pretty clear that you're never going to get the perfect typeclass hierarchy right on the first try. If we ever want to add Semigroup, fix the numeric hierarchy, make an Arrow without arr, maybe even add Apply/Bind as a superclass to Applicative/Monad we're going to have all the same issues. Something that would allow experimentation without breaking existing code would be nice. There are two serious things that prevent these changes right now: is the change worth the extra cognitive overhead of introducing more typeclasses, and is the change worth losing backwards compatibility? Ideally we should only have to consider the first, but the current mechanism forces us to consider the second. Dependent types. There are too many cases where you want stronger type guarantees, and the current situation is very ad-hoc. While we're at it, a Dynamic type that allows polymorphic values. Better serialization capabilities, for things like GADTs/functions/infinite datastructures. Maybe the ability to inspect and serialize thunks at runtime, for a general solution. I'm aware there are things that make this difficult, and that Cloud Haskell is working on improving this, at least for functions. Something to make people aware of stable, mature, and /documented/ libraries. It's easy to find lots of Haskell material about abstractions like Free monads, Codensity, Comonads, Lenses, and other things that might be helpful for programming, but less easy to find material on the actual programming that these things help us with. There are lots of interesting things in Hackage, but knowing which are worth using, and which are just interesting is a little difficult. If there are any podcasts or anything that just introduce a useful documented library each week I would like to know.
Thanks, but it doesn't justify lazy evaluation as the *default*. SPJ gave his reasons and they are largely historical. Everyone here seems to agree about `foldl'` in the core library, about lazy Writer performance, about the pitfalls of lazy I/O and so on, yet for some reason you're so fanatical about forcing (pardon the pun) laziness onto everyone even though it is extremely counter-intuitive to like 95% of programmers out there. It's strange. Just look at the number of bang-patterns in people's code, people.
Pattern matching is great but rather verbose for such common things.
Those are unboxed types, not strict types. While it is true that unboxed values are always strictly evaluated, it's not true that a strictly evaluated value must be unboxed.
Was any claim was made to the contrary?
Alternative fantasy: if [Catch](http://community.haskell.org/~ndm/catch/) worked for GHC, we could use `head` happily as long as Catch statically guaranteed totality (modulo non-termination). Use of `head` is often poorly thought out, but sometimes it is legitimate and awkward to avoid it just because we don't have tools to validate it.
Unlike many other wishlist items mentioned, mine is both something I would unquestionably personally see as better and also will never happen, making it a true fantasy: s-expression-based syntax, quotation and macros a la Lisp.
Here is a systematic sequence of refactorings which I made by algebraic substitution after straightforwardly transforming each point of control in your imperative program into a function. It's still a bit of a mess in the end, but then, so was your imperative program in the beginning. :P I would be very suspicious of the correctness of any such function involving conditional updates in the way that this one does. def foo(x,y): x = f(x) if a(x) -- 1 if c(x): -- 2 x = g(x) else: x = h(x) x = f(x) -- 3 y = f(y) if a(y) -- 4 x = g(x) if b(y) -- 5 return [x,y] -- 6 foo x y = foo1 x y where foo1 x y = foo2 (if a x then f x else x) y foo2 x y = foo3 (if c x then g x else h x) y foo3 x y = foo4 (f x) y foo4 x y = foo5 x (if a y then f y else y) foo5 x y = foo6 (if b y then g x else x) y foo6 x y = [x,y] (n.b. If there had been loops in your program, these would have resulted in mutual recursion as the control would pass from the last statement of the loop back to its beginning, or the place where we test its conditional.) foo x y = foo1 x y where foo1 x y = foo2 (if a x then f x else x) y foo2 x y = foo3 (if c x then g x else h x) y foo3 x y = foo5 (f x) (if a y then f y else y) -- eliminate foo4 foo5 x y = foo6 (if b y then g x else x) y foo6 x y = [x,y] cond p f g x = if p x then f x else g x foo x y = foo1 x y where foo1 x y = foo2 (cond a f id x) y foo2 x y = foo3 (cond c g h x) y foo3 x y = foo5 (f x) (cond a f id y) foo5 x y = foo6 (if b y then g x else x) y foo6 x y = [x,y] cond p f g x = if p x then f x else g x foo x y = foo1 x y where foo1 x y = foo3 (cond c g h (cond a f id x)) y -- eliminate foo2 foo3 x y = foo5 (f x) (cond a f id y) foo5 x y = foo6 (if b y then g x else x) y foo6 x y = [x,y] cond p f g x = if p x then f x else g x foo x y = foo1 x y where foo1 x y = foo5 (f (cond c g h (cond a f id x))) (cond a f id y) -- eliminate foo3 foo5 x y = foo6 (if b y then g x else x) y foo6 x y = [x,y] cond p f g x = if p x then f x else g x foo x y = foo1 x y where foo1 x y = foo5 (f (cond c g h (cond a f id x))) (cond a f id y) foo5 x y = [if b y then g x else x, y] -- eliminate foo6 cond p f g x = if p x then f x else g x foo x y = foo5 (f (cond c g h (cond a f id x))) (cond a f id y) -- eliminate foo1 where foo5 x y = [if b y then g x else x, y] From here, we have a bunch of options for where to go next with making things reasonable. We notice that the function cond a f id is repeated. Perhaps it's important somehow and deserves its own name? Maybe cond c g h does as well. cond p f g x = if p x then f x else g x foo x y = foo5 (f (gh (f' x))) (f' y) where f' = cond a f id gh = cond c g h foo5 x y = [if b y then g x else x, y] We could also eliminate foo5, but due to the way that its parameters occur more than once in its body, we probably want to introduce definitions for them: cond p f g x = if p x then f x else g x foo x y = [if b y' then g x' else x', y'] where f' = cond a f id gh = cond c g h x' = f (gh (f' x)) y' = f' y It's really hard to say how to make this function nicer without knowing what it really does, or what things might be conceptually relevant, but there are many options.
* first class, parametrized modules * first class patterns (though recently this started to become reality?) * proper type-level functions (again, getting closer I think) Recently GHC actually added a lot of stuff I wished for, or at least something close enough, but haven't yet had the chance to play with them. And of course, there is the infamous cabal hell.
If "list" operations were all built off anything which had a "left view" like class Viewl r where type family Element r :: * viewl :: r -&gt; Maybe (Element r, r) then we could instantiate Text and ByteString just the same off Unicode points or bytes and it would all work transparently.
Most of my semantic wishes have been asked here, so I’ll put forth a couple of modest syntactic wishes. First, remove the requirement for `$` in `f $ do …` for `do`, `let`, `if`, `case`, `\`, `mdo`, and `proc` so that you can do things like: example thing = do forM_ xs \x -&gt; do print x when (isBad thing) do putStrLn "This thing is bad." foo &lt;- bar return case foo of Foo x -&gt; x Bar y -&gt; y I think a special case still needs to be made for unary `-`, although my second wish is to make a sign character (`+` or `-`) a part of numeric literals. 
You had me at disjunctive patterns, particularly for enumerations. I would prefer something like `,` over `\/`, if we assume `|` is unavailable because of guards. case x of case x of A -&gt; 0 A, B, C -&gt; 0 B -&gt; 0 D, E -&gt; 1 C -&gt; 0 F -&gt; 2 D -&gt; 1 E -&gt; 1 F -&gt; 2 Also, for modules, `qualified` exports: module Data.Map ( Map , qualified fromList , … ) where … module Main where import Data.Map as Map -- fromList must be used qualified unless imported unqualified main = print (Map.fromList [("this", 0), ("that", 1)] :: Map) 
I'm not sure non-persistent data is actually a good way to operate. That said, an STRef does not seem like the worst plan here.
&gt; Is it undoable or not-been-done? I think it's perfectly doable. I just don't think you can do it in a language where datatypes are lazy by default. Then it seems like the only way to get a strict datastructure is use a bang pattern like construction which is not composable. On the other hand you can get lazy datatypes from strict ones just by using a lazy box (i.e. the thunk datatype).
nice! I'm a newbie, didn't know basic-prelude.
Why don't your try an even simpler monad? Why not the simplest? cond :: Monad m =&gt; (a -&gt; Bool) -&gt; a -&gt; a -&gt; m a cond p x t = return $ if p x then t else x foo x y = runIdentity $ do x &lt;- cond a x (f x) x &lt;- if c x then return $ g x else return $ h x x &lt;- return $ f x y &lt;- cond a y (f y) x &lt;- cond b y (g x) return (x,y) 
&gt; I would prefer something like `,` over `\/`, if we assume | is unavailable because of guards. Yeah, I don't think `\/` would be the right syntax either, I just didn't feel like thinking about what would. Are commas here unambiguous w.r.t. tuples? `qualified` exports sounds nice too :-)
I think that Halvm targets xen is the main problem of it.
My favorite is `uncons :: [a] -&gt; Maybe (a, [a])` because you can pattern match it in the Maybe monad do a:as &lt;- uncons xs ...
Did a rewrite with lenses and modify just because, but I like qZeta's approach better ([link](http://www.reddit.com/r/haskell/comments/2cin7p/is_there_any_way_to_elegantly_represent_this/cjfwny6)). foo = (fooSt .) . (,) where fooSt = execState $ do x &lt;- gets fst when (a x) $ modify $ over _1 f x &lt;- gets fst modify $ over _1 $ if c x then g else h modify $ over _1 f y &lt;- gets snd when (a y) $ modify $ over _2 f y &lt;- gets snd when (b y) $ modify $ over _1 g
Brilliant and clean
And it should be in base as a superclass of `Applicative`.
Ah right. Tuples have to be in parentheses, disjunctive patterns would not be. So that works for `case` but not for equational style: data XYZ = X | Y | Z eitherX :: (XYZ, XYZ) -&gt; Bool -- :) eitherX pair = case pair of (_, X), (X, _) -&gt; True _ -&gt; False -- :( eitherX (_, X), (X, _) = True eitherX _ = False So perhaps `( … | … | … )` is the way to go: -- :) eitherX pair = case pair of ((_, X) | (X, _)) -&gt; True _ -&gt; False -- :) eitherX ((_, X) | (X, _)) = True eitherX _ = False Unfortunately, disjunctions require involving `Maybe` in pattern matching if you allow variables to be unbound in some branches of a disjunction: splitEither :: Either a b -&gt; (Maybe a, Maybe b) splitEither (Left a | Right b) = (a, b) -- === splitEither (Left a) = (Just a, Nothing) splitEither (Right b) = (Nothing, Just b) And there’s other stuff you might want to do with regular patterns on lists, like in HaRP: partitionEithers :: [Either a b] -&gt; ([a], [b]) partitionEithers [(Left ls | Right rs)*] = (ls, rs) And non-linear patterns: instance Eq XYZ where (==) x x = True -- First-class structural equality! (==) _ _ = False Where do you draw the line? 
This list has me drooling.
Would indeed be nice; but could be confusing to use, esp. in combination with ad hoc overloading. What should the compiler do with `fn e :: f a`, when there are two `fn` avalaible: one `fn :: i -&gt; a`, one `fn :: Functor j -&gt; b` (which might not be uncommon in libs) I kind of like the fact that `map` should be stated explicitly, as you have think exactly about the datastructure you are working with, newbies would be really confused about it. Any thoughts?
The `let x = f x` binding is recursive and shadows the previous `x` without using it.
Whoops, indeed. Fixed it.
&gt; Allow anonymous records and sums in subexpressions, that can share their data constructors and fields with row polymorphism as well. This would allow much more precise subexpression types more conveniently. Could you please illustrate this with an example? &gt; Possibly: put laziness/strictness in the types. Allow polymorphism over laziness. I was thinking about this [*a lot*][so] some years ago. I couldn't figure out how it could possibly be made to work. Strictness annotations on datatype field types (which we already have) and function parameter types are easy, after that it gets weird fast. A type `f :: Foo -&gt; !Bar` is nonsensical because `f a` will be a thunk until its value is demanded somewhere, so it *cannot possibly* return a "strict" type. Strictness annotations on type arguments (`[!Int]`) sounds very appealing, but also raises all kinds of really awkward questions (how do you get from `[Int]` to `[!Int]`? From `f Int` to `f !Int`?). Perhaps you know of a better plan? [so]: http://stackoverflow.com/a/16104671/626288 
Wow that should've been laborious. I think the initial refactoring is a great potential solution as I could rename `foo1`, `foo2` and so on to the steps of the cooking: `bakeCake`, `getSpoons`, etc, managing the awkwardness of the tags and keeping the variable names simple. I'm not sure the compressing strategy would result in more readable in practice, though, but I'll test it too. Thank you!
&gt; Unfortunately, disjunctions require involving `Maybe` in pattern matching if you allow variables to be unbound in some branches of a disjunction: ...I would just require all branches to bind the same names (with unifiable types). Non-linear patterns I would rather avoid. Haven't heard about HaRP, but... my fantasy was about disjunctive patterns, not disjunctive patterns and the kitchen sink as well :-) so that's where I would draw the line.
It's not fanaticism, Johan, Lennart, Kmett have written plenty of well reasoned arguments about why laziness by-default is the right choice. There's still some debate about this subject, but it's not like there's an overwhelming amount of evidence that one evaluation strategy is preferable. There are a ton of cases where one exhibits more desirable properties than other and vice-versa. As for the popularity argument ... like a lot of design decisions in Haskell we "avoid (success at all costs)", whether it's counter-intuitive to 95% of programmers isn't going to convince many of the core developers that it isn't necessarily the right decision from a pure PL design standpoint. Purity and monads aren't intuitive to many programmers either, but those aren't going to change anytime soon either.
If you're going to go full lens, `modify $ over _1 f` is `_1 %= f`
While this would make some things simpler, it might not be worth sacrificing consistency, but who knows.
there was a power outage in the data center it turns out http://www.hetzner-status.de/en.html
Also, EclipseFP suggest fixes for problems where available, and it can do most of them automagically with 1 click.
~~Won't point 3 have different semantics due to let-generalization?~~ EDIT: Thanks for the clarification :)
&gt; that's why we have PhD's designing the language :) There's a kind of irony that Simon got his honorary PhD after designing a successful language!
That's an interesting read, thanks!
Yea, Backpack-like modules. 
How about asking Google for "refactoring tool for haskell"? It gives the correct answer, as first result: The HaRe Project. 
&gt; The cleanness is so ubiquitous it removed dirt from my keyboard. Is there some Haskell quote hall of fame I could candidate this for? :-)
The `fibs` problem is easy to solve, because tail can be made total by changing its semantics: safeTail (_:xs) = xs safeTail [] = [] 
`data NEList a = Head a [a]`
I guess you mean `(a, as) &lt;- uncons xs`.
It makes monadic code clunky, verbose, and leaves a bad taste in the mouth of most professional programmers, who are used to writing with an 'ambient' monad. Compare: c &lt;- some_condition if c then do ... else do ... to the pythonic if some_condition: ... else: ... And it gets worse when you have a function that takes a few arguments inside of some_condition, some of which are monadic and some of which aren't. tmp1 &lt;- something tmp2 &lt;- somethingElse c &lt;- some_condition tmp1 tmp2 if ... `liftM2` and other solutions feel clunky, especially when some of the arguments are pure -- why am I writing all this boilerplate in a strongly typed language? The compiler should know what I mean, otherwise why am I providing it so much type information? If evaluation order in monadic code is such a big deal, why don't side-effect-ful languages like C/C++/python have a problem?
`fibs` should return a stream anyway. data Stream a = Cons a (Stream a) fibs = 0 `Cons` 1 `Cons` streamZipWith (+) fibs (streamTail fibs) 
&gt; purity by-default is the right choice You mean laziness by default.
Get rid of `if`/`then`/`else` as syntax. Make lambda case the only way of writing a `case` statement.
I wish there was a way.
I'm sorry, I don't understand what you're trying to say. Well, I have two hypotheses, but neither of them fits the type you gave. Are you saying that the existing `foldl'`, when applied to a `Strict a` value and a lazy list foldl' :: (Strict a -&gt; b -&gt; Strict a) -&gt; Strict a -&gt; [b] -&gt; Strict a would build a large number of thunks instead of eagerly evaluating the function at every step? Alternatively, are you saying that in addition to the functions `strictCons` and `strictFMap` I have implemented, I should also implement a function strictFoldl' :: NFData a =&gt; (a -&gt; b -&gt; a) -&gt; Strict a -&gt; [b] -&gt; Strict a and that regardless of the way I implement the function, that implementation will be guaranteed to have a space leak? Or perhaps you meant something else entirely?
Oh, so no one actually answered your question? Big surprise. The answer is no. Haskell does not support this particular idiom. Now let's focus on a more interesting question: Why can't we find a single person on this subreddit with the balls to admit that haskell might be anything less than perfect?
You might say it’s less consistent with expressions, more consistent with literals. But I think there’s no case where it’s ambiguous except for unary `-`, so I do consider this a grammar bug. If you want to pass the results of two `case` or `do` expressions to a function, you have to bracket them (mixing layout with non-layout): f (case x of Just x' -&gt; x' Nothing -&gt; 0 (case y of Just y' -&gt; succ y' Nothing -&gt; 1) Or name them (introducing spurious names): let a = case x of Just x' -&gt; x' Nothing -&gt; 0 b = case y of Just y' -&gt; succ y' Nothing -&gt; 1 in f a b Whereas if `case` &amp;al. had the same treatment as literals, this form might be allowed: f case x of Just x' -&gt; x' Nothing -&gt; 0 case y of Just y' -&gt; succ y' Nothing -&gt; 1 Or this, which is not allowed at present even though the starts and ends of the `case` expressions are obvious: f case x of { Just x' -&gt; x'; Nothing -&gt; 0 } case y of { Just y' -&gt; succ y'; Nothing -&gt; 1 } Though you could rightly argue that this level of nesting ought to be avoided anyhow. 
&gt; Oh, so no one actually answered your question? Big surprise. http://www.reddit.com/r/haskell/comments/2cin7p/is_there_any_way_to_elegantly_represent_this/cjfwny6
http://www.reddit.com/r/haskell/comments/2cin7p/is_there_any_way_to_elegantly_represent_this/cjg1kkh
...yeah. Apparently I can't Haskell in the morning. I suppose I'll have to turn in my functional programmer's license now.
Yeah, I forgot what Text did. Just ignore me...
&gt; tmp1 &lt;- something &gt; tmp2 &lt;- somethingElse &gt; c &lt;- some_condition tmp1 tmp2 Applicative! c &lt;- some_condition &lt;$&gt; something &lt;*&gt; somethingElse It would be nice if it were easier to use built-in syntax like `if` with functors / monads, though.
I was referring to the proof exercises in TAPL. SF has mostly automated proofs as exercises - it's worth doing the optional exercises on "informal" proofs, since it'll be a good warm up for TAPL and/or post-TAPL exercise.
Are you suggesting the solution is inelegant? Apart from the (concise) definition of `cond`, it's almost identical to the imperative code in the question.
 foo :: (x, y) -&gt; (x, y) foo = branch (b . view y) (x %~ g) id . (y %~ branch a f id) . (x %~ f . branch c g h . branch a f id) where x = _1 y = _2 branch c f g a = if c a then f a else g a 
You can make a refactoring tool quite easily. There are libraries such as HLint, Haskell-Src-Ext and others which do a lot of the heavy lifting. Structured-Haskell-Mode gives a good example of how one might get code from an editor and feed it to a Haskell service for parsing and analysis. 4 out of the 5 concrete examples you gave above are really not that hard to implement using what I just explained above. The Cabal Library is also quite handy. In Emacs, you can have a company-mode back-end populated with HLint auto-refactors up in no time. Give it a shot and share it with others. 
i prefer `fix$scanl(+)0.(1:)` anyway
Exactly my point, it's mechanical which operators to add. If I'm working in the X applicative or the Y monad, let me make that 'ambient'. Consider this poorly-designed extension of do-notation I'll call `with`. example :: forall m. MonadState Int m =&gt; m Int example = with m: x &lt;- get if get &gt; 100: put (get - 100) if get &gt; 10: put (get - 10) else: put (get + 1) return (get * x) Further extensions: data Problem = TooSmall | TooBig example2 :: MonadState Int m =&gt; m (Either Problem Int) -- note lack of boilerplate runErrorT example2 = with (ErrorT Problem m): if (get &lt; 0): throwError TooSmall if (get &gt;= 100): throwError TooBig return get example3 :: Int -&gt; Either Problem (Int,Int) example3 n = with (StateT Int &lt;- n, Identity): example2 Note the complete lack of lifting functions, `&lt;$&gt;` line noise, special `ifM` combinators, etc. You can still write that stuff if you need it (and a well-designed version of this proposal would let you add additional 'stuff' to the desugaring in code).
The Identity monad turns &gt;&gt;= into application. It gives you do notation for lambda calculus, that's all it is. 
Well, right now, we don't use type-directed identifier overloading, so `fn` only has one principal type. I'm not quite sure what your type `Functor j -&gt; b` means as it looks like Functor is being used as a type constructor (or type family) instead of a type class constraint, so I can't address that specific concern. Idris uses something like `[| f a b c |]` to mean something like `f &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c` and a syntax like that might be a good compromise that avoids any possible ambiguities with automatic Functor/Applicative/Monad lifting.
Which is exactly what I needed. 
Which happens to be equivalent to `drop 1`.
It's good to have one of these threads every so often, so people can get the ideas that really should never be added to Haskell out of their systems.
&gt; if a function takes a `Strict a` as an argument that doesn't mean it's in WHNF it just means that it's a thunk which evaluates to an `a`. Thanks for pointing this out, it wasn't immediately obvious. When I reason about strictness, I don't usually think about whether particular values or arguments are thunks, in HWNF, or fully-evaluated. Instead, I consider what happens when a particular expression gets evaluated to HWNF; which other parts also get evaluated as a consequence? In the case of an expression of type `Strict a`, the answer is clear: everything gets fully-evaluated, and there is no more laziness lurking inside the resulting value. Similarly, when I think of a strict function vs a lazy function, I think of what happens to the arguments when the result of the function gets evaluated to WHNF. With a function of type `(Strict a -&gt; Strict b)`, we can deduce that its argument either gets fully-evaluated or isn't used at all, and that once again, there is no more laziness lurking inside the resulting value. In particular, since I know that (unlike `foldl`) `foldl'` evaluates the output of the step function to WHNF before considering the next element of the list, the type `(Strict a -&gt; b -&gt; Strict a)` of our step function tells me that this HWNF evaluation will be sufficient to fully-evaluate any thunk up to that point. Therefore, I respectfully disagree: applying `foldl'` to a `Strict` value and a lazy list will *not* accumulate a large number of thunks. For what it's worth, profiling with `+RTS -hy` indicates that the following program takes a constant amount of memory, and so does the version with `Int` instead of `Strict Int`. strictPlus :: Strict Int -&gt; Int -&gt; Strict Int strictPlus sx y = mkStrict (x + y) where x = runStrict sx main :: IO () main = print $ runStrict $ foldl' strictPlus (mkStrict 0) [0..100000000] Whereas the `foldl` version has a space leak, whether `Strict` is used or not. This, of course, isn't because `Strict` isn't strict enough, but simply because `foldl` is too lazy. I'm not sure why we're talking about this, though. By any chance, when you were thinking about code which would be "polymorphic in strictness", were you imagining a type `ContagiousStrict a` which, when used with `foldl`, would cause `foldl` to behave like `foldl'`?
I really don't know why you're rambling. Of course shadowing variables isn't impure, but it's often a bad idea. 
Can you elaborate? I'd assumed that one construct was essentially sugar for the other.
And turn up swappiness to 100.
`let` and `where` both generalize.
It made a difference for the chart library (https://github.com/timbod7/haskell-chart). When it was hosted in darcs, contributions generally arrived as source files in my inbox (not patches). Now it's in github, I get pull requests, which are far easier to review and merge. Moving to git might have caused a small improvement in the flow of contributions, but has definitely reduced the amount of work required to integrate them.
I'm not too fond of infix myself. (+ a b c .. ) is better than (a + b + c + .. ).
I would write your example foo x0 y0 = let x1 = if a x0 then f x0 else x0 in let x2 = if c x1 then g x1 else h x1 in let x3 = f x2 in let y1 = if a y0 then f y0 else y0 in let x4 = if b y1 then g x3 else x3 in [x4, y1] like this foo x y = let y' = cnd (a y) (f y) y x' = o y' (f . n . m $ x) in [x', y'] where cnd a b c = if a then b else c m x = cnd (a x) (f x) x n x = cnd (c x) (g x) (h x) o x y = cnd (b x) (g y) y You might drop 'cnd' function which is just shorthand for if-then-else. Most state changes and conditions are now captured by functions. Not sure if it is correct rewrite. With appropriate names it should look better.
This is one drawback of not having non-recursive let bindings. For example, Haskell with non-recursive let bindings would allow something like: foo x y = letnonrec x = if a x then f x else x x = if c x then g x else h x x = f x y = if a y then f y else y x = if b y then g x else x in (x,y) This is used in some other functional languages, SML for example. 
normally http://hdiff.luite.com/ also has a mirror, but unfortunately the server is in the same datacenter as http://haskell.org/ , so it was also affected by the power outage. I'm moving hdiff to a different server this week.
HaRe is being (re)-developed at https://github.com/alanz/HaRe and is on hackage. It is being ported to use the GHC compiler as a backend, to get full variable name resolution and type information. It can currently do cross-module variable renaming, land moving definitions up or down in a function. This is a work in progress, and contributors are welcome.
all hetzner servers in one of their data centers had to reboot. it happened for a few of mine as well. why the .haskell servers often get stuck on reboot i have no idea.
Your top line Python program says absolutely nothing about the problem it solves, it just so happens that assignment in Python lets you accomplish updating variables (touché!) so whatever it does, you've been able to code it with a simple notation. For functional programming, you should be thinking about what the program does so you can implement it (mostly-) functionally rather than try to copy a style which functional programming doesn't suit.
Would multiple versions really help? What if lib A uses version x but lib B uses version y of some dependencie but there are breaking changes in version y and now I have a project that wants to use A and B? For my (novice) use cases sandboxes are fine (there is `cabal repl`) and I don't think you can solve the rest - if some dev. decides to introduce breaking changes on the external facing interface of his package you will never get "&gt;a.b.c"
&gt; cabal sandbox is not so comfortable to use (when you want to load it in ghci) Are you aware of `cabal repl`?
`-Werror` makes warnings stop the build *before* tests are run. I would like warnings to stop the build *after* tests are run.
How about * build * run tests * build with `-fno-code -Werror` Maybe not quite what you want but could get you most of the way.
There's a way to get around that, but it is dark and paved with terrors. You would need a repository of libraries, with each commit (or version) generating a unique identification (probably through Sha or hashing of some sort). Every library that is built by combining other libraries would have a automatically generated "include" list that's inserted from your system at compile time (you'd "import..." like normal and it would have a file filled with "this library = this identification code" etc) After doing all that, you would completely standardize the haskell package install to be the same for all installs on that operating system. Then because of that, whenever you do load a program and compile it, it would check for the libraries you need, from the right spot, and if you were missing any, it would automatically download the correct versions and store them and then link and compile. A practically 100% stateless system. When a programmer updates his package, it would come with a new list of the libraries/packages etc, and doing something like "cabal install shiny new thing" would handle everything for you, and "cabal update" wouldn't break anything because you would have several existing versions of a library or package at any one time which would be automatically cleaned out by a garbage collector whenever a package was updated to a newer version. A system of this magnitude and complexity would almost completely remove "cabal hell", but like I said, such a path is dark and paved with terrors 
I use vim over emacs but I really do admire the emacs ghc-mod. I wish there was something like it for vim 
Don't quote me on this, but if I understand correctly it's because return () doesn't "return nothing" like it would in a language like C. It works a bit differently and because of that it's very dangerous to use any sort of return in haskell in a way that makes sense in C. (a do block in main in haskell with a return () will happily continue to the next part and completely ignore the return () line, for example) The other reason is because the if statement is saying "if the directory exists..." with the else being "do this if it doesn't exist". If the directory doesn't exist, you don't want it to do anything, but since IO is impure you can't determine the exact behavior of the function by looking at it, so the exact behavior of the else is undefined because if it wasn't undefined it would be defined and thus deterministic.
&gt; you probably shouldn't eagerly unwrap the ErrorT monad like that. I'm not convinced of this. It seems to me that one great use of monadic power is to allow the implementation to decide what 'features' it wants in order to be implemented in a clean way, while not requiring the caller to know or care. Or would you argue that all functions of type `Int -&gt; Maybe Int` should instead be `StateT Int Maybe ()`? In any order, it's not 'eager', it's 'because the return type wants it unwrapped'. If the return type was an ErrorT, the unwrap wouldn't have happened. And I'm not claiming this design is perfect, or even good, I'm just trying to clarify what I mean when I say "I want more magic from the compiler". And to answer your question, there's tons of other inhabitants of that type, for example `f _ = return (Right 3)`. But I don't think there are any other 'obvious' ones besides `runErrorT`, at least for the correct definition of 'obvious' :)
By the way, the `IO (IO a)` point is super interesting. I've definitely used values of that type in my code, but I don't even know how to express that concept in other languages, so I think I had a blind spot when thinking about how certain concepts from those languages should just port over easily. Is it even possible to express objects of that type in other languages? The best parallel I can think of is, in C++, a function that returns a function object--constructed ahead of time but waiting to be applied later.
Well, I think lazy is the good default, however I would like to be able to turn it on/off on the module level
These are also called "representable functors".
I just realised how weird Haskell looks without whitespace.
I'd like to just not have record syntax at all, replacing it with some kind of lens-based solution. Record syntax simply sucks at composing, but for the time being, if you want to cut down on dependencies when writing tutorials or the like, you have to use it.
People using it as a package manager and not a build tool
Here Gibbons seems to be more explicit about what a Naperian type means (page 43): http://www.cs.ox.ac.uk/jeremy.gibbons/publications/dgp.pdf 
http://comonad.com/reader/2013/representing-applicatives/
500 seconds??? That's pretty shocking.
https://ivanmiljenovic.wordpress.com/2010/03/15/repeat-after-me-cabal-is-not-a-package-manager/
You say you want to "incrementally update variables", in my view the state monad is often the best way to do this as it allows for composition. The problem with your implementation is the verbosity, we can simplify it by using lenses and some simple combinators: ifM :: Monad m =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a ifM c t f = do b &lt;- c if b then t else f whenM :: Monad m =&gt; m Bool -&gt; m () -&gt; m () whenM c a = do b &lt;- c when b a logic :: State (Int, Int) () logic = do whenM (cond x a) (x %= f) ifM (cond x c) (x %= g) (y %= h) x %= f whenM (cond y a) (y %= f) whenM (cond y b) (x %= g) where x = _1 y = _2 cond l pred = pred . view l &lt;$&gt; get runLogic :: Int -&gt; Int -&gt; (Int, Int) runLogic x y = execState logic (x, y) This code requires NoMonomorphismRestriction to compile.
First argument: It only manages the Haskell subset of dependencies: Irrelevant to "cabal hell". Second argument: Pretty much the same, that some dependencies cannot be automatically installed. Third argument: It only manages libraries, not executables. Which is again, pretty much the same argument. Fourth argument: Cannot remove packages. Irrelevant to "cabal hell". In short, the reasons why it is not a "package manager" have nothing to do with what people call "cabal hell".
Yes, Hindley-Milner is a nice example of an algorithm with terrible worst case complexity, but which works perfectly in practice. 
Of course I do. What will `cabal repl` do before it launchs an `repl`? How long will it take to start?
Looks like the default Ubuntu terminal to me.
Ah, that's it. Thank you.
What would instance Ord k =&gt; Apply (Map k) where pair :: (Map k a, Map k b) -&gt; Map k (a, b) pair = ... be defined as? (I find `pair` easier to think about than `&lt;*&gt;`; they're inter-definable. I'm not sure what the right name for it is.) I think the only reasonable definition is to do a kind of union where only those keys which exist in both input `Map`s exist in the output `Map`, with their corresponding values being tupled? (So, I guess, `unionWith (,)`, except `unionWith` isn't polymorphic enough.) What would the use case for this be?
They show up pretty early in Category Theory for the Working Mathematician, too.
&gt; I'm OK to throw away equational reasoning in a principled way I'm not sure you'll find many GHC developers that share that sentiment. 
&gt; Or would you argue that all functions of type Int -&gt; Maybe Int should instead be StateT Int Maybe ()? I would argue that if they use StateT internally, they shouldn't unwrap it, yes. `\v -&gt; State $ \x -&gt; runStateT v x' isn't free (I don't even think there's a RULES pragma to get rid of it.) so you shouldn't do it eagerly in case the consumer wants to use the monadic value. Again, if it might save the user a direct dependency on transformers, it might be worth the eager unwrapping. I'd say most of the `Int -&gt; Maybe Int` expressions I've written do not use StateT internally. &gt; In any order, it's not 'eager', it's 'because the return type wants it unwrapped'. Yes. I was saying you provided the wrong type for the function. I understand that your with syntax only inserted the `runErrorT` in order to have something that type checks. &gt; And to answer your question, there's tons of other inhabitants of that type, for example f _ = return (Right 3). But I don't think there are any other 'obvious' ones besides runErrorT, at least for the correct definition of 'obvious' :) If the answer is not unique, I don't want the compiler inserting it without **some** indication *in the source code* that I am requesting something ambiguous. As it currently stands, I have to choose the particular value, but it only costs &lt; 10 keystrokes when it is the most 'obvious' value.
Discussion goes to very ridiculous way. I was hoping to offer some help to the Haskell community, but what you are doing is treating me as one who is nitpicking the `cabal` tool. I can't find a right word to express my feelings right now. I was just wasting time doing this. Maybe you guys don't care anything about this. Or maybe you just don't want to see the prosperity of Haskell community. Or, the brilliant people have left r/haskell.
Can you give me an example of an actual `Foo`? I don't understand what you're getting at. Also, you effectively pointed out my `Foo` was wrong. It should have been data Foo ?a ?b ?c = Foo (?a Int) (?b Bool) (?c Foo ?a ?b ?c)
Thanks for posting this excerpt. Ever since [this stackexchange question on logarithm types](http://cstheory.stackexchange.com/questions/17006/what-is-the-logarithm-or-root-operation-in-type-space), I have been very curious about them, and what you quoted explains them much more clearly than stackexchange's accepted answer.
I'm afraid to say that although I really like darcs and really hate git I have made many contributions to github projects and zero to anything that uses darcs, simply because github is orders of magnitude more easy to use than any darcs facility.
As a person interested in installing haskell on a mac, can you point me toward what you consider the *best* way to install? EDIT: Having looked at GHC for Mac, that seems like a good bet. I have been on the fence about installing 64 bit because a Euterpea seems to have some issues but the cleanliness of that install seems worth the trouble. I should be able to uninstall simply if I end up needing 32-bit
See also the "Naperian Functors" part of this [this answer](http://stackoverflow.com/a/13100857) by /u/pigworker.
Lens is a bit complex, but can result in very nice code: http://lpaste.net/108742
 import Control.Lens import Control.Monad import Control.Monad.State a = (&lt; 10) b = even c = odd f x = x + 3 g x = x * 2 h x = x - 1 f3 x = x + 2 -- | Monadic if, specializes to ((a -&gt; Bool) -&gt; (a -&gt; b) -&gt; (a -&gt; b) -&gt; (a -&gt; b)) for Monad (a -&gt;). -- -- &gt;&gt; ifM a f g x == if a x then f x else g x ifM :: Monad m =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a ifM test t e = test &gt;&gt;= (\b -&gt; if b then t else e) -- Taking semi225599's version foo1 = (fooSt .) . (,) where fooSt = execState $ do x &lt;- gets fst when (a x) $ modify $ over _1 f x &lt;- gets fst modify $ over _1 $ if c x then g else h modify $ over _1 f y &lt;- gets snd when (a y) $ modify $ over _2 f y &lt;- gets snd when (b y) $ modify $ over _1 g -- Using (%=) foo2 = (fooSt .) . (,) where fooSt = execState $ do x &lt;- gets fst when (a x) (_1 %= f) x &lt;- gets fst _1 %= (if c x then g else h) _1 %= f y &lt;- gets snd when (a y) (_2 %= f) y &lt;- gets snd when (b y) (_1 %= g) -- Using use foo3 = (fooSt .) . (,) where fooSt = execState $ do x &lt;- use _1 when (a x) (_1 %= f) x &lt;- use _1 _1 %= (if c x then g else h) _1 %= f y &lt;- use _2 when (a y) (_2 %= f) y &lt;- use _2 when (b y) (_1 %= g) -- The Monad (a -&gt;) instance is maybe hard to understand -- but results in very terse code, using it now with ifM foo4 x y = execState (do _1 %= ifM a f id _1 %= ifM c g h _1 %= f _2 %= ifM a f id y &lt;- use _2 when (b y) (_1 %= g)) (x,y) -- Combining the _1 (%=)s foo5 x y = execState (do _1 %= (f . ifM c g h . ifM a f id) _2 %= ifM a f id y &lt;- use _2 when (b y) (_1 %= g)) (x,y) -- Moving part out of the State Monad foo6 x y = execState (do y &lt;- use _2 when (b y) (_1 %= g)) (f . ifM c g h . ifM a f id $ x, ifM a f id $ y) -- Moving the rest out of the State Monad, using lenses some more, inspired by cheeseburgerpizza a bit foo7 x y = (ifM (b . view _2) (_1 %~ g) id) (f . ifM c g h . ifM a f id $ x, ifM a f id $ y) -- I smell duplication! foo8 x y = (ifM (b . view _2) (_1 %~ g) id) (f . ifM c g h . fIfA $ x, fIfA y) where -- A more informative name would be better fIfA = ifM a f id
Yes, but my point was that it's not obviously exponential. In a linear scale the difference between an exponential function and a function that's merely polynomial with a very high exponent is not immediately obvious. In a logarithmic scale it's clear at once. I assumed the point about the plot was to demonstrate that we obtain the theoretically predicted exponential growth. Which is to say, this plot only tells me that it grows really fast, not that it grows exponentially. A logarithmic scale is the *natural environment* for an exponential function.
are lens tutorials the new monad tutorials?
Could explicit sharing of type sub-expressions alleviate this?
Surely not, because it's the type itself that's actually enormous, not some intermediate structure in its calculation.
A lens lets you see inside a burrito...
But the type itself could be represented with sharing, and then it wouldn't be enormous? Might need to postpone HM instantiation of polymorphic values to maintain the sharing.
In lens-y terminology you might call your `runVarState` function `zoomIORef`.
With sharing you can get HM to be just exponential, without sharing it's doubly exponential.
Leaving aside the question of how you represent the sharing syntactically, the types look like this 'a -&gt; ((('b -&gt; 'b) * ('c -&gt; 'c)) * (('d -&gt; 'd) * ('e -&gt; 'e))) * ((('f -&gt; 'f) * ('g -&gt; 'g)) * (('h -&gt; 'h) * ('i -&gt; 'i))) How does sharing help there?
So, lenses are straws?
For me, the noisy syntax of `modifyIORef' ref (+ 1)` is the issue. Completely ignoring lenses for a moment, it's of course trivial to make a `(+=)` operator for `IORefs`. If I have to write `runVarState refA $ id += 5`, then that's not really any more concise than the plain `modifyIORef` variant. And looking at most code I've written with IORefs, it's always just some simple type in it, so I would never benefit from being able to use a series of lens state combinators. I likely won't care about being atomic or maintaining any consistent state, otherwise I probably wouldn't have used a bunch of `IORefs`. But say I wanted all that, then for `TVar`s, I'd like to be able to write something like this: atomically $ do tvarA += 5 tvarB = 3 tvarC %= sin which is perfectly atomic and exception safe, but with the `runVarState` solution, I would look like this atomically $ do runVarState tvarA $ id += 5 runVarState tvarB $ id = 3 runVarState tvarC $ id %= sin which is not very different in terms of verbosity from just using `modifyTVar'`. I know the arguments about mutable state being intentionally verbose in Haskell, and I'm very new to using `lens`. I'm still on the fence and have little experience with this style of Haskell, just experimenting here and trying to make up my mind.
Not really, we've known that HM exhibits exponential behavior for certain pathological inputs for a long time now. The textbooks (TAPL) discusses this when introducing the algorithm. It's just that in practice it doesn't really matter.
&gt; we don't use type-directed identifier overloading No, but I'm kind of making such a language, in which this could happen. The function `fn :: Functor a -&gt; i` could, for example, be `length :: [a] -&gt; Int`. The compiler would not know what to do with `length [[1,2],[3,4]]`. Should it be `[Int]` or `Int` (mapped in the structure/applied on the structure). You could argue that the 'compiler-magic' could figure this out with the context (e.g. if n Int is needed or a List and choose the appropriate one; but that would be confusing) The semi-brackets is indeed a nice idea
[ ] gains interpretation too as 'for each element in the list'. You could thus write monadic code 'for each element/pair in the Map'. You could write the cartesian product of the map thus as: do (k1, e1) &lt;- dict1 (k2,e2) &lt;- dict2 return ( (k1,k2), (e1,e2) ) thus: `fn { 1 --&gt; 'a', 2 --&gt; 'b'} { 'x'--&gt; 42, 'y' --&gt; 5} = { (1,'x') --&gt; ('a',42), ... }`
Because no-one in their right mind would write exponentially long type signatures? :)
The talk he gave was attempting to be more reachable, adding a logarithmic scale would make the graph harder to understand, even if those who understood it could extract more information from it.
Uh, isn't proper quicksort over some ST or IO, since it sorts in-place, and thus still requires manual composition?
Yep.
To be fair this is a "Fantasy World Haskell" thread :P
I've been thinking about how to argue this point more effectively - since I agree that "i'm not smart enough for programming w/o types" is not effective. I think it would go something like this: Have you ever written some code that depended on the behavior or state of several other pieces of code? And then you need to change that code, but can't remember how it is "supposed" to interact with its dependencies? Functional programming reduces cognitive load by allowing you to encode otherwise implicit assumptions in the type system - assumptions like the side effects a function might have, what kind of values it takes and returns, and whether those values will always be present. When you make an assumption explicit, you can forget about it, and concentrate on the problem at hand. Referential transparency means you can concentrate on one piece at a time, instead of requiring you to maintain a mental model of the whole system. This means you can solve bigger, badder problems with a level mental effort that corresponds to the complexity of the function you're writing *right now*, not the complexity of the system as a whole. 
&gt; Moving to git might have caused a small improvement in the flow of contributions... Moving to git, or moving to github? (Not being snarky, just want to make sure I'm drawing the correct conclusions.)
reminds me of this DSL for c-like code in haskell http://augustss.blogspot.com/2007/08/programming-in-c-ummm-haskell-heres.html
Hmm, so how does this work? It seems that `Scheme3` should take four type variables, but you're only passing in two. 
yeah, i would like some tutorials that go into each of the different bits in `lens`. i understand most of the basic `.~` / `%~` / `^.` stuff but past that it's all a bit foreign, and while /u/edwardkmett's lovely diagram in the docs is great, its very dense and quite hard to get into
Putting them next to each other, I really like No 3, but No 1 &amp; 2 are rather similar, IMHO. atomically $ do atomically $ do atomically $ do modifyTVar' tvarA (+ 5) tvarA @: modify (+ 5) tvarA += 5 writeTVar tvarB 3 tvarB @: put 3 tvarB = 3 modifyTVar' tvarC sin tvarC @: modify sin tvarC %= sin The question for me then becomes, why integrate with the `lens` library through `MonadState` if the end result is neither as concise nor as composable as the other parts of `lens`. For instance, if I already give up composing like a normal `Lens` fucntion, why not simply have a typeclass like this: class Mutable mut where (@.=) :: mut a -&gt; a -&gt; m () (@+=) :: Num a =&gt; mut a -&gt; a -&gt; m () and so on. At least it has the concise syntax. For just dealing with plain `IORefs`/`TVars` this would be pretty much what I wanted. It of course also lacks composability, so no nice syntax to write to a `TVar` in a ReaderT environment or something like that. Is there any fundamental reason why we couldn't just write a `Lens` for modifying vectors and vars and refs etc.?
[v0] represents the set of variables that get created to instantiate Scheme1 one time. [v1] represents an identical set of variables but with different identities. Scheme2's [v0] and [v1] will each represent a single type variable. Scheme3's will each represent 2 type variables. Scheme4's will each represent 4 type variables.
I see. Is this a standard notation?
I mean shocking to me.
It does, yes! But in an ideal world, I'd like to not only have the efficiency and concise syntax which their DSL offers, but also the composability of a lens.
No, I just made it up :) I'm not sure if this saves the pathological case or not. /u/augustss said it is still exponential with sharing, so probably not.
...definitely. I was responding to this: &gt; Is it even possible to express objects of that type in other languages? so now I'm no longer sure about what it is we're trying to figure out.
YAY WE SOLVED IT Seriously, though, I don't think I've ever written a 'bind' in C++... it's probably way too ugly with the template syntax.
&gt; Is there any fundamental reason why we couldn't just write a Lens for modifying vectors and vars and refs etc.? I don't know that much about monadic lenses. I seem to recall reading a comment by /u/ekmett explaining how they don't satisfy many useful laws, and that's why they aren't a focus of `lens`. I'm sure someone more knowledgeable can clarify this. But more to my point, your `Mutable` class can be distilled down to one function: class Mutable m mut | mut -&gt; m where (@%=) :: mut a -&gt; (a -&gt; a) -&gt; m () Then `@.=` and `@+=` and anything else you want could be defined in terms of `@%=`. I would rather just reuse the combinators defined by `lens`, but that is the extent to which `lens` relates to this problem. I don't think lenses have anything to do with monadic references like `TVars`.
&gt; Dependent types. There are too many cases where you want stronger type guarantees, and the current situation is very ad-hoc. While we're at it, a Dynamic type that allows polymorphic values. This is not a small addition. A language should be developed as a DTL and not have "dependent types" tacked on afterwards. 
Came here to say that i like lens tutorials more as they arent decorated with all those weird metaphors most of the time
The big difference is that lenses are something that actually benefit from having tutorials. There are a *ton* of operators that do lots of things, and some exposure to each improves things. The `Monad` class, on the other hand, has two operations that are both really easy to understand once you have a grasp on higher-kinded types.
Most lens tutorials only explain the most basic operators, though. I'd like a tutorial on e.g. use cases of `zoom`, or how to use lenses with the state monad in neat ways. 
That's a good point, there's no reason for any of these other operators to be in the typeclass. In any case, thanks for all the clarifications and help. It seems `lens` just isn't the right hammer to wield here. I can see how monadic lenses could violate some of the lens laws, but I'd still love to have some abstraction to provide concise manipulation of state, even if it was a weaker than `lens`.
I would like everyone to be clear about the difference between "I wish Haskell was different in such-and-such a way" and "I think we should change Haskell to be such-and-such a way". Based on the name "Fantasy World Haskell" I see this post as a "safe haven" for the former without people caring about what the costs of those changes might be. Let's take an example suggested here: interchange `:` and `::`. I would even probably agree that it would have been better if the Haskell committee had retained the original ML meanings of these symbols. But I certainly don't think we should actually switch them, given where we are today, and I hope no one else does either. There are other examples in this discussion, some of which are quite popular. I invite you to find more! Of course some suggestions would be genuine improvements and it is just a matter of putting in the work to make them happen. Notably most tooling improvements (e.g., better debugging) are of this form, since they help everyone and don't break existing programs.
on the contrary, its very easy to get something basic working with `lens`, even if u dont use many of the combinators outside of `.~` &amp; `^.`, much like with monads and basic `do` / `&gt;&gt;=` stuff its the more advanced stuff like `Iso` and `Prism` and `Plated` that needs proper tutorials, like the docs say: &gt; The combinators in here are designed to be compatible with and subsume the uniplate API with the notion of a Traversal replacing a uniplate or biplate. ah yes, one of those old beasts. it's not the fault of the documentation (which seems to be fairly thorough from what i can tell), it's just that i have no idea why i would want to use a uniplate or a biplate or what they even do and it's this kind of thing that needs tutorials, not "ok here is a lens. u can use `view` and `set` to get stuff in and out" edit: &gt; with named operators even my rudimentary understanding of the library lets me know that there is some kind of order, and that operators like `&lt;.&amp;.=` would be completely useless if u had to actually name them. what are u going to call it? `bitWiseAndWithSetAndReturnResult`? 
i found this tutorial pretty good for learning about `zoom` and some neat uses of the state lenses http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html
&gt;I was introducing Haskell to a programmer I greatly respect, and he had the insight that in FP-land we have `map`, `filter`, `fold`, `unfold`, etc. But, he said, "in C we have all those concepts as well, and they all live under the same abstraction--the `for` loop." Yeah and who needs the concepts for, while and procedures when one can instead use goto for all that? &gt;When you have a `map` and you decide you need a `fold` too, suddenly you have to rejigger your whole computation to work with `mapAccumL` or something, And that is a very good thing! When you see a map you know a lot already. When you see a fold you know slightly less. When you see a for loop you know almost nothing about the nature of the loop without reading the whole thing. 
It's funny. Reading this list of requests really makes Idris look like Haskell, the next generation. Text is default, : v. ::, dependent types, strict by default with lazy types, etc.
I tent to use `error "TODO"` for that purpose.
Isn't functional purity all about avoiding to incrementally update variables? Even if the solution from **qZeta** is syntactically clean and concise, to me this seems a way to use Haskell in an anti idiomatic way. As for imperative approaches, i don't find that all this rebinding of x and y makes the code clear to read
the bits of the api that i *do* understand make a lot of sense (`&lt;` returns the new value, `&lt;&lt;` returns the old value), and im willing to bet that edward didnt just mash his face into the keyboard and come up with the other operators
Edward has done a fine job. He is a great dude. But I am judging the usefulness of a library independent of the reputation of its author, which I am free to do and should be expected to do.
&gt; the same abstraction--the for loop I don't consider the for loop an abstraction. I know there aren't for loops in x86 machine code, but a for loop just doesn't strike me as abstracting anything. You can have a monadic "for" if you want. (Scoping for variables declared in the init-part is problematic to implement without language support, but the rest is not so bad.) forC :: Monad m =&gt; m a -&gt; (a -&gt; m Bool) -&gt; (a -&gt; m a) -&gt; (a -&gt; m ()) -&gt; m () forC init cond upd body = init &gt;&gt;= loop where loop state = do condV &lt;- cond state when condV $ do body state update state &gt;&gt;= loop for :: (m a, a -&gt; m Bool, a -&gt; m a) -&gt; (a -&gt; m ()) -&gt; m () for (i, c, u) = forC i c u But, the "for" loop doesn't generalize the way fold/unfold/map do. Now, 90% of what a for loop is used for is fold/unfold/map and those do generalize, which is why FP long ago chose those actions to abstract/generalize. map to every (covariant) functor, fold and unfold to almost anything representable as a fixed point.
I guess I haven't used Ubuntu in a while. Thanks tho
I do `Data.Function.fix $ (0:) . scanl (+) 1`
Just saw this comment via your blog. Some examples would be useful, if it's possible for you to provide them. It's rather hard to understand in the abstract.
Which is indeed nice, and no problem. The problem rises when functions get automatically applied on the contents of functors, without explicit map. It would be the last feature which (might) give a problem.
Yep. And if they do, it will be linear in the input size when you include the type signature. 
Does this apply for new courses or only for existing academic licenses?
I can completely respect the fact that lens doesn't stratify nicely in a way that makes it easy to figure out what matters. This is something that we should do more to address in the lens documentation.
While I'd like the language to be [homoiconic](http://en.wikipedia.org/wiki/Homoiconicity), I'm pretty sure the "s" in "s-expression" stands for "Satan", as they are clearly the tool of the devil. I never did a lot of Lisp macros; is there really a significant advantage over type-safe TH?
wget -r http://www.gnu.org/software/wget/manual/html_node/Recursive-Download.html#Recursive-Download At least you'll get html. Then you can try to convert it. Pandoc will probably handle this, if not there is phantomjs (headless browser). http://phantomjs.org/screen-capture.html 
We are always open to suggestions that will benefit the community. Please send a note to support@fpcomplete.com and we will be happy to discuss this with you. 
&gt; /u/augustss said it is still exponential with sharing, so probably not. That's because the example in the article is not the most pathological case. Instead consider this one: :t let f0 x = (x,x) in let f1 = f0 . f0 in let f2 = f1 . f1 in let f3 = f2 . f2 in f3 The result type of `fn` is a complete binary tree of depth 2^n , so the total size of the tree is doubly exponential and even with sharing you still have to represent one "spine" of the tree of length 2^n .
&gt; There are a ton of operators that do lots of things, and some exposure to each improves things. Well, in `lens` -- lenses at their base are just get and set.
What do you think of the `runVarState`/`zoomIORef` solution proposed by `cdxr`?
We could take foo to be an endomorphism on a pair of X's, and then factor it into smaller endomorphisms: type X -- = Y because f is called with an X and a Y. type MyState = (X, X) -- Input and output type of foo. f :: X -&gt; X g :: X -&gt; X h :: X -&gt; X a :: X -&gt; Bool b :: X -&gt; Bool c :: X -&gt; Bool -- x = f(x) if a(x) transform1 :: MyState -&gt; MyState transform1 (x, y) = if a x then (f x, y) else (x, y) -- if c(x): -- x = g(x) -- else: -- x = h(x) transform2 :: MyState -&gt; MyState transform2 (x, y) = if c x then (g x, y) else (h x, y) -- x = f(x) transform3 :: MyState -&gt; MyState transform3 (x, y) = (f x, y) -- y = f(y) if a(y) transform4 :: MyState -&gt; MyState transform4 (x, y) = if a y then (x, f y) else (x, y) -- x = g(x) if b(y) transform5 :: MyState -&gt; MyState transform5 (x, y) = if b y then (g x, y) else (x, y) foo :: MyState -&gt; MyState foo = transform5 . transform4 . transform3 . transform2 . transform1 
Because coordination can get more done than fragmentation.
Wow, nice MS shill crew we've got here.
This thread was a good idea, but a little poorly executed (social media is very fickle). I did not down vote the thread, but I can tell you why I did not up vote it. * It used an emotive term that could annoy the cabal developers. * You tied your comment to the post, so I could not up vote the thread (which I might have) without up voting your comment (which I would not have, because I did not understand it). * The two list thing made it hard to follow your explanation. Perhaps one comment for each issue would have been better. It would make it possible for people to up vote the ones they felt were important and would have made the resulting thread easier to follow. * There was not enough detail in your explanations. Links to the issues in the cabal issue tracker might have helped. * You tried to start a "discussion", then disappeared for 4 hours. In that time a perfectly reasonable response (but one based on a misunderstanding of your issue) was the top comment. This made the thread seem like it was a dead end. Please don't let the failure of this thread discourage you.
It is a very good solution to an annoying problem, it doesn't make a lens, but it has the benefit of being legal and possible. =)
`where` has a bit different scoping than `let`. e.g. It closes over multiple guards. On the other hand `let` can be used under a lambda.
Lenses aren't just about records, you know.
Lens being the 'new monad' tutorial or not, probably not enough merit is given to the author of this piece who has basically put himself out there to be judged. I liked the writing style and pacing of his tutorial. We could certainly do with more competent beginner/intermediate content; which the consensus is that we are sorely lacking. Personally, when I was a beginner - even when I thought of moving backwards in my understanding at least I was moving. At the intermediate stage seems like I make little to any progress. The only benchmark that I have is that I find myself slowly starting to understand more of ekmett's work. Take that for what you will, but I am curious as to the fact of if this is the point when someone with a formalized classroom may leap ahead to advanced intermediate/expert. While self study is spins the wheels at an earlier stage for longer. Or I could be nuts.
Well, my understanding of bang patterns is that foo !x = g x is equivalent to the following. foo x = x `seq` g x For bang patterns in datatype declarations, my understanding is that data Foo = MkFoo !Int will create a `MkFoo` constructor which, like `foo`, is strict in its first argument. Since the only impact (as far as I know) of the bang pattern on a datatype is to make a function (the constructor) strict, I think that your `FunctionLike` trick for strictness-polymorphic functions should be enough to simulate strictness-polymorphic datatypes: [Proof of concept #2](https://gist.github.com/gelisam/68a79e4de6c61935ce96) This `FunctionLike` trick is great, I like this proof-of-concept much better than my earlier `NFData`-based version! Thanks for sharing.
That actually makes sense to me. Whenever I've looked at the Lens operations I've always been so overwhelmed, that I never got anywhere. Having a starting point, and a nice heirachy of importance would be amazing!
This looks like a great start. We are currently using grunt for static assets that don't need Haskell values inserted into them (Haskell value insertion is down with shakespeare and needs to be done by compiling the application). One thing we do with grunt is compile many typescript files down to a couple javascript files and run jshint on that, which produces a final static file that can be served up. This is based on watching for changes in files rather than a browser request which would need to understand the many-to-one dependency chain and also would create a delay. Similarly, we compile multiple less files down to one css file.
&gt; While I'd like the language to be homoiconic, I'm pretty sure the "s" in "s-expression" stands for "Satan", as they are clearly the tool of the devil. Why? &gt; I never did a lot of Lisp macros; is there really a significant advantage over type-safe TH? Yes, Lisp macros are simpler to write and use of them is transparent. The `let` syntax looks like this in Lisp: (let ((a 1) (b 2)) foo) So similarly `case` could look like this: (case foo ((1 1) (n (+ (fib (- n 1)) (fib (- n 2)))))) Now, if I want to produce lambda case, I just write a new macro for `case` that accepts either two arguments or a single argument. In the case of a single argument, it returns a lambda. Now I can write: (case ((1 1) (n (+ (fib (- n 1)) (fib (- n 2)))))) In Haskell, I am unable to extend the syntax of `case` itself without modifying the compiler. Thus everyone had to wait years to get it. Or if I wanted `if` syntax for which you could omit the first argument. `(if cond true false) :: a`, but then `(if true false) :: Bool -&gt; a`. And you could have an additional normal `if` function which could be passed around first-class, unlike normal Haskell (no keyword parse errors). If you write a Template Haskell macro, what you have to write is a complete eye-sore, a quasi quoting macro which is far worse than normal Lisp macros, because it's what Lisp calls a "reader macro", it reads arbitrary strings in: `[case|…|]`. A Lisp macro `(foo …)` is akin to writing: $(foo [|…|]). They accept a simple syntax tree, but they are also homoiconic with functions and special operators (if, case, let, data, module, etc). The "type-safe" part of Template-Haskell just means that the tree you construct has to be correct, it's easy to generate badly typed code or code that references names that don't exist and will produce an error when the macro is used rather than when it is defined, just like in Lisp. There's no reason a Lisp couldn't have more restrictive and type-safe macros. The key innovation is cheap quotation, `'if`, `'(module then)` and cheap macros `(foo bar)`.
OCaml has limited disjunctions, like for a vowel matcher, 'a' | 'e' | 'i' | 'o' | 'u' -&gt; True, but disallows variables within them. 
I like how the validation happens through ```Maybe```, though the logic should not be mixed with markup for the view. I also like that Yesod will load a template for the view from ```defaultLayout```. The types are used to make web development a bit safer, though the first tools I would have reached for would have been javascript, html, and ajax.
It gives you the ability to map applicative side effects over functor. I see this as a good abstraction. AWK is a DSL for text processing and Haskell is a general purpose programming language. Not quite comparable. If you write your own special-purpose DSL in Haskell, you might be able to get even more power than AWK. Maybe you can find [hawk] (https://github.com/gelisam/hawk) interesting. I see we used similar paths to learning Haskell, more reading than coding. Even though reading Haskell stuff can be very interesting, but in my experience, if you revers it to more coding and less reading, you might find that the learning curve is less steep and various concepts start falling into place much easier.
If there's not enough evidence either way why is OP on negative votes in a thread that's purposely unrealistic? 
&gt; my understanding of bang patterns is that ... Yes that's right. &gt; the only impact (as far as I know) of the bang pattern on a datatype is to make a function (the constructor) strict Ah right, I think you're on to something here! I think your proof of concept works very well. This is great! So maybe we *can* do strictness polymorphic functions and data in Haskell. Wow. I'm going to have to write up a post about this.
In this case, I mixed the markup with the logic for the sake of simplicity. In a real application it would be separated.
That code looks good to me. You may also prefer using [lookupPostParam](http://haddocks.fpcomplete.com/fp/7.4.2/20130829-168/yesod-core/Yesod-Core-Handler.html#v:lookupPostParam), which would allow you to have completely free-form values on your button, which might make it easier to let this technique scale to many different submission buttons. One thing that happened to catch my attention in the blog post: &gt; Yesod encourages the programmer to use the same path for the form action as for the form itself. Two questions: 1. Why do you say "Yesod encourages" this? It's certainly true that that's how I write my examples, I'm just curious if there's something in the API that pushes you in that direction. 2. Are there other frameworks that push for a different approach and, if so, what are they? I always like hearing feedback on these kinds of things, *especially* from people just starting out with Yesod.
I think the problem here is your use of the word "correct". There are different degrees of correctness people probably won't thing of "bug-free" when they hear it. Also saying "people with that mentality are either wrong..." doesn't help I think. If you had said "I do not think it's possible to write bug free software, unless it's not very complex" then your comment would not have been taken the wrong way. When I read it I had a similar reaction to the person your replying too. 
I wish I could actually get yesod to install. Looks really cool.
In Haskell, function application associates to the left, so `get name anAthlete` is exactly equivalent to `(get name) anAthlete`.
Thank you for your hint, i will try out *lookupPostParam*. &gt; Why do you say "Yesod encourages" this? It's certainly true that that's how I write my examples, I'm just curious if there's something in the API that pushes you in that direction. The API does not push me in this direction at all. Indeed, it is possible to write any kind of form action. However, it is quite reasonable to do it like this as long as there isn't anything that prevents you from this. &gt; Are there other frameworks that push for a different approach and, if so, what are they? I don't know of any. 
I'm not familiar with Heroku, but is there a way to initially build locally and then push the cached build artifacts to the the remote build environment?
Cool, thanks for the clarifications!
By the way, what does GHC use for type inference these days, [this thing](http://www.haskell.org/haskellwiki/Simonpj/Talk:OutsideIn)? Is whatever they use susceptible to the problems in this post?
Disregard packages and modules, let's have dependencies on the fragment level. The units of distribution and compilation would also be individual fragments, (maybe) drastically reducing compile time. Dependencies would be immutable so that you not only get reproducible builds but reproducible behavior. No need to rewrite that small helper function because you don't want to incur a dependency on that huge package containing it. No need to carefully think about whether to include that rarely used function in your package, just submit it into the giant pool of haskell fragments. Want to see examples of a function being used? Just follow the reverse dependencies of that function. A "package" is then a document explaining how a set of fragments can be used together.
Put this way, I get your point. &gt; I would like everyone to be clear about the difference between "I wish Haskell was different in such-and-such a way" and "I think we should change Haskell to be such-and-such a way". ...the previous comment itself wasn't clear to me about this distinction. :) (I.e. it can just as easily be read as "what a bunch of awful ideas", as as "what a bunch of nice, but completely impractical ideas".) &gt; But I certainly don't think we should actually switch them, given where we are today, and I hope no one else does either. I don't think anyone was seriously proposing it either.
Thanks! When I was testing, one of the most interesting things I saw was a tidbit like this: &gt; left, left, right, left, left, left, left, left, right, left, ... It was really interesting because "left," showed up more often, and I realized that this is because of a line in "Soldier," which mimics the classic Army-chant "left, left, left, right, left." If you shove this chant into a markov chain you've got a 100% chance of going from "right," to "left," and a 75% chance of going from "left," to "left," so "right," didn't show up as often. Since those words don't show up in many other places, the simulation got stuck and spit out something like that. It might happen one of these days on @_rapcandy.
I just installed it on Saturday. What problem are you having?
Tried it on both a mac and pc. Both times on installing cabal gave me different dependency issues.
Ah, I did it on linux; I can't speak for either other platform.
Linux will be my next attempt
&gt; Are there other frameworks that push for a different approach and, if so, what are they? For rails if you use the RESTful approach (which they do push), and want to create a new thing then the form would normally be served on the /posts/new page, and send a POST to /posts. If you edit the user then it'd be /posts/:id/edit and POST to /posts/:id Of course you can easily do this completely different, but these are the default urls for an unsaved Post and an existing Post if you use the form_for helper. 
Yes. Since regular HM typability is a subset of what ghc supports, and there's a proof that HM typability is dexptime-complete. So it doesn't matter what algorithm they use; the complexity is a property of the problem.
Oh it is a very good piece. I think that it should be able to stand for quite some time. If asked I would certainly have no problem in directing someone to that page as an introduction to Lens.
Thank you! That's exactly what I was going for. While working on [the issue](https://github.com/tfausak/strive/issues/44#issuecomment-47606662) that inspired this post, I said: &gt; ... I think I accidentally stumbled upon lenses.
&gt; Why? Really just personal preference, I think. Even though I liked writing the little scheme that I did in college, and I used s-expressions for my first interpreter, I've always found that their syntax is harder to read "in the large". They suck you in with their consistency, but once expressions get too deep or too many, that consistency turns into a maze because there are no "landmarks" to guide your way. &gt; In Haskell, I am unable to extend the syntax of case itself without modifying the compiler. Is that not also true of Lisp "special forms"? &gt; The "type-safe" part of Template-Haskell just means that the tree you construct has to be correct, it's easy to generate badly typed code or code that references names that don't exist and will produce an error when the macro is used rather than when it is defined, just like in Lisp. I'm pretty sure we got a new type-safe template Haskell last year that was really type-checked to avoid splicing errors. [[source](http://www.haskell.org/pipermail/ghc-devs/2013-May/001255.html)] &gt; Lisp macros are simpler to write and use of them is transparent. Honestly, I'm not sure this is an advantage. It's good to have markers in the code that indicate (1) this is something specific to this code base (2) what and preferably where to look for the definition. It makes it easier to add / change team members that are working on a coed base.
I know, but I wanted to write a tutorial that didn't over-generalize too fast. I think the posts I linked at the bottom do a good job picking up where I left off. 
&gt; once expressions get too deep or too many, that consistency turns into a maze because there are no "landmarks" to guide your way. For what it's worth, Lispers use indentation to make landmarks. Parents always precede their children, unlike in Haskell. I personally find Lisp far easier to read than Haskell which is far more liberal in how code can be laid out, and people regularly take advantage of that (sadly). &gt; Is that not also true of Lisp "special forms"? You can't redefine `if` in Common Lisp. In Emacs Lisp, you can do so. I'm not sure about Scheme. &gt; I'm pretty sure we got a new type-safe template Haskell last year that was really type-checked to avoid splicing errors. Yeah, I'm aware of that research but I don't think it's in GHC yet. &gt; It's good to have markers in the code that indicate (1) this is something specific to this code base (2) what and preferably where to look for the definition. It makes it easier to add / change team members that are working on a coed base. That's a question of taste, I think. Perl programmers like sigils, I don't. Haskell's laziness means a function may or may not evaluate its arguments. Defining new functions is defining new control structures. Lisp's macrology isn't much different. Your editor can tell you what something is and where it comes from. The benefit of Haskell is that laziness is generally more composable, descriptively typed and first-class than macros, but when you want to solve a problem syntactically, laziness is insufficient and that's where Haskell having Lisp macros would be good.
If others are interested, I'll probably write a few more playing around with various trees -- finger trees, search trees, expression trees, etc.
I like your post, but it parlays knowledge about zippers into knowledge about lenses. I haven't dug into zippers yet, so your post had the effect of teaching me zippers through lenses. On top of that, I think it targets intermediate/advanced Haskellers. Take this pseudo code for example: _l f m = (\a' -&gt; [a'/a]m) &lt;$&gt; f a There's a lot of syntax to unpack there. I understand what it's saying, and it's definitely easier to understand than [template Haskell](https://github.com/tfausak/strive/blob/6121f29/library/Strive/Internal/TH.hs#L101-L104), but I wouldn't expect someone new to Haskell to get it.
Yes please!
Some time ago I wrote a script to scrape the wiki and produce a directory of markdown files. This was before pandoc had a mediawiki reader, so there might be a better way now, but at the time this worked well. https://github.com/jgm/hw2gitit 
Oh, I don't think so.
Someone new to Haskell shouldn't be using lenses. :P
Did you pass the `max-backjumps` and `reorder-goals` flags? As per the [Quick Start guide](http://www.yesodweb.com/page/quickstart), you have to use the following command to install it. cabal install yesod-platform yesod-bin --max-backjumps=-1 --reorder-goals 
Good point, I guess I should have left out that first sentence. You may have gotten to the core of the issue here. When I think "correct", I think "no bugs", because a bug is by definition a place where a program is incorrect. Apparently some people view "correct" to mean "mostly works as intended".
 The examples are of limited utility. The problem is not a few bad apples or a few bad words; were that the case it would be easier to address. The problem is a subtle one: it's in the tone and tenor of conversation, it's in the things *not* talked about, in the implicitization of assumptions, and in a decentering of the sorts of communities of engagement that Haskell was founded on. Back in 2003 and 2005, communities like Haskell Cafe were communities of praxis. That is, we gathered because we do Haskell, and our gathering was a way to meet others who do Haskell. Our discussions were centered on this praxis and on how we could improve our own doing of Haskell. Naturally, as a place of learning it was also a place of teaching— but teaching was never the goal, teaching was a necessary means to the end of improving our own understandings of being lazy with class. The assumptions implicit in the community at the time were that Haskell was a path to explore, and an obscure one at that. It is not The Way™ by any stretch of the imagination. And being a small community it was easy to know every person in it, to converse as you would with a friend not as you would online. Over time the tone and nature of the Cafe changed considerably. It's hard to explain the shift without overly praising the way things were before or overly condemning the shift. Whereas the Cafe used to be a place for people to encounter one another on their solitary journeys, in time it became less of a resting stop (or dare I say: cafe) and more of a meeting hall. No longer a place to meet those who do Haskell, but rather a place for a certain communal doing of Haskell. I single the Cafe out only because I have the longest history with that community, but the same overall shift has occurred everywhere I've seen. Whereas previously it was a community of praxis, now it is more a community of educationalism. In the public spaces there is more teaching of Haskell than doing of it. There's nothing wrong with teaching, but when teaching becomes the thing-being-done rather than a means to an end, it twists the message. It's no longer people asking for help and receiving personal guidance, it's offering up half-baked monad tutorials to the faceless masses. And from tutorialization it's a very short path to proselytizing and evangelizing. And this weaponization of knowledge always serves to marginalize and exclude very specific voices from the community. One class of voices being excluded is women. To see an example of this, consider the response to Doaitse Swierstra's comment at the 2012 Haskell Symposium. Stop thinking about the comment. The comment is not the point. The point is, once the problematic nature of the comment was raised, how did the community respond? If you want a specific example, this is it. The example is not in what Swierstra said, the example is in how the Haskell community responded to being called out. If you don't recall how this went down, [here's the reddit version](http://www.reddit.com/r/haskell/comments/zxmzv/how_to_exclude_women_from_your_technical/); though it's worth pointing out that there were many other conversations outside of reddit. A *very* small number of people acquitted themselves well. A handful of people knew how to speak the party line but flubbed it by mansplaining, engaging in flamewars, or allowing the conversation to be [derailed](http://www.derailingfordummies.com/). And a great many people were showing their asses all over the place. Now I want you to go through and read every single comment there, including the ones below threshold. I want you to read those comments and imagine that this is not an academic debate. Imagine that this is *your* life. Imagine that *you* are the unnamed party under discussion. That *your* feelings are the ones everyone thinks they know so much about. That you personally are the one each commenter is accusing of overreacting. Imagine that you are a woman, that you are walking down the street in the middle of the night in an unfamiliar town after a long day of talks. It was raining earlier so the streets are wet. You're probably wearing flats, but your feet still hurt. You're tired. Perhaps you had a drink over dinner with other conference-goers, or perhaps not. Reading each comment, before going on to the next one, stop and ask yourself: would *you* feel safe if this commenter decided to follow you home on that darkened street? Do you feel like this person can comprehend that you are a human being on that wet street? Do you trust this person's intentions in being around you late at night? And ask yourself, when some other commenter on that thread follows you home at night and rapes you in the hotel, do you feel safe going to the author of this comment to tell them what happened? Because none of this is academic. As a woman you go to conferences and this is how you are treated. And the metric of whether you can be around someone is not whether they seem interesting or smart or anything else, the metric is: do you feel safe? If you can understand anything about what this is like, then reading that thread will make you extremely uncomfortable. The problem is not that some person makes a comment. The problem is that masculinized communities are not safe for women. The problem is that certain modes of interaction are actively hostile to certain participants. The problem is finding yourself in an uncomfortable situation and knowing that noone has your back. Knowing that anyone who agrees with you will remain silent because they do not think you are worth the time and energy to bother supporting. Because that's what silence says. Silence says you are not worth it. Silence says you are not one of us. Silence says I do not think you are entirely human. And for all the upvotes and all the conversation my previous comment has sparked on twitter, irc, and elsewhere, I sure don't hear anyone *here* speaking up to say they got my back. This is not a problem about women in Haskell. Women are just the go-to example, the example cis het middle-class educated able white men are used to engaging. Countless voices are excluded by the current atmosphere in Haskell communities. I know they are excluded because I personally watched them walk out the door after incidents like the one above, and I've been watching them leave for a decade. I'm in various communities for queer programmers, and many of the folks there use Haskell but none of them will come within ten feet of "official" Haskell communities. That aversion is even stronger in the transgender/genderqueer community. I personally know at least a dozen trans Haskellers, but I'm the only one who participates in the "official" Haskell community. Last fall I got hatemail from Haskellers for bringing up the violence against trans women of color on my blog, since that blog is syndicated to Planet Haskell. Again, when I brought this up, people would express their dismay in private conversations, but noone would say a damn thing in public nor even acknowledge that I had spoken. Ours has never been a great community for people of color, and when I talk to POC about Haskell I do not even consider directing them to the "official" channels. When Ken Shan gave the [program chair report](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/haskell2013/) at the Haskell symposium last year, there was a similarly unwholesome response as with Swierstra's comment the year before. A number of people have shared their experiences in response to Ken's call, but overwhelmingly people feel like their stories of being marginalized and excluded "don't count" or "aren't enough to mention". Stop. Think about that. A lot of people are coming forward to talk about how they've been made to feel uncomfortable, and *while telling those stories* they feel the need to qualify. While actively explaining their own experiences of racism, sexism, heterosexism, cissexism, ablism, sanism, etc, they feel the simultaneous need to point out that these experiences are not out of the ordinary. Experiencing bigotry is so within the ordinary that people feel like they're being a bother to even mention it. This is what I'm talking about. This is what I mean when I say that there is a growing miasma in our community. This is how racism and sexism and ablism work. It's not smacking someone on the ass or using the N-word. It's a pervasive and insidious tone in the conversation, a thousand and one not-so-subtle clues about who gets to be included and who doesn't. And yes the sexual assaults and slurs and all that factor in, but that's the marzipan on top of the cake. The cake is made out of assuming someone who dresses "like a rapper" can't be a hacker. The cake is made out of assuming that "mother" and "professional" are exclusive categories. The cake is made out of [well-actuallys and feigned surprise](https://www.hackerschool.com/manual#sub-sec-social-rules). And it works this way because this is how it avoids being called into question. So when you ask for specific examples you're missing the point. I can give examples, but doing so only contributes to the errant belief that bigotry happens in moments. Bigotry is not a moment. Bigotry is a sustained state of being that permeates one's actions and how one forms and engages with community. So knowing about that hatemail, or knowing about when I had to call someone out for sharing titty pictures on Haskell Cafe, or knowing about the formation of #nothaskell, or knowing about how tepid the response to Tim's article or Ken's report were, knowing about none of these specifics helps to engage with the actual problem.
&gt; What I infer from this rhetoric is that Haskell programmers have an idea of what “correct” means which is so wildly divergent from what the rest of the (wildly growing, profitable, multi—bajillion dollar) industry thinks “correct” means... The word "correct", especially the way you used it, has a very binary meaning. But program correctness (i.e. number of bugs) is a very fuzzy thing. IMO it makes no sense (regardless of whether the rest of the industry agrees or not) to collapse the continuum to a true/false value unless true = 0 bugs and false = &gt; 0 bugs.
Yeah that's the guide I was following, but it just gave "fatal errors"
That will likely help you out, a lot. Yesod (like many larger applications) has a lot of fairly strict dependencies which are likely to conflict with your usual package tree if you install anything nontrivial outside of a sandbox. Hope it helps! :)
I would really like to understand more general ways for taking the free type on a signature and cutting it down by laws. I walked through a similar process for Coyoneda a bit ago (https://lobste.rs/s/pz8rkp/abstractivate_from_lists_to_trains_to_functors/comments/f5sunm#c_f5sunm) but as far as I can tell right now it's always pretty ad hoc.
Cool I'll give that a try tonight, thanks a lot!
I just installed Ubuntu 2014.4 and HP 2013.2, and I couldn't get cabal up to 1.18 and so I couldn't install the sandbox function. I tried both stackage and hackage. And of course once one runs 'cabal install' once, any further attempts are doomed by the greedy version choices made in the first iteration.
Just forget about it until someone writes a post walking through how to install yesod on HP 2014.2 on a TBA stable stackage version. Anything else is a waste of time and will ruin your day and night.
No, but analogies are strawmen.
`cabal install cabal-install` is perfectly safe. Everything after that should be sandboxed, though.
I think there's a lot more to Haskell than non-strict by default. I don't think it's fair to say that laziness make Haskell Haskell. 
I'm using yesod extensively several years. But i've never used yesod formlets. And now writing our major applications with angularjs there's even less use (if any) for them. I think web development in general goes away from old "refresh entire page after each click" model of development. 
Nice title, interesting and to-the-point content, but I think there is a bogus argument in the conclusion (which doesn't affect the rest of the article): &gt; We started from a simpler algebraic object (a magma instead of a monoid), translated it directly, did no additional work, and got trees. Compare this with the several steps needed to derive lists from the definition of a monoid, and I think you'll agree that trees are actually simpler than lists! Next time you are choosing a data structure for your tutorial, paper, or project, if simplicity is your primary motivator, choose trees. I would interpret what you say in the following way: if a structure has no equational laws, only generators, it is simpler, and it is better to use it for simplicity. I disagree because quotienting over the good equivalences is often what brings simplicity -- or at least correctness, but what is simple if not "easily correct"? Would you advocate using the type-class Wrongnad, which has the same operations as Monad but no monadic laws, because it is simpler?
Please do!
This free magma also lacks a "zero", blogger didn't really mention this difference in his overview, just that their binop doesn't have to be associative. Also, the canonical representation of a tree as I understood it was `Tree t = Leaf | Tree t [Tree t]` (incidentally this depends on list), this "free magma" specifically encodes non-empty binary trees. EDIT: Oh, I see it now! `FreeMagma (FMAtom t)` works just fine, and arbitrary trees can be mechanically translated into these binary trees. So we wind up with `O FreeMagma FMAtom` being something that generalizes `FreeMonoid`.
This to me seems like a Functional Reactive Programming problem. We've got a couple event streams (A, B, C, D, E) as input, and a couple actions we want to take ( `pumpOn`, `pumpOff`, `alarmOn`, `alarmOff`).
I'd argue that lenses are so useful, Haskellers of all skill levels should be using them. :)
Adding to the choir ... Yes please :D Great article! Edit: fix typo.
It's worth noting that the absence of state isn't necessary; just the absence of **impure** state. It's easy enough to use the `State` monad once you get the hang of it. Then the computation might take a form similar to: Monad m =&gt; StateT PumpState m a Where `PumpState` houses the state of your sensors, `m` is some underlying monad that can handle raising alarms and pumping water (in whatever form that takes), and `a` handles return values for any subcomputations you may use. When I'm presented with stuff like this with a lot of implicit state involved, I typically flesh it out using `StateT` (or `ReaderT`) on a first pass. If I realize an abstraction along the way, or that I'm not using the `State`/`Reader` combinators effectively, I refactor it. But keep in mind that you *can* perform stateful computations in Haskell, you just need to be explicit about *what that state is*.
Thanks a lot for clarifying my doubts.
&gt; A semigroup is simpler; they are almost monoids but do not require the neutral element. A magma is simpler still; they are almost semigroups but do not require the operation to be associative. So, I do mention the lack of a zero in a general magma. It's easy to miss, though. &gt; our first representation had some problems. But they were caused by two things: the neutral element and associativity. When moving from discussing monoids to semigroups, we lost the neutral element, so there's no work to do there. When moving from discussing semigroups to magmas, we also lost associatity, so there's no work to do there. In fact, this is a perfectly valid free magma. I specifically mention here that we don't have a zero/neutral element so we don't have to worry about it causing canonicity problems. &gt; Also, the canonical representation of a tree as I understood it was `Tree t = Leaf | Tree t [Tree t]` This is rose trees, which are pretty general; they are also really easy to deal with if you are already used to lists. I was talking about "binary trees with values of type a at the leaves", but I do often just shorten that to "trees". Free magmas don't have a "zero" element, so `FreeMagma a` is a non-empty container. If you want to be able to represent empty containers, you'll need a Maybe wrapper or something similar.
Oh, I agree, it makes no sense. But too may Haskell/FP advocates seem to partition programs into strongly–statically–typed and bug–ridden. Actually, “bug” is not a term that professionals should be using. A more fruitful model is that system builders make a number of *errors* which injects a certain number of *defects* into a system, which users (and testers) might or might not see cause a *failure* at run–time. It's not clear to me which part of this fans of type–full programming are approaching when they talk about writing “correct” to be just this impossibly hard intellectual challenge (or else what you're doing is “trivial”) in anything other than Haskell. Since programs do exist which, I assert, do non–trivial things and have a number of observed failures as near to zero as makes no difference, but are written in Java and C and even assembler, I struggle to parse the claim as anything other than naïve and grandiose.
FRP is the way to go here. It lets you group events like the sensors and make things like the SumpProbe of the article. If you can't use an FRP library, you could just fake one: you need to make a datatype `Event` which can be run like `listen :: Event a -&gt; ((a -&gt; IO ()) -&gt; IO ())` or something similar, has a `foldp` to put states, and is an instance of `Functor` and `Applicative`.
I did it ad-hoc. The general technique for recognizing that your representation needs to be cut down is having two things that are equal according to the laws that have different representations. Usually, that's as easy as writing your rules down as Haskell (or your favorite pure language) expressions. But, the particular refactoring you have to do once you recognize such an inconsistency is varied. The two techniques I used here I probably learned in (or before taking) the number theory course from my college years. Forcing some arguments to be "small"/atomic is like dealing with the case of a prime number and forcing left/right association is similar to dealing deciding on a particular prime factorization (large-to-small or small-to-large). Identifying a "countable" characteristic that is the only thing that varies between should-be-equal representations, and then collapsing all the "countable" variations is... Well, I can't remember another example of the technique, but I've seen it elsewhere.
So let's assume we've got the following: data Sensor = {- ... -} sensorA :: Sensor {- ... -} sensorE :: Sensor checkSensor :: Sensor -&gt; IO Bool pump :: Bool -&gt; IO () raiseAlarm :: IO () Now let's start encoding the business rules. We'll start with raising the alarm, as that's the simplest: checkAlarm :: IO () checkAlarm = do sensors &lt;- sequence (fmap checkSensor [sensorA, sensorB, sensorC]) when (any id sensors) raiseAlarm There's probably a better way to do the twiddling, but there you go. If any of sensors A-C go off we raise an alarm. Next up we'll make a simple action to turn off the pump if methane is too high: guardPump :: IO () guardPump = do sensor &lt;- checkSensor sensorB when sensor (pump False) Easy enough. Now we need our loop. This problem calls for a simple state machine, so we'll make actions for each state. state1 :: IO () {- When both sensors are off -} state1 = do pump False checkAlarm sensor &lt;- checkSensor sensorD if sensor then state2 else state1 state2 :: IO () {- When both sensors are on -} state2 = do pump True guardPump checkAlarm sensor &lt;- checkSensor sensorE if (not sensor) then state1 else state2 That's my first-blush attempt at the problem. From here I'd go back and refactor to make things nicer and remove redundancy. Turning the pump on and then turning it off immediately if the methane's too high looks especially ugly, so I'd definitely go back and fix that. I basically just wrote as I solved, so hopefully that helps give some insight. A lot of comments are mentioning FRP, which would definitely be a good way to model the problem if/once the business rules start getting especially hairy, but for me with these kinds of simpler problems I prefer to stick to basics. And for someone still trying to wrap his/her head around solving problems functionally looking into FRP is just going to further confuse you. Gotta learn to walk before you learn to run, and all.
Much of Guy Steele's work here pertained to a desire to be able to parallelize calculation. This is a laudable goal. The main issue with a naïve magma approach Steele proposed for Fortress is that you have zero guarantees about efficient splittability. All the mass of your magma could be on one side or the other. The benefit is that without those guarantees infinite magmas make sense in a lazy language. You can have infinitely large trees just fine, that go off to infinity at any point not just at the right. This has a certain pleasing structure to it. Why? Well, lists aren't really the free monoid if you allow for infinitely recursive use of your monoid! You have unit and associativity laws and _by induction_ you can apply them a finite number of times, but reassociating an infinite tree from the left to the right requires an infinite number of steps, taking us out of the constructive world we can program. So ultimately a free `Monoid` (allowing for infinite monoids) is something like Sjoerd Visscher's newtype Free p = Free { runFree :: forall r. p r =&gt; (a -&gt; r) -&gt; r } type List = Free Monoid Here we borrow the assumption of unit and association from the target r and generate something using it. It is an almost vacuous but now correct construction, whereas the association to the right to make a list required us to be able to right associate infinite trees. You can view this as a sort of quotient on a magma, where you guarantee to only consume it with monoidal reductions. Binding/substituting on a (unital) magma can now take longer than O(n), why? Because now I have to walk past all the structure. You can replace this with Oleg and Atze's "Reflection without Remorse", but walking down a _unital_ Magma structure doesn't decrease `n` necesssarily. In the absence of infinite trees, you usually want some form of balance depending on what you want to do with the structure. e.g. turning it into a catenable deque gives you efficient access to both ends and lets you still glue in O(1) or O(log n). Switching to a finger tree gives you guaranteed O(log n) splits, but now merges go from O(1) to O(log n) In a general magma the split is potentially completely lopsided. You can 'steal work' but as often as not you likely steal a single unit, or in a unital magma, possibly nothing. The cost of these richer structures is you lose the continuous extension to the infinite case, but when trading O(n) or worse for O(log n) it is often worth making that trade-off.
This is a dirty example of what something like FRP could give you. Ask questions if you want. {-# LANGUAGE GADTs #-} import Control.Applicative import Data.IORef import Data.Traversable(sequenceA) import Control.Monad -- FRP-like datatype data FRPish a where Pollable :: IO a -&gt; FRPish a Stateful :: x -&gt; FRPish (x -&gt; x) -&gt; FRPish (x -&gt; a) -&gt; FRPish a instance Functor FRPish where fmap f (Pollable io) = Pollable . fmap f $ io fmap f (Stateful init mod convert) = Stateful init mod (fmap (f .) convert) instance Applicative FRPish where pure = Pollable . return (Pollable f) &lt;*&gt; (Pollable a) = Pollable $ f &lt;*&gt; a f &lt;*&gt; (Stateful init mod convert) = Stateful init mod ((.) &lt;$&gt; f &lt;*&gt; convert) (Stateful init mod convert) &lt;*&gt; a = Stateful init mod (flip &lt;$&gt; convert &lt;*&gt; a) foldp :: a -&gt; FRPish (a -&gt; a) -&gt; FRPish a foldp init sf = Stateful init sf (pure id) runFRPish :: FRPish a -&gt; IO (IO a) runFRPish signal = case signal of Pollable io -&gt; return io Stateful init mod convert -&gt; do state &lt;- newIORef init modifierSignal &lt;- runFRPish mod converterSignal &lt;- runFRPish convert return $ do modifier &lt;- modifierSignal modifyIORef state modifier converter &lt;- converterSignal converter &lt;$&gt; readIORef state -- Actual "program" sensorD, sensorE, methane, co, airflow :: FRPish Bool pump :: Bool -&gt; IO () alarm :: IO () -- Feel free to put mockups here sensorD = undefined sensorE = undefined methane = undefined co = undefined airflow = undefined pump = undefined alarm = undefined pumpActivator :: FRPish Bool pumpActivator = foldp False $ (\d e methane pumping -&gt; if methane then False else if not pumping then d else not e) &lt;$&gt; sensorD &lt;*&gt; sensorE &lt;*&gt; methane alarmActivator :: FRPish Bool alarmActivator = or &lt;$&gt; sequenceA [co, methane, airflow] mainLoop = do activatePump &lt;- runFRPish pumpActivator activateAlarm &lt;- runFRPish alarmActivator forever $ do activatePump &gt;&gt;= pump stateOfAlarm &lt;- activateAlarm when stateOfAlarm alarm
&gt; Would you advocate using the type-class Wrongnad, which has the same operations as Monad but no monadic laws, because it is simpler? "It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience." --Albert Einstein If the laws didn't serve a purpose (in a particular context) then of course they should be dropped. However, I think you'll find the monad laws are fairly essential for them to act as "control-flow + function-application contexts". At least, `(ka &gt;=&gt; kb) &gt;=&gt; kc = ka &gt;=&gt; (kb &gt;=&gt; kc)` and `return &gt;=&gt; ka = ka = ka &gt;=&gt; return` strike me as minimal, and I believe those imply the existence of fmap, join, (&gt;&gt;=), etc. with the appropriate properties. &gt; I disagree because quotienting over the good equivalences is often what brings simplicity -- or at least correctness, but what is simple if not "easily correct"? "Simple" is unfortunately a word like "powerful". What's the more "powerful" programming language; the one that has more (valid/well-specified) programs, or the one that (encodes/preserves) more information about fewer programs? What's "simpler"; the data structure that is easy to design, or the data structure that makes your algorithm easy to prove correct? I'm actually in the later camp, on both questions. But, I rarely see the use of lists justified by using the associative or identity properties in later poofs. It's all about not do any quotienting that you don't use the results of later. Still, the way I worded it is probably *stronger* than the result actually is. First, there are many requirements that would eliminate a free magma approach: the necessity for an "empty" container/functor is the first one that comes to mind, but there are others. Second, the view of data structures as a free algebraic structure is likely limited; there are probably data structures that do correspond to any free algebraic structure, an algebraic structure rarely induces certain performance characteristics that may be important, etc. It really is a tenuous argument. It role was actually the role of "ad copy", an attempt to interest readers in more articles about trees. Thanks for your read and your critical eye.
That would be fantastic.
Thanks Rein. &gt; Real World Haskell [O’Sullivan et al. 2008] can be used but it must be read carefully as it is out of date and contains a significant number of errata. Given the other two books are older than this one (by about a decade), why does RWH deserve a caveat and not the others? Is it that while they all three use Haskell, the first two are more concerned with teaching Functional idioms than the language itself?
Most of your comment just makes me realize how little I know about these things, or at least how little I think about the infinite cases. &gt; In the absence of infinite trees, you usually want some form of balance... Spoilers! You'll give away my whole series. ;P
That's the problem with /r/haskell, they're usually fast enough to skip to the end!
&gt; mult x unit = x This is not the 'unit' you are looking for... EDIT: to state laws formally you have to do this in a dependently-typed language that has identity types (e.g. Coq, Agda, Idris, etc.)
You can't do this. There's no way to implicitly define a function and force it to satisfy some declared axioms. Type classes only impose a restriction that some functions be defined, not that they need to satisfy any properties. I think Haskeller only write down axioms as comments. Edit: To add onto heisenbug's comment, things that appear on the left hand side of a = sign are patterns. Patterns are literals, variables, constructors, or some combination of them. The unit in `mult x unit = x` just introduces a variable binding and has nothing to do with the unit you declared earlier.
The first two books are written by academics (from Oxford and Nottingham) and as you suggest concentrate on the thought process and theoretical side more so than RWH. For example Introduction to Functional Programming has a section all about the fusion theorem for folds but nothing about specific libraries. If fact, it would probably be quite hard to write modern, idiomatic programs if you read them in isolation but it gives you a great foundation.
Clearly it should be called Magnad.
Ok, thanks, this is good to know. So there's no way to reason about any axoimatic structures in Haskell? I guess I've been meaning to learn Coq for a while.... 
Have a look at [this](http://www.youtube.com/watch?v=P82dqVrS8ik) for an example of how to so this sort of thing in Idris. 
Thanks for taking the time to write such a detailed response.
/encouragement
Any news on this?
Good question! You pretty much answered it yourself though. The other two are more timeless in that they focus on abstract, theoretical concepts, and aren't tied to a particular era of libraries and tooling. RWH was designed to be a *practical* guide and so focused on libraries, tools, GHC extensions, and etc. that were in use when it was written. These things all tend to become out of date a bit more quickly than, say, `foldr` fusion laws or structural induction. /u/Lossy has a good point as well, and unfortunately I don't know of a good, modern, *pragmatic* guide to Haskell. *Beginning Haskell: a Project Based Approach* could have been that but the book is so poorly put together that I worry that one might become a *worse* Haskeller after reading it, which is a real shame (I suspect that a lot of this is the publisher's fault). The good news is that I have reason to believe that some new Haskell books are on the way soon(ish)! Also I was somehow unaware until just recently that a third edition of Thompson's Haskell book was published in 2011. I'll update my list after I've had a chance to go through it!
Heh. I saw the thumbnail and was wondering what Mac Lane and Birkhoff's *Algebra* was doing on the front page of /r/Haskell before I read the title.
From the project page: https://github.com/agrafix/Spock-scotty-benchmark
This certainly works. I was thinking of having a type class that enforced a certain set of "rules" (i.e. group axioms) on instances of that type class. That way, if you ever had a function that produced a group, you wouldn't need to check that axioms by hand - it would be known at compile time whether the output of the function was actually a group or not. Apparently, it's impossible to reason about things like this with dependent typing. 
Here’s one idea. You can model this as a state machine. You might have a state enumeration: data State = Idle | Pump | Alarm And a function that takes the current state, reads some sensors, and possibly takes some actions before transitioning to the next state: step :: State -&gt; IO State step Idle = withCriticalCheck $ do high &lt;- senseWaterHigh if high then startPump &gt;&gt; return Pump else return Idle step Pump = withCriticalCheck $ do methane &lt;- senseMethane if methane &gt; methaneThreshold then stopPump &gt;&gt; return Idle else do low &lt;- senseWaterLow if low then return Pump else stopPump &gt;&gt; return Idle step Alarm = do critical &lt;- checkCritical if critical then return Alarm else stopAlarm &gt;&gt; return Idle withCriticalCheck :: IO State -&gt; IO State withCriticalCheck proceed = do co &lt;- senseCO methane &lt;- senseMethane airflow &lt;- senseAirflow if or [co &gt; criticalCO, methane &gt; criticalMethane, airflow &lt; criticalAirflow] then startAlarm &gt;&gt; return Alarm else proceed Now your main program simply starts off an infinite loop from the `Idle` state: main = let loop state = loop =&lt;&lt; step state in loop Idle This can also be written with `fix`: main = fix (\loop state -&gt; loop =&lt;&lt; step state) Idle Not necessarily the best approach, but it’s simple, and you can factor out repetition pretty easily. 
Not statically. Generally the typeclass laws for e.g. Monoid, Monad, etc are enforced by contract. They can be verified through [QuickCheck properties](http://www.haskell.org/haskellwiki/Introduction_to_QuickCheck1), and it's generally good practice to do so for new instances. 
&gt; It's not clear to me which part of this fans of type–full programming are approaching when they talk about writing “correct” to be just this impossibly hard intellectual challenge (or else what you're doing is “trivial”) in anything other than Haskell. I don't know about other people, but I think it's impossibly hard in any language, Haskell included. I do think that Haskell does more to help you reduce the number of defects in your code than any other language in mainstream use. &gt; Since programs do exist which, I assert, do non–trivial things and have a number of observed failures as near to zero as makes no difference, but are written in Java and C and even assembler, I struggle to parse the claim as anything other than naïve and grandiose. I never used the word "trivial". I said "meaningful complexity". There is certainly some space on the complexity axis in between "trivial" and "meaningful complexity", and I don't dispute that people can write programs that fall into that space that have very few defects and in languages other than Haskell. I just don't consider that space very interesting. And even if you convinced me that it was interesting, said close-to-correct programs probably were not that correct from the get-go. It took some time for them to iron out the defects. Therefore, Haskell's benefits would have been useful even there.
Thanks so much for this list! I will definately check out the mathematical theory based ones. 
What would it take for O'Reilly (or whoever owns the publishing rights to RWH) to license the book to wikify it for a community-built 2nd edition?
As a beginner I also found Beginning Haskell a mix of fabulous "ahah" moments and just ... getting lost in the wilderness. I'd *love* to see a series of blog posts / screencasts which is basically - we have nothing. I'm going to build a haskell project. Here's how I start and progress. How do you go about building up a project? Small scratch files which you gradually draw together? Mostly REPL which you solidify back into a file? 
Makes sense, thanks again for the information!
Step 1: Business logic, assuming level signals (e.g. alarm should be on). Events (e.g. start alarm) should be derived from levels, if necessary. data Sensor = Son | Soff data Level = Lon | Loff -- alarm a b c alarm Soff Soff Soff = Loff alarm _ _ _ = Lon -- keep the alarm on forever or shut it off as soon as sensors are back to normal? -- pump b d e pp pump Son _ _ _ = Loff pump _ Son _ _ = Lon pump _ _ Son Son = Lon pump _ _ Soff Son = Loff Step 2. Wire it into IO, somehow. I'm not fluent in Haskell, perhaps someone can help cleaning this up? loop = forever $ do a b c d e &lt;- read_sensors pp &lt;- read_state alarm_state = alarm a b c pump_state = pump b d e pp write_levels alarm_state pump_state write_state pump_state Step 3 (optional). Translate levels into events. data Action = Start | Stop | Nothing translate Lon Soff = Start translate Loff Son = Stop translate _ _ = Nothing 
I'd be interested to know what you think constitutes “meaningful complexity”. I smell a True Scotsman. Last time round, when people were arguing about the benefits of OO over procedural programming, there was this very un–edifying kind of one–up–manship between OO advocates telling stories about where it had worked and opponents saying “well, that might have worked for *your* system, but *my* system is so much bigger or more complex or has such tight performance constraints, or whatever, that it could never work for me” and then some OO advocate would say “ah, but *I* do that kind of work and OO has worked fine for me” and then someone on the other side of the argument would come back with a “oh, yes, for *that*, sure, but for *my* bigger, faster…” and so on. The sequence would end with whomever there was who worked on a telco switch (which at the time the exemplar of a big, low latency, high throughput, high availability system) and it was the luck of the draw whether they liked OO or not. It would be a shame if the argument of Haskell went like this: “well, of course you don't see the benefits of Haskell over whatever you're doing now, your system isn't nearly meaningfully enough complex”. 
No currently, but work is in progress to have it work against GHC 7.8.x. Because it is a refactorer, it has to deal in detail with changes in the AST, this makes adapting a longer process than for other software.
Here I construct a ring class and the ring axioms, that are checked at runtime explicitly: http://haskell-web.blogspot.com.es/2008/10/axioms-properties-for-haskell-classes.html 
Why is that benchmark important?
A bit off-topic, but I recently learned that in some critical industrial processes, the overseeing authority (I'll let you guess what kind of processes I am talking about) requires that the programs are "deterministic". What they *seem* to mean (I'm still new with this) by that is that the program : * must take a decision in a given amount of time (the limit being the cycle length) * the decision can only be a function of the inputs So they are basically defining pure functions with an added twist (in practice, most loops are forbidden). They are using some kind of diagram-based language, where they are connecting blocks. This definitely looks like FRP to me.
Your tutorial and the ones you linked are really awesome. Thanks and keep it up! :)
In my experience formal education only helps in understanding the actually difficult parts of Ed's packages.
Not that it's a problem to ask a classic question, but that question has been asked often. A few results from DuckDuckGo: * http://www.learnyouahaskell.com/functionally-solving-problems * http://alvinalexander.com/scala/thinking-functionally-with-haskell-notes * http://pragprog.com/magazines/2012-08/thinking-functionally-with-haskell That being said, thinking in terms of data and data flow is what helps me the most. Trying hard to separate IO from the pure logic too. Trying to leverage the type system to catch as much errors as possible. I feel like a constant beginner in Haskell, so I start as simple as possible and start using abstractions only as the problem complexifies. For example; this is how I would start doing your problem, I'm not using the state monad nor any fancy feature: (Disclaimer, it compiles but I have not tested it ) {-# LANGUAGE GeneralizedNewtypeDeriving #-} module WaterControl where import Control.Monad (when,foldM) {- We have sensors, some of which can be on or off, others have a level Depending on those sensors, we will perform some actions or do nothing: - Start or stop a pump - Raise an alarm Here are some datatypes for that. Levels and thresholds are both int, but I want to make sure I use them in only the right way, so I make newtypes: Haskell will prevent me to mix them in unwanted ways. I would have done the same with Bool sensors ( On of Off instead of True/False) and Pump (Start and Stop instead of True/False), but it's just a quick example -} newtype Threshold = Threshold Int deriving Enum newtype Level = Level Int deriving Enum data Sensor = WaterLevelD Bool | WaterLevelE Bool | CarbonMonoxide Level | Methane Level | Airflow Level data Action = None | Pump Bool | Alarm Alarms data Settings = Settings { settingCO :: Threshold, settingMethane :: Threshold, settingAirflow :: Threshold, settingOkPump :: Bool } data Alarms = AlarmCO | AlarmMethane | AlarmAirflow levelAboveThreshold :: Level -&gt; Threshold -&gt; Bool levelAboveThreshold level threshold = fromEnum level &gt; fromEnum threshold {- Pure business logic here. -} check :: Settings -&gt; Sensor -&gt; [ Action ] -- "When D goes on, we pump out water" ... -- "the pump must not be operated when methane is above a certain level" check settings (WaterLevelD True) | settingOkPump settings == True = [ Pump True ] -- ... "Until E goes off" check _ (WaterLevelE False) = [ Pump False ] -- ... gas sensors ... if any ... become critical, -- an alarm must be raised check settings (CarbonMonoxide level) | level `levelAboveThreshold` settingCO settings = [ Alarm AlarmCO ] check settings (Methane level) | level `levelAboveThreshold` settingMethane settings = [ Pump False, Alarm AlarmMethane ] check settings (Airflow level) | level `levelAboveThreshold` settingAirflow settings = [ Alarm AlarmAirflow ] -- In all other cases, there's nothing to do. check _ _ = [ None ] {- Some funtions needed to connect to the real world -} getSensors :: IO [ Sensor ] getSensors = undefined -- All of the 'return settings' start being annoying -- if the problems expands a bit, it will probably be time -- to refactor and use a state monad. -- for now, it'll do like this. doAction :: Settings -&gt; Action -&gt; IO Settings doAction settings (Pump True) = startPump &gt;&gt; return settings doAction settings (Pump False) = stopPump &gt;&gt; return settings doAction settings (Alarm AlarmMethane) = raiseAlarm AlarmMethane &gt;&gt; return settings { settingOkPump = False } doAction settings (Alarm s) = raiseAlarm s &gt;&gt; return settings doAction settings None = return settings startPump,stopPump :: IO () startPump = undefined stopPump = undefined raiseAlarm :: Alarms -&gt; IO () raiseAlarm = undefined waitSomeTime :: IO () waitSomeTime = undefined {- Connecting it all together -} checkIO :: Settings -&gt; IO () checkIO settings = do sensors &lt;- getSensors let actions = concatMap (check settings ) sensors settings' &lt;- foldM doAction settings actions waitSomeTime checkIO settings' {- Adapting to changes : if we were to have 2 pumps, it's easy to modify : - modify startPump so that it returns a bool - make two versions. Alternatively, we could have added a parameter to startPump. It's just an example -} startPump1,startPump2 :: IO Bool startPump1 = undefined startPump2 = undefined doAction2 :: Settings -&gt; Action -&gt; IO Settings doAction2 settings (Pump True) = do ok &lt;- startPump1 when (not ok) ( startPump2 &gt;&gt; return () ) return settings 
Given an arbitrary (finitary) algebraic theory, it's not always possible to express its free algebras as a (first order) ADT, therefore in particular you can't find a general "canonicalisation" process. A very intuitive way to see this is to think about the [word problem](http://en.wikipedia.org/wiki/Word_problem_for_groups). If G is a finitely presented group, we can define an algebraic theory T, with all the operations and laws of general groups, plus all the generators as additional unary operations, and all the relations as additional laws. The group G itself can be recovered as the free algebra of T over the empty type. Now, if you could express free algebras of T as an ADT, in particular you would have a (very efficient) solution to the word problem, since you could just check if a word is structurally equal to the unit. Here is another (possibly simpler, but more categorical) argument. Take a theory T, and suppose its free algebras are given by a first order ADT F. It is easy to see that this implies that it must preserve pullbacks. Therefore, any theory for which the corresponding free algebra functor does not preserve pullbacks is a valid counterexample. And there are plenty such theories: commutative monoids, groups, abelian groups... There seems to be some similarity between this requirement and whether the theory can be described by an [operad](http://en.wikipedia.org/wiki/Operad#Operad_without_permutations), since they both seem to imply that the corresponding monad is cartesian. I'm not sure if there is any implication between the two, though. For sure, there is an ADT F for a non-operadic monad: `type F X = [(X, Bool)]`, where `join` reverses all the lists that are paired with `True`. This is the monad corresponding to the theory of monoids with an anti-involution, which *cannot* be described by an operad.
Where is your base case?
he had when i answered though
My first Haskell project :-)
Well, a lens family `Lens s s' a a'` can produce a `f s'` given an `a -&gt; f a'` and a `s`. For example: left :: forall f. Functor f =&gt; (a -&gt; f a') -&gt; (a, b) -&gt; f (a', b) left g (a, b) = fmap (\a' -&gt; (a', b)) (g a) As you can see, if you know the type of the arguments, you know the return type.
&gt;Much like a function a -&gt; b can be applied on f a to change it's structure to become an f b I thought that Functors specifically *can't* change structure?
I raise you a fiblist = fix $ (0:).(1:).(zipWith (+) &lt;*&gt; tail)
I also had that _exact_ question in mind while reading this post. Anyways, auto-update looks nice, kudos for making it depend only on base by the way.
http://www.haskell.org/haskellwiki/The_Fibonacci_sequence
I'd not seen the series start at 0 before. fiblist = 1 : 1 : zipWith (+) fiblist (tail fiblist) Wikipedia says starting with 0 is the modern usage - so thanks, I learned something! 
Well, there's no way to reason about axiomatic structures _with the compiler's help_ in Haskell. If you'd turned on compiler warnings (e.g. -Wall), it would have warned you about variable shadowing. If you're feeling adventurous, I second [Titanlegions](http://www.reddit.com/r/haskell/comments/2cq9wb/creating_a_group_type_class/cjhzi65)'s suggestion to consider [Idris](http://www.idris-lang.org/) instead of (or in addition to) Coq; it's closer to Haskell and designed more to be a programming language which is capable of proving theorems than a theorem prover which is capable writing programs, but it's much less mature.
See http://www.reddit.com/r/haskell/comments/jtdrz/interested_in_a_2nd_edition_of_real_world_haskell/ - maybe there wasn't enough interest. All in all, it's the decision of /u/bos, /u/dons and /u/jgoerzen.
I only install packages into sandboxes with it. The main use of cabal is to build packages. Like make, just nicer. The fact that it can pull dependencies automatically from hackage is a bonus thing. When you install a haskell package through a real package manager, it will probably also uses cabal to build the package (if it is a source package, that is).
One way to explain the separate arguments "s and t" and "a and b" is that what we really want to do with a lens is describe something we don't have vocabulary for in Haskell. Ideally what we want is some kind of quantified bundle of arrows going in both directions such that when you compose them it uses some fancy pullback machinery to shrink the indexing set down. We can't have that. All we have are constraints and unification. GHC can't do any sort of composition of higher rank functions yielding a higher rank-function. So what we do is we provide two instantiations of a schema to approximate something called anti-unification. Unification computes the most general unifier of two types. Anti-unification computes the least general unifier of two types. By specifying two instantiations of the schema what we get is a sort of path into the lens at one type in the family and a path out of the lens at another type in the family. For 98% of the usecases this is all you need. One of the conceits of lens is that we try to fuse everything together in order to make more and more of the usecases fit. We get efficiency from this, increased generality and we can continue to exist inside a language without such a weird type system. Every once in a while someone can come up with a case that the little 2-point approximation of anti-unification can't handle, but it is sufficiently rare that most people never notice it. Ideally what we'd want is that you have something like type Lens s a = forall f i j. Functor f =&gt; (a i -&gt; f (a j)) -&gt; s i -&gt; f (s i) where s and a are type families, and have composition automatically generate a new type family by pullback, but that isn't sound in general.
I always do: fibs = Data.Function.fix $ (0:) . scanl (+) 1
And I thought my solution was neat... I like this very much Can we apply this to something like an int-trie to get `O(log n)` indexing as well?
&gt; I'd be interested to know what you think constitutes “meaningful complexity”. I smell a True Scotsman. That's irrelevant because my argument doesn't hinge on any precise definition. As I already mentioned, I think Haskell gives benefits over other languages for things less than meaningful complexity as well. "Meaningful complexity" only came up when we started talking about collapsing the spectrum of complexity to a boolean correctness.
I personally would always use `Either E a` where `E` is an type which enocdes the possible errors. I use `EitherT E m a` if I need to do it in a monad. I can't see that any other approach is particularly Haskelly.
It would be interesting to see what base would look like if it followed this principle. File I/O functions would have a pretty big sum type as their error case, and I wonder how composing these functions with different sets of errors would work in practice.
There's nothing wrong with a pretty big sum type. However, composing functions with different types of errors would be a bit of a headache. It could be sidestepped by using `Data.Dynamic` which is certainly no worse than what happens with IO exceptions.
&gt; Here's the first silly example that comes to mind: &gt; &gt; [ ... ] &gt; &gt; For anonymous sum types, imagine ... Thank you, this sounds very interesting! I assume you're familiar with Daan Leijen's [Morrow](http://lambda-the-ultimate.org/node/174) language, and [this paper](http://research.microsoft.com/pubs/65409/scopedlabels.pdf), etc.? (I haven't read them myself, yet!) &gt; I'm pretty sure Idris got it to work (default is strict, and "Lazy" type constructor makes a value lazy). Ah yes, it's much easier with strict-by-default. I was thinking about `!Int`, `!a`, etc. representing strict types in a *lazy* language (i.e. existing Haskell). But even that seems to be possible, as pointed out in [another subthread](http://www.reddit.com/r/haskell/comments/2chb2h/fantasy_world_haskell/cjgepwj) of this discussion, with the key being to have a strict *function* type.
Familiarity with monads makes working with LINQ trivial in C#. If you can use F# you can even get pattern matching. Scalaz lets you transfer most of your knowledge of Haskell idioms to Scala. Even in python and perl you can still map/filter/etc. your way to glory. In javascript I'll often build up things like parsing combinator libraries in a functional style, because I can test all the combinators in isolation. I learned C++ templates before I learned Haskell, and while going back there is incredibly verbose by comparison to Haskell's concise 'everything is generic' approach, I use parametricity a lot more and partial template specialization a lot less when thanks to the mindset I acquired by working in Haskell. That pretty much covers most of the major language ecosystems folks have to use, other than Fortran. =) Knowing Haskell helps me think in every language I use day to day.
Apropos 2 - use Maybe when you have *partiality* (I'd argue that partiality and errors are not the same thing). If there's an error I'd want it trapping with a **Left** and some message.
I like that as a rough rule of thumb. I think there'd be some meaningful discussion around the line between partiality and error, but my own style would more or less conform.
Aye true, certainly as different as we are, we will absorb his libraries differently. My statement, while true, was thinly veiled tongue in cheek. Since his work is brilliant but generally thought of to be difficult to comprehend. 
Formality in thought even when the language doesn't support it. I program in Ruby for my day job sometimes and often designed operations around "pretend parametricity" or choices of (co)algebras and data hiding that ought to be occurring. Ruby is a particularly bad example since (a) it's loose enough to allow nigh anything and (b) a lot of common Ruby coding styles depend upon (a), but you can still get by. The biggest pain point is mutation/side effects which will be rampant in any other language. The discipline of separation will still be a great tool, but it's more stressful because you feel like every other piece of code you could interact with is potentially a landmine. Ultimately, this mutability scare tends to push me toward Kay-style OO where everything really is hidden from everything else and you have to assume that messaging is the only mechanism for communication. My own personal style here is heavily influenced by Erlang (which, for obvious reasons, goes even further). This really isn't a terrible way of coding "in the large", though.
I don't know if it is important. codygman wanted to know if there was any existing comparison between Scotty and Spock. The only one I could find was this benchmark.
&gt; Scalaz lets you transfer most of your knowledge of Haskell idioms to Scala. Even without Scalaz, immutability by default, anonymous functions, and @tailrec aren't exactly foreign to idiomatic Scala, so some of your Haskell thinking will translate directly. In the JVM languages, we lose parametricity due to unrestricted reflection, but it doesn't feel like a huge loss. Most of my work is either in Java 1.6, or C89 / pre-standard C++. I use a lot more const/final and immutable structures than others on my team. Anonymous and higher-order functions are painful enough in those languages that I rarely use them, although I am going to eventually try and Free/Operational/Trampoline approach to optimize some existing Java code at some point in the future. I wrote it before I understood that idiom/paradigm, and it is "not great".
I really, really dislike `Either String` : ( But then again, I use it constantly because it's easy to write. It just sucks for downstream users. Errors have a lot of structure and I would prefer to see giant nested sums of various error types than unhandleable strings. A good way of handling this might be an optional class like class ShowError e where showError :: e -&gt; String which lets everyone provide the human-readable string rep that is nice for printing but would also let programs more easily handle these big sum constructions.
&gt; Errors have a lot of structure and I would prefer to see giant nested sums of various error types than unhandleable strings. Me too. I personally almost never use strings for errors. Almost. Strings are bad for handling errors properly, bad for printing (maybe I want to show errors in a different language). In Fay we have: compileFile :: Config -&gt; FilePath -&gt; IO (Either CompileError String) In haskell-docs I have: searchIdent :: Maybe PackageConfig -&gt; Identifier -&gt; Ghc (Either DocsException [IdentDoc]) In pdfinfo I have: pdfInfo :: MonadIO m =&gt; FilePath -&gt; m (Either PDFInfoError PDFInfo) And so on. But I was just saying `Either String` to conform to the existing use, didn't want to divert the discussion.
Yup, total agreement. Sorry for the sideline—but I felt it worth bringing up in context of the larger discussion of error styles.
Agreed. It's worth pointing out.
One thing that I tend to do which I've never explicitly thought about or debated is that I catch exceptions of libraries that I'm using, as a library writer. Actually, [HttpException is a good example.](https://hackage.haskell.org/package/http-conduit-2.1.4/docs/Network-HTTP-Conduit.html#t:HttpException) It returns IOException and ZlibException, etc inside its sum type. That's useful, because it means you can use http-conduit/http-client and handle "things from this call", rather than things inside it bubbling up that probably shouldn't bubble up.
If they were *just* `get` and `set`, no one would care. The point of them, all along, has been that they also can be composed for nested access. Ok, that's only three operations, but you really need all three!
That's a really great point... Reification of errors "in this region" or "behind this barrier" as first-order data types in order to stop bubbling.
FP taught me about continuation passing style (and how to convert to and from CPS style). That makes it much easier to understand Javascript's awful callbacks. Also, Promises are a monad.
Quite nice thank you. The explanation of the type parameter was especially helpful for Lens. I feel a _lot_ more confident after this explanation. Also, as with Gabriel Gonzalez's tutorial, for me showing how to create examples without Template haskell first is a better way to understand what's going on (and that there's no magic involved). Looking forward to any further tutorials!
&gt; And every single waking moment I find myself thinking, this tomato could be so nicely cut, if I just had a tomato knife. Great quote, and interesting insight. Thanks!
A Cokleisli composition would be a chain of composed elements each with a type like W a -&gt; b where `W` is some `Comonad`. The composition looks like (.) :: Comonad w =&gt; (w b -&gt; c) -&gt; (w a -&gt; b) -&gt; (w a -&gt; c) g . f = g . fmap f . duplicate -- or just (g . extend f) Ultimately, the exact choice of `Comonad` determines what all of the above means, but recognizing the engine of his code to be such a composition is already a big step in slicing it up into manageable pieces.
Id love to see a short 5-line concrete example of the composition function being called, preferrably with the monomorphic version of the names instead of the generic `=&lt;=`. I didnt really understand how the comonads have to do with the "mutable" values.
You stop using them &lt;ducks&gt; 
The current version does not have vim support, any vim volunteer is welcome to step up.
Haven't seen that paper, sounds interesting, thanks! Will read. I agree the key is strict function types. It isn't a problem, because if the result type is lazy by default, then you still get all the laziness you'd get from lazy functions, if I understand correctly?
Learning Haskell has had a huge impact on the imperative code I write at work. * I use types more. I can't rely on the compiler to check types which it doesn't understand, but I use [naming conventions](http://www.joelonsoftware.com/articles/Wrong.html) which allow me to spot simple type errors without having to rely on a type checker. * I write more pure functions. To remove dead enemies from the list of objects to be updated on every frame, I used to iterate over the elements, appending dead ones to a new list of things I want to delete (because calling remove() on the list I am currently iterating on would lead to problems), then loop over that list of dead enemies in order to call remove() on the original list. Today I would create a new filtered list instead, one which only contains the elements I want to keep. * I create more composable abstractions. Our codebase had logic which was distributed and duplicated among many callbacks with shared state, and it was buggy. I created a Transition class, representing an event which can only fire once, and I added DelayedTransition, LastTransition and FirstTransition subclasses which allowed me to express the logic by combining transitions using those combinators. The bugs disappeared.
He was talking about the `a` to a `b` not an `f` to a `g`.
 Is there a version that builds, even partially against ghc 7.8? Or if I wanted to tackle this, would it make sense to spin up a VM with 7.6 and work against it there? 
This works with most languages, and is a great technique! You end up guiding yourself to writing more reusable, testable components by isolating pure functions. On the other side, your hard-to-test ad hoc IO code is now much more amenable to close inspection if nothing else.
What's the point of `ShowError`? Why would you ever want `showError` to give something different than `show`?
Hmm... I would just have `read` read the human readable version.
Was I the only one who thought this was a program to paste stdout to Reddit at first?
There's also **EitherT**, provided by the either package and re-exported by the errors package. The type is identical to ExceptT, but the libraries have a slightly different API.
If only I could...
Is that because they're easy?
It's mostly a matter of mental agility and gravitating towards problem solving strategies that I know work and scale well. Important examples: * Effortlessly switch mental frame-of-reference between data and code (i.o.w., being able to approach code as data and vv.). * Effortlessly juggle higher-order functions and concepts like currying, partial function application, closures, etc. * Thinking in terms of map and reduce, rather than destructive iteration. * Holding immutability in high esteem ("shared mutable state is the root of all evil"). * Being more conscious about effects. * Having un-learned the OOP programmer's inheritance reflex. * Defining and using EDSLs. * Debugging by dissecting code and verifying parts (e.g. in a REPL, through type system guarantees, or in the form of unit tests) rather than by stepping through it and observing state changes (e.g. by step debugging or trace logging) * Being more conscious about "value domains", even in untyped / unityped languages. For example, HTML source and plain text, while both represented as byte strings in many languages, are not the same thing, and all transitions between these domains should be explicit and well-defined. * A bunch of things finally "clicking" for me: the functional parts of JavaScript (including warts like `bind()` and `apply()`, as well as common functional strategies like continuation-passing style asynchronous programming); decorators, generators, comprehensions etc. in Python; template metaprogramming in C++; and even Unix pipes.
This is kind of amazing. If I'm not mistaken you're showing here that the state monad is sort of irrelevant. You could do everything that the state monad does with just the identity monad.
This is kind of mind blowing. Using this "where" technique you've converted normal imperative (stateful) code into haskell without using monads and do notation. Did you pick this up from someone else or did you invent it ?
Just skimmed the paper. It's interesting, me and /u/yairchu just recently implemented a similar record system (as an extension of the code in Algorithm-W-Step-By-Step paper) to serve as the base type system of Lamdu. We do not like shadowing of record labels, though, so we added a system of constraints: whereby we infer constraints on record type variables (that represent extra fields) to make sure no duplications occur. 
http://blog.ezyang.com/2011/08/8-ways-to-report-errors-in-haskell-revisited/
In PHP, I use closures, array_walk and array_map and other "functional", well, functions. It nicely hides the iteration, and I don't have to explicitly update the array with a callback like: function(&amp;$value, $key) { $value = 'prefix_'.$value; } I also define string constants, stick them in an array, and iterate over the array with a case analysis, throwing an error when the default case is hit: a completeness checker for pattern matching on a "datatype"! It's nice, telling me when and where I've forgotten to update the "pattern match". Really cuts down on the "accessing non-existent index of an array" notices, too. It's like My Little Type-Safety. If anyone else has some fun constructs, please share!
If you have a look at https://github.com/alanz/HaRe/blob/wip/NIX.md it gives one way of testing against the various compilers, using nixpkgs to manage the config. I have recently switched to using this, and it works quite well. And I am actively working on the haskell-token-utils part first, will then work up into HaRe
Fair, I should have been more explicit about compose :)
Conjugation by an Iso conjugate i l = i.l.from i Note this needs to instantiate i at 4 types for its full generality. We also had to jump through hoops to get _Wrapped and _Unwrapped to work right for some similar 4-point cases. Note this isn't the same as just lensing through an Iso.
Conner McDaniel walks you through [a sed implementation in Haskell](https://www.youtube.com/watch?v=0I90MTip-OQ).
So, why purescript over the other Haskell inspired Javascript replacement languages? I'm a newbie, and all these languages look interesting, but there are so many of them to pick from.
I touch on that very briefly in the book [here](https://leanpub.com/purescript/read#leanpub-auto-about-the-author) but the answer is essentially that there are two types of "Haskell-inspired" altjs languages: 1) things like LiveScript, which build on JavaScript, with better syntax, and 2) things like GHCJS, Haste, Fay, etc. which try to run (maybe a subset of) Haskell on a JS runtime, including Haskell semantics. PureScript falls into the former camp, but takes lots of cues from Haskell. If you want JS semantics with Haskelly syntax and a familiar type system, then I'd say PureScript is a good choice. If you want full Haskell, then probably not.
This looks amazing! Thank you!
Cheers. :)
Calm down, I was just quoting a stupid inside joke. You're right though, monoids are a very common pattern and a very useful abstraction.
The class (`RepriceState`) in question is isomorphic to `(a, b)`, with `a` being read-only data, and `b` being data that I want to figure out from the computation in question. The arrows in question really take a `RepriceState`(`(a, b)`)and map it down to a `PriceDeterminant` (`b`). Indeed, every arrow in the chain will have the same "type": `RepriceState PriceDeterminant -&gt; PriceDeterminant`, we'll say. It's not a 1-1 mapping because JS us unityped, but the arrows are functions that take the *whole* structure and map it down to only the variables that I care about at the end of the computation, e.g. final computed price. The good thing about this is that I've defined `extend` and `extract` for `RepriceState` in such a way that the data enclosed in the `RepriceState` that isn't a part of the `PriceDeterminant` can't be changed unless I'm explicitly mutating it during one of the arrows (which I avoid on purpose). I can't use a cokleisli arrow, get a result back, and mess with the data. I'm using the words "mutable" and "immutable" loosely; by mutable I'm talking about variables that change over time and by immutable I mean variables that should never change. They're not actually immutable, but they're never touched. As far as code, I have the following: var coComposeL = function(f, g) { return function(w) { return g(w.extend(f)); }; }; var chainAllL = function() { var fs = Array.prototype.slice.call(arguments); var func = _.foldl(_.tail(fs), function(acc, f) { return coComposeL(acc, f); }, _.head(fs)); return func; }; var runState = Types.chainAllL( functionA, functionB, functionC ); And running `runState(initialRepriceState)` gives me back all accumulated information during the transformation, including the price I'd like to set. That's as much information I can give out, but hopefully that clarifies what I mean, a little bit. 
When I write in OO languages, I still apply a functional mindset, encouraging little state, static methods, and mapping instead of iterating.
Why not Idris with the JavaScript backend? Idris is like dependently-typed, strict-by-default Haskell. Just curious.
Idris to JS is definitely a very compelling option. The generated JS is getting clearer all the time, as far as I can tell.
Hey me too! I work in python and just cringe when I see IO in a function that doesn't need to do IO. I think in general Haskell made me pay more attention to the relationships between functions than in functions themselves.
If I wanted it to run fast, I'd probably adopt the ideas from [An Algorithm for Compressing Space and Time](http://www.drdobbs.com/jvm/an-algorithm-for-compressing-space-and-t/184406478). If I wanted to write it fast, I'd probably try using the [cellular automata comonad](http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html) instead. And of course there are many tradeoff points in between where other techniques make sense.
Went to buy it and saw a message "50% complete"....is the book finished?
No it's 50% complete, but anyone who downloads it should receive update emails every time I publish a new chapter.
Using EitherT, ErrorT, or ExceptT can give a false sense of security in IO since exceptions are always present in IO. You end up with 2 mechanisms for handling errors. As a general approach, I think it is better to use E....T only for pure code. Rather than putting EitherT in your basic monad stack, when in IO use (runEitherT $ pure code). This way when you catch the errors you throw yourself in IO you may also catch some errors you did not expect.
now if only someone could explain rooted types to me :P
I'm pretty sure you can't adapt the `Store` (or any similar) comonad to this, since the concept is not local enough. Basically, the particles update one-at-a-time, not all at once, otherwise there would be a lot of problems, like particles moving to the same spot.
When will the other 50% be available? (I like to skip forward when I read :-)
Corepresentable! That reminds me of my favourite class signature of all time! [class (Choice p, Corepresentable p, Comonad (Corep p), Traversable (Corep p), Strong p, Representable p, Monad (Rep p), MonadFix (Rep p), Distributive (Rep p), ArrowLoop p, ArrowApply p, ArrowChoice p) =&gt; Conjoined p](https://hackage.haskell.org/package/lens-4.3.3/docs/Control-Lens-Indexed.html#t:Conjoined)
I'm going to release chapters as they become available. I'm trying to stick to a [pretty aggressive schedule](http://beeminder.com/paf31/goals/purescript) so I aim to be done in the next month or so.
I apply my knowledge of Haskell by avoiding OO like the plague.
You might be interested in looking at Falling Turnip, written by Tran Ma: https://github.com/tranma/falling-turnip . It uses Repa's stencil convolution for "almost free parallelism". I don't think it's the *fastest* implementation, but I think it gets decent performance.
You are assuming an implementation here. There are certainly ways to do simultaneous updates. The simplest solution is to consider particles that could impact you. For instance in one dimension: (letters being particles, . being empty) . A . B . With A moving right 1 and B moving left 1. With a maximum speed of 1 I only need to look 2 to the right to verify no one is going to hit me. In the above case I notice B is going to do just that. The trick is I now compute my half of the collision without touching B. Then when B runs (given the original state) they will do the same, handling the collision overall correctly.
Hmm, this is true, but I also don't believe in user-defined exceptions. I don't believe I'll get everyone to follow me down that path though ...
How "newbie-level" would you say you're trying to hit with the book? It looks fantastic and I'll have to take a closer look at it soon 
Ideally, I'd like someone with JS experience but no FP experience to be able to pick it up and make progress. It is definitely not a "recipe book" though, and should be a fun challenge. The next few chapters specifically, will cover some fairly advanced material.
Well, first of all, sand game physics are non-deterministic, so you would have to do some slightly tricky things to make it figure out which direction things are moving. Secondly, you would have a huge performance problem, as the area that you need to scan is bigger than you might think in 2d. Consider this sandbox: A B ##### (A and B are water particles; they slide to the sides when on walls; # are walls) For B to know what to do, it will need to scan the following: * The 3 positions below it to see that there is a floor, and that it must therefore slide to the side. * The 4 positions next to it (within a range of 2) to check if it can slide to the sides. * The 3 positions below A to check if it will slide to the side. * The 2 positions left of A to find out how A will move. However, not only B will have to do that. A will too, and so will the area between them, as it is what will get updated. Additionally, this could be extended in a lot of ways to more particles. It seems to me that this has quadratic worst-case complexity, and I think the worst-case might be pretty common.
There is a reason for that rather ridiculous signature. When you write a class there are two things you can do. You can make it more specific to gain more power, or you can make it less specific to gain more instances at the expense of power. Here we can precisely characterize _all_ of the instances `Conjoined` can _ever_ have. So making that constraint more minimal simply ties the hands of the consumers of Conjoined, but it can give no more instances! It is everything we can talk about that is isomorphic to `i -&gt; a -&gt; b` for some i without locking down i. Which means I need to know that I can slop whatever choice of `i` I have to the left or the right hand side of the (-&gt;) via Representable and Corepresentable. I get to know that the representation is a monad because its isomorphic to (-&gt;) i, and I get to know the corepresentation is a comonad because it is isomorphic to (,) i, and all the arrow machinery follows suit. The extra superclasses are used by combinators in lens that work with an indexed traversal. Without virtually each and every one of these extra powers being granted then some of the combinators will at best have to be implemented in a less than optimal fashion and at worse become unimplementable. Being both representable and corepresentable by an adjoint pair of functors both from Hask -&gt; Hask is a very specific set of constraints, and it entails a _lot_ of extra things we can say about any such instance. The alternative is give up the ability to talk about indexed lenses and traversals, but they are too damn used to do that. =)
Well, I'm looking much more at it from a idiomatic perspective than from a performance perspective. It's fine if it's relatively slow as long as it is bearable and the code is nice.
Cool --- appreciate the info.
Which, in and of itself, probably needs updating too. ^^
Nope, I'll take a look. Thanks!
One could use a pull based FRP library like netwire. FRP does completely abstract away the looping... so much that you are supposed to pretend that it doesn't even exist. 
Since I learned about cyclomatic complexity I like to break apart code with high complexity into code with many simple functions. It's an amazing process that reveals so much about the code and its flaws. But it's often a very slow manual process for me depending on how complex the code is to begin with. Maybe what you've shown is that there's an easier more robotic way to start on it no matter how complex the initial state is ..
I actually rather like `Either SomeException`. It's flexible, extendable, and the meaning is unambiguous. And if you use `ViewPatterns`, you can pattern-match on constructors. If I'm working in a particular domain though, I'll typically have my own base type rather than `SomeException` so that irrelevant exceptions can't sneak in. It may be a sub-class though. 
Working on an "embedded" project written mostly in JS and C/C++. * I use Haskell for prototyping new code, which is then re-implemented in C/C++ or JS. Yes, even JS, despite the latter being praised as being very suitable for rapid prototyping (my experience is that you only get more bugs). The original (pseudo-) Haskell usually ends up in the comments. * Preferring composition and parametric polymorphism over inheritance in C++. There are some shallow (one level deep) hierarchies, mostly for packing up existentials, and a couple of hand-crafted vtables. In general, the code has become less OO and more like C with templates, RAII and overloading. I'm trying to cut down on templates, though. * As others have mentioned, much of the code is split in pure and impure parts. I have a half-baked buggy implementation of QuickCheck in C++ which I'm using to test the C++ code. Not so much JS, because it's 99% I/O that cannot be tested automatically. * Using types as a hand-wavy "proof by construction", and passing lot of things by value. There is a `newtype` template which serves the same purpose as the corresponding Haskell keyword, and constructors throw or return `NULL` if the arguments are not valid. * You'll probably find much more `goto`s in my code than in an average C++ program, due to the lack of reliable tail call optimisation. * I almost wrote a specialised lens-like template, but abandoned it in favour of a somewhat more conventional but ugly approach due to running out of time. I still might go back to the lens thing when I'm less tight on deadlines. While it may sound strange, I don't find myself using `map`s and `fold`s a lot in JS, or STL algorithms and `boost::variant` in C++, for various reasons. If not anything else, the resulting code usually is less readable than the alternatives, and there's too much noise.
How about a toy path router: import Data.Monoid data Result a = Match a | NoMatch deriving (Show) -- Equivalent to the `First` `Monoid` instance Monoid (Result a) where mempty = NoMatch mappend NoMatch x = x mappend x _ = x type URL = String type HTML = String handler1 :: URL -&gt; Result HTML handler1 "/hello" = Match "&lt;html&gt;&lt;body&gt;Hello&lt;/body&gt;&lt;/html&gt;" handler1 _ = NoMatch handler2 :: URL -&gt; Result HTML handler2 "/world" = Match "&lt;html&gt;&lt;body&gt;World&lt;/body&gt;&lt;/html&gt;" handler2 _ = NoMatch handlerTotal :: URL -&gt; Result HTML handlerTotal = handler1 &lt;&gt; handler2 main = do print (handlerTotal "/hello") -- Match "&lt;html&gt;&lt;body&gt;Hello&lt;/body&gt;&lt;/html&gt;" print (handlerTotal "/world") -- Match "&lt;html&gt;&lt;body&gt;World&lt;/body&gt;&lt;/html&gt;" print (handlerTotal "/badroute") -- NoMatch You can generalize that to pretty much anything that involves chaining a bunch of "partial" handlers and taking the first one that succeeds.
Yeah, but I want people to be free to violate `read . show == id`.
Yeah, the subclassing mechanism compares fairly with the big sum mechanism. The difference is exhaustiveness. If you want to handle one or two exceptional states and ignore the rest then the sub classing mechanism works great. If you want to be sure that you handled all errors except perhaps one or two then the sum works great.
Some of the reviews on Amazon suggest that _A Survey of Algebra_ by Garrett Birkhoff might be useful to read before Mac Lane's Algebra -- any thoughts on that?
awesome, thank you!
Ok, right - one of the current limitations it has is that (at least, as far as herringbone is concerned) one input file maps to one output file, which is probably too restrictive, given that we want to use things like this, or browserify, or source maps... I am starting to think that watching the filesystem for changes might be a better approach than listening for HTTP requests; it's probably safe to assume that any web library/framework already has a feature where you say 'serve static assets from this directory' so I can get rid of all the http code in herringbone (like the wai adapter). It also means that a dependency graph is easier to handle, as we only need to provide mappings from source files to compiled assets (rather than both ways). I'm considering making the API more like hakyll, or grunt, or even Make. Does that sound like a good idea?
Actually I wonder if hakyll already solves the problem that herringbone tries to solve?
Hmm... to be more Haskelly, I assume I would redefine `fizzbuzz` to be `Integer -&gt; String` (single values instead of lists), and then map it to the input list?
(NOTE: with a standard Maybe type and MonadPlus you can do this too!) This is, actually, an *extremely* common pattern. I used it recently to chain a series of quality control testing functions together. To a less elegant degree (but still cleaner than Python ifelse statements) I use it often in Python and JS. I, at first, implemented this pattern by building a list of partially applied predicate functions then defined a custom "takeJust" function that would recursively walk down the list, apply the function to the value to test, and return the Quality type if it succeeded or continue if it did not. For those interested (both versions, the lazy "take" version and the monoidal version): import Data.Monoid main :: IO () main = do let preds = [ultraPrecise, veryPrecise, precise] vMonoid = show $ predicates preds (SeriesMean 0.2, SeriesStdD 2.0) vTake = show $ takeQuality preds (SeriesMean 0.2, SeriesStdD 2.0) putStrLn vMonoid putStrLn vTake instance Monoid Quality where mempty = Pass mappend Pass x = x mappend x _ = x predicates :: [(SQS -&gt; Quality)] -&gt; SQS -&gt; Quality predicates p = mconcat p newtype SeriesMean = SeriesMean Double deriving (Eq, Ord, Show) newtype SeriesStdD = SeriesStdD Double deriving (Eq, Ord, Show) type SQS = (SeriesMean, SeriesStdD) data Quality = Precise | VeryPrecise | UltraPrecise | Pass | Discard deriving (Eq, Ord, Show) takeQuality :: [(a -&gt; Quality)] -&gt; a -&gt; Quality takeQuality [] _ = Pass takeQuality (x:xs) p = case x p of Pass -&gt; takeQuality xs p v -&gt; v ultraPrecise :: SQS -&gt; Quality ultraPrecise (SeriesMean m, SeriesStdD s) | (m &lt;= 0.5) &amp;&amp; (s &lt; 3) = UltraPrecise | otherwise = Pass veryPrecise :: SQS -&gt; Quality veryPrecise (SeriesMean m, SeriesStdD s) | (m &lt;= 0.5) &amp;&amp; (s &lt; 5) = VeryPrecise | otherwise = Pass precise :: SQS -&gt; Quality precise _ = Precise Then I figured out how I could do that more generally with a proper monoid instance and mconcat!
The second paragraph was basically an explanation of how I understood and rationalized undefined vs "return ()". Since you wanted to do nothing and return () isn't like it works in C, 'undefined' felt like a more explicitly correct option to me. I'm still learning about haskell so I probably have a few things wrong in the post (or I could be way off...) Personally, the "undefined" felt clearer to me than the "return()" But it's possible I wasn't understanding the undefined correctly, either. 
FRP is more of a general approach. Instead of stating how things change at every time step, you instead describe their behavior and relationships at a high level in continuous time. You describe how things relate to each other in terms of "how things just are". You actually aren't even allowed to refer to a "timestep" in your logic -- it is meaningless. Just like if you ask a bicyclist how she deals with "timesteps" when she rides her bike. she would probably just tell you about how her feet push the pedal in a continuous way. 
Simplify your code with this one weird trick. People at work HATE him :-)
When I was still writing python I ended up moving almost completely away from classes to named tuples and pure functions. It made it almost bearable :-)
If you don't care about extensibility and abstraction, that's exactly what you should do. If you do, you would simply abstract another layer out or so (make it work with just about any type, you can modify the function on the fly to change the checks and what the output is, etc etc) In this case a simple function is fine. [there's a nice example here]( http://codereview.stackexchange.com/questions/18852/how-can-i-improve-this-haskell-code ) As a nice exercise, I would write it with mapM_ and if you're feeling creative look at the guy below the top answer and use that as a guideline to abstract out the strings so the user could define their own strings (e.g. Printing out "hello, world, helloWorld). Bonus points if you figure out how to combine the two parameters so the user only needs to pick the fizz and buzz. A further exercise in haskell could be to abstract it out to generic types so you could have a list of floats or ints vs Integers only. 
Thanks. Makes slightly more sense now. edwardkmett's explanation was helpful too.
&gt; passing lot of things by value why not const ref?
Okay... I think monads are about 80% in my brain, but I got caught up playing with continuations instead and now it's way past bedtime so no mapM_ yet. How'm I doing otherwise: replaceOrDefault :: b -&gt; b -&gt; (a -&gt; Bool) -&gt; a -&gt; b replaceOrDefault d r p x = if p x then r else d nonNull :: [a] -&gt; Bool nonNull x = not (null x) fazbaz :: Show a =&gt; [(String, a -&gt; Bool)] -&gt; a -&gt; String fazbaz rules x = replaceOrDefault (show x) tokens nonNull tokens where tokens = foldr1 (++) [replaceOrDefault [] r p x | (r,p) &lt;- rules] isMultipleOf :: Integral a =&gt; a -&gt; a -&gt; Bool isMultipleOf a b = mod b a == 0 fizzbuzzRules :: [(String, Integer -&gt; Bool)] fizzbuzzRules = [("fizz", isMultipleOf 3),("buzz", isMultipleOf 5)] fizzbuzz :: [Integer] -&gt; [String] fizzbuzz = map (fazbaz fizzbuzzRules) *Main&gt; fizzbuzz [1..15] ["1","2","fizz","4","buzz","fizz","7","8","fizz","buzz","11","fizz","13","14","fizzbuzz"] (...there MUST be a better way to handle nonNull...)
Ladder diagram?
I'm having trouble seeing how this wouldn't just make it harder.
nonnull = not . null To answer the original question, I'd probably use where for fz/bz and a map for the list, but what you have is a decent definition. 
http://www.reddit.com/r/haskell/comments/m7uph/whats_going_on_with_id_id_id_id_0/
Here we go. Just because somebody shouts in German from a podium, people have to be all "bad connotations" and whatnot. Listen. Just *listen*. Its like speaking telepathically through a machine. Don't you understand? 
the person talking certainly _is_ aware. that does not make it a good thing of course.
My German is a bit rusty and this seems funny ; can you please put English subtitles in the video?
A monadic solution import Control.Monad type Result = Either String Integer mypred::Integer-&gt;String-&gt;Integer-&gt;Result mypred m s n = if n `mod` m == 0 then Left s else Right n predicates::[Integer-&gt;Result] predicates = [ mypred 15 "FizzBuzz", mypred 5 "Buzz", mypred 3 "Fizz"] fizzbuzz::Integer-&gt;String fizzbuzz n = case foldM (flip ($)) n predicates of Left s -&gt; s Right i -&gt; show i main::IO () main = mapM_ (putStrLn . fizzbuzz) [1..100] 
Probably you'd need to use a mutable unboxed vector for the direct translation from C. I think the convolution using Repa would be the idiomatic way to do it in Haskell. Totally agree that the FRP approach would be useless when dealing with so many particles, maybe it would work but I would think it would be many orders of magnitude slower.
It's not just German, you twat.
I guess because he wants a copy so he can modify it and then return it without altering the passed value. 
Somewhat reminds me of /u/Tekmo's *foldl* library: https://hackage.haskell.org/package/foldl
you might like http://themonadreader.files.wordpress.com/2014/04/fizzbuzz.pdf 
Reducer transformers can definitely be encoded in a lens-like shape. Specifically, if the reducing function has the shape: step :: x -&gt; a -&gt; x Then a function that transforms that would have the shape: k :: (x -&gt; a -&gt; x) -&gt; (x -&gt; b -&gt; x) ... and that is isomorphic to: k' :: (a -&gt; x -&gt; x) -&gt; (b -&gt; x -&gt; x) ... which is isomorphic to: k'' :: (a -&gt; Endo x) -&gt; (b -&gt; Endo x) ... which is isomorphic to: k'' :: (a -&gt; Constant (Endo x) a) -&gt; (b -&gt; Constant (Endo x) b) ... and any `Traversal' a b` will type-check as the above type (because `Constant (Endo x)` is an `Applicative`). So therefore you can write the following function that converts a `Traversal` to a function between `Fold`s (from my `foldl` library): {-# LANGUAGE RankNTypes #-} import Control.Foldl (Fold(..)) import Data.Functor.Constant (Constant(..)) import Data.Monoid (Endo(..)) import Lens.Family2 (Traversal') pretraverse :: Traversal' a b -&gt; Fold b r -&gt; Fold a r pretraverse k (Fold step begin done) = Fold step' begin done where step' = flip (appEndo . getConstant . k (Constant . Endo . flip step)) Here are some example uses of `pretraverse`: -- Wrap a fold to only consume `Left` values pretraverse _Left :: Fold a r -&gt; Fold (Either a b) r -- Wrap a fold to only consume the left field of tuples pretraverse _1 :: Fold a r -&gt; Fold (a, b) r I'll be adding this to an upcoming release of `foldl`. I've opened [this issue](https://github.com/Gabriel439/Haskell-Foldl-Library/issues/23) to remind myself.
Wow, that's awesome.
Seems closer to the "step" functions of stream fusion. (i.e. the composable kernels wrapped in a nice algebra of consumers, transformers and producers). But with odd types. But with a special syntactic forms too? Am I missing something?
For the curious, a few more implementations including a Haskell 9P fizzbuzz fileserver: http://wanha.linkkijkl.fi/blog/fizzbuzz-horror#comment-340 These were part of a not so serious late night coding challenge amongst some student union members back in 2011, comments are encrypted in Finnish.
This is why I wrote unexceptionalio -- then the only thing you need special machinery for is async exceptions, which are really a different thing entirely.
Thompson's book was updated in 2011, and the Apress book looks like a good learning resource generally but I haven't read beyond a few chapters. There's a few odd typos where Mena writes "Platform does something" and he means GHC. (Also, how about Bird's Pearls book? Hudak, School of Music?) http://www.haskellcraft.com/craft3e/Home.html http://www.amazon.com/Beginning-Haskell-A-Project-Based-Approach/dp/1430262508
A reducer is basically a left fold minus the final cleanup at the end that makes it well behaved. data Fold a b where Fold :: (r -&gt; b) -&gt; (r -&gt; a -&gt; r) -&gt; r -&gt; Fold a b That form is very nicely behaved. Why? It is `Applicative`, a `Comonad`, a `Profunctor`, even a Monad if you are willing to have it build up everything it sees as part of its result. You can find that in Tekmo's `foldl` library or as one of a dozen fold types in my `folds` package. It is a crippled form of `Fold` (in either the Tekmo sense _or_ the `lens` sense), but not a full `Traversal`. I've written about this type across several articles on http://fpcomplete.com/user/edwardk buried in the series of posts on cellular automata, PNG generation and Mandelbrot sets.
Awesome. I should give Lamdu a spin some time... so far I've only read the description and thought, "this is the future".
Can you give a quick explanation of what this is doing? Thanks
Any luck?
Cool! Can you talk more about the implementation and which algorithm and optimizations you used? I'm a bit partial to the problem, having taken an NLP class that had [this](http://www.cs.jhu.edu/~jason/465/hw-parse/hw-parse.pdf) as an assignment.
Here is another one using function (-&gt; r) monad. fizzBuzz = do a &lt;- \x -&gt; if x `mod` 3 == 0 then "Fizz" else "" b &lt;- \y -&gt; if y `mod` 5 == 0 then "Buzz" else "" return (a ++ b)
Also the only thing that comes up on hoogle close to this is `foldlOf` from lens, but I'm not sure about the implementation or the semantics: https://www.fpcomplete.com/hoogle?q=%28x-%3Ea-%3Ex%29-%3E%28x-%3Eb-%3Ex%29&amp;env=ghc-7.4.2-stable-13.09 
maybe i'm missing it but how does this print "2" for fizzBuzz 2?
What's the difference between "well behaved" and "nicely behaved"? Composability vs. more instances?
Well, what I mean is this. With the extra (r -&gt; b) at the end you can 'fuse' two folds together without the result being forced to be a product. This lets you write: sum = Fold id (+) 0 count = Fold id (\n _ -&gt; n + 1) 0 Then we can define a `Num` instance for `Fold` using the `Applicative` instance for `Fold a`: instance Num b =&gt; Num (Fold a b) where (+) = liftA2 (+) ... instance Fractional b =&gt; Num (Fractional a b) where (/) = liftA2 (/) And you can compute the mean with mean = sum / count as a `Fold`. (Note: this is not the most numerically stable mean calculation!) With a transducer, from what I'm given to understand, without that final cleanup `(r -&gt; b)` at the end you can't calculate the mean directly, but you need to define something else after. Hiding the choice of `r` inside, existentially allows us to create a ton of standard instances for standard typeclasses over this abstraction. e.g. using the `Comonad` for a `Fold` it is possible to partially apply it to some input. By having that extra modification at the end the transducer itself becomes a `Functor`, but as it is `r` occurs in both positive and negative position, so you're cut off from that option.
It doesn't, you have to add another if..else in return. I believe I copy pasted this from some assignment where numbers not divisible either by 3 or 5 should return empty string.
`m ~&gt; s` is a function that returns `Just s` if its argument is divisible by `m` and `Nothing` otherwise. `3 ~&gt; "fizz" &lt;&gt; 5 ~&gt; "buzz"` is just the combination of the two via `Monoid`. If either side is `Nothing`, it just chooses the other side; if both sides are `Just` some string, it combines them. `fromMaybe . show &lt;*&gt; f` is the result of `f` if it is `Just`, otherwise it's the `String` representation of the argument to `f`. Finally, we `map` the whole thing over the input list, which normally would be something like [1..100].
Oh, ok. That's a neat trick! I was confused; when you said "it is a crippled form of fold", I thought you were talking about the `Fold` type you had just introduced, not transducers. I also think you forgot a `forall r.` in your `Fold` type.
Oh crap that's a `~&gt;` not a `-&gt;`. That's what was throwing me. I thought you were doing some weird function pattern match or something. Thanks!
&gt; nonnull = not . null Ah! Thank you. I tried (not null) in one configuration and got an error about expression type (Bool instead of a -&gt; Bool), tried it in another configuration and was told I had passed too many arguments to not, and both those errors actually made sense to me but there was no hint as to what I should actually be doing to glue those statements together, and I still haven't run into the dot operator in the tutorial (weird, it seems awfully fundamental). This is one of those situations where the big hump is knowing what to search for! :)
A good way to think about this is like when you describe a physical system in terms of a system of differential equations instead of as a function of time. The FRP framework then handles the "integration": the transition from "this is how these values are related to eachother" to "this is how these values evolve over time". Some systems are more naturally described by a declarative network of timeless relationships
You're welcome!
I know what FRP is, I just can't see any method of implementing an FRP sand game with the standard FRP semantics in with a reasonable complexity etc.
You can implement `Functor` and `Applicative` if you add the extra `r -&gt; b` to it.
yes, w/ something like `iso (Map.fromList . IntMap.toList) (IntMap.fromList . Map.toList)` then u can do stuff like `Map.empty &amp; intMap %~ IntMap.insert 5 "hello world"`
That's great! Minor typo: in section 3.4 it says "univerally quantified type" instead of "universally quantified type"
I can't wait to understand this.
If I recall correctly, you can try the syntax you want by modifying `compiler/parser/Parser.y.pp` in GHC to add a production to include `exp10` into `aexp`, like so: fexp :: { LHsExpr RdrName } : fexp aexp { LL $ HsApp $1 $2 } | aexp { $1 } aexp :: { LHsExpr RdrName } : qvar '@' aexp { LL $ EAsPat $1 $3 } | '~' aexp { LL $ ELazyPat $2 } | aexp1 { $1 } | exp10 { $1 } The last line is the one to add. I included `fexp` just for context, that's where application is actually defined. I never tested it thoroughly and don't remember if you needed to explicitly resolve some ambiguities, but it got basic cases like that. I particularly like way lambdas look: forM_ xs \x -&gt; do print x clearly a lambda, but also almost like it's a control structure with `\` separating parameters provided from bound variables for the body.
Thanks :) I'll fix this with this week's update.
 fizzbuzz n = take n $ zipWith (\i fb -&gt; head $ filter (not . null) [fb, show i]) [1..] fizzBuzzStream where fizzes = cycle ["","","Fiz"] buzzes = cycle ["","","","","Buzz"] fizzBuzzStream = zipWith (++) fizzes buzzes
I believe the "forall r." can be left implicit in the GADT version (but don't ask me why I don't have that level of expertise yet ;-)
That's the reducer, but then the "transducer" appears to be the arrow on reducers. {-# LANGUAGE GADTs #-} {-# LANGUAGE RankNTypes #-} {-# LANGUAGE TypeOperators #-} import Control.Arrow import Control.Category import qualified Prelude import Prelude hiding (id, (.)) data Fold a r where Fold :: (a -&gt; x -&gt; x) -&gt; x -&gt; (x -&gt; r) -&gt; Fold a r data Pair a b = Pair !a !b pfst :: Pair a b -&gt; a pfst (Pair a b) = a psnd :: Pair a b -&gt; b psnd (Pair a b) = b newtype (~&gt;) a b = Arr (forall r . Fold b r -&gt; Fold a r) instance Category (~&gt;) where id = Arr id Arr f . Arr g = Arr (g . f) amap :: (a -&gt; b) -&gt; (a ~&gt; b) amap f = Arr (\(Fold cons nil fin) -&gt; Fold (cons . f) nil fin) afilter :: (a -&gt; Bool) -&gt; (a ~&gt; a) afilter p = Arr $ \(Fold cons nil fin) -&gt; let cons' = \a x -&gt; if p a then cons a x else x in Fold cons' nil fin fold :: Fold a r -&gt; [a] -&gt; r fold (Fold cons nil fin) = fin . spin where spin [] = nil spin (a:as) = cons a (spin as) asequence :: (a ~&gt; b) -&gt; ([a] -&gt; [b]) asequence (Arr f) = fold (f (Fold (:) [] id)) aflatmap :: (a -&gt; [b]) -&gt; (a ~&gt; b) aflatmap f = Arr $ \(Fold cons nil fin) -&gt; Fold (\a x -&gt; foldr cons x (f a)) nil fin atake :: Int -&gt; (a ~&gt; a) atake n = Arr $ \(Fold cons nil fin) -&gt; let cons' = \a x n -&gt; if n &gt; 0 then cons a (x (n-1)) else x n in Fold cons' (const nil) (\x -&gt; fin (x n)) ~~You can't really replicate the `take` unless you have mutability, but it could perhaps be done if you wrap a monadic layer into the arrow.~~ The arrow allows us to write `take` (purely! Unlike Clojure's which requires an atom) which I don't think is possible (or meaningful?) as just a `Fold`.
Purescript has [row polymorphism](http://docs.purescript.org/en/latest/types.html#row-polymorphism) (basically records done right, ala OCaml objects). That is, the type of `{ first: "a" }` is `{ first :: Prim.String }` and this returns `"aq"` with no type declaration needed: (\x y -&gt; x.first ++ y.first) { first: "a" } { first: "q", last: "e" } (I hope Haskell gains row polymorphism some day..) Another Functional-to-JS language with this feature is [Opa](http://opalang.org/), but it seems a bit dead right now (and is also licensed under AGPL). Another feature is its [effect system](http://www.purescript.org/posts/Eff-Monad/), instead of a catch-all IO monad it has an Eff monad where the effects are part of the type (using again row types), so you can have a finer grained control on them. Purescript tries to be more general purpose functional programming, while [Elm](http://elm-lang.org/) is more focused on FRP on the browser. (Purescript has bindings to React but that's not really FRP) On the bikeshedding front, the [Function-Monad hierarchy](http://docs.purescript.org/en/latest/prelude.html) was done right!
When a type is left unquantified in a GADT then it's treated as an existential type by default.
TIL. Thanks.
I *really* like this one because it shows a completely different thought process.
&gt; On the bikeshedding front, the Function-Monad hierarchy was done right! Apart from Alternative/MonadPlus, which is still less than ideal. But [we're working on it](https://github.com/purescript/purescript-control/issues/6) :) 
In case you missed something: http://dev.stephendiehl.com/hask/
You need a finer grained Num hierarchy too! (but at same time, I would like to not have too many overhead for numeric code -- I suppose that's a factor in the flat Num hierarchy) Also: is a Monad instance automatically derived from Applicative and Bind? If not then Monad ought to be just something like an alias to (Applicative m, Bind m) I think I saw some of those fun-to-js languages where Num was more finer grained but I don't remember which one (or perhaps it was a Haskell proposal).
For me, the transition described above is seems more like beginner -&gt; intermediate. Because the shocking thing in haskell is just how large the beginner and intermediate categories are. It might even be worth considering an "advanced beginner" stage. I don't think one can say they're intermediate without some contributions to hackage or working on a functioning project as opposed to exercises and tutorials. I say this because I feel I'm at the beginner -&gt; intermediate transition stage and what's described above sounds somewhat similar to my experience so far.
You can learn a lot from reading blogs, participating to open source projects, ... But at the end, I guess you only become a good Haskell programmers whenever you manage to work with it on a day-to-day basis as a full-time job ;-) Anyhow amongst good resources that I know of there are: * http://www.haskellforall.com/ * http://lpuppet.banquise.net/blog/2014/05/10/7-startups-part-1-introduction-and-types/ * every video, papers, talks from well-known haskellers There are so many excellent pedagogical materials out there that is quite amazing really. I usually limit myself to a certain level of difficulty. As with music you don't want to read a manual from a mastery class while your are just starting to grok the basic.
I think the thing to do really, is to start reading the papers that accompany fun libraries, stick around the community and try and absorb the vocabulary. The worst thing you can do is go looking for advanced techniques and trying to slot them in. Most fun techniques have motivating examples that can be pretty intimidating until you run into a real-world issue that maps to the examples. I didn't get the point of generic traversal libs like `SYB` or `uniplate` until I was honest to goodness trying to build a mini expression language. I didn't get the point of `bound` and `de brujin indicies` until I really tried make a toy lambda calcus interpreter. What I did do was keep skimming over everything the haskell community threw my way so that when I ran into problems that could be mapped to the motivating examples from the papers / articles I had read I had the vocabulary to describe the problem and set of vetted solutions to apply to them. Then it's just a case of hammering away until you get it. Just like with maths.
It turns out to be a crippled form of both. ;)
We have `taking` in lens, which does just that, it takes a Fold (or a Traversal!) and truncates it at n elements giving a new Fold or Traversal. In this sense it is a generalized transducer. The notion of a transducer is related to the way Oleg builds mappings between iteratees as enumeratees. Most lens combinators restricted to the case that you have them taking in a Fold and spitting out a Fold are 'transducers'. 
So, this FSG entirely operates using cellular automata, and empty space is considered a particle too. Every pixel on the screen is processed each frame. More efficient implementations might use a quad-tree to partition the space, but that adds a lot of complexity to the implementation. It's quite playable though, if you can get it to compile, it's built against an old version of Repa.
&gt; So basically, the usual/imperative approach would be to have a list of particles and update them one at a time. For speed, there would be a 2d-array to quickly look up if there is a particle at a position. Most FSG's don't do this. They have a quad-tree and update each pixel on the map every frame (skipping over subtrees with no "active" particles). Usually this is based on some cellular automata like rules.
This. I try to find the libraries out there and then ask myself what problem would drive someone to build this solution? Even if I don't learn the answer right away the thought exercise primes me for when I do find the case where I need just that solution later on. If I do find a problem that fits the solution then I pick a fight with it. Why this solution? Why not something simpler? Try to remove part of the solution and see if it can still solve all the same problems. Does it get simpler? If so you might have a pull request to send. What if your problem gets more hairy? Does the library still offer a solution?
&gt;I've written about 11k lines of code Did you write an operating system? :-) I think I haven't written that much Haskell in the three years since I've started to use it for my personal projects. **edit:* forgot the unit (years)
Right, this works provided you don't have too many pixels on the map. It's what Falling Turnip, the only known FSG written in Haskell, does.
Regarding constructing anything from `Void`, you mention: &gt; In something like Haskell we have to resort to trickery, however, either using a “justifiable error” or an infinite loop But there is a nice way of defining `Void` (which I think I was first told about by Conor McBride) that nicely sidesteps this problem: newtype Void = Void { magic :: forall a. a } Now if you get passed a value of type `Void`, to construct any `a`, you can just use magic! For example, your `Show` instance can be written as: instance Show Void where show = magic Now you don't have to call `error` yourself or loop endlessly, but instead you let the liar pass you his lie, and use it against him. (I'm a bit late to the party, but somebody might still find this useful)
There was a project/book called "mezzo haskell" that I haven't heard much about for a while, but this is basically the problem it is supposed to solve. https://github.com/mezzohaskell/mezzohaskell Pinging /u/snoyberg. Is mezzo haskell dead?
What I have seen so far looks more like that : http://en.wikipedia.org/wiki/Function_block_diagram
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Function block diagram**](https://en.wikipedia.org/wiki/Function%20block%20diagram): [](#sfw) --- &gt; &gt;The __Function Block Diagram__ (__FBD__) is a graphical language for [programmable logic controller](https://en.wikipedia.org/wiki/Programmable_logic_controller) design, that can describe the function between input variables and output variables. A function is described as a set of elementary blocks. Input and output variables are connected to blocks by connection lines. &gt;Inputs and outputs of the blocks are wired together with connection lines, or links. Single lines may be used to connect two logical points of the diagram: &gt; &gt;* An input variable and an input of a block &gt;* An output of a block and an input of another block &gt;* An output of a block and an output variable &gt;The connection is oriented, meaning that the line carries associated data from the left end to the right end. The left and right ends of the connection line must be of the same type. &gt;Multiple right connection, also called divergence can be used to broadcast information from its left end to each of its right ends. All ends of the connection must be of the same type. &gt;Function Block Diagram is one of five languages for [logic](https://en.wikipedia.org/wiki/Logic) or [control](https://en.wikipedia.org/wiki/Control_theory) [configuration](https://en.wikipedia.org/wiki/Computer_configuration) supported by [standard](https://en.wikipedia.org/wiki/Standardization) [IEC 61131-3](https://en.wikipedia.org/wiki/IEC_61131-3) for a [control system](https://en.wikipedia.org/wiki/Control_system) such as a [Programmable Logic Controller](https://en.wikipedia.org/wiki/Programmable_Logic_Controller) (PLC) or a [Distributed Control System](https://en.wikipedia.org/wiki/Distributed_Control_System) (DCS). The other supported languages are [ladder logic](https://en.wikipedia.org/wiki/Ladder_logic), [sequential function chart](https://en.wikipedia.org/wiki/Sequential_function_chart), [structured text](https://en.wikipedia.org/wiki/Structured_text), and [instruction list](https://en.wikipedia.org/wiki/Instruction_list). &gt;==== &gt;[**Image**](https://i.imgur.com/PtSFCMe.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:FBS_Maximum.jpg) - *Simple function block diagram* --- ^Interesting: [^IEC ^61131-3](https://en.wikipedia.org/wiki/IEC_61131-3) ^| [^Function ^model](https://en.wikipedia.org/wiki/Function_model) ^| [^Functional ^flow ^block ^diagram](https://en.wikipedia.org/wiki/Functional_flow_block_diagram) ^| [^Block ^diagram](https://en.wikipedia.org/wiki/Block_diagram) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjjth8s) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjjth8s)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
As many notice there's a gap in intermediate level books/resources for Haskell, so a lot of the knowledge is just wrapped up in folklore. As such there's a strong social component to learning Haskell at the moment, much more so than many other languages. The ``haskell`` tag on Stackoveflow is actually pretty active and if you read through the back answers you'll learn a ton, there's a lot of gems there. Apart from that, hang out on IRC and even ask questions on the subreddit they quite often turn into really great discussions.
&gt; Another way to write it: &gt; &gt; data ECT o m a &gt; = Return a -- a -&gt; m r &gt; | Yield o (ECT o m a) -- o -&gt; m r -&gt; m r &gt; | M (m (ECT o m a)) You missed the Church-encoded version (which is in my mind the closest continuation-passing-style equivalent of the above.) newtype ECT o m a = EC { unEC :: forall r. (a -&gt; r) -&gt; (o -&gt; r -&gt; r) -&gt; (m r -&gt; r) -&gt; r } I think, it's very interesting to see all of these representations next to each other and compare their strengths and weaknesses. Thank you! &gt; I wonder what happens when you write... Sorry, do you really want to know? Or did you posted it as an exercise? I'm not sure and I don't want to spoil it. &gt; Regardless, I believe all this code is pretty much isomorphic to "ListT done right." Indeed. :) So, do you know if it exists in this CPS-ed form in some more-or-less standard library?
You're right and it's definitely something that we're working on. We have Linux bindists for major releases available from GitHub, and we plan to have bindists for Linux, Windows and Mac by the time 0.6 is released. This was also a major goal of the purescript-in-purescript project. It is possible to just `npm install purescript`, but the performance isn't good enough yet to replace the Haskell version (approximately 100x slower than the Haskell implementation). We're working on it though!
I think one of the biggest challenges for someone considering using PureScript for front-end development is whether I'll be able to be productive and be able to do all of the typical JS work I need to do in the new language. Basically as a PS user I'm not so much concerned about being shown how to do simple toy snippets in its syntax (I'm comfortable with Haskell though, so I'm not starting from scratch), but more about whether I'll be able to write something worth putting in production in it. Practical examples of manipulating the dom, interacting with existing libraries, writing test suites, potentially using existing MVC libraries etc. Basically how do I get "work" done.
I was just playing with the same idea, but in the applicative style shown by /u/geezusfreeek. m &gt;~ str = ZipList . cycle $ replicate (m-1) empty ++ [pure str] fizzbuzzS = getZipList $ fromMaybe . show &lt;$&gt; ZipList [1..] &lt;*&gt; 3 &gt;~ "fizz" &lt;&gt; 5 &gt;~ "buzz" Note: it also requires the trivial Monoid instance for ZipList.
In ELisp, mutation is hidden behind many pure-looking functions, that kind of style isn't easy...
If you are using a cellular automata(something that a lot of people have suggested already), these problems could be easily solved by designing the automata correctly, so that the important information is easily accessible. I guess that when on a wall, a water particle should slide to the closest corner, if there is any. So we need three kinds of walls: walls where the closest corner is to the left(`&lt;`), walls where the closest corner is to the right(`&gt;`), and walls where both corners are equidistant and thus water will drop to one side or the other randomly(it behaves like `&gt;` in some turns and `&lt;` on the others: we can represent it by `#`). There is also air(`-`) and water(`w`). So we have these rules: * A water block dissapears unless it's surrounded by walls to the left, right and below. * A block of air turns into a block of water if it's in any of these situations: -w &lt;&lt; w- &gt;&gt; w - * Walls obviously don't change(except the nondeterministic ones). For your scenario this would go like this: --w-w-- -&lt;&lt;#&gt;&gt;- -w---w- -&lt;&lt;#&gt;&gt;- w-----w -&lt;&lt;#&gt;&gt;- ------- w&lt;&lt;#&gt;&gt;w (Yes this means that two water blocks can fuse into one. If this bothers you, slightly more complex rules should fix it. I think this behaviour is better). The win is that all rules need to check only their immediate neighbourhood. Of course, checking for update *all* block in the screen(like you normally would do on a cellular automata) is clearly not necessary: you can keep track of where the "active blocks" are(sand and water) and check only in the neighbourhood of those. Together with the parallelism this should be reasonably performant. The information needed for walls can be packaged with the map, or, if the user can edit walls, recomputed each time new walls are added. If this approach does not seem realistic enough to you, the Falling Turnip game also uses a CA, but a more complicated one, so that may be interesting.
&gt; Yes this means that two water blocks can fuse into one. If this bothers you, slightly more complex rules should fix it. I think this behaviour is better It (and other things, but mostly this) actually bothers me a lot, but I can't see how you would solve it without huge performance problems.
What behaviour do you want then? If the water block do not fuse, where do they go? I'm sure there's behaviour you could be happy with implementable with a CA.
That's just minor optimization issues. I'm mostly thinking about the concept of how to implement it.
I liked the walk-through of this problem at https://web.archive.org/web/20121021233736/http://dave.fayr.am/posts/2012-10-4-finding-fizzbuzz.html , ending up in a quite nice, monoid-based solution.
Oh, that's true. And great. I'm going to add a note! Thanks!
Yeah! I was hoping to get to that level of generality eventually, but I kind of wanted to find a path that's a bit more obvious than just jumping to lenses.
Yes, closer to fusion step function transformation/composition. The idea is very simple. A reducing function is the type of function you'd pass to foldl: x -&gt; a -&gt; x and a transducer is a function of reducing function to reducing function: (x -&gt; a -&gt; x) -&gt; (x -&gt; b -&gt; x) That's it. -- Transducers in Haskell mapping :: (b -&gt; a) -&gt; (r -&gt; a -&gt; r) -&gt; (r -&gt; b -&gt; r) mapping f xf r a = xf r (f a) filtering :: (a -&gt; Bool) -&gt; (r -&gt; a -&gt; r) -&gt; (r -&gt; a -&gt; r) filtering p xf r a = if p a then xf r a else r flatmapping :: (a -&gt; [b]) -&gt; (r -&gt; b -&gt; r) -&gt; (r -&gt; a -&gt; r) flatmapping f xf r a = foldl xf r (f a) -- for exposition only, yes, conj is gross for lazy lists -- in Clojure conj and left folds dominate conj xs x = xs ++ [x] xlist xf = foldl (xf conj) [] -- build any old list function with its transducer, all the same way xmap :: (a -&gt; b) -&gt; [a] -&gt; [b] xmap f = xlist $ mapping f xfilter :: (a -&gt; Bool) -&gt; [a] -&gt; [a] xfilter p = xlist $ filtering p xflatmap :: (a -&gt; [b]) -&gt; [a] -&gt; [b] xflatmap f = xlist $ flatmapping f -- again, not interesting for lists, but the same transform -- can be put to use wherever there's a step fn xform :: (r -&gt; Integer -&gt; r) -&gt; (r -&gt; Integer -&gt; r) xform = mapping (+ 1) . filtering even . flatmapping (\x -&gt; [0 .. x]) print $ xlist xform [1..5] -- [0,1,2,0,1,2,3,4,0,1,2,3,4,5,6] I hope that clarifies somewhat. 
It was my understanding that (for example) the Qualcomm MSM7200 was running OK:L4 (and possibly Rex as a task) for baseband but that android wasn't "under" L4 in this arrangement but I'm not all too sure...
It is an exercise that I didn't have time to do myself just yet. Spoilers welcome. &gt;So, do you know if it exists in this CPS-ed form in some more-or-less standard library? I don't know of any such thing.
I think Tekmo and Edward's commentary about lens and traversals are the most interesting developments. I was hoping to draw these things back to basic Church-encoded lists somehow but haven't had a lot of success—but the Fold and Traversal types are much more closely related.
&gt; contributions to hackage or working on a functioning project any tips for finding things to contribute to? 
&gt; Did you write an operating system? :-) I wish. :) No, they're small tutorials/learning projects.
Very cool, great to know that :D
Typo: The package is [generics-sop](https://hackage.haskell.org/package/generics-sop)
No, new versions are the major reason build plans can't be created. But arbitrarily picking some older version to support instead doesn't make much sense to me, sooner or later someone will need to have a lower bound on the latest release. Check open stackage issues to see if some new release isn't widely supported yet. 
I haven't done this yet, my guess is that it's not easy to find such projects, but I think it would be perfectly fine to do something oneself, just being careful to keep the scope limited enough. (also - not implying open source contributions are an absolute requirement - just having something to work on thats not a tutorial)
&gt; It is an exercise that I didn't have time to do myself just yet. Spoilers welcome. It looks like a great exercise. Let's take a look at the different pieces: EC (\return' yield' -&gt; yield' 3 (return' ())) This is the actual code for yield in this monad. I call it `ectYield` in my code, so the above line is *exactly* `ectYield 3`. EC (\return' yield' -&gt; putStrLn "Does it ever get here?" &gt;&gt;= return') Would be the exact code for the `lift (putStrLn "...")`. And it's quite obvious that it's equivalent to: EC (\return' yield' -&gt; do putStrLn "..." return' () ) So, your code seems to be similar to the following `ECT` code: do ectYield 3 lift $ putStrLn "..." Which is an obviously sensible code in a pipe-like monad. But, is it *really* that code? For that we only need to check how does `&gt;&gt;=` bind something, specifically `ectYield 3`, to something. And that would look very differently (we can just do the substitutions mechanically: EC (\return' yield' -&gt; yield' 3 (putStrLn "..." &gt;&gt; return' ())) So, that's not what that code does. But, it does something (it's type correct) -- what is it then? (This is the last point to stop and think, before I reveal the result.) You could just mechanically thread through the `runECT`, which turns it into an understandable "ListT". But, there is also a trick I use: An ECT computation is quantified over that `r` type variable. We have to return an `r` at the end, and there is only two ways to create a value of that unfathomable `r` type: the `return'` and the `yield'`. So, you look at where do the _final result_ of type `m r` come from? Was there a `return'` involved, or was it a `yield'`? And what side-effects were involved on the way? (The `return'` and `yield'` don't have any side-effects of their own.) And then the answer is clear: the resulting `r` comes from the final `return'`, the first `yield'` was just a decoy (its result is ignored). So: EC (\return' yield' -&gt; do yield' 3 (return' ()) putStrLn "Does it ever get here?" return' () ) Is equivalent to: lift $ putStrLn "Does it ever get here?" So, it does print that string. It actually doesn't yield anything! Tricky. Wow, this turned out to be an extremely good exercise, my first guess was actually wrong! I should use it if/when I teach something like this again. (And I really-really hope that I got it right. :))
That's some poor UI, replacing placeholder images with big animated gifs when you mouse over them, forcing the user to wait for some time without any visual feedback. I thought the site was broken at first, but no... the author just has some kind of aversion to html5 video.
`sequencue` ?
Let's try to figure it out! Let's see, we just learned about logarithms, which should be very close to roots, except "the opposite" in some sense we need to make precise. In order to do so, let's formalize our understanding of logarithms. What is log_2(8)? Arithmetic says it should be 3, but that doesn't fit well with the intuition of containers and positions, since the type `Eight` (the type with 8 zero-argument constructors) isn't a container. The isomorphic type `Three -&gt; Bool` (aka 2^3 ), however, is a container with three positions, each of which is filled with a value of type `Bool`, so we can say that `Log Bool (Three -&gt; Bool) = Three`. Generalizing and implementing the relation via a GADT: data Log b a where MkLog :: p -&gt; Log b (p -&gt; b) Unlike type addition via `Either a b`, which is defined for any types `a` and `b`, the logarithm of a type isn't defined for all types. But have we defined it on enough types? With the implementation above, `Log Bool Eight` is not a valid type, even though it corresponds to an arithmetic expression which evaluates to 3. Why isn't `Log Bool Eight` a valid type isomorphic to `Three`? Maybe the implementation isn't general enough, I don't know. Or maybe it is the correspondence between types and numbers which is too limited? From examples such as the following 2 + 2 ~ Either Bool Bool ≈ Four ~ 4 I had concluded that arithmetic expressions were equal iff their corresponding (~) types were isomorphic (≈) to each other, as long as the two types exist. But maybe instead instead of "isomorphic to each other", it should be something else, let's call it "congruent" (≈≈) with each other. To accommodate cases such as `Log Bool Eight`, I would define congruence like this: a type `f a` is considered congruent with `f b` if there is an isomorphism between `a` and `b`, even if `f a` and `f b` themselves are uninhabited or non-isomorphic. This allows us to smuggle a `Three -&gt; Bool` inside the `Log` without having to figure out what the log of a non-arrow type might mean. log_2(8) ~ Log Bool Eight ≈≈ Log Bool (Three -&gt; Bool) ≈ Three ~ 3 With this new correspondence rule, we can verify the usual logarithm laws: log_b(b) ~ Log b b ≈≈ Log b (() -&gt; b) ≈ () ~ 1 log_b(1) ~ Log b () ≈≈ Log b (Void -&gt; ()) ≈ Void ~ 0 a1*log_b(a2) ~ (a1, Log b a2) ≈≈ (a1, Log b (p2 -&gt; b)) ≈ (a1, p2) ≈ Log b ((a1, p2) -&gt; b) ≈≈ Lob b (a1 -&gt; p2 -&gt; b) ≈≈ Log b (a1 -&gt; a2) ~ log_b(a2^a1) log_b(a1) + log_b(a2) ~ Either (Log b a1) (Log b a2) ≈≈ Either (Log b (p1 -&gt; b)) (Log b (p2 -&gt; b)) ≈ Either p1 p2 ≈ Log b (Either p1 p2 -&gt; b) ≈≈ Log b (p1 -&gt; b, p2 -&gt; b) ≈≈ Log b (a1, a2) ~ log_b(a1*a2) Clearly my definition of congruence needs some work, as I am treating the uninhabited type `Void` differently from the uninhabited type `Log Bool Eight`, and I had to postulate that `a1` was isomorphic to `p1 -&gt; b` for some `p1`, but hopefully the idea is clear. Anyway, let's try to apply all of this to roots now. What is the third root of 8? Arithmetic says it should be 2, but in order to obtain an intuition, let's look at the isomorphic type: `Root Three (Three -&gt; Bool) = Bool`. In GADT format: data Root n a where MkRoot :: b -&gt; Root n (n -&gt; b) So the `n`th root of a container with `n` positions, each of which is filled with a value of type `b`, is `b`. Finally, let's check the root laws (which I had to [look up](http://www.mathwords.com/s/square_root_rules.htm)): sqrt(a^2) ~ Root Bool (Bool -&gt; a) ≈ a ~ a root_n(a1)*root_n(a2) ~ (Root n a1, Root n a2) ≈≈ (Root n (n -&gt; b1), Root n (n -&gt; b2)) ≈ (b1, b2) ≈ Root n (n -&gt; (b1, b2)) ≈≈ Root n (n -&gt; b1, n -&gt; b2) ≈≈ Root n (a1, a2) ~ root_n(a1*a2) root_n(a1)^a2 ~ a2 -&gt; Root n a1 ≈≈ a2 -&gt; Root n (n -&gt; b1) ≈ a2 -&gt; b1 ≈ Root n (n -&gt; a2 -&gt; b1) ≈≈ Root n (a2 -&gt; n -&gt; b1) ≈≈ Root n (a2 -&gt; a1) ~ root_n(a1^a2) One major difference between the above formalization and the container-based interpretation quoted by the OP and explained by McBride on StackExchange is that I consider the log and root of a type `p -&gt; b` of kind `*`, whereas containers are type constructors of type `* -&gt; *`. In both cases, the logarithm is only defined if the type or container has a particular form. Their `Log f` corresponds to my `Log x (f x)`, and thus if they had a `Root f`, it would correspond to my my `Root n (f n)`. In other words, a Naperian type is a type constructor of the form `(p -&gt;)`, that is, a container with a fixed number of positions and a variable content, and the log of this container is this fixed number of positions; while a "rootable" type is a type constructor of the form `(-&gt; b)`, that is, a container with a fixed content and a variable number of positions, and the root of this container is this fixed content type. Does that sound about right?
&gt; Familiarity with monads makes working with LINQ trivial in C#. Additionally, the trend these days is that OO languages are incorporating some form of lambdas, and that along with that monads are covertly becoming a common design pattern in libraries (though not a full fledged, first-class abstraction). Two example interfaces from Java 8 (not all methods shown): interface Optional&lt;T&gt; { static &lt;T&gt; Optional&lt;T&gt; of(T value); &lt;U&gt; Optional&lt;U&gt; map(Function&lt;? super T, ? extends U&gt; mapper); &lt;U&gt; Optional&lt;U&gt; flatMap(Function&lt;? super T, Optional&lt;U&gt;&gt; mapper); } interface Stream&lt;T&gt; { static &lt;T&gt; Stream&lt;T&gt; of(T t); &lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T,? extends R&gt; mapper); &lt;R&gt; Stream&lt;R&gt; flatMap(Function&lt;? super T,? extends Stream&lt;? extends R&gt;&gt; mapper) } We also increasingly see the same pattern in third-party libraries.
Thanks for the report! I've replaced the GIFs with HTML5 video (the mp4s seem way bigger than the GIFs, actually, but maybe I'm doing it wrong... the WebMs are good though)
&gt; Can you give an example of how you might use the monoid pattern? I know I'm asking a lot, I'm just trying to understand why they are so useful to you. Well, I'm not OP, but I often notice that some problem I'm working on has monoids involved. For example, I was recently doing some work on randomized collateral selection for asset-backed bond portfolios, where a big part of the solution hinged on representing statistics about the state of the portfolio as a commutative group (associative, identity, commutative and inverses), which for the purposes of the problem I was tackling one could think of as "monoids with 'undo.'" It gives a principled way of separating some of the algorithms from the data types that they operate on—the random selection process just needs to know that the statistics object is a commutative group, and what predicate or function it needs to use to evaluate whether a particular change to it is "good" or "better." In addition, since the product of two commutative groups is also a commutative group, that gives some good opportunities for modularizing further by splitting a complex state descriptor into multiple smaller, decoupled objects that can be assembled together at runtime. Finally, the algebraic structures' laws make for good unit test cases.
&gt; But keep in mind that you can perform stateful computations in Haskell, you just need to be explicit about what that state is. Yup, this is a key idea. Newcomers to Haskell often take the wrong message from the "state is bad" pronouncements. (Or alternatively put, the "state is bad" pronouncements are confusing!) The key thing is that very bad is *state spaghetti*: where the web of state dependency is tangled beyond human comprehensibility. One of the arguments for OOP is that it, ostensibly, gets state spaghetti under control by encapsulating state inside objects. I think that argument is fundamentally flawed. Just because you can't directly observe or effect the mutation of a field doesn't mean that the mutation of the field doesn't affect you in some other way. My favorite example problem for illustrating this is: how do you assign incremental numeric tags to the nodes of a tree? In an imperative language you'd have a counter variable and increment it as you go along. Well, you can do the same in Haskell and it's a good solution (IMO): {-# LANGUAGE DeriveFunctor, DeriveFoldable, DeriveTraversable #-} import Data.Foldable (Foldable) import Data.Traversable (Traversable, traverse) import Control.Monad.State data BTree a = Leaf | Branch a (BTree a) (BTree a) deriving (Show, Functor, Foldable, Traversable) tagFrom :: (Traversable t, Enum b) =&gt; b -&gt; t a -&gt; t (b, a) tagFrom init xs = evalState (traverse step xs) init where step a = do b &lt;- postIncrement return (b, a) postIncrement :: Enum b =&gt; State b b postIncrement = do result &lt;- get put (succ result) return result This is a stateful solution, but it's statically delimited so that the only piece of state that's ever relevant is contained in the scope of `tagFrom`. Which makes it easy to reason about.
For comparison, here's similar code using GHC.Generics: http://lpaste.net/109034
Yes, this clarifies a lot what is intended. Thank you for putting in the types! So I went ahead and refactored the types to make them conform to your terminology: {-# LANGUAGE Rank2Types #-} -- For example using Vector instead of list import qualified Data.Vector as V -- Left reduce type Reducer a r = r -&gt; a -&gt; r -- Here's where then rank-2 type is needed type Transducer a b = forall r . Reducer a r -&gt; Reducer b r -- Left fold class Foldable t where fold :: Reducer a r -&gt; r -&gt; t a -&gt; r class Conjable f where empty :: f a conj :: Reducer a (f a) mapping :: (b -&gt; a) -&gt; Transducer a b mapping f xf r a = xf r (f a) filtering :: (a -&gt; Bool) -&gt; Transducer a a filtering p xf r a = if p a then xf r a else r flatmapping :: Foldable f =&gt; (a -&gt; f b) -&gt; Transducer b a flatmapping f xf r a = fold xf r (f a) -- I changed Rich Hickey's code to be more general than just list -- but accept anything Conjable xlist :: (Foldable f, Conjable f) =&gt; Transducer a b -&gt; f b -&gt; f a xlist xf = fold (xf conj) empty -- build any old Foldable function with its transducer, all the same way xmap :: (Foldable f, Conjable f) =&gt; (a -&gt; b) -&gt; f a -&gt; f b xmap f = xlist $ mapping f xfilter :: (Foldable f, Conjable f) =&gt; (a -&gt; Bool) -&gt; f a -&gt; f a xfilter p = xlist $ filtering p xflatmap :: (Foldable f, Conjable f) =&gt; (a -&gt; f b) -&gt; f a -&gt; f b xflatmap f = xlist $ flatmapping f -- Stuff specialized to lists. -- To use another type, just make it a Foldable and Conjable. instance Foldable [] where fold = foldl -- for exposition only, yes, conj is gross for lazy lists -- in Clojure conj and left folds dominate instance Conjable [] where empty = [] conj xs x = xs ++ [x] -- Note: the type does not say anything about Foldable or Conjable, -- even though the implementation just happens to use a list! xform :: Transducer Integer Integer xform = mapping (+ 1) . filtering even . flatmapping (\x -&gt; [0 .. x]) -- Again, this can munge anything Foldable and Conjable, not just a list. munge :: (Foldable f, Conjable f) =&gt; f Integer -&gt; f Integer munge = xlist xform -- munge a list -- [0,1,2,0,1,2,3,4,0,1,2,3,4,5,6] example1 :: [Integer] example1 = munge [1..5] -- Implement Foldable, Conjable type classes for Vector instance Foldable V.Vector where fold = V.foldl instance Conjable V.Vector where empty = V.empty conj = V.snoc -- return a vector rather than a list; note the fact that munge actually -- internally uses a list example2 :: V.Vector Integer example2 = munge $ V.enumFromN 1 5 
I noticed that nobody proposed an Arrow-based solution. In your special case of only 2 variables, you can cobble together a poor man's state monad out of first, second, and (&gt;&gt;&gt;). import Control.Arrow ((&gt;&gt;&gt;), first, second) -- For functions: -- first :: (a -&gt; c) -&gt; (a,b) -&gt; (c,b) -- second :: (b -&gt; c) -&gt; (a,b) -&gt; (a,c) -- (&gt;&gt;&gt;) = flip (.) -- |if, adapted to the function level. if' :: (a -&gt; Bool) -&gt; (a -&gt; a) -&gt; (a -&gt; a) -&gt; a -&gt; a if' f ifB elseB v = if f v then ifB v else elseB v -- |conditional execution, lifted to the function level. cond :: (a -&gt; a) -&gt; (a -&gt; Bool) -&gt; a -&gt; a cond ifB f = if' f ifB id foo :: a -&gt; a -&gt; [a] foo = curry $ onX f `cond` ifX c &gt;&gt;&gt; if' (ifX c) (onX g) (onX h) &gt;&gt;&gt; onX f &gt;&gt;&gt; onY f `cond` ifY a &gt;&gt;&gt; onX g `cond` ifY b &gt;&gt;&gt; \(x,y) -&gt; [x,y] where onX = first onY = second ifX f = fst . first f ifY f = snd . second f
[Working link](http://www.doxiq.com/posts/doxIQ-data-insight-Haskell-programers-read-code/)