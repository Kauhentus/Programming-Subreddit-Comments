I think the title should be changed to say "android version coming soon" or similar. It implies that there are instructions to download a haskell made android app onto your phone. EDIT: Awesome job so far, I really don't want to diminish the work you've done and look forward to the upcoming android version.
No need to throw seq into such bad company as exceptions and unsafePerformIO. Relational parametricity isn't weakened too much by seq alone. http://www.iai.uni-bonn.de/~jv/JV04.html And, of course, http://www-ps.iai.uni-bonn.de/ft
Hooray, you saved us!
The "logic bits" are just constraint solving algorithms. Logic languages use this as their semantics, whereas functional languages stick to some flavour of the lambda calculus. The type inference algorithms of most functional languages resemble a very simple logic language as they must do constraint resolution and unification to assign generalised types to expressions; but the language semantics are purely based on lambda-calculus style normalisation. There are languages that have both "logic bits" and "functional bits" in their semantics, for example Lambda Prolog, which is a logic language that uses (a restricted form of) higher-order unification, so computation can be expressed either as normalisation of a lambda term or some sort of logical predicate, and expressions can contain lambda terms. Expressions are considered "equal" if they are alpha-eta-beta equivalent. This allows the logic to be very expressive indeed but comes at a big cost: the loss of decidability of unification in general; and also the lack of a single _most general_ unifier in some cases. Nevertheless, broad usable subsets of this problem exist (e.g pattern unification) and provide a solution that works in most cases and does what you want. Dependently typed languages, because they allow arbitrary lambda terms in their types, must use such techniques for their type inference algorithms.
Have a look at: * CodeBunk http://codebunk.com/ "Realtime Collaborative Editor with Code Execution, REPLs and Chat." * Ideone http://ideone.com/ "Ideone is an online compiler and debugging tool which allows you to compile source code and execute it online in more than 60 programming languages." * codepad http://codepad.org/ "codepad.org is an online compiler/interpreter, and a simple collaboration tool. It's a pastebin that executes code for you. You paste your code, and codepad runs it and gives you a short URL you can use to share it."
Does anyone use Heist to template plain text or other non-html formats?
Heist was designed specifically for templating XML and HTML documents. Its templates must be well-formed HTML5 or XML (with a few necessary exceptions). I guess you could do plain text like this as long as your text was also well-formed. I don't know if anyone is doing this though.
Well, you could say that the boxes just look at the values, they do not consume them. Rich Hickey likes talking about values as being observed, not used.
It's GHCi, not an IDE, but [tryhaskell.org](http://tryhaskell.org/) exists.
Analogies are not definitions, so the can't perfectly describe something. I find they still have value in moving understanding forward.
No, I was wrapping my head around Haskell itself. No FRP. Definitely going to use FRP for the next iteration whenever I get time for it (famous last words).
You need to acquire some experience in deciphering the compiler's error messages, but after that it is just "follow the white rabbit". With time you get more daring, and all the weird stuff you keep reading in /r/haskell begins to make sense.
As a technical detail that matters a ton, that's not a valid Monad instance.
Great work Doug! Looks awesome. 
Neato! I was hoping you'd be able to clear that up :) I have a couple morte-related questions: * Are functions taking types as arguments, or am I misreading syntax? (e.g., the `\(Bool : *)` in your pasted example) * Is there a good place to discuss morte? Perhaps a mailing list or IRC channel? I'm very curious to hear about updates!
A coworker of mine pronounces it "hask-ELL"; it bugs me to no end, but it does distinguish.
Taglines like that are a good way to make people doing good work cranky and add to the delusional FP FUD.
On the contrary, the correct pronunciation of the name "Pascal" is pas-KALL, at least according to my experience of people thus named, and [Wikipedia](http://en.wikipedia.org/wiki/Pascal_%28given_name%29). The differing stress on HAS-kell and pas-KALL already suffices to distinguish them. EDIT: On the other hand, what appears to be the web version of the Oxford English Dictionary [seems to disagree with me](http://www.oxforddictionaries.com/definition/english/pascal). Strange, because I've rarely heard anyone pronounce Pascal as PAS-kal. 
Beat me to it. =)
Check the first monad law: return a &gt;&gt;= f = f a but return a &gt;&gt;= f = Counter a 0 &gt;&gt;= f = let Counter x y = f a in Counter x (i + y + 1) which isn't the same. Counter isn't a Monad. You can legally make a notion of a `step` operation that bumps the counter by one, and then count the `step`s, but not the `(&gt;&gt;=)`s. step :: Counter () step = Counter () 0
It would be great if we saw more users contribute to haskell. I think this could become a great resource on getting people challeneged with Haskell. 
I really enjoyed this talk. The Nested datatype given is also pretty nice: data Flat x data Nest o i data Nested fs a where Flat :: f a -&gt; Nested (Flat f) a Nest :: fs (f a) -&gt; Nested (Nest fs f) a
This audio is horrid for the one speaker.
Nice talk! /u/edwardkmett, I'm curious about your comment at 25:45. What's the subtlety you're referring to?
Doesn't seem to be up yet?
It gets much better 5 minutes in.
Yes, they are taking types as arguments! That's one of the cool features of the polymorphic lambda calculus. In fact, `ghc` does this, too, under the hood. In Haskell, polymorphic types are literally just functions of types and you will see this if you inspect the generated System-Fw core. More generally, you can classify the type systems of the lambda cube by whether or not they permit the following types of functions: 1) Functions from terms to terms 2) Functions from types to terms 3) Functions from terms to types 4) Functions from types to types All the type systems in the lambda cube support (1): functions from terms to terms. That's basically ordinary lambda calculus. A "polymorphic" lambda calculus is one that accepts (2): functions from types to terms. An example of such a function is the polymorphic identity function: \(a : *) -&gt; \(x : a) -&gt; a The first argument of that function is a type (i.e. `a`) and the second argument is a term (i.e. `\(x : a) -&gt; a`). A "dependently typed" lambda calculus is one that accepts (3): functions from terms to types. This is what lets you do arbitrary computation at the type level. I don't have a short example of this off the top of my head, but maybe somebody else does. A "higher-kinded" lambda calculus is one that accepts (4): functions from types to types. A good example of this is the `Maybe` type constructor, defined in Morte like this: \(a : *) -&gt; forall (x : *) -&gt; (a -&gt; x) -&gt; x -&gt; x That is a function that takes a type as an argument (i.e. `a`), and returns a new type (i.e. `forall (x : *) -&gt; (a -&gt; x) -&gt; x -&gt; x`). In fact, there is a principled way to enable or disable those four options. This more general framework is called a [pure type system](http://en.wikipedia.org/wiki/Pure_type_system) and Simon Peyton Jones gives an excellent tutorial on them in [this paper](http://research.microsoft.com/~simonpj/Papers/henk.ps.gz). I based the implementation of Morte very heavily off that paper. Once you set up such a pure type system, enabling or disabling something like dependent types is a one-line code change. [This four-line function](https://github.com/Gabriel439/Haskell-Morte-Library/blob/master/src/Morte/Core.hs#L140) is responsible for enabling all four features, and you can turn any of them off by just replacing one of those lines with a `Left` to indicate that the feature no longer type-checks. Regarding Morte discussion, I will create a Google groups mailing list tonight and then reply back here once it is up.
Don't you mean `Counter () 1`?
Also, thanks! Glad you enjoyed it. :) Feel free to email me with any questions, comments, ideas, etc.
Like many others, I have downvoted this post. I think it's important to explain why. I'll begin with the parts I liked. My favourite part about this post is its title. "[MaL] Monads are Like" is a reference to the fact that there are already too many monad tutorials, and it's funny to think that tutorial authors would be familiar enough with the culture of our subreddit to tag their post appropriately, yet misguided enough to write a monad tutorial. I also liked the first sentence. "Turtles all the way down" reminded me of 3-Lisp and the idea of an "infinite tower of interpreters", and since monads are often used for the syntax and/or implementation of small DSLs, I was expecting some Acme package with a DSL for interpreting that very same DSL, or something along those lines. I'm afraid it all goes downhill from there. The word "class" is used incorrectly, the monad instance doesn't satisfy the monad laws, sentences make no sense, "discrete" is used as a synonym for "finite", the code which produces the infinite loop is missing, and claiming that an infinite loop is somehow "breaking" Haskell is na√Øve at best. Additionally, there is more to the "turtles all the way down" story than just the fact that there are infinitely-many turtle, and as such, an infinite loop is not a very satisfying explanation of why "monads are like turtles". Especially given the fact that implementing an infinite loop is even easier if you *don't* use monads. I expected something from this post, and then I was disappointed, which is why I downvoted.
&gt; I think what's missing is strong community desire to change. Which, in turn, is because the change has vanishingly little if any upside, and enormous and obvious costs. Seems like at least in this case, the system is working :)
Well they are directly inspired by the feature in Haskell, which is directly inspired by mathematical set builder notation. A lot of people feel that comprehensions are easier to read. In languages that don't support proper function composition or partial application, they are often right.
Sure, you can sort of do it in C++, which lets you pass functions as type-level parameters: #include &lt;numeric&gt; #include &lt;iostream&gt; #include &lt;vector&gt; using namespace std; // class Monoid a (f :: a -&gt; a -&gt; a) where // mappend = f // mempty :: a template&lt;class T, T f(T const&amp;, T const&amp;)&gt; struct Monoid { typedef T type; static T append(T const&amp; a, T const&amp; b) { return f(a, b); } static const T empty; }; // add :: Num a =&gt; a -&gt; a -&gt; a template&lt;class T&gt; T add(T const&amp; a, T const&amp; b) { return a + b; } // multiply :: Num a =&gt; a -&gt; a -&gt; a template&lt;class T&gt; T multiply(T const&amp; a, T const&amp; b) { return a * b; } // instance Monoid Int (+) where mempty = 0 template&lt;&gt; const int Monoid&lt;int, add&lt;int&gt;&gt;::empty = 0; // instance Monoid Int (*) where mempty = 1 template&lt;&gt; const int Monoid&lt;int, multiply&lt;int&gt;&gt;::empty = 1; template&lt;class T&gt; using SumMonoid = Monoid&lt;T, add&gt;; template&lt;class T&gt; using ProductMonoid = Monoid&lt;T, multiply&gt;; // mconcat = fold mappend mempty template&lt;class M&gt; typename M::type mconcat(vector&lt;typename M::type&gt; const&amp; xs) { return accumulate(begin(xs), end(xs), M::empty, M::append); } template&lt;class T&gt; T sum(vector&lt;T&gt; const&amp; xs) { return mconcat&lt;SumMonoid&lt;T&gt;&gt;(xs); } template&lt;class T&gt; T product(vector&lt;T&gt; const&amp; xs) { return mconcat&lt;ProductMonoid&lt;T&gt;&gt;(xs); } int main() { // This compiles. vector&lt;int&gt; xs { 1, 1, 2, 3, 5, 8 }; cout &lt;&lt; "Sum: " &lt;&lt; sum(xs) &lt;&lt; '\n' &lt;&lt; "Product: " &lt;&lt; product(xs) &lt;&lt; '\n'; // This does not. // (‚Äúundefined reference to Monoid&lt;double, ‚Ä¶add‚Ä¶&gt;::empty‚Äù) vector&lt;double&gt; ys { 1.1, 2.2, 3.3 }; cout &lt;&lt; "Sum: " &lt;&lt; sum(ys) &lt;&lt; '\n'; } You have to specify which monoid you‚Äôre using, but at least this demonstrates how Haskell‚Äôs `Monoid` typeclass is just a special case where the function is partially applied (cf. `SumMonoid` and `ProductMonoid`). 
Saw this earlier, linked to from elsewhere. Saw no Haskell on the initial screen (only scripting languages!), clicked "vote for yours." Saw no Haskell there either. Also no Smalltalk. Closed the page, because screw these hipsters. I never bothered scrolling down. They could have put the "Coming Soon" bit higher up. :)
I hope the other 4 were better. :P
Try saying Hhhhaskell the first time.
is this seriously a problem? you can't correct them? this entire post seems divisive and worthy of ridicule. sorry to be so blunt, but knee jerk posts like this do have effects on communities.
The presentation was great, I had recent been reading about and apply some comonads so this was very timely for me.
Its up. You just have to create an account. Then you will see it.
Its actually already up. Went live earlier this week. You just have to make an account.
not... really...? I'm still an undergrad the haskell documentation is pretty straightforward.
I would like to butt in from my vast high school French class experience and say that technically in French it's pas-kall, but since it fell at the end of the phrase when the speaker said it the kall received prosodic stress. I'm not sure I believe that French has "no lexical stress"... I'd think just reduced drastically, but anyway: http://en.wikipedia.org/wiki/Stress_%28linguistics%29 http://www.forum.french-linguistics.co.uk/forum/topics/stress-in-french
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Stress (linguistics)**](https://en.wikipedia.org/wiki/Stress%20%28linguistics%29): [](#sfw) --- &gt;In [linguistics](https://en.wikipedia.org/wiki/Linguistics), __stress__ is the relative emphasis that may be given to certain [syllables](https://en.wikipedia.org/wiki/Syllable) in a word, or to certain words in a phrase or sentence. Stress is typically signaled by such properties as increased [loudness](https://en.wikipedia.org/wiki/Loudness) and [vowel length](https://en.wikipedia.org/wiki/Vowel_length), full articulation of the vowel, and changes in [pitch](https://en.wikipedia.org/wiki/Pitch_(music\)). The terms *stress* and *[accent](https://en.wikipedia.org/wiki/Accent_(phonetics\))* are often used synonymously, but they are sometimes distinguished, with certain specific kinds of prominence (such as [pitch accent](https://en.wikipedia.org/wiki/Pitch_accent), variously defined) being considered to fall under accent but not under stress. In this case, stress specifically may be called __stress accent__ or __dynamic accent__. &gt; --- ^Interesting: [^Syllable](https://en.wikipedia.org/wiki/Syllable) ^| [^Pitch ^accent](https://en.wikipedia.org/wiki/Pitch_accent) ^| [^Vowel ^reduction](https://en.wikipedia.org/wiki/Vowel_reduction) ^| [^Penult](https://en.wikipedia.org/wiki/Penult) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ckry05b) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ckry05b)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Is this one actually usable ? The previous sites with "programming challenges" in Haskell were using an old version of the compiler (not a deal-breaker) with either only Prelude or an old Haskell-platform. Without unordered-containers, most of the proposed tasks become really annoying.
I think the amout of contributions would increase significantly if GHC migrated to GitHub and started accepting pull requests.
Why not build GHC yourself?
I often literally say "Haskell, not to be confused with Pascal" and sometimes also add that they're both named after dead logicians.
yes
New Phabricator-based setup is really good, you should give it a try. Still not as easy as sending a pull request in Github, but easy enough and definitely a lot better than old trac approach.
You'll fry your laptop, most likely.
You could, it just takes a long time. Especially when your computer is somewhat underpowered, and the changes you're making are simple, it's much easier to let Harbormaster do the building and validating for you.
Once you're signed up, it's arguably easier. One command!
Thank you to all GHC maintainers. You're helping a lot of people.
I second that.
+1 for you, doctor.
Please no, Github is lacking in so many areas, it gets about 60% of the way there on everything then you just don't have the tools available to flesh out your process. There's a reason mailing lists are still king for large distributed development. The lack of structure lets you implement any process you need.
This has definitely been true for other projects we've migrated to Github (e.g. Cabal). GitHub's code reviews aren't quite as good as Phab* and the issue system is a bit weak. It's made up for by GitHub being the place where developers are. *An annoying thing with Phab is that it's very non-Git. For example, it insists on squashing all commits, thereby losing important history, and putting tons of gunk in every commit message.
It's about how to annotate Haskell code with source locations and then preserve these locations through all the optimization stages and code generation, such that when you profile code you can attribute costs back to the right Haskell source code.
Mailing lists aren't king for development (and I'm saying this as a frequent user of e.g. ghc-dev@). Programmers have moved on. Most of them are only on GitHub/BitBucket.
There is a reason I specified large distributed development. For something small githib and bitbucket probably provide enough tooling that they help more than get in the way. The problem is you're stuck within the github workflow and you can't implement your own processes on top due to the limitation of the tools. Compare that to a mailing list which is largely unstructured. Its a pain for a small project but once the project hits what would be a limitation in github/bitbucket you can just implement your own process.
I don't understand. How do I know I have fixed the issue if I am not to build GHC myself? Am I supposed to submit my changes without even knowing whether they compile? Doesn't that waste other people's time?
If you're not sure it will compile or validate you can make the arc subject something like "Unfinished patch: fix FIXME". Then you can do a few iterations with Harbormaster (you will get an email if it fails), and the GHC developers can look at your code once the signal is green and no time is wasted. The continuous integration system is explicitly set up to handle this workflow, if I understand correctly: Quoting /u/aseipp (ghc maintainer) on irc: "heh, i like that your test plan is 'harbormaster'. that means it's working :)"
&gt; I'm still a Haskell newbie Modesty is all nice and such, but since I consider writing a python interpreter a quite advanced thing, this makes me feel even more incompetent. ;-)
Fwiw, to workaround that I simply use one Phabricator code-revision per commit
But in order to use Phabricator, you first need to spend 1 hour on Google in order to find your post :-).
I will just say that a code base without lots of TODOs is simply an undocumented codebase. I'll take 1000 TODOs over 0 any day.
Just because something is more commonly used doesn't mean its a better tool. It's surprising to see arguments in favour of github in the haskell subreddit.
well, to me that means we need to improve https://ghc.haskell.org/trac/ghc/wiki/Phabricator somehow to make it easier to get started with our Phabricator setup... :-)
I love it. This actually seems like it would be a simple "get started hacking on GHC" project.
I still don't get it. Coding involves running/compiling often after small changes. It can't be more efficient to let it do a remote server for you on every line you change, and you can't code efficiently/correctly without compiling often. Building GHC might take long once, but changing some code and recompiling does not take that long, I hope.
I worked on an older c program with lots of TODOs. Unfortunately 90% just said "TODO FIX". Each time I'd spend ages working out what was *probably* wrong and fixing it but I was never sure if I'd fixed the thing that the TODO was about or just some random other bug so removing the TODO felt a bit dodgy. I'm glad I'm not doing that anymore. Edit: I also recall a void* variable that had the comment: //I don't know what this does but if I comment it out the program segfaults I tried commenting it out. The program segfaulted. I tried running it through valgrind and the program didn't segfault. I greped through the rest of the source and it didn't seem to be used anywhere else. I gave up and uncommented it again :(.
This is correct. I'm developing it on my ASUS UX32VD laptop, which is not very fast. First compilation takes really long time(I'd say some time between 30-60 minutes), but after that most of the time I can recompile within few minutes. (that depends on the changes of course, but in my case after the first compilation it's fast enough and I can recompile constantly just to check if my changes compile...) This "don't build GHC yourself" advice looks bad or at least pointless to me.
Right, but there are lots of options that you can use to make development a lot faster. We have options to compile only "stage 1"(so no compiling new version with new version compiled with old version), or compile "stage 2" too, but using `-O0` etc. I don't think this is an argument against "compiling it yourself".
Github also has a [CLI interface](https://hub.github.com/) that makes it extremely convenient to submit pull-requests and fork repos without leaving your comfy terminal ;) I agree, though, that Github's issue tracker sucks, but this isn't the point here.
Right, that's why you post your CTA to places like reddit where you can reach outside the mailing list.
Yes, ‚ÄúTODO FIX‚Äù is evil, especially when the version control system tells you that the idiot who put it here ... is yourself.
I apologize; I'm not doing a good job of conveying Haskell as a force multiplier. This project is beyond my abilities; it won't be within my abilities until I complete it. Haskell does a really good job of helping me train up to that point. The strictness of the language, combined with the pervasively functional/immutable ideology, is really pleasant to get to work in. My workflow on this project consists of dropping in and out of it every few days to work more on specific features. By the time I sit down, there's a 90% guarantee most of what I was thinking about last time has been paged out completely from my brain. Haskell helps a lot there! I think this set up works because I'm interested in interpreters and compilation right now. Previous attempts at learning Haskell/FP didn't stick because I didn't care for the problem domain enough.
&gt; I agree, though, that Github's issue tracker sucks, but this isn't the point here. Speaking of which... another technical issue would be that GitHub's PRs carry a `#&lt;num&gt;`-identifier, which you'd need to avoid ever showing up in any merged commits, as it would annoyingly collide with GHC's own issue database containing slightly under 10000 tickets which use the same syntax. If such a GitHUb `#&lt;num&gt;` ref would ever show up in the commit message, it would cause the commit to be associated with the respective Trac ticket, causing quite a bit of noise and manual intervention to fix it up again.
Where's that half-finished blog post?
Ok, I changed the "don't build GHC yourself" advice. It's just that I kept running into build and validate failures all the time when I started out, possibly hardware related, and it wasn't until I found out about Harbormaster that I actually got a patch in. People could help out with documentation or comment patches too, for which little building is necessary. Anything more than that, and yes, you do want incremental local builds. 
With Phabricator.
It's less that the emphasis is on the second syllable that helps distinguish, but that the "e" sound becomes very clear.
&gt; Unfortunately, the encoding doesn't scale if you don't have impredicativity. :( Scale in what sense? I've been playing around with Church-encoding in Haskell. While I've managed to get recursive types working (where the recursive arguments to the recursor take both a Church-style and a Scott-style recursive argument), I did encounter some oddities. Namely that if I try to simply alias a recursive type to its Church-encoding, e.g. type List a = forall r. r -&gt; (a -&gt; r -&gt; r) -&gt; r Haskell's type inference tends to implode. But it works fine when wrapped in a constructor.
Alright, I created the malling list [here](https://groups.google.com/forum/#!forum/haskell-morte/new), or you can mail the list at `haskell-morte@googlegroups.com`.
&gt; Maybe if you only read the lens docs, but almost everything else, certainly all of the standard stuff is very accessible. Quite often there's a complete lack of documentation and unless you are extremely clever and good at making types line up, or you have a few days to try and fit stuff together, it's really difficult. &gt;Some of the more type-theoretic literature gets pretty crazy I'll admit, but most of it isn't really relevant to day-to-day Haskell programming. The problem is I've seen some of this stuff linked when people have asked for a simple explanation. The other problem I see is the fact that a lot of the commonly used stuff has quite good documentation and there are good beginner guides but if you want to take the next step there's a dearth of easy to read information.
It is just one more hurdle to overcome. Time to learn about ghc, nope actually I have to learn about phabricator first, which I've never even heard of until this thread. Edit: After skimming that wiki yeah sorry I'm not interested in contributing anymore.
Sure it makes it easier to get the right packages, but it still doesn't prevent beginners from misusing the Prelude and incidentally writing extremely slow code because the slow data data structures are more readily accessible. It's just my opinion but, I think the current Prelude still has it's foot in the space of being of a teaching language for FP fundamentals and a lot of times that's at odds with it's growing industrial usage.
where? I just see it in the coming soon section.
I've replied to this topic about 47 times already in at least 32 different places, and GitHub just isn't suitable for GHC: - We still need our own buildbot integrations. Travis-CI just won't work. We could at least mitigate this with Jenkins at least, I guess. - GitHub has horrible notification features compared to Phabricator, from emailing to notifying you about PRs. This is *seriously* important in big projects. I also work on [nixpkgs](http://github.com/nixos/nixpkgs), a large repository with many active contributors, and the lack of granularity in these features mean **GitHub notifications are totally useless.** - Phabricator has much better code reviewing tools in general, from auditing commits that already went by, to just the UI, to allowing complex rule-based behavior to notify you about things. Until recently, GitHub didn't even have side-by-side diffs, which is bonkers. And it's going to be years before they catch up on the rest. - We can't integrate Trac into GitHub into any meaningful sense AFAICS. This is a requirement. - ... and no, we can't move off Trac, because it would just be a stupidly gigantic amount of work to move our DB (one of the largest Tracs around) off Trac in a way that would be palatable to most people. - No first-class meme support (this is probably only important to me and Simon Marlow. :) (**Obvious edit**: Also, fundamentally, there *is* the issue that we can always improve Phab, but we can't ever fix GitHub!) GitHub is great for most projects. But I'm more and more convinced that GHC simply is not one of them, and I personally don't subscribe to the faith that "going where the users are" unquestionably leads to a significantly better overall turnout, at the expense of many other good things. Note the "unquestionably" part: I buy it could be better overall. I just won't buy that without a bit of evidence, or at least a few of my points being addressed. :)
There's no _need_ to learn phabricator, it's just suggested as it will save you work. The built in git collaboration facilities are still there. From the wiki: &gt;Alternatively you can send a patch to the mailing list using git. Please write your patch and then rebase to the latest version of GHC HEAD before sending to us. You can use the following command to send patches via email: &gt; git send-email --to=ghc-devs@haskell.org &lt;hash-id&gt; -1 &gt; where &lt;hash-id&gt; is the hash of the commit to send. If you'd prefer to create patch files and send them via email another way (or attach them to trac tickets) then you can use this command: &gt; git format-patch [-o &lt;outputdir&gt;] &lt;revision range&gt; &gt; Where &lt;revision range&gt; specifies the commit that git should stop at when going from HEAD backwards, creating a patch for each commit in the range &lt;revision range&gt;..HEAD. 
Once you're signed in you can find them e.g. [here](http://www.codewars.com/kata/latest/haskell).
I just don't understand how git send-email --to=ghc-devs@haskell.org &lt;hash-id&gt; -1 is more effort than faffing around with github's ui.
&gt; Quite often there's a complete lack of documentation But that's a different issue to the one being discussed, which is the documentation having too high a barrier of entry. Sure, they're both issues, but they have a totally different set of causes and solutions.
I don't think this is wrong, it's just "Not GitHub"[1], and it has a different kind of feedback loop. And that's not necessarily a bad thing, it's just different. Second, having spent a significant amount of time with GitHub *and* Phab at this point, I just don't think this whole problem is as much of a death-blow as you make it sound to be. For one, we can turn squash behavior off. :) But it's not clear there's a consensus on that. Second, I think actually dealing with *git* vs *the actual code review* is like a 5% vs 95% work effort thing. Here's something I still don't have an adequate answer to: If you don't *want* something squashed into your commit, I have to ask: **why is it not a separate code review**? Clearly it's important enough to merit its own commit, so why not its own review? If the two are so closely interrelated, why aren't they one? You can say "it's important enough to be separate, but not read separately", but I don't really think that's the case. Why would it be so important I couldn't review *the first thing* without it, and why *must it be* separated from the second? If that's the case - **it needs to be one commit, or there needs to be two code reviews**. I think people using GitHub have a mindset that "one branch == one pull request/code review == one unit of work, composed of small pieces". That's just not true with Phabricator, but I don't think that makes it inferior. Instead, the notion of 'branch' does not matter (it's abstracted away) - only *code reviews* are the single units of work, and they too may be composed of commits. And a code review may possibly result in several changes becoming one, if you want - but that's the point, *code reviews are the units of work, not branches, not commits!* You must stop thinking about what should go in your *branch*, which is a hazy notion in Phab. Instead, think about *what would be in a singular piece of code you wanted to read*. That is worthy of a code review. And finally, in general, I think something like this is possibly necessary. I am strongly supportive of the rigor and very strong emphasis on small, piecewise changes I believe Phab attempts to emphasize, which IMO not only scales better, but **is necessary for GHC to remain hackable**. We have (IMO) been too sloppy in the past for something as mission critical as a compiler in both things like our code review processes and development processes. Way too often, we smuggle off massive changes that refactor huge parts of the compiler, and when we come back we have to deal with them. Phab seems to hurt this workflow the most - but maybe that means it's a bad workflow, and we should *not* defend it (and really all review workflows fail at mega-scale.) I think people need to get used to 1) writing down your design/idea, and 2) sending work incrementally and being prepared for it to be reworked or challenged, instead of "do all the work, then do a whole lot more later when it all needs changing from the reviews". Balancing the desire for new contributors, good standards, better infrastructure, and pleasing existing ones is difficult to handle. But I think it is a necessary evil in the world of large projects. One might say it's crazy we're only having this discussion now for such a big project... [1] I had a discussion about the Phab developers with this. In fact, they plan on adopting many more GitHub-esque features down the line, including branches turning into reviews, etc etc. But this isn't a high priority for them. Why? Because, in their experience, about 99% of all the people who have ever given them feedback have said "Yeah, it was a weird change at first, but basically everyone is over it". They have no reason to emphasize this work because empirically, it seems, the "Not GitHub"-shock is only temporary. There may be a bias (2x more people may turn Phab down *because* it lacks this), but for any given *success* case, it doesn't seem to really even be a consideration at all.
ah, that's great.
I hope to finish it this weekend. 
In practice if you send patches by email to ghc-devs they are likely to be lost. Attaching the patch to a Trac ticket with status "patch" or putting it on Phab ensures that we can't lose track of it.
It was just an extra point on docs. It's not that off topic 
Yeh that's what we do at work, we have a plugin that lets you email directly to trac and if the email has a patch attached it updates the bug to the code review status.
In principle we could automatically turn GitHub pull requests into Phab differentials. Could be useful for really simple changes (just need a web browser!) and might be worth doing at some point.
Sorry, I didn't mean to come off as dismissive! I didn't mean to shut down the conversation. I was just point out that, while it's also a case of bad docs, it's a different issue with a different solution set. Also, do you have some specific examples of a lack of documentation for reasonably wide-spread packages? I've never personally run into issues with this.
I'm looking forward to improving my understanding of Haskell and functional programming in general. 
You're right, you can do very fast incremental rebuilds. But doing the actual process of testing it all is what takes significantly longer and is very annoying for some people. Soon, Phab do a 'slow validate' on every contributor patch and commit, which means it runs every of our ~4000 tests in several ways (about 6-8 ways per test). That makes it **significantly** slower to the point it's really burdensome to test on people with under powered machines - it will add hours on to your build just to run the tests once, significantly hurting turnaround. That's where Harbormaster steps in. In this case it's perfect, since it does "the long haul" for you by testing your patches in a good environment, rigorously, on powerful dedicated hardware. Also, soon, we'll be allocating our GHC build machines In The Cloud‚Ñ¢ and on-demand, so one build won't slow down the others. Also, sometimes I'm just absurdly lazy, I don't want to build a change myself, so I just submit it to Phabricator and let it test it, and I do something else in the mean time. Yes, I'm being 100% serious. And yes, I encourage others to try doing the same (within reason, of course). The machines are there - I'm going to make them do the work. :)
oh come on. it took me literally 10 minutes to learn the basics of phabricator, install arc, and put up my first diff. If that's inhibiting, then how can they expect to hack on GHC?
A unique application, seems to be very useful for any visual search task, especially when words are not so useful
That's right. I'm more than 'guilty' enough of just submitting patches without building them, and letting Phabricator do the work and emailing me later. (Sometimes they *blatantly* don't compile at first, even. :) I use quotes because I don't think this is wrong, and I'd encourage others to do the same within reason. Seriously - why would I want to waste like 30 minutes of my time testing a patch for the users guide (which, BTW, does need to be tested to build correctly), or fixing a slightly busted thing, when I can just use Phabricator? It will always do the absolutely-required standard testing of all patches, which takes some time. In the mean time I can actually go work on something that require more of my brainpower, and more of my CPU cores. Seems obvious to me. :) Do note however, Phab *is* currently limited. It doesn't build on Windows yet - but stay tuned, that may change real soon...
As much as I understand the need for something more than github for heavy contributors and maintainers, I think it's a barrier for really casual and small contributions to GHC. I think automatically, just having to learn and use a set of extra random tools for contributing somewhere give me a mental hurdle.
That would probably solve most of the issue I see with the current setup; Provided it's possible, then it would be great IMHO.
Actually, there are eventually plans for Phabricator to do this natively, I think - it will allow branches to become differentials, and it will allow this for external mirrors as well. As well as some other GitHub-esque features, based on what Evan told me. It's just not high on their priority list, since in their experience most people get over the "Not GitHub"-shock of Phab pretty quickly and move on.
Could you list a few of your top reasons it would be worse?
Sorry, I read it a bit more harshly than it was probably meant
Don't underestimate the value of lessening the barrier to first contribution.
Think about how intimidating this sounds to a newbie just wanting to fix spelling errors.
Another real problem I just thought of (obviously) is we can't *fix* any of those issues with GitHub, but we could fix many issues with Phab. I've already submitted a few upstream patches - it can be done and we can fix real problems we may encounter. I do believe we can make our Phabricator extremely easy to use, but if GitHub falls short for GHC, we really have no way to change that, ever. And then switching off would be a huge pain. And I think it does fall short for GHC, unfortunately. Also, as I said elsewhere in the thread, there is a necessary balance between attracting new developers, pleasing existing ones, and providing the necessary support/infrastructure for good standards and development. I've spent a lot of time with all the tools, and IMO I do think Phab actually balances most of these concerns pretty nicely. I hope the need for a few new things won't deter you too much; I did write [a lot about using Phabricator for GHC people](https://ghc.haskell.org/trac/ghc/wiki/Phabricator) to help (please don't let the page size deter you, there's just a lot of images!)
&gt; It's just not high on their priority list, since in their experience most people get over the "Not GitHub"-shock of Phab pretty quickly and move on. Well, and also companies like Facebook are customers of Phabricator right? so it makes sense that their priorities would be catering to situations where there is already a substantial setup cost per contributor (employee), not to accepting drive-by patches with minimum overhead. For example, the process for contributing documentation fixes could be a lot more streamlined than it is today.
Actually HP makes it at all possible to choose the right packages. You may expect platform packages to be available in code competitions etc but no other packages. But yes, Prelude may not provide the most efficient stuff but that is a comparatively smaller problem than not being able to use them at all. 
I appreciate you acknowledging the bias in the anecdotal user satisfaction estimate, but I suspect you underestimate it. Many inventions that fail to gain wide adoption have a small core of enthusiastic users. We know this all too well: everyone who uses darcs seems to love it! So clearly asking people who choose to swim against the current if they think swimming against the current is a good idea... is a bad idea. Your point about code reviews being the unit of progress is great, but I don't really see why it must be in conflict with having multiple commits. The commits are sub-headings in a description of the change. That seems fine to me, and can totally fit in with the review-as-unit-of-progress approach.
&gt; If you don't want something squashed into your commit, I have to ask: why is it not a separate code review? Commits should be as small as possible, and no smaller. (I.e. commits should be just enough changes to make forward progress while preserving automat(ed/able) test status.) This improves the behavior of things like bisect, rebase, and (sometimes) even merge. Ideally, code reviews should be a more holistic look at the branch as a whole. Is this the "right" place to make the change? Is this the "right" change to make? Is this orthogonal to the "right" things and integrated with the "right" other things? This "part" of the code review isn't concerned with how many commits there are. If you are using public branches, this part can also be somewhat delayed to right before the merge into master, but it is useful to do some of it as individual commits come in, so that re-work is minimized. Unfortunately, code reviews also serve as a human gatekeeper for things that we can automatically test -- style, no new ghc warnings, no new hlint warnings, etc., etc. (These vary from team to team, and often start out non-automated.) Those "parts" of the code review should be applied to each commit object, and they should be redone any time the tree (at least) changes, even for things is a "trivial" rebase, merge, or squash. Sounds like Phab focuses too much on the second type ("reductionist")and expects all code reviews to be done per-commit. Code review that require a human sign-off should be more focuses on the first type ("holistic"), IMO. However, I've also seen this bias in my own code reviews; if an automatable test isn't yet automated it serves as sort of a "low bar" I can review to that still weeds out bad commits and minimizes the work I have to do. (I'm the strong, *lazy* type. :P) Still, from what I've seen Phab is actually an improvement over GitHub in this arena even if it has a very different "flavor". I'm sure if GHC developers feedback into Phab development it will only get better.
You can normally cut rebuilds down to &lt; 20 seconds with a few small changes to your `make` commands. With some further tricks you can get a rebuild in just a few seconds on average. Our build system has a lot of tweakable options like this to accommodate tradeoffs between rebuild speed and other needs.
TODO: Make this work correctly
As a person who is *very* interested in contributing to GHC, I really appreciate all your answers in this thread. As long as the GHC dev's are happy I'm happy.
&gt; We know this all too well: everyone who uses darcs seems to love it! So clearly asking people who choose to swim against the current if they think swimming against the current is a good idea... is a bad idea. Very fair, although like darcs users, I do think Phab has some real technical advantages over GitHub, of course. :) You can find a lot of my reasoning elsewhere. (I also might argue Phab has more active commitment by paid developers and bigger full-scale users to back it relative to `darcs`, but that sounds hard to realistically quantify as a plus.) &gt; Your point about code reviews being the unit of progress is great, but I don't really see why it must be in conflict with having multiple commits. The commits are sub-headings in a description of the change. That seems fine to me, and can totally fit in with the review-as-unit-of-progress approach. I suppose it's a mis-communication: in my mind, Phabricator has no problem with multiple commits. The salient bit is *how the changes come out at the end*: all as one, or as a copy of the original commits of the author? And how they come out has impact on how you might organize them in the first place, hence how you organize units of work. This is really a discussion that needs to happen amongst the developers, but hasn't yet. I'm open to changing it, of course, since it is totally configurable (obviously, I don't really think there's much to gain by using `immutable`, but others may disagree.)
I know right!? Haskell is *crazy*. And it just gets better from here man.
I thought they only put the first 150 in Phab OTTOMH, which is why I wrote that. But you're right: I reread it and they put in all [151](https://secure.phabricator.com/T1315#16876) originally.
Of which 150 were the first ones, and one is the next one. ;)
Is `lens` not considered "standard stuff?" What about `arrow`? I mean yeah, syntax tutorials are plentiful and easy; but syntax isn't what makes a language useful.
Well that's nice; but to make an account I apparently have to "prove my skills" in Javascript, Coffeescript, Python or Ruby. What if I'd rather not? ;)
I had a TODO REMOVE THIS comment before a line that was commented out. Was I supposed to uncomment the line or delete it entirely!?
&gt; Commits should be as small as possible, and no smaller. (I.e. commits should be just enough changes to make forward progress while preserving automat(ed/able) test status.) This improves the behavior of things like bisect, rebase, and (sometimes) even merge. But my question was *why wouldn't you just make two reviews if that was the case?* We want bisect to work. It will land as two commits if you do this. And even if it lands as one, in practice, I have found this rarely breaks bisect/rebase (I say this from experience merging many, many patches.) &gt; Ideally, code reviews should be a more holistic look at the branch as a whole. Is this the "right" place to make the change? Is this the "right" change to make? Is this orthogonal to the "right" things and integrated with the "right" other things? Indeed, I see what you mean now. I think GHC is different in that we don't often try to hash these things out in a review. We try to hash them out as early as possible in the beginning, and encourage people to make progress from there, and write up a design. Then it becomes much more clear from the beginning what a change needs to do. GHC is far too complicated for anyone to consistently remember such things, so having written evidence (or a smoking gun, perhaps) is useful. I'd say pretty much anything larger or more invasive than "typical type-driven refactoring" or whatnot probably needs a design laid out and put somewhere permanent, so we can review it first and let the coding happen later. Anyway, yes, the flavor is different than GitHub, but honestly I find it makes you much more thoughtful about how you want to think about your changes and get them prepared for review, and it has many technical benefits for all the current contributors, beyond just that.
They all have unit tests. A kata can't be published without them. They may not have "example" unit tests that you can see. Kata authors sometimes hide the unit tests so that users can't cheat, so you are forced to either just submit and see what happens or write your own. 
Best to do both to be on the safe side.
What do you think GHCJS is for?
Ha! But sadly, the proving method is "correct these broken snippets" and not "pass the unit test." I just rolled the dice and ran through Python. Trivial stuff really, ultimately amounts to "can you use the docs." Downside: now I can't seem to remove Python katas from my list. :(
Did you try hitting submit or "run tests"? "Run tests" only works if you provide your own test cases. 
I'm having trouble getting the big picture here. The original advice was to not build GHC because it's too hard, but people don't like the sound of that. Then the advice was to have a 40 minute turnaround with Harbormaster, so at least you're not tying up your own machine with rebuilds. Now it's less than 20s for a rebuild if you know how to do it. My concern is that all of these are true. I think potential contributors are suffocating under an avalanche of options.
&gt; in practice, I have found this rarely breaks bisect/rebase I wouldn't say it *breaks* those tools, it's an incremental reduction in functionality. The size of the commit that blame lands on is inversely proportional to the accuracy / bits of information that gives you; you have more changes to filter through. For rebase/merge it's usually even less severe, especially if you have a good mergetool. &gt; Indeed, I see what you mean now. I think GHC is different in that we don't often try to hash these things out in a review. It's not always about hashing them out. In fact, if the reviewer doesn't like the commit for a holistic reason; they should be scheduling a discussion at the same time they reject the commit. It's about verifying that the branch does follow the "right"/agreed-upon design -- something that's difficult-to-impossible to do in an automated way and might only be properly judged when seeing the branch as a whole. Or at least, they only need to be checked once across a whole branch; they don't need to be verified with each commit. If all your human reviews are of the "holistic" style, you should be reviewing branches not commits. "Reductionist" reviews should be automated and not require a human to sign off on each commit. So, multiple commits doesn't always need multiple human reviews, IMO, and it would be nice for our code-review tools (Phab) to reflect that. &gt; I find it makes you much more thoughtful about how you want to think about your changes and get them prepared for review, and it has many technical benefits for all the current contributors, beyond just that. That may be quite true. I'm too inexperienced with GitHub pull requests, Phab, and GHC development to say one way or the other. I'm certainly not advocating for GitHub, just trying to explain why I think one commit = one human review isn't the optimal way for something like Phab / Gerrit to work. (IIRC, GitHub doesn't make it easy for me to pass all of the commits in the pull request through a set of scripts for the "reductionist" reviews, so I'd say that's clearly not optimal either.)
&gt; If you need more than 3 levels of indentation, you're screwed anyway, and should fix your program. -- Linus Torvalds I think 5 is more reasonable, but I've been writing a lot of Java recently and our coding standard "costs" me at least one indentation level. (Imagine if you had to indent every line under the top-level module/where.)
It seems obvious that none of these would prevent an alternative *"official"* github repo for GHC. You can personally take on the onus of rolling those changes into your own private convoluted build system with first class support for memes. For the rest of us, it's easier and those TODOs go away at a faster rate. You raise a lot of points in your response but none of them address the fundamental reality of open source today: everyone knows how to use GitHub and everything you've introduced steepens the learning curve for contributions. The GHC team may find that acceptable, but it's not going to help with the problem of clearing up those 1,088 TODOs.
&gt; You're right, you can do very fast incremental rebuilds. But doing the actual process of testing it all is what takes significantly longer and is very annoying for some people. Fair enough, but I imagine that running the tests is the last step after coding and being fairly confident that the code works, and especially, being certain that the code compiles. So running the tests on the remote machine sounds reasonable to me, but not building GHC locally at all does not.
hm, think the message could be even more explicit, and the old message was doing some things better. In the first case, it says there's some string and there's some Bool, and gives some bible citations on where these came from. And only highlights point to what comes from where. Maybe I don't see colors. I certainly can't see from the slide as its captured what visual prompt should lead me to think red is Bool, blue is string. Don't think main info should come from mere highlights, that should only be an additional aid. Used to say "in the expression" and the expression that give rise to these. How about saying there's some True :: Bool and some "a" :: String in some [True, "a"] :: [a] as well as the citations, or some clearer paraphrasing offering all that data?
I think there is a clear upside to not using the character already in use as a module seperator for your most common operator. Providing backwards compatibility would be a matter of offering an alternative Prelude including "." Personally, I would be happy to see . replaced with ¬∑ (plus the &lt;&lt; operator as in Elm), but I recognise that this is probably not a popular idea.
The color was added for presentation only. The interesting bit is the source spans for where the two types came from. You favorite editor should extract those from the error message and highlight them. Personally, I don't find citing source code in the error message to be that helpful.
How much space does it have, and how much does it need? You can probably get the community to help out!
&gt; Sometimes they blatantly don't compile at first, even. :) Maybe a linter will help? And I'm a noob at this, but can't you use ghci with -fdefer-type-errors to test small things?
No worries :) I did word it a bit harshly at first, I apologize.
About 100GB currently for all included state. Last year at ICFP when we set all this up we assumed it would be OK, but Hackage sees a lot more use now, and we slacked on upgrading the block device volume. The new server has a shiny RAID1 set of 500GB block devices.
will this kick off the rackspace migration finally? :) (ie will docbuilders come back to life?)
Oh yeah! I saw that movie. 
Yes, i also stumbled on a few people who confused haskell with pascal. My other favorite misunderstanding is when i ask about functional programming. One guy told me that of course he is a big proponent of functional programming. All his programs have a lot of functionality :)) 
I've generally used HStringTemplate for non-html. I'm not sure how to apply all of what Heist gives you outside of the context of a dom tree, and regardless, when dealing with plain text, usually all I wanted was some substitution (which HStringTemplate is perfectly capable of).
http://www.reddit.com/r/haskell/comments/2hh96i/note_hackage_is_down_disk_space_needs/
Adding instructions on using an unofficial mirror: * Add the following line in`~/.cabal/config` * remote-repo: hdiff.luite.com:http://hdiff.luite.com/packages/archive
looking at the most recent stuff at http://hdiff.luite.com/ the most recent time stamp is 2014-09-25 20:16:00 (GMT)
Sorry, I was on the train, missed this.
I keep getting "Submission timed out. Please try again." and I can't tell if it's my code at fault or not.
While it sucks that hackage keeps going down, at least this time it's something to celebrate. We ran out of disk space because we didn't expect it to be used so much, which means more people are uploading new packages and improvements. 
usually helps to comment out the other default remote repo too afaik :) 
**Old server**: - **8GB RAM**, probably 60%+ of all RAM taken by disk cache. - Combined with the `hackage-builder` process. - 1 core. - Shared ethernet link amongst multiple VMs (no dedicated QOS per VM, AFAIK). No IPv6. - 1x100GB virtual KVM block device backed by RAID1 2x2TB SATA setup on the host. **New server**: - 4x cores. - 4GB RAM, as this should fit comfortably with nginx as a forward proxy. - Hackage builder has its own server (removing much of the RAM needs). - Dedicated 800Mb/s uplink, IPv6 enabled. - Dedicated dual 500GB block devices (backed by dedicated RAID10 shared storage) in RAID1 configuration.
We can't preserve account or permission information this way in this timeframe. Perhaps it's best to let it 'replay' naturally as opposed to going to an inconsistent state if needed.
Newcomers page on the wiki should state a "run `./sync-all get`" before perl boot, since without it the dependencies aren't there. Cross that off the list on impediments for new developers :P
Gave it a shot, still very buggy. A lot of tests fail when they shouldn't. For example I can run code in ghci and it gives the right answer yet in the their environment it returns 0. 
Nevermind, early morning here. I actually copy pasted your git clone command, and followed the rest of the steps from the wiki page.
AÃ∂hÃ∂,Ã∂ Ã∂gÃ∂oÃ∂oÃ∂dÃ∂ Ã∂pÃ∂oÃ∂iÃ∂nÃ∂tÃ∂.Ã∂ Ã∂IÃ∂ Ã∂uÃ∂pÃ∂dÃ∂aÃ∂tÃ∂eÃ∂dÃ∂ Ã∂tÃ∂hÃ∂eÃ∂ Ã∂cÃ∂oÃ∂mÃ∂mÃ∂aÃ∂nÃ∂dÃ∂ Ã∂aÃ∂bÃ∂oÃ∂vÃ∂eÃ∂.Ã∂ Ã∂AÃ∂lÃ∂tÃ∂hÃ∂oÃ∂uÃ∂gÃ∂hÃ∂ Ã∂tÃ∂hÃ∂eÃ∂ Ã∂cÃ∂lÃ∂oÃ∂nÃ∂iÃ∂nÃ∂gÃ∂ Ã∂wÃ∂iÃ∂lÃ∂lÃ∂ Ã∂nÃ∂oÃ∂wÃ∂ Ã∂tÃ∂aÃ∂kÃ∂eÃ∂ Ã∂lÃ∂oÃ∂nÃ∂gÃ∂eÃ∂rÃ∂.Ã∂ Not sure if wishful thinking, but the GHC servers seem slow (reddit effect?). Please use the Github mirror for the time being.
That poem is brilliant.
&gt; Start with those videos. They're the best. Pfenning then harper. I have now watched lecture 1 from both. You did well to insist, they were very clear and they did answer most of my questions. I've also been [reading up on L√∂b's theorem](http://lesswrong.com/lw/t6/the_cartoon_guide_to_lobs_theorem/), and I think there is something which really needs to be cleared up about the "A is true = I have a proof of A" philosophy which made you claim that a constructive version of L√∂b's theorem would be vacuous or false. Apparently, "L√∂b's Theorem shows that a mathematical system cannot assert its own soundness without becoming inconsistent". Presumably, the proof is correct, or mathematicians would have long found the flaw. Also, his proof seems to apply for *any* strong enough (i.e., strong enough to prove basic statements about natural numbers) mathematical system, whether classical or constructive. Assuming, therefore, that the statement "a mathematical system cannot assert its own soundness without becoming inconsistent" is true, the argument shouldn't be that "truth and provability are the same in constructive logic, so the implication ‚ñ°A ‚Üí A is trivially true, so L√∂b's theorem is vacuous". If anything, it should be that "truth and provability are the same in constructive logic, so the implication ‚ñ°A ‚Üí A is trivially true, so by L√∂b's theorem, constructive logic is inconsistent". Of course, I don't think constructive logic is inconsistent. But I think the above argument means we need to think harder about whether the philosophy "A is true = I have a proof of A" really does imply ‚ñ°A ‚Üí A, and if it does, whether that means that constructive logic is asserting its own soundness. &gt; Tho, eta will force it to be true that if you put an A into an intro to get ‚ñ°A, then somehow you need to be able to get out an A somewhere (perhaps only as a var in a case branch). Maybe something like this? introduction rule for ‚ñ°: ‚ä¢ M : A ---------------- ‚ä¢ static M : ‚ñ° A elimination rule for ‚ñ°: ‚ä¢ M : ‚ñ° A x : A ‚ä¢ N : B ---------------------------- ‚ä¢ case M of static x ‚Üí N : B local soundness: ‚ä¢ M : A ---------------- ‚ä¢ static M : ‚ñ° A x : A ‚ä¢ N : B --------------------------------- =&gt; ------------- ‚ä¢ case M of static x ‚Üí N : B ‚ä¢ N [M/x] : B local completeness: ‚ä¢ M : ‚ñ° A x : A ‚ä¢ x : A ---------------------------- ‚ä¢ case M of static x ‚Üí x : A ------------------------------------- ‚ä¢ static case M of static x ‚Üí x : ‚ñ° A I'm not sure what to do about the other two rules, "‚ñ° a ‚Üí ‚ñ° (‚ñ° a)" and "‚ñ° (a ‚Üí b) ‚Üí ‚ñ° a ‚Üí ‚ñ° b", because unlike the examples given in Pfenning's lecture, there is an interaction between two connectives, "‚ñ°" and "‚Üí".
My "next challenge" is also still in JS, and there are other places where Haskell isn't listed along side the original 4 languages. Also, I found it annoying that I had to answer JS questions before I could sign up for Haskell.
They had the podium mic on until 5 minutes in, when they switched to the body mic.
I actually added support for [HLint into upstream Phabricator](https://github.com/phacility/arcanist/commit/4c0edd296e3301fffdda33c447f6fcafe7d1de01), so in theory we could HLint any potential changes (linting automatically happens upon submitting a review), but we haven't done that yet. I've been thinking about it. The more important point is you really need to *build* your changes though - things like `#ifdef` mean a linter won't catch e.g. an import failure on a platform you didn't develop on.
Well, I didn't write the original advice. :) But, to address points: 1. I don't think building GHC is too hard. I've heard the complaint a lot though, so I could buy it. But we should make it easier, then. 2. Harbormaster *can* alleviate your workload for a lot of stuff. But it's a CI system. Yes, it's convenient to just post diffs and have them tested. Yes, I do encourage people to do this, where it makes sense. Things like minor patches, typos, doc fixes, etc can automatically be checked this way (and merged later) with little overhead or time for the end user or anyone else. It is up to the user's discretion to do this. 3. Yes, it's only a few seconds to do a rebuild if you're doing active, incremental development. But this is much different than needing CI. Where you draw the line is of course up to you, but I think you can use common sense to draw some: - If you're just submitting small or simple things, feel free to let buildbots take over. They will test your changes thoroughly, at least, and can leave you time to do other things. For typos or documentation fixes, I think this is totally reasonable. Even for bigger changes, it's reasonable: just use common sense. - If you're developing GHC actively, you'll need to learn some of the workflows to make rebuilding fast anyway. This happens at a different stage in development (when you may just be testing things.) But you don't need to know this immediately, necessarily. - GHC should be buildable on all major platforms, and I don't think it's hard, but if you have problems with [the docs](https://ghc.haskell.org/trac/ghc/wiki/Building), please complain so we can fix it.
But it also lets you do things like test changes on other platforms, when you may not have access to a machine to do so. Ultimately, it is just a CI system. It has all the benefits of a regular CI system. But it's very tightly integrated with the repository and code review, so it's useful for other kinds of workflows, too. That includes "fire and forget" ones where sometimes you're just lazy and want to do something else while a machine double-checks your work. And yes, you *should* run `./validate` yourself if you want to double check everything. But we also need bots to test things, and we need that to be easy, and we should encourage people to utilize them - we know this from empirical evidence where we let things break.
&gt; If anything, it should be that "truth and provability are the same in constructive logic, so the implication ‚ñ°A ‚Üí A is trivially true, so by L√∂b's theorem, constructive logic is inconsistent". This isn't quite true. L√∂b's theorem itself has to be proven *in some logic*, and if we reject that logic, it's not longer a theorem. But that said, even if we accept L√∂b's theorem, all it implies is that constructive logic cannot assert its own soundness. Ok, that's fine. That's the approach most type theorists take -- type theory cannot encode itself without being inconsistent. *Edit* I should add that a lot of the intuitionist philosophy is a reaction to this idea that there are these great cosmic Truths that might possibly be unknowable or unprovable. In some sense, the intuitionist response is to say, the idea that something can be true, but cannot in any way be shown and known, is utter nonsense, so any logic which allows things to be true but unprovable is fundamentally missing the point of logic. ---------- You can loosen your rules a little bit, to get to the standard box modality for type theory, by having introduction rule for ‚ñ°: ‚ä¢ M : A ------------------ Œì ‚ä¢ static M : ‚ñ° A elimination rule for ‚ñ°: Œì ‚ä¢ M : ‚ñ° A Œì, x : A ‚ä¢ N : B -------------------------------- Œì ‚ä¢ case M of static x ‚Üí N : B The interactions with ‚Üí are really the tricky part tho. Those might be addable as oracles. I don't know what the general principles are for oracular additions, tho.
That happens sometimes when there's a major syntax error, weirdly. For instance, one of the problems has a lowercase module name and it causes odd bugs.
If you don't feel like working problems, write some! Let's get some more learning material out there.
After looking through the hackage code it seems that there are many states being used. If we distributed these states on different machines and communicated to them via RPC using the Data.Acid.Remote module we might decrease our I/O bottleneck. Has this ever been considered? 
Yup, I think looking at the logs for hackagebot output is a solid approach: http://ircbrowse.net/browse/haskell?q=hackagebot
With this types of things it rarely about the direct cost even when it is less than 10 minutes. It is the [first step effect](https://blog.kissmetrics.com/first-step-of-checkout/) there are many blogs that document how much sales can be increased by just making it easier to log in or by not requiring one. It is about the common human basis to put that hard thing off and to the tried and true. Mitigating that human basis by making things easier can have an amazing impact when the customer/user/contributor pool is large. At some size though percentage difference is not large enough to justify the loss in functionality or cost in change. GHC contributors are small enough that the above effect is probably not large, but that concept/idea is percolating in everyones head because it is important concept that has seen a lot of light since the internet. While the effect might not be large due to the contributor pool size most people are probably concerned with the compounding effect that small extra percent can make over time. Not obvious to me which is the better choice. I can not advocate for one or other, but small costs often have large impacts it makes sense people are paying attention to them.
My guess is that if anyone in the community has a tool to analyze and edit damaged acid-state logs that would help out.
How much of the disk space is used for the tarballs vs the package metadata stored in acid-state? What's the blow-up factor for the metadata (i.e. for every 1 byte of .cabal file, approximately how much acid-space storage is needed)?
&gt; remote-repo: hdiff.luite.com:http://hdiff.luite.com/packages/archive That seems to work - thanks 
As far as I know we don't have any I/O bottleneck.
The metadata takes about than 800M (of which packages is about 600M), while the other stuff takes ~60G. We've also got 50G of backups.
Fear not, we have nightly backups.
I was about to upload my new major version. I guess it‚Äôll wait :D
* In a logic programming language, your program is a proposition, and program evaluation is search for a proof. * In a functional language, your program is a proof, and program evaluation is proof normalization. A dependently-typed logic programming language is a logic programming language where the language of programs are dependent types. The most prominent example of such is [Twelf](http://www.twelf.org). 
It's up again (/u/dcoutts fixed it)
Ok, it's back up. We didn't have to restore from backup. The only thing we think was lost was yesterday's counts of the number of package downloads (ie nothing of value). We hit a nasty case where we ran out of disk space while writing a transaction log. Fortunately the most active table is the download counts, and so that one hit it. I just deleted (moved) the daily download counts (it's a temp store just for the daily counts, the long term counts is unafected) and started it up. So it's still running on the same host machine. However as aseipp has said repeatedly, the plan is still to move the server to a new host (with more disk space and other resources).
&gt; why is it not a separate code review? Clearly it's important enough to merit its own commit, so why not its own review? If the two are so closely interrelated, why aren't they one? You can say "it's important enough to be separate, but not read separately", but I don't really think that's the case. Why would it be so important I couldn't review the first thing without it, and why must it be separated from the second? If that's the case - it needs to be one commit, or there needs to be two code reviews. It needs to be separate *for source history purposes*. Perhaps I fixed some whitespace or moved some thing around (or did some other simple refactoring). Having those plus the main change in one "did stuff" commit is bad. Note that on GitHub you can review each patch in one pull request separately (just click the commit). So the review can choose how they want to review the changes. Forcing small commits to be separate makes the code review process much more expensive. &gt; I think people using GitHub have a mindset that "one branch == one pull request/code review == one unit of work, composed of small pieces". This is the Git way, not the GitHub way. Each feature is developed on a separate branch and that branch is the unit of work that gets merged. &gt; And finally, in general, I think something like this is possibly necessary. I am strongly supportive of the rigor and very strong emphasis on small, piecewise changes I believe Phab attempts to emphasize, which IMO not only scales better, but is necessary for GHC to remain hackable. No, it encourages the opposite. For example, I will never send small refactoring commits to Phab, as it's too expensive work wise. Instead you get huge "here's my whole feature, commits".
Isn't acid-state ACID? How can the transaction log writing fail and the server still go through with the write (i.e. how can we get to a state where we lose more than the last write)?
Of course, I should have mentioned that the TODO has to actually document the problem. I have a habit of adding TODOs absolutely everywhere in my code to document things that are not 100% "perfect". For example if an API can be improved, I will add an TODO. If a function is a bit large, I will add a TODO. Whenever I considered an alternative approach that might take a bit longer to implement, I'll add a // TODO explore X When I later look at the code, it is super-easy to see what my thought process was like when I wrote it. What I was happy with, and what should be refactored the next time I look at the code. TODOs for me are not about bugs per se, but about documenting the less than stellar parts of the code. It does not make sense to make code 100% perfect the first time you write it. It will never be perfect, and the level of perfectness varies by function and line. There is no way to document this except by using comments, and TODOs are the perfect tool to make the code not only readable, but explain all the warts to a newcomer. I have spent way too much time reading other people's code wondering: "why did they do this?", and asking the programmer, the answer is often "I thought about doing X, but this was simpler at the time". If it had been documented as a TODO, it would have spared me the roundtrip with the developer. 
Yes, acid-state is ACID. Why do you believe that the log writing failed and the server still went through with the write? It sounds to me like the transaction log failed to write because it ran out of diskspace, that triggered an exception that brought down the server, and on startup acid it discovered that the acid-log was incomplete and complained. I believe that is exactly what you want. Deleting the aborted transaction silently does not seem like the correct default behavior? In theory, it would have been nice if instead of going down, hackage switched to read-only mode. But if the system was out of disk space, it seems like even read-only mode could run into trouble do to logging and other features which still require disk. And.. moving to a different server is really a better use of developer time than trying to handle out-of-disk-space errors with clever coding. What is missing is better tools for dealing with recovery. I believe in this case it would have been sufficient to just delete the incomplete log and the server would have started. But it sounds like the database that was affected is one that gets reset every day anyway, so he just whacked it in order to get the server back online ASAP. 
The interview with **dons** in [episode 2 of the Haskell Cast](http://www.haskellcast.com/episode/002-don-stewart-on-real-world-haskell/) has some good data points spread throughout it. Standard Chartered are a big user of Haskell, one of the bigger industrial users out there as far as I can tell. See for example 41:51. Here's a few snippets (typos/mis-hearings are my own fault!) &gt; Interviewer: How has it been for you finding Haskell developers and/or creating them by hiring other developers and training them in Haskell? Has that been a difficult process, do you have a long lead time finding them? &gt; Don: We have hired, I guess, 25 straight-up Haskell developers. Singapore, London and New York. It's not hard to get resumes in. [...] It's not hard to get Haskell developers in the door, there are a lot of good Haskell developers out there now. So that hasn't been so much of a problem. I think it's been remarkably easy to get existing programmers and people who were just literate in programming, not necessarily [with a] computer science background. To get them programming in Haskell hasn't been hard at all, they think of it as "Oh, it's a bit like Matlab, it's a little bit like bash scripting/shell scripting", it's a bit like math from their finance degrees. All they're doing is composing functions. Admittedly, in this case a lot of those people will be domain experts working on models instead of building the "latticework", but it also echoes other stories from industry where the experience seems to be that it's easy to onboard new devs provided that a more experienced Haskeller has already laid down the structure so the new ones can learn it as they go. Not too different from how new Rails developers can be hired and start hacking on the app, while gradually learning the fundamentals of Rails as they touch more parts of the codebase. At other points in this interview, Don also mentions that 100+ developers have worked there at various points in time, with some churn, but due to the type-safety and purity, the system still maintains several very desirable properties (low incidental complexity, agility, performance, ease of onboarding). So it would seem that the perceived advantages of Haskell are, in fact, still there when the app grows and ages, some of the original devs have left etc.
Checkout the [foldl](https://hackage.haskell.org/package/foldl) package on hackage.
This looks like a pattern that might exist: `foldl2 f g z xs = foldl (\(a,b) x -&gt; (f a x, g b x)) (z,z) xs`
The semigroups package has Min and Max types which you could use in a fold(http://hackage.haskell.org/package/semigroups-0.15.3/docs/Data-Semigroup.html)
The statistics package also has a function like this. http://hackage.haskell.org/package/statistics-0.13.2.1/docs/Statistics-Function.html#v:minMax
An optimizing compiler backend. Idris is still very much cutting new ground on *how* to compile dependently typed languages not so much on how to do it efficiently, which is to expected this early in it's life. Right now it's way too easy to blow the stack with Idris because recursion is kind of flakey compared to something mature like GHC. It's also likely that code generated from Idris can be several orders of magnitude slower than GHC with -O2. Basically Idris isn't nearly production ready.
Interesting. Thank you.
Where is it?
It would look like this: import Data.Foldable import Data.Semigroup minMax :: Ord a =&gt; [a] -&gt; Option (Min a, Max a) minMax = foldMap (\t -&gt; Option $ Just (Min t, Max t)) It uses the semigroup instance for tuples and also takes care of the empty list. You can get a `Maybe (a, a)` with unwrap :: Option (Min a, Max a) -&gt; Maybe (a, a) unwrap = option Nothing (\(Min a, Max b) -&gt; Just (a, b))
This just made me start learning emacs. 
&gt; Yes, acid-state is ACID. Why do you believe that the log writing failed and the server still went through with the write? Because the initial messaging said that we lost about a days worth of data. That would only happen if we didn't make to make lots of write durable to disk.
In particular, it gives minmax as an example in its main module.
The ACID and backup story sounds good enough to me. &gt; What is missing is better tools for dealing with recovery. I think that what we are missing is the move to hosted service architectures. Those tools exist for standard databases, but they operate transparently on a hosted service. We are taking time from community members to be database admins. That time could be spent doing things moving Haskell forward. It would make a lot more sense to use a standard database and spend a few bucks (or there may be hosts that would host a project like this for free). Hackage was started a while ago before hosted databases were simple, affordable, and wide-spread, so it makes sense that it operates how it does now. But it would be a good GSoC project to re-evaluate the options.
* Deriving class instances - I suspect this could be implemented with tactics and quasi quoting but I got stuck when attempting it. * Better ffi - I haven't used it a lot but my understanding is that it can't handle some scenarios. * Better package management - .ipkg is underdocumented and only handles simple scenarios. 
Much more than libraries. First off, we need better compiler technology, as /u/hmltyp points out. Some of it is fundamental new stuff, like the erasure support, and some of it is just ordinary well-known optimizations that we haven't gotten to yet. Hackers needed! Second off, part of what makes Haskell great is the community of practitioners who have spent a long time figuring out good techniques for writing programs that make the most of Haskell. Dependent types in general are only beginning to do this, and the community is smaller which means slower progress and fewer resources. Thirdly, we need to grow up from our period of wild experimentation. So far, we don't know what makes a good DT language, so Agda, Idris and others are trying lots of things. Eventually there should be more consensus, which is when a real successor of something like Haskell could arise. Fourthly, we have no module system and no infrastructure like Hackage. This would be needed. Finally, Idris is strict. Haskell is not. A real sucessor to Haskell would have to be lazy. If you'd like to get involved, drop by #idris on Freenode. EDIT: I may have implied with the above that the goal is to be a successor to Haskell - it really isn't. The goal is to make a cool language and learn something in the process.
[internal screaming intensifies]
[/* When I wrote this code, only God and I understood what I was doing. Now, only God knows. */](http://stackoverflow.com/a/316233/2008899)
To use quasiquotes for deriving, we'd really need to extend reflection to also apply to definitions, much like they've done in the latest Agda release. I should do that one of these days. An ITU Copenhagen student also made an implementation of deriving using described types, but it didn't get fast enough to be practical.
There is an [LLVM backend](https://github.com/idris-hackers/idris-llvm). We still need to do more things before generating the LLVM IR though.
Laziness, real typeclasses, and a ridiculously fast parallel backend. You know, the Haskelly stuff in Haskell. ;) 
For the lazy: &gt;&gt;&gt; L.fold ((,) &lt;$&gt; L.minimum &lt;*&gt; L.maximum) [1..10000000] (Just 1,Just 10000000)
I don't think SaaS platforms are necessarily the answer; but I do agree something like an actual database is desirable. PostgreSQL and MySQL are reasonably easy to maintain and secure, and manage (in fact, we already run them for other needs), they have years of extensive testing, and our new infrastructure is quite solid with round the clock backups on these systems; we'll likely add replica slaves too sometime. We have needed very little intervention with these machines. We do have a degree of free hosting with the new infrastructure, but not '100% free', but I wonder about the couple of bucks estimate. :) Some providers do things like charge via IOPS, others do things like tier their DB sizes, meaning more data == more $$$, even if your TPS/IOPS are the same. The IOPS probably sounds like the biggest hit; Hackage is on course to deliver 5TB of egress a month on average Real Soon. We want to keep our costs low where sensible so we can expand later if necessary. Right now, our current DB instances are *over* provisioned in terms of resources, and they're pretty light, and we pay no ingress/egress. If we *could* get those hosted for free though, with standard import/export, I'd be on board though. Hosted DB services just seem quite expensive in general. But ultimately a SaaS backend or a secured hosted backed on haskell.org isn't the biggest problem, we can always change that later - the real problem is it would be a huge amount of effort to rewrite the current Hackage backend code to use e.g. a SQL database. The storage and data layer for packages would need to change significantly to accommodate this. I'm not sure it's realistic to pull off in a GSoC.
&gt; but I do agree something like an actual database is desirable. Can you explain some of the issues you are currently facing with acid-state? 
"Real" type class in the sense that Haskell people use the term means that there is a coherence condition which holds: there is at most one instance. Bob Harper fwiw uses the term type class in a different way in his book, but I do not think it has caught on. IMO Haskell-style type classes are conceptually inelegant for certain reasons, but very convenient in Haskell. I am very doubtful, however, that we have any use for them in Type Theory. Modular type classes (Harper, Chakravarty, Dreyer) on the other hand seem very useful, and this is demonstrated by their latest incarnation as "canonical structures" in Coq.
I think when `edwardk` speaks of 'real type classes' he means almost exactly what is being attacked in that post. Why I would want 'modularity' for eg `Functor` and `fmap`, so that I could work with different implementations etc., God only knows. 
yeah. Personally, I think it would be more fun to improve acid-state tooling that basically rewrite hackage again just to use SQL -- which would just create different problems. Using the remote interface it is currently possible to fire up GHCi and inspect and modify the database on the running server. Your inspections and modifications are limited to the queries and updates already built in to the server though -- you can't perform arbitrary updates. But as a first step, being able to have some level of a command line interface would be nice. I've often thought it would be nice if acid-state produced a .schema file in the database directory that contained all the types required to parse and load the latest checkpoint. All the information should be available to construct that. Replication would be great. We had it in older versions of acid-state (aka, happstack-state). Lemmih claims he figured out the right way to do it for acid-state -- but then got sucked into other projects. It would be great project though. Another fun idea to explore is logging transactions to S3 instead of to the local disk. We also had support for this in happstack-state at one point, but it needs to be reimplemented for acid-state. It's not terrible difficult, AFAIK. acid-state already supports multiple backends -- it is just a matter of implementing a few simple functions to read and write the data. I've wondered if it would be possible to implement some sort of generic acid-state command-line tool that could make modifications using generics or something similar. Or maybe something based around lenses. There is still a lot of low hanging fruit for acid-state tooling and features. Just not sure how to get people excited about working on it. 
Ok, that's not how it worked in, for example, the javascript ones so I was expecting there to be some tests already written.
&gt; Finally, Idris is strict. Haskell is not. A real sucessor to Haskell would have to be lazy. Why is that? One of the things that bothers is me most about Haskell is that laziness is the default.
&gt; In Type Theory, we do not have this problem at all, since we can reason about value equality natively. The problem is that you can't engage in the program transformation of always moving all the dictionary passing to the use site that we do in Haskell. That is still broken even with the ability to reason about value equality natively.
Half the arguments about laziness tend to be muddled in the fact that there's only one language really exploring this point in the design space. We need more data points.
&gt; To use quasiquotes for deriving, we'd really need to extend reflection to also apply to definitions, much like they've done in the latest Agda release. I should do that one of these days. Any pointers on implementing this? I may have some time this weekend to look at it. When I made a short lived attempt I was simply going to try deriving an Eq for a record using reflection. Something along the lines of: record TestRecord : Type where MkTestRecord : (name : String) -&gt; (age : Int) -&gt; TestRecord deriveEquals : TT -&gt; Tactic deriveEquals tt = ?Exact deriveEq' where deriveEq' : TT instance Eq TestRecord where (==) = proof applyTactic `(MkTestRecord) I ran into a couple of issues with the tactic not having a goal type and the TT ast erasing quite a few things that I would have found useful. On a semi related note it would be nice if proof and applyTactic were normal idris methods so they didn't need to be at the call site. Anyway, I'm not really sure this is the way to go but wanted to see if it was possible at the time.
Are you worried about run time or compile time? Erasure could give the right operational semantics - tag the data structure with an erased instance, and make the operations take by some implicit mechanism a non-erased instance and an equality proof. Compile time would need polymorphism and maybe some implicit type parameters.
Actually I can compile very simple apps with the 7.8.2 installation, but cannot compile anything that refers to "System.Directory" (which is needed if I am to compile 7.8.3 from source) or "Distribution.Simple" (which is needed if I am to compile any Cabal package from source). Thanks.
The output of `ghc-pkg list` and `ghc-pkg check` might be helpful here. In the worst case, you can find an older binary version of ghc that works on your distro (preferably one actually capable of compiling 7.8.2, otherwise you might have to incrementally compile up versions until you reach that point). To avoid this problem in the future, I recommend *always* using [Cabal sandboxes](https://www.haskell.org/cabal/users-guide/installing-packages.html#developing-with-sandboxes)! That way, you'll never damage your ghc install and in the worst case you simply blow away the sandbox and create a new one. It's also possible to use a single sandbox for multiple projects, if you want to avoid redundant compilation. 
By braces do you mean ()? that's normal; the "ghc" package (the GHC API) is hidden by default, as it uses unqualified module names that would be likely to conflict with an application's. More concerning is "I cannot compile anything now". Can you be more specific? What exact error message is produced?
Maybe the snails could be replaced by a painter with a canvas. Pictures are given to the painter as arguments. The painter looks at the pictures, thinks, and then paints a new scene based on them. Also maybe the painter could be a robot, so that it always paints the same scene given the same pictures.
`directory` depends on `bytestring` via the `unix` package. You should do as Vulpyne says; the main question, I think, is whether you have broken the boot libraries that came with ghc; these would be among the global packages i.e. the first stanza of stuff from `ghc-pkg list` . (In my case they are Cabal, array, base, bin-package-db, binary, bytestring, containers, deepseq, directory, filepath, ghc, ghc-prim, haskell2010, haskell98, hoopl, hpc, integer-gmp, old-locale, old-time, pretty, process, rts, template-haskell, time, transformers, unix). That `directory` is broken is a bad sign, I think, though it may be a locally installed version that is broken. `ghc-pkg check` will say. In general using a different version of one of the libraries that came with `ghc` is somewhat delicate operation. 
Why is that a problem?
agreed, you would probably make the change one data collection at a time (and it seems like you already kind of do that). I have never had any meaningful database downtime while using a SaaS provided one. I am not familiar with PostgreSQL providers, otherwise I would ask them on behalf of the project about a discounted hosting rate.
Because it enables us to have very simple reusable data types and not having to rewrite them for every intersection of functionality we want, or having to write one off transformations based on all of the properties of thing we compose. The typeclass solution makes everyone pay for every dimension in the lattice of theories you add. This is bad. The module/"implicit" approach makes you pay for every point you instantiate, and then to map in and out of where those features are available you have to pay for all of the transitions you use. This is worse. I build libraries with highly refined sets of properties, so it matters to me that every dimension worth of functionality I add has to be paid for by first doubling the amount of work I do, then squaring that to get back the functionality I have right now in terms of code reuse, or forcing everyone to write one-offs -- all in the name of worry about having two possible instances for a data type.
This is the result of ghc-pkg list {Cabal-1.18.1.3} {aeson-0.8.0.0} array-0.5.0.0 {attoparsec-0.11.3.4} base-4.7.0.0 {bin-package-db-0.0.0.0} {binary-0.7.1.0} {blaze-builder-0.3.3.2} rts-1.0 {cassava-0.4.1.0} containers-0.5.5.1 deepseq-1.3.0.2 {directory-1.2.1.0} dlist-0.7.1 filepath-1.3.0.2 {ghc-7.8.2} ghc-prim-0.3.1.0 {hashable-1.2.2.0} {haskell2010-1.1.2.0} {haskell98-2.0.0.3} hoopl-3.10.0.1 {hpc-0.6.0.1} integer-gmp-0.5.1.0 mtl-2.2.0.1 old-locale-1.0.0.6 old-time-1.1.0.2 {parsec-3.1.5} pretty-1.1.1.1 primitive-0.5.3.0 {process-1.2.0.0} {scientific-0.3.2.1} syb-0.4.2 template-haskell-2.9.0.0 {text-1.1.1.2} time-1.4.2 transformers-0.3.0.0 transformers-0.4.1.0 {unix-2.7.0.1} {unordered-containers-0.2.4.0} vector-0.10.10.0 The output of ghc-pkg check is too long to be copied here 
I tried to get an older binary version, but anything higher than 7.0.1 requires GLIBC &gt;= 2.7, which I don't have. I tried 6.10.1 and it requires libedit.0, which I also don't have. If I compile one from the libedit 0.3 source, the binary complains that the library is ELFCLASS64 and it cannot use that. If I compile one with "-m32" in CFLAGS, the compilation fails because it cannot link to some library on the system. So I am stuck right now.
Unfortunately I installed packages into a global location (where I put the ghc-related things). If I manage to recover the setup I will be more careful next time.
So you can get 7.0.1 working from binaries? My advice would be to get that, then compile the 7.4.2 sources with it. 7.4.2 can compile the latest version. So you'll download the 7.0.1 binary, compile 7.4.2 with that, then compile 7.8.3 with the 7.4.2 you just installed. At that point you (theoretically) will have a working install of the latest version. Incidentally, anything above and including 6.12 can compile 7.4.2, so if you can find *any* working binary at least that recent you should be able to follow my advice. Once you have 7.8.3 installed you just need to build cabal-install. If you religiously use sandboxes, you'll never end up in this situation again. Final step: [Party!](http://nedroid.com/2009/05/party-cat-full-series/)
sorry, I was not very clear. I cannot get anything working from binary now, including 7.0.1
I want to find a binary version that can work, but so far I am stuck.
What I don't understand is why the Haskell compiler couldn't be used to compile Idris. I mean, the dependent types should ultimately be tangential to the evaluation. The core should still be fairly pure LC terms.
It seems 6.10.3 binary happens to work. So I will use that to go to 7.0.1 then to 7.4.2 then to 7.8.3. Hope I can get through all the steps without problem.
You'd need to erase all the irrelevant stuff first? 
The tricky part, I suspect, is going to be how you represent data types. The lambda-y portion is common enough, but what about algebraic data types? And how do you manipulate them? That I think is where the tricky choices will emerge. Tho it'd be a very interesting project to work on, definitely.
This is not working well, I cannot get from 6.10.3 to anywhere. I will open another thread to ask questions related to that, thanks.
Idris is strict, whereas Haskell is lazy. Technically this can be fixed by adding lots of `!`s to the generated core, but that would probably prevent GHC from optimizing effectively.
&gt; The module/"implicit" approach makes you pay for every point you instantiate, and then to map in and out of where those features are available you have to pay for all of the transitions you use. This is worse. Could you explain this more? AFAICT, the way Agda does it is just as powerful as haskell's typeclasses.
For sure. There's a lot of necessary opinionated design decisions.
What OS (be as specific as possible, please) are you actually using? There might be unofficial binaries somewhere. Also, just throwing this out here but some things *might* possibly be easier if you replicate your environment locally using a VM like VirtualBox. It's not necessarily going to help so you'll have to be the judge, but at least you'd have root. If you get a similar VM environment working, then you can perhaps just build stuff locally and copy the binaries rather than trying to build on the remote machine.
windows or linux? 
I used RDS briefly and it worked fine. I am scarred from using MySQL in the past though. I hope today's MariaDB is better, but you are better off asking someone else :)
```M-x run-haskell``` I think?
Looks a bit like http://augustss.blogspot.se/2007/04/overloading-haskell-numbers-part-2.html
I know this post has been around for a while now, so there are probably going to be fewer views, but would there be a market for private hosted hackage repos for a small monthly subscription? It seems like there are enough companies using Haskell seriously now that it would be worth having someone with a small financial incentive paying attention to keeping a working hackage server up and running.
Sounds like you need to add a hook to turn on haskell-mode whenever a .hs file is loaded. Then you just hit F5 or whatever binding you have set for haskell-process-load-file. Did you edit your .emacs yet?
Actually, it occurs to me that you could probably use this AST approach to run AD at template haskell type, and then let GHC optimize the resulting AST expression at compile time. BRB
do it do it do it
OK, I've started a stub, i have a feeling its going to wind up using [bound](https://hackage.haskell.org/package/bound) and such by the end of the weekend :) 
&gt; is to use it to prove L√∂b's theorem As I said, anti-intuitionistic ideas. :) I'm not really convinced that L√∂b's theorem is sensible in an intuitionistic mode. That is to say, I think L√∂b's theorem probably ought to be *refutable* in a terminating intuitionistic setting. But who knows. &gt; For a concrete A for which a proof exists, I don't mind having ‚ñ°A ‚Üí A. What I don't want is ‚àÄ A. ‚ñ°A ‚Üí A. I don't think you're going to be able to avoid that, but maybe. It's certainly a theorem in the standard box modality logic which perfectly well captures the idea of "provable from no assumptions" (ie the "valid" judgment). But again, I feel, if we're very explicit about what the modality means when we say these propositions, it feels natural to say "of course that should be true". ‚ñ°A ‚Üí A is "if A is provable from 0 assumptions, then A is provable from some number of assumptions", duh, that's obvious. That should definitely be true for everything everywhere. We're talking about persistent truths, after all, not resources/ephemeral truths, and so we really expect that the principle of weakening should hold, which this is basically saying. But now, why should *L√∂b's theorem hold*? That is to say, given those intuitions about what the proposition should mean, why the hell should we expect it to be true?
So I was thinking about this and I think it's doable. All the major points of divergence for data types are, afaict, at the type level. In the abstract, they're all just a bunch of constructors. So I think the representations there end up being easy. I think the real question now would become, how can it be made generic enough to avoid forcing various evaluation schemes? Like, Idris is strict by default, Haskell is lazy. Can we define a VM where this is controllable in some way? Or even better, can we abstract away from it enough that the choice becomes a matter of sort of "how you work the knobs on the VM"? And what other things are like this? I think it's best to avoid making these things into parameters you set, because then we'll end up with a zillion parameters.
How did you install haskell-mode?
In Idris, you need type information to erase irrelevant stuff. If you don't do this, it can make your program inhabit a worse complexity class! For example, the (Vect 3 Nat) [0,1,2] is this under the hood: Prelude.Vect.(::) {a = Prelude.Nat.Nat} {n = 2} 0 (Prelude.Vect.(::) {a = Prelude.Nat.Nat} {n = 1} 1 (Prelude.Vect.(::) {a = Prelude.Nat.Nat} {n = 0} 2 (Prelude.Vect.Nil {a = Prelude.Nat.Nat}))) : Prelude.Vect.Vect 3 Prelude.Nat.Nat Note that each of those Nats is a unary construction: 3 takes about three times as much space as 1! We need type information to know which Nats are erasable (the length arguments), and if our common VM didn't understand this, we'd be sunk.
Sorry, I don't really have a good pointer for this - if I did, then it would already work! It may be a mere matter of hacking. If you want to learn the reflection stuff, perhaps consider doing something simpler with it first? Like automating some proofs?
Another thing: it seems you may have misunderstood how `applyTactic` works. `applyTactic` takes a function of type `List (TTName, Binder TT) -&gt; TT -&gt; Tactic` as its argument. This function is passed the current proof context as its first argument and the proof goal is its second. So you might do something like: mkEquals : List (TTName, Binder TT) -&gt; TT -&gt; Tactic mkEquals ctxt `(~x == ~y) = Induction x `Seq` Induction y `Seq` ApplyTactic (repeat solveEquals) instance Eq TestType where (==) = ?inst inst = proof intros applyTactic mkEquals where "repeat" repeatedly applies a tactic until failure and solveEquals does type class resolution for Eq for the parts. This will be difficult! Reflecting the structure of the goal type declaration would allow a more precise approach.
You have to set a haskell-process-type to one of ghci, cabal repl or something else. See the Haskell Mode section in https://github.com/perurbis/emacs24-starter-kit/blob/master/ben.org 
No. I explained why that doesn't work sufficiently well elsewhere in this thread. With that you deny Tree any standard combinators. You make everything for working with it a one-off deal. That is sort of the opposite of the sort of code reuse story I'm looking for. And what about when Tree only some times gains an extra property from a? Or when there are several related refinements of what you can do? What about when the author of the data type doesn't know about a class added later by another author? See other examples about Compose, transformers, etc. in my other replies.
FWIW- I really didn't mean to co√∂pt this topic to talk about typeclasses, and just sought to respond when prompted. I do think Idris is a very interesting language, just not a particularly Haskelly one.
The full message from Emacs is: https://github.com/haskell/haskell-mode/blob/master/haskell-mode.el#L245-L250 &gt; You tried to do an interaction command, but an interaction mode has not been enabled yet. &gt; &gt; Run M-x describe-variable haskell-mode-hook for a list of such modes. Following its instruction should give you: https://github.com/haskell/haskell-mode/blob/master/haskell-mode.el#L422-L448 &gt; Interaction modes: &gt; &gt; * `interactive-haskell-mode` ‚Äî Interact with per-project GHCi processes &gt; through a REPL and directory-aware sessions. &gt; * `inf-haskell-mode` ‚Äî Interact with a GHCi process using &gt; comint-mode. Deprecated. and &gt; To activate a minor-mode, simply run the interactive command. For &gt; example, `M-x haskell-doc-mode`. Run it again to disable it. &gt; To enable a mode for every haskell-mode buffer, add a hook in &gt; your Emacs configuration. For example, to enable &gt; `haskell-indent-mode` and `interactive-haskell-mode`, use the &gt; following: &gt; &gt; (add-hook 'haskell-mode-hook 'haskell-indent-mode) &gt; (add-hook 'haskell-mode-hook 'interactive-haskell-mode) Please explain where in this sequence of docs you got lost so we can make it more obvious. See also the ‚ÄúInteraction‚Äù section in the wiki: https://github.com/haskell/haskell-mode/wiki
&gt;I just don't understand how any shell 1-liner &gt; is more effort than faffing around with any ui anywhere.
I'm pretty surprised that this question was so downvoted... Sure we're in the haskell subreddit so everyone is already aware that Haskell is just a much better language all around that Javascript to build big safe projects but isn't it normal for someone who only know Javascript to ask this kind of question ? So basically I reiterate that Javascript is a pretty terrible language on many points (though there are some good ideas in there) and that Haskell allows one to use and write better abstractions while maintaining a level of safety that's way better than JS. So Haste that allows you to write your client code in Haskell rather than Javascript is a godsend for those Haskellers that would like to develop for the browser but don't wish to suffer through JS awkwardness.
Modules handle Set much better than type classes. The Ord constraint should not be placed on the insert/lookup functions because that exposes an implementation detail to the client code. Client code shouldn't have to care whether the set is implemented with comparisons, a trie, hashing, or something else. You say "I'm not smart enough to get the right API for code I write the first time I write it.". Well, in this case modules drive you in the right direction, whereas type classes drive you in the wrong direction. &gt; So now go define a combinator that swaps out g for g'. The space of dictionaries/modules/implicits/ for Compose f g' now depends on both f and g. You have to write one-off code to build it, or compose them individually at the use site. &gt; When you need to bake them in with something like Set the piecemeal solution no longer applies. I don't understand this part. Perhaps you can explain this with a concrete code example where things go right with type classes and things go wrong with implicits?
Why would it be like that at all? &gt; Also, how will the Haskell compiler see which parts are irrelevant? Why does it have to? Irrelevant things are like the second argument to `const`. It's there, it just happens to not matter.
Yes, and yes. My original comment which spawned this discussion was that you should use collection comprehensions in Python rather than filter/map functions. :)
&gt; That is to say, I think L√∂b's theorem probably ought to be refutable in a terminating intuitionistic setting. If by that you mean that in a logic where (‚ñ°A ‚Üí A) is a theorem, it can be proved that ((‚àÄ P. (‚ñ°P ‚Üí P) ‚Üí P) ‚Üí ‚ä•), then yes, I agree, that's what I've just shown in my previous comment. &gt; But again, I feel, if we're very explicit about what the modality means when we say these propositions, it feels natural to say "of course that should be true". ‚ñ°A ‚Üí A is "if A is provable from 0 assumptions, then A is provable from some number of assumptions", duh, that's obvious. Maybe that's the problem, then: "‚ñ°A" and "A" are not supposed to mean "provable from 0" or "provable from some number of assumptions". They are supposed to mean "provable" and "true", in a way which apparently allows (‚àÄ P. (‚ñ°P ‚Üí P) ‚Üí P) to be interpreted as "a mathematical system cannot assert its own soundness without becoming inconsistent". If ‚ñ°A ‚Üí A only meant "weakening hold in this logic", then (‚àÄ P. (‚ñ°P ‚Üí P) ‚Üí P) would state that "a mathematical system cannot support weakening without becoming inconsistent". That's clearly not what I want. I don't want to reinterpret the symbols "‚àÄ P. (‚ñ°P ‚Üí P) ‚Üí P" in an intuitionistic logic to see if the new meaning of the formula is true, I want to encode the original meaning of those symbols into intuitionistic logic to see if that new formula is true. Maybe I'll need to use different symbols in order to do that. So maybe this whole idea of establishing the truth of ‚ñ°A from a proof of A with no assumptions is what led us astray. I tried to translate Wikipedia's proof into an intuitionistic setting by copying its introduction rules, and you complained that I failed to add proper elimination rules. Despite all my efforts to find elimination rules which would not cause this, I now agree that if I added *proper* elimination rules, I would get "‚ñ°A ‚Üí A". But that doesn't mean that L√∂b's theorem is intuitionistically false, it just means that I haven't encoded it properly. Let's see, L√∂b's theorem talks about other mathematical system. Provability logic uses "provable from no assumptions" to mean "provable", and maybe that's the part which only works in classical logic. You talked about "valid" earlier; that would mean "true in all possible worlds", wouldn't it? That's not what I want either, because it would mean that the formula is provable everywhere. Instead, I want ‚ñ°A to mean that it's provable in a particular system, the one which we want to show is inconsistent. Maybe I need a binary connective indicating which system I'm talking about: (PA‚ñ°A), for example, would mean that A is provable in Peano Arithmetic. Under this new notation, the formula encoding L√∂b's theorem becomes: ‚àÄ P. PA‚ñ°(PA‚ñ°P ‚Üí P) ‚Üí PA‚ñ°P I'm not quantifying over PA, because the theorem only holds for logics which are strong enough to prove basic statements about numbers, such as PA. I could instead encode the required features more precisely, like so: ‚àÄ P. PA‚ñ°(0 : Nat) ‚Üí PA‚ñ°(‚àÄ n. n : Nat ‚Üí S(n) : Nat) ‚Üí ... ‚Üí PA‚ñ°(PA‚ñ°P ‚Üí P) ‚Üí PA‚ñ°P ...but I'm not familiar enough with the fixed-point part of L√∂b's theorem to know which features of PA I need. As I said, I will need to watch more of Pfenning's lectures in order to attempt to find proper introduction and elimination rules for such a many-worlds logic, so it could take a while. But do you think this encoding has a better chance to succeed than an approach where ‚ñ°A means "provable from 0 assumptions"?
It's necessary to compute with irrelevant constructor arguments in order to get something to type check. In the Vect example above, for instance, the Nats representing the lengths of the sub-vects need to exist in the original program in order to check that the length of the append of two vectors is the sum of the length of the inputs - yet adding an extra Nat makes the vector take quadratic space instead of linear! While there's special-case optimizations for Nats, the same problem repeats itself with many other structures. Like Edwin says in the podcast episode, erasure is necessary to get back the complexity class of the program you wanted to write. It's not just something that happily makes your program a bit faster.
Thanks, I'm lazy....
I'm not sure what you mean by "yet adding an extra Nat makes the vector take quadratic space instead of linear". Can you elaborate?
&gt; Modules handle Set much better than type classes. My point with the set example is that there are two reasonable approaches in Haskell. You can bake the ordering into the Set itself, or you can bake it into the methods that work with the Set. This means you can push the use of the typeclass to later on. In modules you also have two reasonable approaches you can make it into the Set itself or into the module. This means you can pull the use of the typeclass forward. If we had a module system in Haskell I'd love to have the option to employ it to curry out more stuff from my types up to the module level. However, I wouldn't trade the power of typeclasses for that feature. None of the code I'm writing lately can work in a language without typeclasses, so you'll pry them from my cold dead hands. ;) &gt; Well, in this case modules drive you in the right direction, whereas type classes drive you in the wrong direction. I suppose reasonable people can disagree on that. I will agree to some extent that the `Set` example is particularly weak, because it only involves one dictionary and no way to switch without going through an already ad hoc combinator anyways. &gt; Perhaps you can explain this with a concrete code example where things go right with type classes and things go wrong with implicits? Having posted several walls full of text here, which I have to say is definitely an inappropriate forum, I think I'm going to stop filling a post about Edwin's lovely little language with noise about a feature it doesn't really purport to offer. I've volunteered to give a talk on the topic of typeclasses vs. implicits at the next Boston Haskell meetup in a couple of weeks. It will be recorded.
&gt; Client code shouldn't have to care whether the set is implemented with comparisons, a trie, hashing, or something else. *How I agree* If you want to talk about an interface for Set-like things, then there is a place for such a signature. In fact I'll even go so far as to say that modules are better at *this* signature than typeclasses, because if you are describing an API for building up and tearing down sets if you wrote code using it in Haskell you'll eventually get stuck w.r.t. inference and need signatures. Haskell is really bad at building that interface. This is the sort of API that backpack is intended to address, if we can work through the issues with its design. *How I disagree* `Set` is a concrete implementation, not an interface. It is an implementation of your signature, not a signature to be filled.
You wrongly assume I only know Javascript, and I agree with your assessment of it. My issue is not with Haskell itself either, it's a well-thought-out language, in contrast to JS, though it's maybe not terribly practical. To me, Haste looks worse than JS because: It's intrinsically slower than plain JS, and you'll need to wrap your libraries in FFI, which will make calls to them slower as well, and you'll have to do ugly inlining. The runtime errors are useless. The debugger is useless. There are no sourcemaps! It's the "price you pay for writing Haskell" so this is worth it? I disagree. As the presenter mentions this amounts to going back many years in web dev efficiency. And the huge ecosystem of JS tooling will be largely useless to you. And converting all your numeric primitives to floats is rife with problems. And there's no modern DOM API. "You don't have to be a PhD candidate (but it helps)". Even these guys who are giving a talk at Strangeloop repeatedly talk about how they don't understand what's going on. Using this stuff will basically ensure you have a hard time finding devs for your project. I understand wanting to use Haste to use an intrinsically better language than JS, and the typesafe AJAX calls are a nice feature, but you're taking so many hits to do this.
Why does this give you better performance than the usual symbolic differentiation? 
&gt; Implicits / agda/coq/idris typeclasses are more powerful in one sense, that you can freely choose whichever instance you want to supply at any use site. The price of this power if this is you can't refactor code freely relying on the fact that you'll always get the same instance. Is this caused by the lack of a uniqueness check or by the ability to manually pass typeclass dictionaries? The former makes sense, but I don't see how the later is the case. &gt; I don't get that out of Agda's notion of typeclasses. They don't chain. They do now, and they only consider functions declared with `instance`! There isn't a uniqueness check though.
&gt; They do now Then I retract that second claim. That's good news! =) &gt; Is this caused by the lack of a uniqueness check The issue is the lack of a *global* uniqueness check. Not just if there is only one instance in local scope. I can smuggle information out and use it later, so if the instance check isn't across the entire program, you get into trouble again by more subtle means. If you have global instance uniqueness, then by all means feel free to plumb them around manually. We do that in Haskell via my `constraints` package pretty easily. I make heavy use of that power in the `hask` project on github. https://github.com/ekmett/hask
&gt; "Real" type class in the sense that Haskell people use the term means that there is a coherence condition which holds: there is at most one instance. That condition doesn't hold. At least not in the last six years.
If you use `IncoherentInstances` you get what it says on the tin. It also provides me a nice flag to know the code in question is something not to incur a dependency on.
I still have no idea how to use emacs as I only started the other day, how do I add a hook to my emacs configuration? Also when I just do I simple function e.g. 3*4 nothing happens?? I've downloaded haskell from [here](http://www.haskell.org/platform/mac.html) and installed the package to my computer and installed haskell-mode to emacs, is there something I haven't done??
Oh yes, I see. Arguably, a good implementation should make heavy use of sharing, so that in fact you're not storing `S (S Z)` and `S Z` as wholly distinct things, but rather that they share. But I suppose in general you want to avoid relying on that. I'm still not sure how that affects the compiler tho. I mean, that type is in principle the same type (modulo type-level stuff!) as data OrnList : Type -&gt; Type where Nil : {a : Type} -&gt; OrnList a (::) : {a : Type} -&gt; Nat -&gt; a -&gt; OrnList a -&gt; OrnList a The fact that the `Nat` value happens to have some impact on the types, rather than being merely an argument to the constructor, should be an invisible fact to the compiler.
Tip: You can indent four spaces to preserve formatting. An easy way to do this is using `sed` and `xsel`/`pbcopy` to write the correctly formatted result directly to your clipboard: $ # Using `xsel` on Linux $ ghc-pkg list | sed 's/^/ /g' | xsel --clipboard $ # Using `pbcopy` on OS X $ ghc-pkg list | sed 's/^/ /g' | pbcopy Then you can just paste the contents of your clipboard directly into a reddit comment.
&gt; IncoherentInstances IncoherentInstances is not required. You can write code which introduces incorrect behavior without adding any flag (or ignoring any warning by the compiler).
A Map needs to be tagged with the order that was used because that's essential to the well-formedness of the data structure. For the other cases it should be enough to have a separate definition of a Functor/Traversable/etc structure. The scoping should work much like having typeclass instances in scope. I could work up a bigger examples, if you have some particular interesting uses of Foldable/Traversable/Etc in mind, but here's the very basics, ala Coq: Require Import Program. Require Import FunctionalExtensionality. Class Functor (F : Type -&gt; Type) := MkFunctor { fmap : forall {A B} (f : A -&gt; B), F A -&gt; F B ; functor_compose : forall {A B C} (f : A -&gt; B) (g : B -&gt; C), compose (fmap g) (fmap f) = fmap (compose g f) ; functor_identity : forall {A}, fmap (fun x =&gt; x : A) = (fun x =&gt; x) }. Ltac solve:=intros;apply functional_extensionality; let H := fresh in intro H;destruct H;reflexivity. Instance pair_functor_2 {B} : Functor (prod B). apply (MkFunctor _ (fun _ _ f p =&gt; (fst p, f (snd p))));solve. Defined. Instance pair_functor_1 {B} : Functor (fun A =&gt; prod A B). apply (MkFunctor _ (fun _ _ f p =&gt; (f (fst p), snd p)));solve. Defined. Definition Compose (F G : Type -&gt; Type) (A : Type) := F (G A). Instance compose_functor {F} (ff : Functor F) {G} (gf : Functor G) : Functor (Compose F G). apply (MkFunctor _ (fun _ _ =&gt; compose (fmap (F:=F)) (fmap (F:=G)))). (* composition *) setoid_rewrite functor_compose. intros;refine (f_equal _ _). apply functor_compose. (* identity *) intro A. unfold compose. rewrite functor_identity. apply functor_identity. Defined. Definition Foo (p : nat * bool) : (nat * bool) := fmap S p. Definition Bar (p : nat * bool) : (nat * bool) := fmap negb p. Definition Baz (p : nat * (bool * nat)) : (nat * (bool * nat)) := fmap S p. (* needed some help *) Definition Mid (p : nat * (bool * nat)) : (nat * (bool * nat)) := fmap (F:=Compose _ _) negb p. Ignoring the suggesting names, Class basically defines a type of dictionary, Instance a particular dictionary or function returning a dictionary, and they are slotted together at the use site sort of like Haskell would.
it'd be awesome if someone could write some example tests for it or something too. i have no idea how to do testing in haskell although i normally write my tests first in javascript.
I find that there's less need for testing in Haskell than in a dynamic language like, say, JavaScript -- the type system catches quite a lot of bugs. If you'd like a testing framework though, try [QuickCheck](http://www.haskell.org/haskellwiki/Introduction_to_QuickCheck2).
Just pointing people to QuickCheck accomplishes surprisingly little. People are *not* used to doing property testing ‚Äì and that's usually where the problem is. People just don't know how to do property testing. They know QuickCheck exists, they know they should do property testing but... where do you start?
Functions need to compute with that argument in order to please the type checker. Thus, it's not obvious which of the constructor arguments are real interesting data and which ones are just for the type checker, unless you have more information. Haskell compilers don't give us a way to provide that information, because they have the type/value distinction. This is why "throw it at Haskell" is not likely to work very well.
In the time it took you to write that cimment you could have learned the 1 line command to contribute by phab or the mailing list.
Fair point, I hadn't thought of there being a distinction between conventional testing and property testing before. Thanks!
Hmmm... Does https://hackage.haskell.org/users/password-reset work for the use case of resetting passwords? Is the process fully automated?
The [specification](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-460003.13) gives a pretty thorough description of how frontend syntax desugars. You can always look at the generated Core for a given expression using ``-ddump-simpl`` or [ghc-core](https://hackage.haskell.org/package/ghc-core) which is basically the core foundational language in which all Haskell is desugared into after type-checking.
In particular, conventional testing vs. property testing requires articulating laws your functions must satisfy. This is productive but not something people are accustomed to.
GHC elaborates everything to a simpler language called "Core", which has much fewer constructs than Haskell's surface syntax. I believe that all the constructs you gave compile to case statements in Core. I don't know much about Core myself, but here is a two-part tutorial I've been meaning to read on the subject: http://alpmestan.com/posts/2013-06-27-ghc-core-by-example-episode-1.html http://alpmestan.com/posts/2013-10-01-ghc-core-by-example-episode-2-evaluation.html
I highly recommend showing (and resolving) all compiler warnings. To do this, add "ghc-options: -Wall" to the Executable section of your cabal file. You might also be interested in using [HLint](https://hackage.haskell.org/package/hlint), a static analysis utility, which would suggest a number of ways to improve the code. While not specific to Haskell, you should also take care to not put end-of-line whitespace in your source code. Regarding "golf," here is what the person in #haskell was referring to: http://en.wikipedia.org/wiki/Code_golf
I'm new to this stuff, but isn't Haskell already polymorphic in strictness *in effect*? Nothing is forcing me to pass a lazy function around, if I define it to be strict. I don't see the value in having polymorphism along that axis? However, non-polymorphic (:-&gt;) seems useful as it will automatically convert a lazy function into a strict one if the algorithm thinks that's a better fit. And since it changes the semantics of a function to go from lazy to strict, it really ought to be represented in the type system. That seems to aid in isolating algorithms from each other through their APIs.
This is a really, really cool idea. Breaking big problems down into small digestible pieces is always extremely valuable. IDEs are generally so gigantic and monolithic that whatever small productivity gains you might gain from them are lost in just dealing with the IDE, plus proprietary lock-in. This approach of breaking down the IDE concept into small digestible pieces is absolutely brilliant!
Thanks for pointing out that the video was about different FRP alternatives, I had incorrectly assumed that it was only about elm. According to the video, if an FRP library is written in Haskell, then it's probably in the category of higher-order FRP. Therefore, while the video was very instructive about the kinds of FRP systems one might find outside of Haskell, it is of little use for distinguishing between the various FRP libraries we have in Haskell.
There are two categories of emacs plugins: those that work well with inexperienced emacs users, and those that don't. I don't think haskell-mode fits into the first category yet. Don't take on too much too soon. It might be best to start with smaller plugins and work through the emacs manual and tutorials. With one caveat, I don't like the heavy and crazy terminology that the built-in GNU Elisp manual uses. It is not consistent, and it explores the language in a very unstructured and unguided way. It might be better to seek other guides for elisp, before approaching the elisp manual. Like anything, emacs takes time and a lot of head scratching, but it's worth it. 
That's right. While the type system is extremely helpful, it is far from a complete replacement for writing tests. You might want to look at: * [tasty](http://hackage.haskell.org/package/tasty) - a framework for running all sorts of tests and collecting the results. It supports all of the following testing frameworks. * [HSpec](http://hackage.haskell.org/package/hspec) - a nice unit test framework. Also provides a nice syntax for specifying simple tests from the following testing frameworks. * [QuickCheck](http://hackage.haskell.org/package/QuickCheck) - Random property-based tests. Properties you specify are tested against a large number of randomly generated inputs. * [SmallCheck](http://hackage.haskell.org/package/smallcheck) - Exhaustive property-based tests. Like QuickCheck, but tests all possible inputs, optionally up to a specified size limit. The property-based tests really are more useful and powerful than traditional unit tests whenever they make sense - which is most of the time. So it is worth the small amount of effort it takes to learn how to use them. Another very useful tool is [SmartCheck](http://hackage.haskell.org/package/smartcheck). It doesn't directly integrate with the other frameworks; it works in its own interactive shell. But it is very useful for making more sense out of a failed QuickCheck test when the reason for the failure isn't clear. It searches for a minimal counterexample, and it searches empirically for a general rule that describes when the test will fail. All of these frameworks have great examples and documentation if you follow their homepage links.
Excellent. Thank you!
It took me minutes to answer 2 python questions to have an account.
As an example - chosen at random, perhaps not a good example - I looked at https://github.com/exercism/xhaskell/blob/master/beer-song/beer-song_test.hs This is a good example of writing [HUnit](http://hackage.haskell.org/package/HUnit) tests on the bare metal. But a good part of the code is re-inventing the wheel of running the tests and making the test definitions more readable. It would be much simpler to write and read if it used [HSpec](http://hackage.haskell.org/package/hspec) to define the tests, and either [tasty](http://hackage.haskell.org/package/tasty) or HSpec's own test runner to run the tests.
thanks : ) tomorrow i'll try writing some tests for this with the tools you suggested.
Since you're not using multiple files, space out the parts into related sections. It's okay to make your code mimics a flow diagram or UML, even if it's a tiny project. Don't let line count drive you from making good choices.
I agree. I've never been happy with big fully featured IDEs. I often say Linux is my IDE and while Linux is great, its still missing a bunch of things that many of the big IDEs have. Things like Neil's GHCID are a step in the right direction.
CIS 194 talks about desugared forms in some lessons http://www.seas.upenn.edu/~cis194/spring13/lectures.html The case statement in lecture two or three comes to mind
another good source for learning about Core is the book The Implementation of Functional Programming Languages by SPJ which explains and demonstrates how to implement Core.
Well, in that case, you can't really ever throw these things away except by inspecting all the functions in the compiled program. This one *does* care about the Nat arg: length : Vec n a -&gt; Nat length Nil = 0 length ((::) {n} _ _) = n
When using some sort of IDE or other automation that jumps you to error locations, the code citations are unnecessary ramblings. But if you're just using an editor in one window and a shell prompt in another, those citations are critical for finding your place after fixing previous errors and changing the line counts slightly. (Yes, fixing the errors from bottom to top sometimes helps, but not always.)
If you are successful in getting SPJ to do this - fantastic! Could you please then also use that same magic to get your wonderful idea of [type signature operator sections](http://augustss.blogspot.co.il/2014/04/a-small-haskell-extension.html) implemented?
I always thought of this being part of the specification of the program. Then again, a lot of personal and open source projects have no specification, let alone a formal one.
I tweeted at it but it's ignoring me
haskell-interactive-mode for haskell-mode also works in the same way: launch GHCi as a pipe and talk with it. Despite the meagre list of productivity gains listed, I expect once Neil is settled with this and integrated into his favourite editor he'll want more features, and will find GHCi lacking.
this is so beautifully short! i'll definitely be spending some time studying this! thanks for the encouragement : ). we use javascript at work with lots of promises and other functional things so i think i had a head start.
Does the interactive mode auto reload on saving? That really makes a difference, I find. I will want more features, of course, but I think you get most of the productivity gains from the pieces listed. The other features are valuable, but my approach here is minimum viable product - I now have a better environment than I did a week ago.
Isn't that more or less what Leksah is already using?
I'm doing type signature sections myself.
I already have live error checking and ghci integration with flycheck and haskell-mode in emacs.
See [here](http://www.reddit.com/r/haskell/comments/2hnmjr/neil_mitchells_haskell_blog_ghcid/ckum4dt).
What is Leksah using?
Thank for the useful reply!
I don't think you are stupid. I'm sure that programmers are smarter than the standard population. The point that I'm not as sure if haskellers are smarter than the standard programmer although I believe so.
Somebody managed to get it to run out of memory 
I don't know what exactly it uses under the hood, but it's also permanent reloading on every safe (so every change, since Leksah perma-safes by default), giving you a list of errors pretty quickly.
I should say that people usually tout simplicity as a feature for the first version, and by the 4th version they are touting actual features as the features :). This project is also definitely not anything new, I was inspired to write it after getting frustrated by hwide, as that forces my editing into a web browser, but realising that I was almost willing to give up a real text editor to get see errors on save. It's meant to be the smallest meaningful increment over manually hitting :r. Flycheck is definitely what I'm after, but sadly I'm not an emacs user. I'm a Windows user, and happy with most reasonable text editors other than emacs/vim, currently with Sublime, but I was on TextPad for about 7 years before that.
Leksah has two modes. The default mode runs "cabal build" in the background and processes the output. If the user makes an edit while this is running the process is interrupted, the file is saved out again and the "cabal build" is kicked off again. The second mode is GHCi based. Leksah starts a process with ghci process (using "cabal repl") that it controls through std in, out and err. It sends ":reload" instead of doing "cabal build" (it is not able to interrupt it when new changes are made, but :reload is much faster). I would describe Leksah's GHCi mode as a rather massive hack. For instance it sends commands to GHCi over the process std in. In order to detect when it is safe to send one it waits for a prompt to arrive on std out. To make sure std error has been read completely (before we move on to the next command) it sends an undefined symbol error before each command so it can look for the error in the std error pipe. Because the commands are sent on std in it is hard to work out a safe way to send user input on std in, so it is not supported and instead you have to code you app not to rely on std in. I wondered about setting up something in ghci to receive the commands from socket, but never found the time. Like you say though there is only so much you can do by sending commands two ghci and at some point GHC API makes a lot more sense. Leksah uses the GHC API for metadata collection, but I imagine it may be lacking a lot of features in that area too (for instance it cannot look up the type at a source location). It would be great to add ghc-mod and/or BuildWrapper support to Leksah.
oops
So, just out of curiosity I benchmarked your implementation against the naive way of doing it which is minmax xs = (minimum xs, maximum xs) The result on [1..50000000] : Your version runs out of memory, the naive version finishes within seconds on my machine. At first I thought you might want to use foldl' instead... unfortunately that gave the same result. I found that a tad surprising. I would have expected the two versions to perform about equally well.
Assuming you're referring to https://ghc.haskell.org/trac/ghc/ticket/2356, that's a bug in GHC. It just hasn't been very high on anyone's priority list of things to fix. (Also, it does generate orphan instance warnings, those warnings just aren't emitted by default, but they will show up with -Wall.)
&gt; This is all nice and good, but Monoid in Haskell doesn't actually mean monoid. Technically, it means something like "pointed magma", since the laws are only in our heads. &gt; &gt; So the usual formulation of parametricity doesn't actually give you the naturality property that you need to - say - prove associativity. (My apologies if I'm misunderstanding something.) It's true that `[]` is a free monoid no matter what, while `Free Monoid` is only a free monoid if the `Monoid` laws are assumed. But if we take away the right to assume that type class laws hold, then we can throw basically any Haskell program ever written out the window. Apart from where type/memory safety are implicated we never compel ourselves to account for the possibility of outlaw instances (i.e. that we have lied to ourselves), so it's not clear why we should take a different approach in this instance.
&gt; make it shorter, more mathematically sound and or prettier. Well, I would've implemented it as a cellular automaton. Bar that I don't really see a reason to golf the stuff you did write, though, it's perfectly fine. Also, [this](http://ethancd.github.io/snake_life/).
That's exactly what I'm saying. It's stupid to assume that typeclass instances are unique in Haskell, because the only Haskell compiler which matters never enforced it, and given that nobody cared about fixing it in the last half decade, it will probably stay broken for a lot longer. Plus, you can't just convert orphan warnings into errors, because that would break a lot more code than just the unsound cases you want to reject. I certainly prefer a language which tells me that if I care about having the same typeclass instance, I should better check it; instead of inviting me to a wishful-thinking-land of what the compiler would do if it implemented the Haskell spec correctly. In Haskell-land we have insane praises about "Haskell's typeclasses are so superior due to X" where X is a blatantly false claim like "being unique". And that's not even taking the work on modularity into account. People who think that Haskell's typeclasses are perfect should face the reality that the current approach can't survive in any remotely useful module system. My advice is that Haskell people should get off the high horse with their typeclasses. There is a reason why no language after Haskell adopted this broken approach.
This is really impressive, especially for a first program. Great work!
What is the reasoning behind your `sample` combinator? Why not simply use something like: `sample = threadDelay sampleLength &gt;&gt; getInput`? Other than that, everything looks very clean. I don't see the need to make it shorter.
One feature that leksah cannot replace is remote development. With emacs i just ssh to my dev machine from anywhere. With leksah i have to install it locally and duplicate all the libraries and everything. I wish our internet lanes would be much faster so remoting into graphical environment would be just as instant and snappy as working with ssh in terminal. But until that happens terminal is my preferred dev. environment. 
The binary (after a successful installation) says: &gt; The version of this file is not compatible with the version of Windows you're running. Check your computer's system information to see whether you need an x86 (32-bit) or x64 (64-bit) version of the program, and then contact the software publisher. I am on 32 bit Vista. Is Leksah really not compatible? Or is the version check a unnecessarily strict? If you absolutely must reject my computer, checking during installation would be better. Regardless, a bit sad :(
&gt; I think the disconnect could (and should!) be made smaller by developing the correct algebraic structures for coinductive types. That may be the case, but I don't think you could win the culture war to get that to become the norm here in Haskell. &gt; Technically, it means something like "pointed magma", since the laws are only in our heads. I personally have no issue with assuming laws I can't write down in the language. I still think of `Monoid` as a monoid and `Functor` as a functor rather than a lawless map operation that might do almost anything when given `id`. Kicking the proof obligations out to the user's ability to do outside of the language, doesn't really remove them as obligations. Break the laws and you lose the ability to reason about your code, but then that is on the user. ;) I figure my job is to make a bunch of primitives that so long as you follow the laws are canonical. e.g. the `lens` library has a ton of combinators in it that are canonical, the only way to do a thing given that the laws hold for the lenses and traversals and the like you give me. If you break the laws, the combinators all still do something interesting operationally, but they cease to be the only way to achieve that aim, so now you have to care about my implementations. But that is now on you, for having broken the laws ;)
I press ,hc in vim to get a list of ghc errors and warnings in the current file, highlighted by line. https://github.com/begriffs/haskell-vim-now
Have you tried the [vado](https://github.com/hamishmack/vado) support? It is a bit experimental, but basically mount a remote directory with sshfs so that you can access the files on your server as if they were local. Vado works a bit like sudo, but instead of running your commands as root it runs them using ssh on the remote machine (it works out the server and remote directory to run the command in based on the current local directory). You can tell Leksah to run all its build and run commands with vado in the Build section of the Leksah preferences. Once this is done add a package from the sshfs mounted directory and it should work as if it was local (except building and running will actually be done on the remote machine). Packages that are not in an sshfs mounted directory will continue to function as before. Vado support was added with developing on local VM images in mind (not remote internet servers) so I am not sure how well it will perform if your ping time is high. You will still need to install packages not in your workspace locally for them to show up in the system metadata, but the remote packages you add to your workspace should get indexed in the same way local ones do. Longer term solution may come when the Leksah UI is migrated to a ghcjs-dom based one. This is mainly to free us from our dependancy on Gtk on Windows and OS X (where it can be hard to work with), but it will also force us to split Leksah into client and server components and that may make remote development easy.
Recent builds are all 64bit. I'll see what I can do about a 32bit build, but it probably won't be until next weekend.
You mean, just like purity and type safety are "insane praises" and "false claims" because of https://ghc.haskell.org/trac/ghc/ticket/1496 ?
Interesting thanks. I'll try vado and see how it works out. Also i was not aware you guys have plans to develop a web based interface. This would truly make leksah a viable option for remote development. 
my sample thing is like a version of timeout that always waits for threadDelay to finish regardless of whether the other io ever completes. threadDelay sampleLength &gt;&gt; getInput -- never times out so you have to press a button for it to work. timeout getInput -- allows you to speed the game up by pressing buttons quickly. sample "samples" an io over a period kind of like an audio sample rate?
Back when I started writing IHaskell, I ran into the exact same dilemma. I started out piping data to GHCi, but quickly found that it was not practical for what I was doing; there was no good way to give it complex input or get complex output. Switching to the GHC API was quite difficult, especially because it is not very well documented. But there's no way you can do as much using GHCi pipe interface as you can do with the API, and the API is not bad enough to be worth trying to do GHCi hacks.
Ah, of course. I didn't realize that getChar is blocking.
IMO that is an ineffective way to visualize that data. A standard bar chart would give a much clearer picture.
BTW, I kept notes as I was trying Leksah - I didn't get far, but there might be something useful in here: * Works with the latest GHC. Good, but not as good as working with my GHC (which is still 7.6) * Installer prebuilt for Windows. Nice, but one version old - although it calls that out, so not too bad. * After the installer completes it doesn't offer to run - it should. If you use NSIS the installer would be nicer and doing that is easy (I recommend using the nsis package on Hackage for a DSL to write NSIS descriptions, but then I did write it). 
Give a programmer emacs, and they'll be productive for a lifetime.
For a second I thought I read "Shake in haskell &lt; 170 lines" =)
 ghcid: fd:7: hGetLine: end of file ghcid: fd:5: hGetLine: end of file ghcid: thread blocked indefinitely in an MVar operation I get this when trying with `cabal repl` EDIT: It's from having things in ghci.conf that don't exist in the current project (and having no .ghci), which happens to me sometimes. Not really an issue for your project, I suppose, though the error is a bit opaque.
Are you going to switch to using fsnotify? Polling for file changes ends up eating a lot of resources as a project gets bigger.
Reminded me of /u/pigworker's [CVI language](https://github.com/pigworker/CVI/blob/master/test.cvi).
After a little bit of squinting I think it is in fact somewhat clearer. Now we just need real-time latex rendering.
It's only relevant if length itself is used in a runtime relevant way.
This is amazing :) btw, have you ever thought about having things flow vertically instead of horizontally? 
There was a little bit of work along these kinds at some point, strict core (for ghc) being the most recent I'm aware of, though I don't remember if it was ever implemented. 
I can see this being used to both document and code FRP.
I've raised an error to make it give a nicer error message - https://github.com/ndmitchell/ghcid/issues/4 . It's not going to work, but that error isn't very friendly.
As of version 0.1.2 (just released) it will use ghci if there is a .ghci file in the current directory, or cabal repl otherwise, so your use case should now "just work".
It's sort of beautiful in it's insanity, though. I like it, I would like to subscribe to his newsletter (and partake of his drugs).
after understanding the semantic it is indeed clearer. For FRP, e.g. with Netwire, I have a strong desire for a network graph. take the [Unreal Engine 4 Graph](https://docs.unrealengine.com/latest/INT/Engine/Blueprints/UserGuide/EventGraph/index.html) as an example. It's nearly perfect for an Arrow-Graph
Right, so as I said, it requires whole-program analysis, meaning you can't pre-compile modules unless the compiled form can also be analyzed. Incidentally, it's possible to define type theories where this sort of non-storage is explicit. For instance with the rule: Œì ‚ä¢ N : Nat Œì ‚ä¢ M : A Œì ‚ä¢ Ms : Vec(A,N) ---------------------------------------------- Œì ‚ä¢ cons(M;Ms) : Vec(A,suc(N)) The term is explicitly missing the size index. There is no clear type for `cons`, however, nor can you just throw some lambdas at it to get it. Apparently, you can also define a version of `‚Üí`, lets call it `‚Üí2` which explicitly forbids the argument from being used in the body of the lambda, something like this: Œì, x : A ‚ä¢ M : B x ‚àâ M ------------------------- Œì ‚ä¢ Œª2x.M : (x : A) ‚Üí2 B I'm not sure if there's a cleaner way to define this without using the non-occurrence judgment tho. But with that, you could perfectly well say Œª2n.Œªx.Œªxs. cons(x,xs) : (n : Nat) ‚Üí2 (x : A) ‚Üí (xs : Vec(A,n)) ‚Üí Vec(A,suc(n)) whereby we know with certainty that `n` is irrelevant and its enforced to be so. Maybe the cleaner intro for `‚Üí2`, which sneaks in non-occurrence by way of context well-formedness, is something like this, which explicitly mentions the types: Œì ‚ä¢ A type Œì, x : A ‚ä¢ B type Œì ‚ä¢ M : B ----------------------------------------------- Œì ‚ä¢ Œª2x.M : (x : A) ‚Üí2 B But I'm not so sure whether or not the judgment for `M` even makes sense, since the type has a var that's not bound in the context. Perhaps a distinction on variables is necessary? some vars are "type-only"? Who knows.
In its current implementation, it's quite hard to say. I've not decided on the best way to do it. Making this side of things rigorous is definitely something that I'll need to do before needle can progress beyone the proof of concept stage.
I did consider it. It probably makes more sense, in terms of the shape of the space that each needle diagram will take up in a file, but I opted to implement it horizontally first for ease of parsing. Hopefully vertical flow is something a future version of needle will support!
this is simultaneously one of the most impressive and most terrifying concepts i have ever seen
*sends to 10 closest friends for chuckles* I'm still beside myself that Haskell is powerful enough to do this. 
Arrow abstractions are really not too great for reasoning about the ordering of effects anyway.... if anything, this would be a problem about the typeclass and it's silly laws rather than about this library :) 
Arrows are exactly about ordering effects, and arrow notation makes the order completely clear!
&gt; Gosh, you actually did it :) I was curious. &gt; your snake moves faster if it eats, if I'm reading that right. Nope, that‚Äôs just the score. 
How do you import just a constructor without mentioning or importing the type? Someone somewhere suggested: import Text (Text) -- import constructor Text import Text (Text()) -- import type Text
I found that compiling it from source wasn't too bad on Linux after resolving something with the text package and gtk2hs. The recent version is quite nice and definitely worth a try.
Wow, great job. This is so much more concise than the Twitter that I currently work which renders [Tweets to LaTeX](https://github.com/passy/ltxbot) :)
Nice Configurator looks good for reading in the oauth keys. I might have to change to using that.
It's quite a lame article.
Maybe 'a lot' is a bit strong. :)
How come?
We switched from NSIS to WiX. Partly because I wanted to try it out, but also because it made it easy to install fonts to make Leksah look nicer.
Yeah, when I first looked at this I thought WTF, but it indeed makes a ton of sense.
Another problem with getChar is that it will queue extra keystrokes (or parts of escape sequences, if you hit arrows keys). Besides finding a nicer input library, one way to work around that is making an IORef that hold the most recent key, and spawning a thread when the game starts that just keeps calling getChar and updating the IORef, like currentKey &lt;- newIORef ' ' forkIO $ forever $ getChar &gt;&gt;= writeIORef currentKey then you'd get input like threadDelay sampleLength &gt;&gt; readIORef currentKey
I'm actually working on something similar (using template haskell to load string diagrams from a dot file). It's hard with the current Arrow class, but /so/ much easier with the monoidal categories classes I'm working on as well :)
The windows version of fsnotify does unpleasant things to my stdout, which scares me a lot. I did look into fsnotify originally, but that put me off - plus polling is simpler. I may revisit later. 
Hipsters still mostly seem to prefer javascript.
How can i implement the ($) operator?
Yeah, I think it‚Äôs an important note, thank you. I‚Äôll try to benchmark with a `foldl2`.
This is what I came with: http://lpaste.net/111798
thanks! i'll definitely work this into the game : )
Has anyone started working on a computer algebra system in haskell? 
I too like the way this is decoupled from the editor. However, the other big feature I want on the path to a fully featured haskell IDE is autocomplete; I can't really see how we'd keep this separate.
I guess the difference is your notation swaps the y axis to be about the return order instead of effect order? I imagine using the x axis for time could be intuitive.
I've never managed to get leksah (or any other program with a gtk2hs dependency) running on OS X.
I wonder if you could use a label as the output and as an input (replacing the } and &gt;) to get a recursive arrow?
It's presently implemented as a whole-program analysis, so writing `length` that way makes your code much slower in difficult-to-predict ways! However, we have the basics in place for a syntactic convention for indicating which fields of which constructors should be erased, then issuing warnings (and eventually errors) when they're not used in an erasable manner. If we ever implement separate compilation, these warnings will need to be errors, and then we will be able to erase with impunity. It's not explicitly a part of the type theory right now - it's a separate analysis. But it could be made part of it, I suppose. That might be fun to explore.
It's the tunnel that really gets me. \ =) \ (= \ That is a thing of beauty.
What did you try? Did you try one of the prebuilt [binaries](https://github.com/leksah/leksah/wiki/download)?
I'd like to see a visual programming language (not ascii based) based on arrows and generates Haskell code so that I/we can use it to script a game with AFRP. 
I commented on making it part of the type theory [here](http://www.reddit.com/r/haskell/comments/2hif58/edwin_brady_on_idris_in_the_type_theory_podcast/ckv1axx). I discussed this also with Jon Sterling, and apparently this is precisely how intersection types work in a NuPRL-style theory, and there's work on this. But, getting back to the original point: the Haskell compiler should still be perfectly happy to deal with this. Relevance is tangential to the capacity of the compiler to work.
I think a two-way binding would be golden, WYSIWYG in principle.
I think what happened was this: * try to write + flush the transaction record to the log * kernel does a partial write and reports the short write (standard kernel behaviour) * acid-state considers transaction to have failed * everything is fully consistent at this stage and further transaction will fail * on server restart, reading the transaction log fails because of the partial record at the end of the log There is no need in principle for reading the transaction log to fail in this case. The extranious data at the end of the log could be ignored instead. I can understand why the inclination is to be very conservative in reading the transaction log, but the out-of-disk-space case means that we can get partial writes in the transaction log file and we have to deal with that somehow (either by preventing the partial writes or handling them on log replay). So the key point is that fdWriteBuf need not fail atomically, it can fail to write all the data, but write some. This is the standard behaviour of write(). It tends not to happen for disk files, but can happen when the disk gets full, or you hit a quota limit.
I think string diagrams can work really well up to a point, and work best when they're drawn. This, like McBride's, is very visually noisy, owing to the textuality.
Have you heard of Bitsquid and their visual game scripting language [Flow](http://bitsquid.blogspot.ca/2010/09/visual-scripting-data-oriented-way.html)? One of my colleagues worked on this, and when he showed me how the system distinguishes between data signals and event signals (Behaviour and Event!), I thought this strategy would be a great way to represent and manipulate a first-order FRP network. Or maybe Flow is already similar to the asynchronous data flow form of FRP, I'm not familiar enough with either technology to tell. Anyway, what my colleague explained is this: in Flow, you have a graph of nodes, each of which have many inputs and many outputs. Those inputs and outputs have types, and you can use wires to connect an output to zero or more inputs. Most of those input and output types are for data, which I assume propagates changes from node to node like an Excel spreadsheet or any node-based compositing tool. But one of those types is the type of events: when you connect an output event to an input event, it tells the engine to trigger the input event whenever the output event fires. Add a few primitive nodes for `filter`, `accum`, `union`, etc., and tada! You have a visual language for describing an FRP network. I'd love to work on a Haskell version of this. What I really liked about this representation was the way in which FRP graphs could be made more modular. I don't know if Flow actually supports this, but it's clear to me that you should be allowed to select a bunch of nodes and collapse them into a single node which would expose some or all the unconnected inputs and outputs, like [Conal Elliott's tangible function composition](https://www.youtube.com/watch?v=faJ8N0giqzw) or [Houdini's digital assets](http://www.sidefx.com/index.php?option=com_content&amp;task=view&amp;id=2677&amp;Itemid=132). I guess the non-visual equivalent of this would be to write a function which takes a bunch of Event and Behaviour as input and would return a tuple of Event and Behaviour as output, but I haven't seen anybody do this yet. Again, I might just not be experienced enough with FRP, maybe it's actually a common pattern.
Why should that be any different? I guess maybe if GHC is going to try to do something special with both functions simultaneously that assumes that their types are the same. But other than that, I don't see why GHC should care. When types aren't involved, the code just becomes Lisp-y, so whatever portions of GHC are type-neutral should all still work.
I can't believe that - not having read up on arrows at all (haskell newb) - that this post and some of the comments in here are making me think I actually get what arrows are. If I'm right, they're something I've been pondering and wanting for a few years now. Most of my false-starts have involved drawing out ASCII graphs, but I never took them anywhere, because it seemed like complete madness.
I think my version is clearer. I call it "hypodermic." fNeedle :: (Int, Int, Int) -&gt; (Int, Int, Int, Int) fNeedle = [nd| ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïû‚ïê‚ï°(+1)‚ïû‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚îÑ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ïë ‚ïû‚ïê‚ïê‚ïê‚ïó ‚ïë ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚îÑ ‚ïë ‚ïë ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïë ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïû‚ïê‚ïê‚ï°‚ïë‚ïû‚ïê‚ïê‚ïê‚ï©‚ïê‚ï°uncurry div‚ïû‚ïê‚ï©‚ïê‚ï°negate‚ïû‚ïê‚ïê‚ïê‚îÑ ‚ïë‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ïö‚ï°(*2)‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚îÑ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò |]
This is what arrows are? I've been wanting this for years. I've played with a bunch of different syntactic ways of drawing out a dataflow graph of code in ASCII art. If this is what arrows are, I'm already sold.
&gt; I think a two-way binding would be golden You mean generating a graph from code? if so definitely.
This is really nice.
Wow, that's actually readable! Now it just needs some way of editing it. Couldn't you just use "‚ï¨" for the bridge now?
Not without upsetting all the plumbers and EE majors.
No, that junction has different implications for the flow of data, surely.
That's actually excellent.
Something like this would be great if it was easy to type.
After the first few months of "Ooo shiny let me.just take 10 minites to configure this..."
but but types trollololol
This approach makes it a little unclear whether the flow of data is upward or downward, in that one vertical section. Does Unicode have curved join characters, like [this](http://en.wikipedia.org/wiki/File:BSicon_ABZq%2Bl.svg)?
A short example of (3) foo : (x : Bool) -&gt; (if x then Int else Bool) foo x = if x then 3 else True
Hi, I am using Linux 2.6.18-348.2.1.el5, but I don't have any privilege on the box, and I have to use whatever I can install into my local directory. The biggest stumbling block now is gmp. The admin did not install gmp, and I had to compile one in my local, but the ghc build procedure refuses to use it (even after I specified --with-gmp-includes and --with-gmp-libraries, and set LDFLAGS environment variable). Thanks.
I'm not an expert on the internals of GHC. However, my understanding is that System FC is type-checked, and that many optimization passes are type-directed.
i agree that a frp thingy would probably be better in practice. the point of this is to help learn / teach haskell though so including big frp packages might distract from that. what you've shown me seems to give a ThreadId rather than a Char? forever seems never to want to stop either lol! maybe i'm missing something but this doesn't seem like the solution.
It doesn't support recursive arrows yet (apart from explicit use of ``loop``) but I'm planning to implement them for the next version. I just have to decide on the best way to represent them; labels are one option.
Just the fact that this is possible in Haskell is nuts. Putting this together is even more crazy. :) Love it.
"how can it be made generic enough to avoid forcing various evaluation schemes?" Might want to have a look at Levy's call-by-push-value (or polarised calculi, see e.g. Zeilberger's thesis), which both generalise CBV and CBN by letting you encode either (at the type level).
Wait, is it really true that two lambda expressions are extensionally equal iff they normalize to the same thing? I don't think that you can decide extensional equality of functions `Nat -&gt; Nat` by just normalizing their lambda expressions (= their source code).
After a lot of tweaking (manual editing of the ghc script and of the makefiles), I finally got the 6.10.3 binary to build 7.0.4, I will see if I can go from there to 7.4.2 and 7.8.3. Thanks.
That indented "do" block should have type IO (IO Char), and give you a currentKey of type IO Char. I indented the whole do-block to make the IORef a local variable that's not in scope past the binding. Here's the initialization with explicit grouping: currentKey &lt;- do { keyRef &lt;- newIORef ' ' ; forkIO $ forever $ getChar &gt;&gt;= writeIORef keyRef ; return (readIORef keyRef) } That should only be done once (otherwise they will fight over getChar results), then call the returned currentKey whenever it's time to sample the current input, maybe after a sleep for timing. Forever doesn't stop, which is what you want to keep the variable current as lots of keys are pressed. Putting it in a separate thread with forkIO lets the rest of the game continue running. A more complicated program would probably need some way to pause or kill that input-slurping thread, but this should be fine for now. Incidentally, "vty" has an input function that takes care of reading multiple bytes to recognize arrow keys and stuff. Maybe other packages do too.
For smoother animation, just clear the screen once at the start, and then reset the cursor position to (0,0) on frames after that. If you do that, characters will be overwritten one by one, instead of blanking the whole screen and scrolling in new lines from the bottom. I wouldn't recommend getting much farther into terminals stuff (like trying to explicitly update just the characters that change - real graphics are probably more fun), but this is an easy change.
This is super helpful, especially for those of us who haven't jumped to emacs yet. Thank you! I look forward to playing with it.
This now says: $ ghcid "--command=echo foo" GHCi exited, did you run the right command? You ran: echo foo So hopefully that should make it easier to get started with.
You need both editor integration and the full GHC API rather than just GHCi, so it is likely to be a complex thing to do (although may well be done in some of the existing IDEs).
Close, but not quite. Arrow allows you to encode a "graph" of computations, but it's not a graph in the traditional sense. Do you know about Category? It's a type class in which you can construct a fixed pipeline of operations, which can potentially have side-effects. Contrast with Monad, where the pipeline is variable, not fixed: you can look at a return value and choose which future operations will run. With Category, you can't. categoryPipeline = sideEffect1 &gt;&gt;&gt; sideEffect2 &gt;&gt;&gt; sideEffect3 You can imagine a Category pipeline as a very restricted graph, in which the only graphs which are legal look like linked-lists. The edges not only indicate how the data flows between the vertices, but also in which order the side-effects occur. There are other graph-like structures where this is not the case. sideEffect1 ---&gt; sideEffect2 ---&gt; sideEffect3 With Applicative, for example, you also construct a computation where the side-effects are fixed, but this time there is no data flowing between the nodes which produce side-effects. applicativePipeline = combineResults &lt;$&gt; sideEffect1 &lt;*&gt; sideEffect2 &lt;*&gt; sideEffect3 Instead, all the side-effects are evaluated from left-to-right, and then all the results are sent to a final combining function, which doesn't perform any side-effects. The dataflow graph: sideEffect1 --. sideEffect2 -----&gt; combineResults sideEffect3 --¬¥ is different from the side-effect execution graph: sideEffect1 ---&gt; sideEffect2 ---&gt; sideEffect3 It is possible to encode both information inside the same graph if we assign a meaning to the position of the vertices: the side-effects are executed from left to right, and the data flows along the edges. sideEffect1 ----------. sideEffect2 ---------&gt; combineResults sideEffect3 --¬¥ Arrow extends Category with a few methods which allows you to encode graphs like the one above. For example, the above graph could be represented as arrowPipeline = first sideEffect1 &gt;&gt;&gt; second (first sideEffect2) &gt;&gt;&gt; second (second sideEffect3) &gt;&gt;&gt; combineResults There are also operations to swap two lines, to fork them, etc. So yeah, there is a close relationship between arrows and graphs, but like I said, it's not a graph in the traditional sense where the only relationship which matters between two vertices is whether they have an edge between them: it's a graph where the left-to-right position of the vertices also matters.
At this point I'd be happy with just getting top level defs (with types) from ghci for autocomplete. In fact I'm going to try to get it to work in sublime text tonight (I never thought of using ghci until I read your post). 
Is there a proof that this is impossible for total languages?
Ah! I was wondering what "\ }" was supposed to mean :) Looking at the source, I see it was supposed to look like this: }==\ }=={arr $ uncurry ($)}=&gt;
thanks for the input! i'll definitely keep trying to work this out. i think i might need to race the game loop against this to kill the thread when the game finishes? either way i'm struggling to get characters out of this but i'm sure i'll work it out so thanks! : ) i managed to stop the buffering a different way: https://github.com/beckyconning/haskell-snake/commit/e77eafc82e88a2218cce4d7ba78118b4567aea4a#diff-d32135f18d55872a2c1acba0373d4010L72 but i'm sure this will give far more responsive controls as it will mean the last character inputted during the sample period will be used to control the snake rather than the first.
btw you said that that do block should have type IO (IO Char) but i'm getting newIORef ' ' &gt;&gt;= \ keyRef -&gt; forkIO $ forever $ getChar &gt;&gt;= writeIORef keyRef &gt;&gt; return (readIORef keyRef) :: IO ThreadId
Let's see... Extensionally equal means that functions return equal results for every argument. If the argument is a Bool, then we can easily check both return values for equality, but if the argument is a Nat, we cannot check every possibility. To turn this argument into a proof, all we need to do is to find a proposition *P(x)* which can be checked to be true or false for any known *x*, but for which we cannot know whether *P(x)* is true for all *x*. How about *P(x)* = "program *A* takes more than *x* steps to terminate?" For any finite *x*, we can run the program for *x* steps and return False if we encounter a Halt instruction. Since this takes a known number of steps, this ought to be doable in a total language. If we had a reliable method to tell us whether this total program is extentionally equivalent to the program "`\x -&gt; True`", then we could know whether *A* terminates. Since *A* is arbitrary, this means we could solve the halting problem. QED!
Sure, suppose you can decide equality of functions. Then for any Turing machine `T` take the function `t : Nat -&gt; Nat` defined by `t(n) = 1` if `T` terminates in `n` steps, and `t(n) = 0` otherwise. Define `t` in your language. Now check whether `t == const 0`, and solve the halting problem.
You beat me to it! :)
It seems you just have the implication backwards. Extensionality -&gt; eta, not vice versa.
&gt; I hesitate to call anything using Mu/Fix/Nu and cata/ana non-recursive. Because of the way they are defined here, the recursion lies in the construction of the values and not in the definition of Mu/Fix/Nu/cata/ana.
I'm not so sure there's an implication. I remember reading (I can't for the life of me remember where, perhaps Urzyczyn + Sorensen) that if = is the smallest congruence containing extensionality and beta reduction, and if =be is beta-eta equivalence, then M = N _iff_ M =be N. 
Doesn't that say that if your equational theory has function extensionality, then it contains eta equivalence? I might be missing something here, but this looks kinda obvious to me (`Œª x . a x` and `a` are extensionally equal, hence eta holds). 
Of course it's doable, you just write the interpreter as a function `State -&gt; State` (this has no recursion, so it's certainly fine), and then iterate it `n` times (via the eliminator of `Nat`).
I second this. X axis for time is very intuitive.
That diagram is actually sensible. Can you show the code that it corresponds to? The bit that throws me is this thing: ‚ïë ‚ïû‚ïê‚ïê‚ï°‚ïë‚ïû‚ïê‚ïê ‚ïë ‚ïö I understand arrows but I have no intuitive inspiration for what this could mean. **EDIT**: Oh, I realised the the rows correspond to tuple indexes. The input tuple's second element is used for computing the value of the fourth element of the output tuple, so the needle has to move downwards onto that row.
Very cool, but "editability" is a huge concern. It took me a while to figure out that inputs come in on the left, and outputs come out on the right. Once you get that, it is indeed clearer. One thing I really like about it is that you don't have to name any intermediate results. You just have to pipe them around to wherever you want them to go.
As of GHC 7.8, GHCi now has a `:complete` command.
Note that it's an if and only if, so therefore does that not also say that if i have eta equivalence then I have functional extensionality?
&gt; Can we get a O(N log N) time sorting algorithm based on cata/ana? http://www.cs.ox.ac.uk/people/daniel.james/sorting/sorting.pdf Section 7 =) 
it's just a tunnel.
Oh, I see. I think the confusion is that you are referring to some simple calculus, maybe STLC without any inductive types? As soon as you add `Nat`, things break down. You can go quite far with eta for Nat, but that's not decidable already.
But doesn't a turing machine require an infinitely large state?
It looks more like a bridge. But neither really helps explain what the point of it is.
ah the $ goes further right than it does in the do notation so it should be newIORef ' ' &gt;&gt;= \ keyRef -&gt; forkIO (forever (getChar &gt;&gt;= writeIORef keyRef)) &gt;&gt; return (readIORef keyRef)
That's true, but since we're only running the machine for *x* steps, we only need a maximum of *x* squares.
Alright, I'm convinced.
Sure, but it's not a problem. The tape can be expressed as something like: Tape = ‚Ñï ‚Üí Bool and you can read a cell by just applying the function, and update it by doing: update : Tape ‚Üí ‚Ñï ‚Üí Bool ‚Üí Tape update tape n b m = if n == m then b else tape n This is going to be very slow, but total. 
Can you please share what you come up with? I'm keen to use Sublime with more IDE-like power for Haskell.
Why not? All you need is the eliminator.
Sure. I'm just looking at the Sublime api docs now and trying to remember how to python :p I have had reasonable success with a modded version of Sublime Haskell (to work with nix), which gives me compile errors on save (including type holes, which are incredible) and autocomplete for the Base libraries. But it's incredibly finicky and prone to failure. I've never got its ghc-mod/hdevtools/hlint modes working and often will just stop working for an unknown reason. 
&gt; If the title is "don't be scared" then why repeat myths that make people more scared? I can at least hope to answer this part through a marketing perspective. If you first get people more anxious about something, then you provide something else to alleviate that anxiety, they'll most probably be more inclined to listen to you. It's about creating a sense of urgency. Making functional programming sound hard means that an article explaining its approachability and simplicity will seem more important. I'm not sure if that's the exact reasoning the OP went through, but it sure as hell achieves that effect.
So in other words, extensional equality might hold for STLC, but when you add polymorphism then it doesn't hold any longer?
Looks good with DejaVu Sans Mono which has a wealth of unicode characters.
Here's how it looks with X axis used for ordering: }={(+1)}=\===============================&gt; \ }===\ \ /=================&gt; \ \ / }=) \ (=={uncurry div}=/={negate}========&gt; \ \============================={(*2)}=&gt; 
I think that would actually be the opposite of what's desired, because it necessitates some conception of what the type system is like. Ideally, a VM for FP would be wholly generic and make no assumptions. It would just be an abstract system that will work for more or less any functional language.
That screenshot looks beautiful. Web however might make it hard to do guides with something like that.
Can you open issues for the unpleasant things you experienced? You will likely be re-visiting the decision as the tool is tried out on larger code bases. A whole different approach to this you might consider is having the editor send the command to do the recompile.
And now I can just imagine a debugging session with a vertical line that slowly moves across and shows the "current value" wherever it intersects a horizontal pipe.
That's a great idea! It could replace the textual version while you're not editing. Makes it easier to reason about what you're seeing and if it doesn't "compile" to an image, you've made an error. 
 -- a turing machine with n states -- a machine can read the current tape element, -- write a new value, then optionally move (or terminate) data Direction = Left | Stay | Right type Machine s (n :: Nat) = Fin n -&gt; s -&gt; (Maybe (Fin n), s, Direction) -- machine state for a Turing machine with n states data State s (n :: Nat) = State { tapeLeft :: [s] , tapeCurrent :: s , tapeRight :: [s] , tapeDefault :: s , machineIdx :: Maybe (Fin n) -- terminated if Nothing , machine :: Machine s n } advance :: State s n -&gt; State s n advance -- exercise, no recursion needed -- total recursion foldNat :: (s -&gt; s) -&gt; s -&gt; Nat -&gt; s foldNat s z Z = z foldNat s z (S n) = s (foldNat s z n) runsLongerThanN :: State s n -&gt; Nat -&gt; Bool runsLongerThanN machine steps = case (foldNat advance machine) of State { machineIdx = Nothing } -&gt; False _ -&gt; True 
I'm not well versed in Arrow, but this is kind of a joke, right?
I've been toying with this idea in Vim since earlier this year (for flow charts, etc), and by "toying," I mean "planning to toy." I drew up a bunch of ASCII art. That's something, I guess. The two representations must be isomorphic.
I am not convinced. [Predecessor and lists are not representable in simply typed lambda-calculus](http://okmij.org/ftp/Computation/lambda-calc.html#predecessor). While there is a `cons` for `List a`, `head` and `tail` probably only work on `Stream a`.
Good to hear that, and you're quite welcome!
The juxtaposition of &gt; automaticly convert normal coded functions into tubes and &gt; real-time latex rendering made me imagine the program being physically modeled with rubber hoses. If you use real-time pasta rendering, you could have literal spaghetti code.
I was trying to show that there existed total languages (I was thinking of Agda) for which extensional equality is undecidable, not that it was undecidable for all total languages. For example, in a dummy language with only one instruction, "Halt", it's clear that extensional equality is decidable. According to [this comment](http://www.reddit.com/r/haskell/comments/2hsea9/a_nonrecursive_sorting_algorithm/ckvxv6f) by /u/augustss, STLC is indeed one of those language for which extensionality is decidable.
What are you using to edit your Haskell code? Depending on the editor, there may be ways to bring up the warnings / style hints every time you save. It may not be the case for everyone, but I know that really helped me write cleaner code when I got that set up.
That's the prettiest I've seen emacs look. What typeface is that, pray tell?
What would the funktions be modeled as? Meatballs? 
 fNeedle :: (Int, Int, Int) -&gt; (Int, Int, Int, Int) fNeedle = [nd| ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïû‚ïê‚ï°(+1)‚ïû‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚îÑ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ïë ‚ïë ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚îÑ ‚ïë ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïë ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ï°uncurry div‚ïû‚ïê‚ï©‚ïê‚ï°negate‚ïû‚ïê‚ïê‚ïê‚îÑ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ïû‚ïê‚ïê‚ïê‚ïê‚ï°(*2)‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚îÑ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò |] Why does everyone keep crossing their streams [ needles? ] unnecessarily? This is much more "clearer".
I would go one step further. If the easiest to understand and use version of something is a diagram, why not just edit it as a diagram? Just have your editor present it as an [editable diagram](https://www.google.com/search?q=pure+data&amp;client=firefox-a&amp;hs=J0r&amp;rls=org.mozilla:en-US:official&amp;channel=sb&amp;biw=1279&amp;bih=1303&amp;tbm=isch&amp;source=lnms&amp;sa=X&amp;ei=WkIqVLrcMafDigK11YDIBw&amp;ved=0CAcQ_AUoAg) and go nuts. The problem that makes this entire library seem silliy is the fact that it makes it easy to read, but possibly harder to edit, so maybe this fixes that.
Then it should be taken care of outside the function. Or wrapped in an outer function that swaps the order if you don't want to burden the existing callers. Order is completely arbitrary inside the function. The only thing that choice of order was doing was complicating the logic.
What hoops did you pass through?
PragmataPro. Costs a fair bit but it's very nice to look at.
I can happily say I'm finally over this particular emacs curve. 
Just saw this post - unfortunately we hadn't yet announced Haskell, just pre-released in beta. If you check again we're now fully supporting it (http://Codewars.com) with Haskell signup from the landing page. 
We hadn't yet announced Haskell, though had pre-released it into beta and removed voting. If you check again we're now fully supporting it (http://Codewars.com) with Haskell signup from the landing page.
Just saw this got posted to Reddit - unfortunately we hadn't yet announced Haskell, just pre-released in beta. If you check again we're now fully supporting it (http://Codewars.com) with Haskell signup from the landing page and in the "next challenge" trainer.
Didn't realize this got posted to Reddit, we hadn't yet announced Haskell, just pre-released in beta. If you check again we're now fully supporting it (http://Codewars.com) with Haskell signup from the landing page.
Does this handle Shellshock? :(
Are there other libraries that use Applicative to design a UI? That's pretty neat. 
This is susceptible to shellshock if your system shell is bash.
Too bad `Parse` has a `BindP` constructor (even though it doesn't expose a monadic interface directly), so a full implementation will have to do some kind of 'wizard' interface (first page with static options, second page with new options depending on first set of choices, and so on).
Yeah, the `BindP` constructor is a hack. It will probably go away at some point.
Have you considered basing that mode around ghcid? I am happy to help make it easier to integrate, and I had to do a number of tweaks to get ghci working smoothly - things like persisting warnings from previously loaded files and flushing stdout/stderr properly on each edit. If you don't reuse ghcid, I recommend you take a look at the code anyway, to steal the bits you need. I look forward to the result!
I must be missing a fundamental part of CGI that I wasn't aware of. Where does the system shell enter into running a Haskell CGI program? Suppose I setup my Haskell CGI program [with Lighttpd](http://redmine.lighttpd.net/projects/1/wiki/docs_modcgi): client ‚Üê‚Üí lighttpd ‚Üê‚Üí helloworld Where in the process does the shell apply? Thanks.
Wow, I can't believe you have me considering spending "a lot" of money for a typeface, instead of my usual "never spent even $1 on a typeface before."
That's been my dream for awhile now. I want ASCII dataflow programming that converts into and back from code.
Modern Vim also has a 'conceal' concept, where matched patterns can be hidden or replaced with other characters. E.g. "lambda" in Python file can be made to render as Œª.
alpha-beta-eta captures the full extensional equality for untyped lambda calculus (this is the "bohm separability theorem"), but not for typed lambda calculus. That is because types prevent some observations and thus "extensionally equal" is something that depends on what type you are observing at.
I think it's important that we let the desired user workflow drive the technical solution and not the other way around. The problems we have today are largely due to the user model being wrong. What we give the users is a mutable, shared package store that we tell them to carefully maintain. Users don't want to install packages, that's not their end goal, they want to e.g. "run my test suite", "open ghci with these packages available", or "build and executable that I can ship"*. Our cabal UI nor its implementation captures this today. Instead we have users do these kind of dances: cabal install --only-dependencies --enable-tests cabal configure --enable-tests cabal build cabal test The first three lines confirm no user intent and no information we can't deduce from the last line, which is the only line that actually talks about what the user wants to do, namely run the test suite. Here's another example which is more directly relevant to this post: I've always emphasized that Cabal sandboxes are not about sandboxes, but about having hermetic builds. Hermetic builds, i.e. having a single `cabal install/build` invocation be completely independent of any past such invocations, is what the user wants. The user doesn't want to manage a sandbox. If we're making the user manage the sandbox (or some other environment) directly, we're doing it wrong. There are many different ways to *implement* hermetic builds. One would be to wipe the whole package DB and re-install all dependencies on every single build. It's hugely inefficient implementation, but it's a valid implementation of hermetic builds. Sandboxes are a more efficient version of the same strategy, as we are free to completely wipe the sandbox (or in reality, compile everything with `--force-reinstalls`) as we know we cannot affect any other sandbox. Nix-style package DBs are yet another implementation of the same idea (with less re-compilation at the cost of a more complex implementation.) Again, the point of sandboxes or Nix-style package stores are not having these two things, but to let the users express their intents and having us do the right thing. *Once in a while they might want to cache something offline because they're getting on an airplane or something. That's a minority use case we can support without breaking the user model.
I couldn't find a reference to events on those pages. Are events typically supported in these languages, or just, as the name seems to imply, the data?
&gt; ... then cabal/nix support. For the cabal + sandbox support cabal-cargs[1] might be of help. [1] https://github.com/dan-t/cabal-cargs
Given that the "users" in this case are almost always software developers, I'd be more inclined to favour solutions that give control over being magic but unpredictable. For example I often share a cabal sandbox between multiple build trees, because I want them to behave the same. I don't think just saying "cabal test" can express that intention.
Any comment about the approach I describe in [my blog post](http://jnordenberg.blogspot.se/2014/09/type-classes-implicit-parameters-and.html)? At least in Scala it can be used to guarantee instance equality (but not necessarily uniqueness).
Hopefully I should just be able to do `cabal repl` to start gchi with the appropriate arguments from the cabal file (and `nix-shell --pure --command 'cabal repl'` for nix)?
Yes, I'll cover this topic in a later post. I want to summarise for everyone the rather detailed discussions that we all had at ICFP (and that we've all been having for months or years now). I wanted to start with the big picture because it's easy to get lost in the details. The big picture includes an overview of all the problems (since people tend to focus on one at a time). The big picture also needs to mention the major solutions that people go on about and how they relate to each other. As you know, I agree with you that the current UI is wrong and leads people astray. I also agree that the UI should be driven by the common and important workflows. Indeed I think you and I agree on almost all of this stuff, and we're just struggling with how to map user intention into a UI. But I'm also of the opinion that an underlying nix-style mechanism gives us a good basis for supporting many UI choices in a sane way. So you might say I'm prejudging that the nix-style mechanism is needed, but I don't think having a nix-style mechanism underneath puts any particular constraints on the UI. So I'd like to attack this simultaneously from below and above: below by adding the nix-style mechanism (and we're making progress on that), and above by thinking through workflows and what UI we want and then how to support that UI.
How do you know whether pipes go up or down?
Perhaps not "cabal test" by default, but "cabal test --sandbox=/projects/.sandbox" would do what you want. I think the idea is to have a sensible default, not to take away the ablility to customize.
Wasn't there a proof of concept of multiple package instances installed simultaneously long ago? What happened to that?
IIRC /u/ezyang and /u/dcoutts are actively working on making GHC 7.10 ready for that
&gt; If you use real-time pasta rendering, you could have literal spaghetti code. I see you've been [touched by his Noodly Appendage](http://en.wikipedia.org/wiki/Flying_Spaghetti_Monster).
Right, and I'll talk more about those bits and pieces in subsequent posts.
Right, it'd be useful for us all to talk about what we want and what we intend in various scenarios. Working out what people mean is half of the problem. So fire away people, give us example use cases and tell us what you think you're after.
This still relies on the user playing nice with you. object o1 { implicit val intOrd = Ord[this.type, Int]("o1.Int") } object cheater { implicit val intOrd = Ord[o1.type, Int]("o1.Int") } Nothing prevents the user from grabbing the same singleton from elsewhere other than convention, so it doesn't seem to actually supply the safety it is aiming for. This also runs afoul of all the pains of the "tagged" approaches I mention elsewhere in the thread, such that changing the applicable instance now has to necessarily go through ad hoc one-off combinators. Finally, once you go to 2 parameter classes in Scala, my recollection is inference goes from bad to worse. =)
But our product _is_ functional programming. You don't get them anxious about your own product! Instead, get them anxious about things that are worth being anxious about, like the fact that most software is bugridden, most systems are insecure, and i consider it a miracle and/or feat of enormous human effort that every electronic device I touch doesn't just spontaneously disintegrate into a pile of null pointer exceptions and melted transistors.
&gt; Nothing prevents the user from grabbing the same singleton from elsewhere other than convention, so it doesn't seem to actually supply the safety it is aiming for. True. I don't see this as a huge problem though, the user might as well for example not follow the monad laws when implementing a monad instance. And the instance identification type could be generated automatically by the compiler (for some imaginative language with singleton types and implicit parameters, or possibly even in Scala using type macros). &gt; This also runs afoul of all the pains of the "tagged" approaches I mention elsewhere in the thread, such that changing the applicable instance now has to necessarily go through ad hoc one-off combinators. Not sure what you mean here. Can you give a small example where the approach fails (or becomes very cumbersome)? &gt; Finally, once you go to 2 parameter classes in Scala, my recollection is inference goes from bad to worse. =) Probably, the inference in Scala is pretty bad, but I'm more contemplating a general solution to emulating globally unique/equal instances when using implicit parameters.
IIRC [LabView](http://www.ni.com/labview/) has this. It's a visual programming language. Wires carry different types of data and functions are blocks with inputs and outputs. This whole thing reminds me of it a lot. I remember threading an unused value through otherwise separate blocks to get an ordering of effects.
&gt; Can you give a small example The examples aren't small. They happen as the problem gets bigger. =) e.g. if you want coherence for all the instances in a whole monad transformer stack. Elsewhere in this thread I talked more about where the tagged approach breaks down in terms of requiring ad hoc combinators to change instances and knowing all the properties of the things you are going from and to at the site where you change instances. Consider something like `hoist` for swapping monads under a monad transformer stack. I can write that as a combinator, one off now in Haskell. If the entire monad transformer stack had to be captured with a single tag I'd need to know every monad transformer involved in the source and target i'm mapping between in order to make a mapping and it'd be a one off map between those types that changed tags as it went, not a 2 line class everyone can share. I'll be doing a talk on this in a couple of weeks, and I'll mention yours as an example of the tagged approach and see if I can't give a nice counter-example for you there.
You need to run `cabal install QuickCheck`
It's difficult to talk about this without thinking of this in terms of our current tools. You explain what you want to do in terms of installing things, while what (it seems) you want to do is to build B against some (locally modified) A and C against some other A. Installing things is a (i.e. the current) way of doing that. It's far from the only one. If I have two packages e.g. `hashable` and `unordered-containers` (`u-c`), where the latter depend on the former, why do I have to install the former to have it used by the latter? Why can't I do e.g. this mkdir -p ~/src/hack-on-u-c cd ~/src/hack-on-u-c git clone https://github.com/tibbe/hashable.git git clone https://github.com/tibbe/unorderd-containers.git edit hashable/Data/Hashable.hs cabal build unordered-containers # uses your modified hashable and have cabal pick up the checked-out version of `hashable` (e.g. by traversing sub directories) when I build `u-c`? You can do this with sandboxes today, in a very round-about why (by manually registering `hashable` into the sandbox and the install it using `cabal install --only-dependencies`), but that's not a scalable approach as the number of packages you want to depend on and still be able to edit grows. Aside: as soon as you start sharing sandboxes and manage them manually you have lost hermetic builds, as it's no longer safe to just rebuild everything.
&gt; I'll be doing a talk on this in a couple of weeks, and I'll mention yours as an example of the tagged approach and see if I can't give a nice counter-example for you there. Fair enough, will be interesting to see what you come up with. :)
I'll bite. Use cases/features * I want to install Haskell an executable like pandoc. I don't care about the dependencies, I just want the executable. Cabal should alert me if it copies to location that is not on my path. It should help me add the cabal binary location to my path. * After installing a Haskell executable I want to fix a bug. I want a forked some source control repo I can commit and push to. I want cabal to help setting this up. * I want to install a package off of github * I want to install the latest version of a package * After installing a package I want to hack on it. * I want cabal to setup haskell project for me, including the github repo, travis setup, etc. * I don't want to ever explicitly uninstall things, to be able to get a set of packages that work. * cabal can perform a semiautomatic package gc and tell me what packages haven't been used for awhile and I can have them removed. * A crucial security fix has been added to a package, I want cabal to alert me. * Hackage is down. It doesn't matter. I can use mirrors, github, or peers easily. * For packages that I am not hacking on, I install using a binary cache. * cabal helps me setup my tests. I generates stubs, it tells me what my coverage is. * I can include executables as dependencies. I can specify if they come from hackage, brew, debian, etc. * There is always a constraint file with every hackage upload. cabal always has at least one package configuration it knows will work. * I can't install a package that failed to build with my version of the compiler. Okay that's off the top of my head. I will think about this. 
Module specifications + semantic constraints on packages/modules + semantic versioning. Of course this will be a huge undertaking but it's also The Final Undertaking, so it's worth it! Sanella and Tarlecki have some nice work on this, btw.
Whatever was originally meant, working on multiple libraries and being sure the resulting versions can be consistently used together is a reasonable goal.
Yeah working now.
Could this be used to support something like tabletop games? How hard would it be to handle something like a simplified D&amp;D? Part of the fun of D&amp;D traditionally has been the "slop" the DM introduces into the rules. Very curious to see what kinds of games this will give rise to.
&gt; I want cabal to setup haskell project for me, including the github repo, travis setup, etc. I find tethering the Haskell ecosystem to Github kind of disconcerting, especially if that means pulling in a bunch of a web and authentication libraries as dependencies of cabal. Who's to say that Github will even be around in a few years? Web services are ephemeral, Haskell is eternal.
Sure, not against using these services just that they shouldn't be part of the core package manager and delegated to a third party utility that could hypothetically be swapped in or out.
some simple games can definetely be encoded in Nomyx. And the winner of that inside game could be defined to be the winner of the whole Nomyx game!
The package manager should have features that serve the needs of the community. Saying it shouldn't be part of cabal because it is ephemeral, is the wrong the focus. Does it help further the Haskell ecosystem? That is the question, and I say the easier cabal makes it to install, build, get, and distribute sources the better. 
Reading through the comments on the Pragmata Pro site, it's been in active development lately. The author is responding to requests and making lots of changes.
Apparently arrows are ordered by starting text column so (*2) there would happen after (+1).
Awesome! I tried a problem but I keep on getting submission timed out errors when I run tests or submit an answer. When I run my solution in ghci (on my machine) it works. Seems they still have a few bugs. The vim editor they have for writing code is pretty sweet though.
If I understand what you mean (I think I do) then this covers the part of the diagram in the blog post labeled wrong constraints. It might also cover the bit covered by the solution labelled private dependencies. But you'll notice that that's not the whole set of problems covered. We need additional solutions to cover those. We do have a tendency to do this: "X is the total solution" because we've not thought about the full range of related problems. For example "sandboxes are the solution", "stackage is the solution", "backpack is the solution". That's another reason I started this series with the overview of the problems and how some of the solutions that people talk about match up.
Let's see... 1. As a user who has just read about the latest new tool (say ghcid), I want to try it out. In this case, I don't want the installation to impact any of the other use cases, except perhaps by reducing their build times if there are some shared dependencies. Currently, for this use case, I create a sandbox, I `cabal update` and `cabal install`, and I create a symlink to the binary somewhere on my `PATH`. 1. As a developer who is working on a private project, I want to write code which depends on a growing number of dependencies. I used to do this by `cabal install`ing packages globally and running my code via `runhaskell`, but nowadays I use `cabal init` &lt;enter&gt;&lt;enter&gt;&lt;enter&gt;&lt;enter&gt;&lt;enter&gt; to accept all the defaults and get a blank `.cabal` file to which I can add package names, then I create a sandbox and I `cabal install`. Sometimes I also add a section for tests, and I run `cabal test`, and sometimes I find that running the full suite is too slow, so I run `doctest -package-db=.cabal-sandbox/*-packages.conf.d src/MyFile.hs`. 1. As a contributor who has just found a bug, I want to get the source for a package, play with it, and test the modified version with a few other packages. Once I'm happy with the result, I want to send a patch to the package maintainer. I want some insurance that my patched copy will not be used in any other installation without my knowledge, because that could lead to hard-to-debug situations where a package behaves differently on different machines. Currently, for this use case, I use `git clone` or `cabal get`, I go to my test projects and I `cabal sandbox add-source` my patched version, and I `cabal install` to see how the test project deals with my modifications. 1. As a contributor who is debugging issues with cabal itself, I want a way to completely obliterate everything which might affect it and carefully reproduce the steps which lead to a bug. Currently, I write a script which removes `~/.cabal` and `~/.ghc` and reinstalls everything from scratch, just to be sure. 1. As a developer who is about to publish something to hackage, I want to figure out which version bounds I need to specify for all my dependencies. I currently use my [cabal-rangefinder](https://github.com/gelisam/cabal-rangefinder) tool, which repeatedly wipes and recreates sandboxes with different imposed versions of each dependency. 1. As a developer who is working on a published project, I want to compare the behavior of multiple versions of my project. I currently install each version I care about in a different sandbox, and I create symlinks to the binaries with slightly-different names, like `mybinary-1.0`. Since my project needs to load some of its modules at runtime, I have code which figures out which sandbox is associated with the binary in order to load the correct version of the module.
I think this sort of thing is doable in a way that doesn't bake things in. For example we can already fetch packages from source control, use `cabal get --source-repository` (or -s). There's support there for darcs, git and several others, and it's relatively extensible. Deployment is a similar area. You could start with just some hooks that call out to customisable scripts. But to be clear: these problems are the "independent" ones. They're not part of that thorny thicket of package ui / environment issues.
Pragmata Pro, not Pragmata. A different font from the same guy.
I completely agree that we should strive for a single "cabal test" instead of the sequence of four commands above. But I think we need to keep in mind that efficiency is critical. In my mind compile times are one of Haskell's biggest drawbacks for significant production development. If hermetic builds result in me having to bite a 5-minute compile time bullet over and over again when I know that a 15-second build should suffice, then that's a deal breaker. If getting the 15-second build means that I have to do that sequence of four commands in the occasional case that I am installing everything, then I will definitely prefer that to paying a 5-minute penalty every build.
Oh, I didn't realize Pragmata was its own font‚ÄîI meant Pragmata Pro. See the Agda section here: http://www.fsd.it/fonts/pragmatapro.htm#.VCruGStdXaY
&gt; So the question we would ask here is, in what context or for what purpose do you want that source version of that package? Do you just want to check that it builds in isolation (e.g. use it alone in ghci, or run its testsuite), or do you know already that you going to want to use it later with something else (e.g. it's a dependency of your current project)? Depends I guess, both sound good. &gt; Again, we would ask about the context. As tibbe keeps saying, asking to install something is not a goal in itself (except for the narrow case of checking something builds ok, for which one often doesn't even need .o files), it's a means to an ends. Our current UI forces people to explain things to cabal in terms of installing things, but installation is really just an implementation detail. Your goal is probably to use that package in some way, in some context not merely to install it. In all contexts, in otherwords, whatever I can do with packages on hackage I would also like to do with packages on github. &gt; Same question as above. But also the additional question of whether you really mean the latest one on hackage, or the latest stable, or the latest one that works with the other things you're currently using. What I mean is I don't want to ever type `cabal update`. Cabal should update its package list itself, incrementally. When I type cabal install foo, I expect the latest version in hackage to install. The other options are also good (latest stable). &gt; Sometimes to achieve a set of packages that work we need to not use certain package versions/instances. So do you mind hiding packages, or creating a new environment that doesn't contain the offending packages if that were suggested automatically by the tools? Either solution, but I would perfer new environment over hiding packages. In either case the tool should guide to a solution and point me to references on what the decision means. &gt; And to be clear, is this about saving disk space, or management of what is visible to you in your environment? disk space. &gt; What do you hope for here? Hackage builds packages against different version of GHC and won't let me install a package that has never successful built against my version. 
I think that's a very old screenshot, I'm actually using version 0.817 so I think it has the same asterisk as 0.818 
I've been doing CodeWars in Haskell for about a week now (it was in unofficial beta before they officially supported it today). Everything worked totally fine, but I'm having the same problem submitting solutions now as you... Anyway, I guess I'm just saying that it's a lot of fun and typically works well; they must just be having some day-zero related troubles. 
Ah, gotcha!
http://tim.dysinger.net/posts/2014-02-18-haskell-with-emacs.html this
I really appreciate how high quality this recording is. 
It's my main grippe point right now, managing a "stack" of various library at different level of granularity is really painful: a minor fix of a library in the bottom of the stack requires to install it, reconfigure all the library depending on it if I plan to continue modifying them. For a more concrete example, using the following packages: JuicyPixels ---&gt; Rasterific -&gt; rasterific-svg FontyFruity / \-&gt; diagrams-rasterific Fixing something in FontyFruity and then say Rasterific force too much recompilation of rasterific-svg and diagrams-rasterific. If somehow we could tell cabal to share and track locally edited packages, "I would be so happy"
So you can define `fix` and `loeb` in terms of `moeb`: moeb :: (((a -&gt; b) -&gt; b) -&gt; t -&gt; a) -&gt; t -&gt; a moeb g t = a where a = g ($a) t loeb = moeb fmap fix = moeb id I wonder if you can define `evaluate` similarly.
For some reason I thought you couldn't have a sandbox without a cabal file. Thanks.
For 2 I believe you can use this workflow: cabal sandbox init cabal install SOMETHING cabal exec YOUR_SHELL Then you can play around as usual with runhaskell, etc.
Have you tried using -v to get more information? cabal install -v tf-random
Thanks for this post, and thanks in advance(!) for the subsequent ones! Unfortunately, I'm not really able to attend the conferences, etc. where such things are discussed and thus not privvy to the "insider" discussions. It's great to have someone summarizing current thought amongst the "insiders". Regarding use cases, here's my contribution: "As a developer of multiple interdependent packages I want to be able to..." * install using a consistent set of dependencies for *ALL* the interdependent projects (perhaps excluding some, such as the "example" project, which should be installable as an executable, but otherwise shouldn't participate in dep resolution to avoid "contaminating" the set of dependencies). * compile everything, just making sure that it's basically sane. For me this usually serves as a basic sanity check that I haven't broken huge amounts of code by modifying lower-level APIs. * compile everything and run all tests (in project-interdependency-order) -- I don't want to have to fix tests in a higher-level library when a test in the most basic of my libraries fails. (I'm happy to have them *run* concurrently, but I only want reports on the lower-tier libraries.) (EDIT: I suppose these are formulated as quite implementation-centered, but I hope the point gets across.)
Please note that some exercise require fast algorithms. I had something that seemed really fast (0.2s) on my computer that was rejected. Once I used a more efficient algorithms the tests passed.
I will also echo complaints about build times. It's **the worst thing about Haskell**. Because the ecosystem is so in flux, every project I work on requires a sandbox, which requires rebuilding the same dreary packages over and over again: text, parsec, blaze-builder, and so on. If you somehow screw up your package set or you add a new dependency, suddenly things that have already been built have to be rebuilt *again*. I wouldn't like to estimate how many times I build the same package each month. The obvious answer to this is: cache those builds. Keep the binaries around. The problem is the way GHC inlines in compilation, you can't trust library binaries and just have to do a rebuild. The -j4 option has never been viable for me because using all four cores heats up my laptop to the point the kernel starts killing the offending ghc processes for making the CPU hot. So I don't even get faster builds. I can add -O0 which speeds up builds a little bit, but then I don't get proper performance when using libraries that only work properly because they're highly optimized with inlining and strictness analysis. I don't see an obvious solution to making builds faster or cachable. So any kind of solution that gears towards not needing to build so often would have a big thumbs up from me.
We're using a recent release - The Glorious Glasgow Haskell Compilation System, version 7.6.3
The unpleasant thing is that suddenly stdout stopped working, it was rather catastrophic. I raised https://github.com/haskell-fswatch/hfsnotify/issues/50 to track it. I did consider having the editor send a command, and that was my first plan, but I gave up since that requires editor integration which makes it less generally useful. I'm quite happy to provide a mode where it only responds to the editor telling it, rather than modtime checking. I personally consider any single project with &gt; 100 modules in need of splitting (out of my projects, only Shake and Hoogle come close, both pushing 90 modules - but for Shake, over half of that is tests, and for Hoogle some of that should be split out).
This got posted recently, though was only in a pre-release to the existing community. We've now fully opened Haskell, with Haskell signup from the landing page, and supported in our kata trainer, which lovingly selects the best haskell challenges for you.
Has "automatic" package versionning been considered? By that I mean having cabal detect internal and external (interface) changes to assign version numbers. That would probably solve many under/over-constraining issues as it would again, "automaticaly" specify the package dependencies. It would also mean that all packages would have consistent and uniform versions. This, I think, could provide a solid foundation for your "Upper-bound build bots" solution.
Congrats on the release! Amazing to see how many improvements can go behind so little api changes.
The question is, how do I tell cabal which version or copy of a package I want to use. Right now that's achieved by installing into the sandbox. What do you suggest as an alternative? &gt; e.g. by traversing sub directories So, in your scenario, suppose you're satisfied with your changes to hashable and u-c, and then want to compile/test some other packages A, B and C with this new u-c. What will you do?
What kind of complex input/output did you want that you couldn't get with ghci? (I'm currently going down the piping to ghci path).
I'm not quite sure what you mean by this, to be honest. I mean, the point of semantic constraints is that it lets you define what kinds of behaviors/properties are required, independent of any concept of "version". Either there is at least one collection of versions which satisfies all of the semantic constraints or there isn't, and no amount of fancy extra bits can do better. Also, the semantic versioning also could make it possible to *blend* versions so that you can ultimately have some kind of sub-package versioning. Maybe that won't solve it all -- I don't fully understand the issue, really -- but all of the non-semantic solutions are grappling with the problem in a way that redefines it, afaict, and thus they seem to lose the ability to really be all encompassing.
What about actually using Nix as a component in the haskell build system? Seems like there are a lot of good features there and they are pretty solid. Synergy rather than community fragmentation... 
I think that the ultimate solution to build time/caching is to build to an intermediate representation (IR) before any inlining. Code generation and linking would only happen when building the final exe or the shared lib for shipping/packaging and as an explicit step. This would also require an end to adding CPP macros to sources (a Good Thing, imo -- how untyped is that in a statically typed language?). For development, just run or repl the IRs. This would lead to less cabal hell and quicker dev cycles. This would also make it easier to deal with multiple instances of the same version (there would be only one instance if done right) and there could even be a global cache of binary IRs that could be shared. And even the potential for AOT or JIT compilation back-ends.
There are various reasons why we cannot reuse nix directly. However many of us accept that the nix concepts are the right way to solve several of the packaging problems. So I don't view it as "not invented here" syndrome, but as affirmation of nix as the right answer.
One nice thing about the nix approach is that it gives us caching of binaries (while still allowing sharing between independent environments), and gives the solver the opportunity to actively try to reuse existing build artifacts (though this is a different policy choice than "deterministic" builds which are supposed to be independent of what you have installed already).
At first I was a little depressed about this, about how many issues needed to be addressed simultaneously. But recently I've become rather more positive about being able to implement things in reasonable steps. In particular there's some work we're doing for the IHG, and the work ezyang is doing on backback that gets us a few steps closer. More on this in later installments in the blog series.
I like that a lot - the argument order is annoying though. You'd want to somehow do away with having to have: a ‚ïæ‚îÄ‚îÄ‚îÄ‚ïÆ ‚ïΩ b ‚ïæ‚ïº flip div ‚ïæ‚ñ∂
Whenever I hear about yet another `$DEPENDENCY_MANAGER` hell, I think about something I came across a while back: http://www.reddit.com/r/programming/comments/2eb53n/semantic_versioning_is_not_the_solution/cjy77uf /u/avodonosov was arguing for adding on a major version number to the _name_ of each package/library on each backwards-incompatible change. So you could have (e.g.) `text_1_1`, and `text_1_2`, etc. Perhaps even the module names would be `Data.Text_1_1` and `Data.Text_1_2`. Then downstream users (other packages) depend on `text_1_1` or `text_1_2`, as needed, and they don't conflict with each other simply because they have different names. At first the idea looked inelegant--like we're trying to solve a complex engineering problem with a hammer and duct tape. But I now think the argument does have merit. It's like immutability in FP--you assign a value (a module) to a name once only and don't change it. That does seem like it should resolve the various dependency hells.
Can you also elaborate on those various reasons? Also, have you seen the [hnix](https://github.com/jwiegley/hnix) project?
A coding font with Agda as an example? Oh. My. God.
Awersome presentation. I like the approach to demystify the language and make people feel less intimidated to use Haskell for enjoyment. BTW: Thanks for your insight; Now Perch has JQuery-like DOM manipulation with "forElems". In the last version in the git repository at: https://github.com/agocorona/haste-perch Example: main= do body &lt;- getBody (flip build) body $ do div ! atr "class" "modify" $ "click" div $ "not changed" div ! atr "class" "modify" $ "here" addEvent this OnClick $ \_ _ -&gt; do forElems' ".modify" $ this ! style "color:red" 
For one thing, you have not initialized glfw, I recommend you look at the C tutorial for glfw, a minimal working example for glfw-b is http://lpaste.net/111893. You also need to read out the keyboard events in the main thread, the way you are doing it right now probably is doomed to fail even with GLFW properly initialized. You might want to look at helm for inspiration on how to write an interface to a frp library from an event loop.
This script completely failed for me on Linux. My guess is that it only works on OS X. This works on Linux for me: cabal_file=$(basename $(find . -maxdepth 1 -name '*.cabal')), but then the cabal file parsing didn't work. It would be better to add this capability to [mega-sdist](https://github.com/yesodweb/cabal-src/blob/master/mega-sdist.hs) so that it works cross-platform
This is unworkable because it prevents modules from interoperating which would otherwise be able to. Let's say I have a library at version X. I add a new backwards-compatible feature and release Y. Now I change that feature in version Z, breaking backwards compatibility with Y but remaining compatible with X. In other words, modules which depend on Z can safely interoperate with those that depend on X, but not Y. If we give X, Y, and Z different names, then this valid interoperability will be prevented. This is quite common in practice. Consider a logging library like commons-logging: you need all the code in your application to use the same version of the logging library (so all the logs go to the same place), but it's unlikely that every one of your dependencies was tested against the latest version (or why bother with versioning). Yet they'll still work together as long as they use a compatible subset of the logging API.
The simple reason is that Nix is an OS and has/wants/needs full control over *all*, also non-Haskell dependencies. We cannot expect every Haskell user to change their OS (whoever is happy to do that can use Nix right now). As a package manager, we will always have less control over non-Haskell things, though. 
Can you paste something minimal here for those of us that don't feel like signing up for an account at codewars and paying the email spam tax just yet.
I'm using the unicode-fonts package. I had to go through and manually set up the primary font for each unicode code page to PragmataPro for all the pages it supports, and then every other page is mapped to a fairly complete but not as good looking unicode font.
&gt; The -j4 option has never been viable for me because using all four cores heats up my laptop to the point the kernel starts killing the offending ghc processes for making the CPU hot. I do most of my development on a big beefy desktop machine so I use -j8 all the time, but it is not sufficient. No matter how much work we do to improve the situation, we're always going to be slower than the mainstream languages because we're offloading a lot more of our drudgery to the compiler. I chose Haskell specifically because I thought this was a bet that would pay off in the long run. We should definitely put effort into reducing the magnitude of this problem, but we should realize that build times will always be a more significant consideration for us than for other languages.
This video is great. Digging into to STG is really exciting. I thought it was exotic, but it turns out it is not that scary. I loved the closure creating example. That was really unexpected.
If you have a 64bit version of Windows and are using GHC-7.8 (comes with the latest haskell platform). Please consider trying [leksah](http://leksah.org/), there is a windows msi linked in the downloads page. I am going to try to resurrect 32bit Windows support this weekend.
AFAIK sublime haskell works fine without hdevtools - so no sweat if you cannot get it
somehow i think one use case i can relate to, did not get addressed by tibbe or dcoutts, so i'll make up some command line interface while talking about it. i want to _use_ pandoc, so the following will install it in my `$PATH` but not clutter my haskell environment (ghci won't find it). &gt; cabal install pandoc I find a bug and assume it's in pandoc, so i want to try to replicate it with pandoc HEAD. &gt; cabal install pandoc --from-source-repository The bug is still there, so I'd like to see the source and hack on it. &gt; cabal hack-on pandoc That will print a directory with a checked out pandoc repository with all it's dependencies in place. i use `cabal build`, `cabal repl` and `cabal test` as usual. I find out, that the bug is in a dependency of pandoc, e.g. tagsoup. &gt; cabal hack-on tagsoup --in-context . will now create a directory with a checked out tagsoup. the pandoc directory will be somewhat linked to the tagsoup checkout, so it uses it (`cabal sandbox add-source $dir` alike.) I find the bug, do `cabal install` in the pandoc directory and it will replace my pandoc in `$PATH` to the fixed version, send a pull-request (or whatever is appropriate for it) and live happily ever after.
Don't use hdevtools on windows. Use https://github.com/mvoidex/hsdev + https://github.com/SublimeHaskell/SublimeHaskell/tree/hsdev or ghc-mod + IDEA + http://plugins.jetbrains.com/plugin/7453
Just one thing: If I don't change anything on my end (i.e. I don't update my GHC or HP for whatever reason), I want a `cabal install foobar` in an empty environment to keep working 3 years from now if it works now, possibly picking up a new consistent set of with a few newer package versions (obviously without the `cabal freeze` sledgehammer). Package maintainers honoring the PVP contract seems to be one way to accomplish this, but are there other ways as well?
The same person (tel) also wrote a minimal "write your own lens" exercise that is interesting.
If you cloned the hdevtools you could try to manually loosen the cabal constraints. When coding in haskell you often need these kind of tweaks in my experience, they not always work though. SublimeText+SublimeHaskell has worked well for me though. The Eclipse haskell plugin work pretty well too but I did not like Eclipse. 
we started the game, please join, don't be shy :) Some interresting rules have already been proposed by players: - we now have a list of friends. - we have bank accounts in a currency called the "Bling". - we'll soon have planets, and maybe spaceships. A lot of things can happen now (what friends allow to do? How to win money? how to trade with planets? etc.).
&gt; &gt; So the question we would ask here is, in what context or for what purpose do you want that source version of that package? Do you just want to check that it builds in isolation (e.g. use it alone in ghci, or run its testsuite), or do you know already that you going to want to use it later with something else (e.g. it's a dependency of your current project)? &gt; &gt; Depends I guess, both sound good. &gt; &gt; &gt; Again, we would ask about the context. As tibbe keeps saying, asking to install something is not a goal in itself (except for the narrow case of checking something builds ok, for which one often doesn't even need .o files), it's a means to an ends. Our current UI forces people to explain things to cabal in terms of installing things, but installation is really just an implementation detail. Your goal is probably to use that package in some way, in some context not merely to install it. &gt; &gt; In all contexts, in otherwords, whatever I can do with packages on hackage I would also like to do with packages on github. Ignore for a moment the point about hackage vs github. The point that tibbe and I are getting at is that the context matters. For a package tool to "do the right thing" it needs to know or be told the context. For example it needs to know what other packages you want to use this package with. Or another example: if for one of your dependencies you want to use a locally modified source version of something rather than a version from hackage. So people say "I want to install this", but left out of that statement is all the information about what they want to use it with (which is important so we can make it work with everything else they want to use).
One of the things I want to do is to **install**. What that means is: I want to build a library, and then I want the result that build to be cached and re-used from now on in this project until something changes that requires it to be re-built. I want my build tool to manage that automatically and figure out which libraries need to be re-built when. When I "install" without mentioning a specific library, I mean that I want *all* libraries in my project, whether explicitly or as implied dependencies, to be built and cached. The basics is to do that per project. But it would be nice if my build system would realize when a build of a library cached for one project also happens to be exactly what is needed for another project, and re-use it for that other project also. Yes, I am talking in terms of our existing tool. Because those concepts were not dreamed up at random. They are good concepts, and they are what I want. True, I also want hermetic builds, very much so. But that is a mostly orthogonal concept. It's possible, but difficult, to get both. Much more difficult than what was originally imagined in the early days of cabal, and that's what led to an all-too-common experience of "cabal hell". But there's no need to give up and throw our cached dependency builds out the window and re-invent them from scratch.
When did this happen? This is great!
Just to be clear: this job position is *not* primarily about writing Haskell code, it is primarily systems engineering. However, given that almost all the code running on these systems will be Haskell, and there will certainly be many opportunities to use Haskell while working in this position, I thought people here might find it interesting.
Here's this script transliterated to Haskell: http://lpaste.net/6609740010373513216 Just as an experiment to see how well real use-case scripts translate into shell-conduit.
This is good, you've been quite clear about how you want these cases to affect or not affect other packages. &gt; As a user who has just read about the latest new tool (say ghcid), I want to try it out. In this case, I don't want the installation to impact any of the other use cases, except perhaps by reducing their build times if there are some shared dependencies. Currently, for this use case, I create a sandbox, I cabal update and cabal install, and I create a symlink to the binary somewhere on my PATH. I was about to say that it's clear here that you want a separate environment, but if you're installing a tool rather than playing with a new library then perhaps you don't actively want a separate environment you just want to install the tool without it affecting the other libraries you've got. So that might involve the package tool making a temporary environment to be able to build the tool. On the other hand if you were wanting to try out the latest cool library for flibbing wibbles, then you would want a new environment with that library. &gt; As a developer who is working on a private project, I want to write code which depends on a growing number of dependencies. I used to do this by cabal installing packages globally and running my code via runhaskell, but nowadays I use cabal init &lt;enter&gt;&lt;enter&gt;&lt;enter&gt;&lt;enter&gt;&lt;enter&gt; to accept all the defaults and get a blank .cabal file to which I can add package names, then I create a sandbox and I cabal install. Sometimes I also add a section for tests, and I run cabal test, and sometimes I find that running the full suite is too slow, so I run doctest -package-db=.cabal-sandbox/*-packages.conf.d src/MyFile.hs. So here your environment or context is this project, and we want that environment to contain all the things needed for that project. &gt; As a contributor who has just found a bug, I want to get the source for a package, play with it, and test the modified version with a few other packages. Once I'm happy with the result, I want to send a patch to the package maintainer. I want some insurance that my patched copy will not be used in any other installation without my knowledge, because that could lead to hard-to-debug situations where a package behaves differently on different machines. So here's another case when you want an isolated environment, so that while you use a modified instance of this library you *don't* use it anywhere else other than this project. &gt; As a contributor who is debugging issues with cabal itself, I want a way to completely obliterate everything which might affect it and carefully reproduce the steps which lead to a bug. Currently, I write a script which removes ~/.cabal and ~/.ghc and reinstalls everything from scratch, just to be sure. Another isolated environment. &gt; As a developer who is about to publish something to hackage, I want to figure out which version bounds I need to specify for all my dependencies. I currently use my cabal-rangefinder tool, which repeatedly wipes and recreates sandboxes with different imposed versions of each dependency. Ok, that's a different problem. That's in the "wrong constraints" part of the diagram. &gt; As a developer who is working on a published project, I want to compare the behavior of multiple versions of my project. I currently install each version I care about in a different sandbox, and I create symlinks to the binaries with slightly-different names, like mybinary-1.0. Since my project needs to load some of its modules at runtime, I have code which figures out which sandbox is associated with the binary in order to load the correct version of the module. Mmm. Multiple environments, but here there's really a lot of them.
You're right, you didn't used to be able to. That has been added for exactly your use case.
In addition to what kosmikus says, * we also have to deal with packages you're currently hacking on, not just complete published package tarballs * we also need (and currently have) a constraint solver to help you work out which combination of packages to use. With nix at the moment you have to pick exactly which combination of packages to use, without a great deal of help But the biggest reason is what kosmikus says, that we have to be a secondary package manager, and work across all platforms, not just those where we can install our own libc (as nix does, even when you use it on an existing system).
This recent video might be of relevance for you: http://www.reddit.com/r/haskell/comments/2hy6cp/introduction_to_low_level_haskell_optimization/ckxaqo2
I also took a stab at it a couple of weeks back, trying to implement it in JavaScript for my own understanding. Maybe someone finds this helpful after going through the linked article: http://passy.svbtle.com/dont-fear-the-reader
(I think there's a typo in the first sentence: "leading providing" should probably be " leading provider".)
&gt; On the other hand if you were wanting to try out the latest cool library for flibbing wibbles, then you would want a new environment with that library. Oh, right, that's another use case I forgot to mention, although it's kind of included in use case #2. I never download a library just to play with it in ghci, instead I always use it inside a private project. I have a dummy project for this purpose, in which I often clear the code, sandbox, and dependency list.
Looks up from here (UK).
Thanks for catching that. The company slogan around these things is essentially "too bad our grammar checker wasn't written in Haskell."
"Too bad our grammar isn't LALR(1)."
It was down for a few minutes for me as well. It seemed to recover pretty quickly this time.
Ah, that's good to know! Thank you! Would this let haskell-font-lock-symbols come from pragmata pro too?
To start you off, [learn you a haskell](http://learnyouahaskell.com/starting-out#im-a-list-comprehension) has some list comprehension content that may help as well as some [wiki](http://www.haskell.org/haskellwiki/List_comprehension) content. Consider that you have your starting list `[1..15]` and your conditions `&gt;= 5`,`&lt;=10` as inputs to `inRange`. As for a full explanation, you may have more luck if you present code that you've already tried to work through. :)
Thanks biscarch, I'll read through those pages. Here's the code I've done so far: inRange :: Int -&gt; Int -&gt; [Int] -&gt; [Int] inRange lo hi xs = [ x | x &lt;- xs, x &gt;= lo &amp;&amp; x &lt;= hi ] but I think it's missing something
Looks good to me!
Missing something like what? That works correctly, doesn't it?
The list comprehension is correct. Do you have a REPL to play around with it? You still have to write a recursive version and a QuickCheck test. 
Yeah, I'm aware. When that happens, though, it tells you that your computation timed out, and it reports how long it ran before termination. The case yesterday was different; it literally said the request to the server timed out.
Well, I feel like a dill. One of the functions that I haven't done yet was left blank instead of as 'undefined' so it screwed it up when I ran the program. My bad. How would you go about doing the same thing but using recursion instead of list comprehension? 
A common reason for tf-random to fail is that the LANG environment variable was not set correctly. LANG="en_US.UTF-8" LC_ALL="en_US.UTF-8" cabal install tf-random might help.
IIRC you can relax the constraints for hackage installs too: --allow-newer[=PKGS] Ignore upper bounds in dependencies on some
That won't work. See that the recursive call has less arguments than what inRangeRec actually needs (1 vs 3). Also, are you sure 'x = x &gt;= lo &amp;&amp; x &lt;= hi' is what you want?
Nix is a package manager, and Nixos is an OS built with that package manager. One can use Nix on ubuntu, for instance, without needing to install the whole OS. If you use the nix package repository for nixos, then yes that does have the capability to install all the things. But, having a separate repository for haskell development should be possible, we're not limited to the nixos repos. 
I have to completely disagree here. We shouldn't be catering to beginners at the expense of the rest of us, especially when learning the traversable typeclass takes, what, 5-10 minutes? All it really indicates is that we need to generate better error messages and better documentation.
In nix you can work around your first point, by making a package in the repo that points at a source code directory. Re your second point, yep currently the nix repositories are basically a curated collection of libraries. Using git you can change to a different version of the collection, but you can't arbitrarily switch versions of libraries independently. Right now its possible to have multiple versions of libraries at once, but its a little awkward. It seems like a solvable problem though, perhaps by dynamically generating a nix-expression with the desired packages. 
What do you think about the multiple Preludes idea that somebody around here proposed?
I fully agree. There rather should be *no* way to fold in the prelude but Traversable/Foldable. Leave all that beginner stuff in a custom prelude, or even for helium. You probably do need to reduce power to have beginner-friendly error messages, but I also fail to see what's wrong with having the actual language and a beginner mode be separate things. They don't drive the tour de france with training wheels, either...
I think it's pretty good - I'd love to have a proper mathematical/scientific prelude. Ok sure, downvote this.
I wouldn't count myself as a beginner, but I prefer the simplicity of having concrete types. It lets me think about my problem, rather than about type classes. My post argues that we need to do this both for beginners, and for practitioners. 
It seems like a good idea. Several people are experimenting with new ones. But this one seems to have been declared the winner...
A no-partial-functions prelude would probably be popular in with some (I'm not a fan, but it's certainly a valid point in the design space).
&gt; Everyone has an easier time reading monomorphic code because it gives the reader more information. Not... really? Monomorphic code introduces more opportunities for bugs and misunderstanding due to more types lining up and more opportunities for confusion.
Completely agree with all this sentiment. Lots to lose, not that much to gain. edit: Additionally I'm very happy that Neil makes the point that simplicity isn't only a win for beginners. I don't consider myself a beginner, but needless polymorphism where I don't really need it (and don't expect it) can slow me down when maintaining a code base.
I'd certainly be interested in that, too!
I think it might take 5-10 minutes for someone with a familiarity with Haskell but for beginners it will take a lot longer. They will need to know a lot of extra things - typeclasses, folds, traversables. Ok that's a fairly weak argument in the case of mapM because you need to know about monads but if other functions were changed it would be true. &gt;We shouldn't be catering to beginners at the expense of the rest of us The rest of us can cope better than beginners. Learning Haskell is already hard enough, and the error messages already difficult enough to read when you start without adding this. I think he also addressed your point about the rest of us: &gt;Practitioners, those who are paid to code for a living, will have greater problems with maintenance. This isn't an unsubstantiated guess... I have taken over a project which made extensive use of the generalised traverse and sequence functions. Yes, the code was concise, but it was read-only, and even then, required me to "trust" that the compiler and libraries snapped together properly. 
I find thinking about the generic functions in the typeclasses helps me solve my problem as opposed to any particular representation.
The annoying part of Traversable and Foldable not being in the Prelude isn't (from my PoV) having to import them, but having to either import them qualified, or import Prelude and Control.Monad with stuff hidden. If `mapM` and `sequence` in Data.Traversable were renamed to, say, `fmapM` and `sequenceM`, then this wouldn't be an issue. (Similarly for `foldr` and friends in Foldable.) The downside of this is that it would be a breaking change.
It's not "at the expense of" if it's easy enough to drop in a new Prelude if you know what you are doing. Officially declaring Prelude to be for beginners strikes me as a good solution, actually. There's perhaps some things still to be done to it like removing partiality and maybe some other little cleanup work, but making it work for beginners is an actionable item that can be concretely tested by those working with beginners, and as you move past beginner, drop in a more sophisticated Prelude, because that's not that hard. In hindsight I'd say a lot of the Prelude debates fundamentally stem from trying to make Prelude meet every use case at once, which is doomed to lead to a solution that works for none of them.
I fail to see how having mapM refer to the traversable as opposed to the list makes a squat of difference in readability, unless you're specifically referencing the type when calling it.
basic-prelude does that to some extent, though it also includes other things like using `Text` and `system-filepath`.
The only thing I can think of is that someone might be confused when it comes to `Data.Map`, thinking that it should be applied to the key/value pair. Obviously the type signatures wouldn't allow it, but I think it could be considered confusing. But for most data types, I completely agree: why should it matter if I'm doing `mapM` on a Sequence, a List, a Maybe, or a Vector?
My other concern is that while people have dabbled with this style of programming, no one has really tried with this new Prelude. As a result, design decisions are still being made. We have far more data about how using classy-prelude works in practice, but still nowhere near enough to change base.
My biggest problem with having `Prelude` be list specific is that it encourages bad code, for *exactly* the reason you just mentioned. It's just annoying enough to have to add: import Data.Vector (Vector) import qualified Data.Vector as V to the top of a modules and then replace `map` with `V.map`, that in many cases people won't do it, and instead use a list when it's not the best datatype. Even worse is people using `String` instead of `Text` for the same reason.
It's called exponentiation for a reason!
I agree. Though initially, your post gives the impression that you are actually against your point (at least to me). I had to read it twice to see what you were saying. 
I completely agree with Neil. It is a huge mistake to make these generalizations in the Prelude. It's not just for beginners - types that are too polymorphic make software engineering more expensive more often than less expensive. It's really not hard to use an alternative Prelude. There is absolutely no excuse to adopt a Prelude that hasn't already been used in the wild and gained wide acceptance. Let us try it first. Leave the existing Prelude as default for 7.10. Publish the new one as a library on Hackage, and tell people that it is scheduled to become the new default in 7.12. Recommend that people begin using the new one. If I am wrong and people love it - great! If people hate it - we can stay with old one. Or, better yet, use our real-life experience to tweak it until people do love it, and then use the result of that as the new default in 7.12.
**I already know what mapM does.** Generalizing the type of mapM does not give me any more information! And often the container type that mapM is being used at is known *by the compiler* to be [] already for some non-local reason (e.g., it is extracted from a record field of type [a]). In that case all using a more general function accomplishes is making the reader work harder to deduce what the compiler and the author of the code already know. (Also, there is a type class Traversable involved here. Your analogy should be comparing `Int -&gt; Int` to `Num a =&gt; a -&gt; a`. Now suddenly there are a lot more than one possible function of that type (in fact, an infinite number) because I have access to the methods of the Num class too.)
I certainly find it harder, so either I'm unique, or (I suspect) there are actually plenty of people who find this extra generalisation harder to understand. I note you've exclusively given examples of container types, but Foldable/Traversable apply to far more things than just containers, specifically records with plenty of fields, some of which are the 'a' type. If is was the "Collections" type class, or the "isomorphic-to-a-list" typeclass then I might be less concerned.
I agree, although I suspect replacing the Prelude wholesale deserves an 8.0!
&gt;because I have access to the methods of the Num class too and only those methods
Note that my post covers that. I say: &gt; Fully polymorphic functions can be simpler than concrete functions, since they declare what you don't need to worry about. But we aren't proposing to generalise `id` so it's polymorphic rather restricted to int.
5-10 minutes? Even after understanding those type classes completely, I once had to spend literally weeks fixing code that someone rendered nearly incomprehensible by using an overly polymorphic alternative Prelude. This nonsense should be stopped.
So, I'm using a windows laptop too and don't want a linux dual boot for driver problems (multitouch and ntrig pen). In the past I tried hybrid systems like mingw or cywin. They worked (sort of, with a lot of configuration), but I constantly had problems, and the overall experience was not very good. Then I shifted to vm, which in my opinion are a great solution. They may not be the hassle you expect: * if you use VMware instead of virtualBox (I don't), installing a linux distro is basically a few clicks operation (and you learn about VMs ;) * when you have a linux box, installing a haskell IDE is just a few commands operation, especially if you're using an editor like vim/emacs. * there are no problems with driver or connectivity because the windows host takes care of them. * you could configure them to use very little cpu (my windows antivirus is more aggressive then my vm) * your code environment is not tied to a particular installation of windows. Just backup the vm. That is the best advice I can offer. However, if you still feel that's not worth the trouble, you can try the haskell platform for windows, and maybe leksah as an editor, as other people already said. If you have problem setting up those tools, feel free to ask :)
I translated most of this script to Haskell and incorporated it into my "neil" tool, available from https://github.com/ndmitchell/neil. If you run "neil docs" after changing https://github.com/ndmitchell/neil/blob/master/src/Cabal.hs#L85 to your username that might work. (The neil tool is my personal collection of helper scripts, others are welcome to use it, and I'd accept pull requests to generalise it, but only if it didn't get any harder for me.) The code is at https://github.com/ndmitchell/neil/blob/master/src/Cabal.hs#L73 if anyone wants to copy from it. Note that having to do this for each uploader individually is nuts. And the docs don't come out nearly as nice as the central thing which can cross-link everything properly. I really hope proper docs come back soon.
My argument is to protect me. Both protecting me when I'm teaching beginners, and protecting me when I'm coding. The fmap/liftM thing is definitely a wart. I don't teach beginners Traversable, and in fact, I don't teach experts Traversable - it's something some people pick up, and some don't, but by the time they wander near it they have enough experience not to get confused by clashing names (its annoying, but not confusing).
Something I always wondered about but never bothered to read up on / try for myself: In practice, is there any difference in runtime behavior between the list versions and the Traversable/Foldable ones? Like, inlining, rewrite rules, fusion, strictness analysis, unboxing etc.?
I use String instead of Text because it's simpler. I use List instead of Vector because it's simpler. These are the wrong defaults for performance critical web programs, but not for learning, and not for 90% of the code most people write.
I'd be curious to know where String is incorrect, do you have any links? Is it something that can be fixed?
Wait, how can String be simpler than Text, but Vector be simpler than List? This is entirely contradictory.
The simplest example is case conversion. The [text documentation itself](http://hackage.haskell.org/package/text-1.1.1.3/docs/Data-Text.html#g:8) gives a pretty good example, from the docs for toLower: &gt; The result string may be longer than the input string. For instance, "ƒ∞" (Latin capital letter I with dot above, U+0130) maps to the sequence "i" (Latin small letter i, U+0069) followed by " Ãá" (combining dot above, U+0307). 
That's true. I guess in my experience, using `mapM` on sequence-like things is the common case, and it's usually pretty obvious from context when it's something that doesn't act like a sequence at all.
I thought the same thing. I mean, introducing breaking changes in a minor version release? What is this, Idris?
Judging by the rest of his comment I suspect Neil meant that List is simpler than Vector.
But you could equally well write a function `toLower :: String -&gt; String`? `Text` is not an abstract data type, it certainly allows writing an incorrect case conversion function. It's more a question of what kind of style the API suggests: if you see a function `toLower :: Char -&gt; Char`, then you'll try to use `map`, whereas a function `toLower :: String -&gt; String` suggests direct use.
I almost always define "lower :: String -&gt; String; lower = map toLower", so I think fixing the toLower API would be a good thing.
tomejaguar has it right, List is simpler than Vector, and my brain is a pile of mush.
&gt; I have to completely disagree here. We shouldn't be catering to beginners at the expense of the rest of us, especially when learning the traversable typeclass takes, what, 5-10 minutes? The same could be said for monads. People could easily learn them in 5-10 minutes if they have a good grasp of higher-order functions.
&gt; Fully concrete types are usually simple &gt; [...] &gt; Higher kinded type classes are significantly more complex Hmm, would it help if the names of the type parameters were chosen to hint at an example concrete implementation? Something like this: mapM :: (Traversable list, Monad io) =&gt; (a -&gt; io b) -&gt; list a -&gt; io (list b)
Well, the problem of a beginners Prelude is that it won't help beginners if it's completely unused in expert code. ("If you want to write toy programs, use this Prelude, real Haskell programmers use this instead.") However, I do agree that importing many modules separately (`Data.List`, `Data.Char`, etc.) does get wearisome quickly.
A thought which creeps up every time I see a discussion about the Prelude and what it should/shouldn't contain: Why not get rid of the implicit import and make people import it explicitly if they want the basic functionality it provides? (Because ensuing breakage, I know :/ ) Simply make everyone choose their Prelude of choice instead of making everyone implicitly use a set Prelude. Providing multiple Preludes for different use cases/levels of Haskell-familiarity to me looks like the best way to go. Ok, I just remembered, that a lot of the types are in the Prelude, which makes this more problematic than it seemed at first thought. Still, it feels like the idea could be going towards the right direction.
But metal is just an object in the category of materials... Seriously however, the comment by Wesner Moise nails this: the article has a valid point if your *sole* concern is speed, but at least in MY opinion the other vaunted advantages of Haskell code such as readability and testability tend to trump this. Of course C is going to be better suited to the creation of ninja-code, map-fusion properties of GHC aside!
The beginners prelude would have to be the default, because they wouldn't know that there was a beginners prelude they should be using.
Yup, the beginner prelude idea removes all complaints about the difficulty of teaching, AFAICS.
Duplicate thread.
&gt; For the Mu Haskell compiler ... the way it is structured means type classes are genuinely free Really? How can you achieve that in the presence of polymorphic recursion, for example?
This is a very Scala-specific article, and strikes me as quite clueless in its generalisations.
Mu doesn't have polymorphic recursion, at least the last time Lennart gave a talk about it.
We should add this feature to `cabal-install`.
Another problem of custom Preludes is (as [GHC #9590](https://ghc.haskell.org/trac/ghc/ticket/9590) sadly highlights), that the compiler needs to know where to find certain definitions for desugaring things like `do`-notation, list-comprehensions, literals, and similar. While there is `RebindableSyntax`, it's not convenient enough to use currently.
This used to be wrong in `Text` as well. It's not an inherent problem with `String` - it's a problem with the way the fix for this problem was done. It should have been done by providing correct functions in `Data.Char`, and then integrating those into both `String` and `Text`. Or, if it's absolutely impossible to do that in a way that can be efficient for both, then the simple reference implementation should first be done in `Data.Char`, and then a `Text`-specific low-level optimized version can be written.
&gt; I can remember three of the 4 of the 7 beginners.... 3 of the 4 of the 7?
The gap between Haskell beginner and Haskell expert is fairly wide. Haskell is hardly the only language with that characteristic, but the type signatures in Haskell specify a very precise skill level required to read them properly, among their other precise characteristics. I'm not sure one module can straddle that divide to everybody's satisfaction. I'd also suspect we'd see what we already see, which is that library modules would use Prelude (and probably import just as they do today to get what they need), and final applications can do as they please. Technically that's already the solution that can exist today, so what I'm proposing is primarily non-technical.
 4, I wasn't counting a new person I just spun up.
Well, I meet almost 2 of those requirements. 
I have to say I agree with Neil. I haven't been keeping an eye on the library proposals recently, and I missed that we were generalising everything. I have a very bad feeling about this. Name clashes are a minor annoyance and are easily worked around, but making documentation opaque is a huge barrier especially to new users. They won't be able to understand idiomatic code even referring to the documentation for the functions it uses. They have to go and read a book first, or a tutorial, or otherwise engage on a learning expedition. With the Prelude we had a smooth path from concrete list functions to the more general Traversable/Foldable that you have to ask for explicitly. Losing this path and dumping beginners in at the deep end is quite worrying. I fear that there are a large number of potential users who will simply bounce off Haskell because of this change. 
foldl vs foldl' is bad for an additional reason: beginners conclude strictness is best. I think it leads to a general conclusion that Haskell is filled with poor defaults, one of which is laziness (I hear this sentiment expressed often). 
Ok, I should have been more precise about the Nix/Nixos distinction. But calling Nix a package manager, while true, is still misleading. Nix installs all dependencies of all its packages on its own, not reusing any of the existing system's packages. It would be strange if this would become the only way to use Haskell. 
A lot of it is that the Haskell tradition of documentation is good at conveying information but not understanding. If a beginner didn't know what ``mapM`` was they'd go the manual and be confronted by a type signature and a list of invariants and kind of left to fend for their own. mapM :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] mapM f is equivalent to sequence . map f I'm generally in favor of more polymorphic types being the one's provided by the Prelude but I think if that were the case then each of the polymorphic function's documentation should indicate what the function does monomorphically for the canonical types ( [], (-&gt;), (a,b), Maybe, Either, ... ).
The time one spends as a Haskell beginner is rather short compared to the time when one has grown beyond the state of needing a monomorphic beginner-Prelude. By optimizing the Prelude for the ephemeral beginner-phase, all advanced users have to endure sub-optimal defaults, causing a much greater global pain. Instead, let's just define a beginner-friendly `Prelude` that can be imported as e.g. `import Prelude.TrainingWheels` or something like that. That's one boilerplate line for beginners, and everyone else who is beyond that can enjoy a state-of-the-art Prelude.
What would it take to allow another module to export a type-specialized version of an imported symbol? module Data.Traversable where mapM :: (Traversable, Applicative) =&gt; ... mapM = ... module Prelude where import Data.Traversable (mapM) mapM :: Applicative =&gt; ... -- no impl. Then beginners could import the easy-to-understand `mapM` but should they ever import Traversable they get an automatically generalized version which auto-"shadows" the Pre
I would like to see an animated gif of : m = [] followed by showing '[]' replace all the 'm' in the function signature, and there should be examples of using all the functions &gt; mapM (\x -&gt; if x == 0 then Nothing else Just x) [1, 2, 3] Just [1, 2, 3] &gt; mapM (\x -&gt; if x == 0 then Nothing else Just x) [0, 1, 2, 3] Nothing
One place where that is *far* from true is just having to know that `Set a -&gt; [a]` happens to be "in that `Foldable` thing".
&gt;I have been programming professionally for years. I probably use map more than any other function. Why do you think it's not useful? Because we have `fmap` which is the same thing but nicer because it is more general.
Sorry, but we should be catering to beginners even if it comes at the expense of the rest of us, unless there is a way to get the best of both worlds. (Which I think there is, but that's not what this particular post is about.) The reason for this is that beginners are the group which keeps a language alive. A language which scares away beginners is a language which is doomed to die in the long run. Learning Haskell currently takes quite a lot of effort and I doubt that increasing that is something we can afford. Also for your own sake, please avoid blanket statements like "it just takes x minutes to learn y". The only thing those accomplish is preaching to the choir, anyone else will most likely start ignoring your other points or take out heir own blanket statements and start to waste your time with pointless arguing.
I sometimes wonder if the RebindableSyntax mechanism could decouple things more. For instance, desugaring `do` notation into binds that then need to find something syntactically equivalent to `&gt;&gt;=` in scope. So you could put a constraint on the type of the expression that requires an instance of `MyMonadish` that declares a method named `&gt;&gt;=`. I've wanted something like this for experimenting with indexed monads, for example.
I think this is a documentation problem. IMO specialized instance methods should show up in Haddock, in the files where the instances are declared.
I don't think the path from the current Prelude to Foldable and Traversable is very smooth. That's the crux of the issue here, so when it came up for debate, enough people wanted the default to change while the option of using a less polymorphic prelude remains.
Your post accurately describes the current state of affairs, too, though. Beginners write one thing, and "experts" write a dozen lines at the top of each module to invoke LANGUAGE pragmas, hide things from Prelude, and import the various polymorphic definitions from their homes. 
The script worked fine for me in Fedora 20 with a warning from `find(1)` that it was confusing (but not wrong) to put the `-maxdepth` argument after the `-iname` argument. What cabal file are you trying this on where the parsing failed?
I think changing types of things in the Prelude deserves a Haskell2014! What happened to GHC being standard-conformant by default and requiring a -fglasgow-extensions (or other flags) to get different behavior? I know we'll have the Haskell2010 (and Haskell98) packages on hackage, but that does help with all the tutorials that say, run `ghci` and then type `:t map` followed by `[ENTER]`.
What's wrong with the cross linking when you upload docs manually? Note that your transliteration is slightly wrong, you shouldn't be expanding $pkg in the haddock arguments.
As a data point, everything I have written compiles with the new Prelude with a sum total of 2 changes, both changes are backwards compatible with the old Prelude.
I would find that confusing when, say, `list = Map Text` and `io = []`. Type variables appear in error messages as well as in documentation.
Fairly uninformed opinion here, but it seems to me that a Sufficiently Smart functional code compiler would be able to transform all tail recursive pure functions into functions that mutate data in memory and branch that data only on a non tail recursive function call. So for example, a series of updates to a Map could mutate the Map in place if the compiler could figure out that there were no outstanding references to it that might be used. Edit: the key point being is that safety is not sacrificed.
Don't expand $pkg in the single-quoted arguments to haddock. Haddock knows what $pkg means.
A beginner friendly `Prelude` is more or less what the role of the `haskell2010` package is transitioning to.
The space of what `mapM` can do is _very_ highly constrained by the laws. It can only `traverse` each 'a' in the target, it visits each and every one exactly once. The shape of what it computes can't depend on previous actions. Basically it has to take all the a's out of the structure, line them up, and do something to them all in sequence, and put them all back precisely where it got them. The only convention involved in `mapM` not constrained by the laws is the particular order it chooses to visit the elements in, and that is constrained by convention that we walk left to right through the elements of the data type.
&gt; Since when should Haskell be targeting newbies? Since the community wants to avoid shrinking to the point of non-existence. If you don't get new people, the language dies; it's that simple.
Performance remained effectively constant across the changeover. This was the biggest concern going in.
`fmap` is nice when you need something general. It's much less nice when you want to map over a list.
But you have always been a habitual user (and purveyor) of vast generalizations.
If I have to think about special cases `fmap` is not general! Also, `fmap` is plenty nice when mapping over lists. It is totally equivalent to `map`.
See [my other comment](http://www.reddit.com/r/haskell/comments/2hzqii/neil_mitchells_haskell_blog_why/ckxn4am). You can make that argument for case folding, but many other examples around normalization are more complicated.
There is also Haskell Meetup Regensburg on [2.10.2014 20:00](http://www.meetup.com/Regensburg-Haskell-Meetup/events/207541082/).
This astounds me. With only a sparse hobbyist's background of (in this order) Applesoft BASIC, Pascal, 6502 assembly, C and Java, I encountered Smalltalk and laziness *instantly* struck me as the best idea ever.
Oh, well if you mean the kind of polymorphic functions that are so simple that they can be derived automatically by Djinn, then yes, of course. But we are talking about generalizing over very non-trivial type classes such as `Foldable`.
Space leaks are atrocious which is what causes that conclusion.
Aren't you then just passing the work onto beginners though? Instead of intermediates/experts having to add imports/qualifications now the beginners have to do it? I'm not sure that's the best thing to do. Firstly they'll ask what that is all about and secondly what happens if they forget to add it one day, or they're not told about it?
Problems with exceptions are a bigger issue AFIACT
How many people use the haskell2010 package? I used the 98 one in the distant past, but regular breakages and incompatibilities with all other code in existence forced me not to. I'd be surprised if it was more than just Malcolm Wallace...
I guess you don't use `(.)` either then?
 inRange lo hi (x:xs) = let ys = inRange lo hi xs in if shouldInclude x then x : ys else ys where shouldInclude x = ...
It doesn't; [you can read about it](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.3729) if you want to. The paper is from 1992, but my understanding is that STG hasn't fundamentally changed (although one major optimization is that it is [no longer tagless](http://research.microsoft.com/en-us/um/people/simonpj/papers/ptr-tag).)
I think that there's room for a couple of beginner-friendly Preludes, because there are several kinds of beginners. For example, an intro programmer prelude with zero typeclasses would almost certainly generate much better error messages for first year students, and make Haskell a better first language. 
&gt; I'm anticipating a huge wave of of bug reports saying "your code no longer works in GHC 7.8 and earlier" because people accidentally rely on the polymorphic features of the new Prelude. If that happens I'd take that as proof that this generalisations are actually made use of... :-) However, if that really happens, it's also easy (except for a few cases where we generalised from `Monad`s to `Applicatives`) to make the code compatible with GHC&lt;7.10 by simply adding some `import`-boilerplate (w/o changing the actual code).
I think there is a documentation problem for polymorphic functions, but we should fix the way we document these functions, e.g. show specializations, show examples with different types, actually show the specialized instance methods in Haddock. Having duplication doesn't get rid of confusion, it moves it. I asked a new Haskeller just this morning what he thought about using the generalized version: "Sounds good. Every time I see two function that seem to both work, I waste time trying to figure out which one I should use." So as it stands now there is a still a learning expedition. 
Glad you enjoyed the question! I also posted another higher-level ones: http://www.codewars.com/kata/folding-through-a-fixed-point/haskell
What about `RebindableSyntax` is not currently convenient enough?
This article has only convinced me that there should be a separate Prelude aimed at teaching beginners, like "Prelude.NooB". A good "Prelude.NooB" module would be as similar to classic Prelude as possible, but perhaps reduce the number of classes, especially all the different classes there are for Num, Rational, Integral, Floating, and so on, and perhaps also make polymorphic some functions more concrete, just for beginners. GHC lets you take care of this easily by using the -fno-implicit-prelude flag. Frankly, I wish Arrow, Applicative, Category, Monoid, and some of the newer packages not yet included in the Haskell Platform, such as Semigroups, were all included in Prelude. As it is now, my template file for creating a new Haskell module always has those modules included, and so does my ".ghci" file, because I uses those functions so often. In fact, I would be in favor of comlpetely renaming fmap to "map" and getting rid of the list-specific "map" function. (comment originaly posted on OP, copy-pasted here, editied for Reddit)
Not sure. I almost never hide things from the Prelude, and I guess that Neil doesn't either. (If you accept this as a small sample of two Haskell experts...) In fact, none of my packages on Hackage use the `Traversable` or `Foldable` classes. I do strive to keep things monomorphic and very much admire the Prelude style. For instance, my Reactive Banana library uses a polymorphic type variable `t`, but it gives people so much headaches that I intend to remove it in favor of a different approach, much like in Threepenny GUI.
The thing is that, unlike Monad, Foldable is completely optional. The list `[]` is the "universal instance" of Foldable, you can always write foldr f a x = Data.List.foldr f a (Data.Foldable.toList x) Being an instance of the `Foldable` class is equivalent to supporting a `toList` function. (That's probably why I have never used the `Foldable` class.) Monads are an entirely different beast; while free monads do exist, efficiency concerns make it necessary to work in homomorphic images. If you learn how Monads work, you have learned something about the very nature of computation, whereas learning how the Foldable class works just gives you a type class that acts as a drop-in replacement for lists but doesn't really buy you anything.
This is similar to how Javadoc pages have a section listing the inherited members of a class.
Going to concur with ndmitchell. I don't know of a compelling reason to hold back AMP for what's probably an extreme minority.
Exceptions are ambiguous across all languages at the moment. They cover a lot of ground and thus are hard to reason about. You have "my invariant was violated" which usually means "crash or reset everything that could be related to me". There is also "something happened that doesn't normally happened but was expected to happen" (network connection failed).
Yes, with which you can implement any function `Int -&gt; Int` (at least in principle).
If I understand the source code of `Data.Text.break` correctly, it doesn't actually do normalization, but works exactly like the list version. (I didn't check `replace`.) The thing about `Text` is that the type system cannot *prevent* abuse, it can only *suggest* a correct way to manipulate text values, because we can always write foo = pack . foo' . unpack By virtue of these two functions, a value of type `Text` still represents a list of `Char` codepoints, whether we like it or not. A correct API for text values can be made for `[Char]` as well, the semantics are the same. As said, the list constructor suggests that more combinations are sensible than a newtype, but the result is the same.
No. `&gt;&gt;&gt;` is the general version of that right? The problem is that it is not in the prelude. I think you actually have to install a dependency to use it.
It looks like /u/edwardkmett wants haskell2010 to be available as a Prelude replacement for beginners, which would necessitate it actually working.
I like that idea a lot!
It definitely doesn't give that many *bits* of information! That's rather a lot :)
Thanks, I figured somebody must have investigated this before switching out some of the most widely used functions in the Haskell world ;-)
Quaified imports clutter code generally. I have a Common module that re-exports serveral modules I want to have available throughout my code base, but that can't re-export Data.Set as S or Data.ByteString.Lazy as BL. So I have 1025 lines of duplicated qualified imports which I could get rid of if there were a more flexible mechanism (#include "Common.hs" would work, but is way ugly).
Trying it right now, thank you for the suggestion. Resurrecting 32bit would be great! Edit: Its woooorking! thank you so much for the tip.
I generally disagree. I'll try to pinpoint a few areas of criticism. I'll be blunt; brace yourself. ---- &gt; What makes a type signature complex? I disagree with a lot of your assessment of "what makes a type signature complex," where "complex" means "hard to use and hard to think about." &gt; Functions with type classes are more complex, since you need to read the type signature while looking at the context, and need to know each class being used. I could similarly say, "Functions with concrete types are more complex, since you need to read the type signature, and need to know each concrete type being used." This is just rehashing your (correct) argument that fully polymorphic functions are simpler. (They are so simple that sometimes their implementation can be derived automatically.) There's no getting around "you need to know the &lt;whatever&gt; being used". Let's just teach beginners Foldable and Traversable sooner. However, I glossed over the "you need to look at the context" part of your argument. This is correct. But the context just gives a list of constraints. It offers you tidbits of additional information. `(Traversable t)` just means that however you instantiate type `t`, you must make sure it is `Traversable`. I concede that this adds complexity. However... &gt; As you add more type classes, the complexity grows faster than linearly. I don't see this at all. I see linear growth of complexity here. One more constraint is just one more constraint. Multiple constraints may be an eyesore, because you have to look farther to the right to see the actual shape of the function's type signature, but that's all. &gt; Higher kinded type classes are significantly more complex than kind * type classes I disagree with "significantly more complex." I agree that they are more complex. But this is because higher-kinded *types* are complex. Generalizing operations on higher-kinded types is only complex because said operations-to-be-generalized are themselves complex. &gt; The reason is that instead of having a hole you fill in, you now have a hole which itself has a hole. You are talking about two completely different kinds of "holes." And I would argue that thinking of typeclassed operations as "having a hole you fill in" is the wrong way to think about them. I would instead teach beginners that typeclass constraints are "additional information." `Show` is the additional information about how to display something. `Foldable` is the additional information about how to turn something into a list. See how I can describe both of those with the same sentence structure, even though the latter is "a hole with a hole"? &gt; The higher-kinded type classes Monad and Functor aren't as bad as the others, since Functor is really the "simplest" higher-kinded type class, and Monad is required knowledge for IO. I'm with you on Functor, but I disagree with your reasoning about Monad being "not as bad" because it is "required knowledge for IO." Because it's really not "required." Just as we have Data.List, with many specializations of Functor, Foldable, and Traversable, we could have some IO module with specialized Monad operations. ioMap :: (a -&gt; b) -&gt; IO a -&gt; IO b mapM :: (a -&gt; IO b) -&gt; [a] -&gt; IO [b] (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b Why don't we do that? That way you don't need to "know monads" to do IO at all! You just have to look up whatever IO combinator you desire. We can just specialize `do` notation to `IO` only, and require `RebindableSyntax` for those who want to use it for Monads instead. &gt; It has two higher-kinded type classes, and one of them is not one of the common ones. Well... the whole point is to make Foldable/Traversable "one of the common ones." The "main idea" of these classes is very simple. Foldable is just a thing that can be turned into a list. Traversable is just a thing that can do an "effectful" version of fmap (a -&gt; m b) instead of just (a -&gt; b). &gt; now all beginners are going to have to wade through the Monad tutorial, their Foldable tutorial and their Traversable tutorial before they start programming They have to wade through the "lists" tutorial currently. Can't we make a Foldable/Traversable tutorial that is just as simple? I feel like we can teach type classes better than we have done. ---- Regarding Control.Arrow, the types would be easier to grok if they used type operators. (***) :: Arrow a =&gt; a b c -&gt; a b' c' -&gt; a (b, b') (c, c') (&amp;&amp;&amp;) :: Arrow a =&gt; a b c -&gt; a b c' -&gt; a b (c, c') -- vs -- (***) :: Arrow (~&gt;) =&gt; (a ~&gt; b) -&gt; (a' ~&gt; b') -&gt; ((a, a') ~&gt; (b, b')) (&amp;&amp;&amp;) :: Arrow (~&gt;) =&gt; (a ~&gt; b) -&gt; (a ~&gt; b') -&gt; (a ~&gt; (b, b')) Just as Foldable and Traversable are thought of "like lists", arrows are thought of "like functions" and with pseudo-function syntax their combinators become a bit clearer. When you want to specialize to actual functions, just replace `~` with `-`. When you want to specialize to lists, just replace `f a` or `t a` with `[a]`. ---- &gt; test = foo . mapM baz . bar &gt; Using the current mapM definition I can, in a fraction of a second, know the approximate shape of what foo consumes, and what bar produces. With the new mapM I don't, and have to keep more context in my head to reason about the code. You still know the "approximate shape" of those things, it's just less concrete and more general. The typeclass constraints are what give it shape. -- doubly specialized mapM baz :: List a -&gt; IO (List b) -- specialized mapM baz :: List a -&gt; M (List b) -- where M is a Monad, e.g. IO -- generalized mapM baz :: L a -&gt; M (L b) -- where M is some Monad, e.g. IO -- where L is some List-like thing, e.g. List &gt; There are often multiple sensible generalizations of a Prelude function. In this particular case (the Foldable/Traversable migration) I know of no sensible alternative. &gt; You have to add more type annotations since types cannot be infered from the functions. This is perhaps the one point I strongly agree with. But my solution to this problem is to turn off the monomorphism restriction and let them be inferred to their generalized types. Also, manaully adding type annotations to top-level functions is usually a Good Thing, because it forces the compiler to make sure that the programmer's intent (as expressed by the type) is being respected (by the implementation). ---- In summary, your argument feels to me like post-hoc rationalization for the status quo, rather than a well reasoned paradigm for how we should design the Prelude.
+1000 this. Also, if I hoogle `Set a -&gt; [a]`, I want Hoogle to tell me that this is the Set specialization of a Foldable operation.
&gt; That tradeoff is avoidable if modules can specify which major versions they are compatible with, ... True, and today you can do that with cabal files. I suppose I'm arguing for moving automatic dependency resolution logic out of the build manager and instead putting the version info into the source code itself and thus into the developers' hands. This may well not be feasible at scale. &gt; In practice there's no way you'll get all your libraries to agree on the exact same version of their dependencies.... True, but I guess I'm fine with not trying to resolve everything to a single version, and instead just using multiple versions on different parts of my app. E.g., you mentioned the logging module which might log to different places across different versions. I would be fine with that once it was a known behaviour because I could program against it.
Both issues are monstrous when encountered. I daresay that space leaks are more monstrous, but far less common. Perhaps they seem more monstrous to me precisely *because* they are less common and I therefore have less experience dealing with them.
Let's remember that many of us spend/t quite a long time as haskell newbies. :) I remember finding the ghc prelude not very easy to follow as a newbie, when I was spending a lot of time looking at the source to common functions to try to figure them out. It had a lot of ifdefs and imports and optimisations. This seems even less useful to read through than I remember it being: http://hackage.haskell.org/package/base-4.7.0.1/docs/src/Prelude.html Maybe I was reading the Hugs Prelude back then, as a more readable resource? A teaching version of the Prelude, with lots of comments etc would be a great resource for beginners.
Unique references as in Clean let you write a function that destructively updates a map but is still pure, because types demand that its argument be the only reference. Rewrite rules as in GHC can already tell the compiler to automatically replace one function with another, but only if the replacement typechecks. Putting them together, it seems it might not take too smart of a compiler to automatically upgrade operations to the in-place version where legal.
Control.Category (.) for more generality, of course.
Run $ ghc-pkg hide base &amp;&amp; ghc-pkg expose haskell2010 and `ghci` from then out gives you something that looks like a relatively standards compliant environment suitable for teaching, if you are choosing to teach to a standard rather than an implementation.
I have no objection to the existence of such packages.
Huh? The changeover is still ongoing and there are tons of packages that don't build with HEAD, how can you claim to know this? Edit: I don't expect there to be any effect on performance in the vast majority of cases (usually GHC will be able to see that the overloaded functions are used at [], inline the dictionary lookup, etc.) but we just don't have a very good way to track GHC's performance on the kind of Haskell code that people run in the real world. If there was a performance regression in bytestring or aeson or lens or whatever we wouldn't know until somebody notices and files a ticket. Also, this isn't specific to the Foldable/Traversable changes! There are lots of other changes going on in GHC that might affect the performance of any given program positively or negatively. We just don't have any good way to know until we hear from the experience of users. Double edit: I forgot to mention that most of the breakage in HEAD is not due to the Foldable/Traversable generalizations. Many still haven't added Applicative and/or Alternative instances for AMP, and there are other unrelated breaking changes in GHC such as https://ghc.haskell.org/trac/ghc/ticket/8883. Actually the most common form of breakage from the Foldable/Traversable changes is name conflicts with modules that define their own names like foldMap or traverse. (Actually, when was it ever decided to export those from Prelude, anyways?)
&gt; In nix you can work around your first point, by making a package in the repo that points at a source code directory. I wasn't aware it could do that. How does it calculate the package hash in that case? That's one of the challenges that we face, how to work out which files are part of the package and which are not, and how to do incremental builds with a changing package hash.
&gt; types that are too polymorphic make software engineering more expensive more often than less expensive. The only "expense" of general types is learning the pieces of the type that *aren't* general. For example: id :: a -&gt; a id a = a double :: Monoid a =&gt; a -&gt; a double a = a &lt;&gt; a Even though both of the above functions have type `a -&gt; a`, the latter additionally adds the `Monoid a` constraint, making it *less* general than `id` and therefore *more* nuanced.
It's certainly almost entirely scala-specific. It's a nice enough article cataloging various things scala fails to optimize and measuring just how much performance can occasionally be gained by forgoing some nice abstractions in favor of raw loops and stuff. The title is the most overgeneralized part and it's not meant as any kind of insult. The author mildly underestimates what GHC can optimize, but there are still things like cache locality where it's not quite Sufficiently Smart.
Having the first two numbers indicate major version is also in the style of [Haskell's PVP](http://www.haskell.org/haskellwiki/Package_versioning_policy).
With this I've caught up with Matt's posts, so I'll be going back to start implementing more features in previous modules. I'm currently leaning towards adding full CSS3 parsing first, if I can figure out which documents actually cover that.
The problem is that haskell2010 is effectively deprecated, and there's no gradual migration from haskell2010 to base - it's one or the other. This isn't a long-term solution. Also it's not clear if we'll be able to support haskell2010 properly after the AMP change anyway.
&gt; efficiency concerns make it necessary to work in homomorphic images ... until recently. Oh, we still have to worry about constant factors, but you can have both fast execution/reflection and fast composition/building using [type-aligned sequences](http://www.reddit.com/r/haskell/comments/29i94o/re%EF%AC%82ection_without_remorse_revealing_a_hidden/).
You can separate interface and implementation without any kind of polymorphism. Just use a module.
Haskell has beaten C, but there's provisos to that. It takes a lot of work to write Haskell to beat C - you have to use a lot of tricks like explicit unboxing, explicit strictness etc. Even the new clever tricks for parallelizing numeric work in Haskell depends on explicit notations. Exploring the middle-ground between high and low-level extremes is worthwhile - it often allows us to gain 90% or more of the performance for 10% or less of the work. But cases like this only really beat C because the C code hasn't had the work done to apply the same optimizations. By doing enough work, you can still write C to beat the Haskell. The point being that although it's good that Haskell is by no means slow, when it beats C that isn't really conventional Haskell code - it doesn't come for free - and beating C performance in a few cases doesn't mean Haskell is as fast as C overall. It may make Haskell more practical, but even then there's issues in the way of that (more relating to the currently quite small ecosystem than performance of compiled code). There will always be cases where a simple lowish-level language can, in principle, out-perform a higher-level language - provided you're willing to do the work to get that performance. If you need that power, the high-level stuff is just in the way - making it harder to get to the metal. However, in time, these cases will probably get rarer - the middle ground being explored by current functional languages will more and more often be sufficient and - given the faster development times - practically superior. This is no different to the transition from assembler to languages like C and Fortran. Though it's worth noting that writing assembler to beat C or Fortran these days - though theoretically possible - is only actually achievable for small routines and, even then, generally absurdly difficult because of the complexity of the metal you're working with. 
At work our String is Text (to a first approximation), which removes that confusion.
I don't buy this at all. It's very similar to saying "I used [a] instead of Vector a because I didn't want to type the extra characters." If an import is what's stopping you from using the right data structure, you've got problems. And the s/map/V.map/ argument doesn't apply either because we're talking about new use of Vector. Refactoring existing code that uses list is a completely different thing.
I'm going to want some easy way of showing/teaching the specification instead of the implementation. When I first learned Haskell (~2008), it was mostly just install GHC and don't tell them about all the extensions. I love GHC as an implementation, but I do not consider Haskell a single-implementation language, and I don't want it to go down that path.[1] To that end, I generally try and introduce people to Haskell, not GHC. [1] I was drawn to Haskell partially because the Haskell98 report existed, and I learned quite a bit about the language before I installed an interpreter. I actually enjoy learning a language from the specification. (Did it with Java and Haskell; I didn't do it with C/C++, but I constantly reference their specifications when discussing them as languages.) I find it actually gives me a better instinct for reading error messages.
That's a lot of noise when you multiply it by the 500+ occurrences of `mapM` in GHC (and it's far from the only function under discussion). Currently I get all this information for free from the type of `mapM`. Meanwhile there's only 17 imports of Foldable and Traversable combined. Why would I want to give up all this free information just to get rid of 17 lines of imports?
IHaskell does a lot more than text input and output. For example, IHaskell supports visualizations; if you are using the `diagrams` library and have an expression of type `Diagram a` (or whatever the types are), evaluating that expression at the top level of an IHaskell cell will result in a PNG of that diagram being shown below the cell. In fact, even more so than that, it supports interactive outputs ‚Äì you can have a `Parser a` value from parsec show up as a textbox into which you can type to have the typed text automatically be parsed and errors highlighted in the textbox. It's way too hard to do that sort of stuff in GHCi. You need to do your own typechecking in order to determine if a value can be shown as an image or interactive display anyways, and if you're doing typechecking, you might as well go straight up GHC API. Also, using GHCi, there's no way you can do stuff like infinite lists easily ‚Äì you have to work a lot on buffering, redirecting output, sending the child GHCi signals, and so on. Basically, there's a lot of things for which using stdin/stdout is highly suboptimal. It might still be possible, but switching to GHC API paid off for me very quickly, because doing the things that IHaskell now does using only GHCi would have been an absolute nightmare.
Ok... it's probably too late to prepare for the November GREs... so taking the ones in the spring would have me starting grad school in fall 2016... I can get you an abstract in six or seven years?
I'm not familiar with that part of GHC but honestly it sounds really easy. You could put some new kind of pragma on Prelude.mapM that says "prefer Data.Traversable.mapM if both Prelude.mapM and Data.Traversable.mapM are in scope". It doesn't even have to be tied to the fact that Prelude.mapM is a type specialization of Data.Traversable.mapM (though of course it would be bad form to use the pragma when the functions are unrelated! much like rewrite rules that change semantics). Since GHC never has to match values (the way it has to match types) I don't see what problems this could cause. I guess rewrite rules do match values, but presumably Prelude.mapM could be inlined to Data.Traversable.mapM before processing rules. Definitely +1 from me if this is possible.
I, too, spent a long time as a noobie before I started catching up. (About 5 years)
normally there's a line in the nix-expression like this: sha256=109845710whatever91237509215; and you replace that with src=/home/bzzt/mycoolproj; That's how I install purescript on there. It requires cmdtheline, which is broken. So I downloaded cmdtheline from git and fixed it, then edited the nix expression as above. I have a couple of little scripts to automate converting the cabal to nix-expression and then adding it to my nixpkgs clone. There's a utility cabal2nix which automatically converts cabal files to nix expressions. So if you change your cabal file it can (in theory) propogate that change to the nix expression without having to maintain both cabal file and nix-expression. Its not perfect but it mostly works. Right now their procedure (as I understand it) is to automatically generate the nix expressions for cabal projects as much as possible. One issue is adding dependencies that are not in the cabal file, like I have a yesod project that also has some purescript, so when I'm working on it I want purescript to be available as a build tool. But that's a slightly different situation than a regular haskell library, I'm not actually using purescript to build the haskell project itself. 
This discussion has revealed to me that this is a complicated issue, and I do not know what to think.
&gt; each of the polymorphic function's documentation should indicate what the function does monomorphically for the canonical types ( [], (-&gt;), (a,b), Maybe, Either, ... ). I think this is a brilliant idea. I know such a thing is redundant, and too much documentation is avoided like the devil in the Haskell community, but those are the things that make other languages easy to use vs. Haskell hard to use.
This has been a big pain point for me as well. I was in disbelief when I did not find a way to re-export qualified modules. This functionality needs to exist.
The author uses this word "complex" over and over again, but fails to define it, and doesn't seem to be using the conventional meaning of "many parts". I *think* he means the common programming vernacular, where "complex" is used as a synonym for "hard" or "difficult". Would be more clear if OP would just say that instead of the ambiguous use of "complex".
Something like this! http://chrisdone.com/fmap
Alternatively, GHC could come with a `ghci2010` and/or `ghc2010` wrapper scripts that would turn on `-XHaskell2010` and bring into scope the `haskell2010` package...
That's nasty. In particular, it relies on the environment. What about import Prelude () import Prelude.Tutorial.One Yes, we can even *spec* a tutorial prelude. I don't see why not. Make it a succession of completely monomorphic and simple stuff, where helium won't just be able to tell you what the error is but also how to fix it, over generalising that monomorphic stuff (finally, no `concatMap`, just `&gt;&gt;=`, which previously was IO-only) to ease people into typeclasses, to the "real deal" (which would be the default prelude), to import Prelude () import Prelude.Categorical.EKmett.BlowMyMind
So you're saying that "use whatever's in scope" is *too* rebindable to be convenient? Interesting. It's quite rare to bring a custom `ifThenElse` (or other such things) into a local scope, so I don't see that as much of a problem. You can use the .cabal file to configure all modules in a package to use the `RebindableSyntax` extension. Then the only inconvenience left is to remember to import the Prelude replacement in each file. It seems that this, too, *should* be something that a cabal config file solves.
The link is to Lindsey Kuper's dissertation defense slides. She implemented LVars (related to CRDTs if you've heard of those) in Haskell, so while this isn't a "Haskell talk" per se (because the work is more general), it's still relevant. Lindsey writes her talks word-for-word in advance, so the benefit for you is that there's also a link there to the full transcript :)
&gt; These are the wrong defaults for performance critical web programs, but not for learning, and not for 90% of the code most people write. ...and yet `type String = [Char]` is one of the major cause for the bad press we get regularly when people give Haskell a try, unsurprisingly use `String`, and then go on writing disappointed blogposts about Haskell being significantly slower and/or consuming more memory compared to Python, Ruby, or some other dynamic language...
Mu doesn't even have recursion. :)
I don't think listing invariants solves the problem of understanding and intuition discussed here. Suppose I look through some code and stumble upon a `Traversable` context. When I look up the [Traversable](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-Traversable.html#t:Traversable) documentation, it gives me a screenful of laws which instances must satisfy, along with a very vague idea of what `traverse` is intended to do. However, is it really reasonble to expect your average programmer to derive immediately what the instance for, say, `Either a` is going to be? Even knowing that some classes' laws permit exactly one implementation for a specific datatype, I could not, off the back of my head, tell you which combinations this applies to or what those instances are. This is not a singular incident, either. [Foldable](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-Foldable.html) says it's "for data structures which can be folded" and doesn't even specify laws. [Functor](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-Functor.html) at least mentions `map` as a specialisation and specifies laws, but I'd argue that it still takes a bit of brain juice to determine the behaviour of the `((-&gt;) r)` instance. Besides, if I'm not mistaken, the `Functor` laws as given there would also allow the following definition for lists: fmap _ [] = [] fmap f (x:xs) = f x : xs This lack of documentation on what (core) class instances actually do has bugged me, as a relative newbie, for a while. As it stands, I don't think an understanding of these classes can be derived (solely) from the official documentation. So if a concerted push towards more ubiquitous usage of type classes is supposed to happen, I think a major documentation effort needs to follow suit (and would be desirable independently).
Yeah I'll say that. 
Using the less general thing communicates to the reader what types you are working with, and that is useful. Maybe someday when we have a kick-ass Haskell IDE that will show you the inferred type of any subexpression anywhere in your program this type of communication will be less useful. But we are nowhere near that today. And it can be a significant burden for a developer to hunt down the actual type of some overly-general construct.
What drug are you on? and can you send me some?
Yeah, I think so. It's super annoying that they decided to split the spec across several documents.
Precisely like this, but in the base docs!
Cool post! I wanted to note, though, that it's pretty hard to read on mobile because of the huge margins/padding: http://imgur.com/tk1t29q -- it would be nice if you could fix this!
On MacOS 10.9.5, I get "Invalid documentation tarball: File in tar archive is not in the expected directory 'Boolean-0.2.3-docs' ".
I also agree with ndmitchell. Supporting both does not seem at all worth the trouble.
&gt; Types are how Haskell programmers communicate their intentions to each other. Yes, yes they do. I teach beginners using parametricity. Specifically, how to read types. No, it is not "impossible" for beginners. It is a challenge, but is the only reasonable economy in investment of effort in teaching, in my experience. If you take this away from me, and therefore those beginners, you disservice us all. Teach someone by having them rewrite `sequence` over and over. They develop an intuition for it, and inevitably ask, "can this be generalised? Can we remove that code duplication?" Why yes, yes it can. A beginner is not to be treated as if they are stupid. This cynical approach bugs me a lot. They just need the tools and vocabulary to express the ideas that they are *already striving for*. Polymorphic types are essential to this. **Give it to them.** Traversable and Foldable in the Prelude *precisely because it will help beginners*.
I dont get the problem, just *massively* improve the documentation, with lots of examples and special cases, then there should be no doubt how to use something like traversable or foldable. The concept of substitution is really not that hard, just give an explicit step by step example how the general function signature boils down to easier ones. It's not like a freshman in college is going to think lists are next best thing after sliced bread and be upset that they have been removed from their unjustified exposed place in the standard library.
Issues such as this one, [this one](http://www.reddit.com/r/haskell/comments/2hzqii/neil_mitchells_haskell_blog_why/), the Applicative-Monad Proposal, and Cabal churn are making it impossible to write Haskell code today and expect it to compile even two years from now without being on a treadmill of regular, tedious fixes that buy you nothing but the privilege of being able to compile on the latest GHC.
Same for: `(a -&gt; b) -&gt; (a, x) -&gt; (b, x)`
I think you would get a lot more traction if you advertised `basic-prelude` instead of `classy-prelude`. `basic-prelude` is pretty much an unequivocal improvement over the prelude.
I wish Haskell had proper ML style modules, then you could get away with almost no polymorphism without loosing any expressiveness.
As a beginner, this is exactly how I always feel. I find myself wishing there were just one of things (aforementioned `map` vs `fmap`) in the standard library. It feels like there's more to learn in addition to the concepts and the sometimes bizarre-looking operators.
So one flaw with this particular proposal is that it's too easy for the generalized version to "leak" in from an unexpected source. In order to know which mapM a module imports I have to check all my (unqualified and without an explicit import list) imports and see whether any of them re-export Data.Traversable.mapM. That might involve reading arbitrarily many files, due to module re-exports. In short there's no way for me to tell easily by looking at a module's imports that it has the type-restricted mapM. But a number of alternative strategies for resolving name conflicts suggest themselves, such as any of * tel's suggestion, but where the module doing the importing has to opt in with a language extension. * only let Prelude.mapM be generalized by an import from Data.Traversable specifically. (But then a replacement Prelude can't re-export the generalized version.) * add a new import type `import {-# OVERLAPPING #-} Data.Traversable` that means "prefer symbols from this import if there is a conflict". Custom Prelude can be one line: `import {-# OVERLAPPING #-} MyPrelude`. * (probably even more controversial) automatically treat imports of modules named `Prelude.`* as "overlapping" in the above sense. There's probably lots more design space here to be explored.
The beginners can just use 7.8.
&gt; If you generalize the type of a function by replacing a concrete type with a variable, but you also add a class constraint on that variable, then there will not necessarily be fewer functions of the resulting type. In fact, there may be many more functions of that type. For example, there are only a small number of functions Bool -&gt; Bool, but there are infinitely many functions Bits a =&gt; a -&gt; a. this true without adding constraints. there are two total functions of the type `a -&gt; a -&gt; a` but only one of the type `() -&gt; () -&gt; ()`. subtypes are not subsets since types are PERs not sets: a more general type can have fewer inhabitants than a less general type but it can also have fewer equations. 
I'm curious if your compiler doesn't have recursion at source level or doesn't have it all? Are things like ``foldr`` and ``map`` baked in at the language level?
I kind of like it in many cases though, as it helps discourage unsafeperformIO. But certainly when you have to worry about timing. 
Nevertheless what I wrote is still true. Consider it a little puzzle. (Hint: you need to use abs and/or signum.)
Richard HAS talked about adding something like this to ghc. not sure if its happening yet, but i think with all the AMP stuff theres room for a good argument that we should figure it out
&gt;but there are infinitely many functions Bits a =&gt; a -&gt; a. Sure, but there are still less than say Integer -&gt; Integer.
&gt;The thing is that, unlike Monad, Foldable is completely optional. I've been coding in Haskell for a year now, and I have never actually used the Monad typeclass, but have used Foldable plenty. I AM THE EXCEPTION THAT PROVES THE RULE
Hopefully you could just add a `@media` query to reduce margins/padding for smaller widths.
Saw her talk at Ricon last year. Really interesting stuff.
I guess I should say that I'm assuming that `Int` is a known finite range of integers and that you are allowed to write programs of size larger than that range. (Hence "at least in principle".) I know how to do this, so I am unlikely to be convinced by "I don't think you can" :)
This get's to the heart of the problem I have with a more monomorphic Prelude is that it's trying to fix a social problem ( bad documentation for beginners ) with a technical solution. The documentation for [Alternative](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Applicative.html#t:Alternative) is utterly useless you already know what the function does, which unfortunatly is like 85% of the core docs. Just giving a few examples of how you can chain together computations that can fail with (``&lt;|&gt;``) with Maybe or represent parser branching would basically solve this problem without needing to cripple generality.
&gt; and too much documentation is avoided like the devil in the Haskell community The essence of the problem with this proposal is that it's trying to solve a social problem of poor documentation with a technical solution that cripples the Prelude. The right solution is improving the documentation of ``Control.Monad`` and ``Data.Traversable`` with better examples for beginners, not monomormophizing them.
For one data point, when I was a starting with Haskell I found the state of the core libraries to be incredibly confusing, specifically because I spent a bunch of time learning about the Haskell way of doing things, and then had to essentially discover that many of the things I learned was basically wrong. I essentially had to keep a mental list of the various 'gotchas' that *many* of the common functions/libraries introduced: There are functions that can be generalized with no real repercussions, like foldr, (.), (++)/(&lt;&gt;). Then there are also functions that could be generalized if it were not for the whole Applicative-Monad thing, like fmap/liftM, and (&lt;*&gt;)/ap. One person told me that mapM/traverse had slightly different semantics w.r.t. to laziness, but the documentation doesn't mention it, so all I can say is I'm not confident I understand the difference between those to. Then we've got things that we should never use, like foldl due to laziness, though when you're told to use foldl' you *later* learn that it still has the exact same problem if you're accumulator can itself be lazy (though maybe that problem is now fixed since we can just point people to Control.Fold). There's also problems like introducing read as a basic example of a typeclass, when you should pretty much always use readMaybe/readEither due to read lazily throwing exceptions in pure code. Likewise, is lazy IO best avoided? Are there any strict IO functions in the Prelude? I'm honestly not sure. I see there's a strict package, and a strict-io package, stackoverflow posts I've read suggest using length or deepseq to force lazy IO, and I think the strict Text/ByteString datatypes have strict IO functions. Why so many alternatives, and why would I favor one over the other? At first I didn't realize the prelude functions I was using were lazy, and ended up encountering some bugs because of it. For a while after that I was using a little internal library I wrote to ensure all my file IO was being done strictly, before I realized I should be using the ByteString/Text apis. Maybe the strict/strict-io packages were written for similar reasons? There's also things like pipes/conduits for streaming IO, so maybe lazy IO is absolutely a bad idea in all cases, except when you're problem is simple enough that you can be undisciplined and be lazy yourself. For Text/ByteString, I initially assumed their interfaces were an example of (1), but *later* learned that they can't be generalized so easily without bringing in something heavy-weight like classy-prelude, which seemed like a good idea to me initially, but then I even *later* learned that I should avoid alternate preludes to avoid needing additional dependencies/losing backwards compatibility due to redefining Monad typeclasses and other issues, yada yada. Then I saw base-prelude which is very lightweight, which seemed like a good idea, and then even more *later* learned that I should avoid it, since if I use it I can't use SafeHaskell, which I had never been told was a thing before that point, but which I am now to understand is important maybe? I'm still not sure if String should ever be used. I can understand using it because working around the fact that it's the default could sometimes be awkward, but are there enough real-world use cases where you absolutely want to use String that make it a sensible default? And if it's not a good default should I still use it for interoperability reasons? A lot of libraries seem to have String-based interfaces. Or should I try to be disciplined and never use it? I have this sneaking suspicion that no matter which datatype I use I'll later learn I made the wrong choice, and the right choice would not have been any more difficult to make if I knew what it was. Then with module namespaces, I had initially assumed that it was okay to include sub-namespaces of things I was familiar with, because that just seems like a sensible thing to do, and in the process ended up upgrading a library GHC depended on and broke my installation. Perhaps this is less of an issue now that cabal sandboxes work fairly well, though I'm sure many new users won't be aware of things like sandboxes initially. And it still makes it difficult to remember which libraries interact well together, and it makes it difficult to read other peoples code: Control.Monad is part of base, Control.Monad.Trans is part of mtl, Control.Monad.Trans.Maybe is part of transformers, Control.Monad.Trans.Either is part of either, Control.Monad.Free is either free *or* control-monad-free, depending on what the libraries cabal file says. I've also been burnt by the fact that cabal doesn't install profiling libraries by default, and is pretty terrible at reinstalling things after the fact. Then having to remember that its Control.Monad, and Control.Applicative, both of which define some functor specific functions, though Functor is part of the Data namespace. Monoid is also part of the data namespace, though I've used it for control flow. I found this very confusing when I was starting to become familiar with the Haskell ecosystem. I would frequently try to import one thing, get an error because I remembered the wrong namespace, fix that, get an error because I was importing multiple functions with the same name, hide them from the prelude, and repeat for the next module. Now I just use a customized prelude, but if I ever want to share anything I guess I'll have to rewrite it. I also recently read an offhand comment in the haskell-libraries mailing list about some issue when `catch` was generalized to polymorphic exceptions. It's not part of the Prelude, but that's not going to mean I'm never going to need it; so does catch have any gotchas I need to know? I have no idea. Maybe the issue isn't really relevant today, but it was an old forum post, and I've experienced enough weirdness from Haskell that I can't be confident either way. Now I feel nervous using it. That's my rant, not all of which is Prelude specific, but coming from Python I've found Haskell to be a very frustrating language. I'm still keeping at it since I find it amazing despite the issues, but Haskell the math-y elegant language and Haskell the bizarre standard libraries are unfortunately packaged together. I'm still not terribly confident I understand which issues are related to what cause, or whether there are any gotchas lurking in base that I'm simply not aware of, but I've been bitten by enough things that I've become very underconfident that I can depend on sensible behaviour from anything in Haskell that isn't in a type signature or listed as a law of a function/typeclass. There's simply too much to learn starting out to keep the important abstractions separate from the interesting-but-not-necessarily-needed abstractions separate from the longstanding historical accidents. There's obviously a line you need to draw regarding how much generality you provide. I was working on a library for GArrows in line with the HetMet paper, and found that using much too general typeclasses ended up making things very unpleasant to work with, that I ended up defining some less general functions specifically to help type inference. But I think the Prelude as it stands could use some improvements, and personally I think Foldable/Traversable (and also Monoid personally) should be part of those improvements. Foldable and Monoid are easier to understand than Monad, and we already expect people to learn that one, so they don't seem like a big jump. Likewise, Traversable is pretty simple if you already understand Foldable/Monad. Hell, if I'm really wishing for something I'd like functions like read/foldl slowly phased out from the Prelude altogether (or at least come with a compiler warning or something). I'm starting to feel like any use of either of those (and probably a few others) is probably a bug if there's no comment saying that it's being used intentionally, and that's exactly the behaviour that I don't want in the only library that we can assume everyone will learn. tldr; I think the big issue with the Prelude is that you experience problems around the time you know enough to find a workaround, but before you have the knowledge to make a sensible choice for what the workaround should be. Forcing the user to choose what the default should be is a terrible design choice; I'm still not convinced I know enough to make intelligent decisions about how things should be changed, even though I'm confident that they should. It's a very unpleasant position to be in when you're learning a language. Even just a big list of things-you-should-know-before-relying-on-the-prelude-for-beginners would be nice to have, because there are a lot, and they're mostly folklore. Something like "foldr is frequently generalized with Data.Foldable, foldl has this problem (explained in more detail here), which is fixed by Control.Fold (with examples how to use it shown here), readFile, getContents, etc should be avoided because of this, examples of the problem, links to the solution, etc ...". And then once we have all the folklore gathered in one place we can decide if it really is easier on beginners to have to know that many exceptions to the rules. Because my experience has suggested it isn't.
&gt; While a type can be a monads in multiple ways it can only be traversable in one Two, unless the type itself is somehow always reflexively symmetric.
I haven't really advertised either of them much. I agree with you that `basic-prelude` is easier to swallow. Perhaps I *should* start writing a few blog posts about it.
That's an incredibly well stated position. I'm in the same boat as you my friend.
::shrug:: I don't really disagree with you. I think adding an extra import for the right data structure is worthwhile. However, empirically, I've seen plenty of code that using `String` instead of `Text` or list instead of `Vector` for convenience purposes. Should those authors have used the right datatype? I think so. However, the current `Prelude` isn't encouraging it.
Just replying to myself, if catering to beginners is what we're going for, as someone who's trying to get past the beginners stage: if a category-theory inspired interface is worthy of being in base, it's probably worthy of being in the prelude. Conversely, if it's not worthy of being in the prelude, get it out of base. Honestly, if the design of the prelude is supposed to help me learn Haskell, I would like to kindly say please stop helping me learn Haskell. I've heard lots from people who have been using Haskell for a long time about how its supposed to make things easier for beginners, but somehow I've failed to hear from beginners about how well-designed the prelude is. Barring some actual data, the argument seems suspect.
I still don't really understand the Alternative/MonadPlus outside of a few specific instances. In general I have no idea why we can't just use Monoid. As an example of something very well documented, I was recently reading the pipes tutorial, which I was extremely impressed with. We need more things like that, because I went into pipes expecting something overly abstract, and while the core seems pretty abstract the tutorial was excellent. For your particular example though, +++ works for sum types and *** for product types, which I think is pretty mnemonic. They also almost obey the associative laws a *** (b *** c) is isomorphic to (a *** b) *** c, same for +++, and *** distributes over +++. Essentially they're the same as the numeric operators + and *, except they manipulate the inputs/outputs of functions (or other arrows) for sum and product types respectively. For &amp;&amp;&amp; I think 'and' (describing a product type (a,b) = a and b) and for ||| I think 'or' (Either a b = a or b). There's an additional symmetry between &amp;&amp;&amp;/||| that makes them easier to remember. Namely: f &amp;&amp;&amp; g = split &gt;&gt;&gt; (f *** g) where split :: b -&gt; (b,b) f ||| g = (f +++ g) &gt;&gt;&gt; unsplit where unsplit :: (Either b b) -&gt; b Granted, I've worked with Arrows quite a bit, so I've developed some intuition for them. I think they're a lot easier to understand if you can describe them in terms of algebra on ADTs, but I don't think the arrow operators will really pay for themselves until someone finds a really common problem that demands an Arrow solution. Unfortunately a lot of the interesting Arrow-like things are excluded by requiring the arr function, which really limits the things the operators can be used for. EDIT: At least, you can write arrows that correspond to the laws of distribution, association, and commutativity for any arrow (e.g., assoc :: (Arrow (~&gt;)) =&gt; (a,(b,c))~&gt;((a,b),c)), which I think is equivalent to the above.
On a moderately related note, what are the chances of seeing a haskell2014 report? While this doesn't solve the problem of not being able to compile against older standards, it would be a modern standard to compile against. I'd also appreciate the confidence of knowing certain extensions are common place enough to know be considered standard. I always feel weird saying that I'm learning Haskell, when the reality is that I'm learning GHC (eg extensions).
This is very disappointing and I feel for my future students if we choose to make their learning path even harder by eliminating the potential to exploit parametricity appropriately. The "concrete path" from specificity to polymorphic types appears to be a thesis that we have all fallen for. I can find no experienced teacher who still believes that it is an appropriate method for helping others learn. Specifically, students find it hard to exploit the value of types as it is. When we introduce concrete types, they are *confused even more* because it opens up the potential for ambiguity to the extent that they hit a wall. Careful coaching can assist a student in properly exploiting the value of parametricity and equipping them for dealing with those ambiguous cases. The idea that this is "too hard" is really sad to hear. This is because a lot of us have worked so hard to figure out to make it not hard at all. Everyone benefits by this, *especially students*. It is also very cynical, in that it treats students as if this knowledge is somehow inaccessible. It isn't; it takes two days *at most*. We have repeated this over and over, independently but by the same methods. I sympathise with other educators who are unable to break this barrier. Make no mistake about the number of failures were required to figure out how to perform this careful introduction. However, that sympathy stops when those very appropriate tools are taken away for a perceived benefit that is not there. And, we **know it isn't there**. To take this away would also take away the idea that Haskell is by far the most suitable programming language to teach these concepts.
&gt; *Everyone* has an easier time reading monomorphic code because it gives the reader more information. In kinda the same way that a 7-year-old has an easier time understanding "2 apples and 3 more apples" than "2+3" because it gives them more information.
&gt; Besides, if I'm not mistaken, the Functor laws as given there would also allow the following definition for lists: &gt; &gt; fmap _ [] = [] &gt; fmap f (x:xs) = f x : xs No it can't due to the required parametricity (is this an instance of "theorems for free"?) of `fmap` 
So this whole generalisation revolution in `base` is about encouraging a more modern style of Haskell code by tweaking the out-of-the-box defaults to take into account that most containers share the property/laws of being `Foldable` and `Traversable`?
Lazy I/O is often made possible by `unsafePerformIO` in the first place... so it just abstracts/wraps `unsafePerformIO` for you, but you're effectively still using it in disguise...
So you have never used `IO` in your code? What about the `forM` function? Nor any package like one of the web frameworks that uses the state monad? Even if you don't see `Monad` in your type signatures, you're making use of the class by using the corresponding overloaded functions.
That's not enough, unfortunately. One counterexample is the probability monad `Prob a = [(Double,a)]` where you need to collect intermediate results at times. Other counterexamples are monads where you need the `MonadFix` instance, i.e. value recursion.
What I have observed about beginners learning Haskell is that they don't learn concepts one by one. It's very uncommon that someone encounters monads for the first time, learns them, and is done with it. Rather, beginners usually start with a vague understanding of how they can use `do` notation and may take a long time before they finally get a solid understanding of Monads. But now they have to grapple with Foldable, too, all the while their understanding of some other core concepts is still incomplete. The same goes for experienced Haskellers, by the way. For instance, I have but a vague understanding of the API from the `lens` package. I know the underlying principles, but I have no birds-eye view of the code. Whenever I read code that uses `lens`, I can guess what it does, but it's guesswork, not informed by an actual knowledge of the API. 
Yup.
I can't speak for "this whole generalisation revolution," but I can say that it's very nice that `mapM_` will automatically work for both lists and `Vector`s.
Correct!
I was discussing this with a friend after a Haskell exam that we had. I've been doing haskell for some time. but he's new to it. but we both bumped into the same documentation problem. So we both understand that we can define a Monad in terms of either `join :: m (m a) -&gt; m a)` or `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b`. And for lists it's clear for me what both those do.. But say I am given an instance of Monad for Cont already, then nowhere in the documentation it will tell me what the effect of `join` i s going to be ... I will have to experiment it with like 20 minutes in GHCi untill I "understand". It would've made life so much easier if _really_ general functions like, for example, join, don't just have general documentation, but also instance-specific documentaiton. It just makes it so much easier to get using a library more quickly.. Documentation is all about communicating intent to the user. And I sometimes a name and a type is just not enough. Especially when we talk about typeclass functions. Another ancedote from my friend: The functor instance of Either maps over Right and not Left. My friend found this really hard to grasp. It's true that you could deduce it from the typeclass instance `Functor (Either a)` but just having this single line of documentation "The functor instance of Either maps over Right. wanna map over both? Check out bifunctor. Wanna choose to either map left and right? check out Jokers and Clowns" would add so much value for a beginner! I think we underestimate how powerful documentation is sometimes.
The documentation for the [time](http://hackage.haskell.org/package/time) library.
I believe parametricity implies that there can only be one functor instance for a datatype. However it's easy to see that this *particular* case doesn't work: it doesn't type check!
I have a dream. In my dream everyone who is about to complain about documentation pauses, improves the documentation, and submits a pull request.
yeah... let's stay compatible with Haskell98 forever....
Why two? For example `(a, a, a)` has 6 `Traversable` instances.
[Her talk at RICON West 2013](http://www.youtube.com/watch?v=8dFO5Ir0xqY) If someone wants a hands-on experience, Riak 2.0 has CRDTs implemented.
...and by the time you grow up you don't express yourself in terms of apples anymore, you simply say the generalised "2+3" without having to explicitly tell everyone you are no longer importing the apples Prelude =)
A new Haskell report would be great... that could finally make some of the language extensions almost everybody uses part of the language
&gt; I teach beginners using parametricity. There is no parametricity involved here because of the class constraint. The definitions of traverse for different types `t` are a priori totally unrelated functions. This is the kind of comment I was referring to [here](http://www.reddit.com/r/haskell/comments/2hzqii/neil_mitchells_haskell_blog_why/ckxom2u). &gt; Teach someone by having them rewrite sequence over and over. They develop an intuition for it, and inevitably ask, "can this be generalised? Can we remove that code duplication?" Why yes, yes it can. No, not really. The compiler will generate the code for you with `DeriveTraversable`, but the code "duplication" still exists, it's just not visible to the end user. 
Well .... people who don't understand something due to lack of documentation are perhaps not in the best position to write documentation for that thing?
There is parametricity involved when I ask them to solve the following: sequence :: (Applicative f, Traversable t) =&gt; t (f a) -&gt; f (t a) as opposed to asking them to solve this: sequence :: (Applicative f) =&gt; [f a] -&gt; f [a] Both are easier to solve for different reasons. With appropriate coaching, and not much of it, the second is significantly easier to solve. This has been performed repeatedly, after much failure guiding it (as is occurring with the original suggestion). The code duplication disappears on this final implementation of `sequence`, which generalises all previous.
On the contrary, I think those that don't understand something are in the best place to contribute documentation because they realise what there is to not understand. To an expert it's all obvious anyway. Granted that beginners have to at least gain a basic understanding before writing documentation, but I really don't think it takes much. As a case in point, my only contributions to the `lens` library were documentation that I submitted when I didn't understand how indexed traversals worked. I'm sure to Edward K it's all obvious, so he didn't bother writing it down, but once I managed to get a couple of examples to work I wrote it down to help others. https://github.com/tomjaguarpaw/lens/commit/3765fe5dc688d638142dc7ef639a08fed8d0d143 https://github.com/tomjaguarpaw/lens/commit/9e999a7e33d07e84fc5d61aba100b0a0d9715ffd 
That's fine if something only takes, say, 10-15 minutes to understand. (Of course, some people who grok thing 'A' after only a few minutes might not be able to grok thing 'B' after even hours of study, and vice versa.) What about situations where someone still doesn't understand after several hours of study? Should they not complain about the documentation until, after weeks of study, they suddenly reach a moment of satori, write a tutorial about how monads are warm fuzzy burritos, then submit that as a pull request? Do we really want to replicate Monad Tutorial Syndrome across Haskell documentation in general? &gt; Granted that beginners have to at least gain a basic understanding before writing documentation, but I really don't think it takes much. Indeed, it seems to me there's an important distinction to be made between situations of: (a) Someone who is comfortable with Haskell in general, trying to use a library lacking documentation, being able to muddle their way into starting to get it to work. In this context, it would indeed be useful for that person to try to write up what they've learned, so that others don't have to waste time as they had to. (This assumes they're not facing time or resource issues that limit their ability to do such write-ups.) (b) Someone who is still trying to get to grips with Haskell, trying to use a library lacking documentation. This person might well be unable to even get things working even after much (real or perceived) effort on their part, such that they don't have the knowledge to describe how to *make* things work.
reminds me of the "use case" trick from scala, that the official documentations don't show you the real type of functions, because it's "too complex" for the users. I think it's a big mistake. It WILL cause unexpected behaviours that'll confuse beginners even more. A type is a contract between the user and the compiler, let's not break it. edit: about scala use-case: http://stackoverflow.com/questions/4106817/scaladoc-use-case
&gt; sequence :: (Applicative f, Traversable t) =&gt; t (f a) -&gt; f (t a) sequence = sequenceA. It's exactly a class method of Traversable. What does anyone learn from this? A thing `t` is Traversable exactly if someone chose a function `(Applicative f, Traversable t) =&gt; t (f a) -&gt; f (t a)`, so just use that function, done! I guess you must be assuming traverse is the only class method of Traversable, and thinking of the definition `sequence = traverse id`. Since traverse and sequence are trivially equivalent given `Functor t`, I think of them as the same thing. The meat is in whichever one you actually implement in your Traversable instance, where you actually traverse over the structure. That code is really different for each `t`.
Take for instance Neil Mitchell's example of [Control.Arrow](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Control-Arrow.html). Perhaps the documentation should include some concrete examples of situations where arrows are useful, and code examples of how they would actually be used in those situations? (Control.Arrow is one example of u/chrisdoner's "module documentation page ... with a reference to a paper at the top".)
Feel free to try to revive the Haskell' committee. There is a mailing list, just nothing happening on it.
She already completed a full draft, and has been working on polishing it for the past couple of months. [Following](http://composition.al/blog/2014/08/18/dissertation-draft-readers-wanted/) the model of [Brent Yorgey](http://byorgey.wordpress.com/2014/08/10/readers-wanted/), the source code is [public](https://github.com/lkuper/dissertation), and a [PDF](http://www.cs.indiana.edu/~lkuper/papers/dissertation-draft-latest.pdf) is automatically generated after every commit. Pull requests, and suggestions via email (see the [blog post](http://composition.al/blog/2014/08/18/dissertation-draft-readers-wanted/)), are encouraged.
I agree that it reminds of use-case in Scala, the main difference being that there would be no implicit parameters. I don't argue that we should break the type (presenting an erroneous type), I argue that we could present the correct general type and some specializations to simplify for new users.
I think all the functions bodies starts just after "=". And if I am not mistaking these are the only things which can go after "=". (Sure you have to make some sanity checks.) And then you just need indentation of current line. Reading line by line till you'll find a line with less indentation, you will get function body.
100% supported. I've been struggling on doing my first real project due to this same lack of docs.
It's an abstraction that people think they understand, but don't. That's why they use the broken time libraries in most other languages and build broken programs. Haskell's time library is one of the first to get it right. That's why improving the documentation is so important - the library is much different and much better than what most people are used to.
Firstly, I don't agree with Neil here. I think the type signatures for `(***)` and `(&amp;&amp;&amp;)` make it perfectly clear what they do when specialised to `(-&gt;)`. Secondly, Neil is certainly capable enough of improving that documentation if he wants. It wouldn't have taken him much more time to have done so that it did to write the blog post! On the other hand I agree with you on "include some concrete examples of situations where arrows are useful". That's not necessarily clear to someone starting to learn about the concept.
You can turn on recursion with the pragma `{-# LANGUAGE Recursion #-}`, but we encourage people to use functions like `map`, `filter`, `until` etc (which we provide, of course).
Using haskell-src-exts or another Haskell parser is the only way to do it if you want to be 100% correct *and* not to lose your sanity. Don't forget that a Haskell module may use braces and semicolons instead of indentation, and so a function may start (and end) anywhere. Take a look at `Language.Haskell.Exts.Annotated` ‚Äî it should be fairly easy to achieve your goal with it.
The `transformers` library
I agree that `sequence` is not the best example to make that point.
&gt; Don't forget that a Haskell module may use braces and semicolons instead of indentation, and so a function may start (and end) anywhere. Indeed, and the requirement "given a line number" is also ambiguous, since multiple functions may be defined on the same line.
There is a lot of Haskell 98/2010 code out there, especially in books, course materials, and published papers. It will be around for a long time. It would be a shame if there is no longer any supported Haskell compiler that can compile it. I agree with Autin's feeling that it shouldn't be too hard to handle this for `do` notation. After all, we already support `-XRebindableSyntax`. I would say give it a try and see how it goes.
Is that really what it takes to submit a documentation patch to GHC? :-O
I agree with /u/apfelmus about that. I'll answer there.
OK, I admit, I knowingly used `sequence` as the example with an inaccuracy. On the first hour of day two, having been carefully guided into exploiting parametricity (not just by `sequence`), students write: replicateM :: Applicative f =&gt; Int -&gt; f a -&gt; f [a] They subsequently enjoy the benefits of having exploited types appropriately to derive a solution. We then move on to more rigorous examples. Currently, we just hide the mess that is the Prelude (absolutely all of it). I would really like to change that. Can I get the Prelude to a "day two beginner" level of accomplishment? What shall I ask my students to do in the future? Am I stuck with the current? 
Ah - I've jumped to a wrong conclusion? I really meant people know what time and dates are - not that they know other libraries in other languages - but if there really is something important I'm missing, all I can say is "whoops!". 
Conceptually, Unicode text really is a list of code points; the Unicode standard explicitly defines it as such. Sometimes you really do want to manipulate it as a flat list, without applying normalization rules, the bidi algorithm, or whatever. The fact is that to use text properly some understanding of Unicode is needed, there is no avoiding it. Functions in `Data.Text` whose names are the same as functions in `Data.List` should do the same thing. Both libraries should have prominent well documented clearly named functions that perform text-specific variations on those, such as "`normalizedReplace`", "`breakText`", "`reverseText`", etc. EDIT: Slight change in wording to make sure no one will mistakenly think I am implying that /u/snoyberg has any lack of understanding of Unicode.
It used to be, but the policy keeps changing, so it's now a case of "try and find the policy" first. It's certainly well more than a pull request. Also, I'm not sure what documentation I could write other than "pretend it has this type", and as documentation, that really sucks - if I should pretend it has a type, why not make it _actually_ have that type. I will be submitting some patches to the filepath library shortly, including improved documentation, so hopefully I'll be able to give the current guidelines after that.
Coming from /u/snoyberg, that is a more important comment than some people may realize. /u/snoyberg was the founder of a whole movement of using some really nice alternative Preludes. That is the right path - let's actually use those, and others, a lot more, and get some experience with them as a community. It is extremely premature to pick one particular alternative Prelude and canonize it.
I understood that he means "complex" in the sense of having more details that need to be sifted through, and requiring more steps of thinking to understand it.
FWIW, the Dijkstra monad is nothing more than the continuized state monad. It's well know that there is an isomorphism a -&gt; b ~ forall r. (b -&gt; r) -&gt; a -&gt; r and anyone paying close enough attention will notice that this is just flipped function composition: fwd :: (a -&gt; b) -&gt; forall r. (b -&gt; r) -&gt; a -&gt; r fwd f g x = g (f x) bwd :: (forall r. (b -&gt; r) -&gt; a -&gt; r) -&gt; a -&gt; b bwd h = h id (Other ken observers will then naturally yell "Yoneda!") The forward direction is also know as continuization, wherein you turn a function into a continuation, where instead of transforming an `a` into a `b`, it transforms a `b` continuation into an `a` continuation. This is the basis of van Laarhoeven lenses, incidentally. The Dijkstra monad, defined as data Dijkstra s a = Dijkstra { runDijkstra :: forall r. ((a,s) -&gt; r) -&gt; s -&gt; r } is obviously just a special case of this more general continuized form of functions, when applied to state transformers. The monadicity of `Dijkstra s` is then completely trivial, since `Dijkstra a` is equivalent to `State s`, a well-established monad. We could readily define the `Dijkstra s` monad instance in terms of the state monad instance because of the isomorphism.
&gt; I'm not sure what documentation I could write other than "pretend it has this type" Well, not exactly "pretend it has this type" but "amongst other specialisations, it has this type". I would find that *really* helpful. [`Lens` does it a lot](http://hackage.haskell.org/package/lens-4.4.0.2/docs/Control-Lens-Fold.html#g:2) and I find it makes things clearer. &gt; I will be submitting some patches to the filepath library shortly, including improved documentation, so hopefully I'll be able to give the current guidelines after that. That would be helpful. I'm actually keen to attempt to improve the documentation for `Control.Monad` now ...
I started writing a blog article on this topic but haven't had time to finish it. But here's a synopsis starting with a a quote that's relevant :-) *"It is not possible to understand any article about monads until you have read a dozen other articles about monads"* So after I finally figured out what was going on (and with thanks to several people in the community for their incredible patience), I sat back for a while and thought about what would have made it easier for me to get up to speed. The biggest problem I encountered was the rather sloppy use of terminology and even some bad naming in places. For example, if **return** was called **wrap** it would have been much more obvious to me what was happening. For another example, the conventional use of x : xs was very puzzling for some time. Had this been written as head : tail the meaning of all of the introductory examples would have been immediately obvious. (yes, I know those identifiers are taken) For a third example, I saw a lot of references to monadic types and this threw me off completely because I couldn't figure out how a monadic type was different from an integer type or a record type etc. As I started to figure out what was going on, that led me to my initial (not quite correct) concept of what constituted a monad: "If you have a data type and some functions whose definitions and implementations satisfy a particular list of rules, then you have a monadic type." I then evolved this to say: "If you have a data type and a couple of functions whose definitions and implementations satisfy a particular list of rules, then you have an instance of a class called Monad." The second thing that confused me terribly was the disconnect between function signatures (that contain only types) and function definitions (that contain only parameters). When you see f :: a -&gt; b -&gt; c and then later f x y z = .... it takes much longer to figure out what's going on than if they were written as f :: aType -&gt; bType -&gt; cType and f aVal bVal cVal = .... For people already familiar with programming languages, the latter approach (**at the introductory level**) would have made it immediately clear exactly what was going on. The third issue (and it's related to the first two) that threw me is related to what it really means to consider functions as first class objects. It took me ages to understand that you could (essentially) pass add x y somewhere and then refer to that 'add' completely separately from the x and y. THAT was why I was struggling to understand how the bind operator worked. There were a few other things but the above were the main impediments that made it take me much longer than it should have to get up to speed.
I'll check it out, thanks.
Yes, I absolutely agree with all of this! Good documentation is very important. My point of view is that once she has finally understood the code, the parabolic programmer is in an equally good position as the original author to write documentation on that issue that she previously did not understand. Furthermore, since Haskell is a volunteer effort, she has an equal burden of responsibility. 
I've contributed docs to [Data.Aeson](http://hackage.haskell.org/package/aeson-0.6.2.1/docs/Data-Aeson.html#g:1), but that was rather straight-forward. I downloaded the Git repo, submitted a PR and Bryan merged it. Has anyone contributed docs to `base` before? And can they make a write-up about doing so? I think that might motivate people to feel like it's something they can achieve in some spare slots of time. Personally speaking, I see base as an opaque library set. I don't really think of it as something I can straight-forwardly change. Maybe others carry the same perception.
These are great insights. I heartily encourage you to finish your blog post!
Missing off an import MyPrelude seems like it would be a very easy thing to do and would confuse people a lot. It would also mean only having one beginner prelude otherwise things would get confusing if you changed tutorials and the one beginners use would be a subjective choice on behalf of the author. 
&gt; My point of view is that once she has finally understood the code, the parabolic programmer is in an equally good position as the original author to write documentation on that issue that she previously did not understand. Furthermore, since Haskell is a volunteer effort, she has an equal burden of responsibility. Which i very much agree with in return. :-)
I took a look at: https://ghc.haskell.org/trac/ghc/wiki/WorkingConventions/Git There is an option to make a patch and submit it to Phabricator. What would happen if you just submitted the documentation improvement to Phabricator and it builds fine?
Of course some people make the leap but I don't think it's many. This could reduce the number. Beginners may learn about typeclasses fairly early on but I doubt they truly understand them for a lot longer. 
Let me know if/when you finish your blog post!
I have a suggestion about how experts could also contribute. When we help a beginner understand a particular library, perhaps we should take the habit of asking them a favour afterwards: "If you found our explanations helpful, please take a few minutes to write down a paragraph of documentation which would have been clear to you. Due to the curse of knowledge, we cannot write it ourselves."
&gt; Missing off an import MyPrelude seems like it would be a very easy thing to do and would confuse people a lot. If your tutorial tells you to use import Prelude() import Prelude.Tutorial and explains that it's to switch the language into "easy mode", to enable error messages to be more helpful to people not yet used to them, then, if you do not do that and/or are confused by it... well, I don't think you'd be able to finish the tutorial in any case. &gt; It would also mean only having one beginner prelude Well, one or two, maybe. One without typclasses (or nearly so), less numeric types etc, one a little bit more advanced, then the default one. I'm quite sure we'll be able to come up with sane ones, they don't have to be all-encompassing, after all. You're not supposed to write actual programs in them, you're supposed to do exercises in them. Once you've drunken the tea, cast away the cup. And if standardising doesn't work out... well, that's not that bad, either. Tutorials can start with `cabal install fancy-tutorial`, then.
Consider foo x | x == 0 = "zero" | otherwise = "blah" Two "=", one function
TBH, what you describe above mostly isn't any big deal. I once wrote a C++ library around similar models just as a learning exercise, and although it only dealt with dates, I included conversions between a couple of different day-number forms. After all, that's mostly just knowing the right constant offset to add. Haskell has a nicer type system, but it's not the first time someone thought of using types to keep track of units, differences as distinct from values etc. Back when I worked in Ada in the mid-to-late 90's, we did a lot of that - yes, for time too. Sorry, but I'm just not impressed by a bit of date/time arithmetic. Working with something that counts in 1/300ths of a second is only a constant factor different from working in something that counts in seconds, or something that counts in days. Calculating `300*60*60*24` really doesn't worry me. The more interesting bit is getting from day-numbers to years-and-days or years-months-and-days and back - and even that's something you should look up rather than figure it out yourself. Not that it's that hard, but it's easy to get it wrong. However, I *have* had a look around the library now, and there appears to be support for leap seconds. That's a nice surprise. 
&gt; monads are often considered monoids in they're own right They're not monoids in that a monad isn't a set of values, a binary associative operator and an identity element. If I understand it right, a monad is a monoid object in the monoidal category of endofunctors. An endofunctor is a functor that starts and ends in the same category, and the Functor typeclass is an example of endofunctors in 'Hask'. A monoidal category is a category equipped with an operation, ‚äó : C √ó C ‚Üí C, and an identity object (up to isomorphism). Since we're dealing with the category of Functors, we can define this as functor composition: newtype Tensor f g a = Tensor (f (g a)) instance (Functor f, Functor g) =&gt; Functor (Tensor f g) where fmap :: (a -&gt; b) -&gt; Functor f g a -&gt; Functor f g b fmap = fmap . fmap The Identity functor provides a fairly simple identity element, since Tensor Identity f a is isomorphic to f a is isomorphic to Tensor f Identity a. Next, a monoid object in a monoidal category is * one particular object, m. Since we're dealing with the category of endofunctors, this is some concrete type f with a Functor instance. * a function, mu, from Tensor m m a to m a * and a function, eta, from Identity a to m a. However, -- mu is like mappend, in a "normal" monoid mu :: Tensor m m a -&gt; m a mu (Tensor mma) = join mma -- eta is your way to inject your already existing identity element from your monoidal category into this monoidal object eta :: Identity a -&gt; m a eta (Identity a) = return a So, categorically, a monad in Haskell is just a particular functor with join and return. If you go through all of that with Set as your category, instead of Functors in Hask, and you take cartesian product as ‚äó and the empty set as the identity, then the normal abstract algebraic monoid appears as the monoid object of that category.
Certainly shouldn't need to build GHC for a documentation patch. GHC core wants code ptaches submitted top phabricatpor these days, instead of the old trac submit. When in doubt, send stuff to ghc-devs list (or in this case, probably the libraries mailing list)
&gt; Sorry, but I'm just not impressed by a bit of date/time arithmetic. You're forgetting about the existence of leap seconds.
Agreed. In fact, please post it here!
&gt; I found it annoying having to learn "beginner haskell" and then learn "actual haskell" when I could have just started out learning the real deal. this! ...and for what it's worth, Haskell's support for abstract things like `Functor`, `Monoid` as well as `Foldable`, and having those baked into its standard libraries is part of what lured me to learn Haskell in the first place. Otherwise I would have just stayed with other "less generalised" type-inferred languages.
Or at least the latest report (which is now Haskell2010)
You didn't read the last paragraph did you? That's the main reason I said "mostly" in the first paragraph. In principle, even dealing with leap seconds doesn't sound massively complex to me - you get the table of details and you apply it. You probably need to calculate days-with-leap-seconds in a range - maybe you want a data-structure to efficently determine the number of keys in a range (an augmented binary tree). But even if this was super-complex, we're talking about documentation for using a library here, not implementing it yourself. 
&gt; You didn't read the last paragraph did you? Looks like I didn't!
In what sense is the routing typesafe?
I was beginning to write an answer to Neil too ... but haven't finished before leaving for an extended weekend. So I'll just wrap it up here: The basic problem seems to be a lack of documentation of type class instances. This especially the case for modules that don't export specialized versions of `Traversable` and co (from the top of my head `Data.Sequence` is such a case). Currently this looks like this: Instances: Foldable Seq [‚Ä¶] I believe much could be gained by changing this to something like: Instances: Foldable Seq: foldl, foldr, foldMap, etc Traversable Seq: traverse, mapM, etc Where each line is extendable to show the specialized versions: Instances: Foldable Seq: foldl: (a -&gt; b -&gt; a) -&gt; a -&gt; Seq b -&gt; a etc Traversable Seq: traverse, mapM, etc Mouse over could probably display the generalized functions. This would require to extend haddock to generate this documentation, but no additional effort by the library developers. Benefit for the beginner: documentation on one datatype is available from a centralized source. "I want to use `Data.List`!" -&gt; lookup haddock page -&gt; happy beginner. No real problem for advanced users. Doesn't even use more vertical space on the haddock page. Even they might benefit from having more information ready at hand. Bonus points for coming up with generalized examples than can be generated for each instance.
I wasn't trying to impress you that a library tracks units, but demonstrate that there're a lot of things that need documenting because how to achieve some task is often a matter of hunting through the API. It's not just "date and time", as you simplified it, and that makes a sparsely documented API less than smooth to work with.
&gt; However, is it really reasonble to expect your average programmer to derive immediately what the instance for, say, Either a is going to be? Yes. I mean, they will have to first learn what Traversable is, but one they understand it, then yes, it is immediately obvious what the instance for Either a is going to be. And yes, I think the average programmer can and should learn what a Traversable is. I think Traversable/Traversals are the most important development in functional programming in the last decade. Unfortunately you won't learn what a Traversable is by reading its associated documentation. You instead have to learn that a Traversable is essentially the same thing as a finitary container, that is every value decomposes into a list of values and the remainder (called the shape). There don't seem to be any good tutorials on finitary containers that Google comes up with. :(
&gt; we also can't stay with the status quo either! Obviously we *can* since we've pretty much stayed with the status quo for the past 16 years. I believe the only changes to Prelude in that time were removing the superclasses of Num and removing Prelude.catch. Changing Prelude for the purpose of changing Prelude is unprecedented (and for good reason IMO). I fail to see a substantial issue here that requires this Foldable/Traversable change. Are there pain points associated with importing generalized versions of Prelude functions or with using custom Preludes in general? OK, then let's fix those! Why is the fix to force a new Prelude on everyone which, obviously, not everyone wants?
What's the use-case?
The types and functions are already documented. The initial description could use some examples and a quick tour of the most important concepts, types and functions, so I *was* stupid to suggest otherwise. With that in mind, maybe my punishment should be to write that. I have a delivery of turf due any minute now, so I'll have to find out how later. 
OK, with this example I understand better what you are trying to do. But I don't understand how the Prelude stands in your way, or how the change under discussion will help you. For starters, Prelude doesn't export replicateM anyways. But let's suppose it did. Would it help you if it had the type `Applicative f =&gt; Int -&gt; f a -&gt; f [a]` rather than `Monad m =&gt; Int -&gt; m a -&gt; m [a]`? Is that the issue here? If you're hiding the whole Prelude anyways then it doesn't matter from a technical standpoint what the type is, right? If that were the only function with the "wrong" type, would you hide the Prelude anyways if it was fixed? Maybe you'd still want to hide the Prelude, so that students can write their own `replicateM`? I totally understand having students write `replicateM :: Applicative f =&gt; Int -&gt; f a -&gt; f [a]` and then wanting to be able to point to `Control.Monad.replicateM` and say "Look, here is the function you just wrote, now you can go use it in any Haskell program" without having to explain why the context says Monad rather than Applicative. I just want to understand whether that's the only issue here. In any case, this example of `replicateM` has to do with the Applicative-Monad Proposal, not incorporating Foldable and Traversable into Prelude. I think this change may already be in the works, separately from the change that this entire thread is discussing. After all there is a big difference between replacing a constraint by a weaker one and introducing an entirely new dimension of overloading!
&gt; I think changing types of things in the Prelude deserves a Haskell2014! Heh, good luck with that. There is not even any informal spec of what the changes we are talking about in this ticket actually *are*. I've been asking for a spec for two weeks and have gotten answers ranging from "generalize the types of standard library functions to Foldable/Traversable where possible" (definitely not true, as Prelude now exports a large number of new names from Foldable/Traversable) to "we'll get back to you on that" (I am still waiting). These changes keep trickling into GHC HEAD and there's no indication at all of when they will stop or where the scope of the changes ends.
I have many of the skills listed, more slanted towards the automation, ubuntu, and AWS side of things. Could you expound on the descriptions of: * Large-scale distributed computing/HPC * High-availability architecture
Thanks to /u/dstcruz!
Traversable can only traverse the last element in the triple. Make that a data type, and sure, we can walk the elements in any order, but by convention it goes left to right through the ADT
Correct, all parameters from the route are passed to the handler as correctly typed parameters. 
&gt; For example (a, a, a) has 6 Traversable instances. Not in Haskell. If you use a newtype wrapper, it is a new type. As such, `(,,) a a` only has one Traversable instance (for each `a`). It's always got exactly one "hole" so there's only one way to traverse it. I suppose you might be talking about `newtype HomoTup3 a = HT (a, a, a)`. That *does* have 6 Traversable instances, but is a bit of an anomaly (a.k.a. counter-example). &gt; Why two? IIRC, there are *some* ordering constraints that are implied by the Traversable laws, but you can always reverse the traversal order and still follow the laws -- so, unless reversing the order of traversal gives the same instance (0 or 1 "hole" instances) you can always get two Traversable instances.
Your plan sounds good. Haskell web frameworks are not so much frameworks, they're more like libraries. Just use one of the larger ones: happstack, snap or warp, and use your choice of libraries for rendering your reports.
In that case though you have to develop a full understanding of what you were trying to understand without any help from the documentation and then you have to go write the documentation. I think that is more than a basic understanding really.
&gt; I suppose you might be talking about `newtype HomoTup3 a = HT (a, a, a)`. That does have 6 Traversable instances, but is a bit of an anomaly (a.k.a. counter-example). Of course, that's what I meant. I don't understand why you think that's an anomaly. It's just a minimal counterexample. For example, the list functor `[]` has infinitely many traversable instances. There is no law that relates the traversal ordering for lists of different length. In general, the number of traversable instances for a finitary container with shapes `S` and `p_i` positions in shape `i` is: `Œ† (i : S). factorial p_i`. 
...and there are *two* separate bodies.
&gt; We shouldn't be making backwards compatibility an "opt-in" concept But, it *is* opt-in. It's a feature and many people aren't interested in it, as it is more a a long-term maintenance feature and they are not ready to "settle" the project, if ever. They want to experiment willy-nilly, and when they release a new version, the old version might as well be sand-blasted off every digital surface for all the support they will be giving it. We shouldn't even try and force backwards compatibility on people that don't want it; if we do, and there is a social technique to avoiding the technical controls we've put in place, they will find it and use it. Hackage *is* a "wild west" sort of repository, a.l.a. CTAN, CPAN, RebyGems, and CRAN.
There's really just one thing that comes to mind as a pain point for me and the hackage docs. With each function, there's a little link on the side that says "Source" that takes me to the code for that function. Even if the function is documented well, sometimes looking at the source is still useful. However, with class instances, all I get are a list of "Monoid Record" "Show Record" etc. None of them have *any* documentation. I know that I can show a Record to turn it into a String, but I don't have any information about that String. What does it look like? How many digits of precision are used. If it's some kind of date record, does the string look like "MyDate "March" 15 1980," "03/15/1980," "1980-03-15" or any of a dozen other formats? Now I'm on a hunt to find the actual code for the Show Record instance. I can click on 'Show' and go to the Show typeclass definition, and I can click on Record to go to the Record definition. But as far as I know, there's no link that takes me to the Show Record instance code. Usually it's in the same file as the Record itself, but other times the Record is buried in Module.Whatever.Internal.Date.Types and now I know I'll have to search for an appreciable amount of time to find my answer. It's pretty annoying. Maybe I'm missing something obvious, I don't know. Edit: After reading this over, I want to stress that I prefer having documented typeclass instances, but failing that, a link to the code would be useful.
I think that a new haskell2014 report is the most obvious solution. Silently changing the behavior of previous reports' packages seems pretty undesirable, and "detecting" AMP/non-AMP for do syntax, even if doable, doesn't seem like the right thing either. If something is change that the previous reports don't cover, it sounds like a new report is needed!
I agree 100% and added my own comment along these lines, only to find yours a few minutes later. :)
There's room for both though. It could be something like: Using traverse (Traversable t, Applicative f) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) as traverse ([], Identity) =&gt; (a -&gt; Identity b) -&gt; [a] -&gt; Identity [b] in... And then it displays the specific signature for the rest of the message. (And perhaps ghc could remove Identity from the signature to make it more clear). Seems a more useful and useable solution than either on their own.
And hunting through the API isn't just the type signatures. Some of the most important and common ways of using the API are documented only by the existence of certain type class instances for certain types - such as `RealFrac` and `Integral` instances.
The use case is a vim text object plugin to select haskell functions. 
I guess it depends what you mean by "write documentation". That description covers everything from one line code snippets to whole books. The indexed traversal documentation I wrote was only a few snippets of code but I'm sure it's helped anyone who was in the same position I was before I wrote them, and it only took a basic understanding. 
I'm not as worried about Hoogle and GHCi as I am about the code itself. When a function uses lots of very polymorphic functions in its implementation, it can be very difficult to understand what the function is really doing just by looking at its source code. Sometimes, to get the concrete type of an intermediate expression which might be the whole key to understanding the function, you need to go on a wild goose chase searching across many disparate modules, dependent libraries, and indirect dependencies, before you finally get your answer. Tool support may help with this problem. But I believe that code should be written to be clear and readable on its own. When some popular OO languages reached the point that code was literally unreadable outside of the IDE, that became part of what drove me to become a full-time Haskell programmer.
I would highly recommend Yesod. This framework is built on the warp web-server, and has loads of great features. 
So in this example passing in a handler lambda which expected 2 arguments would result in a compiler error unless you used `var` twice?
For one, I don't have any PhD, and yet the Foldable/Traversable concepts seem rather natural to me. And calling the beginner prelude `Prelude` and the proper one `Prelude.GimmeTheRealOne` wouldn't address my argument to bias the defaults towards advanced users (as the beginner `Prelude` would still remain the default one), which spend the rest of their lives being advanced Haskell programmers.
I was just saying in the Traversable/Foldable thread that I'm still unclear on if there's any semantic difference between traverse/mapM, or if it's just a result of the monad-applicative issue. I was told they are different w.r.t. bottom, but the code defines one in terms of the other, and the documentation doesn't give any explanation.
Lazy I/O refers to unsafeInterleaveIO which is different from unsafePerformIO.
Yes, all a bit unfortunate right now. FTR, here's sort of what's going on at a very high level right now to perhaps address some of these points, and in the future for people reading. - Most of the libraries that were previously under GHC HQ control are moving to GitHub. About 90% of them. That's because they're submodules, and we don't need to control them anymore. Or someone else owned them already. Most see very little development, and improvements would be really great! A lot are on https://github.com/haskell, and more (the final ones) will be moving soon. - Most of these libraries fall down to independent maintainers, Edward/Libraries committee, or some GHC developers still. But it's still a package you can improve! - The only libraries still coupled with GHC as a repository are `base`, `template-haskell`, `ghc-prim`, and the `integer-*` libraries. - They're unfortunately tied to GHC to the point lockstep development is really important to help us develop reliably. - But we don't want `base` that big either. There is still an ongoing effort to modularize `base` and make it much smaller, so this point doesn't hurt us so much. - Joachim and Luite are, AFAIK, the ones working on this. Note that while most of the libraries may still be *shipped* with GHC, causing some complications, it is perfectly acceptable to upload new versions and update their documentation on Hackage inbetween releases, which I believe is quite feasible and realistic. (In the future, the new Cabal/Backpack work by Duncan, Edward &amp; Co will hopefully alleviate multiple versioning problems.) I think stripping `base` could help a lot, but it's a delicate issue. Perhaps we should change a policy to upload documentation improvements to packages, as a minor update on the latest 'stable' version. Hmm... I also wonder, perhaps, if GHC should make a policy of trying to backport documentation changes into the stable branch for future releases. Hmm... Finally, I also don't think any library proposals are needed for just a documentation patch to `base` or anything, people submit improvements very regularly to [Phabricator](https://ghc.haskell.org/trac/ghc/wiki/Phabricator) these days, and it seems to work well. I hope my review latency isn't too high for you :) But those patches tend to get put in the 'accepted' pile quite easily. We also have nightly buildbots that build docs, but we could maybe get them working for patches. As it stands, you don't necessarily need to build things you submit to Phab, it will test them. But you always want to see the docs... Anyway, sorry if it sounds insurmountable. I think it's solvable perhaps with some minor changes on our side, and some ongoing improvements.
Well there certainly are other ways to go about things, but unless someone with some clout starts proposing them they're not going to happen. One thing I was thinking that would be nice would be another Prelude included in base, but in a different namespace (Prelude.WithJustATouchOfPolish). It would have a few benefits: Beginners who find they've quickly outgrown the Prelude wouldn't need to go searching through all the alternatives without knowing enough to make an informed decision. For a while I was using a prelude that redefined Monad to fix its issues, because I didn't yet know that that would cause all sorts of issues when using other people's code. Library writers could use it without incurring additional dependencies, and it could be kept close enough to the standard Prelude to either serve as a staging ground for any changes that eventually might move to the default, or the default could just be deprecated in real world code if that's the way the community moves. Haskellers can still have a reasonable common language that we can be assured is standard. I think this is the real important one, which limits choose-another-prelude as an acceptable alternative. Defaults matter, and punting choosing what the defaults for the language should be on users is a terrible status quo. On some level, I kind of wish classy-prelude was what we were all working with, but I wouldn't propose it as an alternative. In practice my main annoyance with the Prelude has been having to keep a mental list of what I might need to avoid/hide from which module if I'm using generalized functions. Even if modules were a little more programmatic, and I could do something like import Data.Traversable clobbering (Prelude) to implicitly hide functions with the same name and less general type signatures, or something avoid having to import from a module twice (once to get the datatype unqualified, once to get functions qualified). The whole module abstraction is too impoverished, and changing the Prelude itself is sort of the easiest way to avoid having to fix modules, which is a probably the underlying issue I think, but a much bigger undertaking.
&gt; I argue that we could present the correct general type **and** some specializations to simplify for new users. That is the most important word. We should show the contract / real type first **and** show the "most common" or "expected" specializations. Depending on the best UX, we might "hide" the specializations until after some user interaction, but we'd never hide the *real* type of the function. I think the problem with Scala's use-cases is mainly in how they are presented.
[Did you happen to read the comment I had made on the original post?](http://www.reddit.com/r/haskell/comments/2hzqii/z/cky1khg) 
Good quick overview, and a really cool application of monoids. I don't have much to add besides a few more references if you're interested in learning more... I first saw this described by /u/apfelmus here: http://apfelmus.nfshost.com/articles/monoid-fingertree.html I read through the finger tree paper and have an IHaskell notebook tutorial about how finger trees work (and the monoidal tags, too) here: http://andrew.gibiansky.com/blog/haskell/finger-trees/ The paper of course describes this much more succinctly. I highly recommend reading it, it's clear and well-written. You can find it here: http://www.soi.city.ac.uk/~ross/papers/FingerTree.html
Cute! Should have the same asymptotics as my `evaluate`... You're doing a similar trick to the one I pulled in the definition of `evaluateF`, where you slide a `flip id` inside the comonad to do something like the `ComonadApply` equivalent of `flip`. See what I mean? evaluateF :: (ComonadApply w, Functor f) =&gt; w (f (w (f a) -&gt; a)) -&gt; w (f a) evaluateF fs = fix $ (&lt;@&gt; fs) . fmap (fmap . flip ($)) . duplicate
Indeed, well put.
There is no semantic difference between `traverse` and `mapM`. I strongly suspect that this difference is the result of the monad-applicative issue. However, IIRC there are occasionally situtations where the implemenation of `mapM` can be made more efficent than the implementation of `traverse`, so there is a little bit of justification for keeping both methods around. I would recommend using `traverse` for everything unless you have a compelling reason to use `mapM`.
Hmmm, I believe this is a different kind of safety than the one that Yesod and Happstack mean when they talk about type-safe routes. My understanding is that they have a data structure defining all the site's routes and then they use that to generate their routing table as well as generate links. If you build your whole site this way, then you shouldn't ever have any broken link bugs because links and routes are DRY.
Eventually some maintainer looks at it and merges it with the dev branch. The correct way to go about things when it comes to small fixes is to just send it to Phabricator.
You're absolutely right, of course. My bad. I had considered only the laws and not the type signature, which is a rather dumb error.
&gt; Why can't the author just place a few judicious type signatures, or indeed provide their own local specialised versions of polymorphic combinators? Because that's more work and contrived consideration than simply using the existing monomorphic `mapM`.
I'd also recommend Yesod. Something leaner, that still builds on the excellent Warp/conduit libs, would be Scotty and Spock.
&gt; On some level, I kind of wish classy-prelude was what we were all working with, but I wouldn't propose it as an alternative. This sounds conflicting, how can you wish the one thing without considering it to be proposed as an alternative? (IMHO, `classy-prelude` would be even more controversial than the currently debated Foldable/Traversable-prelude, due to the law-less typeclasses it promotes)
I observed the same; Spock's type safe routes are different from Yesod's. &gt; If you build your whole site this way, then you shouldn't ever have any broken link bugs As far as I understand type safe link construction cannot prevent _all_ broken links, just some fairly common categories of them.
Question: did Spock devs look at --for instance-- Yesod's router? (BTW they just vastly improved it) Maybe template-haskell was the reason to choose against it, I can imagine. Question 2: are you also interested in solving link construction with type safe routes?
I'm guessing you're looking for more than the textbook definition of these criterion. We're not only managing our own cluster systems (e.g., fpcomplete.com), but also advising clients on how to do this for their own services, and in some cases, actually providing those capabilities. We're looking for someone to join the team who not only has the capability to maintain an existing architecture, but to review what's there, propose improvements, and implement them. Even better is someone who can talk to a customer, understand their needs, and help them design something solid. To make that more concrete to the points you highlighted, a good demonstration would be, "Oh, I see you're trying to run a web service. That web services has X uptime requirements, Y resource requirements, and can have its computation distributed across other nodes in a certain manner. I'd recommend setting up your network in this manner, providing failover by using this technology, etc." I hope that makes it a bit more concrete.
You don't appear to use `mempty`, outside of your `search`. Even there you have to introduce a `Maybe` wrapper, and `Nothing` could relatively easily be substituted where you used `empty`. Also, while you do show an example of a place where associativity is used, there's good reasons those trees might have different measures (for example, "depth"), so it doesn't make sense to me that you should make it a requirement of all measures. Because of that, I think having a measure be a [`Magma`](https://hackage.haskell.org/package/magma-0.2.2.0/docs/Data-Magma.html#t:Magma) is a simpler and more powerful abstraction than having a measure (always) be a [`Monoid`](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-Monoid.html#t:Monoid) Even in the case where you need an identity, perhaps a loop or just unital magma makes more sense.
I don't disagree with you on the status quo. I'm saying that this mentality is harmful to Haskell's success in industry. Someone testing out a new feature in GHC 7.8 with a package marked experimental? By all means, go for it. But if you're putting packages out there that you want to tell people to use in earnest, it's *vital* to start thinking about stability. Telling people to fix a bug by upgrading to your new version, with a breaking API change, that requires upgrading seven other packages as well, is simply not acceptable. In this context, I'm not saying we should force backwards compatibility on people; there's no way to do that. But we should make it as easy and as obvious as possible to do so.
Yesod provides a similar routing system as an alternative to the main routing system, called [LiteApp](http://haddocks.fpcomplete.com/fp/7.8/20140916-162/yesod-core/Yesod-Core.html#t:LiteApp), which is also [discussed in the book](http://www.yesodweb.com/book/yesod-for-haskellers#yesod-for-haskellers_liteapp). I think it gives a little more static safety than the current Scotty approach, though I haven't analyzed it in detail yet. It may be worth looking into.
Yep, you're right. This addresses some of the use cases of "type safe URLs", but not all of them.
Maybe there are only 17 imports because imports are painful enough that people just wrote things in a suboptimal way? For example, I often wonder whether to traverse an IO action on my Maybe value or just do ugly, repetitive case analysis so I don't need to do the import dance.
This looks awesome. We have been using Spock for the last 6 months (most recently for [findmelike](http://findmelike.com) ) and it is really great to work with. This is really liberating, when coming from Template Haskell-heavy frameworks. Everything here is just Haskell. One of the really great features of this (and scotty) rarely mentioned is that you can define components as functions running in the SpockT monad (which is in MonadIO) and then define routes based on that work and the setup. This means you can *calculate* routes. This should still work with the new type-safe routing design.
hindent and structured-haskell-mode use this: https://github.com/chrisdone/hindent/blob/master/elisp/hindent.el#L74..L125 It works 99% of the time for me.
Have you guys considered snap? I don't think that it relies on any TH.
It has a scaffolding tool that generates extensive TH for the main function. I don't care if the TH isn't anywhere else or does something super clever. I want to be able to look at the main of my program and know what is going on. In fact I don't want anything that needs a scaffolding code generator - I consider them pathological promoters of code bloat and poor library design. Hello world in Spock is 4 lines of code. No scaffolding required for that. Add a database connection to postgres in another 3 lines. On the other hand, I'd love to work with io-streams, again for its simplicity. 
You need associativity when searching, to reassociate `measure a1 &lt;&gt; ... &lt;&gt; measure ak` according to the structure of the tree (which is some arbitrary tree structure with balance constraints, not part of the semantics of the data structure). You could build a tree with measures where the measures are from a magma, but it wouldn't have the same useful ability to perform searches in general.
 newtype Depth = D Nat Magma Depth D n &lt;&gt; D m = D (n + m + 1) If I use this as a measure, I do *not* want `B (B (B L L) L) L` to have the same measure as `B (B L L) (B L L)`, but it's a perfectly good cumulative measure, it just isn't an associative (or unital) one.
I now have a ticket to do the filepath changes I want to do, https://github.com/ndmitchell/shake/issues/174, but then I have 1000's of tickets...
Agreed, and I'm not totally against fixing String to become Text, if someone put sufficient thought into how to get that to happen.
Polishing the Prelude is not the same as burning the bridges (which is the name of the proposal that is currently being inflicted on my beloved Prelude). 
I think the Applicative-Monad proposal was done carefully, sensibly, and with a good period of overlap. I'm very impressed with the people who did it. Sure, they forgot this minor detail, but as soon as they spotted it they fessed up and asked for guidance. Beers all round. The burn all the bridges proposal is quite different...
Alan, have you considered writing something equally simple on top of the GHC API? It might show up the pain points, and if it doesn't, it would make an excellent tutorial and be practically useful.
[**@DeTreville**](https://twitter.com/DeTreville): &gt;[2014-10-02 21:01:26 UTC](https://twitter.com/DeTreville/status/517781424653348866) &gt;New IOMONAD license plate, because my car lets me go out and do things. [*pic.twitter.com*](http://pbs.twimg.com/media/By-HdQvCQAEIHTx.jpg) [^[Imgur]](http://i.imgur.com/gYJnfP1.jpg) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/2i4qro%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
&gt; I'm saying that this mentality is harmful to Haskell's success in industry. I agree; pulling packages from hackage is just as bad as puling them from CPAN is many ways and both are *no way to operate in industry*. &gt; But if you're putting packages out there that you want to tell people to use in earnest, it's vital to start thinking about stability. Telling people to fix a bug by upgrading to your new version, with a breaking API change, that requires upgrading seven other packages as well, is simply not acceptable. When I do not have my developer / experimenter hat on, I don't pull from CPAN/Hackage. I use the Debian packages. For example, I don't touch XMonad on hackage with a 12' pole; I need it to work, and I don't need to have to change my configuration until the next Debian release (or until I want to). If I want to write Perl scripts for our build system (or whatever you feel in a justifiable use of Perl), I either use the distribution packaged Perl and libraries, or if I'm on MS Windows limit myself to what ActiveState ships. For stability I look for currated sources, not the wild west. But there's plenty of time when I'm doing Haskell, (most of the times, since I don't get played for it, yet) where I do want to experiment with the wild west, and I want it to continue to be available is very minimal restrictions. That said, we do need to make the lives of the maintainers of curated sources easier. Debian does not recommend working on a new package unless *that version* is expected to get upstream support for the entire time stable is supported.[1] It's fairly difficult to maintain most packages without help from the upstream authors, so either you need to get buy-in from them, or you have to be willing to skill-up and put in the time to effectively to the maintenance of older versions yourself. [1] For Sid, since it automatically feeds into testing which will be the next stable. If you are a DD (or can get a sponsor), experimental is a great place to start working on the Debian packaging while waiting for a "blessed" / "LTS" version from upstream.
Which is one reason I had the less accurate definition first. The point is to get across the idea that a monad is NOT a type but rather **just** a name for a type plus some required functions that must satisfy some rules. This is not too hard to understand (I think) if you have already met Eq, for example, which is also just a name for a type plus some required functions.
If the desired association of `measure a1 &lt;&gt; .. &lt;&gt; measure ak` matches the tree structure, you don't need to know reassociation is valid at all, you'll automatically get that association via recursive descent. It's only when you want `measure a1 &lt;&gt; (measure a2 &lt;&gt; measure a3)` to match the measure of `B (B a1 a2) a3` (i.e. association that disagrees with the tree structure) that you need to know the operation is associative.
How does Warp compare to snap-server? Both of them claim to be fast, but what about security? Would you trust them with a public-facing website?
Perhaps we could write a new report with the Prelude changes in it!
GA Tech offers a Online Masters of Computer Science (OMSCS) that does not require taking the GRE and applications for the Spring '15 term should be opening this month, I think. I'm trying it this semester, with the eventual goal of a MS in 3 or so years and then finding a Ph.D. program where I can work in/on a dependently typed language. So... it'll be a while for me, too.
It could be worthwhile, but I am focusing on the annotations for GHC first, I want to make sure it makes GHC 7.10. To me the logical place to expose this would be via ghc-mod, since it already deals with all the tricky stuff of working out the context in terms of cabal files, directories, sandboxes etc, and provides a GHC session.
&gt; If the desired association of measure a1 &lt;&gt; .. &lt;&gt; measure ak matches the tree structure, ... Okay but from the client's point of view, the tree structure is unknown, so how could the client know that the desired association of `measure a1 &lt;&gt; .. &lt;&gt; measure ak` matches the tree structure unless every association of `measure a1 &lt;&gt; .. &lt;&gt; measure ak` is equal? i.e. the operation is associative. Admittedly for a particular predicate `p` you only need associativity to hold "up to `p`", i.e., ~~p ((a1 &lt;&gt; a2) &lt;&gt; a3) = p (a1 &lt;&gt; (a2 &lt;&gt; a3))~~ `p` has the same value on any reassociation of a tree of `&lt;&gt;`s (it's not sufficient to check this for three variables). Maybe you could cook up useful examples of this, I don't know.
Then the "search" operations associated to this measure are something like "find the smallest k such that the first k elements of the sequence span a subtree of height at least h", which is meaningless since it is not invariant under changes to the tree structure. Remember the goal is to be able to do these searches in O(log n) time. In order to that the implementation has to have the freedom to keep the tree balanced; all that is visible to the client is the sequence of values at the leaves.
What kind of predicate on depth is strictly monotone as you sweep left to right across the leaves?
You are individual in a world full of incountable beings with different opinions and experiences, so your assumptions are just base on your own context which forbid you to give a opinion that would generalize the view of others. One example: Me.
Haskell modules are certainly less powerful than ML-style modules. Just about everything is. Haskell seems to support a quite good amount of code reuse anyway, but it would be nice to improve. That's what Backpack is about, I think. Any complaint that supposes ML-style modules are a suitable replacement for the sorts of things type classes are good for is simply made from ignorance. There's some interesting work on building typeclass-like usability on top of modules, but work is needed. Even Harper agrees type classes are pretty handy for things like letting you add various sorts of numbers.
nobody denies haskell's module system is rudimentary. Finally an initiative to change this is under way, google backpack. Re typeclasses, some opinions are against them, there was some recent discussion on them here. seek those out to hear the other side of the story. eg edward kmett gives a strong case for typeclasses, narrowly used, and moreover precisely for the reasons here stated as being their failings, like having global confluence, coherence. Sry I hope I gave enough info here for you to find your way, usually I do the googling and history searching and put links, but I'm sleepy now and I think you can do this as well..
I came to Haskell from Clojure, it wasn't long before Haskell was faster for me to modify or create than Clojure but learning the idioms and libraries took some time. I've written about this before: http://bitemyapp.com/posts/2014-04-29-meditations-on-learning-haskell.html
We did a fairly careful analysis of the Haskell module and class system against the SML module system about 1993, with a view to possibly enhancing the module system for Haskell 1.3/1.4. Although the Haskell module system was supposed to be fairly simple and conservative, we found that the combination with type classes made it surprisingly capable (I think this was happy accident - as far as I know type classes were only motivated in the original design for overloading). Using type classes with Haskell modules, you can get essentially all of the power of the ML module system if you apply some ingenuity, but some things are certainly more difficult to do/need some thought/need more modules/use more names/are less abstract (conversely, as skew says, some things that can be done with type classes are simply not possible/sensible with ML modules). We did do some experiments with e.g. first class/parameterised modules in Haskell, but overall we didn't feel the increase in capability justified the added complexity, so we left things essentially as they were at the time. I think that was the correct decision then: not too much complexity, but a lot of capability. 