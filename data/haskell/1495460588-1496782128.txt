Where we can follow the updates and get the full book when its ready?
I *think* you could still call that sequential.
How do I answer question: &gt; Why something is a Functor/Applicative/Monad?
In Rust, we have a From and Into trait, which are our typeclasses: https://doc.rust-lang.org/stable/std/convert/trait.From.html pub trait From&lt;T&gt; { fn from(T) -&gt; Self; } https://doc.rust-lang.org/stable/std/convert/trait.Into.html pub trait Into&lt;T&gt; { fn into(self) -&gt; T; } The difference is in the blanket implementations, that is impl&lt;T, U&gt; Into&lt;U&gt; for T where U: From&lt;T&gt; means that implementing `From` gets you `Into` for free, and `From` is also reflexive, that is; impl&lt;T&gt; From&lt;T&gt; for T { You usually end up implementing `From`, but using `Into` as bounds on your functions. Anyway, I thought it was interesting; two similar things in two similar languages that have a bunch of difference too.
First [aura](https://github.com/aurapm/aura), then this. More Haskell package management is a good thing.
Love the typed holes. But, shouldn't the definitions from Functor and Applicative be also included in the 'relevant bindings' section? They might be useful as well.
I don't get how laws are even an argument. Why couldn't you just say that a law must hold IF both instances exist?
There's no answer in general. It depends on the thing. The only "real" answer is "because I can write functions fulfilling the signatures and obeying the laws".
I don't see why you can't, but I also think it is a poor law for the _class_ but useful for some subset of instances.
What can happen is that two users write instances for the different classes that are fine individually, but violate the law if used together. Of course, this only happens if both users are writing orphan instances, but that's a different coordination problem.
Because package `A` may provide type `Foo`, package `B` may implement the `ToJson Foo` to serialize to their Python client, package `C` may implement `FromJson Foo` to deserialize from their REST endpoint, and package `D` may import both `A`, `B`, and `C`. Whose fault is it if package `D` tries to rely on the round-trip property, given the law you've stated? (IMO, it's the fault of the statement of the law.)
Hi Ivan, Glad to see you again :) I definitely should try something like self-publishing. When I was researching this theme before I decided to put my book online, I've noticed that there are some issues for me as a resident of Russia. I believe I need more time to find any self-publishing platform that works with Russia, and maybe Leanpub is that platform. And also there are many problems of using Paypal here in Russia, - if we are talking about some money operations. I don't like this situation because Russia seems to be separated from the whole world which is certainly sad and inconvenient. Anyway, thank you for your advice. I'll figure that out.
Is this correct? f :: Bool -&gt; Int f True = 1 f False = 0 So we can represent `f` as `Pair 0 1`
Hi, thank you for your interest! You're right that we can't sometimes distinguish these three paradigms from each other. Keeping all the code inside the IO monad feels like it's a real imperative programming in Haskell, however, there is an open question how much it's imperative and whether it's impure at all. You can find many discussions by just googling "Is Haskell impure" or something. Thanks :)
Do you mean is there some mailing list or will I be tweeting and posting about my progress? I probably will, but I didn't think about it. Anyway, I'll post something here on Reddit. Maybe I should create a mailing list too. You can also subscribe to my Twitter but I'm not an active Twitter user, so there is a chance I'll forget to post there.
I don't believe so. However, if you split the modules up, I believe you could do what you want to do.
Reach out to Pearson? Seems like the kernel of a text targeted towards mechanical and chemical engineers. Perhaps "Functional Design of Control Systems"? 
(&gt;&gt;=) is sequential. you can't run the a -&gt; m b until you run the m a 
I like the frequent releases, deech! Have you considered not bundling the tarball in order to not blow up git repo size with tarball binaries? Depend on release tarball (git tag) of an fltk git mirror?
that makes sense. but why do people inline licenses into each file? 
They are Fortran/Ada/C++/Excel/Matlab developers now :) Controls are mostly Matlab compiling to C? GUIs are mostly Qt. A Haskell Qt book perhaps? Functional Reactive Programming, functional controller design, formal verification of state machines, ... 
Huh. Both of these papers are interesting reads. I think I need to dig deeper into this and try to really grok it!
Hey! Thanks for posting here. Some people mentioned Data Types a la Carte which does sound like an interesting option. Haven't played with it yet, though.
Good luck :)
Overall, definitely try to get compensation for your time spent in whatever channels you can think of. I would shorten the list of topics down more. If I was outside of Haskell, I would hesitate to write about Haskell architecture personally, but it's possible. I would identify the largest Haskell projects available public currently, to get a feel for what people are using in the wild, to complement the topics people usually discuss as ideals in theory. This seems like a book best written by someone at Well-Typed or FP Complete, who has had to architect a large variety of applications. Maybe you could try to get into one of those consultancies, to give you wider exposure? 
Even though this is an interesting problem, I don't think it really happen in Haskell (probably because there is no good solution ;-), maybe also because it's hard to do real plugin in Haskell. If you write an library which needs to be extended, you probably just go with type class, if you write an application, just modify existing types, so the case where you need a "open" type are rare. However, version number (with `sell` being field of a record) 3 doesn't really work as expected. This is usually the recommended way but it suffer the lack of being able to update your data. You can't modify age or brand easily. Another common way to extend an object is to add a parameter in your type. data Instrument ex = AcousticGuitar ... | Piano ... | InstrumentEx ex Obviously, you need to pass the a function to process the extension to your normal function sell :: Instrument ex -&gt; (ex -&gt; Price) -&gt; Price sell instrument sellEx instrument = case instrument of AcoustincGuitar -&gt; ... Piano -&gt; ... InstrumentEx ex -&gt; sellEx ex You can then easily define you own instrument type MyInstrument = Instrument OtherInstrument data LocalInstrument = ... localSell :: LocalInstrument -&gt; Price mySell = sell localSell Bob's your uncle ;-) 
Glance briefly at sentenai's GitHub. Mostly, seems like whatever tweag, takt, sentenai, lumiguide, and myrtle open source will provide the initial boosts in this area.
Please note, this is not a replacement for emerge, rather it could be seen as a more user friendly way (for a haskeller) to configure emerge.
"the functor instance of Two applies fmapped function to only second entry." Nope. Try writing that with the correct type signature and see what GHC tells you. You need to apply the function to both entries for it to be type correct.
well okay, we can apply on both args, as its not against functor rules. and fmapping will be like applying function to all return values. but how about `pure` and `return` or `bind`? how would we do `monad` and `applicative`?
Good post, but I thought I'd see more anti-patterns. Type classes are thorny things for newcomers to use properly especially if they come from C++ and naturally reach for them whenever a C++ coder might reach for a C++ class.
Interesting. This is a great test case for a variety of reasons. I am playing with some variants of your test case. This one gives better information on what's going on: {-# LANGUAGE ApplicativeDo #-} -- foo :: Monad f =&gt; (Int -&gt; f ()) -&gt; Int -&gt; f () foo f n | (n :: Int) == 0 = f 0 | otherwise = bar f (n :: Int) -- bar :: Monad f =&gt; (Int -&gt; f ()) -&gt; Int -&gt; f () bar f n = do _ &lt;- f (n :: Int) foo f (n - 1) This test case is interesting to me for several reasons: **1.** You're introducing a recursive binding. Take away the type declarations, and by the time `-ddump-ds` outputs desugaring (after renaming), an anonymous function is introduced with `letrec` and `foo` and `bar` are stubs which refer to the new binding. So that's interesting, I don't know if there's any interplay there. It looks like not, and I'll show that momentarily. Here's the desugaring of the above. -- RHS size: {terms: 40, types: 48, coercions: 0} ds_d2BU [LclId, Str=DmdType] ds_d2BU = \ @ a_a2B3 @ m_a2B1 $dMonad_a2Be -&gt; letrec { bar_a2At [LclId, Str=DmdType] bar_a2At = \ f_a2Am n_a2An -&gt; &gt;&gt;= $dMonad_a2Be (f_a2Am n_a2An) (\ _ [Occ=Dead] -&gt; foo_a2Aq f_a2Am (- GHC.Num.$fNumInt n_a2An (GHC.Types.I# 1#))); foo_a2Aq [Occ=LoopBreaker] [LclId, Str=DmdType] foo_a2Aq = \ f_a2Ak n_a2Al -&gt; case == GHC.Classes.$fEqInt n_a2Al (GHC.Types.I# 0#) of _ [Occ=Dead] { False -&gt; (\ _ [Occ=Dead, OS=OneShot] -&gt; bar_a2At f_a2Ak n_a2Al) GHC.Prim.void#; True -&gt; f_a2Ak (GHC.Types.I# 0#) }; } in (foo_a2Aq, bar_a2At) -- RHS size: {terms: 8, types: 35, coercions: 0} foo [LclIdX, Str=DmdType] foo = \ @ a_a2B3 @ m_a2B1 $dMonad_a2Be -&gt; case ds_d2BU $dMonad_a2Be of _ [Occ=Dead] { (foo_a2Aq, _ [Occ=Dead]) -&gt; foo_a2Aq } -- RHS size: {terms: 8, types: 35, coercions: 0} bar [LclIdX, Str=DmdType] bar = \ @ a_a2B3 @ m_a2B1 $dMonad_a2Be -&gt; case ds_d2BU $dMonad_a2Be of _ [Occ=Dead] { (_ [Occ=Dead], bar_a2At) -&gt; bar_a2At } As you can see, without the type signatures, the two terms are let-bound in a new function `ds_d2BU` and returned as a tuple. That's neat. If I uncomment the type signatures, that isn't necessary for type checking. (n.b.: I'm using `Monad` constraints so that it will type check and give me the results of the renamer and desugaring.) **2.** I think you're running into a use case of `*&gt;` that I didn't think of, where the last statement is **not** a pure expression **and** no free variables are bound by previous statements. In this `bar`: bar f n = do _ &lt;- f (n :: Int) foo f (n - 1) Ideally this should desugar as `f n *&gt; foo f (n - 1)`. Instead, I see two things happening. First, with only one bind statement and one last statement without a return, the `ApplicativeDo` desugaring did not even fire. Here's the renamer output: Main.bar f_a2CF n_a2CG = do { _ &lt;- f_a2CF (n_a2CG :: Int); Main.foo f_a2CF (n_a2CG - 1) } (A semi-colon separates dependent bind statements in the renamer with `ApplicativeDo`.) And here's the desugaring: bar = \ @ f_a2D0 $dMonad_a2Dl f_a2CF n_a2CG -&gt; &gt;&gt;= $dMonad_a2Dl (f_a2CF n_a2CG) (\ _ [Occ=Dead] -&gt; foo $dMonad_a2Dl f_a2CF (- GHC.Num.$fNumInt n_a2CG (GHC.Types.I# 1#))) But if we change the function a tiny bit, we can trigger `ApplicativeDo` desugaring in GHC 8.0.2: bar :: Applicative f =&gt; (Int -&gt; f ()) -&gt; Int -&gt; f () bar f n = do _ &lt;- f (n :: Int) ret &lt;- foo f (n - 1) return ret Ah, now we have a `return` in the tail position, and two bind statements. Here's the renamer output, and you can see it discovered the independence of the two bind statements. The vertical bar indicates a sequence of `ApplicativeArg`s, each of which is *independent*. Main.bar f_a2Hf n_a2Hg = do { _ &lt;- f_a2Hf (n_a2Hg :: Int) | ret_a2Hh &lt;- Main.foo f_a2Hf (n_a2Hg - 1); return ret_a2Hh } And in the desugaring, we can see it now uses `&lt;*&gt;` and `fmap`: -- RHS size: {terms: 24, types: 24, coercions: 0} bar [Occ=LoopBreaker] [LclIdX, Str=DmdType] bar = \ @ f_a2HA $dApplicative_a2HU -&gt; let { $dFunctor_a2I4 [LclId, Str=DmdType] $dFunctor_a2I4 = GHC.Base.$p1Applicative $dApplicative_a2HU } in \ f_a2Hf n_a2Hg -&gt; &lt;*&gt; $dApplicative_a2HU (fmap $dFunctor_a2I4 (\ _ [Occ=Dead] ret_a2Hh -&gt; ret_a2Hh) (f_a2Hf n_a2Hg)) (foo $dApplicative_a2HU f_a2Hf (- GHC.Num.$fNumInt n_a2Hg (GHC.Types.I# 1#))) That change (adding `ret &lt;- ...; return ret`) is sufficient to reduce the constraint to `Applicative` on both functions. Your test case elucidated a use case I didn't expect, thank you so much! I now have two rules to implement for `last statements`: 1. If the last statement of a sequence `do {stmt_1; ...; stmt_n; lastStmt}` is independent: 1. If `lastStmt` is `return lastExpr` (or equivalent), desugar as: lastExpr &lt;$ stmt_1 &lt;* stmt_2 &lt;* ... &lt;* stmt_n 2. Otherwise, desugar as lastStmt &lt;* arg_1 &lt;* arg_2 &lt;* ... &lt;* arg_n Although according to /u/edwardkmett, perhaps I ought to instead do: lastExpr &lt;$ (arg_1 *&gt; arg_2 *&gt; ... *&gt; arg_n) and (arg_1 *&gt; arg_2 *&gt; ... *&gt; arg_n) *&gt; lastStmt Respectively. (I would use `($&gt;)` except that's defined as `flip (&lt;$)`, it might reduce opportunities for optimization, particularly with `RULES`.) 
Well, write the type signatures out and see if you can fill them in.. That's the best way to learn!
&gt; It is too easy to incidentally type toJSON dinnerMenu instead of toJSON backgroundMusic, and everything will compile What alternative implementation would prevent this? Isn't this also the type of error that a single test will catch? &gt; Also you have to remember how e.g. Either is represented in json. I used `Either` in my example because `Either` is well known. I don't think in an application one would have `Maybe (Either ... )` a custom sum type would be better, but I was using it to make a point. However I'm interested to know why this is unique to the pattern I presented. Is there an alternative that does not require one know the implementation of the types they are using? &gt; IMO To and From typeclasses are antipatterns in most cases, they are suitable only for automatic derivation (via generic, TH, etc) when you don't really care about the representation Isn't possible to use autogeneration and care about the representation? I think it tends to be the case that patterns that are easy for humans to do without errors also lend themselves to automation, but this doesn't mean we have to generate the code to gain the benefits. &gt; From that POV it doesn't make sense to separate these two type classes. Your assuming that you are going to read and write with the same types, but this not always the case. Typically the result from a data base includes computed data that cannot be written to, and the structure for creations can contain data that you will not read (e.g. password). If you combined the to methods you would end up in the same situation as binary, with a type class people write bad method implementations for. I would like to see you develop the argument more with examples for the alternative you think is better.
It is a trade-off. I don't nest `Maybe`s like that, so I would not run into this. It is convenient to have a type that can used for nullable attributes. It should be documented either way.
What I mean is that `Maybe` has no business being encoded that way. It's possible to express null pointers in Haskell (Edward Kmett uses it for `structures`, I believe). And it's possible to design a type around null pointers that would best be encoded using nulls (e.g., an `IORef` that may or may not have something in it). But that's semantically very different from `Maybe`.
&gt; On the "representing JSON", side, I'd think nullable fields should probably be represented by actual nullable types, probably using pointer tricks. It's fundamentally the application's job to decide how to deal with nullability, not the library's. I think this is really the question. If we are going to say `aeson` is doing something terrible we should compare it against an alternative. Pointer tricks tend to be unsafe (can cause segfaults) which sounds much worse then the current situation, but maybe I am not following. A example is going to be the most helpful.
Huh, I thought there was a bot that automatically re-mentioned users mentioned in a post. Did it get tossed from this subreddit or something?
It's great to see that work is being done in this area!
&gt; What alternative implementation would prevent this? `backgroundMusicToJson backgroundMusic`. At least you have to mistype it twice. But it is more important how the implementation works under refactoring. If you change type of `dinnerMenu` to `(Menu, Cost)`, then implementation via `toJSON` will continue compiling, but one that uses `dinnerMenuToJson` will break. Now what do you really need? If you don't care about the representation of this field, then you need `toJSON`. But if you care about the consumer of JSON you produce, then it is better to stop and think about it. You have to either update the consumer or serialize new version in a compatible way, e.g. encode cost into a separate key. &gt; Is there an alternative that does not require one know the implementation of the types they are using? Not sure what you mean here. I'm talking about how Either is represented in JSON, not how it is implemented. Is `Just 42` represented at `{"tag":"Left", "value":42}` or `["Left", 42]`? It is not unique, so type class is a wrong abstraction for that. It is much better to have a set of functions, that serialize data in a way you need right now. Basically that's how the first `bigRecordToJSON` is written in the article, but for some reason you don't like it. &gt; Your assuming that you are going to read and write with the same types When you write `toJSON` you are saying: "I don't care how it is represented in JSON" -- that is my assumption. I can imaging only two cases when it is true - either you are not using the result of the serialization, or it is consumed by a deserializer, which will be automatically updated in case of any change in the serializer. The latter means that both are autogenerated by the same tool and from the same description (e.g. type declaration). In all other cases I'd prefer compiler to generate an error, because I really need to perform some action here.
`toJSON @Music` is just a fancy way to write `musicToJSON`, but requires nonstandard ghc-specific language extension.
&gt; It is not unique, so type class is a wrong abstraction for that. I'm not sure about that. Is representing `Monoid` with a type class the wrong abstraction because you need the `Sum` and `Product` wrappers? &gt; When you write toJSON you are saying: "I don't care how it is represented in JSON" When I write `toJSON` I am definitely saying that I care how it's represented in JSON! In fact, I'm saying that this is exactly how it's represented. Anyway, to your larger point: Using monomorphic functions (`backgroundMusicToJson`) will provide better type inference and better compiler errors than polymorphic functions (`toJSON`). But the polymorphic functions make it easier to write code like `object [ "key" .= value ]` (compared to `object [ ("key", valueToJson value) ]`). 
&gt; Is representing Monoid with a type class the wrong abstraction It is a bit different case because `Monoid` is lawful. If your code relies on the laws, you have no way to express it other then using the type class. Compiler doesn't check laws, but at least you are clear about your intents. &gt; I am definitely saying that I care how it's represented in JSON! Yet you let compiler to infer the encoder instead of specifying it explicitly. It is clearly a contradiction for me. &gt; will provide better type inference and better compiler errors &lt;...&gt; But the polymorphic functions make it easier to write code You are trading safety vs keystrokes here. If compiler infers wrong encoder because of a typo, you are busted.
Well, there's at least one safe option, which is to use a type that can't be nested. I don't know how well the ergonomics will work out, but at least it doesn't lead to silent breakage. type family IsNullable x where IsNullable (Nullable x) = 'True IsNullable _ = 'False data Nullable x where Null :: IsNullable x ~ 'False =&gt; Nullable x NonNull :: IsNullable x ~ 'False =&gt; x -&gt; Nullable x
Except you don't lose the genericity of it, which is useful when dealing with parameterized types for instance. You can, for instance, say `toJSON @(Vector BlogPost) posts` as opposed to `vectorToJSON blogPostToJSON posts`.
Type class laws is the border between vague common sense and equation reasoning. If it is enough for you to rely on "everybody can figure it out", then type class is a wrong abstraction for the particular use case.
You're absolutely right about safety versus keystrokes. And laws are definitely useful for type classes, but I don't think they're always necessary. Especially for type classes like `ToJSON` and `Binary`. &gt; Yet you let compiler to infer the encoder instead of specifying it explicitly. I don't understand what you're saying here. Is this about converting the `Value` into a `ByteString`? Or is it about deriving `ToJSON` instances? If it is about deriving (via generics or Template Haskell), often the derived instance is identical to what I would write by hand. 
I don't know, but it think it would be disingenuous for a company building a chat protocol to not believe in remote work.
&gt; And laws are definitely useful for type classes, but I don't think they're always necessary. I'm not arguing to remove lawless type classes. It would be a nightmare. We use antipatterns everywhere, it is OK while we understand the tradeoffs. &gt; I don't understand what you're saying here. I'm talking about `ToJSON` instance. &gt; often the derived instance is identical to what I would write by hand. It seems to be the well know conflict between "don't repeat yourself" and "single responsibility principle". The derived instance could be identical to the handwritten one, but they usually have different responsibilities. And I usually prefer SRP over DRY.
Maybe in natural language parsing the non-determinism + state monad version would help... 
Can't you separate the two splices by a line like $() to force them to be run separately?
This is a lot to go through, thanks!
FLTK does not release on Github unfortunately. If a bigger repo is a problem I can add something to `Setup.hs` that downloads it first via `wget`.
parsec/trifecta/etc can be seen as something of a middle ground where if you have a parse error that doesn't consume input it gives you something like the former (extended with a set of failed parse types so far), but if it occurs after consuming input it gives you something like the latter. That way you can report the full set of expected tokens at the current site when a parse error finally fails and can distinguish betwee things that need to backtrack (guarded by try to pretend no consumption happened) from actual parse errors.
Though I'd wager GHC's specializer can do much better optimization with the former than the latter, assuming `Vector`'s `toJSON` is marked as `INLINABLE`.
&gt; I'm not too familiar with C (even less the Win32 API). Is there more logic to the layout than (1) adding a tag and (2) taking the maximum of the sizes of each case? You also need to ensure that the alignment of the union field is set to the maximum of the alignments of the union members. I use a type-level approach to deal with unions. Basically I would just have to declare a field which type would be something like: Union '[MouseInput,KeybdInput,HardwareInput] Example: https://github.com/haskus/haskus-system/blob/master/src/lib/Haskus/Arch/Linux/Internals/Input.hs#L1467 The Union type: https://github.com/haskus/haskus-system/blob/master/src/lib/Haskus/Format/Binary/Union.hs
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [haskus/haskus-system/.../**Input.hs#L1467** (master → 112595a)](https://github.com/haskus/haskus-system/blob/112595ab2e400428f197c0744a3592973a1c8a2c/src/lib/Haskus/Arch/Linux/Internals/Input.hs#L1467) * [haskus/haskus-system/.../**Union.hs** (master → 112595a)](https://github.com/haskus/haskus-system/blob/112595ab2e400428f197c0744a3592973a1c8a2c/src/lib/Haskus/Format/Binary/Union.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhwtpo3.)^.
&gt;&gt; It is not unique, so type class is a wrong abstraction for that. &gt; I'm not sure about that. Is representing Monoid with a type class the wrong abstraction because you need the Sum and Product wrappers? I've made that argument in the past, but I only half-way believe it. I'm generally okay with to/from classes, as long as their use is understood. It's even better if the higher-order operations are exposed, e.g. `listToWhatever :: Builder a -&gt; Builder [a]`.
thanks! yeah, I was hoping for an existing library, by someone who's experienced with systems programming, who's dealt with memory layout. I can easily write the nave generic programming myself (using `:+:`, `Either`, your `Union`, or any sum type), but won't have any guarantees that it won't segfault or leak, or be portable. have you thought about extracting `Format/Binary/Union.hs` is a simple package? 
One reason to specify typeclass laws is to constrain the set of valid instances that one can write. For example, consider this `Monoid` instance for tuples: instance (Monoid a, Monoid b) =&gt; Monoid (a, b) where ... There's really only one way that we can implement that instance that can satisfy the `Monoid` laws Now contrast this with the following `ToJSON` instance: instance (ToJSON a, ToJSON b) =&gt; ToJSON (a, b) where ... What's the correct way to implement that? I tried to guess what it would do ahead of time and I was wrong. There are multiple ways we could implement it, and who's to say which one is right? Laws provide a specification for a type class, and without a specification then a type class instance is "not even wrong". Saying that a law must hold if the instance exists is like saying "the specification is whatever the implementation does"
Can you guess what this `ToJSON` instance does without looking at the source code? instance (ToJSON a, ToJSON b) =&gt; ToJSON (a, b)
&gt;and what is the cons of handling the error using the empty list [] It's rare that you want a parser that fails with no message whatsoever.
I think you could use a Variant type as described here: http://hsyl20.fr/home/posts/2016-12-12-control-flow-in-haskell-part-2.html Component would be data types (e.g., A,B,C). Entities would be lists of variants (e.g., [Variant '[A,B,C]]). To define systems, you should be able to use type-classes (as I do in the example with ```MyClass```). I think you can use an OVERLAPPABLE instance to define a default behavior for components that don't support the system.
&gt; Yeah I 100% agree, which is why it is always so frustrating when I hear people arguing again and again for strict by default, why can't people just let Haskell be Haskell, pretty much every other language is strict but I like the huge productivity I get in Haskell from laziness (and other features, but laziness is a big one) and would really rather it not be taken away from me. Agree completely. The arguments against laziness are inevitably wrong for a subtle reason, and often the people putting them forward are incapable of grasping it. And honestly, I agree that failing dramatically is a *good* thing. It means you can fix your bugs. 
Hence the infamous Fib40 benchmark :)
hlearn
On the subject of the Cont type. Trying to explain its instances of functions, applicative and monad, I drew some diagrams, and they made for some really neat intuition. If continuations are viewed as flows of values, then * the functor instance lets you create a new flow by transforming the value from an old flow "mid flight"; * the applicative instance lets you create a new flow by combining two or more old flows into one; and * the monad instance lets you split a single flow into multiple possible outgoing flows. I'm sure this intuition works also for other types. Like, I don't know, maybe replace "flow" with "stream" for conduit? 
"Wire will hire a Haskell dev who won't tire" 
Parsec does not return multiple successful parses, but it does track multiple "expected" tokens until the cursor moves, which now that you've called me on it, is almost exactly the opposite. =)
Well, yes, the thing is, the "expected" tokens are all-or-nothing. When a parser fails, Parsec lists *all* the expected tokens, independent from where and when it backtracked.
ELI5 please. What are symbols and why concatenating then useful?
Same as me. How about starting a Kickstarter campaign?
I don't know a lot about hscurses, but I know 'vty' has good mouse support (IIRC it parses the escape codes for you). I have also read that 'brick' is higher level than 'vty' so it might be even easier.
You can use [generic-storable](https://hackage.haskell.org/package/generic-storable-0.1.0.0/docs/Foreign-Storable-Generic.html).
Polymer is kinda orthogonal to whatever you do on the server side. It's just a small library that [adds some common stuff on top of `HTMLElement`](https://youtu.be/assSM3rlvZ8?t=11m16s). If you want to build **a single page app** (you don't *have* to, you can just use elements in server-rendered apps) with all the best practices discovered by the Polymer team, you should use the polymer-cli server, and your server-side app should be an API-style thing. I'm not sure if Happstack is good for that. [Servant](https://haskell-servant.readthedocs.io/en/stable/) is very good for that though! (And I made [a small library that adds more stuff on top of Servant](https://github.com/myfreeweb/magicbane).)
It returns all expected tokens for the current location. If it moves the cursor backwards with 'try' it should forget the tokens for the deeper position, as it is going to report the earlier location as where the parses failed, not the nested one. As a result, I've always wondered if try should be a binary combinator taking the operation to try and the name to &lt;?&gt; it with. As without a name it is singularly unhelpful, and any errors / location information about attempted parse items that occur after moving the cursor don't get propagated out of the try. So long as we rigorously apply token type descriptions in this manner you can modify this sort of parser to give you arbitrary tab completion, etc. based on the expected set. While trifecta tries to be more careful about not moving your erroneous failed-to-match token types, I'm not sure if parsec gets it right in all cases. I vaguely recall finding what I thought was a tricky parsec bug in this regard when I was first working on trifecta.
I don't think so. In my experience having direct contact to your team mates can be rather helpful. Chat and calls are useful as tools, and sometimes you have to rely on only those, but I don't think they should be the only ones you use if you can avoid it.
Without deep analysis I would say that it's "libpq" or even the performance limit that it approaches. I wouldn't blame anything here. There's just a natural boundary that you can't cross. From the Hasql perspective I'd say that we're using "libpq" in a highly optimized way, so I wouldn't blame Hasql either. One thing to notice though is that in many ways "libpq" does things suboptimally, which is why the coming release of Hasql implements the protocol natively. So there'll be a performance boost.
Sorry, I don't see how it is related to the statements I made. I this subthread I described how lawful type classes were different from lawless ones, nothing more.
Thank you! I appreciate the advice on editing, also the heads up regarding the guidelines; I'll try to vary the submissions more!
It would be useful once it is defined for something more complex : `data Foo a b = Foo Int a b`. `map :: ((a,b) -&gt; (c, d)) -&gt; Foo a b -&gt; Foo c d`
I admit I have chosen a bad example.
There's also TextualMonoid from monoid-subclasses.
It's not bad to have different teaching methodology on the same concept. While this article may not be that useful for a beginner, I found this as an excellent refresher for me on the basics of it.
It's no problem for hackage users, but it will unnecessarily blow up the repo size with each tarball commit. You can create a new repo on gitlab.com which is just a read-only mirror from svn (https://docs.gitlab.com/ce/workflow/importing/migrating_from_svn.html). No manual work required afterwards. I suppose github has the same feature.
I hate commuting. I think it wastes more time for everyone. No way you'll gain back that time by asking your colleagues questions, especially considering now you're interrupting them. Using emails, chats (and skype calls if you record) etc has an additional advantage of leaving a paper trail. You can lookup again what you talked about. These are free minutes. The real issue in my opinion is trust, or the fact that a manager has to trust that work is being done without seeing it. If you are at the workplace, you must be doing work (or related stuff), but if you don't come to the workplace a manager can't know for sure. A solution that would allow these kind of trust issues, that's not too privacy invasive, would surely become very popular.
This feels like a lot of indirection to basically say "see hmatrix and vector".
I encourage you to write this better blog post. Reddit already has a method to deal with content the community doesn't like, down voting. This post is being upvoted. So I think you are confused.
I'll have to disagree on several of these points. While long commutes are of course a pain, two 30 minute trips every day is hardly going to wreck your day - especially if you have other things to do on the way, like grocery shopping - and that's roughly the ballpark you're in for commutes inside of Berlin, assuming you live in the actual city and use public transport. In a normal office environment you don't usually bother your colleagues when you have the luxury to wait for them to respond to an email, and exchanges aren't usually limited to Q&amp;A scenarios anyway. Leaving documentation is always good practice and you should be doing that anyway, regardless of how you communicate. The real advantage in an office environment is facilitating communication between people who aren't directly involved with each other. In one office I worked in we'd have regular lunches with people from different teams, and it is quite helpful to have engineering talk to support or sales outside of a strict as-needed context. Trust on the other hand is much less of an issue I feel; you can tell whether or not someone is delivering results without looming over their shoulders all the time, and I doubt most places would have policies involving constant employee surveillance.
I'm not quite sure. I'd be perfectly happy with some sort of push notification that what you have is done, I'm just on Reddit sporadically. Most places that let you self-publish I think have a notification system built-in as well.
 instance (Monoid a, Monoid b) =&gt; Monoid (a, b) where mempty = (mempty, mempty) mappend (a, _) (_, b) = (a, b) ;O
As a counterpoint, I have a library that produces and consumes JSON. I care very much how my data is represented as JSON. I use the derived instances because they look exactly like what I would write by hand. https://github.com/tfausak/rattletrap/blob/2.2.4/library/Rattletrap/Json.hs#L31
Some lessons are good not because they teach you how to conquer the world, but that you can.
Elm doesn't have type classes, so that's basically what you have to do. While I agree that having `jsonEncDinnerMenu` or something removes a way to shoot yourself in the foot, I also find it really inconvenient, especially when you have lists or sets to encode. With the typeclass approach, I get TH or Generic derivation, which is incredibly handy, especially because I can also derive them in my frontend language in the exact same way. On the other hand, I never write `Binary` instances, because deriving never does what I need, and I don't gain anything from the typeclass.
&gt; The real advantage in an office environment is facilitating communication between people who aren't directly involved with each other. That's certainly a point. The problem is that you just get all of the communication, you can't filter it by what's meaningful for yourself. How much people get distracted by this kind of communication seems to vary quite a bit. &gt; In one office I worked in we'd have regular lunches with people from different teams, and it is quite helpful to have engineering talk to support or sales outside of a strict as-needed context. That's certainly another point. I'm just one of these guys that really likes to clear his mind at lunch time. Having more work talk at lunch time is going to lower my rest. People are just different in what works for them best. 
You might consider an expression monad, where `&gt;&gt;=` performs free variable substitution. See the `bound` package to see where this idea really gets useful. {-# LANGUAGE DeriveTraversable, LambdaCase #-} module Simex where import Control.Monad data Expr a = App (Expr a) (Expr a) | Lam (Expr (Maybe a)) | Var a | Plus (Expr a) (Expr a) | Lit Int deriving (Functor, Foldable, Traversable, Show, Eq) instance Applicative Expr where pure = Var (&lt;*&gt;) = ap instance Monad Expr where App f x &gt;&gt;= g = App (f &gt;&gt;= g) (x &gt;&gt;= g) Var a &gt;&gt;= g = g a Plus x y &gt;&gt;= g = Plus (x &gt;&gt;= g) (y &gt;&gt;= g) Lit x &gt;&gt;= _ = Lit x Lam e &gt;&gt;= g = Lam $ e &gt;&gt;= \case Nothing -&gt; Var Nothing Just x -&gt; fmap Just (g x)
it is a common pattern to create an "mtl-style" monad like this: https://github.com/bitemyapp/bloodhound/blob/master/src/Database/V5/Bloodhound/Types/Internal.hs#L49-L56 
thanks
Heres one in F#: https://github.com/jannickj/Freckle/blob/master/src/Freckle/Feed.fs it's the most complicated monad I've written to date, it can join lazy list of lazy lists by their time stamp, lazily meaning it can support picking n time steps at o(n). the core to any monad is join as `bind f = join (map f)`
I don't really see how that follows. I don't see anything on their page about them advertising themselves as a company facilitating remote-exclusive work, and privacy has nothing to do with why someone would not want their workers to exclusively work remotely.
I found that writing parser combinators is a very good exercise for this. Start from: newtype Parser a b = Parser { parse :: a -&gt; Maybe (b, a) } and build the instances yourself!
Only once have I "invented" a monad. I was playing with the streaming folds of the [foldl](http://hackage.haskell.org/package/foldl) package, in particular with monadic folds over `Except`, which can "fail early": newtype Fallible m r i e = Fallible { getFallible :: FoldM (ExceptT e m) i r } This can be given an `Monad` instance. `return` creates a `Fold` which starts already failed. And `&gt;&gt;=` allows you to continue folding after an error, using a new fold constructed from the error. It's not a very useful monad, to be honest. (It is also a `Profunctor`, and `Choice`.)
How does the dealing with resource management scale with the size of the app? FFI may be painless enough for a proof of concept but seems like it might get hairy once an app gets complex.
the docs only cover basic topics at the moment, but the gitter channel seems to have discussion on active projects beyond hmatrix and vector.
That doesn't satisfy the `Monoid` laws because: mappend x mempty /= x For example: mappend ([1], [2]) mempty = ([1], [])
This post is written to illustrate addition chains using a simplified version of code I'd previously written. The goal of simplifying the code was to make the demonstration of the addition chain concepts clearer. Unfortunately, it seems like I simplified it too much, because the `denote` function does not scale to large inputs. Consider: module Curve25519 where import AdditionChainConstruction import Control.Arrow((&gt;&gt;&gt;)) -- (&gt;&gt;&gt;) = flip (.) -- This will run forever. main = putStrLn $ show assertion where assertion = denote curve25519_q_minus_2 == curve25519_q_minus_2' double_n 1 a = double a double_n n a = double (double_n (n - 1) a) curve25519_q_minus_2 = let b___1 = One b__10 = (double_n 1 ) b___1 b1001 = (double_n 2 &gt;&gt;&gt; add b___1) b__10 b1011 = ( add b__10) b1001 x__5 = (double_n 1 &gt;&gt;&gt; add b1001) b1011 x_10 = (double_n 5 &gt;&gt;&gt; add x__5) x__5 x_20 = (double_n 10 &gt;&gt;&gt; add x_10) x_10 x_40 = (double_n 20 &gt;&gt;&gt; add x_20) x_20 x_50 = (double_n 10 &gt;&gt;&gt; add x_10) x_40 x100 = (double_n 50 &gt;&gt;&gt; add x_50) x_50 x200 = (double_n 100 &gt;&gt;&gt; add x100) x100 x250 = (double_n 50 &gt;&gt;&gt; add x_50) x200 q_minus_2 = (double_n 5 &gt;&gt;&gt; add b1011) x250 in q_minus_2 curve25519_q_minus_2' = 2^255 - 19 - 2 My understanding is that, even though there are a lot of shared subexpressions in `curve25519_q_minus_2`, `denote` cannot "see" the subexpressions are shared, so it recomputes everything every time, without taking any advantage of the shared structure of the object. Is there any minor surgery that can be done to the `AdditionChainConstruction` module to make `denote` efficient without making it significantly more complex? AFAICT, there isn't, but maybe I'm overlooking something.
Bah! Humbug.
Different strokes for different folks, I guess. I don't like commuting and I prefer working at home where I feel much more comfortable than in an office. I need to ask a question, I pick up the phone or make a Skype call. Somebody wants to show me something, I fire up a screensharing session. I agree that one wouldn't pick up that much on the "ambient communication" and watercooler talk, but that's fine for me to be honest.
Fair enough, but that doesn't really mean that an employer needs to be fine with it too.
Well, it's mostly a StateT over a Maybe monad but right now I am making a monad to choose efficient way to compute things about groups (groups in the mathematical / group-theoretical sense). It's my first actual Haskell project so it's probably all ugly and wrong (and I need to finish it by Friday if I want to pass the Nonprocedural programming subject). I will post it here when I finish it.
I think I must have misunderstood. 
[removed]
Thanks for sharing that! For a while I've been looking for a haskell library that does similar things. Could you advice on any?
I feel similarly, but usually working from home all week is too isolating, even with screenshares. I like one or two days in an office. I think most people would choose a cowork space over their home, if it was provided by company and a very short commute from home.
No I wouldn't say that `pure` is in the "codomain" of `F` any more than it is in the "domain." What is in the codomain of `F` is all the types that `F` generates. Each of those types contains zero or more terms. If types are "sets" of terms, and if Hask is a "set" of types, then `pure` is a function on the former, and `F` is a function on the latter. `pure` is a natural transformation from the functor `Identity` to `F`, but how you come about to this fact depends on your categorical definition of "Applicative," of which there are many. EDIT: [This article](https://bartoszmilewski.com/2017/02/06/applicative-functors/) goes over some of the categorical definitions of applicatives (though I think it undersells the definition using strength). It might help you understand exactly how applicatives are different from functors.
there's diminishing returns, i.e. might not be worth your time. but no, even with a mostly static API, a library can always be made faster, have better documentation, have more tests, add instances for newer classes (like Semigroup is now in base, afaik), change its dependencies (replace one parser with another), have more helper functions, etc. 
Is there something like this that can be made to work without being really gross? (I just slapped together stuff until it compiled.) {-# LANGUAGE TypeFamilies, DataKinds, MultiParamTypeClasses, FlexibleInstances, InstanceSigs, TypeApplications, AllowAmbiguousTypes #-} import GHC.TypeLits {- Defns -} type family At (n :: Nat) f :: * class FFF (n :: Nat) f where type Out n f b :: * fff :: (Out n f b ~ f') =&gt; (At n f -&gt; At n f') -&gt; f -&gt; f' {- Lists -} type instance At 0 [a] = a instance (0 ~ n) =&gt; FFF n [a] where type Out n [a] b = [b] fff :: (a -&gt; b) -&gt; [a] -&gt; [b] fff f [] = [] fff f (x : xs) = f x : fff f xs {- Tuples -} type instance At 0 (a, b) = a type instance At 1 (a, b) = b instance FFF 0 (a, b) where type Out 0 (a, b) c = (c, b) fff f (x, y) = (f x, y) instance FFF 1 (a, b) where type Out 1 (a, b) c = (a, c) fff f (x, y) = (x, f y) Then we have: -- works without specifying which index to map over k = fff (+1) [1..5] -- must be specified j1 = fff @0 (+1) (1, 2) j2 = fff @1 (+1) (1, 2)
Ok: I understand how `pure` isn't guaranteed, but I don't understand how `&lt;*&gt;` isn't guaranteed. A functor takes a map to a map. So if we have a `F(f)` of type `F (a -&gt; b)` and a value of type `F(o)`, then shouldn't we be able to just apply `F(f)` to that value? There doesn't seem to be anything special here; it's just function application in the *Functorial* context! So why isn't therefore `&lt;*&gt;` a method of `Functor`, with `Applicative` containing only `pure` and no other method?
Pretty cool to see how much is happening in the Haskell on mobile space recently!
Maybe use IsString ? https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-String.html#t:IsString Disclaimer: I'm a Haskell noob
https://github.com/haskell/hackage-server/pull/551#issuecomment-263046117 It's bundled with the other hsoc work for a major hackage improvement (tag based browsing, reverse dependencies, in particular). Just waiting on that last bit of code review before everyone feels comfortable merging it in and deploying it on the central server.
FWIW, this is one of like 4 competing categorical definitions for Applicative that all result in the same thing on Hask specifically. I think "lax monoidal functor with a strength" is a bit more general, since it imposes fewer requirements on the category.
The results I found were pretty shocking, so I'd love to hear from others, either if I'm missing something or just interpretation of the results. 
It's not mine, but you should check out A [Poor Man's Concurrency Monad(pdf)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.8039&amp;rep=rep1&amp;type=pdf) It describes a monad transformer for interleaving the computations of an arbitrary monad, has a deep embedded DSL for concurrency operations, and uses CPS to pause and resume the various computations. It's a bit old, but I think it illustrates some techniques for working with monads, in a self-contained form. I really liked that you can implement everything in the article from scratch (i.e. without importing stuff like mtl), and with a few adjustments to account for changes to haskell, get the thing working fairly quickly. 
Your `suffixPattern*` suffixPattern :: [a] -&gt; [[a]] suffixPattern x = reverse $ curry (snd . go) x mempty where go ((x:xs), suffixes) = go (xs, if not $ null xs then xs:suffixes else suffixes) go (_, suffixes) = ([], suffixes) are all left folds which means they traverse an entire list before starting to produce anything, so `whnf suffixPattern` forces the whole list's spine unlike `whnf suffixHylo` which is lazy enough. Try something like suffixPattern :: [a] -&gt; [[a]] suffixPattern [] = [] suffixPattern [_] = [] suffixPattern (_:xs) = xs : suffixPattern xs
A nice and easy to understand introduction to Template Haskell. Thank you.
Ah okay, that makes a lot of sense. I will edit the blog post!
This may be helpful: https://stackoverflow.com/a/22605558/1651941
Thank you for the kind words. Let me know if something was unclear or you have additional questions.
Or newtype Parser a = Parser { parse :: String -&gt; [(a, String)] } For something that'll behave more like ReadP.
Category is three things: * a collection of objects * a collection of arrows between objects * an associative operator with identity, written "∘", that composes arrows Keep in mind that a category is an "interface", there's always some underlying structure, an "implementation", that tells us what objects and arrows there are, and how ∘ works on them. Now, let's consider the Hask category. If we limit ourselves to the subset of Haskell without ⊥, we can see how the category "interface" can be implemented: * types of kind ★ form a collection of objects * functions `A -&gt; B` between values form arrows between objects `A` and `B` * there's associative composition `(.)` of arrows with `id` as identity Since Haskell has first class functions (functions as values), there can be some confusion already. If we have `f :: Integer -&gt; Bool`, its type is an object in the Hask category (`Integer -&gt; Bool :: ★`), but `f` itself is an arrow between objects `Integer` and `Bool`. The composition of functions `(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)` gives rise to a bunch of arrows in the Hask category, but it also is the ambient operator that we use to compose arrows. If I was to draw a diagram, there would be many instances of `(.)` on it, but I wouldn't say that I compose arrows using other arrows, categorically this would have no sense because arrows have no internal structure. Let's consider a different category, Hask(op), dual to Hask. We have the same objects, but the arrows are inverted: * types of kind ★ form a collection of objects * functions `A -&gt; B` between values form arrows between objects `B` and `A` * there's associative composition `flip (.)` of arrows with `id` as identity Notice that a function `A -&gt; B` is now considered an arrow between `B` and `A` and their composition is flipped. However, the objects are the same, and it's fine. Just for fun, here's another category, we can call it Inh(Hask): * types of kind ★ form a collection of objects * if the amount of inhabitants in type `A` is less than or equal to the amount of inhabitants in type `B`, there's an arrow from `A` to `B`, and we write it `A ≤ B`. * composition of arrows is the logical statement that if `A ≤ B` and `B ≤ C`, then `A ≤ C`; since `A = A`, we have the identity arrows `A ≤ A`. Check the laws! `≤` is associative. I have presented three categories: Hask, Hask(op), and Inh(Hask). Now, a functor F: C → D between categories C and D is a mapping that: * takes objects of C to objects of D. If A is an object in C, then F(A) is an object in D. * takes arrows of C to arrows of D (preserving source and target). If f: A → B is an arrow in C, then F(f) : F(A) → F(B) is an arrow in D. * takes the compositional structure of C to compositional structure of D. If f ∘ g = h in C, then F(f) ∘ F(g) = F(h) in D; also F(id) = id. There's some overloading going on here. I write F(A) for mapping objects and I write F(f) for mapping arrows. Those are different processes, although with the same notation. With this in mind, *why* is this even a good idea to express functors as a class in Haskell? Well, if we limit ourselves endofunctors on Hask, i.e. functors from Hask to Hask, here's the components of a functor: * something to take objects (types of kind ★) to objects (types of kind ★). What can serve this purpose? Well, one possible option is types of kind ★ -&gt; ★. Indeed, if we have some type `X :: *` and some unary type constructor `W :: * -&gt; *`, it's true that `W X :: *`. * something to take arrows (functions between values) to arrows. We already decided to use type constructors for mapping types to types, so for any arrow `f :: X -&gt; Y` we want an arrow `f' :: W X -&gt; W Y`. In Haskell, how can we map functions to functions? Well, with another function! So we want a function `fmap f = f'`. Looking at the type of `fmap :: (a -&gt; b) -&gt; (f a -&gt; f b)` it's important to remember that arrows in `a -&gt; b` and `f a -&gt; f b` are *categorical* arrows corresponding to source category (Hask) and target category (Hask), but the arrow in between is just something we used to express the mapping itself. * preserving the compositional structure. This is where the functor laws come from. We managed to find a way to map objects to objects and arrows to arrows, but mapping the compositional structure is trickier, so we delegate it to folklore and discipline. In a dependently typed language, we'd want proofs instead. You see, in Haskell there are three kinds of mappings: * mappings from types to types (parametrized data types, data families, type families) * mappings from values to values (functions) * mappings from types to values (classes) and to define the notion of a functor, we need all three. We need mappings from types to types in order to map objects of Hask to objects of Hask. We need mapping from values to values to map arrows of Hask to arrows of Hask. And we need classes to glue those together. First example, the `Maybe` data type. It has kind ★ -&gt; ★, so it can be used to map types to types. The corresponding mapping of arrows looks like this: fmap_Maybe :: (a -&gt; b) -&gt; (Maybe a -&gt; Maybe b) fmap_Maybe f Nothing = Nothing fmap_Maybe f (Just a) = Just (f a) Second example, the `(,) w` data type. It has kind ★ -&gt; ★, so it can be used to map types to types. The corresponding mapping of arrows looks like this: fmap_Writer :: (a -&gt; b) -&gt; ((w, a) -&gt; (w, b)) fmap_Writer f (w, a) = (w, f a) and so on. For a type constructor `W` we want to associate a lawful `fmap` operation, *together* they form a functor between Hask and Hask. So we use a class for this: class Functor (f :: ★ -&gt; ★) where fmap :: (a -&gt; b) -&gt; (f a -&gt; f b) instance Functor Maybe where fmap = fmap_Maybe instance Functor ((,) w) where fmap = fmap_Writer In category theory, we wrote F(A) to map objects and F(f) to map arrows. In Haskell, however, we write `f a` to map types and `fmap f` to map functions. There's no name overloading! The fact that mapping objects to objects and arrows to arrows are different processes is exposed. You might have heard of contravariant functors, defined like this: class Contravariant (f :: ★ -&gt; ★) where contramap :: (a -&gt; b) -&gt; (f b -&gt; f a) -- contramap id = id -- contramap f . contramap g = contramap (flip (.) f g) and as an example of an instance, take a look at `Predicate`: newtype Predicate a = Predicate { getPredicate :: a -&gt; Bool } instance Contravariant Predicate where contramap f g = Predicate (getPredicate g . f) Notice that in the definition of `Contravariant`, we take objects of Hask to objects of Hask, but the arrows `a -&gt; b` to inverted arrows `f b -&gt; f a`, and the composition to inverted composition (one of the laws uses `flip (.)` on the right hand side). We already discussed that a category with objects the same as Hask but inverted arrows and composition is called Hask(op). As an exercise, convince yourself that `Contravariant` is a class of functors from Hask to the dual category Hask(op). 
This blog is hard to read on mobile. 
There is also *Froid*: https://github.com/mchav/froid
Strange: it behaves pretty well when I resize my browser to absurdly small sizes.
`Traversable1` would require bringing `Apply` into base and preferably the class hierarchy. There hasn't been a serious proposal put forth to do so, as I think the folks who would prefer a really fine-grained class hierarchy are holding out hope for some form of super-class defaulting mechanism to reduce the pain on implementors.
I made a [ticket](https://ghc.haskell.org/trac/ghc/ticket/13573) for it (only `Foldable1`), I just have to send a proposal to the mailing list
It's a great idea. But putting semigroups into base and the corresponding type changes is actually a painful change. It breaks a lot of existing code, so it is an ongoing lengthy process with a multi-stage deprecation cycle over several years and GHC versions. The same would be needed for your proposal. Even so, I'm for it, if the GHC team is prepared to go through this again. It's the right thing to do.
Which do you think would be best? * Continue waiting * Bring them in as they are now * Bring them in with some modification that will less interfere with future work
Well, there are also purely 'closed' categories without monoidal structure, but you just don't see them much. Eilenberg and Kelly explored them in the 60s. Closed-only categories like that can be a good way to model things like the lambda calculus. In such a setting, (&lt;*&gt;) and pure are precisely the right operations. In a closed monoidal category properly respecting the closed structure or the monoidal structure mandates the other comes along for the ride. So neither the "strong lax monoidal endofunctor" nor the "closed functor" version properly subsumes the other. If we're going to talk about generalizations I prefer to simply think of it as "monoid object with respect to Day convolution", as Day convolution is more general still than the limited form we use. Go find me useful applicatives on promonoidal categories!
Just to recap: `Apply` is `Applicative` without `pure`, `Semigroup` is `Monoid` without neutral element.. however at present I cannot grasp the implications of having the resulting, weaker version of Foldable in base. Could you perhaps elaborate?
At the moment, they aren't seeing a lot of use in the community in their current home, so it seems to be a poor trade-off to move them into base and make everyone pay for an extra level of abstraction that isn't seeing much use. The most interesting canonical instances of `Semiapplicative`/`Semimonad` (Apply/Bind) are things like `IntMap`, `Map k`, `HashMap k`. That said, `Foldable1` could move into base without pain, though. I believe Iceland_Jack has a libraries proposal along those lines. I have no objection there. The scaffolding is already in place, having Semigroup as a superclass of Monoid, and the fact that it uses semigroup contravariantly compared to Foldable using Monoid contravariantly makes it painless, as it is a subclass of Foldable, rather than a superclass. New subclasses are an easy pay-as-you-go upgrade to base. At present I'd say we're by far most likely to move `Foldable1` into base and see if that encourages folks to think about structures without units to drum up interest in splitting up the class hierarchy further. Moving `Apply` and `Bind` into base would result in a heck of a lot of operator duplication and a messy user migration plan. Every such migration plan I can see with the tools available to us today would result in a pretty big backlash. There I'd advocate the "continue waiting", with a side of trying to get folks to agree on a superclass defaulting mechanism so we can finally start splitting up big classes without inducing undue pain.
You get total versions of `foldl1`, `foldr1`, `maximum`, `minimum`, `head`, `last`... for suitable containers like `NonEmpty`.
We already have semigroups in `base`. They are becoming a superclass of Monoid shortly. It has taken us 2-3 years to get this far. We simply haven't begun the process on the Semiapplicative/Semimonad front. Unfortunately, the breakage surface and the API duplication is much larger there given currently available tooling.
If I limit myself to just monads that I personally have worked on or where I've worked on some variant thereof: [machines](http://hackage.haskell.org/package/machines) provides a monad like pipes for streaming processes. [monadic revision control](https://www.microsoft.com/en-us/research/publication/two-for-the-price-of-one-a-model-for-parallel-and-incremental-computation/) gives a monad for parallel/incremental computation through a form of revision control. [monad-par](https://hackage.haskell.org/package/monad-par-0.3.4.8/docs/Control-Monad-Par.html) provides a monad for deterministic fork-join parallelism [codensity](https://hackage.haskell.org/package/kan-extensions-5.0.2/docs/Control-Monad-Codensity.html) provides a general way to compute a "difference monad" for a given monad. Like a difference monoid or difference list it right associates all of your joins/mappends, which can matter for monads with expensive left associated operations. [bound](http://hackage.haskell.org/package/bound) provides a monad for capture avoiding substitution in syntax trees. It lets you make your expression type a monad in its own right, and provides a monad transformer you can use to help deal with binding variables. [linear](http://hackage.haskell.org/package/linear) uses monads for things like Complex, Quaternion, as they are all isomorphic to memoized vrsions of Reader from an appropriate basis. [ersatz](http://hackage.haskell.org/package/ersatz) provides a monad for building up problems and interpreting results of feeding those problems to external SAT solvers. [free](http://hackage.haskell.org/package/free) provides various flavors of free monads, letting up build up whatever monad you want with a given set of operations. [hyperfunctions](http://hackage.haskell.org/package/hyperfunctions-0/docs/Control-Monad-Hyper.html) -- hyperfunctions actually form a monad. Mealy machines data Mealy a b = Mealy { runMealy :: a -&gt; (b, Mealy a b) } form a monad. Using the same notion of representable functors that makes linear go, we can make representable versions of Mealy and Moore: https://www.schoolofhaskell.com/user/edwardk/moore/for-less These can be useful for running state machines in reduced time over compressed data sets -- because we an make a monad for LZ78 compressed input streams!: [compressed](http://hackage.haskell.org/package/compressed) We can make a monad for multi-pass calculations over a given dataset: [multipass](http://hackage.haskell.org/package/multipass-0.1.0.2/docs/Data-Pass-Calc.html) Userspace RCU: [rcu](http://hackage.haskell.org/package/rcu) Lazy promises: [promises](http://hackage.haskell.org/package/promises) Parsers with fancy error message handling: [trifecta](http://hackage.haskell.org/package/trifecta) Pretty printing with substitution: [wl-pprint-extras](http://hackage.haskell.org/package/wl-pprint-extras) The rabbit hole goes a lot deeper: Every comonad gives rise to a monad transformer: http://hackage.haskell.org/package/kan-extensions-5.0.2/docs/Control-Monad-Co.html Then we can move on to monads in or on other categories/2-categories. e.g. I provide a dozen or so monads _on_ the category of profunctors: http://hackage.haskell.org/package/profunctors-5.2/docs/Data-Profunctor-Monad.html and arrows can be viewed as the strong monads _in_ the category of profunctors. newtype Free p a = Free { runFree :: p r =&gt; forall r. (a -&gt; r) -&gt; r } provides a fancy way to talk about a `Free Monoid` or `Free Semigroup`. Minor variations allow you to talk about a `Free Functor`, which is a form of the Yoneda lemma. This is really only the start, the rabbit hole keeps going deeper from there.
Every type in my parser pet parser combinator library https://github.com/zarazek/parsers has monad instance, from relatively simple: https://github.com/zarazek/parsers/blob/master/src/Text/Parsing/Maybe/Simple.hs to this monstrosity: https://github.com/zarazek/parsers/blob/master/src/Text/Parsing/Generic/MostGeneric.hs
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [zarazek/parsers/.../**Simple.hs** (master → aff3810)](https://github.com/zarazek/parsers/blob/aff3810a2b5052b7718f1ace1e3271f7b7710ee6/src/Text/Parsing/Maybe/Simple.hs) * [zarazek/parsers/.../**MostGeneric.hs** (master → aff3810)](https://github.com/zarazek/parsers/blob/aff3810a2b5052b7718f1ace1e3271f7b7710ee6/src/Text/Parsing/Generic/MostGeneric.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhz826m.)^.
You know what else is helpful? Barracks. Let them sleep together. They'll spend more time discussing ways to make your company more money.
The main benefit of this over `MonadError` is the performance, right? Since exceptions are fast and `(&gt;&gt;=)` with `ExceptT` is less fast? Otherwise I don't see a significant difference =P
There are plenty of workarounds for that, such as `ether`, or even just a function like this: withExceptT :: MonadError e m =&gt; (e' -&gt; e) -&gt; ExceptT e' m a -&gt; m a withExceptT f = either (throwError . f) return &lt;=&lt; runExceptT But yea, point taken. It's definitely not as convenient.
I don't think this would be that unusual: data Foo = Foo data Bar = Bar data FooBar = F Foo | B Bar foo :: MonadError Foo m =&gt; m () foo = ... bar :: MonadError Bar m =&gt; m () bar = ... foobar :: MonadError FooBar m =&gt; m () foobar = do withExceptT F foo withExceptT B bar The type `ExceptT e' (ExceptT e m) a` never concretely shows up. You're just composing `MonadError`s.
Better composition (constraints just pile up), and also `MonadThrow` is a lot nicer than `MonadError` in many cases, IMO. There are more instances, too! (STM, ContT, Q, [], Maybe, ...)
I've never found `MonadThrow` to be nicer to be honest. It's basically just the class of "You can lift and catch IO exceptions." For errors that should be expected, I'm more a fan of non-IO erroring; and for errors that should be unexpected, you can use `liftIO . throwIO`. And I'm not a fan of throwing away the exception in instances like `[]` and `Maybe`, so I don't think the other instances are very compelling cases, except *maybe* STM. Really, I've just never found myself reaching for `MonadThrow` for anything at all. Either I want the safety and it's bad, or I don't and it's unnecessary.
To use Matterhorn with version 3.6.0 of the server, you need to get the matching version of the client: https://github.com/matterhorn-chat/matterhorn/releases/tag/30600.2.4
Rock! ✊ I lose
I found `parseInt`, but it is impure :-(
Anyone implemented the new algo for short Presburger arithmetic? http://www.math.ucla.edu/~pak/papers/presburger4.pdf No new bounds for matrix multiplication :( 
You might consider [repa](https://hackage.haskell.org/package/repa).
I've never used it, but [HROOT](http://ianwookim.org/HROOT/) might be of some interest.
I personally like the `Maybe` version better, as it avoids the very exponential behavior you can get with the `Applicative`, and `Monad` instances of the `[]` version. (e.g `replicateM` is exponential in the `Int` parameter).
About Frege: the latest Maven build is broken, there is no way to understand how to fix it. It looks like there is not much active development on it in the last month.
Yeah, sure. I like the list version for learning because it's fairly easy to reason about, you don't really have to worry about backtracking.
That's a fair point, although I am actually kind of surprised that `ReadP` / `ReadS` is implemented that way.
Whoop! I'm Edvard Hübinette, and I'll be working with the lintypes implementation of the streaming library. :) 
This may be a good time to ask if anyone has a status update on DPH, which is designed to tackle these problems. In particular, the dph examples in the wiki seem out of date. 
That works, thanks
"faster" for what kinds of operations?
A number of years ago, I built GHC under QEMU user mode as an experiment. The build took over 24 hours. At that time building GHC natively on the same machine took less than 30 minutes. 
https://webghc.github.io/design
Mentor for the project here. This is the current plan. GHCJS is far too diverged for us to consider porting it in a summer, but GHC already has a convenient LLVM backend, and LLVM already has a convenient WebAssembly backend. And I think there are very clear advantages to remaining in the ordinary GHC pipeline as merely an alternative backend. Plus, this way we'll need way fewer shims to hijack and replace the C code that a lot of libraries include; we're hoping a lot of that C code will just compile with Clang and work. As for compatibility, Reflex has already moved to being based on JSaddle, which is a GHCJS-DOM replacement that's meant to abstract the implementation of the FFI; we're hoping that when we get to this point (probably not this summer), we can just create a new JSaddle backend that marshals across WebAssembly's own FFI to get stuff like Reflex to work.
Wouldn't typeclasses also work to solve this issue?
I just this week finished translating the beginner articles from LearnOpenGL.com into haskell: https://lokathor.gitbooks.io/using-haskell/content/opengl-getting-started.html It's for desktop 3.3, not ES 3.0, but the general explanations of how to look at C and write the same in haskell might help you use a guide thats focused on ES 3.0 Edit: I noticed you want the OpenGL package and not OpenGLRaw or gl. Unfortunately I can't help there. OpenGL throws a lot of its own special abstractions over the normal ogl workflow.
Fantastic! Best of luck!
Thanks for the link, and by extension, the references. I'm suspicious the property I mentioned has something to do with free Algebras. Is exercise 32 in "Generic Programming with adjunctions" a good exercise to explore this connection? And a related question. For the purposes of understanding programming, is category theory a more "natural" perspective then algebra? To the point that studying categories *before* algebra is actually more useful? 
And, if we are so bold, we can eventually remove the partial functions versions from the `Foldable` type-class!
I don't think there are tutorials for `OpenGL` package out there. I advise use `OpenGLRaw` or `gl` package and follow any OpenGL tutorial for C/C++ language. http://ogldev.atspace.co.uk/index.html for instance.
Is there a reason you chose to build a standalone library rather than build off of `stylish-haskell` or `hindent`? Curious what went into making that choice.
There are haskell bindings to tensor flow (https://github.com/tensorflow/haskell). They are unofficial but worked on by people at google. There was a talk on this at BayHac this year (https://www.youtube.com/watch?v=3jLsN8gHx68&amp;index=14&amp;list=PL5lgjzYOvyYNchlkMzvDqd1F6gS-COCDo). I personally do quite a bit of tensor flow but have not used the haskell bindings. They don't (or didn't last I looked) have bindings for the layers interface which is a higher level abstraction for building models. I use the python API and just grit my teeth. The same thing goes for our other analytics work. We use spark and so build a fair amount of stuff in scala. I spend a fair amount of time in zeppelin doing research and also use another python notebook environment as part of my workflow. If you want to leverage your GPU you can try the accelerate library. I used it quite a bit more before I took up tensorflow and had good results with it. I wish there was better news. I prefer haskell as a language but when building ML/analysis stuff the libraries I can leverage in other languages far outweigh what haskell brings to the table.
Everything is gonna be out in the open! We'll have everything on GitHub, Michael will be blogging plenty, and we're going to open a slack channel for anyone to join. We're hoping for this to evolve into a community effort over time.
Also [Brittany](https://github.com/lspitzner/brittany). One of my only remaining complaints about it is the lack of a highly configurable import formatter.
Wonderful what's the repo/slack channel? Or if you don't have it yet will you pm me it when you do?
Once we have things a bit more organized we'll make a post on /r/Haskell =) The GitHub organization is [WebGHC](https://github.com/WebGHC)
Avoid success at all costs? 
Really hard to read on my moto G4 as well.
Then what do you do if you have type A that has a perfectly lawful and logical instance of class B in isolation, and for class C in isolation, but those instances do not obey the combined laws, then what do you do? You basically have to just arbitrarily prioritize one instance and require a newtype or similar for users who want the other instance, pretty annoying. For a more concrete example, lets think of `Set`, `Ring` and `Lattice`. Lets say someone wanted to make it a law that anything that is a `Ring` and a `Lattice` is a `BooleanAlgebra`. Or in other words: `\p q -&gt; (p /\ not q) \/ (not p /\ q) = (+)` and `(/\) = (*)`. So now you make `(+)` for `Set` be symmetric difference, you make `(*)` and `/\` be intersection, and you make `\/` be union. Now you want to make `Int` a `Ring`, which is simple, `(+)` is addition, and `(*)` is multiplication. But you also want to make it a `Lattice` (or rather you make it a total ordering, which gives rise to a lattice), where `/\` is `min`, and `\/` is `max`. But you have now violated the combined laws, so you have to either drop one of the above, or perhaps come up with some much less intuitive `Ring` and `Lattice` instances for `Int`, to maybe see if you can form some sort of `BooleanAlgebra`. You might now say that the real mistake was making `BooleanAlgebra` specifically an implied law instead of an actual class, and that the concept as a whole is still fine for a more carefully selected set of combined laws. But it seems to me as though actually avoiding such a situation is relatively challenging. So we have clearly established a downside for implied combined laws. I will now show why I think there is little to no upside. The whole point of these laws is so that if you see some function or operator in your code, you will have an idea of what should happen. But if you see an operator for just class `B` in your code, with no guarantee that a class `C` instance exists, then you gain zero code reasoning ability from the combined law. So either you need to require both `B` and `C` together to reason about your code, at which point you should perhaps unify `B` and `C` into class `BC` instead, as neither is a good enough class on its own. Or perhaps you don't need the unifying law to sufficiently reason about your code, in which case you don't gain much from the unifying law, and you pay for it the way I mentioned above. To top it off it generally seems as though no one has been able to come up with many good "combined laws", except for the ones that come from free theorems. For one example `Foldable + Applicative =&gt; toList . pure == (: [])` is violated by `ZipList`, and I can pretty much guarantee that most of the attempts at unifying laws (particularly those aimed at `Foldable`, as many take issue with its lawlessness), have perfectly reasonable exceptions like the above.
&gt;At the moment, they aren't seeing a lot of use in the community in their current home, so it seems to be a poor trade-off to move them into base and make everyone pay for an extra level of abstraction that isn't seeing much use. It could be that people don't like bringing in extra packages, or even know about their existance!
Typeclasses that contain methods with positive-only occurrences of a type, and other methods with negative-only occurrences of a type tend to infer very badly. You wind up putting explicit type annotations or Proxy arguments on everything. This is doable, but it encumbers the entire package API. 
Fair enough.
It's impure? Huh? Do you have a link?
I guess it was really a more a series of events that lead me down this path. First I found `hindent`, but as /u/jmite alluded to, it's fairly inflexible and I wasn't sold on full-file formatting. Then I found `stylish-haskell` and agreed with it's "mostly just touch the imports and above" approach, but was unable to beat into doing what I wanted via configuration. I missed `brittany` some how, so at that point I looked into modifying `stylish-haskell` and found that, probably just due to where I am on the Haskell learning curve, the idea of writing something from scratch seemed more approachable/rewarding. That coupled with the fact that I want to have control so that I can evolve longboye quickly to meet my company's needs left me choosing this particular set of tradeoffs. That being said I'm all for anything we can contribute making it into any of the other libraries and will do whatever I can to help that happen if longboye produces anything of interest.
I have personally found that a `TypeApplications` heavy API (like the one in `react-hs`, previously named `react-flux`), is very practical and usable. IMO more so then avoiding implementing very useful and practical typeclasses and abstractions that don't always infer well. I mean it seems like the alternative is strictly worse than type applications, at least in my opinion. As with the concrete separate functions approach you end having to do things like `List.fold` or, `foldList` every time, whereas with the type applications approach you only *sometimes* have to do `fold @[]`. Also a nice side benefit is that generally you need to import less things. Another side benefit is that the type application tells you exactly what type you are dealing with, unambiguously, whereas with concrete functions you just have to hope that the name is good enough to infer the type you are dealing with. Now don't get me wrong, I love type inference, and I would never suggest adding features that majorly affect it, such as implicit casting. But I do think that positive-only occurrences of a type in a typeclass are better than having a unique name or module for each type.
Could you explain the problem?
It is described here: https://github.com/Frege/frege/issues/327 If you want to try to solve it, you should take a look at this project: https://github.com/talios/frege-bundle
Typeclasses (and/or records with functions) can solve the issue. But sometimes they feel awkward: - If your library only ever uses one type of string at the same time, and its functionality has actually nothing to do with string conversion, requiring the typeclass constraint in each of your functions clutters the API with extraneous detail. And, in a way, it is a violation of DRY. It would be nice to say to GHC: "look, I'm just going to use an abstract type Foo which should implements these operations and typeclasses, don't make me repeat the constraints on each function". - Using typeclasses to parameterize internal aspects of datatypes and functions also feels awkward. Consider the type [Doc](http://hackage.haskell.org/package/wl-pprint-1.2/docs/Text-PrettyPrint-Leijen.html#t:Doc) from wl-pprint, which contains a string. Making the api generic using typeclasses would required adding a type parameter to `Doc`. Using backpack, `Doc` would remain as it is, but its internals would contain a reference to an abstract type.
Does your tutorial have any relation to [atwupack/LearnOpenGL](https://github.com/atwupack/LearnOpenGL) on GitHub? It seems to be going off of the same C++ guide.
No relation. Though, glancing at their cabal file a bit, they do seem to use the OpenGL package, so perhaps that will help you.
`TypeApplications` are indeed useful, but if we find ourselves repeatedly applying the *same* type to make an API work, that's a bit of a smell. `fold @[]` is not like that because in a program we might be interested in folding across multiple types of things
I would say making `BooleanAlgebra` a separate class is most definitely the correct solution and perhaps that should be the case with the `Binary` class as well. If you want to know that `serialize . deserialize == id`, require an additional constraint. That way you know that if there's an instance out there, it had to be aware of which two functions it was talking about.
While games are performance intensive, they're usually not really data heavy. I think John Carmack said that the whole game state for some shooter (don't remember which one) was around 4mb. 
&gt; sort = foldMap id :: [a] -&gt; Sorted a -- Sorted lists are a monoid with merge as the operator. Wow! I learned something new today. Looks like it's n^2 though, might work better on binary trees.
UML diagrams are quite simply misdesigned - they mix all sorts of concerns in an entirely unprincipled way. If you do want a graphical representation of an FP design for some reason (say, whiteboard sketching), you can use string diagrams/birdtracks/proof nets, which basically covers the same space as activity diagrams or data-flow diagrams in UML. I suppose that one *could* also come up with a somewhat more proncipled version of UML "structure diagrams" , perhaps by reusing the aforementioned string diagrams for a "type level" representation - but it would be simply overkill. `data [a] = Nil | a : [a]`is a hell of a lot clearer than something like (X) --&gt;-+ |\ | &gt;-+--&gt; [a] a |/ | ---&gt;+---+ | | |(X)|-+ v +--&gt;+---+ | | | +------&lt;------+
Take a 4000 line module, now parameterize everything in it on the choice of random number generator you are going to use. Those are the cases where we might want a decent module system. You _can_ choke down distributing hundreds of Proxies through all the functions and data types in the module, or you can accept @ annotations and scoped type variables used on every internal function call, and having to AllowAmbiguousTypes and catch compilation issues a week later when you finally go to use the resulting API. But it sure would be nice not to have toQ These are the scenarios I envision Backpack being most useful to help with. We can let Backpack effectively curry that choice out to the module definition, and we don't have to infect all the code in the entire module. If we later on decide we want to lock down the module to a specific case, it is fairly straightforward. For testing multiple possible implementations of maps, and fiddly intricate bits, etc.? Sign me up. I'm far less sanguine about refactoring huge swathes of code against more generic string type APIs and losing insight into which actual operations you should be using for each type to actually construct things with the right asymptotics and the like. There are a bunch of typeclasses that folks have that provide ad hoc conversions or tooling for working with different string types, but none of those APIs really let you capture the relative performance characteristics of those operations today, so you tend to try to use them suboptimally internally, or you just choose to pay to convert at the edges of your API.
"lack of a highly configurable *x* formatter" is a *feature* ;)
Been waiting for the denouement of these posts. Good to hear there is a concrete solution happening. I have some home IOT hobby stuff (target RPi 3) that I started on the assumption that someone, somewhere, somehow would take on the mantle of getting it done. Thank you for doing so. 
When talking about a large MMORPG, keep in mind that you're creating a client and a server. On the client, the same thing applies - lots of textures and not so much game state. On the server, there is obviously more information to process, but then you're not rendering graphics in realtime so GC pauses are not a big deal.
My understanding is that GHC is specifically tuned to handle this kind of thing. https://wiki.haskell.org/GHC/Memory_Management 
Only when writing my own games, at which point I fix the problem by calling [`performGC`](http://hackage.haskell.org/package/base-4.9.1.0/docs/System-Mem.html#v:performGC) on every frame. Doing so causes the frame rate to drop but the pauses disappear; or rather, the pauses occur on every frame, so they aren't distracting as when they only happen once in a while. This approach has a cost: like I said, the frame rate drops, it alternatively you have less time to compute the next frame, if your framerate is fixed. My games are very simple so there isn't a lot of work to do per frame anyway, but AAA game developers usually want to squeeze in a lot of stuff in every frame, which is probably why they tend to disregard garbage-collected languages. Up to now I was thinking that calling `performGC` on every frame would even out the time spent on GC, because there is less garbage to collect each time, but now that I think about it, in GHC the garbage collection time is proportional to the amount of remaining data, not the amount of garbage, isn't it? It would only take less time if I was performing a minor GC pass and only looking at the most recent generation, but then we'd still have to perform a major pass once in a while and so we'd still have pauses. So that puts /u/Darwin226's comment into perspective: having very little data overall allows us to perform major GC passes quickly as well.
Oh, undoubtedly. I'm not saying Haskell/GHC is disadvantaged here compared to say Java, just pointing out that almost all game state is updated every frame and that modern 3D rendering engines make surprisingly little use of temporal coherence for object persistence; it's easier and faster to build everything from scratch every frame especially in C++ where you can control object allocation to use persistent buffers and avoid deallocation, fragmentation etc by just marking the whole thing as unused at the end of the frame. I'm not sure what implications this has for really high end dynamic 3D graphics in Haskell, but I suspect that it's not possible to reach the same kind of rendering complexity with the same hardware.
I feel that Haskell should provide a more fine-grained distinction between the different types of TH expressions. - Pure expressions, useful for generating code (think aeson FromJSON/toJSON instances). No need for IO there. - Access to build-machine filesystem. For embedding files and other data into the binary. - Access to the host filesystem. Please somebody come up with an example, because I don't see a use-case for that. This whole complicated 'TH code must be generated on the host machine' only accounts for a fraction of the use cases. Most are code generation, which is pure.
thanks for sharing that's pretty interesting. Would it make sense to call performGC periodically instead of the same frame and find a balance point between determinism and performance?
Looking forward to this!
Assuming we *had* a multi-target ghc, which I hope we eventually will. We could compile everything for the build and host machine (or design some lazy on demand compilation), and use a Quasi instance that provides all but the `qRunIO` method. Then we could see if a splice could be evaluated IO free on the build system. And only defer to the host if needed. This sadly though wouldn't guarantee that a splice evaluated on the build machine is identical to the on evaluated on the host I'm afraid. E.g. the following library provides a `pure` function, that would result in different values depending on the target it is compiled for. {-# LANGUAGE CPP #-} wordSize :: Int wordSize = wORD_SIZE of course this is contrived, but unless we can guarantee that all CPP macros are target independent, purity alone doesn't provide us with the necessary guarantees I believe. Simply annotating something as safe to be run on the build system, or requiring annotation for expressions that are explicitly *unsafe* to run on the build system might get us there though. With the current setup you actually *can* differentiate between build and host file system. `TH.readFile` will provide you with access to the build system, while `TH.runIO . Prelude.readFile` will access the file on the host. I agree that I have yet to come across use case for this. The only hypothetical reason I could think of, is the intention to read some host specific file during compilation time. (e.g. say you want to embed the version of the system you compile against at compilation time to later verify it's the same version you are running on.) 
Ah ok I see now, so `ReadS` is implemented this way and is very slow, but `ReadP` is implemented in a much less naive way and might sort of perform well?
I am maintaining wl-pprint-annotated. Therefore I have to correct some things: - Doc is not abstract - There is a way to render to html/console via the wl-pprint-console package However there are undocumented functions I have to admit. I tried Text but it didn't give a performance boost since the strings get never really created due to laziness. There is a text branch https://github.com/minad/wl-pprint-annotated/tree/text You might have confused my package withone of the others.
Very cool.
Ok I think I see what you mean, so if standard use involves every function in an entire module / package being used with `@Text` (whenever inference fails), then it probably makes more sense to use backpack or some other more powerful module system? In that case I do see the benefit in a backpack style approach. I still kind of want true first class modules, so you could perhaps pass in a parameter to a module, even a type-level one (`-XTypeApplications` on modules? yes please!). That way you could do something like: import Foo.Bar @Text 1000 (...) module Foo.Bar :: forall a. StringLike a =&gt; Int -&gt; Module module Foo.Bar () where Where in this example `a` is our string interface used, and `Int` is perhaps some chunk size parameter or something like that.
Can't comment on the details of :step, because I have never used it. I can however assure you that it is absolutely not the best debugging tool we have - it's probably one of the weakest, being relatively new and heading into mostly uncharted territory. The reason for this is because step debugging, a classic imperative technique that is very revealing when you are reasoning operationally, is a stranger in pure functional programming, where reasoning is usually denotational / equational (and I believe the main benefit of pure functional languages is that they enable equational reasoning). Equational reasoning means we can use other debugging techniques: property checks unit tests, type checks, refactoring, and ad-hoc invocations. We don't usually care about "what happens next", we care about equalities, equivalencies, and formal properties. Occasionally, we do need to reason operationally, especially when we're interested in performance characteristics, and tooling in this area is a bit of an orphan child, although things have been improving a lot recently.
That does sound ideal! We had a few reasons that we were particularly attracted to Slack (code blocks, documents, separate channels, DMs, push notifications, and some others). I'll take a look at whether Gitter solves that stuff for us, because the GH integration and the use of GH as your login sounds much better than what Slack does.
It's funny, I made reference to the sad state of GHCi's debugger in a talk just yesterday. Indeed GHCi's interactive debugger is a bit tricky to use properly. I think this isn't necessarily the fault of the debugger concept, but rather merely a reflection of the difficulty of writing a usable text-mode debugger. I've generally found it easier to fall back on plain evaluation under GHCi and sometimes `pprTrace` and friends. Occasionally I do use the GHCi debugger, but I generally find myself wishing we had better tooling around it by the end of the ordeal. I think even a simple Emacs mode would greatly improve the situation.
Apologies! I seem to have overlooked that. 
Out of that list of desired features, document upload may be the only one missing. Maybe.
really? weird, this isn't my page and i dont care if it doesnt work hehe but i'm curious whats hard to read about it.. I've tried it on half a dozen things and looks fine on all of them so now i'm just curious
Yeah, I assume the complex type is for efficiency. I'm actually thinking of messing around with it myself, to get a better intuition for how it works.
I recently tried Hoed and found it to be reasonable. The test-based algorithmic debugging seemed pretty powerful if you have good failing test cases. Curious to know if others have better options / have had better experiences. 
I understand that, and I write a ton of unit tests / leverage laws etc... I don't care about specific evaluation order, what I'd like to see is the equational reasoning that ghc performs on a step by step basis. It's ridiculous to think that the 'best' tool we have for that would be manual expansion of expressions. The whole point of having something like that is to figure out where your 'one off' runs in ghci went wrong. Often a bug in recursive functions will be a simple matter of off-by-one or oops you flipped some arguments around, but it's not always clear and being able to step through and see when it goes wrong would be really cool. Laziness also muddles things quite a bit here since we won't actually evaluate thunks until needed but even seeing the unevaluated version of the thunk can be very revealing (assuming you could give it a code representation).
Do you know what kinds of APIs GHC offers for debugging / step evaluation? How difficult would it be to write a custom step-through debugger? 
Seconded. I'm really excited about this work too.
&gt; Often a bug in recursive functions will be a simple matter of off-by-one or oops you flipped some arguments around, but it's not always clear and being able to step through and see when it goes wrong would be really cool. The Chez Scheme REPL had an option to do a command `(trace f)` and then whenever `f` is executed, it prints the arguments it was called with. Super useful for recursive functions and functions that are nested. I frequently wish `ghci` had this feature.
Oddly enough, reddit isn't a homework service. 
Hi: Sorry for the delay. I just saw your message. No, I don't used any. but maybe clay for css would work well. https://hackage.haskell.org/package/clay. 
Sure, if you use the GC wisely you can have Haskell work at C# (Unity) or Java (Minecraft) speeds without needing to do anything too weird.
https://hackage.haskell.org/package/split-0.2.3.2/docs/Data-List-Split.html#v:chunksOf
Yea I was wondering if you couldn't leverage an existing formatter as a library and customize the parts you're interested in. 
There is the `Debug.Trace` module in base, with which you can do something similar, though it requires temporary modification of the code. Not as convenient as doing it on the REPL, but it can be very useful for debugging.
~~Whoa. I thought I had read GHC doesn't have anything like that. I am happily mistaken~~ =(
&gt; In the link I posted above they have a visual debugger for their pure fp language that provides a view into the equational reasoning happening behind the scenes. Indeed, I think it would be great if we had such a thing. If only there were more hours in the day...
Thanks for the article, One of these days I'm going to trip over bottom and fall forever.^There^exists^a^trap! 
I've got a bitcoin reward for people who can format a code block with markdown. Also, please do not post homework here.
Here you go; I prefer version 9: http://www.timetabling.com.au/
Agree. And I'm not saying it wouldn't be useful, just tried to explain why I think it hasn't been done yet.
Ouch. My hopes. =(
my hope is that the compiler becomes much more modular (syntax plugins, gc plugins, cleaner ghc api / ghci, "backend" plugins to better support transpilers like ghcjs and ghcvm, etc). but i think that the community will have to guarantee sharing responsibility, so that ghchq isn't maintaining the indirection without benefit. 
You can always use * `:step &lt;expr&gt;` to step into a specific expression; * `:steplocal` to only step within the current top level binding; and * `:stepmodule` to only step within the current module. [EDIT: fix logic error: I said, "top level *module*"; it should be top level *binding*.]
[removed]
I'd really like to split off the GHC 'package' into at least a package for surface syntax and Core, and if possible also for all the backends. I *think* the passes are largely independent anyway, so there's only the issue of who will do it eventually.
There's also https://github.com/mstksg/hamilton that treats Hamiltonian mechanics in general, in Haskell. In particular, there's a double pendulum example.
This is fixed (I hope). Following the [Quick Install](https://github.com/deech/fltkhs#quick-install) instructions should just work. Please let me know if it doesn't.
I can't possibly imagine why. Why restrict people in that way? There's a small number of things I don't like in Hindent, so since I can't configure it enough, I simply refuse to use it. I don't think that's the desired effect.
Maybe convert to a tail call helper function, accumulating reversed result?
Why would you want that? Unless you don't trust GHC, this is very idiomatic Haskell; they ought to get it pretty close to perfect, no?
Is it necessarily true that these sorts of liftings are the path forward? I've held out hope for another Wadler to solve the ugliness; is it proven that my hope is in vein, or are we still lacking creativity? I'd much rather improve our syntax if possible. 
May I quipe in some quick followup question? I am a beginner, but it seems too me that I should solve as much as possible by pattern matching, as it communicates quite well. Is there such a thing like an overuse of pattern matching?
Haskell's no Lisp for live editing, but I think it's important to note that if you have a fast enough compile cycle you can get some nice REPLish properties. I love that GHC is making fast compiles a priority. The 8.2.1 release candidate already compiles my project 3-4x faster and I think there's a SoC student working on GHC performance. All this makes me hopeful that I can have a moderately fast iteration cycle without losing type safety.
Thanks for another awesome writeup. Had many questions, but most of them were addressed by the discussions in the phabricator patches. What does *GHCSlave* do on the host machine ? Does it do the same work as *iserv-proxy* or is it something completely different ?
This seems like a rather good example. You might want to couple this with some form of version checking though. It also raises an interesting question: the way host IO would be used, would be mostly to access files in the `sysroot`. As GHC is not really a `sysroot` aware compiler (the `sysroot` awareness starts at the llvm toolchain for ghc cross compilers using llvm). And as such, if you wanted to read say `/usr/include/elf.h` the question becomes: do you want the one on the host (e.g. from the `sysroot`) or the one one on the build machine (which might not even exist on e.g. on macOS). This however works right now, by using `TH.RunIO . Prelude.readFile` for the host, or `TH.readFile` for the build machine. Even if we made GHC sysroot aware, how would GHC know which file `/usr/include/elf.h` referred to if everything went through `TH.readFile` if `/path/to/sysroot/user/include/elf.h` *and* `/usr/include/elf.h` exist? Thank you for providing an interesting reason for using host file io.
This helped me a lot: https://www.codewars.com/.
`lens` uses a form of general UML diagram in the intro documentation: http://i.imgur.com/ALlbPRa.png So, heck, even out mechanisms for describing field access can be diagrammed. ;)
I'm wondering, who gave the downvote, and why?
I'd suggest trying to implement Sokoban. It's a pretty comprehensive program but it's actually quite simple in Haskell: http://rubyquiz.com/quiz5.html
I found this after just emailing you about the same thing. As I wrote in the email, using `()` and `Void` you can recover uniqueness or emptiness by putting strictness annotations on the relevant fields of the expression type. Whether that is wise depends on context, but it at least allows the option of precise isomorphism.
That's true, but the current implementation makes it more imperative than it should be. Interactively evaluating the expression tree could be very useful, but the current step command provides very little control over the stepping process. You can limit scope to the top-level function but it doesn't work great. If you could choose which subexpressions to evaluate it would feel more functional. 
You likely want `foo [] = []` as well.
It's less about optimization and more about the mechanics of lazy evaluation. Tail recursion is really important in some contexts in Haskell, but other places it's a terrible idea. The result list, in this case, is operationally built from front to back, not the back-to-front order that always happens in something like Lisp. Think of it as more like building a linked list using mutation in C.
Exactly.
Yea /u/davidfeuer is right. There should be a leading case of `foo [] = []`
Agree whole heartedly. An interactive partial evaluator would be a super useful thing to have.
Criterion is right now preparing a new release and merged 2 of my additions from the past month, so seems reasonably maintained. 
Very cool. Off topic: Does anybody know a good font to render japanese? Most of the characters in the source code end up as question marks in my browser.
I wonder though, wouldn't the program repeated each time even compile? If I have use `do-notation` with the `Maybe` Monad, I can't just `print` which returns an `IO`. Should probably be `pure` or `return` instead, or am I missing something?
I learned Haskell with that also. And after that I learned Monad + Monad Transformer + MTL then I can make actual app that do effects (e.g. practical real world app :) )
Can you elaborate?
There is a language named Curry: https://en.m.wikipedia.org/wiki/Curry_(programming_language) But I suspect /u/tmpler is right. I don't recall if this is touched on in the history of Haskell paper, but it's a good read regardless: http://haskell.cs.yale.edu/wp-content/uploads/2011/02/history.pdf
Non-Mobile link: https://en.wikipedia.org/wiki/Curry_(programming_language) *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^72276
Another interpretation is that Haskell's parents named him after the programming language, just in a monadic context. This also explains the presence of non-well-founded recursion in the language.
Google's [Noto](https://www.google.com/get/noto/) family is pretty good.
[removed]
Thank you!
Comes up around 1h3m into the video: https://www.youtube.com/watch?v=re96UgMk6GQ&amp;feature=youtu.be&amp;t=3784 
What's wrong with being associated with Rocky Horror?
Correct me if I'm wrong, but this looks like a totally insecure protocol which trivially supports running arbitrary IO actions. If so, umm, yuck! I understand that when building in an offline environment, or on a secure network, that might be fine, but it would be best to make it easy to layer on a more secure transport, and to flag up clearly just what the security implication of running GHCSlave is.
I agree with you, I hope they can be replaced by *implication constraints*, where `Show1 f` is written forall a. Show a =&gt; Show (f a)
Thanks for your interest! [Here](http://gelisam.blogspot.com/p/games.html) is a list, and [here](https://gist.github.com/gelisam/27bf515548a9ce551a18#file-main-hs-L43) is a smaller example which demonstrates how to call `performGC` on every frame. `Gloss.play` does it for you, but in this example I demonstrate how to use gloss for rendering while keeping control of your main loop.
Write a command line todo-list app. Running the program should load you into a REPL (read-eval-print loop). A user should be able to enter a command by typing: `COMMAND:argument&lt;enter&gt;` i.e. a single word command followed by a colon, with everything to the right of the colon being the argument, then hitting the enter key. If a command takes no argument then the user should be allowed to omit the colon. Implement the following 5 commands: * "quit" (takes no arguments) - exits the application * "help" (takes no arguments) - prints these command descriptions * "add" (takes any non-empty string as an argument) - adds its argument to the end of the todo-list * "remove" (takes a non-negative integer less than the size of the todo-list as an argument) - removes the element at the index specified by the argument * "list" (takes no arguments) - prints the todo-list to the screen in the format "element_position - message" If the user enters an invalid command or an invalid argument, print a helpful error message and let them try again. Extra credit: * "save" (takes a filepath as an argument) - saves the todo-list to the specified file * "load" (takes a filepath as an argument) - replaces the current todo-list with one contained in the specified file Extra extra credit: If the user has made unsaved changes and tries to quit or load, they should first be presented with an interactive dialog that will help them save to a file if they'd like to. Happy to code review at any stage.
Semla/Semlor (plural) is a Swedish pastry, interesting. 
My problem with ghc debugger is that wheneve I put a breakpoint, I have no idea what the context is. `show binding` only show value on the line, but there is no way know the actual paramater of the function (You can't force them as they are not in scope). I've been told that this was a feature, but I don't understand how one can debug without that. I used cli for years without any problem but I've never managed to get something usefull from ghc debugger. Am I the only one ? 
I agree. What we need is a something in a text editor, when you can select an expression and get it replace by it's value. T
Here is information on the naming: [link](https://youtu.be/LnX3B9oaKzw?t=3m6s) I don't get why the reference to tim curry would be bad, though.
This is where we start spreading the myth that currying is named after tim curry
You may be interested in _ghci visual debugger_: http://felsin9.de/nnis/ghc-vis/#basic-usage
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [Gabriel439/Haskell-Optparse-Generic-Library/.../**Generic.hs#L106** (master → 5632ef2)](https://github.com/Gabriel439/Haskell-Optparse-Generic-Library/blob/5632ef21eee7eba367ed28d72bfd938e3857706a/src/Options/Generic.hs#L106) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply di2nhh5.)^.
The people who have seen Rocky Horror 0 or 1 times will have a different answer from the people who have seen it 2 or more times.
I'm not sure what you mean by "bake" in this context. If you mean "evaulate a Template Haskell splice so that you can use the resulting code later without needing a Template Haskell dependency", then you don't need anything other than GHC itself. You can use the `-ddump-splices` flag to have GHC emit the code that gets spliced underneath the hood. For instance, if you take this code (which uses `lens`): {-# LANGUAGE TemplateHaskell #-} module Example where import Control.Lens data Foo = Foo { _bar :: Int, _baz :: Bool } $(makeLenses ''Foo) and compile it with `-ddump-splices`, you'll get: $ ghc Example.hs -ddump-splices [1 of 1] Compiling Example ( Example.hs, Example.o ) Example.hs:7:3-18: Splicing declarations makeLenses ''Foo ======&gt; bar :: Lens' Foo Int bar f_a6iN (Foo x1_a6iO x2_a6iP) = fmap (\ y1_a6iQ -&gt; Foo y1_a6iQ x2_a6iP) (f_a6iN x1_a6iO) {-# INLINE bar #-} baz :: Lens' Foo Bool baz f_a6iR (Foo x1_a6iS x2_a6iT) = fmap (\ y1_a6iU -&gt; Foo x1_a6iS y1_a6iU) (f_a6iR x2_a6iT) {-# INLINE baz #-}
But that doesn't put it inline, which could be useful in some contexts.
If you were trying to distribute this C++ library alone, how would you do that? That actually seems a lot harder than distributing some Haskell bindings; so I'd just add the Haskell bindings to whatever distribution the C++ library needs.
I think this falls under the "pretty-printing" deviation, and it would be easy enough to have a `Read` instance that matched.
I don't visit there enough to try and force content to move over there (anymore).
What I want , is something (automatic) which replace TH Splice, by it's evaluation in the source file itself, to (hopefully) speed up compilation. We could have (as code generator use to do), markers (comments) delimitting the slices result and keeping the original splice, so that the expanded splices can be regenerated if needed.
I'm using stack , how can pass it the `-ddump-splices` ?
+1 with the amount of TH one needs to write to get around Haskell boilerplate, it's about time TH got some love. 
The library is currently only distributed as source through github. And it wouldn't make sense for the haskell package to be bundled with it. The package does more than just provide bindings to haskell. Also the goal is to be able to just be able to install it with cabal from anywhere.
Hm, seems to be the way to go. Although I would like to be able to also target windows. Thank you.
Your blog post is perfectly fine as it is! I agree that this doesn't need to go into the parser. I think `optparse-generic` and your approach address different concerns that may be composed together orthogonally. command line -+ | | v | helpfully annotated record | with optional fields | optparse-generic | | v | unannotated record | with optional fields -+ | | Partial options monoid v | final record &lt;- default values -+ There are 4 record types here, which can be expressed as variants of a single one, with the various steps connecting them derived generically.
I don't use Windows, but it seems that Nix work in the Linux subsystem in Windows 10. I have no idea how that works, but it's worth looking.
I'm glad it's Haskell, although Steph Curry definitely deserves to have a language named after him.
I use sometimes the Identity trick but found it cumbersome as well. This is why I created [metamorphosis](https://github.com/maxigit/Metamorphosis), so that you can create a parametrized type from a plain one (and convert between each ones). I used it and find it much better than the Identity trick. The downside is the use of Template Haskell.
There is no guarantee apart from. A comment saying this "this code has been generated, don't mess around with it". It's not perfect but usually much better than doing everything manually.
As you are on the Haskell subreddit: Haskell ! :-;
It's hardly like we're overwhelmed here. 
You may have already answered your question by posting here (: I have spent very little time in o'caml, so these are mostly outsider observations. There are some decent blog posts out there comparing Haskell, ocaml, scala. O'caml has an interesting module system (Haskell is working hard on that, but it will be a little while longer), and a number of static analysis tools implemented for imperative languages implemented in it. (Edit: proof assistants Isabelle and Coq are implemented in variants of ML). I don't think immutability is wired as deeply into the o'caml ecosystem (yet?). The more immutable your data is, the more your types can tell the full story. I am not sure where O'caml is on providing dependent type extensions currently. There is a systems level company in CA that uses o'caml - Ahrefs. There are a few network security companies using Haskell. There are data analysis and ML companies starting to use Haskell. There are probably more "real world" tutorials and books for Haskell of late.
In bound, the type of `Expr` winds up being data Expr a = App (Expr a) (Expr a) | Lam (Expr (Maybe (Expr a))) | Var a | Plus (Expr a) (Expr a) | Lit Int deriving (Functor, Foldable, Traversable, Show) instance Applicative Expr where pure = Var (&lt;*&gt;) = ap instance Monad Expr where App f x &gt;&gt;= g = App (f &gt;&gt;= g) (x &gt;&gt;= g) Var a &gt;&gt;= g = g a Plus x y &gt;&gt;= g = Plus (x &gt;&gt;= g) (y &gt;&gt;= g) Lit x &gt;&gt;= _ = Lit x Lam e &gt;&gt;= g = Lam $ fmap (fmap (&gt;&gt;= g)) e fromScope :: Expr (Maybe (Expr a)) -&gt; Expr (Maybe a) fromScope e = e &gt;&gt;= maybe (return Nothing) (fmap Just) instance Eq a =&gt; Eq (Expr a) where App f x == App g y = f == g &amp;&amp; x == y Var a == Var b = a == b Lit x == Lit y = x == y Plus w x == Plus y z = w == y &amp;&amp; x == z Lam e == Lam f = fromScope e == fromScope f _ == _ = False Note that the type `Lam` has inside is `Expr (Maybe (Expr a))`. This means that whole subtrees with no occurrences of the bound variable can be incremented with one `Just`, instead of traversing over and applying `Just` to every `Var`.
Questions rarely go unanswered over there though.
You can always define your own type that has the same Monoid instance as Last but a different ParseFields instance
Indeed
Haskell and OCaml are comparable, while Idris is really something else. Also, Idris has almost no ecosystem...
Was also wondering if those names referred to the pastry or something else
Haskell will force you to give up your old ways and step completely into the land of FP. Go with it first and then slide back a bit toward imperative land with OCaml later if you like.
My motivation to learn Haskell was to get experience with pure FP. OCaml allows imperative code so you lose some of the rigor I think. There's a great Haskell coding intro here: http://learnyouahaskell.com/introduction
it's spammy to cross-post your so question to reddit just to get more eyes on it
Thanks for your help. It clarified some things.
Curry was already used for this awesome food.
Well, I started of with Haskell, because the language forces you to use functional programming and also because I wanted to read Learn You Haskell for Greater Good. I think that Haskell is the perfect intro to FP, so I would suggest to start with it. Although after some time I switched to Ocaml, because it also has a good book called Real World OCaml and also because I got interested in Programming Language Theory(compilers, proof assistants, etc..). I think that Ocaml can only be used to it's fullest if you understand the main differences, between the paradigms that Ocaml offers(FP, imperativ and OOP).
Ocaml is a great language. Ocaml was my favorite language from 2004 to 2008 when I transitioned to Haskell. Ocaml was a great preparation for Haskell. It allowed me to get things done with a mixture of functional and imperative programming. However I ended up switching to Haskell for two reasons; a) Haskell has a much larger and more dynamic community and b) I preferred the way Haskell encodes differences between pure and impure functions in the type system.
No particular reason. It would work just as well with a closed type family, but they are less backwards compatible and I think their main value is in their semantics with overlapping families.
Why not both?
I've used both languages a lot. My current job is in Haskell and my previous (along with two internships) was in OCaml. With that experience, I would use Haskell over OCaml for basically everything. If you're curious about details, I wrote an [in-depth post](https://www.quora.com/Which-of-Haskell-and-OCaml-is-more-practical/answer/Tikhon-Jelvis) about it a few years back. Core points: - Haskell is *massively* more expressive in practice. The OCaml code I worked on was routinely more verbose and less flexible than the Haskell version would have been. Typeclasses and higher-kinded types make an *immense* difference, powering everything from lens to QuickCheck to mtl... - Haskell's parallelism and concurrency story is incredible. OCaml's... isn't. I've used both LWT and Async in OCaml and while they get the job done most of the time (unless you're computaiton-bound), they're not nearly as powerful or convenient as Haskell's threads and high-level abstractions like STM. - I find Haskell's libraries to be much stronger than OCaml's overall. A big part of this is that the language is expressive enough to support some really high-level, general abstractions that would be too awkward to use in OCaml. - Controlled effects (IO, ST... etc) are a *revolution* for software maintainability. I would hate going back to potentially impure code everywhere. At the end of the day there are a couple of things I really miss from OCaml (polymorphic variants and a record system that doesn't suck). On the other hand, I missed *a ton* of things from Haskell back when I was doing OCaml. It all comes back to flexibility and expressiveness, and I found Haskell much stronger in that regard.
Cobol, Algol, Snobol, Pascal, Haskell. Haskell sounds like a computer language. 
I second this! They are really fun in haskell, and for someone who is also just learning there usually solutions in haskell in the threads as well which are nice to compare your code to.
&gt;I read lot about ocaml and find it more easier syntax wise but the lack of libraries in comparison to Haskell "Easier" is a weird design choice. I can't say either is definitively better. Haskell's ecosystem is quite vibrant, though and it has a surprising amount of great tooling - hoogle, criterion, QuickCheck, haddock being my favorites in terms of novelty. 
&gt; I would also like some suggestions on tiny things (projects) where Haskell would shine Haskell makes hard things easy and easy things hard. So I don't know of any super small projects. However, I did build a [small text-generation language](https://github.com/vmchale/madlibs) in about 1000 lines which was quite fun. 
Out of curiosity, what makes it so different? I knew it had dependent types, but does that really change the whole language? 
Right, except even as a joke this makes no sense. Yesod is faster than django and Ruby on Rails. 
 All the code is out there. So yes, this can already be experimented with. In principle you should be able to emulate the host machine. E.g. [Raspberry Pi via QEMU](https://ownyourbits.com/2017/02/06/raspbian-on-qemu-with-network-access/)
&gt; The only result that didn't make sense was the "diversity" one. I can't find a reference to "diversity" in the results, could you please give a pointer? 
In [this talk](https://youtu.be/06x8Wf2r2Mc) SPJ says that there'd be too many ways to joke about the language if it were called "Curry", so it was named after Haskell Curry's first name instead. 
As to taking on dependencies, the survey also shows that many of us don't carelessly pick up dependencies (I'm looking at you, `leftpad`), see the ["I usually do significant research to assess the quality of the package or its maintainers" responses](http://breakingapis.org/survey/downstream.html#abst_dep_aversion?ecos=Haskell_Cabal_Hackage_,Haskell_Stack_Stackage_). 
What I found rather curious were the [responses to "I must expend substantial effort to find versions of all my dependencies that will work together"](http://breakingapis.org/survey/downstream.html#rework_dllhell?ecos=Haskell_Cabal_Hackage_,Haskell_Stack_Stackage_) which imply that Stackage/Stack is roughly as good as Hackage/Cabal, which is interesting given that ["Cabal Hell"](https://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) has been Stackage's major selling point over Hackage.
In practical terms, you lose inference very ofter, and you have to routinely prove theorems in order to satisfy the compiler.
There was an issue in GHC to make the `-ddump-splices` format use valid haskell syntax, which would be needed for this. Has this been done? 
I don't remember it, but was it possible to answer both `cabal/hackage` and `stack/stackage`? Perhaps most people answered both, so there aren't actually two groups but instead just a large single one that is counted for both.
While I seem to recall that it was a radiobox, I'm not totally sure if you could indeed select both at the same time for a single survey. However, even if the choice wasn't mutually exclusive, the theory that most people answered both doesn't hold up if you look at http://breakingapis.org/survey/graphs/participation.png; there you see that more than twice as many selected Cabal/Hackage over Stack/Stackage (and if you look at the completed surveys, 3 times as many completed it for Cabal over Stack). That plot also shows Stack user seem rather underrepresented in that survey, while Cabal users' participation is near the average over-all participation per ecosystem. I don't think you can infer much about the relative popularity of Stack vs Cabal though; however I'm wondering whether so few fully-completed surveys (from the looks of it, about a dozen; for Cabal it's at least somewhere around 50 fully completed surveys) for the Stack-category bear enough statistical significance/confidence for its results.
What do you value most: the ability to reason about correctness, from looking at the code, or the ability to reason about run-time characteristics (speed, memory usage)? In case of the former, I would recommend Haskell, and for the latter OCaml. Purity makes it *much* easier to reason about correctness, from looking at the code alone, since nothing is hidden from view. Laziness, however, makes it difficult to figure out how a piece of code will perform at run-time, without actually running it.
Interesting idea, an arbitrary instance might be tricky. Picking the coefficients too large will result in a black image because we will run off the edge of the input image.
Thanks!
As someone who used both for years, I think OCaml is the better language.
OCaml has laziness, it's just not lazy by default.
Agreed, OCaml is superior.
It's here: http://breakingapis.org/survey/health.html#not_diverse?ecos=Haskell_Cabal_Hackage_,Haskell_Stack_Stackage_ 
Thanks for the rundown! Yes, OCaml has its good sides, too. To add another one: it is (or at least feels like) a much smaller and simpler language than Haskell. The compiler is also simpler and easier to understand, if you want to start hacking on it. Recently, I'm especially impressed by [BuckleScript](https://github.com/bloomberg/bucklescript) that translates OCaml into readable and relatively idiomatic JavaScript. About 3: unsafePerformIO is exactly designed to let you encapsulate benign side-effects---with exactly the same trade-off as in OCaml: the burden of proof is on the programmer. (But pervasive laziness does make that harder.) Laziness is in itself something like a benign side-effect---Okasaki goes into some detail in his Purely Functional Data Structures. About type classes: OCaml modules can do almost anything type classes can do (but can't keep once you add a few ghc extensions.) Their main downside for me is the extra hassle in using them compared to type classes that feel like a much lighter cognitive burden. However, OCaml has [modular implicits](https://arxiv.org/pdf/1512.01895.pdf) in the works, that would make modules about as convenient as type classes for most common use cases. My current job is in OCaml, but I've also done Haskell professionally. I prefer Haskell, but whenever I think like complaining about OCaml, I always remind myself that at least it's not C++, Java or PHP.
Consensus seems to be that the Wikibook https://en.wikibooks.org/wiki/Haskell is a better way to learn than Learn You a Haskell these days. (I do have a soft [Discrete Mathematics Using a Computer](http://www.dcs.gla.ac.uk/~jtod/discrete-mathematics/) is lots of fun, if you are into that sort of thing. [Haskell Programming from First Principles](http://haskellbook.com/) deserves a mention as well. It's definitely better than Learn You a Haskell, but the Wikibook is free and probably works better.
I am using both languages. Syntax-wise I definitely prefer Haskell, it feels so much lighter---but I admit that OCaml has fewer syntax elements to learn.
I’m a bit taken aback by the graphical representation chosen in the results. Line graphs for an x-Resolution of 5, better use bar graphs. There is no info on the actual sample size for the percentages. Maybe absolute numbers displayed relatively would provide more info.
For me, the only bad experience with Cabal was back before sandboxes were working well (or before I was aware of them, at least). As soon as I learned how to sandbox every project, suddenly everything worked fine and I never had any issues.
This is a good explanation but I think you left out a key detail. The difference in rank comes from _where_ the `forall` is in the type. Ie using your first example: `example :: (a -&gt; a) -&gt; (Int, Bool)` _seems_ like it would be the same type but really it has the type `example :: forall a . (a -&gt; a) -&gt; (Int, Bool)`. The example I gave is of rank 1 but the second type is of rank 2. Incidentally, I'm not sure you could write a function for the type I gave but if you can it would be very different from the example your provided. 
No, it wasn't possible to pick both.
I agree that you wouldn't want to infer anything about the relative popularity from this. We found it easier to recruit Hackage respondents, but there are a lot of ways our recruitment could be biased. As for the numbers, 40 people started the survey for Stackage and 17 completed it (89 started and 46 completed for Hackage). This question was down near the end, and in particular it had 45 Hackage respondents and 17 Stackage.
We went back and forth about the best way to portray these; the nice thing about the curves is you can overlay multiple ecosystems on the graph and compare them easily; bars got overwhelming pretty quickly. I agree about adding absolute numbers though.
&gt; We found it easier to recruit Hackage respondents, but there are a lot of ways our recruitment could be biased. It probably doesn't help with recruiting Stackage users if the people running Stackage are ["actively steering away people from /r/haskell"](https://twitter.com/snoyberg/status/865564014868180993).
You could write a less interesting function of the second type: example :: forall a . (a -&gt; a) -&gt; (Int, Bool) example _ = (4, True) :)
Thank you! 
But this doesn't answer the question at all. The other version doesn't even mention any lists.
 stack ghc -- -ddump-rule-firings -fforce-recomp .\scratch.hs -O2 [1 of 1] Compiling Fibo ( scratch.hs, scratch.o ) Rule fired: Class op + Rule fired: iterate ... The important optimization rules here are iterate and iterateFB. You can look at the [source on hackage](https://hackage.haskell.org/package/base-4.9.1.0/docs/src/GHC.List.html#iterate). Tekmo explains what is happening here way clearer than my attempt did. You can get a look at the results of the optimizations via `-ddump-simpl` and specifically for rules `-ddump-rule-rewrites`, though. 
Well, yes, but it'd be much more difficult with a more interesting function: example2 :: (forall a. a -&gt; a -&gt; a) -&gt; (Int, Bool) example2 f = (f 4 5, f False True)
Whoops! I didn't notice that Then the next most likely reason is that `loop` is not strict in the accumulator and GHC's strictness analyzer isn't catching it either
I appreciated the explaination regardless :)
While your first point is true, I do not see how the version with `iterate` does not build up thunks in the exact same way.
First of all, thank you for the many comments and answers. I just tried adding the strictness annotations to the code of **fiboRecur**, using the BangPatterns (being new to this kind of trick, I am not sure it is enough): fiboRecur :: Int -&gt; Integer fiboRecur = loop 0 1 where loop !curr !next n | n == 0 = curr | otherwise = loop next (curr + next) (n - 1) The performance increased a bit, but still, the iterate version is still much faster (4.8 microseconds against 35.9 microseconds): benchmarking Fibo/Iterate time 4.840 μs (4.807 μs .. 4.880 μs) 0.999 R² (0.999 R² .. 1.000 R²) mean 4.847 μs (4.810 μs .. 4.883 μs) std dev 118.8 ns (95.37 ns .. 147.9 ns) benchmarking Fibo/Recur time 35.91 μs (35.28 μs .. 36.39 μs) 0.997 R² (0.996 R² .. 0.998 R²) mean 35.93 μs (35.46 μs .. 36.38 μs) std dev 1.636 μs (1.386 μs .. 2.038 μs) Unless there are more tricks to perform to make it strict, there must be something more explaining the difference in performance.
I tried adding the strictness annotation to the iterate version, and it does not change a thing. So I guess the analyser catches this, along with the build/foldr fusion.
What flags are you compiling the program with? You should also test making only the `next` argument strict If that still doesn't work then the next step is to dump the generated code for each function. Stick them both in a small module and dump the core using `ghc -O2 -ddump-simpl -dsuppress-all` to see what is different about how they are compiled
That's either `const (4, False)` or `const (5, True)`
That's either `const (4, False)` or `const (5, True)`
As another ML shill I wholeheartedly agree.
I tried to recreate the test case and it seems like ghc does create an intermediate list. But then it floats that list to the top so calls to fiboIterate are memoized and criterion runs the test often enough that it mostly profiles list indexing.
Typed Racket has a very specific goal, which is gradual typing for idiomatic Racket programs. Its type system is designed to accommodate the sorts of patterns that occur naturally in existing, dynamically typed Racket. This is a hard problem (since Schemers/Racketeers frequently do things like `(if (string? x) (do-something-with-a-string x) y)`), and it dictates a lot of the design choices TR makes (like supporting arbitrary type unions and not adding any new language semantics). Hackett is totally different because it has a fundamentally different goal. While being able to interoperate with existing Racket libraries is nice (and still attainable), it doesn’t need to be as seamless as Typed Racket. Hackett is a language designed with static types in mind, so it can support features like typeclasses. I have nothing against Typed Racket as a project, but I will be the first to admit it is not for me. I would avoid comparing and contrasting TR/Hackett too much, since their only similarities are that they are statically typed languages implemented on the Racket platform. They aren’t really much more similar than, say, Haskell and TypeScript are.
One thing for sure a superb community like this will Almont make Haskell enjoyable thanks guys!!! 
&gt; Infix operators. In Hackett, { curly braces } enter infix mode, which permits arbitrary infix operators. Most Lisps have variadic functions, so infix operators are not strictly necessary, but Hackett only supports curried, single-argument functions, so infix operators are some especially sweet sugar. that's pretty sweet
Can't say we don't try, though, with paper titles such as "How to make a fast curry"
That's more or less what I had in mind when I said it, yeah. That said, I've repeatedly had some notion (which has never turned into anything) that there might be some way of setting things up so you could work at "both ends of a projection", moving freely between the expanded and unexpanded versions. There are a lot of obvious difficulties there, of course.
I have always dreamt of a lisp surface syntax for Haskell. This looks like a great approach. Question - you talk about slowness. Isn't this just a front-end for GHC? If not - why not?
I'm not sure I understand what you mean by moving freely between the expanded and unexpanded versions. I would assume that anything that is the result of inlining th should be treated as generated code, and not touched on anymore.
If the padding is going to be too undesirable I would say a boxed vector of unboxed vectors is going to be very tough to beat. As the boxing of the outer vectors is actually an incredibly smart way to allow for heterogeneously sized children. And I don't think there is any math or clever data structures that cost less than one single pointer indirection. Could be wrong though, and if I am I would love it if someone explained how such a better data structure would work, I'm it would be pretty novel and interesting.
No, it's not a front end for GHC. He's implemented it using Racket, it only has some type system features because that's all he's implemented. The whole benefit of implementing a language in a lisp is that you just trivially write an s-expression to s-expression transformer and now you have access to the whole of Racket's runtime and libraries. Racket makes it slightly easier with the language pragmas to remove some intermediate steps but you can do the same with: (define hackett (ast) (rename (infer (resolve (eval ast))))) (define rename (ast) (if (eq? 'data (caar ast) ...))) Etc. and then easily: (hackett '((data Maybe a Nothing (Just a)) (def x Nothing)) You didn't have to write a parser and you didn't have to write an interpreter or compiler, Lisp provides the boring bits so that you can play with the type system bit in the middle. 
Thank you for this very detailed answer!
&gt; He's I’m a she. &gt; The whole benefit of implementing a language in a lisp is that you just trivially write an s-expression to s-expression transformer and now you have access to the whole of Racket's runtime and libraries. Racket makes it slightly easier with the language pragmas to remove some intermediate steps but you can do the same with: This is… somewhat accurate, but I think it might be mischaracterizing what Hackett is about. The reason Hackett is implemented in Racket is less about ease of implementation and more about expressive power (though ease of implementation is nice, too). For some insight into the sorts of things the implementation can do that a naïve GHC frontend could not, see [this comment from elsewhere in this thread](https://www.reddit.com/r/haskell/comments/6dqf1n/realizing_hackett_a_metaprogrammable_haskell/di53unf/) and [this section of my previous blog post](https://lexi-lambda.github.io/blog/2017/01/02/rascal-a-haskell-with-more-parentheses/#fusing-haskell-and-racket).
I respectfully disagree. Or at least my experience has been different. Yes, when I discovered sandboxes, that helped a bunch, definitely: It went from completely unworkable for me to tolerable. But as soon as I started to have larger and larger projects with more and more dependencies of its own it was getting back to square one: &gt; Oh, so it seems I just can't use these two packages alongside in the same project? Or maybe just not these two specific versions? Would I have to try all the different combinations of their versions? Hmm, that's very unfortunate. Coming from the Node.js ecosystem with npm, this experience was [unheard of](https://stackoverflow.com/questions/25268545/why-does-npms-policy-of-duplicated-dependencies-work) for me: It never happens (never that I experience or heard others describe) that I couldn't use one package alongside another due to them being incompatible in any way. Talking about Haskell again: I don't think I ever experienced anything like that with Stack since I've started using it, and I think it is because stackage candidates are picked with special care to avoid these kinds of incompatibilities, IIRC.
Maybe? My guess is that it’s too early to say. Template Haskell is pretty fundamentally different from Hackett, though, because TH only has *splices*, while Hackett has *macros*. From [my previous blog post](https://lexi-lambda.github.io/blog/2017/01/02/rascal-a-haskell-with-more-parentheses/#a-programmable-programming-language-theory-and-practice): &gt; By having a simple syntax and a powerful macro system with a formalization of lexical scope, users can effectively invent entirely new language constructs as ordinary libraries, constructs that would have to be core forms in other programming languages. For example, Racket supports pattern-matching, but it isn’t built into the compiler: it’s simply implemented in the `racket/match` module distributed with Racket. Not only is it defined in ordinary Racket code, it’s actually *extensible*, so users can add their own pattern-matching forms that cooperate with `match`. &gt; This is the power of a macro system to produce “syntactic abstractions”, things that can transform the way a user thinks of the code they’re writing. Racket has the unique capability of making these abstractions both easy to write and watertight, so instead of being a scary tool you have to handle with extreme care, you can easily whip up a powerful, user-friendly embedded domain specific language in a matter of *minutes*, and it’ll be safe, provide error reporting for misuse, and cooperate with existing tooling pretty much out of the box. In my experience, Template Haskell is about code generation, but macros are about *syntactic abstraction*. The latter subsumes the former, but they are not equivalent. TH has different needs, so I can’t say for sure if these things would make sense in Template Haskell, but macro hygiene and [support for phase levels](http://blog.ezyang.com/2016/07/what-template-haskell-gets-wrong-and-racket-gets-right/) would be a good place to start.
Nice! Back when you called the language rascal you mentioned in a blog post, that the language was defined in terms of modules like rascal/kernel rascal/data and rascal/case. I thought that was pretty neat, since i would really like a language to Experiment with new datatypes (like elm's records and open sum types). Is Hackett still defined in terms oft modules that could be exchanged/extended?
&gt; I think it is because stackage candidates are picked with special care to avoid these kinds of incompatibilities In this regard, Stackage is similiar to a rolling linux distribution which snapshots some arbitrary sound install-plans. That's one approach to deal with the problem, and while it may appear convenient at first, it's a centralistic, rigid and inflexible trade-off, and it has scalability issues due to requiring every maintainer to coordinate for a single monolithic install-plan (of which the vast majority of packages are likely irrelevant to your project at hand). So it's clearly not a silver bullet. The other approach which the Hackage/Cabal ecosystem builds upon is the constraint solver approach, which requires curated accurate per-package meta-data and in exchange is significantly more powerful, as it allows you to *automatically* infer all (or at least, many magnitudes more) sound install-plans, not just those few Stackage snapshots (which are basically single datapoints in the multi-dimensional configuration space) which may, or may not include the packages you care about (many packages aren't e, or may still be stuck behind with older versions of packages that don't provide the features you need. Now you may say that there's `extra-packages` to escape the walled garden of Stackage, but then you'd be in a much worse situation as you'd be figuring out manually what combinations work especially if we were in a pure Stackage-world with no constraint solver and no version bounds in the `.cabal` files at all, while hoping that the combination you picked manually is actually sound rather than merely compilable. Long story short, [curated collections such as Stackage are not enough to abolish "cabal hell"](https://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/).
That is indeed the closest comparison. If you compare [their version](http://projects.haskell.org/grammar-combinators/tutorial.html#the-grammar-1) of the usual arithmetic grammar example against [mine](http://hackage.haskell.org/package/grammatical-parsers-0.1#readme) you can immediately see both similarities and differences. The similarities are that: * both libraries concentrate on enabling the writing of a grammar, * there is a data type declaration somehow underlying the grammar in both, * the grammar productions are statically typed, * they can refer to each other by name, * they can be left recursive, and * different parsers can be used to parse an input against a grammar. The differences are: * the *parser combinators* in grammar-combinators are non-standard (`||||`, `$&gt;&gt;`, etc), whereas I rely on the standard `Alternative`, `Monad`, and `Parsing` operators, * the productions are defined as top-level function declarations in grammar-combinators and as record field values in grammatical-parsers, * grammar-combinators (AFAICT) require `TypeFamilies` and `GADTs` in user code, grammatical-parsers' only hard requirement is `RankNTypes`, * there is more non-parsing functionality in grammar-combinators, like the pretty-printing of the grammar. Aside from these differences in the interface and functionality, the architecture and implementations are completely different. The grammar-combinators grammars always start out as grammar ASTs which can then be transformed into parsers. In grammatical-parsers, that is partly true only of the LeftRecursive parser, the rest are directly defined. Hence you can use the standard parser combinators to define your grammar, but once you're done you can't pretty-print it. 
Consider adding a few links to relevant papers or prior work to expand on the context more.
How does it handle ambigous grammar, back tracking and priority?
Also, the grammar composability listed as one of grammar-combinators' limitations is a solved problem for parser-combinators. The [examples](https://github.com/blamario/grampa/tree/master/grammatical-parsers/examples) directory of the project combines several stand-alone grammars (arithmetic, boolean operators, conditionals, comparisons, and lambda calculus) together into a combined language grammar. It's still an open area of research, as the line goes, but it seems to work. The other listed limitation, performance, I don't understand. I have not tested grammatical-parsers against UUParse like they did grammar-combinators, but I have found it to be faster than Earley and Parsec. I see no reason for their parsing to be slower, since they have more freedom in the choice of the back-end implementation. 
Ambiguous grammars will yield ambiguous results. They will not help the performance, but otherwise they work as you'd expect. Backtracking is currently implemented only by one back-end parser [PEG.Backtrack]( http://hackage.haskell.org/package/grammatical-parsers-0.1/docs/Text-Grampa-PEG-Backtrack.html), but I'm planning to add ContextFree.Backtrack as well. The remaining parsers are implemented using the list of successes approach. The (operator?) priority I haven't added to the library yet, but I'm using it in the [Lua grammar](https://github.com/blamario/language-lua2/blob/master/src/Language/Lua/Grammar.hs). I had to hack the `buildExpressionParser` definition there because of Lua's peculiar understanding of prefix operator precedence. There's a [parsers' bug](https://github.com/ekmett/parsers/issues/26) logged about that. 
I'll check it out. I remember reading about it passively. 
The Fold example seems to be ill-chosen as it can be expressed more simply as data Fold' a b = Fold' { value :: b , next :: a -&gt; Fold' a b } To see that it is indeed equivalent we can just give the isomorphism: fromFold :: Fold a b -&gt; Fold' a b fromFold (Fold step v extract) = Fold' { value = extract v , next = \w -&gt; fromFold $ Fold step (step v w) extract } and toFold :: Fold' a b -&gt; Fold a b toFold fold' = Fold next fold' value edit: Looking at it again I realise this is a Mealy machine as for example implemented in the [machines package](https://hackage.haskell.org/package/machines-0.6.2/docs/Data-Machine-Mealy.html )
I agreed with this for a while but I think the "don't reinvent GHC" advice is ultimately bad for the language. GHC isn't the most reusable thing out there, its API is a nightmare and its internals aren't clean. Sometimes you need to be able to throw the thing out and start from scratch. Implementing a Haskell inspires confidence in ones own ability to fork or provide competing implementations. PureScript's compiler is relatively simple given the type system support and its performance as a pure language is decent. In that setting they were able to explore effect systems and row types without any hinderance. In many aspects it's more fun and productive than working with Haskell, without all the bells and whistles of GHC. Regardless of the success of other languages, ones like Python and Common Lisp tend to have different flavors of compilers that focus on e.g. User experience, fancy features, being a clean implementation, or raw performance. GHC tries to be everything but I suspect at the cost of not being amazing at any.
Yep, I was just hoping to find a function which would where the inferred type would be `(a -&gt; a) -&gt; (Int, Bool)` which I'm not sure is possible without rank 2 types.
By the way, any advice on making the code more idiomatic would be very welcome! EDIT: I'm especially unsure if an IORef was the best way get atomic state
The existential quantification implementation optimizes better than the recursive implementation. GHC is much better at optimizing non-recursive code than recursive code A related type that requires existential quantification is this one: data Fold a b = forall m . Monoid m =&gt; Fold (a -&gt; m) (m -&gt; b) This is the type I presented in this talk: * [Beautiful folds are practical, too](https://www.youtube.com/watch?v=6a5Ti0r8Q2s)
I think the trouble is that you won't get such efficient `Functor` and `Applicative` instances out of `Fold'`. These instances are really central to the utility of `Fold`.
Actually, you can still define `Functor` and `Applicative` for the recursive version
Another good example: type-aligned sequences (i.e., free categories and semigroupoids). These are useful for [reflection without remorse](http://okmij.org/ftp/Haskell/zseq.pdf), an approach to representing free monads and operational monads that turns out to be important for making certain `LogicT` operations efficient, and that also has some potential to [improve certain stream operations](https://github.com/ekmett/machines/issues/68). Another (related to `Fold`) is `Coyoneda`, a "free functor", which looks something like data Coyoneda f a = forall b. Coyoneda (b -&gt; a) (f b) For any `Functor f`, `Coyoneda f` allows repeated `fmap` for cheap, automatically using `fmap f . fmap g = fmap (f . g)`. Similarly, there are versions for `Applicative`, including free `Applicative` and free `Apply`.
Yes, but I think less efficiently, no? Edit: the `Functor` instance, at least, is surely less efficient, as it lacks the coyoneda improvement.
This is good to know! So it would actually have been a great example if OP compared the recursive and the existential version and explained what existentials buy us in this case.
I don't why not, personally I'm not familiar with enough type theory to attack this. Though... I'm on mobile so it's hard for me to tyoevheck this but what about `example f = let x = f . f in (4, "test")`?
I've used MVars for atomic state in web apps and such, and I've found them to be very nice to work with. You can check them out [here](http://hackage.haskell.org/package/base-4.9.1.0/docs/Control-Concurrent-MVar.html). They have some additional safety features over IORef and IMO are a little more haskell-y to use. The hackage page actually does a good job of explaining their pros and cons so I recommend reading that.
The recursive implementation also doesn't provide a way to ensure that the internal accumulator stays WHNF strict
Looks useful. Thanks for sharing. As a side note, "survey on" sounds like you are conducting a survey where you want people to answer questions. "Survey of Functional Web Development approaches" sounds more like what you've acually done.
Thanks, fixed the actual page.
Here is how [F# does it](https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/lazy-computations). You literally just write `lazy` and then pass the code into it. The expression does not need to be parenthesized, you can indent into a block the same way you would in writting a let statement or a for loop. Adding laziness to a GC'd strict language is not that much harder than adding the if statement from my vantage point.
I have very mixed feelings about this sort of things. Full laziness can be a fairly significant optimization but it also causes space leaks that are basically impossible to track down by only looking at the code. On the plus side, they are really easy to track down via profiling and fixing them is relatively easy by adding indirection with a noinline pragma - if you know that this is a thing that happens. Which most people don't the first time it happens to them.
&gt; a Bool on the RHS of a guard is always redundant (I think), so you can do: Good call! &gt; It would probably be better to have an UnhashedBlock type or parameterize Block and make addHashToBlock :: UnhashedBlock -&gt; Block, that way you don't accidentally used the unhashed version somewhere (maybe this isn't a problem, I've only looked at the code superficially) This is also a great idea, thanks for the feedback. I'll get the code updated :)
As I understand there is not a "haskell package manager". I mean, there is not a tool which is able to let you "install" or "uninstall" a package, which now seem to be concepts foreign to Haskell tools. If you want to use a library as dep, maybe you can try stack. With cmd `stack new` you can create a project, which will contain a `.cabal` file and stack.yaml. You have to put the `music-suite` dep inside the `.cabal`, because deps are managed there. Also you have to add it as extra-dep inside `stack.yaml`, in order to tell stack that `music-suite` should be obtained from Hackage (Haskell general dep repo), because is not available in stackage (curated dep repo used by stack). Cabal (as a tool) could also be used to manage your project, instead of stack. Before this had some dep solving problems, which could cause what was called as *cabal hell*, but **I think** this was already solved in new versions (I did not try to used, I just use stack now). Finally, if you are trying to "install" as package as you would do with other tools, it could fail because it is not the same concept. Stack has a cmd `stack install`, but it only compiles the software and does a copy of the produced binary to some folder (here $HOME/.local/bin). After that there is now way to "uninstall", other than deleting the binaries by hand. If you could post what error did you encounter, maybe we can help you more.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [music-suite/music-suite/.../**default.stack.yaml** (master → d784c6d)](https://github.com/music-suite/music-suite/blob/d784c6dd10e156dc1ca0bb00c0c7c27b7335d3f9/scripts/stack/default.stack.yaml) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply di5oxm9.)^.
Right. I wasn't trying to imitate what `bound` does, exactly, but only to demonstrate the underlying concept that `bound` modifies. Sorry I wasn't clear about that.
You pay slightly more. In exchange you can now grab more than one of them at a time. TMVars take that even further. An IORef, when it works (and you can get away with atomicModifyIORef) is cheaper.
&gt; Coming from the Node.js ecosystem with npm, this experience was unheard of for me Wut? That happens _every single time_ I change dependencies with npm. There is basically not a single time that has not happened - simply because the semantic versioning contract in npm-land is a joke. 
Yes. You can use `lambda` instead of `λ` and `forall` instead of `∀`. In fact, I would probably encourage doing so. Using the Unicode is cute, but I would really prefer things be easy to type, so I probably ought to change the standard library code to use the ASCII names.
Doesnt work for me for some reason ... Guess ill just wair for 8.2.1 rc 2 or something
In practice it doesn't work as well as you would think. As most likely someone has at some point in the stack of functions you are calling forgotten to put lazy, and then everything is immediately forced and you are fucked. Adding strictness to a big stack of functions just requires one `seq` in the right spot. Adding laziness requires tons of `lazy` annotations.
&gt; in hackett/private/prim/base you use def and defn to define functions and i don't get the difference `def` is the primitive definition form. You can use it to define everything if you want. `defn` just provides some sugar for the common case of `def` + `lambda*`, so these two things are equivalent: (def x : T (lambda* [a b] [[c d] e] ...)) (defn x : T [[c d] e] ...) In Haskell, the definition syntax is designed in such a way that both forms can be subsumed by a single syntax. However, you can think of it as the difference between these two things in Haskell: x :: T x = \a b -&gt; case (a, b) of (c, d) -&gt; e ... x :: T x c d = e ... Once there is actual documentation, these things will be documented, of course. &gt; oh and another: why define monad by join? This was a sort of arbitrary choice, but it will change once default methods are implemented. Both `join` and `&gt;&gt;=` will likely be members of the `Monad` typeclass, and you will be able to specify either one (or both).
Right, portions of strict behavior can be recovered through the use of the (ad-hoc) bang construct, but nonetheless inductively defined types do not exist in Haskell. Even if you define, for example, data Nat = Z | S !Nat A given `n :: Nat` may still contain `_|_`.
This whole procedure should no longer be necessary now that Haskell Platform has been updated with the fix. Just download the current version of Platform, 8.0.2a.
There were no comments on rank2classes so far, and I was hoping for some of the resident category theory mavens to pipe in. So let me ask: * What is the category-theoretic difference between these classes and standard ones? The `Rank2.Functor` is not endomorphic, I think I get that, so there can be no `Rank2.Monad`. What else? * Can you suggest a better naming scheme than the `Rank2` prefix? I'm aware that there may be other possible classes with methods of rank-2 types and practical use, I just couldn't think of any more specific name. * Speaking of which, can you point out or think up any other classes with higher-ranked method types that mirror the same hierarchy? Thanks. 
It lets you pass polymorphic functions as arguments, for one thing, which can let you factor out some repetition you otherwise wouldn’t be able to. For example, recently I was writing an `Applicative` instance for a data type with many fields, and I found myself writing this repetitive pattern: fs &lt;*&gt; xs = Thing (thingFoo fs (thingFoo xs)) (thingBar fs (thingBar xs)) … You can’t just factor that into this: Thing (go thingFoo) (go thingBar) … where go get = get fs (get xs) Without a type signature, because `get` is applied to different types of `Thing` within `go`: both `Thing (a -&gt; b)` and `Thing a`. So you can use `RankNTypes` to specify a signature that works: Thing (go thingFoo) (go thingBar) … where go :: (forall c. Thing c -&gt; c) -&gt; b go get = get fs (get xs) (I also had to use `ScopedTypeVariables` and `InstanceSigs` to bring `b` into scope here, but that’s not really relevant.) Higher-rank types are also useful for dealing with existential quantification—and in fact you can encode a rank-n existential as a rank-(n+1) universal—but I’m having trouble coming up with a concise example that’s clearly useful. 
Looks like a plausible candidate! Gotta try that on ghc later! Edit: ghci says `example :: Num t =&gt; (a -&gt; a) -&gt; (t, [Char])`
This looks cool! Would you be open to using `Text` instead of `String`?
&gt; Well, it's up to you if you don't care about that. Personally I don't want my type of Ints to be inhabited by a non-int value, for the same reason I don't want an x :: A to be null/Nothing unless the type explicitly says it can be (i.e., x :: Maybe A). Seems like you completely ignored my point about how strict programs are going to fail in strictly more situations, so laziness actually gives you less errors. And I would say that nothing is fundamentally different between `a` being `_|_` and the function returning `a` erroring out. &gt; The same can be said of not adding ! in the right place. And indeed this is more frequently a problem in practice, because the difficulty of reasoning about the resource complexity of a call-by-need program is difficult, whereas figuring out how a call-by-value program will perform (and hence where it may benefit from suspending some intermediate expressions) is easy. It is absolutely not a more common problem in practice then insufficient laziness. Space leaks are not that common, I know at least that I very rarely get them. Whereas basically no strict language that I have ever seen, truly supports laziness thoroughly and completely. You might want to try reading [this](https://www.reddit.com/r/haskell/comments/5xge0v/today_i_used_laziness_for/deia53t/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=haskell) post by Edward Kmett. Or just further research on the topic as a whole. 
I can admit my two most recent comments were snarky; but prior to that I can't see what I did aside from disagree with you.
Fixed, thanks.
You claimed me disagreeing and stating that Haskell developers use strictness all the time was hostile, which it wasn't at all, I literally did nothing more than provide a counterpoint. You frequently just ignore part of my argument, and just focus on the part that you think you can actually beat, such as ignoring my point about how within `foo !a = ...`, `a` is not inhabited by `_|_`. You make an extremely unfair comparison between `_|_` and `null` / `Nothing`, and you know it, so I consider that somewhat of an insult to my intelligence. And you make arrogant assertions such as "And this is indeed more frequently a problem in practice", even though plenty of very smart people disagree with you and have even provided evidence of you being wrong in that regard. Hell one of the most oft repeated benefits of laziness is that strictness is death by a thousand cuts, whereas laziness is just one big issue that occurs very rarely, that is a space leak.
Use `fail` instead of `error` for throwing a simple `String` error in `IO`. The reason why is that you can accidentally trigger `error` by evaluating it, whereas `fail` doesn't have this problem. Or in other words: error string `seq` () = error string fail string `seq` () = ()
I'm sorry you felt disrespected. Maybe we could both use an adjustment in tone. &gt; You frequently just ignore part of my argument, and just focus on the part that you think you can actually beat, such as ignoring my point about how within foo !a = ..., a is not inhabited by _|_. What I have to say about this: it's actually related to the comparison between `_|_` and `null`/`Nothing`. In that scope you have a dynamic guarantee that `a != _|_`. Here's an analogy: in the Java snippet if (x != null) { ... } You have a dynamic guarantee that `x` us not `null`, but in this code case x of Just v -&gt; ... Nothing -&gt; ... You have a *static* guarantee that `v` is not `null`. In the Java code, the runtime is forced to perform redundant `null`-checks in order to maintain type-safety because the types are unable to prove that `x` is not null even though you just did the check (JIT might catch this sometimes but in general Java can't help these redundant checks); in the Haskell code, there is no need to "null-check" `v`because the `case` provides proof that its actually the underlying value that you want. This is facilitated by distinguishing between the type containing `A`s and the type containing `A` or "absent values" (i.e., `Nothing`). A similar argument can be applied to e.g., distinguishing between the type of `Nat`s and the type of "`Nat`s that might also be `_|_`". You are of course free to be unmoved by that; I care about that distinction.
Great tip, thank you!
Thanks for your work, I really appreciate it! * I'm curious about reducing the JS size. I did do some experiments with js_of_ocaml before, and indeed its output without any other libraries is pretty good. Just adding tyxml grows the output to about 160k, which is not small but can be acceptable. Is there a way to reduce code size while still using Eliom? (Also, I don't buy the caching argument, the slowdown I observed was noticable on localhost - it's the parsing and interpreting of the JS blob that makes things slow). Another thing that might help is support for multiple output JS files; What I've seen on my own sites is that some pages have lots of information but very little interactivity (and these need to load fast), whereas other pages have lots of interactivity and less information, these don't need super high performance and can do with a larger JS file. * A few concrete points if that helps: It took me a long time to figure out what the Eliom App functor was for, what it did and how it was related to other service registration modules. I'd also + [this issue](https://github.com/ocsigen/eliom/issues/332). I did [create an issue on a different point](https://github.com/ocsigen/eliom/issues/489), but that got the silent treatment. :)
It’s possible to parse context-sensitive languages using applicatives if you allow infinite grammars (Brent Yorgey did [a post about it](https://byorgey.wordpress.com/2012/01/05/parsing-context-sensitive-languages-with-applicative/)), but this is sort of against the spirit of the question. Generally applicative = context-free and monadic = context-sensitive.
For the sake of completeness, here's a version using the non-operator counterparts in lens update = over (ball . position) (+ 1)
It could also be written as update = ball . position +~ 1 but it is probably a good idea to restrict oneself to reasonable subset of operators.
There was always at least one instance that was running. It's just that the old instance where the DNS entry pointed to got stuck (and also would have required some changes to support newer GHC versions). However, this weekend I finally switched over the DNS record for `matrix.hackage.haskell.org` to point to the new instance.
But in a strict language you would not aim to make everything lazy, just the particular thing you want to defer. I've used it for example to defer memory transfers from GPU to CPU since they are blocking in my ML library. The rest is just fine being strict, and I disagree that it does not work well in practice. Most of the time in my own programming to get the benefit of laziness, I do not actually use `lazy`, but simply wrap the expression in a function and call it latter. So that sort of thing is a part of regular control flow to me. And when you do want actual laziness, you have the benefit of having an expression be wrapped in a `Lazy&lt;'expr&gt;` type. This is the Haskell sub, so I won't bother convincing you why types are beneficial to reasoning about programs. In my opinion, I think that not being able to track laziness at the type level in Haskell is a design mistake. I know that having that naively would just blow up the type annotations, but for a language that prides itself on correctness and ease of reasoning there should be a way to easily trace its exact control flow. Furthermore, I think you are exaggerating [how easy](https://ianthehenry.com/2016/3/9/lazy-io/) it is to make something strict in Haskell. Putting that `seq` in the right place requires quite a bit of skill.
I agree that it would be great if we had more compilers written from scratch. But that's a huge, huge project. You can expect it to be quite a few man-years of work by very talented and knowledgeable people just to get something minimally useful. So if you start out with the approach that this will be a full blown Haskell compiler that I am writing from scratch, then even with the best of intentions and exceptional talent it's likely to end up as yet another failed attempt. I believe that independent Haskell compilers are achievable via an incremental process that leverages the huge amount of existing investment in GHC along the way. As an analogy, look at Idris. At the beginning, it started out as not much more than a thin syntax wrapper layer in front of GHC, gradually became a complete language compiler written in Haskell and compiled with GHC, and eventually will become a fully dog-fooded Idris-in-Idris compiler. A similar process could be useful for Haskell itself. I think Alexis' approach is actually neither of those. She is starting by leveraging the Racket compiler, not GHC, and gradually adding Haskell features. That can work too. I am just suggesting that she should not be afraid to hybrid that, and leverage some of GHC as well near the beginning of the Hackett adventure. That might make it possible to get something much more Haskell-like much more quickly. Truth is I'm not exactly sure of the details how, but I'm confident that if it's possible then Alexis will figure it out. EDIT: And yes there is also the PureScript approach - choose a limited subset of Haskell-like features, and some non-Haskell features, that is usable in practice. But then you start building up a mass of less idiomatic or totally unidiomatic code from the Haskell perspective. So even if the project succeeds you'll likely end up with something some distance away from Haskell.
I think we are talking about two separate scenarios. I am talking about adding dependencies to a project, and I think you are talking about upgrading certain versions of some of the dependencies of a project. In my experience the former in haskell (with cabal) often lead to incompatible version bounds, unsolvable constraints, e.g. Resolving dependencies... cabal: Could not resolve dependencies: next goal: aeson (user goal) rejecting: aeson-0.9.0.1, 0.9.0.0, 0.8.1.1, 0.8.1.0 (global constraint requires ==0.8.0.2) trying: aeson-0.8.0.2 next goal: bytestring (user goal) rejecting: bytestring-0.10.6.0, 0.10.4.1, 0.10.4.0, 0.10.2.0 (global constraint requires installed instance) rejecting: bytestring-0.10.0.2/installed-4f9... (conflict: aeson =&gt; bytestring&gt;=0.10.4.0) rejecting: bytestring-0.10.0.2, 0.10.0.1, 0.10.0.0, 0.9.2.1, 0.9.2.0, 0.9.1.10, 0.9.1.9, 0.9.1.8, 0.9.1.7, 0.9.1.6, 0.9.1.5, 0.9.1.4, 0.9.1.3, 0.9.1.2, 0.9.1.1, 0.9.1.0, 0.9.0.4, 0.9.0.3, 0.9.0.2, 0.9.0.1, 0.9 (global constraint requires installed instance) Dependency tree exhaustively searched. or if not those, then frustratingly often only nominally different type errors; e.g. Couldn't match type ‘T.Text’ with ‘Text’ NB: ‘T.Text’ is defined in ‘Data.Text.Internal’ in package ‘text-1.2.2.1’ ‘Text’ is defined in ‘Data.Text.Internal’ in package ‘text-1.2.2.0’ Expected type: String -&gt; Text Actual type: String -&gt; T.Text And if I type `npm install foo-bar --save`, it has never said to me AFAIR that it can't/won't install because `foo-bar`, or some lower level dependency versions are incompatible with some other already installed dependencies. But having said all that, I think your separate point is also valid: having a static type system is able to and does catch certain kinds of errors at compile time that only run-time exceptions would reveal in Node.js.
or you could use unsafeperformio :D
With more English and fewer operators: `update = over (ball . position) (+ 1)`
 upvote = over (elvishJerrico . chrisDoner) (+ 1) 
 upvote = over (elvishJerrico . tsahyt) (+ 1)
&gt; I've tried using Hakyll and Jekyll on different occasions and found there was too much magic going on with all of the monadic contexts for me to understand how to customize things for my use-cases. This. IMO, Hakyll is difficult to work with if you want something custom. I'm not a beginner, but every time I needed something custom, I had to 1) re-read the docs and tutorials 2) go to GitHub issue tracker and ask the author directly how to do X. I must admit he does provide great support (thanks Jasper!), but I would be more comfortable with something more straightforward. I'll consider using SitePipe, although I'd prefer [`stache`](https://hackage.haskell.org/package/stache) over `mustache` for templating. I see you already use Megaparsec, so I'm not sure why `mustache` is preferred here. (People have recently ported `stache` to Parsec as `microstache` apparently only to use it in `criteiron`, which I can understand because Parsec supports a much wider range of GHC versions. This is not an issue with `sitepipe` I think.) Another option is to try to use something like `shake` directly for static site generation, which may be more fun and you'll learn `shake`, which is great.
obviously ;)
Do you mean {ww | w ∈ Σ^* }? Because you can [use the pumping lemma to show that one is not context-free](https://cs.stackexchange.com/a/43425/26856).
Or like this: `example f = let x = f . f in (length "test", "test")`
Are your functions already monadic, or entirely pure? If the former, then you can wrap whatever monad you’re using in `ReaderT`; if the latter, you’ll need to add an argument everywhere or convert to monadic style so you can use `Reader` or `ReaderT`. (The basic problem is that Haskell has different notations for pure and effectful code, and while you can be polymorphic in *which* effect you’re using, you can’t be polymorphic in *whether* you’re using an effect.) Fortunately, this transformation is entirely mechanical and the compiler will tell you every place you need to make a change once you start refactoring. You could also look into using the `ImplicitParams` extension if it seems to be a good fit for your use case, but it’ll still involve a lot of code changes.
An Excel file is XML. Read it as XML using your favorite XML library and parse out the information (we use [xml-conduit](http://hackage.haskell.org/package/xml-conduit) with its XML cursors), then print it in whatever format you'd like. We have people in our company who do that kind of transformation all the time. They do it in XSLT, but you can do it just as well in Haskell.
&gt; I just let the compiler do its job and get all the laziness I need for free This is essentially the crux of the contention for me. If laziness really was free as you say there would be no need for debate. Ironically, just a few week ago I am made a similar argument to somebody else in a different context of parsing. I said that my programming style of stacking higher order function is correct and the optimal way to program the parser, but when on suggestion I looked at the disassembly I found that F# is quite poor at inlining higher order functions much to my chagrin. Nevertheless, I felt I justified in saying that the thing is the compiler's fault and that the code I am writing is correct. The reason why that is so is because I am working on a programming language with extreme inlining capabilities and if the parser was rewritten in it, the code would in fact essentially be optimal from a performance perspective in that style. It is true that Haskell is good at optimizing, but I'd hesitate to call it great. My experience with it is that the optimizations underneath it break all the time, and I've read a stories where the program would get slower due to a change in version. Moreover, optimization does get quite a bit more difficult in the presence of recursion. Edward Kmett mentioned the inliner getting confused by it as the reason why monad stacks are slow in some thread here. I am not really arguing that Haskell should stop being lazy, rather I am trying to say that optimization is a hard problem and that if you look under the hood you will find that laziness is hardly free. And the connection with inlining - which is difficult - is that higher order functions necessary for thunks are half of laziness. Hence why you might want to control the degree of laziness explicitly in a language, and not make everything lazy by default. Had I written my ML library in Haskell, it would have been quite difficult for me to ensure that only the GPU to host transfers are lazy while the rest of the code strict. I'd most likely have 'solved' the problem by 'letting' the compiler take care of it - which is to say that everything would be lazy. But if you look at any performance Haskell library you will find that it is full of bangs, pragmas and even uses unsafeIO in places. I dislike that. I'd like to add in my own language, the inlining capabilities are hardly automatic and I have 3 different kinds of functions just because of recursion. Without it the job would be much easier, so from that perspective even the strict and impure MLs do not offer sufficient control to be suitable for some tasks like writing GPU kernels. A lot of Haskell's features that make it great like type inference, typeclasses, higher kinds and higher ranked types, GADTs are in fact language agnostic, unrelated to laziness and can be ported to other languages. I feel that if Haskell offered more control, it would open up whole new domains where it would be applicable. And that advice was applied once more to the strict MLs, I would have no reason to do my own language.
I might misunderstand your question, but you can do it directly with Excel: File/Save as... and choose "save as type": CSV. 
&gt; Interestingly, Shakespeare also has explicit support for templating JavaScript code. Too bad that this doesn't take away the need to write the JavaScript yourself, so I don't see how this is an improvement over some other JavaScript solution that uses JSON for communication with the back end. Good question. The Javascript templating is intended to simplify embedding of variables into Javascript. You're spot on that it doesn't help with the (truly painful) act of writing Javascript, and a different frontend language (like PureScript or GHCJS) is probably a better choice, especially for SPAs.
Unless you're a beginner.
Aside from the underlying parser which IMO is just an implementation detail (granted that pulling in extra dependencies sucks), is there a reason you prefer stache? Mustache has lambda support so you can use helper functions in your templates; which stache doesn't currently support. They both implement the spec so I'm not too concerned about choosing one over the other, let me know if there are other things to consider.
Either supporting or wrapping Shake seems like a cool idea! I'll look into it!
You want syntax for [romantically involved records having secret meetings](http://dictionary.cambridge.org/dictionary/english/assignation)? 
Thanks for the link to the Wikibook!
If it is .xlsx then yes it is a zip containing XML which should be parseable, as others have described. If it is a .xls file it is probably an older version (Office 2007 or older) which will be in Microsoft's binary format and much harder to work with.
isn't there anyway I could import all the functions inside the context of the function which will have the needed variable?
Thank's for the feedback! https://github.com/agrafix/Spock/issues/124 :-)
How do you handle rebuilding? Do you always rebuild everything? At least some complexity in Hakyll comes from the fact that for big websites this is not really an option. You could probably get a nice static site generator with "only rebuild what changed" using the Shake library.
I wish. Unfortunately modules and module imports are not first class. Making them first class would be cool but probably very difficult. 
Did you mean to say "lens"?
If you are interested in the intersection of complexity theory and logics i can recommend you to have a look on the field of Descriptive Complexity. ["Neil Immerman, Descriptive Complexity"](https://people.cs.umass.edu/~immerman/book/descriptiveComplexity.html) is a good introduction book.
Yeah right now there's absolutely no caching, which works fine for small blogs, which is what SitePipe excels at; I'm definitely going to look at shake for a quick caching win!
&gt; Is the meant to be used as a more lightweight alternative? Yup. This just uses vanilla Haskell data types and functions. I wanted the output to be as simple to use as possible, and you can't get much simpler than that.
&gt; It would be interesting to see a generics-based approach to this same problem. I've actually been experimenting with something similar to that! I made a [Generic type class](https://github.com/jdreaver/eventful/blob/73eb93c031c9143abd0761381776ecc569b13179/eventful-core/src/Eventful/Serializer.hs#L145-L222) to extract values from a sum type to a `Dynamic` value, and also possibly re-insert a `Dynamic` value into the sum type if it can. I used this type class to convert between sum types with `Dynamic` as the intermediary. I'm sure that class could be reworked to do something similar with two sum types and without a `Dynamic` intermediate value. Also, I couldn't find out how to do this via `Generic` without the operations being `O(n)` where `n` is the number of constructors in the sum type. AFAIK pattern matching like in the `sum-type-boilerplate` conversion functions is faster (but now I realize I need to validate that theory haha). Ultimately, I decided that this TH solution was simpler and probably more performant.
Well, I kind of prefer to rant about whatever interests me after the initial exchange, but I do have a personal story where go bitten by it once. In fact it made me swear off the language completely. I once tried making a parser to load large multidimensional arrays, 100s of megabytes in size. Now, in reality, in language other than Haskell this would be a day's work for me, and it did take me that much to do it in F#. In Haskell, it took me like two whole weeks and at that point I was so pissed at it, super pissed, because I went way over my allotted time budget to do it. This was roughly a year ago. What happened was that at the start I had some initial difficulties on reinventing the resizable array for which I needed some help because despite studying Haskell for a month before I decided to tackle the project because I did not understand monads as well as I thought I did and my skills in a lower kinded functional language did not transfer well to Haskell. After that it was time to load those 100M integers that I was testing the thing on, and no matter what I tried I could not figure out how to stop it from blowing out my memory. The reason why it took me so long to figure it out is that after the initial flurry of activity where I did ask around on how to deal with it, I thought I had imposed myself enough and decided to look for information on my own. That involved studying various things related to Haskell such as monads, CPS, its various libraries as well as trying to find material on how to resolve the laziness related bug. Did you know Attoparsec is written in CPS and uses unsafe arrays under the hood for performance? I actually tried writing my own parser for Attoparsec with the resizable array which is something that I've done before in Fparsec and would have been quite trivial to do in F#, but I pretty much failed at that too because the ST monad does not mix well with CPS. Although Attoparsec dubs itself as a fast library, in reality it is permanently welded to a subpar data structure like the list and is completely unsuitable for a task like loading a large number integers which is something that would trivial in anything other than Haskell. Back on track, eventually I realized that the `Text` library which is one of the recommended things to use for parsing has `O(n)` for the length of a string and indexing, and realized that it was using some kind complicated tree structure under the hood instead of doing the sensible thing which would be to use a flat array. Realizing that made me try out the `Bytestring` package after which the laziness related problems went away. If you look at `Bytestring`'s functions you will see it has `O(1)` for length and indexing, which a clear indication of it using the sensible thing under the hood. I should have realized this earlier, but the reason why the `Text` package blinded me so much is because alongside `Bytestring` it was a widely recommended library for dealing with strings. Because of that I trusted it to do the sensible thing when it came to strings instead of the stupidest thing possible and spear me through the rectum in the process. The reason why I started that project is because of another laziness related thing. There is a GPU language written in Haskell which when loaded in Atom promptly goes through through gigabytes of RAM and then crashes the `ghc-mod` plugin. Now, with the benefit of hindsight I am not sure whether to blame laziness for that - I've investigated other Haskell libraries and it turns out `ghc-mod` crashes on pretty much all of them. It is good for toy HackerRank problems, but not much else and I am used to having a good IDE experience with F# so I expected the same from Haskell. That having said, for the language I was looking into, it seems that the parser was eating most of the RAM, so I thought I might make a real contribution by rewriting that aspect of it. I've done parsers before so I did not anticipate any difficulties there. Two weeks later and I swore to never use the language for anything serious ever again. It is literally impossible for me based on personal experience, since all of the major problems that I have had with the language are related to it mismanaging memory. It is not farfetched for me to blame laziness for that since why else would it run out of memory? I decided to cut short my losses and just work on my own stuff instead of the obligation to support a potentially promising language because I needed something like it for my ML library. Far from Haskell code being easy to reason about, for me it is just a series of leaks and broken expectations, and back then, incomprehensible, but vaguely alluring academic jargon. Right now I just prefer to watch it from a distance and let other people wade through the minefield. Though, my mind is not clouded by hatred altogether. I recognize Haskell as a center of computer science research and recognize that most of the interesting ideas have come from it in fact. And the interesting features that Haskell has, I am definitely interested in learning about them them and using them. In a **different** language! The reason why I did not respond to the lazy IO aspect directly is because it looked like flame bait and I thought the post I linked to did a pretty good job of delving into the topic already so there was no need to beat a dead horse. And most of the posts in my /u/abstractcontrol account are for the benefit of reinforcing my resolve to program and the other people reading them. I rarely write them for the benefit of the person I am talking to, which is condescending, but that aspect of them will be buried by time so please do not mind it. You might have noticed that Haskell - such a great language - seems to attract an almost irrational amount of hatred. I mean, I do hate it after all. The reason for that is cognitive dissonance caused by the pain getting blown up by its more poorly thought out features and the joy of discovery of new methods, improvements and type level magic owing to its status as the capital language of computer science. There is a saying `can't live with them and can't kill them`, but languages can in fact be taken apart and put back together so they turn out better than the original. That is roughly my stance towards things I do not like. Let me end this post with a punchline. Do you know what sort of data structure the `Text` package uses under the hood? [A fucking array.](http://www.alexeyshmalko.com/2015/haskell-string-types/) It should be a joke, but why am I crying?
Typed TH doesn’t fundamentally change the model behind Template Haskell in any way; if anything, it actually weakens Template Haskell somewhat to get stronger guarantees. It also only applies to expression splices, which, in my experience, are not the most frequently used feature of TH (the most common being declaration splices). As for MetaHaskell, I am not really familiar with it, and I can’t compare directly. At a glance, though, it doesn’t look more powerful than TH in any of the areas I mentioned (though it does seem better than TH in other ways). I am probably not going to spend time answering every little question about this. If you want to understand this more fully, read the [Type Systems as Macros](http://www.ccs.neu.edu/home/stchang/popl2017/) paper and understand it. I’ve explained this as well as I can.
Yes, PureScript’s typechecker *is* based on that paper, though it is heavily modified from it. It doesn’t use unification anywhere in the traditional sense. It’s also worth noting that paper is less intimidating than it might seem; I have no idea what the 80 pages of proofs mean. The entire algorithmic typechecker implementation is actually presented on a single page, page 7, containing figures 9, 10, and 11. I basically only read section 3 of the paper and ignored the rest.
I see. Towards the end the authors hint that full Damas-Milner inference might be possible by extending the approach with some additional rules. Do you think that is viable? Maybe Purescript has that, I've never tried it. In my own language the type inference not being global is my biggest regret, though I have very good reason for doing it the way I did for the sake of inlining and polymorphism. If it is possible, it would give me a concrete reason to really master the algorithm at some point in the future for some other thing. Also, I do not really understand why they call it bidirectional typechecking. As far as I can understand from the tutorials on bidirectional typechecking that particular method is supposed to be more like local inference, while handling of algorithmic contexts in the Easy Bidi paper reminds me more of how Coq solves its equations and is closer to unification.
Isn't pattern matching also O(n)?
I am probably in a very small minority for suggesting this, but consider using Elm for 2-3 days, then transition to Haskell. Elm will let you focus on getting comfortable with Algebraic Data Types, in a beginner friendly setting. Elm also hides Monads somewhat, so you don't have to be aware of them for your first few programs. I would not stay with Elm longer than a week (for this learning exercise).
I think pattern matching is O(1): https://stackoverflow.com/questions/9027384/haskell-ghc-what-is-the-time-complexity-of-a-pattern-match-with-n-constructors
Hmm, I could see how arbitrary pattern matching on complex patterns could be `O(n)`, but I think with this type of pattern matching (simply matching the tag), it can be implemented with a jump table. I'm not an expert in GHC internals, so don't quote me on that :) Even if pattern matching was `O(n)` though, with the generic approach it is still `O(n)` just to insert the value back into the target sum type. With the TH approach in this library that is definitely `O(1)`.
Thanks, this seems so obvious now :)
`GHC.Generics` represents a sum type as a balanced binary tree, so pattern matching is O(log n) without optimizations (not counting the pattern matching to convert the original type to the generic representation, hence my question). With inlining, it can often be optimized to something equivalent to what you would write by hand. I have experience with this from when I wrote [benchmarks for `product-profunctors`](https://github.com/tomjaguarpaw/product-profunctors/blob/master/Bench/Main.hs) where the generated Core from generics is identical to the handwritten equivalent.
That's great info, thanks a lot! I'll take a look at the `Generic` approach again.
Slides are here: https://github.com/Gabriel439/slides/blob/master/lambdaconf/category/category.md
I could see this helping to alleviate some of the boilerplate when converting between different exceptions types.
Am I a [Reaver](https://en.m.wikipedia.org/wiki/Reaver_%28Firefly%29) now?
While in principal I agree with you, the convention is far to established for me to consider breaking it, considering the benefit for breaking it is fairly negligible.
Whoops corrected. I literally thought you were my coworker because of a few coincidental comments between our conversations and his in slack :P. My bad. Do you have a twitter handle btw?
Ha! An actual confusion! I had guessed a typo. I do, technically, have a Twitter handle. But I don't actually use it, or even remember what it is.
&gt; the convention is far to established for me to consider breaking it I've seen plenty of packages that don't follow that convention. More importantly, it's not like *not* following the convention is going to have any untoward effects.
&gt; it's not like not following the convention is going to have any untoward effects Yea I mean at the end of the day, I don't think it matters at all. That's why I don't feel bad following the established convention. Of course there are a good number of exceptions to that convention, but it's still by far the most popular way to do it, and I think the Haskell community is in need of a little more consistency.
Yea I don't really see the point. It's no harder to add a monadic context than it is to add a `MonadReader` constraint.
&gt; Thanks to ST, I now know that this not the case, which is a huge relief. If required, I can do a literal translation of the algorithm, and clean it up (or not) later. That is not completely true, because you can only embed in ST algorithms that follow a rather simple region discipline in the way the effects flow into the program (the computation has to promise to stop performing effects in the future, and at that time its value becomes accessible as a pure value). This captures a large class of interesting programs, but still there are some algorithms that are observationally pure but internally use mutable states in a way that does not follow this discipline. They cannot be "proven pure" by using ST alone. You can of course write them in Haskell, but their type will carry the impurity annotation and may force you to restructure the rest of your program. For a practical example, take laziness: a simple implementation of call-by-need evaluation (where thunks are "memoized" after they have been forced) in a strict language is a function that captures a mutable reference, and mutates it on the first time it is called. This is observationally pure (Haskell uses mutation in this way underneath), but it cannot be given a non-effectful type using `ST` -- its effect discipline is more complex than that, as the initial value (the unevaluated thunk) is returned to the rest of the program while there is still one write to be performed in the future.
Before I was doing a course at university in which we were asked to use various algorithms to solve programming challenges. Problems had time and space limits, and we used mostly C++ to solve them. I wanted to see how far could I go with Haskell, and ended up using `ST` to achieve similar performance to the C++ solutions. It was a pleasant experience all in all, without losing Haskell benefits. Some of my solutions had even better performance that the provided C++ ones. I'm sure that my C++ code could be improved, but it would require too much effort / black magic / whatever.
&gt; Hope that made sense, this function took me years to understand You and everyone else. I think the type signature has been around for decades, but humanity only got around to proving it is "safe" (i.e. it does actually prevent leaking state) this year or last, IIRC.
I have seen three papers attempting to find a way toward ML-style modules in Haskell. Only one of them was worth reading, and even that one acknowledged (correctly) that it was still far from a satisfactory solution.
A true quicksort uses randomized pivots. If you trust the `Ord` instance to actually be a total order in the strictest sense you will ever care about, then you can sort in an `IO`-based randomness monad and wrap the whole deal in `unsafePerformIO`. This generally holds for "Las Vegas" algorithms, but not for "Monte Carlo" ones. A slow alternative is to use a deterministic but robust median calculation/estimation algorithm, such as median of medians.
You can see it working [here](https://github.com/vaibhavsagar/thursday-presentations/blob/6466b312ef07b4289c878356ddfcd6e4fba2f1d6/imperative-haskell/Comparisons.ipynb)! That's one of the things that's definitely cooler as part of a presentation :)
I'd love to see a comparison between the Haskell and C++ solutions if you still have access to that code and are willing to share!
Come back to us! I definitely got a lot more excited about Haskell after I realised I didn't have to read the papers unless I really wanted to :)
Yo, btw it's possible to take imperative style in haskell way further with GADTs and typeclasses. [ImperativeHaskell - example](https://github.com/mmirman/ImperativeHaskell/blob/08de62743d67be12406afe7ac5e0d1257545c739/Main.hs) [ImperativeHaskell - hackage](http://hackage.haskell.org/package/ImperativeHaskell) 
Lalalala I can't hear you. Wadler/Leijen will have to suffice ;-) 
The problem is there is no way to consistently wedge things into some huge universal hierarchy. No one ever specified what the huge hierarchy looks like. The wiki page OP linked does not specify what "Control" or "Data" even mean. I never remember if `Lens` is `Control.Lens` or `Data.Lens`.
The warning you're getting is `-fwarn-unused-do-bind`, I normally simply disable this warning by specifying `-fno-warn-unused-do-bind` in my cabal file and leaving it like you have no.
You can obfuscate the code if you don't like `void`: parseQuestN = read &lt;$&gt; (char '?' *&gt; many1 digit)
There's a (N+1)th way, which is to not use do at all: parseQuestN = read &lt;$&gt; (char '?' *&gt; many1 digit) You could also use `&gt;&gt;` instead of `*&gt;` - the latter is more general, since it's used for applicative functors instead of monads (I'm not sure if the brackets are required or not - don't have parsec handy atm)
To add my 2c, consider starting with the [microlens](http://hackage.haskell.org/package/microlens) library. The way it's structured and documented makes it much more accessible to beginners than the behemoth that is lens.
Excellent!
That's *not* a typical use of RankNTypes, but somehow became the most common...
Oh baby.... that's exactly what I needed.
How unsafe is it to perform an unsafePerformIO retrieval of the value when it's needed? This program needs a string from a file to perform an evaluation.
I find this form more pleasant to read than the `do`-notation form. I think this is what `ApplicativeDo` would desugar to, anyway.
For a simple but real ocsigen application, I have roughly these 200KB of JS. I say roughly because I've actually stopped caring at that point! The JS is smaller and faster than what any other website does. It's several times better on every aspect. It could probably be even better but visitors already notice it and enjoy it. Look at any JS typical library, it's usually huge and it dwarfs ocsigen's output.
I can sort of imagine what you're describing, but I'd really appreciate some reading on this style. Any resources you can recommend?
Actually you don't need to read the papers to use an abstraction. Imagine an instant than for the documentation of the print function in any language you got a documentation which said "To understand this function, you need to understand file descriptors, kernel interruption, computer graphics and font handling". Actually you don't need a PhD in vector font to understand that print will print some font on your terminal. The same for boarding a plane, I don't have any idea on how the hell this can be piloted. I don't know anything about fluid physics involved in the lift neither engine science. However I can board a plane and let the abstraction (my seat) do its work. This is the same about Haskell. You don't need to understand Monad to do some input / output, just accept that results from "side effect operations" are affected using `&lt;-`. You don't need to understand Lens to modify a structure, just use the well named `view`, `over` functions. You don't need to understand lazyness and its implications to write any program. You don't need to understand what a skolem is to fix type error. I don't know why people starting Haskell focus so much on understanding abstraction that they take for granted in any other language. How much of us fully understand C++ template, python object model (metaclass) or the subtlety of memory on nowadays hardware ? However everyone use theses tool without worrying about understanding them. (Sorry for the little rant ;)
yeah! get your fix!!!
Is this the part where I pretend to miss the point of the article and start an argument on the benefits of mergesort over quicksort? ;)
Marlow’s book explains these concepts in detail: http://chimera.labs.oreilly.com/books/1230000000929/index.html
Cool, that's the fourth option (N == 3) =). Which operator - `&gt;&gt;` or `*&gt;` - would be better in the context of text parsing? I read about the `ApplicativeDo`, but that's too theoretical (for me and for now), and without an intimate knowledge of parsec I can't really say if parsing text can benefit from parallelization.
I would not call it "obfuscate" - though I know that for many people Haskell is one giant obfuscation =)
I need a WL pretty printer that uses `ByteString`. Maybe backpackized?
That's the way I would write it. I should have added "sarcasm quotes" around the obfuscate word :)
I personally think: _ &lt;- char '?' is the clearest approach, and also introduces less visual clutter than wrapping it in a function call for example. 
How does this compare to the approach taken by [The Final Pretty Printer](http://davidchristiansen.dk/drafts/final-pretty-printer-draft.pdf) ?
`() &lt;$ char '?'`
That is a solution number 5! Are there any additional costs/benefits of using Functors instead of Applicatives?
I'm quite new to Haskell world, so I still need explicit "sarcasm quotes" =)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [maxigit/Metamorphosis/.../**ExampleSpec.hs#L214** (master → 3b67935)](https://github.com/maxigit/Metamorphosis/blob/3b67935c44eecc87bf42e72e58be5e5556049679/test/ExampleSpec.hs#L214) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply di82o1x.)^.
&gt; I don't know why people starting Haskell focus so much on understanding abstraction that they take for granted in any other language. Part of this might be because the literature is very easily accessible. I think it was Edward Kmett who said something to the effect of &gt; By using the name "Monad" instead of "Branchable", we make 70 years of background research available by a simple google search. If we instead called it "Branchable", we might have a slightly lower threshold to entry, but we also put the ceiling very low.
sorting using a sorting key (with `sortOn` and `sortBy`) can produce undetermined sort output. u/want_to_want gives a good example if you want to sort `[(0, 1), (0, 2),]` using only the first value of the tuples as your sorting key. Both results `[(0, 1), (0, 2)]` and `[(0, 2), (0,1)]` are valid sorted result. This kind of ambiguous result is not an issue for purity as long as the ambiguous is always handled the same way, i.e. you alway get the same ambiguous result. If the result starts to change, due to a random value hidden by `unsafePerformIO`, then it is not pure. Do we care about it, well, in some algorithm, no, in some, yes...
This thing is kind of awesome. Supporting multiple versions of TH is a huge error-laden chore.
Remember to add type signatures! ;) Pedagogical (and production!) Haskell should have type signatures on as many bindings as possible; really helps clarify what's going on when you're trying to teach something.
Shall I wake you up when [September](https://en.wikipedia.org/wiki/Eternal_September) ends?
Because I can never remember the relative associativity between `*&gt;`, `&lt;*` and `&lt;$&gt;`, but yeah, it is much nicer!
Take a look at the functions that actually use the value, and the way in which they are used. Can you rearrange your data types so that the necessary data is embodied in a partially applied function which then gets passed around instead? Its a bit difficult to be more specific without the details of the problem, but your problem is a common one amongst beginners, and the long-term solution is to rearrange the way you design your code. Tramp data is always a code smell.
I guess the argument is invalid because the problem is formulated in the opposite direction. The fact that the literature is available is great if you want to read that literature, but I don't think it increase the need to read that literature. I'll continue with analogy: there is a lot of literature about how a plane fly, but most of the people boarding a plane will never read it. &lt;rant&gt; (Disclaimer, I'm not bashing specifically about C++ here, that's an example, I don't want to start a C++ versus Haskell thread, just show some difference of behavior on how difficulty is perceived between the two communities). When a C++ beginner read something like: std::cout &lt;&lt; "hello" &lt;&lt; f() &lt;&lt; g() &lt;&lt; std::endl; , it concludes that it prints "hello" on the standard input followed by the result of calling `f` and `g` and go one. Perhaps, a few years later, he will understand that it involves namespaces, the STL, operator overloading, pointers, buffer flushing, sequence point, and that the result in not guaranteed depending on the C++ standard you use. That's really complex, but the abstraction is simple to use... On the other hand, when a Haskell beginner sees: do s &lt;- getLine putStrLn s they conclude that Monad are complicated to use and stop learning Haskell. I don't know why... When a C++ beginner starts creating its own types, they have to think about memory management, and this involve a deep understanding of about constructor, copy constructor, move constructor, operator =, move operator =, destructor, rvalue reference, lvalue reference, a gazillions of special cases, rule of 3, rule of 5, rule of 0, implict conversion... memory ownership, concurrency. This usually takes many years, most C++ developers I know (me included) are not able to get that right. Well, that's normal. On the other hand, when a Haskell beginner got his first space leak, he concludes that it is difficult to reason about memory in Haskell. When a C++ or python developer reuses an exhausted iterator and take many hours to understand why that iterator is empty, they conclude that they are stupid, off course the iterator is empty. On the other hand, when a Haskell beginner reuse a huge list which was previously fused, and get the same correct result, but a huge space leak, they conclude that reasoning with memory in Haskell is difficult. When a C++ developer learn about inheritance, virtual dispatching, static inheritance, templates, visitor pattern, functor (i.e.: `operator()`), union, enum, tagged union, static inheritance, final classes, final methods, they increase their toolset as a developer. On the other hand, when a Haskell developer learn about ADT and functions (which covers most of the use case of the previous paragraph in C++), they conclude that's complicated... &lt;/rant&gt; Well, this rant is now too long sorry, I'm just trying to understand why Haskell appears complicated, when in fact it is as complex as many other languages.
Sure! I will look the code again and write about that experience, good idea of yours :D
As far as I can see from the readme you could swap in my algorithm if you wish. It certainly is compatible with using "Text" in the backend.
I agree. Not only is this kind of refactoring annoying, even initially writing a monadic version incurs a significant (codesize and mental) overhead, and it makes other kind of refactoring much harder. I'd strongly advice against prematurely working in the Identity monad when you are not certain that you'll need effects later. In some cases (e.g. (lower-level) logging) I'd even rather make use of a slightly-less-unsafe wrapper around unsafeYouKnowWhat than force all code where I want to (temporarily) add some debugging logging into monadic contexts. That your language allows adding effects more easily seems strange though - after all the added "boilerplate" has one important effect: It specifies the order of effects, in general. Apart from some special cases where the order is not important (e,g. Reader) I don't see how you manage to avoid this.
What problems can we solve with negative and/or fractional types? Are there any well known motivating examples?
Interesting to see that your layouting algorithm uses two document representations internally - is this something that the "predecessors" did as well? I was only aware of the one `Doc` type in those. (In my haskell source pretty-printer, [brittany](https://github.com/lspitzner/brittany), I tackle the same problem, and essentially use rewrites on one internal document data structure.) Brittany uses two traversals to make good choices regarding line-breaking. I have documented this somewhat in [this document](https://github.com/lspitzner/brittany/blob/6868a776f2556453606a27d7cba189535b49cfc2/doc/implementation/theory.md); especially [this paragraph](https://github.com/lspitzner/brittany/blob/6868a776f2556453606a27d7cba189535b49cfc2/doc/implementation/theory.md#the-algorithm). Perhaps this approach would be applicable to a general-purpose document formatter as well?
October is going to be awesome, I can't wait.
Linear types only help you to write more restricted functions. A rational DSL would already have to be much more limited than linear functions in order to guarantee reversibility. Therefore, to write such a DSL in Haskell, I wouldn't use functions at all, I'd define some other Category. And it's already possible to define a Category with operations for manually [swapping](https://hackage.haskell.org/package/categories-1.0.7/docs/Control-Category-Braided.html#t:Symmetric) and [reassociating](https://hackage.haskell.org/package/categories-1.0.7/docs/Control-Category-Associative.html#t:Associative) variables, but without operations for [dropping or duplicating](https://hackage.haskell.org/package/categories-1.0.7/docs/Control-Category-Cartesian.html#t:Cartesian) them, so I bet it's already possible to define a rational DSL in Haskell.
We were able to get the `armv7-linux-androideabi` target for ghc working. But while building `aarch64-linux-android` target, we got some compiler panics. (GHC version 8.3.20170530 for aarch64-unknown-linux-android): ghc-stage1: panic! (the 'impossible' happened) LlvmCodeGen.Ppr: Cross compiling without valid target info. (GHC version 8.3.20170530 for aarch64-unknown-linux-android): Please report this as a GHC bug: http://www.haskell.org/ghc/reportabug LlvmCodeGen.Ppr: Cross compiling without valid target info. Please report this as a GHC bug: http://www.haskell.org/ghc/reportabug tc-DTargetArch=\"aarch64\" -optc-DTargetOS=\"linux_android\" -optc-DTargetVendor=\"unknown\" -optc-DGhcUnregisterised=\"NO\" -optc-DGhcEnableTablesNextToCode=\"YES\" -static -eventlog -O0 -H64m -Wall -Iincludes -Iincludes/dist -Iincludes/dist-derivedconstants/header -Iincludes/dist-ghcconstants/header -Irts -Irts/dist/build -DCOMPILING_RTS -this-unit-id rts -dcmm-lint -i -irts -irts/dist/build -Irts/dist/build -irts/dist/build/./autogen -Irts/dist/build/./autogen -O2 -Wnoncanonical-monad-instances -c rts/RtsUtils.c -o rts/dist/build/RtsUtils.l_o rts/ghc.mk:251: recipe for target 'rts/dist/build/StgStartup.o' failed Any idea what's going on here?
And here's the cherry on the cake. Consider the statement: f(a(), b()); Which method is called first, `a` or `b`? Even in Haskell, with it's laziness, the evaluation order could somehow be tracked, depending on the caller. The C++ standard doesn't seem to think that this is important in any way and simply leaves the evaluation order [undefined](https://stackoverflow.com/questions/621542/compilers-and-argument-order-of-evaluation-in-c). So much for functional programming in C++.
IMO, a great book for understanding the GHC primitives, as long as you've got basic Haskell syntax down. But, for most purposes just using [async](https://hackage.haskell.org/package/async), the other libraries built on top of it, and their API docs is more practical. The book stops around the same place the async library starts, mostly. :/
Seems like there isn't much motivation yet other than extending the algebraic aspect of algebraic data types (touched on [here](https://codewords.recurse.com/issues/three/algebra-and-calculus-of-algebraic-data-types/)), taking them from type rings to type fields. Using them for something like continuations is mentioned [here](https://awelonblue.wordpress.com/2013/08/24/pipes-that-flow-in-both-directions/). I wonder what it would mean to "divide by zero" in type-space. [edit: I guess that would have the same ramifications for type equality as it does for value equality]
The layout algorithms are `Doc -&gt; SimpleDocStream`, which basically just picks the ideal layout from a set. Adding another algorithm is worth looking into for sure!
Which two document representations do you mean? Doc and SimpleDocStream? The Wadler/Leijen algorithm uses the intermediate format, yes.
So, basically, not only is Haskell the [best imperative language](http://code.haskell.org/~slyfox/2014-04-27-haskell-intro.pdf) and [makes you a better programmer](https://www.reddit.com/r/haskell/comments/31tsru/why_should_i_learn_haskell/cq4x3e6/) (even if you don't end up writing Haskell), it's also the best multi-player game language? And, both of these are because (not *despite*) it set out to be a [uniform language for researching lazy evaluation](http://haskell.cs.yale.edu/wp-content/uploads/2011/02/history.pdf). What great things we can produce when [we stop trying to succeed](https://www.reddit.com/r/haskell/comments/2b9hf4/an_idea_to_help_haskell_take_off/cj35fur/) and [start trying to create](https://news.ycombinator.com/item?id=1960243). :)
It's tempting to leave them off to make it seem shorter, but I don't think it helps in the long run; cheers!
Another option is to write parseQuestN = read &lt;$ char '?' &lt;*&gt; many1 digit I prefer to use only `&lt;$&gt;`, `&lt;$`, `&lt;*&gt;` and `&lt;*`. That way I don't have to worry about associativity, and the function that is applied is always in front.
Very cool! I wrote something sort of similar a while ago: https://github.com/WraithM/btcsim/
[removed]
Ah I see, DOC/Doc roughly corresponds to the two.
(The thread was already off the front page for me when you posted this comment yesterday; probably need to make a new one if you want it to get attention. But presumably you've already realized that.)
One small sidenote is that `foldMap id` is just `fold`. 
Yeah, that's the Reddit attention span. I'm not going to re-post, but thanks. 
i agree that sandboxes have shrunk cabal hell to cabal heck. but the rebuilds are still too long and take up too much space (i don't think stack shares files either, but nix does). 
there are more than 50 of us? (jokes aside, that's fair). 
real nix, but yeah, i've tried new-build too, it's great. i guess to be clear: global cabal isn't immutable. sandboxed cabal (which i'd used for years pretty well) is slow. stack doesn't share files. cabal new-build doesn't download binaries. nix is best for installation speed and storage space, but has worse editor support (though the intero fork dante half-works). https://github.com/jyp/dante https://github.com/Gabriel439/haskell-nix/blob/master/project0/README.md
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [Gabriel439/haskell-nix/.../**README.md** (master → 384e1d6)](https://github.com/Gabriel439/haskell-nix/blob/384e1d6d6a2fef457b3b8823c15d5da8add89188/project0/README.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply di8ssdu.)^.
really!? sweet. will it use nix, if nix can be made to work under windows? it seems hard to do reliably without the complete dependency tracking of nix, though i guess system package managers have done all right for a long time. for example, i'd love it if lens took seconds to install (like it does with nix). but, the packages that are most likely to fail to build or to take hours are things like hmatrix or gtk, which have significant external dependencies. 
&gt; will the hackage matrix builder be like stackage? I'm not sure I understand the question. Stackage is currently the most popular family of curated package collections, whereas the Hackage matrix builder is intended to be Hackage's massive CI &amp; meta-data validation system. I don't think the two are comparable. &gt; by practically realized dependency graphs, you mean conditional depends-on? By practically realized graphs I mean those which result from a single actual solved install-plan. There's not only explicit cabal conditionals that can change the transitive dependency closure, but even a different package version of a dependency can result in a totally different dependency footprint. The Hackage matrix builder records empirically built dependency graphs into a big graph database, from which one can then apply various kinds analysis and heuristics to detect problems.
Kitten’s effects are generally carried out in source order, like an imperative language; it’s strictly evaluated, a bit more closely related to ML than Haskell. The types (“permissions”) just describe which effectful operations you’re allowed to use, and you can give the compiler information about which permissions are commutative—so it will be able to safely reorder operations for better performance, e.g., map/map fusion. Like algebraic effects, I’m also working on a way to allow you to discharge permissions with “handlers” in different orders, depending on how you want some code to be interpreted.
Haskell doesn't have a value restriction so you can use `unsafePerformIO` to create a function of type `a -&gt; b`. For example see https://stackoverflow.com/questions/22114757/unsafecoerce-implementation-in-haskell
Is `*&gt;` problematic?
Well, this only argues that it's the best for mutliplayer games with very unrealistic development conditions. "Client side integrity" is not something you can usually get away with. Transmitting events also doesn't work because you need to get all of them to reconstruct the correct state which means you need to use a reliable protocol which is also something you can't get away with.
Haskell is great when you don't care about performance very much. The only time you should use lock-step is when you have so much state that it's infeasible to send it across the wire. If you're dealing with that much mutating state, then Haskell probably won't give you the performance you need.
&gt; Type-classes are really nice, and OCaml doesn't have them I expected 'Type constructor polymorphism is really nice, and Ocaml doesn't have it'.
My understanding is that type classes (and thus the work on modular type classes) provide this in limited sense, but not fully as in system F. Am I wrong? I confess I wouldn't know what to use "full" type constructor polymorphism for.
I meant higher kinded types.
Maybe a third option in terms of constraint solving will emerge. Something involving analyzing directed graphs dependencies, and identifying decompositions of the hackage universe into something akin to strongly connected components, more vigorously curating those components. The idea isn't fully fleshed out yet.
That's exactly what [we](https://obsidian.systems)'re working on, along with /u/angerman! As much as possible will be upstreamed into nixpkgs itself.
Ah, that's why some posts appear twice! I browse posts by date, not by popularity, so that reason had not occurred to me. I usually downvote the second one as a duplicate, I guess I should stop doing that!
Yay! Nice. Is `onCreate`the place to call `initHS()` from? Will there be any issues when e.g. phone screen orientation changes and so `onCreate()` is re-run?
Well, thanks! :-) As mentioned, you really want to call this **only once**, primarily because `initHS()` wraps [`hs_init`](https://github.com/ghc/ghc/blob/master/rts/RtsStartup.c#L127-L131), which in turn calls [`hs_init_ghc`](https://github.com/ghc/ghc/blob/master/rts/RtsStartup.c#L141-L295), which [silently ignores any subsequent initializations](https://github.com/ghc/ghc/blob/master/rts/RtsStartup.c#L144-L148). Edit: I suggest making sure you call it only once, because you might end up extending `initHS` to pass RTS options to `hs_init`, and then call `initHS` with a different sets of RTS options, wondering why nothing happens ;-)
Yes, hence my question: is `onCreate` really run-once enough even in the face of screen orientation changes?
* `seq a b` simply makes it so demanding b also demands a. It doesn't say what relative order they evaluate in. * `pseq a b` says the same as `seq a b`, but also says a gets demanded _before_ b gets demanded. It does this in a sort of tricky way preventing the compiler from rewriting it into a different order by playing with how the combinator is flagged for strictness. * `par a b` enables a 'spark' to compute a concurrently in the background _while_ you compute `b`. sparks are sort of "best effort" jobs that can be dropped on the floor if there get to be too many of them, but can be used to put otherwise idle cores to work. My favorite example of `par` is a combinator: spec :: Eq a =&gt; a -&gt; (a -&gt; b) -&gt; a -&gt; b spec g f a = guess `par` if a == g then guess else f a where guess = f g Pretend we have a two stage pipeline. One stage computes 'a'. The other produces `f a`. `spec g f a` takes a guess `g` at what the argument 'a' will be when the first stage of our pipeline is complete. `g` should be cheap to compute. If you guess right, this enables the first and second stage of your pipeline to execute in parallel. If you guess wrong, then when 'a' finishes and the comparison of a == g completes and is proven false, then the spark (running or not!) gets garbage collected, and f a is computed instead. If you have an 80% accurate guessing strategy this can gain 80% utilization out of an otherwise idle core. (Ignoring data migration issues, other stuff on the machine, etc.)
Ahh! No in a real application, you would not want to call `initHS` in `onCreate`. In this case it's just there to keep the sample code concise. I'll add a note. Thank you!
OK so I went and looked at the [Android Activity Lifecycle doc](https://developer.android.com/guide/components/activities/activity-lifecycle.html) which says that [`onCreate` is called only once](https://developer.android.com/guide/components/activities/activity-lifecycle.html#oncreate): &gt; `onCreate()` &gt; You must implement this callback, which fires when the system first creates the activity. On activity creation, the activity enters the `Created` state. In the `onCreate()` method, you perform basic application startup logic that should happen only once for the entire life of the activity. so I guess screen rotation goes through the entire app lifetime including shutting it down and restarting from scratch. So `onCreate` might be the right place to do this after all. Also, in [Handling Configuration Changes](https://developer.android.com/guide/topics/resources/runtime-changes.html): &gt; Some device configurations can change during runtime (such as screen orientation, keyboard availability, and language). When such a change occurs, Android restarts the running `Activity` (`onDestroy()` is called, followed by `onCreate()`).
Yeah, exactly. Obviously, everything works when the types are the same, but that is a special case. The only way I can think of is adding a `Comonad` (well, a `CoApplicative` if it exists) constraint and using `extract` to obtain the solution. Obviously, that restricts Monad instances to unions of types which are also Comonads, but that's better than nothing, I suppose?
Congrats; I hope it gets traction. I was hoping to achieve similar results with https://github.com/ekmett/wl-pprint-extras. From the looks of it, prettyprinter does not support effects, but does support annotations. Could I beg you to add the second type parameter and bifunctor instance? Separately, I have always found the Pretty instance for Doc itself to be in poor taste, as it sheds annotations (and effects, in the case of -extras), which seems like more of an accident-waiting-to-happen than a desirable feature. (It is, of course, possible to splice like that explicitly, should one wish!)
What do you make of `duplicate`? Is there a reason for this type to be `Applicative`? I distrust `Comonad`. It may be useful, very circumstancially, but it almost always comes up in a discussion just because it has `extract :: w a -&gt; a` with a very convenient-looking type, in order to implement an unmotivated abstraction. This looks like such an situation. I may be a killjoy here, but I think `extract` is a trap similar to `unsafePerformIO`: it seems tempting to use it to escape an unfamiliar context (an abstract `w :: * -&gt; *` instead of `IO`), when instead one should learn to remain inside it. However, rather than being unsafe, that call to `extract` will likely end up being useless.
Haskell doesn't need Google as its main source of documentation, opposite to other languages. Does the extension https://marketplace.visualstudio.com/items?itemName=jcanero.hoogle-vscode help in your case? It is still opening a browser window/tab, but no Google needed at all :) Sometimes the function name and its type suffice.
Congrats on the release! I would be interested in hearing what /u/bgamari thinks about it, and if he thinks there is any chance of it playing a central role in GHC's ASM generation. I have [been trying for a while now](https://github.com/haskell/pretty/issues/42) to extend pretty to support alternative textual representations in order to use that instead of GHC's own fork (more in-depth information is available at that GH link I've posted). As this new library seems to depend only on `base` and `text` looks like a promising candidate, but I'm too far inexperienced on the topic to speculate ;)
Do fractional types have anything to do with quotient types in dependently-typed programming?
Well, kinda. `stability` maintain the relative order of records with equal keys, this is a stronger guarantee than purity of the sort. Purity guarantee that whatever the arbitrary order of equal keys, it does not change with multiples calls. Stability guarantee that it does not change and that it is not arbitrary.
I don't understand the C++, but this method seems very generic, almost to the point it could be made a library. Is that possible?
Hi, can you detail the feature in the haskero gitlab ? https://gitlab.com/vannnns/haskero/issues I'm thinking about such a feature for a long time and if we could share our ideas, samples from other IDE, and so on. It could be great ! For now, i was thinking about an *universal goto definition* where you can press F12 on an identifier and if it's not in your code, it opens the related hackage doc in your browser.
&gt; So, basically, not only is Haskell the best imperative language and makes you a better programmer (even if you don't end up writing Haskell), it's also the best multi-player game language? It also happens to be the best dynamically typed/gradually typed language out there. You can choose just how dynamic you want your objects to be. You want a type that's dynamic enough to store arbitrary JSON values, but not non-serializable things like file handles? Use a `Value` from the `aeson` package. You need file handles and foreign objects too? Put a `ForeignPtr` and a runtime tag somewhere in your dynamic ADT and you're done.
No, it seems to be different: http://www.hedonisticlearning.com/posts/quotient-types-for-programmers.html To my understanding, quotient types are types with a new equivalence relation. Again to my understanding, negative and fractional types are types that impose constraints on the flow of execution...
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/ZDSbvxv.png ^[Source](https://github.com/AUTplayed/imguralbumbot) ^| ^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^| ^[Creator](https://np.reddit.com/user/AUTplayed/) ^| ^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^| ^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20di9hc3x) 
An `Applicative (Sum f g)` instance exists when there exists a natural transformation between `f ~&gt; g` respecting the monoidal structure (see [*Abstracting with Applicatives*](http://comonad.com/reader/2012/abstracting-with-applicatives/) and [*Constructing Applicative Functors*](http://www.staff.city.ac.uk/~ross/papers/Constructors.pdf)). I assume the same is true here. As an example [`data Lift g a = Pure a | Other (g a)`](https://hackage.haskell.org/package/transformers-0.5.4.0/docs/Control-Applicative-Lift.html) is a special case of `Sum Identity g a` but with an `Applicative` instance using the monoidal natural transformation type f ~&gt; g = forall xx. f xx -&gt; g xx nat :: Applicative g =&gt; Identity ~&gt; g nat (Identity a) = pure a
Perhaps you could define a "HasId" class with a "getId" method and make your records implement it. 
Using `String` instead of `Text` is a trivial change, and won’t take long to implement. I will not add `String` support (beyond the `ShowS` backend and `pretty :: String -&gt; Doc ann`) to this package though because I think it’s wrong to use it for strings, but I agree that GHC doesn’t have the liberty of choice here.
Create all your lenses in a single module, then the compiler will stop complaining about ambiguous typesclass instances and all that jazz (I'm assuming you're defining these lenses with `makeFields` or another TH function which generates typeclasses named `HasId`, &amp;c). {-# LANGUAGE TemplateHaskell #-} {- other pragmas required by Lens's generated code -} module Domain.Lenses where import Models.Client -- ... import Control.Lens.TH makeFields ''Client -- ... That way you only have to import a single module -- `Domain.Lenses` -- whenever you wanna use lenses. I see you're using Opaleye. Note that Esqueleto has been problematic for me due to using similar operators to Lens (`(^.)`, &amp;c), but I have zero experience with Opaleye. Of course, with this solution you can't use lenses in the actual `Domain.*` modules defining the data structures, unless you wanna deal with GHC's handling of recursive modules... PS: `id` is not a bad name. Stephen Diehl's [`protolude`](https://hackage.haskell.org/package/protolude), for example, renames the `id` function to [`identity`](https://hackage.haskell.org/package/protolude-0.1.10/docs/Protolude.html#v:identity), so that you can use `id` for your own, well, *actual* ids and stuff.
If you use the class method you won't need qualification. I'm assuming your records are used a lot and that amortizes the extra work of declaring the class and the instances. Edit: perhaps it would let you define a generic "restrictById" arrow as well.
I also did a comparison of the (at the time) most common web frameworks during my time at university: [Presentation](https://github.com/beerendlauwers/school/blob/master/School/Master%20Utrecht/AFP/AFP%20Presentation/doc/Presentation%20-%20Comparing%20Haskell%20Web%20Frameworks.pdf) [Report](https://github.com/beerendlauwers/school/blob/master/School/Master%20Utrecht/AFP/AFP%20Presentation/doc/Report%20-%20Comparing%20Haskell%20Web%20Frameworks.pdf) Of course, a lot has changed since that. As you mentioned, &gt; Many of the libraries are not specific to the framework and can be used together with other frameworks This was not always the case. It is interesting to see from which web framework a particular (now independent) package originated from, though. 
We got it working. Thanks a lot!
No, it shouldn't. You cannot reliably prevent people from messing with the client and revealing hidden information.
It's kinda similar, but with fractional types you're always quotienting by a single type (a product component), so you can go between (*a*×*b*)/*b* and *a* (they are isomorphic), but that's all. With quotient types, you're quotienting by a groupoid (a.k.a. an equivalence relation), so (*a*×*b*) // (let *x* ≈ *y* iff fst *x* = fst *y*) is also isomorphic to *a*, but that's just a special case of a far more general construct. Similarly for subsets and negative types, by the way.
If it's purely textual duplication, you can always use CPP macros.
I have no pain with Maybe NOW --- I'm talking about initial learning curve. One thing that is important to remember is that when you know stuff deeply, it's easy to forget how hard it was to get up to speed or how hard it might be for others. As for Swift optionals, I'm not advocating for them (I don't use Swift myself nor C# for that matter, but I keep up with what's going on in language development) I was simply noting that other languages are implementing (or trying to implement) stuff that has been found to be useful in Haskell. &gt; the reason why we need functor, applicative and monad I rest my case --- new users' eyes will glaze over when they see terms like functor, applicative and monad and/or they'll have to go off and read a ton of stuff (papers/blog articles etc) to figure out what's going on. You can't just use that stuff properly without really understanding at a much deeper level than you need to understand to use many other languages. You know, there's a lot of truth in humor and when you think about how many monad tutorials there are where each author essentially writes, "here's yet another one", which as I plowed through (and also of course wrote my own) eventually led to MY theorem: **No article about Monads can be understood until you have read two dozen other articles about Monads.** A suitably recursive theorem for a Haskell audience, don't you think :-)
If all your id types are `Blah Int64` you could define `newtype Id a = Id Int64` and have the have `type PhotoId = Id Photo`. Then it is possible you could write generic instances for `Id a`. Not neccessarily possible in your case, but many times it is.
Maybe Dash or Zeal can help? VS Code extension: https://marketplace.visualstudio.com/items?itemName=deerawan.vscode-dash
Indeed. There is no reason to go through these steps though. You can put the `HasId` class somewhere that won't change and write the instances yourself manually. Or if that sounds onerous, you might want to think about using an Entity type like `data Entity a = Entity { id :: Id a, value :: a }` then you will only have to write one instance, although working the `value` will become slightly more work. 
I obviously don't know how your app is structured. Maybe it is possible to have a single Types module where you place all data and newtype definitions? Then the lenses only depend on that file - you can use lenses in the Models.* modules and changing them doesn't dirty everything. I also find this structure quite helpful because it gives one place that summarizes everything the program does.
If I understand this solution correctly, this would allow `PhotoId` to be passed where a `BookingId` is expected (`type` vs `newtype`). In which case, it would defeat the purpose of having an `XId` in the first place.
Don't use the name `id` everywhere. Use `bookingId`, `tripId`, `clientId`, etc. This is direct consequence of the fundamental law of the universe that name uniqueness/complexity should be proportional to scope. In Haskell, functions have global scope unless you qualify them. So if you don't want to qualify them, you need to make the names more unique.
Haskell is great when the performance numbers you care about are throughput or even median latency. The only performance number that's hard to hit with haskell are the worst-case latencies. Those make hard real-time almost impossible, and game clients sometimes implausible , depending on the style of game. I suppose you probably were implicitly talking about a game context, but your statement was very blanket and easy to take out of context. 
Only if you wanted that to be the case. if you hand a signature like `foo :: PhotoId -&gt; IO ()` you could not pass a `Id Book` or any other `Id` besides `Id Photo`. My solution is not less type safe, it is merely an application of phantom types: https://wiki.haskell.org/Phantom_type.
Actually `id` is not the only field which has this problem. There are a number of fields that represent the same "concept" and occur in different record types, eg: * createdAt * updatedAt * name * description * currency * amount * timezone * properties * photoId * userId * accountId * and more...
Besides having an id, are there characteristics common to all records? Or does each record have a particular subset?
You may be interested in how Persistent does this thing. They have a type class for entities. The type class has an associated data family `Key` which is used for the primary keys of these entities. class PersistEntity rec where data Key rec instance PersistEntity Photo where newtype Key Photo = PhotoKey { unPhotoKey :: Int64 } type PhotoId = Key Photo This gets you a little more type safety, in that a data constructor `PhotoKey` makes a `Key Photo`, and you can also specify non-Int64 primary keys. Otherwise, an un-annotated constructor like newtype Key a = Key Int64 -- inferred type is `foo :: forall a. Key a` foo = Key 32 can be inferred to be any kind of key, which might not play super well with type inference.
You're correct on all those things gelisam, but I don't learn by listening and interracting with teaching staff during lectures etc. I best learn by studying around lecture content at home and it's just I am having trouble understanding this particular subtopic in Haskell therefore I am asking on this forum. I do understand that Haskell is lazy and will only evaluate stuff when necessary otherwise it is stored as a thunk in space - correct me if I am wrong! Also, now from what I have just read (and understand), par takes in 2 arguments and evaluates both of them in parallel (on separate cores(?)) except the second argument gets returned while the first argument is a spark and has a lower priority compared to other computations/evaluations?
i google as frequently when programming in haskell as any other language. there's still a lot of patterns, libraries, language extensions, complex types, etc. 
http://www.composeconference.org/2017/program/ – the video is not yet online.
Each record has a particular subset.
Any info regarding local (cross) compiling with GHC 8.x + FLTK on (for) a Rasperry Pi 3?
SublimeHaskell is another (great) option.
Hmm, I'm not sure? Is it the same person posting? I'd expect they'd put something in the title to make clear that it's a new thread and why? I haven't noticed any of these duplicates I don't think (and I would notice, at least, from the comment thread starting fresh), except where two separate people accidentally submit the same thing.
(EDIT 3: This is all probably moot—see EDIT 3 below.) If we represent the permutation in the right way, we can write a "successor" function that takes *amortized* constant time, where amortization is over all the permutations. We can do this by thinking about permutations as numbers in little-endian "base factorial". Here's how it works. Imagine building the permutation from left to right. We do this by repeatedly removing elements from the list of available items. For example, we start with ABCDE available, and if we start our permutation with C, we are left with ABDE. We can now represent the permutation we're building as a list of digits: *the nth digit is the index in the list of available items of the nth element of our permutation.* (Examples below.) Incrementing of a number represented as an immutable list of digits takes constant amortized time, because you have to rebuild on average 1 + 1/n + 1/[n(n-1)] + 1/[n(n-1)(n-2)] + ... &lt; 2 elements each time. And though it's a little fiddly, I think we can write the permutation "successor" function in much the same way without explicitly going to and from the numeric encoding, giving us the same amortized constant performance. Examples: here are how the first few permutations of ABCDE look, along with their little-endian numeric representations: * 00000 ABCDE * 10000 BACDE * 20000 CABDE * 30000 DABCE * 40000 EABCD * 01000 ACBDE * 11000 BCADE * 21000 CBADE * 31000 DBACE * 41000 EBACE * 02000 ADBCE EDIT: formatting and minor wording tweak. EDIT 2: Hmmm, writing the successor function without doing a round trip through the numeric representation might actually take longer than the numeric increment—would be curious if someone got it to work! EDIT 3: ... if you're going to use each entire permutation, then you have to spend O(n) time just to read it, so you should probably be willing to spend O(n) time doing the successor operation anyway. Eh, at least the base factorial thing was cool.
Definitely! If you care about cheating, then this is entirely the wrong model. Maybe there's some immensely clever way to fix it up with fancy cryptography tricks, but I don't see it. Fortunately, in this environment, we have the liberty of not caring. The goal is to let children share and play with the projects they create, and worrying about security is sort of like worrying that a doll house doesn't have working locks on the doors.
&gt; randomized pivot selection I don't see why this is an issue when pseudo-random generators are in fact deterministic yet for purposes of pivot selection the seed does not need to be fresh on each invocation barring an adversarial context, in which you'd just pass in the seed from the effectful part of the program.
 main = do bs &lt;- B.readFile "colors.json" print $ bs ^@.. values&lt;.key "color"._String _Duh_
The main point of having documentation popup right under your cursor is that you don't need to switch out of the IDE. Not to mention no typing required. It's a very basic feature in modern IDEs nowadays.
I spent a while digging through hackage revdeps and it looks like Effects are less used than I would have guessed. That said, wl-pprint-terminfo, which was not ported over to the bifunctor wl-pprint-extras version, has an Effect type which emulates Annotation for some things (https://github.com/ekmett/wl-pprint-terminfo/blob/master/src/System/Console/Terminfo/PrettyPrint.hs#L78) but considers ringing the terminal bell an effect at a particular point, rather than an annotation (https://github.com/ekmett/wl-pprint-terminfo/blob/master/src/System/Console/Terminfo/PrettyPrint.hs#L103) I hope I haven't missed something; but as it stands, I think I agree with your assessment that annotations might suffice.
Haven't been writing much Haskell lately, but in the past I've downloaded documentation with Dash (it can download straight from Hackage, or there are projects to make docsets for it from your specific project), then used the sublimeText integration to open documentation for a highlighted function. 
I'm not sure which are your 'other languages' you're mentioning. In Java, a good library comes with its Javadoc documentation, all you need to do is pressing a hotkey and the documentation popup appears. Usually that would be enough for you to have a good idea of what it's supposed to do. For Haskell, even `:info`in Prelude gives me nothing other than a type signature. Or maybe I'm missing something here?
[removed]
literally all other languages, IntelliJ doesn't store and search through stackoverflow or blog posts, that's what google is for. you're missing their point, which i was disagreeing with. yes, haskell has much more consistency than other languages: parametric polymorphism means that you can't show or hash an arbitrary unknown type; operators are user-defineable and can be written as prefix, most are bound to typeclasses; etc. but ghc's type errors are almost illegible (like "Expected versus Actual"), and it takes a while (or never) before you could just code offline for years from your own knowledge of the ecosystem and compiler. 
Until we have a model, it's all speculation. Fractional types (p/q) seem to be types with |p| inhabitants, each of which have an internal symmetry for which you have exactly q proofs of that symmetry. [You get this from unwinding the exact Groupoid model for the identity type, and using the Baez-Dolan definition of cardinality]. My current thinking on this is that these are types that require (|p|*|q|) bits to represent on a computer, but |q| of them are not needed to represent the actual values. So there are |p| values but |q| annotations, which are irrelevant for value computations, but nevertheless represent additional information about the data. Negative types still seem to be obligations. Because "global" computations (as we know them) must go from positive types to positive types, the use of negative types internally seems to be a useful way to record obligations and march on, knowing [because of types!] that these *will* be resolved in a timely manner.
Maybe you're going too far from what I'm looking for. I'm asking for a very basic feature: show me the documentation that comes with the function I'm using in my code if there is one. Period.
Yes, we can make that interpretation of negative types work -- as long as you do not want these to be coherent with product types. Because that does not work (the obstructions are 'classical', and come from algebraic geometry!). Ongoing work: We're trying to use ideas from Baez-Dolan and HoTT to find a model where these types (negative, fractional) are somehow natural.
That dialect of brainfuck is the favorite pastime of haskellers nowadays
This package looks excellent and appears to cover all of GHC's current and planned use-cases. Great work, /u/quchen! For better or worse GHC doesn't currently depend upon `text`; changing this is not trivial but can be done given a good enough reason.
I can golf the "color-upper" a little, but I'm still sure there's something better: λ bs &amp; values %~ (\o -&gt; ⋮ o &amp; _Object . at "color-upper" .~ (o ^? key "color" . _String . to T.toUpper . re _String) ⋮ ) "[{\"color\":\"red\",\"value\":\"#f00\",\"color-upper\":\"RED\"},{\"color\":\"black\",\"value\":\"#000\",\"color-upper\":\"BLACK\"}]" **edit**: a little shorter, and getting rid of the `_String . {-...-} . re _String`, which I found the most bothering λ bs &amp; values %~ (\o -&gt; ⋮ o &amp; _Object . at "color-upper" .~ (o ^? key "color" &amp; _Just . _String %~ T.toUpper) ⋮ ) "[{\"color\":\"red\",\"value\":\"#f00\",\"color-upper\":\"RED\"},{\"color\":\"black\",\"value\":\"#000\",\"color-upper\":\"BLACK\"}]" **second edit**: perhaps (YMMV) a smidgen simpler: λ bs &amp; values . _Object %~ (\o -&gt; ⋮ o &amp; at "color-upper" .~ (o ^. at "color" &amp; _Just . _String %~ T.toUpper) ⋮ ) "[{\"color\":\"red\",\"value\":\"#f00\",\"color-upper\":\"RED\"},{\"color\":\"black\",\"value\":\"#000\",\"color-upper\":\"BLACK\"}]" 
The algorithm described in the paper that fixes the problem of “going back in time” arbitrarily far relies on in-order-delivery. But if we did not have that (but maybe had sequence numbers), something similar could be done as well.
`=`, `&lt;-`, `.` and `$` are pretty well established. Only 2 extra operators are added here: `^@..` and `&lt;.`. Lens has a lot of operators, but conceptually, it has only a few basic ops that compose: `^.`, `^?`, `^..` are the indexing operators: get-one, get-zero-or-one, get-list. Then there's `@` which means "indexed version of", so if you stick that inside an operator it always means "the indexed version of the operator". So `^@..` is just `^..` (`toListOf`) but with indexes. The `&lt;.` and `.&gt;` and `&lt;.&gt;` operators are analogous to `Applicative`'s `&lt;*`, `*&gt;`, and `&lt;*&gt;` operators. They compose optics that have indexing - but keep the indexing only from the left, only from the right, or combining both indexes from left and right.
GC is indeed terrible for low latency guarantees. Haskell won't give you the best lag-free experience for good graphics. If you're willing to tolerate a bit of lag and performance suboptimality, it can be nicer to code in than other alternatives.
Not supporting String is something easily changed, given a good enough reason. I think GHC has one as good as it gets, so a kept-close-to-the-original fork is worth thinking about, should the current printer not support certain needed features. Switching GHC itself should not be that hard, but given GHC's build times it might consume an afternoon of boring monkey coding. 
You may be able to get more consistent GC times by calling [`System.Mem.performGC`](https://www.stackage.org/haddock/lts-8.16/base-4.9.1.0/System-Mem.html#v:performGC) at regular intervals. For instance, you could call it every frame or every second. I've never done this myself and I can't determine if it's even a good idea, so I'd love to hear from someone who has tried this.
Because of how the types work out, theres a high chance that your graphics data is already in a Vector of some flavor, since that makes it easy for pushing to the GPU with minimal overhead (unsafeWith). Either a Vector backing an image file (eg: as produced by JuicyPixels), or just a vector of vertexes, or whatever you like. Moving data from CPU space to GPU space takes exactly as long in any language (since the graphics API does that). If it's in an IOVector then your updates to this data are destructive updates, naturally, just as it would be with an array of values in C++, so you'll also stay fast there. You can try to keep your per frame garbage low so that you never have to GC mid-frame, use performMinorGC like they say after you draw and before you flip buffers (when you would otherwise sleep anyway), and you should at least land in the C# and Java range for speeds. That's plenty enough for most games. EDIT: another note is that the size of your retained set is the biggest factor in gc times. If none of the new data in a frame is kept into the next frame that entire gc generation can be blasted away instantly.
At this point, the equivilant of ghci is running on Android in the slave process. I wonder how close it is, then, to running command-line ghc on Android. The termux package manager app for Android has some open PRs to add programs like pandoc and git-annex that are blocked on a ghc port.
For CI purposes, it would be handy if there was a android emulator preloaded with the ghc slave, and perhaps preconfigured to start it on boot. I don't know how slow android emulators tend to be (the one I tried was slow but this was some time ago), but the ghc slave is probably not using a great deal of CPU.
I like providing the output on `dot` in the READMEs of my libs.
i do that too, but most don't (it's the libraries I haven't written, whose transitive dependencies I want to immediately know). and it's either manual or yet another thing in your build.
&gt; Graphics programmers never seem to recommend using things like haskell. [John Carmack](https://twitter.com/ID_AA_Carmack) never made a AA game with it, but did [recommend it](http://functionaltalks.org/2013/08/26/john-carmack-thoughts-on-haskell/).
The `$` operator has the lowest precedence meaning all your other operators get their arguments first. Your code is equivalent to `factorialFun x = (x * factorialFun) (x - 1)`
&gt; And what happens when two tables have a trip_id column? You use whatever method is appropriate to disambiguate them. Usually this ends up being exactly what you said, a prefix with the table name: `fooTripId`, `barTripId`, etc. The question about whether to use classy lenses or not is a question of how polymorphic you want things to be. And no one answer is objectively superior. There are times when you want to have a monomorphic function. But there are also times when you might want to have a polymorphic one. The good news is that you don't have to choose just one. You can make data types with monomorphic field accessors that have fully disambiguated names, and then you can supply type class instances giving you polymorphic ones where appropriate.
The slave process is really only wrapping `iserv`. I hope we will eventually have `aarch64-linux-android-ghci`, but that one would run on the build machine, and just communicate with `iserv` on the host. So not exactly what you envision. However, why are `pandoc` and `git-annex` blocked by a ghc port? Aren't `pandoc` and `git-annex` binaries? Should it be possible to simply cross compile them binaries and provide the binaries to termux?
Does this still happen even using ST?
May be mistaken, but I thought lambdacube went from a Haskell EDSL to its own DSL?
I know this algorithm but didn't realized the amortization fact. Thanks for this interesting answer and such a nice write up!
Wow, lots of resources, thanks! Just curious: have you tried any of these mentioned algorithm?
The link is broken. Found it here: https://purelyfunctional.org/posts/2017-05-30-haskell-binding-c%2B%2B-template.html
It's probably also worth mentioning that large static structures that always survive GC can blow up the GC latency. Compact Regions in GHC 8.2 will help there.
I mean because Hoogle and Stackage (also a Hoogle) I normally search all directly there. When I know the name of a function, I search it. When I don't know the name, I search for the type that I expect. If that fails, maybe stack overflow has the answer :) 
No need to mention other editors. VSCode also provides this. Try with Typescript and JS. And it is available for authoring. Felix Becker's PHP IntelliSense uses it.
Seems to work fine for me, probably the `+`s are not being correctly escaped in your browser for some reason.
Funnily enough, I wrote this up in a dependently-typed manner a couple weekends back. You can implement the permutations themselves (i.e., independent of what they're permuting) and get the successor function easily enough (just walk down the "list" until you find room to put the successor into). Then you just need to apply each of the permutations to the original list, if you actually care about that. ...Alas, I've yet to get a type-safe implementation of application in *O(n)*. I have one in *O(n^2)*. And I have an idea that, if it pans out, will reduce that to *Θ(n\*log n)*. But still no ideas that'll get it down to *O(n)*. Of course, if you don't care about proving correctness in the types, you can always take shortcuts. If you can lookup the *i*th element of a sequence and return a sequence sans that element, all in *O(1)* time, that's all you need to get the whole thing down to *O(n)*. Should be doable with an array, a bitmask of the elements you've "removed", and some clever bit bashing.
Hm actually it looks like the url stored by reddit (when fetching the webpage via curl to make sure the browser is not doing anything weird) already escapes the `+` characters so I’m not sure whats causing problems for you.
Great write up!
There was a an indie 2D platformer game called "Nikki and the Robots" that came out a few years ago; the base "engine" and level editor is free (and I did buy the extra story content which was great). It's still available on github: [https://github.com/nikki-and-the-robots/nikki](https://github.com/nikki-and-the-robots/nikki). The game didn't do very well for commercial reasons, but it ran smoothly - no noticeable GC pauses. 
How do you figure `O((n!)^2)`? Seems to me that both of those algorithms do at most `n` extra operations per permutation, so it's `O(n(n!))`. Since the requirement here is using the permutations sequentially after they are generated, that is the best possible complexity.
Haskell is a bit cumbersome to works with for graphics, mainly because of the notation for vectors/arrays. For performance reason you want to store everything in a Storable vector (for C/GL/whatever interop) or at least in an unboxed vector, storing a Word8 in a sum type still store a full Word in memory. While optimizing Rasterific, the most notable gain was reducing the GC presure, while good, if you can avoid to trigger it needlessly is always a big win. Achieving that boils down too is writing a pseudo C in Haskell inside the ST monad (see [an example](https://github.com/Twinside/Rasterific/blob/3295996f2ced6ae702afc2e28e263ce3ed7fcab5/src/Graphics/Rasterific/Shading.hs#L100)) . At this point using Haskell is not really a win, and you don't get optimizations already tuned for this kind of workload by C/C++ compilers (I've not tested with LLVM backend though). But gladly, once the low level plumbing is done correctly, you can get okaish performances with a nice high level API to consume.
I have to piggyback on this question, since the same applies to Emacs/Spacemacs with Intero. It's possible to get type signatures, but no text. Is it possible to get function/argument info?
This is clearly really annoying to write using something like parsec. Here is a try, which satisfies your test cases: data Pok = Pok { _name :: String , _nick :: Maybe String , _gender :: Maybe Gender , _item :: Maybe String } deriving (Show, Eq) data Gender = M | F deriving (Show, Eq) manywords :: Parser String manywords = unwords &lt;$&gt; (some letterChar `sepEndBy1` some spaceChar) parens :: Parser a -&gt; Parser a parens = between (char '(') (char ')' &lt;* space) pok :: Parser Pok pok = do nickOrName &lt;- manywords realName &lt;- optional $ try $ parens $ do r &lt;- manywords r &lt;$ guard (r `notElem` ["M","F"]) let (name, nick) = case realName of Just n -&gt; (n, Just nickOrName) Nothing -&gt; (nickOrName, Nothing) gender &lt;- optional $ parens $ anyChar &gt;&gt;= \c -&gt; case c of 'M' -&gt; return M 'F' -&gt; return F _ -&gt; mzero item &lt;- optional (string "@ " *&gt; some anyChar) return (Pok name nick gender item) 
For one, are you sure that the slaves are still running when you start the master ?
tl;dr version: yes, when they're unboxed. Very nice article. My takeaway is that we need a nice pretty-printer-with-renaming for Core :)
I've had GC-related framerate drops with much smaller examples though, so I suspect that the technique is more important than the amount of data.
As sure as I can be. I'm just following the instructions. First I run these ./example slave localhost 8080 &amp; ./example slave localhost 8081 &amp; ./example slave localhost 8082 &amp; ./example slave localhost 8083 &amp; and then in another terminal ./example master localhost 8084 and I just get an empty array back
My understanding is that GC pauses occur when the amount of allocated memory reaches a certain limit, and that due to immutability, Haskell code tends to allocate a lot of garbage. Rewriting the part which allocates the most memory in C, with statically-allocated structures, might reduce the rate at which garbage is allocated, but as long as there is still one piece of code which allocates one byte of garbage every frame, you'll still see GC pauses, just a lot less often. I know that GHC is sometimes able to optimize code in a way which doesn't allocate any garbage at all. So instead of rewriting more and more of your codebase in C until there's basically no Haskell left, a better approach would be to write Haskell code in a style which we know can be optimized in this way. Ideally there would be a way to make it a compile-time error if a particular optimization we really need doesn't fire.
You might check if the processes are still running with `top` before firing the master node. For a quick-check, you may also start a slave in the foreground by removing the `&amp;` character and see if it returns immediately. I never used Cloud Haskell myself but looking at the code, it seems that node discovery relies on network broadcasts. Do you have strong firewall rules in place ?
The slave nodes don't return immediately. How can I ensure my firewall is configured correctly? I tried this at random: https://gist.github.com/juliojsb/00e3bb086fd4e0472dbe But I do not know how to read iptables
What does a iptables -L -n return ?
https://paste.fedoraproject.org/paste/BZwYFYR2zWlLRJ6pFlBX0V5M1UNdIGYhyRLivL9gydE=
[removed]
I don't see anything particularly suspicious. Just in case, I'd still try to disable the following rules: DROP all -- 0.0.0.0/0 0.0.0.0/0 ctstate INVALID I'm out of ideas...
How do I do that exactly?
Did you make this example work for you or did you not try it?
Yes, it works for me. I used stack with a fairly old (6.22) LTS version. Unfortunately, I don't have any idea left and can't help you much longer, but let me know if you find out.
Go for it! It's not like we're swamped with posts here. Rust does it every week as well and I always enjoy the summaries. Perhaps it would be possible to include slightly more information somehow besides some posts and a few packages? The packages part is great but if you read the subreddit constantly, 90% of the posts won't be new to you, and so the weekly Haskell report will be just a super quick skim. (I'm fully aware that someone who reads the Haskell subreddit constantly is not in the normal demographic for the weekly subscription)
Given: data Foo = Bar | Baz Is there a Haskell construct that allows you to remove Baz from the sum type? I'd call it subtraction for lack of a better terminoligy.
This ties back nicely to ['Lazy Functional State Threads'](https://pdfs.semanticscholar.org/bcb8/0827317a37d7896d4c22e10a613548c4b440.pdf) which recommends implementing references as an array of one element.
Did you implement the fix for the gcc issue with newer versions of Windows? If it's out of date it can cause all kinds of issues and actually resulted in me jumping ship on Windows.
Why exactly is the build failing? I have a separate msys2 installation and I'm using cmder (ConEmu) as my terminal with zsh as my shell, installed via pacman. I don't use the msys2 that ships with stack but I never explicitly messed with the path it prepends to everything when it's building. Never really had a problem. Edit: Actually, looking around I've found config.yaml in my stack root folder. Apparently I did this at some point: skip-msys: true extra-include-dirs: - C:/msys64/mingw64/include extra-lib-dirs: - C:/msys64/mingw64/lib - C:/msys64/mingw64/bin 
Since stack installs MSYS2, you can just use pacman to install/update to whatever version of compilers/libs. &gt; stack exec pacman ...
&gt; some immensely clever way to fix it up with fancy cryptography tricks, but I don't see it. how do you decrypt the information without also having the key in memory such that a hacker could decrypt all the encrypted information If anything like this were possible, game devs would've done it by now because hacking and the limitations it imposes are a huge cost center for them. It ruins multiplayer games. A lot.
Since nobody's mentioned it yet, Haskell's been used for many experiments in functional reactive programming (FRP). This is quite a departure from the mainstream imperative-stepped-integration style of game/graphics programming though. In [his interview on the Haskell Cast](http://www.haskellcast.com/episode/009-conal-elliott-on-frp-and-denotational-design), Conal Elliot describes how his [FRAN](http://conal.net/fran) system (Functional Reactive ANimation) ran very quickly using HUGS (a Haskell *interpreter*) on Windows 98. The key is that FRP (in its original form; the term's since been appropriated to mean "event handling") encodes behaviour as a function of time. This makes it easier for complicated behaviours to be composed out of smaller units. A given example was noticing that some particular object's behaviour was expensive to calculate, so rather than sampling it on every frame (like everything else in the scene), it was only sampled 10 times per second and wrapped with a linear interpolation behaviour, with the results being indistinguishable from doing the exact calculation every time. Conal's original implementations unfortunately have some pretty bad space leaks. Newer work tries to overcome these (e.g. [FRP Now!](https://hackage.haskell.org/package/frpnow))
Definitely all valid points! I have noticed the interesting disparity between activity of the two groups. I think a lot of it seems from Haskell being "weird" and rust being "game changing and exciting" for an incredibly huge market (systems and "speed critical" development). I wonder if Haskell's dedication to preserving huge amounts of backwards compatibility (along with a fairly mature and old language) is the reason why there aren't a lot of RFCs and proposals? A lack of general funding probably doesn't help either. And, to be honest, the language itself doesn't seem to be evolving at all; the compiler is, for sure, but the fact that everything new has to be a pragma feels odd to me compared to the fairly rapid development of Rust-lang 'proper' Although pretty much all of that is entirely beside the point... I do like the weekly reports just fine as they are, though!
Using `Functor` is generally better in the sense that it is more general and should never perform worse than an equivalent `Applicative` solution. Now of course doing what you want is impossible with `Functor` alone, but there is no downside to mixing the two, so when possible IMO you should use `Functor`. Note that if you are comparing their answer to some of the others, that if you just plug `() &lt;$ char '?'` into your example above, then you are using `Monad` (unless you use `ApplicativeDo`), which is generally "worse" than applicative in the sense that it works on less types and is often slower.
Thanks this seems to work. I've also confirmed that the CloudHaskell example works on other machines on the same network, so it's something to do with my laptop. Other than that I have nothing to go on.
Obligatory: Emacs can visit URLs (documentation, searches, etc.) internally, using e.g. EWW or emacs-w3m.
also, stack will get confused by -params passed to exec'ed apps double dashes are [pretty standard] way to signal that whatever follows, it's not flags/params for stack &gt; stack exec -- &lt;command&gt; ...
Could you tell me a bit about the thought process that you engage in when designing novel types? Where do you start? How do you know that the types you create make sense? What do you do when you're stuck? It's something that I'm thinking of pursuing for a graduate degree, so I'd be interested in hearing your thoughts as someone who is working on these kind of problems. 
It's funny that a bunch of people (including me) all of a sudden need to learn how to use cloud haskell (: To get going and avoid getting too deep into cloud haskell specifics, I used one node, with multiple processes. I am going to focus on the protocol and messages first. Then, I plan on converting each process to use its own node later on.
There is [this](https://github.com/yav/dump-core) pretty-printer which is nice. It produces outputs like [this](http://yav.github.io/dump-core/example-output/Galua.OpcodeInterpreter.html)
Thanks man, now that final piece of puzzle (zlib) is in place, i can finally build gi-gtk on windows. I've filed the bugreport, btw: https://github.com/commercialhaskell/stack/issues/3154 
From my limited exposure so far... The types of features that GHC might consider might be more complex on average, such as extensions moving towards Dependent Types, so you won't have as wide an audience that can readily understand, vote, and contribute. That might be facilitated by having more diagrams, examples, and write-ups for a given proposal, to make them more accessible and have Haskell beginners able to understand the relative pros/cons of a proposal. (I am guessing someone has already thought of this, and may have started this effort).
What's unsafe about what appears in the article?
GC lags exist, but barely noticeable in practice. Unless you are developing something like an AAA game or hard-realtime systems, it shouldn't be a serious issue. Normal 3D graphics is perfectly OK. Of course it also helps if you structure your program so that it doesn't do too much allocations.
You really probably want to run a custom msys environment and tell stack to skip the built-in one. 
I'm afraid you will need to give us a bit more if you want a useful answer. Modular code is code that is structured in a modular fashion, that is, it is organized into modules, such that unnecessary dependencies between modules are avoided. This isn't specific to Haskell though.
A nice comment on lobste.rs https://lobste.rs/s/7pvnuo/are_mutable_references_haskell_fast#c_o2coio
Luckily in this case such an instance would not be law abiding, so we don't accidentally get lawful but intuitive instances from `Comonad`. See [here](https://www.reddit.com/r/haskell/comments/6eck5p/are_applicativemonad_instances_possible_for_this/dic5fid/). Although since this law relies on `pure`, I think you might be right about such a dirty `Comonad` based implementation working for `SemiApplicative`.
 data Bar data Baz data BarBaz class HasBar a instance HasBar Bar instance HasBar BarBaz class HasBaz a instance hasBaz Baz instance HasBaz BarBaz data Foo a where Bar :: HasBar a =&gt; Foo a Baz :: HasBaz a =&gt; Foo a Then you get: Foo BarBaz - Baz = Foo Bar Here is an actual type checking implementation of the subtraction: removeBaz :: Foo BarBaz -&gt; Maybe (Foo Bar) removeBaz Bar = Just Bar removeBaz Baz = Nothing But I will admit the boilerplate isn't very fun. I would love it if someone gave me a less boilerplate heavy version (without TH or anything like that). I think the fundamental issue is that you can't really have constraints on values, only on types, so we can't do the usual thing of using `-XDataKinds` for phantom types, and that is generally how you get more concise (and more correct, because of `data Foo (a :: FooX)`) code.
Nothing I can see. But keep pushing performance and you'll get there soon. When you do, maybe give C a call. Another thing to try is parallelism. Though performance wins are surprisingly hard to achieve in real applications, this counting test is embarrassingly parallel. Should be able to get close to the ideal Nx speedup out of N cores. In something more real-world, like the parsing example linked above by /u/chrisdonner, I've been able to get 2-3x out of N cores -- and only with a not of effort and compromise, which is rarely worth it and definitely not in my case, but I was having fun.
I was just about to report my comment here too... &gt; It’s worth keeping in mind that an IORef a not a mutable a, it a mutable reference to an immutable object of type a - a possibly fairer comparison would be to use volatile int * ref which dereferences the value increments it, allocates a new location for this incremented int, writes the value to this new location, then writes to the pointer pointing to that new location. Also the most appropriate comparison would be not using mutation at all, and using sum [1..10000], which should actually produce code much closer to what gcc would in the for loop, and if passed to LLVM would actually just compute the constant. &gt; Using IORef actually prevents the compiler doing many many optimisations, because it has to maintain the correct semantics, including the ability to implement atomicModifyIORef, an incredibly useful function which allows any immutable data structure to be modified concurrently without contention. After trying many alternative implementations for a concurrent linked list, it was found by far the most efficient was standard Haskell lists wrapped in an IORef: https://pdfs.semanticscholar.org/2f9e/cc815906c4359cb02674123a1c3b06ec735b.pdf
You're looking at a list comprehension. The syntax is roughly: [return-expression | elem &lt;- collection, predicate] which corresponds to: Gather all values "return-expression" for every "elem" of the list "collection" where "predicate" is true. In the case of [not a | a&lt;-[x,not x], a] This reads as: Gather all values "(not a)" for every "a" of the list "[x, not x]" where "a" is true. An interesting thing to note is that if you pass False or True as "x" to the function "f", you get the same "collection" inside the list comprehension (modulo order of elements). That is, for the list [x, not x] If x == True: [x, not x] == [True, False] but if x == False: [x, not x] == [False, True] One thing you may want to play with is having multiple collections inside your list comprehensions, just to get a feel for them. For example, what is going to be the output of the following? [(x, y) | x &lt;- [1..3], y &lt;- [1..3]] -- and [(x, y) | y &lt;- [1..3], x &lt;- [1..3]] As always, you can try some of these plain-old-data/plain-old-Haskell expressions out in [tryhaskell.org](https://tryhaskell.org)
Are there other parsing approaches in Haskell that are more suited to this kind of thing? It looks like Parsec is giving me more trouble than it's worth for this case, and I've not yet found a Haskell regex library that supports "lazy" (non-greedy) matching (with `?`).
Making the test more explicit makes it a little easier to read: f x = [not a | a &lt;- [x, not x], a == True] 
While not quite the same idea, Curry allows functions to be nondeterministic, which makes them reversible in principle. It's a logic-programming version of Haskell.
For lifted types, mutable reference use `MutVar#` underhood which have quite high double indirect overhead: data Foo = Foo {-# UNPACK #-} (IORef Int) +-----------+ +-------------+ +---------+ | Foo | * +---&gt;+ MutVar# | * +---&gt;+ I# | i# | +-----------+ +-------------+ +---------+ I wrote a [package](https://hackage.haskell.org/package/unboxed-ref) to improve this situation a little by use `MutableByteArray#` data Bar = Bar {-# UNPACK #-} (IORefU Int) +-----------+ +------------------------+ | Bar | * +---&gt;+ MutableByteArray# | i# | +-----------+ +------------------------+ The downside is it can only hold `Prim` values, and each `MutableByteArray#` carry a length field which is useless for this purpose.
I'm not sure that question really makes sense. It's like asking "Are arrays the right way to do mutable data in Java?" Like, sure, it's *a* way. But refs are fine ways to make the fields of something mutable: data Foo = Foo (IORef Int) Ultimately, it comes down to what you're trying to do. There's a mutable hashmaps library for good mutable maps, `vector` gives you great mutable arrays, `mutable-containers` gives you more specialized ref types (OP really should have focused more on this; it would have solved his problem much more idiomatically), MVars give you really good thread safety. There's no "the recommended way" to do something as ridiculously broad as "mutability."
These are different sites: the usual Zariski topology is too bad for almost anything, so Grothendieck generalised what "topology" meant. A site is a category with some notion that replaces "covering of open subsets", for instance the small étale site ("et" in your link) is given by choosing a scheme X, considering all schemes Y with a fixed map to X that is étale, and then saying that a covering family is a collection of maps (Y_i --&gt; X) such that they together are surjective (i.e., every point of X has something mapping to it). The usual Zariski topology can also be considered as a site: the maps are "open immersions" instead of "étale", and as every open immersion _is_ an étale map, the étale topology is _finer_. The diagram is just a list of several common and uncommon sites of a scheme and how they relate to each other
regex-applicative
Great, I am really looking forward to all progress regarding lambdacube! I am totally out of my depth here, but in dream world using something like lambdacube for graphics programming seems like good choice. Especially if I can also do other "not strictly graphics" computations on GPU, while having Haskell as backend on CPU. Sprinkling some async architecture to keep both CPU and GPU as busy as possible. 
Good for you, the Object Oriented programmers. That demonstrates that Haskell can be the best OOP language for managing gigantic and badly designed states in programs that mix the worst of FP and OOP programming styles. But I`m a functional programmer who don't need that.
That's what `MVar` is for from base.
Thank you, this is great insight :)
"Seems like there's something new every time I turn my head" Yep! That's why I fell in love with this language in the first place
~~How about "time"? You can get pretty funky if you go down this road. IIRC /u/bitemyapp did some funny stuff with it a while back as an experiment which allowed you to do something like `3 days + 1 hour`.~~ Edit: /u/bitemyapp is right. Use monoids as /u/m0rphism points out. It has all that you're looking for!
Thanks. I was looking to flush out my LambdaConf talk last year on algebraic refactoring into an working Haskell library. https://www.youtube.com/watch?v=Mhw4FD0pCU8 Essentially curry/uncurry plus all the other algebraic identities we learned in Jr. High Algebra. The nasty bits are sum type "subtraction" and product type "division". Once I get it working I was going to write a search routine where you give it two type signatures and it tries to find a path of algebraic refactorings. 
FWIW, I dug up [this commit](https://github.com/sgraf812/feed-gipeda/commit/bd904287fa1a92cb8fe5c2516fd65a416276ef38), where past me discovered that Linux would bind `localhost` as IPv6 only. But since that doesn't seem to be an issue, you could give [`distributed-process-p2p`](https://hackage.haskell.org/package/distributed-process-p2p-0.1.3.2) a shot. I did that when gave up on multicast, as Travis (and therefore the Ubuntu LTS container) didn't support it.
I think both the negative and the fractional case can be understood as obligations. However in the fractional case, the obligation is to provide "annotation/padding" bits *q*, in order to convert *p*/*q* to *p*. Whereas the negative case is asking for some fresh 'states' not belonging to the type, in order to convert *p*-*q* to *p*. In either case, there are analogies with function calls, but with subtle differences - the *p*-*q* case behaves more like coroutine processing while the *p*/*q* case is consistent with a 'flow-based' model.
Didn't know about `Reverse`, seems to be `Backwards` but only for traversable and foldable? Makes the intentions of the code really clear which seems awesome. Speaking of making intentions clear - thoughts on using data LZipper a = LZipper (Reverse [] a) a [a] zipWith ($) bf bx vs. data LZipper a = LZipper (Reverse ZipList a) a (ZipList a) bf &lt;*&gt; bx ? Technically clearer but at some point the wrapping and unwrapping really gets in the way of pattern matches and lenses/pattern synonyms seem like overkill.
Yeah, `Reverse` is analogous to `Backwards` but for sequences rather than effects, it just makes the traversal order go from right to left rather than left to right. (`Reverse`'s `Traversable` instance basically just traverses in the `Backwards` applicative.) Didn't even think of using `ZipList`. I usually don't embed newtypes into data structures. It's only really useful when you want to influence the code generated by `deriving`, as I did here with `Foldable` and `Traversable`. (For another example of the same thing see [this stack overflow answer of mine](https://stackoverflow.com/a/41522634/7951906).) TBH if I was writing that code again today I'd probably roll my own snoc-list (with an infix constructor called something like like `:&lt;`) for the reversed lists, rather than using `Reverse`. It's nice when the code looks geometrically like the data it's modelling, saves you having to reverse things in your head when you're trying to reason about things.
It's an extremely bad idea as far as something you'd actually want to do but it motivates the instance-local functional dependency trick for getting type inference to work again. I have been _dying_ for an excuse to teach instance-local-fundeps just so I could show this off. Also I like taking Ruby DSLs and seeing how close I can get to their syntax in Haskell. It's a fun design constraint.
Roughly speaking, in modular code the references between different parts (e.g. the places where constant and function names are used) look something like [this](https://math.aalto.fi/~lleskela/SDLRG/GeometricGraphClustered.jpg). In code which isn't modular, the references will look something like [this](http://www.cs.cmu.edu/~christos/TALKS/09-KDD-tutorial/tut_files/friendship_network.jpg). "Modules" are the blobs at the top, centre, bottom-left and bottom-right of that first image: things inside the module (e.g. function definitions) may have many references to other things in the same module, but there are few references which go outside the module. This may be due to design/effort, or it may be enforced by the language (e.g. in Haskell this can be achieved with export lists, `exposed-modules` in Cabal, etc.). A module's *interface* is the set of parts (e.g. functions) which are *designed* to be referenced by things outside the module. For example, we might have an `insertionSort` function and an `insert` function which it uses. If we don't specify or enforce that this is a module, anyone might download our code and use those `insertionSort` and `insert` functions. Later, we might decide to use `mergeSort` instead, and write a `merge` function for it to use. We can add these functions to our code, but we won't get any benefit until we change all of our references to `insertionSort` to instead use `mergeSort`. Once we've made that all of those changes, we get a nice speed improvement due to the faster algorithm. Since we're not using `insertionSort` any more, we might decide to remove it and `insert` from our code. However, this update would break the code for everyone who's downloaded it as a dependency. Since our code's not modular, there may be many references pointing at any part of it, that we'll never even know about since they're on someone else's machine. Just like the second image. A more modular approach would be to write our `insertionSort` and `insert` functions as a module. Since we only want to use the `insertionSort` function we can just provide that as our interface. Even better, since we don't care *how* the sort is performed, we can just provide a `sort` function as our interface, which calls the `insertionSort` function. This is like one of those nodes in the first image which connects two of the "blobs". Later, we can add `merge` and `mergeSort` to our module like before. Since our interface only has the generically-named `sort` function, we can change that to call `mergeSort` instead or `insertionSort`, and all of our code, and that of anyone else, will automatically get a speed boost without having to be changed. Since the `insertionSort` and `insert` functions were not exposed in our interface, we can throw them away now that they're not needed by `sort`, and we don't have to maintain them any more.
I played a bit with Control.Lens.Cons so the newtype instances are composed without getting in the way while pattern matching. Which honestly doesn't add much except being cutesy, I probably would just use a normal lists for both ends when the code would need to be readable. Anyway, outside of some horrendous orphan instances I had to change surprisingly little. The signficant changes were: fwd, bwd :: LZipper a -&gt; Maybe (LZipper a) fwd (LZipper ls m (r :&lt; rs)) = Just $ LZipper (ls |&gt; m) r rs fwd _ = Nothing bwd (LZipper (ls :&gt; l) m rs) = Just $ LZipper ls l (m &lt;| rs) bwd _ = Nothing up, down, left, right :: Grid a -&gt; Maybe (Grid a) up = composed $ bwd down = composed $ fwd left = composed . traversed $ bwd right = composed . traversed $ fwd -- made the mkZipper constructor take NonEmpty because I already had polymorphic unconsing mkGrid :: NonEmpty (NonEmpty a) -&gt; Grid a mkGrid = Compose . mkZipper . fmap mkZipper instance Applicative LZipper where pure x = zipper (repeat x) x (repeat x) (LZipper bf f ff) &lt;*&gt; (LZipper bx x fx) = LZipper (bf &lt;*&gt; bx) (f x) (ff &lt;*&gt; fx) coords :: Grid (Integer, Integer) coords = Compose $ xAxis &lt;$&gt; yAxis where xAxis = traverse (,) ints yAxis = ints But yeah, pretty sure `instance (Cons (f a) (f b) a b) =&gt; Snoc (Reverse f a) (Reverse f b) a b` is a horrible idea. Plus it'd break for snoc -&gt; cons.
I've been using this for a decade ... https://hackage.haskell.org/package/ghc-core 
When a is true: for a list of two elements, x, and not x give me all a and then invert them to false. This is the worst possible example function. It's probably hard to understand because it's an extremely stupid example that does nothing. Whoever wrote this as an example should be ashamed.
on on openSUSE Tumbleweed we have now almost complete LTS8 :) (last 50+- packages waits for review) + as Peter says in presentation all packages checked by lawyers
I'd remove operators and precedence from Haskell if it were my choice. Type classes are hot stuff from a PL perspective but the other stuff I'd find hard to make a talk about it except about what's bad about it.
My understanding is that the `cabal` library (and thus the `cabal-install` and `stack` programs) will only *build* dependencies -- not install them. To have a piece of software -- an executable -- bundled as a `cabal` package be available, we need to `install` the package. Since this won't happen automatically if we build a package that depends on it, we need to explicitly install it. What I'm basically saying is that you should run `stack install alex happy` before trying to build the `language-java` package, and `stack` will try to build `language-java` while trying to build your package. Even though `alex` appears in the `.cabal` file for `language-java`, it only appears as a library dependency -- the program needed by your build will not be installed automatically. From my experience it will also complain about `happy`, so best to install that too.
first of all let me thank you for your explanation and useful reply. I liked the trick of using a function like `sort` and expose it while this function will call another function that might be changed and it will solve the problem of dependencies. However, let say I have a module `DBInterface` and this module expose `10` functions, wouldn't it become tedious to create 10 function - as aliases - to keep my code modular, it turns out that in order to keep my code modular I have to provide another `10` functions as interface for `DBInterface`.
you are right, I just wanted to know how to write modular code so I make sure that I am using well structuring my code.
Maybe look into something like https://hackage.haskell.org/package/vinyl ? I think it has a competitor but I can't remember its name.
I think the issues of modularity and programming to interfaces are slightly different: code can be modular while still exposing/defining *some* of its internals as a public interface, rather than defining a separate set of functions just for the interface. The general advice I've seen is to only define a separate interface when you already have more than 1 implementation, so in your case if you only have e.g. `PostgresModule`, then it may be fine to expose a few of its functions as an interface and code to that. If you *already* have e.g. `PostgresModule`, `MariaDBModule`, `TestMemDBModule` etc. then it would make sense to specify a generic interface. Note that when I say "interface", I don't *necessarily* mean some particular language feature (like `interface` in Java, or `class` in Haskell); I mean the recommended way of accessing/using some piece of code (which could merely be "what's recommended by the documention").
To be precise, BuckleScript is the OCaml-JS compiler, Reason is the JavaScript-friendly OCaml syntax, and react-reason is the bindings to the ReactJS library written in Reason syntax and compiled by BuckleScript.
What do you base your intuition on? I don't disagree, but we've gone through dozens of ideas, each one of them rather plausible sounding, and most turned out to be dead ends. Papers, for example? The analogies with function calls is 'clear', in the sense that if the types can be coerced into a traced monoidal category, then one does get functions to come out of that. 
Looks like a bug to me. You should file it. You're right, type checking shouldn't loop unless there's an 'Undecidable' extension. Looks like it's looping when trying to solve as ~ Product (Divide a as) as a result of the definition of 'divide' in the first instance divide (a :* as) = (a, as) GHC 7.10.3 correctly determines that something's wrong Could not deduce (as1 ~ (a1 : Divide a1 as1))
I will file it. Thanks! I can see how that might infinite loop if not implemented carefully! And yeah I realized my mistake, I forgot that divide returned a tuple.
Please file it, but then set a milestone of 8.2.1 and close it as fixed.
If you pattern match on `Mem f` and `Mem g` on function parameters, it becomes a bit cleaner instance Monoid a =&gt; Monoid (Mem s a) where mempty = Mem $ \s -&gt; (mempty, s) mappend (Mem f) (Mem g) = Mem $ \s -&gt; case g s of (a1, s2) -&gt; case f s2 of (a2, s3) -&gt; (a1 &lt;&gt; a2, s3) There are many ways to make it shorter but I think this is the one is the most clear. Edit: Wow! I just found that tuples have a monad instance on GHC8 like this: # GHC/Base.hs:344 instance Monoid a =&gt; Monad ((,) a) where (u, a) &gt;&gt;= k = case k a of (v, b) -&gt; (u `mappend` v, b) So, below code works: import Control.Monad ((&gt;=&gt;)) instance Monoid a =&gt; Monoid (Mem s a) where mempty = Mem return mappend (Mem f) (Mem g) = Mem (f &gt;=&gt; g)
I would probably write it like this: mappend (Mem f) (Mem g) = Mem $ \ s -&gt; let (a, s') = g s (a', s'') = f s' in (a &lt;&gt; a', s'') Or like this: mappend f g = Mem $ \ s -&gt; let (a, s') = runMem g s (a', s'') = runMem f s' in (a &lt;&gt; a', s'') 
No problem! And you could make the `Elem` implicit like this: class IsElem a as bs | a as -&gt; bs where el :: Elem a as bs instance IsElem a (a : as) as where el = Here instance {-# OVERLAPPABLE #-} IsElem a as bs =&gt; IsElem a (b : as) (b : bs) where el = There el divide' :: IsElem a as bs =&gt; Product as -&gt; (a, Product bs) divide' = divide el Oh, but instances overlap if the `Product` has two elements of the same type. Hm...
In terms of *editing* the expanded version, there seem to be three options: 1) Build the edit as a patch atop the generated code. 2) Reflect changes back into the code that produced it, where possible. 3) Disallow it. I agree that 3 seems probably best in most settings. But in any case, there's nothing saying I shouldn't be able to edit the surrounding code with an in-situ view of the expanded code. And as I've mentioned elsewhere, it would be great if identifiers used (or created!) in generated code were visible to code search tools (grep or otherwise).
Thanks, I knew there was a better way! I haven't gotten to the monads section of the book yet, so that would probably explain the ugliness.
Don't worry about the monads, I honestly think they're unnecessary on this case. You can only look at the first solution, which is much clearer. It's basically the same as your solution, the only difference is that it uses pattern matches instead of `fst` and `snd` functions.
I'd love to see a writeup of GraphQL and Haskell. I've got so many questions: are the Haskell GraphQL libraries production ready? Since we have servant do we need GraphQL as much as other language ecosystems? When to pick one or the other? That kind of thing.
Thank you very much for the informations 
In such a case i often prefer this: data Query = ByDate _ | ByUser _ getArticles :: Query -&gt; _ 
Overlapping type families can cause soundness problems, so you can't have them. module A where type family F a module B1 where import A {-# OVERLAPPABLE #-} type instance F [a] = Bool module B2 where import B1 {-# OVERLAPPING #-} type instance F [Int] = Int module C where import B1 -- only the first instance is in scope foo :: F [Int] -- ~ Bool foo = False module D where import B2; import C -- both instances bar :: F [Int] -- ~ Int bar = foo + 4 -- oh no 
In this case I might create the module `Foo.Articles` and name the functions `byDate`, `byUser` etc, then `import qualified Foo.Articles as Articles` so that `getArticlesByDate d` becomes `Articles.byDate d`, `getArticlesByUser u` becomes `Articles.byUser u` etc. This could be combined with /u/Strake888 's suggestion for `Articles.by query`.
I mean that is identical to the situation with type classes. They are unsound in the presence of orphans. It's just that type classes deal with values so the unsoundness is value-level whereas type families deal with types so the unsoundness is type-level. Simple solution is disallow any overlapping or overlappable when dealing with orphans. 
awesome thanks
many thanks
I don't see how this improves anything, and it's not terribly extensible. Then again, I don't understand the OP's desire.
Here's how to do it type family F a where F Int = Char F a = a class C a instance C Int instance {-# OVERLAPPABLE #-} F a ~ a =&gt; C a Btw, here's how you can avoid overlap altogether type family F a :: Bool where F Int = 'True F a = 'False class F a ~ f =&gt; C' f a instance (a ~ Int, F a ~ 'True) =&gt; C' 'True a instance F a ~ 'False =&gt; C' 'False a class C' (F a) a =&gt; C a instance C' (F a) a =&gt; C a 
Actually, the upcoming release implements the protocol natively. It's close to finish and undergoing polishing and benchmarking now. It spawns what you refer to as "pipelining" as well. As for the performance, GC is the bottleneck. I'm struggling to work around it and benchmarking the library heavily.
How is the connection to the DB established: through TCP/IP or Socket? I'm asking because it can make up to an order of magnitude of the difference in performance. For reference on how to specify socket in the connection string see [the Postgres docs](https://www.postgresql.org/docs/9.3/static/libpq-connect.html#AEN39491).
It's defining instances like this instance (a ~ SomethingElseConcrete) =&gt; C SomethingConcrete a where It's similar to a fun dep because knowing the first parameter will grab this instance which will then determine the second parameter. The difference is that there are cases where you can't have a fun dep (usually due to a tricky amount of polymorphism). This works in those cases. For the above case with time it's probably because you want instances like instance Num (Hour -&gt; Time) instance Num (Minute -&gt; Time) but if you know that the first parameter of a function is `Hour` or `Minute` then the result MUST be time and there must be no other instances so you write instead instance a ~ Time =&gt; Num (Hour -&gt; a) instance a ~ Time =&gt; Num (Minute -&gt; a)
Awesome :) How does it measure up to the libpq version (as-is)?
~~This looks like a semi-direct product (http://hackage.haskell.org/package/monoid-extras-0.4.2/docs/Data-Monoid-SemiDirectProduct.html), but I haven't attempted to verify that.~~ Strike that, it's not a semi-direct product. Even if it's not, you might want to look at Brent Yorgey's `monoid-extras` package (the link above).
I would like to see that too! I'm aware of three GraphQL libraries in Haskell: [graphql](https://github.com/jdnavarro/graphql-haskell) by Danny Navarro, [graphql-api](https://github.com/jml/graphql-api) by Jonathan Lange, and [datagraph](https://github.com/dropbox/datagraph) by Chad Austin / Dropbox. I don't know if they are ready for production or not. Servant handles defining and serving REST APIs very well. GraphQL is different because the client decides which information is returned. As far as I can tell, Servant would have a hard time handling requests like that. 
Prelude is good. Maybe also look into intero and spacemacs. I've been using Haskell IDE mode with Atom and I've been liking that.
Nothing forbid you from programming in Haskell like Java or C#. That's ok. Some of us can feel sorry for the haskell future, but you are in your right to sink functional haskell in an ocean of OOP shit where the main contributions of Haskell are not the wonderful techniques and abstractions of 20 years of Haskell, but getters, setters and loops. (Oh my Lord, What a fucking shit, what a crazyness). Getters and setters are something that every newbie can understand. But doing it in Haskell, thanks to the CT mambo jambo that surround lenses, every dumb cooL kid feel like an overpowered super-category theory master. He is NOT. He was and still is an average OOP programmer with pretensions. Adding one hundred tons of Category Theory mambo jambo over it does not change that. CT is like boolean logic. It does not tell if something is good or bad. Everithing in computing is CT compliant like everithing in computing is boolean. The ideal solution would be that E. Kmett and all the rest of C# programmers like you create a new organization, cshaprhaskell or Haskell# perhaps?
Oh come on now, it's an actively developed, robust tool that is quite often the right fit for the job - definitely including Haskell development. What does spreading this crap accomplish for anyone? 
I'd give Visual Studio Code with Haskero/Intero a shot—I've moved from Haskell for Mac to (neo)vim to VSCode and I'm very happy!
It's not _impossible_ per se, but if your only problem is VimL you can already write vim plugins in Python, Ruby and Lua.
With *neovim* it is possible: https://github.com/neovimhaskell/nvim-hs
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [vim/vim/.../**version8.txt#L41** (master → 6e62da3)](https://github.com/vim/vim/blob/6e62da3e14d32f76f60d5cc8b267059923842f17/runtime/doc/version8.txt#L41) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply diesb61.)^.
Thank you! Of course it was tongue in cheek! Having said that, I've personally never been a fan of emacs, I remember first using it in the early days on a SUN SPARC and struggling to figure out how to actually exit. I'm well aware many people continue to love it but I've always been a fan of trying new things. I would note that if Emacs was in fact so perfect people wouldn't still be creating editors. Just for grins I downloaded the latest Emacs for OS X and ran it --- it still looks so old (just look at those icons on the top) and of course I had completely forgotten how people try to make Emacs do everything (like reading mail inside it). My own opinion is that it doesn't hold a candle to modern editors like SlickEdit or Sublime Text but I know there's a lot of religion involved :-) More seriously though, I would note here that the problem is not so much that Emacs is good for Haskell development but rather that there aren't any really good modern IDE dev. tools for Haskell and so you might as well just use a text editor. Where's the Haskell version of JetBrains or Visual Studio or even the old Delphi RAD? I was hoping that Leksah might be it but not yet. &gt; definitely including Haskell development 
I guess I'd never be drawn to Haskell if it didn't have the operators (not that this is the sole reason, but without the operators all the other goodies would be shadowed), I can't imagine using `Applicative` without `&lt;$&gt;` and `&lt;*&gt;`. The whole feeling of embedded DSLs everwhere would disappear instantly.
Both vim and emacs are ... different than what you may be used to. Emacs is a little more "native" on OSX than it is on other platforms, because the standard shortcuts you're used to, such as `⌘X` and `⌘V` for cut/paste all work out of the box, whereas you have to use very different shortcuts on unix or windows (emacs is older than the standard that created those shortcuts) That said, you might still prefer Atom or VSCode. Most of them are all using the same backend for haskell development (namely intero) so they should all be at feature parity more or less.
I'm not an expert but it seems like you should be using something like https://github.com/ondrap/json-stream instead of trying to parse JSON by hand with parsec.
If you took the time to understand what lens is about, you'd be ashamed of your attitude. Lens' purpose is not to do OOP in Haskell; it's so powerful in every direction that it just subsumes OOP, but it's also powerful in other directions that I won't bother educate you here.
would you mind sharing with us some screen shot for the look and feel ? 
I haven't made the switch (yet), but many people seem happier with emacs using a vim emulation layer, e.g. via spacemacs. Supposedly it's a much saner environment to write plugins and configuration in.
Here's another infinite loop in the compiler, it's a WONTFIX though :) newtype Y = Y { ($$) :: Y -&gt; Y } y = (\x -&gt; x $$ x) (Y $ \x -&gt; x $$ x) main = (y $$ (Y $ \r -&gt; r $$ r)) `seq` return () 
Instead of parseObj :: String -&gt; Vector3 parseObj line = do let splitLine = endBy " " line let head = splitLine !! 0 return (vectorFromList (map (read::String-&gt;Double) (tail splitLine))) I think you want parseObj = let splitLine = endBy "." line head = splitLine !! 0 in vectorFromList (map (read::String-&gt;Double) (tail splitLine)) I'll leave it to others to explain why.
Also, `head` appears to be unused (not to mention it shadows `Prelude.head`).
It's easier to give a helpful answer if you are more precise about what you are confused about, what you think an error message might mean, whether you've thought carefully about the error, whether there are certain words you don't understand, etc. In this case you have: parseObj :: String -&gt; Vector3 parseObj line = do ... return (vectorFromList ...) ...claiming that `parseObj` should return a `Vector3`, but we know based on how `do` notation works and the type of `vectorFromList` that `parseObj` must actually return some monadic action, of the shape: `Monad m=&gt; m Vector3`. Presumably you want something like: parseObj :: String -&gt; Vector3 parseObj line = let splitLine = endBy " " line head = splitLine !! 0 in (vectorFromList (map (read::String-&gt;Double) (tail splitLine)))
well, i think i know why: Couldn't find executable alex in directory .../.stack/snapshots/x86_64-osx/lts-7.19/ghcjs-0.2.1.9007019_ghc-8.0.1/bin/ i guess that's not what you expected? 
And that's the rub: if what you have is a full-fledged co-product (for example), then negatives collapse. See for example https://mathoverflow.net/questions/100158/negative-objects-in-categories. Some of the folklore is just *wrong* on that point. Similar problems arise for products. So you have to move to monoidal categories, i.e. the product/co-product cannot be 'internal', it *has* to be external. Thus our work on Rig Groupoids (ESOP 2016). The other problem, which I've mentioned before, is that these fixpoints don't want to co-exist. You can build either the multiplicative or additive easily enough, but not both at once. For this, you need to do "rig completion" (Google is your friend), which is mightily complicated.
I'm a beginner with Haskell trying to shove stuff together and make it work. Thanks for the tip, I'll have to read up some more on what `do` does.
[Here](https://gist.github.com/gelisam/848fd5794a045d57fb6e013fe6958be2), I wrote you a variant of SimpleLocalnet which doesn't use broadcast packets so you can continue following the tutorial.
https://ghc.haskell.org/trac/ghc/ticket/13784#ticket
Try [spacemacs](http://spacemacs.org/) if you want a more modern emacs without doing much configuration.
&gt; `map` Surely this is only a verb because it's the name of a function!? 
If you google the definition of map it comes up as both a noun and a verb. But if anything the fact that people turn function names into verbs probably indicates that verbs make good function names. 
Louis and Clark mapped the western US.
Estimates alone, probably way off. Python: 50% bugs C++: 30% bugs (5% why won't it compile) PHP: 60% bugs Hack: 40% bugs (the code base was mostly PHP, 5% why won't it compile) Haskell: 10% bugs (80% why won't it compile?) 
I feel like when I get a bug in Haskell it's usually much harder to track down than in other languages. Haskell eliminates most of the common bugs so the ones that are left are particularly nasty. This being said at the end of a couple bug fix cycles you are probably going to have a app close to perfect!
I have tried that a couple times, but it just didn't work for me. Being successful at doing so depends on how much of a seasoned vim user you are, if you're relatively new to vim, the switch will be easy, but if you relay on much of the more advanced and deepest vim features, it feels like a thin, fragile layer of emulation on the top that breaks apart way to easy
&gt; And that's the rub: if what you have is a full-fledged co-product (for example), then negatives collapse. I agree, *fully-general* products/coproducts in the sense you point out in your link are a non-starter. But I don't think you need anything like that in order to make the "fixpoint" constructs work in practical cases. &gt; The other problem, which I've mentioned before, is that these fixpoints don't want to co-exist. Yes, having a category with multiple monoidal structures brings extra difficulties, but these are well understood, to some extent. Here too, it's most likely a matter of figuring out what extra conditions are needed for the constructs to make sense.
Do you know about the Yi editor?
Write it more generally: {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Ap f a = Ap (f a) deriving (Functor, Applicative) instance (Applicative f, Monoid a) =&gt; Monoid (Ap f a) where mempty = pure mempty mappend = liftA2 mappend With that type Mem s = Ap (State s) looks pretty clean to me. There was discussion of adding this as Ap (or App?) to base on the libraries@ mailing list. I'm not sure of the current status of that proposal, but it doesn't look like it is present in base 4.10 at this time.
But the associativity of literally all the operators (including `*&gt;`) are the same: `infixl 4`. Although I can understand wanting the function being applied to be up front.
On the contrary, it seems to be designed to test understanding, and it does a good job of that. 
There is nothing imperative about indices - use a data structure that supports indices. The complexity is `O(n)` for each permutation, and so `O(n(n!))` for the entire enumeration. The neat thing about `Data.List.permutations` is that it achieves the same best time and memory complexity for abstract iterations, while at the same time achieving maximum laziness.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [danidiaz/vivav/.../**Main.hs** (master → 3c8edb2)](https://github.com/danidiaz/vivav/blob/3c8edb212a76e2b797829070ca205e9493626149/src/Main.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply diful0v.)^.
Good point on the query-spec-transforming vs query-spec thing. With your version getArticles could use specialized but still overwritable defaults, I think? I am not sure whether there is a universal solution for the extensibility problem. Spitballing for attempts: Cps style functions could look something like `&amp; sortWith ?~ comparing (view date)` which but that can be a lot less readable. Type classes might allow for a partially tagless style and the call site wouldn't change. That's a lot of effort if it is only called at one place, though. class Auth t repr where call :: t -&gt; repr -&gt; repr data NoAuth = NoAuth instance Auth NoAuth repr where call NoAuth req = req -- in application code: data MyAuth = MyAuth Text Text instance Auth MyAuth Request where call (MyAuth username password) request = error "too lazy" defs &amp; auth .~ MyAuth "foo" "hunter2" ? Doesn't add much over cps except that it can be polymorphic over different request representations for testing. It'd also be possible to split getArticles into smaller functions that are also exposed. getArticles only would be a convenient way to call them. Exposing that much internal stuff could be seriously painful for refactoring, though. Not at all sure what the best solution for this would be.
Seems like near 0. Our manager won't let us work on that, we need to work constantly on servicing feature requests from our customers. Of course, that wouldn't be possible unless the application is already basically working once it compiles. I guess that's one of the down sides.
I'd like to discuss further - but perhaps reddit is not the best spot? Could you email me [it should be obvious who I am!]
I'm not fond of the way this title is phrased. It sounds like a rigorous empirical evaluation is expected, but the way people reply is actually gut feelings. And this is to be expected, because almost nobody in fact tracks their time precisely enough to be able to answer that question. Combine that with the bias and incentives in the way the question is asked to make Haskell "look good" compared to other languages, and basically the discussion is designed to produce feel-good comments with the appearance of an evaluation.
Not exactly. In fact, it might even be needed sometimes to do exactly that. The point is that whenever you mention a GADT in a type signature, it needs an existentially quantified variable. Unless you really do want to limit it to just the constructors that match a single parameter type, but that would rarely be the case. For very simple type signatures you can sometimes get away with just a type variable in the GADT, quantified by the implicit `forall` at the top level. But quite often you can't write the type signature without explicit quantification.
What's goose?
For those interested, I've made it easier to install stutter, see [here](https://github.com/nmattia/stutter#installing): * Nix-based install * CI built executable I'm still working on the debian package (CC /u/christian-marie )
&gt; produce feel-good comments a contraire, good sir. Haskell has it's costs: less-mature tooling, paying your technical debt up front, etc. So the fact that _this_ question happens to be about a _strength_ doesn't excuse one for recognizing the other tradeoffs. Specifically, _my_ intent in asking it was to keep my "feel goods" in check, to keep my expectations aligned with reality. If haskell's good w.r.t. bugs, I want to know. If it's neutral, I want to know.
could you give us an example about the `query-spec-transforming` please ? 
It really depends on how fully you utilize haskell's type system. If you all your data types are strings, integers and booleans then haskell cannot help you much, and you'll be making most of the same mistakes as in other languages. If you put in your time to properly type your data, the number of mistakes cropping to production (bugs) drops significantly.
Thanks 
&gt; I mean, you don't always need to find an abstraction. Right, and I think not finding an abstraction is a reasonable default. My point was that a common class of bugs in Haskell is not abstracting when you should have, causing later changes to break things unexpectedly. It's not that you should *always* abstract, just that sometimes, not abstracting leads to bugs. It's hard to tell that you're doing the wrong thing ahead of time, but that's how all bugs are born =P
I know nothing about stack... but I find it odd that it's looking for the `alex` binary in a GHCJS directory. That looks like a problem.
This is something I can share. Check it out. http://book.realworldhaskell.org/read/
I started with it, but it definitely has an "in one ear, out the other" quality to it. Also my workflows now are very, very different from what I learned initially. 
Thanks for these, quite interesting in any language
Atom. ide-haskell is great. Ideally you should only need to do `stack build ghc-mod` inside of your stack project. Feel free to ask for help if (hopefully not when) things go wrong.
It does do that, you just have to enable it. See here: https://stackoverflow.com/questions/31866379/haskell-non-exhaustive-pattern-matching-in-haskell
For completeness' sake, it's worth noting that exhaustiveness checking becomes a much harder problem with stuff like GADTs. IIRC, it's still not quite perfectly right.
Yeah, I remember reading a paper about that a couple of years ago(1), but I don't know if it got merged in. (1): https://www.microsoft.com/en-us/research/publication/gadts-meet-their-match-pattern-matching-warnings-that-account-for-gadts-guards-and-laziness/
many thanks 
The compiler knows, but it's not about the compiler, it is about the human - and to the human, `&lt;-` means "bind the result from the right to the thing on the left".
it is. in 8.0 https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/8.0.1-notes.html &gt; A rewritten (and greatly improved) pattern exhaustiveness checker
Hack, manipulate, mold, ...
It's undecidable for GADTs. (Equivalent to the halting problem I think.)
`(m -&gt; a, n -&gt; a) -&gt; (Either m n -&gt; a)` would be achievable, but you can't have a function that accepts more than one type without having some unifying type. 
I mean I guess, to me `_ &lt;-` is extremely clear, it means "ignore the result", but I guess to each their own.
Indeed, this is the "either" function. 
Have an example? I've never seen this library in use.
Great timing. Just earlier today I was having a conversation about this. Haskell does have non-exhaustive pattern checking, but that's actually not always sufficient. The topic we were discussing was the situation where you have something like this: case foo of FooConstructorA -&gt; ... FooConstructorB -&gt; ... f -&gt; ... That last pattern wild card can be the source of bugs, especially if you refactor the code to add another constructor. It would be nice to have some kind of checking for this.
That's brilliant, I can't believe I didn't think of that. That would be much easier to deal with.
This is known as let-floating, and GHC does perform this transformation. See here: https://www.microsoft.com/en-us/research/wp-content/uploads/1996/05/float.pdf But of course, as with anything related to optimization, things can go wrong. The paper gives a great account of the issues. If you want to absolutely make sure it happens, you can either trust GHC and look at the generated core (not for the faint of heart) to ensure; or put them on the top-level yourself. The latter, while rather ugly, is always safe.
The biggest trouble there tends to be when the last case is a wildcard. The problem with a warning is that it's extremely obnoxious if the type in question is extremely stable. For example, `[]`, `Maybe`, `Either`, and `Free` are never ever going to sprout new constructors.
I tend to mentally partition types into two classes: those that may change in response to changing requirements (domain representations), and those that are intended to never change (algebraic representations). It might be useful to distinguish them somehow in code.
We do have one in the upcoming GHC 8.2, https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes However, it seems intended as for efficiency, and does not come with a coproduct function.
I agree. It's worse than some, for example Java but with Java you simply can't work productively without all the bells and whistles.
I think it's a good question. I might be hard to answer but that's something else.
Agda and Idris sometimes don't let you do a pattern match even though it is exhaustive.
Wow, will it soon be possible to construct completely flattened inductive types - as in, there will be no pointer indirection except at the recursive hole(s)?
I think the hand-written way to make sure this happens is: collect d = go where ... everything computed based on only d ... go str = first : go (drop (length first) start) where ... everything computed based on str ... 
Just did a quick scan of the article, but aren't you either losing data (during a crash/restart), or making deployment depend on all other services being up (if you wait until the queue is empty)? What I'd like to see is a combination of `persistent` and working out-of-the-box postgres 9.5 queue features.
For the binary case, `Either` already is exactly the boxed thing one would hope for. There are two reasons why I find it lacking, though: 1. Lacks transitivity, as in easy constructing/unpacking n-ary sums. A construct like in the unboxed sums proposal `(|||)` or `(| (second, case, fields) |)` might help here. 2. Other than that, a builtin syntax/type operator for `Either`. `|` would probably work, if it was available.
You’re right, that was a bad example. I meant to convey that the thing you last changed is probably the thing you meant to change, and it would be nice if error messages could reflect that, if only for establishing a priority for the order in which error information is presented to the programmer.
We have it turned on in all of our cabal files.
Not necessarily -- you could compose the underlying query in many cases, too. Also, reducing the API size from `N x M` to `N + M` (especially as more dimensions are added) is worthwhile even if the implementation size is large. 
That sounds like an easier and more robust solution, too. We use the same approach not for emails, but many other things that can fail and need to be retried.
What does the set subtraction operator `\\` have to do with anything here?
Perhaps I'm misunderstanding what unboxed sums and products means. I just mean that in a language like Rust, a `struct S { a b c d : i32 }` is not a pointer to somewhere on the heap: it's actually 128 bits of data. When you iterate through a `Vec&lt;S&gt;`, you're not making heap dereferences and cache misses at every point, you're actually traversing directly through contiguous memory in 128-bit chunks. In contrast, in Haskell (as far as I understand), even if you use something like `Vector` instead of `List`, the elements are still all pointer-wrapped, meaning you don't actually gain much because it's just the pointers which are sequential in memory, while the data itself is still all over the heap.
Indeed, GADTs would help you to retag the tagless encoding: -- | we don't need `m` parameter here. data Spec f a where It :: String -&gt; f a -&gt; Spec f a Describe :: String -&gt; [Spec f a] -&gt; Spec f a BeforeEach :: f a -&gt; Spec f (a -&gt; b) -&gt; Spec f b BeforeAll :: f a -&gt; Spec f (a -&gt; b) -&gt; Spec f b which can be encoded like in `purescript-freeap`: https://github.com/ethul/purescript-freeap/blob/master/src/Control/Applicative/Free.purs *Free applicative* and not *Monad* makes sense, as test structure is statically known. It would be different if `beforeAll` blocks could change which tests are constructed (and run)!
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ethul/purescript-freeap/.../**Free.purs** (master → f208e92)](https://github.com/ethul/purescript-freeap/blob/f208e92f621c0c96ad0d8a9f3e70c0b99e82918b/src/Control/Applicative/Free.purs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dihcyby.)^.
I discribe graceful shutdown and read the Further Considerations section. If you want to ensure delivery in the event of crashes etc, you can setup a low freq task to poll the db.
&gt; But why would I care so much what version of stack I have? Because Stack's creation represents such an important event in Haskell's ecosystem - even more important than GHC - because Cabal Hell was defeated and so from that point on we use Stack's version instead of the Gregorian calendar. We shall all rejoice that we've already reached 1.4.0 **A**nno **S**tack **S**alutis and counting...
Not really, you'll need a writer there still: describe :: String -&gt; Writer [Spec f a] () -&gt; Spec f a describe name = Describe name . execWriter So it's orthogonal to the `Spec` data structure (how represent the tree of specs), as it's DSL engineering.
And now as I think more about it, you'd probably be ok with just BeforeEach :: f () -&gt; Spec f a -&gt; Spec f a But as in you do pass the result of setup-blocks into specs you'll need a free monad actually: BeforeEach :: f a -&gt; (a -&gt; Spec f b) -&gt; Spec f b (and you probably better using `Pure a | Free (f (Free f a))` style for that).
If no one else beats me to it, I might write it using Free monad as well. I'd ping you then. :) 
It means domain specific language. There are some blog posts and explanations on the Internet, e.g the [Haskell-wiki]( https://wiki.haskell.org/Embedded_domain_specific_language).
This works for me too. But I find myself using VSCode with haskero more often.
&gt; ... have anything [built-in]. is probably what he meant.
They take the opposite approach of Haskell. They have false negatives, where you do have exhaustive set of pattern matches but they claim you don't. Haskell does not have this, but does have false positives (which they don't), where you don't have an exhaustive set of pattern matches but Haskell doesn't complain. You have to have one or the other, and for practical purposes IMO Haskell should never tell you that you don't have exhaustive pattern matches when you actually do.
Please put a little more effort when formulating your questions. If you don't explain what you've managed to understand so far and what are the parts you'd like to understand better, it looks like you haven't even bothered to google for an answer, which doesn't make me want to help. Also, without that context, we can't give you an answer which is tailored to your current understanding, so all we can do is to parrot-back the generic explanation you'd get via google anyway, so you and I are both wasting our time.
The `_` case at the end looks like what you're talking about. It's a wildcard, so it'll match everything. There's no need for an additional `otherwise` after you already have a case that matches anything. However, exercise care when using `error`. It's often better to use something like the `Maybe` or `Either` types for functions that may not be able to provide a result rather than crashing the whole program.
My compiler gives me a parse error on input '-&gt;'. So I assumed what I did above is incorrect. Any idea why? Thanks for the advice on the Maybe type. I ended up using that. 
Could also be viewed as `fmap` for a functor `Foo` newtype Foo :: Constraint -&gt; (Type -&gt; Type) where Foo :: (ctx =&gt; a) -&gt; Foo ctx a from the category of `Constraint`s and entailments `(:-)` data Dict :: Constraint -&gt; Type where Dict :: ctx =&gt; Dict ctx type Cat k = k -&gt; k -&gt; Type newtype (:-) :: Cat Constraint where Sub :: (ctx =&gt; Dict a) -&gt; ctx :- a to the opposite functor category with natural transformations `(&lt;~)` newtype (&lt;~) :: Cat (k -&gt; Type) where OpNat :: (forall xx. g xx -&gt; f xx) -&gt; f &lt;~ g -- Foo : (:-) -&gt; (&lt;~) instance Functor Foo where type Dom Foo = (:-) type Cod Foo = (&lt;~) fmap :: (ctx :- ctx') -&gt; (Foo ctx &lt;~ Foo ctx') fmap (Sub dict) = OpNat $ \(Foo r) -&gt; Foo (withDict dict r)
It's always great to see more Haskell event sourcing articles! Nice work. I do have a few comments, just based on my experience in this space: 1. I think you've conflated commands and events for aggregates (that could have been your intention, I'm not sure). Oftentimes when working with aggregates you use a command to *tell* the aggregate the action you want applied, and then the aggregate may publish some events if the action succeeded. The distinction here is useful: * Commands are imperative and are more like requests. In your example, you would probably have a `RegisterUser` command that can optionally produce a `UserRegistered` event. If it fails then no event is produced. * On the other hand, events are statements of fact. They are things that have happened and we cannot change them. Thus, when replaying your events to get some state (using the fold function), rebuilding that state should never fail. Events are things that have already been validated by a command handler, so they should be valid. 2. You could probably simplify your `Aggregate` by not forcing state to be `Maybe` and removing `aggInit`. If you want to have a state that can only be initialized by some particular event/command, then just use `Aggregate event (Maybe MyState)`, and only flip to `Just` in your fold when the appropriate event is encountered. 3. I've found it helpful to remove the concept of command errors and just use events that signify failure, like `UserRegistrationFailed`. Not only does this simplify the `Aggregate`, but this means that inter-aggregate listeners like process managers and sagas can listen to this and act to, for example, send an email to that user. Also, this means you can send off commands without needing to synchronously get the result back, so you could put commands in a durable queue. 4. I've found it very useful to separate out rebuilding state from events and validating commands against that state. For example, the state rebuilding function could be `state -&gt; event -&gt; state`, and the command validator can be `command -&gt; state -&gt; [event]`. That way you don't need to populate `aggRaisedEvents` using monadic actions. See an example [`Projection`](https://github.com/jdreaver/eventful/blob/73eb93c031c9143abd0761381776ecc569b13179/eventful-core/src/Eventful/Projection.hs#L19-L32) type for the first case and another example [`Aggregate`](https://github.com/jdreaver/eventful/blob/73eb93c031c9143abd0761381776ecc569b13179/eventful-core/src/Eventful/Aggregate.hs#L18-L27) type for the second case. 5. I made a monad similar to yours that transparently handles failure and the aggregate's version. This may just be a byproduct of the domains I've worked in, but I've found it super rare to apply more than one command to an aggregate at a time, so I scrapped at is unneeded complexity. I would love to see examples of multiple commands being applied at once though, it sounds interesting. I think event sourcing and CQRS are exciting ideas. Unfortunately, they seem to be monopolized by the enterprise .NET crowds, and they have only recently become more widespread in other areas. Haskell is, in my opinion, the ideal language for this. It's exciting to see more folks starting to explore it. Thanks again for the article! 
/u/chrisdoner got it - indentation problem. The code is actually correct for what you want to do other than that.
I found your problem. You have an extra space before your wildcard. You've aligned it to the `0` on the previous line instead of the first `'`. Here is that last line fixed charToInt :: Char -&gt; Int charToInt x = case x of '1' -&gt; 1 ... '0' -&gt; 0 _ -&gt; error "Nope" This took me a couple minutes haha, I couldn't find what was wrong visually until I manually rewrote it. By indenting, the compiler thought you were adding onto the previous case. The indentation of all the cases needs to match.
That's slightly sad having to do that build step. Emacs intero integration installs intero into each project automatically. I don't see why this couldn't be the case for other IDEs.
Cool! I'll check this out.
These days `FastString` really is just a wrapper around `ByteString`, see https://downloads.haskell.org/~ghc/8.0.1/docs/html/libraries/ghc-8.0.1/src/FastString.html#FastString.
You can also signal the worker instead of only pooling the DB. That will make for fast deliveries, and keep your resource usage in check. You'd probably still want to pool in order to catch any signal that failed for some reason, but now you can have a really big interval without issues.
1 million connections is around the maximum possible in the smallest VPS around (I've tried). But you will lose responsivity if you go to those extremes.
The work I do on our PHP codebases is almost entirely bug fixing. The time to implement a feature and fully debug it is pretty huge. The work I do on our Haskell codebases is almost entirely feature implementation. Bugs tend to be logic errors, so the error report is something like "this is happening in this situation and we expect this to happen." It's typically trivial to write a unit test for the logic in question and fix the bug.
No no, if you try to build it with optimizations enabled, you get a compile-time failure on too many simplifier iterations.
Darcs also had its own internal version of ByteString for a few years (until I ripped it out). I wonder how many of them there were scattered over the place?
Ah interesting. Well i'd recognize the omega combinator anywhere, but I am unsure exactly why it is causing a compile time infinite loop. I'm guessing it's something to do with how GHC tries to make newtype wrapping / unwrapping a no-op at runtime, and does it at compile time?
That sounds like a good idea. I would think one pragma for potentially changing, one for changing, and a user-determined default for unmarked types. Would you like to put together a GHC proposal to do that?
Woohoo! This is how I think of things too. I've never mentioned it to anyone else and I'm glad to see someone else has reached the same conclusion independently. (I still avoid wildcard patterns in algebraic representations anyway!)
Yeah, could be helpful to put .local/bin on your PATH. If "alex" is there, then it will be used. If "alex" isn't there, it ought to be installed . Not sure why it doesn't get built properly via ghcjs. I've tried adding "-build-runner", but still no executable for it. Hrrm
A funny thing, you can actually give the result a throwaway name (and ghc will warn you with `-Wall` if it doesn't start with `_`): '0' -&gt; 0 x -&gt; error "Invalid character" or, if you're feeling cheeky: '0' -&gt; 0 otherwise -&gt; error "Invalid character" since 'otherwise' is just a valid identifier for a pattern match
The free resources are great, but I've found Haskell Book to be the best structured from start to finish. Chapter 1 is key. Bear with it; you'll thank the authors later for it. http://haskellbook.com/
Trivially defined: ``` cocurry f = (f . Left, f . Right) ```
It likely started out as not "just a wrapper" but people have refactored the code over the years to make it "just a wrapper" because ByteString is very solid and why duplicate tons of development work with hand rolled strings?
My favorite paper has been "trees that grow"; it's not 100% Haskell related but it's fairly awesome.
Sure, I’ll see what I can come up with, but I’m not sure how to make it terribly useful. Just thinking aloud, I suppose this would add: * A concept of “type intent”, specified by a pragma `{-# INTENT Maybe algebraic #-}` or `{-# INTENT Term domain #-}`. * Warnings flags `-fwarn-domain-intent-wildcard` and `-fno-warn-domain-intent-wildcard`. I think `-fwarn-domain-intent-wildcard` should be implied by `-Wall` even though warning-clean people will kvetch about it. * A warning emitted by that flag when a `case` (or equation) uses a wildcard pattern when matching on a type with `domain` intent. * A flag to specify the default intent, `-fdefault-type-intent=domain|algebraic`. This works with only static snapshots of the source; it would be nice if GHC could use information about changes to a type as well, but I expect that would involve writing too much new machinery. It also adds a hook on which we can hang other “intents” in the future, for warnings &amp; errors or optimisations. 
Here's a link for anyone else: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf Looks really interesting, thanks!
[Beautiful concurrency](https://www.microsoft.com/en-us/research/publication/beautiful-concurrency/)! ah, ten years ago....
Here's an old blog post of mine http://tojans.me/blog/2014/02/26/cqrs-and-functional-programming/
No; they're "tagged" in the sense of "tagged unions"; each constructor corresponds to a runtime tag that needs to be matched. One of the potential appeals of finally tagless is that in some cases, runtime pattern matching can be avoided in favor of static dispatch.
I'm a bit puzzled at the Linux VM problem here. Yes stack will put it's config in your users home (should be no big deal) - but why don't you have write permissions to your own home? (from which folder you run does not really matter here) In case you are still interested (the vs.code on windows is probably a better idea anyway) can you post all the output you get from `stack setup`? 
Postgres has a pub-sub notification system, so you don't have to poll. And I know skedge uses postgres. :-)
&gt; it's a measure of detected bug density Not even that. I wanted to take a look at comparable projects. Given that many Haskell high-star projects are compilers, I looked at Clojurescript. Turns out you can't file Github issues for this project, as they use an external JIRA. Also, I *think* many projects use issues as a TODO list.
I suspected that after I wrote that, heh. Thanks for the confirmation!
&gt; […] compare the number of issues labelled "bug" […] He used /some/ heuristic to determine if an issue is a bug or not. He didn't blindly take the number of issues as an indication of the number of bugs. Not saying that his analysis isn't flawed, but not as badly as you think. 
[Supercompilation by Evaluation](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/supercomp-by-eval.pdf) Good introduction to compile-time open code evaluation and termination checking.
You're quite right, that's a bit more than a pure ByteString wrapper. I confess I didn't look at the source that was linked; I was on mobile and it didn't occur to me to question the maintainer of ghc-mod's opinion of FastString since I'm certainly not an expert at ghc's underpinnings yet.
[Typed Tagless Final Interpreters](http://okmij.org/ftp/tagless-final/#course-oxford) is a great introduction on implementing extensible DSLs.
[Data types a la carte](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf) 
Yup, I kept getting hammered with errors :(
I see, thank you :) I'll add it as an example tonight. Or else, you could submit a PR with the file ;) 
This is a minor gripe, but is symptomatic of a huge issue in classical Haskell libraries. When, in the midst of some insanely complicated application interfacing with dozens of external services, there is a network issue, the application will crash with a message that looks like: getAddrInfo: something funny happened This is a function in the `network` library. Would it kill the authors to tell us *which* address/socket/&amp;c was unreachable/reset unexpectedly/&amp;c? A language without sane stack traces, combined with less-than-useful error messages in the standard libraries, makes for a really bad time when the exceptional happens. I am reluctant to open an issue because I have an inkling that by doing so I might be asking for some de facto standard to be broken, because why else hasn't this been fixed yet?
[Freer Monads, More Extensible Effects](http://okmij.org/ftp/Haskell/extensible/more.pdf) I especially like the conclusion: &gt; The ambition is for Eff to be the only monad in Haskell. Rather than defining new monads programmers will be defining new effects, that is, effect interpreters.
i added ".local/bin" to my PATH, but it still doesn't work. I'll try opening a ticket on github, but the amount of open tickets is a bit discouraging. Is there anything more frustrating than tools getting in the way of being productive? :D EDIT: stackoverflow question: https://stackoverflow.com/questions/44385763/using-build-tools-alex-with-stack-and-ghcjs
I'm pretty sure that `Vector` is just a high-level interface around `Array` which is just a high-level interface for C-style arrays, so I think you should get pretty good locality. I think this is the case for unboxed vectors, anyway. 
[How to make ad-hoc polymorphism less ad-hoc](https://pdfs.semanticscholar.org/cc7f/2242dba6f09023128897762d07517f13ba4a.pdf) by Philip Wadler and Stephen Blott in 1988. It's hard to imagine Haskell being Haskell without type-classes. 
Note that author intention was to ask about compiler errors. `network` library is de-facto, but not by any means "standard" library. So yes, they need your report, or even better a PR. From looking at the code https://github.com/haskell/network/blob/fe7003293c9a08497a9df6cc18bb3868c96bda8f/Network/Socket.hsc#L1451 there is _ -&gt; do err &lt;- gai_strerror ret which is used to assemble the message. So maybe `something funny happened` comes from your POSIX implementation?
`something funny happened` is not the point. Here's an actual, real-world example (on Ubuntu): getAddrInfo: does not exist (Name or service not known) Did something happen to the DNS? Did I mess up an environment variable? Did I accidentally deploy a dev configuration on a production server? Is it the email, SMS, Redis, Postgres, Mongo, or some other IP/address which I have misspelled? `getAddrInfo` *knows* which "Name or service" it is I'm trying to access, but neglects to say, making a simple "duh" moment turn into a potential troubleshooting nightmare. getAddrInfo: 'http://unreliablecheaprackspace.com' does not exist (Name or service not known) would reduce a tonne of friction. &gt; Note that author intention was to ask about compiler errors. The evidence for that is scant. I will look into creating a pull-request. 
Yes, `does not exist (Name or service not known)` is exactly what C `getaddrinfo` gives you. I'm 99% sure your PR will be accepted which does a little change like: - Nothing) err) + Nothing) $ maybe "" (\n -&gt; "node '" ++ n ++ "': ") node ++ err) and do everyone a favor, go thru all `ioError` in the library to improve error reporting of other error cases too. Remember, bascially everything in this community is based on a voluntary work. Small things like PR above will help experts concentrate on expert issues (e.g. in `network` case: make it compile and work on all supported platforms).
I believe the darcs one (`FastPackedString`) was a precursor to `ByteString` - you can still see David Roundy's name in the copyright messages of `ByteString`.
Any screenshots?
I thought it was weird that the site had none. :/
this looks interesting but I can't even get it to launch in my browser...
Much more importantly is the avoidance of pattern matching in the interpreter itself. That adds a huge amount of overhead. Part of the paper was to show that you can write interpreters (and compilers and partial evaluators and CPS transform and ...) without needing to dispatch on tags. As other have said: all you really need is a 'fold' principle over your syntax.
One just came up the other day: https://www.reddit.com/r/haskell/comments/6fe9w5/the_otherwise_else_equivalent_of_a_case_statement/
We are working on uploading them! :D
We will fix this in the near future, meanwhile, could you make a symbolic link to `/usr/local/bin`?
Why does it even assume Stack? As a Nix user, this makes it impossible for me to use. It's fine if Stack is the easiest and recommended way to install it, but I can't think of anything Stack offers that could make it a strict requirement.
There is a GHC wiki page capturing thoughts on error messages, for rework. See [here](https://ghc.haskell.org/trac/ghc/wiki/PrettyErrors)
That's true, you need to run `stack setup --stack-yaml=client-stack.yaml`
How does TH work in this case? Forgive me, I only skimmed the article so maybe I missed if any setup was necessary. Lacking TH was my reason forever ago to not go past the Hello World GHC iOS example.
»But processors are imperative, so they like running C more than Haskell« - **Wrong.** [Implementing Lazy Functional Languages on Stock Hardware](https://www.microsoft.com/en-us/research/publication/implementing-lazy-functional-languages-on-stock-hardware-the-spineless-tagless-g-machine/) is a description of the STG, which is the machine running Haskell (in GHC). It’s basically a much simplified version of it with specified evaluation order, and while it is still clearly functional, it also has a relatively direct mapping onto real hardware. Learning about the STG was probably the most valuable lesson about Haskell that I’ve learned, and I recommend learning something about the STG to every intermediate+ haskeller. I wrote an implementation of the paper for human consumption, which interested readers can find [here](https://github.com/quchen/stgi).
That's a good point, I'll have to think about this. If we could guarantee there were no gaps in the primary key then this could be worked around by keeping track of gaps in read models, but that isn't a guarantee we can make either. Thanks for the heads up!
Just added them, thanks for noticing! \^_^
Super! Thanks!
Thanks. Now I just need to pack the types into an Either before applying uncocurry? Left :: a -&gt; Either a b Right :: b -&gt; Either a b
Might be useful, automatically scrubbed memory : https://hackage.haskell.org/package/memory-0.14.5/docs/Data-ByteArray.html#t:ScrubbedBytes
why not https://hackage.haskell.org/package/pwstore-fast ?
`StM` is a type family, so those reductions are [by definition](https://hackage.haskell.org/package/monad-control-1.0.1.0/docs/src/Control-Monad-Trans-Control.html). For transformers, `StM` is defined in terms of `StT`: instance MonadTransControl IdentityT where type StT IdentityT a = a -- ... instance MonadTransControl MaybeT where type StT MaybeT a = Maybe a -- ... instance Error e =&gt; MonadTransControl (ErrorT e) where type StT (ErrorT e) a = Either e a -- ... instance MonadTransControl (ExceptT e) where type StT (ExceptT e) a = Either e a -- ... instance MonadTransControl ListT where type StT ListT a = [a] -- ... instance MonadTransControl (ReaderT r) where type StT (ReaderT r) a = a -- ... instance MonadTransControl (StateT s) where type StT (StateT s) a = (a, s) -- ... instance MonadTransControl (Strict.StateT s) where type StT (Strict.StateT s) a = (a, s) -- ... instance Monoid w =&gt; MonadTransControl (WriterT w) where type StT (WriterT w) a = (a, w) -- ... instance Monoid w =&gt; MonadTransControl (Strict.WriterT w) where type StT (Strict.WriterT w) a = (a, w) -- ... instance Monoid w =&gt; MonadTransControl (RWST r w s) where type StT (RWST r w s) a = (a, s, w) -- ... instance Monoid w =&gt; MonadTransControl (Strict.RWST r w s) where type StT (Strict.RWST r w s) a = (a, s, w) -- ... 
I'm trying to standardize on two editors, Visual Studio Code and Intellij. While I really liked Haskell for Mac while I was learning Haskell, the full access to the Stack ecosystem was pretty compelling. Plus, I'm one of those idiots who prefers vim keybindings, and VSCode had that *and* is actively developed, so I switched over. Honestly, I recommend VS Code with Haskero—it's pretty good, it's fast, and is actively developed. Take it from me—don't over think your editor!
&gt; I wish there were a good way to support both Stack and Nix nicely ...and don't forget about us cabal users!
It's kind of hard *not* to support Cabal =P If you have a cabal file (which, incidentally, haskell-do does not), you've done about as much as you can.
&gt; The "`ST`" stands for "a **stateful transformer**", which we take to be synonymous with "a stateful computation"; Mystery solved! 
That's amazingly underdocumented then. Supposedly it only chages `negate (fromInteger 1)` to `fromInteger (-1)`. I don't see what that has to do with parsing.
What is the type of `coolStuff`? is it String, or do you have an Html type with an IsString instance?
This documentation? &gt; [9.3.3. Negative literals](https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#negative-literals) &gt; -XNegativeLiterals &gt; Since: 7.8.1 &gt; Enable the use of un-parenthesized negative numeric literals. &gt; &gt; The literal `-123` is, according to Haskell98 and Haskell 2010, desugared as `negate (fromInteger 123)`. The language extension `-XNegativeLiterals` means that it is instead desugared as `fromInteger (-123)`. &gt; &gt; This can make a difference when the positive and negative range of a numeric data type don’t match up. For example, in 8-bit arithmetic -128 is representable, but +128 is not. So negate `(fromInteger 128)` will elicit an unexpected integer-literal-overflow message. The 8-bit arithmetic example is pretty confusing, the "Enable the use of un-parenthesized negative numeric literals" summary is much better. Proposed replacement: &gt; According to Haskell98 and Haskell 2010, `-123` is not a literal, but rather unary negation applied to a literal, so `negate (fromInteger 123)`. The language extension `-XNegativeLiterals` enables negative literals, meaning that `-123` is instead desugared as `fromInteger (-123)`. Similarly, `print -123` is normally parsed as an ill-typed binary subtraction, so `(-) print (fromInteger 123)`, whereas with the extension, it is parsed as `print (fromInteger (-123))` instead. &gt; &gt; This can also make a difference when the positive and negative range of a numeric data type don’t match up. For example, in 8-bit arithmetic -128 is representable, but +128 is not. So `negate (fromInteger 128)` will elicit an unexpected integer-literal-overflow message. 
Seems like two totally unrelated changes put into the same extension.
Good point, but you can't unsafeCoerce to get it
Even better would be to just make `fromText :: Text -&gt; IO Password` and hash it out of the gate. If you need to introduce IO somewhere you might as well introduce it during construction instead of access.
Yeah probably, I did that way to make it easier to use in `parseJSON`. I could still make a instance for IO Password I guess.