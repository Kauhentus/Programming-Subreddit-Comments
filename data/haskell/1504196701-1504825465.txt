Thanks, will do. :D
Seems like a good book, but a bit too pricey for professional broke folks like me. :P
Haha, thanks. Yeah, my primary interest for learning haskell is to apply the knowledge effectively in JS.
Do you have a link for the FutureLearn course? Thanks.
All links on that "Engineering Large Projects in Haskell: A Decade of FP at Galois" page either point to /u/dons's homedir on galois.com and 404 by now (talk slides), or seem to be a repurposed doman (λondon HUG). A pity. Does anyone know of a substitute link? (here or edit SO directly)
You can read it online here: https://www.scribd.com/doc/19502765/Engineering-Large-Projects-in-Haskell-A-Decade-of-FP-at-Galois If you want to download it without signing up, there is dlscrib.com
Yup, I'd use deterministic cookie names. You'll always want them scoped to the domain (+ sub-domains perhaps), *and* the name randomization does nothing for you (cryptographically) other than perhaps hiding even the presence of the specific middleware. I'd say having non-random IVs is a far more serious issue.
I definitely echo the sentiments to learn as you go, and that it's fine to learn if you're new. That said, when I was starting, I was extremely confused by the applicative and functor infix notation. And, Monads are everywhere. So while you don't need to know any of the theory behind them, I'd do some reading on fmap and &lt;$&gt;, &lt;*&gt;, and do notation.
I need a physical copy of this, I've learned too much not to pay for it :D
The book is worth its price many times over. It's the best textbook / technical teaching resource I've read, let alone Haskell. The comparative cost in your effort to learn effectively without it will be a lot higher than the price tag. That said, you could try emailing the authors about the price -- I've heard they take mercy on professional broke folks who want to learn Haskell :)
It may or may not be finished. Read the comments on the Lawvere theories post.
This works for this particular example, but what if we change the program slightly? (let ([x mempty] [y (mac x)]) (x y)) Uh oh—now we know nothing at all about `x`’s type! It’s simply a fresh solver variable, `&lt;a&gt;`. This means that now `mac`’s first clause will match, and `&lt;a&gt;` will unify with `Bool`. However, we will now typecheck `(x y)`, which will produce a type error, since a `Bool` obviously cannot be applied. This is unfortunate, because this program *could* typecheck. If we had chosen the second case of `mac`, then `y` would be `nothing`, `x` would be assigned a function type, and the expression would typecheck (assuming we actually defined the appropriate `Monoid` instances for `-&gt;` and `Maybe`, which I admittedly haven’t yet). Now, this problem could be solved by *even more backtracking*, but in general, backtracking is too costly. We do not want to typecheck the same program an exponential number of times. We need to provide stronger guarantees than that so that users’ programs don’t end up taking hours to typecheck. That said, your proposal isn’t useless—there are cases in which the limited notion of backtracking would likely be useful—but I don’t think it’s predictable enough to be the right default. It’s too powerful, and that power makes it easy for macro authors who don’t understand all the details of the typechecker from shooting themselves in the foot. I think we need to protect macro authors from having to know all that information to be able to write useful macros.
I'd love a hardcover book to buy as reading over the monitor decreases the flexibility of my reading positions.
I'm self-taught as well, and Haskell is the first language I learned well (I dabbled in Python / JS before that). I couldn't do anything until I understood the syntax, what a "type" is, what a "type class" is (to understand problems with `Num a`, for example), how to assign variables with `let` or `where`. No deep knowledge -- just being aware of what a type class even is. This is a short phase (could be a few hours), and then you have the minimum knowledge IMO. The next phase was learning about function composition, the `$` operator, simple types like `Maybe`, pattern matching, guards, partially-applied functions getting passed around, and that sort of thing. At this point I could comfortably solve simple programming problems and follow tutorials, but I still struggled to build anything non-trivial. I didn't feel capable of really understanding what I was building until I completed the `Monoid`, `Functor`, `Applicative`, and `Monad` chapters in the Haskell Book. [This resource on adit.io is also great](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html). Understanding these took me from building trivial projects to building involved ones. There are still so many topics for me to learn about, but by the time I reached that third phase, I was comfortable building web applications, command line tools, etc. in Haskell.
`#haskell` and `#haskell-beginners` on IRC are both great, and there's a great community for functional languages generally here: https://fpchat-invite.herokuapp.com
That's just a matter of personal preference. I also enjoy `LambdaCase` sometimes: listLength = \case [] -&gt; 0 (_:xs) -&gt; 1 + listLength xs
This series (and the [lectures on YouTube](https://www.youtube.com/watch?v=I8LbkfSSR58&amp;list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_&amp;index=1)) has really opened my eyes for abstract mathematics and has been incredibly intresting to work through. I encourage all of you other haskellers to look into it! It's really quite something when you see the categorical definition of i.e. a functor and everything just clicks with what you've learned in Haskell.
As usual for lists, it's much easier to write a *bottom-up* mergesort: type family Compare (x :: k) (y :: k) :: Ordering type instance Compare x y = CmpSymbol x y type instance Compare x y = CmpNat x y type family Merge (xs :: [k]) (ys :: [k]) :: [k] where Merge '[] ys = ys Merge xs '[] = xs Merge (x : xs) (y : ys) = Merge' (Compare x y) x y xs ys type family Merge' (xy :: Ordering) (x :: k) (y :: k) (xs :: [k]) (ys :: [k]) :: [k] where Merge' 'GT x y xs ys = y ': Merge (x ': xs) ys Merge' _ x y xs ys = x ': Merge xs (y ': ys) type family BreakUp (x :: [k]) :: [[k]] where BreakUp '[] = '[] BreakUp (x ': xs) = '[x] ': BreakUp xs type family MergePairs (xss :: [[k]]) :: [[k]] where MergePairs '[] = '[] MergePairs '[xs] = '[xs] MergePairs (xs1 ': xs2 ': xss) = Merge xs1 xs2 ': MergePairs xss type family MergeAll (xss :: [[k]]) :: [k] where MergeAll '[] = '[] MergeAll '[xs] = xs MergeAll xss = MergeAll (MergePairs xss) type MergeSort (xs :: [k]) = MergeAll (BreakUp xs) 
I like the Tweag blog posts, happy to have you post them here! On array programming, has anyone done any Haskell coding involving convolution or something similair? I would like to implement some basic DSP-algorithms in Haskell for funsies but I am not sure how you would deal with the indexing or matrices in a "Haskell way". I mean one could have an Mvar as an index and iterate "imperatively", but it feels like there is a better solution.
It's the same breakage. AFAIU the 8.0.3 fix doesn't definitively solve the problem. It just pushes the problem point out a little further. You can still go over the limit if you have too many libraries.
You mean just the convolution of vectors? `hmatrix` has this operation [here](http://hackage.haskell.org/package/hmatrix-0.18.0.0/docs/Numeric-LinearAlgebra.html#g:27).
Thank you, will forward the compliment to the kythe.io guys - the UI is actually the [Kythe debug UI](http://kythe.io/examples/#visualizing-cross-references). Note that ideally one would have a nicer UI, something like the [Chromium codesearch](https://cs.chromium.org/chromium/src/net/der/encode_values.cc?l=29).
Ah, I could look at the source code in there and hopefully grasp ut. Thanks! 
Haha oh no
Sure, here it is =&gt; https://www.futurelearn.com/courses/functional-programming-haskell
I've a feeling that @anotherturingmachine is talking about `nix-shell`. I don't know how much `nix-shell` messes up with your shell environment (like how it would interact with oh-my-zsh), but the utility it provides should dwarf any minor inconvenience.
oh, yes
These are all very nice points, but I think to an outsider, they'd appear to be nifty gimmicks, instead of signaling an extremely solid foundation. When I need to give an example of something that truly differentiates Nix from all other options, I say, if I need a version of `Nginx` on my system, compiled with a specific flag, I just need to add two lines to my NixOS configuration, and rebuild. This a natural outcome of the fact that no other tool approaches covering the ground that Nix spans alone, further amplified by the fact that Nix itself is a purely functional language encouraging modular code.
&gt; However, we will now typecheck `(x y)`, which will produce a type error, since a `Bool` obviously cannot be applied. [...] Now, this problem could be solved by _even more backtracking_ The example I chose didn't demonstrate it well, but it is this "even more backtracking" behaviour I had in mind: `Bool ~ &lt;fArg&gt; -&gt; &lt;fResult&gt;` causes unification to fail, so we backtrack and try the second clause. &gt; [...] but in general, backtracking is too costly. [...] It’s too powerful, and that power makes it easy for macro authors who don’t understand all the details of the typechecker from shooting themselves in the foot. Ah, I understand: backtracking is as powerful as having an oracle telling us which clause we should use, so it's not really an inference algorithm, it's just brute force in disguise. In that case, the idea probably can't be saved by minor modifications such as "what if backtracking is disabled unless you explicitly ask for it?" or "what if we keep track of which macro clause led us to which equalities?". &gt; assuming we actually defined the appropriate `Monoid` instances for `-&gt;` and `Maybe` This is unrelated to the current discussion, but could Hackett perhaps not give an instance of Monoid for Maybe? Haskell's instance is [controversial](http://haskell.1045720.n5.nabble.com/Maybe-Monoid-spilt-milk-td3173596.html), and giving it a different, more correct instance (whatever that is) might cause Haskellers to write silently-incorrect Hackett programs.
There is a somewhat common pattern where people write a recursive function using a helper to make renaming it easier. For example, in the `streaming` library, we have a function [maps](http://hackage.haskell.org/package/streaming-0.1.4.5/docs/Streaming.html#v:maps): -- Don't worry if you don't understand what this function does. -- For the purposes of this example, it doesn't matter. maps :: (Monad m, Functor f) =&gt; (forall x . f x -&gt; g x) -&gt; Stream f m r -&gt; Stream g m r maps phi = loop where loop stream = case stream of Return r -&gt; Return r Effect m -&gt; Effect (liftM loop m) Step f -&gt; Step (phi (fmap loop f)) Notice that instead of `maps` calling itself recursively, it introduces a helper function `loop` with a `where` clause. The advantage of this is that if the author ever wants to rename `maps`, they don't have to change the function body at all. Or, and this is more relevant to your case, if they want to copy `maps` to make a similar function, they don't have to worry about accidentally calling the old `maps`. It's not always worthwhile to do this, but it's always an option if you keep having trouble with this kind of thing.
&gt; Use lots of "model/view/controller" style programming: parse external data as soon as possible into purely functional data structures, operate on those structures, then once all work is done, render/flush/serialize out. Keeps most of your code pure +1, but why is this called "'model/view/controller' style programming"?
https://nixos.org/nix-dev/2016-December/022386.html [Nix-dev] Introducing Nixpkgs Overlays
&gt; Ah, I understand: backtracking is as powerful as having an oracle telling us which clause we should use, so it's not really an inference algorithm, it's just brute force in disguise. In that case, the idea probably can't be saved by minor modifications such as "what if backtracking is disabled unless you explicitly ask for it?" or "what if we keep track of which macro clause led us to which equalities?". I think this behavior is generally a bad idea. Keeping enough state around to permit arbitrary backtracking would be rather expensive, and it would need to be very non-local. In the above example, the backtracking would need to happen after control flow has already left the dynamic extent of the `mac` syntax transformer. This would be extremely expensive and complicated to implement. GHC has a firm rule against introducing any backtracking as part of typechecking. I think Hackett should adopt the same. &gt; This is unrelated to the current discussion, but could Hackett perhaps not give an instance of Monoid for Maybe? My understanding is that Haskell’s `Monoid` instance for `Maybe` is controversial for two reasons: 1. It is inconsistent with the `Alternative` / `MonadPlus` instances: `Just x &lt;|&gt; Just y = Just x`, but ``Just x `mappend` Just y = Just (x `mappend` y)``. 2. The instance is `Monoid a =&gt; Monoid (Maybe a)`, not the more correct `Semigroup a =&gt; Monoid (Maybe a)`. This is merely due to historical reasons. I think the first point is highly subjective, and I find the difference between the instances sometimes useful. I believe /u/edwardkmett has often pointed out that the current `Monoid` instance for `Maybe` lifts a `Semigroup` into a `Monoid` (assuming the second point were fixed). That’s useful. For the second point, we can just fix that problem, since `Semigroup` is already a superclass of `Monoid` in Hackett.
Bravo Bartosz!
Good idea! I'll add that sort of information and see if I can get it linked in or otherwise written into Trac
Is this at all relevant to beginners?
As someone who recently-ish finished Category Theory in Context, what sections apart from those on topoi and enriched categories (the only topics in the ToC not discussed in that book) are still worth reading?
1. is Conor's favorite talking point, but it runs directly into Conal's favorite talking point about lifting arbitrary monoids pointwise into your applicatives, so I'm content to let them fight it out. Frankly, I don't think Conor's position is defensible as far more data types are Monoids than are Alternative. 2. is being fixed in 8.4!
What are the necessary prerequisites? Edit: more specifically, assuming that I did my undergrad in computer science and some experience programming in Haskell, how accessible will this book be for me?
Yup that's right, as far as I know.
I'm not sure exactly what you mean with beginner, but if you are just getting started I would not say that it is relevant. I have dabbled in Haskell for almost a year now and I grok all the functors, monoids, monads and lenses etc, and I found the mentioned lectures very helpful and interesting while learning! If you feel that you are familiar with some of the mentioned type classes and are intrigued by the underlying theory then it is absolutely for you. Of course one can be very proficient in Haskell with out any of this, but knowing the theory has helped me reason about what data transformations my code actually represent better, and made it possible to extract code into type classes (different functors and monoids mostly), improving my code. Hopefully the formating is readable, I am on mobile so can't reply properly! 
alternative title: see how little people use your code ;_;
Oh this is really clever. Thanks for showing me, I hadnt seen this pattern yet. 
Nope. It uses the famous(?) rsync algorithm which uses something called a rolling hash to look for changes within each file. It is basically equivalent to hashing the file into chunks and comparing the chunks, except the chunks *continuously overlap* so that (e.g.) if a single byte is *inserted* it will detect that the chunks after it have moved and it won't need to resend them. It's a similar algorithm to what's used for deduplication. There's a great talk by the author of the program that goes into some detail... http://olstrans.sourceforge.net/release/OLS2000-rsync/OLS2000-rsync.html
This notation didn't make any sense to me until I learned a little about dependent typing. The `=&gt;` notation is, morally speaking, *exactly the same as `-&gt;`. The `Num a` on the left is an **argument** to the function. It is a data structure -- a record -- containing all of the `Num` methods implemented for the type `a`. The only difference is that this argument is passed implicitly by the compiler. You can think of it like this: 3 + 4 translates to (+) intNumInstance 3 4 where `intNumInstane` is an object of type `Num Int` which is created for you automatically by the `instance` keyword and contains the machine code for how to add, subtract, and multiply integers. (Obviously, this isn't how GHC works, but it is a valid semantics). I think there is a good talk and set of slides on type classes that explains this in more detail, but I can't find it at the moment. Google might be helpful for finding it.
As a further reference on type classes, one can also check Simon Peyton Jones's talk, "Adventures with types in Haskell", from [Oregon Programming Languages Summer School 2013](https://www.cs.uoregon.edu/research/summerschool/summer13/): [videos](https://www.youtube.com/watch?v=6COvD8oynmI) [slides](https://www.cs.uoregon.edu/research/summerschool/summer13/lectures/ClassesJimOPLSS.pdf)
The original paper on type classes remains so far the best introduction that I read. It is very approachable and short. http://homepages.inf.ed.ac.uk/wadler/papers/class/class.ps.gz
&gt; (Obviously, this isn't how GHC works, but it is a valid semantics). Actually, that's exactly how GHC works =P It just very often gets optimized away by a combination of the simplifier and specializer.
You can also watch his lectures, which are based on this material on youtube in a huge series of very good videos that are a good suppliment. https://www.youtube.com/watch?v=I8LbkfSSR58
I'd be quite interested in the source for that, as I've experienced similar compilation slowness with bookkeeper and rawr AFAIR. !RemindMe 1 week !RemindMe 1 month
I will be messaging you on [**2017-09-07 22:46:31 UTC**](http://www.wolframalpha.com/input/?i=2017-09-07 22:46:31 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/6x6h7p/type_level_merge_sort/dmee5eg) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/6x6h7p/type_level_merge_sort/dmee5eg]%0A%0ARemindMe! 1 week) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dmee636) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
It means the left side establishes constraints on the right side. `a -&gt; a -&gt; a` is fully parameterically polymorphic, any type can be used for `a` (but, of course, only "one at a time"). `Num a =&gt; a -&gt; a -&gt; a` is constrained parametrically polymorphic, a `Num` instance for the type `a` must exist. Haskell only has type class constraints, also known as a "context" in the report. GHC also has type equality constraints, and constraint kinds for composing your own constraints from simpler ones or otherwise manipulating them. In this case `(+)` is a type class member. However `sqsub a b = (a + b) * (a - b)` also has type `Num a =&gt; a -&gt; a -&gt; a`, but it is clearly not a type class member. If it helps, you can think of the left side as an extra, implicit argument.
You might like to have a look at this paper: http://benl.ouroborus.net/papers/2011-stencil/stencil-haskell2011.pdf
I mean the downsides of programming in Python would outweigh this small benefit. You can use records to get names parameters with defaults but admittedly in this circumstance the extra parameter needed to store the bundle of optional parameters is just as annoying as just having the parameter be required.
pandoc code is good for exploring, is a mid-sized project with easy readable codestyle, it help me a lot with my learning.
That's like asking why car manufacturers do not imitate airplane designs. Some things are so fundamentally different that it is simply impossible. Take for example nullable types. Despite the obvious desire to bring them into mainstream languages it is not possible to replace the existing behavior due to billions and billions of lines of legacy code. 
Purescript has much of the same capabilities as Haskell, and has a nice bidirectional type system. Also, in defense of Rust, it has higher ranked trait bounds, and trait-associated type constructors (roughly like HKT) has been proposed, they're just taking a gradual route to get there.
Seems like a highly ambiguous system. For instance: does `return (return 1)` desugar to `return 1` or `return (return 1)`? It'd be very hard to have the type system disambiguate this stuff. You need something more like idiom brackets; a syntax level indicator of what needs to be bound out and what should be passed pure.
It should be very accessible. It doesn't assume any Haskell at all, and barely even assumes C++. Knowing these things at an undergrad level is far more than enough
I tried running it through [this](https://epub.press/) to make an epub and it worked beautifully. Looks like they do mobi too.
I support convolutional neural networks in grenade, using the im2col trick to make the convolution operations fast.
I may also be young and naive, but I have the same problem. I want languages that enforce pure total functions (and even Haskell doesn't really live up to that, by default head still is unsafe, protolude is a good solution but I think we can do better). Haskell is as good as it gets at the moment. Ocaml, Erlang, F# etc. have a lot of the benefits but aren't nearly so strict when it comes to enforced separation of concerns (like Haskells IO monad etc). I'd love to be told that I'm wrong and XYZ language is sooo much better. 
I assume you mean the case where `return (return 1)` is the last expression in a do block? For the sake of argument, let's say it's a simple syntactic check (this is what applicative do and monad comprehensions do). So if the last line starts with `return` (or `pure`) it's left alone. If not, then a `return` (or `pure`) is inserted if the return type is a monad (or applicative). For function arguments it'd be sort of like idiom brackets, but it would take type information into account so instead of always assuming that `f a b` is `f &lt;$&gt; a &lt;*&gt; b` it would look at the types of `f`, `a`, and `b` and might alternatively desugar to one of: `f a b`, `f &lt;$&gt; a $ b`, or `f a &lt;$&gt; b`. Maybe idiom brackets do this already... not sure. I should probably look into them more. Here's an example: let r = crazy-do let a = 1 let b = pure 2 let c = return 3 f a b c would automatically desugar to: `let r = f 1 &lt;$&gt; (pure 2) &lt;*&gt; (return 3)`. In this case the `return` can be omitted because we can use `fmap`.
https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/idiom.html
Really not sure why people bother with perfectly dividing the lists... your goal is 1 element lists, just do that!
Because then `f a b` is ambiguous. Is it regular application, or bound application? Some functions take monadic arguments, some don't. That said, I think `&lt;$&gt;` and `&lt;*&gt;` are more than important enough to us that they should have single symbol aliases. Perhaps `#` and `&amp;`, which are each immediately to the left of `$` and `*` on the keyboard. In general, polymorphism makes it very easy for inference to become impossible. `toEnum . fromEnum` is easy, `fromEnum . toEnum` is not, and this is like the latter.
Although the two type systems differ in a vast variety of ways (a full discussion of which is off topic), Scala allows you to have (incoherent) higher kinded type classes, so you can program with `Functor` `Applicative` `Monad` and so on. It's not as nice as Haskell for several reasons, but still far better than the rest of the mainstream (Scala is definitely more mainstream than Elm or Rust)
There are, but they're super niche compared to Haskell, which is already super niche. We are, after all, talking about a parent language with a ~0.5% penetration in the programming market last I checked. Being amazing and being popular are orthogonal.
I'm confused about what's ambiguous. Here's my intuition: if I try to apply an `m a` to an `a -&gt; b` then obviously I must want `&lt;$&gt;`. Alternatively it could be a type error, but this seems like a case where it seems pretty clear what I want. Same if I try to apply an `m b` to a `m (b -&gt; c)`: it seems clear that I want `&lt;*&gt;`. So how is `f a b` ambiguous? If we have an `a -&gt; b -&gt; c`, `m a`, and `m b` then it means `f &lt;$&gt; a &lt;*&gt; b` -- what else could I possibly want?
Language design doesn't quite work like that, especially when you get into type systems. You can't just take a bunch of concepts, glom them together, and have a working system in the end--much less a *nice* system! Many seemingly small decisions have far-reaching repercussions, some of which aren't discovered for quite a while after they're made. Some features rely on some language attribute being present and others being absent. Some bring elegance or simplicity to one aspect of the language, but complicate another one. Haskell itself was the result of carefully choosing elements from a small family of rather similar languages that had grown up over the course of a decade or so in university projects and classrooms; the basic theory of the parts was well-understood and the creators consciously chose not to innovate much in building their language. It only really became practical outside of academia over the course of another decade or so of gradual refinement due to the efforts and study of many. There is quite a bit of distance between having a general understanding of the concepts that go into a language and its implementation and being able to fully design and implement one, especially one intended to compete in today's language landscape. And when you do, some "stupid kid" will come along and question all your decisions, wondering why you didn't incorporate the insights of their favorite language into your masterpiece. :)
Ok, that's pretty close. But why do we need the `~` notation for pure values (why can't it be inferred) and why do we need the `@` notation for splicing computations? If I have a computation with side effects in another language it's not ambiguous where I want the effects inserted: I want them inserted at the call site. Why can't we just make that the default, with no special syntax, then add some additional notation to do something *different* than that (or tell people to not use this special notation for that)? In other words: why can't we have an impure syntax that treats computed values with effects (`m a`'s) and pure values (`a`'s) the same, syntactically, in bindings and applications? Idiom brackets also can't support monadic sequencing. They only work with applicatives. So I can't do `do { a &lt;- f; b &lt;- g a; return $ h b }`. I believe that even this can be inferred from a notation that looks like: let a = f let b = g a h b In other words: a syntax that looks like ordinary imperative code. The only thing new here is a dependency graph between let-bindings to figure out where we need to use `bind` vs. the applicative functions. That work has already been done for Haskell as part of the applicative-do notation extension.
It's also pretty easy to write an *unstable* top-down merge sort (the OP mentioned that approach as an alternative). That would be just fine for `Symbol`s, but for other purposes it wouldn't be so hot. Writing a stable top-down merge sort gets into the length-dividing mess. As for *why* one would write a top-down merge sort, I think a pretty good answer is *parallelism*. That, of course, is quite irrelevant here.
If you want to parallelise a merge sort below O(n) you simply can't use a linked list (if O(n) is fine then bottom up works). Do we have type level Vectors? Is that a thing now?
You don't really need dependent types for that (although maybe it could help to understand them? Not sure). As /u/ElvishJerricco says, this is how GHC works internally. You can write this sort of thing by hand as well (no dependent types necessary). Here's an example: -- "Type class" -- data MyShow a = MyShow { myShow :: (a -&gt; String) } -- Sidenote: `myShow` has type `MyShow a -&gt; (a -&gt; String)` -- "Instances" (in this setting they are aka "dictionaries") -- intMyShow :: MyShow Int intMyShow = MyShow (show :: Int -&gt; String) -- Explicit type annotation for clarity -- This is like instance Show a =&gt; Show [a] where ... myShowList :: MyShow a -&gt; MyShow [a] myShowList dictionary = MyShow (\xs -&gt; "[" ++ go xs ++ "]") where go [] = "" go [v] = myShow dictionary v go (v:vs) = myShow dictionary v ++ "," ++ go vs -- A short example usage -- showTheList :: MyShow a -&gt; [a] -&gt; String showTheList dictionary = myShow (myShowList dictionary) showInts :: [Int] -&gt; String showInts = showTheList intMyShow example :: String example = showInts [1,2,3]
One barrier to Haskell adoption is how difficult it is to learn. The advanced type system is a big part of that. Language designers are probably unconvinced that the advanced type system features are worth the cost in complexity to the language. Now, when you're on *this* side of the fence, you know that's silly. Rust has macros and syntax sugar for `Result` and just introduced special case compiler support for generators/coroutines, when they could have just introduced monad syntax and accomplished much the same thing. Elm's lack of type classes (or other means of polymorphism, I guess let's not rule out ML modules or Java interfaces) cripples it as a general purpose language. The Java community is arguing about whether `Option` is *ever* a good idea, or if they should include tail call optimization in the language. "I had too much stuff. My machines came from too far away."
I proposed [something like this](https://github.com/ghc-proposals/ghc-proposals/pull/64) (still with explicit indicators for effects) and started looking into implementing it, but there’s no consensus yet on whether it’ll end up in GHC. For what it’s worth, the Simons seem cautiously in favour, but anyone should feel free to go thumbs-up or thumbs-down that proposal so I can gauge the level of interest.
Well Erlang is probably a poor example. The language is dynamically typed, so the language isn't strictly typed in any way, let alone the much more strict and total sense about which the poster and you are talking. I would love to see a statically typed language on the BEAM though and totality is something I want, but it isn't something I'd expect any time soon. The BEAM just isn't built to include it or make it easy to implement. I'm also a new guy to programming (~ 1.5 years), but I do use Erlang/Elixir primarily and would love to develop a new variant of the BEAM that allows that stuff easily, instead of hacking a solution together on the existing platform. The existing stuff is hands-down great for its purpose, but I think it could be better. I just have no idea how to go about doing it, especially without completely breaking the huge projects built on a dynamic type system that wouldn't conform to a static type system.
For your specific criteria, I think Agda might be good. I don't have any experience with it personally but as I understand it, it's basically Haskell but total and with more tools for ensuring correctness programming (like dependent types). 
This work is amazing! when dependent Haskell is arriving in GHC?
Fair points. I mentioned Erlang because of it being functional, thanks for clarifying Erlang's type situation.
Thanks I'll have a look. I've heard of it but know very little. 
For further clarification, Clojure and most Lisp dialects are also dynamically typed and fit into the category where it just doesn't apply either, for better or worse.
Blog post from the researcher spearheading this effort: [Dependent types in Haskell: Progress Report](https://typesandkinds.wordpress.com/2016/07/24/dependent-types-in-haskell-progress-report/). By his estimates, less than a couple of years.
This may not be useful criticism for you because it's pretty general, but I'm against all syntax-only extensions to GHC for the next few years. Haskell starts out with pretty heavyweight syntax, and GHC Haskell keeps loading on more and more stuff to the point that [things are really starting to creak](https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/glasgow_exts.html?highlight=datakinds#distinguishing-between-types-and-constructors). This isn't good from either a correctness perspective or an adoption one (especially because it's really hard on editor tooling people), so I don't think we should continue it. EDIT: /u/maxigit has an even better argument I hadn't thought of: https://www.reddit.com/r/haskell/comments/6xa7jg/why_cant_haskell_have_nicer_syntax_for_impure/dmf6d07/
That’s totally reasonable. I mainly proposed it because: * There are occasional questions here, SO, &amp; elsewhere asking for such a feature, and the answer is always “idiom brackets (SHE) and !-notation (Idris) exist but we don’t really have anything like those” * It fits neatly into a spot in the grammar that has sat unused for a long time—I think this is a crucial point for syntax extensions, not treading on anything * Syntax extensions that are considered “natural”, even if they tread on existing syntax like `DataKinds`, have gone through recently without much apparent fuss * I think something of this nature should have been added to Haskell earlier on, when monadic `do` notation was added (although to be fair it’s much more tractable post-AMP) 
Where one can track the progress of implementation of DependentHaskell in GHC? I was checking up Trac https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell, but I don't see on which branch/fork the work is done, or which tickets (only https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell/Phase1) are responsible for that.
This is great, thank you! Can't wait to get into it! 
I love it. Obviously a more modest proposal, but probably more palatable and definitely more well-considered. Have you thought much about Simon Marlow's suggestion to convert directly to applicatives? I think it solves a bunch of your problems (e.g., you could convert under conditional branches, your scoping problems are resolved, and you wouldn't have to worry about reordering let bindings). I really like your `&lt;-` syntax, too. I think it captures an intuition that will be easy to communicate to newcomers: the arrow points to where your side effects go. On that note, I was thinking as I read about how it's considered a code smell to have side effects in the sub-expressions of a function application. It's generally considered a bad idea. Interestingly, the applicative / monad separation in Haskell indicates when doing so is safe. This is probably well known, but it just occurred to me. I'm still curious whether you think it'd be possible to infer the operation and get rid of the `&lt;-` completely.
&gt; which are each immediately to the left of $ and * on the keyboard. Which keyboard? Not mine!
 I'm not sure what the actual technical limitations are for this, but I'm a little concerned about how new people would approach this kind of syntax. I know there is a small issue in the Elixir community where people coming from Ruby try to write Ruby in Elixir when they should actually write functional code instead of imperative OO code. And that's just from having the same general syntax. I'd say that offering this option really blurs the line between what is normal in Haskell and what is normal in Imperative languages. That may not be a bad thing at first, but people may have a problem understanding the more complex things of Haskell if they learn to approach things from this basis. I don't think there's a clear transition from this kind of understanding of the language to one that requires defining a custom Functor, Applicative, Monad, or something further down the rabbit-hole. I am just speculating though and could be completely wrong. I actually really hope I am wrong, I'm just playing devil's advocate a bit since I am new to the language and I'm just reaching the Monad Transformer understanding stage. I want more people to use Haskell and I think the definite break between the Imperative or other Functional languages and Haskell is helpful in understanding the concepts involved, but if a simpler syntax for using these concepts allows for a greater accessibility to the language then I'm happy.
I must say that I absolutely *hate* the syntax in that proposal. It's *unbelievably* noisy for very little real gain. (Idiom brackets, I'd probably be fine with. At least you're not littering the noise *inside* expressions there.)
You mean car is easy for people to buy. But airplane is not.
There's [smtp-mail](https://hackage.haskell.org/package/smtp-mail).
Just to be clear, by “noisy” do you mean that the `&lt;-` is too long, that it requires parentheses sometimes, that each effect is marked, or something else?
It's the presence of "&lt;-" and the fact that it's up against other symbols (parens). Marking of effects is a good thing, but it really is just the aesthetics of having the "&lt;-" symbol there. IMO something like "!" would be better. Though it'd still take some getting used to, I don't find reading Idris code *that* bad, compared to if it used "&lt;-". It's *possible* that syntax highlighting might alleviate this, but I'm doubtful. (Overall, I actually generally dislike this feature since it makes code needlessly dense, IMO. Disclaimer: I haven't used Idris, so I might grow to actually like the feature with usage.)
`!` unfortunately isn’t possible because it’s a valid operator (which was already annexed in pattern context for `BangPatterns`), but it could certainly use something else. Unfortunately there isn’t much ASCII real-estate left and `&lt;-` just happened to be available (and mnemonic). It could also be changed to have *higher* precedence than function application, like `!` in Idris; I just felt that might be confusing, but it would allow `f &lt;-x &lt;-y` (or `f ←x ←y`!) instead of `f (&lt;- x) (&lt;- y)`. Then again, the version with parentheses seems less visually dense to me, so it’s a tradeoff I guess. Thanks for your thoughts. :)
Yeah, I realize ! is taken -- what about | (pipe)? Seems a bit odd, perhaps, but I don't think it's taken in an expression context? (Or maybe I'm missing something obvious). Anyway, I *do* think that `(&lt;- x)` is better than allowing `&lt;-x`.
Yea, I hear what you're saying. Here's my perspective though: I sort of see Haskell as two languages. A pure monadic-style functional language and a mini-language that makes composing monadic operations more natural (do notation). I love the pure language. It's got a lot going for it. But I feel like do notation is extremely limited. When I start a do block I'm usually thinking about splicing together effects and sequencing computation, but I'm forced to constantly think about monads and monad stacks and lifting and mapping and binding and whatnot. Meanwhile there's a loooooong history of languages that people use to sequence computation pretty effectively. Why not take some queues from them? The distinguishing characteristic of those languages is that they treat effects as behavior, not values, so they don't show up in types. Pure functions and functions producing effects can be treated uniformly. Ordering of computation is given by the language's (probably not formal) semantics, and hopefully indicated by syntax. Back to Haskell: when I've gone into "do notation" mode I've decided that I don't want to be treating effects as values any more. Instead, I'm thinking about ordering effects by composing monadic values produced by various functions. By ordering the effects I should be able to discard them and gain easy access to their underlying values. In other words: I've entered a mode where I'm no longer thinking about the difference between pure functions and functions producing effects. I want to be able to treat them uniformly. You're probably right that this would be a potential trap for newcomers. They might get stuck in this mode and never learn how to write proper Haskell. I have two counterarguments: - This might help ease the learning curve and get more people into the language more smoothly. The symbol soup for composing these things makes Haskell code impenetrable for newcomers. There are no clues to what `&lt;$&gt;` does from looking at it: it's not even clear if it's a function or language syntax. It's so polymorphic that even once you've figured out that it's a function, looked at the type, and even looked at the source, there's *still* a good chance you don't understand what it's doing. Someone has to tell you in english. All that for application. - This is still a feature with limited power that you have to opt into. And it's still pure. I don't see how a polymorphic apply and implicitly binding a monad as any less functional than a polymorphic `+` and implicit conversion `fromString`. So I'm not advocating anything impure (I don't think). Ultimately, it's clear to me that imperative languages make composing effects more natural. Unfortunately, they also make the order of effects harder to reason about. I think there's a compromise position between the everything-is-explicit syntax in Haskell vs. what people are used to where they can ignore the mathematical nuance of different sorts of applications and compositions of function-like-things and argument-like-things. I guess this is a question of whether Haskell wants to "avoid (success at all costs)" or "avoid (success) at all costs" ;). Or maybe my intuitions here are just all wrong :).
The counterargument is that the existing syntax forces you to needlessly name a variable, or to have the same density *and* a bunch of symbol soup. The choice of symbol is kind of nit picky, frankly. I feel like it's something that'd be easy to get used to, and it's pretty well constrained by what's already been claimed. As I said above, it's also nicely aligned with the existing `&lt;-` inside of do and can be given a nice memorable intuition: the arrow points to where your effects go.
Using `|` would overlap with `MultiWayIf`, and maybe other things. At the moment, the proposal specifies that the parentheses aren’t *always* required: you can write things like `function $ &lt;- action` or `if &lt;- condition then … else …` because it’s at the same precedence as `let`, `do`, `\`, and so on. So with `|`, after `if |` you don’t know exactly what you’ve got until you get to `-&gt;` or `then`. Dunno if that’s a problem. It looks like all the single-character options are taken by fairly common operators, existing notation, and extensions. `@` would have been great—I think it was already illegal in expression context—but of course `TypeApplications` just snagged it. `&lt;-` was the best I could come up with. 
I maybe should have written down the specs, sorry about that. In a nutshell: when creating the channel an upper limit to the number of unread messages is given, if that limit is hit producers will have to wait until consumers read from the channel. 
Very nice. Is this going to be pushed back into the released Haskeline? I love Brick and good integration with command entry library would be great.
http://benl.ouroborus.net/talks/2016-HIW-Escape.pdf https://youtu.be/wG8AErq6Bbo
I don't, but the api is pretty straightforward.
I'm not really speaking about modifications to languages but entirely new languages. I recognise that changing Rust's traits entirely, for example, would break lots of legacy code, but I don't understand why higher-kindedness wasn't implemented *from the start.*
Haha, good point! I'll keep that in mind, thanks!
Scala? It's not pure, but it actually has had a weird form of dependent types for a while. Implicits aren't confluent, but neither are Haskell type classes if you throw in enough orphans. It's not pure, but the type system in Haskell is mostly separable from the purity. I think C++ got HKTs in the 2011 standard. The syntax is still very much C++, but the capability is there. It's easy to add more libraries to a language, and only a little harder to remove them. It's hard to add syntax to a language, and nearly impossible to remove / reuse it (See `auto` in C++). Changing the type system / theory for a language is a much "deeper" change than even syntax.
Ah, right, hadn't thought of MultiWayIf. Shame :(
Giving things names usually makes code more readable for me[1], plus it makes the ordering of effects completely explicit rather than implicit. [1] Obviously it's possible to go *overboard* on this, but...
It's more ambiguous if `a -&gt; b` is polymorphic. For example, `show :: v -&gt; b` might have instances for both `v = a` and `v = m a`. In the case of `m = IO`, the gut intuition is "of course, `show &lt;$&gt; act`". But what about when `m = []`? What should `show [3]` return? `"[3]"` or `"3"` ? (this is a variant of https://www.reddit.com/r/haskell/comments/6xa7jg/why_cant_haskell_have_nicer_syntax_for_impure/dmeismm/) edit: I think I mean `"[3]"` vs `["3"]`
Let's kickstart a physical book! &lt;3
Every one of those languages has another primary concern: Rust is concerned with compiling for real time stuff. Scenarios like polymorphic recursion complicate their story for how to deal with compilation, reason about resource consumption, etc. They need to know that eventually all the generic nonsense goes away. They want to be a better C++. Swift is concerned with basically being a better Objective-C. They need to tie themselves to the awful reference counting semantics of the underlying platform they need to run on, and a largely imperative developer set. Elm is concerned with not making things too complicated for users coming from javascript. Typeclasses are too hard for their users, so they actively push users who want those features over to Purescript. They want to be a better javascript Yes, they are all inspired by Haskell to some extent, but they all have some other overriding concern that rather cripples their ability to port ideas from Haskell.
But you can use something like [Kotlin](http://kotlinlang.org/), which runs on the JVM, and is [intended to be very easily integrated with your legacy code](http://kotlinlang.org/docs/reference/java-interop.html), but goes so far as to make [it's avoidance of the Billion Dollar Mistake](http://kotlinlang.org/docs/reference/null-safety.html) one of the first four bullet points on its front-page.
Maybe you are also interested in the answers to [this SO question](https://stackoverflow.com/questions/45923629/how-to-read-this-strange-type-signature-foo-sing-n-knownnat-n-r-r).
You may want to be careful about the partial field accessors you have defined here: data Response_AddOrder = Response_AddBid { _resAddOrder_trades :: [Trade] } | Response_AddAsk { _resAddOrder_trades :: [Trade] } | Response_AddMBid { _resAddOrder_mbidRemainder :: Maybe MBid, _resAddOrder_trades :: [Trade] } | Response_AddMAsk { _resAddOrder_maskRemainder :: Maybe MAsk, _resAddOrder_trades :: [Trade] } deriving (Eq, Show, Generic) What I mean is that ff you use the ` _resAddOrder_trades` accessor or a value built with one of the other constructors you will get a run time exception, which is probably something you want to avoid in something like an exchange. 
&gt; Swift is concerned with basically being a better Objective-C. That's an important observation, and it's very common. I think that many programming languages designers are very biased. This bias could be oversimplified to "Objective-C is currently the best language and I'm making something better hence my new language will be the best". I'll add a few more examples to this trend: * D: A better C++ * Go: A better Java * Dart: Better than JavaScript * Ruby: Python with curly braces * C#: Java, but only for Windows 
Man, you're overcomplicating it. do a &lt;- ma f a is sugar for `ma &gt;&gt;= (\a -&gt; f a)`, and do ma mb is sugar for `ma &gt;&gt; mb` It doesn't need to be any more difficult. --- I don't really understand your comment. There is no such thing as "functions producing effects". There are just functions. All functions map an input to an output and do nothing else. 
- Purescript: A better Haskell - Scala: ?
Any possibility of convincing Bartosz to put this on LeanPub or similar so that I an throw money at him? I also prefer PDF for various reasons. Either way, I've been pretty excited about this series, even reading a few of the earlier chapters before deciding to hold off until it was complete. It's up next on my list of technical books to read not related to grad school.
How to properly write a cabal file for a multi module project with shared dependencies and modules depending on each other? I'm coming from Scala and SBT and multi-project build is pretty nice to have. I'm sure this can be defined in cabal file when using Stack, but not exactly sure how.
Also, how to mix and match packages from Hackage and Stackage when using a cabal file with Stack?
Much improved. Still some vocabulary I don't know, but your highlighting them (Layer for instance) gives me something obvious to look up. Thank you, sir!
[IHaskell](https://github.com/gibiansky/IHaskell)! It can be fiddly to set up and doesn't support Windows, but if you can live with that it's excellent for this.
Stuff my not be ambiguous if the code is correct, but things get really complicated when the code doesn't typecheck. When `f a b` doesn't is it f , a or b which is wrong ??? What's wrong with `liftA2 f a b` ?
I answered the question assuming that the original poster was looking to use NixOS as a desktop system. I have a different answer for people considering it for a production system. For example, a desktop user doesn't care whether or not they can recompile nginx with new flags
Thanks, I appreciate the support. :) Haven’t thought much about it, but I’m definitely going to try different desugarings to see how they work out. Desugaring straight to applicatives makes sense, especially when `ApplicativeDo` is enabled; and there can be some special cases like `join (f &lt;$&gt; x)` = `x &gt;&gt;= f` if it turns out to matter. I imagine one reason it’s considered a code smell to have `combine_things(side_effectful_thing(), another_side_effectful_thing())` is that in C and C++, the evaluation order of the arguments is unspecified. Whereas in Haskell, `combineThings &lt;$&gt; sideEffectfulThing &lt;*&gt; anotherSideEffectfulThing` always does the effects in a specified order, at least morally—for commutative monads you can’t observe the evaluation order of `&lt;*&gt;` so it’s safe to break `(&lt;*&gt;) = ap`. In practice I think people always expect left-to-right order. I don’t think it’s possible in general to infer where to “run” effects when you reify them as monads like we do in Haskell, especially when type inference is involved. The whole point of modelling things that way is to distinguish an action from its result because you care about the distinction when laziness is afoot. One problem with inferring this is that the cases overlap. If you had a class `Apply` for desugaring function application: class Apply f x where type Result f x :: * apply :: f -&gt; x -&gt; Result f x -- (1) instance Apply (a -&gt; b) a where type Result (a -&gt; b) a = b apply f x = realApply f x -- (2) instance (Functor f) =&gt; Apply (a -&gt; b) (f a) where type Result (a -&gt; b) (f a) = f b apply f x = f &lt;$&gt; x -- (3) instance (Applicative f) =&gt; Apply (f (a -&gt; b)) (f a) where type Result (f (a -&gt; b)) (f a) = f b apply f x = f &lt;*&gt; x -- (4) instance (Monad m) =&gt; Apply (a -&gt; m b) (m a) where type Result (a -&gt; m b) (m a) = m b apply f x = f =&lt;&lt; x The type family `Result` conflicts, and (1), (2), &amp; (4) are overlapping: in `a -&gt; b` in (1) &amp; (2), `b` could be `m b'`, overlapping with (4); and in (1), `a` could be `f a'`, overlapping with (2), (3), &amp; (4). So this would need to be somehow disambiguated by context, type annotations, or…an explicit operator! ;) And you only need one: (f &lt;$&gt; x) = pure (f (&lt;- x)) (f &lt;*&gt; x) = pure ((&lt;- f) (&lt;- x)) (f =&lt;&lt; x) = f (&lt;- x) 
Indeed, it's surprising how hard it is to find a function with a decent buch of references - but note that this is only Stackage. Probably one should include popular Hackage (or private) packages as well, especially applications. Libraries tend to avoid huge dependencies (for good reason).
What do you mean? A means of code beautifying to reorder functions according to usage? Or I'm missing a pun.
How different from Haskell can a copycat be without being either too different or not Haskell (with some a new extension, or just a new syntax wrapper)? Once you have decide to design a pure, lazy language with type inference and use space as the function application operator, there is not much left to design something else than Haskell. Strip off things from Haskell, then you don't consider it good enough, and if you want to add stuff, then it's probably better to create a new extension. There are things like hack, haxle though, which add for example type inference to PHP. They don't bring however all the power of Haskell.
Looks related to [servant-quickcheck](https://hackage.haskell.org/package/servant-quickcheck), but more general.
Thanks! Follow up question: what are he typical prerequisites for Category theory in general? From my very very limited knowledge I was under the impression that one should have some knowledge of Abstract algebra. Would I do well to read a book on the topic before reading this book?
I wouldn't say anything of the sort is necessary. This series basically starts from scratch. Though you might find more applications on your own if you knew some abstract algebra
Thanks!
Concretely I have a module in one of our projects that became huge and needs to be split up but since things depend on other things all over the place it's very hard to do. If there was a tool to sort the module so that a definition only depends on names defined before it, it would be trivial to split up. Of course, you can't really do that due to cycles, but any approximation would be a great start.
What do you mean by "a nice *bidirectional* type system"?
I doubt anyone would make the claim that PureScript is a better Haskell. The goal of PS is pretty clearly "A *subset* of Haskell optimized for a JS VM, avoiding a few of Haskell's mistakes."
&gt; I doubt anyone would make the claim that PureScript is a better Haskell. Not sure about that as it appears there's ["people saying it feels like Haskell should be"](https://www.reddit.com/r/haskell/comments/4cwctz/is_purescript_the_future_of_haskell/).
Well, that OP said he didn't know anything about PureScript, and all the comments disagreed for the most part =P
I'm using VS Code on my PC without any problems. Now I installed it on my laptop with Haskero and Intero, but I can't do Ctrl + P stack or anything like it. Also, no error messages. Did I miss anything?
Your first paragraph makes it sounds like you just have multiple haskell modules/files in a single project, which is feasible with a single cabal file. But your second paragraph makes it sound like you want to have multiple *libraries* in a single `.cabal` file. That's not possible, sorry. There may be only up to one `library` section per cabal file. However, [you can use `stack` in order to package several libraries in a single package](https://docs.haskellstack.org/en/stable/yaml_configuration/). +- stack.yaml +- library |\ library.cabal || src | ... +- library-additional-package-1 |\ library-additional-package-1.cabal || src | ... +- library-additional-package-2 |\ library-additional-package-2.cabal ... Whereas the `stack.yml` contains all those sub-directories in the `package` field: # the stack.yaml file. resolver: lts-9.2 packages: - library - library-additional-package-1 - library-additional-package-2 You can then build all of your packages with `stack build` from that directory. You may of course add `library` as a dependency in `library-additional-package-1` and so on.
Perhaps bidirectional in the sense of bidirectional type inference, but I'm not sure why I should care about the inference engine as a user.
Maybe you should have a look at [Ceylon](https://ceylon-lang.org/). It's not a Haskell copy, but it has a powerful and expressive type system.
Thanks! I think this is the most satisfying answer so far. &gt; one reason it's considered a code smell ... evaluation order of arguments is unspecified That's certainly the biggest reason. Even when it is specified, though, relying on arg evaluation order to order side effects is usually discouraged. It's an aesthetics and readability thing. It's harder for people to count nested parenthesis and whatnot, and it's *really* easy to mess something up during refactoring (e.g., by pulling out one function call and giving it a name). In other words, it's *really* fragile. The general rule I've always followed is that ordering of side effects flows down, not across. That said, nobody follows this rule 100%. I've never had a good intuition for the times I've thought to myself "eh, it's ok to do this here". But now that I think about it, they roughly correspond with things that are commutative where the order doesn't really matter (e.g., if `g` and `h` make HTTP GET requests I probably wouldn't mind `f(g(), h())`). The applicative / monad distinction puts some information about when this is ok into the type system, assisting with intuition, perhaps even enforcing the distinction. Where I would still be annoyed is with stuff like logging, where I really don't care about the ordering even though ordering technically matters. &gt; you care about the distinction when laziness is afoot Curious why you brought up laziness... Does the story change under eager evaluation? &gt; Overlap... Ok I think you've convinced me. I'm going to think about this some more though. Even though there's overlap, it still seems like there are some obvious "do what I mean" cases. One operator isn't so bad though. Lots of great stuff here! Thanks!
Pretty much none. If you are a haskell programmer that will do fine
Video lectures are incredible. Thanks for this
wow, that's pretty cool. I had no idea it was that sophisticated. Thanks for the info.
But it was implemented from the start. Just not in Rust but in other languages (Purescript, Idris, Agda etc) Your inquiry contradicts itself. On one side you are asking why **new** languages do not implement modern type system features (which is not true, they do). And in that same inquiry you are talking about **mainstream** languages lacking those features: &gt;Why haven't other (more mainstream) languages imitated Haskell's base type system? So your expectation is wrong. You want something new, bleeding edge, yet at the same time already mainstream. Becoming a widely used language does not happen in a couple of months. 
Haskell has wrecked me. After spending time with it and Scala to a lesser degree it's hard for me to take a language seriously when it's somehow less expressive than *Haskell* *98*.
And? What's your point? is Kotlin mainstream? No. Can already established mainstream languages like java or csharp make such a foundational change? No. 
&gt; Why haven't other (more mainstream) languages imitated Haskell's base type system? The primary reason is that most programmers from amateurs to professionals aren't functional programmers. Most programmers work and play with imperative languages and as such what they want from a programming language aren't necessarily the ideas that you find in Haskell. To be sure, a lot of small FP constructs have crossed into the IP (imperative programming) world but they're still framed within an ad hoc world of state mutation and manipulation. While some have carved out a pure FP subset from an imperative language that has borrowed heavily from Haskell conceptually, e.g., scalaz and cats within scala, they're still a very small community. &gt; I don't see how it could be a speed issue either; the way Haskell passes typeclass functions is cheap as-is, and with compile-time monomorphism it could be zero-cost. I'm not sure whether you're referring to run-time or compile-time speed. However, in either case GHC doesn't generally produce fast binaries nor is it that fast compiling even relatively small projects. And the tooling and scope of packages around GHC are generally lacking--driven by not having a critical mass of programmers using the language. &gt; Haskell finds a sweet spot between, say, Coq and Javascript The notion of a sweet spot between Coq and Javascript can by anything based on whom you ask. Coq is a theorem prover known mostly to a very small subset of mathematicians and some form of javascript is run at least billions (or some other astronomical number) of times a day. A programming language, rightly or wrongly, addresses a set of concerns faced by some subset of programmers at some level of proficiency. They're comfortable and productive in that language. It's going to hard to convince them that Haskell is the way to go due to its superior type system on that alone. As the technical merits of a programming language isn't the driving reason why it's widely adopted. 
You may want to give Idris or Agda a go
Ok I thought about this a little more. What happens if you provide an explicit operator to indicate when you *don't* want reification? I feel like that's the less common case... (Obviously I think this is unlikely to happen in Haskell, just a hypothetical.)
Site is down, can someone a copy to me?
The same thing that would be wrong with having to do `liftNum (+) 2 7`. Of course it's possible, but it's annoying... especially when you start composing stuff. It hides intent in a bunch of irrelevant scaffolding. As a human reading the code, I have a good intuition of what `f a b` means. Even if there are some ambiguities (like there are with `Num`) it feels like this could be a valuable addition to the language as long as there were simple and intuitive rules for disambiguation. I'm not sure that there are... and there might be unaddressable technical hurdles that I'm not aware of... but it seems like it'd be nice!
My problem with Option in Java is two fold: 1. Even *if* an `Option&lt;Object&gt;` can't contain `null`, it can *be* null, so it mostly just moves checks around. 1. If you can't have `Some(null)`, then `Option` isn't a proper functor. For `f = const "foo"`, and `g = const null`, `fmap f . fmap g` != `fmap (f . g)`, and this applies to any `g` that may return `null` and `f` that accepts a null argument. In C# you could do `Option&lt;T&gt;` as a structure type, to avoid the first point, but you can't really avoid the second point. You can argue that `Option` is a "morally" correct functor, if you treat `null` like `undefined`, but when interacting with the existing JVM / CLR ecosystem that's a good way to reason yourself into a `NullPointerException` / `NullReferenceException`
Scala is supposed to be the better Java.
Pure, definitely. Lazy is fine, but so is strict, or something else. Type inference is essential, and I'd actually like to see it infer Rank-2 types, too. Space need not be the function application operator, but I do want a compact syntax for partial function application. I enjoy Idris.
&gt; let's say it's a simple syntactic check Please no. Consistency is king.
&gt; I'm confused about what's ambiguous. Basically anything dealing with `m (m a)` values and sometimes things daling with `m (n a)` values (where both `m` and `n` have `Monad` instances).
&gt; All functions map an input to an output and do nothing else. And that's what makes Haskell pure.
I'm not sure I am. Here are a couple of the problems: - I have to come up with names for things that I don't need / want to name (so I end up with `g` and `g'` and `g''` and `gs`) - I have to know whether the thing I'm working with is an a or an ma even if I don't care (and even if my code could work with either) The distinction also reduces polymorphism, so instead of having one `map` we have `map` and `fmap` and `mapM` and `traverse`. If the one I want could be deduced from context that'd be awful nice! I'm sure there are cases where you might want a *particular* one, but that doesn't mean we can't make the 90% case simpler syntactically and more polymorphic. Maybe it's impossible, but I think it's a worthy goal! This distinction also reduces code reuse in the same way that calling an asynchronous function with continuation passing in an imperative language does: it's clear what you want both syntactically and semantically, but the language doesn't support it. Instead, you're forced to change the API of calling functions all the way up your stack to also pass their continuations. That's why promise libraries were developed in Javascript. Promises (or futures) are monads! But you still have to change APIs... they just make the change a little easier and more palatable. Now we have async/await. That actually solves the problem! What I'm looking for is effectively the same thing, but for monads. Side note: javascript doesn't infer when to `await`. I feel like it could do this inference though: `await` by-need. I'm not sure why it doesn't do this. I guess explicit `await`s let the programmer control synchronization points, but they could be optional... Side side note: /u/evincarofautumn's proposal below seems pretty similar semantically to "await for monads".
Agreed -- I don't think it's a good idea *in Java*. The impression I get from conversations in the Java community opposing it seem to indicate that they *like* having implicit nullability.
To a certain extent naming things improves readability. But `f (g a) (h b)` is *far* more readable to me than: a' &lt;- g a b' &lt;- h b f a' b' which is a very common pattern that seems like it could be addressed. The alternative of using `f &lt;$&gt; g a &lt;*&gt; h b` isn't much better, in my opinion. There's a ton of unnecessary cognitative overhead in parsing those weird symbols all the time. Then, once you're good at it, you just ignore them unless it's some sort of special case. If you're ignoring them for the common case then why not try to make them invisible for the common case?
PureScript's row polymorphism is the one clear place where the language is definitely better. Everything else is a question of standard library.
Mr. Milewski video lectures are amazing. I have no background in math or computer science but going through them was a breeze, so informative and exciting. I still joyfully remember his first lecture with that amazing motivation for the rest of the series. Thank you so much, Mr Milewski.
Stackage is, for now, as far as I know, a reduced mirror of Hackage. If a package exists on Stackage, then the same package, with the same version number, and the same code, must be on Hackage (modulo some of the "features" which allow Hackage metadata to be modified after a package upload...). Moreover, with the `stack` tool you can also add dependencies as, for example, Git repos. But yeah. You can't "mix" packages from the two, since Stackage is, by definition ("**S**table Hackage"), a subset of Hackage. 
&gt; If I have a computation with side effects in another language it's not ambiguous where I want the effects inserted Actually, often it is. In C, for instance, argument evaluation order is implementation defined (and sometimes a source of bugs). 
[Here's wayback machine](https://web.archive.org/web/20170617111644/https://epub.press/) of the site from a week ago, and [here](https://chrome.google.com/webstore/detail/epubpress-read-the-web-of/pnhdnpnnffpijjbnhnipkehhibchdeok) is the chrome web store page for it.
Yeah, I think only Scala has managed to get to a point where it's actually considered commercially viable by a lot of companies and at a same time it can have a lot of nice FP things via ScalaZ and similar libraries. I would love to see Haskell being used more (and thus having better tooling like IDEs [I am an IDEA user and HaskForce isn't best and breaks very often], module system without weird repetition [coming from JVM this is strange that I have to declare modules on various places, why can't it all do the compiler?], proper documentation for libraries [maybe I am too green, but I have not yet encountered a Haskell library which would have documentation on par with Java or JavaScript world] and so on).
In my practical experience with Scala, null pointer exceptions are very rare - I run into one or two a year on average. Admittedly, that's mostly because the Scala ecosystem doesn't use nulls; you'll usually only run into issues when you're using a Java library. Using option in java probably requires more work because of a lack of libraries that eschew null.
Ah. I'm not sure what new argument I could put forward to convince them. I prefer a more descriptive language, one where I can express non-nullability in a way that the compiler will check.
Okay, yes, this makes more sense now. This will be very useful.
Could you post the .epub? I can't get that extension to work (some JSON parsing error).
It's great but this pres is from 2009. In the meanwhile Hs adoption,tooling and libraries have improved a lot
I'd say the main difficulty with Rust coroutines is that they want it to compile into code that (a) does not trip over the borrow checker (b) is efficient and avoids allocations.
C++ had HKT(template-templates) since 03.
&gt; but I'm not sure why I should care about the inference engine as a user In relation to OP's, it's because it's a clear sign that the language is taking cues from current PL research, something that can't really be said of, say, Elm. Not a criticism, it fits their niche, but it's not what OP is looking for.
The creator of PureScript has said that its type system is based off of [this paper](http://www.cs.cmu.edu/~joshuad/papers/bidir/).
Somewhat related: ["Beware of readFile"](http://www.snoyman.com/blog/2016/12/beware-of-readfile).
So if I used bytestring, it could have been 40x faster? That sounds amazing! 
Yes, I suppose that is true. In that sense the only true solution here is [#11587](https://ghc.haskell.org/trac/ghc/ticket/11587).
https://en.m.wikibooks.org/wiki/Haskell/do_notation
Honestly I think its worth reading multiple presentations of a lot of things! You might get different examples, different insights, different proofs, or just pick up subtleties you missed. Give a section a try and see how you like the presentation style...
you can do it!
Check out the Haskell Report, [§3.14 Do Expressions](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-470003.14), which describes the desugaring: 1. `do { e }` = `e` 2. `do { e; stmts }` = `e &gt;&gt; do { stmts }` 3. `do { p &lt;- e; stmts }` = `let { ok p = do { stmts }; ok _ = fail "pattern match failure" } in e &gt;&gt;= ok` 4. `do { let decls; stmts }` = `let decls in do { stmts }` This desugaring is applied recursively until no `do` expressions remain. Briefly, (2) says `do { a; b }` is desugared to `a &gt;&gt; b` (perform `a`, discard its result, perform `b`); (3) says `do { x &lt;- a; b` is desugared to `a &gt;&gt;= \ x -&gt; b` (perform `a`, bind its result to `x`, perform `b` with `x` in scope) with some additional logic for calling `fail` in the case of pattern match failures; and (4) says `do { let x = a; b }` is desugared to `let x = a in b`. You can think of `&gt;&gt;=` or its flipped cousin `=&lt;&lt;` as a kind of function application for monadic actions. You choose different combinators for applying functions based on whether they’re actions you want to *run* or expressions you want to *evaluate*: f :: a -&gt; b x :: a f $ x :: b f :: a -&gt; b x :: m a f &lt;$&gt; x :: m b f :: m (a -&gt; b) x :: m a f &lt;*&gt; x :: m b f :: a -&gt; m b x :: m a f =&lt;&lt; x :: m b 
Have some code of this?
It does nothing!
The fact that C has an implementation defined evaluation order isn't really relevant. That's not a necessary language feature, just a C peculiarity. Further, C's evaluation order is only implementation defined for function argument evaluation. Statements are evaluated in source order (or at least any reordering has to preserve the causality relationship in the source, so evaluation order is not observably different than source order). So it's pretty much the same thing as `a &gt;&gt;= (b &lt;$&gt; c &lt;*&gt; d)` where `a` happens first, but the order of `b` `c` and `d` are undefined. Applicative-do actually seems to *loosen* ordering in Haskell to match c: binds may be re-ordered as long as re-ordering preserves causal ordering. Anyways, my point is that C programmers would just write this as `a(); b(c(), d())`. That's a natural syntax and, in the context of a C program, programmers know what that means. I don't see what's so bad about providing the same level of abbreviation in a `do` block. Unless it's just technically infeasible for some reason.
Eh, it's not perfect but it's an easy rule to remember and works in most cases. Like I said, there's precedence in GHC: MonadComprehensions and ApplicativeDo both use a syntactic checks for `return` or `pure`. Anyways, just a strawman. I guess here it wouldn't be semantics-preserving though... I know it's semantics preserving in applicative do, but not sure about monad comprehensions... it must be, but I'm not sure how.
Great talk! Thanks so much for sharing. I looked into information flow control a long time ago and this talk reminded me of the techniques there. If I remember correctly, IFC techniques use the same trick of tracking required capabilities on the subsumption rule. So these same techniques could be used to track who's accessing a "sealed" object, I guess? That could be neat. Basically you'd invert control of who can unseal stuff. Instead of the library deciding, the program can decide through type annotations. I had a couple thoughts / questions as I was watching: - Do any other languages do this? - What's the connection to free/freer monad? - Do you know if he ever got around to "dealing with continuations" (he mentioned that state + continuations gives you all possible effects, but he had only done state) - Curious about the incompatibility with lazy languages. Maybe that's what's preventing this sort of thing in Haskell? Anyways, I'll do my own research, but if you have any more info / resources you could share I'd love to learn more!
&gt; avoiding a few of Haskell's mistakes." so basically a better haskell.
&gt; ApplicativeDo I complained about it then, too.
Sorry, I learned on C++98, and the compiler at work is pre-standard C++. KDE 3 and Qt 3/4 didn't exactly use cutting-edge features. I think the only C++03 or better code I've ever even looked at is the source code for Paludis. So, I still think of C++ without vararg templates, template templates, compilers that crash on perfectly valid nested templates, lambdas, move semantics, etc.
I mean you left out a fairly critical part of my comment &gt; A *subset*...
This line: putMVar counter (maybe 1 (\x -&gt; max limit (x + 1)) mMVar) putMVar hangs because a writer thread manages to make the counter mvar not empty. if you change it to a non-blocking tryPutMVar it doesn't hang but I don't know enough about the problem to know if it will always work as intended.
it's not a subset. maybe it's mostly a subset.
i'm actually really curious now
-_- yes. I know that. The term was convenient to apply, because an overwhelming amount of the language overlaps with GHC Haskell, and many more of the unshared parts are unique to GHC. It would be more apt to say it *resembles* a subset of GHC, in that there's only a little that PureScript has that GHC doesn't have, but there's a lot that GHC has that PureScript doesn't have.
Try Text.IO.interact $ Text.unlines . etc. You might have to do 'System.IO.hSetEncoding ...IO.stdin ...IO.utf8' first. Or use 'cassava'. That's Thai, btw, not Chinese.
Short answer: `x &lt;- f` is basically ` f &gt;&gt;= (\ x -&gt; ...)` All statements in the do block are interspersed with implicit `&gt;&gt;` between them. `let` is pretty much the same as always except it doesn't need a corresponding `in`. The simplest way to think of it is to pretend that anything on the left of `&lt;-` "drops" it's outermost Monad type, and that you just do a bunch of things and return the last one. I found it super helpful to just spend like day or so writing everything in terms of bind and lamdas, and then refactor to do notation. It's one of those things that clicks pretty fast, even though there's a lot going on under the surface.
You really do like consistency :).
the things purescript has that haskell doesn't matters a lot to many people. if this is strictness, explicit partiality, anonymous records, row polymorphism, nice `_` syntax and others. Haskell does a lot of things right and purescript copies from it, but it does not stop there and tries to improve on it. I'm trying to drive the point that purescript did not stop at Haskell and have done enough changes to be deserves to be called a language in it's own right and not just a haskell clone or subset. and some people do think that as a language it might be a "better haskell".
&gt; I've heard they take mercy on professional broke folks who want to learn Haskell :) Yep.
Why just professional broke folks? What about the unprofessional broke folks?
What would you recommend as a practical introduction to monad stacks/monad transformers? I don't really know what I'm looking for as a beginner who wants to be productive. :s
You're overcomplicating something, whether it's your mental model or your explanations. &gt; - I have to come up with names for things that I don't need / want to name (so I end up with `g` and `g'` and `g''` and `gs`) The solution to this is pointfree style. I don't see how this is a problem with `do` notation. &gt; - I have to know whether the thing I'm working with is an a or an ma even if I don't care (and even if my code could work with either) Something of type `a` and something of type `Monad m =&gt; m a` are conceptually different. It would be a mistake to try to unify them. &gt;The distinction also reduces polymorphism, so instead of having one `map` we have `map` and `fmap` and `mapM` and `traverse`. `map = fmap` and `mapM = traverse`. There's no dichotomy- `map` and `mapM` don't exist. &gt; ... The rest doesn't make sense. I don't see the connection to `do` synax. 
&gt; In my opinion, Haskell finds a sweet spot between, say, Coq and Javascript. I'm pretty sure the other languages you're mentioning as not being as advanced / good as Haskell also think they've found a sweet between X and Y. ;)
 import System.IO main = do let utf8' = mkTextEncoding "UTF-8//ROUNDTRIP" -- make output match input even for invalid utf8 hSetEncoding stdin utf8' hSetEncoding stdout utf8' interact $ unlines . map (intercalate "," . convert . splitBy ',') . lines
Yes, share info plz 
Well we'll have instance chains very soon, hopefully, so that'll be at least _two_ improvements :P
Based off a few papers, in the sense that I read them before writing the type checker. I took a few ideas from a few different papers. We use skolemization, for example, which is one thing which doesn't appear in the complete and easy paper. 
I would suggest forking the repository, then try to get it to build using whatever build tool you care about. You will need to bump version bounds and likely fix the resulting problems that arise with a new version of the compiler. This is *not* going to be a "changed some bounds, worked OK" fix.
Point-free style with monads and applicatives means using the symbol soup I'm trying to avoid. I don't agree with you that `m a` and `a` are conceptually different. At least not always. Lots of languages have features that have a monadic denotational semantics without forcing you to use a special syntax for application. &gt; The rest doesn't make sense. I don't see the connection to do syntax. `async` is a lift into the `Promise`monad, which speculatively executes the operation asynchronously (in a new task or thread or process or something). `... (await x) ...` could be characterized by replacing the `await x` with a fresh variable (let's call it _x_) and doing `x &gt;&gt;= \_x_ ..._x_...`. The links from /u/dtellerulam below are super interesting, and make the connection more clear. I think it's more obvious how `async`/`await` map to `run`/`box` in the linked material. There's also some indication of why this sort of system might be problematic for Haskell. It seems like in full generality it requires an effect system that's separate from the type system, which Haskell doesn't have, and it's incompatible with call-by-value. It also requires (some) annotations for type checking to be tractable (although I think only effect annotations). Anyways, check it out. It's super interesting: http://benl.ouroborus.net/talks/2016-HIW-Escape.pdf https://youtu.be/wG8AErq6Bbo The bang-notation in Idris and [this propsal](https://github.com/ghc-proposals/ghc-proposals/pull/64) for Haskell are sort of similar, but not quite the same. One of the Haskell implementors brings up bang-notation in the linked talk, and there's some clarification of the differences.
Cool, good to know!
03 and 98 are basically the same. But certainly lots of compilers back then choked on template-related things (or have bugs) that were long standardized in 98.
It lets you write monadic code in a way that looks like more familiar imperative code. It all maps to functional haskell expressions in an "obvious" way. 
I followed instructions in this gist: https://gist.github.com/expipiplus1/e571ce88c608a1e83547c918591b149f Couldn't quite get it to work (each time I start my terminal I'm warned that some nix directory does not have the right permissions). and then I realized that I don't actually need multi-user so I just followed the official instructions. Pretty much just `curl https://nixos.org/nix/install | sh` and add `source ~/.nix-profile/etc/profile.d/nix.sh` to my shell rc. Seems to work fine - I can `nix-env -i hello` and then run `hello` no problems :) EDIT: I'm not entirely sure whether there are additional steps needed apart from my steps above, as I have some lingering files (like the systemd service files) from attempting the multi-user setup :P This is why I want to use nix/nixos!
The word "myth" looks and feels weird now, thanks.
[It's on it's way](https://github.com/DanielG/ghc-mod/pull/904)
I mean what's wrong with `liftA2 f (g a) (h b)` if you really don't like `&lt;$&gt;` and `&lt;*&gt;`. Personally I think both of those solutions work great. I'd also be kind of opposed to such an extension, due to it complicating the language that bit more. Making it just that much harder to get new people into the language and also making it harder for another compiler besides GHC to be created. It also adds yet one more way to do the same thing.
Ah, I guess it does work... No, no type-level vectors, unless you mean length-indexed lists.
&gt; That's Thai, btw, not Chinese. Ye I know, but the .csv also contains Chinese letters and the sentence sounds better with 'Chinese' after all.
Hackage is your best bet when looking for Haskell packages. It looks like you're in luck. https://hackage.haskell.org/packages/search?terms=discord
https://lettier.github.io/posts/2017-08-30-haskell-gtk-video-player.html
&gt; Rust doesn't support higher-kinded types in its 'traits', Swift's 'protocols' are weaker still, and Elm's constraint system is... just awful. In the case of Rust, lack of enough developer time. Every project makes choices about what to focus on. &gt; we have languages like Idris and Agda that take those concepts even further to their pinnacle of type-data equivalence, though maybe a little further than necessary. I actually think dependent types will be big. Not everything has been sorted out yet, but many of Idris' rough edges will become more pleasant with time. 
Also strict rather than lazy. 
Thanks Bartosz. I owe it to you for "getting" Haskell and category theory.
Well, it's also strict and used in a different domain. 
Error messages, for one.
thank you.
&gt; One barrier to Haskell adoption is how difficult it is to learn. Programmers often get "easy" confused with "thing I'm familiar with". &gt; Language designers are probably unconvinced that the advanced type system features are worth the cost in complexity to the language. More often, they simply make a bad decision (for any number of reasons) and then it gets set in stone. 
Agda is pretty far from Haskell. Both in terms of how easy it is to be productive and also just general workflows. 
&gt; Being amazing and being popular are orthogonal. There are definitely hugely irritating problems with anti-intellectualism among programmers, but I think this is an exaggeration. Haskell is becoming far more popular, and it is penetrating the wider world - people are suddenly interested in monads, etc.
&gt; GHC doesn't generally produce fast binaries nor is it that fast compiling even relatively small projects. GHC isn't as fast as it could be, but it certainly produces fast code. &gt; And the tooling and scope of packages around GHC are generally lacking Haskell has the most fertile ecosystem in FP. Some of Haskell's packages are unique to it. &gt; As the technical merits of a programming language isn't the driving reason why it's widely adopted. Well, the technical merits (if they bear fruit) are precisely why Haskell is now becoming popular.
Oh wow. 
My intention is to make a PR to Haskeline, yes. Ultimately the decision is on the author. Since Brick introduces quite a few transitive dependencies, some consideration is needed. Currently the Brick part is hidden behind a package flag. I'm hoping that will suffice. 
I've been in a "Haskell job" at a company that uses Haskell as its main language for nearly 18 months and its incredible! My colleagues are smart, dedicated, motivated and enthusiastic. We have a great code base, great processes and a great culture. I have never been more productive or fulfilled in any other job in my life. One of the great things is that working in Haskell can make even relatively mundane tasks interesting. I really could not imagine working in a non-Haskell job after this one.
given amount of haskell jobs out there, it doesn't sound that great :)
This makes me really excited, but I'm still on OSX and that makes life harder when trying to build things. I made it to the point where haskell-gi installs (after setting where to find libffi), but when i ran: haskell-gi -o lib/gi-xlib/xlib.overrides -O lib/gi-xlib xlib I got this error: * Generating GI.Xlib haskell-gi: Could not determine the pkg-config name corresponding to "xlib". Try adding an override with the proper package name: pkg-config-name xlib [matching pkg-config name here] CallStack (from HasCallStack): error, called at lib/Data/GI/CodeGen/Util.hs:49:10 in haskell-gi-0.20.3- 9nwXY1I1260GlS1AHBdPUX:Data.GI.CodeGen.Util A `brew search xlib` doesn't turn up anything, so I'm not sure how to proceed. Out of curiosity would be interested in using Nix to help with the non-haskell dependencies?
Cassava. I can't speak to performance, but cassava has become my go-to for any csv problem due to the insanely simple ease of use.
There's the discord-hs wrapper in hackage that you've been told about, but funnily enough, I've also developed a Discord bot in haskell as a personal project, and it even supports a feature that the discord-hs wrapper doesn't, namely voice support. The code is rather messy (in some cases, very messy), not nearly as professional as discord-hs, as I'm nowhere near a haskell expert, but If you're interested, contact me, and I can share the files with you.
I'm also interested.
discord-hs is pretty good but there were major overhauls in the latest two versions and all the examples are obsolete. This was a while ago so I don't think people are actively working on it.
&gt; Also, as far as I understand monad is just a wrapper for any value, which has a bind operation defined for it. In the particular case of `IO` it's more like a "recipe that, when executed, produces a value, possibly involving side effects". For example, it would be wrong to think about `getLine :: IO String` as a wrapped string. The string doesn't exist until the action is executed and the user is asked for it! And bind means "once we are executing this action and we get its result, select the next action to perform using this function".
There is something about the language that makes impure function unviable, namely laziness. This means the the order in which things are evaluated is undefined - say you had a function `getStrLn :: String`, write `[getStrLn, getStrLn]` and the user inputs `1&lt;cr&gt;2&lt;cr&gt;`. The list might contain either [1, 2] or [2, 1]. However this is a blessing in disguise since this forced haskell to be one of few actually pure languages. Purity is incredibly useful in its own right because it allows you to reason about code by substituting equivalent bits which is called equational reasoning. This is what makes user defined rewrite rules that aren't incredibly fragile possible as an example of its power. So basically, IO defines the relative order of effectful actions which allows us to keep equational reasoning.
Relevant: [This](http://dev.stephendiehl.com/hask/) is a great list of things which are helpful to know sooner rather than later, entitled "WHAT I WISH I KNEW WHEN LEARNING HASKELL" It won't much help you on, say, your first few hours of your project, but after that the items listed are very helpful :)
The initial reason for purity was lazyness. Lazyness and side effects don't mix well together. Since you don't know in what order expressions are evaluated, handling side-effects becomes very difficult. If you do IO, you want the actions to be performed in a deterministic order. Monads are a way to separate pure (lazy) computations, and (strict) computations which perform side-effects. So you get the best of two worlds, lazy computations that evaluate only when needed, and strict IO actions that give a deterministic order of evaluation. While that was the initial reason, purity has also shown to have many other advantages, such as easier concurrency, better reasoning about programs, etc... Monads aren't necessarily wrappers (though they can be). In the context of IO, monads allow you to perform side-effects without the effect _escaping_ the IO action. The types are: `(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b` and `pure :: a -&gt; IO a`. As you can see, the result of these functions is always another IO action. So you cannot simply take an IO action, and extract a pure value from it. Turning a pure value into an IO action is no problem though. Really the only way to use these actions is to pass them to the `main` function, which does all the magic to perform the side-effects, pass the values to other actions and through pure functions. Well, there is also `unsafeperformIO`, but as the name suggests, you shouldn't use it unless you have a good reason, and _really_ know what you're doing!
Counterquestion: Why would you want a function of type `foo :: String → Int` to be able to display a pop-up window?
The entire language is built around the idea of a pure function, so yes, you could say that there's something inherent about the language that makes impure functions unavailable. Pure functions are cool because if you call them with the same input, they'll return the same thing, regardless of context. This makes them very easy to reason about. Don't think of IO as the IO monad - it's the IO *type*, and it happens to be a monad, but we don't strictly speaking need the monad instance. You can think of IO as a type that lets you ask the haskell runtime to please go interact with the real world on your behalf. The monad stuff is a convenient way to chain these requests. 
Ok, so AFAIUI you're arguing for a narrow-scoped avoidance of naming things? Unfortunately, the proposal doesn't mandate a narrow scope (or anything of the sort). (See also /u/Tysonzero 's reply.) &gt; There's a ton of unnecessary cognitative overhead in parsing those weird symbols all the time. Oh, please. Bigger cognitive overhead than mandating that everyone understand what the hell `(&lt;- x)` means? Get real.
Haskell doesn't need monads for IO in the sense that one could define it without the referring to the Monad typeclass at all. But programming with IO is inherently monadic: if you have a two programs which take input, do some computation, and then output something, you can hook them up so the output of one becomes the input of the next. This is equally true of Java, C or Haskell's IO. In a way, Java and C have *less* functionality than Haskell, because they can only compute "inside the IO type".
You got me at "Electron free"
It certainly is more enjoyable to work with a more powerful language, but at the end what you're working on seems to be a lot more important. If you're not interested in your work, then no language in the world will change this.
Lucky you, nowadays there are more Haskell jobs (even more than OCaml or Erlang I think). I'm waiting for Perl 6 jobs to appear someday in the future... :)
We are looking into it, but it will take some time ...
I'm pretty interested.
I successfully created ebook editions with the Chrome extension mentioned below (and changed the cover): [ePub version](https://www.dropbox.com/s/g3vlbpwpr2yu817/Category%20Theory%20for%20Programmers.epub?dl=0) [mobi version](https://www.dropbox.com/s/xpgpfdb18w04noa/Category%20Theory%20for%20Programmers.mobi?dl=0)
I quote an [earlier comment of mine](https://www.reddit.com/r/haskell/comments/6oybvj/demand_for_haskell_is_on_the_wane/dklo9h7/): &gt; I'd rather be almost broke and live on the cheap in the woods with my 4G dongle while doing some hobby Haskell dev—than make $150k/year in some soul-killing Java job in SF (where I'd probably still be living paycheck to paycheck anyway due to inflated real estate prices).
I'll pm the source to you two in about 5 hours. Sorry for the delay, but I'm busy until then.
Eg. the Debug.Trace.trace :: String -&gt; a -&gt; a function uses unsafeperformIO internally. It’s type suggests that it is pure (has no side effects) but obviously it outputting a trace string it is in fact impure. Outputting trace statements is an example of where you don’t want to mess with the pure/impure typing of your code but temporarily output effects.
I have definitely enjoyed my experiences, mostly working on simpler db web apps that have interesting nooks like a report definition DSL or using complex db libraries. I feel limited in options in Haskell generally. I used to work on a map drawing engine before switching to Haskell. The map drawing engine, although interesting with some unique algorithmic problems, had a lot of code that we couldn't change because our test coverage was low and the c# types didn't provide enough confidence to refactor without needing extensive regression testing. I haven't found many Haskell positions that use algorithms extensively or focus on high throughput. The ones that do exist, you have to compete with senior engineers throughout the US for. The availability of remote positions is both interesting in terms of more jobs available and daunting because of the potential expectations and competition when geography is no longer a filter on how many people apply. I wish there were more things like the Broad Institute or Azavea, but using Haskell. In Scala, if you don't like your company's culture or the job doesn't turn out as interesting as you expected, you move to another local Scala company. With Haskell, you have to decide whether to leave Haskell or start competing harder for remote positions. It is amazing how much less cruft there is. This allows you to think bigger in the features you build and techniques you use. Glance at LeapYear, Adjoint, Digital Asset, the Facebook static analysis and Haxl/spam groups, Sentenai, Superpowers Corp, Unison, or maybe TechStars for starting your own thing. Consider grad school.
Me too, thanks
Any open remote positions? :)
this page is 7th link in google search for "row poltmorphism", and none of the first 6 named as "row polymorphism is ..."
The error is telling you that GHC-MOD is incompatible with `base-4.10`, which is the new version of `base` that comes with GHC-8.2. And indeed, since GHC-MOD works very intimately with GHC, this is not merely noise caused by overzealous usage of dependency upper bounds -- GHC-MOD really is incompatible with GHC-8.2, as others in this thread have pointed out. No, it is not true that "you're not supposed to mix `cabal` and `stack`." if you're trying to install a program, there's really nothing wrong with creating a `cabal` sandbox, and then `cabal install`ing the application on to your computer, especially for those packages which are incompatible with `stack` (not part of your global LTS version). You can even use `stack` to install `cabal-install`! I would humbly recommend you stay away from the Haskell Platform. Delete all traces of anything Haskell-related from your computer, install `stack`, run `stack install cabal` to get `cabal install` , and your life will be far simpler. This won't solve your specific GHC-MOD problem, of course, but it will in general make things far better for you. 
You see a monad is like a DVD case... (Very cool I can't wait to try btw!)
What if all you hear about Haskell jobs being great is a lie? What if it's not all rainbows and unicorn farts? What if it sets you up with unreasonable expectations, the same kind you had when you interned at the "hip" companies working on the "hip" problems? Working with a great language on a project with a ton of potential is all that much more disappointing when you run into the same politics, people getting hired who shouldn't have been hired, sloppy coding, lack of imagination you ran into when you were doing Java, C#, Python and JavaScript. Except now you care more and so are more upset by it all. After an experience like that, working on a shit C# codebase with crappy developers, pleasant working conditions and getting a fat paycheck is a relief. You wanna do something cool? You wanna work on something fun? Do it at home or start your own company. Creating a "magic fun" software job is way easier than finding one.
Are you trying to say that you don't know what row polymorphism is and that a google search didn't help? If so, here's an explanation. Elm calls the feature "[extensible records](http://elm-lang.org/docs/records#record-types)". If you write an Elm function of type toRadial : { fs | x : Float, y : Float } -&gt; { fs | velocity : Float, angle : Float } It will work on any record which has Float fields named `x` and `y`, and possibly more, and it return a record which does not have fields `x` and `y` anymore, but instead has fields named `velocity` and `angle`, and also keeps all the other fields it previously had. Like in Haskell, the lowercase `fs` is universally quantified and the function is polymorphic over all `fs`s, but unlike in Haskell, the `fs` is ranging over all sets of field names (and their associated types) excluding `x`, `y`, `velocity` and `angle`, it is not ranging over types. So it's a different kind of polymorphism called "row polymorphism", presumably because it's abstracting over a whole row of things and not just one. In Haskell, we *do* have a library supporting extensible records, it's called [vinyl](http://hackage.haskell.org/package/vinyl) and it's pretty cool. But we don't have the builtin support in the language which Elm has, so the syntax, the inference, and the error messages are not as nice. In particular, vinyl abstracts over a *list* of types, not the *set* of types the OP is asking for. This means that while in Elm, the types `{x : Float, y : Float}` and `{y : Float, x : Float }` are equivalent, in vinyl the types `HList '[Const Float "x", Const Float "y"]` and `HList '[Const Float "y", Const Float "x"]` are not equivalent. So instead of writing the type of `toRadial` as toRadial :: HList (Const Float "x" ': Const Float "y" ': fs) -&gt; HList (Const Float "velocity" ': Const Float "angle" ': fs) we would instead write toRadial :: (Const Float "x" ∈ fs, Const Float "y" ∈ fs) -&gt; HList fs -&gt; HList (Const Float "velocity" ': Const Float "angle" ': RDelete (Const Float "x") (RDelete (Const Float "y") fs)) because while we don't have type-level sets of *types*, we do have type-level sets of *constraints*. That is, the order in which we write `Const Float "x" ∈ a` and `Const Float "y" ∈ a` does not matter.
This is cool. I suppose also sdl2 could be used with gstreamer for a cross-platform video player. I've found sdl2 to be easy to install on all platforms: https://github.com/chrisdone/wish#building
The way I understood it was if this page - which was ~3 hours old at the time of that comment - is among the top results for "row polymorphism" it must be a fairly obscure feature so there's no good reason to expect it to have made it into Haskell. EDIT: Some clarification because of the downvotes. I personally don't think row polymorphism is obscure. This is just how I interpreted the opening comment.
Or maybe Google gives more weight to new pages now? Googling for "Movie Monad" and "Discord bot in Haskell" both yields the corresponding currently-quite-new /r/haskell results.
&gt; Why are Haskell functions pure? Is it just because the Haskell committe doesn't like impure functions, or is there something inherent about the language that makes pure functions unavailable? It's because separating pure and impure code makes your program more reliable. Impure functions are harder to reason about by their nature. &gt; Also, as far as I understand monad is just a wrapper for any value This is not quite correct. Monads can be thought of as containers, but that is often not the most productive way to think of such things. A monad guarantees that you can always chain together an arbitrary pure function with an impure one, and get back a value in that monad. That's how they handle IO - you can chain together essentially anything with monads, but still *write* pure functions when you like. 
In my experience as a paid Haskeller, the fulfillment aspect you're talking about ultimately comes from being on a well-organized team that trust each other, respect each other, and all believe in the real-world value of what the project. Working in Haskell correlates with a few of those aspects nowadays, but it's ultimately a secondary upside.
&gt; In a way, Java and C have less functionality than Haskell, because they can only compute "inside the IO type". Much like math: the more constraints on a structure, the easier it is to say intelligent things about that structure. 
&gt; Monads are a way to separate pure (lazy) computations, and (strict) computations which perform side-effects. Haskell's IO is in fact lazy, which is why you can't do do x &lt;- readFile "file" writeFile (x ++ "string")
&gt; Tell me. What is your first person experience working these haskell jobs? Good. I like my colleagues, they're smart, and I like my colleagues that don't write Haskell too :) I'm also working remote, which works great for me right now. In general, I can't really say what will make you happy. As you have noted, a lot of the things that are "hip" aren't conducive to happiness. Haskell is more pleasant than e.g. Java but it's not a panacea. 
There's no shortage of Reddit clones, including ones that "still care". The value comes from the community, not the technology. [The top Haskell post on Lobsters](https://lobste.rs/s/wycagj/whats_new_purely_functional_data) has 56 points; [the top Haskell post on Reddit](https://www.reddit.com/r/haskell/comments/50389g/resignation/) has 489 points. So as an extremely rough estimate, Reddit's Haskell community is an order of magnitude bigger than Lobsters's. 
Electron is the thing that lets you run browser stuff as a desktop program, right? What does electron free mean for a Haskell program? Haven't there been native video players for decades? Just a little confused.
It seems the author wrote a previous version using the browser technology. So this is just a more standalone version by comparison. 
Read this to understand : https://josephg.com/blog/electron-is-flash-for-the-desktop/
Ah! Thanks.
I'm not sure I understand how EvalTerm works. Is it loosely related to monad-control, perhaps?
You can take a DVD from a case and do something with it, but at the end, you still have a DVD case, possibly empty though. ^(Please don't write an article "Monads are like DVD cases")
&gt; Electron is the thing that lets you run browser stuff as a desktop program, right? No, it's a tool for building desktop UIs using web tools. It is an actual browser. You can write whatever is generating the UI and serve it locally. The good thing is it makes writing desktop UIs very easy. The bad thing is, it's a full browser. Once you have a couple Electron apps running, the performance/memory hit is very noticeable not to mention they ignore all the OS' native UI elements. 
If I had to guess, I'd say: 1. There is no consensus on the best form of row polymorphism. 2. There is no consensus that row polymorphism is the best way to implement records and variants. 3. Forms of row polymorphism are provided by several libraries, and it isn't clear that adding one to the language would improve things enough to be worth the increased complexity.
I don't understand this equivalence. In the `Num` case, `(+)` has polymorphic type and thus explicitly allows to be applied on arbitrary Nums. If the other example worked without `liftA2`, there'd be implicit overloading of the implicit function application "operator". In my eyes this is a completely different layer of magic. (What would be the type of `liftNum`, anyways?)
This is not relevant. I'm afraid I have to remove it from the subreddit. 
Sadly, no one has made a credible alternative to Electron. Certainly no GUI toolkit comes even close to offering a simple, cross-platofrm, accessibiltiy-friendly platform for graphical apps. GTK+ and Qt certainly aren't up to the task. Swing is abandonware. The actual second best cross platform option is winforms! This app is a cool project, and there is a place in the world for pandering to very or small machines where 4gb is a lot of RAM. But it's somewhat unfair to generalize outwards like the article linked in this thread does just because the Slack folks refuse to write a decent network interfaces for slack desktop.
&gt; there is a place in the world for pandering to very or small machines where 4gb is a lot of RAM. I have 8GB of ram on my desktop, with a couple of Electron apps running, it sits at 6GB used. This is not a Slack problem, it's an Electron problem. It's just that Slack is probably the most widely used Electron app. Can you provide any examples of Electron apps that don't eat all your ram? I mean, I'd love to be in a world where 32GB of ram was an option, and maybe things will catch up. But I shouldn't be swapping if I have 8GB of ram and only 3 very basic apps open.
I don't think bringing laziness into the argument at all is necessary. Your second and third paragraphs are even more convincing without the "we were actually kind of forced into purity by this other thing" first paragraph.
Isn't row polymorphism roughly equal to a structural typesystem (as opposed to a nominal typesystem?)?
If you google most topics, one of the top results is usually a recent article or post. That doesn't mean that that topic is obscure...just that there was a recent post about.
I believe this is called “structural types”.
This seems like a good place to point out that we're making good use of row polymorphism in PureScript, and extending it in some interesting ways: - We use rows to represent the fields in records, which is the obvious application, but also to encode variants ([`purescript-variant`](https://pursuit.purescript.org/packages/purescript-variant) by /u/natefaubion), JS-native effects ([`purescript-eff`](https://pursuit.purescript.org/packages/purescript-eff)), algebraic effects ([`purescript-run`](https://pursuit.purescript.org/packages/purescript-run) also by /u/natefaubion), and a few other things. - Combined with functional dependencies and a bit of magic in the constraint solver, we can do some interesting and useful things with rows: - We get a cheap form of polymorphic labels by solving a [`RowCons`](https://pursuit.purescript.org/builtins/docs/Prim#t:RowCons) class which conses a new label (represented by a type-level string) and type pair onto an existing row. Or we can run it backwards to remove a type from a label. This means we can write [one lens for any record field](https://pursuit.purescript.org/packages/purescript-profunctor-lenses/3.2.0/docs/Data.Lens.Record#v:prop), or one prism for any variant constructor. - Solving a [`Union`](https://pursuit.purescript.org/builtins/docs/Prim#t:Union) constraint gives us the ability to do things like merge two records (inspired by the similar feature in Ermine) - Solving a [`RowToList`](https://pursuit.purescript.org/packages/purescript-typelevel-prelude/2.4.0/docs/Type.Row#t:RowToList) constraint lets us convert rows into a linked-list form and back. In this form, we can iterate over the labels and types in a row using the existing type machinery. For example, this gives us things like return-type polymorphism for arbitrary records, so that we can [write an `Arbitrary` instance once for all records](https://github.com/purescript/purescript-quickcheck/pull/82). People have been using `RowToList` to do all sorts of interesting things lately, like zipping, folding and traversing records. I think this shows that there is plenty of benefit to be had from adding support for row polymorphism to the compiler. One thing I've wondered is whether it's possible to implement rows using a type checker plugin, but I don't know enough to say. Has anyone considered that?
[Row polymorphism is more powerful than structural subtyping](https://brianmckenna.org/blog/row_polymorphism_isnt_subtyping) but they look pretty similar 
Cool. I occasionally also want "relet", i.e. a binding which *should* shadow some previous binding. 
Seems like what we really need is an HTML renderer with some kind of lightweight event framework, and not all the bloat of a full-on browser. That would cover a whole lot of ground in terms of desktop GUI use-cases.
Thanks for the invite offer and allow me to share my thoughts on the issues with HN, lobste.rs and reddit I've experienced compared to conventional systems like Usenet or mailing lists. I don't think reddit going closed source matters because neither reddit nor lobste.rs is a decentralized discussion board (unlike usenet or haskell-cafe) so it's unlikely anyone could mirror the subreddit on a different server bidirectionally without banning by the reddit admins. /r/haskell has a huge advantage in that it is a specific topic, attracting users/fans/etc, leading to a productive discourse among a selective crowd. lobste.rs is small enough that most topics are civilized so far, but they already have some of the discourse-incapable HN crowd signed up, and this means the no-topic structure and voting based sorting will lead to the usual madness known from HN. Lisp had trolling behavior by one or two folks consistently, and the one troll Haskell had seems busy with other activities these days. If /r/haskell was large like /r/politics, there would be serious problems, but it's small and has folks genuinely interested in Haskell or actively using Haskell. A better meeting place for users/fans of Haskell, OCaml, SML etc to come together is LtU or an imaginary shared subreddit or mailing list.
Wait, do you mean unencrypted (and unsigned) names? I don't understand your point about scoping cookies to the domain? How does encrypted names prevent that? I think having encrypted names with a null IV and encrypted values with randomly generated IVs should be fine.
Discord uses Electron last time I checked and it's currently sitting at around 75MB of memory usage.
I have yet to use a GTK or QT application that has a native feel on Windows.
Is not so easy to find entry-level haskell position, even if you sr grade with other languages and technologies.
That particular function is, because it uses unsafeInterleaveIO. ByteString.readFile and Text.IO.readFile, and probably most modern libraries will use strict IO, even if they have to use pipes or conduit to get streaming back. So IO is strict, unless you use one of those unsafeSomethingIO functions. Or perhaps bind an FFI function without IO, which I think is equivalent to putting an unsafePerformIO on it.
Historically, purity was a consequence of laziness. Now, we've realized that purity is desirable in and of itself, but I don't think that anyone believed that until fairly recently.
&gt; and probably most modern libraries will use strict IO ByteString uses strict IO because it's strict :) You can replicate the above using lazy ByteStrings. &gt;That particular function is, because it uses unsafeInterleaveIO Where does `readFile` use unsafeInterleaveIO? I don't think it does.
Slack is poorly written, but many other election apps are not. Slack is also terrible. If you want to fight for liberation from ceaseless interruptions then you have my sword.
This is, of course, most of the work of a browser.
VSCode is a massive programmible text editor and often uses less memory than slack, too.
This is what I usually want more, too. foo &lt;- bar for foo $ \foo -&gt; ... For example. We might have `bar :: IO (Maybe Int)`, but in that `for` body, I don't care about the old `foo`.
Totally! I'm just saying we should update our propaganda to take that into account:)
It's another way of scratching the same itch, but row polymorphism is much nicer.
What about algebraic subtyping instead? If type classes end up working well with algebraic subtyping, it seems like a mostly superior option.
No one has done the work! We'd need a GHC (or language?) proposal and someone with the will and know-how to implement it, and then you'd need to gain support that it's a good enough idea that it's worth the effort and maintenance burden. An unfortunate problem is that GHC Haskell "records" have stolen the nice syntax we'd want for a real record system, so any change we make will either 1) have different record syntax to allow backwards compatibility, or 2) be a dramatic change in the way the syntax and semantics work based on a language extension. I don't think there's another language extension, save perhaps `Strict`, that would alter the meaning of Haskell code like a real row polymorphic record system.
From the documentation: &gt; `unsafeInterleaveIO` allows an IO computation to be deferred lazily. When passed a value of type IO a, the IO will only be performed when the value of the a is demanded. This is used to implement lazy file reading, see `hGetContents`. &gt; The `readFile` function reads a file and returns the contents of the file as a string. The file is read lazily, on demand, as with `getContents`.
Haskell's IO in general is not Lazy. While lazy IO exists, the general opinion is that it is an historical artifact, and to use streaming libraries like _pipes_ and _conduit_ instead. Like all unsafe functions, it can be occasionally useful, for example in the MonadFix instance of IO, but that's the exception rather than the rule.
Hrmmmmmm... Something tells me a JS interpreter / runtime isn't cheap.
Stack, Travis, Docker, Elastic Beanstalk w/ spot instances. Infrastructure managed with Terraform.
… and this is why Electron-free is not necessarily a good thing.
Could you summarize what's algebraic subtyping? Google yields a link to an [entire thesis](https://www.cl.cam.ac.uk/~sd601/thesis.pdf) on the subject, but that looks like a long read. All the other hits I found simply summarize as "parametric polymorphism + subtyping + inference", which doesn't say a whole lot about how this would compare with row polymorphism. Would `{x : Float, y : Float}` be a subtype of `{x : Float}`?
To further illustrate this point, there are several languages that descend from Haskell that are strict but still pure: Idris, PureScript, and Elm. All of these languages saw value in purity even in the absence of laziness.
There are other reasons to do this too. So that may or may not be intent if you see it in code. 
I don't get the hate. It's not perfect, but the average electron app are a hell of a lot better looking (and typically better designed) than the average swing or C# app by a large margin. Also electron is *great* news for Haskell, because it means that haskell can leapfrog the lack of a native-haskell GUI framework (which can take decades to mature) and jump right to good-looking apps that are backed by Haskell (either in the backend or full-stack via GHCJS).
&gt; I don't get the hate. All technology is beautiful. Okay, jokes aside I don't mind electron either. Electron + Pixi.js + &lt;insert your favorite alt-js&gt; and you have a reasonable way to make simple games. Do they need a lot of resources to run properly? Possibly. I think whether that's an issue or not depends on lots of things.
How would that avoid the pitfalls of regular let? It seems like it would only be a way to temporarily disable the name shadowing wanting, but that it doesn't avoid the problem with shadowing in the first place, which is that unwrapping a layer of scope captures names.
Our team at work usually calls that `mfoo` as in "maybeFoo", but shorter. It's not great, but `foo` for a `Maybe Int` is inaccurate too. 
You can do a lot of the things you'd do with row polymorphism with the kit provided by [vinyl](http://hackage.haskell.org/package/vinyl). When you put constraints on what components you need, the (opaque to the callee) rest of the value comes along for the ride. You can add fields, update fields, etc. Baking the feature in to GHC would require a cataloging of the usability issues of `vinyl` (or similar) and how they would be smoothed out by compiler support. However, even if that were done it is not a slam dunk case as more conservative folks would fairly argue that such a feature is not a huge leap beyond using `Has`-style type classes, and would add complexity to the type checker. PureScript is doing an admirable job exploring this design space, so that's a good place to watch to see how the technique reacts to encountering real world use. For Haskell, check out [Frames](http://hackage.haskell.org/package/Frames) to see a few of the things you can do with the technique and a strong stomach. (Disclosure: I maintain both `vinyl` and `Frames`. They're fantastic! 😀)
In my understanding, that's why WebAssembly is being pushed.
I enjoy my Haskell job at Awake Security because Haskell lets me focus on solving interesting problems relevant to the business instead of dumb problems related to null pointers, malformed strings, and missing fields
Look at the source to hGetContents and lazyRead.
One technical reason why we need a pure Haskell is that Haskell is lazy, making the evaluation order undetermined (hard to determine for the programmer), but side effects need determinable evaluation order. If you understand monad well enough, you should know that monadic code is inherently sequential, which is a perfect fit for model side effects. As a counterpoint, applicative functors could be used to model parallel operations, as used in the Haxl library.
I think folks using PureScript are giving glowing experience reports, as are folks using Elm. That environment is, to understate the case, record heavy.
I doubt I'm the only one who would be perfectly happy to see Haskell's record syntax completely co-opted by something obviously better.
Do you just mean a non-recursive `let`?
lobster.rs' signal to noise ratio is, for me at least, several orders of magnitude better that reddit's though, so that's something (though, invitations like this don't help keeping the plebs away from my favourite hipster tech news site! :) I also have invites if people want them - feel free to PM me.
Those might be the only instances of those two things :)
Does Elm have row types? Sounds very un-Elmish to me. Edit: There's an example below, I guess I'd missed that feature.
Hard to imagine you're going to get even a tiny fraction of the participation on an invite only system.
Oh, my mistake.
&gt; Now, the question is whether I should delay the project until I learn lens, template haskell, and Free Monad. No. Write the program now, however you can. If you find that a particular pattern is becoming troublesome, then perhaps lenses, TH, or free monads can help. But perhaps you'll want to use arrows, instead, or maybe you'll be happier with a tagless final encoding. Don't preemptively solve problems. &gt; What tradeoffs should I be aware of when I adopt those concepts? Generally speaking, the barrier to contributors is less "are there lenses involved?" and more "how well documented and structured is this code?"
What is tagless final encoding?
Free monads are an "initial encoding" -- you encode the operations in your DSL as a data structure. Tagless final uses type classes instead, where your operations are defined as a class, and your interpreters are instances on the class.
You seem to imply that typeclass instances are an example of tagless final encoding. I'm still confused about your encoding concept. Can you give me a tutorial on this? I don't think I need to understand it to write a program, but I want to know what it is without going deep into it.
By definition: you can't have too many articles about monads. ;)
Basically yeah. `data`: zero or more constructors, each can contain zero or more values. `newtype`: similar to above but exactly one constructor and one value in that constructor, and has the exact same runtime representation as the value that it stores. `type`: type synonym, compiler more or less forgets about it once it is expanded.
&gt; and has the exact same runtime representation as the value that it stores. What do you mean by that? Isn't that what type does?
`type` and `newtype` both do that. So: `data Foo = Foo Int`: `Foo` and `Int` totally different. `newtype Foo = Foo Int`: `Foo` and `Int` different at compile time but same at runtime. `type Foo = Int`: `Foo` and `Int` same at compile time and runtime.
Ahhh okay. Thanks. 
I wonder, why newtype isn't just a special optimized case for data. I remember there's some obscure distinction in treating undefined, but is it worth it or needed at all?
Newtype is optimized out by the compiler, it has no runtime effect but gives you type safety. The trick is then that, since it disappears at runtime, its behavior is a bit different than data with respect to laziness.
I downvoted this, as I think it could be phrased much more constructively, given the newness of the original post-er to Haskell and declarative programming.
This obscure video was the one that makes MTL "clicks" for me. https://youtu.be/9Af2ZUK3Z4U
You can use hailgun, which is a haskell binding for mailgun.
&gt; Now, the question is whether I should delay the project until I learn lens, template haskell, and Free Monad. &gt; According to my brief research, lens makes it difficult for most haskell programmers to read. Most haskell programmers aren't familiar with Free Monad, either. Unfamiliar concepts could deter contributors. Pick up lens along the way if you need it. Don't worry about readability with regards to lenses. 
&gt; undefined Yes, that's it. See https://wiki.haskell.org/Newtype#The_messy_bits. TLDR; `data Foo = Foo Int` observably differentiates between `undefined` and `Foo undefined`, but `newtype` does not. Bottom is like an additional constructor to all `data` declarations.
In case you missed it, there is "Elm implementation in Haskell" called "Miso". Have not tried myself.
Have a look at section 2 of http://okmij.org/ftp/tagless-final/course/lecture.pdf.
[removed]
It does, but it's not comparable to PureScript's.
I don't see why `newtype I = I Int` is not rewritten as `data I = I !Int`. They are isomorphic, after all.
Interesting Read! What I don't get is that the c disappears in the structural subtyping example. I always thought that structural subtyping would preserve it, just like row polymorphism does. But apparently it doesn't...? I the way In understood subtyping is like interfaces in Java: you get a limited view of an object, but under the cover it's still the whole thing.
Heh, nice way to put it :)
Consider this: case I (error "boom") of I _ -&gt; 1 Play it through in your head with both declarations. The `newtype` variant will evaluate to 1, as the pattern match isn't there at runtime, rather it gets converted into a coercion and then into nothing. In the `data` case, we blow up, because the strictness annotation tries to force the error term. Strictness annotations get rid of the `I undefined` case, while `newtype` doesn't; rather it makes it so that `undefined = I undefined`, e.g. gets rid of `undefined`: case undefined of I _ -&gt; 0 Evaluates to 0. So, strict `data I` contains `⊥` but not `I ⊥`, whereas `newtype` contains `I ⊥`, but not `⊥`, by coercing the latter into the former.
Your point is that in the `newtype` case `I (error "boom")` is indistinguishable from `error "boom"`. You are correct. But my point is that in the strict `data` case the same property holds.
The situation is a bit different for Idris as it is a dependent type language. Impurity just does not make sens with dependent types.
No remote positions sorry.
Well, kind of... Actually my point was that for strict `data`, both examples diverge, whereas for `newtype` both don't. But that also means you can't rewrite `newtype I = I Int` to `data I = I !Int`, because that would incur a change in semantics. That's what I was trying to point out with that last sentence about ⊥.
Fair enough. What I would like to say here is that the question "Why are Haskell functions pure?" itself makes an implicit but very big assumption, namely that the default for functions is to be impure. Understanding Haskell means to dispense with this assumption. Once you do that, you find that the question "Why are Python functions impure?" is equally valid, and only then makes it sense to discuss the technical advantages/drawbacks of programming language where `→` denotes pure/impure functions.
but when does this matter?
Correct, you cannot perform that rewrite. You would have to go explicitly through a pair of mutually-inverse functions. That's why I said they're "isomorphic" and not "the same"! EDIT: Oh, no I see the source of confusion. I said "I don't see why `newtype I = I Int` is not rewritten as `data I = I !Int`". I should have said the definition can be rewritten as long as all `case` expressions are also rewritten as necessary.
I'm not sure that's quite true. Equational reasoning for programs, and consequently purity were fundamental from the start. It was the primary feature it was sold to me on when I was first introduced to Haskell in the 90s (it was at that point hard to do practical programming, and I dropped it for more than a decade). What S P-J says is that other FP languages ended up compromising on purity when they needed side effects for practical programming. Haskell's laziness meant that wasn't an option, so Haskell had to solve the problem in a more principled way.
It matters to the compiler, not to the programmer (not much, that is). The programmer could use `data` everywhere `newtype` appears ~~without noticing a difference~~ [not much of a difference, anyway](https://www.reddit.com/r/haskell/comments/6xri4d/whats_the_difference_between_newtype_type_and_data/dmi19pd/). But by using `newtype`, we give the compiler important information, which is then able to optimize much better (by erasing the `newtype` wrapper altogether at run-time).
I'm not sold. What if you have `data B = B !Bool`? You can't unpack `Bool` (until GHC 8.2). `newtype B = B Bool` is more compact. Even in GHC 8.2, if I understand `UnboxedSums` correctly, we get a more compact representation for `data B = B !Bool` as something similar to `data B = B Word#`, but that's still not `data B = BTrue | BFalse`. (Maybe it is, because of some static tags optimisation, but it certainly isn't for `Either` instead of `Bool`) And this is before we consider that whole [`Coercion`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Type-Coercion.html#t:Coercion) machinery that works for `newtype`s, but not so much for these `data` things. Edit: From a semantic perspective, that rewrite should be safe, though. I just don't think the compiler should represent `newtype`s that way. I'd say `newtype` is more a way to introduce a new type with zero-cost coercions, which can also be understood as a strict, single-field, single-constructor `data` type.
I usually use `fooM` so that I can distinguish betwen `fooMs` and `foosM` (`[Maybe Foo]` and `Maybe [Foo]`), but it feels (and it is) Hungarian notation (which I don't like).
To be honest, I found the the code totally unreadable. It's more than an overkill , it's like a nuclear bomb to solve a problem which could be resolve in many other way (even though none of them are statisfying). I was thinking, it would be nice indeed to be able that a variable should only be used once (a bit like in linear type) but shouldn't lambda suffice ?
So hungarian naming then. Sure, we do this to - there's no real alternative, but it's noisy.
Um, I think level of "informedness" for compiler for newtype and for data with a single field is essentially the same. No heuristics needed.
Except the semantics are different. See https://www.reddit.com/r/haskell/comments/6xri4d/whats_the_difference_between_newtype_type_and_data/dmi19pd/. This allows the compiler to treat `newtype`s differently. Not sure if that machinery would be possible only with strict data fields.
I probably phrased it confusingly, but I'm really just saying that encrypting the cookie names is unnecessary.
You could also look at the [async](http://hackage.haskell.org/package/async-2.1.1.1/docs/Control-Concurrent-Async.html#g:9) library for concurrent actions. But really: just get something working no matter how basic and refine it afterwards.
I believe they mean a let that can only bind to already bound names and intentionally shadows them. let x = 42 in relet x = 43 in x Type checks but let x = 42 in relet y = 43 in y does not (or at least warns).
It's somehow ironic that you're asking this. ;) For games there's this saying: do you want to develop a game or a game engine. For haskell it might be: do you want to develop a program or apply category theory.
This is very relevant (minus the biting tone and the conclusion). Haskell can be a kicker, but by itself isn't a factor. There is no language so great that it would prohibit you from accumulating technical debt at neck-breaking speed, make a weak business model fly or let you overcome intellectual or behavioral shortcomings of your coworkers. I think it is useful to look at what languages are used by a company and some language might guide you to companies with more enthusiastic and able developers at average. But the first thing I would ask myself is always: Would I still want to work here if my favorite language wasn't used? If the answer is negative I run as fast as possible. If the answer is always negative for you because you can not imagine working with anything else then your favorite set of languages, then you're in for some rather fundamental reevaluation of priorities when the first job wen't downhill.
Meta: I don't think this thread should stay stickied throughout the week. Maybe a day or two. Currently it's really hard to notice that a new thread even showed up. 
In case you're wondering how `newtype` is useful, sometimes `type` won't do the job. For example, if you do: instance Whatever Int where { ... } and then type Flib = Int instance Whatever Flib where { ... } This won't work, because it's the same as just declaring two conflicting instances of `Whatever Int`. On the other hand, you can wrap it in a `newtype` and create another instance for data with the same underlying type. You could just use `data` to wrap it, but that actually has a runtime cost.
The type constructors [Cont](https://hackage.haskell.org/package/transformers-0.5.4.0/docs/Control-Monad-Trans-Cont.html#t:Cont), [ContT](https://hackage.haskell.org/package/transformers-0.5.4.0/docs/Control-Monad-Trans-Cont.html#t:ContT), [Yoneda](https://hackage.haskell.org/package/kan-extensions-5.0.2/docs/Data-Functor-Yoneda.html#t:Yoneda), [Codensity](https://hackage.haskell.org/package/kan-extensions-5.0.2/docs/Control-Monad-Codensity.html#t:Codensity), and [Ran](https://hackage.haskell.org/package/kan-extensions-5.0.2/docs/Data-Functor-Kan-Ran.html#t:Ran) seem closely related: data Cont r a = Cont { runCont :: (a -&gt; r) -&gt; r } data ContT r m a = ContT { runContT :: (a -&gt; m r) -&gt; m r } data Yoneda m a = Yoneda { runYoneda :: forall r. (a -&gt; r) -&gt; m r } data Codensity m a = Codensity { runCodensity :: forall r. (a -&gt; m r) -&gt; m r } data Ran m m' a = Ran { runRan :: forall r. (a -&gt; m r) -&gt; m' r } The pattern suggests three more constructors to fill in the gaps in this family: data Cont r a = Cont { runCont :: (a -&gt; r) -&gt; r } data Foo r m a = Foo { runFoo :: (a -&gt; r) -&gt; m r } data ContT r m a = ContT { runContT :: (a -&gt; m r) -&gt; m r } data Bar r m m' a = Bar { runBar :: (a -&gt; m r) -&gt; m' r } data Baz a = Baz { runBaz :: forall r. (a -&gt; r) -&gt; r } data Yoneda m a = Yoneda { runYoneda :: forall r. (a -&gt; r) -&gt; m r } data Codensity m a = Codensity { runCodensity :: forall r. (a -&gt; m r) -&gt; m r } data Ran m m' a = Ran { runRan :: forall r. (a -&gt; m r) -&gt; m' r } Do those already have names, and are they useful for anything? As a quick reminder of the usefulness of those obscurely-named constructs, a Cont computation can capture the current continuation, ContT is the monad transformer variant of Cont, Yoneda is useful for [fmap fusion](https://www.reddit.com/r/haskell/comments/6v9da9/coyoneda_and_fmap_fusion/dlyl793/), Codensity computations can [defer cleanup actions](https://twitter.com/ProgrammerDude/status/899961144718348288), and Ran computations can [change the inner monad for the remainder of the computation](https://www.reddit.com/r/haskell/comments/6vu2i4/fun_exploration_right_kan_extensions_swapped/dm3y2tn/). *edit*: co-question about the dual types: data Store s a where Store :: s -&gt; ( s -&gt; a) -&gt; Store s a data Foo' s w a where Foo' :: w s -&gt; ( s -&gt; a) -&gt; Foo' s w a data Bar' s w a where Bar' :: w s -&gt; (w s -&gt; a) -&gt; Bar' s w a data Baz' s w w' a where Baz' :: w s -&gt; (w' s -&gt; a) -&gt; Baz' s w w' a data Quux' a where Quux' :: s -&gt; ( s -&gt; a) -&gt; Quux' a data Coyoneda w a where Coyoneda :: w s -&gt; ( s -&gt; a) -&gt; Coyoneda w a data Density w a where Density :: w s -&gt; (w s -&gt; a) -&gt; Density w a data Lan w w' a where Lan :: w s -&gt; (w' s -&gt; a) -&gt; Lan w w' a *edit 2*: [I sense an imbalance in the Force](https://twitter.com/haskell_cat/status/904789531945312256).
Interfaces and inheritance are indeed good examples of subtyping. In the example below, `ABC` is a subtype of its superclass `AB`. public class AB { public int a; public int b; } public class ABC extends AB { public int c; } And if you give an ABC to a function which expects an AB, the function won't be able to see the `c`, but your ABC will indeed still have its `c`. Similarly, if `abc = {a: 1, b: 2, c: 100}` and you give it to a function which uses structural subtyping and expects an `{a: Int, b: Int}`, the function cannot see the `c` but after the function call, the `abc` still has its `c`. But in both cases, the value *returned* by the function clearly doesn't have a `c`! The function doesn't see the `c`, so it can't construct a result containing it. It's just that in Java, mutation-based APIs mean that the function will probably not return anything and you will continue to use your old ABC after the call, whereas in an immutable language, the function is likely to return a new version of its input and that's the version you want to use instead of `abc`, so the fact that the new version doesn't have a `c` anymore is problematic.
I encounter this, and also when writing explicit state or cps code, the scope is populated with `s,s', s''` or `s0, s1, s2`...
Hey, I feel like I know what you are talking about. And the myth is pretty much there. I've been working at on Haskell projects at Tsuru Capital, Google and FP Complete (you can read more about my experiences there [here](https://www.reddit.com/r/haskell/comments/54umkh/haskell_for_large_codebases_looking_for_first/d87ohxk/)). All of them have been great. Haskell teams attract people who have the mind set you are looking for; they try hard to avoid grind work, and care about doing things right without compromises. Go get the Haskell job. (But still make sure to work on a project you're interested in. Likekly, any project will be more satisfactory with Haskell, and sometimes that is enough to make an otherwise boring project interesting, but more interesting projects are always better.)
I was also thinking the code used to split the list was needlessly complicated, you can find the middle of the list by passing along the list twice, dropping one element and two elements, until you reach the end of the list (or one element) - avoiding traversing the list three times.. But yes, your implementation was my first thought too.
What is the best guide to make web apps end to end? Is there something similar to ember-cli or angolar-cli for Haskell frameworks?
Haskell doesn't need monads for IO. One could do IO without monads. However monads make IO code much simpler. Consider that in Haskell every function must be referentially transparent: given the same input, it must return the same output. This poses a problem for a function like readLine :: FileHandle -&gt; String You want each call to return the next line in the file, but referential transparency means it must always return the same result. To work around this problem, you need to add an extra field which is the current byte position in the file readLine :: FileHandle -&gt; Int -&gt; (String, Int) There is a performance issue here however, as good file-IO uses buffering, so you'd probably have a FileState object with a buffer, the position of a cursor in a buffer, and the position of the buffer contents in the file. readLine :: FileHandle -&gt; FileState -&gt; (String, FileState) Now calling this repetitively will be tedious as you'll have to keep on unpacking the new states. Whenever repeatedly calling a function due to state become tedious, your mind should move directly to monads. In this case we could use the `State` monad. readLine :: FileHandle -&gt; State FileState String That'll work for reading files, however we also need to write to files. Will the same state work there? What about reading and writing to Unix pipes, TCP sockets and the rest? To simplify all this the Haskell designers created a generic IO monad — which maintains a single state which in turn keeps track of the states of all open system resources — and so we got readLine :: FileHandle -&gt; IO String Now one can argue that IO is a little too big, and it could be a bit more fine-grained (particularly once it started getting used for pointers and storage of global variables). However it works reasonably well in practice, and certainly at the time of its invention the "effectful" style of programming wasn't widely understood. The reason `putStrLn` is in IO is because Haskell was, and is, largely developed on Unix systems and in Unix you read from and write to the console by writing to special magic files. A last note: in the State monad one can eventually unwrap a state object and get the value by typing `runState` but you can't in IO. Why? The reason is that all the IO functions just add an action to a queue of actions to be executed, and the IO subsystem takes items off the queue at its leisure and executes them. Essentially every IO value is a `Future`/`Promise`. At the time it was thought this would extend Haskell's lazy-by-default semantics all the way down to machine level, and provide scope for optimisation. Unfortunately in practice it's caused more accidental performance regressions (and outright bugs) than automatic optimisations, which is why people now use the Conduit and Pipes libraries for steaming file IO
for your `Baz` type, this discussion might be relevant: https://www.reddit.com/r/haskell/comments/6pgsy3/is_having_a_a_b_b_equivalent_to_having_an_a/
Two questions: 1) Because of Curry-Howard, you can see a type `id :: a -&gt; a` as corresponding to the tautology `A =&gt; A`; `const` with the tautology `A =&gt; (B =&gt; A)`, etc. What I'm wondering is when you add type classes into the story, specifically I'm thinking about [nullary typeclasses](https://www.reddit.com/r/haskell/comments/1atlvu/new_ghc_extension_nullary_type_classes_apparently/): -- correctness depends on the generalized Riemann hypothesis isPrime :: RiemannHypothesis =&gt; Integer -&gt; Bool isPrime n = assumeRH (...) My question is: does this somehow correspond to placing `RiemannHypothesis` on the LHS of a turnstile? {RiemannHypothesis} ⊨ Integer -&gt; Bool 2) Some laws are able to be enforced by the compiler. Let's consider a magma: class Magma m where binOp :: m -&gt; m -&gt; m The only requirement is that `binOp` is closed, which is reflected in the type. On the other hand, we can have something like class Group m =&gt; Cyclic m where generator :: m Since [cyclic groups are isomorphic to Z or Z/nZ](https://en.wikipedia.org/wiki/Cyclic_group#Additional_properties), they [can't be axiomatized](https://math.stackexchange.com/questions/537919/showing-that-the-class-of-cyclic-groups-arent-axiomatizable) (or else we could use upper Lowenheim-Skolem to get a cyclic group of arbitrary cardinality). So this is on the other extreme: the compiler can only enforce that a given group's `generator` is an element of that group; any instances would need their lawfulness guaranteed by the implementer. My question is: what is the fragment of logic from which laws can be enforced by the compiler? Is it possible to look at the laws of a typeclass, recognize where they land on some spectrum of enforceability, and conclude that their lawfulness cannot be guaranteed by the compiler?
I started some time ago writing my own configuration management system in Haskell: https://github.com/jimenezrick/cook.hs In reality it's just a toy DSL to not to have to use shell script / python to configure my VPS. But nevertheless is a fun vehicle to explore ideas. I'd say, basic use of lens isn't confusing at all. In reality they give you very terse and intuitive code to manipulate data structures. Maybe more advance use can look like black magic :/ About free monad, it's actually nice to build an interpreter to execute the steps that your configuration management system has to execute, again, a not very advance use it's easy to follow. Keep us posted when you start, I'll be interested in seeing other similar projects :) **Note**: I keep thinking all the time that average level Haskell code is easy to follow, remember the pyramid: https://www.reddit.com/r/haskell/comments/6usuav/the_haskell_pyramid_has_a_wide_base/ - (that post is pure truth and this community despite being very friendly is sometimes scaring beginners from joining just because mostly we discuss very advance topics, sometimes we should discuss publicly very basic stuff like other communities do)
I'm confused. What is an "end to end" web app?
HTML to database, with all the layers in between.
1. I am not aware of a Curry-Howard correspondence that includes type classes. However, after type inference, type classes are translated into dictionaries which are passed around as ordinary arguments. In other words, `isPrime` would be translated into isPrime' :: RiemannHypothesis → Integer → Bool This would roughly correspond to what you have in mind. However, this translation misses the point about type classes — type inference — so there could be more Curry-Howard to them than my answer suggests. 
Awesome, im always looking into haskell guis
To your question: yes. The latter paper you mention is more-or-less a summary of the thesis, and includes subtyping for records, as well as adding lattice operations (meet and join) to types. An extension mentioned in chapter 9 of the thesis gives a means provide subtyping for variants, as well. There is even a prototype online. See the Lambda the Ultimate discussion, the Hacker News discussion, etc.
Also this video, but it does go into some of the nuances that may or may not be important here: https://www.youtube.com/watch?v=JxC1ExlLjgw
Type classes are a way of structured program synthesis, and do not add anything to the core theory. In GHC `=&gt;` desugars to `-&gt;`. &gt; My question is: what is the fragment of logic from which laws can be enforced by the compiler? Depends on the language. Coq or Agda can embed almost all of mathematics. We can also write a reasonable definition of cyclic groups in Coq/Agda, and the linked limitation is not quite relevant in this context (as far as I can tell).
This isn't a repost per se, but basically the exact same thing was posted yesterday: https://www.reddit.com/r/haskell/comments/6xjy70/movie_monad_an_open_source_desktop_video_player/
When I read your question, it seems like it ultimately boils down to "why do you want purity?". My answer is because purity allows us to have many more guarantees about what a function a function can and can't do. Think about all the bugs you've ever encountered. Many of them probably happened because a function was doing something you thought it didn't do. Buffer overflows are one well-known class of bugs that fall into this category. Purity allows us to get rid of this problem and many others. I recently gave a presentation where I talked about this: http://mightybyte.net/three-essentials/#purity-by-default
I can't seem to get past the GdkX11-3.0 step $ haskell-gi -o lib/gi-xlib/xlib.overrides -O lib/gi-xlib xlib * Generating GI.Xlib + gi-xlib.cabal + cabal.config + Setup.hs + LICENSE $ haskell-gi -o lib/gi-gdkx11/GdkX11.overrides -O lib/gi-gdkx11 GdkX11-3.0 ✘ * Generating GI.GdkX11-3.0 haskell-gi: Did not find a GI repository for GdkX11-3.0 in ["/usr/local/share/gir-1.0","/usr/share/gir-1.0"] CallStack (from HasCallStack): error, called at lib/Data/GI/GIR/Repository.hs:85:13 in haskell-gi-0.20-8EapoUUjJieKuTOe4nSxWm:Data.GI.GIR.Repository $ grep -rn pkgconfig-depends ✘ lib/gi-xlib/gi-xlib.cabal:34: pkgconfig-depends: x11 &gt;= 1.6 lib/gi-gdkx11/gi-gdkx11.cabal:36: pkgconfig-depends: gdk-x11-3.0 &gt;= 3.22 $ sudo apt-get install gdk-x11-3.0 Reading package lists... Done Building dependency tree Reading state information... Done E: Unable to locate package gdk-x11-3.0 E: Couldn't find any package by glob 'gdk-x11-3.0' E: Couldn't find any package by regex 'gdk-x11-3.0' $ sudo lsb_release -a ✘ No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 17.04 Release: 17.04 Codename: zesty 
I was trying to follow along and I'm having trouble installing one of the dependencies for `haskell-gi`: $ stack install haskell-gi [...] Configuring haskell-gi-0.20.3... Cabal-simple_mPHDZzAJ_1.24.2.0_ghc-8.0.2: The pkg-config package 'gobject-introspection-1.0' version &gt;=1.32 is required but it could not be found. I installed `gobject-introspection` using `apt-get` but that doesn't seem to help (it says version `1.46.0-3ubuntu1`). Suggestions? **EDIT**: Okay, the issues seems to have gone after installing `libgirepository1.0-dev` as suggested [here](https://askubuntu.com/questions/319568/i-cant-configure-rhythmbox-as-gobject-introspection-1-is-not-installed). Now it's stuck at In the dependencies for gi-gdkx11-0.3.18.20: gi-gdk-3.0.14 must match &gt;=0.3.18.20 &amp;&amp; &lt;0.3.19 gi-gio-2.0.14 must match &gt;=0.2.48.20 &amp;&amp; &lt;0.2.49 gi-gobject-2.0.14 must match &gt;=0.2.48.20 &amp;&amp; &lt;0.2.49 needed since gi-gdkx11-0.3.18.20 is a build target. **EDIT 2**: So I nuked my Haskell installation and installed the Haskell Platform 8.2.1 from the website and now I'm stuck with another error. ;_; &lt;command line&gt;: cannot satisfy -package-id parsec-3.1.11DPgnR92AWEaFOaixmwipet: parsec-3.1.11-DPgnR92AWEaFOaixmwipet is unusable due to shadowed dependencies: mtl-2.2.1-19EL8AGBsN3DnnOhrC9xY3 text-1.2.2.2-EGUst8sqNAZCw1xLPcmcMH 
Can't say it's the best, but I am loving the ideas behind it: https://www.yesodweb.com/
I didn't mention any paper, but thanks for pointing out that there is one! I found the [ICFP 2015 presentation](https://www.youtube.com/watch?v=E3PIKlsXOQo) of the paper short, entertaining, and enlightening. I can now answer my own questions: &gt; Could you summarize what's algebraic subtyping? Add union types `a ⊔ b` and intersection types `a ⊓ b`, with subtyping rules `a ≤ a ⊔ b`, `b ≤ a ⊔ b`, `a ⊓ b ≤ a`, and `a ⊓ b ≤ b`. Restrict those new types so that only input types can use `⊓` and only output types can use `⊔`, and tada! Inference becomes easy, even if you throw in parametric polymorphism. The restriction doesn't unduly restrict expressibility because it is input values which can be used in both an `a` way and a `b` way (thus requiring `a ⊓ b`), and it is output values which can either come from an `a` codepath or a `b` codepath (thus producing an `a ⊔ b`). &gt; how this would compare with row polymorphism Instead of toRadial : { fs | x : Float, y : Float } -&gt; { fs | velocity : Float, angle : Float } we'd have toRadial : a ⊓ { x : Float, y : Float } -&gt; a ⊔ { velocity : Float, angle : Float } where `a` can either be the fields other than `x`, `y`, `velocity` and `angle` as before, or if inference determines that we still need `x` and `y` in the result, `a` could also be instantiated to all the fields including `x` and `y`.
I think you can generally substitute explicit implementation dictionaries for typeclasses, i.e. class Ord a where compare :: a -&gt; a -&gt; Ordering instance Ord a =&gt; Ord (MyType a) where compare = ... That can be simply translated into explicit type dictionaries: data OrdInstance a = OrdInstance { compareInstance :: a -&gt; a -&gt; Ordering } deriveMyTypeOrdInstance :: OrdInstance a -&gt; OrdInstance (MyType a) deriveMyTypeOrdInstance = ... So a parameterless class instance would just correspond to certain definitions.
I have no experience doing this, but you should be able to call any JS library code from haskell in ghcjs. It sounds like a haskell wrapper library might be a neat project.
I definitely wouldn't call it the best anything but, for a quick start covering dev environment, database, web server and deployment, you can see this [blog post I wrote](https://ilikewhenit.works/blog/2).
Thank you! The correct link: https://ilikewhenit.works/blog/2
further meta: there are like 3 very much beginner questions on the front page right now, while none of the threads so far here are really beginner questions (imo).
Yeah sorry I'm on my phone and messed the link up. Should be correct now.
In addition to what everyone else said, some extra clarification on the first bit: `data` only constructs one (potentially parametrized) type. That type can have multiple data constructors, though, which all create values of that type.
Are you asking about language design choices , or about the logical mechanisms? That's two different answers. I will try to answer the design question as simply as I can. Why purity: Because it means all code can be considered in isolation. Once you understand a pure function, you understand it in all possible contexts. This is different from a procedure that depends on external factors to produce a result - ie, if there is an external variable your procedure calls, you have to think about everything that could potentially touch that variable. Because you can understand a pure function completely in any context, it can be reliably re-used in any suitable context, which we refer to sometimes as 'composability.' Why monads for IO: Because in the real world things happen in a specific sequence, and if that sequence is interrupted in an unexpected way, the procedure should abort. The IO Monad let's us enforce that sequence by use of `&gt;&gt;=` and `&gt;&gt;`, and it connects sequences of operations together so that exceptions will cause them to abort deterministically, ie, exceptions thrown at the head of the queue mean the stuff that comes later doesn't get called. That's important in haskell because execution order is usually not explicit, which would get really bad when doing IO things. It's not important that the behavior we need out of IO procedures is implemented via a Monad, it just so happens that monads provided a nice interface for it.
That would be cool. Though, p5 is funded by Processing and NYU with the vision of it being a one-stop-shop for educators and artists in the otherwise noisy JS ecosystem. In this way, JS graphics libraries often contain a lot of unnecessary stuff simply due to lack of standard libs, OO tendencies, and general uncertainty around dependency management/interop. In ecosystems like haskell, we don't really have this problem, it's trivial to gather individual libraries for specific use cases you need (math, rendering, user-IO). If you're working with SVG or WebGL, Elm may be worth looking into. 'create-elm-app' does live-reloading.
As a part of the WebGHC project, we've been [working on a Nix "library"](https://github.com/WebGHC/wasm-cross) that takes a target platform as input, and produces a cross compiling toolchain for fully static binaries; libc and all. (EDIT: Shoutout to John Ericson for developing the foundations of Nix cross compiling that this depends on). I recently got it building Haskell binaries this way for aarch64. At least in qemu, it seems to work flawlessly, supporting the whole RTS. Since the whole toolchain is built from scratch (libc/musl, compiler-rt, etc.) it was a lot easier to just say "do it all statically and link it ourselves." Especially since trying to abstract over arbitrary platforms' dynamic linking sounded hard. Ironically, getting it to produce a native toolchain is proving a bit harder due to some Nix-isms specifically with native, but I'm guessing it won't be too hard to iron that out. In the meantime, I'll take aarch64 as a win, since it means the toolchain will be mostly ready when the parallel work on GHC for WebAssembly needs it. It almost just worked automatically for raspberry pi, but there seems to be a weird problem with the linker. I really couldn't imagine doing this with anything but Nix though. There just would have been no way to do it with Docker.
I'm not quite sure I understand what scope, form and length that you have in mind for this kind of training, but I sometimes do personal remote training for a limited time. Since I'm the author of reactive-banana, I figured that this might be useful here. PM or Email me if you like. You can see examples of my presentation/teaching style on my [hackhands profile (→ posts)][1] or in a [talk about FRP][2] I gave a while ago. [1]: https://hackhands.com/apfelmus/ [2]: http://bobkonf.de/2016/apfelmus.html 
You could use [multi-stage Docker builds](https://blog.docker.com/2017/07/multi-stage-builds/) to build everything in one go. Multi-stage builds also allow you to end up with a tiny image with few layers. 
Your cabal file doesn't really know anything about Stackage. Stackage is a curated subset of Hackage with specific versions that are guaranteed to work together. You can depend on any package in your cabal file. If it's in Stackage, you'll get the version from there. If not, you'll need to add it to the `extra-deps` field in `stack.yaml`, with an exact version. Look at the documentation for the yaml file for details.
Thanks a lot :)
The keywords have always felt a little swapped around for me. `data` is the only way you can create your own new types. `type` is essentially syntactic sugar for type synonyms (I say mostly, since there are some slight caveats when it's used with `ConstraintKinds`). `newtype` apart from the performance considerations which everyone has pointed out, allows you to clone the *structure* of an already existing type, and then selectively (via `GeneralizedNewtypeDeriving`) decide which parts of its *behaviour* you also wish to clone. The compiler will prevent you from confusing the two types. 
Interesting, I understand the difference now.
Is the difficulty that the old thread is stickied even though there's a new one? 
I think the idea behind these threads is to have a day in the week for beginner questions. Not having an all-week-long thread.
I haven't looked at the `monad-control` package. Although after a quick look, that seems more focused on flow control, whereas the EvalTerm is all about abstracting the different terminals, i.e. dynamically choosing the kind of terminal at runtime. Check how in [Backend.hs](https://github.com/judah/haskeline/blob/master/System/Console/Haskeline/Backend.hs) different [`RunTerm`s](https://github.com/judah/haskeline/blob/f17e58f06e238acf1daddd578e28fb8aa8a75dc8/System/Console/Haskeline/Term.hs#L32) are chosen. The `RunTerm` type (contains the `TermOps` record) contains the `EvalTerm` that evaluates the different monad stacks implementing the different terminals, eg [`DumbTerm`](https://github.com/judah/haskeline/blob/f17e58f06e238acf1daddd578e28fb8aa8a75dc8/System/Console/Haskeline/Backend/DumbTerm.hs#L32) or [the POSIX term](https://github.com/judah/haskeline/blob/f17e58f06e238acf1daddd578e28fb8aa8a75dc8/System/Console/Haskeline/Backend/Posix.hsc#L277), which are implemented in terms of a StateT or a custom PosixT respectively.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [judah/haskeline/.../**Backend.hs** (master → f17e58f)](https://github.com/judah/haskeline/blob/f17e58f06e238acf1daddd578e28fb8aa8a75dc8/System/Console/Haskeline/Backend.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmirxu9.)^.
Hello fellow Haskellers! Has anyone here done stuff in Hakyll? I'm putting together a blog and would like to customize the printing of blog tags, but I'm not sure what way it is done best. Can one write tag templates or do you have to out that insider the main function? Any suggestions? 
For sure, there is debate about the specific features. I think in general they're the best low-overhead way we have to deal with these big composite records. It's also a pretty nice way to work with Eff. You can compose effects really easily and it shows. I wish PureScript was more aggressive in using that pattern rather than reiterating Monad transformers with an eff slot.
Seconding this, Lobsters also has a very hacker-newsy crowd which leaks into a lot of otherwise interesting technical discussions with tedious challenges like, "Why don't you just use Javascript?"
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/isanybodyhere] [Anyone Who Has Done Stuff In Hakyll](https://np.reddit.com/r/IsAnybodyHere/comments/6xvoyh/anyone_who_has_done_stuff_in_hakyll/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
In OOP, I can represent classes with UML diagram... Is there such diagram for functional programming? if not a diagram, some notation?
I played around with the idea, see https://github.com/robinp/indextool - it lets you do a render of the callgraph, which should give at least some insight to the refactoring. Other ideas include using some graph library to compute some optimal cuts etc, but I don't want to snatch all the fun.
That's really amazing to hear -- I gained a lot of appreciation for how easy Golang makes it to cross compile a fully static binary by doing this work, and was a little disappointed that the haskell binary wasn't as static as one produced by golang (as far as I understand). I honestly just assumed I was doing it wrong (I still do), and not reading enough on GHC and Stack's abilities to build a fully static binary. If you have any tips on what I did wrong/what you've found, where would be a good place to read up? Also, I've always wanted to use nix from inside a container -- is that a thing yet? I want a NixOS image, but last time I checked it was only the nix package manager installed. Nix is in my mind, the holy grail for reproducible builds (and of course, that would be a benefit in a container with os-level isolation like lxc)
~~Thanks for the suggestion :). Now I'm stuck with another issue as changed my Haskell installation and have a different error, which I'll have to fix first before getting to your fix...~~ **EDIT**: I just realised that it's the main installation I screwed up whereas Stack is working just fine. Your suggested fix works great. Thanks you so much! **EDIT 2**: Oops, I started celebrating too early. Now it's stuck again [1 of 2] Compiling Main ( /tmp/stack18529/gi-gst-1.0.14/Setup.hs, /tmp/stack18529/gi-gst-1.0.14/.stack-work/dist/x86_64-linux/Cabal-1.24.2.0/setup/Main.o ) [2 of 2] Compiling StackSetupShim ( /home/varun/.stack/setup-exe-src/setup-shim-mPHDZzAJ.hs, /tmp/stack18529/gi-gst-1.0.14/.stack-work/dist/x86_64-linux/Cabal-1.24.2.0/setup/StackSetupShim.o ) Linking /tmp/stack18529/gi-gst-1.0.14/.stack-work/dist/x86_64-linux/Cabal-1.24.2.0/setup/setup ... Did not find a GI repository for Gst-1.0 in ["/usr/share/ubuntu/gir-1.0","/usr/share/gnome/gir-1.0","/usr/local/share/gir-1.0","/usr/share/gir-1.0","/var/lib/snapd/desktop/gir-1.0"]. CallStack (from HasCallStack): error, called at lib/Data/GI/GIR/Repository.hs:89:20 in haskell-gi-0.20.3-9nwXY1I1260GlS1AHBdPUX:Data.GI.GIR.Repository
Hey thanks for the tip -- this is exactly the kind of knowledge that I could hvae used when I was exploring this stuff, this looks like the proper way to do what I was doing with docker. For those who want to learn more the full documentation is here: https://docs.docker.com/engine/userguide/eng-image/multistage-build/ I assume the caching works similarly for intermediate containers under multi-stage builds? They don't show up in the listing of images, but surely the layers are saved for reuse later?
This seems like a very broad scope, and meeting all 4 points will be more intensive than most would be willing. Part of the issue is that no one uses that breadth of web tech, every group has a favorite they stick to. Honestly, it is probably not worth your engineers' time to learn all of them either. What a consultant CAN help you with is narrowing down the right choice, (Elm, Purescript, Reflex-dom, etc.) and then teaching that (or connect you to someone who can). That said, Obsidian invented reflex-dom so they are far-and-away the most qualified to teach it, but my impression is that they are not focusing on evangelism until the ecosystem stabilizes. /u/ryantrinkle can weigh in if I've misrepresented them. On the other hand, WellTyped is the most training-focused shop I know. even if they don't specialize in JS, I'm sure they would put together something for you with enough $$$. My team also has a fairly extensive "hitchhikers guide to fullstack haskell" internally, that's (very) slowly being converted into a book for public consumption. If there's a lot of interest we can dedicate more time to it. Feel free to PM me as well.
`type` is the product of the only half-decent set of syntactic and naming choices among the whole family of data/type stuff in the language (`data` and type family decls (including the actual label "type families") being the worst/most head-scratching) in that the `=` actually means `=` rather than "here comes some more stuff"
yeah, i went through these phases as well and got the application to run. However, it now seems to just show me a frozen/blank screen where a video is supposed to be playing (bigbuckbunny.mp4 in this case). ¯\\\_(ツ)_/¯
Lucky for us, John had already done most of the work on coaxing GHC into doing everything statically, so I have no idea how hard that was =P Our work on static linking had more to do with the libc / compiler-rt / cc-wrapper (a nix thing) stuff. As for containers, I'm not sure about running NixOS as a guest. But if NixOS is your host, `nixos-container`s are pretty good. It basically takes an ordinary NixOS `configuration.nix` style file, and runs that configuration in a `systemd-nspawn` container. You can get pretty crazy with it by `nix-build`-ing a `&lt;nixpkgs/nixos&gt;` derivation yourself, meaning you can pin the whole container to whatever version of `nixpkgs` you want.
Did the dependency installation work out fine after the `git checkout` step for you?
Thanks. &gt; Honestly, it is probably not worth your engineers' time to learn all of them either. I kind of agree, but I don't know how else to get people to think far beyond the popular solutions. The broadness is partly a chicken and egg problem. People won't consider frameworks outside of what they know well, until they are aware of them and their benefits, but people aren't willing to read about anything that might be considered too academic or impractical. I see reactivejs, fantasyland, and Elm as good gateways to build comfort and expand people's curiosity and willingness to use abstractions. I encounter people who know React well, and will listen to me when I talk about Elm, but they need a strong justification for learning to climb any higher up the FP/typed ladder. I don't think they will climb higher up the ladder for a while, but they would consider it after using Elm for maybe six months, but only if they are aware that Elm is the beginning, not the end.
This is a great clarification that I agree with entirely. I remember asking myself this question when I learned VB in high school, having previously only seen HTML markup and Excel spreadsheet programming.
I personally found Hakyll extremely difficult to work with; so I wrote [SitePipe](https://github.com/ChrisPenner/SitePipe) instead. If you decide to go with Hakyll good luck, it's a bit tricky!
Sorry! I somehow missed the "Beginner" part of "Beginner Saturday" and assumed that any question was fair game.
what about packages versions? Arch has the latest versions of everything (and the new versions are available in a matter of hours usually), how is NixOS compared in this respect?
No, but it's likely you're going to need SOMETHING besdies Haskell here, as you probably don't want to compile in language modes for the inevitable editor that gets written with it. Let's hope PureScript thrives on webassembly. PS really is a joy to write.
Yup, they get cached just like normal images. It's the best of both worlds! Before multi-stage builds you had to choose between good caching (but huge size and lots of layers) or small images (but no caching and one huge complicated `RUN` step). 
I've had the exakt same thought! "why not just marshall values with aeson?" Hopefully I'll figure it out today, otherwise i might just change to your framework. 
&gt; I just don't think the compiler should represent newtypes that way I didn't mean to suggest that, necessarily. In fact it would probably be better to represent strict data of a single constructor as a newtype. That would mean you wouldn't need newtype.
To be fair, in the previous discussion about these threads I asked if questions had to be beginner or if anything was fair game. The answer I got was beginners are invited, everyone is welcome, so you're fine!
Oh that's great! I'll update the blog post right now [EDIT] - Blog post updated -- feel free to drop me a PM if you don't want your reddit username credited in the post!
Hmm.. I was thinking about an example that defeats intuition like data T1 = T1 !Int data T2 = T2 !Int !Int case undefined of T1 _ -&gt; 0 case undefined of T2 _ _ -&gt; 0 Where the first expression would evaluate to 0 (under the assumption of `newtype` conversion) and the second would diverge. But that would easily be circumvented by annotating the pattern match with a bang: case undefined of T1 !_ -&gt; 0 So this might actually work, if the compiler was willing to track this and insert the necessary `seq`s by itself.
Yeah, I understand it perfectly well now! :)
Of course impurity can make sense in a dependently typed programming language, just take C++ as an example. It arguably has a much more useful set of features than Idris and C++ templates are a somewhat quaint pure functional programming language with unbounded recursion. 
&gt; "Why don't you just use `-XImpredicativeTypes`?" FTFY ;) 
&gt; I believe the myth is feasible because it's how I feel writing haskell by myself: dope, consistently, without much that feels like manual labor, and without the discouraging emotional compromises in correctness and "good"ness I have to make in the languages and tooling at work. As it happens, much of the professional world revolves around understanding and practicing tradeoffs and compromises. Did you look at academia ? Often, research can give you more leeway in your code, as your main output is not code, but papers. However, the paper competition part of research can be soul crushing, too.
Just to address your second question, why should functions be pure. Purity provides three things 1. Easy concurrency: if all your functions are pure, it is guaranteed that you can parallelise `map` without any issues, indeed `pmap` does exactly this. This is also why only pure functional languages like Haskell and Purescript have STM implementations. 2. Easy to reason about: if you know all functions have no side-effects, it's a lot easier to understand what code does, and _does not_ do. This can help reduce bugs 3. Easy to prove correct: essentially an extension of (2), it's easy to mathematically prove code is correct, typically by induction, if functions are pure and simple. While not always necessary, in certain specific fields this is useful. 
C++ is not a dependently typed language. But what i meant was a coherent language. An truely impure function (i.e. with oversavational impurity) can not be defined in a coherent language without breaking equality laws.
Hi, I work at [Tweag I/O](http://tweag.io) , which you forgot to mention : ) Tomorrow I'll be giving a quick introduction to using functional programming for building webapps (servant + reflex) at my local web meetup [in Zurich](https://www.meetup.com/Web-Zurich/events/242642751/). It's meant to pique the curiosity of javascript developers, don't hesitate to stop by if you're in the area!
That's the point. They people who are there spent a lot more effort than a kid hitting sign up so they will have much higher quality content. Also if you get banned its much harder to come back.
&gt; The alternative of using `f &lt;$&gt; g a &lt;*&gt; h b` isn't much better, in my opinion. I don't get this. I personally really like seeing this when I'm reading code. I would hate it if all I ever saw was ` f (g a) (h b)` and had to go digging who knows where to know if `g` and `h` are pure or are actually returning functors. For me, the power of Haskell syntax is exactly this demarcation that lets one understand things about the functions without having to look at their definitions. The more magic going into the syntax the harder it will be to look at a random part of code and have any confidence about what it might be doing. One might say that's a trivial concern but as Haskell gets used more and more on very large projects, this kind of visual analysis without having to understand the entire code base is exactly what sets it apart. 
If you want to build your project into a static binary add a 1. ` ld-options: -static` in your executable section of the cabal file 2. Mount your project directory into your container 3. stack clean ; stack install --split-objs --ghc-options="-fPIC -fllvm" For more info, look here https://www.reddit.com/r/haskell/comments/5lk33p/struggling_building_a_static_binary_for_aws/ There is no need for the `crtBeginS.so/crtBeginT.so hack` and adding `-optl-static` is the wrong way of doing it If this solution is working for you, please edit your blog post in order to avoid spreading miss-information 
So we've traded Electron for... X11? I'd rather have Electron.
Well, one could say that the [commutative diagrams from category theory][1] are the diagrams for functional programming. [1]: https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/F_algebra.svg/440px-F_algebra.svg.png
Sure but recent usually means, this month, not 3 hours ago.
Google is increasingly good at filtering out results that aren't relevant to you. Like those "upvote this to the front page so Walmart is replaced by a Nazi picture in Google search" posts? It works for the redditers and basically nobody else. Likewise, people who frequent r/Haskell will get those results for everything relevant as well.
I like it! Does anyone know if there are any implementations anywhere or any research into how they interact with type classes?
Amazing! I initially though it'd be sufficient to build something like an FHS env in Nix and just do the (few, probably libc, libgmp only?) links with this, producing an "almost completely portable" binary in the end.
I find that for most people, there is a specific "trigger" that ignites their "whoa... I need to learn more about this". It tends to be different for most people but some similarities occur; in general, if you see your coworker just blazing using some fancy language, or fancy tool... You'll probably want to know more. Or if you have a strong personal experience where you spent so much pain and misery to accomplish something using old method X and then did it painlessly in new method Y (or your coworker showed you how to...), That's another common way I see people getting hooked on things. The main common thread is a concrete and immediate reason to care that works for that person. Types are awesome, but most JavaScript and PHP users wouldn't care about strong, expressive types. Typeclasses are great but very unlikely to impress a Java programmer. Template Haskell won't spark that "whoa" in a C++ programmer familiar with templates. Hackage is great until you show it to a Python programmer... Etc Perhaps you might have luck taking the approach that whenever someone comes to you for a problem, give the 2 minute their-language solution and then say "but in language X..." And give the 20 second Haskell/FP solution. Or when someone complains about how many tests and bugs they deal with, pull out some comparison numbers with FP. Of course it's somewhat tricky to do this without coming off like an ass at times, so moderation is key, but it is helpful for showing where weaknesses and limitations occur in existing solutions. (Lastly, I think it's totally fine if an advocate starts someone out with Elm and doesn't even really mention Haskell. Once someone's somewhat comfortable in Elm they'll likely enjoy it much more than their older language and I'd imagine they would be far more receptive to the idea of a more powerful language at that point. No need to try and convince someone to start out with a language already expecting to "graduate" from it)
We should probably try to get this info in the GHC user guide or in the docs of cabal and stack. This topic keeps coming up but most people (myself not necessarily excluded) end up with weird hacks and workarounds which only make things more difficult than they should be.
I'm not quite sure how strongly that comparison really holds. Commutative diagrams are usually between categories are they not? Haskell is really only inside the category Hask (and you need to squint quite a bit to be able to say that in any seriousness). The truer comparison is that FP doesn't really have a direct equivalent because it's not really needed. In Haskell and other FP languages, data is organized in algebraic datatypes so any sort of "dataflow analysis" is likely to be the way to go.
One of the times this was brought up on this subreddit, it was mentioned that there's some code out there which would break subtly if strict data of a single constrictor was turned into a newtype (because of the small differences in where bottom exists and how inspection behavior works)
&gt; Does anyone know if there are any implementations anywhere Yes, the type-checker used in the presentation is available [online](http://www.cl.cam.ac.uk/~sd601/mlsub/) and on [github](https://github.com/stedolan/mlsub). &gt; any research into how they interact with type classes? Since the research is fairly new (there's also a [POPL 2017 presentation](https://www.youtube.com/watch?v=-P1ks4NPIyk)), is presented using an OCaml-like language, and is implemented in OCaml, I'd guess not, at least not yet. What kind of extra difficulties are you worrying about, though? Isn't type-inference on an expression of type `C a =&gt; b` practically the same as if it had type `C a -&gt; b` instead? Plus some extra work once a concrete `a` has been picked and instance search discovers new constraints, but that only occurs after `a` has been selected, so it shouldn't make the process of inferring `a` any harder.
It's essentially for carrying around 'configuration' data that will not change. In other words, a state monad where the state can't ever change. Why would you want this? Well, if you KNOW certain configuration data won't ever change, it is nice to specify that at type level so that the assumption is built into your code and checked at compile time. If you just used a state monad to carry around what should be constant data, there is no restriction that it really remains constant. For example, in a video game, the texture map for objects would probably be loaded once at the start and be accessed with the Reader monad. Whereas the location of the objects in the game would change and be in the State monad.
Hey, The reader monad is used when lot's of functions need access to the same variable. For example when I was writing the villefort application, Shameless plug, I used the reader monad to pass a giant config datatype to all functions. So functions like queryDatabase don't have to have an explicit config variable passed to it. It also mean't I could just forget about how I was going to pass the config datatype to every function. The Reader monad allows a function to ask for the config whenever it needs it. So for the function that generates the html it can just ask for the user defined colors. And I don't have to think about passing it from the server function to the parsing function and then to rendering function it just happens automagically. Then I wan't to run this chain of reader monads I can use runReaderT genIndex config and all the functions that need it will get the config
Equational reasoning only holds without reservation in a lazy language, so it was actually laziness that was fundamental from the start. Of course, in order to have the laziness have correct semantics unconditionally, purity was required pervasively; as far as I'm aware, the purity requirement derived from a need for laziness.
https://stackoverflow.com/questions/14178889/what-is-the-purpose-of-the-reader-monad
mm, well, I do not really follow a lot of packages unless they break or I need some new features. My impression is that they try hard to make it latest, but in some cases they do not have enough people. So for example - again julia does not get update as often as in arch :) but probably more popular packages do... on the plus side here - it is quite easy to make the package of the version you want, as simple as, for example packageYouWantToModify.overrideAttrs(old: { src = "add a function here which gets new source"; }) you may need to change more attributes of course, but you get the idea.... moreover there are different version unlike in arch. and they all can easily coexist. if you need to try something new, you can easily do it affecting only things you want. and if you update and have those bleeding age break your system :) you just press one button and everything is back to normal for the guy who have lots of troubles with NixOs due to julia, I still thing it is literally the best os I tried or read about. EDITS: make the example with override more easy, and corrected some words.
It's a Monad that holds a function on the configuration or say "the environment". newtype Reader env a = Reader { runReader :: (env -&gt; a) } So to get the final result, you need to apply its contents to the initial environment or configuration record. action :: Reader env a -- usually with more specific types action = ... main = (runReader action) initialEnvironment * You can get the environment within the MonadReader action with *ask* * or you may evaluate another action with a locally modified environment with *local*, as defined in the typeclass -- class MonadReader env m | m -&gt; env where ask :: m env -- get the environment local :: (env -&gt; env) -&gt; m a -&gt; m a -- evaluate an action with a modified environment You have an extended example in [Encoding the global state in a Monad - The Reader Monad](https://www.schoolofhaskell.com/user/griba/The_preferences_problem_varying_global_state#encoding-the-global-state-in-a-monad---the-reader-monad)
I like to use it when carrying around references to things that can be updated by IO type functions, for example `Handle`s and `MVar`s. For example, let's say I have many API functions that perform some IO computation on both a `Handle` and an `MVar`. The bad way to do it would be to require the user pass a parameter to all of these functions: import System.IO import Control.Concurrent.MVar import Data.Text pullNextLine :: Handle -&gt; MVar Text -&gt; IO () pushCurrentLine :: Handle -&gt; MVar Text -&gt; IO () swapNextLine :: Handle -&gt; MVar Text -&gt; IO () updateCurrentLine :: MVar Text -&gt; (Text -&gt; Text) -&gt; IO () viewCurrentLine :: MVar Text -&gt; IO () As you can see, all of these functions take one or two parameters, which are usually going to be the same. If you then want to write a very long procedure you would have to do something like this: main = do handle &lt;- openFile "./localfile.txt" ReadWrite mvar &lt;- newMVar "" pullNextLine handle mvar a &lt;- viewCurrentLine mvar when ("yes" `isPrefixOf` a) $ do updateCurrentLine mvar (++ " ==&gt; start") print a swapNextLine handle mvar updateCurrentLine mvar (++ "\n----------") pushCurrentLine handle mvar It would make things a lot cleaner if we didn't have to create our own `handle` and `mvar` and then pass the one or both of them on every single line of code in our `main` function. We could create a data type `data FileBufferLine = FileBufferLine Handle (MVar Text)` that would combine the two elements into a single value, and pass this value to each function. But this is not much of an improvement because we still have to remember to pass the value around. This is where Reader comes in. I usually like to wrap it in a `newtype` to keep the internal implementation hidden. data FileBufferLine = FileBufferLine Handle (MVar Text) -- this will be the type inside of the ReaderT. newtype LineEditor a = LineEditor { unwrapLineEditor :: ReaderT FileBufferLine IO a } deriving (Functor, Applicative, Monad, MonadIO) instance MonadReader FileBufferLine LineEditor where ask = LineEditor ask local f (LineEditor m) = LineEditor (local f m) Then we also provide a `runLineEditor` function which hides away all of that value-passing inside of the Reader. runLineEditor :: FilePath -&gt; LineEditor a -&gt; IO () runLineEditor path (LineEditor rdr) = do handle &lt;- openFile path ReadWrite mvar &lt;- newMVar "" runReaderT rdr (FileBufferLine handle mvar) Notice that this function creates our `MVar` for us automatically, and also opens the file with the `ReadWrite` mode, so we don't need to do it ourselves in the `main` function. These are implementation specific details that you shouldn't have to worry about outside of this module. Now we can rewrite our API functions as our reader type `LineEditor`, and notice now it is no longer necessary to include the `Handle` and `MVar Text` types as parameters, they have been hidden inside of the Reader. pullNextLine :: LineEditor () pushCurrentLine :: LineEditor () swapNextLine :: LineEditor () updateCurrentLine :: (Text -&gt; Text) -&gt; LineEditor () viewCurrentLine :: LineEditor () and our main function will use the `runLineEditor` function: main = runLineEditor "./localfile.txt" $ do pullNextLine a &lt;- viewCurrentLine when ("yes" `isPrefixOf` a) $ do updateCurrentLine (++ " ==&gt; start") liftIO $ print a swapNextLine updateCurrentLine (++ "\n----------") pushCurrentLine See how this code is much cleaner, and you don't have to worry about defining `Handle`s and `MVar`s (`runLineEditor` does that for you) nor do you need to pass these paramters as values to every step of the procedure. All of those details are hidden inside of the Reader. Also notice that in order to use `print` we have to precede it with `liftIO`. This is a little inconvenient, but if you don't expect to mix IO functions with `LineEditor` functions too often, this inconvenience is an acceptable trade-off for having a cleaner API that hides implementation details inside of a Reader.
yep - i never installed the haskell platform. After installing gobject-introspection and following instructions (including the git checkout at the end), the program installs dependencies and builds properly. It runs as well, but after opening a video file, I get a frozen rectangle where I think the video should show up, but is instead some junk background pixels.
You can change a `Reader` environment using `local`. You're going to need `reflection` for truly constant data.
Hey thanks for pointing out what I was doing wrong -- I've updated the blog post (it's redeploying now), as this approach works just fine for me. Agree with cocreature's comment -- I'm not sure where I would have found this information, but I guess I just didn't look hard enough.
&gt; Commutative diagrams are usually between categories are they not? No, commutative diagrams are inside a single category. Maybe you have seen some commutative diagrams in the category Cat (of categories), and this is what made you think that.
Sounds great!, will give it a try. Thanks! :)
There are various UML diagrams. The most common which depicts the relation of classes (forgot the name) is about types (classes), their interfaces, and their interrelations. In Haskell they say just writing out type signatures without actual implementation is similar to that. Actually writing `theImplementation = undefined`, the compiler can check if the "scheme" in terms of type signatures is correct.
What about, say, a diagram expressing that something is a covariant functor between two categories? C D x -F-&gt; F x | | p F p | | v v y -F-&gt; F y
This is an important point. `Reader` can be thought of as corresponding to local state, while `State` corresponds to global state. For example, take these two programs; stateex :: State Int (Int, Int) stateex = do i &lt;- get modify (+1) j &lt;- get return (i, j) ex1 = evalState stateex 1 readex :: Reader Int Int readex = do local (+1) $ do i &lt;- ask return i j &lt;- ask return j ex2 = runReader readex 1 `ex1` will return `(1, 2)`, since the variable being passed around was altered by `modify`. On the other hand, `ex2` will return `1` since that `(+1)` only applies within the scope of `local`, not outside of it. Both of these usages are common in the implementation of type checkers and programming languages. Whenever a definition is added to the program, that's usually done via a state monad, as the list of definitions that a program uses is part of global state. On the other hand, when type checking there are frequently temporary assumptions (such as assuming an input variable `x :: Int`). Since these assumptions are local, not global, a reader monad is used instead. edit: [here's](https://github.com/AHartNtkn/Dependent-Binary-Lambda-Calculus/blob/master/DBLC.hs) a simple example language.
This is a sort of diagram which comes up a lot in CT, but it's not what's usually called a commutative diagram. A _diagram_ is a directed graph that is drawn on paper (on screen). There is then a mapping of that graph to the underlying graph of the category under consideration (provided by labels). One then generates the category over the graph with all paths (sequences of arrows) with equal source and target equated. To say that the graph is _comutative_ is to say that the morphism of graphs (implied by the labelling) extends to a functor. This turns out to be quite an economical way of writing down formulas for situations in categories, but it's not the only sorts of diagrams that come up. For example, we also take (co)limits over diagrams, and we certainly don't expect all those to commute.
Mmhmm, but you know that anything of type `Reader r a` won't change the environment.
`fooM` is also a convention for monadic equivalents of `foo`.
Just a comment on question #2: Note that using a language extension like Liquid Haskell allows you to express richer constraint types, which I believe means more of the "spectrum of logical propositions" is enforceable by the compiler when using such a language extension.
Also worth noting you can store mutable vars(such as IORef, TVar, MVar, etc..) inside a reader, though in order to use them you need to be inside the IO monad, so purity is still enforced.
Well, sonyandy mentioned type classes so I just kinda went with that ¯\\\_(ツ)\_/¯ That does make sense about the type inference though. I'm excited to see how it pans out
even State isn't quite "global" state, because it is still immutable; meaning changes to the state don't affect others that already have a reference to the state. however, it is "global" in the sense that when the call to run state returns, it can contain an updated value.
How is that better / simpler / clearer than just passing that configuration data as an argument to a function? You still see the information at the type level, and it is simpler to do that than to create additional structures (Reader Monad etc) 
Ah, nothing in your OP mentioned that you perceived a quality problem here. Best of luck. Out of curiosity, how do they make it harder to come back from a ban? It seems like changing IPs is pretty hard to counteract.
Obligatory [CCC comic reference](https://ro-che.info/ccc/29)
Thanks for the suggestion. I'll add it to the list.
True, I'll adjust the category.
Added it to the list, thanks for the contribution.
Added it to the list, thanks,
If it's a practical Haskell project then it should be on the list ;) 
This is a great heads up that can help people avoid a lot of frustration.
Added to the list, thanks
I don't have an account the Haskell Wiki but it's a great idea. I'll apply for one and transfer the list over if no one else beats me to it.
Added to the list. Thanks for the contribution. 
None as widespread as UML is in the OOP world, that's for sure! There are a few notations used in a few different FP niches though. The lens package famously uses a [UML class diagram](http://hackage.haskell.org/package/lens) to illustrate the sub-typing relations between the different "optics" it provides. The [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia) uses a [similar diagram](https://wiki.haskell.org/File:Typeclassopedia-diagram.png) to illustrate the relation between different type classes. Since only type classes use sub-typing, not types, few Haskell packages make heavy use of sub-typing, so this usage is rare. [Arrow diagrams](https://www.haskell.org/arrows/syntax.html) are sometimes used to describe how the data flows between the components of an Arrow computation. In a program which used an Arrow-based API to combine its different top-level components, I think this notation would make quite a good alternative to UML diagrams to illustrate the overall program organization. In practice, however, Monad transformer stacks are a much more popular choice than Arrows for top-level components, and I don't know of any convenient diagram notation for those. Finally, [Commutative diagrams](https://en.wikipedia.org/wiki/Commutative_diagram) are encountered much more often in Haskell-related blog posts than the other notations I've mentioned. It's a notation which succinctly illustrates how a number of different sequences of transformations are supposed to be equivalent. It seems like they would be a good match for illustrating type class laws, I'm surprised I couldn't find any example of that. They would not be a very good match for summarizing the overall architecture of a program though.
**Commutative diagram** In mathematics, and especially in category theory, a commutative diagram is a diagram of objects (also known as vertices) and morphisms (also known as arrows or edges) such that all directed paths in the diagram with the same start and endpoints lead to the same result by composition. Commutative diagrams play the role in category theory that equations play in algebra (see Barr–Wells, Section 1.7). Note that a diagram may not be commutative, i.e., the composition of different paths in the diagram may not give the same result. For clarification, phrases like "this commutative diagram" or "the diagram commutes" may be used. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Not sure what happened to `code.haskell.org/~dons`, but the mail archive seems to be accessible via the wayback machine: https://web.archive.org/web/20161015084731/http://code.haskell.org/~dons/haskell-1990-2000/maillist.html
I don't mean to be mean to the author but Medium posts are a bit "flat" to me, a bit average. That explains the name, I suppose. Reading the comments, I think I'm just not the target audience; others dig it! That's good.
code.haskell.org was taken offline this week (after many false starts and limited resources recently), and only the `projects.haskell.org` component and `code.haskell.org` counterparts were publicly mirrored, not people's `public_html` We do still have the homedirs but they're not accessible publicly (for obvious reasons, and recovery purposes) but we can probably make exceptions on request pretty easily. I've forwarded this thread to the person who created the archive to ask them about it.
&gt; If we live in a quantum multiverse, this is exactly what happens when you punch someone in the face. I'm happy someone else made this comment, so I didn't have to make a medium account.
a few notes: - Haskell is about stating effects explicitly. Want to do side effects? want to use mutable variables? We got you covered! just use `IO` and `IORef`. They are simple to use. But they are very very often just not the right and maintainable solution to your problem! And if you want more guarentees you can use `MVar`, `TVar`, `ST` or even just `State`. - It is possible to write cryptic code in any language. This has nothing to do with Haskell. But I'd rather to be able to have a range of possibilities ranging from very short and concise code to bigger more verbose code than only having the verbose option available. And Haskell can give exactly that. - &gt; In the real world, if I punch you in the face I will change your state in place. I won’t be creating a new version of you with a black eye. - [No](https://www.infoq.com/presentations/Are-We-There-Yet-Rich-Hickey). - There are places where the Haskell ecosystem is lacking, but there are also places where it is mature or best in class. Just like any ecosystem. - "Purity can backfire" - Even if you program in `IO` all over you are just in the same place as any other language. Also, many times when you see everything in `IO` that might be an opportunity to step back and think if there are better models for what you are trying to do. - No language will help you build the right thing if you don't know what you want! so this argument doesn't matter.
 What is the author actually even saying about immutability? Immutability doesn't model reality? In what way is that relevant? Neither does the observer pattern, or nearly any other weird, complicated series of hoops that OO programmers jump through on a daily basis under the bizarre assumption that additional layers of indirection are the solution to all problems. OOP is fine. It's obviously a successful paradigm that's built many pieces of wonderful, long lived, useful software. It doesn't model reality. That is a brutal, stupid farce that really needs to die. It's not even a goal most software should aspire to. Pretending that OOP is good because it 'models reality' is a batshit crazy thesis that helps no one. There are plenty of legitimate reasons that immutability might be a bad design choice, but 'modeling reality' isn't related to any of them.
It is a good thing to finish something.
&gt; In the real world, if I punch you in the face I will change your state in place. I won’t be creating a new version of you with a black eye. Isn't there a whole "thing" in philosphy about how you can't distinguish between these two things?
It feels flat to us, because it does not provide specific arguments (e.g. "immutability is bad in this or that piece of code". This is contrast to articles likes Hughes' classic "Why functional programming matters".) Instead, it expresses a general sentiment of cautiousness about functional programming, that apparently resonates with the target audience and elicits an emotional reaction.
Not all pork chops are well cooked. True or false?
That is perfectly valid of course, but passing configuration data to all of the functions that need it gets old after awhile, especially in larger applications. First you add configuration data arguments, which is simple enough at first. Then you discover that you want to add logging, and maybe run some IO actions in specific monads like Except for error handling. Now your type signatures are a mess; you have to change 500 different call sites to handle your new input and output signatures, and every time you change them thereafter. It's much easier to go change your return types from `Reader` to `MyMonadTransformer` and have all of that new logic Just Work than it is to manually pass it all around. And once you've done that you never have to change the type signature again, you can keep stacking more monads on as you need them and get that additional functionality with no work at all.
Or you could just shadow x yourself to something else before entering the code you don't want to reference it. data Unlet = Unlet let x = "seecrit"; x' = hash x in let x = Unlet in ... now ... can't reference the old x so I'd feel pretty safe about it. With that you don't lose access to all sorts of fixity info, etc. I'd feel better about it if ghc had some way to say 'don't warn about this shadowing in particular' though.
For a pure flavour of the use of `Reader` (without `IO`), I can refer you to my [construction of the dominator tree](https://github.com/bollu/tiny-optimising-compiler/blob/master/src/TransformMem2Reg.lhs#L151) algorithm. There is an ambient data structure that is repeatedly used in this algorithm: the `DomTreeContext`. Passing it _over and over_ would be really painful and would make the code much more verbose, so I choose to make it `Reader`. Here is the [use site of this with `do` notation](https://github.com/bollu/tiny-optimising-compiler/blob/master/src/TransformMem2Reg.lhs#L185) If I didn't have `do` notation, I would have had an extra parameter to each of these functions, which would be way more clunky. It stops you from having to consider this extra `DomTreeContext` structure, so the code is much more readable.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [bollu/tiny-optimising-compiler/.../**TransformMem2Reg.lhs#L185** (master → ee173fd)](https://github.com/bollu/tiny-optimising-compiler/blob/ee173fda3c17c0752854f97cb14fb64614a80d65/src/TransformMem2Reg.lhs#L185) * [bollu/tiny-optimising-compiler/.../**TransformMem2Reg.lhs#L151** (master → ee173fd)](https://github.com/bollu/tiny-optimising-compiler/blob/ee173fda3c17c0752854f97cb14fb64614a80d65/src/TransformMem2Reg.lhs#L151) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmkhuos.)^.
It's automated. Once you get to building bigger programs, it's nicer to not have to pass the configuration into every function by hand.
I wrote a post showing how you can use Liquid Haskell to catch out-of-bounds errors at compile time: http://www.haskellforall.com/2015/12/compile-time-memory-safety-using-liquid.html You should also check out the official tutorial if you haven't already: http://ucsd-progsys.github.io/liquidhaskell-tutorial/01-intro.html
I've found `ReaderT` to be much more useful when building mtl-style effects. `reflex(-dom)` has a number of good examples where they define effects with a class, and implement them with a transformer. Many of them are implemented just as a newtype around `ReaderT` in order to get the `Monad`/`Applicative`/`Functor` instances for free, and then the environment is used to get the info necessary for the effect instance.
&gt; Bonus points for links to a small readable example for LH beginners The front page has a number of small examples that you can mouseover to see how they work, and links to blog posts explaining the concepts behind each one: https://ucsd-progsys.github.io/liquidhaskell-blog/ &gt; Can someone please explain separately WHAT LiquidHaskell adds to Haskell, and HOW I could currently use those features? Sure: we interpret 'special' comments (liquid type annotations) to let you check assertions (logical propositions) at compile time. So, for example, (taken from the [tutorial](http://ucsd-progsys.github.io/liquidhaskell-tutorial/03-basic.html)): {-@ type NonZero = {v:Int | v /= 0} @-} {-@ divide :: Int -&gt; NonZero -&gt; Int @-} divide :: Int -&gt; Int -&gt; Int divide _ 0 = error "divide by zero" divide n d = n `div` d `divide` here is just a haskell function that wraps `div`, and the funny comments enclosed in `@`s are "liquid type annotations": the first declares a type alias `NonZero` for `{v:Int | v /= 0}`, that is, the type of `Int`s that are not equal to zero. The second gives a liquid type to `divide`: it takes an `Int`, a `NonZero`, and returns an `Int`. Now if you save that code in `div.hs` and add main = print (divide 3 0) This should be invalid, because the type of `divide` says that the second argument must be `NonZero`. Indeed, liquidhaskell rejects it: anish@goto:~$ liquid div.hs [snip] **** RESULT: UNSAFE ************************************************************ /home/anish/div.hs:1:15-24: Error: Liquid Type Mismatch 1 | main = print (divide 3 0) ^^^^^^^^^^ Inferred type VV : {VV : Int | VV == (0 : int) &amp;&amp; VV == ?a} not a subtype of Required type VV : {VV : Int | VV /= 0} In Context ?a : {?a : Int | ?a == (0 : int)} But if we change `0` to `1`, main = print (divide 3 1) this becomes a valid program, and liquidhaskell can verify that: anish@goto:~$ liquid div.hs [snip] **** RESULT: SAFE ************************************************************** 
UCSD has a nice page with some motivations and interactive examples here: http://ucsd-progsys.github.io/liquidhaskell-tutorial/01-intro.html 
You should use Just Enough Functional Programming. I would also advise breathing Just Enough Air and eating Just Enough Food.
To come back you need to get a new invite. If a user is inviting too many people who get banned then they will get banned as well. The whole invite tree is public so you can see where trolls come from.
I don't think that's an inaccurate description for posts on Medium in general. All the content that I've read on Medium is far too hand-wavy, vague, or worse gibberish. &gt; Code that is 100% pure, immutable, parallelisable and free of side-effects doesn’t guarantee a working production system. I know of no working application, production or otherwise, that is free of side-effects. Not even, the well known "Hello, World!" program is free of side effects. &gt; I’ve seen incredibly beautiful functional code doing the wrong thing, creating too much coupling between components of the application, or not handling failures at all. There is nothing in the above statement that's inherent to FP. Replace "functional' with "imperative" or "object-oriented" or whatever and it applies equally well. In the end, I find these observations and anecdotal contrivances not very meaningful for a fruitful discussion about FP or programming in general. 
I didn't dig into your code very hard, but did you consider using GHC generics instead of TH? Nothing in your description flags needing TH to me.
I guess then that linear algebra would be a suitable use case for liquid haskell, where many operations between tensors are not defined if their shape is wrong. This would help skipping many runtime errors.
Ah, nice. I like that. 
Have you looked into [Dhall](https://hackage.haskell.org/package/dhall-1.6.0/docs/Dhall-Tutorial.html)?
Is it any chance LH would be integrated in mainline GHC?
It seems nice.
I agree, although OOP ideally helps in aligning the mental model of the programmer with that of the business or domain expert. DDD and its ubiquitous language help in achieving that even further. Sadly, there are very few people who know how to apply these principles correctly while still keeping their eye on the correct architecture and programming solution.
And in addition to all that, you can usually argue equally well that both mutability and immutability "models reality" depending on which level of abstraction you pick, and what you assume about the surrounding code. The article says: &gt; In the real world, if I punch you in the face I will change your state in place. I won’t be creating a new version of you with a black eye. But is that really true? I mean, if someone punches me in the face, and three seconds later my cousin in Australia gets asked to describe me, will they describe me as "short, brown-grey hair, pointy nose and one black eye"? Nope. "Reality" has synchronisation mechanisms that you don't get with plain mutability. Plain mutability implies various ridiculous things like * Knowledge spreads nearly instantly and telepathically everywhere in the universe, and * If you speak of something right at the same time as it's changing, you may construct half your sentence based on the old value, and half on the new value, among other things. Immutable values are not incompatible with propagation of updates. In fact, propagation of updates can often be done more sanely with immutable values, which leave you in full control over the propagation mechanism.
That would be interesting to see because as far as I can tell `data N = N !O` has exactly the same inhabitants as `O`. Perhaps some code could break if it depends on unsafe, GHC-specific, things?
The closest I've seen to "purity backfiring" in around 5 years of playing with Haskell is realizing one value of type `ListT (Writer (Endo [Board])) Board` actually had to be changed to `MonadRandom m =&gt; ListT (WriterT (Endo [Board]) m) Board` in order to do everything I wanted it to.
Thanks! That looks great! Ok, I can characterize now what exactly my library does in contrast to Dhall. Dhall is a (neat) custom language, I am using YAML. Dhall lets you combine records with (//), but it combines Dhall-records, which you can not access directly as it looks to me. My library gives this feature to Haskell-records directly and you don't need anything special to access the values afterwards. Each configuration file in yaml-conf is like a Dhall-record with a subset of the fields and a set of such configurations is folded together using my equivalent to (//). But what you get is a plain Haskell record type in the end. You don't need stuff like input auto "{ foo = True, bar = 2, baz = 4.2 }.baz" :: IO Double to read a field.
I indeed considered it, but I don't see how to do what I want with Generics alone. An explicit design decision was that every field in a config is optional and each config can be seen as an update/diff relative to some other. But then for every field I need to be able to state "do nothing, this field does not change", that is why I need a different, related data type that has each field wrapped with Maybe. Can Generics generate new data types and inspect whether a type has some instance? Maybe I am not creative enough or did not find a good tutorial, but I don't see how to achieve that with Generics. In fact, I would be happy if you would prove me wrong!
A GADT is probably what you want here. Tag your request type with two phantom variables for version and key. Same for the response type. Then you just need to write a function `Request api ver -&gt; IO (Response api ver)`. The compiler will make sure you don't mess up. Of course, you need to make sure you don't ignore exhaustiveness warnings.
&gt; Not even the well known "Hello, World!" program is free of side effects. Actually, it is. Strictly speaking, "side effect" refers to functions, e.g. to the semantics of the type `a → b`. In Haskell, functions are always pure, no side effects. Now, the `IO` type constructor models computations with effects, but the nice thing about it is that is is completely unrelated to the function type constructor.
While LiquidHaskell seems to be geared towards arithmetic type-level computations like bounds checking, it is actually very close to a full [proof assistant][1]! [1]: https://arxiv.org/abs/1610.04641
In my opinion everything in `~dons` should be salvaged. Quite a few libraries on hackage link to repos and documentation there. Probably also Ian's stuff, but I'm not sure if it was all in his home dir. Please contact Duncan and ask him what else needs to be salvaged. Could I please have my own home dir so I can move it somewhere else? I have a few repos on hackage that point there? Thanks.
Reader monad seems to simply allow wrapped action for readers, is it right?
Also, the universe can be viewed as a pure function of time. Physicists actually model the world as closer to this view of the world than the "mutate-in-place" view of the world.
&gt; Even if you program in IO all over you are just in the same place as any other language. That's not really true - because Haskell is very suboptimal for this kind of programming. In C: foo[x[i]][y[i]] = effect(); In Haskell this becomes: xi &lt;- readArray x i yi &lt;- readArray y i e &lt;- effect fooxi &lt;- readArray foo xi writeArray fooxi yi e It could be nice to have `!` sugar in Haskell that allows you to write the same as: writeArray !(readArr rfoo !(readArr x i)) !(readArr y i) !effect And then think about operators for `readArr` and `writeArr` and that could look nearly as nice as C with syntactic annotations about where effects are happening.
As does FRP
I would say that an "action" is distinct from a "function" in that it may return a different result each time it's called with the same arguments (think: sampling a random number or performing IO). This distinction in turn enables some powerful optimizations, such as functions being referentially transparent and memoization of function results.
&gt; don't ignore exhaustiveness warnings. In general, anything 'live' should have been complied with Stack's `--pedantic`(or similar). 
"Action" is just an alternative name for "monadic value", really. It is however typically used when the monadic type in question represents some sort of sequential computation, e.g. `IO`, `Cont`, `State`, etc.; you will not see people use the word "action" a lot when discussing list monads or Maybe (although it would still be equally valid). In other words: anything with a type signature that matches `Monad m =&gt; m a` is an action, but the following are not: - `m a` (not a `Monad`) - `a` (not a `Monad` either) - `Monad m =&gt; b -&gt; m a` (not a value, but a function; we can however say that this function *returns* an action, and we often call such functions "monadic functions" - but then, the term "monadic" is a bit vague, it means little more than "Monads are involved") So no, "action" does not generally equal "promise", but it does for some monads (e.g. `Cont`), because promises *are* conceptually monadic, and `Monad` instances can typically be written for them. When discussing promise / continuation monads, then your question makes sense, and the answer is yes, monadic promises guarantee that a promise either realizes to a specific type, or errors to another specific type. So for example, if you have `ajaxRequest :: Request -&gt; Cont Response`, and `handleResponse :: Response -&gt; Cont ()`, then `ajaxRequest &gt;&gt;= handleResponse` will type-check, but `ajaxRequest &gt;&gt;= (\x -&gt; return (x + 1))` will not. Or, put differently, given these type signatures, `ajaxRequest` is not allowed to resolve to anything other than a `Response`.
They are more different than that. Functions are relationships that associate each possible input (parameters) with a specific output. They do not *do* anything. Actions, on the other hand, do things; but don't have parameters at all! So basically, actions and functions are nothing alike. They do, however, go together well. For example, putStrLn (type: String -&gt; IO ()) is a function (not an action), but its results are actions. So (putStrLn "Hello World") is an action.
`tr` `ty` `tbl``tnf` `tynf` `lvl``ctx` `xnf` `tyw` `nwhnf` `ty1nf` `atynf``tr` `tr'` `tr1` `ity` `itynf`... Seriously, what is this type of encryption? Is the author afraid somebody could accidentally understand his code?
In short, Liquid Haskell allows to constraint values of a type and propagate it. For example, you might be interested only in positive integers, odd ones. List with at list 3 elements etc ... Let's say you have a function which creates an hardcoded list with 1 element. You know that concatening two list increase the size of the list. Therefore, if you take the original list and add it to another one, you have a new list with at list one element. This mean that it should be safe to get one element back from it. In another word, it's safe to call `head` on `[a] ++ something`. It would be nice if the compiler could check and ensure that. That's what LH do. 
If nothing, one deterrent is the reliance on an SMT solver, which is a dependency GHC isn't likely to take on lightly.
That's within the reach of modern Haskell (without inordinate amounts of pain): https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html
Core dump was probably not such a good idea, it consumes as much as all of the virtual memory which is 1TB. Not enough free disk space ..
When I switch to thread view in top, I get this: PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 11478 gonimob+ 20 0 1.001t 9972 1800 R 99.0 0.5 266:55.25 ghc_worker 28034 gonimob+ 20 0 1.001t 9972 1800 R 99.3 0.5 266:49.09 ghc_worker 
Are the slides available somewhere? [Googled for them, nothing obvious found.]
And in physics too
I guess that's true.
Do you have app-server AND apache/nginx logs? Can you share the logs from the timespan of hang +/- 5 mins? Also, do you have CPU and memory monitoring on your servers? Can you share +/-24 hour graphs of CPU &amp; memory?
if only Reality devs would implement a time travelling debugger ...
Historically I've mostly worked with mutable structures in Haskell in the context of concurrency and so really prefer the Haskell version where the order of effects is clear (you'd probably expect some memory barriers too in that context if you were concerned about the order of reads, but just pointing out the C version instantly made me uncomfortable for that reason).
[nginx log](http://lpaste.net/3007847682803761152) is not very helpful. Messages before have nothing to do with the problem and the ones after only say timeout. timeout messages go on and on. (Clients are up and requesting) CPU and memory monitoring: Unfortunately not yet - we hopefully have time soon to take care of this. Any system you would recommend? What I can say, memory consumption currently is just fine at about 10MB. Actually that's weird! On the production server the same process consumes 16MB, so it seems the crashed service consumes less RAM than a working one. In what way would a CPU &amp; memory graph help? Almost forgot, app server logs can be found [here](http://lpaste.net/358222) - looks normal too. Output simply stops suddenly.
&gt; what could cause such a behaviour Deadlocks in concurrent code, improper exception handling... What is supposed to happen when `handleRequest` fails in https://github.com/gonimo/gonimo/blob/a993c842b23001bd07ee372e7382f5f69f79f672/back/src/Gonimo/Server/Subscriber.hs#L122? &gt; how to debug this Since you can't easily reproduce it I would start with adding logging to help you check that main code invariants hold.
The order of effects can be clearly specified in the C syntax too (even though C does not). So clear order does not preclude concise syntax.
I found [the slides](http://wr.mondet.org/slides/Compose2017-BioTTFI/smondet-biottfi-compose17.pdf) by googling "we trivially and elegantly solved that problem 20 years ago" from the freeze-frame.
Any functional programmer that get serious about purity and immutability should buy a new computer for every new program :)
That's true, though the SMT solver could be an optional dependency. There are advantages to integration with GHC beyond better integration into the compilation pipeline. For example, I would love for Haddock to be aware of liquid types.
&gt; timespan days to weeks Maybe a race condition? It can be quite unlikely for the threads to interleave in just the right way for the bug to occur, so you might have to wait a while until it does occur. For example, the following code deadlocks after an amount of time which is both random and proportional to `threadDelay`'s argument ([plot](http://fooplot.com/#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--). X is threadDelay's argument, Y is the number of seconds before a deadlock). import Control.Concurrent import Control.Concurrent.Async import Control.Monad import System.Environment mkThread :: Int -&gt; String -&gt; MVar Int -&gt; MVar Int -&gt; IO (Async ()) mkThread microseconds threadName mvar1 mvar2 = async $ forever $ do threadDelay microseconds n1 &lt;- takeMVar mvar1 n2 &lt;- takeMVar mvar2 print (threadName, n1, n2) putMVar mvar1 (n1 + 1) putMVar mvar2 (n2 + 1) main :: IO () main = do microseconds &lt;- read . head &lt;$&gt; getArgs mvar1 &lt;- newMVar 0 mvar2 &lt;- newMVar 0 thread1 &lt;- mkThread microseconds "thread1" mvar1 mvar2 thread2 &lt;- mkThread microseconds "thread2" mvar2 mvar1 wait thread1 wait thread2 &gt; our backend hangs. It will consume all CPU time In most languages this would be the symptom of an infinite loop, but GHC is pretty good at detecting infinite loops: -- | -- &gt;&gt;&gt; fib 10 -- error: &lt;&lt;loop&gt;&gt; fib :: Int -&gt; Integer fib n = fib (n-1) + fib (n-2) So you must be having a *productive* infinite loop instead, one which computes something and looks like it has an exit condition, but the computation is such that the exit condition will never be reached. -- | -- &gt;&gt;&gt; fib (-1) -- (hangs and consumes CPU) fib :: Int -&gt; Integer fib 0 = 1 fib 1 = 1 fib n = fib (n-1) + fib (n-2) -- | -- &gt;&gt;&gt; fib 10 -- (hangs and consumes CPU) fib :: Int -&gt; Integer fib 0 = 1 fib 1 = 1 fib n = fib n + fib (n-1) &gt; and won't serve requests any longer That seems like an important clue! If each request is forking a separate thread and the productive loop affects one of those threads, it shouldn't block the other requests, it should merely make them slower because some of the CPU is being used up. So I see three possibilities: either your main thread is looping, or there is some lock which the main thread needs but is held by the looping thread, or your main thread successfully forks threads but all threads need a lock which is held by the looping thread. Some extra logging in your main thread should allow you to distinguish between those three scenarios, and should give you a better idea of which computation is looping or which lock is being held. Good luck!
First off, I am a complete novice in Haskell. So my suggestion might waste your time. It appears that you might have a "space leak", and here is [a paper about learning how to manage it with arrows](https://pdfs.semanticscholar.org/cab9/4da6e4b88b01848747df8297fccf1ea6b35a.pdf). The wiki has an article on [memory/space leaks](https://wiki.haskell.org/Memory_leak) (I'm not that sure of the difference in terminology myself). You also might find [this information about space leaks useful as well](https://github.com/ndmitchell/spaceleak).
How do you tell which monadic context you are in? Is it related to the first `do` statement? How do you take value out of a Maybe monad? For example: in a snippet: type TaskMap = Map.Map String Task getDiffs :: TaskMap -&gt; TaskMap -&gt; Map.Map String (Maybe Bool) getDiffs olds news = Map.mapWithKey diff olds where diff key value = do newTask &lt;- Map.lookup key news let hash = pageHash value let hash' = pageHash newTask return (hash /= hash') How do you convert the `Maybe Bool` to `Bool`? Also, how do I print values in a Maybe monad? something like _ &lt;- return (print hash) doesn't work.
Well, in this particular case: It will cause the WebSocket connection to get terminated, causing [cleanup](https://github.com/gonimo/gonimo/blob/a993c842b23001bd07ee372e7382f5f69f79f672/back/src/Gonimo/SocketServer.hs#L82) to run. I will check further. I think I am only using STM (websockets use IORefs I believe), so deadlocks should be pretty unlikely - right? In any case, it looks a lot like a dead lock. I'll check the exception handling and all shared state. Thanks for the tip! More logging will certainly help too!
What makes you think this is a space leak? What would the symptoms of a space leak look like?
What makes you think this is a deadlock? What would the symptoms of a deadlock look like?
&gt; eating Just Enough Food Never thought Haskell would help with my dieting, yet here we are. Is there anything it can't do?!
I was surprised not to see someone mention threadscope and events, this might tell you things like how many active threads there are, GC pause times etc.
I didn't realise the code/projects domains were being used to access personal directories. All user directories were still publically accessible via community.haskell.org: http://community.haskell.org/~dons/haskell-1990-2000/maillist.html However, I've now restored redirect exceptions on code/projects for URLs starting with /~.
Nothing has changed for SSH access and I've added redirect exceptions for /~user dirs which I wasn't aware were also being served on project/code domains. If you're having any issues migrating – please ping me and I'll be happy to assist.
Ah, OK got it. So this isn't a catastrophic shutdown at all. Yet. It was a good exercise though. We really ought to finish moving stuff off of that old server. I'm one of the guilty parties, though I think for me it's only one or two web pages now.
[Here's the link I was thinking of](https://wiki.haskell.org/Newtype). Very similar but not quite exactly the same
Using a ton of RAM. As he said it's &gt; Core dump was probably not such a good idea, it consumes as much as all of the virtual memory which is 1TB. Not enough free disk space ..
No nothing _intentionally_ catastrophic – but nobody is very comfortable with the state of that server, so no promises!
No-one has mentioned http://hackage.haskell.org/package/ekg, which allows you to monitor GHC runtime over http. Could be helpful (although I haven't used it much).
I don't think I'm going to be very helpful and it's unlikely you overlooked this but is your timeout message the message that's in this line: https://github.com/gonimo/gonimo/blob/master/back/src/Gonimo/SocketServer.hs#L229
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [gonimo/gonimo/.../**SocketServer.hs#L229** (master → 9c178db)](https://github.com/gonimo/gonimo/blob/9c178dbe6039cf6abef6b93723678d06514146e6/back/src/Gonimo/SocketServer.hs#L229) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmldl8b.)^.
How useful is this when you're not using only literals? My wild guess is that in general it is undecideable whether a function application evaluates to 0.
Thanks for this tip! I was using `-W -Werror` instead of `-Wall -Werror`. The latter looks like it's equivalent to Stack's `--pedantic`. I just went though and fixed all of my name shadowing problems, missing top-level type declarations, and unused-do-bind violations.
resident memory is very small though. Its posted in another comment. Would that really cause memory issues. 1tb virtual usage is real weird though.
Awesome. I'll try this out. Thanks!
What does the stack look like if you strace the hung process?
Hmm, I'm out of my depth here. You're saying that there are two kinds of memory, the "resident memory" and the "virtual memory", and that the fact that the core dump is 1TB means that the application uses 1TB of "virtual memory", while also only using 10MB of "resident memory"? What's the difference between those two kinds of memories, is it just that the "resident memory" includes the pages which the program is actively working on and thus bringing back into RAM each time it touches something in them, while the "virtual memory" includes the pages which the program isn't actively working on and have since been paged out? If so, I guess that indicates that the program is producing a lot of garbage, as most FP programs do, but that some reference is keeping all that garbage alive? Could it simply be the stack, which is growing indefinitely because the looping thread I'm suspecting isn't tail-recursive?
But would the program hang?
A promise or future will give you the result once, if and when it is resolved. Whereas a monadic action can be run multiple times. It represents a way to get that value, plus possibly some side effects, in the [Monads as Computation](https://wiki.haskell.org/Monads_as_computation) meme.
Why would you want to use generics when TH is a good fit? I find TH easier than generics. Traditionally the biggest objection to TH has been that it is deeply GHC-specific, but that is also true of generics.
Sure, this may help: http://www.parsonsmatt.org/2015/11/15/template_haskell.html
It can make a literal but will the literal compile into zero-initialization data? Last time I checked it did not
I only know Linux and I don't know about how Haskell programs actually run. I guess there is something like a vm since it is garbage collected? A real quick overview for virtual memory. On Linux since multiple programs can run memory address space is abstracted away from the real memory. A program can ask for memory from the OS through malloc/sbrk. It gets a virtual address to use that isn't mapped to real memory. When the memory address is actually used the kernel will use real memory for that virtual address. When it's used its part of resident memory. That's really rough. There's better articles and info around memory in Linux. I don't think it would be the stack. It would overflow? That would also be memory written to and it would be part of resident memory. Oom would be triggered eventually too. If rss is spiking and dropping it could be gc. Idk much about memory management in Haskell programs. Just a little but about jvm and nodejs Could be a data base issue. If connections to the db aren't being cleaned up properly 
You can do it like this: https://gist.github.com/chpatrick/bd1569f6f3e322aa1423
You can make raw byte string (not to be confused with `ByteString`) constants that AFAIK have no initialization: {-# LANGUAGE MagicHash #-} import GHC.Exts data AnAddr = AnAddr { getAddr :: Addr# } {-# NOINLINE v #-} v :: AnAddr v = AnAddr "some bytes here"# main = let sixthChar = C# (indexCharOffAddr# (getAddr v) 6#) in putStrLn [sixthChar]
Well, I have never used Generics, from my perspective it looks like you can get some of the stuff you can get with TH in a "cleaner" way, TH, while more powerful than C++ templates, feels more like using the C preprocessor in the fact that is just pure syntactic replacement. Well, at least it is syntactically correct, but I have no assurance whether it "clicks" with the other stuff as it should. When I do it right, it is awesome, but when I make mistakes, it blows up with error messages that I can only get information from if I have a clear idea where I constructed what in which way. I'm sure this is a common criticism but it does indeed feel like more of a hack. I thought that working with Generics might be more pleasant. But I don't know, have not tried it yet. Maybe I am just expecting too much from what assistance can be given for such meta programming.. 
In the latest episode of The Haskell Cast, there is some discussion about Liquid Haskell, and how it compares to dependent types in the language itself. I recommend a list because the podcast is awesome in general. http://www.haskellcast.com/
Hello, I am really interested in this project!
Clever!
strace! How could I forget about that! I'll try!
[hmmm](https://gist.github.com/eskimor/6be92109c23d55a35150d58514b7c947) .... 
Most of those are pretty obvious in context, or they rely on some technical knowledge. `nf` means "normal form", `whnf` means weak-head normal form, `nwhnf` means name-removing-whnf, (which I infer from the comments), `tr` means term, `ty` means type, `ctx` means context, `tbl` means table, `lvl` means (De Bruijn) level. `x` is just a standard variable name, so `xnf` is the normal form of whatever `x` is. Though the names could be longer, I don't think they're that illegible as-is.
The best way of finding out which monadic context you are in is looking at the type of your expression. But looking at the type of the right-hand-side of the first assignment with `&lt;-` might give a good hint. You can use [maybe](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Maybe.html#v:maybe) to convert `Maybe Bool` to `Bool`, by also providing a default value. This won't work for all monads though. Edit: As in, not all monads will have a function `m a -&gt; a`, an example being `IO`. `Maybe` has a show instance so just adding a statement `print hash` should work. Wrapping a `return` statement around `print hash` is probably not what you want, it nests it in another layer of monad. (return . print) :: (Show a, Monad m) =&gt; a -&gt; m (IO ()) Edit: for the printing part, /u/mbruder 's answer is probably what you want, I misunderstood your question I think.
In theory: TH is very different than C++ templates, and not a hack at all. It's a language generating its own syntax trees. That is arguably the single most important feature of Lisp, for example. Of course our TH will never be the same as Lisp macros, and shouldn't be, but that is still a much better analogy than C++ templates. What we really need is [typed TH](https://ghc.haskell.org/trac/ghc/wiki/TemplateHaskell/Typed), but unfortunately work on that seems to have slacked off. In practice: For a library, I find TH easier for users than generics. While constructing the splices/quasiquotes can be a bit tricky for the library author, once it's done correctly the user code is simple and intuitive. Whereas with generics, the complexity leaks into user code. For application code I wouldn't recommend either one.
If it's supposed to be constant across all runs of the program can you not write a separate script that generates it and before compilation append a literal `myList :: [whatever]\nmyList = whatever was generated\n` to the .hs file you want it in? Maybe I'm misunderstanding.
I think this is a great work since it will be a first step for crypto people to use Haskell if they do not know it well. ( But some people might know well like him.) Since I have not earned enough karma, I would like to post here. Maybe I have had a similar philosophy since 2014. The following text is what I want to do and the reason why Haskell is important, which I tried to drop into r/ethereum and r/cofounder and both failed owing to karmas: (I am familiar with cryptocurrency to some extent but I do not intend to use Haskell only for them but also for some more computational things; e.g. rendering 3D or solving equation with FEM) =============================== Hi, I am an old ethereum boy who is now interested in the development of education. Until 2013, I was a carpenter, a labor in construction sites for 4 years. Since I had been a mathematics teacher for Japanese doctor college entrance exam applicants and had graduated from a most educated university in Japan, I could see what Vitalik described in his papers exactly. When I saw solidity at first time, I felt some kind of out-of-date prospection compared with a language backed with type theory and there had been a lot of works of category theories. Then I had already started to learn what functional programming was in a modern situation which was supported by category theory rather than by set theory. Here I mean Haskell and ml rather than lisp. In addition to category theory, there is formal category theory, which handles 2-morphisms which is an abstraction of natural transformation in category theory. This 2-morphism abstraction allows varieties of a language in a very formal and structured way. Since I am a house architect, what I want to do is not only studying theorem provers but also studying general languages backed with algebra (to adopt into designings) which is built on type theoretic foundation, I mean a very strict and formal way unlike set theory. I mean Haskell quite at all because it will support dependent types in the future. Since it is also a top world talented project, I want to make a company which studies mainly about Haskell and category theory. In the future it may have some researches of FEM or some 3D rendering, geometric representation of algebraic geometry or matrix representation of some transformations in a more educational way. Now I come here London and searching a way to start my business, because of my nationality I need a help of UK guys or European people. Or I want to see those who are eager for such things in UK or in Europe. If you have got interested in me, please send me some messages. Thanks.
I would probably choose neither and go for `megaparsec`.
Okay, "typed TH" is probably what I would want, too. And a limitation I really dislike is that I can not have a TH function that generates some declarations, AND get it into the context immediately. I had to work around it by generating names of stuff that is going to be spliced in the instances. Especially ugly is the fact that I can not combine makeConfig und deriveConfigJSON into a single function to be called by the user, because when deriveConfigJSON is called the type it is to create instances for does not exist yet! That annoyed me the most. I would like to be able to say "splice this NOW, so that I can reify it again, immediately". I can not work off the stuff I am generating without breaking it up into multiple splice commands, so I can not abstract stuff away as much as I'd want to. What I also dislike is that with TH the ordering does play a role, so that I have to generate instances bottom-up for nested stuff for it to work. But I accept that this is probably inevitable.
&gt; How do you convert the `Maybe Bool` to `Bool`? See the `Data.Maybe` module for several ways (typically `fromMaybe False` will do). &gt; Also, how do I print values in a Maybe monad? You can't. See it as a pure computation that can fail. Outputting text is only possible in `IO` (`putStrLn :: String -&gt; IO ()`, `print :: Show a =&gt; a -&gt; IO ()`). You can still collect log messages with `Writer` or `WriterT`. In the following way (by using `traverse` or a special version in this instance): let loggingFun :: String -&gt; Int -&gt; Writer [String] Int loggingFun k v = do -- Log the key when the value is bigger than 1 when (v &gt; 1) $ tell [k] -- Somehow produce a new value, might as well be your equality on hash thingy. pure v in runWriter $ M.traverseWithKey loggingFun $ M.fromList [("foo", 1), ("bar", 2), ("baz", 0), ("qux", 3)] The result is a new map and the log: (fromList [("bar",2),("baz",0),("foo",1),("qux",3)],["bar","qux"])
We're having a similar hang at work. The process just stops working. Also happens once a week. I'm suspecting that there's a partial/infinite loop function happening somewhere, but it's not throwing an exception. Naturally, the problem only seems to happen in Docker, which makes debugging this more annoying...
The issue is that `myList` will still have to be evaluated every time the program is run. When you have large quantities of raw data, you would like to just store the bytes of the data in the executable instead of generating assembly that run every time to build up `myList` in memory.
Welcome ghasshee!
Someone already mentioned this, but a GADT is probably the way to go for this kind of thing. Also, I noticed a repeated field in all of your data constructors. This can always be factored out. Instead of: newtype ApiVersion = ApiVersion Int16 data KafkaRequest = ProduceRequest ApiVersion Int16 TimeoutMs [TopicData] | FetchRequest ApiVersion Int32 Int32 Int32 [FetchRequestTopic] | OffsetListRequest ApiVersion Int32 [OffsetListRequestTopic] You may consider writing: newtype ApiVersion = ApiVersion Int16 data KafkaRequest = ProduceRequest Int16 TimeoutMs [TopicData] | FetchRequest Int32 Int32 Int32 [FetchRequestTopic] | OffsetListRequest Int32 [OffsetListRequestTopic] data VersionedRequest = VersionedRequest { versionedRequestVersion :: ApiVersion , versionedRequestRequest :: KafkaRequest } It might not be helpful in your situation, but it's worth considering. This factoring out of common fields is equally applicable for GADTs.
I think the bare minimum is to explain each name in a comment, when it is first used.
Why stop there? Everytime you run a program, the universe changes slightly. You'll need to throw out the whole universe and start from scratch if you really want purity.
So what's the difference between megaparsec and the other two?
It would be nice if there were pragmas like `{-# WHNF foo #-}` or something like that. Where something will be evaluated at compile time without any TH or similar hackery.
http://kormacode.blogspot.com/2011/11/c-style-haskell_10.html Don't forget you can write all the operators and type classes and even type families / GADTs etc. that you want to suit whatever style and paradigm of coding you are using.
It is generally a bad word to use because Haskell values don't "do" anything.
Do you mean that {-# WHNF foo #-} foo :: Int foo = fib 30 would expand to this? foo :: Int foo = $(lift $ fib 30) 
`putStrLn "hello"` doesn't do anything either. It is a value, like `"hello"` or `True` or `1`.
Thanks, l just send email to the company of these nice guys. Quite nice thing I think.
https://github.com/mrkkrp/megaparsec#comparison-with-other-solutions
The paradox is that it is necessary to program garbage collectors using imperative constructions for highly impure pointer management to create the fiction of infinite memory. That is required for the leaky abstraction of purity. Many haskellers are not fully conscious of that. Managing impurity internally is key in the creation of useful libraries with controlled effects. Many pure but efficient data structures and STM for example, use internal mutability.
I mean the implementation isn't important, internally the compiler can do this as simply or as magically as it wants. The key thing is just that the binary would have the WHNF baked into it, and if that evaluates other things in the process those are also kept hardcoded into the binary as opposed to recalculated if they are referenced in a different part of the program. Which I'm guessing is what `$(lift $ foo)` does?
no the message comes from ngnix, but thanks anyway!
you are right, it can't be a dead lock with locks, because then there would be no CPU load.
That was really helpful, thank you!
No, no. It's not using much RAM at all. Haskell processes allocate 1TB virtual memory by default on 64 bit. It does no harm, except when you want a core dump :-P
I am not sure that this will work with an occurrence rate of once in a few weeks, but I will have a look! Thanks!
Ah, interesting, thank you. Not something I'm familiar with.
 meal :: Maybe Enough Food
Forgive me for asking a rather simple question. Is the difference between the liquid haskell variant of the code and the plain haskell code being while plain haskell gives you a runtime error, LH gives you a compile time error? For example, the following plain haskell code divide :: Int -&gt; Int -&gt; Int divide _ 0 = error "divide by zero" divide n d = n `div` d does indeed throw an exception because of the pattern matching. So, I'd assume the `liquid` type `NonZero` must help at compile time exception handling. Is that so? In any case I, being ignorant of LH, would like the type of `divide` (once you have defined the LH type for `NonZero`) to be `divide :: Int -&gt; NonZero -&gt; Int` i.e., the special liquid types created in LH should've carried over into regular haskell. Why is it not so? To be able to run the same code in plain haskell if one wishes?
It may be a bit of a hack, but I’ve done something like this with the FFI before: // lookupTable.c double const lookupTable[] = { ... }; size_t const getLookupTableSize(void) { return sizeof lookupTable / sizeof *lookupTable; } double const *getLookupTable(void) { return lookupTable; } -- LookupTable.hs foreign import ccall unsafe getLookupTableSize :: IO CSize foreign import ccall unsafe getLookupTable :: IO (Ptr CDouble) lookupTable :: Ptr CDouble lookupTable = unsafePerformIO getLookupTable 
Can someone help debug my Haskell code? I suspect at some point my list of even numbers fails to work. import Data.List import qualified Data.Map as M -- Taken from https://wiki.haskell.org/Prime_numbers -- based on http://stackoverflow.com/a/1140100 primesMPE :: [Integer] primesMPE = 2:mkPrimes 3 M.empty prs 9 -- postponed addition of primes into map; where -- decoupled primes loop feed prs = 3:mkPrimes 5 M.empty prs 9 mkPrimes n m ps@ ~(p:pt) q = case (M.null m, M.findMin m) of (False, (n2, skips)) | n == n2 -&gt; mkPrimes (n+2) (addSkips n (M.deleteMin m) skips) ps q _ -&gt; if n&lt;q then n : mkPrimes (n+2) m ps q else mkPrimes (n+2) (addSkip n m (2*p)) pt (head pt^2) addSkip n m s = M.alter (Just . maybe [s] (s:)) (n+s) m addSkips = foldl' . addSkip -- end of taken stuff combinations n = do let p = take n primesMPE a &lt;- p b &lt;- p return (a, b) evens n = filter (\x -&gt; x `mod` 2 == 0) (sort (nub (map (\(a,b) -&gt; a + b) (combinations n)))) 
There are subcategories of Hask such as for example `a -&gt; [b]` or `[a] -&gt; [b]`. You can have a mapping between them such as `(a -&gt; [b]) -&gt; ([a]-&gt;[b])` which is just monad stuff.
&gt; I mean the implementation isn't important, internally the compiler can do this as simply or as magically as it wants. Discussing a concrete implementation helps to iron out possible misunderstandings over the behaviour which the feature should have. For example, the name "WHNF" suggests that you only want the value to be evaluated to weak-head normal form, but I suspect you want the fully-evaluated normal form instead. The other reason I asked about this concrete implementation is because as others have pointed out, a literal at the top-level can still cause evaluation to occur when this literal is first accessed, for example `foo = [1,2,3]` might still become `1 : THUNK` after the head is printed. If this doesn't work for you, because you really want the fully-normalized value to be inside the executable, then this feature might be harder to implement than you realize. The cons cell still needs to be implemented as a pointer to a value and another cons cell, but if you don't want this second cons cell to be a thunk which allocates the next cons cell into memory when first accessed, and you really want the first cons cell to point to a byte offset into the executable, then it seems to me like this creates a new kind of pointers which the runtime will need to know how to handle. &gt; Which I'm guessing is what `$(lift $ foo)` does? [lift](https://www.stackage.org/haddock/lts-8.19/template-haskell-2.11.1.0/Language-Haskell-TH-Syntax.html#v:lift) converts a value into an AST representation of that value. `$(...)` then splices this AST into the code, so the net result is the same as if you had written a literal there. If the expression evaluates other values, those other values may or may not end up in the resulting AST; it's the AST of the result of the expression, not the AST of the expression itself.
Be sure to try if it reproduces with GHC 8.2.1 - it fixes https://ghc.haskell.org/trac/ghc/ticket/13751. If this is the problem and you are stuck on "older" GHC, can try to limit to +RTS -N2 -RTS, since the problem didn't seem to occur with just 2 cores. Edit: the bug name mentions it's a crash, but maybe that's the cleaned-up version. When it happened to me, the program just seemed to hang (all the forked threads crashed). Sorry if my memory serves me bad (and hey, it's always worth trying a new GHC).
No need to post a second comment to point out minor typos! There is an "edit" button under your comments which you can use to correct any mistake.
Thanks. Just I try now.
To be honest: My comment was meant as an addition to your comment. ;)
Yeah I intentionally went with WHNF to allow for more flexibility. You can always call `{-# WHNF rnf foo #-}` or similar. Since WHNF is sort of the fundamental piece of strictness in Haskell that everything is made of. And oh ok, so then not quite like `lift`. I just want to have something the same as adding ``foo `seq` pure ()`` to the beginning of main but where that statement is executed at compile time. So that would include evaluating and thus caching all the things foo depends on. EDIT: formatting. 
What do you gain by including it in the executable? Why not just come up with a fixed byte serialization scheme, store that in a file, and mmap the contents? Then you can just store the byte positions of the data you want in a vector, and fold the vector with a function to access the map and parse the contents. It's more or less the same thing anyway, you just have an extra file. 
I've never seen code commented like that. That policy really doesn't make sense for variables, since they'll all refer to different things if used multiple times. Do you really expect a comment telling you that `x` is a variable when it's first used? That's ridiculous. Of the things you mentioned, the only one that wasn't a variable was be `nwhnf`, which was actually explained in a comment.
that could be it. I will try the non threaded runtime! Let's see whether this helps.
I will tell you if I find the reason. Could take a bit, because, well it takes weeks until I know for sure that a fix worked.
I've had the opposite experience; my last two internships were at Uber ATC (self-driving cars, C++) and Awake Security (network security, Haskell/Nix). Network security, while interesting, is certainly not as impactful/interesting as self-driving cars, yet I enjoyed the latter internship much more than the former. I definitely feel like, at least for me, who I'm working with matters the most, the tech stack is #2, impact is #3, and interestingness of the problem space is the least important.
I suppose you mean that evaluating the expression doesn't do anything as a *side* effect, and that's certainly true. But it is I/O action, and as an I/O action, it certainly does do something -- namely, print to the screen. Much like my merely having a hammer and looking at it doesn't cause nails to drive themselves into boards as a side effect; but I think you could safely say that a hammer *does* drive in nails.
I have no strong preference for how this is accomplished. This extra file method doesn't sound any easier than doing something like Template Haskell. But I will look into both.
FWIW, Kotlin is now an official _Google-supported_ dev language for Android, so it may very well have passed the "threshold of immortality" on that basis alone.
Elaborating on some of the other replies, I'd point out that promises are a subset of "actions" (which I am taking to mean things of type `IO a`), and also that that fact isn't particularly helpful in understanding them. I'd suggest not using that metaphor at all at this stage. Later you can circle back and the connection will be obvious, but I think it's a misleading direction for now.
It is indeed undecidable in general, so liquidhaskell has to do a lot more work when it's not just literals. The view from 5000ft is that we use an SMT solver to try and prove the fact, and if it can't, then we say "unsafe" and give you the opportunity to prove it yourself "by hand". The last example on the [homepage](https://ucsd-progsys.github.io/liquidhaskell-blog/) shows how you'd supply a manual proof of something we can't prove automatically. We just pushed a release that makes this process a little easier with some automation, but I don't think we've documented it very well (if at all) yet.
I know the feeling :D Currently throwing a ton of log statements in there to at least see the last thing it prints before it dies.
&gt; Is the difference between the liquid haskell variant of the code and the plain haskell code being while plain haskell gives you a runtime error, LH gives you a compile time error? This is pretty much it on the nose. &gt; the special liquid types created in LH should've carried over into regular haskell. Why is it not so? To be able to run the same code in plain haskell if one wishes? Right now, the way it works is that `liquid` just reads the comments and the code; the liquid type annotations are just comments from the perspective of the haskell compiler. Ideally, yes, `ghc` would be aware of the liquid types (probably hidden under some `-XRefinementTypes` or some such), but that would require some more work still.
I made a "splash" page with a few small examples to explain the motivation here: https://ucsd-progsys.github.io/liquidhaskell-blog/
Thanks for all your awesome work on vinyl and Frames!
PureScript's `Eff` rows are entirely phantom types, and aren't really comparable with monad transformers or extensible effects.
How does structural subtyping and algebraic subtyping differ? Why would row polymorphism be inferior?
This is really great, thanks! I think it succinctly describes use cases, and quickly conveyed to me the benefits LH is trying to bring to Haskell. 
Fun read! It even includes comparisons with several other parsing libs (attoparsec, Earley, parsers).
http://hackage.haskell.org/package/file-embed &gt; Use Template Haskell to read a file or all the files in a directory, and turn them into (path, bytestring) pairs embedded in your haskell code.
What is the problem with this code? It seems to run just fine.
It might also be quite long.
This looks promising, I will give it a try.
So I tried this and it seems to work as I want it to. By chance the lookup array I was using was made of Word8 characters. Do you know how well how might work for more general data types? Seems like it's best for bytestrings/word8 data.
Well in that case we aren't actually talking about GHC but just other tools in the Haskell ecosystem. It would be neat, but someone would have to step in to perform a rather thankless task that would go unused (or with little use) in the near term.
Space leaks are never going to cause a program to hang, just use lots of RAM and slow down. So this is definitely not a space leak.
Thanks!
The main advantage of `trifecta` is better error messages. There's no real advantage to using `parsec` over `megaparsec`
I should note your code formatting is messed up. Use the four spaces technique instead to avoid that issue.
Dhall lets you read in records into Haskell by deriving Interpret for Haskell records. So, for example, you can define: data Example = Example { foo :: Bool, bar :: Integer, baz :: Double } deriving (Generic, Interpret) ... and then you can do: input "{ foo = True, bar = 2, baz = 4.2 }" :: IO Example
`megaparsec` has been improving in that area recently https://markkarpov.com/post/evolution-of-error-messages.html#displaying-of-offending-line-in-error-messages and while it’s still not quite on the level of `trifecta`, it’s getting closer.
It's a dumb joke asking people to prove the Goldbach conjecture.
Trifecta is apparently beautiful, but it's one of those edward kmett cliches. It's probably great, and super nice, but the documentation is so terse I don't know how to get started. Defenitely worth the try I think, as it apparently has better error messages `parsec` has been deprecated in favor of `megaparsec` afaik and there's lots of material for it. So : are you learning about parsers, I'd go for megaparsec Are you advanced haskeller building a compiler? : trifecta Edit: Ekmett cliche wasn't meant as offending. He makes very nice libraries with beautiful abstractions. But they're very abstract and take time to learn.
FYI, for inline code containing backticks you can use double backticks: ``foo `seq` pure ()`` ``foo `seq` pure ()`` 
Agreed, I will try to look for it
An action is a value that represents something to be done. Typically that means a value of type `m a` where `m` is an instance of `Monad` (or at least `Applicative`) and `a` is the type of the result of running the action. All of the most common “action” types have a dual interpretation as “containers”, and not all actions are guaranteed to produce a result. For example: * `Maybe a` is the type of containers that can store 0 or 1 elements of type `a` (optionality), and *also* the type of actions that may return 0 or 1 results of type `a` (failure) * `[a]` is the type of containers that can store 0 or more elements of type `a` (lists), and also the type of actions that may return 0 or more results of type `a` (nondeterminism) * `Either e a` is the type of containers that can store 1 element of type `a` or 1 element of type `e` (choice), and the type of actions that may return a result of type `a` or “bail out” with a result of type `e` (exceptions) So promises are a particular example of actions, but not all actions are necessarily similar to promises. 
Could this bug explain your issue? https://ghc.haskell.org/trac/ghc/ticket/13751 I will try a single threaded runtime for now, until we can upgrade to ghc-8.2. Let's see whether this helps.
Nice list. I wonder if Earley parsing fundamentally can't support state, or whether there is a way to incorporate it. I think it could support it, but it would need integration with the parsing algorithm because Earley wants to know which productions are the same as other productions. If you have a parameterised production `foo n` then it needs to know that two instances of `foo 3` are the same in order to keep its O(n^(3)) promise and in order to support left recursion. When `foo 3` left recurses on `foo 3`, possibly via a chain of other productions, it needs to know that it ended up in a cycle to shortcut the process, so the algorithm would need to make use of equality on the argument of foo. The same goes for the O(n^(3)) guarantee: that relies on only parsing each production once for any given (start,end) input range, regardless of how many times the production is referenced in the grammar, so it needs some way to determine that `foo 3` is the same as `foo 3` but different than `foo 4`.
Okay, nice! I'm almost convinced to replicate the behaviour based on Dhall! I would just need the ability to "diff" Dhall records, i.e. given records a and b, I can obtain a record c such that a+c = b and so that a and c have no shared fields (same field, same value). Do you think this can be done easily? And is there a way to serialize a record back into the Dhall format? I want that an application configuration can be updated within the application (e.g. game settings), so that the changes (and just the changes) relative to some base config are written into a user config file, That is why I need the diff and serialization capabilities.
I continue to choose Parsec for now. I'll quote what I wrote recently to a client for whom we're writing a parser presently: &gt; I'm able to get great error messages out of parsec. The obvious improvement in megaparsec is supporting your own data type for errors, e.g. if there's a consumer of your API that wants to know what type of error it was. &gt; &gt; I'm in favor of *eventually* moving to megaparsec, but it's a bit too new to use in production, I think. Parsec is 16 years old, whereas if you check out the [list of bugs](https://github.com/mrkkrp/megaparsec/issues?utf8=%E2%9C%93&amp;q=is%3Aissue%20label%3Abug%20) on the megaparsec tracker, there are many reports that say "this works in parsec", and many that are regressions from one version of megaparsec to another. Writing parsers is tricky enough without bugs in the parser library itself. &gt; &gt; Regarding speed, unless you're parsing e.g. 100K lines of code, it's not going to come up. I'm working on a parser for another client that parses MS SQL and it can parse a 200K line file of complex stored procedures in 9 seconds. 22Kloc/s is fine.
Isn't it going to be even more work to compile Electron?
Neat. You can use the same trick with `NamedFieldPuns` if you prefer explicit imports ;)
But IIUC, that relies on taking `IO` actions as inputs only to bind them. This removes the distinction between effectful and non-effectful actions, so basically throws away purity altogether.
No, because Electron is distributed as a precompiled binary for the supported platforms. In other words, other people are compiling it for you.
So if Movie Monad was distributed precompiled for supported platforms, would you prefer it over an Electron-based app?
I believe this is what it's meant by record-as-a-module that I've heard people say. You can also write tmod :: TMod tmod = TMod { pack = …, unpack = …, length = … } where TMod{..} = tmod to allow recursive implementations.
Conan Elliot wrote a fun blog post on that: [The C language is purely functional](http://conal.net/blog/posts/the-c-language-is-purely-functional). There's a distinction between technically and effectively :)
Depends on what level you're reasoning. Some Hakell values very much *represent* things that "do" something, even though the values themselves are just dumb inanimate values. `putStrLn "Hello"` doesn't "do" anything itself, but it does represent an effectful program that prints "Hello" to stdout. Maybe the inaccuracy is that we call it an "action" when technically we should say that it's "a value that represents an action". And from there, the habit of redefining "action" to also mean "a value that represents an action", and from there generalizing to *any* monadic value, or at least any monadic value that conceptually represents something action-like.
Well, you can already sort of do that, by adding type annotations to the top-level identifiers when and where they are used. Example: a :: Num a =&gt; a a = 5 main = print (a :: Double) This obviously doesn't show up in the Haddock though, but I'm kind of confused how you would achieve that. In the case of `lens`, the situation is slightly different; the alternative type signatures given there are generally based on convenient type aliases. [The `Lens` type itself](http://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Type.html#t:Lens) is actually a good example of this: type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t So instead of `Functor f =&gt; s -&gt; ((a -&gt; f b) -&gt; s -&gt; f t) -&gt; a`, we can have `s -&gt; Lens s t a b -&gt; a`, which makes it much easier to figure out what it does in terms of conceptual lenses (takes a data structure and a lens into it, and extracts the thing that the lens focuses on from the data structure).
The entire point of this would be checked documentation. Of course, we would need to upgrade Haddock to understand these additional signatures and render them in some clear way, same with other tooling. For the lens example, I'm talking about things like this: http://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Setter.html#v:.-126- At the bottom of the `.~` docs there are multiple type signatures given so you can get a better idea of the common use cases for the function. Unfortunately, there aren't type checked which is what I'm proposing to fix.
As an app user, I would expect a precompiled binary anyway, so either is fine. As an app author, however, precompiling Movie Monad does not help me. The actual issue is that compiling and installing the supporting GUI framework/library is hard. Here, the advantage of Electron is that it comes precompiled and there is no need to link it.
Would it accomplish what you want to introduce a new toplevel definition for each type signature, like aInt :: Int aInt = a Of course naming becomes an issue ...
I know of Conal's post, but I believe that it is incorrect. Sure, you can stretch the definition of "purely functional" to include C, and also pineapples, but by any workable definition of purely functional, C is not a purely functional language.
Your list of bugs links to an entirely closed list of bugs. So while it might be true that megaparsec is/was buggy, it's got a very active maintainer. This to me is almost more important than being bug free. Plus parsec has simply accepted some bugs as normal behaviour, as hinted by &gt; Some quirks and “buggy features” (as well as plain bugs) of original Parsec are fixed. There is no undocumented surprising stuff in Megaparsec. Though it doesn't say what these quirks are.
Well, you have to export the name for it to show up in the docs which means that now you're polluting the namespace with useless functions. Also, the docs will just show a new function, and will not relate it to anything so you'll have to explain that in prose. And this relationship again isn't checked but relies on convention.
I just wonder why the `Lens` type is not encapsulated in a `newtype`? Is this to get free composition with `(.)` or is there any other reason? 
Also see https://ocharles.org.uk/blog/posts/2014-12-04-record-wildcards.html (second section) and http://www.haskellforall.com/2012/07/first-class-modules-without-defaults.html
That's quite a cool trick :) However &gt; Thus, RecordWildCards give us something pretty close to first-class modules and open from ML derivatives; the only problem is that record fields are globally namespaces, RecordWildCards is unfortunately still quite far from ML modules (let alone first class ones, which no language actually has, excluding 1ML). In particular, with Haskell records you can't do any type abstraction. It's too bad BackPack is only geared to large scale abstraction. 
I don't know, I didn't write `lens`; but one reason I can think of is that this way it is possible to expose a lens-enabled API for your types without depending on `lens` yourself - you just provide suitable functors, and `lens` will work on them. Whereas if `Lens` were a newtype, then you'd have to import that newtype from the `lens` library in order to provide lenses yourself, and thus you would have to explicitly depend on `lens`.
You can parameterize a "module" on a value by making it a function `X -&gt; TMod` and you can parameterize it on a type by adding a type parameter to `TMod`, so I don't see how this is any less powerful than ML modules. 1ML is based on the encoding I just described.
&gt; Unfortunately, there aren't type checked which is what I'm proposing to fix. You can get alternative type signatures checked already, although it's a bit of a dirty trick: Add top-level aliases for the function in question that are annotated individually (which causes the compiler to type-check them), but don't export them (thus keeping them internal). So, for example: set_Setter :: Setter s t a b -&gt; b -&gt; s -&gt; t set_Setter = (.~) The only thing missing here is linking this to the documentation.
Yes, but that's the main thing that's missing. See my reply here https://www.reddit.com/r/haskell/comments/6yelke/multiple_type_signatures/dmmspoc/
It seems the best way is to [open `Data.ByteString.Internals` and make a Storable vector.](https://stackoverflow.com/questions/18682527/how-to-convert-between-bytestring-and-storable-vector)
I've used this trick in a [synthesizer written in Feldspar](https://github.com/emilaxelsson/feldspar-synch/blob/fd2797a4034c8ca3a0d9bd9ccc369e6b28341a87/examples/Synth.hs#L142-L147): synthPolyMain :: Run () synthPolyMain = do alsa@(ALSA {..}) &lt;- importALSA pcm &lt;- newPCM n &lt;- initPCM pcm Playback 1 bufferLength periodLength execSystem $ runSynch $ synthPoly alsa pcm n `importALSA` declares bindings to the ALSA C library and exposes methods such as `newPCM` as a record. Using the normal module system isn't possible because the methods need to be declared in the code generation monad before they can be used.
Indeed - thanks!
Apple stuff is so shiny, making it almost unimaginable to abandon. What was your experience, /u/maxigit and /u/mightybyte, in that regard? “[Steve Jobs] figured out a way to make computers that are jails for their users and make them so shiny that millions of fools would beg to be jailed.” — [Richard Stallman](https://www.youtube.com/watch?v=fkkDvKGcNSo#t=18m). Maybe I'm just being a fool.
Yes, I'd like something like this. As functions have been made more generic, with changes such as foldable appearing across the prelude, this has come at a cost in understanding and identifying appropriate functions from their type. Alternative, more specialised type signatures can help with this, particularly for new users. Since this is really a documentation issue, If a change to the haskell language is not a popular, then implement it as Haddock syntax within documentation comments. Teach GHC to parse these and warn if they're not valid.
It's just a matter of stability versus instability. Parsec is stable. Megaparsec is unstable. Parsing is hard enough as it is. I wouldn't recommend Megaparsec to anyone without that qualification. 
It's as if other "production-ready" libraries in Haskell ecosystem were bug-free. I just accept that if a project is moving forward actively, then there will be unexpected issues. But take a look at the last bug from the list https://github.com/mrkkrp/megaparsec/issues/245, it was closed the same day it was reported, new test cases added, new version published. Most of these bugs are really subtle, and some of them are also in Parsec, unfixed and unreported, undiscovered.
In my experience, using ``parsers`` and ``trifecta`` has mostly been like using ``parsec``. Plus better behaviour with backtracking. I tend to use ``trifecta`` for parsing configuration files, and user-written input files -- including command-line parsing of formulas and stuff like that. ``attoparsec`` for any big files with lots of data (mostly data files in bioinformatics).
I'll probably be using megaparsec eventually, but not today. Other people are happy to be early-bird testers, everyone's happy. 
Eff is literally just the standard IO monad with a phantom tag describing the kind of IO you're doing. This is useful to make sure you're only doing certain kinds of IO, but it doesn't do anything beyond what the Haskell IO monad does. You still need monad transformers, etc.
I just started using a new laptop and am comparing the experience. I haven't used it very much yet, but the biggest thing that jumps out at me is the hardware quality is way lower. The most glaring things are the touch pad and the speakers. In both cases the MBP is vastly superior. If I had to guess right now on the route I'll end up taking, I'd say I'll probably set up a Linux dual boot on my existing MBP.
There it is: https://www.youtube.com/watch?v=amTG4sGbXsk&amp;feature=youtu.be
Oh, pineapples. Isn't that his point? 
Thanks. I got spoiled by github's triple-backtick support and didn't realize that reddit doesn't accept that.
I still feel "Megaparsec *is* unstable" is quite a bold claim that isn't currently verified by the bug tracker. Has been, sure, but I'm optimistic things only get better (as backed up by the tests that accompany the project).
A few years ago I posted [an example of what lens would look like if it did use data constructors](https://www.reddit.com/r/haskell/comments/2413fc/idiomaticlenshs/), and the main reason given for why the real `lens` is better than my version is that the real `lens` can use subtyping: * Using subtyping for composition. Traversals and Getters are both Folds, and when you compose them with `(.)` you get a Fold. That's because the Traversal works on any `Applicative f` and the Getter works on any `(Contravariant f, Functor f)`, so their composition works with any `(Contravariant f, Applicative f)`, which is exactly what Folds do. With data type wrappers, we'd need three different composition operators for combining two Traversals into a Traversal, for combining two Getters into a Getter, and for combining a Traversal and a Getter into a Fold. * Using subtyping to avoid duplication. My version implemented a `fstGetter`, an `fstSetter`, etc., while `lens` only needs a single `_1` which is both a Getter, a Setter, a Lens, a Traversal, and a Fold. Same thing for operations like `over`, I would need to implement both `overSetter`, `overLens`, and `overTraversal`. There was also a discussion about type inference. Having the operation and the optics combine their constraints onto a single shared `f` has very good inference. For example, `view` adds `f ~ Const r` and Traversal adds `Applicative f`, so we need to use the Applicative instance for `Const r`, which requires `Monoid r`. But with `view` and Lens, the constraint is only `Functor (Const r)`, which does not require `Monoid r`. As a result, this same operation `view` can be used both with Traversals which point to elements which have a Monoid instance, and with Lenses which point to any element. Using type classes to reproduce those complex constraints with the datatype-based implementation clearly needs at least multi-parameter type classes with no functional dependencies. So there would be a lot of type parameters to fill, a lot of degrees of freedom, and so type inference will often need extra type annotations to resolve the ambiguities. The final reason given was that thanks to this type inference, it was possible to discover new optics by writing an interesting composition, erasing the type signature, and asking GHC to give the type of whatever optic results. This would not be possible with a datatype-based implementation, because we need to define the types of our composition operators in advance, so we already need to know the type of the composition.
You can help with this Sphinx conversion effort: https://github.com/abingham/categories-for-programmers
Pattern matching is better than `head`/`tail`. Attaching a single element to the end of a list repeatedly is a smell: It takes quadratic time and you could have returned the first element immediately, before eating through the rest of s. This way you don't even `go` for passing around the ss. Arguments that don't change for recursive calls are usually put in front. This cooperates well with many combinators. `takeWhile` and `dropWhile` can be merged into `span`. mySplitStr char [] = [] mySplitStr char (c:s) | c == char = mySplitStr char s mySplitStr char s = let (l,r) = span (/=char) s in l : mySplitStr char r
What do you understand is verifiable by seeing a bug reported roughly once a month on the bug tracker? To me that indicates people finding issues (the last of which was 23 days ago), and I suspect because it only just started being used by people. The issues are split between incompatibility with Parsec, and regressions with previous versions of itself. Combine that with the age of the project, I don't know what else to call it other than unstable. Instability doesn't mean "broken", it means that it changes. It means: if you're going to use this project, account for that overhead. 
Dhall does not support diffs of values out-of-the-box, but the language is a library, too, so can implement it if you need to. You can also render expressions back to valid source code using the `pretty` function This README summarizes what you can do with Dhall and links to further resources: https://github.com/dhall-lang/dhall-lang
This is also imo the best way to use `RebindableSyntax`!
Doesn't adding a type parameter expose the specific concrete type being used in your module implementation? That's not modularity in the ML sense, if records can't define types in them. Other differences are in signature matching, e.g. implicit subsumption (a consequence of the fact that matching is nominal rather than structural) Also, a "module" ought to be able to contain every other component of the core language, and there's still the huge problem of what to do with type classes and instances. The fact that records could almost encode modules has been Haskell folklore for a long time, but every attempt to actually follow through (that I'm aware of) would require changing the core language as well (e.g. see [First class modules for Haskell](http://www.cis.upenn.edu/~bcpierce/FOOL/papers9/shields.pdf)), which is what ultimately led to BackPack
&gt; Doesn't adding a type parameter expose the specific concrete type being used in your module implementation? That's not modularity in the ML sense, if records can't define types in them. Other differences are in signature matching, e.g. implicit subsumption (a consequence of the fact that matching is nominal rather than structural) Oh, for that feature I think you'd use an existentially-quantified type variable. [This paper](http://dl.acm.org/citation.cfm?id=45065&amp;dl=ACM&amp;coll=DL&amp;CFID=805914949&amp;CFTOKEN=10561513) is relevant IIRC. The real feature that BackPack gives us AFAIK is zero cost module parameterization, since type classes and functions are only zero cost in the cases where the inliner does its job.
The type for `myCallback` must be `(Message, Envelope) -&gt; IO ()`. However, with partial application, any function with more arguments can become this function. You can define a function `callback :: Context -&gt; (Message, Envelope) -&gt; IO ()` and later get a `(Message, Envelope) -&gt; IO ()` with partial application. You code will looks like: context &lt;- createContext ..... -- I'm sure it will be `IO`. let myCallback = callback context consumeMsgs chan "myQueue" Ack myCallback
 -- signatures data SET set elem = SET { empty :: set, add :: elem -&gt; set -&gt; set, mem :: elem -&gt; set -&gt; Bool } data ORD t = ORD { eq :: t -&gt; t -&gt; Bool, less :: t -&gt; t -&gt; Bool } -- SIG where type param2 = t2 data sig `WHERE_TYPE_2` t2 where OPEN :: sig t1 t2 -&gt; sig `WHERE_TYPE_2` t2 opaque = OPEN -- functor (opaque ascription) set :: ORD t -&gt; SET `WHERE_TYPE_2` t set ORD{..} = opaque SET { empty = [], add = insertBy (\a b -&gt; if eq a b then EQ else if less a b then LT else GT), mem = \x -&gt; not . null . findIndices (eq x) } -- local import foo (OPEN SET{..}) = mem 2 . add 1 . add 2 $ empty doubleSet = set ORD { eq = (==), less = (&lt;) } :: SET `WHERE_TYPE_2` Double -- qualified main | OPEN doubles &lt;- doubleSet, OPEN bools &lt;- set (ORD (==) (&lt;)) :: SET `WHERE_TYPE_2` Bool = do print . mem doubles 1 . add doubles 1 $ empty doubles print . mem bools True . add bools False $ empty bools
Uhh. I mean I thought the whole point of this discussion was talking about how you could use Haskell in a mutable and imperative way, so of course things aren't going to be pure to the same degree. Even with that said there still is a lot more purity going on, as any Haskell functions you use from libraries that aren't in IO are as pure as ever. 
The fact that `Wrapper (Trick ())` is inhabited by `Wrapper Nothing` means that it cannot be isomorphic to `Void`. Whatever it is that you are doing in line 4 is wrong. A haskell data type cannot have a negative number of inhabitants, nor can it have a fraction number of them.
What I would like to see is dependent types in Haskell allowing for a "dumbed down" refinement types syntax where I can write `foo :: (a : a /= 0)` or something like that. Perhaps just borrow straight from liquid Haskell current syntax. But a nice easy way to put small constraints on functions and have them handled by the dependent type system without needing to know much about the complex machinery would be awesome.
I believe the `SPECIALIZE` pragma will be verified.
How do you determine which signature is the "most general" and therefore the "canonical" type signature to be used in type-checking occurrences of that identifier? GHC Haskell (with any of a number of extensions) doesn't have principal types, which means that for a given expression, there may not *be* a most-general type. Or, alternatively, is the suggestion that the first-occurring signature be used as the "canonical" type of the identifier? In this case, I'd rather have the alternates hidden in some kind of pragma, rather than stated at top level.
&gt; A haskell data type cannot have a negative number of inhabitants, nor can it have a fraction number of them. I'm just curious why not. I'm partially going off the fact that List x = 1/(1-x) and Tree x = (1-sqrt(1-4x))/(2x)
&gt; In particular, the methods of zeta function regularization and Ramanujan summation assign the series a value of −1/12 Can you justify that these summation methods apply to type arithmetic?
The algebra here is wrong as well. No serious mathematician will ever claim that the infinite sum of the natural numbers is a non infinite number. If you use "zeta function regularization" and "ramanujan summation" you can get a y intercept of -1/12 which is very different than the actual infinite sum of the divergent series. What you're doing is you're setting up Trick as a type with infinite inhabitants (the natural numbers) and then pretending to sum them up to a negative fractional number using bad math (which a type can't have negative inhabitants in Haskell anyway, although some type theories do indeed have a logical meaning for fractional and negative types). Then you wrap that illegal type in an arbitrary wrapper and calculate the final inhabitants wrong. (Fractional types, which are different than quotient types, are weirder than that in type theory). Don't feel too bad, though, your logic was pretty sound for the most part, and I've seen far worse fake math :) Have you read any articles about the algebra and calculus of algebraic data types? That, along with zippers, might interest you.
I'm not sure what's the error in the proof above, but you can trivially construct values that inhabit your type: * `Nothing` * `Just (Nothing, False, False, Left ())` * `Just (Nothing, False, False, Right (Left ()))` * etc. As you can see, there is in fact an infinite number of values of `Wrapper (Trick ())` (this should be trivially provable by induction). I'm guessing this is what the sum in the 4th line of your proof represents, but you have replaced it with `-1/12` using a semantic trick from completely different field of mathematics.
Where did you get that from? Those equations do not model number of inhabitants (although they probably model something else). Think about it: how many inhabitants does `List Bool` have? Infinitely many. We have: [] True : [] False : [] True : True : [] True : False : [] False : True : [] True : False : [] ... However, with the equation you provided, we would calculate: List Bool = 1 / (1 - 2) = -1 Which we know must be wrong.
Feel free to join us on the functional programming discord!
Thanks for your answer. I do know about partial application and the like. I am still unsure how is that `context` going to help me, because everything is immutable and the code above creates `context` once and only once and then applies it to the callback. How is this callback going to have a fresh set of promises to resolve? I was thinking about something like: consumeMsgs channel qName Ack (\(message,envelope) -&gt; -- some_magic will fetch new app model myCallback (some_magic) (message, envelope) ) Now, each "myCallback" invocation will get current version of model, where it will be able to look for promises for (and what else). How can I make some kind of bridge between event handlers and `some_magic`?
I was getting the list value from List x = 1+x+ x^2 + x^3 + x^4... which does model the number of inhabitants and is the Taylor series for the expression I gave. Still, I freely admit that List Bool being -1/2 doesn't make any sense to me. I guess I'm just too used to things not making sense. Edit: fix formatting
 Another solution to this issue would be to use a StateT monad transformer - This would give all functions in the context of the monad transformer access to a shared state. I suspect this solution would likely result in the greatest parity with a java implementation. Here's a really basic example of a StateT monad transformer - https://gist.github.com/IronGremlin/c7f8dfc96d10d4c68267e9cd13133061 The type machinery going on with transformers is kind of complicated to understand, but really simple to use. For basic use, you can simply use this as boilerplate, and change `MyAppState` into a more complicated record type fitting your logic. Here is a super basic explanation of what's going on: The newtype declaration creates a 'composite' monad that inherits the traits of all of the monads used in it's type signature. The `transformers` package supplies a collection of commonly used monads, implemented in such a way that they can be composed with each other. When you declare your newtype to string your monads (in this case, StateT and IO) together, you also declare a constructor function. This function basically just shuffles types around so that all your monads can act together as one. It has no real purpose aside from defining the type signature. When doing more complex stuff with 2-3 monad transformers at once, the order that these types go in can change your behaviors, but for now, just stick to putting IO all the way on the right. The constructor function should be wrapped with a "kick-off" function, which is necessary to actually construct an instance of your custom monad and return the result of your operations "back into" the `IO` monad. Again, the stuff that's going on under the hood is complex and neat-o, but actually using it isn't really that difficult. You just match up the type signatures, and `transformers` takes care of the rest. 
You can stash an `IORef` (or `TVar`, or...) into the `Context` data type.
I'd recently read several articles about the topics that you mentioned, which started me down this silly path. I'd seen people taking square roots of types and I was wondering how far I could push it. It looks like I probably need to read them a few more times so I don't make an ass of myself.
Most things are sub-optimal at being C if what you want is to write some C.
The key thing here to keep in mind is that all the numbers represent inhabitants of a type. The algebra is determining how many possible values a type can represent. In fact, the safety of a strong type system comes from chewing a type down to the minimum amount of inhabitants that are necessary to make the function work (and in the process removing all the inhabitants that can break the function). A polynomial expansion can be thought of as showing the inhabitants of an infinite structure in a step by step form. So the inhabitants of a tree with a depth of one is 1, the inhabitants of a tree with depth 2 is 1 + 2, the inhabitants of a depth 3 tree is 1 + 2 + 4... And the polynomial expansion will give you the inhabitants of a tree of infinite depth should you manage to write the infinite series out. To be honest it seems it's the mathematical understanding that's limiting you more than the type theory bits.
I think it's not the job for compiler. I proposed a variant to doctest (and haddock) -- | -- type&gt; forOf :: Functor f =&gt; Iso s t a b -&gt; s -&gt; (a -&gt; f b) -&gt; f t https://github.com/sol/doctest/issues/153 It shouldn't be hard to implement in either tool. --- Except that example is bogus, the two types below don't unify: forOf :: LensLike f s t a b -&gt; s -&gt; (a -&gt; f b) -&gt; f t forOf :: Functor f =&gt; Iso s t a b -&gt; s -&gt; (a -&gt; f b) -&gt; f t
Ahh, that's the part that I got confused on. For some reason it never registered that these were just called diagrams and not necessarily commutative diagrams.
This is funny and all, but I really wish that the documentation on the function would tell me what exactly is so unsafe. Can anyone explain?
The paper I linked above discusses the drawbacks of using existentials for that purpose in section 3.1
How do you pass a `StateT` as a `(Message, Envelope) -&gt; IO ()` function?
That's the problem with formal power series: series are studied as objects, **with the understanding that no one cares whether or not they actually converge**. Here we see what happens when we jump from formal power series to analysis without formally proving convergence. 
There is some previous discussion on the issue [in this old Reddit thread](https://www.reddit.com/r/haskell/comments/2cbgpz/flee_traveller_flee_or_you_will_be_corrupted_and/).
Funnily enough, I was taking a look at the `ghc-prim` library today (while trying to get a better feel for `RuntimeRep`), and I saw that it exports the `IO` constructor and GHC *really does treat `IO a` as something like `StateT RealWorld a`* (it's not merely a didactic device). And now I'm shown code where someone pattern matches and disassembles this constructor, towards some pernicious goal... 
Wonder if doctest could handle this? -- | Whetever. -- &gt;&gt;&gt; a :: Int -- 5 -- &gt;&gt;&gt; a :: Double -- 5.0 a :: Num a =&gt; a a = 5 
&gt; CPU and memory monitoring: Unfortunately not yet - we hopefully have time soon to take care of this. Any system you would recommend? We run EKG at work. I've written [`monad-metrics`](https://hackage.haskell.org/package/monad-metrics) as a convenient wrapper around EKG metrics, and [ekg-cloudwatch](https://hackage.haskell.org/package/ekg-cloudwatch) to push metrics to Amazon Cloudwatch. There's a whole bunch of backends for EKG metrics, including statsd, bosun, etc. https://hackage.haskell.org/packages/search?terms=ekg
Great stuff! I have some stuff which can definitely benefit from this type of testing. Maybe this'll be the final push to actually get it done! :)
`IORef`, `TVar` does not sound familiar to me, but now I know what to search for. Thanks!
Thanks for the example. I do not get it now, will try to run and tinker with it. There is a line `modify (+1)`, the function comes from some Control.Monad.x package and it is supposed to provide a new application state, right? When I look at it all, it seems like it's breaking all the fundamental laws of pure FP though, strange…
Is there any reason why not use deriving show whenever possible?
The links in the documentation do a pretty good job explaining it. I think the [`packByte` issue](https://github.com/haskell/bytestring/commit/71c4b438c675aa360c79d79acc9a491e7bbc26e7) highlights it best. The key phrase from the documentation itself is: &gt; stabs you in the back and aliases all of your mutable buffers As a very minimal example, let's take `malloc` from `Foreign.Marshal.Alloc` and specialize it: malloc :: Storable a =&gt; IO (Ptr a) mallocWord32 :: IO (Ptr Word32) Now, what would you expect this to do: foo :: Bool foo = let a = accursedUnutterablePerformIO mallocWord32 b = accursedUnutterablePerformIO mallocWord32 in a == b The `Eq` instance for `Ptr` simply checks to see if the pointers refer to the same location in memory. It is a pure operation. We would expect `foo` to be `False`, but it's actual value depends on whether or not GHC applies Common Subexpression Elimination to effectively turn the function into: foo :: Bool foo = let a = accursedUnutterablePerformIO mallocWord32 b = a in a == b However, this is **not** what is happening in all of the five issues that the `accursedUnutterablePerformIO` docs link to. It's a little harder to see it in those cases. Let's look at the one that deals with `packByte`: mallocByteString :: Int -&gt; IO (ForeignPtr Word8) accursedUnutterablePerformIO :: IO a -&gt; a packByte :: Word8 -&gt; ByteString packByte c = accursedUnutterablePerformIO $ mallocByteString 2 &gt;&gt;= \fp -&gt; do withForeignPtr fp $ \p -&gt; poke p c return $ PS fp 0 1 {-# NOINLINE packByte #-} We need to rewrite some of the types to remove the `IO` newtype wrapper. I'm going to append `#` to the function names, which is sort of a convention, but keep in mind that these types are identical to the ones above. All I've done is removed the newtype wrapper. I'm also going to specialize `&gt;&gt;=`: mallocByteString# :: Int -&gt; State# -&gt; (# State#, ForeignPtr Word8 #) accursedUnutterablePerformIO# :: (State# -&gt; (# State#, a #)) -&gt; a (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b bind# :: (State# -&gt; (# State#, a #)) -&gt; (a -&gt; State# -&gt; (# State#, b #)) -&gt; State# -&gt; (# State#, b #) Alright. Now, let's rewrite the first part of `packByte` with these: packByte :: Word8 -&gt; ByteString packByte c = accursedUnutterablePerformIO $ mallocByteString 2 &gt;&gt;= \fp -&gt; ... packByteAlt :: Word8 -&gt; ByteString packByteAlt c = accursedUnutterablePerformIO# (\s -&gt; bind# (\s1 -&gt; mallocByteString# 2 s1) (\s2 fp -&gt; ...) s) Getting closer. Now, let's inline `accursedUnutterablePerformIO#` followed by inlining `bind#`: accursedUnutterablePerformIO# :: IO a -&gt; a accursedUnutterablePerformIO# m = case m realWorld# of (# _, r #) -&gt; r bind# :: (State# -&gt; (# State#, a #)) -&gt; (a -&gt; State# -&gt; (# State#, b #)) -&gt; State# -&gt; (# State#, b #) bind# m k = \s -&gt; case m s of (# new_s, a #) -&gt; k a new_s packByteAlt c = case (\s -&gt; bind# (\s1 -&gt; mallocByteString# 2 s1) (\s2 fp -&gt; ...) s) realWorld# of (# _, r #) -&gt; r ==&gt; (reduction 1) packByteAlt c = case bind# (\s1 -&gt; mallocByteString# 2 s1) (\s2 fp -&gt; ...) realWorld# of (# _, r #) -&gt; r ==&gt; (reduction 2) packByteAlt c = case (case mallocByteString# 2 realWorld# of (# new_s, new_fp #) -&gt; (\s2 fp -&gt; ...) new_s new_fp) of (# _, r #) -&gt; r At this point, we could proceed to do case-of-case analysis and make it prettier, but it doesn't really matter. We've already found the culprit: the expression `mallocByteString# 2 realWorld#` is a constant. GHC is free to float it out to a top-level definition. Now, we get this: floatedFp = mallocByteString# 2 s1 packByteAlt = case (case floatedFp of (# new_s, new_fp #) -&gt; (\s2 fp -&gt; ...) new_s new_fp) of (# _, r #) -&gt; r Uh oh. Now, when we call `packByteAlt` twice in a row, it's going to reuse the same buffer. Remember: &gt; stabs you in the back and aliases all of your mutable buffers So, how does `unsafePerformIO` prevent this? It comes down to a very magical function [runRW#](http://hackage.haskell.org/package/ghc-prim-0.5.1.0/docs/GHC-Magic.html#v:runRW-35-), which is used by [unsafeDupablePerformIO](http://hackage.haskell.org/package/base-4.10.0.0/docs/src/GHC.IO.Unsafe.html#unsafeDupablePerformIO), which is called by `unsafePerformIO`. GHC delays the inlining of `runRW#`, which would prevent reduction 1 from happening in the example above. If I'm wrong about any part of this, I would appreciate any corrections. I don't hack on GHC, but this seems to be what was happening in the `packByte` issue.
Yeah, the first one would be the canonical one. The others are just glorified comments. I'd be fine with any syntax.
Sounds good.
 mySplitStr l c = (\(cur_string, list_of_strings) -&gt; cur_string : list_of_strings) $ foldr (\x (cur_string, list_of_strings) -&gt; if x == c then ([], cur_string : list_of_strings) else (x : cur_string, list_of_strings)) ([],[]) l An important consideration to keep in mind when accumulating characters is that it is possible to do so without needing to reverse the order by starting from the back. Once you have that you can hold two accumulators and move data between the two as you go along, which gets you what you want in a single pass. Apart from using lists, this is probably the most efficient way of doing it.
Thanks a ton for doing this. We're getting to a stage where we need to decide a testing workflow for our webapp and this piece of documentation is very well timed for us. As a side note, I had faced the \NUL bug as well, and released https://github.com/vacationlabs/qc-instances Might help the next person who would otherwise waste a day trying to track this down. 
There is a function called [rmap](http://hackage.haskell.org/package/vinyl-0.6.0/docs/Data-Vinyl-Core.html#v:rmap) in `vinyl`. But then you're not using tuples anymore. I've never seen anything that does this, but with the right awful typeclass, it should be possible. Something like this: class Crushable a b where type Crushed a b crush :: a -&gt; b -&gt; Crushed a b instance Crushable (a,b) (c,d) where type Crushed (a,b) (c,d) = (a,b,c,d) crush (a,b) (c,d) = (a,b,c,d) instance Crushable (a,b) () where type Crushed (a,b) () = (a,b) crush (a,b) () = (a,b) Your biggest problem would be that haskell has tuples of size 0, 2, 3, 4, etc. but is missing tuples of size 1. So, people have to define stuff like this: http://hackage.haskell.org/package/Only-0.1/docs/Data-Tuple-Only.html
That is why instead of trying too hard to take the nice aspects of Haskell into other languages, I have instead taking the approach of relatively aggressive evangelism.
Honestly I think strictness by default, while reasonable when wanting to compile to JS, is worse than laziness by default. Due to the fact that a lot of the benefits of laziness don't jump out at you, but the space leak every once in a while does, people really don't appreciate how fantastic it is. I really like [Edward Kmett's comment](https://www.reddit.com/r/haskell/comments/5xge0v/today_i_used_laziness_for/deia53t/) regarding this, and hell that entire thread is great. I also like how he quite effectively nullifies the "explicit can do all that" argument with [this comment](https://www.reddit.com/r/haskell/comments/5xge0v/today_i_used_laziness_for/deleyqr/). I'd also suggest reading through all of [this thread](https://www.reddit.com/r/haskell/comments/4cwctz/is_purescript_the_future_of_haskell/), PureScript has some very non-negligible downsides, and most of the upsides are fairly surface level. I'm going to stick to Haskell/GHC for sure for any non-frontend stuff (I mean that is easy to justify based on just compile and runtime performance alone). Even for frontend stuff I'm going to stick with Haskell/GHCJS for the long haul, WebAssembly when supported will make it all around better than PureScript in my view, and things like being able to use Haskell/Reflex to develop mobile apps (coming in the next few months) while also using Haskell/Reflex for front-end is just fantastic. Plus even pre-webassembly and reflex-mobile Reflex and React-hs on the browser are wonderful.
What do you mean by "first used"? Code isn't linear like that. EDIT: I guess you could make an argument for a `TERMINOLOGY.md` file or something like that, though. (At least for the common abbreviations.). However, spelling out the abbreviations in the first comment of every file using it is a bit much, though.
Uhh... because many things just won't type check without bidirectional type inference. Basically anything that involves `pure`, `mempty`, `read`, `from***` or even just number literals is not really going to work.
There's no shortage of `Only` types: - [cassava](https://www.stackage.org/haddock/lts-9.3/cassava-0.4.5.1/Data-Csv.html#t:Only) - [csv-conduit](https://www.stackage.org/haddock/lts-9.3/csv-conduit-0.6.7/Data-CSV-Conduit-Conversion.html#t:Only) - [fixed-vector](https://www.stackage.org/haddock/lts-9.3/fixed-vector-0.9.0.0/Data-Vector-Fixed.html#t:Only) - [mysql-simple](https://www.stackage.org/haddock/lts-9.3/mysql-simple-0.4.1.0/Database-MySQL-Simple.html#t:Only) - [OneTuple](https://www.stackage.org/haddock/lts-9.3/OneTuple-0.2.1/Data-Tuple-OneTuple.html) - [Only](https://www.stackage.org/haddock/lts-9.3/Only-0.1/Data-Tuple-Only.html#t:Only) - [optparse-generic](https://www.stackage.org/haddock/lts-9.3/optparse-generic-1.2.2/Options-Generic.html#t:Only) - [postgresql](https://www.stackage.org/haddock/lts-9.3/postgresql-simple-0.5.3.0/Database-PostgreSQL-Simple.html#t:Only) - [text-format](https://www.stackage.org/haddock/lts-9.3/text-format-0.3.1.1/Data-Text-Format.html#t:Only) Consider using [`Identity`](https://www.stackage.org/haddock/lts-9.3/base-4.9.1.0/Data-Functor-Identity.html#t:Identity) (or [`Unit`](https://www.stackage.org/haddock/lts-9.3/ghc-prim-0.5.0.0/GHC-Tuple.html#t:Unit)) instead :) Also, fun fact: GHC's tuples "only" go up to 61 elements. See [GHC.Tuple](https://www.stackage.org/haddock/lts-9.3/ghc-prim-0.5.0.0/src/GHC-Tuple.html#line-170) for the `data` declarations.
Ah, I had never noticed `Unit` before. That's kind of neat. I agree though that `Identity` is basically always the way to go.
Yeah I don't think I'd actually use `Unit`. I try to avoid stuff from `GHC.*`. (Except `GHC.Generics`, since that doesn't live anywhere else.) 
I understand your opinion and I'm not going to argue for or against laziness/strictness, but do understand that there are people who prefer strictness over laziness. &gt; PureScript has some very non-negligible downsides, and most of the upsides are fairly surface level. Could you elaborate on a few points?
This is definitely not idiomatic Haskell. Here's an improved version of the same idea: mySplitStr :: String -&gt; Char -&gt; [String] mySplitStr str delim = foldr f [] str where f :: Char -&gt; [String] -&gt; [String] f x (s : ss) -- If we hit a delimiter, -- start building a new string | x == delim = [] : s : ss -- If we don't hit a delimiter, -- just add the character to the most recent match. | otherwise = (x : s) : ss The state inherent in the fold is the list of matches so far. We add new characters to the most recent match until we hit a delimiter, at which point we start a new match. 
So, I am not an expert by any means. I think my pattern ends up being similar to what /u/Gurkenglas did with the `mySplitStr` function. I'm hoping that it is just another way to think about the problem. &amp;nbsp; * `chunk` gets the part of the string up to (but not including) the delimiter that you choose. * `chunked` gets the remaining part of the string not taken by `chunk` but also removes the delimiter(s). (It works even if you had more than 1 delimiter in a row.) * `chunks` combines these two functions with recursion to get a list of strings. &amp;nbsp; chunk :: Char -&gt; String -&gt; String chunk char string = takeWhile (/= char) string chunked :: Char -&gt; String -&gt; String chunked char string = (dropWhile (== char) . dropWhile (/= char)) string chunks :: Char -&gt; String -&gt; [String] chunks char string | string == [] = [] | otherwise = chunk char string : chunks char (chunked char string) ------ edit: Downvoted right away. Dang. There are certainly better ways of doing this, but OP wanted to only use techniques that have been discussed by Chapter 9 of HPFFP. 
I do understand that some prefer strictness, but I do think that once they have sufficient exposure to both in the same context (pure and immutable language) I think a huge amount will rethink that decision. For example very few people use `-XStrict` in Haskell. &gt; Could you elaborate on a few points? I mean that's why I linked the thread, so I don't have to repeat everything the people there have already said. Other stuff I guess I would add is the fact that it compiles just to JS, which means its basically ONLY suitable for front-end dev, using PureScript on the backend with node is just downright evil. Plus once WebAssembly becomes mainstream it won't even be good at frontend, as GHC compiling to WebAssembly via LLVM will blow it out of the water in performance. It also lacks most of Haskell's extensions it seems, like GADTs and TypeFamilies and so on. And it lacks various things that you expect from a general purpose language, such a good multi threaded runtime, multiple optimized compile targets, and of course it lacks in libraries. I mean one of the core devs even says in the thread they aren't trying to compete with Haskell as a general purpose language. In my view PureScript really does nothing but fracture the Haskell front-end community for little long-term benefit.
People print for different reasons and if you want something robust you can look into the other responses about printing. If you just need to do some printf-style debugging, you can use trace functions like [traceShowM](https://www.stackage.org/haddock/lts-9.3/base-4.9.1.0/Debug-Trace.html#v:traceShowM) (Note: These functions are for debugging only, don't leave them in your code)
Unfortunately, its quite non-trivial to encode refinements using current Haskell syntax, primarily because you need support for binders-in-types, that is you want to write things like: ``` foo :: x:Int -&gt; y:Int{y /= x} -&gt; Int ``` where you name some parameters and you want later parameters to refer to those names. In theory its possible with, say, class constraints but it gets very clunky very soon... 
Thank you all for the examples, I'll work my way through them and get back with any questions. Coming from a C background I was concerned about using '++' and in earlier versions, I was working backwards through the list or doing a reverse on the return guard but I decided it reads better this way while studying.
This is definitely better than my proposal, since mine goes through unnecessary calculations. 
I think I had super/sub backwards in my head. And I think type b constraints is a typo. Sorry!
&gt; I do understand that some prefer strictness, but I do think that once they have sufficient exposure to both in the same context (pure and immutable language) I think a huge amount will rethink that decision. For example very few people use -XStrict in Haskell. Many people that write PureScript write Haskell as well. Many people that write Haskell decided they prefer Strictness. Everything in programming languages is about tradeoffs and some people made the tradeoff of Laziness for other things. Just because you do not agree with their opinion does not make it any less valid. &gt; Other stuff I guess I would add is the fact that it compiles just to JS, which means its basically ONLY suitable for front-end dev It doesn't compile just to JS. It also compiles to C++ and Erlang. And maybe tomorrow some will decide they want to compile it to x64 or LLVM. The language itself is not bound to JS. Also quite a few tools are written in PureScript which are not front-end, for their own reasons. For example pulp and pscid &gt; Plus once WebAssembly becomes mainstream it won't even be good at frontend, as GHC compiling to WebAssembly via LLVM will blow it out of the water in performance. I'd love for Haskell to be as usable as PureScript or even more. &gt; using PureScript on the backend with node is just downright evil. This is harsh. &gt; It also lacks most of Haskell's extensions it seems, like GADTs and TypeFamilies and so on. They might be useful, but they are also feature I never used in Haskell so I'm not bothered by that. I'm more bothered by the fact Haskell doesn't have anonymous records, let along row polymorphism. &gt; And it lacks various things that you expect from a general purpose language, such a good multi threaded runtime, multiple optimized compile targets. That's totally true. However it is a property of the implementation and not the language. I never argued the implementation is better than GHC. Also maybe we'll have better runtime and optimized targets in a few years, who knows? &gt; I mean one of the core devs even says in the thread they aren't trying to compete with Haskell as a general purpose language. And rightly so, Haskell is a great language with a great implementation. But I think it will be much easier for me to contribute and take PureScript for a direction I might like to take it than to try to do the same with GHC. To name a few examples, proper IDE support like PureScript already has, I'd like better error messages, I'd like incremental parsing, this are somethings I think I can tackle for psc if I'd like to but I shiver at the thought of touching GHC. To name a few more, I'd like purescript to have a native target with a low latency garbage collector. Which is something I believe I could implement for PureScript with quite a bit of work but am sure I won't be able to for GHC. &gt; In my view PureScript really does nothing but fracture the Haskell front-end community for little long-term benefit. PureScript has different goals than GHCJS and have achieved them. It also improves quite a few things over Haskell and has quite a few successful projects built with it. People want to use it today and it's here to stay. 
Ah, I see. I now realize that Conal's post was probably satire.
Yeee! My man
Any one have a reference for this magic number (61 element max tuples)?
I guess it probably depends on the way you use your computer. I admit I really fell in love with Apple touch pads years ago which where vastly superiour to the Microsoft equivalent. Things have changed and even though Apple are still probably superior, touchpad on high end Microsoft laptop are more than good enough. Anyway, I tend to use the touchpad as least as possible, on my MPB I spent my day coding in vim+tmux and firefox+vimperator, switching between tasks using quicksilver. No real need for a super touchpad neither than using many of Mac OS native application. The things which really bothered me with MBP where - lack of control key on the right ( I like a symetrical keyboard) - performance issue (which I am apparently the only one to have) - lack of port. My old MPB had an ethernet port, the new don't ( I believe). I know it's to get lighter and thinner - lack of native window snapping. but I don't care, I was happy with the current size and weight of my old one. I switched last year for Thinkpad P70 (for about the same price a new MPB) what I gain is - a full keyboard, with 2 control keys, numeric pads, 3 mouse buttons (handy when using 3D software) - a 17'' screen I absolutely love it - a touch screen. Didn't really need did but thought it would be handy to emulate mobile device when testing UI. I actually use it much more than expected for all sort of things (including drawing with a pen). - more RAM - faster CPU (at the time) - full servicable laptop. I can change the batteries, add some hard drives (there is space for 4), etc ... - I can run Linux with XMonad on a virtual box. Didn't work on my old MBP. - I have all possible ports , DVD , SD cards etc ... What do I miss ? Actually nothing. Ok there's a few thing which I didn't get natively but nothing I couldn't find a decent linux alternative. For example, I use to use Quicksilver a lot (to switch between application), I can do the same with XMonad. What do you think you'll miss ?
One repl-session per Emacs process sounds Bad. But it could be that I'm ignorant and it is, in fact, Good. Tell us more? My use case: I often work on more than one project at a time, and I'd really rather not reload everything from scratch each time I switch between, say, the client and the server of a web app.
? Those things type check just fine with just algorithm W, no?
If you get a top spec laptop (not a super slim one, but a "business" one) for a price equivalent to a MBP you'll get a good quality hardware. As I said in my answer to /u/haskellgr8, thouchpad are probably a touch behing, but good enough for many user. About the speakers, laptop speakers (including MBP) have never been great but can easily been solve with external ones. Which are laptop are you comparing with ?
Well, there goes all the useful stuff that I wrote. Fortunately, Intero provides everything that has been lost in the systematic lobotomy of haskell-mode. Intero was carefully written to have no dependency on haskell-mode, so its possible to fold the major mode into Intero any time.
`modify` takes the state, in the example, an `Int`, and applies a function to it. `get` fetches the state `put` replaces the state Think of the state as basically an implicit argument to the next function in the monadic chain of operations - It's purely generating a new state , passing it forward, and discarding the old one. The monad just 'wraps' the value being passed along so that you don't 'see' it. check out: http://brandon.si/code/the-state-monad-a-tutorial-for-the-confused/ For an example. Note, I threw out a bunch of architecture here in terms of gluing the stateful computation to the callback - You will still probably need to use partial application on a shared, mutable ref, or a channel construct of some kind (TChans from the `stm` library are pretty easy to work with for this purpose). The benefit of wrapping multiple channels in a state transformer is so that you can more easily share state between multiple subscribers so that you don't have to pass the entire application state around as a single IORef / STM Ref, what have you - instead, you can build one (or more, depending on how you want to slice it) larger state container and control updates to the state in one 'place'. I find that to be useful if you have a lot of stateful logic to worry about. For the most basic implementation, a partially applied callback over a simple `TVar` (also from `stm`) or `IORef` will allow you to do stateful things over a singular shared value. Of course, there is also the 'obnoxious FP guy' answer to the quandary as well, "Are you sure you need state at all?", which although obnoxious and pedantic, is totally a legit question you should think deeply about where possible.
That's why I designed it to have multiple sessions. This is just a regression to what it used to be like in 2009.
The unfortunate thing there is that if you want to update an implementation of a "module function" the existing functions on that module will still refer to the old implementation.
&gt; One repl-session per Emacs process sounds ...unusable.
The comment says: "Including one more declaration gives a segmentation fault." I can't find a source for that before 2001: https://github.com/ghc/ghc/commit/900b4a144c29b194fa88c87f72eaee8e9ca8741c#diff-2c20e3742724dfb99aa3f78ea9872645R140
Thank you for this work!
 Verbosity mostly. 
This is one of those haskell-isms that seems unbearably obtuse for a very long time. The strong point is that using generic abbreviations forces the reader to think generically because there is a fairly minimal amount of information contained in the name. The weak point is that often, deriving even that level of minimal information can rely on some unfamiliar conventions or terminology. You get a bit more used to it, though admittedly this is a pretty intense example of this particular 'ism'. I will point out that you're currently looking at a git repository for something titled `Dependent-Binary-Lambda-Calculus` - Expecting the code to be straightforward and readable without special knowledge may not be the most sound of expectations. 
Tangent: Every time I see Haskell and emacs mentioned in the same sentence, I think "wouldn't it be nice to be able to write elisp functions in Haskell?" [Haskell-emacs](https://github.com/knupfer/haskell-emacs) originally allowed for this, but it seems to have been broken at some point in the past year and I can't figure out why. Would be amazing if someone in the community could tackle this one :-)
Thanks for going through all the cruft, the code is in much better shape after you are done! Looks like editor work attracts many naysayers... it is a shame, because it will further hold back volunteers from contributing to the tooling around Haskell. 
That sounds a bit hasty. Isn't the big part that you're talking about still under review and unmerged? It can always be declined, and changes can always be reverted.
How about consing named tuples? https://hackage.haskell.org/package/labels-0.3.3/docs/Labels.html See the cons function.
thanks for that, I'm still getting my head around the last line though I understand it from afar :)
I know they are not usable without bidirectional type inference. Forward type inference is definitely insufficient. 
So coming to Haskell after programming in Ocaml for a little bit (where I use Merlin), I don't understand why a repl needs to be running for type checking. Does it have something to do with the intermediate files generated by the Ocaml compiler?
... or [whispers, sexily]... amazing? No, it's unusable. Or [sexily] is it? EDIT: More seriously: Could you expound on what makes it unusable for you? (etc.)
I usually have a minimum of 3-4 repl sessions open at the same time and often want to use them concurrently.
Hi folks, I'm building Megaparsec-based parsers for various data formats and am having trouble getting the behavior I need for processing huge data sets. I understand the need to avoid things like ambiguous rules that can consume huge sequences before failing and the like. Here's the problem I'm having: I have a solid JSON parser `json` that will parse any single JSON object into, say, an Int. I want to apply it to a huge file of sequential JSON objects and get a list of Ints: so, `many json`. I `runParser` that on my data of 5000 Reddit comments, it works great, takes 20s, uses near-constant memory, presumably just the size of the list of Ints plus space associated with the current object-parse. Here's the problem: if I `take 40` from my `runParser` invocation, it *still takes 20s*. So, I tried building the list in this fashion, where `p` is my single-object parser: unfoldM (\s -&gt; do (s', e) &lt;- runParserT' p s case e of Left x -&gt; return Nothing Right x -&gt; return $ Just (x, s') ) state Still takes the full 20s when I only ask for the first 40 elements. This was more surprising, because list construction is no longer fully passed off to Megaparsec, and the `unfoldM` could certainly stop at 40...so why does the result insist on fully parsing all 5000 objects? Thanks! EDIT: since this hasn't gotten any response yet, thought I'd phrase it as a simpler and more general way: how can I build a list such that it has the same behavior as: take 40 [1..] and what pitfalls would cause a function that returns a list to *not* behave in this lazy way? Pointer to documentation on it would be great, I've looked but may not understand what I'm looking for well enough to find it. (I also realized that building a list by consing is exactly the wrong way to do this, right?)
I have difficulty understanding why one would want such a thing, but... thanks? So, I think what you're asking for is multiple separate(!) "inferior processes" (in Emacs terminology) running GHCI?
Note names matter when you're talking about chords, especially diatonic chords in a scale. For chords built in thirds, you want to use every other letter. Say for instance you want to name the notes in an A7 chord. You want note letters A C E G, with certain accidentals, and in fact you want A C# E G. Then C# is the leading tone in D major, which goes with the dominant function that A7 fulfills in D major. If you wrote C# as Db instead, it would completely confuse the function of the chord, and the parent scale it comes from. At the end of the day, music theory is supposed to help bring a bit of clarity about how the music is constructed, so it can be important to distinguish things which have the same pitch but different function. Just like your example of G mixolydian, which has the same notes as C major but functions differently. Another comment – in my opinion you're probably better off having notes be given by numeric data rather than letters, as much of the manipulation is numeric in nature. Personally I like basing everything off the major scale (scale degrees 1 2 3 4 5 6 7) and then alterating with accidentals. That way you can automatically generate modes of a scale, and have all the note names be correct (within reason; you're never going to manage good naming within the diminished scale for instance). I've also wanted to make a Haskell music theory program to help me quickly write down scales, diatonic chords within them, and relationships between them (e.g. finding pivot chords for modulation). I was hoping to have an UI that is based on chord relationships like in [this diagram](http://i.imgur.com/OvDot0y.png), but I realised while making that diagram by hand that I missed several relationships; really I would need to write an algorithm that generates a diagram like this automatically (given user-supplied chord nomenclature). It's nothing complicated, just a bit of musical set theory, but I haven't gotten round to it... yet.
&gt;&gt; I'm hoping that it is just another way to think about the problem. Yes, it is and thanks for your consideration regarding where I'm up to, cheers.
This is great! It's going to take me some time to digest it, but it all seems like good advice. Cheers!
I'm not sure if it qualifies as a quintessential early Haskell paper, but if you're interested in list comprehensions, there's [Bringing Back Monad Comprehensions](http://db.inf.uni-tuebingen.de/staticfiles/publications/haskell2011.pdf), and the related library [DSH](https://hackage.haskell.org/package/DSH). [Comprehending Ringads](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/ringads.pdf) is also related.
Does anyone have advice on debugging why my IHaskell 8.2 PR is [failing](https://travis-ci.org/gibiansky/IHaskell/builds/272355382#L2398)?
There is a notion of convergence but it is w.r.t the p-adic (or in this case the x-adic) norm. 
[Lazy Functional State Threads](https://www.microsoft.com/en-us/research/wp-content/uploads/1994/06/lazy-functional-state-threads.pdf) is a good introduction to the theory behind `IO` and `ST`.
I'm comparing it with a System76 Galago Pro. It's pretty light. Not sure the detailed specs.
I have been experimenting with efficient Haskell arrays lately, and I recently came across this paper from 2002: [An Approach to Fast Arrays in Haskell]( https://www.cse.unsw.edu.au/~chak/papers/CK03.html ) It makes a good complement to the [Lazy Functional State Threads]( https://www.microsoft.com/en-us/research/wp-content/uploads/1994/06/lazy-functional-state-threads.pdf ) paper mentioned by another comment, because it shows a practical application (mutable arrays) for the concepts introduced in that paper, namely the ST monad and how you can encapsulate an isolated state-transforming algorithm and still prove (using the type checker) that it is pure, referentially transparent.
This is a neat idea. Thanks!
I don't know what an inferior process is. I just want multiple repl's (across different projects) running at the same time in Emacs, like I have currently. The description lead me to believe this would no longer be possible. Edited to add: A simple example of why someone would want this has already been given (working on client and server code at the same time) but another simple (and relatively common for me) example would be that I'm working on work stuff in at least one repl and I decide to take a break and play a code clash on codingame for which I need a separate repl, and I don't want to close down my work repl because it has a bunch of state built up in it. Relatedly, I might be working on something at work and someone asks me to look at something else in a different project, and again, I don't want to lose the state I have built up.
A related aspect of this is that intervals can also have multiple names. For instance, the interval between C and the Ab above it is a minor 6th, but the interval between C and the G# above it is an augmented 5th. This is relevant when you are thinking about different types of chords. An augmented triad is a major 3rd and an augmented 5th above the base note (C, E, G#). It greatly obscures the meaning/function of the chord if you call the interval a minor 6th instead or write the G# as an Ab. If OP is going to fully support all of this, then he/she will also need to consider how to represent double-sharp and double-flat notes. A diminished triad based on Db would be Db, Fb, Abb. (However, depending on the situation, one might decide to be less technically correct with the theory and just rewrite this as C# diminished, which doesn't require any double-flats.)
Yeah, I guess you are right. 
&gt; Another comment – in my opinion you're probably better off having notes be given by numeric data rather than letters, as much of the manipulation is numeric in nature. I prefer to use the `Enum` typeclass to go to and from numbers. So if you need to do numeric calculations on notes, you use `fromEnum` and `toEnum`, but you keep the clarity of the sum type. 
Even though I'm sure System 76 do decent machines, I'm not sure it if fair to use them as benchmark to compare Mac vs PC, especially with product in a different price range. The direct competitor to the MBP seems to be the Dell XPS 13 or 15, or any Thinkpad with similar price.
[Bananas, lenses, and barbed wire](https://maartenfokkinga.github.io/utwente/mmf91m.pdf) is one of my favorites. Not explicitly Haskell-related, but it's targeted at lazy functional languages so it implicitly is. Simon Peyton-Jones also has a nice talk [here](https://www.youtube.com/watch?v=hlyQjK1qjw8) on parallelism and declarative languages. If you're interested in recursion schemes, [this paper](http://www.iis.sinica.edu.tw/~scm/pub/mds.pdf) has a nice example of a zygomorphism. 
 &gt; fingeringOf $ transpose A' major [2,3,1,2,3,4,5,1] Are you sure this is right? It looks like it puts the thumb (finger 1) on the last `A'` (A#?), which is a black key.
That was a fun read thanks :) I'd like to suggest renaming your scale creation function from transpose because *[it already has a well established musical meaning](https://en.wikipedia.org/wiki/Transposition_%28music%29)*. How about makeScale or generate..., create..., construct... I'm intrigued by your approach to modes. I would never have thought of calculating them as the difference of the intervals (from the root) to those of the major scale. My approach was (i.e. I was taught) to learn them as a list of semitone intervals and go from there. Ionian: 2 2 1 2 2 2 1 Dorian: 2 1 2 2 2 1 2 Phyrgian: 1 2 2 2 1 2 2 ... I find it easier to think this way because it helps when switching to a different tonic and defining them as semitone intervals means you can use something similar to your `transpose` - which of course is now called `makeScale` :) In fact I would define all my scales using a list of semitone intervals rather than intervals from the root. I suspect everyone tries to figure out some algorithm for dealing with modes but to really benefit from music theory you need to know all of these things instinctively rather than calculate them from first principles. Rote memorisation is painful and slow but if you find your approach is a shortcut along that path, that's fantastic. 
**Transposition (music)** In music transposition refers to the process, or operation, of moving a collection of notes (pitches or pitch classes) up or down in pitch by a constant interval. The shifting of a melody, a harmonic progression or an entire musical piece to another key, while maintaining the same tone structure, i.e. the same succession of whole tones and semitones and remaining melodic intervals. For example, one might transpose an entire piece of music into another key. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
`Intero` is also carefully written to work with `stack` only: https://github.com/commercialhaskell/intero/issues/361. 
You can do this for both directions with bidirectional pattern synonyms. Assuming `lens-aeson`: pattern JSON :: (FromJSON a, ToJSON a, AsJSON t) =&gt; () =&gt; a -&gt; t pattern JSON a &lt;- (preview _JSON -&gt; Just a) where JSON a = _JSON # a let's you go all the way to/from JSON from a `String`, `ByteString`, `Text` or `Value`. e.g. I use it to both pattern match and construct, say, hover requests for visual studio code with something like: https://github.com/ekmett/coda/blob/cd3eb5cc5e5e0cd6a216cd9123d1f79d0fd5a7ee/src/Language/Server/Protocol.hs#L1198
It's the reason why I'm still using `haskell-mode` instead of `intero`.
I note that `accursedUnutterablePerformIO` should be deprecated and purged now that [`runRW#`](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/ghc-prim-0.5.1.0/GHC-Magic.html) exists, which gives us completely safe and fast inline `unsafePerformIO`. Since GHC 8, [`unsafeDupablePerformIO`](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.10.0.0/src/GHC-IO-Unsafe.html#unsafeDupablePerformIO) uses `runRW#`, and is thus safe and inline.
You might need a bit more insight into how one even arrives at that -1/12 claim. [Here is a video that might help](https://youtu.be/jcKRGpMiVTw). But the bottom line is that what you built your claim on is true in certain contexts but not universally.
This is interesting. How should we deal with chunks of length zero? My version keeps them: mySplitStr _ [] = [[]] mySplitStr x (y : ys) = if x == y then [] : z : zs else (y : z) : zs where z : zs = mySplitStr x ys So e.g. when you split `"abcadeaa"` on `'a'`, your code gives `["bc","de"]` while mine gives `["","bc","de","",""]`. Your code gives cleaner results, but mine gives a bijection between strings and nonempty lists of chunks, whose inverse is this: myJoinStr _ (y : []) = y myJoinStr x (y : ys) = y ++ [x] ++ myJoinStr x ys I guess your algorithm makes more sense for splitting on *runs* of the delimiter, like splitting a string on whitespace, while mine is about splitting on the delimiter per se, like parsing a CSV file where some values are empty.
For big types (even if they're fairly simple, like a list of countries), `deriving` can be very slow, so in such cases if you don't need to derive something, then you shouldn't, as it will slow down your development cycle.
Was there seriously both `interactive-haskell-mode` and `haskell-interactive-mode`? Who names these things? 
Could you back this up, because Haskell is not type checked with bidirectional type inference (afaik), yet all the things you talk about work just fine in Haskell. That, or we have different notions of "working sufficiently"
Just my random rant because I'm struggling with random records-related BS at this very moment. Haskell needs to have a much, much better way to deal with records as part of the core language. Not a library. Not some template haskell. Not some exotic type-system extension. Baked. In. The. Language.
&gt; Of course, there is also the 'obnoxious FP guy' answer to the quandary as well, "Are you sure you need state at all?", which although obnoxious and pedantic, is totally a legit question you should think deeply about where possible. This is the question I am thinking about deeply :-) My goal is not to copy Java (or NodeJS) program's implementation. I just need working solution and I do really care about best possible solutions, strong and weak sides of them. My goal is to be able to introduce Haskell into the system. Once in a while new microservice gets created, sometimes one is removed and replaced and I want to try Haskell. If I succeed, then maybe other colleges will follow :)
I have a problem with an exercise in HaskellBook, I'm supposed to implement **and test using QuickCheck** a Semigroup instance for the following type: newtype Combine a b = Combine { unCombine :: (a -&gt; b) } The problem is the testing, because I need to generate functions in QuickCheck, the hint is to use CoArbitrary but I'm lost. Here's what I have: instance (CoArbitrary a, Arbitrary b) =&gt; Arbitrary (Combine a b) where arbitrary = do return (\x -&gt; coarbitrary x arbitrary) Trying to compile that gives me the following error: &gt; Couldn't match type `a0 -&gt; Gen b0' with `Combine a b' &gt; Expected type: Gen (Combine a b) &gt; Actual type: Gen (a0 -&gt; Gen b0) I have found an answer on SO for how to turn a -&gt; Gen b into Gen(a-&gt;b) by reordering arguments, using the fact that Gen b is an alias for something else, but trying to follow that I couldn't make it work either. Here's the SO link: https://stackoverflow.com/a/16220336 I don't believe using QuickCheck to generate functions can be this complicated, am I missing something?
That's correct, more details here: https://github.com/commercialhaskell/intero/pull/52#issuecomment-223744909
It is more likely that dependent types will be added, since that is on /u/goldfirere's agenda. When they have been added, you should be able to prove things like in Idris. I don't know if it makes sense to add LiquidHaskell at that point.
&gt; `data Note = C | C' | D | D' | E | F | F' | G | G'| A | A' | B` This is quite wrong IMO. You're assuming enharmonic equivalence, but that won't get you far in music theory, which is largely based on diatonic considerations. The best representation for the sort of "note" you're dealing with here, AIUI, is as a record: `Note { baseNote :: DiatonicNote; semitoneAlteration :: Integer }`, assuming `data DiatonicNote = C | D | E | F | G | A | B`. Then `(C, 1)` is C-sharp, `(C, -1)` means C-flat (not the same note as B!), `2` means double-sharp and so on. You *can* of course have functions for converting these "notes" into enharmonic pitch classes. You can even perform a conversion in the reverse direction, but that requires that the conversion be parameterized by a local key note. `noteFromEnharmonic (F, 0) (enharmonicize (A, 1))` (i.e. what is the "proper" spelling of A#, assuming that F natural is the key?) should return `(B, -1)`, i.e. Bb. Conversely for a key of `(B, 0)`, a spelling of `(B, -1)` should be converted into `(A, 1)`. Similar considerations, though perhaps even more complex, apply to intervals (i.e. differences between notes - here, we need a semitone alteration to make interval 'augmented' or 'diminished' whenever appropriate, and also a special treatment for seconds, thirds, sixths and sevenths (the seventh between C and Bb is called a "minor seventh", not a "diminished seventh", and similarly for the third and sixth! Also, the third between D and F# is a "major third", not an "augmented third!" But we still need the semitone alteration of "augmented" and "diminished" intervals- e.g. the seventh between C# and Bb is diminished!).
Since standard Haskell is hardly used, GHC could adopt Perl's philosophy and just say "Haskell 2018 is defined by the GHC test suite" :P
[This preprint](http://www.cl.cam.ac.uk/~sd601/papers/mlsub-preprint.pdf) is shorter.
Is the extra empty context "=&gt; () =&gt;" a feature or a bug? I glossed over the PatternSynonyms and ViewPatterns documentation but I couldn't find it.
Nice writeup! Another duo I have successfully used in production (and that I love) is Prometheus + Grafana :)
Just it case it helps you, DuplicateRecordFields + RecordWildCards are beautiful to work with if your situation allows it.
My Haskell is definitely a bit rusty, so I agree that is not idiomatic or even clean. It would have been a better with the `|&gt;` operator though Haskell does not have it built in. You did good in reducing the problem an extra step to a single accumulator. I think that your example would have been perfect had you not assigned an intermediate variable for the lambda that is only used once - I would tend to see that as a poor form in my own programming. I'd also go for giving variables long names as a form of documentation rather than comments since they go stale easily.
What is the notion of "better"? What makes trifecta's error messages better than parsec (or megaparsec)?
The approach Haskell has has lead us to insights like Monads and Lenses. Once discovered and fleshed out, sure, (at least variations of) such things should find their way into the standard libraries. If by "baked in" you mean syntax, I disagree strongly. Haskell has plenty of syntax as it is.
The last comment on there sounds a bit harsh, its not about supporting nix but about supporting cabal. A very popular tool for haskell
Very nice. Do you only track runtime metrics in Prometheus or also benchmark results?
Does `DuplicateRecordFields` really work as advertised? Last time I played around with them, in most of the call-sites it was forcing me to put type annotations.
`RecordWildCards` just put all the fields in the local scope so they're already completely inferred identifiers. 
About Haskell, Prof. Richard Bird published "Algebra of Programming" with early Haskell implementation "gofer" with src. In the book, he also tells about lists (listr/listl). It has many exercises and is very instructive. For the category theory I do not know which book to recommend, but Awodey's "category theory" (maybe you know) might be good. If you have a list(listr or listl) backed with a pair of adjoint functors "Free" and "|-|" (forgetting functor), you can draw list functor as a monad exactly. I think that is enough for them. 
Just yesterday I was playing with my ukulele and thinking that it would be really nice to have my computer teach me scales and automatically generated etudes! By the way, I stumbled on this little sketch of some music theory in the form of grammars in GF: https://github.com/GrammaticalFramework/gf-contrib/tree/master/music The author cites this paper for inspiration: https://dl.acm.org/citation.cfm?id=2633638.2633645&amp;coll=DL&amp;dl=GUIDE ("Functional generation of harmony and melody" by José Pedro Magalhães and Hendrik Vincent Koops) I happen to remember that José Pedro Magalhães is a cofounder of Chordify, which is a really cool consumer web app with a Haskell backend doing some pretty sophisticated musical analysis. http://dreixel.net/ https://chordify.net/ Haskore is another venerable Haskell music project. https://wiki.haskell.org/Haskore Oh, there seems to be a Haskore successor... http://www.euterpea.com/
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [GrammaticalFramework/gf-contrib/.../**music** (master → 8accbb0)](https://github.com/GrammaticalFramework/gf-contrib/tree/8accbb0b0a67eaabefabee067fd9495e928069bb/music) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dmokdin.)^.
`()` simply represents a context with no constraints. It's not special to `PatternSynonyms` or `ViewPatterns`—you can use `()` just about anywhere you could use a non-empty context! foo :: () =&gt; Int foo = 42 data Bar = Bar instance () =&gt; Show Bar where show Bar = "Bar"
Very nice. I'm eagerly awaiting some pull requests for `criterion` to merge these features in :)
Looks like the closest to appending tuples. Is the main advantage with this over records that one can use these without defining the record first?
about the line 2. it looks like a product to me, so Trick () = () * Trick (()+1) , right?
Thanks! The implementation you write is the one I was hoping someone else had done for me :). Was also wondering that maybe `(1, "2") &amp; _3 .~ Just True` would magically turn into `(1, "2", True)`, but this seems not to be the case. 
I don't really see what hyperion improves over criterion. With the later you can generate the exact same measurements.
http://haskellbook.com Haskell Programming from First Principles seems a go-to choice of the community nowdays.
You didn't gloss hard enough :) According to the [documentation](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#typing-of-pattern-synonyms), pattern synonyms have two sets of constraints, the "required" constraints and the "provided" constraints. In the example above, we can only use the pattern `(Json a)` to pattern-match on an `a` which has those json instances, and if the pattern succeeds, we don't learn anything new about the type of `a` (only about its value), so there are no "provided" constraints. Pattern-matching on simple types like `Maybe a` doesn't provide new information about the type `a` either, but pattern-matching on a GADT can provide some extra information. In the example below, IsStringOrInt is a "type witness", that is, it doesn't itself contain a String nor an Int, but having a value of type IsStringOrInt is evidence that type `a` can only be type String or type Int. data IsStringOrInt a where IsString :: IsStringOrInt String IsInt :: IsStringOrInt Int showStringOrInt :: a -&gt; IsStringOrInt a -&gt; String showStringOrInt s IsString = s showStringOrInt i IsInt = show i Pattern-matching on IsString provides the constraint `a ~ String`, which is why we can use `s` as a String on the right-hand side of the first case but not in the right-hand side of the second case. By using the second context to list the constraints provided by your pattern-synonym, you can create patterns which have this kind of effect on their right-hand side.
+1, although I haven't given it a very thorough pass, I think that it more or less includes everything someone would need to start learning haskell, and even some more advanced stuff
You *can* use empty constraints anywhere, but in this case the second constraint does have a special meaning which is specific to PatternSynonyms.
Yes that's right! We love criterion as well. However hyperion allows the benchmark results to be tagged with the input data ([`series`](https://github.com/tweag/hyperion/blob/b806e9cca6ab812f857180a0c4c92f29a4b68f0c/examples/micro-benchmarks.hs#L22)), which is mostly for convenience, and makes things much easier when working with elastic search. Hyperion also packages some utils for working on the report's structure. Also, when we started work on hyperion, criterion did not have an option for exporting to json (or at least it wasn't released). By having our own tool we can quickly try out new features, gather feedback, and contribute the better ones to criterion. But to be honest the real star here is elastic search!
Making it easy to build cabal-install, stack, and nix based projects without switching back and forth between all three tools is a worthy goal. I think there's another approach besides a new command line tool that's worth exploring though. What if we built a tool to convert between `stack.yml`, `cabal.config` (or whatever the cabal-install freeze files are called after new-build) and `default.nix` without losing information? There are some details to be worked out: for instance `stack.yml` has information about things besides dependencies like the package name that we would ignore, since we just care about the dependency related stuff like `resolver` and `extra-deps`. I think the general idea is clear though. Then no matter what tool was originally used for the project, you would be one file conversion (or one run of cabal-install's solver and one file conversion) away from being able to use it with your tool of choice. 
 GHC doesn't have a virtual machine, but you're right that it's got something to do with the garbage collector. At startup, it maps 1TB of anonymous, non-reserved memory to get nicely aligned addresses, and [simplify memory management](https://downloads.haskell.org/%7Eghc/master/users-guide/8.0.1-notes.html#runtime-system).
Haskell Programming from First Principles: http://haskellbook.com It should be good no matter how much programming experience the reader has. Going through the book and doing the exercises was an amazing experience, I'd even call it addictive at times. The authors did a really great job with the pacing of the book, so that by the time you start getting comfortable with the latest new concept you are already feeling eager to understand the next thing...and when you get it, it's incredibly satisfying. All this relies on actually doing the exercises, but I didn't find them tedious or like an "obstacle" like I did with other books. Because Haskell's paradigm is so different from what I learned in school, it was like learning programming for the first time all over again. This meant that it was a slow process, but tremendously rewarding. Edit: so yes, to be clear, it does absolutely teach programming from scratch (right from the lambda calculus on up). But I had about a decade of professional software development experience and found the approach to be perfectly appropriate.
Isn't `series` the same as having a `benchGroup` with multiple benchmarks, each executing the same function with a different argument?
https://github.com/well-typed/optics is the most recent attempt I know of that does essentially this.
Almost, note the subtle difference in the signatures: ``` haskell BenchGroup :: String -&gt; [Benchmark] -&gt; Benchmark, Series :: (Show a, Enum a) =&gt; Vector a -&gt; (a -&gt; Benchmark) -&gt; Benchmark ``` `series` records the (int-like) benchmark parameter in the report, so that it can be indexed by elastic search and plotted against: ``` [ ... { "bench_name": "pure-functions:15/fib", ... "x_1": 15 }, ... ``` (note `x_1`, snippet from the post) Let me know if this wasn't clear enough in the write up, I'll be happy to highlight it. 
You want multiple GHCi sessions because you're working on different projects that use different versions of GHC or depend on different versions of some library. The goal ought to be to sandbox stuff as much as possible, and this is a regression for that
Thank you. I was actually looking at the [wiki page](https://ghc.haskell.org/trac/ghc/wiki/PatternSynonyms) but it's in there too.
Yeah. They're "anonymous" records. It's just tuples with a label in them.
D'oh, it now occurs to me that I didn't answer their *real* question at all! Thanks for doing so.
I think that's a rather bold assumption. It should be clear by now that no one has thought of the obviously best record system. PureScript is up there, but I've got my own issues here and there with it. I don't think we can confidently say "records should be baked into the language" until we know *exactly* what that looks like. There are just loads of unanswered questions. Are there higher kinded records (e.g. the open union types from the extensible effects libraries)? If so, what is the kind signature of whatever this type is? It can't inhabit arbitrary kinds, since that would allow you to add elements to the `Bool` kind. Should records be extensible? If so, again, what are the kinds for this stuff? And, how do we handle the type inference, especially in the presence of `fix`, where fields can be allowed in the type but not initialized in the expression? Which brings us to laziness. Are we strict in the spine? Is there a way to union records, and is that partly lazy like Nix's `//`? How do we keep this stuff from being insanely slow? Using hashmaps to back records is so slow compared to just knowing the offset in memory (e.g. C structs) that every major VM based on this design tries its best to optimize the hashmaps away with runtime information (and GHC does zero runtime optimization). So functions using records need to always be specialized to concrete record types so that they can get constant time access, but this leads to a whole host of problems with GHC's inlining and specializing mechanisms. I have not been convinced that anyone has thoroughly answered a sufficient number of the questions with records to define a solution.
This seems relevant: [The GHC reading list](https://ghc.haskell.org/trac/ghc/wiki/ReadingList)
&gt; Haskell needs to have a much, much better way to deal with records as part of the core language. Say it with me: Haskell doesn't have records! I agree. I don't think we're there, yet, though! PureScript has by-far the best record system in any language I've ever worked with. It is miles above Haskell, or any other language. OCaml's structural subtyping lets you do a lot of what you might want to do with records, too. But other than that? Most statically typed languages don't have good records, at all. When I looked for examples on how people used records, nearly everything came from dynamic languages and their uses of hashes/dictionaries/maps/etc. If you want (in terms of type safety) a `HashMap String Dynamic`, then you can already do that. But we want something better. We want all that same flexibility and power, but we also want is to be totally type safe. PureScript's record/type system has only *recently* acquired the features that allow it to be as flexible as we'd want, and that's something they're baking into the language pretty hard. Haskell's general commitment to deferring design decisions is great. I wish Haskell *had* deferred on record syntax instead of baking something into the language before we had the Right Idea.
&gt; What if we built a tool to convert between stack.yml, cabal.config (or whatever the cabal-install freeze files are called after new-build) and default.nix without losing information? That sounds impossible, at least for Nix. You can convert to Nix just fine, but once there, since Nix is a programming language and not a data format, normal Nix expressions defining Haskell packages won't retain enough information to produce a stack.yaml. They define sources as an expression, then consume those by putting them into expressions that define Haskell derivations. The resulting derivation does not know where the source came from, merely where it is now in the Nix store. So you can't look at a derivation and say "oh that's aeson-1.2.1.0 from Hackage" because there's no way to prove that. Even if the name were sufficient, the Nix expressions could be applying patches to the sources or specifying Cabal flags.
Exactly. We're going to pay a massive backwards-compatibility cost whenever we switch over record syntax to some superior solution, so it must be worth it.
&gt; I wish Haskell had deferred on record syntax They tried! [A History of Haskell: Being Lazy With Class](http://haskell.cs.yale.edu/wp-content/uploads/2011/02/history.pdf), section 5.6: &gt; 5.6 Records &gt; One of the most obvious omissions from early versions of Haskell &gt; was the absence of records, offering named fields. Given that &gt; records are extremely useful in practice, why were they omitted? &gt; The strongest reason seems to have been that there was no obvious &gt; “right” design. There are a huge number of record systems, &gt; variously supporting record extension, concatenation, update, and &gt; polymorphism. All of them have a complicating effect on the type &gt; system (e.g., row polymorphism and/or subtyping), which was already &gt; complicated enough. This extra complexity seemed particularly &gt; undesirable as we became aware that type classes could be &gt; used to encode at least some of the power of records. &gt; By the time the Haskell 1.3 design was under way, in 1993, the user &gt; pressure for named fields in data structures was strong, so the committee &gt; eventually adopted a minimalist design originally suggested &gt; by Mark Jones: record syntax in Haskell 1.3 (and subsequently) is &gt; simply syntactic sugar for equivalent operation on regular algebraic &gt; data types. Neither record-polymorphic operations nor subtyping &gt; are supported. &gt; This minimal design has left the field open for more sophisticated &gt; proposals, of which the best documented is TRex (Gaster &gt; and Jones, 1996) (Section 6.7). New record proposals continue to &gt; appear regularly on the Haskell mailing list, along with ingenious &gt; ways of encoding records using type classes (Kiselyov et al., 2004).
One thing I wish I could have made clearer in this talk is that: there isn't "a record problem." There are a bunch of problems! And we're typically used to solving them with records in dynamic languages. We try to solve those problems with Records in Haskell and it sucks. Those problems typically have much nicer non-record solutions in other languages.
Thanks for the link, I'll have to dig into that :)
Good catch -- I guess it's an especially bad fingering algorithm :)
&gt; There are a bunch of problems And a bunch of solutions no one had thought of as being particularly related to "the records problem" prior to `lens` and its prior art (i.e. I don't think anyone expected "hey if record selectors were first class I could do X" to lead to what `lens` represents today)
The comments I've gotten from publishing this article have already made writing it worthwhile :) Describing notes in this way seems like the right approach, given that you and /u/presheaf have both implemented them in the same way. Are there deterministic rules when something is augmented vs major?
[Fixed link](https://downloads.haskell.org/~ghc/master/users-guide/8.0.1-notes.html#runtime-system)
I have found an article about publish-subscribe variables: https://www.schoolofhaskell.com/user/agocorona/publish-subscribe-variables-transient-effects-v I think it could potentially resolve my issue: the some event handler could tell query consumer that it is waiting for the query response. Both of them could acquire some kind of bidirectional relation in order to complete the query/response cycle.
Perfect intervals (the fourth and the fifth) are altered as follows: -1 semitone: diminished natural: perfect +1 semitone: augmented For the other intervals, it goes: -2 semitones: diminished -1 semitone: minor natural: major +1 semitone: augmented So the natural default is the major scale. Then it comes down to what's diatonic, and what the correct letter names are. For instance you spell a C augmented chord as C E G#, so the G# is an augmented fifth and not a minor sixth. C diminished 7 is C Eb Gb Bbb, with the diminished 7th Bbb (enharmonic to A, a major sixth).
Elm tutorials and repl for a few days to a week, then start Haskell Programming from First Principles.
Oh, damn. Seems like the X11 clipboard has failed me once more; Thanks!
I prefer defining scales in terms of "scale degrees and alterations", like saying Dorian is "major b3 b7". Your approach with consecutive intervals works well for most 7 note scales, but it needs adjustment when working with other scales. For instance, you need to know that in a minor pentatonic scale, you have scale degrees 1 3 4 5 7. Just writing the intervals (3 2 2 3 2) doesn't specify which scale degrees are involved. (FWIW I think modes are a vastly overplayed concept in music pedagogy; a solid grasp of functional harmony in major and minor keys should IMO precede the introduction of modes)
That's definitely a fair point. For note names I prefer to go with "letter + alteration". Personally I would have the internals fully numeric, and convert to letters when dealing with the user (input / display), but that's not really a meaningful difference with what you are saying (you end up converting either way).
I've got to say, that's one hell of an explanation from someone who doesn't even hack on GHC :)
It's like clang: it shows the original text (optionally with syntax highlighting) and adds a caret pointing to the error
Maybe we have different notions of bidirectional type inference. To me it means where you can retroactively infer the input types of functions based on how the output of that function is used. Something that is completely absent in basically all other languages that aren't Haskell derivatives. 
Thanks :) One of these days, I swear I'm finally going to get my feet wet.
is this slowness during compile-time or run-time?
Compile time. Try making `Data Country = Australia | Austria | ... deriving (Show,Read,Eq,Ord)` and compiling!
Awesome, thanks!
Is the elm part because you assume no familiarity with functional programming principles?
Thanks :)
&gt; What do you think you'll miss ? It's mostly psychological I think (https://en.wikipedia.org/wiki/Reality_distortion_field). I'll probably be trying out some PC laptops and try to break free from the Apple prison :)
For which the authors seem to give [student discounts](https://twitter.com/argumatronic/status/905547925937377281)~ &lt;3
Don't get me wrong. I don't mind working within a small niche market that is Haskell. I just thought that despite being small that Haskell was growing a lot. I guess I was wrong.
Maybe Haskellers simply prefer to get their answers from somewhere else, here and on IRC for example? Using [redditmetrics](http://redditmetrics.com) to compare the yearly growth in terms of number of subscribers, I get +32% for /r/python and +18% for /r/haskell. Better than the -2% on StackOverflow, but still way behind python!
Nice! I might use this in the future. One thing I've thought about doing is making a stack template for the book that sets up a stack project designed to integrate well with the book, modules per chapter (or however it most logically makes sense), with undefined stubs of the exercises. One thing that might be really cool is to set up a bunch of automated checking for most of the problems so someone can very easily verify they did them correctly. I'm not sure how to do that without leaking the solutions in plain text somewhere in the project (or repo), though. (Maybe even testing the testing; but how meta do you wanna get, anyway?)
Do it! Ask on #ghc for a simple ticket that's bugging someone or if you can help out on someone else's ticket and go from there. There's been plenty of low level activity happening recently that's fairly approachable.
&gt;One thing that might be really cool is to set up a bunch of automated checking for most of the problems so someone can very easily verify they did them correctly. This is what [Ask-Elle](http://ideas.cs.uu.nl/AskElle/) does.
I'd say that's the case. I know that when I'm learning Haskell I tend to ask on the IRC because the answer is either trivial (and I can figure it out eventually) or I'm missing some fundamental deeper piece of knowledge that can't really be answered in a QA format. Python, on the other hand, I don't need to know it to actually get stuff done in it; I can have no deeper understanding of what I'm doing as long as I look at stack overflow for all the answers to my somewhat syntax related "how to do X in Python". I rarely wonder "how to do X in Haskell", though; usually it's "what am I not understanding that prevents me from accomplishing X" and that's not suited to QA format at all. Edit: I'd also say that Python would make sense being a very visited language on SO because lots of new frameworks (like machine learning) tend to come out for python so people who might already know programming would be much more likely to pick up Python "on the fly" in order to get something done. In contrast, I'd imagine the majority of Haskell programmers are having to learn Haskell the slower, more academic way since it's so different from "regular programming".
It could also be because of Haskell's structure. Haskell is a language built on a small collection of fundamental principles in a way that Python simply isn't. Once the questions about concrete principles have been answered, it becomes harder to formulate new ones. This tends to lend to questions that are best asked over IRC / Reddit, because they're exploratory, instead of 'how do I do this specific thing' -EDIT- Anecdotally, although I find the haskell SO community -super- helpful, I find it really obnoxiously hard to formulate a good SO question.
Yeah, elm has a much lower abstraction level than Haskell so it's easier to pick up and tends to have more "super ultra beginner" hand holding tutorials than Haskell. It's definitely one of the better ways to start learning FP
Thanks, I'll keep that in mind.
Neat! I had no idea that existed. It shows just a blank page on mobile but I'll try and get it working on my laptop later.
That's interesting. Thanks for letting me know 
I don't think I would consider stackoverflow representative of actual growth. I always found Haskell much more scarce on SO than other languages that are just as niche, like Lua. Meanwhile we've been seeing more and more people show up on Reddit and IRC. I've never asked a Haskell question on SO, and I think that's true of a lot of the Haskellers I know.
There's a saying: the perfect is the enemy of the good. There was lazy list IO when that was obviously bad, and now there is IO and mtl, even though they have many problems. "do" notation is baked in, though there are legitimate gripes. No one really knows how to "solve" typeclasses either, but I like what we have, orphans and all. It's interesting that most "inspired by typeclasses" languages try something a little different, so the designers of rust, scala, idris, etc. obviously don't think haskell got it exactly right either. And of course now ghc has two incompatible overlapping solutions for multiple parameters, and no one knows which is better and which should remain. Personally I think I'd be happy with a non-extensible C-struct, with its own namespace, where the answers to all your questions above are "no." I've never used extensible records seriously though, so I don't know what I'm missing, but I have used C (and python, java, etc) structs and I've been missing them in haskell for 15 years. Is there a "why extensible records matter" document out there that motivates them? I always assumed the reason trex didn't make it was not just because it was complicated, since we have complicated features, but that it didn't justify its complexity by having users who loved it so much they were willing to port it to ghc and lobby for its continued existence.
The former only. I think that for the latter Elasticsearch is pretty much the only viable option :)
This is proportional growth, no? Since SO itself is growing, this probably means that new programmers are learning Python as a first language, which seems reasonable.
I'm interested to hear what improvements do other editors introduce in their Vim keymaps. Spacemacs? Atom? VS Code?
&gt; I often go from insert mode to normal mode just to go back a character or two. But there already is a very popular shortcut for going back a character: &lt;C-b&gt;! [...] Of course, there are other ways in Vim (and Yi) to move the cursor one character left, like &lt;C-o&gt;h and the left arrow (no judging). I just find &lt;C-b&gt; to be the easiest. Hmm, I do use &lt;LeftArrow&gt;. I must be using vim wrong. I'd guess that &lt;C-b&gt; is better because `b` is easily accessible from the home row? Ctrl isn't though...
The thing is, all of those things you mentioned had a lot of research put into them, and a lot of questions answered. They're not perfect, but they are good. There are just too many un-researched questions with records right now.
Anyone interested in music and Haskell should check this package: http://music-suite.github.io/docs/ref/
That's exactly what this feels like: Reaching for 'perfect' when we already know several 'good' solutions... most of which would be a *vast* improvement on what we have now. (Records are one of very few areas where I'm sort of 'ashamed' of Haskell as a language.) (Which is especially annoying since what we have now is actively *bad*. I conjecture that Haskell *without* records-as-they-currently-exist would actually be better since you'd at least be 'forced' to use e.g. lenses, vinyl or whatever.)
&gt; And of course now ghc has two incompatible overlapping solutions for multiple parameters I'm not sure which features you are referring to. Is there an alternative to MultiParameterTypeClasses? Maybe associated types? &gt; Is there a "why extensible records matter" document out there that motivates them? [Here](http://www.parsonsmatt.org/overcoming-records/) are the slides to a recent talk on the subject ;)
Would PureScript or Elm be a better stepping stone to learn Haskell? Is PS about as hard as Haskell? 
Megaparsec has the caret too [now](https://markkarpov.com/post/evolution-of-error-messages.html#displaying-of-offending-line-in-error-messages)
 A couple things: 1. The metrics they used to measure were actively *bad*. In fact, they give a boost to languages that generate unclear error messages or have poor documentation. There are other problems with the metric to, like assuming that "using Stack Overflow for help" isn't correlated with the language in question. 2. Python has a lot of high-level libraries and an IDE. It makes it *really* easy to write a tutorial that accomplishes something genuinely cool an is still accessible to a very large set of people. I think the Haskell community could take pedagogy more seriously, but I would nonetheless be really disappointed if we took the route of the Python community. And I don't expect to see an IDE in the immediate future. 3. Python is used by introductory classes and science labs, in part due to curricula already being written. Moreover, its status as a programming language for teaching probably inflates its numbers by generating views for things like "how do I loop over a list in Python".
Judging by recent election-based decisions in various countries, popularity might actually be a sign of inferiority.
Yeah, I meant functional dependencies and the type families stuff. &gt; Here are the slides to a recent talk on the subject :) That doesn't really motivate it though. Let's see... we have named arguments. Plain non-extensible records would work for that. To be honest I don't feel like I need those, but to each their own. And... was that the only motivation? "Why functional programming matters" talked about modularity and gave specific examples.
&gt; I must be using vim wrong. But that's on the same level of impossibleness as "using the proper subset of C++"! I just have Ctrl in CapsLock position, so that it's in the same place on all keyboards. Arrows can be small and hide cowardly in a corner, especially on laptops.
Haskell is not growing in the [PYPL index](http://pypl.github.io/PYPL.html) either (disclosure: I created it). One possible explanation is to look at platform / killer app where the language is used. Objective-C was very small until the iPhone appeared. Python has a lot of traction in AI/Deep Learning. Haskell is very strong in high-concurrency applications, but how much is that growing ? 
Trifecta generates very nice error messages. That's why Dhall and Idris both depend on it. It's unfortunately not as well documented - see for instance [this module](https://hackage.haskell.org/package/trifecta-1.7.1.1/docs/Text-Trifecta-Rope.html#t:Rope). Parsec is old, and you'd be better off using `megaparsec`, its speedier successor. It is featureful and relatively concrete - you're still using monadic parser combinators, but it turns out to be a fairly intuitive library to work with once you do understand monads.
Now I understand why multiple sessions would be a good idea. This use case of server and client could use multiple sessions. Why not run two Emacs processes?
Another vote. I ask questions constantly on FP chat or IRC and get thorough explanations of what I'm misunderstanding. Never asked a question on SO, and it's rare that I find the answer to a question there, either. Mostly blog posts, documentation, or chat -- lots of Google searches turn up old Haskell Cafe email threads more often than SO results! The only times I do end up there is when it's a small, simple, well-defined task like "How to flatten a list of lists." Measuring a language's growth by the number of questions asked like that is shaky at best.
That may be, but it's been a long time! It looks like 1.0 in 1990 had no record syntax, which is why you used to see files full of accessor definitions. It looks like records came in at 1.3 around 1997. But my impression was that records were too much to bite off for 1.0, and by 1.3 there was still no solution but a stopgap was needed, as simple as possible. If only the simple as possible "stopgap" had been just a little less simple and included real namespacing and nested updates. I'll bet we'd still have lenses and still be arguing about row polymorphism, but meanwhile would have saved a lot of hassle, and keyboards and fingerbones. Typeclasses were there since the beginning, so if you count experience with ML and Miranda as research time for typeclasses, then you could also extend records back to the and 60s and 70s. IO and "do" were definitely around by 1.3 as well, that's "merely" 7 years. Maybe where we disagree is that while I admit C / java / rust / python / ruby / etc. records are not perfect, I think they are good! In fact all of the things I don't like about them in those other languages haskell fixes by its nature: immutability and easy Eq, Ord, and Show basically covers it for me. That's why I was interested in examples for motivation. I've used records extensively in python as well, but always in an entirely static non-extensible way. Where I worked it was standard practice to teach python beginners to never extend their records. I don't remember that anyone ever argued against that, and they were mostly youngsters who grew up with ruby and Javascript.
Yeah. I often use records and then when a project grows I have to rewrite with lenses
Oh man I thought this was r/pcj
I'd like this if there was a way to prix them like modules so that I can take two items of the same types
We could reuse the syntax around modules and remove the weird non functional record syntax.
1. As shown by [RedMonk rankings](http://sogrady-media.redmonk.com/sogrady/files/2017/06/lang.rank_.617.wm_.png), Haskell is equally ranked when you consider Stack Overflow questions or GitHub pull requests, meaning that the suggestion Haskell is underrepresented on SO would need to assume it's also equally underrepresented in GH activity. As another commenter notes, it's at a similar ranking, and not growing, in [Google searches](http://pypl.github.io/PYPL.html) 2. Far from being "much more scarce", Haskell consistently gets more than [twice as many SO questions per month as Lua](https://insights.stackoverflow.com/trends?tags=haskell%2Clua). 3. Regarding Reddit and IRC: I think Haskell is specifically overrepresented among **people who talk about programming languages as a hobby.** As evidence I'd point out that Haskell is the language most overrepresented in Stack Overflow traffic on [weekends](https://stackoverflow.blog/2017/02/07/what-programming-languages-weekends/) and [outside working hours](https://stackoverflow.blog/2017/04/19/programming-languages-used-late-night/).
Don't think of missed opportunities, but soon to be Haskellers ;-)
Emacs runs on a single thread. If you are doing some task (opening a big file, starting some process, etc.), Emacs is going to get stuck for a while. In the meanwhile you might want to edit something in the other session, the only problem is that Emacs won't respond to `C-x b` or anything for that matter. Emacs was not designed to handle concurrent things. I think running two Emacs process will be the solution if you want to work on things simultaneously. Running multiple Emacs process is not taxing on a modern computer. I think doing things without leaving Emacs is a little overrated. I don't think multiple sessions is the correct solution to work on two different projects simultaneously. 
I bind tab and shift-tab, in normal mode, to cnext and cprev.
Unfortunately multiple sessions don't work too well with `emacsclient`-based setups or when you have a big init.el which starts up services which assume to be running on a single exclusive instance.
This is a fairly old (but still informative and relevant) blog post. For people who are interested in this, I highly recommend Duncan Coutts's several papers on stream fusion. A few links for the lazy: https://www.dropbox.com/s/zkmx66tvqelrnst/Stream%20Fusion%20%282007%29.pdf?dl=1 https://www.dropbox.com/s/64dovc1b7n9hin1/Stream%20Fusion%20%282010%29.pdf?dl=1 A newer development: https://www.dropbox.com/s/8jk6jupjxbsi75p/The%20HERMIT%20in%20the%20Stream%20%282014%29.pdf?dl=1
We cannot have different notions of it because it's a concrete thing :) https://people.mpi-sws.org/~joshua/bitype.pdf I think you are just talking about Hindley-Milner.
Regardless of whether you can go from `default.nix` -&gt; `cabal.config`, you could still make a `cabal.config` &lt;-&gt; `stack.yml` converter which I think would be a nice thing to have.
The expression zip (fmap f xs) xs can also be expressed as fmap (\x -&gt; (f x, x)) xs so `classify` can be simplified to classify cs = fmap (\p -&gt; (V.minIndex (fmap (`dst` p) cs), p))) --- I'd probably write `(V.filter ((i ==) . fst) m)` instead of pattern matching on the tuple, but that's a stylistic choice. (1 / fromIntegral (length l)) * sum l Isn't that equivalent to sum l / fromIntegral (length l) ?
One thing I haven't really figured out is to easily combine parallelism and fusion. Is it even possible to combine them? Let's say I have a lazy list and I'm applying a pipeline of changes (a few filters, a few maps, followed by a final fold) and one step is particularly slow. So we parallelize it using perhaps parBuffer from Control.Parallel.Strategies. And then that seems to be destroy list fusion. 
Funny, today John Wiegley at ICFP mentioned it and showed it's responsible for a cool 15% performance improvement wrt his own work on a provably-correct ByteString 
What a silly remark. The english language frequently and remorselessly gives the same name to multiple things. I'm not just talking about hindley-milner, outside-in also fits under "bidirectional type checking". Whenever I have heard bidirectional type inference discussed, particularly more colloquially, it has been under my definition.
Thanks, very cunning to express the zip as you says. And of course is equivalent the latter expression! 
Elm would be much easier. Purescript's documentation isn't yet that good for beginners. I would say yes to your second question.
I use DreymarR [extended keyboard](https://github.com/DreymaR/BigBagKbdTrixXKB) which allows to create a new layer of keys when pressin caps-lock. In its default layout, Caps-lock+J maps to &lt;LeftArrow&gt;., caps-lock+&lt;space&gt; return etc ...
Ffdddff
Here's a question about controlling access to state: Suppose I have a record, with restrictions on some of the fields, or say invariants across multiple fields. If I keep it in a MonadState, then I can write: data R { _int :: Int } modifyInt f r = do int &lt;- modify &lt;$&gt; State.gets _int if even int then State.modify \r -&gt; r { _int = int } else Except.throwError "odd" If I put the monad in a newtype and don't put it in MonadState, then we can't use State.modify, only modifyInt, and it will be hard to accidentally get an odd number in `_int`. However, I'd like to compose these modifications ala lenses: data State { _sub :: R } sub = lens _sub (\f r -&gt; r { _sub = f (_sub r) }) -- non-monadic lens -- say %= is the modify in state operator: xyz = do sub.int %= (+1) -- should throwError ... What's the "best practices" way to do this nowadays? I can turn modifyInt into an fclabels2 lens, since it allows lenses with monadic effects, and I assume van laarhoven / ekmett lenses can do the same, since they have that functor in there. With fclabels I have to promote pure lenses to monadic ones, but it's not so bad with some extra operators... not sure if ekmett lenses could do that automatically, or a typeclass trick could do it. But I can't get any of MonadState operators without being in MonadState and hence exposing State.modify. I'd like to build the monad into the lens itself, but as far as I know they don't work that way... and can't really, since they have to be composable. So any way I look at it, I'm stuck exporting a State.modify. It would be ok if I could guarantee it was only usable with one of the exported lenses, but I can't think about how to accomplish that. Could the `modify` take a lens with some kind of unforgeable type that you can't get outside its module? I suppose I could not export the record itself, but that would break everything that works with it outside the state monad. It's ok to export a State.modify if I verify the invariants inside it, but let's say I only want to verify the ones that could have changed, since it's too expensive to do them all. My current solution is that I stick all the fields with invariants in field A place, and the ones without in field B, and export a modifyB that can be used with lenses, and everything in A has dedicated modify functions. It's getting awkward though, because a few things in B are starting to get invariants too. This is just standard get/set OO stuff, and writing manual modify functions for everything is the old-school OO way, so it's not like it's worse than what lots of people already put up with. Still immutability has the aditional problem in that you can't do `a.getB().modifyC(...)`, you have to do `modifyB $ modifyC ...`, at which point you are exporting modifyB, which leads back to restrict modifyB.
Nice! I must not have been paying attention because I was pretty sad llvm-general only went to 3.5, I didn't know about llvm-hs but it looks like it's been around since 4.0 The packages (llvm-general) seem to have the same description, what is the relationship between them?
I don't mean to brag, but I have something quite similar but, eh, better. I even did similar stuff with modes and circle of fifths and so on. I need to sleep, but I'll come back to it. No bamboozles.
I think your instincts are noble, but verification comes through writing code and testing it manually in the REPL. We all have the instinct to automate, but in this case I would argue that it's putting the cart before the horse. They need to do it themselves. More to the point, they need to fail on their own terms so they can figure out why they failed on their own terms. A verification tool can give a thumbs up or thumbs down, but it doesn't beat learning how to read and interpret compiler errors. Thanks for your comments and your thoughtful response—I'm glad you found this resource useful.
Skip Elm. Go straight to Haskell. The Haskell Book makes it feasible.
I'd submit that a large portion of Python's growth is that it has become the language of machine learning, and machine learning has exploded in recent years.
I like this style a lot more. Much better information density. When reading LYAH, I felt like half the text was exclamation points.