The obvious example of a monad that doesn't have a run function would be the IO monad. You write code that uses the IO monad not because you want to get out a pure value at the end of it but rather because you want the side-effects that the IO monad causes. Other monads exist which have effects that can be purely extracted but which might have nothing to do with their wrapped value. When you have a kind of object that wraps up a value inside itself and then allows you to call regular functions on that value then you have what is known as a "functor". The fact that monadic functions return value wrapped in monads is part of what makes them so powerful, because the monadic functions can return monads that do monadic things and can thus do more then just manipulate the wrapped up value. If you want to look at monads that don't contain any wrapped value then the first place to look is the Maybe monad, which is sometimes also called the option monad.
Think of it in terms of the type. (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b The m must describe the same monad, that is, you could not have IO String on the left-hand-side and State String on the right-hand-side.
Here's a random sampling from the haskell blogosphere. I'd have a more organized outline if I knew more about this :) Important typeclasses fieldguide: Brent Yorgey's famous [typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) Tekmo [writes](http://www.haskellforall.com/) about CT for his pipes work, and also has a couple posts on lens. Edward Kmett, Gershom Bazerman, Dan Doel at [comonad reader](http://comonad.com/reader/) Edward again writing about [comonads and pictures](https://www.fpcomplete.com/user/edwardk/cellular-automata). [Dan Piponi](http://blog.sigfpe.com/)'s blog. [Here](http://www.reddit.com/r/haskell/comments/1ht4mf/books_on_category_theory_for_beginners/)'s a /r/haskell post on CT books from a while ago, if you haven't come across it. If this kind of stuff is what you're looking for - shortish blog posts that touch on CT and Haskell things (as opposed to longer-format, books, etc), there is no limit.
OK, so it's just the behaviour that has to be the same. This is a sensible constraint; it would be bad if a monad's interfaces and logic could start mutating halfway through a chain. 
For the technical definition of a monad, you're dealing with a single monad and thus "same" would be "the same construct". For purposes of being definite about what "bind" means, it's valuable to say that "same" means "the same construct". If you want to generalize it to any subtype of some interface and you've got a function `cast :: (s &lt; t) =&gt; s -&gt; t` then you may as well just do it as a separate step -- given class Monad m =&gt; MonadEffectKind m instance MonadEffectKind M instance MonadEffectKind N a :: M A f :: A -&gt; M B M &lt; N g :: B -&gt; N C bind (cast (bind a f)) g :: N C
For type theory, read Practical Foundations for Programming Languages (by Harper, [here's a pdf](http://www.cs.cmu.edu/~rwh/plbook/book.pdf)) and Types and Programming Languages (by Pierce). 
Not a good book for introductions, IMO, compared to PFPL or TAPL.
1. Got it. The 'run' function is not a requirement, and doesn't have to just na√Øvely hand me the wrapped value. It might not even have to exist / give me it at all if all I want is the side effects of the computation. 2. So, you're saying that I can pass a bound function that might be its very own monad chain, and effectively push the output of it into the new context? So the bound function could return a different type of monad from the monad its being bound to? That is, can I bind a chain of IO monad calls to a log writer? Will my log writer do a write for each IO event or will it do a single write for the whole lot? I feel like my understanding is a little wooly here. 3. I'll look into that promises implementation to try and get a feel for putting monads to work in JavaScript. Even the concept of a functor is a powerful-looking new pattern for me.
Some other monads that don't have run functions: `Maybe`, `[]`, and `Either`.
In order to avoid the overhead of HTTP, I've always used jsonrpc-conduit and network-conduit. This way you get a bare TCP socket to do RPC over. Though I haven't yet found a good client library either. For now I just use aeson to build call messages.
[Types and Programming Languages](http://www.cis.upenn.edu/~bcpierce/tapl/) by Benjamin Pierce covers type theory, and systems of type inference that we can have, and the ones we can't and why. [Pearls of Functional Algorithm Design](http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383) by Richard Bird covers how to think equationally about code. It is probably the best guide out there on how to "think" like a Haskeller. Not directly about a discipline of math you can apply, but the mindset is invaluable. [Wadler's original papers on monads](http://homepages.inf.ed.ac.uk/wadler/topics/monads.html) are probably when they finally clicked for me. The original [Idiom](http://strictlypositive.org/IdiomLite.pdf) paper is also a golden resource for understanding the motivation behind applicatives. Jeremy Gibbons' [The Essence of the Iterator Pattern](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf) motivates `Traversable`, which so nicely encapsulates what folks meant by `mapM` over the years. Uustalu and Vene's [The Essence of Dataflow Programming](http://lambda-the-ultimate.org/node/988) captures a first glimmer of how and why you might want to use a comonad, but it can be fairly hard reading. Awodey's [Category Theory](http://www.amazon.com/Category-Theory-Oxford-Logic-Guides/dp/0199237182) is probably the best general purpose category theory text book. For folks weak on the math side Lawvere and Schanuel's Conceptual Mathematics can be used to bootstrap up to Awodey and provides a lot of drill for the areas it covers. Dan Piponi's [blog](http://blog.sigfpe.com/) is excellent and largely set the tone for my own explorations into Haskell. For lenses the material is a bit more sparse. The best theoretical work in this space I can point you to is by Mike Johnson and Bob Rosebrugh. (Pretty much anything in the last few papers linked at Michael's [publication page at Macquarie](http://comp.mq.edu.au/~mike/pub2000.html) will do to get started). I have a [video out there as well from New York Haskell](https://www.youtube.com/watch?v=cefnmjtAolY). SPJ has a much more gentle introduction on [Skills Matter's website](http://skillsmatter.com/podcast/scala/lenses-compositional-data-access-and-manipulation). You need to signup there to watch it though. For comonads you may get some benefit out of my site [comonad.com](http://comonad.com) and the stuff I have up on [FP Complete](http://fpcomplete.com/user/edwardk/), but you'll need to dig back a ways.
Their "run" functions are: fromMaybe, foldr, and either, respectively. In order to "run" the state monad, you have to provide the initial state. The "run" functions for `Maybe`, `[]`, and `Either` are similar, except the additional information you have to provide is specific to each construct. All useful data types in Haskell provide a way for you to access the components of that data in some way. The promise monad also has a "run" function, but instead of giving it additional *information*, you simply have to block and give it *time* to compute.
Here's the right way to think about monads: a monad `m` is a "style of computation" (e.g. "nondeterministic" or "stateful" or "parameterized" or whatever). `return` is a way to take a single value and turn it into a computer that computes that value -- that is to say, `return x` is the trivial `m`-y computation that computers `x`. `bind` is just fancy function application: given a computer producing an `a`, and a function that takes `a` values and produces computations of `b`s, you can apply the function to the `a` computation and produce a `b` computation. That's how I'd suggest you think about it. All else is fluff or detail.
You are right.
Awesome.
Another monad that doesn't have much of a 'run' function is the empty monad: data Empty a = Empty instance Monad Empty where return _ = Empty Empty &gt;&gt;= _ = Empty It respects the monad laws trivially, since they are all concerned with equality and every Empty is equal to every other Empty. 
Re: 5. I found the best intuition for how it works is 'It should behave like a sane programming language'; that is, these functions are the same: example1 m f g = do { x &lt;- m; y &lt;- f x; z &lt;- g y; return z; } example2 m f g = do { y &lt;- helper m f; z &lt;- g y; return z; } helper m f = do { x &lt;- m; y &lt;- f x; return y; } By the monad laws, `example1 m f g` and `example2 m f g` must return the same value and have the same effects, no matter what `m`, `f`, or `g` are. These programs desugared into bind/return: bind = (&gt;&gt;=) example1 m f g = bind m (\x -&gt; bind (f x) (\y -&gt; bind (g y) (\z -&gt; return z))) helper m f = bind m (\x -&gt; bind (f x) (\y -&gt; return y)) example2 m f g = bind (helper m f) (\y -&gt; bind (g y) (\z -&gt; return z)) Since Haskell is a referentially transparent language, we can freely inline `helper` without worrying about missing side effects or any other issues: example2 m f g = bind (bind m (\x -&gt; bind (f x) (\y -&gt; return y))) (\y -&gt; bind (g y) (\z -&gt; return z)) We can use the following rules to convert `example2` back to `example1`: Eta reduction and/or expansion: `(\z -&gt; F z)` = `F` Right unit: `(bind A return)` = `A` Associativity: `(bind (bind M F) G)` = `(bind M (\temp -&gt; bind (F temp) G))` 
I learned a lot about type theory from Augustusson's excellent blog post, [simpler easier](http://augustss.blogspot.co.il/2007/10/simpler-easier-in-recent-paper-simply.html?m=1).
There is [LambdaCubeEngine](http://www.haskell.org/haskellwiki/LambdaCubeEngine) which may fit the bill. For purely GPGPU stuff, there is [accelerate](http://hackage.haskell.org/package/accelerate)
 The Accelerate language TODO: describe the Acc and Exp language. (...) Last edited by tmcdonell, 2 years ago OP will surely deliver. (: Seriously though, the LambdaCubeEngine won't work for me as I actually need my own shaders and data organization. So, I'll take a deeper look on accelerate. Do you know if it is suited to realtime GPU applications? 
I like Pearls of Functional Algorithm Design, but for anyone who gets lost in that I recommend [Introduction to Functional Programming using Haskell](http://www.amazon.com/Introduction-Functional-Programming-Haskell-Edition/dp/0134843460/ref=sr_1_6). It starts off introductory, but it covers a lot of material, and covers it in depth.
Accelerate is for GPGPU computations, not for graphical work necessarily. It's actually quite well maintained and documented: https://github.com/AccelerateHS/accelerate
&gt; If you don't have to have a 'run' method, though, how do you get the final result of the computation? `IO` doesn't have a `run` method. You never "get the final result" of its computation. Instead you assign it to the `main` function and the runtime executes the IO actions.
Ah, now I think I 'get' point 2. If I can bind new monadic functions to my first monad, I can chain things however I like. So I could go from doing some maths to get an int (with a monad doing null / NaN checks) to returning a string from an enum to a new monad that performs string operations, carrying a transaction log within the stringbuilder monad for reversibility. If I've understood you correctly, functions that maintained this contract - allowing you to move from chain to chain - would be quite powerful.
While the IO monad is a good example, many monads don't have any way, or have more ways than one, to provide a 'run' function. A value of type Maybe a might be None, and there's no way to get any a out of it. Similiarly, a value of type [a] may be empty, or contain any nymber of values of type a.
My advice: don't get so hung up on Monads. They are just one of the cool things you can do in Haskell. If you're desperate to understand Monads I would advise you to read [this](http://learnyouahaskell.com/functors-applicative-functors-and-monoids). To see why a Monad is interesting, it's important IMO to see what more basic constructs can do and what Monads bring to the table. Start with Functors, then move to Applicative and finally to Monads.
On this bit... &gt; This is just a degenerate version of Functor, in which two type arguments have become one. This simplification means that an Endofunctor is not required to be a parameterised type, unlike a Functor. Actually, I think it *is* a parameterised type. Looking at the class... class Endofunctor a where efmap :: (a -&gt; a) -&gt; a -&gt; a You still have a varying part. IMO it's not the parameter that's missing, it's the function. If it was the parameter that was missing, you'd have a constant type - `Int` perhaps. Maybe you could claim there's an implicit identity function to which `a` is applied. 
It truly is very simple. I looked into genetic algorithms a while back and wanted to write a library that would be able to convert (almost) arbitrary datatypes into chromosomes, leaving very little work to the user. As I understand, this is possible, and there are libraries that do this, but I did not get very far myself. Nonetheless, very interesting to see how minimal an implementation can be.
&gt; So the bound function could return a different type of monad from the monad its being bound to? That is, can I bind a chain of IO monad calls to a log writer? Will my log writer do a write for each IO event or will it do a single write for the whole lot? I feel like my understanding is a little wooly here. You can't do that, but what you can do is use a monad transformer (WriterT) to get what you're looking for. 
Something is bugging in your final generated code: Why is there a list indexation and not an unboxed vector instantiation in it?
There's a branch in the official git repos that's been unmerged for over a year or so. I admit I haven't thought very deeply about how effective generics would be for arbitrary; I've just noticed that a lot of my instances are highly generic. It would be nice to have a generic option, but of course you can still roll your own instance should the need arise. I haven't seen any rationale for not merging it, and perhaps there is one, and I'd like to know it!
&gt; Even the concept of a functor is a powerful-looking new pattern for me. The common `Functor` example is lists/arrays and `map`
Your case statement is sometimes called `maybe (...) (\v -&gt; ...) (foo =&lt;&lt; bar =&lt;&lt; baz)`. It obviously doesn't matter which form you choose, but the idea is the same‚Äîwe're eliminating the `Maybe`.
I disagree that usefulness is only of the specific instances. The *generalization* itself is useful too.
He's trying to understand monads in order to deal with all the talk about monads in the context of Javascript promises.
Are there any applications for the empty monad or is it just a fun, trivial solution to the monad laws?
Functor is the name for the pattern where you have a wrapped value (putting aside for the moment that you might not have a value at all, just the wrapping!) and you want to perform a function on said value without having to unwrap it and rewrap it. As you can imagine, that's very useful.
To expand on this: the generalisation is useful because, among other things, it allows us to write single functions that works with *all* specific instances. It gives us a common language to talk about *all*, very different, kinds of monads. Sometimes the generalisation also gives us the implementation for free because it needs to satisfy the laws. The "real work" is still accomplished by the specific instances, which at the end of the day decide what happens when you run the code.
there _is_ a mirror, given above! And we could have more/better mirrors, but we just haven't pushed through to set them all up. its not a gsoc project per se, just an investment of time from infrastructure people. Relatedly, if someone wants to jump in on setting up mirrors, we have some server space potentially lined up but could use people willing to set everything up and/or someone willing to organize the whole endeavour.
I suppose this feature is underdocumented, but jmacro-rpc provides code to automatically create rpc client calls including unmarshalling: http://hackage.haskell.org/package/jmacro-rpc-0.2/docs/Network-JMacroRPC-Base.html It is written without a specific transport or endpoint in mind, so the generated stubs need to be passed a function of `(ByteString -&gt; IO (Either String ByteString)` that connects to the designated endpoint and returns either a result or an error. I had intended at some point to write some backends for this using different http client libraries, but they were all so straightforward it hardly seemed important. This is particularly the case since client-side calls are so trivial that it generally isn't worth the hassle. jmacro-rpc _also_ provides (among many other things) a json-rpc server that operates over snap and happstack backends.
First read [The Genuine Sieve of Eratosthenes](http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf) [pdf]. It's a great paper on the topic. Essentially, you're not running the actual sieve of Eratosthenes here. You're comparing each number against all the multiples of the primes less than the number, rather than just checking if it's been marked as a multiple of some prime. You can speed this up somewhat by only comparing against multiples less than the number - removeMults i = let mults = [i*i, i*i + i .. n] in filter (\x -&gt; Just x != safeHead $ dropWhile (&lt;x) mults) But this still isn't going to be the sieve of eratosthenes.
That's a pretty impressive changelog. Some questions: - How are Hackage Haddocks generated? Will all the docs be upgraded, or does it take an explicit upload for this to happen (due to version incompatibility, for example)? - Is there a reason `show-extensions` is not enabled by default? Also, I support `--print-missing-docs` to be a default flag. I prefer having some extra line noise for empty docs for a handful of functions when it reminds me to finally write some docs for that function with a bus factor of 1.
Given that the list monad is the context of PROLOG-style nondeterministic functions, I'd think a better "run" would be something akin to `head` (give me the first valid answer you can find, if there is any). `foldr` is a bit more general. You could actually see it as a strict superset of the `Maybe` monad which provides the context of partial functions.
Translating from a Haskell environment to a .NET environment is much easier if you use F#, Microsoft's functional declarative language similar to Haskell.
&gt;Debugging imperative code is special, though, because once you've executed an action, you can't go back to exactly how things were before the step. .net has intellitrace, http://msdn.microsoft.com/en-us/library/dd264915.aspx which is super slick and kind of let's you rewind time like you want. There's no reason haskell couldn't have an even better version. 
That's LambdaCube :) e.g. here is variance shadow mapping in pure functional style: http://lambdacube3d.wordpress.com/2012/10/14/variance-shadow-mapping/
What language features are supported by Template Haskell is a function of what language features its users want it to support, plus an unknown time delay (a "constant of implementation", if you will). Here's the [trac ticket](https://ghc.haskell.org/trac/ghc/ticket/8761), if you're wondering. Obviously it's not going to be in the upcoming release, but if they know users are actively interested it's less likely to be deferred indefinitely like some TH features have been.
Great job! Btw. The link to http://elm-lang.org/learn/Inputs.elm under breaking changes is broken.
Yep, I found that ticket. I'd love to go ahead and do it myself if I had the time and energy away from my day job (although it's more turning into a day and night job...). 
- The `NFData` superclass is unnecessary for `Chromosome`. - "pop, ps, mp, stopf, gnum" are not descriptive names. - Lines much longer than 80 characters. - `Chromosome` has no instances. - "(hint: use currying or closures)" - it's standard Haskell to do this
Ahhhh, okay, that should be fixed along with another broken link to a bunch more examples! Also, thanks :D
The video won't load for me.
Have you tried contacting any of the haskell.org crew? Responsible disclosure and all that?
I've already fixed it, I'm going to notify the right people.
Responsible disclosure? This isn't a zero day. /r/haskell seemed a faster way of reaching someone with privileges. Whose interests are served by not pointing out a longstanding defect on an important wiki page? If the docs need attention, informing the community is the only solution.
I agree, this isn't a responsible disclosure issue at all.
amazing job, well done
This appears more that it was a valid link 6 years ago, and the domain probably expired sometime later and now points to a squatter. So not something malicious, but just linkrot.
Why `Void` and not `()`? You shouldn't really be able to make a value of type `B'CI (Void, Void, Void, Void, Void)` but `B'CI ((), (), (), (), ())` is fine and does exactly what you want, since you just ignore that type anyway. My thinking on this is that the setup should be a typeclass class IsoUnit a where toUnit :: a -&gt; () fromUnit :: () -&gt; a which is supposed to witness an isomorphism between `a` and `()`. Dually for `Prism`s we could have class IsoVoid where toVoid :: a -&gt; Void fromVoid :: Void -&gt; a I think the dual case of your gist corresponds to exhaustion checking for pattern matching.
And the author told me today that a new edition of the book will come out later this year. It sounded very cool. :-)
I'd be interested in working on the LaTeX backend. Who should I talk to? EDIT: Also, &gt; Double underscores are used to denote that something is bald. Do double asterisks denote that something is hairy?
I had an aha moment one day when trying to understand why monads are more powerful than functors, which I will attempt to share below. My apologies if this leaves you less enlightened after reading it, or if I use any terms incorrectly. If you look at the type signature for `fmap :: (a -&gt; b) -&gt; f a -&gt; f b` it makes it clear that the function that you map over the functor cannot change its "shape". Two examples will hopefully make this more clear: Given a value of type `Maybe Int`, you could have such values as `Just 3` and `Nothing`. Since the function you're applying (using fmap) returns a pure value it cannot change a `Just` value to be a `Nothing`. Given a value of type `[Int]` (ie a list of integers), mapping a function over the list will return a list of exactly the same length as the input list. Because the applied function is pure, it cannot add or remove elements to or from the list. On the other hand, from the type signature of bind `(&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b` it is clear that the function returns a value wrapped in the given monad. What this means in practice is that the function, since it is responsible for wrapping the given value, can change how the value is wrapped: isPositive :: (Num a, Ord a) =&gt; a -&gt; Maybe a isPositive x | x &gt; 0 = Just x isPositive _ = Nothing `Just (-3) &gt;&gt;= isPositive` will return `Nothing`; you could not implement this using fmap. The best you could do is return `Just Nothing`. doubleUp :: t -&gt; [t] doubleUp x = [x, x] `[1,2,3] &gt;&gt;= doubleUp` will return `[1,1,2,2,3,3]`; you could not implement this using fmap. The best you could do is return `[[1,1],[2,2],[3,3]]`. In both cases, you can see that trying to change the shape requires that you be able to make choices about how the value is wrapped. Monads allow you to do this, functors do not. That said, if you don't need to change the shape of a value, the fact that the type signature of fmap guarantees that you cannot is an advantage!
Haskell at base is indeed cross-platform. And most of the core libraries are similarly so -- certainly the Haskell Platform exists and is supported across Linux, Mac, and Windows equally. However, many things you may want to do are platform-specific, or at least a pain to do in a cross-platform way. GUIs are notably the worst offender here. I know cross-platform GUIs can be done, but it is a pain, and limits your choice in libraries considerably. If you spell out what you're looking to do in more detail, it would be possible to provide better advice...
Haskell type system tracks types (Sets), not values (Elements inhabiting the Sets). Here the type of the RHS is the same as the type of the LHS : Shape. Therefore, it is properly typed. By the way, I have never seen pattern matching at top-level, I didn't even know it was legal. What is the point of this kind of declaration ?
As a non-Haskeller, your comment here would go a long way to convince me that the type system is just overhyped and useless. To prevent bugs I want the type system to prevent improper kinds of *values*, not types. Fortunately, I am a Haskeller, so I do know the type system is useful, and the OP has a point: It's exactly the kind of problem the type system is meant to catch. Haskell's type system does not give an error or warning about let-pattern-matches that are not comprehensive. IMO, this should change. I believe such non-comprehensive pattern matches ought to explicitly be irrefutable with ~. Similarly, `&lt;-` pattern matches that are not comprehensive should yield either a warning or the good old upgrade from a basic Monad to a MonadZero constraint upgrade. 
Wow, I'm very surprised to learn that GHC does not give a warning for non-exhaustive pattern matches in a let. Personally I'd prefer a language where pattern matching in a let was only possible for single-constructor datatypes.
I do need a GUI. I want to create a simple word processor for complex glyph handling. I need to incorporate the harfbuzz libraries somehow. QT has quite decent support for pango and harfbuzz. Why is cross-platform GUI's painful to do in Haskell?
How about `RecordWildCards`? contactMakerN :: Map String String -&gt; Maybe ContactInfo contactMakerN m = do _ciName &lt;- Map.lookup "name" m _ciStreet &lt;- Map.lookup "street" m _ciCity &lt;- Map.lookup "city" m _ciProvince &lt;- Map.lookup "state" m _ciPostalCode &lt;- Map.lookup "zip" m return ContactInfo{..} Or using better types than `String` that you can‚Äôt mix up, or using record syntax instead of positional syntax for a record constructor?
What about a program which, for any number, gives the last number in the subsequent hailstone sequence? You can't prove it will terminate for all inputs without proving the Collatz Conjecture.
Compile your program with the ghc option ```-fwarn-incomplete-uni-patterns``` and you'll get the warning "*Pattern match(es) are non-exhaustive*" on your definitions. [The docs](http://www.haskell.org/ghc/docs/7.6.3/html/users_guide/options-sanity.html) say that incomplete pattern warnings are not "enabled by default because it can be a bit noisy, and it doesn't always indicate a bug in the program." As already discussed, it's not actually a type error, and making it an error would disallow many valid pattern matches, for example: if length xs == 1 then let [x] = xs in x else 0 The specifics of this example aside, it's impossible in general for the compiler to prove that the pattern match in such code is safe. There are quite a few functions in the Haskell standard library that can trigger the equivalent of a pattern match error at runtime: one example is the list function ```head```, which you could imagine being implemented like this: head (x:_) = x This gives an error on an empty list. It's an example of a [partial function](http://www.haskell.org/haskellwiki/Partial_functions). The [Safe](http://hackage.haskell.org/package/safe-0.3.4/docs/Safe.html) library defines a number of safe alternatives to partial functions in the standard library. 
Well, cross-platform GUIs are painful to do in general. On top of that, binding to large-surface-area-APIs is rather a pain from Haskell, made worse by the fact that these APIs are moving targets, and made double-worse when those APIs are C++. Now combine all this together, and recognize that you'll need your xplat GUI to build well and cleanly in the same version in all platforms, and then have a set of bindings targeting that version, and then have the proper toolchain (on windows especially) to get the Haskell bind properly to the library matching the correct name mangling convention... now that's all doable, but it is historically a bit of a pain. And even if you _do_ get it all working, then fighting against bitrot (if you hope to distribute code rather than binaries) is _also_ hard. Sorry :-(
&gt; Humm.. what's the difference between the pattern above from the next one, which I presume to be very popular? &gt; (x,y) = (10, 20) One difference that can be significant is that the type of a tuple depends on the types and number of its elements, so for example (x,y) = (10,20,30) is a type error, not just a pattern match error, i.e. the types of the terms on either side of the = sign are different. But in this case: (Rect x y, Circle z) = (myFunc1, myFunc2) ...the issues are the same as addressed in my other comment: if you enable the appropriate incomplete pattern match warning, it'll warn you about this. 
&gt; By the way, I have never seen pattern matching at top-level, I didn't even now it was legal. What is the point of this kind of declaration ? This is idiomatic Haskell. The short quick answer is you may declare functions multiple times for the cases. It's covered nicely in [Learn You A Haskell](http://learnyouahaskell.com/syntax-in-functions#pattern-matching)
I agree, except for explicitly irrefutable pattern matches (via ~ syntax). 
If you want the compiler to complain about the second pattern match then you can use two different types. data Rect = Rect x y data Circle = Circle z (Rect x y, Circle z) = (myFunc1, myFunc2) Now if `myFunc1` does not return a `Rect` or `myFunc2` does not return a `Circle` it will not allow the program to compile since it would be type mismatch. I know in dependently typed languages you can check those pattern mismatches at compile time. You example looks trivial, to do the pattern matching check, in general though this does not always seem to be the case. [x, y] = [1, 2] -- works fine [x, y] = [1, 2, 3] -- pattern match fail. Take for example: [x, y] = myFunc3 Now if you want to check at compile time if this pattern match works you need to know at compile time if myFunc3 returns a list with two elements or not.
It's just great that the semantics of Haskell programs is entirely unaffected by inserting the top-level declaration True = False If only electoral democracies could achieve such sophistication.
No, it's not idiomatic. What you'r pointing at is for function definitions; this guy pattern matches a value constructor at a top level. Nor does your link, or any other place till now have I seen that.
&gt; Is Haskell type system as strong as claimed? Yes. Haskell's type system is, indeed, quite "strong", in the sense that it can help you catch more mistakes than most other languages can. But, importantly, it doesn't catch *all* mistakes, and nobody claims it does. That's actually a good thing! There are a few advanced languages which are "stronger" (more precise) than Haskell, and they do catch many more mistakes, but they also reject many more programs. In my opinion, Haskell is at a very good sweet spot between too many unchecked mistakes and too many rejected programs. That sweet spot is: Haskell will catch any error that can be expressed as a type error, and will miss any error where everything has the right type but the values are wrong. Your example equation equates terms with matching types, but mismatching values, which is why Haskell doesn't catch the mistake. Don't be disappointed so soon: over time, you'll learn to structure your programs in such a way that most mistakes will be type mistakes instead of value mistakes.
All thunks are cached to avoid recalculation. But thunks can be GC'd, and CAFs can too (it tends to be more difficult, though).
Out of curiosity, where are you taking this SICP-based course? There's a possibility that you're at my university... :)
With LiquidHaskell, it is an error. http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1395714290.hs (press "Check")
I've been lurking on various ghc related mailing lists while this was being worked, and from what I saw the effort was even more heroic than it appears. Thanks and congratulations.
My mistake, I thought kstt was referring to the head (x:_) ... function just above.
Dino picked the wrong idiom, but these are just declarations, exactly the same as declarations in a ```let``` or ```where``` expression. All of these cases are handled by the same grammar rule: in the Haskell 98 grammar for [Declarations and Bindings](http://www.haskell.org/onlinereport/decls.html#declarations), the rule in question is called ```decl``` - see the rules at the top of the page as well as 4.4.3, "Function and Pattern Bindings". If the ```decl``` rule were removed from the ```topdecl``` rule, it would no longer be possible to declare e.g. ```x = 1``` at top level.
Oh. I hadn't heard about those; they certainly make it much nicer. My main objection to record syntax had been that the natural use of it would seem to require typing the field name or equivalent variable name three times. However, that `RecordWildCards` syntax extension seems to take care of two of those repetitions, making it so that you only need each field name once. Huh - I wonder why I hadn't seen that extension before, since it and `DisambiguateRecordFields` seem to solve so much of the ugliness of Haskell's record syntax that is what initially drove me to start using Lens. I still think that there might be a use for the Haskell equivalent of the Builder pattern, but this makes it much, much narrower.
[nLab!](http://www.ncatlab.org)
IO is also an instance of PrintfType :) stopf :: SinInt -&gt; Int -&gt; IO Bool stopf best gnum = do let e = err best printf "Generation: %02d, Error: %.8f\n" gnum e return $ e &lt; 0.0002 || gnum &gt; 20 
Short answer: They dont. Long answer: If you were to write a backend for LamdaCube that uses the Haste FFI to call WebGL, you *might* get it to work, but it would be a lot of effort.
Using a browser as GUI could be a possibility for him, no? AFAIR there`s a multitude of options there that are easy to tinker around.
I didn't prove termination, I modelled it as a coinductive stream of states. Extracting the final result inside the language would require me to prove that it terminates for that input though.
This code is much better, thanks!
You are cheating by including the pragma {-@ LIQUID "--totality" @-} Which is not part of the example. Remove that, and it is declared safe.
Deconstructing on patterns that could fail is a (n unfortunate) possibility in Haskell. In a [more disciplined language](http://etorreborre.blogspot.com/2012/06/strong-functional-programming.html), this kind of partially-defined language construct would be rejected by the compiler. However, Haskell makes this among many other sacrifices in the name of pragmatism. A similar related issue that has bitten me more than once is when I accidentally define a value recursively. (Somehow, it happens to me every time I toy around with Haskell... usually when I don't notice I've already used the same name in the same scope). I really wish structural (co)recursion were the default and there was a keyword for when I needed general recursion.
Incredible work!
-1 on the `--print-missing-docs` for the same reason we don't have `-W` on by default in GHC. It can create lots of noise in e.g. `cabal build` output that hides more important problems. That said, please write docs!
I stand by my statement. In my experience it's easier to learn via contrast. First learn what Functors are and see the capabilities they give you. Then you run into something they can't do. Then you learn about Applicative and now you have new capabilities on top of Functors. With Applicative you can get quite far. Then you run into something Applicative can't do. *Then* we learn about Monad and the capabilities they add.
I never ever do anything without the GHC option -Wall which includes incomplete patterns
It doesn't include `-fwarn-incomplete-uni-patterns` though. Don't know why. 
great work, thanks!
(Speaking as someone who uses `-Wall` and `-Werror`.) Here's a line of code I wrote earlier today: primary : secondary = T.splitOn "&lt;br&gt;" vPrimary This can't fail because `T.splitOn` never gives back an empty list. But this is not encoded in the type. How should I rewrite that line of code to avoid a warning if `-fwarn-incomplete-uni-patterns` is enabled, without obfuscating what's going on? Is there a pragma to disable a warning for a single declaration?
Why do you find it more acceptable for irrefutable pattern matches? An irrefutable pattern match in a function definition is just sugar for a let, after all.
Most of your functions have a type which can be simplified to: f :: t -&gt; g -&gt; (a, g) This is a candidate for being abstracted using the Control.Monad.State monad. Functions of the above type can be converted to: f :: t -&gt; State g a Functions of this form: f :: g -&gt; t -&gt; (g, a) Should have the g and t parameters re-arranged to fit the State monad. If IO is important, for example if you need a function with the following type: f :: t -&gt; g -&gt; IO (a, g) then you should use StateT in the same module, so this function would be equivalent to: f :: t -&gt; StateT g IO a It is best practice to define your own newtype wrapper around State or StateT: newtype Genetic g a = Genetic { runStatefulGA :: StateT g IO a } deriving (Functor, Applicative, Monad) Then you need to instantiate MonadState: instance MonadState g (Genetic g) where { state = Genetic . state } Be sure to use -XGeneralizedNewtypeDeriving , -XDeriveFunctor , and -XMultiParamTypeClasses when compiling so the "deriving" clause works, and so you can instantiate a MonadState with a polymorphic variable g. You could also instantiate the "MonadIO" class in the Control.Monad.IO.Class: instance MonadIO (Genetic g) where { liftIO = Genetic . liftIO } That lets people perform arbitrary IO operations like "putStrLn" or "newStdGen" within the Genetic monad like so: geneticStdGen :: Genetic StdGen geneticStdGen = liftIO getStdGen In your function definitions, you can use the procedural programming style to update the state. To modify the random generator g you would use "get" and "put" functions, which are automatically defined when you instantiate MonadState. The class instantiations will cause the type checker to automatically infer the following types for "get" and "put", even if you do nothing to define them yourself: get :: Genetic g g put :: g -&gt; Genetic g () For example, to define a function in the "Genetic" monad that generates a random integer using System.Random.next: geneticRand :: (Random a, RandomGen g) =&gt; Genetic g a geneticRand = do g &lt;- get -- get the current random generator let (a, g') = random g -- step the random generator using System.Random.random put g' -- put the updated random generator back into the state return a -- return the integer part But just so you know, the above 4 lines of code I wrote there are exactly the same 4 lines of code that define the "state" function in the "Control.Monad.State" module, so the above function can be defined simply as this: geneticRand :: (Random a, RandomGen g) =&gt; Genetic g a geneticRand = state random Also your "zeroGeneration" function could be defined as: zeroGeneration :: Genetic g a -&gt; Int -&gt; Genetic g [a] zeroGeneration = flip replicateM replicateM is defined in the "Control.Monad" module. Yes, just "flip replicateM" is all you need to define that function. If you flipped the parameters so the generator came first then the population size came second the function "zeroGeneration" could be defined as such: zeroGeneration :: Int -&gt; Genetic g a -&gt; Genetic g [a] zeroGeneration = replicateM That is all. It will work as you expect it to. In fact, it seems you really have no need for the "zeroGeneration" function. Just explain in the comments for everyone to use "replicateM". The "runGAIO" function can be defined as such: runGAIO :: (Random a, RandomGen g) =&gt; g -&gt; Int -&gt; Double -&gt; Genetic g a -&gt; (a -&gt; Int -&gt; Bool) -&gt; IO a runGAIO randGen popSize mutProb makeChromosome stopP = evalStateT myOldRunGAIOFunc randGen where myOldRunGAIOFunc = do chromosomes &lt;- replicateM popSize makeChromosome -- make a population ri &lt;- state random -- generate the next random number from the generator ... -- other logic for running the algorithm using the generated chromosomes. You could even simplify the above by requiring the chromosome data type to instantiate the System.Random.Random class rather than passing the random chromosome generator to "runGAIO". It would look like this. runGAIO :: (Random a, RandomGen g) =&gt; g -&gt; Int -&gt; Double -&gt; (a -&gt; Int -&gt; Bool) -&gt; IO a runGAIO = randGen popSize mutProb stopP = evalStateT myOldRunGAIOFunc randGen where myOldRunGAIOFunc = do chromosomes &lt;- replicateM popSize (state random) -- make a population ... -- other logic for running the algorithm using the generated chromosomes. I hope that helps :-) 
You could disable the warning with a pragma at the top of the module. I'm not sure if that's possible for an invididual declaration. In any case, if `splitOn` never returns an empty list my preference would be that it actually returns an explicit `NonEmptyList`. This does, however, cause a bit more noise. It's not everyone's cup of tea.
I hope someone has a good resource on Church encoding, it sounds really interesting.
Great! I started working on a game in Elm and hit an issue with the stack size, so hopefully Trampoline can solve some issues. :) thanks for your hard work!
[this](http://okmij.org/ftp/tagless-final/course/Boehm-Berarducci.html), maybe?
This one's alright http://jozefg.bitbucket.org/posts/2014-03-06-church.html
A few more suggestions: **Monads** * [The essence of functional programming](http://homepages.inf.ed.ac.uk/wadler/topics/monads.html) *Wadler* - which gives motivation on why monads are important. I think it's more important to understand the motivation first. **Applicatives** * [Applicative programming with effects](http://strictlypositive.org/IdiomLite.pdf) *McBride* * [The essence of the iterator](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf) *Gibbons* **Parsing** * [Polish Parsers, Step by Step](http://www.staff.science.uu.nl/~swier101/Papers/2003/p224-swierstra.pdf) *Hughes, Swierstra* * [Combinator Parsing: A Short Tutorial](http://www.cs.tufts.edu/~nr/cs257/archive/doaitse-swierstra/combinator-parsing-tutorial.pdf) *Swierstra* Parsec is a fantastic library, but it often gets assumed it's the only way to do parsing. `uuparsing-lib` has a fairly weak API, but is (imo), a much more interesting take on parsing. Swierstra's writing on the subject is very interesting. **Coroutines** * [A Poor Man's Concurrency Monad](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.8039) *Claessen* There are other papers on this, but here's another take on using a free-monad type construction to implement round robin scheduling.
F# is not similar to Haskell; it's more along the lines of ML. But it's sure a lot more similar to Haskell than C# is.
&gt; `uuparsing-lib` has a fairly weak API Do you mean "weak" is in "not providing many features", or is there something wrong with it? I watched Tim-from-Barclays's talk yesterday and he had a lot of praise for that library.
Feature-wise it's fine, I mean weak as in I find it a fairly poorly laid out API. For example, * Stuff like the `CHANGELOG`, `README` and `Demo` is non-standard practice. * A lot of the things in `Utils` are all prefixed with `p`. It would be easier to not have this prefixing * `Core` can't decide on what type of naming convention it wants. `likeThis`? Or maybe `like_this`? Who knows! * What is this `Idiom` stuff all about? They are minor concerns, but they do ruin the experience - especially for a library that has a lot of potential. It just needs someone who's good at designing APIs to apply a bit of polish.
Does this solution require that I know ahead of time what all of the keys present in the database are?
nLab, if nCafe is any indication, won't be exactly an easy read, even with a proper math phd background...
The current construction lets you nest queries deeper inside the applicative, which may or may not be useful. On top of that though, I don't know how you would run multiple queries with a single `runQuerying` function. In production, we have something like the following: classes &lt;- runDbR (Pipes.toListM getClasses) pg FT.runQueries $ FT.withQuery (FQNew.getClassTypesById pg) $ \classTypesById -&gt; FT.withQuery (FQNew.getStudiosById pg) $ \studiosById -&gt; FT.withQuery (FQNew.getUsersByTrainerProfileId pg) $ \trainerProfilesByTrainerProfileId -&gt; FT.withQuery (FQNew.getClassBookingsByClassId pg) $ \classBookingsByClassId -&gt; for classes (enrichClass classTypesById studiosById trainerProfilesByTrainerProfileId classBookingsByClassId)
This is extraordinarily neat! I have a few comments. Firstly, I have typically done things the "Oof. That got a lot more complicated!" way, that is, run the first query, look up all the necessary keys, and then look up all the associated data in one further query. Your approach seems to abstract this away nicely. Secondly, a lot of cases where you might want to do this kind of thing can and should be done with joins on the database side. However, I'm sure you know which ones can and which ones cannot, and for the ones that cannot your approach seems to add a lot of value. Thirldy, in the cases where you *can't* do it with a join on the database side this is often a weakness with the RDBMS in question. Table-valued queries would be really helpful. Your API seems to model something like table-valued queries but abstracts the "artificial implementation" away, so the API consumer doesn't need to know the RDBMS doesn't support table-valued queries. Very nice. I have one question. Why is it data Query k v = Query (IORef (Map k [MVar (Maybe v)])) rather than data Query k v = Query (IORef (Map k (Maybe (MVar (Maybe v))))) (sorry about the even deeper nesting!). It seems that if the value of any particular key is a non-empty list of `MVar`s, every `MVar` in the list gets filled with the exactly same value. Is that right? If so, why not just make it one or zero `MVar`s, i.e. a `Maybe` of `MVar`s? (In fact since the absence of key implies the absence of a value, you may be able to get away with just the `MVar` rather than a `Maybe` of an `MVar`).
Great links! Any good resources on working with laziness? Like [The Haskell Heap](http://blog.ezyang.com/2011/04/the-haskell-heap/) or [Reasoning about space leaks with space invariants](http://apfelmus.nfshost.com/blog/2013/08/21-space-invariants.html)
I see hilarious reaction of my team members when the link i suggest them has a word "paper" in it :)) That single word probably hurt adoption of haskell more than anything else. 
&gt; That's actually a good thing! There are a few advanced languages which are "stronger" (more precise) than Haskell, and they do catch many more mistakes, but they also reject many more programs. Would you mind to name a few of those languages? 
Typo/thinko: "that we can guarantee that the MVar is never read for before it is written from" should be "... never read from before it is written to".
Could you explain further why you need a list, perhaps using an example? I can't grasp it. Since in for_ mvars $ \mvar -&gt; putMVar mvar (Map.lookup k qResults) you put exactly the same value into each element of the list at key `k` I don't see why you need any more than one element in the list.
Agda, Idris and Coq are examples. 
the best source for learning about lenses is this: https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation
oh yeah, you're right. That's dumb. It might be on youtube somewhere.
Really enjoyed the paper. I hadn't thought about using a different data structure (sadly). I had it working that way (removing the elements that are not primes) 'cause I couldn't think of a way to mark the elements. Thanks for the help :) 
`--@ foo` is a legal comment? Are you sure? (right at the end of the page.) edit: um, anyway, lots of great stuff in this release! Thanks you all your hard work!
"Applicative" parsing in uu-parsing predated Conor and Ross's Applicative paper and library by quite a few years. I think the original Control.Applicative class didn't have the (&lt;\*) and (\*&gt;) operators so uu-parsing couldn't use it without losing some opportunities for optimization. (Disclaimer - I haven't used uu-parsing for quite a while, so may be off-message here).
If I'm not mistaken, Applicative based parsers can only handle "context-free" due to the limitation of Applicative. Not exactly a bad thing though. By "context-free", I mean the parser itself cannot be dynamically re-structured based on the values being parsed. I'm not sure whether this limits the grammar it can parse to "context-free grammar". 
The solution outlined in this article seems to counter the problem presented. If the problem is too many dependencies, how is the solution collapsing packages together so that projects must depend on more things? Similar packages from one author and one ecosystem are unlikely to have any kind of weird dependency issues, since they are maintained together.
I had no idea that syntax was even allowed. If I had to something like that, I would do something like data X = Weighted WeightedRecord | NonWeighted NonWeightedRecord 
I didn't realize it was a problem until I noticed that the lens derivation of a type actually does it correctly and I asked someone about it. Then I found about four obvious-in-hindsight bugs in my project and vowed never to name fields in sum types again.
Ah, maybe this has something to do with the semantics of `MVar`. When you read from it in one place it disappears, so you can't then read from it in another? OK as another proposed alternative, it would suprise me then if you couldn't use data Query k v = Query (IORef (Map k (IORef (Maybe v)))) but perhaps there's something else important that I'm missing. It just seems that writing `n` times to the same key is unnecessary.
You are right that this doesn't explain the logic for the solution, but just its execution. Collapsing packages together means the application developer or library author will have a smaller number of packages in their cabal file. You are right that it does risk ending up with a larger total dependency tree (including packages not listed in the cabal file), that depends on how consolidation is done. Packages in the Haskell Platform usually present a smaller installation issue, so if you only end up only adding those dependencies it still may be an easier install. I do feel that this is the opposite of how software is supposed to work. In practice though we install with cabal and cabal seems to do better with fewer large monolithic packages than many smaller ones that form a larger dependency graph.
The way I first grokked monad transformers was by hand-expanding uses of `MaybeT m a` to get rid of the transformer‚Äîinto `m (Maybe a)`. As for free monads, the thing that proved the most helpful for me to get started on that topic was in fact the [operational monad tutorial](http://apfelmus.nfshost.com/articles/operational-monad.html). No, it's not the same as free monads, but it's certainly a helpful bridge...
&gt; Not All Sunshine and Rainbows Pipes and cabal sandboxes would perhaps help here.
You're not writing to the *same* `MVar` n times, you're writing to n *different* `MVar`s. If you wanted to do what you're suggesting, you'd have to have some way for the `MVar` to be shared. Going back to the above example: liftA2 (+) &lt;$&gt; usersAgeById @? 1 &lt;*&gt; usersAgeById @? 1 If you only stored a single `IORef` under the key `1`, how do you communicate the response in two different places? How would the second `@? 1` know that it has to "share" the same `MVar` as you introduced to the first time you used `@? 1`?
Idiotum?
Not sure I understand you now. I made the changes I suggested and it seems to work fine. The second `@? 1` "knows" it has to use the same `IORef` as the first because it looks it up in `keysRef`! Nothing magic there. -data Query k v = Query (IORef (Map k [MVar (Maybe v)])) +data Query k v = Query (IORef (Map k (IORef (Maybe v)))) (Query keys) @? k = Querying $ Compose $ do - result &lt;- newEmptyMVar - modifyIORef' keys (Map.insertWith (++) k [result]) - return (takeMVar result) + keys' &lt;- readIORef keys + case Map.lookup k keys' of + Just v -&gt; return (readIORef v) + Nothing -&gt; do + result &lt;- newIORef undefined + modifyIORef' keys (Map.insert k result) + return (readIORef result) ... - flip traverseWithKey keys $ \key mvars -&gt; - for_ mvars $ \mvar -&gt; - putMVar mvar (Map.lookup key qResults) + flip traverseWithKey keys $ \key ioref -&gt; + writeIORef ioref (Map.lookup key qResults) Same API and the same results: *Main&gt; example Looking up [1,2] Just 3 Have I got something wrong? Here's my entire code: http://lpaste.net/4589675659655118848
One of the things that's always impressed me about the Haskell community is how much more broken-down packages are, and how well cabal manages to handle this situation.
Unfortunately your positive experience with cabal is not universal. For me, cabal has been doing a great job since I started sandboxing and freezing (although upgrading from a freeze can easily be too complicated for a less experienced user), and I wonder how many of these efforts to make installation easier will be needed once those features are the default usage.
I tried both `-fwarn-incomplete-uni-patterns` and `-Wall` following the advices in several comments, but the compiler was still silent! [UPDATE] Ahh, I didn't know that the GHC compiler somehow has a `make` feature, and checks if recompilation is needed using the source file's timestamp (interesting!). I made a small change to the file, and now I see many warnings, going to check them :) [UPDATE2] Well.. `-fwarn-incomplete-uni-patterns` gave a warning even for `Rect xx yy = Rect 11 22`, saying: `Patterns not matched: Circle _`. That's insightful in showing how the compiler thinks, but probably it would not be easy to catch issues with it, assuming that most lines generating this warning would be harmless (as the Rect one 2 lines up is).
Works fine here with `-fwarn-incomplete-uni-patterns`. % cat test7.hs data Shape = Rect Int Int | Circle Int Rect x y = Rect 33 22 -- pattern matches, OK Circle z = Rect 11 24 -- pattern mismatch, obviously main = do putStrLn ("the value of x is: " ++ show x ++ ", the value of y is: " ++ show y) putStrLn ("the value of z is: " ++ show z) % ghc -fwarn-incomplete-uni-patterns test7.hs [1 of 1] Compiling Main ( test7.hs, test7.o ) test7.hs:3:1: Warning: Pattern match(es) are non-exhaustive In a pattern binding: Patterns not matched: Circle _ test7.hs:4:1: Warning: Pattern match(es) are non-exhaustive In a pattern binding: Patterns not matched: Rect _ _ Linking test7 ... 
Ah, of course! I forgot that `@?` had access to the whole `IORef` :) Indeed, that simplifies things further. Thanks! However, I think using an `MVar` is nicer as you don't have to put `undefined` in there - it encodes the idea of being empty. Sorry for taking a while to get there, it's funny when I don't even understand my own code ;)
That's funny because I just went the opposite direction: I've splited my game (Nomyx) into 4 packages: the web GUI, the back-end, the DSL and one more to link them all. I thought that would ease the additions of extra GUIs, for example. The DSL is also pretty much independant. That also exposes the high-level relations IMO. I also use 4 GIT repos, although commiting horizontal modifications is now a bit more tedious. Was it a good idea? :)
Meanwhile in [other places](https://ghc.haskell.org/trac/ghc/wiki/SplitBase)...
I disagree to some extent; having some OOP background, it's hard to be convinced that `Rect x y = Circle 10` passing through the compiler without a complaint is an acceptable compromize for the sake of being a practical language. This is especially noting that such assignment is not allowed in Java for example, not without an explicit type cast. &gt; Don't be disappointed so soon: over time, you'll learn to structure your programs in such a way that most mistakes will be type mistakes instead of value mistakes. Thanks for the advice :) I'm really enjoying learning and practicing Haskell, and totally agree with this part.
Do you have proof this generates fewer DB hits, IE, avoids the N+1 problem? I don't see anything in your example that proves this works. I mean, the "Key collection phase" is what causes the +1 in the the N+1 queries. Unless you do a server side join, or knows the keys to begin with, you can't avoid it.
I support both the split base effort and the efforts to consolidate packages that are needlessly fine grained. Split base is in the wings to legitimately support a real need, so we can work in places that aren't posix-like. Consolidation is necessaries when imagined distinctions turn out not to matter after all. Without both forms of package evolution available you wind up with everything ratcheted to one extreme. Neither is healthy in the limit.
You still haven't killed the +1 of the N+1 then, because you still grab the keys, which is the source of the +1 to begin with. So this is all very pointless. users &lt;- getAllUsers ( +1 ) runQuerying $ &lt;- N
&gt; using an MVar is nicer as you don't have to put undefined in there - it encodes the idea of being empty If you use an `MVar` instead of an `IORef` I think you're swapping a crash on an `undefined` for a deadlock if you hit an empty `MVar`. &gt; it's funny when I don't even understand my own code We've all been there! http://www.memecreator.org/static/images/memes/2544249.jpg
In the example I ask for two queries and perform only a single lookup, these are the *n* queries I am collapsing. As a further example to illustrate my solution, you may have let userIds = [ 1..100 ] users &lt;- runQuerying $ withQuery getUsersById $ \usersById -&gt; traverse (usersById @?) userIds Now this expands out to 100 "lookups", but the way things are constructed only calls `getUsersById` once. Doing this in just the IO applicative would incur 100 database lookups, and would require the programmer to explicitly collect everything up by hand. This isn't hard to do in the above example, but it can be a lot harder when you have much bigger structures that you want to enrich.
Yes, thanks. I updated my comment :)
Thats not really shown... Your toy example should hit a real db. I suspect this is not working as your think it does. I don't think your toy example even expresses the N+1 problem, as the getUserAgesById is already a simple SQL query, "SELECT ID, AGE FROM USERS;" Any ORM would get this right. "select user.age, user.id from user" would not trigger any joins in JPQL as they hit the same table. So N+1 doesn't apply. The N+1 problem occurs when you select from one table, and the data you need is in another table, and you do a synthetic server-side join (most commonly done by ORMs behind the scenes) or manually in say PHP.
The `Rect` one is "harmless" in the sense that it won't crash, but you should never write code like that! Just write `(x, y) = (33, 22)`.
Again, I don't think this solves this. You're not showing any generated sql or joins, so I don't even think it applies. If you already know the userIds, then N+1 doesn't apply anyways let userIds = queryForUserIds() &lt;=== +1, run query to grab ids users &lt;- runQuerying $ withQuery getUsersById $ \usersById -&gt; traverse (usersById @?) userIds getUsersById I assume does the actual lookup for each id, so it would be run N times, where N is the number of userIds. In a conventional N+1 nested select, you don't run the usersIds query N times, you run it ONCE, then select each user. To run it 'once', you would need something like. Of course, there is a limit to how large a in clause you can make. "SELECT * FROM USER WHERE ID IN (.... userIds..... )" There is nothing your example showing the ACTUAL SQL that would be generated, or what the DB actually gets.
&gt; Deconstructing on patterns that could fail is a (n unfortunate) possibility in Haskell. I'm not so familiar with the terminology (*deconstructing*), but I think I agree :) [UPDATE] Well, *deconstruction* is obviously the act of decomposing a construct based on a pattern.. sorry, it's 1:45 AM :) &gt; However, Haskell makes this among many other sacrifices in the name of pragmatism. In Haskell, **strong typing** combined with **type inference** is a very highly promoted package in the most popular language resources I found on the web... for me, this does not go very well with the words *sacrifice* and *pragmatism* :) I did not get your last example about recursion, probably since I'm a new comer to Haskell.
I don't think it is just minor. I think the lack of clear documentation (if we say the API is part of the documentation) is almost the entire reason the library isn't used much. I spent a little bit of time trying to figure out how to use it but decided that if my goal was to spend a few hours writing a parser and share that code with my team I needed to just use parsec.
I can't understand what you're saying, but ocharles' code doesn't just apply to SQL or databases. It applies to any situation where you first retrieve keys and then retrieve data relating to those keys. You can retrive the key, and then do N more retrievals, one for each piece of data (N+1). ocharles also knows you can do it the "Oof. That got a lot more complicated!" way, where you retrieve the key and then retrieve *all* the data at once (1+1). He's come up with a DSL that wraps the complicated 1+1 way in a nicer syntax.
This is not part of real code I wrote, and I did not mean to promote such style in coding anyway. It's only an experiment to find out how strong is Haskell's compiler error detection.
Then this doesn't appear to be a valid objection :) &gt; it would not be easy to catch issues with it, assuming that most lines generating this warning would be harmless 
Interesting! It compiles successfully and it's harmless. My opinion is that such statement should not pass by the compiler.
I disagree. Building the SQL which looks up data for all the keys in a single query is not complicated. It's just runner :: [KeyType] -&gt; IO (Map.Map KeyType ValueType) runner ks = runThisSQL "SELECT value FROM table WHERE key IN ( ... ks ...)" 
What's simply a map over the results of the first step?
&gt; He's still grabbing the keys ( +1 ) then processing each key ( N ). If processing the key hits the db, its still N unless he generates some different SQL. No he's not. Again this is *completely independent of SQL or databases*. The point is that if you define your `[k] -&gt; IO (Map.Map k v)` correctly (which is pretty trivial in most cases) you issue only one request to retrieve all the data associated to all the keys. 
getAllUsers is one query (+1) returning N users. So WithQuery runs each subsequent command N times ( once for each user). If those commands are queries, then that is still N+1. So still N+1 database hits. You will only save on this if you have a way to take these functions, and turn them into SQL performing joins or IN selects.
Can you show me where "WithQuery runs each subsequent command N times"? What I can see is qResults &lt;- runner (Map.keys keys) i.e. `runner`, the "subsequent command", is run exactly once.
I think you've misunderstood what the example is. It's a DSL to avoid "Oof. That got a lot more complicated!" not to write SQL queries. ocharles takes the implementation I wrote above as a given.
excuse my pseudo code userIds = fetchUserIds() users = userIds map id -&gt; getUserForId(id) 
&gt; The point is that if you define your [k] -&gt; IO (Map.Map k v) correctly Well thats a given. thats where you need to have it generate sane SQL that uses a join or in clause, or the equivalent constructs to avoid N queries. But thats independent of the toy example. Its a cute toy framework, but the real meat is that function, which you must get right to see any benefit. Thats the HARD PART. Not the wrapping bits, which there are many ways to write.
&gt; Its a cute toy framework, but the real meat is that function, which you must get right to see any benefit. I agree you have to get that function right. I don't agree it's the "real meat". It's pretty darn trivial to write an in-clause.
I'm afraid I can't understand your pseudocode.
The key point is whether "grab user for each" requires one request total (1+1) or one request per userId (N+1). ocharles has come up with a nice DSL for doing the 1+1 case.
Only if you write the [k] -&gt; IO (Map.Map k v) correctly. But the SQL generation (or generating it for whatever query system you use) is the hard part. 
It's indeed very uncommon to write partial pattern matches in Haskell code. I consider it very bad style.
Agreed.
&gt; having some OOP background, it's hard to be convinced that `Rect x y = Circle 10` passing through the compiler without a complaint is an acceptable compromize I will try anyway :) The definition `Rect x y = Circle 10` might look like the statement `Rect r = new Circle(10)` in an OOP language, but the two are completely different. The main difference is that subclasses are open, while algebraic datatypes are closed. That is, when you write subclasses: class Shape {} class Rect extends Shape {} class Circle extends Shape {} You know that more `Shape` subclasses could be defined later. So when you write code which works with a `Shape`, you don't have a case for `Rect` and a case for `Circle`; instead, you write abstract code which works for all shapes, delegating the specifics to overloaded methods. It is possible to use something like methods in Haskell, but you chose instead to write an algebraic datatype: data Shape = Rect Int Int | Circle Int With an algebraic datatype, you are sure that `Rect` and `Circle` are the only possible forms of `Shape` that can exist. So when you write code which works with a `Shape`, you typically have a case for `Rect` and a case for `Circle`. But that's not what you did! You used a destructuring definition, which is not typical at all. &gt; such assignment is not allowed in Java for example, not without an explicit type cast. `Rect x y = ...` might look innocuous to you, because you're not used to Haskell yet, but that's actually what our cast statements look like. You are telling the compiler "trust me, I know what I'm doing, I know that the value on the right will end up being a `Rect`". We rarely use this, unless a type has only one constructor.
&gt; In Haskell, strong typing combined with type inference is a very highly promoted package in the most popular language resources I found on the web... for me, this does not go very well with the words sacrifice and pragmatism Huh? Language design is the same as any other kind of design. There are major trade-offs to every decision. Haskell allows you to cheat, just as any other language. But Haskell is arguably more refined about what it considers cheating. &gt; I did not get your last example about recursion, probably since I'm a new comer to Haskell. Primitive (or structural) recursion is recursion where you get a guarantee about termination. (And respectively, structural corecursion guarantees productivity). There are strict requirements about what arguments you may call yourself recursively with. General recursion is unrestricted recursion, where you may call yourself with any argument. However, nothing stops you from writing an infinite loop (a classic Haskell example is `fix id` which evaluates to `id (fix id)` which evaluates to `fix id` and starts over again). These are analogous to the difference between a `foreach` loop and a `while` loop in an imperative language. You generally don't worry about `foreach` loops, as they iterate over a (hopefully finite) collection, and you're done. With `while` loops, you live your life in constant fear that you didn't create an execution path where the condition never fails. 
TBH, I'm not entirely convinced that the "genuine sieve" is genuine either - though I have doubts about whether I'm right too that I'll get to at the end. From the paper, one step in the genuine sieve algorithm is "cross off all the multiples of that number in the table". That doesn't just specify how much work is done - it specifies the order. All multiples of two are crossed off *before* looking at three. Another quote - "Can the genuine Sieve of Eratosthenes also be implemented efficiently and elegantly in a purely functional language and produce an infinite list? Yes!" - (when cut out of the context, of course) seems to claim that it's possible to mark off an infinite number of twos in an infinite sequence of numbers, yet still find primes 3 and upwards. Obviously that's not what O'Neill implements. O'Neills approach changes the evaluation order. Instead of marking off multiples of each prime as soon as it's found, it queues each prime when it's found to be compared against larger candidates later. Though she explicitly says that later - "Whereas the original algorithm crosses off all multiples of a prime at once, we perform these ‚Äúcrossings off‚Äù in a lazier way: crossing off just-in-time." - it's even true in O'Neills first trial division implementation... primes = 2 : [x | x &lt;‚àí [3..], isprime x] isprime x = all (\p ‚àí&gt; x ‚Äòmod‚Äò p &gt; 0) (factorsToTry x) where factorsToTry x = takeWhile (\p ‚àí&gt; p*p &lt;= x) primes The queue here is `primes` itself - its data structure is made (in part) of lazy evaluation thunks. What's the evaluation order? To try (and fail) to be awkward, let's not even demand the first prime first - what happens if you request e.g. the 10th prime (29). You can't know whether 29 is the right candidate for the 10th slot (let alone check whether it's prime) until you have proven that 28, 27, 26 etc aren't prime and therefore aren't in that 10th slot. That applies to any position in the list you look at, so filling the slots in the list must progress in the obvious order. So it turns out that the evaluation order is fairly simple and predictable here - laziness does just enough to make the code work - pausing when it's done enough, but preserving the relevant order. Primes are filled in in increasing order irrespective of demand order - though only up to the point you demand. I haven't considered evaluation order for the later versions in detail, but I believe a similar argument holds - the data structure arrangements depend on the exact sequence of primes found so far so the evaluation order really is deferring things via the queue, and not somehow re-ordering that to match the sieve for the part demanded so far - and of course O'Neill never claimed it would. It's only the evaluation order that's wrong - the performance is correct. Even so - one last quote from the paper - "The algorithmic details, such as how you remove all the multiples of 17, matter.". For a while I thought maybe it's possible to implement a genuinely genuine sieve in pure functional code, but it would need a finite upper bound and a constant-time-access data structure whose arrangement can be determined purely based on presence/absence of values - not caring about things like their positions in the sequence. Given the finite upper bound, I thought a trie might work. Rather than having a set of everything then removing the multiples of primes, I suspect keeping the set of composites will work better - rationalize that as the set of check-off marks. The problem is that the sequence in which elements are added to the trie still determines the structure of the trie at any point in time - whether certain nodes are present or not. Appealing to laziness to solve that would, of course, be cheating - it would mean the check marks get queued in the thunks again. *Unless* you count the thunks as part of the set-of-check-marks data structure. In that case, the checks are there - it's just that we'll rearrange things a bit when we first query each one. If you can ensure there's at most a constant amount of work for each check mark, *maybe* both the order of evaluation and the complexity are correct. Though I still have doubts. Anyway, I can hardly count the thunks as part of my set of checkmarks without allowing O'Neill to do the same. As I said, I haven't tried to analyze her algorithms in detail. TBH, unless I can see a shortcut like the one above, algorithm analysis isn't a strong point of mine. 
This is true haskell seems like a human class system
I'm not sure what has prevented it. I've heard people have attempted it in the past but failed due to it being "too hard".
A lot of people have expressed interest in type classifying Prelude. I reposted this because I think it's a cunning alternative; it allows different modules i.e. Data.Map, Data.Set, Data.List to all have similar (but not type class restricted) interfaces, without needing to prefix every usage. You could still revert to the old behaviour using the `qualified` keyword i.e. let x :: Map ... let y :: Set ... import Data.Map import Data.Set import Data.Set as S insert x -- Data.Map.insert insert y -- Data.Set.insert S.insert x -- Type error, compiler is explicitly told -- Qualified import behaviour import qualified Data.Map as A import qualified Data.Set as A A.insert x -- Data.Map.insert A.insert y -- Data.Map.insert
My first look is that your code is suferring from boxing and thunking, try adding some strictness annotations for some arguments and your data types. Edit: my second look is that you are running code on lists on haskell and in arrays on js, look for data.vector and move from list to vectors. edit2: my god that one line js is hard to parse. Your haskell code has a many list ++, and afair they are known to be slow. This gets aliviated with vectors, but I don't seem to see any of that on the js. Maybe you could get rid of it altogether. Anyway I forgot the most important thing: compile your code with -prof -auto-all and run your exe with -hc +rts -p, the memory profile and the time profile will give the right direction to optimize your program.
I am a Haskell newbie, this is my second program ever, so I honestly don't know how to do it. I was expecting it to at least be faster than my toy compiler, though. I didn't have to profile my JS program at all, so that doesn't make sense.
I'm a Haskell newb myself, so I can't offer huge insights, but I just downloaded your code and got a 3x speedup by changing the two foldl calls to foldl', which is strict (the accumlator is evaluated immediately): http://lpaste.net/101756 Original version: jamess-mbp:tmp james$ time ./meshbuilder &gt; /dev/null real 0m0.357s user 0m0.311s sys 0m0.044s Revised using foldl' jamess-mbp:tmp james$ time ./meshbuilder2 &gt; /dev/null real 0m0.105s user 0m0.099s sys 0m0.005s
From a quick look-through, I see that you're using `foldl` on lists. You should instead use `foldl'`, which is the strict version of `foldl`. `foldl` for lists basically has no practical reason to exist and is, unfortunately, a huge gotcha to programmers coming from strict functional languages. Beyond switching to `foldl'`, the mere desire to do a left-fold suggests that you shouldn't be using Haskell lists in the first place.
&gt; What has prevented something like TDNR from being implemented? I've looked into it a bit, and the problem mostly stems from additional coupling between typechecking and symbol resolution (which assigns types to symbols). My proposal was both simpler and crazier than TDNR: for any ambiguous symbol, typecheck the program with all versions of that symbol. If exactly one of the results typechecks successfully, the program is correct. If more than one passes, the program is ambiguous and will fail to compile. If none pass, it's a type error. A naive implementation leads to exponential cost to compile, however, since a program with N instances of some ambiguous symbol `x` with two implementations could require up to 2^N passes through the typechecker. SPJ challenged me to write a type system for this design which could possible improve this problem, but I ran out of time before I came up with an answer and had to go back to my day job writing games.
Replacing foldl with foldr in build_layer sped up the runtime more than 10x. Avoid using foldl for computing data which will lie unused most of the time, that creates lots of delayed computations. To profile, do this: ghc -O2 -prof -auto-all meshbuilder.hs &amp;&amp; time ./meshbuilder +RTS -p -hc -RTS then check out meshbuilder.prof file.
But the algorithm doesn't work with `foldr`, it is a left fold (it builds a 3D path as it goes from root through the nodes)...
Once you properly understand what those statements mean, your opinion might change. The reason the behavior of these statements is unfamiliar to you is because languages like Java have no equivalent to them, on several levels: they don't have sum types, and partly because of that they don't have pattern matching on types. So attempting to compare these statements directly to Java results in confusion. I'd suggest reading a bit on sum types: [this post](https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/sum-types) might be a good starting point. Note the first example of a sum type in that post is Haskell's Bool, defined as ```data Bool = False | True```. As that post points out, there's no way to define such a type directly in Java, which is why the Java language has the built-in type ```boolean```, which cannot be defined in an ordinary Java library. Of course, you can partially fake the type, as Java's Boolean class does, but that relies on some shenanigans in order to successfully simulate a type that has only two values, and it still fails in various ways, e.g. Boolean instances can't properly be compared using ```==```. Now, if you consider the ```boolean``` values ```true``` and ```false``` in Java as two values of a built-in sum type, you'll notice that there's no scenario in which Java can, at compile time, catch an error where a ```true``` value is expected but a ```false``` value is received. That's because ```true``` and ```false``` are two different values, not two different types. Exactly the same thing is true of the Rect and Circle cases of your Shape type. Rect is a constructor for values of the Shape type, and so is Circle. The pattern matching we've been discussing is fundamentally a runtime operation relating to values, not a compile time operation relating to types. As such, Java and Haskell are identical in that neither attempt to inspect values at compile time in order to determine whether a variable has one value or another. The type checker is only interested in types, not values. Your headline question, "Is Haskell type system as strong as claimed?", does not actually apply to this issue - it's not a type system issue, it relates to the behavior of runtime pattern matching. The closest thing to a pattern match in Java would be a case statement. In Haskell, we might write an area function for shapes as follows: area :: Shape -&gt; Double area (Rect x y) = fromIntegral (x * y) area (Circle z) = pi * fromIntegral z This can be thought of as syntactic sugar for the following: area :: Shape -&gt; Double area shape = case shape of Rect x y -&gt; fromIntegral (x * y) Circle z -&gt; pi * fromIntegral z Now, in code like that, the value of the ```shape``` variable is irrelevant to the type checker. The only relevant point is that it is of type Shape. Let's use this example to express your failing example, ```Circle z = Rect 11 24```, in a way similar to the above: areaOfRect1124 :: Double area = case Rect 11 24 of Circle z -&gt; pi * fromIntegral z Because of the structure of this code, this will throw a warning with the -W, -Wall, or -fwarn-incomplete-patterns options. But the warning tells us not that the patterns don't match, but rather that the pattern match is incomplete - it's missing a possible case. The type checker doesn't know or care what the *value* of the Shape is, it only knows that you're doing a pattern match on a Shape, and not considering all the possible patterns. If you add the Rect case back, the above code is fine: areaOfRect1124 :: Double area = case Rect 11 24 of Circle z -&gt; pi * fromIntegral z Rect x y -&gt; fromIntegral (x * y) The line containing the Circle will never match in this code, and the compiler will in fact notice this and optimize it away, but it's never going to complain that the line shouldn't be there. It's valid code. The only problematic aspect was the missing Rect pattern, and the incomplete pattern match warning will tell you about that. Similarly, ```Circle z = Rect 11 24``` is perfectly valid code. It looks funny if you're not familiar with what it does, but it really corresponds to a variable being bound by a case expression, something like this: z = case Rect 11 24 of Circle z' -&gt; z' Written like this, you can see there's nothing wrong with this code, except that it's missing one of the Shape patterns. Add in a pattern for Rect and the code is fine. Something similar applies to ```Circle z = Rect 11 24```, and to ```True = False```. They're pattern matches that don't match. In the latter case, since no value is needed to assign to a variable like z, there's no problem, and no error occurs. ```True = False``` can be thought of as equivalent to something like: _ = case False of True -&gt; undefined It's basically a no-op. 
It's times like this that I wonder why foldl even exists, or why it is so easily accessible for new people to unknowingly use. 
Wouldn't you just be able to reuse the hole technology? Make them function holes, infer the type and then see after 1 pass if you have any functions with that name which match the type inferred.
I did not get into details, but from the first glance you could just reverse the order of operations, and go that way instead.
Your use of `V.++` is almost certainly the reason for this. Vectors are not intended to be appended efficiently! With the lists, `(++)` (conceptually) traverses the first list, then bumps a pointer somewhere. With vectors of length `m` and `n`, a new vector of length `m+n` is allocated, and the `m+n` elements are copied into this new vector. Can you find a way to avoid the unnecessary allocations by computing the size of the vector before filling it with its elements?
I know the size in advance, but I don't know how to fix that in code... I'd need a way to fastly "memcpy" a vector into another. How?
I see, but the algorithm is a left fold, not a right fold... it builds a 3D path starting from the root node. How'd that work with foldr instead!?
Is [Data.Vector.slice](http://hackage.haskell.org/package/vector-0.10.9.1/docs/Data-Vector.html#g:6) what you want?
your code shows 25% of time in show method, 25% in indexes, 25% in position, 25% in colors. The append complexity is killing performance here. Do you have a way to build those from right to left?
Yes \: it creates a 3D path on space. It must start on the root and grow step by step.
I know this is mostly a question about the native haskell performance, but for the record ghcjs is a little faster than fay... $ fay -O meshbuilder.hs -o fay.js $ time node fay.js &gt; /dev/null real 0m5.429s user 0m5.222s sys 0m0.235s $ ghcjs meshbuilder.hs generating native generating JavaScript Linking meshbuilder.jsexe (Main) $ time node meshbuilder.jsexe/all.js &gt;/dev/null real 0m2.888s user 0m2.733s sys 0m0.163s 
It doesn't look like a left fold to me; it looks (almost) monoidal. The calculations of the pivot points may be a left fold, but those aren't what are killing you. I'd use `mapAccumL` to build the pivot points and the (small) mesh for each point, then `foldr` to glue the meshes together. If you throw away the state result (the pivot) at the end of `mapAccumL`, and consume the resulting list from left to right lazily (like `foldr` will), your state data calculation will get interleaved nicely with the mesh generation and gluing. Alternatively, a more minor change would be to use [difference lists](http://hackage.haskell.org/package/dlist-0.5/docs/Data-DList.html) instead of lists, then convert the final result to a `Mesh` when you're done. This will likely have about the same performance, although you'll want to use `foldl'` instead of `foldl` to avoid a horrible space leak while constructing the data, and you may need to pay attention to strictness inside of your accumulation functions as well to maximize the value of `foldl'`.
No, I'm highly interested in Haskell-&gt;JS. This is very important to me, as I'm trying to get away from JS. If I manage to get it as fast as my JS version I'll be really, really happy. I guess that is not happening any soon, though. Good to know GHCJS is faster than both Haste and Fay - I couldn't install it to test.
If you're interested in Haskell-&gt;JS, while it may not be properly Haskell, you should test PureScript too.
I can't see it, are you sure you edited? Also, pardon, you said the ++ operation is fast for immutable vectors, but you want me to use mutable vectors? And how'd that look, exactly? I was thinking something like `memcpy` could solve my problem, considering the bottleneck is probably on `V.fromList` or `V.++`. I'm not sure, but I guess it is making lots of intermediate representations, right? So if I could just go all C and allocate a big vector in advance, and then just fill it, that'd be fast, maybe?
`foldl'`?
Nope, I guess it is IP-based? Is the link you see still 101742? You could paste a new one: http://lpaste.net/ I get it now. Let me ask a question then, what is V++ doing? Does it make a complete new copy of the arrays? If so, that'd be my bottleneck right there. I'm now reading/learning on how to use mutable arrays.
Sorry, I do not understand that output? That was supposed to generate a 3 big array of numbers (as this is what the Shader expects)...
Also: $ time -l node 101743.js &gt; /dev/null 0.12 real 0.07 user 0.04 sys 17002496 maximum resident set size 0 average shared memory size 0 average unshared data size 0 average unshared stack size 12540 page reclaims 0 page faults 0 swaps 0 block input operations 0 block output operations 0 messages sent 0 messages received 14 signals received 31 voluntary context switches 47 involuntary context switches $ time -l ./vec &gt; /dev/null 0.07 real 0.07 user 0.00 sys 4890624 maximum resident set size 0 average shared memory size 0 average unshared data size 0 average unshared stack size 1214 page reclaims 0 page faults 0 swaps 0 block input operations 0 block output operations 0 messages sent 0 messages received 7 signals received 0 voluntary context switches 2 involuntary context switches
Data.Sequence is quite a bit faster than lists for concatenation. I don't think this is the most optimal approach but it is worth considering. This also includes the foldl' change and some unboxing. https://gist.github.com/NathanHowell/f5f0e22f883f94846154/revisions $ time ./bar-baseline &gt; /dev/null real 0m0.424s user 0m0.366s sys 0m0.044s $ time ./bar2 &gt; /dev/null real 0m0.038s user 0m0.026s sys 0m0.004s 
Isabelle does this. It's awful.
Oh well, that was very witchcrafty for me. 0.034ms, really fast. I don't understand what you did there, but I'll be studying it. Very cool, thank you! On a note, now I'm interested in seeing how fast it can become. I'm studying mutable vectors to see if it can help.
Yes, the syntax is not exactly Haskell, so it probably would not compile on a first attempt. If you're interested though, #purescript on Freenode is generally a good place to get help.
Yes, V.++ is making a fresh new array of target length and copying old arrays over. So it's expensive. I could imagine an improved immutable vector implementation that grows the left vector's length smarter (e.g., doubling) and hence avoid expensive copying. But currently Data.Vector isn't doing this, at least in version 0.9.1 as I last checked.
I see, interesting, so that's it, I'll be trying to write the mutable vector implementation.
What would be the harm in replacing foldl with foldl'?
I took the liberty of porting the code to PureScript for you. https://gist.github.com/paf31/64cdb688355b0ff1184e Here's what I get, for reference: real 0m0.423s user 0m0.321s sys 0m0.103s compared to JS real 0m0.073s user 0m0.053s sys 0m0.016s There is still quite a lot of optimization that can be done on the PureScript methods being used, especially methods on arrays such as `replicate`. Edit: If I provide non-recursive versions of `map` and `zipWith`, it reduces to real 0m0.105s user 0m0.084s sys 0m0.018s so thank you for providing a very useful performance test case :)
Yes it does. You are doing something similar to `foldl (\x y -&gt; x ++ [y])`. What you need to do is something like `foldr (:)`. They are the same order, but consing is way faster than appending. The difference between foldl and foldr is not what direction they take, it's how they associate. `foldl (+) 0 [1,2,3] = ((0 + 1) + 2) + 3` and `foldr (+) 0 [1,2,3] = 0 + (1 + (2 + 3)))`. Notice how the order is the same.
Can you explain what you mean? It seems like it accepts strictly more programs than the existing standard, and allows the compiler to use type information to help you instead of vice versa.
Because irrefutable patterns are a useful construct. You want the pattern-match to be done lazily if/when the value is used. Thus, you cannot really do exhaustiveness checking at pattern-match time. Also, if you use an ordinary pattern-match, intending to catch the single constructor there is, but a new constructor is added, an ordinary pattern match would become a bug. That's why its unacceptable there. If you use an irrefutable pattern match, then you've made it clear that your intent is to assert that the constructor *must* be that one, and more constructors may exist.
When I was first getting interested in Haskell, I read a great blog post from someone who works in the financial services industry, detailing a method for calculating [monetary value], using a complex formula that had to query multiple values from the database. They showed a method of using a typeclass, and 2 instances to first evaluate the formula, pulling out all the data dependencies, then fetching those dependencies from the network, and finally evaluating the formula given the data provided. I wish I could find that article again, because it was a great read, but I've come up blank. The thing that I took away from it was that the whole process (minus the database fetch) was done entirely purely, and it seemed to work very well.
&gt; foldl for lists basically has no practical reason to exist and is, unfortunately, a huge gotcha to programmers coming from strict functional languages. Why is that?
I don't have time to try all the examples in this thread with ghcjs, but here is what I get for this one (currently the top comment)... $ ghcjs meshaccum.hs generating native generating JavaScript Linking meshaccum.jsexe (Main) $ time node meshaccum.jsexe/all.js &gt; /dev/null real 0m1.062s user 0m1.012s sys 0m0.050s For comparison, here are the results I got with the original version again... $ fay -O meshbuilder.hs -o fay.js $ time node fay.js &gt; /dev/null real 0m5.429s user 0m5.222s sys 0m0.235s $ ghcjs meshbuilder.hs generating native generating JavaScript Linking meshbuilder.jsexe (Main) $ time node meshbuilder.jsexe/all.js &gt;/dev/null real 0m2.888s user 0m2.733s sys 0m0.163s
Hmm, well I acknowledge that a lazy pattern match is useful sugar when there's only one constructor. However, if there's more than one constructor I see a lazy pattern match as a non-exhaustive set of patterns. If I *know* that a value has to be of a particular pattern then I always encode this in the type system, except in the rare cases where this makes the code a lot more messy. 
This looks very similar indeed to what we're doing in the Haxl project at Facebook. Ollie - have you taken a look at that? What are the differences?
FPComplete Hoogle searches all the packages provided in the FPComplete online IDE, while haskell.org Hoogle only searches some standard packages.
haskell.org's Hoogle searches a smaller set of packages by default. You can add additional packages to search using +packagename: http://www.haskell.org/hoogle/?hoogle=parseJSON+%2Baeson I don't know if there's a flag to make it search *all* Hackage packages, like the FPComplete one does. I think that's the sensible default, actually.
Very little. IIRC we discussed this recently here.
We're programming in Haskell, not Agda. So there are still cases where we can prove something but not encode it in the types. In these cases, you might want to have an explicitly-partial pattern-match. You also might want that match to happen lazily. In that case, what alternative do you have for `~`?
I don't think it is out of data but it is limited to base and a few other packages, it does not index all of Hackage.
No one seems to have mentioned this so far: let's have some bang patterns on those datatypes please! I got about a 5% speed up by adding these bang patterns to roconnor's version http://lpaste.net/101773 data Qt = Qt { qx :: !Double, qy :: !Double, qz :: !Double, qw :: !Double } deriving (Show) data Vec = Vec { vx :: !Double, vy :: !Double, vz :: !Double } deriving (Show) data Col = Col { r :: !Double, g :: !Double, b :: !Double } deriving (Show) data Pivot = Pivot { pivot_len :: !Double, pivot_rot :: !Qt, pivot_rads :: [Double], pivot_cols:: [Col] } deriving (Show) data MeshState = MeshState { meshstate_rot :: !Qt, meshstate_cpos :: !Vec } deriving (Show) data Mesh = Mesh { mesh_rot :: !Qt, mesh_cpos :: !Vec, mesh_positions :: [Double], mesh_colors :: [Double], mesh_indexes :: [Int] } deriving (Show) 
&gt; So WithQuery runs each subsequent command N times ( once for each user). If those commands are queries, then that is still N+1. No, it doesn't - that's the whole point of this work. You write it *as if* you were going to do *n* queries, but the applicative re-organises things to perform only a single query.
Right, I agree there are cases where we can't encode what we are sure of in types and there are also cases where we *could* encode it in types, but it gets too messy. Just to be pedantic, you *can* always avoid `~` because it's just sugar for a let. In fact it may sometimes be clearer not to use `~` and use a function instead, although at this point I'm mostly just splitting hairs. f ~(A x y) = ... x ... y ... could be f a = ... unsafeFst a ... unsafeSnd a ... unsafeFst (A x y) = x unsafeSnd (A x y) = y 
I have a feeling that this version of `Query` looks a bit odd because it's conflating two separate processes: building the query, and executing it. I've created a version that separates these two ideas, to show that the type can actually be as simple as: data Lookup k v a = Lookup (Set k) (Map k v -&gt; a) instance Functor (Lookup k v) where fmap f (Lookup k h) = Lookup k (f . h) newtype Query k v a = Query (Free (Lookup k v) a) deriving Functor Here the `Applicative` instance builds up a `Lookup` value which references all of the keys that will be needed by any inner references. It leaves the actual execution of the query to code outside of the `Query` entirely. The full example is here: https://github.com/jwiegley/notes/blob/master/Query.hs
it's trivial with mysql-simple's and postgresql-simple's `In` type. it's not as simple when you don't have nice query substitution support in your db wrapper. maybe crusoe is not familiar with good db wrappers.
I got a 3x speed-up by using appropriately unboxed types and changing `foldl` to `foldl'`: http://lpaste.net/101778 The rest will need to come from thinking about the algorithm and whether using a linked-list makes sense. Edit: Combine with roconnor's `mapAccumL` changes my version is now ~10x faster. Curiously enough using `mapAccumL` loses some of the unboxing benefits. I haven't investigated why.
Love the fact that the video is tagged ‚Äújquery‚Äù.
That's what I thought. (Fun fact: except for `--:`. :p)
I will eagerly await that paper then. Because yes, I'm probably re-inventing Haxl... though as you say, there's nothing for me to really go on at the moment except a few slides :)
I see what you mean! I'll think on it some more, but you are right, the types get compounded in a way that's hard to work with.
It also makes understanding what a program does entirely context-dependent.
If you look at foldl: foldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a foldl f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = lgo (f z x) xs You will see that it will *always* process the whole list (since it always calls itself recursively). Since it is guaranteed to process whole list, there's no reason to create thunks and then forcing them. If you look at foldr: foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b foldr _ z [] = z foldr f z (x:xs) = f x (foldr f z xs) You see that it may stop at any stage, because f may not use it's second argument, so there will be no need to compute "foldr f z xs". I think this is the base reason why foldl doesn't make a lot of sense to exist (please someone correct me if I'm wrong).
I don't think ordinary let should do lazy pattern-matching. I believe ~ should do that exclusively.
That's interesting. What criteria would you use to determine what patterns should be allowed in a let? Would you allow let (x, y) = (a, b) Would you also allow data P = P Int Int let (P x y) = P a b ? There shouldn't be any reason to distinguish baked-in tuples from user-defined tuples. And would you forbid data A = A Int | B Int data A x = A a ? 
If you actually want many ++ you probably wqnt data.sequence or dlist
Also, it seems the link from http://elm-lang.org/learn/Interactive-UI-Elements.elm to the Elm Public Library is broken. If only Elm had some way of statically ensuring correct links ;)
`foldl` is the prototypical example of the tail-recursion gotcha in Haskell. Instead of eagerly applying the accumulator function every iteration, it builds up a chain of thunks naively hoping that they might not need to be executed. But, they always have to be executed anyways, so it's a total waste. `foldl'` forces the accumulator function every iteration, and thus behaves more like what an ML programmer would expect.
In the 90s I guess.
If you have a datatype without bang patterns (as is standard) data A = A Int Int then the `Int` fields are not actually `Int`s (necessarily) but potentially unevaluated thunks that evaluate to `Int`s when forced. If you were to do printA (A x _) = print x then `x` would be forced when this IO action is run, but there's possibly still a big thunk living in the second field and using up a lot of space. If instead you use bang patterns data A !Int !Int then every time you pattern match the `A` *both* fields are forced as well. This tends to lead to data which is in fully evaluated form more often which uses less memory and can sometimes help with speed too.
So it's basically a strictness annotation? Neat!
Try Hayoo http://holumbus.fh-wedel.de/hayoo/hayoo.html
I can't think of any reasonable situation (that isn't highly artificial and contrived for the specific purpose) where plain `foldl` is preferable. In some cases (i.e., ones where it gets optimized into something equivalent to `foldl'` because the compiler is clever) it might be as good as the alternative, but that's all. Using `foldl'` or `foldr` as appropriate is a good rule of thumb in any case. EDIT: I may have misread, if you were asking whether to prefer `foldl'` or `foldr` that depends entirely on what you're doing. `foldr` lets you consume the list lazily if the result value is also consumed lazily, while `foldl'` consumes the whole list strictly to avoid nesting thunks. e.g., you'd write `map` with `foldr` and `sum` with `foldl'`. if using the result in any way requires the entire fold, use `foldl'`, if you expect it to work with infinite lists use `foldr`, &amp;c. Compared to `foldr` and `foldl'`, `foldl` essentially has the intersection of their benefits and the union of their drawbacks, and there's not much left (ha, ha) after everything that rules out.
I will be recording it and will post the files and links to the meetup site
I would allow all patterns in let, I'd not make the match lazy, though, but I'd match it strictly like case, and warn/error if the match is non-comprehensive. For lazy match, ~ could be used explicitly.
Things that didn't ought to need saying: &gt; Haskell is way better than PHP as a systems language But an interesting read. Looks like they've been building quite a bit of infrastructure, though I don wonder about building their own build tool. I can easily imagine wanting to ditch cabal, but myself I'd be inclined myself to build something on [shake](http://hackage.haskell.org/package/shake) rather than an in-house Haskell build tool.
A problem with your original implemenation is that lists in Haskell are essentially linked lists, and thus the append operation, e.g positions = (mesh_positions step) ++ [(vx vpos),(vy vpos),(vz vpos)] takes linear time. Since you do this append at every step of your loop, your overall algorithm takes quadratic time. Presumably appending to arrays is faster in your javascript program (constant time?) and your javascript takes linear time. Using `mapAccumL` eliminates the slow append operation and restores the desired linear time behavour.
You can use `foldl` to define `reverse`. That is the only use case I am aware of.
Pretty much never.
Glad someone proposed sequence. Also pleased to see a nice speed up. Sequence needs more haskell love!
Similarly to /u/roconnor, I got a 10x speedup as well. However, I took a different approach. I rewrote it completely, but the overall structure and algorithm is the same. The rewrite was simply for stylistic reasons - you're using a lot more brackets than are really necessary (have a look at the default fixity declarations), and I have a slightly different preferred indentation pattern, but that's pretty inconsequential. I've made heavy use of strictness annotations and `UNPACK` pragmas. I'm using unboxed vectors wherever I can instead of lists, with one exception - the `positions`, `colours`, and `indices` fields of `Mesh`. As others here have noted, `++` on lists is slow, but it it's also slow for `Vectors`. So what I've done is build up a list of Vectors, but cons-ing rather than appending. When it comes time to print the `Mesh`, I reverse the list and `concat` the vectors together. [Here's the code](http://lpaste.net/101785). $ ghc --make -fllvm -O2 mesh0 &amp;&amp; time ./mesh0 real 0m0.359s user 0m0.328s sys 0m0.028s $ ghc --make -fllvm -O mesh2 &amp;&amp; time ./mesh2 real 0m0.034s user 0m0.032s sys 0m0.000s Another benefit of my rewrite: 143 MB total memory in use vs 2 MB total memory in use **However** something that's unnerving me slightly is that my program refuses to compile under GHC 7.4.2 when using `-O2`. It just hangs for as long as I'll let it. However, when I turn on profiling (using `-prof -auto-all` it will compile, taking approximately the same time as `-O` with profiling. Can anyone using a more recent version of GHC confirm that this has is still the case/no longer the case?
When I was out there in October they were talking about using it for some web services as well.
Using immutable vectors, I got the exact same speedup as /u/roconnor (as exact as I can tell using `time`). My code is [here](http://www.reddit.com/r/haskell/comments/21doog/why_this_haskell_program_runs_considerably_slower/cgcdmx8).
I suspect the comment was sarcastic.
I also could not get it to load, but found a link to DL the slides without signing in: http://codemesh.io/slides/tim-williams.pdf [pdf]
http://lpaste.net/101790 the print statement in your code shows basically half the resources, so plenty of optimization left! 
At the very least put a warning on it.
This is a very good resource: http://learnyouahaskell.com/chapters
What would be the type of `(.reset)`? Seems like it would need to accept either a `Button` or a `Canvas`.
I, for one, hope this proposal never gets implemented. It is the easiest thing you come up with without thinking much about the problem. Which also means it's the worst. It's just some ad-hoc crap stuffed into the compiler that will cause enough people to stop complaining that a good solution will never be thought of/implemented. For instance, last summer of code there was work on an overloaded record fields proposal that was actually based on type classes, so you can write actual constrained polymorphic functions that access fields, and you can automatically get lenses for the overloaded record fields. If TDNR had been implemented, presumably this never would have been conceived, since "fixing" record fields is the big selling point people use for TDNR.
Do you understand how do-notation is transformed into explicit binds? If not, here is a primer. do x &lt;- whatever rest is simply a nicer way to write whatever &gt;&gt;= (\x -&gt; do rest) and do onlyASingleExpression translates to simply `onlyASingleExpression`. Note that the following to pieces of code are equivalent and will essentially become the same expression: do _ &lt;- whatever rest do whatever rest And, uh, if you still don't understand the translation, you should probably look it up or something. I don't know. Anyway, secondly, there is a syntax called let-in, which is similar to the do-let that you've used. This is how it works: let x = y in body is (usually, roughly) equivalent to `body` with all instances of `x` replaced by `y`. There are some performance benefits, and let-in is allowed to be recursive, but that doesn't matter for this part. Now, the do-let can actually be translated to a simple let-in: do let x = y rest is translated to let x = y in do rest So, in essence, do-notation consists entirely of syntactic sugar. With regards to modifiyng your structures, things may or may not get hairy. Usually, they are small enough or lazy enough (or fusionable enough or ...) that you won't have to worry about making copies. It would be a good idea to consider if this is the case. If it is not the case, you will likely want to define a mutable data structure and load it into that, and modify it using the ST monad. ST lets you write pure code that has access to mutable memory. It then lets you 'run' the monadic actions, leaving you with only the pure result. But in reality, you will probably want to use laziness (or other libraries, perhaps pipes?) instead of mutation. Also, if you only wish to modify a small part of the structure, a [persistent](http://en.wikipedia.org/wiki/Persistent_data_structure) structure might be a good idea.
Let the games begin. http://xkcd.com/356/
[Image](http://imgs.xkcd.com/comics/nerd_sniping.png) **Title:** Nerd Sniping **Title-text:** I first saw this problem on the Google Labs Aptitude Test. A professor and I filled a blackboard without getting anywhere. Have fun. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=356#Explanation) **Stats:** This comic has been referenced 15 time(s), representing 0.1053% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)
Load that file in ghci, what is the type of loadMidiFile? I assume it's something like :t loadMidiFile loadMidiFile :: String -&gt; IO ByteString You "extract" the value out of the monad (in this case IO) with `&lt;-`. However when running other computation that doesn't happen inside a monad, `let` suffices for assigning a label to that computation. Also, remeber that you can compose functions; so that you don't have to define a lot of intermediary labels unless you need them. For example the body of main can be: content &lt;- loadMidiFile midiFile print . instrument . snd3 $ fromMidi content return () Or alternatively lift the entire computation to the monad (you need liftM from Control.Monad), so that result &lt;- liftM (instrument . snd3 . fromMidi) $ loadMidiFile midiFile print result return () Most probably you got your question suspended on stackoverflow because your question is about the basic misunderstandings of Haskell, in regards to how you program within a monadic context and how immuntable data structures really behave. Unless the Midi library you use tells you explicitly that copies will be done upon each modification, that will not happen since that's not how immutable data structures work. See learn you a haskell, and persistent data structures; from which I will take a wiki excerpt: &gt;While persistence can be achieved by simple copying, this is inefficient in CPU and RAM usage, because most operations make only small changes to a data structure. A better method is to exploit the similarity between the new and old versions to share structure between them, such as using the same subtree in a number of tree structures. However, because it rapidly becomes infeasible to determine how many previous versions share which parts of the structure, and because it is often desirable to discard old versions, this necessitates an environment with garbage collection.
In Scheme at least, let is just shorthand for a lambda that you immediately invoke. `let x = [Y] in [Z]` is basically just a convenience for `(\x -&gt; [Z]) [Y]`. You use it because either you need to use one result in multiple places, or to split a large expression into multiple pieces to make it easier to understand.
I'm reminded of [this](http://sebfisch.github.io/haskell-regexp/regexp-play.pdf) functional pearl (ACM [cite](http://dl.acm.org/citation.cfm?id=1863594)) in which Haskell's laziness allows infinite regular expressions, allowing one to use regexes to parse CFGs. I wonder if this extends to infinite parsers...
What do you not get? `x` is the result of running an IO action to load a MIDI file, and the `let performance = fromMidi x` runs the function `fromMidi` on the value `x` and stores it in the variable `performance`. (Although this is done lazily so running the function `fromMidi` is not actually done until you use `performance`).
Obscuring the issue somewhat is the fact that GHC's strictness analyzer turns `foldl`-like recursion into something more like `foldl'` just often enough to lull you into a false sense of security.
What is your `time` command here? How do I get that much info with `time`?
Note that an unannotated let is also just a shorthand for that in GHC Haskell now, because let bindings are not generalised by the typechecker as they are in HM languages. 
Try adding `-fno-spec-constr`. https://ghc.haskell.org/trac/ghc/ticket/5550
Oh, that's the `-l` flag for BSD time. I think similar information can be extracted from GNU time with the '-v' flag. For instance $ gtime -v ./vec &gt; /dev/null Command being timed: "./vec" User time (seconds): 0.07 System time (seconds): 0.00 Percent of CPU this job got: 97% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.07 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 19562496 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 0 Minor (reclaiming a frame) page faults: 1284 Voluntary context switches: 0 Involuntary context switches: 2 Swaps: 0 File system inputs: 0 File system outputs: 0 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 7 Page size (bytes): 4096 Exit status: 0 $ gtime -v node 101743.js &gt; /dev/null Command being timed: "node 101743.js" User time (seconds): 0.09 System time (seconds): 0.07 Percent of CPU this job got: 32% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.53 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 70041600 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 1822 Minor (reclaiming a frame) page faults: 10903 Voluntary context switches: 41 Involuntary context switches: 123 Swaps: 0 File system inputs: 4 File system outputs: 6 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 14 Page size (bytes): 4096 Exit status: 0 GNU time is actually rather more informative, so I'll probably switch to using that one.
`let` is just a tool for aliasing pure values. It lets you use `i` instead of writing `and3 performance` over and over. `&lt;-` is a way to bind names to the result of a monadic expression. You can't replace your uses of `performance` with `fromMide x` because the types won't match so this is why you needed tou use `&lt;-` here instead of `let`. -- We can substitute all uses of things in a let let i = Main.content (and3 performance) -- We can't do that with monadic values though let context = and3 (fromMidi x) --type error
I've got this book (and pretty much every other Haskell related book as well). These books are generally excellent but there are always little bits missing and without a structured environment (a class, for example), it's quite difficult to learn/understand some of these things. 
Yes, see [my article](http://alpmestan.com/posts/2013-10-02-oh-my-laziness.html) or the various pages/posts/etc about it. My document does try to summarize the key bits to know about this.
Wow --- thanks for all the comments - very much appreciated --- it's going to take me a while to study them so apologies if I don't respond to most of them immediately.
No, my question was specifically "How do I load a MIDIFILE with Haskell" and apparently that was off-topic! Fortunately, before the suspension, someone responded with a reference to Hudak's new book on Euterpea and that has been quite helpful. As for the rest, the problem is solely with doing this stuff in Haskell. I could trivially implement everything I needed with C++, Pascal, Python, Objective-C and indeed have done these kinds of things before. So I understand deeply things like immutable data structures, persistence, etc. I also understand things like function composition, higher-order functions, currying, etc. But before I use shortcuts (like composition to avoid extra labels), I still need to UNDERSTAND better what's going on. My difficulty is solely in figuring out how to "let go" my historical non-functional approach to development and try to embrace the Haskell way. For example, when you say "lift the entire computation to the monad", you might as well be talking a completely foreign language....I have no idea what that means. I know what is the type of loadMidiFile (I have the source for its implementation from Github) but I'm still learning how to read (and more importantly) how to programmatically find my way around them.
No, I get that part (including the lazy aspect of it). I was not deeply understanding what "Let" was doing nor do I understand how to access (traverse/iterate/extract/view) components of the 'performance'
I didn't write the build system, but my understanding is that it generates a Makefile by parsing the cabal file. To keep our options open as things change, we ensure that building via cabal still works.
If you do get that `let` (very loosely speaking) assigns a value to a variable then I don't understand what you *don't* get. You presumably access components of the `performance` using functions that are designed to access components of the `performance`. I guess you should check your MIDI library.
Well, partially it's that "very loosely speaking" piece. Clearly I'm getting access to a value but also clearly it's not a variable assignment the way it would be in an imperial language. I.e, you can't write let j = 1 let j = j + 1 Clearly, without the 'let', the first would just be a (constant) function definition and so it almost looks like a way to define a function inside the 'do' part of the main program but that doesn't seem like the full explanation either. Edit: smog_alado's comment above is starting to point me in the right direction though
OK --- I think that explains 'let' sufficiently. But what exactly does it mean to 'bind' something. I know that terms shows up all over the place and one 'binds' things to get access to results but what is actually happening under the cover when you 'bind' something?
I think the core problem with which I am struggling is what exactly 'bind' does.
The D language has something similar calling it UFCS.
[Cunningham's Law](https://meta.wikimedia.org/wiki/Cunningham%27s_Law) is in full effect.
Well, bind is an operation with different implementations depending on which monad you use. In general, bind just has to follow some simple laws, but for the IO-monad, you can think of it like this. IO actions, that is, values whose type are of the form `IO a`, are in a sense descriptions of how to produce an `a` using IO. For example, one can view `getLine` as a description of how to read a line. Let's look at the type of bind (for the IO monad only). (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b This takes a way to produce `a`s using IO, along with a function from `a`s to ways of producing `b`s, and shows how to produce `b`s using IO. Reading it inside-out, it has to figure out how to produce a `b`, and it is allowed to use IO to do so. It is told how to produce an `a` (with IO), and for each `a` it is told how to produce a `b`. So, the way one would produce a `b` is obviously to first produce an `a`, then feed the result of this to the second parameter, which gives it a way of producing a `b`. Then it merely needs to produce said `b`. In fact, it might make sense to look at the translation in my first post, but reversed. action &gt;&gt;= rest is equivalent to do x &lt;- action rest x In general, many monads will follow the how-to-make-things-with-effects pattern. Examples include `State`, `ST`, `STM`, `Writer`, `Parser` and others.
&gt; For example, when you say "lift the entire computation to the monad", you might as well be talking a completely foreign language My bad then, because I tried to emphasis that by giving a sample code before/after, whereas in the before code, the computation took outside the monadic context. The monadic context is the "place" from where you extract the result with `&lt;-`. While others have covered better the approach to monads, of which I don't want to get into... Consider that the `liftM` code I wrote is equivalent to result &lt;- loadMidiFile midiFile &gt;&gt;= (\midiFileContents -&gt; return (instrument (snd3 (fromMidi midiFileContents)))) That's what we mean when we lift something into a monad. `return` lifts a value into a monad. Maybe the word lift brings inherent ambiguity, think of it as: push inside a context, execute in scope, etc.
Binding is basically just setting a name to be equivalent to some expression. This is the same as where, and giving function arguments names. These examples are all the same basic concept: myfunc = let x = 1 in x + x myfunc x = x + x myfunc 1 myfunc = x + x where x = 1 mymonadicfunc = do let x = 1 return (x + x) Each of those functions returns the result of `1 + 1` (except the last which returns the same in a monadic context.) There are two forms of `let`, just in case that is causing confusion. 1. Outside a do block. This takes the form: `let` *&lt;bindings&gt;* `in` *&lt;expression&gt;* 2. Inside a do block (monadic code). This takes the from: `let` *&lt;bindings&gt;* `let` in a do block is useful for doing pure computations in your monadic code.
Shhh
That is very impressive. I'll be taking a look on its source code for the next days (:
Note the difference between binding and the "bind operator" (`&gt;&gt;=`). I think OP was asking about the latter.
Yes, we're also experimenting with writing web services in Haskell. See [this answer on Quora](https://www.quora.com/What-is-Haskell-notably-good-for/answer/Tikhon-Jelvis) for a list of reasons why we're excited about these experiments. As Edward ([and HP](https://www.quora.com/Haskell/Which-companies-use-Haskell/answer/Peter-Halacsy)) noted, the idea has been floating around here at least since October 2013. I wish I could tell you more, but we're still in stealth mode. Stay tuned.
I'm not sure how that could be. Foldr starts at the last element of the list, but it is not known in advance. One needs to run through all the initial elements in order to find the last position... Edit: to make it more clear. foldr f i [a,b,c,d] = (f a (f b (f c (f d i)))) The first calculation performed is `(f d i)` - but `d` is the **last** pivot in the sequence so it is supposed to work on the **last position** - it needs the results of the previous computations. `(f (f (f (f i a) b) c) d)` is correct, as the last function call receives the accumulated computation plus the last pivot. The problem here is that `f` is **not** associative, it is inherently sequential, so the order matters. In order to use `foldr`, you'd have to reverse the list first (which is a `foldl`), losing the point.
UH huh, that is interesting. I wonder if it would be faster with mutable vectors?
Doh. Oh well, perhaps my post will still help someone.
I did something similar a while back, but on a more elementary level ‚Äî with strings. http://honza.ca/2012/10/haskell-strings
No. as DR6 said, both foldl and foldr start from the first item in the list, and work until the last: foldr f z [] = z foldr f z (x:xs) = f x (foldr f z xs) foldl f z [] = z foldl f z (x:xs) = foldl f (f z x) xs Note that both algorithms do exactly the same, the difference being just how they associate. As a consequence, foldl can't work with infinite lists, but OTOH it works quite well for finite sequences where you don't want to delay computation (it's basically a foreach). foldr works with infinite lists AND is specially good if you have delayed computations. Basically, extracting good performance out of Haskell requires understanding where you should use laziness and where you should use strict semantics. That's why the same algorithm you'd write in JS won't be well optimised for Haskell. Edit: Plus, `(:)` is always an O(1) operation, while `(++)` is always O(n). So of course `x ++ [y]` will always be slower than `x : [y]`, since the whole first list has to be cloned with concatenation. Edit: fixed the algorithm for foldr.
Haskell lists are singly linked, so all list algorithms need to start at the head. The actual computation `foldr` performs is from back to front, but it's done "backwards" as the list is traversed. `foldr foo acc list` performs `foo (head list) (foldr foo acc (tail list))` `foldr` works fine on infinite lists as long as `foo` doesn't force evaluation of the next element of the list. 
No. The **function** starts from the front, but the **reduction** starts from the back. As I said, according to that definition itself foldr f 0 [1,2,3,4] reduces to (f 1 (f 2 (f 3 (f 4 0)))) As the only redex here is `(f 4 0)`, this is where the **reduction** starts. In other words, (f 3 (f 4 0)) depends on the result of `(f 4 0)` and so on. The computation is done from back to head. You can't compute `(f 1 _)` before computing `(f 2 (f 3 (f 4 0)))`. Feel free to try to define that algorithm using `foldr` (without reverses).
Stick with it, these ideas may seem a little foreign now but with a little time they'll be second nature. 
Generalized how?
JS arrays are magic. I saw a comment by Andreas Rossberg to the effect that the V8 JIT will change the underlying representation of a JS array at runtime depending on how the array is being mutated ... So I guess threads like this one (and space leaks) will all be solved once ekmett writes his JIT compiler ...
One general learning-Haskell comment: it can be very, very tempting to try to speed up your learning process by mapping unfamiliar Haskell stuff to concepts you know well from imperative languages. But you'll be better off if you don't do that. You really want as much Beginner's Mind as you can handle. For instance, it's best not to worry about the performance implications of persistence vs. mutation at this stage. Likewise, either do a deep dive on monads --- read [Dan Piponi's tutorial](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) and/or [Wadler's papers](http://homepages.inf.ed.ac.uk/wadler/topics/monads.html), and then implement a few monads and prove that they obey the laws --- or else focus on getting things working for a while while your intuition catches up. (Either is fine; it's a matter of taste.) The short version: learning this stuff takes time, and patience is your friend.
Nope. Haskell is lazy, it won't use anything unless you ask it to. In the case of foldr: f 0 (f 1 (f 2 (f 3 4))) You can partially evaluate `f 0`, for example, and you'll have a partial response back. Or you can evaluate `f 0 (f 1 _)`, and you'll still have a partial response back. You can delay these computations until you strictly need them. An example with List is simpler, I think: 0 : 1 : 2 : ... Because of the lazy semantics, you don't need to evaluate `...`, and you can still use the items before that. With foldl, this doesn't happen: f (f (f (f ... 3) 2) 1) 0 Unless `f` is only lazy on its second parameter, which would be a weird thing to do since it doesn't tell you much. IOW: use foldl when you need to traverse the whole thing right away, since it's tail recursive and all that, use foldr when you can delay computations, because it can exploit the laziness in `f`.
Very good, thanks!
No! It has nothing to do with laziness, it has to do with **order of evaluation**. If `f` is not associative, then, generally, (f a (f b c)) != (f (f a b) c) As an illustration, see this simple function: Prelude&gt; foldl (/) 1 [1,2,3] 0.16666666666666666 Prelude&gt; foldr (/) 1 [1,2,3] 1.5 As you can see, the result is not the same due to division not being associative. The function used on my exemple is not associative due to the use of quaternion multiplication to travel a path in space. Quaternion multiplication is not associative, so the whole function isn't, by extension. You can't replace my function by `foldr`. Feel free to try.
Yep, now a question. Is it possible to make sure Haskell is computing everything, without using `print` to force evaluation? EDIT: I guess it is `deepseq` what I need, but now I have to figure out how to use it...
seq?
Ok I see where the misunderstanding is happening. When people say "replace `foldl` with `foldr`" they're not talking about just drop in replacing `foldl f a ls` with `foldr f a ls`. You also provide some new accumulating function `f_new` such that `foldl f a ls` is isomorphic to `foldr f_new a ls`. The trivial example is that `foldl (\a x -&gt; a ++ [x]) [] ls` is equivalent to `foldr (:) [] ls`
GHC 7.6.3 still hangs, but GHC 7.8.0.20140317 seems to work fine for me.
Where does generalization occur then? Just at top level?
HM type inference includes a LET rule where unconstrained type variables are "generalized" to type schemes involving forall. Effectively this is how HM generates polymorphism during inference‚Äîwithout let generalization inferred functions would be a lot less useful.
The actual story is kinda of convoluted. All of the dependencies, even transitive dependencies are stored in the cabal file. We have only strict equal dependencies (ideally this should only be for the final executable, but I think that is not the case). We have a script that parses the cabal file and (sandboxed) package database and makes sure that what is in the package database is what is specified in the cabal file. If that is not the case, the offending packages are uninstalled, along with the dependent packages, and the correct versions are installed. We parse the cabal file to find the modules we should compile and then use GHC's -M flag to generate a makefile. http://www.haskell.org/ghc/docs/7.6.3/html/users_guide/separate-compilation.html#makefile-dependencies Then we generate a final makefile with all of the packages that we need to link with. Why a makefile? It was 30% faster than Shake with -j8 before I learned to turn off the parallel garbage collector along with some other RTS flags Nathan Howell showed me. We are moving to use Shake, and if cabal+GHC can build fast enough with -j, we will use that. 
In my GHC 7.6.1 `(\f -&gt; (f True, f "abc")) (\x -&gt; x)` fails to typecheck, whereas `let f = \x -&gt; x in (f True, f "abc")` works as expected. I'm not sure if and how this is going to change in 7.8 though.
hi, i'm the CTO @prezi. So are using Haskell for lot of things: * webservices in a microservice SOA architecture * generating JS and C++ for multiplatform development (DSL, EDSL) * prototyping algorithms, quickcheck of algorithms And we're Haskell to raise awareness of FP. I also tend to think that we should use haskell 1-pager code instead of architecture diagrams. 
Totally understood ---
I share your sentiments. and am also very curious about this.
Thanks for the effort to explain. Actually I tend to like Haskell's way in defining data types more than OO languages' way in defining classes representing real world objects to which you send messages such as `circle.draw()` or `engine.start()`. However, I have 2 comments: &gt; So when you write code which works with a Shape, you typically have a case for Rect and a case for Circle. Yes, but eventually one can make a small mistake by forgetting one pattern (say, after 12 hours of continuous work :) ), and in that case, wouldn't he be grateful to the compiler to object on a statement such as `Just x = y` since `y` would contain `Nothing`? And more importantly (to me at least), why wouldn't the compiler do so? &gt; You are telling the compiler "trust me, I know what I'm doing, I know that the value on the right will end up being a Rect". Humm.. so in this particular case at least, I guess we could say that Haskell is more permissive and more confident in the programmer than Java, as it even does not require you to do an explicit casting.
Very helpful. Question: does x get its actual value during that 'let' statement (expression?) or are we still lazy and f y z doesn't get executed until we (for example) print x? 
But what does it mean for a value to be stuck inside a monad?
The latter. In terms of implementation in GHC `let x = f y z` creates a "closure" which is essentially a delayed computation. If you later `print x` or somehow use `x` in a way which requires it (or part of it) to be evaluated, the computation will happen at that point.
Stuck might be a bad term to use. Damn you are forcing me to take care of my phrasing today :) Monadic types are parameterized by the type of the values they "contain". They always look like `m a`, where `m` is a type constructor and `a` is some other primitive type. For example, `List Int` (written `[Int]`) is the type of lists that contain integers and `IO String` is the type of IO actions that produce strings. The thing with monads is that if you pay attention to the abstract interfce, you can take a value of type `a` and put it "inside" the monad: return :: Monad m =&gt; a -&gt; m a but you can never take things out of the monad once they are inside. -- This doesnt exist in general -- Some monads, like List and ST have a function that will do this -- but others, like IO don't getOut :: Monad m =&gt; m a -&gt; a The best you can do is use the values "inside" the monad to build other monadic values (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b The function you pass to `&gt;&gt;=` gets to look at the values "inside" the monad, but in the end you will still end up with a monadic value in your hands.
To lazy to try, but I wonder if added bangs to the tuple patterns will speed things up, e.g. buildLayer step (!j,!pivot) -- EDIT I think using some of the strictness that enigmo81 adds would probably help too.
It's not an easy topic. Recently a bug was found in GHC itself where somebody had used foldl (++) [] instead of concat or foldr and it was causing O(n^2 ) slowdown. A good resource is [this talk](http://www.slideshare.net/tibbe/highperformance-haskell).
I'm not an expert on performance in Haskell, but from various threads and resources, the main problems people have seem to be understanding how laziness works. Laziness is wonderful for composing things, but it builds up a lot of thunks (deferred computations). Knowing when these thunks are getting built, and when they are getting computed, is not obvious (to me, at least). I've looked around a bit, and the obvious resource on "How to fast Haskell" is Johan Tibell's [Haskell Performance Patterns](http://www.slideshare.net/tibbe/highperformance-haskell), which has several good suggestions on making your code fast. What it doesn't do is help build your intuition about what kinds of code is fast, and what kind of code is slow. ~~Slide 9, for example, is not an obvious thing. It's also something that I rarely see in the wild.~~ (sorry, thats from the short version on Johan's website) I think there's a lack of resources on how to make Haskell code performant, and on the behavior of different patterns. It seems like the available resources are "tips and tricks" style, or an academic paper, like the stream fusion stuff. Like many Haskell topics, we don't really have an approachable middle ground yet. Oh, and use [Criterion](http://hackage.haskell.org/package/criterion). It's quite nice.
This is just how things have worked for me, rather than any kind of claim for how it is for the wider community. There's a number of big concepts you need to hold in your head for any language. In C++ there the various naming resolution rules, thread and exception safety, and quite a few more. I think there are fewer big concepts to get up and running in Haskell relative to a lot of other languages. How laziness and evaluation work are among them. It can feel like a big struggle because Haskell as a whole can be pretty different enough from what people are used to. Quite a few of the performance tips (foldl', using bang patterns where appropriate) come from an understanding of how Haskell does evaluation. I liked "Introduction to FP using Haskell" by Bird to come up to speed on that, but it was a lot of work. There's some good blog posts here and there, but I'm not sure if anything is a definite shortcut to understanding. A lot of the rest of those tips were coming from understanding the big-O time complexities of the operations, which comes from familiarity with the libraries / techniques that are out there. For me, that is still ongoing, but has come from reading stuff online and asking people on IRC, and rolling up my sleeves and banging out some code. It seems to be coming together over time. Sometimes there's an interplay between the performant libraries / techniques and laziness, with either a positive or negative effect on performance. Purely functional data structures are a big win, I dimly remember that there were troubles with the spine strictness / laziness of Map recently. Once you get to the point where that interplay is high on your list of things to watch out for, you'll probably be pretty comfortable with either trying to work it out on your own, or knowing what to ask in IRC / mailing lists. There are also some great [performance patterns](http://johantibell.com/files/haskell-performance-patterns.html) out there, but for me it was easier to know what was going on / when / how to apply them once I had a solid understanding of laziness and evalution. I hope some of the above is helpful.
Haskell's biggest weakness, in my opinion, is the pervasive laziness. It's helped to keep the 'purity' honest; with a strict language like O'CaML it's tempting to allow unprincipled impurity to sneak in, whereas with laziness, doing so makes your programs impossible to predict. On the other hand, I'm not sure what I would do differently if I was designing a language. Laziness eliminates the need for whole classes of programming language features and makes real composability possible--see the classic [Why functional programming matters](http://www.cse.chalmers.se/~rjmh/Papers/whyfp.html) paper for an explanation. You can't implement short-circuiting `&amp;&amp;` in a strict language without extra language features! When I write Haskell code that I want to be performant, I focus on the systems programming aspects. What data structures do I want? What fields does it make sense to be strict -- this is kind of like the thought process that goes into "should this be a class (reference) or a struct (value)" in C#. My guidelines is that value types generally contain all strict fields and reference types lazily compute their elements. Do I need mutable arrays and ST, or can I make the algorithms I want run purely? The rest is just learning common pitfalls and understanding the evaluation model. Understand not just "use `foldl'` intead of `foldl`", understand *why* that's a pitfall and how `foldl'` solves the problem.
I think it's eminently plausible that a strict Haskell-like language with explicit thunk datatype could be successful. There would have to be a little bit of syntactic sugar to make the laziness easier to work with. I've thought about this a lot and I'm pretty convinced it could have the almost all the best of both worlds.
First of all, thanks for your great effort in writting this organized comment. &gt; I'd suggest reading a bit on sum types: [this post](https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/sum-types) might be a good starting point. I checked that post, and I already appreciate Haskell's data representation, which I see both elegant and powerful more than equivalent concepts in other languages. However, I still disagree with some parts, for example, when he says: &gt; This ensures that the only way we can access the value is if division succeeded. This protects against run-time failures where we try to access a non-existent result if division were to fail. Actually if we don't match `DivisionByZero`, we will have an alternative run-time failure, so effictively we just replaced a division-by-zero with a missing pattern error, and both are run-time error. &gt; That's because true and false are two different values, not two different types. OK, for me, Haskell's **data** is capable of representing more than just different "values" (enum values in other languages). For example, I could use it as an equivalent to OOP class hierarchy (even a better one!), so why limit it with such narrower (IMHO) perspective? &gt; As such, Java and Haskell are identical in that neither attempt to inspect values at compile time in order to determine whether a variable has one value or another. In imperative languages (at least those that I know of), you still cannot change the value of a constant, like saying `5 = 6`, not even pretentiously such as `True = False` in Haskell. Your examples are really useful, and lead me to an observation; in fact, looking at the following 2 cases: 1) case x of Circle a -&gt; ... -- pattern for Rect is missing 2) case Rect x y of Circle a -&gt; ... Rect b c -&gt; ... The 2^nd one (your variant of my initial example) for me is not the same as the 1^st one: * the 1^st one has a missing pattern, but on the *right-hand side* (I don't know if there is a more accurate term for this). This is OK for me, I'm used to it and never complained about it, though I see the compiler would still better at least warn about it, without the need for a specific compiler switch. * the 2^nd one OTOH is what I'm not convinced of till now, as the non-exhaustive pattern is on the *left-hand side*, and my intuition tells me this should be an error, not even a warning. I'm not sure if this difference (RHS VS LHS) is significant to Haskell (to functional languages in general). I think this example describes my problem the best (thanks to you). And another important point is that I still see that adding such restrictions would not limit Haskell's power at all (a counter example would be nice).
Great, thanks for checking.
Hey, I'm this guy. Yes, I posted the thread and, being, too, a Haskell noob, this is kinda what I feel. But I understand that feeling. See, Haskell's design is outstanding in the world we live. It is miles ahead of anything else in the market: Python, C++, Java, C#, Ruby, JavaScript, name it. The high order functions, the purity, the type system are simply ahead our time. But there is a problem: the rest of the world. Most of our hardware is hand-tuned for C-like code. Our OS is made in C. Everything talks imperative. Sequential processing is the default, regardless of Haskell computation (beta-reduction) being inherently parallel! It is not a stretch to say that, in a way, that Haskell programs are emulated in C processors. So, regardless of being a lone wolf, regardless of never having millions invested in optimize it, regardless of being different from everything else and not even fitting our current hardware, GHC is still able to produce code that is as fast C++ in many cases! That alone is impressive enough to excuse it from not being able to do so without efforts from the programmer. So, my opinion on all that? It is just too early. I see a future where languages like Haskell will be more and more common, investment will increase and, then, there is simply no chance the old paradigms will stand; FP is destined to take over the world merely because it is a superior technology by design. When parallel reducing machines start being produced, when better tools such as those structural interactive editors for FP are created, when something like Hoogle becomes the actual, de-facto Google of algorithms - when creating cool stuff here becomes miles easier than anywhere else - people will simply come. We are watching the natural evolution of PLs. For now we can only wait and help in being part of all this (:
This may also help you: http://lpaste.net/101815 It defines a datatype and an extremely simple monad instance for it.
As I commented elsewhere, I agree it may not be common practice, but a common mistake, may be? (i.e. forgetting to match against one pattern, especially with complex nested data types.
Using constructs with good performance is an issue in any language. In low level languages, it's reasonably straightforward because lines of code tend to correspond to constant numbers of instructions, so estimating performance is basically an issue of counting loops. High level languages are harder because they hide the loops on you. Is foldl a loop? Two nested loops? Does it allocate memory equal to the size of the input? Can the compiler pipeline it with other operations? Turns out all those things depend on the language. You have to figure out how the language works and get a feel for what the right tools to grab are for various classes of problem. Just be glad you're not working on a high level distributed language where you need a mental model of the implicit data locality and inter-node communication.
Intersting, for a moment I thought it was an actual Haskell-&gt;JS compilation. I'd like to test it for you, but for some reason I can't compile this program - can't find the `Data.ByteString.Builder` import. Weird, isn't that default?
I see [Idris's](https://github.com/idris-lang/Idris-dev/wiki/Unofficial-FAQ) approach as a fit answer to that. Everything is strict, because every program is guaranteed to terminate. Many say that dependently typing make programming much harder, but I'm not sure about that. I think it is really elegant and promising. Haskell proponents will fight me to death, but I think reduction strategies should **not** be part of a language specification. 
Also, now that you pointed it, I really like visual explanations to those kinds of things. foldl (+) 0 [1,2,3,4] = foldl (+) (0 + 1) [2,3,4] = foldl (+) ((0 + 1) + 2) [3,4] = foldl (+) (((0 + 1) + 2) + 3) [4] = foldl (+) ((((0 + 1) + 2) + 3) + 4) [] = (((0 + 1) + 2) + 3) + 4 = ((1 + 2) + 3) + 4 = (3 + 3) + 4 = 6 + 4 = 10 foldl' (+) 0 [1,2,3,4] = 0 + (foldl' (+) 1 [2,3,4]) = 1 + (foldl' (+) 2 [3,4]) = 3 + (foldl' (+) 3 [4]) = 6 + 4 = 10 And this is why `foldl'` is superior, to anyone wondering (:
I imagine writing a Haskell program in C would be similarly non performant. All that mallocing of intermediate representations because there is no deforestation.
Actually: foldl (+) 0 [1,2,3,4] = foldl (+) (0 + 1) [2,3,4] = foldl (+) ((0 + 1) + 2) [3,4] = foldl (+) (((0 + 1) + 2) + 3) [4] = foldl (+) ((((0 + 1) + 2) + 3) + 4) [] = (((0 + 1) + 2) + 3) + 4 = ((1 + 2) + 3) + 4 = (3 + 3) + 4 = 6 + 4 = 10 
I played around with writing a twitter bot, before I found out I'd have to give them a phone number. Once you get past writing the http requests (if there's a twitter api 1.1 implementation for haskell I can't find it) you can do a lot in a really small amount of code. It was quite fun to mess around with.
Well, reduction strategies *aren't* part of Haskell's specification--the only specification is that it evaluates as if it was "non-strict"--it's not even necessarily lazy. This means the compiler can optimize provably strict functions to be called strictly. That said, understanding the allocation and evaluation model are at the heart of performance optimizations for *any* language, so saying that they should be entirely out of the purview of the programmer is a mistake, in my opinion. I dream of a language where I can specify the representation of data directly and yet still take advantage of compile-time correctness proofs--a dependently typed systems programming langauge, if you will. From the FAQ you linked to, re: short-circuiting operators: &gt; However, in a total language we don't have undefined and non-terminating terms so we need not worry about evaluating them This is wildly inaccurate if you care about performance. Short-circuiting functions are even more important for performance than they are for correctness.
I'm not all that experienced with writing high performance code in Haskell but I would like to mention that every nontrivial example I've seen of "C like" performance has involved incredibly ugly code. That's actually the reason I started using Haskell, to break my habit of premature optimization.
I think in these tight algorithmic settings, strictness always ends up being the better thing. But it isn't that way in the wild. In the second episode of the haskell cast w/ Don Stewart he talks about Lazy parts for the big trunk or spine structures of a code and then using Strict branches. 
I use haskell as a web-app developer, which means I hardly ever have to worry about these issues. The serialization libraries, wai, and yesod libraries are all excellent. But I have had occasion to write several parsers, sorters and stuff. I think the IO system is the biggest bottle neck. I can get everything optimized and running nicely except for IO. I think this is gonna be fixed soon so I don't worry about it too much. Laziness gets really maligned on these "My performance was bad" threads. But often I have found that making things lazy has helped. (was elated to hear Don Stewart say stuff like this on the haskell cast!) For instance if I need to retrieve a list of (Maybe BigRecords) from a DB, I have had it end up being the lazy version was faster several times. 
I think for the most part it's just like learning to write efficient programs in an imperative language for the first time. That process isn't automatic, it takes years of experience with what does and doesn't work so well. Even then, you can run into things every so often which surprise you, and be forced into using profiling tools and passing to lower level representations of the code to help understand what's taking place, but those cases become more and more rare with experience. I've been programming in Haskell for about 12 years now, and at this point, I'm usually able to spot things which are at least potentially performance issues without necessarily having to profile. Sometimes I'm pleasantly surprised and GHC is better at optimising things than I expect. For the most part, I don't think down at the level of thunks and pointers and real machine execution when optimising. I think in terms of expression graphs and graph reduction (or even just plain expressions, when I think I can get away with it). You really can stay at quite a high level most of the time. I think this is an area which there's really not enough in the way of resources for beginners. [Simon Marlow's new book](http://chimera.labs.oreilly.com/books/1230000000929) does go a fair distance though, even if you're only interested in sequential performance and don't care about parallelism at all. The key picture which stuck in my head and helped a great deal over time was watching HOPS programs be evaluated. [HOPS](http://www.cas.mcmaster.ca/~kahl/HOPS/) is an experimental graphical programming language due to Dr. Wolfram Kahl at McMaster University. I think it's sadly still not available to the public, but [some animated GIFs](http://www.cas.mcmaster.ca/~kahl/HOPS/ANIM/index.html) are available to show what it looks like while programs are running. This view makes space and time behaviour of functional programs much more intuitive. (Space is proportional to the number of nodes and edges, so is roughly the amount of paper required to draw the graph. Time is, of course, proportional to the number of reduction steps required.) Even though I have to imagine the graphs in the case of a Haskell program, they're still pretty much there in memory (with various graph nodes encoded by closures of various sorts including thunks). It would be great if GHC could elaborate a program in such a way that these things could be visualised as the program runs. But aside from that, there are just certain things which show up in real code, and you eventually learn to spot them pretty quickly. If you're writing a recursive loop, it helps to think about which things are being pattern matched on or otherwise having their evaluation forced, and consider how unevaluated expressions might accumulate in variables. For the specific case of concatenation, there's a standard trick which you ought to be aware of. I'll illustrate it with an example. Suppose we have this tree type: data Tree a = Tip | Branch a (Tree a) (Tree a) and we'd like to do an in-order traversal, collecting the labels in the branch nodes of the tree. Naively, we might write: inorderNaive :: Tree a -&gt; [a] inorderNaive Tip = [] inorderNaive (Branch x l r) = inorderNaive l ++ [x] ++ inorderNaive r However, the list concatenation xs ++ ys takes O(length xs) steps to fully reduce: [] ++ ys = ys (x:xs) ++ ys = x : (xs ++ ys) That means if our tree leans to the left at all, inorderNaive will take quadratic time in the number of nodes of the tree. So, the standard trick is to pass from working with lists of type [a] to functions of type [a] -&gt; [a] that add elements to the beginning of a given list. Then the empty list is replaced by the identity function, a singleton list [x] is replaced by the function (x:), and concatenation of lists (++) is replaced by composition of functions. This is much more efficient because composition (f . g) = \x -&gt; f (g x) always takes exactly one step to reduce. So, making these substitutions: inorder' :: Tree a -&gt; ([a] -&gt; [a]) inorder' Tip = id inorder' (Branch x l r) = inorder' l . (x:) . inorder' r and we can recover an ordinary inorder function of the type we originally wanted by simply applying the resulting function to an empty list: inorder :: Tree a -&gt; [a] inorder t = inorder' t [] This inorder traversal now has time cost linear in the number of nodes in the tree. This technique doesn't only apply to lists. At one point while I was working for iPwn on an action RPG, I was tasked with improving the performance of a bit of our game's map editor. This bit of code was responsible for "baking" the final map, manipulating many chunks of binary data, building indexes and gluing things together appropriately for the game to read it later on. This was taking somewhere on the order of 45 minutes and used a couple gigabytes of memory in the process. It was somewhat convoluted, but when I saw that it was effectively chopping up and concatenating various lazy ByteStrings, even before I bothered to profile it, I tried blindly replacing ByteString by ByteString -&gt; ByteString and concatenation with composition. The thing then ran in a couple of minutes and about 20 megabytes, and we were satisfied enough by the performance at that point to be done with it.
You have to be very careful. You're definitely coming off like a fanatic, and whilst it's great to be enthusiastic, it's also a good idea to maintain some sort of context. You're definitely not approaching the problem from a balanced viewpoint and taking a step back might help you appreciate the qualities that make Java, Python and the other languages you mentioned so popular. Haskell is great, and the performance is great (like most programming languages). It's not being emulated on modern CPUs by any means, and indeed its qualities make it highly optimisable on modern CPUs - they're the same ones that make statically typed functional languages so fast in general. As a point of history, processors aren't optimised to run C - C was optimised to run on processors (a design goal was that every line of C could be converted into assembly directly).
That is currently only true if you enable some advanced extensions such as GADTs or TypeFamilies. (If I recall correctly. I couldn't find the exact list of flags implying MonoLocalBinds in the online GHC manual.)
Quaternion multiplication is associative; it's not commutative. Octonions are not associative (or commutative). I don't know enough to comment on the rest of the discussion.
How is that a bug? O(n^(2)) is exactly what I'd expect from previous discussions about the performance of `(++)`. EDIT: Oh wait you mean someone wrote that in GHC's own code? :D
That's really exciting! Webservices &amp; microservice architecture has been of great interest to me lately. I would really look forward to any and all posts on your http://engineering.prezi.com/ blog about them :)
Don't you mean `cabal-install`?
The pitfalls involved are of a few quite different sorts. Some of them are not specific to Haskell, such as linked list appending being quadratic. That `String`s are linked lists, and that you should use things like `Text` if text processing is a bottleneck, is more of a surprising quirk, but the situation is not too different from other languages with their own peculiarities. Finally there is the laziness/strictness conudurum, which is a proper Haskell gotcha. As someone who was a noob not long ago, I feel that it does get better: after being bitten a few times and getting a little understanding of what can go wrong and how you can diagnose it, you tend to develop some intuition for trouble spots and learn to be vigilant when you need performance. P.S.: As pointed out by other posters better than I would be able to, often there is confirmation bias involved when we, with a space leak in our hands, despair over laziness. It is easier to notice things when they go wrong rather than when they do as well as, or better than, expected. P.P.S.: On diagnosing excessive memory consumption (one of the nastier problems that can hit a newcomer), here is a [very good blog post](http://blog.ezyang.com/2011/05/anatomy-of-a-thunk-leak) by Edward Z. Yang, which did a lot towards demystifying that issue for me.
Ok, I understand closures. Thanks
Talk it out with people (here, on #haskell, at meetups) and find something to work on with people.
This is very helpful, thank you
Except that let bindings that could be floated to top level (don't refer to any variables bound in an intermediate scope) are still generalized.
Let's flip bind: `f =&lt;&lt; x = x &gt;&gt;= f`. Now: f :: a -&gt; m b x :: m a f =&lt;&lt; x :: m b This looks a lot like function application, and that's basically what it is. `f` is a function that produces a `m`-y computation that yields a `b`, e.g. if `m = NonDeterministic`, given an `a`, `f` gives you a non-deterministic `b`, or non-deterministically computers a `b`. But what if you have an `m`-y (e.g. non-deterministic) `a`? Bind says you can still apply `f` to the value, and the result is again `m`-y (non-deterministic). So bind is "just" function application that works for fancier kinds of computation, not just boring old pure LC style computation.
The rule of thumb is to use `foldl'` if you are trying to reduce the entire list to a single value (i.e. not create another list). Use `foldr` if you just want to lazily transform the list. Some other useful trivia: * You can implement `foldl'` in terms of `foldr` (and it's sometimes more efficient because it triggers build/foldr fusion!). See `Data.Foldable.foldl'` to find out how. This is also what my `foldl` library does. * `foldr cons nil`, just lazily replaces all `(:)`'s in a list with `cons` and replaces `[]` with `nil`. In fact, that's what the "canonical" fold for every type should do: just lazily replace constructors.
Haskell has enough structure that it ought to be possible to right a "strictness linter" to automatically annotate a program with advice of the sort that tibbe gave in his talk.
&gt; I tried blindly replacing ByteString by ByteString -&gt; ByteString and concatenation with composition This is the magic of "functional programming", and it is right there in the name, but most people skip right over it. 
Also, follow @HaskellTips on Twitter
&gt; it's reasonably straightforward because lines of code tend to correspond to constant numbers of instructions In fact, this is an explicit feature of C, one of the reasons C fans dislike C++, where `+` or even `=` can be an arbitraritly complex function call. 
We are still stuck with the Von Neumann architecture, for all its brilliance, and have been for 70 years. We have had Lisp machines, but they were mostly still stack-based machines with some interesting hardware for parallelized type-probing. To my knowledge, nobody has come up with an efficient hardware design for implementing the lambda calculus (though monkeys have been proposed) and even if one did, we have decades of experience scaling Von Neumann machines. At some point in the next 20 years, we will be programming biological computers. I hope that we learn from many of the mistakes that led to ideas like C++ and Java. Chris Voigt at MIT (http://web.mit.edu/voigtlab/) has some interesting ideas. But basically, our understanding of programming languages has outpaced our understanding of how to make computational machines (yes, we have many-core machines and we have many languages that support them poorly, but that's not the point). My hope is that the next, forced rethinking of computational architecture forces us to break out of the C/ASM mold.
&gt; Some of them are not specific to Haskell, such as linked list appending being quadratic But in impure languages, resizable arrays are ready at hand for convenient use. Haskell thrusts Linked List to the forefront with privileged syntax. 
I'm glad. If you have any questions about my post or examples, feel free to ask.
Which is nearly useless... If I knew what package I wanted, I could just look at it.
I almost have a sense of fear if something like conduit or pipes functionality were composed with twitter bots.
#haskell on Freenode is awesome.
Crap you beat me to it.
These things exist in python too, though. In fact, this exact problem does and everyone who's written any python that matters knows that python strings are immutable and building up a long string by concatenation via "+" is disastrous. Instead you have to pull out a magical trick with ''.join(list_of_strings) that no one would ever do if they hadn't been told to.
&gt; how well cabal manages to handle this situation. cute
Those are some informative slides! Thanks for the link.
haskell should change its defaults
Yup.
It won't change in 7.8, but it does change if you enable, for example, -XGADTs.
More info here: http://www.slideshare.net/master_q/20131006-osc-hiroshimaajhc
&gt; Is there any way in Haskell to directly write the image to the file without sacrificing much of the code purity? Yes, it's a fairly advanced technique, and there are multiple libraries for it. I like pipes, but there's also conduit.
&gt;I‚Äôm not here to complain or troll. I‚Äôm curious to know how do these things look from an experienced developer‚Äôs view. Are you constantly thinking ‚Äúhow not to ruin the performance‚Äù, or does it just become a native thing at some point? It becomes a native thing at some point. Now, I don't realy try for ultra-performant code; I don't really care if my portfolio optimizer takes 3 seconds or 0.3 seconds to run, but I am experienced enough that my designs rarely have huge performance problems. I do want to note that now I have the opposite problem: I find it difficult to write performant code in strict langauges. The most common example is when I want to write some producer / consumer pair of functions (which is essentially all the time). In a strict language, you typically have to tell the producer up front how much you will want to consume, but you might not really know exactly how much you will need, so typically you will overestimate and waste work. In ML, I can sometimes work around this by passing in continuations to the producer. In C++ I just cry. Another example is that in a strict language, I might want to abstract out some functionality somewhere, but if I use a function to perform this abstraction it also means that I must force all the parameters I pass in to get normalized at that point in time. Forcing this early normalization may cause extra work to be performed that will go to waste when parts of the data structure end up being unused. To program in a lazy language such as Haskell, you need to let go of the notion of computation "doing things". I instead think of dataflow: how bits of data are linked by dependency chains and how information flows through the program. Strict imperative programming is a machine with a crank. You put data in to a hopper; turn a crank; and output is squeezed out the bottom. Lazy functional programming is a woven tapestry where your output is a string that you pull on, and bits of the tapestry spin to generate your output as you pull. Lazy programming becomes much more natural once you learn to see things in terms of data-dependencies instead of instruction sequences. When you achive this level of understanding you will see that foo :: a -&gt; State a a foo x = do a &lt;- get undefined put x return a is perfectly legitmate (though contrived). That `undefined` "instruction" in the middle is fine there because it isn't connected to the rest of the program.
I'll check it out when I'm more expireneced. Thanks!
That's a good tip. I should stop treating the laziness as "a feature which I don't care how it's implemented" and start learning what's going on under the hood.
Now that you mention it I do realize that most of these pitfalls are in Prelude. This might be confusing for a beginner. It's not intuitive to expect the the most basic feature of the language might suffer from performance penalty.
C is full of other horrifying pitfalls, no doubt about that. However, when I did write the same program in C performance were good without investing too much thinking into that. I make an unfair comparsion, however, since I'm much more expireneced with C (Not to mention the time wasted debugging C problems which cannot even occur in Haskell to begin with)
You are right about that. However, as an expericenced Python developer, I feel that there are much less performance pitfalls that I should remember when coding in Python. Also, some of the current pitfalls are solved in Python 3 (pickle uses cPickle automatically, map/filter become iterators, etc...) I have very little experience with Haskell and I already fill that there are some quite pitfalls to remember in terms of performance.
I'm glad my comment was helpful! I hoped it might be. I'm going to respond out of order: &gt; &gt; As such, Java and Haskell are identical in that neither attempt to inspect values at compile time in order to determine whether a variable has one value or another. &gt; In imperative languages (at least those that I know of), you still cannot change the value of a constant, like saying 5 = 6, not even pretentiously such as True = False in Haskell. But ```True = False``` is not even an attempt to change anything - it contains no variables! I think you are reading ```True = False``` as a simple assignment, because that's what it looks like if you interpret it as you would in a language like Java. But the key difference is that in Haskell, it's not just an assignment, because Haskell includes a pattern matching is feature as part of the syntax of assignment. Also, True and False are names of constructors (essentially names for specific values in this case), not variables that can have a value bound to them. Given this, reading ```True = False``` properly as Haskell code, it's clear from the capitalized identifiers that the expression contains only constructors, and no variables, and that no assignment is taking place. It's just a degenerate case of a more general syntax for defining variables using a pattern match, but in this case there are no variables, so nothing is defined. Because of that, there's not even any way that this code can be executed, since there's no way to reference it - keeping in mind that Haskell is lazy, and won't execute a statement whose result isn't needed. &gt; the 2nd one OTOH is what I'm not convinced of till now, as the non-exhaustive pattern is on the left-hand side, and my intuition tells me this should be an error, not even a warning. I'm not sure if this difference (RHS VS LHS) is significant to Haskell (to functional languages in general). In this case at least, the LHS/RHS distinction is not really meaningful. In Haskell, ```Circle z = Rect 11 24``` is completely equivalent in meaning to: z = case Rect 11 24 of Circle z' -&gt; z' The latter statement provides a more detailed model of the meaning of the original one: it defines a variable z, whose value is determined by the result of the pattern match in the case expression. The pattern match is runtime logic, and whether it appears syntactically on the left side or right side of an '=' sign makes no difference. The compiler will generate almost identical code for both of these cases, with the only difference between the two being the exception message that is generated in the case of pattern match failure is different: "Irrefutable pattern failed" for the first one, and "Non-exhaustive patterns" for the second. &gt; And another important point is that I still see that adding such restrictions would not limit Haskell's power at all (a counter example would be nice). The compiler will already warn you of incomplete pattern matches if you ask it to. Are you proposing just that the special case should be reported on where a literal value is incompatible with a single pattern, as in ```Circle z = Rect 11 24```? That would probably be quite easy to add to Haskell, and I agree it wouldn't limit Haskell's power, but in practice it wouldn't add any power, either. The only scenario in which such a restriction would arise is when you're deconstructing a literal value, and that's not something that usually appears in real code. Pattern matching is primarily for matching values that aren't known at compile time. I can only think of one exception offhand, which is matching a literal tuple, e.g. ```(x, y) = (5, "foo")```, but in that case any errors would be indeed be caught by the type system, as you would expect, since the type of a tuple depends on its arity and the types of its component values. &gt; OK, for me, Haskell's data is capable of representing more than just different "values" (enum values in other languages). For example, I could use it as an equivalent to OOP class hierarchy (even a better one!), so why limit it with such narrower (IMHO) perspective? Because there's no simple alternative that would do anything useful. You're seeing a powerful way of defining values in a language and thinking something like "wouldn't it be nice if these were more like (sub?)types". But it's really not that simple. There are fundamental differences between values and types, and regardless of what your type system does, useful programs need to be able to deal reliably with values at runtime. The purpose of the pattern matching system is to deal with runtime values. This tutorial page provides a detailed explanation of pattern matching in Haskell and its purpose: [Case Expressions and Pattern Matching](http://www.haskell.org/tutorial/patterns.html). It points out that "Pattern matching provides a way to 'dispatch control' based on structural properties of a value." Dispatching control is a runtime activity, and testing the properties of values is a runtime activity. If you're interested in getting into more depth on these topics, I highly recommend the book [Types and Programming Languages](http://www.cis.upenn.edu/~bcpierce/tapl/). There's significant theory behind all of this, that's not specific to Haskell. &gt; &gt; This ensures that the only way we can access the value is if division succeeded. This protects against run-time failures where we try to access a non-existent result if division were to fail. &gt; Actually if we don't match DivisionByZero, we will have an alternative run-time failure, so effectively we just replaced a division-by-zero with a missing pattern error, and both are run-time error. That's why the compiler generates missing pattern match warnings! I'm sure you know that it's not possible, even in theory, for a compiler to detect division by zero in general, in a Turing complete language. But if you write your program following the standard Haskell approach, and use case expressions or function definitions to destructure values of types like DivisionResult, then the compiler will warn you when you have missing pattern matches, as long as you use one of the standard warning settings like -W or -Wall. So the compiler helps you write code that performs the necessary runtime checks to ensure that the runtime errors, in this case division by zero, cannot occur. Without the DivisionResult type, you would instead have to use tests like ```if value /= 0 ...```, but the compiler can't help enforce the inclusion of such tests. Another common example is the Maybe type. In Java, any variable that has an object type can also contain ```null```. This often results in the dreaded null pointer exception at runtime. In Haskell, this pattern is expressed with the Maybe type. Case statements and function definitions match on the two constructors ```Nothing``` and ```Just```, and the compiler can warn you if you leave out a case. As a result, Haskell allows you to write programs which guarantee the elimination NPEs (or their Haskell equivalent). In Java, people can and do religiously write an ```if (x == null) ...``` code around accesses to variables, but the compiler cannot warn you if you omit this - and there's no reasonable way to write your program so that it does warn you. One last issue is the rationale for the default pattern match warning behavior. Here's a summary: 1: By default, with no warning flags specified to the compiler, missing pattern matches are allowed anywhere. This is a reasonable choice, since any real program is built with various compiler options, typically including -W or -Wall, which provide the desired level of rigor. Keep in mind that Haskell can be used like a scripting language, and you can even defer many type errors to runtime if you like (with ```-fdefer-type-errors```), so there's tremendous flexibility. You don't necessarily want the most rigorous mode to be the default. 2: Even with -W and -Wall, incomplete pattern matches are allowed in constructs that only allow a single pattern. This is a tradeoff for convenience. Elsewhere in the thread, Dave4420 gave an example from real code: primary : secondary = T.splitOn "&lt;br&gt;" vPrimary Here, ```splitOn``` (from the Data.Text module) always returns a list of at least one element, so the pattern match will always succeed. But the compiler can't prove that. To allow code like this, warnings for these cases are disabled by default, but can be enabled with ```-fwarn-incomplete-uni-patterns```. In this situation, there's no perfect solution. If such cases were disallowed entirely, programs would become more verbose, full of cases handling situations that cannot possibly occur, such as matching an empty list in the above example. I doubt you'd find many Haskell programmers who consider that a worthwhile tradeoff, and I think once you're familiar with writing real programs in the language, you're likely to agree. 
I have to agree. Linked lists in Haskell are a basic language feature. In C, not only they're not basic features, they don't exist at all and you have to implemented them. I wrote quite some C programmes and most of the time preferred to use an resiable array. I also think that if I do have to implement in linked list in C then I can find some creative solution for the append problem, like always keeping a pointer to the last link in the list at the beginning (I didn't actually do that, though, so I might miss something here)
&gt; As a point of history, processors aren't optimised to run C - C was optimised to run on processors (a design goal was that every line of C could be converted into assembly directly) You are right, but I think that what /u/SrPeixinho ment is that while C might have been designed to be "close to, but more simple than assembly", Haskell defines its own different paradigm and let GHC figure out how to translate this different paradigm into assembly. C in its nature or not very different from x86 assembly (I don't really know other assembly languages so I don't want to make a general statment about that)
Finally, there is someone who cannot wait anymore.
&gt; Our OS is made in C. Everything talks imperative. Sequential processing is the default That's an interesting point. A couple of days ago I found [this](http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/). I still didn't found time to read it but it looks interesting. I was kind of suprised that it was written by a Microsoft employee. P.S. - Glad to see you're still optimistic :)
Check out Dataflow Architecture ( http://en.wikipedia.org/wiki/Dataflow_architecture ).
Take a look at the [RC2 status page](https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-7.8/RC2). You'll see there is one bug left unfixed at highest priority which is [64-bit windows cabal.exe segfaults in GC](https://ghc.haskell.org/trac/ghc/ticket/8834). Seems like quite a serious bug. I suspect that 7.8.1 would be released once it is fixed. I really want it out so people get the impetus to upgrade their packages, I had to stop using RC2 as I got into dependency hell. 
`seq` will only force to WHNF - in the case of a list, only the first element will truly be evaluated, and maybe a bit of the second. `deepseq` will make sure it's 100% evaluated.
However, linked lists are excellent in Haskell. Just not for storing large amounts of data. "Lists for control flow, libraries for data" as the saying goes. I think the privileged syntax is a good thing, given that it makes writing control flow code a smooth process.
I'm sorry, but could you run this through anything with grammar analysis? I had to stop reading around the bootloader section. It's actually hard to figure out what you're trying to say in some parts. This is an impressive project, and I don't mean to demean you. I'm sure that with a little proofreading this would be very readable.
Really? What language isn't that true in? I think strings are a great example here. In C/C++, python, and others the basic string append functions are O(n) meaning if you append a bunch of string you'll have serious problems. More advanced libraries exist though (say, ropes) to give you O(log n) append performance. Basic means not being complicated, and optimization is, simply put, complicated. It always requires deep understanding and careful choice of implementation.
Also, RC2 doesn't work at all on Win7 with i386. Compiling the simplest program segfaults.
Oh OK I knew it looked weird. Edited!
As [SPJ recently wrote](http://comments.gmane.org/gmane.comp.lang.haskell.ghc.devel/3941), &gt; Although few people develop GHC on Windows, many people *use* GHC on Windows, so we can‚Äôt really release in this state. So even if GHC/Win64 users may seem a minority (compared to Linux &amp; OSX users), they are still deemed important enough to hold up the release. However, the real problem here is that the GHC project has too few Windows-proficient developers (working on GHC) in order to keep GHC from breaking on the Windows platform on a regular basis.
The comments on that Trac page are simply marvellous to read. There's a whole world inside GHC, and an entirely different platform-specific world behind it.
You have a point there... I didn't think of it deeply
This bug is temporarily fixed. Something else holds up the release.
In what ways? Most of the improvements to processors over the last 10/20 years have improved the experience for all programs - out of order execution, improved branch prediction, more caching etc.
I just don't think that it's a valid argument to make that Haskell is slow because people haven't made processors specifically tuned for it. Firstly, Haskell is not slow - benchmarks have made that clear. Furthermore, there's the simple point that if I see a Haskell solution to a problem that's faster than C, I can just study the assembly code, understand what makes it fast and write some C that matches up - however fast I can make Haskell (in the purposes of a canned benchmark, which seemed to be the point of /u/SrPeixinho), I can always make faster C.
I haven't been able to find a nearby meetup --- I wish I could find a local expert from whom I could take a few lessons to get me up to speed
I have read in a few occasions on Intel official statements notes about how that or that feature would make certain pattern in C faster. It is not that they purposely try to optimize for it, but the fact C is used by most of their designers on their daily testing, benchmarks, programming in general (and couldn't be otherwise) kinda makes it happen naturally. If, by historical accident, Unix was written in a Lambda-Calculus-Like language, you could certainly expect that by now our processors would be really different from what they are. Certainly much less sequential, much more parallel and maybe even implementing application natively. Not on purpose, but by natural evolution.
He is obviously not a native english speaker (as is the rule, not the exception around the world). I think the post is still readable. I think if anything it deserves appreciation to take the time and write a blog post in a language you are not at all comfortable with in order to promote a topic you think is important. Learning a second language, especially if you never really had contact with other languages in your youth, is not that easy.
(:
&gt; Firstly, Haskell is not slow I never said that it is. My intention in this post was to say that: 1. Writing efficient code in Haskell is less trivial than I thought 2. If you go wrong in terms of performance then the penalty you pay is relatively high
&gt; I'm not a hardware guy, but is it feasible to build computers tuned to a pure functional language like haskell? The classic case of building a machine from the bottom up to use a functional language is the [Lisp Machines](http://en.wikipedia.org/wiki/Lisp_machine)
I'm German and I've nver been in an english speaking country. I manage to write comprehensible English nonetheless, at least I hope so. I never actively learned English besides at school, it just came to me when beginning to program and reading through books and the internet. IMO, a lot of proof reading and reformulation is mandatory, for which the author probably didn't invest the time... I admire his ambitions to write his blog posts in English instead of lang X, though! 
As of 16 minutes ago, there is also a bug on [linux](https://ghc.haskell.org/trac/ghc/ticket/8933).
PureScript is a really exciting project -- lots of activity on it! They're also working on a self-hosted compiler that should enable [some awesome properties](https://github.com/purescript/purescript/issues/248).
&gt; However, if the combining function is lazy in its first argument, foldl may happily return a result where foldl' hits an exception -- Haskell wiki - [Foldr Foldl Foldl'](http://www.haskell.org/haskellwiki/Foldr_Foldl_Foldl%27)
&gt; It's incredibly easy to write elegant code that runs really slowly I couldn't phrase it better :)
A lot of people complain about cabal a lot, but there are *much* worse situations out there
Not quite. Haskell let is recursive (letrec)
Wow, that's terrible. More reasons to stick to standard Haskell
German is English's, roughly speaking, fourth-closest relative, though, so picking it up is relatively easy. Only Frisian, Low Saxon, and Dutch are closer, Japanese OTOH is about as far away as you can get, unless your native language happens to be Klingon.
Your statement is true, that this is the magic of "functional programming," but in case somebody interprets this incorrectly (or perhaps even you mean it in the way I disagree with)... The "functional" doesn't mean "having to do with functions" but "functions in this style are real functions." In particular, functions treat values as real values. Functional programming is really all about values, not functions. The real magic here is that you can replace one representation of a value (and its various primitive functions) with another representation of the same value, and as long as they mean the same thing you have not changed your program's semantics. This optimization was easy to make because it was easy to simply change representation. That the representation is a function is actually not the point. 
&gt; native **american** speaker Well... I guess.
It's always fixable by adding a generalised type signature. Moreover, when they added this they examined the effect of it by disabling generalisation in all of Hackage. A great deal (majority) of it still built. All of the standard library still worked, with a couple of places (from memory less than 5) where signatures had to be added.
Well... thanks. I, too, should probably have done some more proof reading.
If you're so impatient of having it released, you should definitely go help GHC HQ, I guess Austin in particular has a lot on his plate and would love some help :-) Also, you can already play with it by building GHC HEAD (or the appropriate branch since HEAD might have some more patches). You could even provide feedback on bugs and give a shot at fixing them.
Scots (*not* Scots Gaelic) and Appalaichan are considered separate languages by some and both are nearly mutually intelligible with English. It's harder to get farther from Japanese in a sense but that's not anything special about Japanese, we don't have any good phylogeny for natural languages to really be able to make very definitive statements like "Chinese is more like English than Japanese is".
Yep, Chinese is about as far away as you can get, too. Along with all the others you can categorise into "no discernible relationship".
True. 
I realized that it's not the overall operating system adoption that matters most but rather the breakdown of GHC users per system. I looked for such stats but couldn't find any. Can anybody point to some stats? Or maybe even the download numbers of Haskell Platform per system?
There is no denying that Haskell is very different from C, and that it would be unfair to blame newbies for being surprised. But once they get past the "culture shock" they will realize that here, too, they have good data structure alternatives for the situations when lists do not cut it. For instance, a simple, list-based "creative solution for the append problem" are difference lists as in the [`dlist` package](http://hackage.haskell.org/package/dlist), the basic idea of which is well explained by cgibbard's top comment. In lieu of a "resizable array", I could mention [`Data.Sequence`](http://hackage.haskell.org/package/containers-0.5.3.1/docs/Data-Sequence.html) from `containers`, a purely functional data structure with very good appending performance at both ends. (How easy it is for newbies to find out about such things is a valid, if somewhat distinct, question.) 
Thanks for your advice. I'm quite aware about monads and monad transformers but still you blew my mind :) First of all, I don't understand how you managed to hide all randomness of zero generation inside replicateM. Could you explain this moment please? Also as I understand you suggest to use Random typeclass. Lets say fitness function depends on data received by network. I believe in this case received data should be stored in every Chromosome. But I don't see any possibility to pass this data to already defined instance of Random typeclass through carrying or somehow else. How is it possible?
You are quite wrong. We are switching from XP to Win7, but we are still using 32 bit tool chains for just about everything, partly because we still need to support XP for quite a while. We would still like to switch to ghc-7.8 when it's available. And yes, this is in a production system.
I wonder if HalVM could help with the first bullet point. Basically, it would let you run haskell services in isolated domains on top of xen (and might help with resource usage too?). HalVM is usually for prototyping OS services, but I could imagine it having a place in webserice prototyping too. I can tell you from experience that haskell snippets make great design docs. We do that informally and semi-formally all the time when haskell is not an option (and just plain use haskell otherwise). Another thing you could consider is using haskell to replace scripts in shell languages. I started doing that a year ago and I think it helps. Once I get to about 10-20 lines of bash/perl/python or I need control flow, command line parsing, or abstraction I switch to haskell. So far it's a win every time. You just need the script to be sufficiently complex to warrant making the cabal file.
I think the two big things you need to know are just the evaluation model and the layout of data. You don't need to know much about laziness beyond that thunks are made for expressions whose results are not needed right away, evaluated the first time the result is actually needed, and then the thunk is replaced by the result so later uses of the value don't have to do any extra work. If you do know that much, then yes it is a bit more complicated, but I don't think it's very bad (of course, I probably wouldn't write Haskell if I did) - the basic question is whether you want something to be evaluated later, and how much of the input you require to produce how much of the output. One payoff is that a little consideration for laziness lets you write more reusabe and composable functions. For example, a chain like f . g . h $ someList can run in one pass over the list, rather than needing to compute each result in full before moving on to the next, or rewriting all your functions in terms of something like generators. If you don't understand that much about the evaluation model, then it's no surprise you find performance surprising - you'd find Python performance surprising if you weren't quite sure and didn't care whether if condition(): cheap() else: expensive() would always call both of cheap and expensive or just one or what. The basics of data layout is just that a value has one or two header words, then a pointer for each constructor (unless you use the UNPACK pragma), and "changing" a value always allocates a fresh value - except for pointers outside the Haskell heap, and arrays which are contiguous arrays of either pointers to values, or just contiguous data if it's an unboxed array, and may be mutable. Everything else is built up from those, and ought to document how it behaves. That implies for example that (++) is going to copy the whole left argument, rather than being faster or even mutating the end pointer like an nconc.
Hang on, I am a bit confused by this explanation. I thought it didn't matter if you used ([x]++) or (x:) because in both circumstances the application occurs lazily so either way you get O(n) performance. For example: Branch 1 (Branch 1 (Branch 1 (Branch Tip) (Branch Tip)) (Branch Tip)) (Branch Tip) If you use "inorderNaive" it will basically be converted to a thunk graph resembling this: ((([1] ++ []) ++ [1] ++ []) ++ [1]) ++ [] whereas using "inorder" will convert to a thunk graph resembling this: ((((1:) . id) . (1:) . id) . (1:) . id) [] In both cases, reducing the thunk graph requires traversing the same number of thunks and therefore requires the same computational time of O(n) where n is the number of nodes in the tree. If in fact the "naiveInorder" function does take more computational steps, I would think the optimizer could reduce the number of steps to make it the same computational complexity as the "inorder" function. Or am I understanding this wrong? 
Where are you located?
 sourceTwitter $$ filterC (not . isStupid) =$ mapC ("Retweet: " ++) =$ sinkTwitter Implementation of `isStupid` is left as an exercise to the reader.
While `([x] ++)` and `(x:)` are essentially equivalent (and likely identical after optimizations), `(++ [x])` is a very different matter. The fact that the first version contains at least one instance of appending a single element to the end of a non-empty list while the second only uses the actual constructors says a great deal.
Unfortunately, it seems that most of what we know about the Windows-specific world comes from one or two brave explorers venturing forth with a tattered map that labels 99% of the territory "here there be dragons" as their only guide.
C is particularly nasty here, because figuring out where a C string ends is an O(n) operation.
Breaking down everything into very small independent packages looks beautiful from a mathematical perspective, but it's absolutely horrible from a practical point of view, at least today. I personally consider the cabal hell worse than the infamous dll hell of windows. Because of this, I strive for minimal dependencies in all my libraries.
Regarding `zeroGeneration`: you're giving it a function `rnd` that generates a random value and a population size `ps` that says how many values to generate, correct? You wrote it as a fold over a list `[1 .. ps]` but you ignore the counter value anyway, so consider that you could instead write it starting with `replicate ps rnd` and then folding that list by plugging the rng seed output of one function to the input of the next, while collecting a list of the generated values. Threading a state value like an rng seed through multiple functions is the purpose of the State monad, so what you'd basically be doing there is turning a list `[State g a]` into a value `State g [a]`, which is precisely what the function `sequence` is for. `replicateM` itself is just a handy function that uses `replicate` followed by `sequence` for you.
But in that case the first version (using concatenation) is O(2n) where as the second version (using function application) would be O(n), couldn't the optimizer shave-down the first version to O(n) complexity? Or does it not do that?
What is ByteString -&gt; to ByteString. Are you referring to the id function or some something else?
I suppose it *could*, in theory, but I doubt it *would*. Optimizations that involve significantly changing algorithms are difficult and risky to do automatically. Users tend to get unhappy if the compiler gets too clever and ends up pessimizing their code instead.
I compile all of my code with `-Wall`. Out of all the Haskell code I have written, this pattern never occurs. YMMV
Consider: (((([1] ++ [2]) ++ [3]) ++ [4]) ++ [5]) This builds `[1,2]`, then builds `[1,2,3]`, then builds `[1,2,3,4]` then builds `[1,2,3,4,5]`, each by taking apart the prefix and prepending it to the single-element suffix. This is O(n^2). By contrast: ((1:) . (2:) . (3:) . (4:) . (5:)) [] (doesn't matter how it's associated) will be prepending single elements to get its result. It effectively builds `[5]`, `[4,5]`, `[3,4,5]`, `[2,3,4,5]`, `[1,2,3,4,5]`. This is O(n). The number of thunks is not the problem. It is rebuilding parts of the list. Prepending to a list lets you share the suffix. Appending to a list requires rebuilding the prefix structure.
Well, the 'Control.Monad.State.Class.state' function has this type: state :: MonadState st m =&gt; (gen -&gt; (a, gen)) -&gt; m st a The 'System.Random.random' function has this type: random :: RandomGen gen =&gt; gen -&gt; (a, gen) The 'System.Random.random' function takes a random generator state, uses that state to generate a value while simultaneously updating the random generator state to it's next position. Both the random value and the updated generator state are returned. This is exactly what the 'Control.Monad.State.Class.state' function was designed to do, so you can simply pass 'random' to 'state'. Every evaluation of the expression (state random) will update the state in the transformer monad and return the value generated. Now, how can you perform this action N number of times and get N different randomly generated values? 'Control.Monad.replicateM' takes a single monadic function and evaluates it N times, collecting each return value into a list. It is essentially defined as: replicateM n doThis = sequence (replicate n doThis) which uses 'Data.List.replicate' to lazily generates a list of 'n' copies of the 'doThis' function, and then 'Control.Monad.sequence' evaluates list item in order. So if you pass (state random) to the 'Control.Monad.replicateM' function, it will essentially expand to this: do a1 &lt;- state random a2 &lt;- state random a3 &lt;- state random ... aN &lt;- state random return [a1, a2, a3, ... , aN] Remember, the every use of the bind notation causes the state to be removed from monad tranformer, possibly modifying it, and then placing back into the monad transformer, so every value returned will have been generated by the next position of the random generator. So here is a quick quiz question: if I were to write: do a1 &lt;- state random str &lt;- return "hello" a2 &lt;- state random return [a1, a2] will binding "hello" to 'str' using 'return' update the random generator? Or will this produce the same two values (assuming the same random seed and a pseudo-random generator) as this function: do a1 &lt;- state random a2 &lt;- state random return [a1, a2] The answer is: using "return" has no effect on the random generator state, so the first form and second form will return the same lists assuming the pseudo-random generator was started with the same seed value in both cases. In the state monad transformer, 'return' will remove the state from the transformer, but it will be placed back into the monad transformer with being modified. So only functions that actually remove the random generator state and replace it with an updated value will have any effect on the random generator state. So you can be assured that random numbers are not wasted. To answer your question about receiving a random value from a network, remember that you can instantiate the MonadIO class with a StateT monad transformer: instance MonadIO (Genetic g) where { liftIO = Genetic . liftIO } With the liftIO function, you can evaluate IO monadic functions inside of Genetic monadic functions without effecting the random generator. So lets say your "genes" can be parsed from text received from the standard input of your program, so you have a 'Chromosome' data type which instantiates the 'Prelude.Read' class. You could write this: getGeneFromSTDIN :: Genetic Chromosome getGeneFromSTDIN = liftIO (getContents &gt;&gt;= readIO) Here I used 'System.IO.getContents' to read text from standard input, and 'Prelude.readIO' to parse the text to a value of type 'Chromosome'. Of course, you can receive genes from 'MVar's, or Sockets, or file 'Handle's all in the IO monad using liftIO. For example: takeChromosome :: MVar Chromosome -&gt; Genetic Chromosome takeChromosome = liftIO . takeMVar Finally, above I explained how to define 'runGAIO' using only the instance of 'Random' so you do not have to pass a 'Genetic' monadic function to it, but this may not be very flexible. You could try defining two functions, 'runGAIOwith' and 'runGAIO' like so: runGAIOwith :: RandomGen g =&gt; g -&gt; Int -&gt; Double -&gt; Genetic g a -&gt; (a -&gt; Int -&gt; Bool) -&gt; IO a runGAIOwith randGen popSize mutProb makeChromosome stopP = evalStateT myOldRunGAIOFunc randGen where myOldRunGAIOFunc = do chromosomes &lt;- replicateM popSize makeChromosome -- make a population ... -- other logic for running the algorithm using the generated chromosomes. runGAIO :: (Random a, RandomGen g) =&gt; g -&gt; Int -&gt; Double -&gt; (a -&gt; Int -&gt; Bool) -&gt; IO a runGAIO = randGen popSize mutProb stopP = runGAIOwith randGen popSize mutProb (state random) stopP As you can see, (state random) is passed as the 'makeChromosome' function to 'runGAIOwith', but this assumes that the polymorphic return type is a type that has instantiated the 'System.Random.Random' class so we can infer the appropriate instance of 'random' based on what the polymorphic 'a' return type was decided during type inference. 
Jump to 2:45 to get to the presentation.
&gt; I was kind of suprised that it was written by a Microsoft employee. GHC's primary home, since growing up from Glasgow University, is Microsoft Research in Cambridge, so you shouldn't be too surprised. 
And jump to some time after 15 minutes to skip the poker talk.
How about a query "What's the most recent release among all dependencies of a given package?" It would be useful to know when to restart travis jobs. Bonus points if it understands that if package A depends on package B &gt;=1.0 &amp;&amp; &lt; 2.0, then release B-2.1 is irrelevant for A.
It's likely that it is related to languages like Turkish and Mongolian, it just hasn't been proven "beyond a reasonable doubt". However there is some evidence for it from the features of the language itself and if this particular understanding that Japanese and Korean are "Altaic" languages is right, it makes a lot of geographic sense.
And then jump even *further* to get to lenses.. :P (I kid) (And now feel compelled to say I enjoyed the presentation and learned useful things.)
I've just found a workaround for this bug. The actual root cause eludes me still. A fix is on the way to Haddock.
Windows **is** a Tier-1 platform. And it's one that worked before. This is a regression. I'm simply not going to make the release without it. Sorry.
That only effects unregistered compilers, and does not affect normal 32/64bit i386/amd64 Linux, OS X, or Windows distributions. The unregistered build is used on other platforms like PPC64.
The whole situation continues to confuse me. It's a compiler, your users are *by definition* developers, in the general sense. Likewise, anyone working on GHC is also a user. How is it that one platform vs. others has tons of users but not a single one of them is interested in GHC development? I suppose it's likely that, say, students taking a course using Haskell but no long-term interest in the language would be disproportionately Windows users, but I can't imagine that would account for all of it. Is it just an issue of awareness, that there are potential contributors who don't know how badly you could use their help? Or is there something deeper?
&gt; The whole situation continues to confuse me. It's a compiler, your users are by definition developers, in the general sense. Likewise, anyone working on GHC is also a user. How is it that one platform vs. others has tons of users but not a single one of them is interested in GHC development? Yes, it's quite confusing to me as well. Honestly I'm not sure what kind of community and/or social aspects might be at play here. One question, I guess, is if this phenomenon is similar for other languages and communities (non-negligible Windows userbase, but with barely any dedicated developers.) &gt; I suppose it's likely that, say, students taking a course using Haskell but no long-term interest in the language would be disproportionately Windows users, but I can't imagine that would account for all of it. Yes, I don't think this is all of it. So, there *are* certainly some technical aspects at play: Windows has rather poor community support by many standards compared to Linux (anything using `./configure` scripts or `pkg-config` is DOA, which is basically 'everything using the FFI'). As a result, this probably has something a negative feedback loop for users who may be interested in contributing (but think everything is broken). But its hard to say how much of an impact it actually has. Another question I guess is, how many people in the community are long-term, regular Windows users, and also use and contribute back to the community regularly? Perhaps the desire is mostly fleeting because of this... &gt; Is it just an issue of awareness, that there are potential contributors who don't know how badly you could use their help? Or is there something deeper? The first one is probably a big factor. I'm pretty much not joking when I say *anyone* who has done Windows development and has some familiarity would be of exceptional use. Kyrill, or /u/awson elsewhere in this thread for example, jumped in and within days made some especially intricate fixes to 64bit Windows that were baffling me. He's done a huge amount of work towards fixing 64bit Windows bugs, and he's clearly more skilled at it than me. So I know people awesome like Kyrill exist! Fixing the marketing is probably one thing. But there are also some tech things that need to be sorted out too to fix some stuff. But we need people for that :/
A few things that I would personally like to have, in order to incorporate into [IHaskell](https://github.com/gibiansky/IHaskell): * Given a module, let me know which language extensions it uses. * Given a set of modules or packages, let me know which ones are used the most on Hackage. This would improve autocomplete. * Given a typeclass and a data type, let me know whether there exists a module that exports an implementation of that typeclass for that data type, and if so, what module and what package that is. * I would love something that improved on Hoogle. In other words, I want to create a search query programmatically and then search for it on Hackage. Search queries would be for identifiers and identifiers matching certain types. This is probably duplication of effort, though; we'd be better off improving Hoogle. * Given a module, give me all of the identifiers it exports, along with their types. (Also whether they are values, types, constructors, etc.) A lot of these are aimed towards improving Haskell autocomplete, so anything else for that end would be great. Right now capabilities are fairly small.
&gt; Given a module, let me know which language extensions it uses. If you have the source for that module, you can use [readExtensions](http://hackage.haskell.org/package/haskell-src-exts-1.14.0.1/docs/Language-Haskell-Exts.html#v:readExtensions) to obtain this information.
&gt; (Not to mention the time wasted debugging C problems which cannot even occur in Haskell to begin with) Learning how to write fast code in Haskell can take some time, but the segfaults will never come back :)
[OFFTOPIC] You don't by chance know of any Haskell meetups in gbg? Haven't find any information on this while I believe there's plenty Haskellers located here (I know one guy, then there's some on Haskellers.com, and there's peopbably plenty more I think GbgUniv teach Haskell for CS students.) [/OFFTOPIC]
Instead of concatenating the lines just write each chunk out to the file using the standard I/O functions. Pipes/Conduit are really nice for this, but they do have a learning curve. I am still learning them also.
I put signatures on my top level (at least when I'm done prototyping), so for me good inference is all about where clauses
German and English are both West Germanic, English, Frisian and Low Saxon are [Ingaevonic](http://en.wikipedia.org/wiki/Ingvaeonic_languages), a sub-group of West Germanic, and Dutch is somewhat in between Ingaevonic and the rest of West Germanic. The Scandinavian languages are North Germanic. Note that relatedness doesn't necessarily imply ease of learning, it just somewhat hints at it. Going from "ten" to "tein" or "zehn", it's not much of a difference but a consonant shift, and not the hard part of language acquisition: I've heard the Standard German case system is hell for English speakers... and Standard German is quite an outlier inside the whole West-Germanic group, anyway, being a rather artificial construct turned Dachsprache.
I can't agree more about your examples of the division by zero, the `Maybe` type, and the NPE in other languages, and that's why I enjoy and appreciate Haskell already after doing only a few coding exercises myself. So to conclude, and concerning these 2 examples: primary : secondary = T.splitOn "&lt;br&gt;" vPrimary -- by dave4420 if length xs == 1 then let [x] = xs in x else 0 --by you in an earlier comment I still prefer (till now at least) the compiler to reject them since as you said, *the compiler can't prove that (they are OK)*, and the more it helps me to detect possible errors in my code, the better. I'd accept the cost of the added code verbosity, especially when such verbosity adds clarity (which I think to be the case here). Thanks again.
My opinion (as I tried to highlight in other comments as well) is that whenever the compiler detects code that seems to make no sense (such as `Red = Blue`, `True = False` which only add "noise" to the code), and more importantly code that may lead to a run-time error such as ` Irrefutable pattern failed for pattern`, then I will be grateful to it to object.
It's interesting if you consider that taxonomically, German is more like English than Danish, Norweigan, Faroese, Icelandic, or Swedish. It speaks to how heavily England was influenced by their successive occupiers.
No problem dear.. OK I agree it's casting but in different syntax :) and this is not good news for me as I didn't expect to find it here in Haskell too, noting that in OOP, explicit casting is against polymorphism :(
Thanks anyway, but even if I comment the unsafe line, it keeps reporting *unsafe*.
Oh, it takes the left value if it is a `Just`, or the right value if the left one is `Nothing`. Since its right value is a `Just`, I know that the result will always be a `Just`, so I tell the compiler to trust me on that instead of performing a case analysis and saying `error "never happens"` in the `Nothing` branch. But that wasn't supposed to be important in order to understand the example; I just wanted to compare the syntax of `case ... of ...` vs the syntax of `let ... = ... in ...`, so you could see that they were two different features.
That's the type of functions which accept a ByteString parameter, and produce a ByteString result. In fact, I was only ever using such functions which added something to the beginning of their argument. For instance, rather than using a constant string like "Hello", I might've used (\x -&gt; "Hello" ++ x)
Don't worry, irrefutable patterns cannot be used to cast a polymorphic value to a particular type. Haskell's polymorphism is very different from the polymorphism you find in OOP. Our polymorphic types behave more Java's generics and Java's interfaces than like Java's subclassing. The code we have looked at so far is not using polymorphism. I know it's confusing, since `data Shape = Circle Int | Rect Int Int` looks like it is defining two subclasses of `Shape`, but that's not the case at all. `Circle` and `Rect` are two *constructors* for the type `Shape`. There is no polymorphism, because there is only one type.
It's pretty easy to get started. (Mind you I dev on Mac ). If you're confused/stuck/need guidance / etc, the ghc-devs mailing list and #ghc on freenode irc are available 24/7. I've actually spent a huge amount of time helping people get started with ghc hacking in the past 6 months. There's a very very simple trick to getting productive: choose a single piece of ghc/ bug you wish to understand, and focus on it. Mastery of a single piece of ghc greatly helps with eventually understanding the rest!
Are the slides or the code available? Edit: found the code https://github.com/nbouscal/poker.hs
That's exactly my situation, fwiw. I don't even have a Windows machine at home and working with .NET doesn't really help much with understanding native compilation issues.
... how he will do that from haskell?
And jump [to 8:40](http://i.imgur.com/zjIRVrK.png)
We don't necessarily want more programs to work. If I typo something and it accidentally gets a different type, I don't want it to still work because the compiler is trying to guess what I meant. I haven't used Isabelle, but Scala does stuff like this (to an even more ridiculous degree) and it's awful there too. 
I still disagree. Most of the pitfalls are actually the same few things: Don't build up thunks unbounded and try to unbox whenever you can. No one (comparatively) uses python3, unfortunately, just as no one seems to really use all of the scads of 'fixed' prelude packages out there, so these points stand in my mind.
There are orders of magnitude more learning resources for Python, though. We only have a few accessible and relevant resources for Haskell, which makes it less likely that you'll read about the pitfalls before they bite you. Also, I don't agree that Python has fewer of them as I explained [here](http://www.reddit.com/r/haskell/comments/21g6uo/performance_in_haskell_from_a_noobs_perspective/cgdw8x2).
If you can have infinite production rules, you can turn a context sensitive grammar into a context free grammar, which you could then encode with applicative parsers. Edit: wording.
Please don't overvote/overcomment this post, a single answer does it. I love you guys and your helpfulness, but I'm starting to feel overexposed/abusing this subreddit...
I'm not the author, just stumbled upon it by chance. The author himself is Japanese. This was the only English page on the website and that's why I pointed the link to it.
Note that foo a b = a + b is not actually the same as function(a,b) { return (a + b); } It's actually more like function(a) { return (function(b) { return (a+b); };) } How would you call this from normal javascript? Not in any natural or normal way! Also, someone earlier mentioned, what about foo a b = a in Haskell, if you do `foo 1 (long computation that never finishes or crashes the program)`, then it'll end up fine. But in JavaScript: foo (a) { return (function(b) { return a; }); ] will probably crash if you do the same thing. In Haskell, we take advantage of this sort of behavior a lot, actually...and the semantics kinda matter. Also...what about lists? arrays? lists in Haskell are just an ADT, data List a = Cons a (List a) | Nil. How would that work on JavaScript? We'd have to define an entire way of translating ADT's. We might use lists as javascript lists, but what about something simple like take? take 0 _ = [] take n [] = [] take n (x:xs) = x : take (n-1) xs But you'd have to then implement pattern matching? in addition to implementing ADT's. Because ADT's can be hard to use without pattern matching to take info out of it. And you can't avoid ADT's because lists are probably an import part of any language. You could define array functions and use only arrays, though, I guess? Also, you'd expect take to work on infinite lists. And it does, in Haskell. But how would you even store an infinite list in JavaScript? You could store it...lazily...store the list as a to-be-evaluated thunk, and then evaluate it as you need it. But that requires you to have some sort of runtime system to manage thunks, unwrap them, evaluate them... The more you look into how things work in Haskell, and how very important semantics work...and the evaluation strategies...the more you see that you need more and more of the runtime system to make anything work.
The simple answer is that the Haskell and Javascript examples you proposed don't have the same semantics. For example consider that the ``foo`` Haskell function is lazy, curried and has an implicit ``dNum`` dictionary passed to it resolve the ``(+)`` function. The Javascript version takes a tuple of two arguments, is strict in both, and defers to Javascript's builtin addition which don't have the same numeric behavior as Haskell's and wouldn't work for arbitrary definitions of ``Num`` instances. You could hypothetically do something like this, but the source language wouldn't really be Haskell anymore if core ideas like laziness and overloading were removed. It would just be something with a similar AST endowed with an entirely different semantics.
there is not such thing as "overvoting", IMO. Its not as if you are posting fluffy memes.
Yes, but that is the point. We **would** have to define ways to translate the concepts, but as long as that mapping was well designed, we could have very good results. Specifically, every ADT could be mapped to... JS arrays, simply! List a = Cons a (List a) | Nil function Cons(a,b){ return a.concat([b]); } function Nil(a){ return []; } Tree a = Leaf a | Node Tree Tree function Leaf(a){ return [a]; } function Node(a,b){ return [a,b]; } Any kind of ADT could be automatically translated in a similar fashion. Notice types are lost, but that shouldn't be a problem - we've already type checked. Infinite lists could be encoded as generators. You're missing the whole point, though: it is not supposed to be a lossless translation, it is supposed to be a translation of pure functions to something that would be conceptually similar to a human. The point in this being that you could code complex applications as pure functions in Haskell - imagine, for example, a Tetris game as a pure function that receives keys - and then export it to a JavaScript function that would operate **very similarly** and that you could use with **not much effort**, without having to do any manual translating work. Or, in other words, it does not generate **equivalent programs**, it generate **programs that are different in a predictable way, following a set of pre-determined specifications**. For example, Haskell lists are encoded as JS lists? OK, now I know that, if I'm translating a function that returns a list, it will return a JS list.
Dropping 32-bit Windows would be leaving behind approximately 20% of users (about 30% of the number of 64-bit Windows users), according to steam: http://i0.wp.com/gearnuke.com/wp-content/uploads/2014/01/ms.png Maybe we should also drop support for FreeBSD and all Linux distros, as by the standards given their userbase must be *ancient*-tier!
[Part 2](https://vimeo.com/85909474)
Just so you know, there is a Haskell-like language that is specifically designed to be compiled to JavaScript Its called "Elm" http://elm-lang.org The Elm compiler and server is written in Haskell. I think this might be more along the lines of what you are talking about. My own two cents on this are: Haskell has a foreign function interface that can make calls to operating system specific functions. The ST and IO monad are like an abstraction layer that insulate you from the operating-system specific implementations of functions like "putStrLn" or "forkIO". If you want to compile to JavaScript, you need to write all of the foreign functions that support the IO monad, which means writing the runtime system to call into the JavaScript interpreter. I guess you could cut out large chunks of the standard Prelude module, anything related to IO, and get rid of "System.IO" and "Control.Concurrent" and other such modules, but then you wouldn't really have Haskell. You would have a language with the same syntax as Haskell. Again, I think either using Elm, or actually writing a correct Haskell Runtime System in JavaScript are your only choices. 
Well then...that language would no longer be Haskell. You'd be writing a new language with new semantics and new logic and new usage and new behavior. (Also, infinite lists are the same ADT's as list, so your list ADT would have to also a generator) Anyways, you're basically proposing to make a new language with similar syntax as Haskell, using Haskell's type checking...yet with new semantics, new behavior, almost *everything* meaningful behaving differently than in Haskell. Different enough that you can't even use most libraries written for Haskell, you'd have to write every library from scratch. Most Haskell libraries rely on Haskell's semantics and runtime and evaluation models. So at that point...you have the language syntax...but you lost compatibility with all Haskell code base, and you lost all of Haskell's semantics. So...why even use Haskell, or call it Haskell? Why not create a new language with similar syntax, and with well-defined semantics, based on the translation? Why not new language, with Haskell-like things or Haskell like syntax...yet with straightforward javascript translations? If "let go" of Haskell syntax, you aren't losing much --- you can build a type inference yourself, ADT systems that work and make sense, etc, everything good about Haskell that would otherwise be translated over, and you don't lose any libraries because they were already lost anyways. You can also feel free to *add different syntax* or *change syntax* that might be more suitable for a javascript-translatable language. Some syntax things just make more sense if you know you are going to be compiling to javascript...maybe explicit laziness instead of implicit, maybe explicit currying... but you can't do this if you want to stick to Haskell. But it's easy if you want to make an almost-Haskell-with-extra-stuff. Why adhere rigorously to Haskell syntax, or call it Haskell? Then you might have something like [purescript](http://functorial.com/purescript/), a language with function Haskell-like things, with type inference and type checking, yet translates into idiomatic-ish javascript :) The creator of [elm](http://elm-lang.com) went through a similar thing too.
Because if you create another language, then you'd still have to code your tetris game twice. That's the point you're missing. If purescript had an identical syntax to haskell's, it would allow something like that: code your game once, get it working on both haskell's platform and the browser. 
Core is "barebones Haskell", without all the syntactic sugar. So I think it's sensible to at least start there. Starting lower may be useful depending on what you intend to target and on what kind of strategy you'll be using in JS to integrate/mimic the RTS.
Problem is Elm's syntax is not identical, so you still have to code the application twice. Solutions that port the whole runtime are too slow at this point. What we need is something that allow us to code in a single language **once** and have it working on both Haskell's environment and the browser.
In that case, one could define a new language that could straightforwardly be translated to both, maybe, then? That would perhaps be a viable solution?
OK, but there is a minor thing I don't get. Isn't Haskell supposed to be just system F? So much is said about how small and simple it is on its core. Then you want to translate it to something, and suddenly you need a behemoth sized runtime? That doesn't seem honest. At this point I'm seeing Haskell as a huge, complex language with lots of design decisions and details, not the simple system it is sold as. Where I can read about what Haskell **actually is** as a whole? Is there a specs for it, such as R5RS? Is there a complete self interpreter for it, and how big it is?
Yes, Elm is a **Haskell-like** language, not identical. If you want to code it once and have it run anywhere, even in the browser, that will definitely require porting the Haskell Runtime System to JavaScript or asm.js . It would also be nice to have a back-end to the "diagrams" package, like "diagrams-js" where you can code a diagram in Haskell and rather than outputting an SVG, it places an SVG rendered in the browser's script engine right into the HTML document.
http://www.haskell.org/onlinereport/haskell2010/ the "simple" people usually refer to is the syntax constructs, and how things most other languages usually have as syntax are actually just normal functions/libraries in Haskell. the Haskell report does not explicitly specify how to implement Haskell to maintain its semantics, but the runtime system of GHC is a neat example.
That's true about the resources, but the quite simply are less of them in python. I'd also add that a lot of them are easy fixes (eg using join) where as finding a space leak is far more difficult. Python is an easier language, not because it's too familiar or I'm too used to it, but because it is easier. 
I was referring to the GHC/Win64 port which doesn't even have a HP distribution... Also, if Windows **is** Tier-1, how come it's the worst one in terms of multi-threading performance, FFI handling, Hackage library breakages, and whatnot?
You have good questions that deserve good answers. Nowhere near abuse yet!
Ultimately, it isn't possible to handle this for all cases, as it requires solving the halting problem. Moreover, laziness means it actually isn't an error until you force a variable matched by the pattern. That can matter! You might match `(x:xs) = ...` in a destructuring bind in one pattern of a `where` clause and check if its `null` in clause pattern, and only force `x` when the `null` check fails. That is a perfectly cromulent abuse of laziness that your check would rule out some of the time, while letting it through if the compiler doesn't outsmart itself second guessing you.
So you're saying I should go buy and install Windows, learn about Windows idiosyncrazies, so I can help GHC HQ even though I mostly care about the Linux port? I've been using GHC HEAD, but it's frustrating if I need to patch Hackage packages, since upstream authors generally frown (if they react at all) upon pull requests for GHC HEAD support... :-(
I'm confused. Instead of a constant you're using a function to append something to a constant?
I'm also confused as to how you got the types to match up when you had functions accepting ByteStrings and you were passing them a function of ByteString -&gt; ByteString. And if you or anyone else has a moment to expand on how this transformation speeds up things I would be deeply grateful.
Fay actually works quite similarly to what you describe. It does a translation from the Haskell source level. EDIT: I would encourage you to look at the Fay source code to see how this is done. It's a neat implementation, and readable!
A prior chapter introduced the unbuffered channel using MVars, which is itself quite intricate. Moving to STM requires very little code change, yet it overcomes a subtle potential deadlock in the MVar version (it's not possible to implement an un-get operation with good semantics, consider unGetChan and readChan vying for the channel's read end mvar).
I'd love to hear more about your escapades doing finance in Haskell. As to your bit about other languages, you can do the same in C++ for example via the STL. In this case you would create a generator by creating e.g. a forward_inserter. Admittedly, it doesn't look near as pretty as Haskell!
People *use* GHC on Windows, it's just very few of the GHC developers use Windows. 
You seems to conflate all developper jobs in a single one. You can type code, so you can do anything right? I don't think the "GHC user" == "GHC developer" equation is valid. Maybe it is purely a perception problem, but I feel that Hacking GHC is a massive investment. What's more, I feel that working with haskell without the haskell platform on windows is a pain. You have to use MSYS, which feels unnatural to me at least. Every time I wandered outside the platform environment, I have spent considerable time trying to get things to run. Sometimes I did not succeed at all. I think also that being on linux, you are more likely to see things as open/hackable. On windows, I am more used to downloading pre-compiled binaries, even for development. So I would say the lack of windows ghc developper comes from the windows culture &amp; maybe some technical issues/lack of doc for windows users.
One problem with mapping everything to arrays is pattern matching would no longer be possible - in your Javascript code, how would you differentiate between Left and Right for an Either when you have no information about which data constructor they came from? You'd still have to have something like ["Either.Left", x], in which case it's not really any different from using an object like { ctor: "Either.Left", value: x } to represent the value, similar to the way purescript does.
You've been unlucky. Most of the library/tool authors I know are pretty damn happy when people come and fix/prepare things for them. And I'm saying that if you're so impatient about having GHC 7.8, either get it from the git repo or help them get the official release out. These people are incredibly busy and you're basically saying "you're not making progress fast enough". Hence my answer.
One thing I would like to see is the big-O notation included with the function definition. That was one thing STL got really right.
Python is easier, I agree. But there is a cost: you have to do a lot of unit tests that Haskell does for us out of the box. Further, lots of the unit tests that we *both* would have to do are just some quickCheck properties for us and a bunch of lines of code for you. Honestly, it was the unit testing that drove me strongest toward Haskell. Every line of code in a program is potentially a bug. And as unit testing gets more and more prevalent, *unit test* bugs become much more dangerous (unit tests are also lines of code and carry the same risks). In Haskell you often have the option of writing your code in such a way that you can simply specify properties that should hold for pure functions and not actually have to write code using it. This doesn't make that single line less likely to have bugs but it does make it easier to spot any bugs (since it's less lines of code).
So if I understand correctly you're suggesting that my function should return [ByteString] (the list of lines) instead of the concatenated string, and then I should mapM_ the result to hPutStrLn?
That is done in quite a few of the libraries aimed at performance, like Data.Sequence and Data.Vector.
Disciple maybe ? http://disciple.ouroborus.net/
Disciple has a new, interesting, type system. I'm not talking about anything nearly as sophisticated. Just exactly what we have with Haskell, but strict.
There's a lot of money in enterprise. Someone cares. 
Just because Microsoft has EoL'd XP doesn't mean that our company will not have many XP users. My guess is that we probably have 20,000 people still using XP.
MSys is not needed for windows GHC for any packages with "Simple" build-type and trivial setup. It is required if the package wants "configure" step, but, even in this case it is often possible to manually edit .cabal file and add necessary flags manually without "configure" step. Also, packages having nontrivial setups are often written by *nix men and require manual editing. I've never used the platform. Learning at least the basics of cabal is important though.
The Haskell code foo a b = a + b is not even close to the same thing as function(a,b){return a+b}; Remember overloading.
Sorry about that one. I found that haddock broke on about 1% of my packages when I switched to 7.8.1 on OS X under Mavericks with Clang and there was nothing that could done as a user to fix it.
... because Windows does _everything_ differently than the rest of the world.
I for one am happy to take/adapt most any pull request that solves an issue for a user. I'm not sure who you've had the displeasure of working with, but please don't take it as indicative of the community.
Spineless Tagless JS Machine?
&gt; This is not the same, but has the same human-expected behaviour in most cases. Unfortunately, a computer program doesn't know what 'human-expected behaviour' means, so it also can't ~~compile Haskell to JS~~ derive JS from Haskell in this way.
Why not have a DSL, and one interpretation of it produces JS, and yet another runs it in Haskell. This way you can choose your own semantics and at the same time avoid having to write code twice. I seem to recall at some point I saw something like this, but cannot remember which project it was.
Furthermore, I suspect Fay is the most similar it's possible to be while retaining any hope of reusing code between standard Haskell and a JS translation.
In addition, here are the slides for the talk: https://dl.dropboxusercontent.com/u/58525/NY%20Haskell%20Meetup%20-%20Conquering%20Hadoop%20with%20Haskell.pdf
the haskell community has tended to be over-focused on chasing c-like performance and then having to produce c-like programs to achieve this non-goal. idiomatic haskell will not produce programs that are generally comparable to the performance of well-written idiomatic c, but that shouldn't matter, idiomatic haskell performs well enough and delivers substantial benefits beyond performance.
It might be possible that many windows users have switched to linux because of the poor windows support. I only use haskell for toy project at home and have been thinking in that direction for a while.
`fail` is needed for pattern matches (which can fail). You can still desugar it using `case`: do pat &lt;- expr1 expr2 becomes expr1 &gt;&gt;= \x -&gt; case x of pat -&gt; expr2 _ -&gt; fail "pattern match failure" where `expr1` and `expr2` are arbitrary expressions and `pat` is a pattern.
It's really hard to say without seeing the code (what the function that produces [ByteString] does). One of the things about lazy evaluation is that the performance of a piece of code can change depending on the context that it is run. If you can link to a gist of the code or something I'd be happy to re-factor/offer suggestions.
Will the allow us to fix the bug with [snaplet-postgresql-simple](https://github.com/mightybyte/snaplet-postgresql-simple/issues/6)?
Is there any way to have a list of lambdas and determine which of the lambdas fail at pattern matching a specific value? What I envisioned was "matching the lambda parameters" in (&gt;&gt;=) and if that fails call fail inside (&gt;&gt;=). I suspect there is no way to "match lambda parameters" though. Thanks for the clarification. It makes me appreciate do-notation be realize it's not necessary.
None of these changes particularly impact that issue, although the `Values` constructor would allow you to avoid explicit transactions in some cases by issuing one query instead of several. So basically, no, that's a separate issue. One that should be solvable if somebody puts in some effort; sopvop has one solution, I have a similar but slightly different [solution](https://github.com/lpsmith/postgresql-simple-implicit) that attempts to preserve the existing interface, but isn't expressed in terms of snaplets yet. Personally, I use my own snaplet that exports the `withPG` operator for now.
There is nothing preventing that from being fixed. There's actually a fix there from sopvop that we're using just fine in production. It would be nice to see it officially included and resolved though.
&gt; Is there any way to have a list of lambdas and determine which of the lambdas fail at pattern matching a specific value? I'm not quite sure what you mean by that. You can catch exceptions in the `IO` monad. `PatternMatchFail` is one such exception: import Control.Exception catchPMF :: a -&gt; IO (Either PatternMatchFail a) catchPMF x = try (x `seq` return x) Then you can map that over your list of functions: do let fs = [ \[x] -&gt; x, \[] -&gt; 0, \xs -&gt; 1] -- apply every funtion to [1] and catch pattern match failures print =&lt;&lt; mapM (catchPMF . ($ [1])) fs This prints [Right 1,Left &lt;interactive&gt;:21:23-30: Non-exhaustive patterns in lambda,Right 1]
There are more neat libraries around than you could ever learn because they are created more frequently than you can grok them. If you wanted to know everything before starting a new project, you would never start - you'd become an academic.
What I meant was you could remove Monad fail and change the &gt;&gt;= implementation to catch the PatterMatchFail and then return what the prior fail value was. Of course, that would require exception handling and IO inside every &gt;&gt;= that wanted to have behavior similar to the current Monad fail/do-notation combo. It's not worth it. This whole thread is born out of my misunderstanding of do-notation and my novice attempts at redefining Monads and do-notation to match my expectations. I have learned a lot though, and thanks again for your responses.
Here's the [thread at StackExchange](http://codereview.stackexchange.com/questions/40269/ppm-pgm-image-processing-library-in-haskell) the encode method of the Image typeclass is the problematic one. Thanks!
go to github.com/ekmett, scroll through repositories with your eyes closed, stop, click, read documentation, become enlightened
&gt;For example, there is a library that allows you to write a list-operating function once and automatically get its inverse. What's the name of the library? And how does it handle something like (reverse . reverse) [1..]
Knowing about lenses sooner might have saved me from writing a lot of ugly record-update code.
As per the title: I wish I had read the Haskell Report earlier. I haven't read the whole thing, but the first 6 chapters we really good to know. Understanding how to parse Haskell expressions. I went for a long time without really understanding the details. What unification is. How it is similar to pattern matching. How instance selection works for type classes. Knowing all the things you can do with Maybe. I could keep going ... 
* Replace any type or expression with () and get a easy to read type error that shows what should go in that location (https://www.youtube.com/watch?v=52VsgyexS8Q) TypedHoles in GHC 7.8 will make this even better. * Use ghci when developing as much as possible, it's so much quicker. You can import multiple packages with `ghci -ipkga/src -ipkgb/src pkgb/src/Main.hs`. I often have more than 300 modules in a single session this way.
What does this mean? I don't know, but as a beginner, I would like to know.
Yesterday, I read excellent posts about Free monads by Gabriel Gonzalez, e.g., http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html?m=1. And it just occurred to me that a free monad represents valid ordered polymorphic sequences. Re: your question, this can mean "all valid sequences of transitions ". So you declare a datatype for your transitions, derive a Functor instance, and declare a Free monad type. With that, you could even use QuickCheck to generate valid traces. 
GADTs (http://en.wikibooks.org/wiki/Haskell/GADT) can help with this.
Or good old fassioned [phantom types](http://www.haskell.org/haskellwiki/Phantom_type).
That is a great advice right there. I'm definitely doing this ASAP. Thanks!
The thing is that reverse is the inverse operation of itself, so `reverse . reverse = id`, and if the library is able to know this somehow, it will just return `[1..]`.
http://www-ps.iai.uni-bonn.de/cgi-bin/bff.cgi I didn't study it in depth though!
Data.Functor, Control.Applicative, Data.Foldable, Data.Traversable, Data.Monoid are all useful for manipulating the "weirder" values and I wish I paid attention to them earlier. (Of course, Control.Monad is useful too, but I found that early enough.) Functional programming in general is something I'm just now starting to understand, after more than seven years of Haskell on and off. Do you think you use functions a lot? No, you don't. Nowhere near as much as you could.
As usual, /u/tekmo has [put it better than I could](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html). TLDR, typeclasses are not first-class values in Haskell, which can be more pain than it's worth most of the time.
I'm guessing it was one of those "I have a hammer" situations. Beginner learns about type classes and starts making an (over designed and probably unnecessary) typeclass for everything. 
Did you understand the example for lists? It's the very same transformation, thinking of a ByteString as being a list of bytes. The point is to transform the algorithm as a whole, so that instead of working with ByteStrings directly at all, it works with functions that prepend some stuff to the beginning of an arbitrary ByteString. In place of anywhere that the algorithm used the empty ByteString, you use the identity function, in place of anywhere that it used a constant ByteString s, you instead use the function LBS.concat s (which is asympotically constant time, because s is of fixed length). Most importantly, in place of anywhere that the algorithm used concatenation, you use function composition. Since function composition is constant time where concatenation takes time linear in the length of its first parameter, this can reduce the overall time complexity, usually from quadratic to linear time. In the end, instead of computing a ByteString, you'll have computed a function ByteString -&gt; ByteString. In order to get the final ByteString result, you can just apply that function to the empty ByteString.
And now I suddenly understand monads.
You shouldn't leave Vim to reload your code. Preferably, you should reload your code after every line you write, just to make sure the types check out so far. Leaving Vim just introduces a lot of extra stuff you need to do.
I had the same experience
&gt; Indexed monads Do you have any recommended reading on this? A quick google search revealed [Control.Monad.Indexed](http://hackage.haskell.org/package/category-extras-0.53.5/docs/Control-Monad-Indexed.html), but this wasn't very informative.
Would an [example](https://gist.github.com/gelisam/9845116) help?
Do all rewrite rules respect bottoms? I thought that was an often skipped over bit.
Yes, it is GADTs which allow me to do that, but I wouldn't call that non-exhaustive. Not only do I cover all the possible values of type `Program Closed j a`, but in addition, if I add the "missing" clause runClosed (Bind Close f) = runClosed (f ()) GHC complains: Couldn't match type ‚ÄõClosed‚Äô with ‚ÄõOpened‚Äô Furthermore, if you enable warnings, GHC won't complain about non-exhaustive patterns.
&gt; In other words, (f 3 (f 4 0)) depends on the result of (f 4 0) and so on This is where you're wrong. The usefulness of foldr is because generally f can produce something useful without fully evaluating it's arguments. 
Using basic-prelude with NoImplicitPrelude to lower the amount of importing I do, and it tends to steer you into Text based solution instead of Strings. Just making types for everything. Even if it is just a newtype wrapper around Int. It is worth having for keeping track of things as your program grows. If you have enough types things start to document themselves. That's something I miss in other languages.
[This](https://github.com/dalaing/dotfiles) is my vim setup, a bit of a mess and the vundle instructions _might_ be out of date, but the .vimrc has some comments on how to get going (what to install, run, etc...). Things it gives you: * pretty descent completion * some indenting support * ghc-mod (for type checking) and hlint (for style) on save * unicode dots and lambdas The main thing I use it for - you can check the type of the expression under your cursor (F2), expand the size of the expression (F2 again), or clear the selected expression (F3). The type information is gained using ghc-mod, so that only works if the file compiles. If you get stuck, GHC will accept 'undefined' in a lot of places, so you can put that in, hit F2, and get some information on what you should be working towards. As an aside, if you're going to do something like the [NICTA course](https://github.com/NICTA/course) I'd suggest not using that feature - building up experience reasoning about types seems to be part of the experience. My vim setup still has some rough edges, other people might have better suggestions. I also use ghci quite a bit as well.
How about http://blog.ezyang.com/2011/04/the-haskell-heap/? 
 {-# LANGUAGE RebindableSyntax, GADTs #-} ... -- States, which must be represented as types -- so that they can appear as type indices. data Opened data Closed I'm surprised this does not require [EmptyDataDecls](https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/data-type-extensions#emptydatadecls).
I thought it invoked a normal fail function, having nothing specifically to do with Monads.
*Sighs* no it is not. I understand laziness, I understand how things like `take 10 $ [1..]` work and it has nothing to do with it. You just don't understand what I'm saying. `foldr and foldl are not equivalent for non-associative functions` That's the only claim I'm making, feel free to prove otherwise.
At least in some situations, GHC will complain about inexhaustive patterns anyway because even if the other constructors aren't possible with the given type, a bottom value is.
[well I'll be](http://www.haskell.org/haskellwiki/Haskell_2010)
i think the confusion comes from the fact that a foldl is just a particular type of foldr, however, when you rewrite f such that foldl f == foldr f, you're just recreating foldl. if you really wanted to get some extra performance maybe repa would help but i'm not clever enough to tell whether it would be able to do much for your particular algorithm 
If you're used to (or envious of) the way high-end IDEs highlight syntax errors directly in the editor view, use ghc-mod or hdevtools to integrate the compiler with your editor of choice. http://www.mew.org/~kazu/proj/ghc-mod/en/ https://github.com/bitc/hdevtools
You cannot match the bottom value, so GHC will not complain that you are not matching on it. (You can sort of partially match some bottoms with things like exceptions, but to do it in the general case, you'd allow programmers to solve the halting problem).
This is really low-brow compared to some of the answers here, but it really helps me a lot, so I will mention it.... If you want to see all the values passed into a function with multiple patterns, just add one more pattern at the top that looks like this "func x | trace (show x) $ False = undefined".... Extremely useful.
Can you give an example?
[missingH](http://hackage.haskell.org/package/MissingH-1.2.1.0) has some really useful functions which are helpful to me.
I think this has nothing to do with existentials, which is what Luke Palmer's article criticises. The approach taken by freegame seems quite similar to that taken by Diagrams, for whatever that's worth.
Smelly?
That's top notch
Why don't `{postgresql,sqlite,mysql}-simple` use common types? IOW, how can I write an application for which the database can be configured to be one of those 3?
&gt; It's not immediately obvious that typeclasses aren't just like interfaces. what's the most important difference (causing problems) typeclasses exhibit compared to interfaces?
I think what I meant to say is that typeclasses aren't used with the same ubiquity or abandon as interfaces might be, in agreement with the recommendations against them. I'm not an intermediate enough Haskeller to really understand the reasoning. Part of it, to be sure, is that newcomers to Haskell, myself included, tend to over-type programs, where it will often do to just use lists, tuples, and a couple of simple `data` types.
While it contains quite a few useful functions, it takes the lazy approach in terms PVP-adherence... so it's ok to use it when you work in GHCi or code up some simple application for yourself, but I would never use it as a build-dep for package to be uploaded to Hackage, as I can't trust `missingH` to follow the PVP, and would otherwise compromise the PVP-compliance of my own package.
You can't reason about the semantics of a C++ program without understanding what every function does right down to the leaves of your call tree, because any function can cause any side effect. In Haskell pure code is more easily partitioned from side-effecting code, and knowing that calling "`f x`" will produce an equivalent `y` now and forever (modulo cosmic rays) is a real help when trying to understand what your program will do.
Libraries on hackage are not often considered part of GHC.
It is not without problems. About a year ago, I used ghc-7.6.3, and I have trouble installing this package for ghc-7.4.1, which was used in travis.
Oh, I do. That's also something I discovered early enough, so it's not under the category of "stuff I wish I knew earlier."
As an aside, I think the "Make illegal states unrepresentable" is one of the great slogans of functional programming.
In modern Haskell they can be! http://joyoftypes.blogspot.ru/2012/02/haskell-supports-first-class-instances.html
[Reifying type class instances.](http://joyoftypes.blogspot.ru/2012/02/haskell-supports-first-class-instances.html)
How do you disallow invalid transitions in this way? E.g. if your transitions are `data Operation = Open | Close` (assume we don't need any extra data), and you want `Open; Close; Open; Close` to be legal, but `Close; Close` to be illegal.
Try to write your proposed version of `&gt;&gt;=` e.g. for `Maybe`: Nothing &gt;&gt;= f = Nothing -- the easy part Just x &gt;&gt;= f = if {x matches the pattern checked in f} then f x else Nothing The problem is that Haskell doesn't allow you to check this condition! If x doesn't match, `f x` will throw an exception, but you can't catch this exception outside `IO` monad and anyway catching it would change the meaning (because it could also be thrown somewhere inside `f`). I also think having any pattern other than a variable on the left-hand-side of `&lt;-` is a bit of a code smell. If you avoid it, you never need to care about `fail`.
Irrefutable patterns on the left hand of &lt;-, such as tuples, are fine.
Fair enough.
also consider that bind and return etc. have meanings outside of haskell :) 
And which approach is that? While I've heard of diagrams, I haven't used it. I suspect that if your intuition is correct that it could greatly assist in not only understanding the benefits to such an approach, but also in navigating the module's source as well.
So this isn't Haskell, but Idris is enough like Haskell that it's hopefully interesting, and perhaps you can get some intuition that you can backport. Edwin Brady wrote a [Hangman game](https://github.com/edwinb/idris-demos/tree/master/Hangman) in Idris, where the type system guarantees the rules of the game. 
I like them, but *don't* like `Control.Lens`. I think the lenses themselves are pretty useful, but I prefer to just use them the way one "normally" uses `traverse`, not the way the library encourages to use them pervasively. Maybe I'm doing it wrong, but whenever I try to use `Control.Lens` as intended my code just become very verbose, and not any easier to understand. Edit: Somebody elsewhere in the thread says if they had learned of lens earlier it could have saved them from writing ugly record update code. I can see how lens helps with that specific pattern, but I have never actually found myself having to do that. I think I just code differently from others, somehow. Or maybe the domains I work in just don't benefit as much. I don't know.
How to compose functions like `map`, `fmap`, `first`, etc: map (first f) pairs = (map . first) f pairs left (fmap (second f)) stuff = (left . fmap . second) f stuff This is obvious once you think of these as higher order functions that lift their (first) argument. But you do have to think about it (I did, anyway).
These classes are intended to enable us to handle not only Game monad, but also monad transformers such as `StateT s Game`. The Game monad combines all the features of the classes so it is better to use the monomorphic one. PS. The free-game's approach differs from what [this post](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) pointed out. Because the root of all evil is the *poly-in, mono-out* design, but not *mono-in, poly-out* which is used by free-game. Only the latter gains theoretical sense while keeping convenience since it generalizes the behaviour of ADTs precisely.
In Gabriel's post, the datatypes always have the 'next' param. I see now that its type allows for something like Close; Close. Looks like you indeed need GADTs to change the type of 'next' to Open for Close and vice versa. I'll try to write an example. 
Byorgey's Diagrams is a Typeclass tour de force. Was that a mistake?
"expand the size of the expression" -- what's that?
I think I understand church encoding, but in what cases is it helpful to efficiency?
You don't need the $ in that expression.
The guard evaluates to false, which is why it doesn't match.
Are you saying that it is useful for the typeclass-constrainted type variable to be the return type, but not the argument type? And is that because the caller can supply a dictionary when needed for an argument? But it seems a caller can apply a dictionary to a return value... 
I think the most compelling use case for lenses for the average Haskell programmer is modifying fields buried deep within a nested data type. This is very awkward to do with records.
You can use `reflection` instead an retain the ability to reason about your code.
Recommended reading is https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf though it's a little rough. Here is an implementation of the file-handle example from the paper using edward's indexed library and the `DataKinds` extension (which is based partly on the ideas presented in that paper.) https://github.com/ekmett/indexed/blob/master/examples/FH.hs It is slow since it doesn't go on to make the further transformation the paper recommends in the section on 'speed for free'. The main point is that you can't get back to an IO action except with runFH :: Free FH (At a FClosed) FClosed -&gt; IO a which (I'm going by a memory of how it all works) means basically that whatever transitions are involved in the operation, the handles must begin and end closed. This is similar to gelisam's idea but takes advantage of the DataKinds business. The way the i-monadic operations work is defined in https://github.com/ekmett/indexed/blob/master/src/Indexed/Functor.hs 
`-XKitchenSink` ;D
Mixing Lenses with Traversals is were lens really shines IMO. 
Great work. Thanks Leon!
I believe that joke is originally due to Conor - I think I forgot to say that.
Thanks so much to the Haskell DC group for recording this, and to Ben for doing such a great job putting it together with that viewer! Far above and beyond the call of duty!
It sounds like you're suggesting a `MonadFail` typeclass, which I think is a very good idea.
The introduction says &gt; for us a type is an omega-cpo and we consider only continuous functions between types (categorically, we are working in the category CPO). so although I don't understand the gritty technical details I think you can just read "--&gt; forms the function space D -&gt; D' of functions" (without "continuous") and your understanding will probably be fine.
It isn't `id` in the presence of infinite lists either.
Sufficiently polymorphic code does this. Making illegal states unrepresentable forces you to write the only sensible state transitions possible between valid states. It really does work. We've been working on the compiler for Ermine for a while now, but were force to stop cold in the middle of working on the type-checker for a while by circumstances. After a long six month hiatus we got back into it. We finished it and compiled it, and had it work first time. The scala version required us to fight with it for 2 years and still doesn't work as well. It, however, expresses fewer if any of the invariants about name capture in the types, while the Haskell version required us to get those aspects right to proceed at all.
thank you
I would suppose that GHC is smart enough to notice that foldl is guaranteed to process the whole list, and thus implement it as if it were strict on that list. I mean, it *can* turn on eager evaluation arbitrarily as long as it doesn't make a terminating program non-terminating. Does GHC make this kind of optimization at all?
Thanks, that helps a lot. I think it'll take me a few more re-readings to fully understand, but following along with your example really helped fill in the gaps.
This is incredibly helpful to me (and I'm sure for others as well). Thanks for taking a clearly significant amount of your time to write this.
Sure, though I think I use type aliases a lot more now, rather than new record types or classes.
Those people could stay on 7.6.x until the bug is properly fixed. Until it get fixed, most platforms (linux,unix,osX) would get to play with the 7.8 branch and the ecosystem could be made 7.8 ready (individual packages, platform ..etc). I would prefer that people do that during the rc stage (and my packages have been ready way before that), but truthfully most people use whatever their packages manager provide. Blocking the release at this very very late stage, means that everyone is hurt by this bug(s), for no good reason.
The use of `Set` in the wikipedia definition is just because in traditional/basic category theory our hom-"sets" are sets. You can see this from the three prototypical profunctor constructions: (1) the Hom-functor is a profunctor, (2) for any functor `F` we have the profunctor `F^*(c,d) = Hom(c, F d)`, and (3) for any functor `F` we have the profunctor `F_*(c,d) = Hom(F c, d)`. I.e., all of these prototypical examples use `Set` only because that's what the Hom-functor returns. However, once we move beyond traditional/basic category theory, our hom-sets become hom-objects in any suitably interesting ambient category. This is done to capture the fact that often we have additional structure on the collection of morphisms from `A` to `B`. The big example is saying that each collection of morphisms forms a category (e.g., we consider `Cat` to be a 2-category, where the objects are small categories, the 1-morphisms are functors, and the 2-morphisms are natural transformations). Another extremely common example is additive categories (etc) where we have a notion of morphism addition. So there's nothing wrong with calling `P : Cpo^op * Cpo -&gt; Cpo` a profunctor, it just captures the fact that the space of morphisms between CPOs is itself a CPO‚Äî whereas in traditional/basic category theory we'd just forget that fact and call it a set.
Sure, more type specificity is always nice. But coming from the imperative world ‚Äîwith their notion of what "types" are‚Äî it's very easy to get yourself all tied up into knots: too specific in some areas, not specific enough in others. The ultimate problem here is that in imperative languages you're forced into certain kinds of monomorphism, so you use specific kinds of polymorphism to overcome those limitations. Whereas in functional languages our polymorphisms are different because our monomorphism problems are different. Trying to apply the one's solution to the other's problems is where learners typically go wrong. Hence, beginners often overuse type classes but use them in the wrong ways (thinking they're like OO classes); intermediate users wean themselves off the type classes, realizing they solve a different problem; but then experts start (over)using them again once they run into the problems type classes were designed to solve.
Probably the simplest way to remove `fail` entirely would be to disallow lambda expressions with non-irrefutable patterns. Since lambda expressions can only provide one case branch, any non-irrefutable patterns could result in a pattern match failure at call-time. Since `do`-notation desugars into using lambdas, if lambdas can't have non-irrefutable patterns, then we never have to worry about pattern match failure when desugaring `do`-notation. Notably, this is a syntactic trick (albeit not a terrible one). It does not prevent pattern match failures when using the `(&gt;&gt;=)` operator explicitly, since the function argument need not be a lambda. But also, notably, this syntactic trick means we wouldn't need to alter any of the definitions of `(&gt;&gt;=)`, since the pattern matching is handled outside of that operator's definition.
Well, what do you expect from a platform where the standard approach to problem solving is fixing symptoms (e.g. reboot, reinstall,...) instead of finding causes.
Well, what do you actually want to accomplish? Make it a game that a user plays?
I'm writing up a post on this but it may be a week or two before I complete it. I can summarize it roughly like this: `ghc` can optimize non-recursive code well, but it has trouble optimizing recursive code. Church encoding is one way you can convert a type into a non-recursive type and also convert all recursive functions on that type into non-recursive operations. This greatly improves `ghc`'s optimization behavior and you get other nice properties, such as shortcut fusion, for free.
You should define a notion of game state. Then you should define some IO that involves asking the player about their guesses and then changing the game state in response, and then responding to the player.
Not quite. The class consist of *mono-in poly-out* methods corresponds a sum type, and these essentially fit monadic things. getLine :: IO String putStrLn :: String -&gt; IO () returnIO :: a -&gt; IO a bindIO :: IO a -&gt; (a -&gt; IO b) -&gt; IO b This can be expressed as: data StdIO a where GetLine :: StdIO String PutStrLn :: String -&gt; StdIO () Return :: a -&gt; StdIO a Bind :: StdIO a -&gt; (a -&gt; StdIO b) -&gt; StdIO b In the same way: class Monad m =&gt; MonadStdIO m where getLine :: m String putStrLn :: String -&gt; m () What free-game has is equivalent. The typeclasses form an extension of Monad, and Game is an another representation of IO. A *poly-in mono-out* style can't get along with "constructive" structures, as shown in that post. If you like to write destructive structures (e.g. comonads or accessors), *poly-in mono-out* may be a good idea.
There's always the trade-off of using a total language instead... 
First I'd ask why you ever run into `(reverse . reverse)`? It's a serious question. Depending on why you're running into it, one way to fix it is the time-honored "don't do that". But say, for the moment, that you can't avoid it for whatever reason. In these cases usually what the problem amounts to is the fact that lists aren't the proper abstraction for what you're doing. For example, if your particular lists are guaranteed to be finite, then there's nothing wrong with the rule‚Äî provided it's guarded such that it only applies to your `FiniteList`, `SpineStrictList`, `Vector`, `Array`, `ByteString`, etc. However, none of those types are the same as lists; which is why restricting to them fixes the problem: none of these types can represent infinite sequences. If you look into the papers on stream fusion, a lot of the intellectual novelty there is in figuring out what the proper representation is for the notion of streaming sequences. That is, figuring out what data type enabled the authors to get the rewrite rules they wanted to have. While this was motivated by the idea that lists can capture streams, they had to abandon lists as the actual representation of streams because of analogous problems about the rules not working out quite right when phrased for lists. So the non-evil thing would be to: (a) don't do that, (b) accept the performance cost of doing it, or (c) figure out why you're doing it and how you can capture the necessary invariants which will allow you to use rewrite rules to safely remove it during compilation.
Okay. So you're able to get this: ["a","25 Mar 2014", "123"] That is the goal of HXT. It's not intended to do any more for your problem. So the question is, how do you know that "123" is a number, "25 Mar 2013" is a date, and "john" is a string? If we know because they're always in that fixed order (i.e., a table row is always a string, then a date, then a number), then if we have these functions: readDate :: String -&gt; UTCTime readInt :: String -&gt; Int We just write the partial function f :: [String] -&gt; [Field] f [x,y,z] = [StrF x, DateF (readDate y), IntF (readInt z)] If this is indeed your problem, you'd do better to make a record type which includes those three types (say, name :: String, birthday :: UTCTime, age :: Int) than using a list of Fields, which is more general than you want. But maybe the problem is more general, and we want to choose the type of the field according to what the string appears to be. Then maybe we have functions: readDate :: String -&gt; Maybe UTCTime readInt :: String -&gt; Maybe Int and so we can make a general parsing function, that could perhaps look like this (the order in which we apply the parsers depends on what behavior we'd want) readField :: String -&gt; Field readField x = fromMaybe (StrF x) ( (DateF &lt;$&gt; readDate x) &lt;|&gt; (IntF &lt;$&gt; readInt x) ) Here, I'm using several convenient functions from the Control.Applicative module. If readDate succeeds, we take that; if not, we try readInt, and it that fails too, then we always succeed by reading as a string. And then we write the more general (and total) function f :: [String] -&gt; [Field] f = map readField
I doesn't *have* to be Hxt. I primarily picked it because of the convenience the XPath support offered. As for your suggestion to try the various field types and see which one works, I might end up trying that route. Would it involve pulling in something like parsec or IO though? Thanks a lot for the help!
Thanks for the input! Also, thank you for clearing up the rationale behind HXT's design a bit. I wasn't sure whether I was simply trying to use the library in a way that it wasn't intended for, or that I didn't understand the library. With that said, are there any drawbacks to returning a list of strings, and then making sense out of them outside of the arrow computations? I had figured that I would end up going that route if I couldn't do what I assumed to be the more "proper" route ( that is, similar to how parsec might be used ). Just to clarify, you are suggesting this general approach then? parseHTML :: String -&gt; [Field] parseHTML = parseData . runLA (xread &gt;&gt;&gt; extractData) ... where parseData does the stuff you've suggested. Thanks again! While this all has been very frustrating, this did force me to get to know the Hxt library pretty well... 
Thanks for chiming in. While the example you provide seems clear enough, I'm struggling cognitively with this design decision. What I mean to imply by that isn't that it was necessarily a bad decision, but one which is above my Haskell-Novice brain. I suppose some reading of papers on polymorphism, monomorphism, and the implications of either are in order. Thanks, as that at least gives me a direction to start heading in the pursuit of enlightenment, where as before your post I was completely lost.
Thanks for the suggested reading! I'm slogging my way through it and learning a lot of terminology along with it. And thanks for the repo links, but oh man, so many language extensions to learn. I guess I need to remember that [T.T.T.](http://www.poemhunter.com/poem/t-t-t/) =D
Well, there are some pretty substantial differences in terms of query language, concurrency model, and even data model between each of those databases. My feeling is that database portability is a touch oversold. It is doable, but the intersection is pretty anemic, and there are substantial advantages to committing to one database and making use of features specific to that database, both in terms of simplicity of development and functionality. And what do you really gain for all that pain? That said, I certainly do think there is a place for something like HDBC, and I do hope that somebody will create a worthy successor to HDBC someday. But in the mean time, I'm hoping that the *-simple libraries will create a corpus of work that can be analysed to figure out how to make a better HDBC. (And as an aside, mysql-simple hasn't been touched in several years, while sqlite-simple and postgresql-simple have seen continued development. Like Haskell, PostgreSQL is very feature rich and yet manages to maintain a pretty cohesive whole. I certainly cannot be hobbled by a library that is pretty much set in stone for the time being in my effort to better support that richness.) Interestingly, there's an instance of the expression problem in HDBC versus postgresql-simple. One of the primary goals that motivated the postgres project was to create an rdbms that was much richer in terms of the domains (type + operations + predicates, basically) provided by the database. With the introduction of GiST and GIN roughly 15 years ago, it's even been practical to create some very interesting domains completely outside of the core project. (Look at hstore, ltree, Temporal Postgres, and PostGIS for example... and temporal postgres basically turned out to be a prototype for the new range types added in 9.2, and hstore heavily influenced the jsonb implementation coming in 9.4) And one of the key implementation considerations for postgresql-simple is for clients to be able to add support for those extended types without modification to postgresql-simple itself, and have that support look and feel native. Basically, postgresql-simple can add types, whereas HDBC can add backends. So, how do you create an HDBC that supports extended types? It might be possible, but unfortunately it's a pretty ugly problem as you have to consider all the vagaries of the various popular databases, and try to formulate a higher level interface that encompasses or can reasonably patch over all of them. Actually, something like an indexed class might work here. So basically you'd have something that looks like an MPTC, but it'd be (mostly or fully) abstract, with additional methods defined by the first parameter, e.g. class FromField (* :: Database) * class FromField PostgreSQL a where fromField :: ... class FromField MySQL a where fromField :: ... But I don't know how sensible this construct is, or if there might be a reasonable encoding of this idiom in the existing type system.
&gt; I'll try to write an example. This would be awesome, thanks :)
You are welcome. =)
Seeing Idris for the first time is kind of blowing my mind like Haskell did when I first saw it. Adding invariants &amp; rules about the structure of your data types into the type system itself seems awesome. I was very interested by this quote: "If you want to understand a Haskell program, it's not enough to know Haskell but you have to know the particular dialect of Haskell created by the combination of whatever extensions are at the top of the file." It's something I've felt before. Haskell is slippery and being a slightly older guy I get this fatigue every time I discover that there's a new dialect for me to learn, in a sense I just give up with the newer Haskell extensions. But I see now that these new extensions are Haskell trying to be a bit more Idrisy. So is Idris the new solution (or the winner) because it is taking a fresh look at the unification between the type language and the value language of Haskell or will Haskell be able to clean itself of its dialect issues by absorbing all the ideas of Idris as first class ?
Indent your code by 4 spaces to make Reddit display it properly. Why are you explicitly matching `3` on the left hand side of your equations? Have you tested your code?
 % cabal install cabal-db Resolving dependencies... Configuring cabal-db-0.1.6... Building cabal-db-0.1.6... Preprocessing executable 'cabal-db' for cabal-db-0.1.6... Src/Main.hs:36:8: Could not find module `Graph' Use -v to see a list of the files searched for. Failed to install cabal-db-0.1.6 cabal: Error: some packages failed to install: cabal-db-0.1.6 failed during the building phase. The exception was: ExitFailure 1 
oops. fixed. I'm going to need to make a cabal-check-sdist-is-working tool ;-)
I don't really have an answer to your question, but Haskell does sit in a sweet spot in terms of type inference. (Also, I like Agda too -- there are other Haskelly DT languages.)
Ah, good point indeed. I've been in this situation myself. In such cases I usually include the original license in the module containing the code copied, but I've never thought about implications for the cabal license field. (Now that I think of it, this also makes it hard to comply with the second clause of BSD3, should someone want to redistribute the binaries based on my source code. I wonder what the common practice here is.)
I think the reason for avoiding lists wasn't semantics changing in rewrite rules but having non recursive list generators (the recursion hides in the "driver" function: streamToList). This is so that the inliner can fully inline everything and get a tight fused loop.
And then generalize that to effectful maps aka traversals or lenses.
well, it's not a magic tool. it only get the known information and list them conveniently. The only thing that the tool could do is mention that there's C libraries mixed and that the result is unknown. For the same reason, if someone mix licenses, or doesn't set the correct license in the cabal file, the tool has no way to report the proper thing. Regardless, the tool is not to get an absolute answer as to whether your code is legal to ship, it's just a simple helper.
Great demo, thanks for putting it together. Kind of that kick I needed to take a look at Idris. Looking at some of the examples on GitHub. Not a big fan of all this syntax/dsl stuff built in. Seems kind of, megh, but OK.
&gt; The only thing that the tool could do is mention that there's C libraries mixed and that the result is unknown. That'd be great; even just annotating packages that depend on FFI-libs with an "`(?)`"-marker after the licence would be useful already IMO.
I think the real solution is that when it really matters, consult with a lawyer. Cabal isn't a lawyer and will never replace one. 
In the cast Gabriel mentioned not having found a way for prompt resource finalization, but as of pipes-safe-2.0.2 it announces prompt finalization as a feature. Any comments on this?
Handy, now as soon as BSD2 becomes widely enough available in cabal's license settings, I'll have to tool available monitor my migration.
 import Data.List import System.Random diff_select :: Int -&gt; IO [Char] diff_select 3 = diff_select' 3 ['a','b','c','d','e','f','g'] diff_select' 0 _ = return [] diff_select' _ [] = error "too few elements to choose from" diff_select' 3 xs = do r &lt;- randomRIO (0,(length xs)-1) let remaining = take r xs ++ drop (r+1) xs rest &lt;- diff_select' (n-1) remaining return ((xs!!r) : rest)
I'm looking forward to that.
Is there anyway to download the slides w/out watching the video?
Nice to see another i3 user!
Using `seq` to ensure that they are undefined. that's a novel solution (better than using a catch-all), and at least ensures that if * you add a new runtime OR * add a new command that you will get an error at runtime if you try to call 'go' with one of the new data. Is there no way to get an error at compile time? I suppose you could at least write some quick check property to ensure it can handle all states. I don't know. I really want the compiler to tell me when I add a new state which functions I need to add patterns to. But this might be less helpful than I imagine it to be.
There's absolutely no need to have `fail` in the `Monad` class. You can put it in a `MonadFail` class. The desugaring will the result in something that will need `Monad` or `MonadFail` depending on how you have used this. I've tried this, and it works perfectly.
&gt; You can't implement short-circuiting &amp;&amp; in a strict language without extra language features! It seems like those language features would be about as complex as the ! in haskell to make things strict. In other words, a simple, optional extension.
Heh - I learned about i3 recently from an /r/haskell comment thread :)
I'm looking forward to the large table mentioned at the end!
Is that green paper thorough imploring, or just ordinary imploring?
I agree, it's a shame to lose the ability to add a new constructor and have the compiler pinpoint every single location where we need to add new clauses. How about this instead? go x (Command Close) = P.undefined go x (Command Get) = P.undefined go x (Command Open) = P.undefined It's more verbose, but it does cover all the cases, and we'll get a warning if a new transition is added.
I don't see why it needs to go through IO, parsec might help, I don't have experience with actual parsing in Haskell, though I have in other languages like C.. I am afraid I can't offer much more advice than from a methodology standpoint.
That was actually pretty fun!
Laziness is a lot more than a terseness-booster. It totally changes the semantics of the program. There are lots of algorithms and datastructures that fundamentally rely on lazy evaluation to achieve their asymptotic performance. If you don't use a lazy language then you probably aren't very familiar with them, but it's not like they aren't good or useful. We haven't even touched on composability yet.
List is missing linear typing, 2/10 would not bang
&gt;String concatenation and bad ORM usage are the two common examples brought up. However, beyond these two, what is there? There's plenty. 1. http://pythonconquerstheuniverse.wordpress.com/category/python-gotchas/ 2. http://stackoverflow.com/questions/1011431/common-pitfalls-in-python 3. http://stackoverflow.com/questions/530530/python-2-x-gotchas-and-landmines
For searching for operators (and Haskell functions in general), you should be using either [Hoogle](http://www.haskell.org/hoogle/) or [Hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html), preferring Hayoo because it indexes all of Hackage. Save `&lt;&gt;`, all those operators are from the `lens` package. `&lt;&gt;` is a synonym for `mappend`, which is a generalized `++` much in the same way `fmap` is a generalized `map`. For lists `&lt;&gt;` is equivalent to `++` (the same way `fmap` is equivalent to `map`).
Did many languages ever implement linear typing? I can only think of Clean off the top of my head. (and I might be wrong about Clean)
&gt; Dependent Types Still, in my opinion, an open problem.
Monoid, right. I knew I'd seen that one. Thanks. I'm aware *of* lenses, but that's about it. I saw you used them, so it makes sense that those operators would be part of lenses. I'll have to look that up tomorrow. Thanks.
I hope I didn't give the wrong impression'; I didn't write anything.
[Rust](http://rust-lang.org) is a big one.
There is now! https://github.com/HaskellDC/Meetup-Materials/tree/master/Meetup-2014-03-26
joke unprofessional and in poor taste. 0/10, would not read again.
Hmm, I thought Rust has region analysis (implemented as "borrowed pointers.") Can you back this up with any source?
Try adding `{-# LANGUAGE BangPatterns #-}` to the top of your code, and it becomes much easier to bang.
&gt; Code snippets stolen (with permission! :)) Side question. Does anyone know of a good way of not making this smiley look like it has a double chin?
It's not obvious to me that all of these are good ideas that should be supported in new languages, nor is it obvious to me why these ideas are more deserving of consideration than certain things that didn't make the list. Why isn't support for sockets, or data serialization, or regular expressions on there? Why are exceptions and restarts both worth inclusion?
Can't speak for the OP, but most of the things he lists would be difficult or impossible to implement as libraries. The three things you listed can be implemented as libraries in most languages, though it depends on exactly what you mean. Regexs represented as strings can be done in a library, but regex literals usually cannot. Sockets can be implemented as a library for any language that has a FFI, but multiway select usually cannot. Data serialization can be implemented as a library in languages with a reflection API and an identity equality operator.
If we're going to skip anything that can be implemented as a library, why is support for domain-specific languages and infinite-sized integers on the list? Why are a debugger, an interpreter, a compiler that outputs machine code, and a comprehensive library set on the list? It still seems like the choices of what was included and what was not are arbitrary.
Please don't hesitate to complain in issue tracker about anything you miss from other editors. Regarding splits: Currently each frontend (Vty and Pango) implements layouting independently and the way forward seems to be unifying layout code and making each frontend simpler. Back in 2011 there was some progress[1] in this area, but this work wasn't finished. Patches welcome! [1] https://github.com/yi-editor/yi/commit/77d069a7c4cfc2e81a06ab6ddeabb6d0e55a5f5e
I agree with you on infinite sized integers, but not on DSL, compilers, debuggers. 
use a space. (with permission! :) )
That's an amazingly written article! :P
unsolved problem https://xkcd.com/541/
[Image](http://imgs.xkcd.com/comics/ted_talk.png) **Title:** TED Talk **Title-text:** The IAU ban came after the 'redefinition of 'planet' to include the IAU president's mom' incident. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=541#Explanation) **Stats:** This comic has been referenced 30 time(s), representing 0.2043% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)
Right. Can you say "feature creep"?
I think the things on that list are good ideas *in some context*. I wouldn't want them all in my system language, for example. I wouldn't want the dynamic, type-defeating stuff in my Haskell either.
I have a hunch many of the downvoters don't know the meaning of "bang" in linear logic.
Great job! I am always astonished how easy it is to build something like this in Haskell. As for the "might want a Monad" thing you might want to look into free monads. They are the perfect tool for this kind of job.
Hoogle also indexes all of hackage. Just look in the cabal file and include relevant packages.
Am I the only person that can't resolve joeyh.name?
Well, the problem is that some of these features are orthogonal. Like dependent types and type inference. Also some features might cover the same problem space, so having all of them would only be a source of major confusion.
It did??
Isn't that affine typing?
I guess this has to be read mostly as "please don't make your new language just Java/C/javascript plus your one new pet feature, instead take the state of the art into account" more so than "PUT ALL THESE FEATURES INTO EVERY LANGUAGE".
to guess in which library an operator is defined (to include `+lens` in your hoogle query), look at the import list.
We used the experimental "classy prelude" for a while. I disliked it intensely. One of the most important features of Haskell for me, as a developer, is the huge amount of information you get from the types of each symbol in an expression. It makes every line so much more readable and easily understandable. That decreases the cost of maintaining software significantly. Every bit of polymorphism you introduce decreases that readability. Polymorphism and type classes are important and useful tools, of course, and if you limit them to just when you really need them, they don't significantly reduce the readability of code. But when nearly every commonly used function became polymorphic, the code became as undecipherable as - well, as the code in an OO language. And that's also exactly why I never liked this TDNR proposal.
Agreed. The stated purpose of the proposal is richer namespaces. Creating yet another kind of type ambiguity is just a side effect, but for me one which would rule out using this feature. My counter-proposal to this is: make it easier to create richer namespaces directly by allowing nested modules within the same file. Provide automatic unqualified import of outer namespaces into inner ones, and automatic qualified import of inner namespaces into outer ones. That minor extension would use the existing system of Haskell namespaces to provide a simple and lightweight syntax to create rich hierarchies of namespaces, without the headaches of a new kind of gratuitous quasi-polymorphism.
Exceptions can be typesafe. GHC exceptions just aren't.
True, but many people are still constrained to use traditional distributions, for many reasons.
I totally agree! And if this makes it a little bit easier for someone else, and maybe gives them an *easy* first or second program to write in haskell, like ~/.xmonad/xmonad.hs was for me, it's a total win. btw, http://hackage.haskell.org/package/propellor Re nixos, its nixops is quite interesting for managing multiple systems. I would rather nix used a more capable language though (the scheme fork of nix, guix, has a monad..) If you squint at docker the right way, it's kind of nixos without a single package manager (but still immutable hashed data underneath). It solves most of the same problems as does nixos in a less elegant but less constraining way. I am thinking about using propellor to provision docker images, configuring the system both inside and outside of the container.
&gt; A smartphone's extra features can be ignored without cost Right up until you consider security implications, it turns out.
I respect your opinion, but be aware that in linear logic, the bang ("!") connective makes a resource available an arbitrary number of times; "would not bang" is a play on words, to mean "would not make available for one than more use [because I wouldn't need to use it more than once]".
As a well-specified research language, you may be interested in [Mezzo](http://protz.github.io/mezzo/): linear types, bits of separation logic, race-freedom, controlled mutability.
Sorry, I didn't check the username and just blindly assumed you were the OP. Thanks for the help.
I don't think you meant "orthogonal" but rather "mutually exclusive". In my understanding, orthogonal features are those that don't interact with each other, and thus can be implemented side-by-side with minimal added complexity.
Isn't that just a really big double chin?
On the contrary, I think most of the people that would downvote sexual jokes would do it even if the wordplay is "geeky bad taste" rather than just "bad taste". And I think your own comment comes out a little smug, even though it was probably just an expression of support.
&gt; Dynamic scope seems like it would be really tricky to keep any typing consequences out when you consider separate compilation units. You can just represent a context using a dependent tuple type. &gt;Exceptions are not type safe at all. What about checked exceptions?
I've often wondered how to mix `recursion-schemes` and `bound`. It'd be nice to generate that fold automatically by seeing `Term` as a fixed point of a base functor. In the past, I've just chosen to go with named vars so as to open up type recursion. It nicely lets you annotate your tree with `Cofree` as well.
This is especially true if you want to be able to read others' code since they may choose to use more exotic language features. 
Trotting out one of my favorite quotes yet again: &gt; Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary. ~ r6rs
The tricky part is the polymorphic recursion in bound. You need the "Initial Algebra Semantics is Enough" approach for that, not just a straightforward fixed point scheme.
Yeah, anyone can write toyish scheme or pascal in 24 hours in their basement. But creating a language with comprehensive set of features and "sufficiently smart compiler" requires a large team of talented engineers, a lot of time, effort and money, and backing of a big corporation (java - Sun,Oracle,IBM; haskell - MS; go - Google; Rust - Mozilla, etc) I think this blog post trivializes the process of creating programming languages. I'm picturing in my head a language "designer" (singular!!) who is so clueless in what he is about to embark on, that he scours internet blogs searching which language "features" he should mix in his salad :)) 
Over on HN for the same submission I can win twice that!
Thanks! Typos fixed. Your rewrite of foldTerm is much nicer. So much nicer that I just updated the post to reflect that.
Sounds interesting, perhaps still a bit rough around the edges. I have actually been thinking about something like this too for a while now. Puppet is great compared to no configuration management at all but it would be nice to catch more errors without having to use entire test VMs or worse, deploy untested code to the live systems (workable if the live system is itself not a production system, a bad idea otherwise). Puppet also gives off this impression of being a really bad implementation of a really good idea (with its intermittent failures due to caching errors and whatever causes those empty sub-second runs in the 3.4.x versions to take just two examples of many). One thing in particular I have been thinking about with a Puppet replacement has been multi-level templates, templates where some substitution is done on the master but some is left for the agent to fill in. This would be particularly handy for secrets that really shouldn't be on the master at any point (e.g. passwords generated on the node shouldn't be stored on the master). Facter, Puppet-DB, Hiera, exported resources and in general many things in the Puppet ecosystem just seem to be plain badly designed compared to the quality level of even the average Haskell library.
I tried reading that paper long before I had any chance of absorbing its contents. I'll put it back in the queue and try this as a project when it swings back around. Thanks for the pointer!
**EDIT:** The [rules have been updated](http://blog.helloworldopen.com/post/81408303181/updated-rules-regarding-licenses) (updated rules page [here](https://helloworldopen.com/rules), explicitly stating that contestants retain rights to their code. I retract my cynicism and encourage any interested programmers to compete. [end of edit] 1. Take the money you would have spent on developer salary implementing an AI; 2. Put it up as a prize for an AI contest; 3. Get hundreds of AI implementations instead of the one or two you otherwise would have gotten; 4. Reap the rewards of free work. This contest is brought to you by a mobile game company, folks. Don't let that stop you from taking part if you enjoy it, but just don't kid yourself about what these contests are about. Personally I find it really misleading and exploitative when these things are marketed as coding competitions, when that's *clearly* not the intent of the event.
I'm more interested in the fact that someone is reimplementing [Puppet in Haskell](https://hackage.haskell.org/package/language-puppet). I wish I was still a devops guy instead of just a dev or else I would be using and contributing to this. I prefer Puppet's declarative style.
Thanks Luke. If you're ever curious about the code and have a look, I'd love your feedback. I've already benefited from looking at racemetric, even just the html :), and much more from the design. And I'll be using snaplet-postmark to verify email addresses pretty soon. Any chance you're in NYC for the upcoming hackathon?
DSL notation is kind of a specialised thing, but I have never found a neater way of embedding a language where I could overload the notion of binding. I've found it very useful for playing with things like embedding linear types or resource management, for example. What is it you don't like about it? I am a bit scared of syntax extensions, though. But, again, we've found them really useful for embedded DSLs. Ideally, we'd have as few features as possible to support the nice, readable (that's really important to me) high level programs that we want to write, but no fewer. Maybe we haven't got that balance right yet, but we'd like to have fun working it out :).
Does this help? Done :: Toy b next -- (typing) Toy b next :: * -- (kinding) Toy b :: * -&gt; * Toy :: * -&gt; * -&gt; * Fix Done :: Fix (Toy b) -- (typing) Also, think of `type Fix f = f (f (f (f (...)))`.
[The competition is organized by Reaktor](https://helloworldopen.com/rules), which is a software house. My guess is that Supercell is helping them out / sponsoring the event. As a company, Reaktor is generally held in high esteem, treating their employees well and keeping their customers happy. I'm not affiliated in them in any way, but I'm pretty sure this is not a "grab free work" -thingie. If anything, it is an event held to get more good publicity. Similar to their conference [Reaktor Dev Day](http://reaktordevday.fi/2013/). 
thanks!
Puppet manifests are nice to read and generally pleasant to work with, however I can't shake the feeling that sometimes (e.g. when generating one file that needs information from multiple resources) it would be much easier if they wouldn't mix data and code so much.
A quick check example. Suppose we want to make a unit test library, "QuickCheck", for testing arbitrary user functions, of unknown arity (number of arguments). We can't know ahead of time whether the user will want to test a function like `is_string_reversible(string)`, `are_two_numbers_equal(int, int)`, or `is_equation_true(operand, operator, operand, value)`. Each of these takes a different number and type of arguments. One way to do this is to write a function `bool for_all(property_function, collection&lt;generator_functions&gt;)`, that our users will run over their custom functions, in order to test that the functions behave in expected ways. How do we write implementation code for `for_all()`? We can't call `property_function(collection_of_arguments`, because that's not the same to a computer as `property_function(individual,argument,vector,one,by,one)`. But we can often make use of a builtin function `apply(function, collection_of_arguments)`, such as provided in many Lisp languages. What if we needed to implement `apply()` itself, such as in C and other languages that don't come with it? One way is to loop over the argument collection, currying the values with the property function, reducing the arity until it becomes zero (no arguments). Then we evaluate this closed, constant function, and return the result to the user. This is how some implementations of the QuickCheck unit test framework are written. http://www.yellosoft.us/quickcheck Apologies for using a hodgepodge of C, Java, and Lisp notation :P, and for rushing through this example so quickly.
About 4: do they really gain rights for using submitted code for whatever they want? edit: well "All code created by teams must be licensed with Apache License v2.0.". It permits them to include your code in a proprietary product.. for only $10000 to the 1st, 2nd and 3rd places.
I'd love to actually use it for haskell development. But for that it would have to have a haskell-mode plugin with the same functionality emacs haskell-mode has. 
Why do you care about having a monad in the first place? The system is currently working as is. Other than the fact that a monad is doable in such languages, as it's just about carrying state, except there's no "do" syntax.
&gt; I'm also not sure that I really follow the types and kinds; for example, the type of Done is Toy b next, and the kind of that type (sorry for the weak wording) is * -&gt; * -&gt; *. However, the kind of Fix is (* -&gt; *) -&gt; *, so how is Fix Done properly typed? No, the kind of both is `*`. `Fix Done` typechecks because `Fix :: f (Fix f) -&gt; Fix f`, and `Done :: forall b next. Toy b next`, so the typechecker assigns `next = Fix (Toy b)`(it can do that because `next` can be anything) and `f = Toy b` and everything works out. As for `Fix`, think of it like this: `f a` is the type of one laye of `f`, `f (f a)` is the type of two layers of `f` and so on. `data Fix f = Fix (f (Fix f))` means it has as many layers as itself plus one: that means it must have infinite layers `f (f (f (f....)))`. That allows programs to have arbitrary length here. Because `Done` doesn't actually use any elements of the type it wraps, once you get to a `Done` all the remaining layers get ignored.
You must mean `map (uncurry (+)) (zip xs ys)`. Your function doesn't typecheck.
A publicity event where the only mention of the company is a tiny logo on the front page, and three mentions in fine print? I won't call it impossible, but I'm not buying it. As for Reaktor's reputation: Reaktor is a company which delivers things its clients want. Its client wants a contest hyped as a programming competition, so they're delivering that (and doing well by the look of it). I'm not really sure how their reputation is relevant here. Again, not saying Supercell is evil or anything, but I'll pretty much eat my hat if they don't make a racing game in the next two years (and I say that after that one redditor literally ate a hat recently for making such a claim and being wrong).
You almost never use curry, because Haskellers like to define their functions in curried form. Sometimes you will see uncurry used to map with a function that takes two arguments &gt; map (uncurry (+)) [(0,1), (2,3)] [1, 5] 
Now make it polymorphic. :)
I'm interested to hear about further improvements although I have to say I think `time` is an exceptionally well-designed library.
If I have a datastructure like this data Basic a = Foo a | Bar Char a a | Baz Int then the fixed point (call it `FBasic`) is (isomorphic to) the same datastructure, but where the `a`s are replaced with recursive `FBasic` values, specifically data FBasic = Foo FBasic | Bar Char FBasic FBasic | Baz Int Here's another example data Cell a = Nil | Cons Int a then "`FCell`", the fixed point of `Cell`, is (isomorphic to) replacing occurences of `a` in `Cell` with `FCell`, that is data FCell = Nil | Cons Int FCell And this is just (isomorphic to) `[]`. Neat eh?
right but it also obviously plays on the other denotation of "bang" and the phrase also takes on a different (sexual) character, and the joke is precisely that the phrase can be taken in two ways. as evinced by the pattern of votes here, the haskell subreddit has (and i approve of this) collectively decided that ribald jokes of questionable taste, clever or not, are really not appropriate here. i find this a welcome change from the rest of reddit.
The nix package manager, nixpkgs, works on most linux distributions and is orthogonal to the distributions package manager. Many libraries end up getting duplicated, but other then that there are no additional downsides to my knowledge.
Could you elaborate?
Agreed Puppet's quality is also annoying me.
Author here! I was going to not post this to r/haskell and just maybe leave it around on twitter or something so I wouldn't clutter up precious r/haskell mindshare, but I realized that I would be missing out on an opportunity to receive mass, scathing critique to be able to improve the tutorial (or even rewrite it) so people in the future can not be lead astray by my noobness. Hopefully this is not an abuse of the subreddit! The tutorial was written as a part of a growing movement, I feel, to address to the "Post-LYAH/RWH" audience; there are lots of advanced or intermediate haskell tutorials and questions but noticeably fewer beginner/intermediate transition ones (especially full projects) , and I get the feeling that a lot of people are working hard to fix this space. This is my contribution; the first part covers the State monad and two types of binary tree data types and interface hiding with newtypes. The next couple will go over using Binary to serialize data and Pipes to do streaming file processing, among others. I am bracing myself for your criticism so feel free to not hold back!
It might be that I just didn't find a nice way of doing it, but I found the polymorphic recursion of bound pretty awkward to work with, especially if you are working with algorithms that are more "dynamically typed" wrt to scoping. I'm not sure the bound approach always pays for itself - variable binding well-formedness is just one of many invariants one might choose to encode at the type level, and enforcing it there comes at some cost.
uncurry is pretty common, as others have already mentioned. there are some rare, hacky uses of curry. Usually they come when you see a function that takes a tuple of some sort... like a coordinate or a simple 2-vector, for example, and you want to pass the tuple components in one at a time. note that this doesn't really have much to do with they curried/noncurried dichotomy directly. 
I'm appreciating all the replies, thanks everyone.
What would you change in how this event is organized so that you wouldn't feel exploited like that?
So this is really just timezone handing? time is very carefully designed, I wouldn't dismiss it. Oh, the long discussions we had about leap seconds, I still have nightmares about that. The main thing I think the time library needs is good documentation, with lots of examples. Perhaps some API refactoring, to make the easy things easy, wouldn't go amiss too.
Like, the polymorphic lambda calculus/System-F.
I don't understand how it improves on timeseries and timeseries-olson either.
Ah, ok. It's on my list!
Hmmm.... Maybe it would help to be given a more comparative example. So, :t curry curry :: ((a, b) -&gt; c) -&gt; a -&gt; b -&gt; c :t uncurry uncurry :: (a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c For example, how is map (uncurry (+)) [(0,1), (2,3)] different from map (+) [(0,1), (2,3)] I mean, I understand that you can't just map the (+) operator to a list of pairs, because you can't add pairs. But what is actually going on behind the scenes that allows the mapping of (uncurry (+)) to return a list of sums of the elements in each pair? 
I btw recommend Gabriels tutorial on free monads: http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html It's a real eye opener and I guess everyone will be able to use the `free` package after reading it.
Time zone handling is the biggest _missing_ functionality for me. But the biggest pain point is efficiency. The Integer based boxed data types make it really efficiency unfriendly. Switching to `thyme` or rolling my own data types gives a 10x performance boost regularly. But then I also have to duplicate functionality to not go through the `UTCTime` and `LocalTime` and that just feels wrong. But yeah, you are right, I shouldn't dismiss `time`. And I actually remember how much I liked it when I started using it, how well thought-through it looked. It's just now that I am used to it I notice the deficiencies much more than the good things. So yeah, I want to _improve on_ `time`, not throwing it away! Sorry if I sounded dismissive. Adding some missing functionality, replacing the data structures with efficient ones, some API refactoring and we'll have a perfect time library not just a good one!
There are two things that I was missing from `timezone-{series,olson}`: 1. Efficiency. There is a 100-200 _times_ speed difference between our libraries for my use-case. That means that I just cannot use `timezone-series`. 2. I want a library that ships an up-to-date time zone database with it. So I can use it independently of what time zone files are installed on the system (or if there are any at all). This isn't that important, but still.
Based on this discussion, perhaps a different approach is called for. 1. Incremental efficiency improvements to the Time library. It has fairly good abstraction, as I recall, so we can just alter the data types where appropriate assuming we can preserve semantics. Similarly we can also improve time zone handling. 2. Production of a library _on top of_ `time` that makes "doing simple things easy" but can rely on the robustness and care in the underlying implementation. 
I have tried it but the master/agent part of Puppet is actually a very good fit for the use cases I see. Ansible seems to be designed with more adhoc, manually triggered tasks in mind from what I saw from my brief experiments.
You can certainly do API changes to make easy thing easy. That's what thyme did, and I think that duo is very-very good already. Time is a good foundation and thyme provides a very good and convenient lens based API. On the other hand, where /u/reklao is correct are the efficiency issues. The wrapping and unwrapping done everywhere in the current architecture and the Integer stuff all around results in subpar performance compared to what is possible. This is totally acceptable for normal, everyday usage though. Klao was a bit modest when he was talking about performance and his optimizations. To put it in context, the tz library's utcToLocalTime function that he implemented is sometimes faster now than the one in glibc. (This is possible, because libc is doing a linear search, where tz is doing a binary search.) Once again, this is not important for normal users, but important for us. And we think it's possible to design a library where the API is easy and the performance is right. But we don't know about every use-case and would like to share the design process. We wouldn't dismiss time, we use it regularly where appropriate. 
I tried using bound for dependent types. The problem I found was that once you have type variables, you have to shuffle around indices in irritating ways. For instance, the type of `x` will be either `1` or `2` depending on which position in `Œõa. Œªx:a. f x (Œªy:a. x)` (ie `Œõ. Œª:0. f 0 (Œª:1. 1)`) you look at, but the type of the whole thing shouldn't have something like `1 -&gt; 2`, it should be uniformly `forall a. a -&gt; a` ie `forall. 0 -&gt; 0`. So somehow you have to juggle the interaction between things in scope, since you no longer have trivial scopes. I found it easier to just write an alpha conversion library.
Some things I dearly miss in time that could maybe be added: * an efficient Int64 type for "micros since 1970" -- but only good to the year 31193 :) * a more efficient version of DiffTime that uses the same representation as "struct timespec" * out-of-the box access to the POSIX realtime/monotonic clocks, especially CLOCK_MONOTONIC_COARSE -- there's a package on Hackage to do this but we should have reasonable fallback implementations on Windows and BSD
Uncurrying is really useful for conversion between tuples other data types - or for calling functions which take multiple arguments when you only have a tuple, without the need to use fst/snd or pattern match to unwrap the tuple (which also enables nice point-free style). An example, if I have some coordinates `(232, 320)`, and some data constructor, `Point { x, y :: Int }` - I can use the tuple as the argument to Point with uncurry. These functions are isomorphic: toPoint' tup = Point (fst tup) (snd tup) toPoint'' (x, y) = Point x y toPoint = uncurry Point I don't find Prelude's `uncurry` all that useful by itself because it's limited to duples - but the technique can be applied more generally - as I've used in a question I asked a while ago [here](http://www.reddit.com/r/haskell/comments/1v1bx9/is_there_a_simplermore_idiomatic_way_to_pass/), which allows for uncurrying n-ary tuples, and even applicable to your own data types (since Tuples are just a data type/constructor - `data (,) a b = (,) a b`). By defining uncurry as a typeclass, we can overload it to "unwrap" any type by providing instances for them. class Uncurry f c r | f c -&gt; r where uncurry :: f -&gt; c -&gt; r -- "isomorphic" to Prelude.uncurry instance Uncurry (a -&gt; b -&gt; r') ((a, b)) r' where uncurry f' (a, b) = f' a b -- uncurry a triple instance Uncurry (a -&gt; b -&gt; c -&gt; r') ((a, b, c)) r' where uncurry f' (a, b, c) = f' a b c -- "uncurry" an arbitrary datatype instance Uncurry (Int -&gt; Int -&gt; r') Point r' where uncurry f' (Point x y) = f' x y The `toPoint` will still work the same, but now we can also provide `fromPoint = uncurry (,)`
http://hackage.haskell.org/package/base-4.6.0.1/docs/src/Data-Tuple.html#uncurry uncurry :: (a -&gt; b -&gt; c) -&gt; ((a, b) -&gt; c) uncurry f p = f (fst p) (snd p) uncurry (+) (2,3) == (+) 2 3 == 5 I'll leave the unrolling of map to you
Disclosure: I'm from Reaktor, even though I'm not participating in the competition organisation, PR or anything. We've held very successful internal company trainings called "coding camps" for some years and lately the playful competitions in them have become really awesome fun. This is how the current Hello World Open competition came to be as well. And personally I'm really happy that we can share such a cool thing with a wider audience. I can say from experience that the programming assignment at hand is great fun, even if you don't win :) Reaktor has already gotten a nice amount of media coverage here in Finland because of organising this event. But feels like we're pretty well-known in the Finnish software and design consultancy field already as it is. (It's a small country afterall, so you cannot be around for many years without bumping into a big part of the other programmers working here...) The way I see this competition is that here we have an excellent opportunity to gather more fame also in other countries, especially when hanging out with such a cool brand as Supercell, which is internationally far better known than Reaktor -- at least for now ;) I doubt that the code created in the competition would be directly usable in game development, but some ideas that are implemented in the solutions could be I guess. Then again I don't know anything about game development, I'm more of a pale serverside nerd type. This is just my personal view on the matter -- if you want to get an official statement, you better contact the organising people via the website or Twitter: https://twitter.com/HelloWorldOpen .
I remember that post. I'm excited to get further along and do system F and maybe even some dependent typing. I will be revisiting your gist when I get there.
I've never used Thyme, but if this is indeed true I'm sure it's because it was closely inspired by `time` :) I wouldn't be surprised, in fact, if significant performance improvements could be made to `time` whilst keeping the same API.
I seem to recall some copyright issue with olson. Is that still a problem?
&gt; the tz library's utcToLocalTime function that he implemented is sometimes faster now than the one in glibc Nice!
&gt; Sorry if I sounded dismissive. Saying `time` is "mediocre" was a bit hard to hear, because I think it's actually designed really well, but in general I think it's great that you (amongst others) are giving new impetus to this area! There are clearly some pain points for you and fixing these will help us all, so thanks for your input :) 
Maybe the choice of words was a bit unfortunate, but you have to take into account, that we're not in the US. If an API states that a timezone is nothing more than a time offset and that converting between UTC and LocalTime is not pure, then we will call it 'mediocre'. Europeans... :)
Did I post it here? Hm. I don't remember doing this, but ok. :)
Author of those libraries here. Those libraries are designed to be a correct representation of timezones, easy to verify that they are correct (without relying on lots of unit tests), and fast enough for most normal use. This fits in with the philosophy of Ashley's excellent time library. I think they achieve those goals quite well. However, if you need to do billions of timezone calculations, or if you need to process thousands of them in a context where differences of microseconds make a difference such as in high-speed finance applications, you need something faster. In that context, you are willing to use optimizations which might significantly obscure the code, and you are willing to rely on unit tests to verify that your more obscure code is still correct. That is a very valuable contribution. The need to rely on external Olson files is also sometimes quite awkward, and I'd be very happy to see one or more alternatives. However, I disagree that these new libraries are *replacements* for the existing ones. They are alternatives for specialized use cases. For all of my own applications that I have used time and timezones for so far over the years, I would not have considered using them. On the other hand, I wouldn't be surprised if I do need them some day, and I am thrilled to have them.
As you can see from the type of uncurry it takes two arguments, a function and a tuple. So when you call uncurry (+) it creates a new partially applied function that takes a tuple and returns something. :t uncurry (+) uncurry (+) :: Num c =&gt; (c, c) -&gt; c So you are really mapping a function that takes a tuple and returns something onto a list of tuples, in this case the result of the addition. If you look at what mstksg posted with the code to uncurry it should make sense.
1. Efficiency improvements to the time library make sense if they do not obscure the clarity of its correctness. Currently, you can verify that most aspects of the time library are semantically correct just by examining types, and the rest by immediate inspection of simple code. I wouldn't want to see fusion/vector/CPS/etc. optimizations creep into the time library. It's plenty fast enough for most uses. If you need something faster for your application, it's great to have other libraries such as thyme or these new libraries. Those can be verified against the time library for correctness. It would be great if their APIs are as close to the time API as possible to make that verification easier, but of course some variation will be needed. 2. There have been many efforts for API wrappers of time that are "simpler". I have never seen one that is not subtly wrong in its semantics in some ways. I believe that the time library API is very hard to improve on while remaining semantically correct.
Most of your questions have already been answered, but here's a simple spot that doesn't seem to have been covered: &gt; However, the kind of Fix is (* -&gt; *) -&gt; *, so how is Fix Done properly typed? `Fix` exists at both the value level (as a constructor function) and at the type level (as the name of a type). It might be easier if you rename the constructor to separate the names more clearly: data Fix f = MkFix (f (Fix f)) test = MkFix (Output 'A' (MkFix Done)) Now we can derive the type of `MkFix Done`: -- constructor MkFix :: f (Fix f) -&gt; Fix f -- constructor Done :: Toy a b -- instantiate "b" to "Fix (Toy a)" Done :: Toy a (Fix (Toy a)) -- instantiate "f" to "Toy a" MkFix :: Toy a (Fix (Toy a)) -&gt; Fix (Toy a) -- apply MkFix Done :: Fix (Toy a) 
Well, I'll just remind you of all the pain we had dealing with name capture issues for binding groups when working on scala version of ermine. By way of contrast, when Dan and I finished Haskell version of the code using bound it worked flawlessly from the first time. We pretty much had to do all the reasoning about it from scratch because of the new dictionary plumbing, guards, pattern compiler, etc. so this wasn't just due to lessons learned. I'm not saying bound is perfect, but if you are willing to contort your thinking to fit it, and work sufficiently polymorphically, it forces you to deal with most ways your code could possibly go wrong.
c2 is not correct - it depends on the DTD you are using. The XML standard definitely provides for elements that are required to be in a certain order. 
http://www.reddit.com/r/haskell/comments/2019i4/comments_wanted_alpha_conversion_and/ Unless you meant something different?
Right, but too many people use this sort of reasoning as a justification for making crappy, underpowered languages. For example, Go.
Full disclosure, I'm an employee at Reaktor and part of the HelloWorldOpen organization. trolox. Can you explain to me how getting access to thousands of probably undocumented and messy AI implementations is a solid business plan? It makes a nice story but just image the pain of having to go through all those repositories. Too much work for no guaranteed results. If I was working in a gaming project I would hire an AI specialist for a few weeks. Guaranteed results, better performance and a lot cheaper.
I can definitely recommend xml-conduit. We use it extensively. It's easy to write and maintain parsers and renderers, and it produces fast code. If you deal with XML structures that are somewhat complex, you may want to look at the XML cursors that xml-conduit provides. They are very powerful, but they do have a learning curve.
I don't think anyone's saying "ease of use is all that matters" or is even the primary consideration.
I worded it like that because he seemed to be ignoring how the API looks for whether the test cases pass. Final programs can do that but libraries need to worry about their API. Also lots of hand waving "its not that hard" going on.
The immediate thought I had on seeing the title here was "unsafeInterleaveST". However, if you are careful, it seems to me that lazy ST should really do the same thing. Have you looked at using lazy ST in combination with a strict writer?
I'm not sure if I propose making the library simpler. Rather, perhaps, a more expressive grammar of things that you _can_ do, with more special purpose functions wrapping common tasks? I.e. functions for date arithmetic, perhaps different modules exporting the same set of functions but with different uniform semantics, etc. My understanding is that the `time-lens` (http://hackage.haskell.org/package/time-lens) package is a definite step in this direction.
The typical caveat about `unsafeInterleaveX` is that it means the `X` effects may be lazily deferred. This causes reasoning and safety issues like the troubles with lazy IO. See more (scary) details here: http://www.haskell.org/pipermail/haskell-cafe/2013-April/107547.html In any case, `pipes` is a good method for avoiding these pitfalls much like it can be used to better model lazy `IO`. instance STW (Producer w (ST s)) where type Yield (Producer w (ST s)) = w type RawState (Producer w (ST s)) = s yield = Pipes.yield readArr arr i = lift (Array.readArray arr i) writeArr arr i v = lift (Array.writeArray arr i v) newArr ix def = lift (Array.newArray ix def)
If you found a bug in timezone-olson, please report it. If there are unit tests that you want - patches are gratefully accepted. :) Slower? You cannot improve on a human-scale time impact of zero in my programs. Timezone calculations just aren't the bottleneck in most cases. But if you do need those optimizations - and some people definitely do - then great! This is very different than the situation for, e.g., the containers library, where any tiny difference in performance will likely have a significant impact on many programs. The main issue in a time library is semantic correctness. Most time libraries are built on a semantically incorrect and incomplete design, and as a result there are many buggy programs out there. Haskell's time library gets it right. There is huge value in being able to see those semantics clearly and directly from the type signatures and from simple, straightforward implementations of functions.
`unsafeInterleaveST` is often less safe than `unsafePerformIO`. https://www.fpcomplete.com/user/edwardk/oblivious/deamortized-st
It had linear implicit parameters. And they were crazy.
Is there any reason to think the BSDs wouldn't have such POSIX clocks? It seems like an odd thing to suggest, the BSD's are usually better are keeping to the POSIX standards than the linux world.
Indeed. I find go's lack of generics disturbing.
Others have answered better than I can. I just want to add that the notation really is confusing, you aren't dumb :-) Haskell notation tends to use "A B C" to mean "A and B and C" are related, but the structure of that relationship differs depending on whether the expression is on the left or right side of "=" sign, and whether the equation is defining a function or type.
Lots of good answers here but we haven't heard from OP yet so I'll try to guide your intuition. Remember that `Fix` is used here as both a type and data constructor. I'll rename the data constructor to `Wrap`. data Fix f = Wrap (f (Fix f a)) `f` has kind `* -&gt; *`; let's assume it's a `Functor`. Think of a functor as a *value* with some *context*. In this case, `f`'s *value* is yet another `Fix f a`, but it carries a `context` as well: namely, it's a `Toy b`, so it has the shape of either `Output b`, `Bell`, or Done. The mental hang-up you are having may be due to the fact that `Fix` seems to be frustratingly recursive and yet do "nothing at all", if its value is just another instance of its own type. Remember that the `f (Fix f a)` inside `Wrap (f (Fix f a))` carries its own context. In this way Gabriel shows how to write an interpreter for a free monad, which involves unwrapping two layers: the free monad itself (`Fix` or `Pure`), and the functor underneath's data constructors (`Output`, `Bell`, etc).
I just tried that, it doesn't seem to produce any output until the entire list is ready (which leaks memory, as you might guess!)
I would bet there are a number of people who want to write and/or modify autocomplete/snippet/multi views/automate x/etc in the editor they use. However they do not want to spend the resources to lear elisp or vim script to do so. Even if they do they would rather do the automation in Haskell for the same reason they choose it over elisp or other languages in general. 
That seems to lose the whole point of ST, letting you encapsulate your pure algorithms that rely on mutable state. Still, thanks for the enlightening reading--I'd been meaning to look into a couple of the new iterator libraries (pipes, conduit, etc) and this was a good dive for me.
&gt; What is wrong with defining an answer to these questions and using that? It matters not what your answer is so long as you are consistent about it. The problem is - how do you know, and how do we know, that you have asked and answered *all* possible questions explicitly in your documentation? In fact, you almost certainly won't, because it would make your documentation convoluted and difficult to use. And how do you decide what is "consistent"? There are many different ways to be consistent. In many older programming languages, what you describe was the only way to proceed. But in Haskell, our expressive type system gives us a better option. We can describe the relationships between all the components of time exactly, and all of those questions fall away. &gt; And if all I want to do is flag something to happen a calender month from now it is a lot easier to do. Perhaps you feel that it's easier, but that's subjective. (Someone here spoke about "hand waving"...) Personally, I wince when I need to go back to older style time libraries in other languages that are more like what you are describing.
Yeah I thought about writing a solution like that using the product of a `Writer` and a `ReaderT` applicatives. The reason I didn't end up commenting that on the original Reddit thread was that this has a very different performance profile compared to the original, since you now need to do the key-&gt;value lookup on the client side as well, not just on the database side.
Here's an updated `testP` function for your use case: https://gist.github.com/9906683. Running `testP debruijnM 10 9` writes out the results as soon as they are computed.
Yes, the solution with the Lazy ST monad is salvageable. The solution is to combine `pipes` with the lazy `ST` monad, like this: {-# LANGUAGE RankNTypes #-} import Control.Monad (forever) import Pipes import Pipes.Prelude (toListM) import Control.Monad.ST.Lazy import Data.STRef.Lazy purify :: (forall s . Producer a (ST s) ()) -&gt; [a] purify p = runST (toListM p) example :: Producer Int (ST s) r example = do ref &lt;- lift $ newSTRef 0 forever $ do n &lt;- lift $ readSTRef ref yield n lift $ writeSTRef ref $! n + 1 -- This runs in constant space main = print (purify example) The reason your initial attempt to use `WriterT` leaked space is because `WriterT` has an implementation of `(&gt;&gt;=)` notorious for leaking space no matter what you do. The reason your `ContT` version eliminated the space leak is that you essentially replaced `WriterT`'s `(&gt;&gt;=)` with `ContT`'s `(&gt;&gt;=)`, which does not leak space. You may also want to read [this post I wrote](http://www.haskellforall.com/2014/02/streaming-logging.html) which goes into detail about the advantages of using `Producer a` instead of `WriterT [a]`.
Thanks Tekmo! I see where the space &amp; time leak leak is now, actually--your comment about ContT cleared it up. ContT right-associates all the binds (something I well know from working on `Prompt`), and left-associative bind causes `Writer` to leak because it leads to left-associative calls to `++`, just like the earlier post where a user was having problems with his mesh-gluing loop that used `foldl`. If I understand correctly, I could have used WriterT (DList Integer) as well, which also would take care of the left-association problem, or just ContT + ST Lazy to reassociate the binds in Writer.
These all seem like type gotchas rather then performance or memory gotchas, which is what the context was.
OP, take a moment to read up on Haskell namespaces, and then rewrite the definitions so that no two different entitiesuse the same name. Then look at all again. To get started: FixT f = Fix (f (FixT f)) And think of it using a more clear notation: "FixT is a concrete type constructor that constructs a type when given an argument type that matches the variable 'f'. Values of that type (for a given concrete instance of the variable 'f') have the form: Constructor named 'Fix', applied to a value that conforms to type 'f (Fix f)'. Thus, a valid instance type for 'f' must have kind '* -&gt; *'. 'Done' has type 'Toy b next', so we can use f='Toy b' and '(Fix f)=next' . 'Done' doesn't construct using any value for 'b' or 'next', so we get to ignore any potential trouble about what the seemingly infinite value '(Fix f)' might have to be-- it doesn't have to be anything, it no value is ever put into the program! That is a bit of sleight of hand. The key to the magic is that a type variable can be part of a type constructor, but that type can be completely ignore, not 'undefined::b' -- actually absent(!) in the value construction.
Supposing that `tz` is finalized and mature in a year or two, would be be likely to see it come packaged in the Haskell platform?
I'd just be happy with 7.8.1 to be released now :p
I'm not sure. When I initially wrote that I thought it was leaking for a different reason (demanding all results until the very end) but in retrospect I couldn't have been correct because then you would have never gotten any results. I think your interpretation (right-associating `(++)`) might be the correct reason. `DList` would definitely be the way to test that hypothesis.
Aw, I didn't see anything about including quasi quoted JavaScript compiled to a referentially transparent Haskell function.
At HN, the submission mentions the total prize money of 10000‚Ç¨, the winner get 5000‚Ç¨.
Thank you for sharing. Very nice :D Aside from some minor points (like not using your own type synonyms, for example Row) I like your style.
Good sir, may I interest you in [ansible-pull](http://www.stavros.io/posts/automated-large-scale-deployments-ansibles-pull-mo/)? No daemon involved, only git and cron necessary. As usual with Ansible, no reinventing the wheel, just apply grease where necessary.
It was about around the fourth one that I realized it.
Racing game AIs are supposed to be fun and varied. Why would we need a handful of AIs that are all better than humans? GPL is a fair suggestion though. We'll discuss about that today, thanks :)
First rule of Reddit: do not believe to anything posted on April, the 1st :P
your cabal file name reminds me that long live haskell.
Currently working on a vb.net app. The `-fon-error-resume-next` flag joke made me sad :-(
you import Control.Lens, but don't make use of it. You could get rid of some basic functions like `tileIntFunction` or `setVal`. Maybe I will try to use Gloss some time. It looks great.
Lost it at clopen typeclasses.
whenever i see the a priority queue or similar things implemented in tutorials i am left wondering why there is missing the paragraph telling me, that it's only for illustration, see the containers package for the most performant implementation. why isn't a heap in containers? doesn't it fit in there (pun intended)?
Oh yes, sorry! I've been meaning to report it to you after I found it, I'll send you a proper bug report soon. But briefly, while unit-testing `tz` parsing and comparing the results to `timezone-olson` I found that `timezone-olson` parses most of files wrong. Because it parses both the ver. 0 part and ver. 2 part and merges them together (which is btw. unnecessary), the resulting default time zone in the `TimeZoneSeries` is wrong. Consequently, you get wrong results for dates far back in the past. I have to say, I strongly disagree with your views on this. You present a false dichotomy between semantic correctness and clarity on one hand and efficiency on the other. It's false in both ways: 1. Efficiency doesn't have to sacrifice clarity. It mostly means that you have carefully think about representations, use-cases and have to measure things. Quite often efficiency considerations can result in clearer code! And efficiency clearly doesn't have to sacrifice correctness, why would it?! The premise is that you are looking for efficient solutions _within_ the space of correct ones. 2. It's an illusion that simple code means correct code. That you can just inspect the code in the `time` library and just see it as correct. In some special domains this is the case (notable example /u/tekmo's core of the pipes library), but time and time zone files and the like are way too messy for this being "obviously true". You have to work for it; think about what needs testing and how. And if you don't do it, you'll miss stuff.
http://hackage.haskell.org/package/control-monad-exception
Well for example CLOCK_MONOTONIC_COARSE is called CLOCK_MONOTONIC_FAST in BSD -- otherwise they might be able to share an implementation. Windows probably has similar APIs to bind to, my point was that this should be abstracted at the Haskell level if possible.
Hm. I did mention right away that there are better and more optimized priority queue libraries. I can't claim to know why there is no priority queue in containers, but to me it feels like it would stand out from the rest. Containers has efficient sets and queues and stacks and maps and rose trees and graphs, but something like a priority queue seems like it'd be the odd one out of the bunch to me... but that's a pretty cop out answer. I guess the others are sort of "passive" data structures in terms of their higher level interface? But I do understand that containers probably wouldn't export a heap, as heaps are only a means-to-an-end intermediate kind of data structure, and do not offer too much for being manipulated directly. But I do understand you meant to ask about priority queues, and not actually about heaps. 
How exactly would it work? Wouldn't the `m (a, w)` in the standard `Writer` cause problems (in contrast to `(m a, w)`)? 
/u/sclv is at it again. [Last year's](http://www.reddit.com/r/haskell/comments/rnwgn/haskellcafe_a_modest_records_proposal) was actually a link to a cafe post from April 1 of the previous year.
unfortunately there is much to be desired for ansible as well. i am using it pretty heavily as sysadmin and, for one, i would love some static guarantees before running the ansible playbooks. that you cannot `include` a task multiple times (using `with_items` et al), is also a downer for certain tasks that makes modularity hard to express. there are many other things not to like, but it's still way better than the others. (i used all of cfengine, puppet and some homegrown tools extensively)
&gt;You can just represent a context using a dependent tuple type. But then your tuple has to be so generic as to not really be providing much useful type information, right?
also see tsac, the tokyo society of application of currying. but do not get confused by the similarly named tokyo sch√∂nfinkel appreciation club. http://www.starling-software.com/en/tsac.html http://www.cynic.net/tsac.html
Sorry, I don't understand what you're saying.
you are (of course) right, that you mention it. it was more a general remark. in a way haskell packages don't get stale, but the [priority queue you mention](http://hackage.haskell.org/package/PSQueue) has not been updated the last 5¬Ω years. it might still be fast (it unpacks the fields in the internal representation), but you don't get the idea when only shortly looking at it.
This is a talk about the [api-tools](http://hackage.haskell.org/package/api-tools) library. That DSL, targeting JSON, is similar to some other DSLs such as: * [persistent](http://hackage.haskell.org/package/persistent). persistent is a more SQL-table-like data DSL, but it targets both SQL and JSON based database backends. Many specific databases are supported. If you are using this kind of DSL to create web routes, you may also be interested in: * [yesod-routes](http://hackage.haskell.org/package/yesod-routes), a DSL which associates data types to REST API routes (Note that yesod-routes does *not* depend on any part of the [yesod](http://hackage.haskell.org/package/yesod) web framework. The opposite is true - yesod uses yesod-routes for specifying and generating REST APIs) * [wai-routes](http://hackage.haskell.org/package/wai-routes) which generates REST API code for [wai](http://hackage.haskell.org/package/wai) from yesod-routes specifications. It is interesting to explore how these DSLs can interact and combine with each other. This Well-Typed talk seems to be about generating code for a REST API directly from an api-tools data specification. It might be a better idea to generate a yesod-routes specification, which could in turn generate various kinds of API code. Some other ideas: * Generate a REST API from a persistent specification * Integrate data-api with persistent, for automated marshalling to and from a database * Combine yesod-routes with api-tools or persistent in some way, so that you can customize or override the route structure that is generated from the data specification EDIT: You can also think of [acid-state](http://hackage.haskell.org/package/acid-state) as being an EDSL that does something similar. In acid-state, you describe things more from the Haskell side than from the database or API side: you define Haskell types and functions, and then the database schema and code are automatically generated. Whereas in persistent and api-tools you use the DSL to describe what you want the SQL or JSON schema to look like, and then the Haskell types and functions are automatically generated.
there're still PLeaf leftovers on the first page (should be PTLeaf, right?)
I got it only when they have mentioned `-fvia-s3`...
And already started to Google about D-Wave quantum computer to purchase it "for home hacking"
&gt; Saying ‚Äúfrom the left‚Äù or ‚Äúfrom the right‚Äù is a description of what foldl and foldr calculate, with the parenthesis nesting to the left or to the right. At runtime of course we always have to start from the left (front) of the list. I think this is better explained as "associating to the left" or "to the right"
If Duncan is suggesting this, then I think we should take it seriously. To be fair, though, here are a few mitigating points for `foldl`: * Simple cases, like sum, are now caught by GHC with the standard -O optimization setting, are they not? * The Haskell 98/2010 spec requires `foldl` the way it is, not foldl'. Normally we try to be compatible with the standard by default, with digressions turned on using a pragma or option. This is in a library though... So this is part of the more general discussion about how we should go about improving the Prelude, which is well overdue in a number of ways. * The extra laziness provided by `foldl` for `sum` is different than what `foldr` provides, as becomes clear from the discussion at the end of the post. So it's not exactly fair to say that if we really wanted laziness for `sum` we should have used `foldr`. But all of that is moot, really. Because in reality it is extremely rare for anyone ever to need any of those kinds of laziness for `sum`. * A more moderate proposal could be just to re-export `foldl'` in the Prelude. Beginners would still scratch their heads over the prime, but at least you would avoid the need for the import every time. All that said, I am still +1 for changing `foldl` to one of the stricter versions as Duncan proposes.
A bit serious given the date? I just read the whole thing looking for the punchline!
That D-Wave "for home hacking" still costs even more than a 3-D printer. Man, I have to speak to my boss, I didn't realize how far behind my salary is lagging.
If it was meant as a joke, we would be dealing true poetic justice if we actually implement it.
And thus you've been truly pranked: put out, searching for a joke that is not there! Oh paradox!
&gt; So if we‚Äôre going to fix `foldl` to be the strict version, then perhaps it should be the fully strict version, not just the ‚Äústrict after the first iteration‚Äù version. The containers package has had a fully strict `foldl'` for ages and no one has ever complained. I've talked about the "lazy base case" problem in at least one of my performance tutorials. It makes a practical difference. For example, I remember the performance difference between lookup :: Ord k =&gt; k -&gt; Map k a -&gt; Maybe a lookup = go where go !_ Tip = Nothing go k (Bin _ kx x l r) = case compare k kx of ... and lookup :: Ord k =&gt; k -&gt; Map k a -&gt; Maybe a lookup = go where go _ Tip = Nothing go k (Bin _ kx x l r) = case compare k kx of -- 'k' strict due to 'compare' ... being about 10%. I think we should change the behavior of `Prelude.foldl` to the fully strict version. 
native haskell in vim will _not_ happen. do not look in vim's code even though you will find the answer why there. if neovim takes off, it will most likely be easy to extend (neo)vim with haskell libraries.
but not, when your list of variables is dynamic (or even listed in the inventory and not the play). the following does _not_ work: tasks: - include: wordpress.yml user={{item}} with_items: "{{users}}" edit: i also don't know anything that is better. unfortunately.
http://paczesiowa.blogspot.com/2010/01/pure-extensible-exceptions-and-self.html
You're right, there are certainly a lot of possibilities to explore. Thanks for the great summary! &gt; This Well-Typed talk For the sake of properly assigning credit, I should clarify that Chris works for Iris Connect - though those of us at Well-Typed have been helping out. Perhaps my post should have been clearer.
Thanks for the feedback. Gloss is great. The paradigm is really nice (initial world, function to step world, function to handle input, function to draw world state). That said, it is a bit limited since it just does vector graphics.
I just hope this Haskell April Fool's doesn't come true. [Like it did last year.](http://www.reddit.com/r/haskell/comments/1bfojn/functor_is_now_a_superclass_of_monad_in_ghc_head/)
I kept meaning to look at this. Pretty cool.
"Connected Type Families for Free!"
That makes sense to me now. Thank you for taking the time to explain that.
The issues tracker over at GitHub was turning into more of a Q&amp;A session than an issues tracker, so I threw something together to serve as a better focal point for general info and discussions.
DSLs when embedded typically requires a flexible syntax. Infinite-sized integers in principle can be a library in any language but have you tried using a multiprecision (e.g., gmp) library in C? Or Java? If you're not going to support fancy syntax overloading, large integers are nice as a first class thing, which is why I'd guess they're listed here separately.
You're probably right, but also remember that what most people call type inference isn't what you're giving up with dependent types. I write Agda for fun and Scala for work and am forced to write a hell of a lot more type annotations in the latter than the former. Hell, the former doesn't even have built-in type annotation syntax! Fancy unification does wonders!
I think the only point was to show potential designers the breadth of language design. If anything I see it as discouraging people from just taking their pet language, changing one piece of surface syntax on it, and calling it a new language.
I know at least a handful who know that and still chose to downvote. Taste over artificial in-joke elitism.
Uniqueness typing is a (more pleasant) form of linear typing. I don't have the distinction super crisp, but it's something about being able to treat non-linear variables as linear. I think uniqueness prevents that and regular doesn't. Or something. I'm on my phone but Google knows more :)
I don't think any of those other tools handle schema migration which is pretty fundamental to API design. Acid-state is close but it does not model the change explicitly. Api-tools looks interesting but it is when this stuff is combined with a deployment system that real progress will happen. When the deployment tool can detect the order services need to be update in, inconsistencies in a production environment etc then it'll be huge.
For me it was at -XImpossibleInstances. I had my suspicions at -fno-state-hack and -fmore-state-hack but wasn't sure...
&gt; First rule of the world FTFY
If you want to have a honest discussion about it, maybe you should start by asking the users a list of problems, instead of enumerating yourself a list of problems (good !) which you almost defend straight away as features ! (I'm glad to hear that apparently it's down to me not being able to read type signatures and haddock) Considering how many people have either implement their own thing, or thought about it (including me), I'm certainly not the only one to have problems with time.
If the spec is wrong then we should change the spec. Isn't this what we have a committee for?
Stongly?
Good points, I'm still against the proposal. It breaks symmetry with foldr. Symmetry is good for instruction, and also reduces mental load for experts. BTW, I also don't find your "better" implementation last to be better. I prefer code that uses a recursion scheme (e.g. foldl or foldr) instead of explicit recursion, so I prefer the version you have that uses foldl. For this reason, I'd not want to be without a lazy foldl in base, so I'd at least like to see this proposal include a new name for the old foldl. I would be in favor of a foldl' in Prelude as part of Haskell 2015, as well as other, coupled changes (e.g. sum/product definitions), to make it suitable for use in instructive material before modules and imports are discussed.
i was not aware on api-tools. it looks nice. two things. first haddock is not generated, i am looking into it and will (when successful) do a pull request. **edit:** well, it generates fine on my machine... second, it was hard browsing the changes you made, because you have a spurious `-n` as first line in (all of?) your commits.
http://www.stephendiehl.com/llvm/
1. As long as correctness is preserved, I don't think it necessarily has to be clear. If there's a real correctness&lt;-&gt;efficiency trade-off, I prefer the library in the platform (time) be correct. Otherwise, I'd expect the package to be both correct and efficient, even if that means it no longer have a simple interface. (More likely, we can use a facade pattern to provide a simple, if not quite as efficient interface, too.) 2. Agreed. Time is one of those things that programmers wish was simple, but it really, *really* isn't. I'd rather our core libraries err on the side of correctness, but I've been scarred by having to write PHP in the past.
I guess Apache License has been chosen just to make organizing more simple, because then there's no need to think about what exactly constitutes as commercial use. I mean, if this event is considered promotion for the company, isn't that of commercial interest? IANAL, but there might be some restrictions that haven't been considered. Putting Apache License is just simpler, and there's no need consult anyone. edit: [GPL is now accepted](http://blog.helloworldopen.com/post/81408303181/updated-rules-regarding-licenses)
I would expect a baseline universal time type without access to helper functions. But that is typically the easy part so not too interesting to discuss. Fundamentally necessary for certainty f results though.
Product, not monad. It's isomorphic to AnnotatedT.
Internet is unusable one day a year. Sigh. 
Haskell's extensions are solving a bit of a different problem. They're getting you a lot of the way to dependent types, while maintaining nice properties like lots of type inference, predictable erasure, and compatibility with existing features like type classes. These properties are very useful, and languages like Idris don't have them. You can definitely get a clearer view of how certain features of new Haskell work by learning something like Agda or Idris, but I don't think that Haskell can become Agda or Idris and still be Haskell in a meaningful sense.
Just a warning, though - the slides cover a fairly small part of the talk. Most of it consisted of various demos, which probably don't make sense without the actual demonstration. I don't really design materials for later perusal out of the context, because I think it makes the talk worse.
&gt; I also don't find your "better" implementation last to be better. I prefer code that uses a recursion scheme I'm in your camp on this point, although the issue is easily sidestepped by keeping the original `foldl` as `foldlNonStrict` or similar EDIT: or using this observation: http://www.reddit.com/r/haskell/comments/21wvk7/foldl_is_broken/cghe92q
Thanks for the kind words on the demo! Like kamatsu said, Agda may be more to your liking. Agda and Idris are close cousins, and many techniques and skills will be transferable. 
Chuffed to bits with this!
Took me until the nodejs bit to realise. (Having said that I can imagine a few people who would say that with a straight face) 
One can happily use `foldl'` to implement `last` &gt; data Thunk a = Thunk { runThunk :: a } &gt; let last = runThunk . foldl' (\_ y -&gt; Thunk y) (error "empty list") &gt; last [1,undefined,3] 3 I think this is really nice, actually.
Indeed. If you're going to make the best language possible, the problems in the spec must be addressed and rectified. There are a lot of fun aspects to JavaScript, but it's buried under a mountain of shit because no one wants to break the standards. Might be slightly hyperbolic, but it seems that the point of Haskell is to make the best language possible, and I think that means that sometimes the spec needs to break in order to make things better for the language going forward.
So you think IO manager on top of node.js is a good idea? // I actually only realized 1st April on that one
ah, i see. _if_ it is acceptable, to rewrite git's history, you can run a script that converts the commit messages into the proper format using `git filter-branch --msg-filter` and a suitable shell script (well, can also be haskell).
&gt; Good points, I'm still against the proposal. It breaks symmetry with foldr. Symmetry is good for instruction, and also reduces mental load for experts. I like your objection! Let me explain it like this... Suppose that we could access the back of the list just as easily as we can access the front of the list, like an array. In that case we can implement all four folds, yes **four**: * `foldl` -- lazy left fold * `foldl'` -- strict left fold * `foldr` -- lazy right fold * `foldr'` -- strict right fold It turns out that `foldl` and `foldr'` are best implemented by starting from the back and doing a reverse traversal, while `foldl'` and `foldr` are best implemented by starting from the front. You can see this if you go look at the [vector](http://hackage.haskell.org/package/vector) package (e.g. see [`foldr'`](http://hackage.haskell.org/package/vector-0.10.9.1/docs/src/Data-Vector-Generic.html#foldr%27)). Now of course with lists we can only start from the front so by rights the only ones that we should have are `foldr` and `foldl'`. It's actually `foldl` on lists that is weird and unnatural and is breaking symmetry!
I like how this is somewhat believable until the D-Wave backend proposal.
To emphasize Duncan's point. `Data.Map` has all four just because we can traverse the map starting at either end: http://hackage.haskell.org/package/containers-0.5.5.1/docs/Data-Map-Lazy.html#g:15
Unless the text area and "Try it" button are greyed out, it'll work anyway; there's a glitch that happen for some people where the red, nasty text doesn't go away when commanded, even though the server connection works just fine.
Same. I actually think that explicit exceptions make much more sense.
I found it easier to solve than the original game. Is there something to be said on randomness? (And yes, I should have better things to do than playing 2048 clones.)
It's a question really. I'm wondering now if I should go back and merge them in the same repo. I usually like to have 1 cabal file = 1 repo, so I can have the README file used by GitHub, and the travis build. I've seen projects such as Diagrams having several repos. However, my 4 Nomyx packages really live together (they have the same version number), checking them in or out is more tedious now, and I don't know where to put the issues (some issues/features do impact the 4 packages) So there are pros and cons...
The usual problem with changing your standards is that existing programs out in the real world will break. I'm not sure if this is the case in this instance, though. Are there any actual programs that work correctly under the non-strict behavior, but break or perform badly when converted to foldl'?
persistent does handle (simple) migrations automatically.
Changing the Prelude is one of the things where updating the spec gets tricky. Historically the haskell' committee didn't want to accept changes which weren't already demonstrated in practice; however, there's no way to demonstrate non-conservative changes like these without diverging from the previous spec‚Äî unless you really want to fork people's dependence on `base`, which ain't pretty. All the same, progress must be made. And tbh this wouldn't be the first time GHC flagrantly violated the spec in hopes of getting the spec changed eventually.
&gt; I prefer code that uses a recursion scheme (e.g. foldl or foldr) instead of explicit recursion Notably, explicit recursion is equivalent to using `foldr`. Thus, the examples where non-strict `foldl` occurs are better done with explicit recursion precisely because they're better done with `foldr`!
I had come across the slight non-strictness of the existing definition of `Data.List.foldl'` before. From the point of view of unboxing though, the explicit loop is more important than the bang pattern or even seq. I did a bunch of little experiments, but even the standard lazy foldl is faster than the definitions Duncan considers for the dumb sum program `main = print $ foldl (+) (0::Int) [1..1000000000]`. The loop in the widely despised foldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a foldl f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = lgo (f z x) xs permits the compiler to write, at the point of use, something like Rec { $wlgo [Occ=LoopBreaker] :: Int# -&gt; [Int] -&gt; Int# $wlgo = \ (ww_s2Ib :: Int#) (w_s2I8 :: [Int]) -&gt; case w_s2I8 of _ [Occ=Dead] { [] -&gt; ww_s2Ib; : x_a15g xs_a15h -&gt; let! { I# y_a15y ~ _ [Occ=Dead] &lt;- x_a15g } in $wlgo (+# ww_s2Ib y_a15y) xs_a15h } end Rec } whereas even with the bang pattern fold :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b fold f !a [] = a fold f !a (x:xs) = fold f (f a x) xs we end up with stuff like this main_$sfold [Occ=LoopBreaker] :: Int -&gt; [Int] -&gt; Int main_$sfold = \ (sc_s2Gx :: Int) (sc1_s2Gy :: [Int]) -&gt; let! { I# ipv_s2GF ~ a_Xv1 &lt;- sc_s2Gx } in case sc1_s2Gy of _ [Occ=Dead] { [] -&gt; a_Xv1; : x_auX xs_auY -&gt; let! { I# y_a13j ~ _ [Occ=Dead] &lt;- x_auX } in main_$sfold (I# (+# ipv_s2GF y_a13j)) xs_auY } end Rec } However I arranged things, for this particular program I got something like `real 0m11.432s` for the definition with `lgo` and something like `real 0m14.336s` for the recursion with `foldl` itself. So I think the right definition must be something like fold :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b fold f z0 xs0 = go z0 xs0 where go !z [] = z go z (x:xs) = go (f z x) xs But I suppose that what Duncan was actually proposing to write. 
You're right of course.
Why is it that Haskell is considered amazing for compiler-creation? I've also heard a lot of people say that they've used it to make compilers.
It's a good port from the [original](http://llvm.org/docs/tutorial/)
Given recent work on basic-prelude, classy-prelude, numeric-prelude etc., some people have actually been using in practice Preludes that are quite radically different from the standard. So it might be a little easier now. Anyway, the issues with `fold` vs. `foldl'` are very well understood by now.
To name a few things: ADTs, pattern matching, generics libraries, parsec, libraries for name-capture, llvm bindings, metatheory libraries. A lot of the ML family languages have some of these, but I don't know of an ecosystem with as many as Haskell's.
It's a bit tricky to know what is good; I actually haven't used any of the hackage priority queues in high-performance situations and haven't been able to make a comprehensive comparison. There is a whole stack overflow post on the matter, and it kind of makes looking for a good implementation daunting. I chose to put PSQueue because I have seen people that I respect use it and advocate it in recent times even, despite it being rather old. Sometimes that's really the only thing you can do :/ Wish there was some sort of better way to tell how 'good' a package is by looking at it on hackage.
You're right, thanks for the tip :)
Sure sure, it's easy enough to use an alternative to the official Prelude. (You're just importing a different module afterall.) My point was that for the official Prelude itself, things have to break at some point if they are to change. (Because it's the same module with conflicting APIs.)
Now it's possible for the teams to select the license they want to use. We also updated the rules to make it more clear on where the AI code is going to be used: just the competition. How does it look? [blog.helloworldopen.com](http://blog.helloworldopen.com/post/81408303181/updated-rules-regarding-licenses)
Seems pretty trivial to me: last = maybe (error "empty list") id . foldr (\x -&gt; Just . maybe x id) Nothing Of course, it's a bit trickier to optimize away the use of `Maybe` (see below). &gt; Is all recursion equivalent to foldr? Weakly speaking, yes. That is, we can define just about[1] any *function* we want because catamorphisms (e.g., `foldr`) capture primitive recursion. However, to capture just about any *algorithm* we want, catamorphsims are too weak‚Äî we need paramorphisms. That is, catamorphsims and paramorphisms are extensionally equivalent, but paramorphisms are operationally more powerful.[2] [1] I say "just about" because cata-/paramorphisms only give primitive recursion, not general recursion. [2] For example, try implementing factorial on the Peano representation of natural numbers. Paramorphisms allow asymptotically faster implementations.
It seems that Sabry is saying that "pure" means some sort of independence from evaluation order. The equivalence of call-by-name and call-by-value seems to suggest a commutativity property, and the equivalence of call-by-name and call-by-need suggests idempotence. However, I look at things from a different angle. I think a notion of purity only makes sense if it is relative. You are right that a value `x :: IO ()` is certainly pure from some viewpoints. You can pass it around to Haskell functions and nothing can change it. However, once you plumb it into `main` it does actually cause the runtime system to do something impure, so I think to say from the point of view of the runtime system it is not pure. Likewise, from the outside a value `x :: State ()` is pure. However, if you are *inside* the `State` monad then there is a sense in which `x` can be "plumbed in" to the monadic computation in an impure way. I do not have a good theoretical basis for describing the difference I am trying to describe. I would be interested to hear if anyone knows of anything along these lines. 
That is awesome, good for you guys! I've updated my original reply.
point them out, so they can be fixed. operation spring cleaning!
Will definitely have a usage for this some time, thanks for sharing.
There's a pretty simple algebraic definition, but it counts partiality as impurity. In case people are interested, it is based on the data types `Void` (that is, the empty type) and `()` (that is, the unit type). Consider these functions: absurd :: Void -&gt; a trivial :: a -&gt; () trivial x = x `seq` () We say that a function `f` is strict when `f . absurd = absurd` and pure&amp;total when `trivial . f = trivial`. Additionally, both `trivial` and `absurd` are required to be `id` when the typing makes sense. This implies that they are both strict and pure&amp;total, which prevents them from having certain pathological implementations. In particular, it is important that `trivial` is not the constant function. Stuff like this can be extended to other constructions. In addition, it makes sense in many categories. And yes, it essentially places equal importance on strictness and totality.
Yes. basic-prelude actually uses a strict foldl, IIRC
Seems like a good application for category theory.
Hmm, Lazy ST + DList still 'pauses' during evaluation, indicating some kind of leak to me, although not as badly as the [] version did. I wonder if the leak is happening inside the ST monad, holding some unevaluated pairs of state? If it's not in the ST monad, a 'reverse state monad' on `[Integer]` with `yield x = modify (x:)` should avoid any problems with Writer leaks, right?
for those wondering at home, the module `Conduit` is from [conduit-combinators](https://hackage.haskell.org/package/conduit-combinators).
&gt; existing programs out in the real world will break. This is part of the "avoid success at all costs" mantra though.
For starters, I'd love something like the following: newtype Offset = Offset { microseconds :: Int64 } newtype Instant = Instant { epochOffset :: Offset } data TimeZone = TimeZone { timeZone :: String } -- TODO data DateTime = DateTime Instant TimeZone data Interval = Interval Instant Instant TimeZone Though I'm not quite sure how to represent a time zone; it's a very politically influenced concept as I understand it, so perhaps a label is the best representation. Of course, at any given point in time, you can obtain up the current UTC offset and an optional DST start/end.
Thanks! Don't hesistate to contact me if you face any issue or for feedbacks/suggestions.
Or simpler: last xs = foldr (\x r a -&gt; r x) id xs (error "empty list") 
Is it bad that I didn't realize the joke until the Clopen typeclasses. Went straight on past `instance data if`...
You mean `trivial . f = trivial`, correct?
Haskell is amazing for creating embedded domain-specific languages. In other words, before writing your compiler you capture the language you want to compile (the source language) in a type. Expressions of that type then represent terms in the source language and may actually look quite similar to it, despite being in Haskell. Then you represent the target language as another such type. Finally the compiler is really a function from the source type to the target type and the translations are easy to write and later read. Optimizations are also regular functions that map, say, the target language to the target language. This is why functional languages with lightweight syntax and a strong type system are considered amazing at creating compilers and a lot of other things that translate stuff.
Isn't this why we have versions of the spec? Haskell 98, Haskell Prime, etc? That way they can be radically different and have compilers have support for them?
Still pretty mad about head/tail throwing exceptions.
Purity and referential transparency are definitely distinct things, regardless of whether you use the correct definition of RT or the incorrect one. All pure languages with binding constructs necessarily are such that not all contexts are RT (or alternatively, using the wrong definition, that the language itself is not RT).
Luckily I got it as soon as Node.js was brought up.
Yeah.
amen!
If I specifically want to write a parser (I already have my datatypes and whatnot set up) is there a good guide to using parsec to make a parser or just writing one from scratch? This seems like it shouldn't be that hard but has proven unexpectedly difficult. 
Seems the links completely fail to work on IE 9. Guess I won't be looking into this stuff at work.
This is arguably the most popular possible change to the Prelude: old programs that this would affect are broken anyway, and it's a strict improvement over the previous functionality. If we can't fix something like this, then we basically resign ourselves to never improving the Prelude ever again.
So maybe my original intuition was correct, and the issue is that even though `ST` is lazy in the generated values, `WriterT` is not. You might want to try using the lazy `WriterT` if you haven't already (sometimes this works). If that still doesn't work, consider using `pipes`. Although `pipes` is already a pretty light-weight dependency, if you want to inline the minimum logic necessary for your library to shed even that dependency then I can show you how to inline the implementation of a `Producer`-like type in your code.
&gt; Yeah, I'm not very happy with mergeSH. I have been playing around with rewriting a few things, using some meaningful names, where/let clauses, i'll probably put something up better later tonight. I thought I had noticed some edits! I read it off and on throughout the day at work, so I refreshed a few times. &gt; And as for the state tuple ordering...I also have no idea :) It sort of makes "more sense" to me, as it's a tuple with the result and the attached state. I understand. I see why State s a makes sense, but I can see why you'd want the metadata of the state last in the tuple. Each part is perfectly sane individually, it's only when you put the two together that it's weird. &gt; For PQueue (WeightedPT a)...I want to "look at it" as if it was a priority queue containing a bunch of WeightedPT's...not as a distinct, fundamental weightedpt-pqueue idea. Makes sense, it was triggering my "I see duplication" sense. It's not the easiest to read, but you're only using it in a few places so far so I can also see why it wouldn't be worth saving the typing. I spent part of my day working with Java that had things like Map&lt;String, Object[]&gt; passed everywhere. Being able to define a type alias would have really cleaned that code up. Instead your only choice is to either let it be or make a wrapper class. &gt; And note that you don't have to define wrapper/unwrapper functions for type aliases. That was a guess. I couldn't remember off the top of my head if that was needed or not. After a quick Google, it seems that it was newtypes that I was thinking of. &gt; The main idea was to show that all folds can be re-written as composed state functions That was a bit weird, but I understood the point with runListFreq. By the time I got to listQueueState and runListQueue it wasn't clear if that's what you were still doing or if there was another reason. The little note at the end helps a bit, but I wasn't sure. &gt; So when you see [...] it is really just a newtype wrapper around [...] OK, that makes perfect sense. I think not seeing the full type signature is also what was causing my confusion on the 'working tree' stuff. Since I was guessing at the type I was thinking that the updated WeightedPT was getting pushed back onto the PQueue sort of like if a fold used the first element of the list as the accumulator. That seemed a bit odd, so I'm glad it actually works in the way I would think you would write it. I've been reading about Haskell for a few months now, but I haven't gotten around to actually writing any yet because I've been busy with other projects. Some of this is simple stuff that just keeps slipping out of my working memory because I haven't had it beaten into me by the complier yet. Sort of like forgetting semicolons in C based languages.
Way to go! Good work as always, Valderman!
Is it possible to get the lazy base case back if you need it? Obviously the whole `foldl` expression is evaluated lazily by default irrespective of its strict internals, and that's *probably* enough. But what if you have a wierd case really needs to be lazy, even though the rest of the fold should be strict? I *think* you just need to pull the first application of `f` out of the `foldl`, so the lazy base case is handled outside the `foldl`... foldl_lb :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; a foldl_lb f x [] = x foldl_lb f x [y] = f x y foldl_lb f x (y:ys) = foldl'' x' ys where x' = f x y Am I missing anything? 
Write Yourself a Scheme in 48 hours covers parsing pretty thoroughly.
Whatever happened to avoiding success at all costs? Isn't the point of avoiding success that the language can grow and evolve without sweating the small compatibility details? Otherwise you become the Win32 C API, and you can never, ever, ever change anything.
I agree, like the `maybe :: (a -&gt; b) -&gt; b -&gt; Maybe a -&gt; b`, destructor. headM :: [a] -&gt; Maybe a tailM :: [a] -&gt; Maybe [a] head :: a -&gt; [a] -&gt; a head default = maybe id default . headM tail :: [a] -&gt; [a] -&gt; [a] tail default = maybe id default . tailM So the default versions are derivable easily from the `Maybe` versions. 
If we're going to do this, we may as well make all prelude functions total while we're at it (or at the very least, head and tail. It makes no sense that head and tail aren't total while lookup is).
Well, GHC is written in Haskell... so I guess that's your canonical way of writing a compiler in Haskell. You could also look at the source for elm's compiler for ideas. Some great libraries for writing a compiler: * [`parsec`](http://haddocks.fpcomplete.com/fp/7.4.2/20140319-310/parsec/) is a library for writing parsers in a terse and straightforward way. `attoparsec` is a specialised, high-performance variant. ([video](https://www.youtube.com/watch?v=r_Enynu_TV0), [tutorial](https://www.fpcomplete.com/user/adinapoli/the-pragmatic-haskeller/episode-5-a-simple-dsl)) * [`bound`](http://haddocks.fpcomplete.com/fp/7.4.2/20140319-310/bound/index.html) is a library for performing capture-avoiding substitution through a simple `Scope` monad. ([tutorial](https://www.fpcomplete.com/user/edwardk/bound), [typechecker](http://benstuff.so/posts/2014-03-29-fumblings-with-bound.html)) * [`uniplate`](http://hackage.haskell.org/package/uniplate) is a library for performing transformations on recursive structures like ASTs. It makes writing optimisations fun an easy! ([tutorial](http://blog.sqreamtech.com/2013/09/using-haskell-at-sqream-technologies/))
A simple solution might be foldr (\xs acc -&gt; acc + sum xs) 0 $ chunksOf 4 [0..10000] but not so sure about performance. Simple sum [0..10000] is more optimized. (chunksOf is from Data.List.Split) 
Ultimately getting things like foldl' (+) 0 [0..10000] to just turn into optimized SIMD code is the goal of [Geoffrey Mainland's work on generalized stream fusion for vectors](http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/haskell-beats-C.pdf)
&gt; but it counts partiality as impurity. As it should! People refuse to see the Truth about Nontermination, but the evidence is overwhelming: Haskell is an impure functional programming language. That said, I don't really believe in your characterization which uses `seq`, which is not understood much better than nontermination; and thus a dubious foundation on which to build an algebraic empire. Besides (this is rather minor), it would probably require lifting to higher-functional types: currently it would count `\() () -&gt; undefined` as pure.
As someone 2 weeks in to Haskell, yes please! I had a hard time figuring out why one of my first programs was overflowing the stack because of foldl.
&gt; All pure languages with binding constructs necessarily are such that not all contexts are RT Can you give an example of Haskell displaying this property? 
Yes please!
`maybe id` is `fromMaybe` 
Read up on **Domain Specific Languages** (DSLs) http://www.haskell.org/haskellwiki/Embedded_domain_specific_language A compiler is just taking DSLs one step further and generating binary code for a target machine. The "Data.Binary" moduile and the "Data.ByteString" modules are excellent for constructing binary data. The 'Control.Monad.MonadPlus' class is indispensable for doing graph searches, so learn how to use that. The "Control.Monad.State" module and the 'Control.Monad.State.StateT' moand transformer, coupled with the lenses library let you construct virtual machines that let you do full or partial evaluation of abstract syntax trees. For examples, permit me a shameless self promotion: https://github.com/RaminHAL9001/dao The data types constructing the Abstract Syntax Tree are all here: https://github.com/RaminHAL9001/dao/blob/master/src/Dao/Interpreter/AST.hs The interpreter is here: https://github.com/RaminHAL9001/dao/blob/master/src/Dao/Interpreter.hs In the interpreter please notice the 'Executable' class which provides the 'execute' function. Every data type in the AST module instantiates this function, so evaluating any one of the 'AST' data types with the 'execute' function will produce a computation result. The "ExecUnit" data type in the Interpreter module contains the state of the program as it is executing, and the "Exec" module makes use of the StateT monad transformer to hold the state. As the "execute" function recursively traverses the AST, changes are made to the "ExecUnit" state. If I wanted to compile the language I would also provide a "Reducible" class to reduce the AST to a simpler intermediate data type, an "Optimizable" class for reducing the intermediate data types to the most efficient possible form. Then I would instantiate all the intermediate data types into a "TargetBinary" class that made use of the "Data.Binary.Put.Put" monad to convert the optimized byte code to a binary string. Finally, I could make every step of the process a separate module and pipe them together using the "pipes" library read source file ~&gt; tokenize ~&gt; parse ~&gt; reduce ~&gt; optimize ~&gt; encode to bytestring ~&gt; write file Although my language doesn't make use of "pipes" as of right now, I have been considering refactoring my code to make use of it in some later version of my software. 
&gt; As it should! People refuse to see the Truth about Nontermination, but the evidence is overwhelming: Haskell is an impure functional programming language. Yeah, it probably should, but we would first have to convince people that errors are impurity. Also, using `seq` is not strictly necessary, as one can just leave `trivial` to be undefined, except for some rules imposed on it. For more details, see Declarative Continuations and Categorical Duality.
Sure. The correct definition of RT is as follows: a context K[ ] is RT if, for all M and N s.t. M = N, then K[M] = K[N]. For binding constructs this fails. Consider the context K[ ] = `let x = 5 in _` and consider the expression `x`. It's value at the top level is `undefined`, but it is not true that `let x = 5 in x` = `let x = 5 in undefined`. Variables are inherently "context sensitive", and binders act to change a variable's value. Similarly, using the incorrect definition, you get a similar effect.
It's lazy, so you won't break it like that unless you try to print the entire thing which would have broken in ghci too.