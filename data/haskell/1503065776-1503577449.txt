react-hs uses type applications quite liberally and I personally have found it very nice to use. 
The upcoming release of PureScript 0.12 goes this route. Since we don't have polykinds, @ syntax is a special construct, but it will be sugar for a Proxy. 
It's picking up `n` from the definition of `SomeNat`: SomeNat :: KnownNat n =&gt; (Proxy n) -&gt; SomeNat and the problem is that you don't know *which* `n` that is, and you can't just unify it with your `k` (it's existentially bound). The solution is to provide an evidence of the type equality `n :~: k` (from the Data.Type.Equality module). The `sameNat` function does exactly this: `sameNat :: (KnownNat b, KnownNat a) =&gt; Proxy a -&gt; Proxy b -&gt; Maybe (a :~: b)` Which can be used as such: calendarDate minFirstWeekDays d m y = do let p = Proxy :: Proxy k SomeNat someNat &lt;- someNatVal (fromIntegral minFirstWeekDays) Refl &lt;- sameNat someNat p return $ (CalendarDate d :: Date (Gregorian k)) Pattern matching on a `Refl` of type `a :~: b` brings the evidence of the type equality into scope
Heh, which one?
That type doesn't seem to make sense. The `s` doesn't come from anywhere, so it must be phantom, but that forces the whole thing to bottom. So you must mean something else. Edit: this comment no longer makes sense, as the comment above was edited.
Or this beauty `curry (uncurry (&amp;&amp;) . (g *** h))`
The `Foo`s in the two functions seem to serve rather different purposes. In `indexOf`, `Foo` says how to map indices to locations in a container. In `tabulateOf`, `Foo` says both how to map indices to locations and what shape the container should have. The "shape" here could be `M23 ()`. But if you want the `Foo`s to be the same, you need to augment the `indexOf` variant to include the shape.
Is it more rich, or just more abstract?
Here is an awesome article that explains and derives the Y-combinator: http://mvanier.livejournal.com/2897.html
the very fact that the video is from 2013 and literally nothing happened since then really tells everything you need to know about gamedev in haskell.
In spare moments, I'm working on documentation/release process improvement in the hledger project. megaparsec 6 was released recently. This requires a new minor release of the hledger packages so that both can coexist in stackage. This means figuring out what other changes to include, and making changelogs and release notes, which I find toilsome and tend to procrastinate. I'm [working on](https://github.com/simonmichael/hledger/commit/ffbb71c4efe3efb4305de4286f5aa6a20196d1c3#diff-b67911656ef5d18c4ae36cb6741b7965R1185) better tools and process to transform new commits to an editable org outline to final release docs. Wondering if it's worth making a generic changenotes.hs tool to replace my make rules.
Fun write up. Thanks for sharing!
Thanks for your help. That did get things compiling but it seems to be wrong. If I use the `calendarDate` function like this: example1 :: Maybe (CalendarDate ('Gregorian 3)) example1 = calendarDate 1 1 February 2017 This compiled even though the types should not agree. If I remove the type declaration in an attempt to see if the correct number is chosen then, I get: No instance for (KnownNat k0) arising from a use of ‘calendarDate’ • In the expression: calendarDate 1 1 February 2017 In an equation for ‘example1’: example1 = calendarDate 1 1 February 2017
Thanks for your help. I tried and encountered the next problem, detailed [here](https://www.reddit.com/r/haskell/comments/6uhi1a/knownnat_specialisation_problem/dlsvify/).
That *compiles*, but is the result a Just or a Nothing? I expect a Nothing, because `sameNat` will compare the 1 and the 3 and answer Nothing, meaning no, the Nats are not the same. When you remove the type annotation, `sameNat` doesn't know which `k` to compare 1 to, hence the error message. *edit*: Try asking for a `Proxy k` instead of an Int, this way you'll already have the value at the type level and you'll be able to bring it down to the value level using `natVal`. In particular, by not receiving both a value level Int and a type-level Nat, you won't have to compare them to make sure they're the same.
Yeah, it's definitely up there in my head as one of the more questionable experiments that's ever been in GHC, along with: * LinearImplicitParams - State monad implicitly everywhere. Gone now. * Strict - Good luck understanding anything about how your code will run. Hint: it's almost completely but not entirely unlike you're using a strict evaluator. * NondecreasingIndentation - My code looks at a glance like it does something completely different from what it actually does because a block started halfway through and you didn't notice. Yay? * DoAndIfThenElse - Let's help beginners avoid learning how to indent if/then/else correctly for a few more days by allowing semicolons to randomly occur in the middle of if-expressions. Actually, let's make this the default and get it into the Report right away, awww yeah. * OverlappingInstances, IncoherentInstances - This stuff is actually fine, so long as any class which it affects never gets exported from the module it lives in.
Oh I see. I have misunderstood how this works (I suspected that was the problem from the start): what I want is the user to pass that number, as a value, that they want for the minimum number of days for the start week and that number become part of the type which is returned. If this is not possible then I'll have to think of another way to provide this interface. EDIT: Btw, you were correct that it was a nothing. I was unable to see because I didn't have a show interface. I'm a bit rusty I'm afraid. :)
&gt;Having ADTs and pattern matching at their disposal, why do Rustaceans still choose to use visitors? You can still use ADTs with the "visitor" pattern. pub fn walk_stmt&lt;'a, V: Visitor&lt;'a&gt;&gt;(visitor: &amp;mut V, stmt: &amp;'a Stmt&lt;'a&gt;, data: &amp;V::Data) { match stmt.stmt_kind { StmtKind::FnDef(ref fn_def) =&gt; visitor.visit_fn_def(stmt, fn_def, data), StmtKind::VarDef(ref var_def) =&gt; visitor.visit_var_def(stmt, var_def, data), StmtKind::StructDef(ref s) =&gt; visitor.visit_struct(stmt, s, data), StmtKind::EntryDef(ref entry) =&gt; visitor.visit_entry(stmt, entry, data), StmtKind::Assign(ref assign) =&gt; visitor.visit_assign(stmt, assign, data), } } This essentially makes the accept interface useless. I personally just used it to remove all the boilerplate code. This is probably subjective but I think custom serialization in serde is pretty nice https://docs.serde.rs/serde/de/trait.Visitor.html Maybe /u/dtolnay can say more about it. I mostly copied the visitor pattern from the [Rust compiler](https://github.com/rust-lang/rust/blob/master/src/libsyntax/visit.rs#L52) which I don't think is that nice in practice.
It is not possible for an input value to influence the returned type, but it is possible for the *type* of an input value to influence the returned type. In particular, callers can take an Int, convert it to a `Proxy k`, and pass that input to you. One alternative API, if you really want to receive an Int and not a `Proxy k`, is to use continuation-passing-style. So instead of returning a `CalendarDate ('Gregorian k)`, which you can't because you chose not to receive this `k` as input, you can accept as an extra argument a continuation of type `forall k. CalendarDate ('Gregorian k) -&gt; r`, which you can call with the `k` of your choice. Then your function returns the `r` which the continuation returns.
Getting same exact error, couldn’t figure out for the life of me how to get it to succeed. I’ve tried [editing my `ghc` settings](https://ghc.haskell.org/trac/ghc/ticket/13805), [upgrading LLVM](https://bugs.llvm.org/show_bug.cgi?id=33394)… Nothing works. Here’s my stack trace—strikingly similar to yours (I use macOS 10.12): [1 of 2] Compiling Main ( /private/var/folders/0_/5v1z6sg51wj660f32n15qzch0000gs/T/stack11246/terminfo-0.4.1.0/Setup.lhs, /private/var/folders/0_/5v1z6sg51wj660f32n15qzch0000gs/T/stack11246/terminfo-0.4.1.0/.stack-work/dist/x86_64-osx/Cabal-1.24.2.0/setup/Main.o ) [2 of 2] Compiling StackSetupShim ( /Users/Fallback/.stack/setup-exe-src/setup-shim-mPHDZzAJ.hs, /private/var/folders/0_/5v1z6sg51wj660f32n15qzch0000gs/T/stack11246/terminfo-0.4.1.0/.stack-work/dist/x86_64-osx/Cabal-1.24.2.0/setup/StackSetupShim.o ) Linking /private/var/folders/0_/5v1z6sg51wj660f32n15qzch0000gs/T/stack11246/terminfo-0.4.1.0/.stack-work/dist/x86_64-osx/Cabal-1.24.2.0/setup/setup ... Configuring terminfo-0.4.1.0... configure: WARNING: unrecognized options: --with-compiler checking for gcc... /usr/local/bin/gcc-4.9 checking whether the C compiler works... yes checking for C compiler default output file name... a.out checking for suffix of executables... checking whether we are cross compiling... no checking for suffix of object files... o checking whether we are using the GNU C compiler... yes checking whether /usr/local/bin/gcc-4.9 accepts -g... yes checking for /usr/local/bin/gcc-4.9 option to accept ISO C89... none needed checking for setupterm in -ltinfo... no checking for setupterm in -lncursesw... no checking for setupterm in -lncurses... yes configure: creating ./config.status config.status: creating terminfo.buildinfo configure: WARNING: unrecognized options: --with-compiler Building terminfo-0.4.1.0... Preprocessing library terminfo-0.4.1.0... System/Console/Terminfo/Base.hs:42:30: error: error: editor placeholder in source file (&lt;#&gt;), ^ System/Console/Terminfo/Base.hs:188:2: error: error: editor placeholder in source file (&lt;#&gt;) :: Monoid m =&gt; m -&gt; m -&gt; m ^ 2 errors generated. `gcc' failed in phase `C pre-processor'. (Exit code: 1)
Or, equivalently (with -XExistentialTypes), define, well, an existential type: data SomeGregorianDate = forall cal. SomeGregorianDate (Date (Gregorian cal)) and then return a `Maybe (SomeGregorianDate)` from your function.
I know the *visitor pattern* only from an OOP context. Is there any other definition? You use a virtual `visit` method (dynamic dispatch) in the objects you want to visit in which you call the matching method of the visitor you passed in. The problem with this is that you can't easily add new types to your class hierarchy because all the visitors you already wrote lack the corresponding method. It's however easy to add new visitors. It has been solved by [this paper](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf). You might also want to [try it yourself](https://www.codewars.com/kata/data-types-a-la-carte). (To be honest: I'm a Rust beginner and I can't say that I really understand your code, but I hope my answer helps anyway.)
In my experience the worst that can happen is the compiler hits bottom, which you encounter at compile time ;) so if it compiles - you're fine.
Coders strike back is much more fun than what I'm working on. Drat you! :)
Afaik, the forall is implicit in every function where you don't explicitly provide it. The type variables are usually implicit at the callsite. TypeApplications allow you to explicitly provide the implicit parameters to your forall. To illustrate how useful this is try writing a function that uses a type variable AS a constraint - then pass the specific constraint with type application.
Type families are not first-class; how do you feel about them?
I try not to use them. I try not to even use typeclasses!
Sure, I understand this. But `Proxy` is another way to do this that is first class so I prefer it.
&gt; Strict - Good luck understanding anything about how your code will run. Hint: it's almost completely but not entirely unlike you're using a strict evaluator. It's quite painful to read, on Hacker News and other programming-related forums, comments saying "It's hard to understand the performance of lazy programs but at least Haskell now has -XStrict!". 
&gt; OverlappingInstances, IncoherentInstances - This stuff is actually fine, so long as any class which it affects never gets exported from the module it lives in. Does this drastic restriction also apply to the newer `OVERLAPPABLE`/`OVERLAPPING` pragmas?
I mean pattern matching isn't first class. The following does not work: genericFromMaybe just d mx = case mx of just x -&gt; x _ -&gt; d fromMaybe = genericFromMaybe Just You can use the first class approach to pattern matching via prisms: genericFromMaybe just d mx = case mx ^? just of Just x -&gt; x _ -&gt; d fromMaybe = genericFromMaybe _Just But it is syntactically heavier and hence is generally reserved for situations where you actually need it, such as maybe a parser. Don't get me wrong I love when things are first class, and it would be cool if first class pattern matching was a thing. But IMO syntactically light non-first-class features are fine, as long as they aren't too complicated.
I guess it depends what you think the power to weight ratio is. It seems to me that pattern matching, especially *nested* pattern matching, has *huge* power to weight ratio. Visible type application is just some mild sugar over what we could do locally anyway with `Proxy`s or some other way. Maybe in some use cases it's really valuable, but I haven't come across one in my own code yet.
Cool, thanks! This looks pretty similar to [free monads](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html). 
Don't quote me on this but in strict languages I think you could write it with CPS and get the same effect. That being said writing CPS in other languages kinda sucks.
&gt; Dejafu could be a lead. Quite possibly! I actually tried some concurrency testing with Kontiki a while back. I wasn't using dejafu, as I wanted to play with ideas like bounding the amount of message loss and re-ordering, which dejafu can't really do (because there's no message-passing primitives in Haskell, so none in dejafu). So I'd implemented a small dejafu-like with the primitives I wanted to model a distributed system. I didn't actually find any pre-existing bugs, but I was able to reliably reproduce seeded ones.
* I’ve ported [llvm-hs](https://github.com/llvm-hs/llvm-hs) to LLVM 5.0 in preperation for the LLVM 5.0 release. * A collection of [middlewares](https://github.com/cocreature/miso-middleware) including a time travelling debugger for [miso](https://github.com/dmjio/miso).
Type-classes are actually strictly speaking first-class, though it's pretty useless: f :: (* -&gt; Constraint) -&gt; String f _ = "a type class" putStrLn $ f Eq
While of course you could encode it in a string (data is data), it looks like it does have some structure under the hood...
Or if there's another reason. The pointfree style might sometimes generalize to `Category`, `Arrow`, `Profunctor`, etc., when the pointy version doesn't.
But you can't do anything with that, can you? It would be cool if you could! I thought all you could do was attempt to cast.
Yeah, pretty much. Basically, the problem is that once you start using overlapping at all from multiple modules, you can easily end up with incoherent use of instances, that is, two differently behaved instances of the same class being used at the same type. It doesn't require the IncoherentInstances pragma to occur, just any more specific instance becoming available from a separate module. So, that means changing which modules you import can quietly change the behaviour of your program, even if you don't seem to be using anything explicitly exported by them. Remove a seemingly redundant module import to fix a warning, quietly get a new bug for free when a more specific instance goes away and your code switches to a more general one. (Okay, the warnings could be improved, but the meaning of your program could also change unexpectedly when adding a new module import.) See the example at the end of [this section](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#overlapping-instances) of the GHC user's guide. It illustrates the case of two different instances being used at the same time for the same type, which is bad, but yeah, the really bad stuff is what happens when you're modifying code and changing module imports, pulling new overlapping instances into scope, or accidentally causing them to go away. Why is that worse? Well, in cases where more than one behaviour is in use at once, you'll be confused and it might spoil various data structure invariants, but at least you're confused about the broken behaviour of the code that you're working on right now. In the case of importing modules, you might be quietly trampling the behaviour of someone else's code, which you might not even think to re-test right away. The code you're trampling might not even be in the module or even the project you're editing, but some module which imports it later. You might not even have access to that code. Clearly not a situation you want to be in. Thankfully, people don't use this stuff all that much. Coherence of instance resolution is vital to the health of the Haskell ecosystem. Now, even if you do export your class with overlapping instances, it might be fine if you can just *by convention* keep all the instances in one module. However, in a big project, anything which is permitted by the compiler is probably going to happen eventually, so don't count on it too much. You might want to define a new (trivial) subclass of the class with overlapping instances, and export that instead, so as to guarantee nobody else can write instances of it.
Thanks! Out of curiosity, is that a first-impression sort of like, or had you heard of the project before? I’m always interested in feedback about the design.
&gt; in strict languages I think you could write it with CPS and get the same effect I quoted you anyway.
I think it needs to see some large GUI apps built with it. I have built toy examples with it (in the examples folder), but due to some personal stuff, haven't yet found the time to build something larger. I'm sure performance could be improved as well (I have a few ideas there). BTW, I appreciate the link to a GUI challange which is not a todolist! I'll give it a go when I'm able!
We are working on a full quake 3 remake with lambdacube 3d: https://github.com/lambdacube3d/lambdacube-quake3
Suppose we decide to write a simple lisp AST, leaving the exact value used as a value `v`: data Sexpr v = Sym Text | Primitive v | Lambda [Text] [Sexpr v] | Quote (Sexpr v) | Call [Sexpr v] We would have to implicitly disallow the empty call and empty lambda, obviously there are trade-offs. If I just tell you to write a visitor for that then it's not too hard: a visitor is a list of actions to take on visiting each of these nodes. Let's throw in a monad `m` to make this a little more fun; our visitor structure needs to look like: date SexprVisitor v m z = SexprVisitor { visitSym :: Text -&gt; m z, visitPrim :: v -&gt; m z, visitLambda :: [Text] -&gt; [Sexpr v] -&gt; m z visitQuote :: Sexpr v -&gt; m z visitCall :: [z] -&gt; m z } Here I've taken the attitude that when visiting a lambda or quote, you might not want to automatically visit its sub-expressions, for example if one of your visitors is an evaluator for this language. However if you find yourself visiting a normal lisp call expression `(abc def ghi)` the claim is that you definitely want to automatically visit `abc` and `def` and `ghi` before coming back to the `Call` node. Once you've written what the visitor is, it's trivial for this data structure to accept the visitors: sexprAccept :: Monad m =&gt; SexprVisitor v m z -&gt; Sexpr v -&gt; m z sexprAccept (SexprVisitor sym prim lam quo call) = go where go (Sym t) = sym t go (Primitive v) = prim v go (Lambda ts s_vs) = lam ts s_vs go (Quote s) = quo s go (Call cs) = mapM go cs &gt;&gt;= call Note that the process of going into the `Call` elements is explicitly handled by `sexprAccept` via this `mapM go` construction; it would reduce to just `call (map go cs)` in the identity monad but if you're writing an evaluator of type `SexprVisitor Value IO (Either Text Value)` then you want this ability to sequence together programmatic actions. Or you just may want to thread some state through the visiting with the `State` monad.
How does this compare to [LambdaCube3D](http://lambdacube3d.com)?
I mean basically every single one of your complaints and worries disappear if you don't have orphan instances. And IMO orphans (at least in their current form) are what should be avoided, not necessarily OverlappingInstances.
Torsor? http://math.ucr.edu/home/baez/torsors.html
Thanks! Commit `f820f09aa65056c908e8e02cd859cf880da09784` is working for me. Now to try it out :)
But my goal for the API is that it's impossible *at compile time* to compare, say, a `Date (Gregorian 3)` and a `Date (Gregorian 2)`. Will that work with this trick?
Shit that was it! This is the blog post: https://ro-che.info/articles/2013-01-08-torsors Thanks!
&gt; Value application can be and is abstracted as `$` This isn't actually true in the presence of higher-ranked types. infix `$` is a syntactic form in GHC, not a regular function, to allow `runST $ do ...` to compile (among other things)
I was happy to have learned something new, and then I tried it, and I was disappointed to discover that it doesn't work. `* -&gt; Constraint` is a valid kind: {-# LANGUAGE KindSignatures #-} import GHC.Types data Foo (c :: * -&gt; Constraint) = Foo foo :: Foo Eq foo = Foo And with `TypeInType`, it's also a valid type, but `Eq` is still not a valid value.
That is a big part of the worst cases I was referring to with regard to module imports, but you can't easily know if modules you're importing have orphan instances in them. Ordinarily, the worst thing orphan instances cause is that your code can't compile and you can't use two modules together, which is really unfortunate, but it's not subtle. With OverlappingInstances, orphans in other people's code become able to silently change the behaviour of your programs based on what you import, with no complaint or warning from the compiler that you'll see (they'll get the warning about the orphan, but they might ignore it). Also, while I did focus on the import-related stuff pretty heavily there, because it's scary, there's still plenty of weirdness that can go on with incoherence even if you have a linear chain of modules.
The example he linked to in the GHC documentation doesn't use orphans, does it? One module defines and gives instances for the class, the the other defines a new type and gives *more precise* instances involving the new type.
Thanks for the example. Maybe Haskell needs a `sealed` keyword, like in Scala, to indicate that all the instances of a class (in Scala, all the subclasses of a class) must be defined in the current module?
I personally think the sugar is more than significant enough to justify its existence. If we embrace TypeApplications we can also start using a lot more very generic code and features like overloaded* concisely: Examples (that is once we have type applications on literals): blah (fromList [1, 2, 3] :: Set _) blah $ fromList @Set [1, 2, 3] blah ([1, 2, 3] :: Set _) blah $ [1, 2, 3] @Set show . (read :: Int -&gt; String) show . read @Int map = (fmap :: (a -&gt; b) -&gt; [a] -&gt; [b]) map = fmap @[] And so on.
`Proxy` to me feels like a bit of a hack. To trick the unification algorithm to doing the unifications you desire. `TypeApplications` directly represents what is actually going on internally.
Is it an inherent property of torsors that they should all possess a 'direction', or is this just an artifact of all of the examples chosen being represented by a discrete quantity? IE, August 18 - August 15 is (3 days) - But August 15 - August 18 would be (- 3 days), which seems nonsenical, but not really, because we're representing a direction in time, so it has meaning. Likewise with all of the examples given in the article it seems that there is an axis along which the torsors operate. This seems to imply when talking about things that are not numbers, (or things which are representable as such) but are representable as torsors, that the net result of the subtractive operation would have to carry additional information along with it to indicate how it should be applied additively back into the group from which the torsor was derived.
Something have been toying with idea for a while, is to use implicit parameter for that. For example getFooQuery :: (?isAdmin:: ()) -&gt; .... Now, every function calling indirectly `getFooQuery` will have `(?isAmin::()) in it's type signature and will need `?isAdmin` to be setup in the function hierarchy using let ?isAdmin = () You probably can do something more clever using a custom type with a hidden contructor, so that people can't set it up manually. 
&gt; Almost all uses of TypeApplications are really just instances of dependent-types-envy. Yeah, IIRC, both Idris and Agda do "visibility override" by surrounding the value/type with curly brackets / braces like `Nothing {String}`.
Yeah, though it doesn't quite demonstrate the import-badness I was discussing either. If you can ensure there are no orphan instances of the class anywhere, then the quietly-behaviour-altering import stuff doesn't happen, though the disagreeing behaviours from code compiled in different modules still does.
StrictData is my boy tho 
wow, I had tried to figure something like that out on my own and assumed it must have a name but never bothered to ask. This is great!
I am not a mathematician but my intuition tells me the results of the subtractive operation must form a group. (On mobile so cannot expand.)
Yeah, we got "closed type families" a little while back, it would make a lot of sense to have "closed type classes" as well, and this would basically be a complete death-blow to all the Overlapping stuff for me, since the only case where I want it is already where I know what all the instances are going to be and am just doing some type level computation.
Why not just add another parameter to getFooQuery that requires an access token, as returned by checkFooAccess? You could always define checkFoo access to short-circuit on the presence of the Admin role, and realistically, it's probably a better default to always explicitly check access for a user - It'd be better to accidentally check access when you knew you didn't have to than to to accidentally NOT check access when you really should have.
`DataKinds` plus `data Complex (thing :: Thing) = Complex thing` should work, right?
Oops, you're right.
Note that what you're really asking is whether the group that the torsor is a torsor over necessarily possesses a direction. The answer is no, since the group can be any group, and there are lots of directionless groups, such as permutation groups or finite cyclic groups.
You can't prevent a comparison between two `SomeGregorianDate`s based on the `k` they contain, but you can prevent a `Date ('Gregorian 1)` from being compared with a `Date ('Gregorian 2)`. More annoyingly, they won't be able to compare a `Date ('Gregorian k)` with a `Date ('Gregorian k')` even if `k` and `k'` were obtained from the same Int. Which is why I suggested using a `Proxy k` argument: this allows your users to pick one or more Ints, to construct one or more `Proxy k`s, and to use them to construct one or more `Date ('Gregorian k)`s without being able to accidentally mix two of them if they weren't constructed from the same `Proxy k`. You will lose the link between the original Int and the Dates no matter what you do, but you'll retain the link with the original `Proxy k`s and the corresponding Dates, so you should let your users hold on to it. By using CPS or `SomeGregorianDate`, you make the user's life slightly simpler because they can pass in an Int instead of a `Proxy k`, but that's only simple if they only construct a single date. As soon as they construct more than one, each Date will have different `k` variable and so they won't be comparable even if they were built from the same Int.
It's not my first time looking at kitten. I've even compiled and played with it in the repl for a bit. There are many things there that I like, the whole stack and evaluation model and post fix notation, the permissions system also looks cool but I'm not sure how I feel about having `map` be used as `mapM` as well. I still need to test it on actual programs. Regarding things I don't know yet, does kitten have higher kinded polymorphism? also, how do you overload `+` for example? typeclasses? There are a few things where I think I would've done differently. In terms of primitive types I'm not sure if `String` as `List&lt;Char&gt;` is what I'd choose. Also I'd really like having anonymous and extensible records. In terms of syntax, which might not be that important, there are also a few things I would do a bit differently, such as the parametric polymorphism syntax to be more haskell like and less java like and also separate the type signature from the definition. I'd probably also get rid of `{}` syntax for blocks and have a more haskell like adt declarations. all in all I thinks it's a really interesting project and I think it's of great value. I plan to draw a lot of inspiration from it if I get to design a programming language. I wish you all the best!
LambdaCube3D is for graphics, my language (Futhark) is for arbitrary computation (and it is not particularly good at graphics).
I mean you are defining `instance MyShow ([] ...)` so it is kind of an orphan. It seems like with the current implementation you have to avoid semi-orphans like the above to avoid certain strange behavior. Also that example uses `{-# LANGUAGE OverlappingInstances #-}` which is deprecated. Although it does look like you can replicate the behavior with the new `{-# OVERLAPS #-}` and similar. That is admittedly pretty annoying behavior. I'm actually not sure how it is possible. It seems like adding `OVERLAPPABLE` to `instance MyShow a =&gt; MyShow [a]` should force `showHelp` to have type `MyShow [a] =&gt; [a] -&gt; String` which fixes the incoherence, but for some reason it doesn't require that type sig unless there is a genuine overlapping instance in scope. Also I don't see why it is possible to define `instance {-# OVERLAPS #-} MyShow [T]` without an `OVERLAPPABLE` on `instance MyShow [a]`. I thought the whole point of the new pragmas was to require bidirectional consent on overlapping instances to get rid of incoherence (except in the presence of orphans). Perhaps it is because I am just using `:l` in ghci and not a proper project? Because yeah the current situation seems a little unsafe. Darn.
For many applications it is not necessary to explicitly walk up and down the tree. An old technique, called attribute grammars, can be used in Haskell. Its implementation is straightforward when using lazy evaluation. A manual of the Utrecht University Attribute Grammar System can be found at: http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarManual It contains many applications and introduces the technique in a stepwise fashion. In my view it simply obviates the need for writing any visitor pattern or functions at all. Actually having to think about an explicit evaluation order in many cases makes finding a solution to your problem harder. In http://www.cs.uu.nl/research/techreps/repo/CS-2004/2004-025a.pd I describe the development of an online linear time pretty printer. In the appendix of the paper you find the attribute grammar underlying the code. Comparing this code with the explicit code in the paper should prove my point: do not write explicit traversal functions ever. In Atze Dijkstra's thesis it is explained how a complete Haskell compiler can be built using this formalism. The system also contains flow analysis phase so it can unravel a traversal order for the trees to be attributed automatically. In this way the technique can also be used for strict languages. It should not be too difficult to extend the system such that it can generate code for other programming languages. The inside of the system is remarkably Haskell independent. The latest paper describing techniques for finding an ordening between the attributes which makes that they can be evaluated in a strict language can be found at: https://link.springer.com/chapter/10.1007%2F978-3-642-27694-1_14 If you like this way of thinking but do not want to resort to a preprocessor you may take a look at the first class attribute gramma library http://hackage.haskell.org/package/AspectAG. The design of this library can be found in the thesis of Marcos Viera. 
That seems like it is fixable: instance MyShow a =&gt; MyShow [a] where myshow xs = concatMap myshow xs Should IMO be impossible to overlap, you should need: instance {-# OVERLAPPABLE #-} MyShow a =&gt; MyShow [a] where myshow xs = concatMap myshow xs And the above should make: showHelp :: MyShow a =&gt; [a] -&gt; String showHelp xs = myshow xs Invalid, and instead require it to have type: showHelp :: MyShow [a] =&gt; [a] -&gt; String showHelp xs = myshow xs Which fixes the inconsistency. GHC/Haskell is making some inherently incoherent assumptions in allowing the compilation of the original example. It is assuming it is safe to jump from the `MyShow a` instance to the `MyShow [a]` instance using the instance that directly encodes this jump. Which is NOT a valid assumption in the presence of `OVERLAPPABLE`. It is also allowing you to overlap an instance that hasn't specified it is actually legal to overlap it. IMO if you want to overlap an instance that isn't overlappable you should have to specify `{-# UnsafeOverlap #-}` or similar. I guess for now I should be careful with overlapping instances, as the current implementation does not seem sound. I am OK with orphans causing import shenanigans, as orphans and imports are well known to interact dangerously and are well avoided. But I am not OK with this behavior you have demonstrated.
Maybe a vector space? In geometry, "subtracting" one point from another gives a vector, not a point. There is the `vector-space` package on hackage: http://hackage.haskell.org/package/vector-space-0.11/docs/Data-AffineSpace.html 
Yeah, unfortunately that's not what OVERLAPPABLE means. That pragma just means any overlapping instance in future modules is okay, as if OverlappingInstances is on.
It's either/or with OVERLAPPABLE and OVERLAPPING. If either one is present, GHC will accept the overlap. I agree it would be much better to require both.
Well shit
I'd be ok with `UnsafeOverlap` or similar that allow only one to be present. But this seems rather unsafe.
That doesn't seem right. In what situations can `AllowAmbiguousTypes` cause compile time non-termination?
This is the right way, *except* that this doesn't make it any easier to print out as a grid, since multidimensional arrays don't have support functions to talk about single dimensional slices so far as I am aware. That is to say, if you explicitly have an array of arrays, you can just say `mapM_ (putStrLn . foldMap show)` to print a nice grid, but with a multidimensional array it's harder. On the other hand, arrays of arrays are much nastier to work with in Haskell for basically everything else.
I think all you can do is `cast`, I think you need `Generic` or similar to do other things.
Nlab is aimed at mathematicians but their definition is much more down to earth than wikipedia in this case: https://ncatlab.org/nlab/show/torsor
I think you missed the most interesting part, which is that you cannot build `fmap` from `return` and `join`, so while `return` and `&gt;&gt;=` give you a monad, `return` and `join` are only give you a monad with the help of `fmap`.
I personally like the ability for the caller to override visibility. For example `read` and `show` should by default be invisible. But being able to do `show . read @Int` is still very convenient, much more convenient than `show . (read :: Int -&gt; String)`.
I mean storage space is expensive these days. So personally I just go with whichever version uses the smallest number of characters.
Subtraction and addition are the same though, right? So if addition is a Sum Type in category theory, and an Either type in Haskell, is subtraction the *equivalent of taking a subset of a Sum Type tree*? Maybe if we have a tree of Either A or B, (A = Either C or D, B = Either E or F ... etc), then subtraction on this object is selecting a subset of the Sums, effectively getting a smaller object of the same type (tree of sums). ~~This is also a Monoid.~~
A little bit? https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Typeable.html#v:splitTyConApp
My understanding is that you need Generic (/Data) to get at the *data* in your value with only dynamically known type, but you can ask some questions about the *type* with just Typeable.
An affine space will fit better. We have two types: points of affine space (dates from example) and their differences (time spans from example). 
Ah ok cool. That makes sense. 
You could just apply it to `[ [ a!(i,j) | i &lt;- [iMin..iMax] ] | let ((iMin, jMin),(iMax, jMax)) = bounds a, j &lt;- [jMin..jMax] ]` And all the methods above work just fine with `Vector (Vector a)`
Pointers in C are another example.
That’s helpful, thanks. :) The difference between pure &amp; effectful `map` is still reflected in the types, so hopefully it’s still clear when something is using effects or not; if you want a guaranteed-pure `map` you can locally revoke permissions, e.g. `{ … { … } map } with (-IO)`. Higher-kinded polymorphism is planned, and should actually be pretty easy once I finish the current refactoring. `+` &amp;c. are overloaded with “traits”, which are essentially typeclasses with a single method; laws are defined by relationships between traits, e.g., `zero` should be the identity of `+` if a type has an instance for both. Like Haskell 98, traits are always statically dispatched; if you need dynamic dispatch, you can use a closure. I’m uncertain about using `List&lt;Char&gt;` for string literals too—it just seemed like the simplest thing that would work. At least it’s backed by an array and not a linked list! I’d like to add a real `Text&lt;Encoding&gt;` data type, but I don’t yet know if string literals will keep `List&lt;Char&gt;` or what. With compile-time evaluation, you’ll be able to have “suffixed” literals like `"hello"#utf8`, and that might be good enough? Extensible records and variants are planned; some codegen semantics &amp; notation just need to be figured out. (I’m running out of ASCII!) As for syntax, I’ve tried *many* things to see what feels right and interoperates well with other tools—there are what I feel are strong reasons behind most decisions: * Universal quantifiers were originally `@ a b` (using `@` as a mnemonic for ∀) instead of `&lt;A, B&gt;`, and namespace selection was `.` (at one point `_`) instead of `::`. I changed them partly to enable better out-of-the-box GDB support: if the Kitten syntax is largely the same as C++, and the name mangling is the same as the platform C++ convention, then you can write `break foo::bar&lt;Baz&gt;` and it works as expected. * `{}` blocks were inspired by Haskell: having them available makes it easier to generate source code, use brace-matching and auto-formatting in editors that may not be aware of Kitten, and use a screenreader. Plus, inline blocks are way more common in Kitten than Haskell, and it’d be a pain to write them all with indentation. * Type annotations were originally separate from definitions, but I felt there wasn’t a significant advantage to this because (for now at least) they’re required, owing to how Kitten’s type system works. * ADT declarations look the way they do in order to support features (like records) that haven’t been implemented yet, which is why they’re a bit clunky at the moment. * Much of the syntax is designed to highlight well with syntax highlighters that don’t know about Kitten; for example, it irks me that OCaml and Rust use `'a` for type/lifetime variables, and Haskell uses `'Foo` for data kinds and `x'` for primes, because many highlighters render this as a broken character literal. 
&gt; Nlab is aimed at mathematicians but their definition is much more down to earth than wikipedia in this case: That's pretty remarkable :)
Thank you! I was doing some work on the difference between indexes and offsets in arrays, and this article's terminology will come in very handy. 
"When I finish", good one :) So far it's mostly a tool for me to notate and learn mridangam lessons, and works well enough for that, but I'm gradually extending it to support more concepts. I'm not sure how much appeal it would have beyond that very specific niche.
&gt; Why not just add another parameter to getFooQuery Yep, I was wondering how far we can get it. For example, we see that something simple checkFooAccess :: FooID -&gt; Bool getFooQuery :: Bool -&gt; m Foo suffers from the fact that `Bool` is too weak for it (can be easily circumvented). Let's make some datatype with smart constructor then: newtype HasFooAccess = HasFooAccess FooID checkFooAccess :: FooID -&gt; HasFooAccess getFooQuery :: HasFooAccess -&gt; FooID -&gt; m Foo but we see, that without dependent typing, this type signature can be circumvented as well! (it doesn't check that `HasFooAccess` is a proof that we have checked access to `FooID` (second param)). PS. Mind that my question (I should've maybe mentioned that), aims to check the ability of Haskell typing, and not at something that's simplest and most practical (practically I should just probably follow your advice to check it always) // This is how far I could get it: {-# LANGUAGE DataKinds #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE RankNTypes #-} newtype FooID = FooID Int data FooAccess (i :: FooID) = FooAccess checkCourseAccess :: forall (i :: FooID). i -&gt; FooAccess i checkCourseAccess _ = undefined which gives an error: • Expected a type, but ‘i’ has kind ‘FooID’ • In the type signature: checkCourseAccess :: forall (i :: FooID). i -&gt; FooAccess i from what I recall `TypeInType` was supposed to merge types and kinds, but I probably had wrong idea what does it really mean, as it does not help in this case.
Right, I wonder exactly about that - how far we can take the type system, to not allow setting it manually.
Thanks for that, need to find time to digest it.
Cool project! I cannot promise that I'll have the time, but I'd be interested in doing a review. Please ping me when you need one.
But all groups have inverses, which are the same except for direction.
Not quite: data Complex (thing :: Thing) = Complex thing -- ^ -- Expected a type, but 'thing' has kind 'Thing' `data Complex (thing :: Thing) = Complex (Proxy thing)` works though. 
No. The examples are just geometric. Give two lists containing the same set of district elements, you can find their difference. The difference will be a permutation (a function which shuffles a list). Permutations don't have a "direction". 
How do you define direction? It's not like you can always have a direction function, which assigns a direction to each element and assigns x a different direction from x^(-1).
I translated your foldExpr function 1:1 into Rust https://play.rust-lang.org/?gist=1f7e5a01ec3d0010c50ecbccf93ad66b&amp;version=nightly (just for fun)
Thanks for your detailed response! Regarding `map`, what I meant was that in Haskell for example you can have `map putStrLn [1,2,3] :: [IO Int]` - times where you may want to distinguish between `map` and `mapM`, but I'm not sure how this works for kitten. I understand the rest of your arguments but don't think I currentlly agree with them. For example I would rather not optimize for compatibility with other tools like gdb and syntax highlighters, but maybe you are right and this is the right approach! I am very interested in seeing what's next for kitten so do keep us up to date please! maybe in /r/concatenative?
Do share it here once you do!
Don't be coy Lennart... we all know you're writing *another* Haskell compiler.
Another useful if unintuitive thing is to use an overly polymorphic type, use the derivable functions, and expose those instantiated to specific types. For instance: data Expr' a = Num a | Add (Expr' a) (Expr' a) &lt;...&gt; deriving Functor data Expr = Expr' Int Then `mapNum = fmap :: (Int -&gt; Int) -&gt; Expr -&gt; Expr`
Ah, yeah, if you wanted first-class actions like that in Kitten, you’d need to use closures, something like `[1, 2, 3] { -&gt; x; { x say } } map` or `[1, 2, 3] { function \say compose } map`, which has type `List&lt;(-&gt; +IO)&gt;`; then you could `\call each` or whatever. When I finish up this round of work, I’ll write another “Lately in Kitten” and post it on /r/concatenative, and the /r/kittenlang that Rich McKinley kindly created. :) 
Any news on this front perhaps? I know ghc-8.2.1 uses `gold` by default, but can I still use gold for dependencies via stack if ghc-8.0.2 is required?
For continuous groups (and where it exists), I'm comfortable calling an infinitesimal generator dx of x such that x = exp(k dx), where k is positive, its direction. For discrete groups, splitting into direction classes by orbits of x should work in some cases... But no, it's not a universal feature.
That's awesome! And a nice way for me to learn Rust :)
&gt; Ah, yeah, if you wanted first-class actions like that in Kitten, you’d need to use closures, something like [1, 2, 3] { -&gt; x; { x say } } map or [1, 2, 3] { function \say compose } map, which has type List&lt;(-&gt; +IO)&gt;; then you could \call each or whatever. That's good enough I think! &gt; When I finish up this round of work, I’ll write another “Lately in Kitten” and post it on /r/concatenative, and the /r/kittenlang that Rich McKinley kindly created. :) great :)
I've thought about this, and I think your best bet is toList, and if you need set like operations on the remainder to use toAscList and Data.List.Ordered. Remember that our list implementation, bemoaned by many, ought to be The Best Representation for some problem. That problem, of course, is `uncons :: t a -&gt; Maybe (a, t a)`. Maybe you'd like some balanced tree internally, where each `uncons` keeps the trees somewhat balanced; certainly possible, but also a use-case so specific that you've pretty much define the internal structure already and might as well write the ~80 lines you need to implement it. 
Are you sure you meant to say 'orbit'? Because all orbits of elements in torsors are the same.
Books aren't "too long". They cover a certain amount of content; HPFFP covers a lot of material, way more than most intro books. If you're finding yourself learning faster than the book "let's you", just skim ahead to where you get stuck. I did the first 5 chapters of HPFFP in one day and was on chapter 10 within a week because of some prior math, programming, etc experience I had. "Need more practical applications" is also a bit strange of a complaint from HPFFP; it's one of the most practical "let's get you started on using Haskell Right Now" books I've ever seen. Almost zero ivory tower, theory, talk about bottom, strictness, equational reasoning, or "how to think like a fp programmer" (at least for the first half). It's almost impossible to get any more immediately practical than that book. LYAH is a poorly done book and you're not going to learn much of anything from it. (PS: almost all intro books stop before HPFFP even hits halfway through. You can stop at chapter 20 or so if you want and be just fine and productive for a lot of real-world code)
With `TypeApplications` you have to think of types as parameters as well, you wouldn't just go changing normal parameter order and expect things to work :) In this case if you want to change `tuple ∷ Int → b → (Int, b)` without breaking callers you can write it as `tuple ∷ forall b a. a → b → (a, b)`. 
That's great! Add some documentation or code example please.
Wow, I love that article. His examples are very good! Each new example gave me a memory, together with: "oh, that explain so many things..."
Or do they?
What It seems like it might be something relevant to my interests (I do some work on analyzing conversations) but I'm not sure I understand what you're describing. What is the ultimate goal?
Indeed, I actually mean "subgroup generated by x".
In the post-backpack world, will lens signatures be common for data models? unit fruit-indef where signature Fruit where data Apricot data Pit pit :: Lens' Apricot Pit
Yeah, but that's not exactly a nice clean solution. At that point you might as well have just used a list from the start.
Just use [non-empty](https://hackage.haskell.org/package/semigroups-0.18.1/docs/Data-List-NonEmpty.html) lists for lamba and call. Correct by construction yo.
All the problems with overlappable instances are present without orphan instances. module A where data X a = X a instance {-# OVERLAPPABLE #-} Ord a =&gt; Ord (X a) where X a &lt;= X b = a &lt;= b foo :: Ord k =&gt; k -&gt; k -&gt; Map (X k) Int foo k1 k2 = fromList [(X k1, 1), (X k2, 2)] {-# LANGUAGE FlexibleInstances #-} module B where import A data Y = Yes | No deriving Ord instance {-# OVERLAPPING #-} Ord (X Y) where X a &lt;= X b = a &gt;= b foobar = foo Yes No In this example, `foobar` will yield a map with a structure that is illegal only in the `B` module or modules that import `B`, because `foo` is not able to use the overlapping instance, but any code with `B` imported will. I'd wager that for any problem with overlapping instances given by orphan instances, I can construct a scenario where clever module imports also give the problem.
the Y combinator is a specific implementation, among many, of the fix function.
This is exactly what catamorphisms are for. Don't let the name scare you, they're easy... newtype Mu f = In {out :: f (Mu f)} cata fn = fn . fmap (cafa f) . out Done defining catamorphisms. Really. Don't worry^[1] about how this works, it just does. instance Show (f (Mu f)) =&gt; Show (Mu f) where show (In f) = show f data ListF a b = Cons a b | Nil deriving (Show, Functor) type List a = Mu (ListF a) -- use Mu to make the data type recursive, into the last type parameter cons a = In . Cons a end = In Nil And so in the REPL ... &gt;&gt;&gt; cons 'a' end &gt; Cons 'a' Nil Perfect. sumList = cata $ \case -&gt; Nil -&gt; 0 Cons a b -&gt; a + b &gt;&gt;&gt; sumList $ cons 1 $ cons 2 $ cons 3 end &gt; 6 ... but that's a bit boring... data ExprF un bin a b = Val a | Un un b | Bin bin b b val = In . Val un op = In . Un op bin op a = In . Bin op a evalOp un bin = cata $ \case -&gt; Val a -&gt; a Un op b -&gt; un op b Bin op a b -&gt; bin op a b At this point, we haven't even decided what operations we're going to support, but we can *already* traverse them. Let's add... data Peano = Suc | Pred data Arith = Add | Sub | Mult | Div eval = evalOp un bin where un Suc = (+1); un Pred = subtract 1 bin Add = (+); bin Sub = (-); bin Mult = (*); bin Div = (/) &gt;&gt;&gt; eval $ bin Add (val 1) (val 3) &gt; 4.0 -- it's 4.0 instead of 4 because we're supporting (/) I think there's a way to turn a data type expressed this way into a zipper automatically, but I don't remember how... or that may have just be remnant of a weird dream. Also note that right up until this block, this code had nothing to do with math expressions. data Simple = Print | ReadFile data Complex = WriteFile evalIO = evalOp un bin where un Print a = a &gt;&gt;= print &gt;&gt; return "()"; un ReadFile a = a &gt;&gt;= readFile bin WriteFile a b = a &gt;&gt;= \x -&gt; b &gt;&gt;= writeFile x &gt;&gt; return "()" ... and this *kinda* works as well. It's not as pretty because this simple version requires every op to have the same return type, so this is pushing the simple `ExprF` *too* far, since `print` and `readFile` fundamentally aren't the same. The fact that there are monads used here is *not* a problem! You can still use GADTs to make it work with different types, but this is probably enough of a start - just don't use it like this last block. ---- [1] It works because you have a value `Mu f`, you peel off the `In` constructor getting a value of type`f (Mu f)` which`cata f :: Mu f -&gt; a` wants to work on the *contents* of. We assume `f` is a functor, so you can `fmap (cata f) :: f (Mu f) -&gt; f a` over `f (Mu f)` to get `f a` which is what `fn :: fa -&gt; a` wants to work on, giving you `a`. 
You wouldn't actually want to do this in practice, because when you are writing neural networks you want to combine both integer and floating point tensors in the same model, and occasionally even want to control which tensors live on CPU vs GPU. So you want actually to code against the specific implementation, not against some general Tensor type.
for the record (in case this arises in future googles) here's the related ticket, which describes what was done to resolve the issue: https://github.com/haskell/haskell-platform/issues/281#issuecomment-323498623
just to be clear, the 8.0.2 platform was also updated to work with the creators update.
Your second link is broken, you need to append an 'f'
&gt; cata fn = fn . fmap (cafa f) . out You probably meant `cata f`
While having good (gpu backed) tensor operations are definitely one piece of the puzzle, perhaps the most difficult part in Haskell is defining the network structure. Not having referential equality makes defining graphs a little trickier. I actually think that proposal makes sense, because for a given network you need floating point numbers and integers but what those don't need to be tied down to a concrete type... e.g. if you're running on an embedded device, you'll probably have fp16. Of course things like data-reify and such let you preserve sharing, which have been used quite succesfully by libraries like accelerate. Then on the other side it's important to be able to pull apart models - rearrange the parameters, cut and paste bits of models into other models etc. So, I think there needs to be something more - a way to float out parameters and manipulate them by name. Perhaps some interface akin to pytorch could work: parameter :: String -&gt; [dims] -&gt; Network (Tensor [dims]) linear :: String -&gt; n -&gt; m -&gt; Network (Tensor n -&gt; Tensor m) linear name n m = do weight &lt;- parameter (name ++ ".weight") [n, m] bias &lt;- parameter (name ++ ".bias") [m] return (\input -&gt; weight `mm` input + bias) Excuse the pseudo dependent types, I'm assuming things like 'n' and 'm' could be GHC TypeLits naturals or something similar. I feel like just using some kind of monad with a unique store could be fine for defining model parameters, along with data-reify or the like for defining functions on those parameters. It's just a very large design space. Unlike pytorch, if we want to name our parameters we cannot just use the name of the member variable unfortunately! Perhaps another alternative is to use data structures with references to the parameters, thus allow the user to choose how to refer to them? data Linear n m = Linear { weight :: Parameter [n, m], bias :: Parameter m } linear :: n -&gt; m -&gt; Network (Linear n m) applyLinear :: Linear n m -&gt; Tensor n -&gt; Tensor m 
Can't we do hash-consing to "discover" sharing? Or resort to a monadic interface?
In the case of dates you would still choose the difference unit (years? Days? Seconds? arbitrarily ("dilation symmetry"). Why not go for the Planck time as inherent unit? If you choose the big bang as natural zero, you can just use ℝ! 😜
I guess hashing the expressions should work for re-discovering the sharing in the expressions (instead of data-reify), so for the above code snippets we assume 'Tensor' is some kind of expression type (akin to pytorch's 'Variable'). Then we just need a way to label the leaves (the parameters), for which a monadic interface which generates unique names/unique store for the actual values seems reasonable to me?
A shitty webapp with GHCJS that can classify cat pictures with a neural network using Grenade. 
So the issue is with the current implementation of overlapping instances. I incorrectly assumed it was implemented coherently and with sound assumptions. The following is NOT coherently typed. Considering the overlappable `Ord (X a)` instance. foo :: Ord k =&gt; k -&gt; k -&gt; Map (X k) Int foo k1 k2 = fromList [(X k1, 1), (X k2, 2)] It should be: foo :: Ord (X k) =&gt; k -&gt; k -&gt; Map (X k) Int foo k1 k2 = fromList [(X k1, 1), (X k2, 2)] Which completely solves the issue. The former should NOT type check. As it incoherently assumes that using the `instance Ord a =&gt; Ord (X a)` dict is unambiguously the correct one, when clearly that isn't guaranteed. 
Substraction does not give a monoid: it is not associative. So that's a big difference between addition and substraction. 
Okay I went ahead and sunk a few hours into it! [Demo](https://ajnsit.github.io/concur/examples/kssu.jsexe/index.html). [Source](https://github.com/ajnsit/concur/blob/master/examples/KirbySuperStarUltra.hs).
Still not quite there. If the point is that inverse elements should have different directions, then you'd need to say "submonoid generated by x".
That's true, thanks.
That would be totally fine if the `forall` bindings were considered part of the interface in any other part of Haskell. The problem is that they are not, and indeed they are generally invisible.
Oh that's very cool!
I think you mean show . (read :: Int -&gt; _) :) With that addition the only example that's compelling is map = (fmap :: (a -&gt; b) -&gt; [a] -&gt; [b]) map = fmap @[] and then woah! Stop right there! How do we know `@[]` applies to the functor parameter and not `a`? This is getting worse and worse ... 
[EDIT: I got a bit confused writing this. /u/spaceloop gives a better example] Here's another potential source of headaches. type Pair a b = (,) b a mkPair :: a -&gt; b -&gt; Pair b a mkPair = (,) Which of these is correct, mkPair @Int (1, "Hello") or mkPair @Int ("Hello", 1) (I don't actually know because I don't have 8.0 installed but I'd appreciate an answer.) If it's the former then that's Really Bad^TM. It means that substituting a type synonym breaks consuming code. If it's the latter that's just fairly bad because it means I can't know the interface of `mkPair` just from looking at its type signature. Furthermore, replacing `type Pair` with `newtype MkPair` breaks consuming code even if that consuming code only used `Pair` abstractly! [EDIT: Fixed type, thanks to /u/spaceloop]
Sounds like a [group action](https://en.m.wikipedia.org/wiki/Group_action). Edit: A torsor seems to be a group action, where there is only a single orbit, e.g. a transitive group action. Edit 2: the ncatlab link also mentions freeness as a requirement, so torsors are [*regular* group actions](https://ncatlab.org/nlab/show/regular+action)
Non-Mobile link: https://en.wikipedia.org/wiki/Group_action *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^102650
**Group action** In mathematics, an action of a group is a way of interpreting the elements of the group as "acting" on some space in a way that preserves the structure of that space. Common examples of spaces that groups act on are sets, vector spaces, and topological spaces. Actions of groups on vector spaces are called representations of the group. Some groups can be interpreted as acting on spaces in a canonical way. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Serde is an interface for multiple formats – JSON is only one of them. So you can't just write a single JSON ADT, you need something more general. In addition to flexibility, Serde cares a lot about performance, so creating an intermediate ADT that immediately gets torn down is wasteful and causes lots of unnecessary allocations. As a result, deserializers are instead written in a cooperative CPS style. Moreover, the continuations are not stored in plain sum types A (A -&gt; r) | B (B -&gt; r) | C (C -&gt; r) | ... but instead they are simply methods of a trait { self :: self, a :: self -&gt; A -&gt; r, b :: self -&gt; B -&gt; r, c :: self -&gt; C -&gt; r, ... } so the compiler knows statically what function gets called and can easily inline the functions (because they are a free functions, not closures).
We know that because type applications first apply on type parameters with constraints. The whole "left to right" thing. IMO it would be nice if forall syntax was very lightweight like say lambda syntax and if it was required to have an explicit forall to use type applications. And even the minor syntax reduction on the other examples is more than enough for me. I really want that generic `fromList` and similar and I want it to be as lightweight as possible to use. 
&gt; We know that because type applications first apply on type parameters with constraints. So I can break my consumers by removing a redundant constraint. &gt; if it was required to have an explicit forall to use type applications. Something like that sounds much better. &gt; And even the minor syntax reduction on the other examples is more than enough for me. That's great for you. Unfortunately I have to read code written like this against my will.
It's strange.. this was a project I've been thinking about doing, so I was really surprised to see your post. I'll probably still do it someday. Anyways good luck
What is ADL?
I think, they’re referring to https://github.com/timbod7/adl.
that's pretty good! thanks!
Oops - I've amended to include the link.
If you don't care what Foo is, the obvious answer is `Iso' xs (i -&gt; x)`!
Have you heard about/tried [ghcid](https://hackage.haskell.org/package/ghcid#readme)?
I'm not sure choosing a unit is necessary, since converting between those is an isomorphism. You could get back an abstract time-difference type with a hidden internal representation, and then you could say: now I'd like to convert it to `Float` seconds.
I discussed it with /u/hanshogl and this works "fine" apart from strictness of `V2`, `V3` tabulateOf :: (Representable f, Representable g) =&gt; AnIndexedSetter' i (f (g a)) a -&gt; (i -&gt; a) -&gt; f (g a) tabulateOf rep f = iover rep (\i _ -&gt; f i) (pureRep (pureRep undefined)) if we define the vectors as `V2 a = V2 a a`, `V3 a = V3 a a a` this works exactly as we want &gt;&gt; tabulateOf rowMajor id V2 (V3 (0,0) (0,1) (0,2)) (V3 (1,0) (1,1) (1,2)) &gt;&gt; tabulateOf rowMajorAddr id V2 (V3 0 1 2) (V3 3 4 5) &gt;&gt; tabulateOf colMajor id V2 (V3 (1,1) (1,2) (1,3)) (V3 (2,1) (2,2) (2,3)) &gt;&gt; tabulateOf colMajorAddr id V2 (V3 1 3 5) (V3 2 4 6) ---- Another approach is populating the structure with units, now `V2`, `V3` from *linear* work but it's not a legit lens, `()` is never used so maybe a cheeky `unsafeCoerce ()` helps tabulateOf' :: (Representable f, Representable g) =&gt; AnIndexedSetter i (f (g a)) (f (g b)) a b -&gt; (i -&gt; b) -&gt; f (g b) tabulateOf' rep f = iover rep (\i _ -&gt; f i) (pureRep (pureRep (unsafeCoerce ()))) and now it works for the vectors from *linear* &gt;&gt; tabulateOf' rowMajor id V2 (V3 (0,0) (0,1) (0,2)) (V3 (1,0) (1,1) (1,2)) &gt;&gt; tabulateOf' colMajor id V2 (V3 (1,1) (1,2) (1,3)) (V3 (2,1) (2,2) (2,3)) 
You're right. Take a look at my [response to /u/davidfeuer](https://www.reddit.com/r/haskell/comments/6udl0i/representable_functors_parameterised_by/dlu628e/), where it seems like `AnIndexedSetter` is sufficient to `tabulate` any applicative structure
Yes, I tried it a while ago. At that time my workflow was cabal+vim+tmux. And I found it absolutely AMAZING. However, what was missing was a way to jump from ghcid error to the code. I since switched to spacemacs+intero+stack, and never really managed to make ghcid work with stack. It breaks all the time. It seems the way I launch it, doesn't pick everything from the stack configuration and I have pass lots of arguments on the cmd line (like source directory etc ...). I managed though to get it working inside of Spacemacs (using `compilation`), even though it's not ideal. At the moment it's broken anyway :-( Also, I understand that ghcid use dynamic reload from ghci, but is that not what intero is doing as well ? In that case, ghcid shouldn't be faster than intero should it ?
## [link to excellent paper](https://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/aplicative.pdf) Yes I have, it doesn't answer my question but it has a lot of potential applications for parameterized representable ('*Naperian*') functors
Oh yes, I know exactly what you mean wrt jumping to the code. That was a pain-point for me too for quite some time. At the moment I am quite satisfied with how I resolved it: I am able to ctrl+click any file 'links' on my terminal e.g. `src/long/relative/path/to/file.hs:2203:12` and that re-focuses/opens the file in my `$EDITOR` at the exact line and column. Maybe you can set something like that up for your terminal+editor too? Or if you are interested I can tell you more about my setup and share the file-clicking plugin I put together for that too. Maybe others can answer you about intero, since I don't know much about that.
Now we can create `newtype`s newtype RowOrder f g a = RO (f (g a)) deriving (Functor, Show) newtype RowAddrOrder f g a = RAO (f (g a)) deriving (Functor, Show) newtype ColOrder f g a = CO (f (g a)) deriving (Functor, Show) newtype ColAddrOrder f g a = CAO (f (g a)) deriving (Functor, Show) for each ordering type RepBy r = (Representable r, Traversable r) instance (RepBy f, RepBy g) =&gt; Representable (RowOrder f g) where type Rep (RowOrder f g) = (Int, Int) index :: RowOrder f g a -&gt; ((Int, Int) -&gt; a) index (RO matrix) = indexOf rowMajor matrix tabulate :: ((Int, Int) -&gt; a) -&gt; RowOrder f g a tabulate f = RO (tabulateOf' rowMajor f) instance (RepBy f, RepBy g) =&gt; Representable (RowAddrOrder f g) where type Rep (RowAddrOrder f g) = Int index :: RowAddrOrder f g a -&gt; (Int -&gt; a) index (RAO matrix) = indexOf rowMajorAddr matrix tabulate :: (Int -&gt; a) -&gt; RowAddrOrder f g a tabulate f = RAO (tabulateOf' rowMajorAddr f) instance (RepBy f, RepBy g) =&gt; Representable (ColOrder f g) where type Rep (ColOrder f g) = (Int, Int) index :: ColOrder f g a -&gt; ((Int, Int) -&gt; a) index (CO matrix) = indexOf rowMajor matrix tabulate :: ((Int, Int) -&gt; a) -&gt; ColOrder f g a tabulate f = CO (tabulateOf' colMajor f) instance (RepBy f, RepBy g) =&gt; Representable (ColAddrOrder f g) where type Rep (ColAddrOrder f g) = Int index :: ColAddrOrder f g a -&gt; (Int -&gt; a) index (CAO matrix) = indexOf rowMajorAddr matrix tabulate :: (Int -&gt; a) -&gt; ColAddrOrder f g a tabulate f = CAO (tabulateOf' rowMajorAddr f) and we get this business &gt;&gt; tabulate @(RowOrder V2 V3) id RO (V2 (V3 (0,0) (0,1) (0,2)) (V3 (1,0) (1,1) (1,2))) &gt;&gt; tabulate @(RowAddrOrder V2 V3) id RAO (V2 (V3 0 1 2) (V3 3 4 5)) &gt;&gt; tabulate @(ColOrder V2 V3) id CO (V2 (V3 (1,1) (1,2) (1,3)) (V3 (2,1) (2,2) (2,3))) &gt;&gt; tabulate @(ColAddrOrder V2 V3) id CAO (V2 (V3 0 1 2) (V3 3 4 5)) Hopefully this means we can **derive** different strategies with [this](https://www.reddit.com/r/haskell/comments/6ksr76/rfc_part_1_deriving_instances_of/) newtype Matrix f g a = Matrix (f (g a)) deriving via ColOrder f g (Functor, Distributive, Representable) deriving via Co (Applicative, Monad, MonadReader (Int, Int)) deriving via Co (FunctorWithIndex (Int, Int), FoldableWithIndex (Int, Int), TraversableWithIndex (Int, Int))
I've kind of sorted the jump problem by using emacs, which detect source lines automatically. The problem is I use a compilation window, which doesn't expect refreshing (as ghcid does). Otherwise, I just tried to run ghcid and that's what I get ghc: panic! (the 'impossible' happened) (GHC version 8.0.2 for x86_64-unknown-linux): getLabelBc: Ran out of labels Please report this as a GHC bug: http://www.haskell.org/ghc/reportabug ]0;All good (running test) - Fames Running test... &lt;interactive&gt;:10:72: error: Variable not in scope: main :: INTERNAL_GHCID.IO a0 :-( 
Oh, and I almost forgot to respond to another point of yours: &gt; and never really managed to make ghcid work with stack. Now this sounds very weird for me, because I'm using ghcid with stack all the time, maybe on like a dozen different projects or so. Here are some examples on how I invoke it: - `ghcid -c 'stack ghci'` - `ghcid -c 'stack ghci proj-name:exe:proj-name'` - `ghcid -c 'stack ghci proj-name:exe:proj-name --ghci-options=-fno-code --no-build'` - `ghcid -W -c 'stack ghci' -T ':set args ["--color"]' -T main` Are you invoking it similarly? Or some other way?
I was referring tongue in cheek to the link /u/dave4420 gave.
&gt; which doesn't expect refreshing (as ghcid does) Hmm, on the surface, to me that sounds like a similar issue as described here: https://gist.github.com/lspitzner/5ae6638554f9e7457dccb462cb415987 Do you think you could use the `ghcid -o=ghcid.txt` + `sleep 0.5; cat ghcid.txt`trick in the emacs '`compilation` window'? That way you could trick emacs into thinking it is doing the compiling, while avoiding the redraw attempts by ghcid in that window.
Well, it's really strange. I have 3 exe in my project, and I am only interested in project 2. If I do `stack ghci` it picker the wrong one. If I don `stack ghci Fames:exe:Fames` it complains it can't find `Fames` package (the main lib). However, If remove all the other exe, it seems to launch fails on other errors. 
The next step is reifying values like such as `IndexedTraversal`s to the type level to influence deriving, that way we can define a single `newtype` newtype Order i f g a = O (f (g a)) deriving (Functor, Show) once and for all newtype ReifiedO i f g = ReifiedO (forall a. IndexedTraversal i (f (g a)) a) instance (Eq i, Given (ReifiedO i f g), RepBy f, RepBy g) =&gt; Representable (Order i f g) where type Rep (Order i f g) = i index :: Order i f g a -&gt; (i -&gt; a) index (O matrix) i = indexOf indexedTraversal matrix i where ReifiedO indexedTraversal = given @(ReifiedO i f g) tabulate :: (i -&gt; a) -&gt; Order i f g a tabulate f = O (tabulateOf' indexedTraversal f) where ReifiedO indexedTraversal = given @(ReifiedO i f g) then we can pass them as values ReifiedO @(Int, Int) rowMajor :: (Traversable g, Traversable f) =&gt; ReifiedO (Int, Int) f g ReifiedO @Int rowMajorAddr :: (Traversable f, Traversable g) =&gt; ReifiedO Int f g ReifiedO @(Int, Int) colMajor :: (Traversable f, Traversable g, Distribute f, Distributive g) =&gt; ReifiedO (Int, Int) f g ReifiedO @Int colMajorAddr :: (Traversable f, Traversable g, Distribute f, Distributive g) =&gt; ReifiedO Int f g and hopefully effortlessly jump between row-order and col-order newtype M23 a = M23 (V2 (V3 a)) given ReifiedO @(Int, Int) rowMajor deriving via Order (Int, Int) (Functor, Distributive, Representable) newtype M23' a = M23' (V2 (V3 a)) given ReifiedO @Int colMajorAddr deriving via Order Int (Functor, Distributive, Representable)
Could you share perhaps the exact error message and (parts of) your cabal file for that project? E.g. I'd like to see the package name and exe name in the cabal file. Since sometimes executables are named `Fames:exe:Fames-exe` or different.
Basically I need to add a few more options to make it work. (some `-i`) but when it works it crashes with `Variable not in scope: main :: INTERNAL_GHCID.IO a0` error. 
How would you scale this approach to a more complex language and more complex traversals? Consider: import Control.Applicative data Expression a = LetRec a [Statement a] (Expression a) | Var a String deriving (Show) data Statement a = Statement a String (Expression a) deriving (Show) type Scoping a = [String] -&gt; a ask :: Scoping [String] ask = id local :: ([String] -&gt; [String]) -&gt; Scoping a -&gt; Scoping a local f g = g . f scopingExpression :: Expression a -&gt; Scoping (Expression (Maybe Bool)) scopingExpression (Var _ x) = do xs &lt;- ask return (Var (Just (x `elem` xs)) x) scopingExpression (LetRec _ statements expression) = do let ys = map (\(Statement _ y _) -&gt; y) statements statements' &lt;- local (ys ++) (traverse scopingStatement statements) expression' &lt;- local (ys ++) (scopingExpression expression) return (LetRec Nothing statements' expression') scopingStatement :: Statement a -&gt; Scoping (Statement (Maybe Bool)) scopingStatement (Statement _ x expression) = do expression' &lt;- scopingExpression expression return (Statement Nothing x expression') example :: Expression () example = LetRec () [Statement () "x" (Var () "y")] (Var () "x") We want to annotate for each variable if it is in scope or not. We have the following requirements: 1. Our AST consists of different mutually recursive node types. 2. Each AST node is annotated. Perhaps we want to change the type of annotation. 3. We want to traverse with an arbitrary `Applicative`. (This allows us to recover `map` and `fold`) 4. Sometimes we want to modify the recursive call, for example if we want to run it with a modified environment. How would you minimize boilerplate subject to these constraints? For example `scopingStatement` feels boilerplatey to me. Can we avoid it? 
 ~/devel/mae/Fames/ stack ghci Fames:exe:Fames Fames-0.0.0: initial-build-steps (lib + exe) The following GHC options are incompatible with GHCi and have not been passed to it: -threaded Configuring GHCi with the following packages: Fames Using main module: 1. Package `Fames' component exe:Fames with main-is file: /home/max/devel/mae/Fames/app/main.hs GHCi, version 8.0.2: http://www.haskell.org/ghc/ :? for help &lt;command line&gt;: cannot satisfy -package Fames-0.0.0 (use -v for more information) stack: callProcess: /home/max/.local/bin/stack "ghci" "Fames:exe:Fames" (exit 1): failed The cabal file (as well as the fulll project) can be found [there](https://github.com/maxigit/Fames/blob/master/Fames.cabal). It's generated by hpack. Also the command line I use to launch ghcid can be found in the [makefile](https://github.com/maxigit/Fames/blob/master/makefile#L70). 
Splitting up the project into different packages likely won't make a difference. However, breaking up modules almost certainly will. The module is GHC's unit of compilation. This means that any small change in a large module will require that GHC recompile it and probably many of modules depending upon it from scratch. Splitting up large modules isn't always easy and can require some thought but the results are generally worth the effort. Another thing to keep in mind is that `INLINE` and `INLINEABLE` pragmas carry a real cost. Say you have a module called `Lib` which exports a function called `func`. If `func` is inlineable (either by pragma or inferred by GHC) then any change in `func`'s implementation will require recompilation of all modules calling it. However, if `func` is not inlineable then you can make any (calling-convention preserving) change you want to it and GHC only needs to recompile `Lib`. Another helpful tip is to compile with `--disable-optimization` during development. GHC can be quite speedy when it doesn't need to optimize (and if you aren't using stream fusion the regression in runtime performance can be quite acceptable).
I've tried (probably not enough) splitting modules ( I guess you mean file). but I end up at the end which one file including 100 modules). I ask about package, because my giant files, pretty much never change, however, they seems to be big burden on ghci (unless I am wrong, but I'm not sure how to track the problem). `--disable-optimization` how is that differetn from `-O0` or `stack build --fast` ?
Yes, exactly what I mean. 'Tensor' is a deep embedded syntax tree under the hood, that should open up possibilities for optimization.
How does it compare to https://github.com/HuwCampbell/grenade ? Also, seen http://www.datahaskell.org/ ? Fairly active chat-room there. 
That's correct, for instance x can *equal* x^-1 , in which case no function can take different values on them.
Other than to-do lists, do you have any suggestions? Is there such a list for Rust or Go or something?
I know of: - [Making a website with Haskell](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html) - [Streaming Huffman Compression in Haskell](https://blog.jle.im/entry/streaming-huffman-compression-in-haskell-part-1-trees) - [Shake Manual (Compiling C with Haskell)](http://shakebuild.com/manual) - [Compiling Lisp to JavaScript From Scratch in 350 LOC (self plug)](http://gilmi.xyz/post/2016/10/14/lisp-to-js) [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/index.html) also contains a bunch of application tutorials. If there are specific tutorials you'd like to see please request them. Maybe someone will pick up the glove.
To your first question the only way I can recover an indexed traversal from a `Representable` toIT :: forall r a b. (Bounded (Rep r), Enum (Rep r), Traversable r, Representable r) =&gt; IndexedTraversal (Rep r) (r a) (r b) a b toIT p ra = sequenceA (liftR2 (indexed p) positions ra) where positions :: r (Rep r) positions = set (unsafePartsOf traverse) [minBound .. maxBound] ra --- data Pair a = a :# a deriving via Co Pair (Functor, Distributive) instance Representable Pair where type Rep Pair = Bool index :: Pair a -&gt; (Bool -&gt; a) index (f :# _) False = f index (_ :# t) True = t tabulate :: (Bool -&gt; a) -&gt; Pair a tabulate f = f False :# f True &gt;&gt; itoListOf (toIT @Pair) ('a' :# 'b') [(False,'a'),(True,'b')]
I have been meaning to work on something like this but just couldn't get beyond reading. The article is gold, and am looking forward to the updates and contributing if possible!
Why are the arguments in the function-argument of foldl and foldr interchanged? 
- [Let's build a tiny CLI app in Haskell (Teleport)](http://bollu.github.io/teleport/) - [Let's build a tiny **optimising compiler** in Haskell](https://bollu.github.io/tiny-optimising-compiler/) Note that the optimising compiler posts are WIP, and I have too much on my plate right now :( 
Exactly. For instance, consider the element 1 in Z/2Z.
Hey, not a directly related question, but I wanted to know why Typescript and not Elm, GHCjs or Purescript?
I know the basics of Python, how hard will it be to learn Haskell in a weekend?
Very difficult, speaking from experience. It's not only a different language but a different paradigm.
So they are the right way round if you are thinking of the function as an infix operator: foldl f z [x0, x1, x2] = ((z `f` x0) `f` x1) `f` x2 foldr g z [x0, x1, x2] = x0 `g` (x1 `g` (x2 `g` z)) We want `f` to take `z` as the first (left hand) argument but `g` to take it as the second.
I actually wrote a todo.txt app in Haskell and created a set of blog posts about the process. I use stack to do building/dependency management, data types and app structure, Hspec for Unit Testing, Parsec for parsing the user input. [Here](https://commentedcode.org/blog/2016/07/30/haskell-project-stack-and-data-types/) is the first post in the sequence. Its made to be a much more basic app, not getting too deep into advanced topics. The code is available on [github](https://github.com/jecxjo/todo.hs) and contains a majority of the base functions in the todo.txt shell script. 
Almost everything is different, lazy, not mutable, no while or for loops. But its great to get a new perspective on things
I see Haskell as a long term investment! Personally a weekend was not enough when I was starting out, but got me hooked into function programming. While learning any programming language, you always have some sense of accomplishments when you understand something that seems arcane, before Haskell, for me was: pointers, generics and finally lambda (C#, Java). Yet I got more breakthroughs learning Haskell than all other languages: monoid, functor, applicative, monad, monad transformer, lens (just hit the surface). I firmly believe this led me to become a better programmer (language independent)! I still have so much more to learn.. That's what I like the most about Haskell.
Makes sense. Thanks! 
I second very difficult. If you put a lot of effort in you can learn most of it in a few weeks. There is so much more to haskell that python when you take into account language extensions.
That makes me really want to take on Haskell as I'm looking for a way to quickly ramp-up my CS skills. Perhaps forcing my way through weeks or months of tutorials and projects in Haskell will let me finally break out of my multi-year noob phase in programming! :)
It's a web server (user frontend). I have some subsite (in the Yesod meaning) which I could try to split in different package, but at the end it will be on application. 
I dislike StrictData. Now I look at a datatype definition and I can't really know how it behaves without looking at the language pragmas.
I can't find anything to back up that statement so it's possible I cargo culted it from somewhere - or am attributing this rule to the wrong extension.
I can completely understand your pain. It is shocking that we don't have a solution for such a basic everyday need. Typesafety amounts to nothing if you can't get your app to market on time. We need correctness AND speed. Anyways, I'm solving this issue by a manual code gen script (WIP) which I'm planning to open source once I've confirmed that it actually solves the problem. It looks at the DB and auto-generates all model + type definitions into individual modules (I have about 300+ auto-generated files in my project right now). The imports are set up in such a way that no single file becomes a compile bottleneck. You end up recompiling only what is strictly required. Pray that this succeeds. I have found no other way to solve this problem. 
re: `mapalgebra`. The code itself is pretty heavily documented, and I'll have an extensive README before I publish a 1.0 to Hackage. 
So how do you actually call that function then? Supposedly the caller must also use such a constraint. Some module has to be the one to decide what's canonical.
What is the most active and best documented library/framework that can be used to make REST services?
&gt; . It looks at the DB and auto-generates all model + type definitions into individual modules (I have about 300+ auto-generated files in my project right now That's what I do as well, except I'm generating Persitent model, which are then fed to TH : Persistent TH generation and route. I tried splitting it in different modules, but there is still one module which import everything. Also linking takes Aaaaaaaages .....
You might want to have a look at haskellbook.com then. Seems to be the best resource for haskell beginners.
&gt; The imports are set up in such a way that no single file becomes a compile bottleneck Maybe that's what I need to do, but I'm not sure how.
Lock-step simulation is very much related to [time traveling debugging](http://debug.elm-lang.org/). Also, database transactions. A typical example where the discipline imposed by the language reaps huge benefits. Note that this requires to keep a log of all past events! Although we could 'garbage collect' all events which have happened before all players' last response, so we probably should be fine. Edit: Just 10s after having written this, Joachim talks about 'materialising'/folding the the log to a rigid snapshot.
Most active perhaps Servant and it allows you to write your REST API separately from the implementation, a bit more time investment than something like Scotty, but totally worth it.
Depends what you mean by learning Haskell. If you mean having a basic understanding on the syntax and the type system (excluding typeclass) and being able to solve most of [99 problems](https://wiki.haskell.org/H-99:_Ninety-Nine_Haskell_Problems), then I would say it's doable. If you start now, you should be able tell us by tomorrow how hard it is ;-)
I have `[A String]` where `A` is an applicative functor. Can make this an `A [String]`?
Let's collaborate on this offline, shall we? 
What is the single file that imports everything? 
Great article! The name "Haskoin" is already taken though.
The main, basically. Should be `Foundation.hs` in the Yesod scaffolding.
Uhh... you just call it... in any module. Test it yourself. It's possible I am wrong but I am pretty certain with that with the new type signature and no orphans you won't succeed in breaking anything or observing any incoherence. 
**[Code School Angular Intro](https://www.codeschool.com/courses/shaping-up-with-angularjs)** - The app was so simple (an online store) that we could really focus on how the framework worked and with that knowledge, use it to build apps. **[Scotch Angular 2 Starter](https://scotch.io/courses/getting-started-with-angular-2)** - Like above, the app was dead simple (contact list) allowing the author to focus on introducing the framework and Typescript. It again provided a great reference for building future applications. With the above two examples it's really the way the content is presented that makes them noteworthy. They do a good job of explaining what is happening and how it works. There isn't too much of a need to have to scour the internet to fill in the gaps.
Yes, that is `sequenceA`.
Would it be better if GHC were a link time optimizing compiler? That is, instead of spending time optimizing all functions ahead of time, if GHC merely output an unoptimized representation of Core, then optimized all modules together at link time, you could save a lot of redundant work from happening. A function would never have to be specialized to the same type twice, functions marked as inlineable could only force the reoptimization of functions that actually inline it (and not the whole module), and the part most people actually care about in their dev cycle (type checking) would be extremely fast. Basically it would be moving all the compile time to the linking phase, except I bet with all the information available at once, the optimizations can be made to take much less total time. Plus there are new features possible with this. It would be possible to specialize functions that aren't marked as inlineable. Actually, I supposed the INLINEABLE pragma could basically be deprecated, except to mandate phase control.
`sequenceA` from `Data.Traversable` sequenceA :: (Traversable t, Applicative f) =&gt; t (f a) -&gt; f (t a) In this case, `t` is the list functor. You can use [Hoogle](https://www.haskell.org/hoogle/?hoogle=%28Applicative+f%29+%3D%3E+%5Bf+a%5D+-%3E+f+%5Ba%5D) to search for simple functions like this.
I've added them to the list, thanks. BTW. your posts don't seem to have a &lt;title&gt; tag Edit: I haven't seen such a list in other languages. I can usually dig up some good posts on Google. However, I'm having some trouble doing that in Haskell so I figured I'd ask.
You can use `sequenceA` ([link](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Traversable.html#v:sequenceA)). And the relevant [Hoogle search](https://www.haskell.org/hoogle/?hoogle=Applicative+f+%3D%3E+%5Bf+String%5D+-%3E+f+%5BString%5D).
Well no my point is that any function that calls it must also constrain with `Ord (X k)`, and that some module has to actually decide when to plug in an actual instance. How is that decided? Is it just when the constrain is fully monomorphized? That will fail for any constraint depending on an unused phantom type variable, especially when using the `reflection` package.
It sounds like it covers a lot of the development process. Can't wait to have a read. 
I mean how is that different from basically all constraints? Yes you eventually have to pick a type for `k` and thus in the process pick up the instance dictionary or you will get a compile time ambiguity error. Can you give an example where the existing dangerous implementation will work (correctly) but my change will cause compilation to fail?
I found a bunch of Haskell books on Packtpub. I"m ok using it as I get free access via my company. What do you think of this one: https://www.packtpub.com/big-data-and-business-intelligence/advanced-data-analysis-haskell-video
Actually I think I was thinking about something wrong. My concern was that any instance you might choose could always be overlapped and make you wrong. But I suppose if only OVERLAPPABLE instances can be overlapped, then this could never happen. That said, this is unfortunately not how GHC works, and would be a breaking change. So I can't recommend the usage of overlappable instances. Also, I think there may be better generalizations here. I feel like there could be an algebraic logic to all of this, rather than all the special rules we have now.
Is there an ELI5 for MonadFix and RecursiveDo? I've read the ocharles post on RecursiveDo but it went over my head a bit.
Yeah the whole "requiring both overlap and overlappable" thing is completely necessary for coherence. But it is also sufficient when combined with no orphans and requiring the type signature I specified above. I've definitely run into situations where overlapping instances would be convenient. Such as with having a backup implementation of a typeclass. E.g Show when writing a pretty printing type class. I guess overall I haven't found them to be a complete necessity, just convenient. IMO the above changes would be nice as they would make them safe to use. And it would work out nicely for situations like pretty printing. 
Yesod has the most active community and the best documentation. Servant is for advanced Haskellers only IMO.
See also [this answer](https://stackoverflow.com/questions/12288818/why-does-fold-left-expect-a-b-a-instead-of-b-a-a/12289254#12289254). The argument order starts to make sense once you draw as a diagram what folds do.
I like to have a separate `models` package for whatever thing I'm working on. This contains very low churn code that mostly defines the database models and related types. Since it is a separate package, GHCi doesn't have to reload it, and it removes a lot of strain from Intero.
Nobody has mentioned it, but is this lack-luster compile-time performance because of Template Haskell? Or is the compile-time of normal Haskell always an issue in larger projects?
the problem isn't that it's difficult, it's that it's fundamentally different. Most people are used to learning new language as different syntax + one or two new concepts. Coming to haskell from python, you rethink what programming is from the ground up. It helps pretend you are learning programming from the first time and see things with fresh eyes. So yeah, not a two-day activity.
Does Foundation import all modules and consequently most files import Foundation? If yes, then this is the exact problem I'm solving with my code generation. 
+1 for servant. it's often used for servers, but it works well for clients. 
A "book" that's more like just a series of articles: * [OpenGL Basics](https://lokathor.gitbooks.io/using-haskell/content/opengl/) * [Roguelike stuff](https://lokathor.gitbooks.io/using-haskell/content/roguelike/) (incomplete at the moment)
I have also tried this https://github.com/noughtmare/hypercube. I got stuck trying to figure out if it is better to use mutable vectors or immutable vectors... I guess I am optimizing prematurely.
You forgot statically typed.
In your example I don't think the type synonym is a problem. I guess you meant this type: mkPair :: a -&gt; b -&gt; Pair b a mkPair = (,) So https://ghc.haskell.org/trac/ghc/wiki/TypeApplication mentions &gt; One does not need to have the “-XExplicitForAll” extension turned on to use “-XExplicitTypeApplication”. Without the forall flag, generally types will be inferred by simply stacking all of the foralls at the beginning. Which according to the examples is in a left-to-right order. So you would write this, regardless of whether the type synonym is expanded mkPair @Int 1 "Hello" and end up with ("Hello, 1) :: Pair Int String But I guess sometimes substituting type synonyms might actually break application, e.g. refactoring mkPair :: a -&gt; b -&gt; (a,b) mkPair = (,) to this: type MySig b a = a -&gt; b -&gt; (a, b) mkPair :: MySig b a mkPair = (,) would make this stop compiling: mkPair @Int @String 3 "Hello" But no GHC at hand either to check...
Most recently: * A package providing throttling functionality for general IO functions: https://github.com/mtesseract/io-throttle * A package built on top of io-throttle to provide throttling for Conduits: https://github.com/mtesseract/conduit-throttle * A package providing bindings to the [Nakadi event bus](https://zalando.github.io/nakadi/): https://github.com/mtesseract/nakadi-haskell
While it's good that Stackage is generally very reliable, is there any need for the immediate Hackage bashing? &gt; Stackage is having a problem, but Hackage is so much worse because X, Y, and Z
&gt; That makes me really want to take on Haskell as I'm looking for a way to quickly ramp-up my CS skills. I know this is Haskell sub, but in my experience I had much easier time transferring my Python knowledge to another functional language - F#. With Haskell I was fine with basics, but suddenly everything started to look quite complex/complicated.
Mine's much more of a toy than grenade, which I actually hadn't seen before. I only (currently) implement fully connected layers, for example, and I haven't been concerned remotely with speed. Also, my base type looks like `(Floating f, Dim a, Dim b) =&gt; Network f a b`, where `a` and `b` are the sizes of the input and output respectively. I think this is a lot simpler, but although you can define a Category instance it's a bit trickier to work with. I was aware of dataHaskell but didn't realise they had a chatroom! Thanks for pointing that out to me
I'm not sure I understand but wouldn't that make thing even worst ? At least, once something has been compiled on optimised it doesn't have to be done again. What you are suggesting seems to mean that optimisation would occur each we link ?
there are quite a few on youtube - this is probably my favorite [writing sokoban in Haskell](https://www.youtube.com/watch?v=mtvoOIsN-GU)
When It might Import instead of Foundation, but basically, everything (model and database) is imported and reexported so that, you can use everything from any Handler. This is easily splittable, but of course the main module, which contains dispatch all the routes, needs to know everything. Having said, that I'm still not sure when the time is spend exactly.
There exist nontrivial permutations which are their own inverses, but directions on inverses are supposed to be different, so therefore permutation groups can't have directions.
Oh interesting. Well, it's attached to a larger music sequencer thing, which I do plan to release in not too long. I'll let you know when that happens.
I've seen many time that TH is slow, but is it really the case ? What I mean is, is it the code generation which is slow, or the compilation of thousand of hidden generated lines. I have written lots of code generator (using Haskell and slow String) generating thousands of lines of code, and it takes less than a second. I'm not sure why TH should be 100 slower ... Also, I tried the other day to bake the code generated by TH (using `-ddump-splies` and I think it didn't change anything. (To be honest, I might also have gave up because it was much harder than expected, but one way or the other,had I managed to gain significant performance, I would have remember ).
That's an interesting approach. Could you elaborate ?
Can Backpack signatures packages and "-indef" packages that depend on them be uploaded to Hackage yet?
Thanks to both of these groups. Generally, both Hackage and Stackage have saved me enormous amounts of time and I rely on them daily.
What are you generating and from what ? How do you split the import ?
But wouldn't it make more sense to have the same order for both of them so that it's easier to switch them from one to the other?
What is your development setup? On Windows I think I would use Notepad++ for longer scripts and the GHCI REPL for interactive testing. On Linux it would be Vim in one terminal and GHCI in another terminal. I've only played with GHCI on Linux...thinking about getting back in.
Just depends. It's not clear to me how much less work the optimizer would have to do in total with that strategy. I suspect it could be made to do quite a bit less work since it has much more information available to it. Plus, between the ability to minimize this phase during development cycles, and possibly the ability to cache parts of the optimizer results, I think you could *definitely* make it take less time. Regardless, the goal of link time optimization isn't to minimize incremental build times. The goal is to provide type checked solutions that can be dumbly linked just as fast as before, while having an optimizer that can perform much smarter whole-world optimizations to make a production build much faster.
I don't think so. Having them switched is pretty critical to not confusing them. I know I would have confused the important differences between them way more often if I didn't recognize the `foldr` argument uniquely has a `(:)`-like shape. The whole semantic difference between them is defined by this. The fact that you can ask "How would this operator look if I just chained it manually over all the elements?" to see how they would behave differently is pretty important to me.
In case linking takes the most time for you, did you switch from `ld` to GCC's `ld.gold` or even LLVM's `ld.lld`? IIRC you can get 3x and 10x linking speedup with those compared to plain `ld`, respectively.
Consider a project like: app app-models `models` contains all of the model code that takes *forever* to compile, but also rarely changes. You don't want to compile this more than you need to. So, when you load `ghci` for `app`, it doesn't load and interpret the `models` package. It only loads the `app` package and uses the previously built `app-models` package. `ghci` only needs to interpret the packages in your load targets. If you don't have a package as a load target, it is able to use the compiled library instead.
No. I haven't. How do I do that ?
I see. I didn't realize you actuall meant *package* and not *module*. That makes sens. That's a really good idea (even though extracting a little package without pulling everything else might be a bit tricky in my case).
Here is an example how for gold: https://www.reddit.com/r/haskell/comments/63shj7/link_with_the_gold_linker_for_faster_build_times/dfwphwi/ I haven't managed to figure out lld yet. If you do please do share.
Or http://learnyouahaskell.com/ which is free to read online
Ah. I looked up `sequence` but saw it needed a Monad.
On macOS, I use TextMate in one window and the Terminal in the other window. Works fine for me. :-)
How does Data.Memocombinators work? I understand using a lazy list to "purely" memoize single integer argument functions but I can't follow how the code works beyond that, or what data structure it's actually using.
Why is it worth it?
No, it's not necessary, but I can guess where it came from. In a separate thread, which I'm not going to link to (but if you dig deep enough you'll probably find), the opposite commentary is going on: Stackage is bad because of this situation, Hackage is much better. This Tweet storm seems to me to be a response saying that, in fact, the opposite is true. Personally, I'm ignoring both discussions, I recommend others do the same. As a small update on what's going on here, it's pretty simple: * Stackage documentation generation is working just fine (though see next bullet). The reason Stackage can reliably build documentation is because it has a simpler problem it's solving versus the Hackage documentation builder: it will only build docs when a known-and-tested good build plan (a Stackage snapshot) is found and tested. * This is a relatively minor issue where the cron job that generates the database with the list of modules is failing. Right now, it's simple to fix: kick the server (again, keep reading). * I already made a small fix so that the main package page on Stackage links to the most recently available docs, which will bypass the problem. * Overall, the current architecture of Stackage Server is stupid. I take blame on that one. In my defense: it made sense when Stackage was first starting and we had a dozen snapshots. But with the current collection of snapshots (hundreds? I haven't actually counted) it simply isn't scaling. The dumb architecture is storing an SQLite database in S3. Yes, it's really that dumb. It made initial testing easy, and made sense when we were planning on allowing arbitrary people to upload custom snapshots. That all disappeared years ago, so it's worth switching over to a PostgreSQL database. As is wont to happen of course, the thing just kept working, so neither I (nor anyone else) thought about it. Now's a good time to update things. On that note: if anyone is interested in participating in some relatively straightforward web development activities to clean up Stackage Server, please ping me. Something something great glory, destroy imperative programming, whatever I said last time :)
Why is Haskell "not very useful in isolation?" What does one typically need other languages for?
This is exactly the kind of level-headed leadership Haskell needs! Don't let the bashing get you down and please keep up the great work you're pouring into Haskell! We need more community members like you!
For want of anything to do, I started on a 2D game with OpenGL, but now I've gotten pretty focused on making my sprite-drawing app good and making parts for it that I can reuse on other projects. By the way, where should I look next time I don't know what to program? It's not all that hard to come up with some toy project, but there must be a bunch of open-source stuff in need of improvement out there that I could take a crack at.
Surprisingly, I think `IO` has one of the easiest `MonadFix` instances to understand, but it's based on `unsafeInterleaveIO`, which is the basic primitive for lazy IO. Essentially, `unsafeInterleaveIO someIOAction` will return a new IO action that will not do any IO when run. Instead, it returns a lazy thunk, and when that thunk is evaluated, *then* it will execute `someIOAction` and return that. Now, as far as I know, `unsafeInterleaveIO` is not unsafe in the same way as `unsafePerformIO`, in that I don't believe it violates any of the laws of pure functional programming or any monad laws or whatever else. I'm pretty sure it's only considered "unsafe" because it means IO will run at times that are much harder to predict. It's "safe" because you can pretend that it just schedules `someIOAction` to run at some later time, and then predicts the future about what it will return. There's nothing saying `IO` can't time travel! Anyway, with that groundwork laid, here's the `MonadFix` instance for `IO` instance MonadFix IO where mfix f = do var &lt;- newEmptyMVar -- 1 ans &lt;- unsafeInterleaveIO $ takeMVar var -- 2 result &lt;- f ans -- 3 putMVar var result -- 4 return result -- 5 Let's walk through this line by line in terms of our little time travel analogy: 1. Create an empty mutable variable. 2. Predict the future value that will be contained in that mutable variable. 3. Call the function `f` with the predicted future value. 4. Store the result of `f` in the variable, thus fulfilling the prophecy as required by line 2. 5. Return that result. In short, `f` will be called with the prediction of what `f` will return. If `f` attempts to evaluate the thunk that it was given, then it will create a temporal paradox; i.e. it will forcefully execute `takeMVar var` when `var` has not yet had a result put into it. For the most part, this is usually only useful when you want to work with lazily cyclic structures. Like, when you need to cough up a thunk to an IO action so that it can do some IO and create a data structure with that thunk placed somewhere inside, but you know that it doesn't actually need to force that thunk. [The wiki has some decent examples](https://wiki.haskell.org/MonadFix). import Control.Monad.Fix import Data.IORef data Node = Node Int (IORef Node) mknode = mfix (\p -&gt; do p' &lt;- newIORef (Node 0 p) putStrLn "node created" return p') The most compelling example I've found has been Reflex, the FRP library. The most common reason `MonadFix` is needed is that you want to define to widgets in a certain order, but you need their logic to go in the opposite order. fmap snd $ mfix $ \eventAndText -&gt; do text &lt;- textBoxWithClear (fst eventAndText) e &lt;- button return (e, text) The `button` action returns an `Event` that we want to use to clear the text box, but we want the text box to come before the button in the layout. Now, we could have solved this by making text boxes totally mutable, and allowing the clear event to be changed at any time, but that's not nearly as nice as the pseudo-pure approach we can take by relying on `mfix`, which requires no mutation at all (as far as we can see; remember, there is an `MVar` in the `mfix` instance driving this whole thing). Also note that it's critical that we don't pattern match on `eventAndText`, since that would force the thunk (even if it was only to force the `(,)` structure), which would create the time paradox. So we use `fst` instead. So now let's try defining a `MonadFix` instance ourselves. `Maybe` is an easy one. It will have the signature `mfix :: (a -&gt; Maybe a) -&gt; Maybe a`. The time travel analogy still works well here. We are given a function, and we have to call that function with a prediction of what it would return. The problem is: What if it returns `Nothing`? Well, we're in luck. Since we've already mandated that it's a paradox when the function is strict in its argument, we can just pass in a thunk that would error in the event that `f` returns `Nothing.` Since `Nothing` doesn't contain any values, we can be certain that it can't leak the false prediction. So it's basically safe for us to unilaterally predict that `f` will return `Just`, since it doesn't matter when that prediction is wrong. So at this point, the function begins to sound a little bit like an `a -&gt; a` rather than an `a -&gt; Maybe a`, which should remind you of regular `fix`. instance MonadFix Maybe where mfix f = let a = f (fromJust a) in a -- Or: mfix f = fix (f . fromJust) With this instance, as long as `f` is not strict in its argument (a requirement for fixed points anyway), it can just return `Nothing`, and we don't care, or it can return `Just`, and we'll happily provide it with that result. The instance for lists is similar, but more complicated. mfix f = case fix (f . head) of [] -&gt; [] (x:_) -&gt; x : mfix (tail . f) We're sort of relying on the same idea, but taking it a bit further. We can take for granted that `f` won't strictly use its argument, so we can use `head` to give it whatever it returns, using the same trick with `[]` that we used with `Nothing`. But we can't call `f` with just the head element. Calling it with only one of its elements would be arbitrary and weird (and I bet it would violate the `MonadFix` laws, which I won't get into). So we pattern match on it, and when it has a head element, we use that, but then we call `mfix` recursively with `tail . f`. We can do this because we already know that `f` is going to return a list with a head, so this time we're going to have the `fix (f . head)` return the *next* element in the list. This turns out giving the list monad some weird, but cool behavior. Normally, you would expect each bind to sort of introduce a loop. We iterate over the elements being bound out, and for each of them, we do all the statements that comes after it with that element bound immutably to a variable. But with `mfix`, it's starts to look almost mutable: mfix $ \b' -&gt; do a &lt;- [1, 2, snd b'] b &lt;- [3, 4] return (a, b) &gt; [(1, 3), (1, 4), (2, 3), (2, 4), (3, 3), (4, 4)] We start with `1`, and iterate over `[3, 4]`, giving us the sublist `[(1, 3), (1, 4)]`. Then we do the same thing with `2`. And finally, we try to do the same thing with `snd b'`, except that `b'` depends on the statement that comes next because of `mfix`. Since we were lazy enough, this works out, and we end up with the `a` thunk being the same as the `b` thunk, which means that on each iteration of `[3, 4]`, we're changing what `a` is bound to. That's how those last two elements of the list ended up with changing `fst` values, rather than constant. Yea, `MonadFix` is weird =P
While it's not bad, I don't quite like the monkey see monkey do style.
&gt; Don't let the Hackage gang grind you down ~~This is literally exactly the kind of talk he was just denouncing.~~ They edited this quote out.
I don't have your code, so I can't do empirical measurements, but I suspect TH contributes significantly. Building with `-fno-code` is probably slow for you because the TH phase still has to generate code. If you didn't have the heavy reliance on TH, `-fno-code` should be a much bigger win.
Can you explain what exactly `-fno-code` do? Is there way to identify where is GHC bottleneck on a particular project ?
Do positive and negative position get their names from successively applying de-Morgan's laws to turn a nested implication to something like CNF or DNF? Seems like if you do that, something in positive position will end up with an even number of negations, and something in negative will have an odd number.
for one you can use the same spec to write a server and client. Write a server, get a client almost for free. You can also use the type-level spec to automatically code-generate clients in other languages like javascript or python.
i'm a beginner and i find servant just fine to use.
A basic memo table used by `memocombinators` is `Data.IntTrie` which is essentially a binary tree. At the top of the tree is `0` with its left subtree containing the negative numbers and the right subtree containing the positive numbers. In a subtree at node `k` you branch left for `2k` and right for `2k+1`, so a lookup in this tree is logarithmic. This allows you to memoize any function `f :: Integral a =&gt; a -&gt; r` and this is what `integral` does. Now suppose you want to memoize a function `g :: Char -&gt; r` for some type `r`. `Char` is not `Integral`, but we know that we can map `Char` values to integers and back using `ord` and `chr`. Note that `g . chr` has type `Int -&gt; r` and thus can be memoized using `integral`. And therefore `(integral (g . chr)) . ord` is a memoized version of `g`. This is the idea behind the `wrap` combinator. 
If anyone wants to lend a hand with any of the linked hackage issues -- patches welcome, please. Note that about half of them have to do with how documentation interacts with the candidate feature -- which is something under active development, and fixing the documentation issues is a key part of getting it up to snuff. The only one that drives me a bit nuts is the long filenames one, because it seems to run into some fundamental limitations of the tar format that are rather subtle. There's only about one package in the entire db that trips this though, so just making the docbuilder slightly more resilient with regards to the failure seems good enough?
I have stack project. I want to be able to pass my own user-defined arguments to Setup.hs so that I could customize building process. How can I do that?
At helix, we often write code to be handed over to customers. Given it's widespread acceptance and Microsoft heritage, Typescript is an easier sell. It's a decent language, and a huge improvement over over javascript. I'd like to write a purescript ADL backend sometime, but it's a low priority given I'm not actively working in the language. (Note there's no need for a GHCjs ADL backend, the regular haskell one would work fine.) 
neovim or vim if necessary: https://github.com/TomMD/configuration/blob/master/init.vim
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [TomMD/configuration/.../**init.vim** (master → 6e668f2)](https://github.com/TomMD/configuration/blob/6e668f24d6a1c6f7d90b512d843e3c5499edec8c/init.vim) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dluwqle.)^.
At our shop we have Haskell devs on Windows using Notepad++, and they are happy with it. For comparison, you might also want to look at Atom.
Generally, customizing Setup.hs is not the way people customize their build processes nowadays. What kind of customization do you need?
[Let's build a terminal-based game in Haskell using Brick](https://samtay.github.io/articles/brick.html)
It's unprofessional. The professional response is to create a status page with uptime and graphs. On another note, I'm super happy that stackage provides stable documentation, and I cannot find a single good reason why there *shouldn't* be multiple copies of this documentation. It's like arguing that one git repo is better than another when they both contain exactly the same set of commits.
`recursion-schemes` has a couple of classes, `Recursive` and `Corecursive`, for doing this stuff. `cata` is really pleasant to use for surprisingly many common tasks.
You should publish this as a blog post.
I'm glad to hear that! Some of the type errors I've run against in my use of Servant have been really difficult to understand and debug, and I've been doing this for a few years now :)
Why do parsers in the parsec family commit to a branch after they consume a token, requiring judicious use of `try`? I thought it was for performance, but the post "[“try a &lt;|&gt; b” considered harmful](https://www.reddit.com/r/haskell/comments/25ulsv/ezyang_try_a_b_considered_harmful/)" implies it's in order to improve error messages.
I have post-building hook which builds JS files. I want an ability to disable this hook when I don't need to rebuild js files, because it takes a lot of time and I want to optimize developing process.
That’s an interesting observation. I think it refers to covariance (+) and contravariance (−)—that is, in category theory, if a type parameter occurs only in positive position then you have a covariant functor; negative, contravariant. But those notions probably come from logic in the first place.
No, not yet: https://github.com/haskell/hackage-server/commit/770e8f913a450fc2c7f663ee67880b6d0ff76986 But the support is improving and there are some basic signatures up right now on http://next.hackage.haskell.org:8080/ -- which you can go ahead and start using yourself, by adding that repository to your configuration file (check out the home page) and depending on them.[0] You can probably get an account too, in order to upload your own, if you ask on `#hackage` or ask /u/hvr_ ? Also, `cabal new-build` is mandatory for usage of Backpack, as well. So you probably want to actually get the most recent version, or build from git directly. (This is pretty easy if you have an existing version of cabal 1.24+ or above) --- [0] Related, it would be nice if cabal had a feature like an enhanced `apt-key`, that could quickly import repository info, with an optional key set, from a file. Probably wouldn't be used often, but useful nonetheless...
VS Code has a good cross-platform IDE-type experience for Haskell that’s easy to set up. (I never could get Atom, Leksah, &amp;c. to work.) Most of the time I just use terminal Emacs and Stack for builds.
Just as an example of where I’ve found `MonadFix` useful: it lets you rewrite a tree, in one pass, with monadic effects, where the result depends on effects that happen later; I use this in a typechecker, annotating the AST with `substitute inferredType finalTypeEnv` where `finalTypeEnv` is the type environment *after* running inference, with all the constraints available for replacing variables in the `inferredType` with concrete types.
Weird how your account is a brand new one, reminiscent of the old [haskdev user who does nothing but post things about Stack](https://www.reddit.com/user/haskdev) all the time (always *strangely* asking about why anyone would use anything else) -- but with a very similar 3l33t username, and how literally all of your content up to now is about Snoyberg and Stack (and curiously asking and sniping at anything else!) Weird how it's clear that you're nothing but a cowardly, grifting shitposter and everyone can see it. I'm not sure if you're merely stupid (and think it's non obvious) or just a troll (fur lulz lol). There's no difference. Here's my suggestion for what the Haskell community needs: delete your account.
Intellij-Haskell enables me to put the cursor anywhere, including non project sources (eg on a Maybe), and press ctrl+b to go to definition inside the IDE. Spacemacs seems to only have hoogle lookup that opens a browser. Is there no similar feature to go to definition inside spacemacs?
&gt; My laptob is already a monster and close to the fastest laptop you can get, so upgrading is not on option. If you're in a team, an option not mentioned is to actually upgrade your compilation machine. Rent/buy *one* giant machine for your team, and use docker on it. If properly setup, all team members will get their personal "VMs" (docker), and you will get a much faster machine than your laptop. As compilation is bursty you will all get great compilation performance. 
Just to confirm, you have two professional developers using Haskell as their primary language and Notepad++ and the command terminal are good enough? They don't miss a massive IDE like VS that creates squiggles when they mess up, as the compiler errors an and language design are good enough?
GHC has traditionally shied away, as far as I know, from keeping around "source code" for the input code inside things like object/interface (completely unoptimized, relatively close to source input). It's generally always been designed around the assumptions of separate compilation... I've never really known *why* this is the case but last I remember, /u/simonmar, at least, isn't a particular fan of this (there were patches at one point for example for "fat interface files" that had enough information to recreate an object file from them, but these were rejected). Though there is a big spectrum between "keep source code around" and "re-optimize at link time", to be fair...
I made this screencast 2 years ago on making a blog app using Yesod. I still get good feedback on it, though since making it there have been scaffolding changes that will make it not 100% perfect. https://youtu.be/SadfV-qbVg8
I hadn't considered live coding. Good suggestion.
Thanks, adding it to the list
Added it to the list, Thanks,
say I have some monad transformer newtype OpT w o r m a = OpT { runOpT :: w o -&gt; m (a, w r) } Where w is some alternative (typically Maybe). Typically I'd combine this with a state monad to build up independent stateful computations. I'd like to be able to kliesli compose functions of the form (a -&gt; OpT _ _ _ StateT ...) Without exposing the entire state to each component. I know I can use Zoom from Lens to do this, I was wondering how I could use Magnify to limit the scope of the o parameter, so that each block only handles a certain set of operations, and the global operations is the sum of all the composed components operations. 
I personally use the docs on Hackage. Probably only because I am used to the styling, I dunno. Generally it just works. Occasionally docs for some particular version are missing, and I go back a version or two; no biggie for me. But anyway, many thanks to the Stackage team for providing an alternative. That's always good. Some people like it better - probably because unlike me they actually compared the two - so go for it.
Yeah can we please fix the tar library already? I have discussed this with Duncan in the past. The library hasn't been touched for years. Back then there was a huge mess of different partially-incompatible formats. Ian chose to support some combinations that made sense at the time. But nowadays the world has finally standardized, and unfortunately our old tar library doesn't support that.
Is HList a recommended library, or has it been subsumed by newer libraries or language features? The [documentation](https://hackage.haskell.org/package/HList-0.4.2.0) for the latest version seems to have disappeared. It seems to be one of the go-to examples of type-level programming, but I don't see too many things depending on it. What are people using for tuple-like things where you know some constraint applies to each of the fields?
According to the [original author](https://web.archive.org/web/20140529211116/http://legacy.cs.uu.nl/daan/download/parsec/parsec.html) this was done for speed. The blog post seems to be more about how to get/keep good errors using try given parsecs behaviour. Not about why the behaviour is that way imo.
Can't confirm any numbers - I'm not sure exactly who is using Notepad++ right now, but I know some are. Other than that, yeah. People just want to get work done. Each person uses whatever tools are simplest and most comfortable for them.
Tools like stack and cabal are meant to be used just for the GHC compilation part. If you have other build process needs, use a more general build tool on top of that. I recommend looking at [shake](http://shakebuild.com/). Or if you already know make or some other tool well, use that.
Thanks for the pointer. I found the issue if anyone wants to jump in :-) https://github.com/haskell/tar/issues/1
That's right, GHC does not automatically generalize the types of let bindings, although it does so for top-level function definitions. Did you define `newProductPrice` at the top level? You can generalize the type manually by providing a type signature for `go`. It's hard to help you with how to do that, because you haven't provided any type signatures at all.
How about this solution: have the post-build hook check an environment variable that indicates it should be disabled, and set this environment variable when running stack.
Thanks for finding this post! It does [say](https://web.archive.org/web/20140529211116/http://legacy.cs.uu.nl/daan/download/parsec/parsec.html#Predictive%20parsers) that speed is the reason: &gt; For reasons of speed, the (&lt;|&gt;) combinator is predictive; it will only try its second alternative if the first parser hasn't consumed any input. It also gives a third reason which is neither performance nor error messages: &gt; Simplicity. Parsec is designed to be as simple as possible, making it accessable to both novice and expert users. Parsec is predictive by default, only backtracking where an explicit `try` is inserted. This makes the operational behaviour easier to understand than the `normal' approach where backtracking is the default behaviour. That's the first time I hear this one! In my opinion it's the opposite, I find backtracking parsers simpler than non-backtracking (apparently that's called "predictive"?) parsers, because it allows me to think in terms of taking the union of the languages accepted by the parsers instead of thinking operationally in terms of parsing tokens and backtracking. So, follow up question: are we talking about a little bit of extra speed because the parser doesn't have to backtrack and waste a little bit of time trying an unfruitful alternative, or are we talking exponential blowup as the parser tries all the possibilities? I mean, it's certainly possible to construct a degenerate parser such as `aaab = try (char 'a' &gt;&gt; aaab) &lt;|&gt; try (char 'a' &gt;&gt; aaab) &lt;|&gt; char 'b'` which will blow up in that way. By for most languages, I expect that alternatives will be sufficiently different from each other that if a lot of work is spent on one, the others are almost guaranteed to fail very quickly, thereby making the predictive approach justified but also not that much of a performance boost. No?
Thanks! I just wanted to confirm your devs could use Haskell effectively without an IDE. Verbose languages like Java need an IDE because there is such a large amount of code. To contrast, a single line of APL goes a very long way.
Hey! I actually recognize your username and hope Kitten is coming along well! Is it easy to use Haskell in emacs? 
It’s worth noting that this is not the case in Haskell 98 or Haskell 2010. Generalization of local bindings is disabled with [the `MonoLocalBinds` language extension](https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/glasgow_exts.html#let-generalisation), which is implied by `TypeFamilies` and `GADTs`. The reason for this is that let-generalization becomes very tricky in the presence of type families or GADTs, so GHC disables it by default when those extensions are enabled. The paper [Let Should not be Generalized](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tldi10-vytiniotis.pdf) goes into this in more detail. As a final point, the aforementioned GHC docs include this note: &gt; You can switch [`-XMonoLocalBinds`] off again with `-XNoMonoLocalBinds` but type inference becomes less predicatable if you do so. (Read the papers!) I’ve never tried this, though, so I have no idea what it would do.
I'm on linux, I use vim with [haskell-vim-now](https://github.com/begriffs/haskell-vim-now) minus a couple of the plugins and some custom vimrc edits. I have another window with ghci open, and then a third for compiling or whatever else i might need. 
Sorry, a question: I'm a newbie and I'm using emacs with haskell-mode and company-ghci. Why does nobody mentions emacs? Is this a sub optimal setup?
Thanks for posting. This is the first time I've seen the Compose talk. If you'll be at ICFP, Joachim will be giving a shorter talk on the paper there. I was thinking again about the question about hidden information. (Joachim mentioned the example of a game where players race to click a card and look at it, which we've discussed a lot.) I'm still not fully convinced we made the right choice here. As Joachim explained in the talk, the current choice was made because we weighed the additional complexity as more important than the seriousness of the problem, when the domain is restricted to children's games. But there is a nice solution, which only adds minimal complexity. The solution is simply to add a parameter to the time-step and event handling functions, informing them whether the change is speculative or not. At one extreme, a game could simply ignore this one extra parameter, and get precisely the behavior described here. At the other extreme, another game could always return the state unchanged when this parameter is true, and effectively disable client prediction entirely. But the more interesting trade-offs are in the middle. There are certain effects that are reasonable to apply speculatively, and others that are not. So a game could reasonably keep most effects as-is, but just ignore the mouse clicks to draw that card unless the speculative flag is false. Another case where this is problematic is when a player "wins" and all the state resets for the next round or level; it can be quite distracting to have the whole screen change to show a new level, and then a half second later, revert to where it was. It's easy enough to leave all the logic within a level as-is, but just add `&amp;&amp; not speculative` to the condition to start a new level. The same might apply to updating the score, or declaring a winner. It seems somewhat perverse to award a player a win, only to back up and take it away again! And remember that speculative branches are always abandoned, so you need not worry about how to ensure that the point is awarded eventually; it will be in a different timeline. The more I think about this, the more I like it... but, alas, there is the cost of that one extra parameter.
Each Haskell source file contains precisely one module. I'm afraid I don't follow this, &gt; I've tried (probably not enough) splitting modules ( I guess you mean file). but I end up at the end which one file including 100 modules).
Generating opaleye model definitions and lenses automatically after looking at the DB schema. Every lens-class, eg `HasId, HasName` goes into a separate module. Every new type, eg `UserId, UserStatus` etc goes into a separate module. Every model, eg `User, Post, Comment` etc goes into a separate module and imports (and re-export) all the individual lens-classes and newtypes it needs. Any endpoint that needs to use a model, imports specific modules individually, not one top-level `Models` module (which is the first thing that introduces a compile bottleneck) 
Do you know **why** TH is slow? 
Just to be a pedantic git, that scheme tutorial is not a compiler, it's an interpreter, which is fairly simple (scheme/lisp is designed to be simple to parse), it even claims to be good for new programmers.
[Bittorrent Client in Haskell](https://blog.chaps.io/2015/10/05/torrent-client-in-haskell-1.html)
[`SPC m g `](https://github.com/syl20bnr/spacemacs/tree/develop/layers/+lang/haskell#key-bindings), or `gd` with the vim bindings. It doesn't work very well though; sometimes it works, it finds its own TAGS file and goes to the file in your project which contains the definition, sometimes it gets confused by its own duplicate TAGS files and asks you if you want to "keep both", sometimes it gives you a redundant choice between two definitions, which in fact are just the type signature and the implementation which are next to each other (and the window to choose which definition to go to annoyingly stays open until you close it, unlike most other temporary spacemacs pickers), and sometimes it just gives up and simply goes to the first occurrence of the word in the current file, like vim does. *edit*: That was for definitions within your project though, not for external definitions like Maybe. For those you can use [`SPC m h i`](https://github.com/syl20bnr/spacemacs/tree/develop/layers/+lang/haskell#documentation) to run `:info` on the identifier.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [syl20bnr/spacemacs/.../**haskell#key-bindings** (develop → 3e98896)](https://github.com/syl20bnr/spacemacs/tree/3e98896816644cf8c09201b90d992f09fbe58d1e/layers/+lang/haskell#key-bindings) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dlva8gq.)^.
Would my [Git in Haskell](http://vaibhavsagar.com/blog/2017/08/13/i-haskell-a-git/) count?
/u/parsonsmatt and I did a little workshop on building a very simple web client for reddit here: [Build a Thing With Haskell](https://youtu.be/ZkqtLACkg9g)
UndecidableInstances is the one which can lead to non-termination. The worst thing enabling AllowAmbiguousTypes can do is reduce the quality of the error messages. The situation in which this occurs is if you write a function `foo` with a type like `Show a =&gt; String`. When you call `foo`, GHC will complain that type `a` is ambiguous at that call site, so you might try to add a type signature, but frustratingly that won't be enough to disambiguate the type. So normally, GHC would perform an "ambiguity check" on `foo`'s type and give you an ambiguity error at the definition site, which is much better because that's where you can fix the problem. By using `Show a =&gt; proxy a -&gt; String` instead, the ambiguity check passes as there is now a way for the call sites to specify which `a` they want. With AllowAmbiguousTypes, the ambiguity check at the definition site is disabled, but GHC will continue to complain about the ambiguous `a` at the call site. With TypeApplications, AllowAmbiguousTypes makes sense because there is now a way to fix those individual call sites: a type signature still won't help, but a type application might.
Possible typo: where you say &gt; So we use `fst` instead. should it be &gt; So we use `snd` instead. ? Edit: oops, never mind.
I wholeheartedly agree with the spirit of the post but... &gt; It’s perfectly possible to be productive in Haskell without understanding anything about monads! I'm not sure I agree with this. I wager that the bar is right ~~below~~ above monads. Without monads, you don't really understand what is going on with `IO` (which you'll need for `main`). At least that was the point when _I_ started being able to actually do anything useful. 
Oh man, these sound like fun weekend coding adventures :)
To me(and apparently a lot of people) it's more that there isn't any way for people who can't do much complex math to learn HS other than what seems to be endless trying and all and every single book assume that : * **A** : You're very experienced in C or C++(maybe sometimes Java) * **B** : You're a math *wizard* And most people isn't either. There is a huge problem with IDEs too, in fact there isn't any way to create anything comfortably without editor plugins(not necessarily a bad thing but read further) whose feature set and ease of install vary a lot(and is not complete) according to the wiki and people on this subreddit. The error messages don't make any sense to the total begineer **but** the program does make sense to him/her so this only leads to frustration. Your analogy is good but instead of slowly crawling i'd say it's even worse than that because others can have access to "helicopters" but you're not even guaranteed to have your two full "arms" and "legs" to climb with. By the way this is not a problem exclusive to HS.
Wouldn't your description mean that the bar is right *above* monads?
I found this: https://two-wrongs.com/the-what-are-monads-fallacy to be quite enlightening.
The intro to [reflex-frp](https://github.com/reflex-frp/reflex-platform#tutorial) is a tutorial on creating a simple calculator. There is also some documentation on creating a todo list, but I can't find it right now.
I think it’s also worth pointing out that making a reliable Hackage docs builder is significantly harder than making a docs builder for Stackage: Hackage does not have a sufficient amount of information to build packages and thereby also not for generating haddocks. The missing information includes things like foreign libraries, insufficient dependency bounds, …. Stackage is basically a giant CI server and it does have all the information (e.g. when you add a package to Stackage you declare how foreign libs are installed) so a haddock builder is somewhat reasonable. Still I think documentation builders are a silly idea. If `stack` and `cabal` would just upload documentation by default this problem would not exist and the resources could be used for other things.
Your work on this is greatly appreciated!
Many thanks for the tutorial, it really did helped me a lot.
Lately the Rust folks have been trying to bring their `rand` crate up to a "1.0" quality level, and I've been in that discussion a bit. It's kept Rust on my brain a lot lately, but I will return to Haskell eventually.
Embarassing chicken and egg problem here...
I guess, if you think parsec as not 'combining grammars' but a framework for nice recursive descent then predictive *is* easier to understand...
&gt; ...the opposite commentary is going on: Stackage is bad ... Hackage is much better Maybe I didn't dig deep enough or it was deleted already? But it seems odd that I couldn't find that other thread with the "opposite commentary" you conveniently don't want to link to which is used as an excuse to absolve the unprofessional tweet storm. 
&gt; That's right, GHC does not automatically generalize the types of let bindings, I wasn't aware of that there were a difference for too level binding and local ones. That makes perfect sense now .Thanks
Thanks for the clarification.
That's a Google idea indeed. I might be able to do something similar with my own generator.
At the very top of the pyramid is the knowledge that most of the stuff that people discuss on social media actually gets in the way of being productive in Haskell and that simply functional programming gets stuff done. [Evolution of a Haskell programmer](https://www.willamette.edu/~fruehr/haskell/evolution.html) is still relevant in the age of DataKinds.
To answer part of my own question, apparently there have been some issues with Hackage building documentation, guessing that's why it's missing here. It looks like some of the records libraries, like vinyl, may support, e.g., mapping across fields. I'm still curious what people are using in practice, though.
Note: I'm the Pyramid author, not the web-page post author -- I'm totally fine and happy that someone transformed my Google slides into an HTML page :-). I beg to differ about IDEs. I'd say you're seeing another Pyramid effect with people showing-off how nice the latest Emacs plugin is. It's a good thing that these exist but IDEs plugins are a distraction to get the next important stuff done: actually learn stuff (yay, type-holes completions, sum-types expansions, but WTH is all of this if I'm new to Haskell?) What matters to newcomers is the experience to the "first commit". I wish we could have a community-wide service-level-agreement (SLA) to onboard newcomers. If I were to sketch an SLA for Haskell newcomers experience, it would likely be: - a newcomer should be able to overview how a package is structured, what compilation means in half a day - have setup your dev environment and run an example on the first day - a simple commit on the next day. Such an SLA doesn't prevent setting the expectations right: mastering Haskell is a long journey, there will be the "monad chat" at some point, but there's a reason why people (and big names) use or want to use Haskell: it delivers. At work we ramped up an Haskell newcomer (who uses no IDE) and we met the above onboarding SLA. Even though we're among "experts", we make sure there's everything a newcomer needs on their first week: READMEs on how to setup the bare-minimum for our projects, an example demonstrating features of our curated set of packages (including minimal tests) and a stack template (thanks's so much Stack team for the templates) setting up the basics. Then, I learnt a lot mentoring this new hire... but that will be another talk :-).
Which extensions are you using in VS Code? In Atom there are so many... I installed Haskelly and stack run doesn't work and some of the other stuff doesn't work either... 
I wrote the initial Pyramid then Patrick ended with this last sentence. I somehow agree with him and hence I fee like I can answer this one: I'd say for a naive program you'll need to understand the do-notation and the difference between **let** and **&lt;-**. You don't need to reach monad enlightment to get stuff done. I surely hadn't got around _understanding_ monads when I wrote zmcat ; everything's in IO and there's not a single **let** (1st commit: https://github.com/lucasdicioccio/zmcat/commit/e9ba61fc5b634c45b50c36fb07bcabdc5b6625bb).
My first serious Haskell project was a raytracer. I had no understanding at all of monad, io, anything. It was full of cascade of case to unpack / depack `Maybe`. I used some `return` inside IO instead of let because I understood IO as "in IO, there is side effect, so affectation are done with `&lt;-`, special pure line are special cased with `return`, that's stupid, but well, why not". I'm sure that you don't need to understand `Monad` to do some stuffs. Actually, does any beginner C++ developer understand sequence point when they write code with side effect (i.e: roughly any line of a C++ code)?. Does a python programmer really understand the reference counting GC (and its issues) and the global lock when he write his first python code (or ten years later ?).
There is a VS Code extension that gives you jump to code for ghcid that I use. That extension took a few hours, so I'm sure you could do something similar for your editor. 
Not sure about other people but I was writing chat servers and clients and parsers without knowing much about monads. All I knew was the `do` syntax was being desugared to code that uses `Monad` methods. Surely it wasn't the best Haskell code, but it worked fine for my purposes.
If you raise the details on the ghcid bug tracker I can take a proper look. 
I agree. When I chose to learn haskell I picked a small project (implementing some numeric algorithms) that required nothing in the way of IO, and which was modest enough I could write with a plain text editor lacking even indentation support with no trouble. As I started to grow, I still avoided IO and monads as long as possible, and just focused on getting better at recognizing folds and occasions for recursion patterns, etc. I think one problem is a lot of what people think of as "beginner projects" in languages with different ecosystems are really projects that are built on top of large libraries with simple APIs -- graphics things, games, etc. And even in the haskell webapp world, where we do have such libraries, beginners are often steered to use the more full-featured, complex ones first, instead of the most basic ones.
Tl;Dr. However what you mentioned seems to be what is call [bracketing](https://wiki.haskell.org/Bracket_pattern) in the Haskell community. And yes, it can be really useful.
I think it 's bugh `ghci` itself, so I'm not sure it really make sense to add it to `ghcid` tracker. But I can if you think you can do something about it.
As I said, I add the jump problem when I was using tmux+vim. Now, I'm using spacemacs it's partially sorted. However, being able to launch `ghicd` in a terminal an getting it to jump in the current emac would be awesome.
What do you use for job queuing and scheduling? I deliberately mentioned both, because: a) I'd like to queue a job for things like sending mail notifications, so that they remain async b) I'd like to have them persisted, so that if application crash before handling such queued job, I can resume c) I'd like to have possibility of scheduling job at some specific point in time. I have a PostgreSQL DB in place, so it should prefereably use this. So far, the https://github.com/noteed/humming looked like best fit for me, but it has some dependency issues (minicron? I couldn't even google up that package, just had to manually find it on humming author's github), and I haven't yet been able to try it out Other that I've looked at: * https://hackage.haskell.org/package/postgresql-simple-queue - this is just a queue, meaning I'd have to use something else for scheduling
[from the user-guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/phases.html#ghc-flag--fno-code) &gt;-fno-code &gt;Omit code generation (and all later phases) altogether. This is useful if you’re only interested in type checking code. The problem is TH still requires code generation to typecheck IIRC.
I believe the [managed](http://hackage.haskell.org/package/managed-1.0.5/docs/Control-Monad-Managed.html) library is related to this. It helps to avoid the "indentation staircase of doom" when combining multiple managers. There's also [Data.Adquire](http://hackage.haskell.org/package/resourcet-1.1.9/docs/Data-Acquire.html).
I believe the current state is that no general-purpose Haskell blockchain libraries exist, but we have `haskoin`, which is a Haskell implementation of the Bitcoin protocol. To be perfectly honest, I’m not even sure what a general-purpose blockchain library would contain. What functionality are you looking for, exactly? Also, for Bitcoin there’s the `secp256k1` library, which is a wrapper around a super-fast implementation of the ECDSA curve used by Bitcoin. http://faucet.xeno-genesis.com/ is a Bitcoin testnet faucet powered by Haskoin. It uses all three Haskoin libraries (`haskoin-{core,node,wallet}`) for parsing blocks and transactions, interfacing with the Bitcoin P2P network, and keeping track of balances.
&gt; In your example I don't think the type synonym is a problem. I guess you meant this type: Yeah, I made a bit of a mess of that. Your example is better. &gt; Which according to the examples is in a left-to-right order. Yes, I understand that but my example was bad. What I mean is, is it left-to-right order before or after type synonyms are substituted? &gt; would make this stop compiling: If so, that's *really* bad.
We have replicated delayed-job in Haskell with a little bit of type safety on top. Is that what you're looking for? We plan to open source it once it stabilises internally. We had the same problem, hut couldn't find a solution. 
I don't know man... DataKinds are pretty damn cool.
Computer science being a branch of mathematics, I don't see a problem with assuming mathematical maturity from someone by the time they are learning individual programming languages (which should be later than learning the language-agnostic theoretical foundations).
I have a stack question (and cabal one). When you pass option to `stack build` which have an impact and how the code is compiled, for example `stack build --executable profiling` or `stack build --fast`. Is it persistent ? I mean does it erase the current build or does it store it somewhere else. What happend if you just do `stack build`, is everything cleared, or does it remember that is in profiling mode, etc .. ?
I ended up using this solution, but CLI arguments is more preferable choice for me.
Thanks, I will take a look at this one.
I've wanted a flag like this as well, but my idea was ignore shadowing of imported names regardless of types. 
In a similar vein, local compiler flags. Disabling warnings on the whole module is not precise enough, and with things like undecidable instances it's even worse.
See https://ghc.haskell.org/trac/ghc/ticket/602 for some discussion on this.
+1
I suspect much of your annoyance with `-Wall` would disappear with the following simple rule: locally bound names (function arguments, let-bindings etc) should be allowed to shadow any global name (i.e. top-level definition) anywhere without warning. In fact that's more or less what GCC does: https://gcc.gnu.org/onlinedocs/gcc-7.2.0/gcc/Warning-Options.html#Warning-Options
Possibly, but that doesn't stop you to redefine a function which exist at the top level by mistake.
The ticket is 12 year old, so I don't think there is any chance it becomes a priority :-(
[bracket](http://hackage.haskell.org/package/base-4.10.0.0/docs/Control-Exception.html#v:bracket) and friends exist as prior art, and they do handle exceptions, but as you point out, impose IO. A property of context managers worth being aware of if you're coming to Python with Haskell or C++ experience is that `with` is a binding form---it doesn't introduce a nested scope. I'm assured that this is a [feature](https://stackoverflow.com/questions/6432355/variable-defined-with-with-statement-available-outside-of-with-block). It should be noted here that I am not Dutch.
Doesn't normal vim tag command like `Ctr ]` work? You can also use `helm-search-tag` (from the top of my head). You can use `codex` to tags everything including your dependencies. I personally use dash (from spacemacs `SPC d d` it will open a dash window corresponding to the word at point) . Its super fast and its basically having the hackage doc online).You configure it to use your actual dependencies. Needs a bit of set up though.
After having young lots with the idea of extensible and anymous records, in practice it's seams much easier to just use plain record and write everything manually (or using TH). HList is great to deal with list of phantom types though (to model SQL query for example).
In practice you're probably right. One of the guests on the Haskell cast (I think it was Neil Mitchell) said he programmed productively for years without understanding them, but that was a while ago and the ecosystem has changed some since then. That said, you really shouldn't have to learn monads to work with `IO`. If you learn concrete versions of the monad functions (`(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b`, etc.) that gives you everything you need until later. Monads are required for generalization, not `IO`.
&gt; Data.Adquire (Data.Aquire) 
If you want it to be a priority, get involved! GHC devs are generally quite willing to help newcomers. And you don’t even need to write code to help: Moving the discussions on the design forward by summarizing previous comments and proposing solutions to open questions is already quite helpful and makes it easier for other people to work on the implementation.
&gt; every single book assume that I'm pretty sure this is not the case with Haskell From First Principles.
My native tonge [trips me up again](http://www.wordreference.com/es/en/translation.asp?spen=adquirir).
I have not encountered the acronym "SLA" before; what does it stand for? It doesn't seem to be "service level agreement."
Haskell has features and syntax that are based on mathematical objects/patterns. You may not need to know that in order to use the language, but eventually that is the direction you would go in if you continue using Haskell. Eventually you do have to understand what a Monad and Monoid are in order to produce good Haskell. This is actually good because it means you learn mathematical patterns that describe computation, which once learned translates really well to other languages that use those same patterns. In contrast, learning how Java 8 does streams does not help in learning Python lambda syntax. The patterns other "easier" languages have, are not strictly based on math, and therefore you have to keep learning their arbitrary style every time, even though it might be the same computational pattern (so if you do learn the computational pattern, you can even translate that to the designed languages)
There is also http://hackage.haskell.org/package/hvect
I probably should have gone for `caffeine f`, since I clearly wasn't awake enough.
There is a difference between being able to produce some Haskell code and being productive at it. About Monad, I still remember the first day I got stuck with a list of IO and being stuck with it. After a few years doing Haskell, I still bang by head every week about sequencing and traversing stuff because I'm using async which use IO but I'm in MonadIO reader etc ... At some point you need to understand what you are doing otherwise you'll get stuck. Anyway, what is point if programming in Haskell if you are not interested in the underlying theory ?
For me SLA means Service Level Agreement. I've done two years of "reliability engineering" and now like the idea of having an SLA to quantify expectations about systems, but also about $stuff. I'll edit my above message to make things clearer.
I think that he does intend to mean "service level agreement", in the following sense: Our internal infrastructure and support for new hires is such that we "guarantee" that they will be able to do these things on the first day, these other things in the first week, etc.
Thanks - modelling a set of columns with phantom types is actually exactly the kind of use case I have in mind.
It's useful. just not on a hundreds of lines project. A couple thousand lines and you already gain by generalizing your basic functions. At ~10k lines most of those things will make your life easier, not harder.
What is the go-to solution for making HTTPS requests? Is there a close equivalent to the Python requests library?
I used HList for that a few years ago and it work really well (merging Columns and so on). However type inference was a bit tricky. For example if you have `merge :: table as -&gt; table bs - &gt; table (as ++ bs)` you want to be able to deduce `as` from `as++bs`. This was before injective type family, it might be possible now.
Attoparsec works on bytestring/text and keeps track of its position via offsets which makes backtracking basically free. Parsec can work on Strings which are linked lists. Whenever you may backtrack it keeps a reference to the list. But now the gc can't collect it which can lead to serious memory leaks.
I'll get involved if there is a chance that this can go somewhere. Considering the popularity of this post, I'm not anybody is really interested.
Elm is also a great jumping off point. Haskell is large and it is easy to get lost down an unproductive path. Playing with a small ML derivative can get you past some of the initial shock.
Why do you want to abstract over bracket style functions? That'd only be really useful if you wanted one function to be abstract over the resource but I can't come up with a situation where that'd be useful. Do you have a use case where you couldn't use a concrete function? Anyway, you can't be exception safe outside of an IO context. Of course you probably aren't doing anything that has to be exception safe since you can't have side effects either, though. If you want an exception safe function in IO you probably should partially apply to the bracket function since the finer detail can be hard to get right. Like, what if another thread kills our thread with an aync exception while we are done with the callback function but haven't started with the cleanup yet?
Well, presumably at least one person is interested and that's all it takes
Isn't Managed `Codensity IO`? Guess the name is nicer, though.
Try as hard as you can to attend courses on a campus. If money is a constraint, go to the cheapest place you can find (state college possibly). If you are working as a programmer already, most companies will pay for up to $6K in courses. Having in person time, and professor spending time giving you deadlines and grading your work really speeds up your rate of learning. Also, for those people who you have met who have interest in Javascript, pair up with them and learn Elm for a little while.
I understand that a backtracking parser isn't a constant space stream processor, it must keep the entire input stream in memory so it can backtrack. I also understand that a bytestring is packed more densely than a String and so takes less memory to store the same number of characters. But I don't understand why you are worried about memory leaks. The "reference to the list" which is kept whenever we may backtrack isn't a reference to a brand new string, it's a reference to some node in the linked list which holds the entire document. In the worst case, a reference to the front of the list will mean that Parsec will have to hold the entire document in memory until parsing is complete. How is that any worse than Attoparsec? Attoparsec also has to keep the entire bytestring in memory, otherwise it won't be able to dereference those offsets.
`-fno-code` only does type checking and skips code generation, optimization, linking, etc. This speeds up the compile-fix-errors loop substantially. But if you have TH, GHC has to fully compile your TH stuff AND do code generation on it just so it can run the TH to generate the Haskell code that will then be type checked by `-fno-code`. It's an inherently more complex set of things to do.
Seeing above, I note `getLabelBc: Ran out of labels` - that's entirely `ghci`'s fault. However, if it was just the absence of `main` that might be on that was `ghcid`, but not with an internal panic first.
&gt; most of the stuff that people discuss on social media actually gets in the way of being productive in Haskell and that simply functional programming gets stuff done There is truth in this, but there are (many) exceptions. If that was true, a simpler language would be vastly superior. Yet, after having worked a bit in Elm, I can see how missing all the stuff above "simple functional programming" is crippling. Now, `lens` gave me a huge productivity boost, and is not "simple".
If you aren't already, you should pass `-O0` to the compiler to disable optimizations. It disables almost all inlinings. 
I usually use [http-conduit](https://haskell-lang.org/library/http-client). But I think both wreq and req packages offers a much better API for consumption.
I've recently started http://rust-class.org/ for Operating Systems, and am planning to work through https://www.cs.princeton.edu/~appel/modern/ml/ using Haskell. Depending on your timelines, I'd be interested. 
Two things: 1. You didn't dig deep enough. https://github.com/haskell/process/pull/102#issuecomment-322794371 2. Why does it matter if my personal Twitter account is professional or not? I am not a Stackage or Hackage maintainer. 
The browser engine posts aren't really very advanced, and they don't go far enough to get into the really awful stuff. That said, the code in them almost certainly doesn't compile with ghc 8, I've been meaning to update them. Side note: reading the HTML RFCs is a great way to spend an evening if you want to get _really really_ angry at people.
This. Ideally it should be possible to install a single package and start working without wasting more time. Most used features (coloring, autocompletion, show info on hover, etc) should just be there, without manual config.
The order of the arguments of `foldr` is actually a bit peculiar. Following more modern conventions for eliminators, we'd have foldr :: b -&gt; (a -&gt; b -&gt; b) -&gt; [a] -&gt; b But history is history; we're stuck with tradition. Thinking about the implementation of `foldl` in terms of `foldr`, it would be more natural (but less convenient, and stranger for non-list foldable types) to have foldl :: (b -&gt; a -&gt; b) -&gt; [a] -&gt; b -&gt; b
Yes, nice. That actually adresses the problem I have with Atom and Haskell! &lt;3 Edit: I'm getting some ghc-mod related Errors. I'll do a clean install and then Report back.
Regarding the safety of interleaved I/O, one should surely read /u/doliorules (Dan Doel)'s blog post on the subject: http://comonad.com/reader/2015/on-the-unsafety-of-interleaved-io/
Hi guys! I was watching a video that introduced this method of generating a list of Fibonacci numbers: fib = 1 : 1 : [a + b | (a, b) &lt;- zip fib (tail fib)] How on earth does it work?? Why is there only one call to zip? and how does zip get the second to last item in the list? Thanks!
99 problems is a terrible way to start. The problems are awful Haskell translations of (not too bad) Lisp translations of Prolog problems. They are often phrased in terms that only directly make sense in untyped/unityped/dynamically typed languages with functions that can take variable numbers of arguments. None of those match Haskell, so people solving the problems in Haskell must first translate the ideas behind them (as best they can) into ones that make sense in Haskell; that's not the right way to start learning the language. Aside from those serious problems, the exercises also call for functions that are just not idiomatic Haskell; for example, they tend to take their arguments in the "wrong" order. Again, not a good place for a beginner to start.
The `turtle` tutorial teaches people how to use `IO` without ever teaching anything about `Monad`s: https://hackage.haskell.org/package/turtle/docs/Turtle-Tutorial.html
Haskell book actually teaches you lambda calculus in the first chapter and it's a very good thing!
I agree with the general gist: simple abstractions go a long way. Just remember that what's in "normal" Haskell and what people consider "simple functional programming" is fundamentally arbitrary: new features and abstractions can be simple and classic ones can have poor power-to-weight ratios. It's worth critically considering what is and isn't worth using, but neither the Haskell standard nor the content of your favorite intro Haskell book are good guides for this.
I don't think `HList` ever was really a recommended library. It feels much more like a mostly-abandoned playground for exploring the limits of several related ideas. I believe a lot more packages have stolen fragments of it or taken ideas from it than chose to actually depend on it.
Indeed! I am currently looking into if it's possible to configure other plugins from a different plugin, to help setup some sane defaults and such. Ideally you just install this one thing and you're all set. This is only a first version (and first time doing an Atom plugin), so I'm more than open to suggestions on anything that might need including :) (also, if anyone is working on an atom-intero plugin, I really need that!! Or if HIE adds integration via [atom-languageclient](https://github.com/atom/atom-languageclient))
Dope!
I'm using Haskero with stack. Note that you must `stack install intero`, otherwise you won't have any kind of auto-complete.
Indeed. It's just improving the tools for low level array manipulation. I hope I've been helpful on the code review so far ;)
&gt; I also want to become better at Functional Programming (Lisp) and maybe even try out Haskell. I guess, it partly depends on a lisp dialect, but if I were you I'd prioritize Haskell in case of FP. I quite extensively played with (and learned) lisp (well mostly Common Lisp which is Lisp-2 so it's not so nice for FP) and I say if you are going to learn lisp then don't focus on FP, lisp's (Common Lisp's) strength is in metaprogramming.
The use of list comprehensions in this case is not really necessary. Let's look at another version: fibs :: [Integer] fibs = 0 : 1 : zipWith (+) fibs (tail fibs) &gt; Why is there only one call to zip? Why should there be multiple? A term I often heard in my education in functional programming is *wishful thinking*. Assume that `fibs` is the fibonacci sequence. Shift the sequence by one (`tail fibs`) and add it together with the sequence itself pair-wise (`zipWith (+) :: [Integer] -&gt; [Integer] -&gt; [Integer]`, `(+) :: Integer -&gt; Integer -&gt; Integer` in this context): fibs = 0 : 1 : zipWith (+) fibs (tail fibs) = 0 : 1 : zipWith (+) [0, 1, 1, 2, 3, 5, 8, 13, ...] (tail [0, 1, 1, 2, 3, 5, 8, 13, ...]) = 0 : 1 : zipWith (+) [0, 1, 1, 2, 3, 5, 8, 13, ...] [1, 1, 2, 3, 5, 8, 13, ...] = 0 : 1 : [1, 2, 3, 5, 8, 13, 21, 34, ...] = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...] &gt; and how does zip get the second to last item in the list? The list itself is infinite. &gt; How on earth does it work?? I will use an analogy: When you *look* at `fibs` one element at a time the following happens (think of it as a conveyor belt): You put 0 and 1 on the belt. Then in every cycle you put a new element on the belt by looking at the first and second position and adding them together. Then the belt is advanced by one position.
I either hide or don't list symbols from common libraries that I want to shadow.
Thanks for your input. I'm more focused on Lisp as I intend to read a few books and do courses and they use Lisp. Sorry about that :)
Same. I suspect this problem is greatly reduced if you use import lists or qualified imports for everything except prelude, which I tend to do anyway as it makes it easier to see where everything is coming from (and also reduces the likelihood of a new library version breaking my build).
Hey. We're interested in similar domains. I'm kind of excited. Will message you in a bit. 
Appreciate your concern. I graduated (majored in Finance) last year. Switching to a computer science course isn't allowed in India. The people of /r/haskell are super nice and welcoming. Thanks :)
The first step on supporting ghc tickets is to subscribe to them, which is taken as a show of interest.
Sounds like every context manager contains a partially-applied version of `bracket`. `withFile` is one example of this, I think.
This is good. A tear of vindication rolled down my cheek at his comments on Scala.
Minor note: you start with 0,1, but the person you're responding to started with 1,1
[I guess](https://stackoverflow.com/questions/31220903/haskell-how-to-create-most-generic-function-possible-that-applies-a-function-to/31223058#31223058) it [depends on](https://stackoverflow.com/questions/38603014/type-signature-for-function-with-possibly-polymorphic-arguments/38606908#38606908) your definition [of 'useless'...](https://stackoverflow.com/questions/33586720/list-of-showables-oop-beats-haskell/33587837#33587837) (full disclosure: I wrote those three answers).
You can fix the original, coincidentally, with explicit type applications: {-# LANGUAGE TypeApplications, ExplicitForAll, KindSignatures #-} import Data.Constraint f :: forall (c :: * -&gt; Constraint). String f = "a type class" main = putStrLn $ f @Eq (ping /u/AshleyYakeley)
Well, that makes code generation a better option than TH for infrequently changing code, like DB models. 
I think it's easier to think about `mfix` for (conceptually) length-indexed vectors first, rather than lists: newtype Vec (n :: Nat) a = Vec (Vector a) Now instead of thinking about heads and tails, you can think about positions in a vector. It's a lot easier to think about what's going on there.
[removed]
Not necessarily a full major, but maybe individual courses. If you can't find it locally, then maybe Coursera, etc, online. Something with a paid grader.
For speeding up type-checking on GHCi, you can give `:set -fobject-code`. It'll make GHCi only recompile changed files and it will be much faster when you're editing files other than those giant ones.
Sure. Will look into them.
Does anyone have any sample projects of dockerized stack projects. Note, I am not looking for something using the stack docker integration. I'm looking for a project using stack from within a docker container, so no need for a local stack install. 
Wow, awesome!! This makes perfect sense now. Thank you so much for the explanation!
Could you paste some code snippet to give a taste of the API? Quick look on that gem readme shows that it leans heavily on Ruby's dynamic nature.
&gt; Like C++, and I say this as the former head of the world's number one C++ product, its a catastrophe when is comes to writing bugs. Both Scala and C++ are fabulous ways to write bugs and you can write code that even other members of your own team can’t read, and even you yourself two years later can’t be sure. Can confirm.
Wait how can such a type have a Monad instance? How do you take `n` vectors of length `n` and produce one vector of length `n`?
Self selection bias. Haskell has a steep learning curve and is polarizing. So, either one likes it and realizes that nothing else is quite like it (e.g. "go" Haskell), or dislikes it /cannot figure it out and dismisses it quickly. I'd say the people who frequent this group are more likely to be the former.
I have a project which pulls in many many dependencies (off the top of my head, servant, websockets, conduits, socket handling, imap parsing, acidstate, fftw) which produces windows binaries in the 35-40mb range before any compression or anything fancy is done. another project with much fewer dependencies had windows binaries in the 10mb range. Yes, larger than I would like, but I binary size hasn't ever got much attention from me/customers beyond a minor remark. In contrast, the binaries for a linux (nix) build are around 23mb with the complete set of dependencies (closure) coming in at around 80mb (all the way down to glibc and including some x libs etc for plotting). Again - larger than I would like, but no big deal. 
Haskell is good stuff for sure. Even if the language weren't so clean and elegant (FP), it'd still beat out most alternatives just by virtue of the documentation system (Haddock) which renders nicely and provides links directly to actual *readable* source code.
Currently, on Windows we only support static linking, so you get a copy of the runtime, garbage collector and all packages you use included in the binary. (that's a bit simplified but close enough.) In 8.4 we'll (likely) also support split-sections on Windows (which is a bit finer grain selection of what's included In the binary), so the binaries should get a bit smaller. D and Nim are likely both dynamically linked, which means shared code are in external libraries. This is why you probably need a runtime installed for them. Dynamic linking is possible for GHC On Windows as well, but it's rather tricky since we're generating files with so many symbols for certain packages that we actually run into limitations of the binary format on Windows, due to Microsoft thinking no file would ever have more than 2^16-1 functions when the format was created. I have a working dynamically linked GHC, but I have yet to work out the final bits and track down an interface file issue. (just priorities and lack of time, so maybe 8.4 will have it, but likely 8.6). lastly, the binaries produced by GHC by default are not stripped. They contain a lot of debug info. Stripping the debug info reduces the size a lot: Tamar@Rhino MINGW64 /r $ cat hello.hs main = putStrLn "Hello world" Tamar@Rhino MINGW64 /r $ ghc hello.hs [1 of 1] Compiling Main ( hello.hs, hello.o ) Linking hello.exe ... Tamar@Rhino MINGW64 /r $ stat hello.exe File: hello.exe Size: 3252844 Blocks: 3180 IO Block: 65536 regular file ... Tamar@Rhino MINGW64 /r $ strip -s hello.exe Tamar@Rhino MINGW64 /r $ stat hello.exe File: hello.exe Size: 1121809 Blocks: 1096 IO Block: 65536 regular file ... So as you can see, more than 2/3rd of that 3.3mb is debug symbols. stripping brings you down to ~1mb, and split-sections will further reduce this to the 500-700kb range once I finish solving a few issues. Dynamic linking brings it down to the 200-300kb range. TL;DR: We currently only support static linking on Windows, It will get better. and your binaries contain a lot of debug information which can be stripped from them to reduce size. 
I do something similar, with just Atom as the text editor. But usually instead of the REPL Im running `stack build --fast --file-watch` to get an automatic rebuild on save.
Thanks for the long and detailed reply (I appreciate it)! I'm fine with the size for now, but glad it will be getting smaller. Quick dumb question: Does Haskell Platform for Windows use MinGW? I guessed it might, but that all seems to be nicely hidden away. When installing Nim it was pretty obvious that Nim was transpiling my Nim source to C and then using the GCC compiler via MinGW to build a small executable. After years of being away, I'm pleasantly surprised with Haskell. Oh...getting back on subject, if Haskell Platform on Windows is using MinGW, I can strip the executable like you did I suppose, but does GHC not have that as a built in windows release flag option?
Yep. Historical baggage =/
Ok, that is what I figured and isn't a problem. 40 MB is really big though, so it makes me wonder what isn't being optimized...etc. 
What does the "fast" flag do? I guess just a quick compile? That reminds me I need to start looking at Stack and Cabal (been focusing on GHC)
Sometimes you go Haskell, then get a need for blazing speed in your blood, and you waffle constantly between Rust and Haskell.
That sounds great, keep up the good work :D
It's a rather different `Monad` instance, so perhaps not so fair. instance Monad (Vec n) where join = mapWithIndex (\k v -&gt; v ! k)
I think I can manage that in Haskell too ... I honestly just need a few lens operators and point-free style to do this easily 
To be accurate, GHC uses `Mingw-w64` as the backend, which is technically a different project from `MinGW` since `MinGW` is deprecated. It's all bundled in the `mingw` folder in the package. GHC itself doesn't have a concept of a "release" build per say, what we do have are different optimization levels and a debug runtime. So if you use `-debug` you get a runtime linked in which has asserts and debug logging still in it. `cabal-install` how ever does, and by default I believe binaries installed with cabal are stripped (though I'm not so sure about that atm). In a way, GHC leaves it up to you to decide what you want to do with your file. I usually tend to just leave as is for debug builds, for "release" builds I run https://github.com/rainers/cv2pdb to convert the CodeView debug info in the files to PDB files and keep those for debugging. One day, if we could generate PDB directly we could do that by default..
&gt; Since you can have your programming be purely functional in other languages too, it makes me wonder how much FP matters. While this is true in principle, it's a lot harder in practice. For example, I encounter something like `badUsers : [ Maybe [ UserId ] ]` in Haskell, I can immediately write `(traverse . traverse . traverse) banUser badUsers` precisely because I can see the type signature. In JS, it would be very difficult to discern the type `[ Maybe [ UserId ] ]` simply by inspecting the `badUsers` expression. Many of the more interesting and useful shapes in Haskell (like `Traversable`) have type signatures that are just very difficult for the human brain (at least mine) to track manually. So yes, in principle there's nothing stopping you from making these fancy shapes in JS, but in practice it's not practical at all.
I don't decide you shadow symbol, it just happens. A typical case is is when you group things. You have somewhere groups = group list And somewhere else for groups (\group -&gt; ...) An option is to use `g` or `groupOfThings`, but that's not ideal.
I think this is very very bad advice. Yesod is a terrible beginner web framework. Yesod has some of the worst error messages I have ever experienced. It vomits template Haskell everywhere and template Haskell error messages are awful. It's recommended base project is so complicated it's incredibly daunting to a beginner (and to experts in my experience). I have good success teaching beginners servant and if they can't wrap their head around it, Scotty is a much better option than servant or yesod. tldr: easy web framework use scotty
Hence: 🙂 &gt; With `TypeApplications`… it does change the way you want to write things 
You think _that's_ bad. You must not know the pain of your code not compiling with all sorts of weird type errors because you forgot to import some implicit conversion object. Or the dreadfully unhelpful "implicit not found" message when debugging typeclass instances. I'll take the operators and point free style any day. At least I always know where to start debugging (just follow the types). Even when something _is_ overloaded, I don't have to debug it all the way up a ridiculous inheritance tree. And I don't need to worry about things magically transforming themselves anywhere along the way because of implicits. Also: state. It gets woven in in the smallest but most subtle ways. You _want_ to be completely pure but Scala's support for tail recursion starts being wonky just when you need it in the tougher cases so you have no choice but to be imperative.
What helped me the most was to stop trying to use the best, the latest and the most theoretically sound solutions in Haskell land. Like many languages Haskell has some warts, ugly parts and just weird but wonderful stuff. When doing "real world" programs, instead of just learning examples, you'll quickly run into them. For example: records, monad transformers / mtl and callbacks (of course there are plenty, plenty more). At first when you run into them, some people will go into a "must find the best optimal solution" mode for them. So they end up with extensible records and backpack and lenses and deep black magic. If they get that to run, they'll pile on extensible effects, free monads and more black magic. If that were not enough, then you'll try to wedge FRP into it, maybe with arrows and even blacker magic. In the end you get a horrible mess that you can't understand, there's no way to fix type errors and you've spent a lot of time and effort on essentially plumbing, not on your domain problem. Just use the simple thing that works until you get a feel for the land. Prefix your record accessors with the record name, write out by hand some transformers (and stuff everything in IO) and use regular functions for callbacks. Get your stuff working, solve the problem, and then when you have a working program try to upgrade your code up the pyramid.
Me. I've worked with Haskell for about 4-5 years now; not a single line for work. None of my jobs ever required more than 5% of my intelligence. I try to compensate by working on personal projects after work and it's definitely hard. You are not alone. It's a bit like being Batman. 
The real thing to learn here is how to use Hoogle
I'd say Emacs is only ever good newbie advice if the newbie already uses Emacs.
&gt; This is good. A tear of vindication rolled down my cheek at his comments on Scala. As somebody writing an SQL parser in Java: [you lucky, lucky Bastard!](https://youtu.be/zPi76KvQF1g?t=35s).
This looks like a colax monoidal functor where you're choosing the coproduct in Hask as the tensor. Edit: change lax to colax, since it goes the other direction.
What's `flipEither`? 
Or you use eDSLs like accelerate. I converted some code over to it the other day and got a 10000x speedup, which has left me wondering if there's some kind of bug in one of my implementations...
Also, high level abstraction (like monad transformer) would be really hard to work with without the help of a typechecker.
"bad"? no (there is actually no need to try to convince me of Haskells pros ... I just think I saw some cons too) BTW: ,y comment was about readability of code (assuming it is correct and compiles) and yeah I can produce a unreadable mess in any language and yes I think Haskell can easily lead to quite unreadable code (exactly because some love their operators and point-free styles to much for my liking - my mental capacity can not hold many ops at a time sorry) ... and weird compiler errors ... well let's say GHC is not Elm
`lens` has a simple interface and a complex implementation, which is better than the other way around
That's very insightful, thanks for sharing this with us! EDIT: It appears you accidentally deleted your comment. But you're lucky because I took [a screenshot](http://imgur.com/a/v5mCK)!
It is indeed a coproduct-preserving functor.
https://hackage.haskell.org/package/errors-2.2.1/docs/Data-EitherR.html#v:flipEither
This really is a great talk, thanks for sharing!
Possibly helpful https://rybczak.net/2016/03/26/how-to-reduce-compilation-times-of-haskell-projects/
I'm going through the Haskell Book and this Typeclasses exercise doesn't make sense to me. This gives an error: " Could not deduce (Fractional a) arising from the literal 1.0" f :: Num a =&gt; a f = 1.0 To me, this is illogical, if I require that a value supports operations of the Num typeclass, then a Fractional supports those operations too!
I don't find it *that* simple. It is consistent once you know how it works, but I was very surprised the first time I was confronted to the monoidal summary of `view`.
&gt; difficult for the human brain (at least mine) to track manually. Good luck keeping that after a refactor, I would add.
Do you mean: &gt; with the law `split . f = f . fmap split` ? 
Yes, that's what I mean. Sorry, I don't understand! Please explain what you mean. I still do not understand. Can we talk about Madonna?
That bot again... 
I would be curious to read a transcript of the talk, but I won't take the time to watch the video.
Actually I mean flipEither . split = split . fmap flipEither thanks!
Ah yes, of course.
Maybe one of our rejuvenated moderator team can ban it.
You can write code that is purely functional in other languages, but you can't *rely* on code being functional—unless you're willing to trust conventions. I've found that this makes Haskell *qualitatively different* even from similar languages like OCaml (which I've also used extensively).
 -- this typeclass and all the relevant infra is provided, and shouldn't need to be tweaked class (HasDatabase m) =&gt; HasJobQueue m where scheduleJob :: UTCTime -&gt; JobPayload -&gt; m Job createJob :: JobPayload -&gt; m Job createJob pload = do t &lt;- getCurrentUtcTime scheduleJob t pload -- the following ADT needs to be define by you. It associated the types of jobs, their arguments, and their types in a type-safe manner data JobPayload = JobConstr1 (jobarg1, jobgarg2.., jobargN) (jobresult1, jobresult2,.... jobresultN) | JobConstr2 (jobarg1, jobgarg2.., jobargN) (jobresult1, jobresult2,.... jobresultN) -- and so on. -- the following function needs to be defined by you. jobAction :: Maybe User -&gt; JobPayload -&gt; AppM JobPayload jobAction mUser (JobConstr1 jobInput _) = (JobConstr1 ji) &lt;$&gt; (runAppM $ customJobRunner1 jobInput) jobAction mUser (JobConstr2 jobInput _) = (JobConstr1 ji) &lt;$&gt; (runAppM $ customJobRunner2 jobInput) -- and so on
Right, thanks 
Don't worry, the English spelling is not Aquire either (acquire). Like they say, the language of science is **heavily accented English**
I think swapping the last two arguments of `foldr` would actually make more sense: foldr :: (a -&gt; b -&gt; b) -&gt; ([a] -&gt; b -&gt; b) foldl :: (b -&gt; a -&gt; b) -&gt; (b -&gt; [a] -&gt; b)
&gt; Since you *can* have your programming be purely functional in other languages too, it makes me wonder how much FP matters. Can you? It is certainly possible to write pure functions in any language, but if doing so isn't the norm in that language's community, it's going to be quite difficult to write all your code in this style. 2. You won't be able to use other people's libraries. As a simple example, if your language uses `array.sort()` to sort an array in-place and you want a pure API instead, you'll have to write a wrapper `sorted(array)` which first copies the array. As a more complicated example, GUI libraries are usually based on callbacks, forcing you to mutate some state inside those callbacks in order for them to have any effect. In functional communities like Elm and Haskell, we think we can do better, and so we have been experimenting with alternative APIs like Gloss and FRP for literally decades. Finding an elegant way to wrap an imperative API is not always as easy as copying the input! 3. Some abstractions cannot even be expressed in some languages. For example few languages have “higher-kinded types” (used e.g. to write a single operation which works on many different types of containers) and even fewer have type classes (used e.g. to describe the subset of containers on which an operation works), and so you might have to write a lot of duplicated code (e.g. one copy for each type of container) compared to what you would write in Haskell. 3. The language syntax and semantics might not support FP idioms very well. As a simple example, in Haskell `add = \x y -&gt; x + y` is a curried function (meaning `add 1` is a function which adds 1 to its input, not a `wrong number of arguments` error), whereas in javascript `add = (x, y) =&gt; x + y` is not curried, you have to write the slightly more verbose `(x) =&gt; (y) =&gt; x + y` instead. As a more complicated example, in a language which does not perform “tail-call elimination” (an optimization which allows recursion to use the stack as efficiently as a `for` loop) you might not be able to write your loops using recursion, forcing you to use more complicated setups such as “trampolines” and “recursion schemes”. 4. Your colleagues probably won’t let you write code this way. If you write code functionally at home and find that it really helps you out, you might want to spread the gospel at work in order to get the same benefits, but if your colleagues are used to your language’s idioms, they might not be willing to try your approach even if it solves their problems. For example once I was working on a large codebase and we had bugs because all execution paths had to call some expensive function exactly once but some didn’t. In Haskell I would change that function’s signature so that it returns a wrapper type indicating that the call has been made. This way, at all points along every execution paths there will be a clear point where we have a value of the raw type and we need to call a function expecting the wrapper type, and that’s the point at which we must call the expensive function in order for the compiler to accept our code. It was the perfect solution to our problem and it wouldn’t even suffer from any of the other problems I mentioned above, but my colleagues laughed it off because “this is C++, not Haskell”. That’s when I started looking for a Haskell job: not because I want to use Haskell at work, although that’s certainly nice, but because I want to debate which FP idiom is the best solution to our current problem, not whether FP idioms are a good idea at all.
Good documentation is now an advantage of Haskell? [Wow, we have really improved over the last few years!](https://www.reddit.com/r/haskell/comments/6u1c4h/datahaskell_impromptu_workshop_at_icfp17/dlpbeal/) 
I'm still a novice in Haskell, so I thought it could be useful to share my experience on the matter. My first "real" project was a Servant Web Application. I would stare at the types and natural transformations in awe and I still do, without understanding everything so well. I went on to write something that worked, until I had to connect to a database. The connection-pool package then required me to have my monad transformer stack supply instances of MonadBaseControl IO, which was really hard to understand (both the need for it and how to create those instances, since I then had to learn about type families). Of course, long before reaching that point, I had already struggled implementing a monad transformer with instances for MonadIO, MonadReader, MonadState, MonadTrans and so on.. lots of different concepts at once. Not to mention the number of different language extensions I had to use, always having to lookup what they really meant and if they were dangerous. Looking back now that I understand (more or less) all the things I had to do, it all seems not too hard. But to be honest it took me a very long time to even have a web app that connects to a database. If there is any advice that would've suited me well, it would be: "Don't worry too much if you don't understand something from the language, just copy and paste from some example and move on. Also, language extensions are extremely common in practical Haskell, so don't be afraid to use them right now".
That makes sense. I'll try that. Nothing else, just Haskero?
You're welcome to open a thread like that if you want to. What'S the worst that can happen?
&gt; I want to debate which FP idiom is the best solution to our current problem, not whether FP idioms are a good idea at all. Agreed.
Couldn't this be a case of swapped generality? A Fractional is Num, but not all Num are Fractional. E.g. an Integral is still Num, but no longer Fractional.
Your signature says that you can produce any type of value, as long as that type implements `Num`. The only way to construct arbitrary `Num`s is using the `fromInteger` function that takes a whole number. This is what implicitly happens when you write an integer literal. Similarly, when you write a fractional literal (like you have done) this implicitly calls the `fromRational` function that takes a `Rational` and produces a value of any type that has the `Fractional` constraint. So what's the problem here? Well, you're saying you can give me any `Num` so give me an `Int`. Unfortunately, the way you're constructing my value involves calling the `fromRational` function which requires the result to have an `Fractional` instance, which `Int` doesn't have. Effectively, you're saying you can construct any `Num` but you're only able to construct `Fractional`, which is (strictly) more constrained. It's hard to see where your intuition doesn't match up with what you're seeing but I can take a wild guess and assume that you expected something similar to how inheritance works. In an object oriented language you might have something like this class Num class Fractional extends Num Num f() { return 1.0; } and that would work. The difference is that this `f` makes a different promise. It says that it can make _some_ `Num`, and then the caller of the function is the one constrained to only using the `Num` functions. In Haskell the signature says you can make _any_ `Num` and the caller is completely unconstrained and can actually choose which `Num` suits them.
How do you plan on using Haskell in Visual Studio?
* [Richard Eisenberg's thesis](http://cs.brynmawr.edu/~rae/papers/2016/thesis/eisenberg-thesis.pdf) * [Papers from ICFP 2017](https://github.com/gasche/icfp2017-papers) * [Embedding F](http://homepages.inf.ed.ac.uk/slindley/papers/embedding-f.pdf) (embedding *System F*) * [Deriving Law-Abiding Instances](https://arxiv.org/pdf/1708.02328.pdf) (*Liquid Haskell*) * [Guarded impredicative polymorphism](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/07/impredicative-Jul17.pdf) * [`APL`icative Programming with Naperian Functors](https://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/aplicative.pdf) (what Jeremy calls *Naperian functors* are [`Representable`](https://hackage.haskell.org/package/adjunctions-4.3/docs/Data-Functor-Rep.html) functors) * [**Function Pearl**: Compiling a Fifty Year Journey](http://www.cs.nott.ac.uk/~pszgmh/fifty.pdf)
To compile, you can either write b :: Fractional a =&gt; a b = 1.0 or c :: Num a =&gt; a c = 1 If it still doesn't make sense after reading both of these comments of mine, I can maybe go into more detail as to why.
I'm following Bjarne Stroustrup's Principals and Practices using C++, but I've already successfully completed it. Which I will edit in my O.C.
(Warning: self-promotion ahead). If you liked that one, you might also like *Computing with Semirings and Weak Rig Groupoids" by Carette and Sabry [official link](https://link.springer.com/chapter/10.1007/978-3-662-49498-1_6), [non-paywalled PDF](https://www.cs.indiana.edu/~sabry/papers/weak-rig-groupoid.pdf). It's also first-order, but it adds a complete level 2 of program transformations.
I guess it depends on what brings you to it. For me, I'm not emotional at all when it comes to programming languages. I'm only interested in how powerful it is. I can use a language, and love a language for years, then find a more powerful one and drop the previous language over night. So far I've never found anything more "powerful" (in the ways that matter to me) than Haskell, though I'm paying very close attention to Idris.
If you could Private Message me, I'm sure we could figure out how I plan on using FullTick b/c I have no idea what sub this is. 
It seems that this type of functor is also known as _decisive functor_ [1](https://stackoverflow.com/a/23316850/474311) [2](https://web.archive.org/web/20150925214139/http://sneezy.cs.nott.ac.uk/fplunch/weblog/?p=69). See also [this thread](https://www.reddit.com/r/haskell/comments/qsrmq/coapplicative_functor/) on a related discussion. 
As a guy who is about to probably take a scala job, this bothers me a bit. I had never heard such a scathing opinion of scala until this moment. But I guess scala is certainly better than the languages I currently use.
If it's infrequently changing, then you shouldn't have to rebuild that module very often anyway, so I doubt the difference would be very big in practice. After all, code generation is exactly what TH is doing. And it's doing it in a more automated way than what you would come up with if you rolled your own.
thank you so much, now I get it!
What storage does it use?
I think this is going the opposite direction of a monoidal functor. What you're describing would be `Either (f a) (f b) -&gt; f (Either a b)`
I feel the same way.
Great answer! Thanks. Now the question is "Does this exist on Hackage?"
You can write good software in most general purpose languages. But as a tool not all languages are yielding the same benefit. Comparing Scala to more proper FP languages, I think that the comment is spot on. Comparing it to JavaScript or PHP, it is in many aspects a much better tool. 
That may make sense at some later time, though there are bigger priorities now :-D Thanks for the idea.
He recommends people use Clojure, F# or Erlang if they don't pick Haskell. I consider myself a _typed functional programmer_, and if I had to give up types or first-class functions, I'd probably give up first-class functions. My experience is that sometimes I'm smart enough to write beautiful higher-order code, but I'm always dumb enough to get it wrong without types. What causes Aaron to neglect types so freely? Is there a reasoned argument somewhere? Or is this just CTO's missing the greatest formal verification tool ever invented?
Cool! Would it make sense to write tests using single-cell 'channels', i.e. a simple MVar, where the 'send' function is implemented like - If MVar empty, put message in it - If MVar is not empty, take out existing value and replace with new message which wouldn't model message-reordering, but would model message loss I believe? Anyway, *if* I get the lib up and running, would you be interested to collaborate on finding what can be done wrt DejaFu-or-similar testing of it? When I read the (first?) DejaFu paper back in the day, modeling of message passing was the first application I thought of (though I'm biased given earlier projects :))
I think side effects stand in the way.
As someone who writes Scala every day, trust me that Scala has no redeeming features. &gt; You're obviously exaggerating. No, and I can qualify it if necessary. No. Redeeming. Features.
We weep for you.
Last month I read [Notions of Computation as Monoids](https://arxiv.org/abs/1406.4823) which gives a common setting to study Monads, Applicatives and Arrows, the associated free constructions and some optimizing transformations. A lot of it went way over my head but it's still a fascinating read. I will have to come back to it.
While Erlang's (offline) type-checker [Dialyzer](http://learnyousomeerlang.com/dialyzer) is not even on the same planet as Haskell's type system, the only semi-large Erlang project I have worked on (four years and running), used types from day one and every new deploy is fully type-checked. Judging by recent books on Erlang I think it's quite common to use Dialyzer – I sure wouldn't deploy a project without using it.
Thanks for the info. I knew Dialyzer was cool, it seems it's even more cool in practice. 
10,000? That sounds like a major bug in your normal version :/ Either way, with Rust you don't have to manually unbox your ints and crap to get the top grade performance, so it's a simpler experience when you want the speed. If you stick to simple enough Haskell function APIs it's often pretty direct and simple to convert them to Rust once you've got your prototype done.
Would you rather write in Python (serious question)? (Less seriously) What about PHP, or Perl, or BF or Intercal?
&gt; Can Coercible be modified to (or have instances written for it that) find a way around unsafeCoerce? [This paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/coercible.pdf) discusses ideas that allow coercions like the `Map Foo Int -&gt; Map Int Int` coercion that you present here. I remember hearing about this proposal in [the haskell cast](http://www.haskellcast.com), but I don't exactly remember which episode and I can't figure it out just by reading the Episode descriptions.
Came here to say this ...
"... as fast as c++ on a good day." i like haskell, but this stretches the meaning of "on a good day" a bit.
I’m pretty sure that paper discusses ideas to allow coercions in general while disallowing coercings like `Map Foo Int -&gt; Map Int Int`. Specifically you cannot coerce nominal type parameters. Imagine what would happen if `Foo` and `Int` have a different `Ord` instance. Then the invariants enforced by `Map` would all be broken by that cast.
[Making a fast curry: push/enter vs. eval/apply for higher-order languages](https://www.microsoft.com/en-us/research/publication/make-fast-curry-pushenter-vs-evalapply/) Working on a custom Haskell compiler on top of ghc. The paper is a nice tutorial of the STG intermediate language.
[On the resolution semiring](https://www.normalesup.org/~bagnol/phd/these_book.pdf) [Transcendental Syntax I: deterministic case](http://iml.univ-mrs.fr/~girard/trsy1.pdf) [Cyclic multicategories, multivariable adjunctions and mates](http://www.math.jhu.edu/~eriehl/mates.pdf) 
I see your point. Yes, types and compilation are definitely the redeeming feature there compared to those languages. I'd definitely not want to write any of those languages you listed. If you'll allow it, I'll move the goal posts. Scala has no redeeming features in: - FP. The points in the video stand (plus more I could give) - the Java ecosystem. Spark? Nope, there are plenty of bindings to other langs for that. Scala's a better Java? Nope, that's Kotlin. - [Native](https://github.com/scala-native/scala-native). Divorced from the JVM, what are you left with? Scala's syntax and an empty ecosystem? There is always a better option than starting a new project in Scala.
I think you would have to keep a distinction whether you are working with pure functions (functions in the base category) or working with functions with side-effects (functions in some Kleisli category). Which for Haskell is done by it's type system (and by the fact that with the squinting no function of type `a -&gt; b` has side-effects).
[Expert, no direct relevance to Haskell :)] This very colorful paper [String Diagrams for Double Categories and Equipments](https://arxiv.org/abs/1612.02762) because I've been interested in double categories and equipments recently in my research on gradual typing.
Having come back to Python it's so painful to look through the documentation and think "just show me a type signature!".
The opposite direction would be an [oplax monoidal functor](https://ncatlab.org/nlab/show/oplax+monoidal+functor) I think.
I dunno - I tried to use it with Elixir and was disappointed with what it didn't catch. Ratcheted up the conservativeness of the inference and everything via the options it provided... also no support for parametric polymorphism either. I decided it wasn't really worth it.
I would say, pick the best tool for the job. In my case, I have to interface with system API and other lower level layers often. In which case I just break out C and use C or assembly and expose a simplified C interface and bind that to a Haskell driver. I do this instead of spending hours writing ffi wrappers around structures and functions which provide very little in my opinion. I've also used Haskell for logic code before, compile to a dynamic library and load that in e.g. C# for UI. I don't think there is any one language that solves all life's problems, so I just pick the best solution for the job. My personal opinion is that FP languages doesn't matter as much as FP concepts. But I suppose it may be the domain I work in. 
Just finished reading [datatypes a la carte](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.4131). It's a much easier and shorter read than most papers I've downloaded to my Documents folder, I'd recommend it.
Why ask about Python and PHP in the conversation about scala? The obvious alternative is java. And yes, I'd rather write in java than in scala. 
Ah, yes, I mean *colax*.
*oplax* sends comonoids to comonoids.
Not quite, that requires the opposite direction, f (Either a b) -&gt; Either (f a) (f b)
The *lens* library has a module [`Control.Lens.Reified`](https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Reified.html) that wraps its optics in `newtype`s newtype ReifiedLens s t a b = Lens { runLens :: Lens s t a b } newtype ReifiedTraversal s t a b = Traversal { runTraversal :: Traversal s t a b } which can be stored in data types but they cannot be composed with the usual function composition, thinking out loud newtypish Lens'N s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; (s -&gt; f s) hd :: Lens'N [a] a hd f (x:xs) = do x' &lt;- f x pure (x':xs) hdhd :: Lens'N [[a]] a hdhd = hd . hd could get translated into newtype Lens'N s a = Lens'N { getLens'N :: forall f. Functor f =&gt; (a -&gt; f a) -&gt; (s -&gt; f s) } hd :: Lens'N [a] a hd = Lens'N $ \f (x:xs) -&gt; do x' &lt;- f x pure (x':xs) hdhd :: forall a. Lens'N [[a]] a hdhd = Lens'N (getLens'N hd . getLens'N hd) Using coercions: hdhd :: forall a. Lens'N [[a]] a hdhd = Lens'N $ coerce @(Lens'N [[a]] [a]) @(Lens' [[a]] [a]) hd . coerce @(Lens'N [a] a) @(Lens' [a] a) hd 
FWIW, here's what a message/event handler looks like, a fairly (yet unfinished) translation of the paper's prose algorithm: https://github.com/NicolasT/kontiki/blob/9bdf46c55c1d5a0660efc22184b5739af8261439/kontiki-raft/src/Kontiki/Raft/Follower.hs#L70
+1 for owl-like library in haskell! (improved hmatrix, what not). However, it probably should be designed in a way to make smart use of typeclasses, if that is possible. Unfortunately, I distanced myself from haskell lately as I am moving toward finishing my phd and I have to just get things done((( julia is a good help here by the way. But hoping to resume my interest in the future :)
btw, /u/ocramz how is your petsc-hs project is doing? Petsc is something I am going to do soon :) Related to that. Have you thought about making petsc-hs as DSL like accelerate which compiles to C instead of inline-c? I always thought about that. Do you think it will be a good approach?
I wonder if never exposing type synonyms really helps getting better error messages. Two of the more common libraries where people often complain about this are `lens` and `pipes`. But in my experience the confusing part is not so much that everything is expanded but that some parts of the error use the type synonym while others don’t. However, very often the parts that don’t use the type synonyms in error messages are also simply not defined in terms of those type synonyms since they are actually more general. So in this case never expanding type synonyms will still leave you with error messages that at least I would consider confusing and often even more confusing than if the type synonym would be consistently expanded.
In JavaScript, it's easy to find real-life problems that keep me motivated to practice. What are some practical Haskell problems that Haskell devs encounter?
Well, not really specific to Haskell or PL research, but I went through [Sorting and Selection in Posets](https://arxiv.org/pdf/0707.1532.pdf?) for inspiration of a variant of `Data.Map`/`Data.Set` where keys are only partially ordered. This is for a rudimentary, half-tested, non-documented project to be found [here](https://github.com/sgraf812/pomaps/). (Maybe this would better fit a '[Weekly] What are you working on', but whatever) Edit: To actually give a more 'relevant' paper, I really enjoyed reading [Theory and Practice of Fusion](https://www.cs.ox.ac.uk/ralf.hinze/publications/IFL10.pdf), which cleared things up considerably for me. Discusses foldr/build, destroy/unfoldr, stream fusion and the connection to church encodings. Key insight: Church encodings allow recursive definitions to be realized as non-recursive unfolding or folding functions (e.g. (co-)algebras). This was very enlightening, as it (referring to stream fusion) enables fusion without any need for rewrite rules and thus are relevant to compilers other than GHC. The closest we have right now seems to be [Data.Conduit.Internal.Fusion](https://www.stackage.org/haddock/nightly-2017-08-20/conduit-1.2.11/Data-Conduit-Internal-Fusion.html), not sure how actively that approach is pursued. Edit2: I just dug up the [`vegito`](http://www.yesodweb.com/blog/2016/02/first-class-stream-fusion) series of blog posts by /u/snoyberg, where he does exactly what I'm currently interested in. I wonder how it fares with the problematic `concatMap` combinator.
Judging from &gt; 36:26 safe zero-cost coercions it's probably Ep. 14 (the latest one) with Richard Eisenberg/goldfire(re).
Jeremy Gibbons. [Origami Programming](https://www.cs.ox.ac.uk/jeremy.gibbons/publications/origami.pdf). 2003 Difficulty: 2/5. This article is a guided tour in the world of folds and unfolds on various data (lists, natural numbers, trees). One of the most interesting applications of the paper is writing sorting algorithms (_e.g._, insertion sort, bubble sort, selection sort) elegantly as folds or unfolds. The paper is pedagogical as it comes with more than fifty exercises. I would say the paper is light on pre-requisites, the most advanced pre-requisite being some maturity in proofs using equational reasoning. I feel the paper is a good introduction to the more advanced topic of [recursion schemes](https://hackage.haskell.org/package/recursion-schemes), as it gives examples of hylomorphisms, paramorphisms, apomorphisms.
The the body is small enough to be a lambda, `g` should work fine. BTW, That was me deciding *not* to shadow a symbol. Of course it is a decision to shadow or not.
Not compiling is still better than running and silently doing the wrong thing.
Oh and that too! Did you know that `PartialFunction[-A, +B] extends (A) ⇒ B`? Think about that for a moment, then compound that with the fact that `{ case ... }` expressions are `PartialFunction`s. Tell me, what do you think happens when you have code like `val (a,b,c) = if (true) "bar" else Some(10)` (lifted from &lt;www.lihaoyi.com&gt; on his Scala warts post)? If you answered "it compiles and fails at runtime on a 'MatchError', you guessed correctly. 
That being said, I honestly don't think this is so much "rust is fast and Haskell is slow" as it is different priorities for the different languages. If Haskell had an easier way to use unboxed types and data structures, had poured money into certain compiler optimizations, and so on, there wouldn't be as large of a difference between the two in speed. Rust prioritizes speed and memory safety. Haskell prioritizes pureness and equational reasoning. If Haskell cared more about speed, I'm sure even its idiomatic code would be within 10x of rust.
This thread was an awesome idea! I hope the mods consider stickying it.
I think this is mostly a difference in types of documentation. Haskell's API documentation (types and such) is excellent because of haddock and everyone using type signatures. It's softer "how to use" documentation is generally... Lacking a bit. That's where Haskell gets its infamous "no documentation" stereotype from. Unfortunately, softer documentation is generally much more difficult to write than the harder API centric documentation which almost writes itself.
Thanks. Although most posts aren't paying attention to the tags. 😂
Hey, at least it fails. In C++ it would just silently keep running and interpret arbitrary memory locations in arbitrary ways. I feel your pain, though.
Perhaps? Didn't look too closely. However it's probably helpful to realize that typecheck is almost negligible in terms of Haskell's total compilation time.
The errors I get with Servant are vastly worse than the errors I get with Yesod. I'm impressed that you got beginners using it -- do you have any methods or materials you like for that?
Same here. Or an outline of the talk. 
In that case, probably not, as this is indeed just focusing on parsing and typechecking.
You have to squint a lot harder.
With `type Foo = Int`, I don't think you can have a different Ord instance. With `newtype Foo = Foo Int`, you can and will unless you use generalized newtype deriving.
I prefer your term "nominal alias" instead of "nominal newtype". All `newtype`s are nominal, that's the point; they have the same representation have the type they wrap.
Here's an edge case to consider: type Age = Int x = 2 :: Int y = 25 :: Age z = x+y How should error messages report the type of `z`? My understanding is the GHC picks `Int` instead of `Age` in this case, and that the only times that GHC "forgets" the type synonym is when it has been used in conjunction with a non-synonymed type as in this example. It seems like your proposal would have to forget in this situation as well.
Thanks! Kitten is going pretty well, and people have been showing more interest lately, which is always pleasant. :) Emacs is fine for Haskell; I mainly use it because I was already comfortable with it, not because it has any particular advantage. I really don’t use many IDE-type features when programming in any language, so `haskell-mode` is more than sufficient. For autocomplete I just use the dumb `dabbrev-expand`. And I typically navigate a codebase by memory, or occasionally with `ag`/`grep` or a local Hoogle instance. The single most valuable programming-related Emacs extension I use is probably `git-gutter-mode`, which just indicates the lines where you’ve made additions/modifications/deletions and lets you preview &amp; revert individual hunks.
Haskell is a really fantastic language for code reuse, and is quite intuitive. Having such a powerful type system with 100% global type inference (until you turn some fancy extensions on) is honestly amazing. Laziness means I don't really need to worry about destroying performance when I make my code more modular and composable. The flexibility of the type system means I can write absurdly generic code and know that I'm using it mostly right. Haskell is great partially because it *enables* you to do so much, but also because it *encourages* you to do things in a way that maximizes safety and code reuse.
I've used Haskell for about 2 years, and Rust for about 9 months, but it's been my experience that (assuming you mostly know the libs you want to work with and such) Haskell is faster to write than Rust but doesn't get as much raw speed in the end. You are somewhat correct: it is not simply that Rust is fast and Haskell is slow. It's that Rust revels in primitive/unboxed operations and Haskell boxes everywhere by default. So much so that manually unboxing can _drop_ your speed in some cases just because other things only expect boxed values so you have to rebox every time you pass a value into a library computation. If you want to do games programming that pushes as hard as possible on the hardware, you want Rust. That all said, the boxing, immutability, and GC do let Haskell really shine when your code paths aren't as totally clear. I'll take STM and IO composition over Rust's current best efforts any day. It might not be the fastest, but its far easier to reason about, and you can focus on the details that are important, not the fiddly stuff GC should handle. For writing a server type deal with lots of concurrency, or anything else that isn't harshly bounded by the CPU, I'd use Haskell.
You don't go back, you go coforward.
[LVars: Lattice-based Data Structures for Deterministic Parallelism](https://www.cs.indiana.edu/~lkuper/papers/lvars-fhpc13.pdf) -- _Advanced_ -- A lovely deterministic formalization of fork/join parallelism. A pretty easy read, though since it builds on both type theory and concurrent programming, I'd label it as more advanced.
Why's that?
Which project is good to contribute to for beginner haskell enthusiast?
Surely that *should* be called `swapEither`. Or maybe `coswap`?
* I learned Haskell in 2 days * We do a LOT of data transformation, in multiple stages * Haskell is great for writing parsers. * Purity is great for testing * Pure functions can be their own microservice, easily. * Libraries: we have so many, but they are all so small. In JS, I can basically just get a whole user management system as a library, and in Haskell you have to build everything. * Stack is good, but we have to build on top of it with shake (Makefiles suck) * IDEs could be better, I hope someone is working on this. * GHCJS is really cool; share code between the frontend and backend because they are the same language, just different targets. Probably some other stuff, I sort of zoned out in the middle, too.
I finished [the STG paper](https://www.microsoft.com/en-us/research/publication/implementing-lazy-functional-languages-on-stock-hardware-the-spineless-tagless-g-machine/) and found it very cool. Currently reading the Smalltalk 80 blue book and trying to make an implementation in Rust.
Any speedup is a good speedup and typechecking will become more difficult and time consuming when things such as dependent types start rolling into Haskell's typesystem. But yes, I'd be surprised if it made much of a noticeable difference outside of massive programs.
I'm not sure it's just a "wanting to get stuff done"; perhaps it's more that the FP toolbox has expanded. Before it was equational reasoning backed by a type system but now the type system is almost the starting point. Entire languages based on "programming with types" are being fleshed out. Equational reasoning and using laws has sort of taken a back seat in popular literature now.
I think the cases add Empty [] [] add _ (x:xs) (s: stack) Aren't covered. Ghc should tell you what cases you missed if you have -Wall and -Werror on
I often watch this kind of thing when doing dishes or similar.
Wait, is Google patenting mathematics?
You shouldn't be able to meaningfully switch from one to the other, because they mean different things! :) That's like saying we should sell hot coffee and cold coffee in the same cup so it can be easy to switch orders.
I miss "Hask anything" :'(
Do you have some example outputs of such a function? It seems like you would want to have a tuple as the output? Because I would appreciate a generic function of that form. Although I think witherable already gives you that. 
This is neat, was thinking about making something like this. It would be really sweet if someone made the blockchain type safe using dependant types. Also using an LPGA for mining defeats the purpose of the "one-CPU, one-vote" principle, maybe try implementing an egalitarian mining algorithm like [cryptonight](https://cryptonote.org/cns/cns008.txt)?
https://www.edx.org/course/introduction-functional-programming-delftx-fp101x-0 I was doing this a while ago, it was a pretty nice and easy to follow approach. Tho, I already had a lots of production experience with c++ (and some with rust) at that time.
Honestly if I were giving a Haskell novice advice it would be this: "Don't use servant. And *definitely* don't use your own monad transformer stack." Just do everything in IO with a simple library like Spock. Most devs come into Haskell and ask "What's the *best* tool for X?" That's a perfectly fine thing to ask in most language ecosystems. But in Haskell it's not the best question to ask. We need to qualify it with "as a novice." Haskell's ceiling is extremely high. Most ecosystems I've worked in don't have nearly as much spread (C++ being the exception). In Python/Ruby/C# etc. all the alternatives are mostly around design taste and not around taking advantage of more or less advanced concepts, like is the case in Haskell.
Thanks u/gelisam, that's it!
Included is a small sample app https://github.com/reflex-frp/reflex-sdl2/blob/master/app/Main.hs I also wrote a little blog post http://zyghost.com/articles/reflex-sdl2/
Have you looked at `fanEventWithTriggerRef` for the host? It might be a bit more efficient. I wouldn't really know how to benchmark the two versions, but I have a version that uses that [here](https://github.com/dalaing/reflex-sdl2) if you want to take a look. 
Why? Mostly for the fun of proving I can. But references to existing librairies that deal with some of the use cases of context manager, fully abstract or not, is what I was expecting/hoping.
Now that I think about it just requiring OVERLAPPABLE should be sufficient for coherence. 
I would say that either way, one realizes there is nothing else quite like it. :)
* [Hazelnut: A Bidirectionally Typed Structure Editor Calculus](https://arxiv.org/abs/1607.04180) [Medium-Advanced] * [Abstracting Definitional Interpreters: Functional Pearl](https://plum-umd.github.io/abstracting-definitional-interpreters/) [Advanced, lispy] * [Frank Pfenning's lecture notes on Substructural Logics](https://www.cs.cmu.edu/~fp/courses/15816-f16/). [Medium-Advanced] Highly recommended for linear logic, call by push-value, etc * [Dependent pattern matching and proof-relevant unification](https://lirias.kuleuven.be/bitstream/123456789/583556/1/thesis-final-digital.pdf) [Advanced, but this is a thesis, so lots of exposition] * [Talking Bananas: structural recursion for session types](http://homepages.inf.ed.ac.uk/slindley/papers/talking-bananas.pdf) [Advanced] * [Sequent Calculi and Abstract Machines](https://www.cs.indiana.edu/~sabry/papers/sequent.pdf) [Advanced]
Oh! Great! Looks like we both used the same lib name as well! No I haven't used `fanEventWithTriggerRef` yet, though I picked this implementation because even though it's very repetitive and ugly, it's dead simple. It also has the benefit of only triggering events that have been subscribed to, the rest are discarded. `fanEventWithTriggerRef` (and friends) may do that, I'm not sure, some profiling would definitely be a good thing. 
&gt; Why ask about Python Because my experience with Scala has been using it as an alternative to Python. &gt; Why ask about [...] PHP Because PHP is a language that people actually use and most people agree is bad. Parent said "No. Redeeming. Features." and emphasized that they weren't exaggerating, so I inferred that they rank Scala below PHP and asked to make sure.
I'd say that Scala is better for scripting and writing in the small than Java. It also has a REPL. I agree that this is not justification to start a large new project in Scala. As a tangent, why to you mention Kotlin rather than Ceylon? Every time I've looked at both I've been much more interested in the latter.
I like this talk but I think the Scala bashing is unfair. I do it for a living and there's a lots of issues but Haskell could learn a lot from Scala's emphasis on easy interop. I don't care for OO myself but there's at least 2 generations worth (and counting) of useful code out there to which we in Haskell land have practically no access. I'm not saying add OO to GHC, but maybe just an small extension to the FFI for calling pre-made objects without doing something ghastly like using strings for method names. Manuel Chakravarty's `inline-*` efforts are very exciting for this very reason.
 split $ Identity $ Left 'a' ==&gt; Left (Identity 'a')
&gt; why do you mention Kotlin rather than Ceylon? Haha, because I'd never heard of it. Or if I had, I'd forgotten about it.
A Jobs table in PG.
Oh oops. I missed the "identity" and "writer" part. I'm a little confused as to the purpose of such a function. I guess it only works well on functors with 1 or more values and that don't require any sort of context (so IO, ST, Reader, and State don't work). 
You could write an instance for nonempty that follows the law can't you? Just arbitrarily decide that the first element defined whether you go Left or Right in the result. 
Yeah possibly. Conor provides more candidate laws in his blog post.
Ha, good point!
I hypothesise it only works with functors of the form `Writer w`. I want it because I think it's exactly the class of functors `g` that makes `g a -&gt; f b` a `SumProfunctor` (for `Functor f`).
Having experienced both I find I prefer the hard docs (not that they are mutually exclusive!).
I agree. &gt; which first copies the array And when your application is even a little performance sensitive, this can make using a pure programming style a no-go. For example, it's not OK to copy the whole data structure just to insert one element. [Persistent data structures](https://en.wikipedia.org/wiki/Persistent_data_structure). solve this problem, but most programming languages don't have widely used, high quality persistent data structures. Haskell and a few other languages (e.g. Clojure) do. &gt; The language syntax and semantics might not support FP idioms very well Again, for performance reasons, even if a language does have support for some FP idioms, their performance may be so low that it's a non-starter to use the FP idioms wherever/whenever you want to. E.g. in the last decade, C++ added support for lambdas. However creating a lambda is relatively expensive, so you should not use them in an inner loop. Thus, for performance reasons you don't use FP idioms because the language does not have high performance FP idioms. Again, Haskell does. &gt; Your colleagues probably won’t let you write code this way. Yes. I've been questioned and been told to 'cut it out' for using FP idioms. E.g. a coworker created a loop that mixed pure stuff with side-effecting stuff. At some point I separated these into separate loops to make the whole thing more readable, and my coworker gave me the hairy eyeball and wasn't convinced it was a good idea. (Maybe I did a poor job of explaining why it's good...)
Probably because Kotlin is picking up a lot of steam and is being pushed by jetbrains, so the IDE support is first rate. It's also supposed to be great with Android, while Ceylon was just abandoned by Red Hat for the open source graveyard. Disclaimer: I've never used either, but that's all from articles I've read in the last 6 months.
Not OP, but one day I started learning Scala and I couldn't believe how much of a pain it is to get some code to run. The tooling was disappointing. Yes Python/Perl are slower than the JVM for everything but short little scripts and they don't have static types, but it's also super simple to just write something.
Glad to hear you're getting good feedback. When I've got more HS experience, I'll have to check out the compiler for Kitten (written in HS right?). I like lightweight coding, so emacs is interesting. Btw, if you get the chance and want to see an interesting new compiler talk, check out Aaron Hsu's Co-Dfns compiler for Dyalog APL. He has a 5 pages of APL one-liners in Notepad that converts the APL input to C++ for the GPU (or something like that).
What's the answer for databases and migrations in the immutable infra world? That is THE most important part of your infra which is inherently mutable and cannot be transitioned from one state to another easily. 
What is that domain?
This sort of thing gets more interesting when you look at contravariant functors: http://hackage.haskell.org/package/contravariant-1.4/docs/Data-Functor-Contravariant-Divisible.html I imagine that there's a proof that any `Decisive` as you give is iso to `(a,_)` for some `a`. This would be dual to all `Distributive` being iso to `e-&gt;_` for some `e`. 
You might be interested in Nikolaos Papaspyrou's 1998 doctoral dissertation, "[A Formal Semantics for the C Programming Language](http://www.softlab.ntua.gr/~nickie/Papers/papaspyrou-1998-fscpl.pdf)" [PDF].
But what library? persistent, by any chance? or postgresql-simple? Asking, cause I'm a bit trapped now - I have test suite where I don't mock the database, but instead I use in-memory sqlite database instead (I use persistent, so it's easy to have sqlite in tests and postgresql for normal run). And now, when I'm trying to integrate [humming](https://github.com/noteed/humming), I have problems to make it work in tests ;)
Sorry. I stand corrected.
Unity's primary language is C#, not an unmanaged C++ Engine, GC Stalls are common in Kerbal Space Program with a lot of parts on screen.
If GHC provides / we implement [`type family O (n :: TYPE rep) :: TYPE rep`](https://ghc.haskell.org/trac/ghc/ticket/13592#comment:5) we get packing / unpacking functions for free: class O n `Coercible` n =&gt; Newtype n instance O n `Coercible` n =&gt; Newtype n pack :: Newtype n =&gt; O n -&gt; n pack = coerce unpack :: Newtype n =&gt; n -&gt; O n unpack = coerce -- type instance O (DList a) = [a] -&gt; [a] newtype DList a = DL ([a] -&gt; [a]) f :: DList a -&gt; DList a f xs = pack @(DList _) (unpack @(DList _) xs . unpack @(DList _) xs) that will work for lenses once [`#9269`](https://ghc.haskell.org/trac/ghc/ticket/9269) is sorted, the question is how those casts get inserted
That's interesting work. `SmallArray#` is a first for me! By the way, the [record in extensible](https://www.schoolofhaskell.com/user/fumieval/extensible-records) is known as improvement of drawbacks of the mentioned packages in the article. How does SuperRecord compare to this?
Compiler engineering 
Sounds like the guy wants Nix and Haskell-IDE-Engine.
Huh, maybe I overestimated the proportion of C++ code. Wiki says the "runtime" (whatever that means) is in C++, but the "Unity API" is in C#/UnityScript. I may have misinterpreted these proportions, or maybe some devs just don't like the builtin mechanics and so write their own in the high-level APIs.
Java users should use ANTlr, it's a good parser generator.
pg-simple, mostly. My experience has been that it's not a good idea to use a different DB or mock-out the DB during tests. Don't you have any constraints or trigger in the DB that need to be in place during the tests as well?
C++ has few performance advantages when writing high-level code, which is surely the point of C++, otherwise I could just use C. High-level C++ code features pointer chasing, cache unfriendly layouts and non-deterministic memory management (e.g. smart pointers). Haskell is actually much better at optimising such code. For example, at my place of work, a boost spirit parser ended up much slower than the equivalent in Haskell's uuparsinglib.
I think `scanl (+1)` in the definition of `cassify_prob` unnecessarily blows up the frequencies. The rest of the pipeline assumes that the represented values are k_s, not the strictly increasing series of accumulated frequencies. I get where this comes from, though: In a prior version the author probably had in mind to do binary search with `n-beta ´mod´ t`. Just to prove my point, if you delete that line you get: &gt; iso = ans_iso $ classify_prob [0.1,0.9] &gt; encode iso (replicate 4 True) 5 &gt; encode iso (replicate 4 False) 11111 &gt; logBase 2 11111 13.439701045971955 &gt; 4 * logBase 2 (1/0.1) 13.28771237954945 Which is much closer to the lower bound. Edit: This is best understood when using a uniform distribution: &gt; iso = ans_iso $ classify_prob [0.5,0.5] &gt; encode iso (replicate 4 False) 16 &gt; encode iso (replicate 4 True) 31 This is *without* `scanl (+1)`, still skewed, probably due to the issues wrt. block allocation or sth. Note that it's never more than one bit difference (e.g. the ratio never exceeds 2)! Whereas *with* `scanl (+1)`: &gt; iso = ans_iso $ classify_prob [0.5,0.5] &gt; encode iso (replicate 4 False) 121 &gt; encode iso (replicate 4 True) 8 It's much more skewed. I found this really interesting, but am also a little sad that someone beat me to fooling around with ANS in Haskell.
"As an online discussion grows longer, the probability of a comparison involving PHP approaches 1." the Reductio ad PHP, PHP card. Godwin's rule applied to programming languages. It is very safe to say any programming language is good, if we compare it to PHP.
I do use ANTlr. The problem with ANTlr is, that it uses the visitor pattern. That means that the code has to be distributed over `enterXY` and `exitXY` statements, is thus intertwined with the control flow and that a state, which becomes complex quickly, has to be handled. In Haskell you use parser combinators and get an AST and then fold over it. 
I'm not sure I'd call it a tutorial. It assumes a lot of the reader, and is sparsely commented. Perhaps I'm at a disadvantage, not having used reflex nor sdl2 in earnest, but I had to toggle between haddocks and the tutorial to make sense of it. Perfectly fine for a unit test or example, but I'd expect that being a tutorial, I would have been introduced to concepts, vocabulary, etc. On the other hand, it's really cool that you found binding SDL2 to Reflex so easy! 
Just noticed that here you call it a sample app, in the blog post and documentation, you call it a tutorial. 
I like this a lot. It would also be good if we had a .syntax like elm.
Worth noting that regular `Yoneda` also provides this benefit, but more often needs an actual Functor
Given the interest and response this post had, maybe it could become a page on the Haskell wiki?
(Author here.) You are spot on about the mistake and its origin — thanks a lot for pointing it out! (Except you seem to confuse `scanl1 (+)` with `scanl (+1)` — the latter would not typecheck.) The article is now fixed; here's the original (incorrect) version for the record: classify_prob :: Show s =&gt; (Bounded s, Enum s) =&gt; [Double] -&gt; (Natural, Natural -&gt; s) classify_prob probs = let beta = 2 -- arbitrary number &gt; 1 t = genericLength l l = concatMap (\(s, t) -&gt; replicate t s) . sortBy (comparing (Down . snd)) . zip [minBound..maxBound] . map round . scanl1 (+) $ map (/ minimum probs) probs g1 n = l `genericIndex` ((n-beta) `mod` t) in (beta, g1) 
Thanks for the quick response and fix :)
PureScript has a `_ { bar = baz }` syntax but Haskell already uses `_` in entirely incompatible ways.
I don't understand why the ordinary Functor laws are not sufficient to fuse the `fmap`s? Can anyone explain?
PureScript's use of the underscore in record creation/manipulation: https://github.com/purescript/documentation/blob/master/language/Syntax.md#records Record literals with wildcards can be used to create a function that produces the record instead: { foo: _, bar: _ } https://github.com/purescript/documentation/blob/master/language/Syntax.md#additional-forms-with-records _.propertyName This is equivalent to: \rec -&gt; rec.propertyName https://github.com/purescript/documentation/blob/master/language/Syntax.md#record-updates Wildcards can also be used in updates to produce a partially applied update: rec { foo = _ } This is equivalent to: \foo -&gt; rec { foo = foo } An underscore can also appear in the object position in an updater: _ { foo = 1 } This is equivalent to: \rec -&gt; rec { foo = 1 }
This is interesting! &gt; I imagine that there's a proof that any Decisive as you give is iso to `(a,_)` for some `a` Aha! I hypothesised this in [another comment](https://www.reddit.com/r/haskell/comments/6v1imc/name_for_splittable_functors/dly885n/). I didn't know of the result for `Distributive` and that lends nice support to the hypothesis. 
To answer your title, I assume it to be the start of the `lens` things. So I guess it bothered everybody. Now, with the lens package,it's less of a problem. It's also be proven that lens like solution are much more powerfull that just assignment function. However, it would be nice if somehow, lenses (or part of it) would be baked in and integrate smoothly with current accessor syntax.
This is a nice idea, although I think it may be superseded by lenses. Maybe you can make `{ bar = }` an infix operator so you can write quux { bar = } baz 
Can't `{ x = y}` just be generalized to a function ? So you could do as before x { bar = baz} but also `f = {bar = baz}` or even `{bar=}`. Of course it would be nice to be able to do `{foo.bar = baz}`. 
Disclaimer: Scala developer here I know a lot of people in this reddit are probably not a fan of Scala, but this guy is logically inconsistent/irrational when it comes to his critique for Scala. He mentions F# and Clojure but these languages get a free pass (they are even **worse** compared to Scala when it comes to mutability/side effects). Scala actually has very widely used libraries that promote purely functional programming (cats/scalaz), these kind of things are rare/don't even really exist in Clojure/OCaml ecosystem. In Clojure its harder to verify this due to their absence of a proper type system, and F# has a lot limitations on their type system (no HKT's) due to having to interopt with CLR Furthermore, Clojure has the exact same problems that Scala has when it comes to Java interopt, and by extension you can argue the same thing when it comes to Haskell with C interopt (granted its a lot less common). This kind of bashing of Scala in particular makes it very easy for one to be suspicious of his motives for doing so.
His argument also logically inconsistent. He bashes Scala (which is actually better than Clojure/F#/OCaml when it comes to being a typed functional language, although its not as pure Haskell).
Disclaimer: Scala programmer &gt; You think that's bad. You must not know the pain of your code not compiling with all sorts of weird type errors because you forgot to import some implicit conversion object. Or the dreadfully unhelpful "implicit not found" message when debugging typeclass instances. Haskell has the same problem in different areas, you can easily get very weird cryptic error messages especially when you use a lot of the GHC extensions. This is just cherry picking an example to suit your argument (in fact, many strongly typed static languages tend to have cryptic error messages in weird off cases) &gt; Also: state. It gets woven in in the smallest but most subtle ways. You want to be completely pure but Scala's support for tail recursion starts being wonky just when you need it in the tougher cases so you have no choice but to be imperative. This is due to JVM, and any other language on the JVM has the same issues. scala-native doesn't have any such limitations, it has proper full support for TCO.
[Algebraic effects for Functional Programming](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/algeff-tr-2016-v3.pdf) (previously: [Do Be Do Be Do](https://arxiv.org/pdf/1611.09259.pdf)) Maybe I will finally grok all this effecty continuationey coroutiney stuff one of these days.
Laws are not enforced in Haskell, so from the point of view of the compiler it is not correct to optimize `fmap f . fmap g` to `fmap (f . g)` in general. In practice, there are cases where type class laws don't hold structurally (i.e., in the strict sense that GHC understands), but they still hold at a weaker semantic level (e.g., QuickCheck's `Gen` is structurally not a monad, but it is if you think of it as a probability distribution). You can still use rewrite rules to perform fusion, but that is a syntactic transformation only: `fmap f . fmap g` has to appear in that form at some point during compilation for a rule to rewrite it to `fmap (f . g)`. In a situation where `fmap f` and `fmap g` are applied at two distant locations, then that optimization does not apply, so if your functor is a large tree, you still end up traversing it twice. I would compare the choice between `Coyoneda f` and `f` to the choice between `[]` and `Vector`, it's a matter of choosing the right data structure for the job. For repeated random access, use `Vector`; for repeated`fmap`, use `Coyoneda`.
You would have to teach the compiler about them (via e.g rewrite rules, which are very, very dependent on inlining happening enough but not too much, so that the compiler sees exactly the pattern you want to replace in the rewrite rule), whereas just switching to Coyoneda and back around your (`fmap`) transformations fuses them, all the time and all of them, because of Coyoneda's `Functor` instance.
* Putting the cursor on a `Maybe` and doing `SPC m g` or `gd` just goes to the import at the top of the file. * `Ctrl ]` opens some other tags window that also doesn't do what I want. * `SPC m h i` opens some kind of info which is almost what I want: https://i.imgur.com/tFCa2hw.png How can I get the IntelliJ behavior in Spacemacs?
* Putting the cursor on a `Maybe` and doing `SPC m g` or `gd` just goes to the import at the top of the file. * `Ctrl ]` opens some other tags window that also doesn't do what I want. * `SPC m h i` opens some kind of info which is almost what I want: https://i.imgur.com/tFCa2hw.png How can I get the IntelliJ behavior in Spacemacs?
If it is coded badly sure, but all of this (memory management/cache layouts) are completely in control of the language. In any case, this argument is really stretching it. Sure if you do a trivial `map` over a list then it will compile down to efficient native code, but this only happens if you write your code in a certain trivial way so the Haskell compiler can pick it up and optimize it. Once you have more complicated data structures and business flows, a properly tuned C++ implementation will blow Haskell out of the water. Haskell also has the problem where its impossible to avoid boxing in certain areas, and persuasive use of features such as HKT have to introduce boxing to actually work. Being lazy by default ironically make it much harder for the compiler to optimize what is going on, because its very hard for a compiler to figure out when and how certain code segments are being run. Note that I am not saying Haskell is slow (opposite, for being such a high level language its very fast) but lets call a spade a spade. A properly coded C++ implementation is going to be much faster than idiomatic Haskell code. Of course for this reason, C++ is a lot more complicated and error prone. EDIT: Furthermore, in C++ you have libraries which provide these `map` combinators over collections. Although they are definitely not as powerful as they are in Haskell, in modern C++ code you don't really have this manual iteration. This is doubly so for languages like Rust (which are imperative by default) with their Iterators
&gt; High-level C++ code features pointer chasing, cache unfriendly layouts and non-deterministic memory management (e.g. smart pointers) [..] Haskell is actually much better at optimising such code. Do you have some pointer about theses issues in C++ and the comparison with Haskell / GHC? What you describe is exactly the issues I see *in Haskell* when it comes to performance. Haskell data structures are full of indirections, leading to unfriendly layout. The GC helps a lot by allocating in the nursery, so we got a bit of cache coherency, but values are so big, due to the info pointers and all the indirections, than they span across many cache lines. (I confess that I don't fully understand what happen when using strict fields and `UNPACK`, so the issue may be less dramatic that what I think). Regarding the non-deterministic memory management, in C++ you don't need to wait a GC pass to get your memory back, and actually it is easier in C++ to control the allocation in "high performance" algorithms. Most of the algorithm I'm using in C++ do not allocate, or allocate in memory pool which can be freed in `O(1)` at the end of the process, so allocation performance and determinism is not an issue at all.
Yeah I don't see how you can say that GHC doesn't produce cryptic error messages, especially when you use various GHC extensions (which is very common practice, particularly in libraries) you can get error messages which leave you scratching your head for hours. The only strongly typed languages which have put a lot of effort in producing nice error messages afaik are Elm, Purescript and Dotty (new Scala compiler currently being written)
&gt; Tell me, what do you think happens when you have code like val (a,b,c) = if (true) "bar" else Some(10) (lifted from &lt;www.lihaoyi.com&gt; on his Scala warts post)? If you answered "it compiles and fails at runtime on a 'MatchError', you guessed correctly. This is being removed in Dotty (Scala compiler rewrite) as are a lot of other warts
I think I have to check out that Dotty. Elms error messages are great (and I think F# is getting there too) - but of course it's a lot easier to produce having a much less powerful type system
`Yoneda` can be stricter, which makes it worse in some cases.
See https://github.com/lampepfl/dotty/issues/1589 for an issue regarding better error messages. There is actually a huge community effort into making sure the experience is nice for end users in this regard
I think somewhere in this thread we lost track of what we are discussing, which is whether it's reasonable to claim that Haskell is as fast as C++ *on a good day*. That's a very fuzzy claim but /u/willtim provides some reasonable evidence that it might be true, for fairly weak interpretations.
I think it's fair to say that FP Complete have a vested interest in propping Haskell up as much as they can. As functional languages go, Scala is very popular so it makes sense they would want to place considerable effort in to impressing upon listeners that Haskell is far superior to Scala, and that even if you don't use Haskell, avoid Scala if you can. I don't think they're being _that_ underhanded here given that you would hope the people listening have critically viewed the talk in the same way that we are. This guy is essentially in the business of language sales (and they specifically sell Haskell consultation). They're using what I would guess (I am in no way a salesman) to be typical sales strategies.
Happy to see vegito mentioned, thanks for the ping. I've played on-and-off with the idea of creating a standalone, stream fusion based library. My most recent start on this was called [foreach](https://github.com/snoyberg/foreach) (to try and distinguish it from some variant on the names streaming and conduit, or using a silly name like vegito). If you're interestng in pushing through with a stream fusion-focused library, either based on my work or on something new, please do let me know, I'm very interested in it, but don't have the time right now to focus on it.
Well for one thing, I wouldn't claim "as fast as". More accurate would be "as fast as within a factor of 2 or 3" (which is still very fast) Secondly, the kind of code that would get optimized so well isn't the kind of code which is typical to find in Haskell. People don't code programs which are hello worlds or do trivial maps over a list. So yes, you can argue that "on a good day". But this good day would probably happen once a blue moon. In conclusion I think its a non point because in any case, if you are writing code in Haskell thats going to be fast as C++, its probably going to be as error prone because you are going to be dealing with memory management. And if you are writing completely trivial Haskell programs which do happen to optimize well, you can argue you can get similar results in other languages without being exposed to as many correctness issues (since your program is trivial in the first place)
Sure, I have my suspicions that he is bashing Scala because its direct competition to his business (if you want to make good money nowadays, and actually want to have a job, consultancy in Scala is the way to go). Clojure/F#/OCaml aren't as popular in this regard so there is less risk in "propping them up". &gt; I don't think they're being that underhanded here given that you would hope the people listening have critically viewed the talk in the same way that we are. Personally I think they are being underhanded, but for the same reasons that sales people deliberately are misleading. 
Heh yeah, my app is so simple it seemed like good idea at the beginning. And hey - the tests are so fast and I don't need to mock anything :) I've workarounded the issue anyway :)
Has something gone wrong with your formatting?
To go slightly off-topic: what _really_ grinds my gears is our collective insistence on trying to use data types as records. The `TraditionalRecordSyntax` extension (which is enabled by default) allows us to shoehorn the record syntax on top of data types. Nowadays we think that _this is the way it was meant to be all along_, and spend an inordinate amount of time scotch-taping an ugly hack. What we need is a totally new construct, distinct from `data`, with maybe an unsafe (partial functions) shim to move from `data` to `record`. It can be extensible, row-polymorphic, and support a decent projection and update syntax. The first step is to stop trying to use algebraic data types as records...
[Idris](https://www.idris-lang.org/) has `record { a-&gt;b-&gt;c = val }` syntax for nested record updates. And such updates are also first-class objects. You can read more in [documentation](http://docs.idris-lang.org/en/latest/tutorial/typesfuns.html#records). 
&gt; You can still use rewrite rules to perform fusion, but that is a syntactic transformation only: fmap f . fmap g has to appear in that form at some point during compilation for a rule to rewrite it to fmap (f . g). In the blog post, you explicitly have `fmap (^2) . fmap (+1) . fmap (*2)` which is why I think you'd indeed expect them to get fused. I take it `base` doesn't declare any rewrite rules for `fmap`?
I think you need to write this up as a blog post ...
fixed.
&gt; I learned Haskell in 2 days Just in case he comes over as arrogant: I met him in person and he is very down-to-earth and willing to explain everything, really nice person! (Not that I assumed that /u/bss03 implicated that Alexander is showing off here, but some users might get that impression.)
Edit: I forgot that the post uses `Tree` specifically. There are no rules for polymorphic `fmap`, so fusion would rely on `transform` being specialized at the right functors (the handwritten `Tree` with no rules not being one). There are fusion rules for lists, and the other common functors in base seem simple enough that regular inlining and optimizations just work.
Why does `ArrayArray#` exist? My impression is that `ArrayArray#` is the same object as `Array#`, and every `ArrayArray#` primop and use case can be implemented with (a newtype of) `Array# Any`. Plus, there is no `SmallArrayArray#`, so most of the time it's in fact preferable to use `SmallArray# Any` instead of `ArrayArray#`.
It could be taken as "I'm so smart..." or "the language is so easy...". I watched that part of the video and it was the latter actually. His point was "yea, you might not understand Monads or how to make them but you won't need to for a while anyway" which is exactly how I think people should approach Haskell. A lot of programmers could get a long way (and some will never need more than) just taking it to be an interface and don't think beyond that.
The argument seems to be "of the popular ones, use this". He mentioned OCaml (which is what I'd expect to use if not Haskell) but said the community didn't pick it. He had a similar critique of F#. So I don't think it's that he's neglecting types so much as assuming Haskell is the only relevant one *with* types.
stack + Sublime text 3 + SublimeHaskell plugin. Does not work really well with stack (I haven't tried to get it to work though), but still works pretty well.
He discussed DBs at the end: they consider this outside of their "device" concept. DBs go in the same place users do.
&gt; I do it for a living and there's a lots of issues but Haskell could learn a lot from Scala's emphasis on easy interop. Please no. I'd rather live with the pain of missing key library for a bit then wreck the syntax and make critical things impossible to do just so we can use some (very likely poor, at least relatively) libraries that eventually won't be needed anyway if we rewrite them.
I fully agree with the intention to switch from the duality of boxed/unboxed to the trinity of boxed/unboxed/unlifted, the latter being the actually truthful categorization of runtime representations. I think one thing which would play along nicely with containers of unlifted data would be array and text libraries *not* based on slices. Currently, `Text`, `Vector` and `Data.Array` are all slices, yielding eye-watering overheads in many cases. Currently, a singleton `Vector` of a single-character `Text` takes up 16 words, and to get to the character I need to dereference three or four pointers (depending on whether I start from an unpacked `Vector`). If we switch to an unlifted small array of an unlifted packed bytearray, it goes down to 6 words and two indirections. If we don't have unlifted arrays, then slicing is more acceptable: why not just add slice data if we need to box things anyway? But in an unlifted-aware world, baked-in slices are just wasteful. Instead, slices should be wrappers around arrays, and common operations can be abstracted over with usual typeclass machinery. Baked-in slices are also non-compositional in the sense that the overhead precludes their use in data structures, so e. g. `unordered-containers` can't reuse `Data.Vector` and needs to roll its own primitive insert/delete operations on primitive arrays. Hypothetically, if we intend to use minimum-overhead data structures, then pretty much *every* container would need to specialize for all three representations, including associative data structures (where key and value specialization yields *nine* different versions!) and lists and tuples; unfortunately that's rather unwieldy with current GHC's inability to abstract over representation. However, I don't know if unifying the runtime treatment of boxed and unlifted types is feasible. It seems feasible enough to me, GHC would need to just not do anything when trying to force an unlifted value, instead of throwing an RTS exception as it is currently done. Then, `Array# a` would have kind `*`. Maybe a GHC expert can comment on this?
`Array# a` can only hold values of type `a`, where `a` is a type with kind `TYPE LiftedRep`. `ArrayArray#` can only hold values whose type has kind `TYPE UnliftedRep`. In other words you cannot do this: foo :: Array# (Array# Int) -- does not typecheck Using `Array# Any` to try to sidestep this will just give you a segfault at runtime. The GHC runtime expects that values inside an `Array#` are lifted. Notice in the docs for [`unsafeCoerce#`](http://hackage.haskell.org/package/ghc-prim-0.5.1.0/docs/GHC-Prim.html#v:unsafeCoerce-35-) that it only ever talks about casting **lifted** types to and from `Any`. So, you should not cast `Any` to or from `Array#`. `ArrayArray#` exists so that we can put things with an **unlifted** representation in an array. Unfortunately, prior to the levity polymorphism improvements that were merged into GHC 8.0, there was no way to talk about a type variable that represented a type with a non-lifted representation. This is why `ArrayArray#` doesn't take a type variable argument. It should (just like `Array#` does), but our type system wasn't expressive enough until somewhat recently. So, here is what the ideal interface for `ArrayArray#` would look like: data ArrayArray# (a :: TYPE UnliftedRep) data MutableArrayArray# s (a :: TYPE UnliftedRep) writeArrayArray# :: forall s (a :: TYPE UnliftedRep). MutableArrayArray# s a -&gt; Int# -&gt; a -&gt; State# s -&gt; State# s readArrayArray# :: forall s (a :: TYPE UnliftedRep). MutableArrayArray# s a -&gt; Int# -&gt; State# s -&gt; (#State# s, a #) ... This could eventually be done, but the current definition of `ArrayArray#` and all of its related functions would need to be deprecated. The new interface would have to be introduced, and we'd have to sit tight with both interfaces in `ghc-prim` for three release cycles. In other words, it would take a while to completely oust the existing semi-typed `ArrayArray#`. Related things to read: - https://www.microsoft.com/en-us/research/publication/levity-polymorphism/ - https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#unboxed-types-and-primitive-operations
I agree, although I would be careful to say that anything is easy, it could turn out demotivating to people who feel they are not making enough process. About the later part I am with you... Beginner should not need to learn about FP design patterns right from the start for the same reasons that OO design patterns are not taught right from the start. I feel that you one can get further without design patterns in Java than in Haskell, but it is okay to be a bit handweavy at the beginning for sure. That is one fundamental truth of teaching: You will lie anyway, just be clear about when and how you are lying.
&gt; I would compare the choice between Coyoneda f and f to the choice between [] and Vector, it's a matter of choosing the right data structure for the job But this falls apart for when I'm providing a data type in a library - how do I know what my users are going to do with it? Vector at least has some trade offs, but does using Coyoneda?
&gt; it could turn out demotivating to people who feel they are not making enough process. It's probably critical to point out that the speaker in the video was talking about from within the confines of a haskell team. If you're doing something on your own that's a completely different world. If you're working on a team, I don't really see why someone couldn't go their whole career without ever understanding e.g. a Monad. Such a person would never be a Senior developer on the team but there are plenty of people in the industry who do beginner/intermediate level programming until the point that they move into some non-programming role. Not everyone loves this stuff as I (and probably you) do and we don't need everyone to to be able to use it.
I agree that this might solve it, and that it is surprisingly simple in terms of API. If only we were programming python where you can have named parameters with default values…
Seconded... sounds like the root of the problem to me. Even just for the sake of ditching partial record selectors
&gt; It's probably critical to point out that the speaker in the video was talking about from within the confines of a haskell team. If you're doing something on your own that's a completely different world. Even then - the fact that one is earning a living with programming does not make one immune from frustration. The less frustration, the better. &gt; If you're working on a team, I don't really see why someone couldn't go their whole career without ever understanding e.g. a Monad. Such a person would never be a Senior developer on the team but there are plenty of people in the industry who do beginner/intermediate level programming until the point that they move into some non-programming role. Not everyone loves this stuff as I (and probably you) do and we don't need everyone to to be able to use it. I think it is unrealistic to assume that one could or should make living as a Haskell programmer without understanding the usefull abstractions. Working together with someone like that will cause frictions, because he will not understand the abstractions of his coworkers, and his coworkers will constantly wonder why he keeps duplicating code over and over. I think that one get far in Haskell without a deeper understanding of FP design patterns, but not far enough to be an professional coder. Disclaimer: I only know FP from academia, my working experience has only been within OO languages.
&gt;I think it is unrealistic to assume that one could or should make living as a Haskell programmer without understanding the usefull abstractions. As a working programmer for nearly 2 decades: it happens in every other language, so once Haskell gets used enough it's bound to happen here too. But I suppose it's not that relevant of a point even if true, so nevermind all that.
&gt; If we switch to an unlifted small array of an unlifted packed bytearray, it goes down to 6 words and a single indirection It seems like there are two indirections in this case. Regardless, I like your thoughts on the "sliceless" array approach. This is basically what the `primitive` library offers, although it's missing a bunch of useful combinators (things like map, mapM, or scan over a byte array would be nice). The container specialization problem is indeed a pain. It can be done with backpack, although obviously what you end up with would be slightly less pretty than it would if we were able to use GHC's normal mechanisms for dealing with polymorphism. Edward Yang has been [working on making backpack deal with levity polymorphic code](https://phabricator.haskell.org/D3825), resolving [#13955](https://ghc.haskell.org/trac/ghc/ticket/13955). Edward hasn't made any kind of commitment about when it will be ready, but I suspect that it would land in GHC 8.2.2. This would let you sidestep the problem of "unifying the runtime treatment of boxed and unlifted types" that you talked about in your last paragraph. Oh, and you'd have to rewrite the whole `containers` library to make this work ;)
&gt; I hope someone is working on this. lol this is the attitude everyone has about haskell ide's, it seems
Please bear with me as I try to understand this sentence. Let's see, what's [Yoneda](https://hackage.haskell.org/package/category-extras-0.52.1/docs/Control-Functor-Yoneda.html#t:Yoneda) again? newtype Yoneda f a = Yoneda { runYoneda :: forall x. (a -&gt; x) -&gt; f x } From the types, I'd say it's a convoluted way to represent an `fa :: f a`, by wrapping it as `Yoneda $ \k -&gt; fmap k fa`. Okay. "`Yoneda` also provides this benefit". That would mean that `fmap g $ fmap h $ Yoneda $ \k -&gt; fmap k fa` will also combine the `g` and the `h` together before applying the `fmap`, instead of simply applying two separate `fmap`s to the underlying `fa`. So that double-fmap expression should expand to `Yoneda $ \k -&gt; fmap (k . g . h) fa`, not `Yoneda $ \k -&gt; fmap k $ fmap g $ fmap h fa`. So the Functor instance for `Yoneda f` can't just delegate to `f`, like this: instance Functor f =&gt; Functor (Yoneda f) where fmap f (Yoneda fmapkfa) = Yoneda $ \k -&gt; fmap f $ fmapkfa id Instead it has to temporarily forget that the `f` it contains is a functor, and implement `fmap` by modifying `k`: instance Funcor (Yoneda f) where fmap f (Yoneda fmapkfa) = Yoneda $ \k -&gt; fmapkfa (k . f) All right, I agree, Yoneda also combines the functions given to successive `fmap` calls into a single function, which "at the end" is applied to the initial `fa` in a single call to `f`'s `fmap`. "At the end" means "when the `k` is finally provided, usually `id`". Okay. "but more often needs an actual Functor". Well I now know that [Coyoneda is the free Functor](https://www.reddit.com/r/haskelltil/comments/4ea7er/coyoneda_is_just_the_free_functor/), meaning it adds the ability to `fmap` to an `f` which isn't necessarily a Functor on its own. So of course Coyoneda doesn't require `f` to be a Functor. And since Yoneda wraps a `\k -&gt; fmap k fa`, clearly Yoneda does require `f` to be a Functor. Okay. The sentence doesn't say "needs an actual Functor", though, but rather "*more often* needs an actual Functor". So is it possible for Yoneda to hold an expression other than `\k -&gt; fmap k fa`, one which doesn't use `fmap`? Ah! `fmap :: (a -&gt; b) -&gt; f a -&gt; f b` works on *all* `a`s and all `b`s, whereas the expression only needs to work for all `b`s. So if we had a GADT where only some constructors had restricted types, for example data PossiblyBool a where DefinitelyBool :: Bool -&gt; PossiblyBool Bool ProbablyNotBool :: a -&gt; PossiblyBool a Then I could not implement a Functor instance for PossiblyBool because `fmap id (DefinitelyBool True)` must be `DefinitelyBool True` but `fmap show (DefinitelyBool)` can't be `DefinitelyBool "True"`. I can, however, write a restricted `fmap`-like function which only works on the `ProbablyNotBool` constructor: fmapProbablyNotBool :: (a -&gt; b) -&gt; {-PossiblyBool-} a -&gt; PossiblyBool a fmapProbablyNotBool f ({-ProbablyNotBool-} a) = ProbablyNotBool (f a) And then I can wrap it in a Yoneda as `Yoneda $ \k -&gt; fmapProbablyNotBool k 42`, and this wrapped version does have a Functor instance and thus can be used with any reusable code which expects a Functor. Okay. So "more often" here would mean that there are circumstances in which the above isn't good enough? Maybe the idea is that there is an existing codebase which needs to be optimized, and the code traverses the same data-structure many times but only to eventually apply a function on the leaves. To optimize those multiple traversals into one, sprinkle a bit of Coyoneda, replacing each traversal by a `fmap` on this Coyoneda, and add a post-processing step which traverses the data-structure once, applying the function inside Coyoneda at the leaves. Works every time. Alternatively, sprinking a bit of Yoneda might also work, and might perhaps optimize even better because Yoneda is a newtype? But this alternative technique is perhaps applicable in fewer circumstances, because not all traversals can be transformed into a `fmap` on the Yoneda. In this case a traversal which leaves the `DefinitelyBool` constructor alone could be wrapped, but not one which modifies the leaves of both constructors.
It would also be cool to see if Backpack can express this interface ;) (We might need the levity polymorphism for Backpack patch https://phabricator.haskell.org/D3825)
&gt; Using Array# Any to try to sidestep this will just give you a segfault at runtime. It works fine. I've been using it since GHC 7.10 in a number of places (e. g. [here](https://github.com/AndrasKovacs/trie-vector)). Obviously, we can't generally store unlifted data in `Any`, we only use it to plug *something* in `Array#` - that something could be any other boxed type, as we `unsafeCoerce#` the primops anyway. The primop implementations for `Array#` and `ArrayArray#` are exactly the same (except `newArrayArray#`). We get a problem when we try to force an unlifted value, in that case we get an RTS exception (not segfault) like "internal error: ARR_PTRS_FROZEN object entered!". Obviously, we don't want this to happen, but I have found that it's not difficult to avoid. 
What's taking up most of the compilation time then? Optimization?
&gt; It seems like there are two indirections in this case. That's right, edited.
Well perhaps a more compelling example would be this not-`fmap`: notFmap :: (a -&gt; b) -&gt; PossiblyBool a -&gt; PossiblyBool b notFmap f (DefinitelyBool b) = ProbablyNotBool (f b) notFmap f (ProbablyNotBool a) = ProbablyNotBool (f a) liftPossiblyBool :: PossiblyBool a -&gt; Yoneda PossiblyBool a liftPossiblyBool a = Yoneda $ \k -&gt; notFmap k a But yea, I do believe `Yoneda` tends to inline better than `Coyoneda`, but I have no concrete idea why that's true. I think being a newtype around a lambda just usually makes it easier for the simplifier to put it in context and simplify it.
Backpack is overkill for usecase like this, people may want to use `PrimArray` and `Array` together in the same module, all we want is unified `newArr` or `read/writeArr` interface. Not a fixed data type with cabal configuration.
I kind of like the PureScript approach. Solves the problems with using records in ADTs. The immediate problem we'd face would be about having to pattern match to get the record out of a data type, but I think whatever syntax gets invented for accessing/setting a record could also be allowed to work on any direct `newtype` of a record. I am frequently a fan of `NamedFieldPuns`, so something to enable matching like that would also be nice. Also, the *biggest* problem I see is that representing all your records as dictionaries is insanely slow unless you have a JIT or something. So we would have to actually make records have a normal form with constant time indexing, *even when code is polymorphic over the (extensible) record type*. That last part is hard because it means every function that takes an extensible record has to be made specializable, and since it's totally possible for different modules to duplicate work on specializing common functions, we could see a major compiler performance problem. The final problem is that PureScript's extensible `#` kind is black magic hackery. Maybe this is fine/necessary. But I'd like to see something more principled for a feature this fundamental, preferably without leaving the kind `Type`.
&gt; It is very safe to say any programming language is good, if we compare it to PHP. I'd agree with that with one small amendment: "any _in use_ programming language". I would rather write PHP than Intercal.
I don't like the idea of adding an entirely new construct just for records. The only way this would be sensible to me is if we called it `codata` and made `data` strict by default, so there'd actually be an effective difference between them, but I think codata is a separate issue and needs more thought than that. Row polymorphism is already handled (conceptually) by typeclasses, even if not effectively in practice. So I'd prefer a solution that improves the typeclass machinery we already have, rather than adding a hacky special case to the type system like purescript does. If we're going to get row types I'd like to do it in a unified way that also gives us sum subtyping (polymorphic variants) I think so far the best solution is to replace record update syntax with autogenerated lenses, since its: 1. Easy to implement 2. Easy to understand 3. Doesn't change the type system, data layout, or add syntax The major problem with deriving lenses everywhere would be compile time regression, though if it's built into the compiler I could imagine better performance than the existing TH implementations
Yeah, that's exactly why `ArrayArray#` exist while the underlined closure type is same with `Array#`: We don't want to enter the ulifted types closure GHC provides, e.g. `MutVar#`, `ByteArray#`, etc. You can think that `ArrayArray#` is a limited version of `Array#` that only store unlifted type so that we never have to enter them, thus we can avoid the error above. If you come up a work-around to abuse `Array#` for this purpose, I'd say it would very similar to create an `ArrayArray#` type, isn't it? 
&gt; If you come up a work-around to abuse Array# for this purpose, I'd say it would very similar to create an ArrayArray# type, isn't it? My point is that `Array#` and `SmallArray#` abuse works for this purpose, and since `ArrayArray#` does not have a `Small` version, it makes sense to use `SmallArray#` instead.
OK, in that's the case, maybe a `SmallArrayArray#` should be added.
Why? Quite a few of us have been down in the trenches of Scala. We all know and agree on what is down there. Where've you been?
&gt; get error messages which leave you scratching your head for hours. Show me.
It gets worse.
None. Zero.
What really bothers me is that SublimeHaskell + stack can not launch `stack test`. You can remedy this by writing a custom build system. However, if you want to use external tools that SublimeHaskell does not know about (like `weeder`) or building dependency trees (with `stack`) you will be writing these systems anyway.
Yes. Many who have been down in the trenches of Scala, would prefer to write JavaScript.
&gt; This kind of bashing of Scala in particular makes it very easy for one to be suspicious of his motives for doing so. Because it's crap. That's why. Ask anyone who has used Scala to any degree of serious application.
Four years of it pricking my skin with sewing needles. Each prick isn't much, but in aggregate... I've worked with Play and Akka, and done large Spark apps tied with the Hadoop ecosystem, AWS integration, you name it.
These days any code is easily accessible as a (micro)service. Haskell already has a mature ecosystem that covers modern general programming: web servers and frameworks, database connections, file handling routines, email, encryption, cloud interfaces (amazonka etc). I simply do not find myself reaching for a functionality in libraries written in other languages when coding in haskell nowadays. 
I wrote Scalaz. Sorry for any prickles.
Here is an example https://www.reddit.com/r/haskell/comments/54l3ug/readable_error_messages_like_in_elm/ Googling is quite helpful here. tl;dr Both languages have confusing error messages in certain circumstances, welcome to strong static typing
I've been writing Haskell for 15 years. When did I spend hours on an error message? What was the occasion?
Cryptic errors are confusing for new people, not people who understand the language because they have been using it comprehensively. Also this comment reeks of confirmation bias, its easy for me to claim that "I have been writing Scala for 7 years and I didn't spend hours on an error message."
I've been writing Haskell and Scala longer than you. So have hundreds of my colleagues. None have experienced "scratching your head for hours" at a Haskell error message. Show me.
The point of programming in Haskell is to write programs in Haskell. I enjoy writing programs in Haskell, and I find the programs I write to be useful, and so do my colleagues. I did not need to gain an understanding of why the monad laws dictate that monads form a category, or why having type-classes that form a category is good. I did not need to learn what all these greek symbols in all these papers mean. I did not need to learn higher order math or attend university lectures. I only needed to write some code, and watch the scenario play out, and note what code compiled and what code did not compile. I still can't write you a blog post about category theory, and I only sort of understand -why- it breaks things to violate the monad laws, but I can write perfectly functional code that does what I want. Not everyone learns the same way. I could read 100,000 more papers about category theory and spend a lifetime attending lectures on this subject, and I would probably still have no idea what any of it meant. But if you show me a machine written in code, and explain just a bit of it to me, I will take it apart and reassemble it over and over again until I understand how to replicate its function in any given context. If you show me a dozen machines written in the same code, I will then come to understand how to make my own from scratch. At no point will I likely be able to connect the dots between the code I'm writing and the paper someone published on the subject, but my code will look a lot like their code, and the program will do the same thing.
As someone who has never used reflex, TIL that reflex can be used outside of GHCJS. Thanks!
I can make that file a literate haskell file. What kind of ramp up do you specifically need? SDL, Reflex or FRP in general? I can at least add links to other articles as an introduction. This has come up a number of times when I write about FRP so I'm trying to do better.
You're very welcome :) Glad this gave you some new context!
Equivalently, sendDoerMessage :: Int -&gt; Redis () receiveDoerMessage :: Redis Int ;)
Eh, I don't really care about IDEs. I write my Java, C, and Haskell code all using the same tool: vim. I've experimented with some plugins, but none have proven essential.
I personally enjoy writing small command-line utilities that I miss in my daily life. `optparse-applicative` is a great library to have in your toolset for any occasion!
Elm's `.field` syntax will never fly in Haskell at this point. `.` already has way too many meanings (not the least of which is function composition). However, using lenses and `&amp;` (flipped `$`) you can recreate something that *feels* the same (even though it's not quite as easy on the eyes).
Yet another reason to use lenses...
You mean like Rust's `struct` vs `enum`? I got really excited for a second as I thought that `NoTraditionalRecordSyntax` might disallow records on sum types. Unfortunately it disallows _all_ records. I wish we had an extension to disallow only partial records. 
As my real codebase now stands, that would result in 220 different functions. 😕 And not all services are implemented yet. This way there isn't a proliferation of functions... 😊 Also, there is another level to the API. Most payloads are wrapped in messages, some are naked (such as simple pings), and there are a few queues which are synchronous, and so the wrapper needs to specify the unique response queue (and other data). So we have another type family: type family AssociatedWrapping (a :: AppId) (b :: QueueType) = (r :: PayloadWrapping) data PayloadWrapping = Naked | InMessage | InMessageExpectingResponse type family WithWrapping (w :: PayloadWrapping) (v :: Type) :: Type where WithWrapping 'Naked a = a WithWrapping 'InMessage a = QMessage a WithWrapping 'InMessageExpectingResponse a = QMessageExpectingResponse a receiveMessage :: forall (a :: AppIdentifier) (b :: QueueType) c t w . (KnownSymbol (AppIdSymbol a), KnownSymbol (QueueTypeSymbol b), c ~ AssociatedQueueType a b, t ~ WithWrapping w c, w ~ AssociatedWrapping a b, Store t) =&gt; Redis.Redis t It gets hairy, but if you screw up then GHC will tell you. 😊 
In general, for any language, we can construct an associated category with types as objects and total (pure) functions as morphisms. But for many languages, that gives far less than for Haskell. In particular, as people have noted, in a side-effectful language, then most functions you write won't exist as morphisms in the category. Furthermore, without first class functions, tuples, etc. you don't get cartesian closedness, and soforth. So there is a "standard construction" to build such a category -- it just might be one which is so constricted compared to the base language that it doesn't let you do anything interesting with it. This means that the "puns" where we internalize categorical constructions into versions represented within the language itself don't really work the same way. But what you _can_ do is "desugar" that weird irritating effectful language into a nice functional or type theoretic semantics that underlies it, and construct the category arising from that. This is to say, to construct a categorical _semantics_ for the language. And that's what e.g. flexibeast's link in this thread does.
I'm glad to see that these issues are being addressed. This thread had freaked me out a little.
Thanks for your hard work, the "real fp" libs help take the edge off.
Can't tell if sarcastic?
I reckon that eventually in-place updates with linear types could find their way into this type class. Cool idea!
&gt; but does using Coyoneda? Yes. If you lower a coyoneda multiple times, you will duplicate work. lowerCoyoneda :: Functor f =&gt; Coyoneda f a -&gt; f a lowerCoyoneda (Coyoneda f a) = fmap f a foo :: Coyoneda MyFunctor Foo foo = ... bar :: Coyoneda MyFunctor Bar bar = fmap bigFunction foo foobar = let x = lowerYoneda bar y = lowerYoneda (f &lt;$&gt; bar) -- Ouch! in ... In `foobar`, the fact that `bar` is lowered twice means that `bigFunction` will be run twice instead of once. So dupping `Coyoneda` dups fmaps.
Yea most `fmap` instances on non-recursive data structures inline fairly well and don't need rewrite rules. Join points might allow some recursive `fmap`s to inline, but they'll probably never fuse.
I also disliked this order dependence, until I had a look at [Eisenberg's PhD thesis](https://github.com/goldfirere/thesis/blob/master/built/thesis.pdf) (the design for `-XDependentTypes`). The paper adds a bunch of new quantifiers, over and above the usual `-&gt;`, `forall .`, and `=&gt;`. `TypeApplication`s are used to make invisible quantifiees (introduced by `forall .`, `pi .`, and `=&gt;`) visible. Cool. But the point is, these non-visible quantifiers, are in the same "family" as the usual `-&gt;` quantifier -- they serve the same purpose. Now, we can write f :: a -&gt; b -&gt; c and understand that the argument of type `a` should be before the argument of type `b`. It _seems wrong_ that when we write f :: forall a b c . a -&gt; b -&gt; c the order in which the type parameters appear in the `forall` should matter, since it's a single `forall` -- a single lump. But then you find that `forall a b .` is **actually shorthand for** `forall a . forall b .` (this syntax does not work in current GHC, though). Each quantifier adds a new positional argument, and we're used to treating `forall a b .` as a blob, when it is actually shorthand for `forall a . forall b .`. So we have f :: forall a . forall b . forall c . a -&gt; b -&gt; c and it makes all the sense in the world that the type applications/visibility overrides should come in a specific order. 
The problem I have isn't with `forall a b c.` inducing a specific order on the type parameters -- you're listing them explicitly there, that's fine. The problem is with `a -&gt; b -&gt; c` resulting in a somewhat arbitrary implicit order on the type parameters, so that if you later want to add a forall, you have to be cautious to match the implicit order. I don't think explicit type application should be available at all for terms whose type wasn't provided with explicit foralls.
I guess I'm not sure in what sense it "narrows" as we move "up"?
I've tried the "haskero" and "haskelly" extensions in the past and they didn't work for me and were very slow (for developing Lamdu). But this one so far works great and is very fast!
&gt; Thus we use fromJust and fromRight. YOLO. =D Fun to see other things than vectors for dependent types, but I find it a bit scary with all the extensions and :: forall (a :: AppId) (b :: QueueType) c . (c ~ AssociatedQueueType a b, Store (QMessage c), KnownSymbol (AppIdSymbol a), KnownSymbol (QueueTypeSymbol b)) 's
~~If you derive a `Data` instance for `AST` and `Expr` you'll be able to write~~ manglePhase :: Data a =&gt; AST a -&gt; AST a manglePhase = transformBi (doStringLits :: Data b =&gt; Expr b -&gt; Expr b) ~~which should work.~~ ~~The idea is that `a` can be polymorphic, but uniplate can only work with it if it has a `Data` instance.~~ ~~edit: Maybe you need `Typeable` as well? I don't remember.~~ edit 2: ignore me, this is wrong. i should test before I reply.
How do you deal with backwards compatibility?
I do derive a `Data` and `Typeable` instance for every type of my AST. What you've described does not work however; I get the exact same error. Also, with the language extension ScopedTypeVariables, wouldn't you be able to reuse the a from the type declaration? Then you wouldn't need the `Data b` typeclass thing right? (Because I tried that as well and it still didn't work). {-# LANGUAGE ScopedTypeVariables #-} manglePhase :: Data a =&gt; AST a -&gt; AST a manglePhase = transformBi (doStringLits :: Expr a -&gt; Expr a) Still gives me the same error. EDIT: The actual code now looks like this: data CGInfo a = CGStrLit String | CGWhat a deriving (Show, Functor, Eq, Data, Typeable) annotateStrLits :: (SymShow a, Data a) =&gt; CAST (CGTok a) -&gt; CAST (CGTok a) annotateStrLits tree = transformBi (doStrLit :: (Data b) =&gt; Expr (CGTok b) -&gt; Expr (CGTok b)) tree doStrLit tree = case tree of StrLit (tok, tyInfo, _) -&gt; StrLit (tok, tyInfo, Just $ CGStrLit "a") x -&gt; x The error: • Could not deduce (Data b0) arising from a use of ‘transformBi’ from the context: (SymShow a, Data a) bound by the type signature for: annotateStrLits :: (SymShow a, Data a) =&gt; CAST (CGTok a) -&gt; CAST (CGTok a) at /home/bobe/pld/casm/src/C.hs:251:1-74 The type variable ‘b0’ is ambiguous These potential instances exist: instance Data DataType -- Defined in ‘syb-0.6:Data.Generics.Instances’ instance (Data a, Data b) =&gt; Data (Either a b) -- Defined in ‘Data.Data’ instance Data All -- Defined in ‘Data.Data’ ...plus 39 others ...plus 102 instances involving out-of-scope types (use -fprint-potential-instances to see them all) • In the expression: transformBi (doStrLit :: (Data b) =&gt; Expr (CGTok b) -&gt; Expr (CGTok b)) tree In an equation for ‘annotateStrLits’: annotateStrLits tree = transformBi (doStrLit :: (Data b) =&gt; Expr (CGTok b) -&gt; Expr (CGTok b)) tree 
Nope, just adding zeroes. Completely agree.
Your example code here isn't using scoped type variables; it's only enabling them. You need to explicitly use `forall` to get the effect of scoped type variables. {-# LANGUAGE ScopedTypeVariables #-} manglePhase :: forall a. Data a =&gt; AST a -&gt; AST a manglePhase = transformBi (doStringLits :: Expr a -&gt; Expr a)
What about `rec :{field}` Nestable as: `rec :{field }:{field2}` Where `:{field} :{field2}` reduces to an infix function with faux signature `fun :: (HasField a, FieldType b) =&gt; a-&gt;b -&gt;a` Obviously that's not a real type signature, but it should be a decidable question at compile time, I think? Obviously it's not as extensible as lenses, but it would cover the simple 'assignment' usecase
The type of `transformBi` doesn't allow for a polymorphic instantiation. In Haskell, type variables (in this case `a` and `b`) can only be instantiated with unquantified types. The result is that what you've written here will eventually result in an ambiguous type error arising from the use of `b` transformBi :: Biplate from to =&gt; (to -&gt; to) -&gt; from -&gt; from 
That did the trick! For the record, the code is as follows now: annotateStrLits :: forall a. (SymShow a, Data a) =&gt; CAST (CGTok a) -&gt; CAST (CGTok a) annotateStrLits tree = transformBi (doStrLit :: Expr (CGTok a) -&gt; Expr (CGTok a)) tree doStrLit tree = case tree of StrLit (tok, tyInfo, _) -&gt; StrLit (tok, tyInfo, Just $ CGStrLit "a") x -&gt; x Why is the forall needed? Can't haskell infer that `doStringLits`can work with a just fine? Or should I just read up on scopedtypevariables to understand this?
I tried very hard to explain where everything comes from, and _why_ you have to use those extensions. But please tell me if I screwed something up. In (nearly-)dependent Haskell we mix `Type`s with `Symbols` (strings), `Nat` (natural numbers), `Constraint`s, and our own data type constructors. We want to use data constructors "at the same level" as regular types (in type declarations), so we need to enable `DataKinds`, to convert data types into brand new kinds, and their constructors into new types that can't have any values. `TypeFamilies` are "type-level functions" (they're in general not actually functions, since [they aren't injective by default](https://ghc.haskell.org/trac/ghc/wiki/InjectiveTypeFamilies)). They map combinations of types -- from any kind -- to other types. We need to tell GHC what the type constructors are, at call sites. Since only types in kind `Type` have values, the only information we can give GHC has to be to tell it what the type constructors are (for regular types of kind `Type`, which can have values, GHC can figure out the types by looking at the type of the parameter values given when the function is called). To do this, we can either use the olde-fashioned [`Proxy` method](https://stackoverflow.com/questions/22116363/what-is-the-purpose-of-data-proxy) (which took me quite a while to grok, I must admit), or the "new" (it's always existed in GHC Core), fancy `TypeApplications` extension. We also need the _caller_ to tell GHC what the types are, and since we don't use many of the type variables to the right of the final `=&gt;`, we have to enable `AllowAmbiuousTypes` to assure GHC that we will tell it what those types are later on (using `TypeApplications`). In order to use `TypeApplications`, we need to have explicit type signatures so we can tell GHC the order of our quantifiees (otherwise GHC will complain). In order for GHC to not assume that all our type variables are of kind `Type`, we need to specify their kinds using the `forall` syntax, otherwise it tells us `Couldn't match kind ‘*’ with ‘AppId’` at call sites. I thought that maybe [`PolyKinds`](https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/glasgow_exts.html#kind-polymorphism-and-type-in-type) would alleviate this restriction, but turning it on doesn't seem to make a difference. We also need the `forall` syntax so that we can [use the same type variables in the type and in the function body](https://ocharles.org.uk/blog/guest-posts/2014-12-20-scoped-type-variables.html). We can direct GHC a bit in its type inference by [telling it that certain types should be equal after inference](https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/glasgow_exts.html#equality-constraints), so we use the `~` syntax in our constraints. My very shallow understanding of PROLOG allows me to have a strong intuition for "unification" (constraint solving), but I imagine it won't be very difficult for someone without the same background to understand the idea of trying all combinations until you get a (single) match on both sides. See? All the pieces fit together, once you understand _why_ you would need a certain feature. Most times GHC is also quite helpful in suggesting extensions you should enable to make your program typecheck (but don't follow its advice when it suggests `UndecidableInstances` and `IncoherentInstances` -- it usually means you fucked something up in your design). Beyond that it just requires _trying it out for yourself_, which is how I discovered this solution after a few days' worth of blind typing. Lately I've been reading [Eisenberg's PhD thesis](https://github.com/goldfirere/thesis/tree/master/built) (he was instrumental in bringing almost all of these extensions to fruition), particularly the 2nd, 3rd, and 4th chapters (the only ones I seem to be able to grasp without first learning type theory), and also trying to figure out [what the point of `TypeInType` really is](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell/Phase1). Learning is hard, I'm hoping that this blog post can make it just a little bit easier for other people who are just as confused about Haskell type-level gymnastics as I am. 😊
Maybe... it wil come back. :P
right. sorry. my mistake.
You are right, I was thinking of simple function application, but it's the over way around .
I've spit my project into different package (putting the Yesod model responsible of 99% of the template haskell) and it seems to make a major difference. The first being that ghci doesn't crash anymore :-) The trick with stack seems to set the sub package as extra dependency like this packages: - location: big-model extra-dep: true
I put the big TH file in a separate sub-package and it solves `Ran out of labels` project. I can launch `ghcid` now. I'll just have to see how it behaves in a emacs window.
Supplemental material referred to in paper: https://github.com/pa-ba/McCarthy-Painter
Yeah. It'd be nice if the record syntax gave you lenses instead of accessors. I'm surprised there isn't an extension for that, but I guess the TH w work around is easy enough that no one bothered.
&gt; I don't think explicit type application should be available at all for terms whose type wasn't provided with explicit foralls. I agreed.
&gt; The final problem is that PureScript's extensible # kind is black magic hackery. How so?
Shouldn't `Only` be a `data` type to match the laziness of tuples?
&gt; I tried very hard to explain where everything comes from, and why you have to use those extensions. But please tell me if I screwed something up. Not at all, and I appreciate the exposition! I guess I just wish there was less to explain in the first place in order to use this kind of thing ;-) 
I think any of the following would be acceptable: - call it an example instead of a tutorial - a paragraph explaining the naming scheme (assuming names are derived from the libraries this is built on) - links to prerequisite reading I think that covering FRP can be left as an exercise for the reader. I could also do some digging myself. I've seen some of your other stuff and was always impressed by the level of documentation, so found myself disappointed here. 
FYI: Your text is still mentioning the old number 14641.
You can write a parser which finds the longest symbol, and a parser which, given the length of the longest symbol, parses your format. Then use the `lookAhead` function to run the first parser without consuming any input, and use its result to run the second parser.
Well, that's kind of a problem, though &amp; helps. I understand the "avoid success at all costs" approach but eventually someone needs to program software with correctness gaurantees and Haskell seems to be a good start at doing that.
Why need the morphisms denote pure functions? What axioms of category theory are violated if we allowed side effectual morphisms?
In the area of government contracting, BAE Systems has a group that does formal methods research (mostly for DARPA). It's generally public access work, and has allowed me to program in Coq and Haskell full-time for the past several years. There are other, similar companies like Galois and Mitre. This may give you some non-academic options.
I wish the atom packages did error checking like ghcid does. Currently the use ghc-mod which on my project checks a file in about 5-6 when ghcid reloads new errors instantly.
Neat
Thanks! That will help with something else I'm working on :) In this, lookahead isn't the only problem. The # and ## operators seem like the same operator: after all, `a # b` == `a ## b`. But in fact their behavior is path-dependent. Compare ` X = a # b ## c` = ((a,b),c) to Y = `a ## b ## c` = (a,b,c) A parser working through either of them, by the time it's gotten to the space after the `b`, has parsed the expression `(a,b)`. The remaining portion, `## c`, has to treat those two equal expressions differently, based on whether they were generated by the `#` or the `##` operator. A natural response would be to include a number in each tree, indicating the number of hashes in the top-level operator that generated it. That gets complicated by the inclusion of parentheses: `(a ## b) ## c` has to be treated the same as `(a # b) ## c`. So rather than a number, my current strategy is to include a value of the following type: data EO = EO { inParens :: Bool , inLevel :: Int } deriving Eq instance Ord EO where EO a b &lt;= EO c d | a /= c = c &lt;= a | otherwise = b &lt;= d Doing that will, I think, obviate the need for lookahead. It might be the simplest way, but I'd love to be wrong about that.
I can't comment directly on Adjunction versus Representable, but I'm really enjoying using Representable for vectors and matrices. When matrix multiplication crunches out as a quite performant `mmult x y = tabulate (\(i,j) -&gt; row i x &lt;.&gt; col j y)` and `(&lt;.&gt;) a b = sum $ liftR2 (*) a b` who can complain? Having said this, I'd like to write the same code with adjunctions (given a better understanding) and see what pops out. I suspect that adjunctions gets useful when you need to start Align'ing things that don't have exactly the same shape.
If you have to keep track of parentheses yourself, you're probably not taking enough advantage of your parsing library. Do you know how to parse operators with different precedence, e.g. how to make sure that `1 + 2 * 3` gets parsed as `1 + (2 * 3)` not `(1 + 2) * 3`?
Correctness guarantees aren't really at stake. As long as you don't combine sum types and record types in the same `data` definition, the compiler will never let you get/set a field that isn't known to exist on a record. Many extensible record libraries offer the same guarantee.
Functions like your `(&lt;.&gt;) :: (Foldable t, Representable t, Semiring n) =&gt; t n -&gt; t n -&gt; n` remind me why I love functional programming. Zipping two things together and aggregating the result is a very idiomatic thing to do. Being able to do those two things together tersely and generically is beautiful - that's what I was referring to when I said "functors that are both Traversable and Representable are awfully useful".
Ah, sorry wasn't clear enough. I'm saying we need Haskell's correctness and we need to be able to use Haskell. So, records should be far more easy to use.
Most effectful functions don't cleanly compose, and further if you have pure types as objects and effectful functions as morphisms, then your category doesn't really capture what's going on in the semantics of the language, as there's no object-representation of the effects. If you have a monad that represents your universe of effects, then you can create an associated category where objects are objects in effectful contexts, and morphisms are kleisli arrows (i.e. `a -&gt; m b`). But that's no longer the same as the construction of `Hask` -- its a new construction.
Oh. Hah. Yes I agree with you there!
If you mean Text.Megaparsec.Expr, I'm [pretty familiar](https://www.reddit.com/r/haskell/comments/6v0zi2/some_studies_of_textmegaparsecexpr/). I am basing my code on it.
I think that Representable is the easier abstraction to work with in Haskell. The relationship to adjoints is really nice, but since in Haskell we can only work with the one hom-set adjoint, then the more "hands on" structure is the better choice.
The description of the package addresses this question. There's a different package that is like that: https://hackage.haskell.org/package/OneTuple
A ton of languages have experimented with row types and row polymorphism. OCaml objects and Elm records come to mind. I'm disappointed that Haskell saw this be adopted in many languages but ignored it, despite clear benefits.
Obligatory (and shameless) mention of the Tardis library. http://hackage.haskell.org/package/tardis-0.4.1.0/docs/Control-Monad-Tardis.html Using mdo is probably the easiest way to write time traveling code, but if you find yourself wanting more fine grained control over the "mutable state" that can affect both future and past, try out a tardis.
This is a really good post! 
for something like this, personally, I'd treat parentheses separately (i.e., everything between parens is a blob wrt the current parser, and the recursive parser will deal with it) and then have a second pass over the parse tree to clean things up by flattening where appropriate
Fixed, cheers!
the obvious bottom-up approach would be to have the parser generate a parse tree that keeps track of the size of the operator names, and treats the operators as transformations on this state. So, for example, given "a # b ## c # d": * Start off with the empty work queue: `Nil`. * parse "a" to `a`. For the sake of uniformity, let's say that atoms have 'strength' zero and convert `a` to `Foo 0 [a]`. Having nothing better to do, we push this onto the work queue: `Nil :&gt; Foo 0 [a]` * parse "#" to see it has 'strength' 1. So now we have the work queue `Nil :&gt; Foo 0 [a] :&gt; Foo 1 [_,_]` where the holes stand for things that need to be filled in. But we know that `1 &gt; 0` so we can perform a reduction to get the work queue `Nil :&gt; Foo 1 [Foo 0 [a], _]`. * parse "b" to `b`, hence `Foo 0 [b]`, hence the work queue: `Nil :&gt; Foo 1 [Foo 0 [a], _] :&gt; Foo 0 [b]`. For now, let's just leave it there. * parse "##" to see it has strength 2, so we have work queue: `Nil :&gt; Foo 1 [Foo 0 [a], _] :&gt; Foo 0 [b] :&gt; Foo 2 [_,_]`. But, since we have that `2 &gt; 1` we know the `Foo 0 [b]` must fall under the `Foo 1 [Foo 0 [a], _]` rather than falling under some tree to the right; so we can reduce this work queue to `Nil :&gt; Foo 1 [Foo 0 [a], Foo 0 [b]] :&gt; Foo 2 [_,_]`. And since `2 &gt; 1` we can reduce again to get `Nil :&gt; Foo 2 [Foo 1 [Foo 0 [a], Foo 0 [b]],_]`. * parse "c" to `c`, hence `Foo 0 [c]`, hence the work queue `Nil :&gt; Foo 2 [Foo 1 [Foo 0 [a], Foo 0 [b]],_] :&gt; Foo 0 [c]`. Again we have to wait, because we can't be sure whether `Foo 0 [c]` is immediately dominated by the `Foo 2 [...,_]` or whether it's dominated by some tree to the right. * parse "#" to see it has strength 1, so we have the work queue: `Nil :&gt; Foo 2 [Foo 1 [Foo 0 [a], Foo 0 [b]],_] :&gt; Foo 0 [c] :&gt; Foo 1 [_,_]`. Now since `1 &lt; 2` we know the `Foo 0 [c]` must be dominated by `Foo 1 [_,_]`, so we can reduce the work queue to `Nil :&gt; Foo 2 [Foo 1 [Foo 0 [a], Foo 0 [b]],_] :&gt; Foo 1 [Foo 0 [c],_]` * parse "d" as `d`, hence the work queue: `Nil :&gt; Foo 2 [Foo 1 [Foo 0 [a], Foo 0 [b]],_] :&gt; Foo 1 [Foo 0 [c],_] :&gt; Foo 0 [d]`. Again we have to wait. * parse the EOF (or end of whatever enclosing parse context), so now we know the work queue is closed off and nothing more will be coming in on the right. This allows us to reduce to `Nil :&gt; Foo 2 [Foo 1 [Foo 0 [a], Foo 0 [b]],_] :&gt; Foo 1 [Foo 0 [c], Foo 0 [d]]` and then we can reduce to `Nil :&gt; Foo 2 [Foo 1 [Foo 0 [a], Foo 0 [b]], Foo 1 [Foo 0 [c], Foo 0 [d]]]` and since we have a singleton work queue the parse succeeds and that's our answer. * Clean up the parse tree to remove depth labels or whatever, as desired. It's all the same as how to parse any collection of infix operators with different precedents. The only interesting bits are (a) the set of symbols isn't fixed a priori, and (b) the operator(s) is(are) associative. Re the first point, a simple augmentation of the usual shuntingyard-style algorithm will do it (e.g., what parsers of Haskell itself do to allow user-defined infix operators). Dunno if megaparsec has the generalized version built in already or if you'd have to roll your own. Re the second, that just means whenever we have two adjacent things in the work queue at the same strength/precedence, rather than branching left or right (as with `infixl` or `infixr`) we instead want to merge/flatten them into a single node.
It just occurred to me that we actually do support normal dot syntax for modules. Why aren't modules and records implemented more similarly? Shouldn't modules be records of functions (that happen to be from other files)?
Do it. 
Mind blown.
&gt; The traverseExpD function is not recursive, which means that the compiler can inline it. Looking at the code it isn't indeed. However, how does it traverse every children ? Does the recursion has to be in `f` then and have the responsibility of incrementing the depth itself ? That doesn't sound really safe.
A good way is the [shunting yard algorithm](https://en.m.wikipedia.org/wiki/Shunting-yard_algorithm).
Non-Mobile link: https://en.wikipedia.org/wiki/Shunting-yard_algorithm *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^104456
this is a cool use http://wall.org/~lewis/2013/10/15/asm-monad.html
Take a look at some of the examples. E.g. in `lowerCase` the recusion happens by passing `lowerCase` itself to `mapExp`. There is no need to manually increment the depth yourself, that happens in the `mapExpD`.
This is very cool. Is there any hope that the compiler could somehow reject functions that would cause infinite loops in this case? Or is that again undecidable? The major problem I have with this is that you can't really use it properly if you don't completely understand how it works because you can easily shoot yourself in the foot.
In the lowerCase example, the code actuall calls `mapExp`, which map only to the immediate children. About `mapExpD`, I don't understand how it works, could you explain ?
&gt; Does the recursion has to be in `f` then Yes, here's the recursive case of `freeVars` for example: freeVars d x = foldExpD freeVars d x &gt; responsibility of incrementing the depth Is still done by `foldExpD` in this case, so the programmer must pass its `d` along. I think that's OK.
You are right that `mapExp` maps only over the immediate children. However you are mapping `lowerCase` over those children. `lowerCase` will then either stop because it found a `Global` or it will again call `mapExp` which will map over the children of that child. That way you end up traversing everything.
dig the metaphor of fixed points as self-fulfilled prophecies. 
And the instances of [`Representable`](https://hackage.haskell.org/package/adjunctions-4.3/docs/Data-Functor-Rep.html#t:Representable) tell you a lot about what you can do. As an example, a [product of representable functors](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Functor-Product.html) is representable: -- basically (f a, g a) data Product f g a = Pair (f a) (g a) (Representable f, Representable g) =&gt; Representable (Product f g) which means you can place two matrices, chess boards, spreadsheets side by side, where you index with a sum (`Left` indexes into first, `Right` into the second) data Pair a = P a a instance Representable Pair where type Rep Pair = Bool index :: Pair a -&gt; (Bool -&gt; a) index (P f _) False = f index (P _ t) True = t tabulate :: (Bool -&gt; a) -&gt; Pair a tabulate generate = P (generate False) (generate True) index @(Product Pair Pair) :: Product Pair Pair a -&gt; (Either Bool Bool -&gt; a) ---- You can also [compose two representable functors](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Functor-Compose.html) -- f (g a) newtype Compose f g a = Compose (f (g a)) (Representable f, Representable g) =&gt; Representable (Compose f g) This means you can nest representable functors within one another, a nested `Pair (Pair a)` is a **2 x 2** matrix index @(Compose Pair Pair) :: Compose Pair Pair a -&gt; ((Bool, Bool) -&gt; a) ---- With [`Cofree`](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Comonad-Cofree.html#t:Cofree) we get infinite branching trees where the branching structure is determined by a representable functor data Cofree f a = a :&lt; f (Cofree f a) Representable f =&gt; Representable (Cofree f) where `Cofree Pair` is an infinite binary tree that you can index into using a sequence of `Bool`eans index @(Cofree Pair) :: Cofree Pair a -&gt; (Seq Bool -&gt; a) so you not only get individual representable functors, but means of composing them
Could this be done with Control.Lens.Plated.para by making "para" return a function that takes the depth as parameter?
How does the compiler pass `freeVars` as an argument if it has been inlined?
You can use Cabal to build and install a package and all its dependencies in a single step, see [here](http://cabal.readthedocs.io/en/latest/installing-packages.html#installing-packages-from-hackage). The typical workflow is to either use per-project [Cabal sandboxes](http://cabal.readthedocs.io/en/latest/installing-packages.html#developing-with-sandboxes) or [Stack](https://docs.haskellstack.org/en/stable/README/).
I sketched this post out pretty quickly, so it's a bit of a brain dump. Lemme know if I went too fast anywhere!
It does not inline `freeVars`, but `foldExprD`. That's pretty much the recursion you would write by hand, which I think is the author's (easy to misunderstand) point.
But is there any compile-time guarantee for all those nice properties?
Oh, I think I finally understand. But why can't GHC do this transformation itself?
Oh, that's pretty neat, I didn't know about that part of Megaparsec! So the language you want to parse is basically this table, but megaparsec doesn't support infinite tables, right? table = [[binary (replicate i '#') Branch)] | i &lt;- [1..]] What I'm suggesting is that you can instead write table n = [[binary (replicate i '#') Branch)] | i &lt;- [1..n]] to produce a finite table, and then you can fill in `n` by using lookAhead. If parentheses work in the example with multiplication and addition, then they should work in this case as well.
I see, but lowerCase call lowerCase itself and `mapExp`, which makes lowerCase recursive and lowerCase and mapExp correcursive (is it the right term). I'm probably missing something, but the recursion is just move from `traverseExp` to the user function, so what difference does that makes in practice ? The Author claims it's better because it can be inlined, but the user function can't, so I'm a bit confused.
The author is not trying to claim that it’s better than manual recursion. He is trying to say that it’s not worse than manual recursion since after inlining `mapExp` you basically end up with the same code.
It's a bit unfortunate that haddock doesn't seem to respect newlines in [`type ListenBrainzAPI`](https://hackage.haskell.org/package/listenbrainz-client-1.0.1/docs/Web-ListenBrainz.html#t:ListenBrainzAPI), it makes the type more intimidating than in the source.
While true, I don't really expect people to need it. I'd be tempted to move it into `.Internal`. It's just exported because I have no reason to hide it.
Another consequence is that the API-as-a-GADT approach doesn't let you do more complicated effects like `try :: m a -&gt; m (Either e a)`. This is a drawback, on one hand, but it's also a positive because you guarantee that your API can be lifted through any transformer stack.
I consider those to be effect handlers, but the type would of course come out differently. [Effect Handlers in Scope](http://www.cs.ox.ac.uk/people/nicolas.wu/papers/Scope.pdf) does try and tackle this problem though, I believe.
Sure, I'm in the same camp. Sometimes though you can get stuck if you try writing recursive functions because they result in infinite transformer stacks.
The point is that it's nicer to write the way shorter definition by hand and ending up with the code you'd expect via inlining and a bit of magic (e.g. cutting off the inaccessible branches).
Ok
It might be possible, in analogy with "MVar blocked indefinitely" checking.
Well, I think that sort of runtime checking already happens when you get `&lt;&lt;loop&gt;&gt;`.
This. The #1 rule with `Functor`s, `Applicative`s and `Monad`s is you don't talk about functors, applicatives and monads.
Technically PLT is the name of research group behind racket. There are many strong places for PL research ;)
You keep saying that the enclosed data is hard to access, giving examples of sum types in which there might be no data at all. I think this gives the wrong intuition. It makes it sound as if the datatypes are opaque and that the only way to modify their contents is via `fmap` and friends. You later reinforce this incorrect intuition in your Java comparison. In reality, none of those types are opaque. You can easily inspect them, pattern-match on them, and access their contents. You do not need Maybe to have a Functor instance in order to write a `fmap`-like function for it. Your post implies that if Maybe has a Functor instance, *then* you can use `fmap` to manipulate its contents, when in fact the implication goes the other way: if you can manipulate Maybe's contents by writing a `fmap`-like function, *then* we can give Maybe a Functor instance. So the reason we would give Maybe a Functor instance is not at all so that we can manipulate its contents, as we can already manipulate its contents without a Functor instance. The reason we use type classes is to write generic, reusable code which works with many different types. For example [`replicateA`](https://www.stackage.org/haddock/lts-8.19/containers-0.5.7.1/Data-Sequence.html#v:replicateA) allows you to execute the same action N times regardless of what you mean by "action", because the Applicative type class allows you to explain what "action" means for your particular type. It's tricky to find a good intro-level example of reusable code which only needs a Functor instance, because you can't do much more than applying `fmap`. I think the trick might be to ask for extra functions which act on the container, without using a type class to get them? Here is my attempt: -- | -- given functions which find the minimum and maximum values of a container, -- rescale all the values to the range [0.0, 1.0] -- -- &gt;&gt;&gt; rescale minimum maximum [1,2,3,2,1] -- [0.0,0.5,1.0,0.5,0.0] -- -- &gt;&gt;&gt; import Data.Tree -- &gt;&gt;&gt; let tree = Node 4 [Node 1 [], Node 2 [Node 5 []], Node 3 []] -- &gt;&gt;&gt; putStr $ drawTree $ fmap show tree -- 4 -- | -- +- 1 -- | -- +- 2 -- | | -- | `- 5 -- | -- `- 3 -- &gt;&gt;&gt; putStr $ drawTree $ fmap show $ rescale minimum maximum tree -- 0.75 -- | -- +- 0.0 -- | -- +- 0.25 -- | | -- | `- 1.0 -- | -- `- 0.5 rescale :: Functor f =&gt; (f Double -&gt; Double) -&gt; (f Double -&gt; Double) -&gt; f Double -&gt; f Double rescale findMin findMax container = fmap f container where lo = findMin container hi = findMax container f x = (x - lo) / (hi - lo) 
Why do I have to sign in?
Blank page with a `Something went wrong` message after logging in with GitHub...
I really like Eta, and it's an impressive feat, but it just feels like such a shame and missed oppourtunity to explicitly fragment it from Haskell proper. Over time the ecosystems will likely diverge significantly. Ideally Haskell packages shouldn't mean "GHC packages", and GHC/Eta/GHCJS implement the latest Haskell standard + some subset of extensions everybody agrees on + their own platform-specific extensions. Maybe uniting starkly different Haskell targets can be where Backpack really shines? I think currently GHCJS have a backpack-esque approach already for replacing e.g. `text` with their own implementation.
&gt; It's tricky to find a good intro-level example of reusable code which only needs a Functor instance One need not look too far: `Functor f, Functor g =&gt; Functor (f :.: g)`!
GHCJS doesn't replace `text`, I don't think. The only libraries that GHCJS replaces is the C level code for the RTS and some libraries, for which it provides JS shims. `reflex-platform` uses Nix to replace `text` with an API-compatible version that is based on native JS strings instead, since they're ridiculously faster. Anyway, I think we're way too far gone for any Haskell compiler that isn't 100% GHC compatible to succeed. Any project I write in Haskell uses like 200 different packages. If 0.1% of lines of code on Hackage depend on an extension not supported by my compiler, then my use of 200 packages virtually guarantees that I can't build my project with that compiler. I think GHCJS did the only reasonable thing: Use GHC as a library so that you necessarily support everything GHC supports, and it's easier to update.
&gt; Ideally Haskell packages shouldn't mean "GHC packages" That would be nice, but that simply isn't possible at this point. Anything that transitively depends on `ghc-prim` can only build with GHC (or GHC derivatives like GHCJS and ETA). Let me name a few libraries that depend directly on `ghc-prim`: `bytestring`, `text`, `vector`, `hashable`. If you use any of these libraries or if you use anything that transitively depends on these libraries, your project can only be built with GHC. The reason this happens is because the Haskell language report doesn't specify any fast low-level primitives. But ultimately, a lot of applications need arrays with O(1) lookup, and then you're going to be depending on `ghc-prim`. I think that if the language report talked about things like `ByteArray#` and `Array#`, there's a chance that your dream of a reusable package set could possibly come true though.
Can you please file an issue here https://github.com/typelead/eta/issues with the title [eta-tour]? We can keep the debugging discussion there.
I tried another time and it worked
I was confused by the forward arrow button at the top left of the start page; when I hover over it, the tooltip says "Reset", which was not at all what I wanted to do. (On subsequent pages, both arrows say "Reset" as their tooltip.)
Why not: class Monad m =&gt; MonadListenBrainz m where liftListenBrainz :: Coyoneda ListenBrainzAPICall (m a) -&gt; m a instance Monad m =&gt; MonadListenBrainz (ListenBrainzT m) liftListenBrainz (Coyoneda k e) = go e &gt;&gt;= k where go :: ListenBrainzAPICall a -&gt; ListenBrainzT m a instance Member ListenBrainzAPICall effs =&gt; MonadListenBrainz (Eff effs) where liftListenBrainz (Coyoneda k e) = send e &gt;&gt;= k Disclaimer: untested, not even compiled.
This might be the single best introduction to Haskell I've ever seen. Kind of a shame that it's branded 'Eta,' honestly - No offense to the Eta project, which I'm quite excited about, but it'd be kind of nice to have some of this wonderful polish and presentation feed back into the larger Haskell community.
Have you looked into Attribute Grammars? (See also: UUAGC)
Mostly because it means that `liftListenBrainz` means something - it's a monad homomorphism. In this case, I'm not quite sure what `liftListenBrainz` actually means. Having a meaning means that I know things like `liftListenBrainz (getListens ..) &gt;&gt; liftListenBrainz (submitListens ..)` is at least equivalent to `liftListenBrainz (getListens .. &gt;&gt; submitListens ..)`, for example. Essentially, I get some reasoning tools. However, I do find your suggestion interesting, and I'll think on it.
The idea of generalizing reaching into a type to act on it's data is sort of implied by discussing types that you see literally everywhere. You'd have to pretty dense not to pick up on the idea that a lot of stuff has a Functor instance. The intuition of 'using fmap to modify values in a container' still holds in the face of your example here. It's ok for people to come to these sorts of realizations in stages. A lot of learning is 'unlearning' former assumptions about the way things work. I agree that the comparison to Java is a bit forced and off the mark, but the tutorial as a whole is an excellent introduction to a number of subjects that are frequently intimidating to newcomers, and I think the community benefits a great deal from these sorts of tutorials.
&gt; Haskell is the only relevant one with types. That and Scala, except that he has some personal issue with Scala 
Good luck then, every native mainstream GUI library is modelled with OOP/subtyping. At least if you want Haskell to be used in more serious GUI applications, then this needs to be addressed somehow
&gt; Instead of saying "adc 0" they'd have to say "adc (0::Integer)" and at that rate they'd rather just go back to "adcz 0" instead. I wonder what /u/quietfanatic thinks of the `TypeApplications` version `adc @Integer 0`.
&gt;reflex-platform uses Nix to replace text with an API-compatible version that is based on native JS strings instead, since they're ridiculously faster. Ah, yeah, this is what I was thinking of.
Yeah, really not a good first impression of whatever Eta is. 
Not sure I really see the benefit. This will still incur the same performance problems as free monads if you use this to define all your effects (actually worse, since you have to hoist each command and iterate down a recursive structure instead of just lifting something directly, which is much more inlinable). You're just moving the definition of the API out of a typeclass and into a GADT. Semantically it's still the same.
I find moving the definition into a GADT is actually quite useful, because then I only really need to write a single low level running function, and then I can re-use that for the definition of my type class instances. When the API grows, both my monad transformer implementation and extensible effects implementation's are both extended by definition. Plus, if you don't want to use any of those approaches, you've still got access to some syntax (`ListenBrainzAPICall`) and its low-level reference interpretation (`performAPICall :: Manager -&gt; ListenBrainzAPICall a -&gt; IO (Either ServantError a)`). I'm aware it's semantically the same, but I've found this pattern to be quite nice to work with from an engineering perspective. Certainly nothing new, but a nice way to organise things :) WRT performance, maybe - but nothing has been benchmarked.
Yea but I believe that only holds true when you have effects of the form `f a -&gt; m a`. Which basically means: whenever all the transformers can simply use `lift` to define the effect. But with `DefaultSignatures`, you already have this. Any effect that can be automatically lifted for all transformers can have a default signature, meaning that adding new effects can always be done without adding it to every instance definition. Then the "single low level running function" is just the core implementation instance.
Or `type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t`=P
is there any advantage of using fix f = let x = f x in x instead of fix f = f (fix f)
You should check out the aeson-better-errors package. After a quick glance it seems like it might have what you are looking for. Hackage: https://hackage.haskell.org/package/aeson-better-errors-0.9.1.0/docs/Data-Aeson-BetterErrors.html Tutorial: http://harry.garrood.me/blog/aeson-better-errors/
They expect me to log in to get a language tour? That's a new low.
Author here: Yep. I briefly mentioned this in the post, but the advantage is in the number of times that `f` is run. Basic example: `fix (1:)`. With the `let`-based definition, this is equivalent to `xs = 1:xs`. This creates an infinite list, but it's self-referential. So no matter how far down it you traverse, it still has only allocated one `(:)`, and it has only entered the function `(1:)` one time. With `fix f = f (fix f)`, it will re-enter `f` at every "self-referential" point in the returned structure, causing more allocations and more calculations. So the advantage of `let x = f x in x` is that we share the one result thunk in all places. And the problem extends to `MonadFix` to some degree. The reason we can't just do `mfix f = fix (&gt;&gt;= f)` is because this would re-enter `f` and run the action infinitely many times. This works for lazy monads, but for stuff like `Maybe` or `IO`, it simply can't work. You need a version that only enters the function one time.
just to make it clear after 6 hours, the answer is 100% no in almost every situation.
Why use `Only` when `Identity`'s already in `base`?
This actually should already happen exactly. With `IO`, you will get `thread blocked indefinitely in an MVar operation`. With most pure monads, you will get `&lt;&lt;loop&gt;&gt;`. Ruling these things out at compile time though is the same as totality checking.
&gt; I really like Eta, and it's an impressive feat, but it just feels like such a shame and missed opportunity to explicitly fragment it from Haskell proper. I agree. It's great that effort is going into ETA and if it succeeds it will help Haskell. But there is still a need for a JVM backend for GHC, and ETA is not that.
Why `Applicative`? Why not `Functor`? Indeed generic traversals can be generalized further. [The mother of all recursion schemes is Hylomorphism](http://www.cs.ox.ac.uk/people/nicolas.wu/papers/Hylomorphisms.pdf), which is defined as the solution to the following equation, where `F` is a base functor: ``` h = a · F h · c ``` In order for a unique `h` to exist, algebra `a` and co-algebra `c` must satisfy certain conditions, and hence the paper introduces the conjugate rule. But for all practical purposes, it is easy to define most if not all other recursion schemes on top of `h`. Certainly the depth based traversal of expression tree would fit the picture. In the past I've implemented the more traditional named-based variable substitutions this way, but I'm sure de Bruijn indices will work out just as well.
+1 for Stack
I am very impressed by this, I am currently convincing my company to adopt Haskell instead of PHP, some guys are very scared just because of the name, I will tell them that there is a new language available for the JVM that is worthy to try without saying it is a Haskell variant. Maybe Haskell needs re-branding; if IE did, why couldn't Haskell do the same? Well played Eta.
Just solve any of the JS problems in Haskell instead. Works for any "JS" problem non-specific to the DOM, unless you're using GHCJS, in which case, you can include those too. Don't look around for Haskell specific problems - Just solve any old problem with Haskell.
Yesod is certainly a lot bigger than the others but provide a lot too. And I don't think it will lock you with some libs, even if it tends to provides sensible defaults (Persistent, Shakespearean Templates, etc) I don’t consider snap or scotty bloated, at least it wasn't last time I tried them. I never really checked out happstack. I guess it is a matter of taste, and (available time to reinvent the wheel), but for me until now, the right balance is Spock (https://www.spock.li/) I might use Yesod, depending of the requirements. Using Wai/Warp is certainly possible, but depending of your requirements, you could find yourself re-inventing things that already exist in a better way. 
From the scary Eta privacy policy: "Collection of Personally Identifying Information In order to write information into the Eta telemetry server, users may decide to provide certain personally identifying information including but not limited to: email address, username, password, personal website, and account names on other services such as GitHub, Twitter, and IRC. If users do not want their information tracked in this manner, they can opt out by following the instructions here. However, this means that some features of the Eta website will be unavailable to them."
Is this something the Haskell Prime committee is looking at?
That sounds like something that should be brought up to the next report committee! We should definitely provide a _standard_ set of primitives on which all implementations can build packages.
I got over my sign-in concerns and accessed with Github. Totally worth it! The Tour Of Eta is pretty incredible.
Hi, haskero is based on intero. Did emacs with intero was also as slow as haskero ?
The way how I see it, is type classes are in some ways like an interface for a specific data type (and specific classes of data types in general). I know the data type is specified to strings, but passing it as an argument to a function that accepts strings won't work. I could write my own recursive function (with lists) or use pattern matching (with maybe) to apply a function to the inner value, but that breaks the dry principal. Again neither Lists nor Maybe (nor several other data types) implement the Commonad type class, so getting at the inner data from them will never be as simple as applying them to extract
It's kind of tricky though. Look at the type signature for `writeArray#`: writeArray# :: MutableArray# s a -&gt; Int# -&gt; a -&gt; State# s -&gt; State# s The haskell report definitely isn't going to commit to GHC's internal representation of `IO`, so maybe it could just say that a friendlier boxed variant needs to be provided: writeArrayIO :: MutableIOArray a -&gt; Int -&gt; a -&gt; IO () But packages like `text` aren't using this; they need a variant that runs in `ST`: writeArrayST :: MutableSTArray s a -&gt; Int -&gt; a -&gt; ST s () But, the haskell report doesn't talk about `ST` anywhere. So that would need to be added as well. And to avoid the proliferation of duplicated functions (one for `IO` and one for `ST`), you'd have to try to standardize something like [PrimMonad](http://hackage.haskell.org/package/primitive-0.6.2.0/docs/Control-Monad-Primitive.html), but that uses an associated type family. I'm not saying it can't be done, but I am very skeptical. You would need to add a lot to the standard, and I'm not sure how much it would really matter since ultimately, it's mostly GHC being used at this point. If any of your dependencies used any GHC-specific feature (`par` from Concurrent Haskell, the `I#` data constructor for `Int`, levity polymorphism, `DataKinds`, etc.), you'd be in exactly the same boat you're in now: only being able to build the project with GHC.
Even if we can't support _all_ extensions and every library being able to share a core set would be huge, no? There are a lot of libraries that don't care about a many GHC specific features, if we could support that use-case we could at least constrain dialect divergence.
Thank you for your reply. I will take a look at spock. But i think i dont have to reinvent the wheel by using plain wai. There is wai-extra that Provides things like authentication and so on. For any other requirement is another cool Library (html templating and so on). So i Can mix my own set of libraries to prevent reinventing the wheel. Do you think that could cause any Problems with compatibility or anything Else? I just wondered why there is no one doing it this way.
That's true. I feel like getting `ST` in would be a big win. But no matter what gets added, `bytestring` is never going to build with anything other than GHC. So, if the goal is to constrain dialect diverge, as you say, then yes, I think there's some benefit to be had. If the goal is to ensure that most of hackage can be build by things other than GHC, I'm not too hopeful.
Is it better than the interactive introduction found here? https://www.tryhaskell.org/
Some people do use plain wai for apps, as most of the wai api is very easy to use. However "routing" can be quite tedious, for which you can use a library like [wai-routes](http://hackage.haskell.org/package/wai-routes) (which I wrote).
Entirely correct. I happen to like the first class nature of this, mind you.
&gt; Are there any secret reasons to not use bloated frameworks? Yes, the reason is that they are in fact not bloated. Yesod for example is merely just a monad transformer (`HandlerT`), some Template Haskell to generate and dispatch routes, a templating system, a great book, and a full eco-system. What you might consider "bloated" would not be the different packages, but the Yesod scaffoldhing (code you could/should write manually already done for you). Also, pretty much everything you find in Yesod(session, cache, db, streaming, authorization, etc ... ) , are stuff that you'll probably need a some point anyway. I'm saying Yesod, but that's probably the same with Snap. I've tried Scotty (years ago) and I actually find it much easier to not use some feature from Yesod until I need them, than had new ones in Scotty. That' was years ago, I probably would have a different opinion today.
But unless you are doing an ordering independent operation such as inserting into a set, then switching between the two will change what your code does. 
Not that I disagree with your argument about rebranding haskell, but I don't think anyone has really been fooled by IE's rebranding ;)
I'm guessing their reasoning is that it makes foldr a sort of "lift" on functions of the form `a -&gt; b -&gt; b` into a function on foldables. Similar to how fmap lifts a function to work on functors.
I can see the logic behind that change but I think foldr is used partially applied too often for people to accept that.
It's certainly undecidable. Deducing *anything* about the behavior of a function is undecidable (that's Rice's Theorem, pretty much). But undecidable problems can often be solved in special cases. To give an easy example, we know the Halting Problem is undecidable, but do you have any difficulty concluding that the following function will always halt? square n = n*n So, sure, there is hope that the compiler could reject *some* of those functions. But offhand, I couldn't say how.
Yes, although the focus is a bit different, so the comparison isn't entirely fair. The Eta tutorial covers a lot more ground, is significantly lengthier, and covers meatier subject matter, whereas tryhaskell is essentially just an introduction to basic expression syntax.
Have you checked out e.g [servant-client's reverse dependencies](http://packdeps.haskellers.com/reverse/servant-client) ? There are clients for all kinds of APis, hopefully that will help. Please let us know on the issue tracker though if you think there's something missing in the client section of the tutorial :)
https://hackage.haskell.org/package/listenbrainz-client-1.0.1/docs/Web-ListenBrainz.html, as blogged about earlier :)
This is an awesome resource, didn't know it existed! Looks like a lot of useful examples here (Telegram API in particular). Thanks!
My question was whether this whole problem falls into the class of "special cases" where it can be decided. Obviously the condition here is that a function is lazy in it's parameter in a certain way. Perhaps this can be checked.
I had to make some changes on "Bindings (4/14)" powerLevelLimit :: Int powerLevelLimit = 9000 main :: IO () main = do putStrLn $ fullName ++ " has power level " ++ show powerLevel ++ "!" when (powerLevelLimit &lt; powerLevel) $ do putStrLn $ "It's over " ++ show powerLevelLimit ++ "!"
Please let me know when the FFI and Stdlib parts are ready! I want to pass this around the office again when that happens.
I guess a major reason to keep the list last, is currying. You want to be able to define things like `foldr (:) []`.
"Category Theory" is the black hole at the center of the Haskell galaxy. This black hole is part of why "you don't go back." However, one can treat Haskell as a small language, on the scale of Scheme, and thoroughly enjoy programming in Haskell without getting torn apart by the gravitational pull of this black hole. Then, add theory bit by bit as it makes sense. Advanced Haskell gives us a glimpse of the future of programming, but one can be quite happy looking away. 
Let's take `foldr (:) [] [1..5]`, which is equivalent to `1:2:3:4:5:[]`, ie `[1..5]`. If you use `foldl (:) [] [1..5]`, you probably think of `[] : 1 : 2 : 3 : 4 : 5`. Which doesn't type check. You need to use `flip (:)`, which totally changes the result, you get `[5..1]` instead. Having a `flipped_foldl` so that `flipped_foldl (:) []` typecheck, wouldn't change anything. Left folding reverses list : If you start with a `[]` you can only prepend things to it, whereas foldr keep the list in order.
I know you started about 4 weeks ago, but can I still join in? How far in the course are you? I think I might be able to catch up as I've been doing Haskell exercises for about a year now. Thanks, Harry T.
That's interesting. I guess one place where I would want to swap foldr and foldl might exactly be when I want the input to be processed from the other end. But maybe not, I haven't needed to use folds very often in my Haskell 'career' so far (or maybe I've just been able to avoid them). And I am aware of using flip, maybe that's what I used in these cases, and just wondered how nice it could have been if all I needed to change was one letter.
I haven't tried emacs.
That means that I will be unavailable for Eta.
A bit late but I'm using emacs with the emacs prelude for general emacs configuration; haskell-mode for general syntax highlighting etc, dante-mode/mafia-mode to get typechecking/type holes/syntax checking/repl. It all works similarly to a good intero setup just without stack, which isn't supported on most projects I work on. 
I always use plain wai/warp. I even blogged about it (though the post is a tad old now): https://singpolyma.net/2013/09/making-a-website-with-haskell/
UCSD has a great PL group. The focus has traditionally been on program verification (with refinement types, eg LiquidHaskell, and theorem provers), but that's expanded recently to include research on type errors, gamification, floating point numbers, and probably many other things I can't remember off the top of my head. There are also 3 new faculty in the group with specialties in security, program synthesis, and cyber-physical systems, so it's a great time to join! (I just graduated so I'm super biased :)
`intero` in VSCode took something like 4GB of memory on start up with our company's (smallish, maybe 70 package) Haskell code base. This is reminiscent of running `stack ghci` at the top level, instead of `stack ghci &lt;specific package&gt;`. Does anyone else experience this lack of scaling, or is it just some bad code I've got somewhere?
https://gitlab.com/ismail-s/JTime/blob/35a24a36ddb30f6d14adc9cd4da773c6b6d81029/JTime-servant/src/HTTPClient.hs Note that this code uses type families to try and make the API definitions a bit more DRY (see https://stackoverflow.com/questions/44007111/issues-using-pattern-matching-with-servant-client/44010473#44010473 for more info).
I think scotty and servant are pretty minimal addition on top of WAI. Its basically just routing + request parsers + response builders.
[Here](http://eta-lang.org/) you go. I logged in because I was curious. I initially didn't want to, but it is a huge interactive tour that would be nice to have progress. After logging in, I have a lot less hatred towards them making me log in then I did at first.
You could also do this—tediously but straightforwardly—by rewriting `even` and `times2` in combinator form and then solving `B even times2` = `KK` (where `B` is `(.)` and `K` is `const`). Of course, in combinator calculus you also wouldn’t bother to write these functions recursively. `times2` is `W plus` (given *n*, return *n* + *n*) and `even` is `V not true` (given *n*, apply `not` *n* times to `true`, so 0 → `true`, 1 → `not true`, 2 → `not (not true)`, …).
Is there in Haskell?
Sidetracking the thread a bit, but your mention of chess boards made me think; there was [this thread](https://www.reddit.com/r/haskell/comments/6rzylm/yet_another_conways_gol_100_lines_using/) recently, about modelling automata with comonads. Is there a good way to model things in this fashion that involve _choices_ at each step (like board games), instead of evolving unambiguously?
Dang. I was not expecting this to be as poorly received as it was. I think it's fantastic news when more ecosystems get to enjoy the benefits of pure strongly-typed functional programming, even if I'm not a fan of Scala or the JVM. I guess a lot people don't like Scala no matter what.
Nope: BetterErrors&gt; let asTuple = (,) &lt;$&gt; nth 0 asIntegral &lt;*&gt; nth 1 asIntegral BetterErrors&gt; parseStrict asTuple "[\"a\", \"2\"]" Left (BadSchema [ArrayIndex 0] (WrongType TyNumber (String "a"))) Expecting it should return error for ArrayIndex 1 as well ... But good library though, it does make the error better. Thanks :)
Most of the frameworks that you mention are built on top of wai/warp. I think you may want to approach your problem from a different angle, i.e., work out the requirements of your web app and then see which framework, if any, best fits those requirements. Most of these frameworks came about to solve practical problems and their solutions may overlap with yours. That said, you're obviously free to reject the above and give wai/warp a go. It'll be great learning experience either way.
I've spent quite some time with this problem and come to the conclusion that this is not possible with Aeson. Look at the IResult core data structure of Aeson and you'll probably concur with my conclusion. It's a shame though. Forces people to use abominations like digestive functors for simple things like validations. 
So you used this package https://hackage.haskell.org/package/digestive-functors-aeson to solve the problem (although you are not happy with it)? 
Not in general, but it's possible to craft an algorithm that sometimes can say one way or another. This is what strictness analysis is all about. GHC's version will never say it's definitely lazy in any useful situation (because GHC only cares whether it's definitely strict or definitely completely unused), but other versions should be able to attempt it.
Here's some code I wrote using servant a while back, targeting the Spotify API: https://github.com/tdietert/auto-playlist/blob/master/src/Spotify/Api.hs
Related: [The algebra (and calculus!) of algebraic data types](https://codewords.recurse.com/issues/three/algebra-and-calculus-of-algebraic-data-types/) by /u/joelburget
Yes. It forces you to write the parser by hand. And I hate the applicative syntax which forces me to use records in a positional manner instead of field name &lt;&gt; field value manner. 
Nope. For example polymorphic recursion: foo :: Show a =&gt; Int -&gt; a -&gt; String foo 0 x = show x foo n x = foo (n - 1) (x, x)
What about ApplicativeDo+RecordWildCards? There's probably also some way to leverage Generics to mix records and applicative functors.
\&gt;poorly received \&gt;10 upvotes Stop feeling sorry for yourself
Maybe aeson in a future version... I mean that this seems like a reasonable feature for aeson to have. Would you like to open an issue? https://github.com/bos/aeson/issues
&gt; 62% upvoted I'm more concerned about the downvotes. It's not about my feelings or my worthless internet points. I'm genuinely surprised that so many people aren't excited or interested in this.
Initiality of \[zero, succ\] repackages the induction principle. If you have a proof relying on initiality of \[zero, succ\], and you "inline the package", you get a proof by induction. Is the converse true, that a proof by induction can be rewritten in terms of initiality of \[zero, succ\] in a rich enough category? The base case phi\_1 "proves" P(0) (or, True -&gt; P(0)), the induction step phi\_2 "proves" "for all n, P(n) -&gt; P(n+1)", so they look a bit arrow-like. Then [phi\_1, phi\_2] would somehow form an algebra, and the catamorphism out of [zero, succ] would somehow "prove" "for all n, P(n)". Am I making sense?
Yes, blog post please. I'm not even sure what you mean. What does it mean to separate data types from records?
The weird debugging information from GCC is likely a red herring. You can't really trust the compiler to preserve truly accurate debugging information at high levels of optimization. It's an approximation at best. GHC also *does* support SSE2 and even things like SSE4 primitives when it does code generation (if you ask for it). Here's the secret: for GHC it only works on floating point types. On 64bit intel platforms, SSE2 is always supported and is far superior to the x87 FPU (wider registers, better implementation). I think GHC defaults to it on 64bit and you must ask for it on 32bit. The linked email thread is about something different, which are features like AVX2 and AVX-512. They are logical extensions of SSE with wider data types. GHC does not have a model for these instructions. But this is the easy part: I don't think it would be fundamentally complicated to add these register classes to the backend. It's just a bit of work (the code generator is old and few people work on it.) The story is more complicated by the fact GHC *does* support these features with LLVM (but nobody added support to the ordinary backend which sucks). It's all very tedious in practice. The fact that these instructions can work for scalar integer types isn't exploited at the moment. Doing so for scalar, "loopy" code is much more complicated than "upgrading" generic floating point instruction set calls, because it requires automatically vectorizing the loop body. Code that is "one byte wide" has to be re-organized (and moved around) to support N-byte wide operations. And that can be hard.[1] Vectorization is difficult, but I think there's probably a good compromise: GHC should really just add support for, and expose, the primitives for using packed integer operations on various platforms. And we should just use them. Right now it models them with fairly ISA-agnostic datatypes which I think is an OK tradeoff. (You have a type like `data Double64x4` which is a quad-packed 64-bit double and some operations over them.) In practice even if we can't get automatic vectorization, we have enough "type directed" programming techniques to probably do *pretty* well anyway. Combined with a loop unroller pass, I think you could see a lot of speed up for typical loops with some care, I think. (And [what a surprise](https://github.com/thoughtpolice/unroll-ghc-plugin), there's an ancient GHC unrolling plugin on my GitHub that never went anywhere. I never got a chance to really explore it after Max abandoned it...) You can also skip that all with a manual slow/fast path instead and still see speedups. I think most of this (except autovectorizing) isn't hard. It just requires someone dedicated... --- [1] But if you want to read about vectorization, here are some links: - https://en.wikipedia.org/wiki/Automatic_vectorization - http://www.leafpetersen.com/leaf/publications/icfp2013/vectorization-haskell.pdf - https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/haskell-beats-C.pdf The third link never really made it into GHC proper, unfortunately.
I've tried **a lot** with GHC.Generics and failed miserably. The Generics infra in Haskell is just too damn hard for me to wrap my head around it.
Isn't toLowerFFI 'wrong'? As in, it mutates the given ByteString in-place, which is AFAIK not permitted. Similarly, both versions (toLowerNative and toLowerFFI) 'do' something different: the native version creates a new ByteString based on the input (i.e. performs allocation), the FFI version doesn't. This obviously doesn't relate to GCC doing vectorization and creating more efficient loop code :)
This article illustrates the same kind of ideas for the particular case of functions on lists: http://www.cs.nott.ac.uk/~pszgmh/fold.pdf
I'be been writing Haskell professionally for about 1 year now and I'm still waiting for the moment I'll start blaming laziness by default. It seems to be a recurring issue for a lot of people working with the language.
Indeed, type classes are pretty unintuitive for heavy type-directed metaprogramming. I thought of something like [`codec-0.1`](https://hackage.haskell.org/package/codec-0.1.1) had, where you can sequence parsers of record fields in any order, but with extra annotations, and they would be reordered implicitly. Is that what you're looking for? codec defined the "annotations" with TH, and I think I can derive them with Generics, but I lost interest because `ApplicativeDo+RecordWildCards` seemed like a better alternative. data Record = Record { x :: Foo, y :: Bar, z :: Baz } $(... {- some TH generating f_y, f_z, f_x -}) parseRecord = Record $&gt;&gt; f_y &gt;-&lt; parseBar &gt;&gt;&gt; f_z &gt;-&lt; parseBaz &gt;&gt;&gt; f_x &gt;-&lt; parseFoo
Can equational reasoning be used for this sort of thing? Would it not by the same token be a valid argument to say that `forall f , g : fmap f . fmap g = fmap (f . g)`. Leading to the same conclusion. I realize that the question sounds similar to the next one in line but my point is specifically about whether you can draw such conclusions from equational reasoning. Using equational reasoning we could show that the functions are equal but that does not say anything about how many steps it takes to compute: ``` x _ = () y n | n &lt; 0 = () | otherwise = y (n-1) ```
&gt; &gt; This makes the operational behaviour easier to understand than the `normal' approach where backtracking is the default behaviour. &gt; That's the first time I hear this one! In my opinion it's the opposite, I find backtracking parsers simpler than non-backtracking (apparently that's called "predictive"?) parsers, because it allows me to think in terms of taking the union of the languages accepted by the parsers instead of thinking operationally in terms of parsing tokens and backtracking. Note that it's saying it makes the *operational behaviour* easier to understand. Not that it makes writing parsers easier. You're right that backtracking parsers are easier to write, but it's harder to understand how efficient your parser actually is. If a parser never backtracks, then understanding how efficient it is, is easy.
Also a lot of people (me included) aren't fans of the post's author.
Thanks for the article! I was wondering how the general proof technique looks like. Is it something along these lines? Given `f, g : I -&gt; A`, with `&lt;I, in&gt;` an initial F-algebra (a recursive data type in `Hask`), then `f = g` if `f` and `g` can be expressed as catamorphisms (folds) of the same algebra `&lt;A, h&gt;`; so the challenge would be to find a suitable function `h : F A -&gt; A`. 
I'm fixing up an API binding I wrote using servant currently :) https://github.com/FintanH/Scaleway-Haskell/tree/develop-servant
Why you were downvoted for this i have no idea.
With `ApplicativeDo` and `RecordWildCards` you can have do field1 &lt;- getField1 fieldBar &lt;- getFieldBar pure Record{..} and the fields will be filled into the `..` part. I wonder if you would like this.
I recently read about "a community consensus on law-less typeclasses". While I have some intuition to what "law-less typeclass" is, I am not sure what is the widely approved way of dealing with them and why. Could someone explain it?
&gt; if `f` and `g` can be expressed as catamorphisms (folds) of the same algebra `&lt;A, h&gt;` I think you understood well, although "expressed as catamorphisms" sounds odd because uniqueness characterizes *the* catamorphism. I would say the condition to prove is that `f` and `g` are both algebra morphisms (with the same domain and codomain `&lt;I, in&gt; -&gt; &lt;A, h&gt;`). This statement can be made independent of any claim of initiality. The fact that `&lt;I, in&gt;` is initial then implies that `f` and `g` are both equal to the catamorphism of `&lt;A, h&gt;`.
Doesn't the Tardis generally have poor performance, especially with respect to space or is it equivalent to monadfix?
Let's interpret the Set data structure as Set x = 1 + x + x^2/2 + x^3/6 + ... That is, it's either empty, or a singleton, or a pair where the order does not matter, etc... Then the derivative of Set is just Set! In words: having a hole in a set, just leaves you with a set and nothing changes. Not sure whether this means something useful, though...
I think `B.useAsCStringLen bs` will make a copy of `bs` and pass along the pointer to that array.
I think the assembly dump of `toLowerNative` does not contain any relevant code at all. `xorl %rax %rax` is an idiom to set `%rax` to 0. The actual logic happens in `Data.ByteString.map_closure` which is jumped to via `jmp stg_ap_p_fast`. Strange enough, `map` is not inlined. Maybe `-g` overrides -O2`? Edit: Of course, `$ ghc -ddump-asm Mystery.hs` misses `-O2`. This is `ghc -ddump-asm Mystery.hs -O2`: ==================== Asm code ==================== .section .text .align 8 .align 8 .quad 4294967301 .quad 0 .quad 15 .globl Mystery.toLowerNative1_info Mystery.toLowerNative1_info: _c4n8: leaq -8(%rbp),%rax cmpq %r15,%rax jb _c4nf _c4ng: movq $block_c4n5_info,-8(%rbp) movq %r14,%rbx addq $-8,%rbp testb $7,%bl jne _c4n5 _c4n6: jmp *(%rbx) _c4nj: movq $16,904(%r13) jmp stg_gc_unpt_r1 .align 8 .quad 0 .quad 32 block_c4n5_info: _c4n5: addq $16,%r12 cmpq 856(%r13),%r12 ja _c4nj _c4ni: movq 7(%rbx),%rax orq $32,%rax movq $GHC.Word.W8#_con_info,-8(%r12) movq %rax,(%r12) leaq -7(%r12),%rbx addq $8,%rbp jmp *(%rbp) _c4nf: movl $Mystery.toLowerNative1_closure,%ebx jmp *-8(%r13) 
`useAsCStringLen` creates a copy for the `ByteString` so it’s not the `ByteString` itself that’s being mutated.
You can get good speed in haskell to, using unboxed stuff, vector, unrolled loops. Yes it looks ugly, but it's fast.
Doesn't get more intro-level than that
Yes, you are indeed making sense. The induction and recursion principles are logically equivalent. Making the induction principle precise in categorical terms takes a bit of work, but once you are there the proof is very straightforward. See for example def. 2.1, 2.2 and prop. 2.3 [here](https://ncatlab.org/nlab/show/inductive+type)
Perfect! Thanks for fixing my sloppiness. Your formulation makes a lot of sense and it's exactly what I was looking for.
Thank you! The equivalence of induction and recursion principles is a nice way to put it.
It's not "a personal issue with" Scala. His feeling appears to be that Scala is comprimised to the point of not being useful in the space and I don't think he is alone in that oppinion. Haskell has enough of a problem with bottom. We don't want a world where purity is never more than convention and we *should* be moving away from languages for which this is the case. If we wanted to do that we could juse use C++ and pretend.
Sadly collecting all error messages is impossible for monadic parsers. Think of this somewhat silly example: do typeTag &lt;- parseTypeTag if typeTag == "string" then TString &lt;$&gt; parseString else TInt &lt;$&gt; parseInt You can't give errors for parseString/parseInt if there is an error in parseTypeTag. That's the problem monadic parsing run into and to fix it we have to forbid parsers that depend on previous results. If we do that we end up with Applicative parsers though I don't know whether there is a decent applicative aeson variant. It might also be possible to let the compiler figure out data dependencies in your parser via ApplicativeDo and make error messages a best effort thing but I don't know of any parsing libraries that do this. Shouldn't be too hard to add on top of aeson, though. I also still feel a bit uncomfortable about the whole `ap != &lt;*&gt;` thing although I can't imagine how it'd cause problems in this case.
I don't care what every mainstream library does **today**. When someone finds a nice solution things can move very quickly so it's better to keep focusing on what's right even to the exclusion of what's "right now", IMO. As for serious GUI applications, I have my own solution to that that doesn't care what the mainstream GUI libraries want.
You should checkout eta-lang.org ;-) Just what you are describing!
I don't think that's true, FRP seems to be very promising. Have you tried http://elm-lang.org/ ?
This happens to me too. Intero eats huge amount of memory and hangs on somewhat big modules
&gt; Is the converse true, that a proof by induction can be rewritten in terms of initiality of [zero, succ] in a rich enough category? I think so. Do you have an example in mind where you don't think it works out? Your second question is a bit unclear for me to answer.
I agree with Syrak's comment. I also wanted to point out that `A` can also be a more fancy type such as an exponential object `Int^Int` which let's us prove things about functions such as `[Int] × Int -&gt; Int` as well. This is what happens in Graham Hutton's paper posted above for the `suml'` function.
Nice article, thanks for the link!