To be pedantic, characters in ASCII range are statically allocated, so their list cells only take three words.
I'm a huge fan of typed holes, but it sounds like most of the argument here boils down to "typed holes don't display typeclass constraints", which is fair criticism. Is this unfeasible to change?
Coming from Haskell, Elm feels very restrictive and very 'small'. When attempting to develop with it I felt forced into certain patterns I considered very ugly. Yesod doesn't do any Haskell to Javascript conversion, it's a server side web framework. It has templating languages that run Haskell server side to modify the served HTML/CSS/JS. There's a sample project integrating Fay with Yesod. Fay is similar to GHCJS, as it compiles Haskell to JS, but when I tried it many language features were not supported. Purescript is a very nice language from the bit I've done. Seems to even fix some of the legacy library issues in Haskell. It seems like it might be even slightly harder than Haskell for new users to grasp however. 
The entity component system tries to solve deep class hierarchies representing game objects. Instead of having several base classes representing the properties of a game object, a game object has a list of components representing these properties. When adding a new property to the game object then just its component list has to be extended, instead of having to add a new base class which doesn't scale that well.
Sounds a bit like dynamic aspect/mixin based programming.
I'd just like to thank you for these. I've played about a bit with reflex-dom and coming at it from that side was largely impenetrable. Working up from reflex itself is making a lot more sense.
That's true.
I use emacs `hi2-mode` and just indent to whatever level it seems to suggest when I press tab.
ClojureScript is quite nice, it is both quite similiar and very different from Haskell as well as having its own strengths and weaknesses, but it's a great fit for web dev stuff.
Thanks, didn't know about `find-module`! But what if I'm interested to know the origin of a type within some module? Let me clarify a bit. In the example I posted above there's no explicit `import Parser` line, so as a first time reader (and beginner) I have no clue on where it came from. In my `.cabal` file I have only `base` and `turtle` listed as a dependencies. So I know that probably `Parser` came from one of libs `base` or `turtle` depend on, but from which one? Which library docs should I google for?...
You're welcome! The DOM stuff starts in the next post, so hopefully that one is helpful to you. I just hope I get through this series before people get sick of all of these posts :) 
I believe they are only replaced by the statically allocated chars at GC time so you're still increasing memory pressure during allocation, but you're right they don't increase residency long term.
Hi, this must be asked a lot, but I've been doing Haskell for about 6 months on and off and just have no idea how to get exposure to Haskell in a more real world way. I'm trying to get an internship/grad job but I feel as though I'm not ready for industry, am I overthinking it or is there some way I can prepare myself? Thanks,
Does it mean that 'bytes maximum residency' is the value that `-K` GHC runtime flag has effect on?
Yeah, Slick solves the problem of managing life cycles as well.
Care to give a quick snippet on how Slick solves for life-cycles? I'm too lazy :)
Naaa ... it's much simpler ... on the simplest level it's just a bunch of key:value stores. When you "create" an entity instead of creating a object/struct you initialize the corresponding values in the stores. Eg: a player object has values in the *position, velocity and health* stores. The key is probably an integer id. So handing around an entity is just handing around an integer. The interesting part is how to make this type safe. Haven't looked at `apecs` yet, but I would figure a *newtype* that is annotated with a set of *HasPosition/Velocity/Healty* on the type level would work somehow.
But isn't that just about the storage aspect of ECS systems? To me, the component is a combination of both the data and code, not just the data. I'm not talking specifically Haskell, but ESC systems in general. Components can be fairly complicated, though the preference of course is composition of more granular components.
Thanks. That's good to know.
&gt; I guess I installed GHCJS through Vagrant but now I don't know what to sign in as, or where to go from there. If that's supposedly the best choice to come up with JS code from Haskell, they sure make it hard to install, and it totally isn't obvious how to get started. May I request you to please post your experience to this thread: https://www.reddit.com/r/haskell/comments/6z8e3m/benchmarks_ghcjs_reflex_miso_purescript_thermite/dmueqye/ The last time I brought up GHCJS tooling on this sub-reddit, it just _seemed_ that people were getting unnecessarily defensive about it. It's a real issue, and it needs to be acknowledged first, for it to eventually get fixed. Are you planning to use Haskell in a large team or a small team? Is this serious production use or internal-tools/side-projects? How is your risk-profile when it comes to your technology stack? Here are my recommendations: * If you are planning to use Haskell in a large team then your risk-profile **should** be low. You're setting yourself up for failure if you are assuming that the entire team, across the entire stack, can adopt Haskell. Go for Haskell in the backend and Purescript / Typescript / Elm in the frontend. If you wish to get a big repository of well-designed UI components then Angular2+TS is your only real choice. Anything else will burn your candle at both ends -- you'll be glueing libraries in the backend (Haskell) as well as the frontend. Do NOT use GHCJS in a high-risk environment. You have been warned. * If you are in a low-risk environment, then please try GHCJS (frontend) and Haskell in the backend. You can also use Purescript in the frontend, as well, with one of the existing purescript bridge libraries (they generate the Purescript data-structures based on your Haskell data structures). Again, since this is a low-risk environment, time spent on glueing together libraries or figuring out UI widgets, may be acceptable. * If you are deciding between Haste and GHCJS, then I'm assuming that you are willing to take risks. Go for GHCJS. You'll struggle with the tooling, but the payoff will be better. Hope that helps.
What? Don't applicative functors correspond to lax monoidal functors with a strength? Lax monoidal functors are almost as basic to category theory as functors (they are the right functors between monoidal categories) and the strength is what makes things "internal" as in you can define it inside Haskell because it respects the context.
I mean if your main type is meant to be used to do reporting is fine, but if you have type, used by lots of function and you have to make it parametric for report sake, you are adding lots of noise you all the "normal" function. For example I have a "payroll" type app, which allows among other things, the user to enter and display staff shifts. The main type would be something like data Shift = Shift { employe :: String , stype :: ShiftType -- Holiday or Work , date :: Day , duration :: Int -- in hour , cost :: Double } That's the main type handy to process timesheet and displaying them. However, I also need of every week , to aggregate shift by employees (regardless of the day), or by day (regardless of the employee) etc ... So I end up with this type instead, index by a key. data Shift k = Shift { { key :: k , duration :: Int , cost :: Double } A normal shift would be `Shift (Employe, Day, ShiftType)`, the day summary `(Day, [Shift Employe]` and the employe summary `(Employe, [Shift Day])`. This approach is actually pretty neat. It allows me to define a SemiGroup instance `SemiGroup k =&gt; SemiGroup (Shift k)`. Ok, `Employe` or `Day` is not a SemiGroup`, but `First Employe` is ... There is a lot of way of getting a `Shift` parametric. Mine lose the information about , employe, type and date, which are actually the main part of the shift. I could have done instead somehting like data Shift fe ft fd = Shift { employe :: fe Employe , stype :: ft ShiftType , date :: fd Day , durtiaon :: Int , cost :: Double } In this is example, it's not a problem because the main purpose of the `Shift` data types is to be aggregated. But let's say now, I want to save each shift to a DB, implement a scheduler which decide of people next shifts, my scheduler my actually be cleaner to use the original type, instead of the complicated one. In that sort of scenario, I found it easier to have different types specialised for different uses and converter between them, rather having a big (too) general types for all. 
I had a look at Ermine a few year ago, and my understanding was, that ii wasn't really meant be used (unless really good reason), and really maintained. I might be wrong.
Hi @fpsenpai - What are you doing to prepare now? I've been trying to learn Haskell for about the same amount of time, and if you look at some of my posts on this thread, I still can't contribute to issues marked as "beginner". 
In my experience of learning Purescript and Haskell at the same time, Haskell has far more learning resources and better documentation (perhaps just because you'll be using older/more widely used libraries, which will tend to be well documented). Purescript, however, seems like a more straightforward language (no language pragmas, no TH, less advanced type level programming).
&gt; Nothing against Elm but I might just avoid that one as I get the feeling that too many things are missing from it comparing to Haskell. That leaves you with PureScript of GHCJS. &gt; The goal is to just try and put Haskell to practical use, having some background in web development and everything. For that you do not need a compile-to-JS language. :) &gt; I'm aware of Yesod as well, and I see that's more than just Haskell -&gt; JS. (web development in general I suppose) Yesod is bunch of libraries and tools (a framework) for server-side web dev't. &gt; I guess I installed GHCJS through Vagrant but now I don't know what to sign in as, or where to go from there [...], they sure make it hard to install, and it totally isn't obvious how to get started. &gt; If that's supposedly the best choice to come up with JS code from Haskell Currently yes. I'd say Fay is the 2nd, Haste the third and there are some runner ups as well. Best is to focus on GHCJS now, as it has the momentum. &gt; If PureScript is "almost" Haskell, I suppose that's a good option too? Very good option (maybe the best considering your requirements) as it compiles to more readable and much more compact JS. PureScript as a language is in many ways very close to Haskell, but in some very specific ways it is different from Haskell in order to be more close to JS (like evaluating strictly instead of lazily, and some code data types). To answer your original question: &gt; Elm, Haste, PureScript, GHCJS, etc. Are there any of these you would NOT recommend? Personally I'd not recommend only Haste, mainly because the community is much smaller and it ties in a lot with how you construct your app.
Haste seems to be a limited version of GHCJS as of now. I am not sure how much development it gets these days. If you have much experience with Haskell, Elm is pretty painful - no typeclasses or really any way to abstract along data types. It hurts on bigger projects. &gt; I guess I installed GHCJS through Vagrant but now I don't know what to sign in as, or where to go from there. If that's supposedly the best choice to come up with JS code from Haskel You'll probably want to use Nix if you are working with GHCJS.
IMO the component should not contain any code. That is what systems are for. 
In fact, loading this file in ghci gives _two_ errors. The first error is precisely the typeclass constraint: • Could not deduce (Show a0) arising from a use of ‘show’ from the context: Show a bound by the type signature for: pleaseShow :: forall a. Show a =&gt; Bool -&gt; a -&gt; Maybe String at hole.hs:1:1-49 • In the first argument of ‘Just’, namely ‘(show _a)’ In the expression: Just (show _a) In an equation for ‘pleaseShow’: pleaseShow True a = Just (show _a) | 3 | pleaseShow True a = Just (show _a) | ^^^^^^^ 
Indeed, this blog post really ought to be a bug report, so I've taken the liberty of reporting this as [Trac #14273](https://ghc.haskell.org/trac/ghc/ticket/14273). (In fact, the typed hole suggestions in GHC HEAD are even worse, since they suggest "valid" substitutions that have the wrong type, such as `(-&gt;)`, which doesn't have a `Show` instance.)
I would actually say it would be quite helpful. If you are not 100% sure what a function does or are just curious how it is implemented getting a look at its definition is often very helpful. Maybe instead of making it magically appear in place of a failing Show instance (which I agree has issues) have it appear when you type `:code` or something. That way it can also work on things that aren't functions (but still may often not be showable).
criterion on ECS: https://medium.com/@Improbableio/the-future-of-the-game-engine-564c7ec07e92 &gt; The limitation of this approach comes from the fact that all these systems run on a single server with limited capacity; each system is competing for cycles with every other system.
Elm cripples you enough that it can't really be placed in a class with the rest of the alternatives presented here. Haste is still there. And has been used to create a couple of websites [*cough*](http://cokmett.github.io/cokmett/), but is pretty limited. That really only leaves PureScript vs. GHCJS. as the two serious contenders I am left with when considering doing browser-side development in a Haskell-like setting. PureScript gets you much closer to JavaScript, which is both good and bad. It makes smaller bundles of code, but you can't share the code with the rest of your Haskell code base on the server side. This is usually the trade-off I'm willing to accept when doing web development. GHCJS gives you the full unabridged power of ghc in your browser. All of the finalizer support is there, all of the garbage collection tricks, everything. It is considerably heavier to ship than a typical purescript front-end, but you get full code reuse between the server and client side of your application if you need it. If I'm writing a thin application layer, I go with PureScript. If I'm writing a full blown web-application and need proper laziness and all the bells and whistles or want to recycle any existing code I go with GHCJS when the time spent sending the application is small in proportion to the total amount of time the user is going to spend on the page.
Well the easy way is to ask GHC (i): % ghci dimsuz.hs ... *Main&gt; :i Parser type role Parser nominal data Parser a where ... -- Defined in ‘Options.Applicative.Types’ Notice the **Defined in** line. Then you can refer to my above point about ghc-pkg: % ghc-pkg find-module Options.Applicative.Types ... /Users/tommd/.ghc/x86_64-darwin-8.2.1/package.conf.d optparse-applicative-0.14.0.0 The fancy way is to ask GHC (-mod) or `hdevtools` using `syntastic` or `neco-ghc` plugins (for vim, nvim) or `flycheck` (emacs) or whatever your editor of choice supports. In this manner you can move your cursor to the type of interest and hit a key to get the type and defining module in an info box/pane/what-have-you.
You can sort of do that now by presenting an interface that forces the key/value pairs to be ordered. It's not pretty or correct-by-construction but it does work.
Right, you're correct. I've noticed there's two interpretations to ECS, that's the more classical one. However I often see it being approach in an object-oriented fashion, like with Unity, where rather than "systems" behavior is attached to the component itself.
I've seen several libraries that take that approach. They certainly work as long as you don't need to do anything too fancy over records that are polymorphic. The fact that they aren't correct by construction prohibits certain kinds of proofs. For example, if I know that `x` is an element of `xs`, I should also know that, for all `ys`, `x` is an element of `Union xs ys`. It would also be nice to be able to prove that union is commutative and associative.
Really like the what you have done with this library
Unless I'm mistaken the article is basically proposing replacing systems with multiple parallel workers that run across a subset of components. Which someone could totally do in a similar style to the `apecs` library
I agree that GHCJS itself has very poor tooling. However, *if* your framework supports jsaddle, then this problem *almost* vanishes because you don't actually need GHCJS tooling. You just use GHC for everything except the final build. This doesn't solve the original problem, but it conveniently avoids it. Reflex-DOM supports this workflow but I cannot speak to any of the others. Because of this I think it's not accurate to say that *all* GHCJS solutions are equally bad because of tooling. That's really just not true.
Thanks for the comment and definitely thank you for the Accelerate package. Right now I am in the process of installing Haskell into my home directory in the supercomputer cluster. Once I find the time, I'll figure out with my primes example which one of the three potential solutions (parallel, repa or accelerate) I can get working easily. I'll report back.
Yes, laziness can make recurrence relations very elegant. fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
Yes. Haskell has very good support for recursion. I would have to have more details before I could be of much help, though I will note you *probably* don't want a tuple to hold a binary value.
I'd recommend any of them over writing JS directly. There similarly to Haskell goes: GHCJS &gt; Haste &gt; PureScript &gt; Elm.
I imagine that in Haskell the classical approach would work better. Of course helper functions that work with specific components in specific useful way (e.g adding a position and a velocity with some custom operator) could definitely be very useful and could sort of be thought of as indirectly adding behavior to a component.
I agree typed hole are a bit confusing, however, I usually use typed hole, not to get the type of the hole but the type of all the surrounding bindings. I would definitely recommend using holes to beginner for this reason. 
No idea how to make `apecs` parallel. When `cmap` or `modify`, it is a `System w ` monad. `forkIO` cannot work on that. Another side, `Store` is using `IORef` other than `MVar`
I'm renewing my question about possible implementation of `awaitWithTimeout` in Conduit. /u/dukerutledge sugested, that I should to look at `System.Timeout.timeout` from `base`. Because it should work not in `IO`, but in `ConduitM`, I need lifted version of `timeout`. There is one in `lifted-base`, but it requires `ConduitM` to implement `MonadBaseControl IO`, and `ConduitM` doesn't implement it. Is there a deeper reason for this omission? Is there any other way?
No, I think the `-K` flag changes the stack size, i.e. allowing for more (or less) recursion (don't take my word on this I don't know a whole lot about this). I think the `-M` option changes the maximum heap size. The maximum residency means the maximum amount of memory that is allocated at the same time. The functional and immutable by default nature of haskell makes it so that it allocates and deallocates a lot of memory during the execution of the program. The profiling info is telling you that it allocates about 4GB in total, but only about 77MB at the same time, so the program could be run with only 77MB, because that is the maximum amount of memory it needs at the same time.
Yeah I'm basically at the same point as you, going through various parts of the haskell book. I really want to start building something but I feel as though I need to know more in the way of application of concepts like monads, monad transformers and all that kind of stuff. 
I have now tried it and it doens't quite run with `-M77MB`, but it does work with `-M100MB`.
Forking could be done with something like forkSystem :: System w () -&gt; System w () forkSystem sys = do w &lt;- System ask void . liftIO . forkIO $ runSystem sys w This will be susceptible to race conditions of course, but it might still be useful for spawning a separate render thread. The reason parallelism isn't in the API yet is not that it is impossible, in fact, it used to be there, and I hope to add it back in once I figure out a useful way to do it. In earlier versions of apecs, you could define stores in the STM monad, but what I found was that unless your game logic is much more expensive than storage logic, this would not actually make your game faster because of the STM overhead. My current goal is to introduce a family of maps that will automatically be ran in parallel, e.g. `pmap stepVelocity`. I like this approach as it requires little overhead, is easy to use, and fits with the rest of the design.
[Pandoc filters?](http://pandoc.org/scripting.html)
&gt; Say I want to add a new tag, and I just want it to put something in italics, I'd have to add this trivial tag in to my Haskell program and recompile everything. You don't have to necessarily *recompile everything*. In addition you get better performance and type checking. However, you won't be able to easily generate a mapping between function names and functions. I'm not sure what the exact use-case is: * Are users programmers with access to the source code? * How often is the markup extended? * Can you name other typical markup examples? A (simple) DSL seems to be better suited (for your simple example): rule first-use: html: "&lt;em&gt;" "&lt;/em&gt;" tex: "\\emph{" "}"
I'm a bit of a Purescript neophyte, but from what I've done I didn't notice too much type level programming missing compared to Haskell (without language extensions). It's probably hard for me to judge as well, having done quite a bit of Haskell before ever looking at Purescript. I do think Haskell is still the 'bigger' language, at least in the sense of there being more facets to learn. But unless you're digging around in some libraries, I think you can avoid quite a bit of it for a long time. I'm still unfamiliar with TH internals, as well as many language pragmas, but still feel like I'm a capable Haskell developer. Laziness seems to also increase Haskell's complexity compared to Purescript, at least in terms being able to write efficient/practical code. Going in the other direction however, I think Purescript's granular effect types, and the record typing, can make Purescript's types harder to follow. I think the documentation and learning resources might make the biggest difference. Another thing along that line is 'feedback' from issues, type errors, etc. I think Haskell might be better here, but again I'm more used to GHC's output than Purescript's. I'm curious what your experience learning both at the same time was? Did one seem easier to you?
Okay, it's pretty cool, but I just couldn't help myself from thinking this while reading. &gt; Check out this commuting diagram! &gt; *draws a tree*
Well, that's quite interesting, because there are a lot of profiling guides mentioning only `-K` and doesn't cover `-M` at all.
Uuuh, didnt know how to draw one properly with LaTeX without packages apart from AMScd, so that's my personal fault! \^_\^"
I think AMScd is fine. It's just weird to call a diagram with only one path a "commuting diagram", since there's nothing to commute with. Commuting diagrams say some interesting, nontrivial thing only when there are two different paths that both start at the same point and both end at the same point. Of course it's perfectly good as an architecture diagram, and may say something interesting and nontrivial in that context!
I struggled with different things in each. Purescript's effect types and row kinds have bamboozled me a few times. Haskell has provided me with more moments where I don't understand the code that I'm writing but it works so whatever. My experience is writing web front ends in purescript and web back ends in haskell, so it's different domains I guess, but I feel like there's less to learn in purescript to become productive. I was able to write a mobile app (using react native) without fully understanding monad transformers, for example. Writing a back end in yesod and persistent, I had to get to grips with type families, and constraints using ~ (I'm not sure what they're called). Which is no bad thing, I don't have a problem with having to learn stuff :) To conclude a rambling answer I would say I didn't find one easier than the other. 
Oh, perfect, thanks! Turns out I was missing the key. Thanks a ton for doing these tutorials and putting so much work into them. I tried to get into reflex before but was stumped frequently about how to structure things or how to find the correct combinators. So this series is really helpful!
&gt; Check out this tree! &gt; *draws a list*
&gt; Check out this list! &gt; &gt; *Draws the nil constructor*
Wonderful! Thank you for the hints, I will make use of them! 
Take a look at the following functions from [`Data.List`](http://hackage.haskell.org/package/base-4.10.0.0/docs/Data-List.html): iterate :: (a -&gt; a) -&gt; a -&gt; [a] unfoldr :: (b -&gt; Maybe (a, b)) -&gt; b -&gt; [a] Basically, `iterate` always generates an infinite list, and `unfoldr` may generate an infinite list. Note that `unfoldr` is more powerful since you could actually define `iterate` with it.
Well, *technically* it still commutes... https://m.youtube.com/watch?v=hou0lU8WMgo
Great, thanks! :)
Thanks. Get lots of inspiration from your package. Nice work. Agree. In game design, it is suggested to separate render clock and physics update clock. As for concurrency or parallel, I think `Strore s = MVar (IntMap MVar s)` is probably a good choice. (MVar is faster than STM).
The problem with Elm isn't that it's "small" *per se*—you could have a small language that feels just like Haskell and, if anything, Haskell is too bloated for its own good. Rather, Elm isn't expressive enough to *grow*, which means you can't build the abstractions you want. In particular, I think we absolutely *can* have a language as *small* and simple as Elm while being similarly expressive to Haskell. In fact, I think there's a language like that inside Haskell, just smothered in 20 years of legacy design and language extensions. Guy Steele has a wonderful talk about this idea: "[Growing a Language](https://www.youtube.com/watch?v=_ahvzDzKdB0)". I highly recommend watching it—it really changed my understanding of programming language design.
But that's also true of LISP. Not much of a reason we can't print source when we have it. That said, I'd like it to be a separate command, rather than tacked onto `:info` - we usually have short functions, but there are exceptions.
Yeah, I think the guides mention limiting the stack size to trace down excessive recursion. The ideal case is when ghc optimizes all recursion away and just produces a loop from originally recursive code (which means the stack can stay very small). To make sure you have achieved this closed loop you can limit the stack size (which limits how "deep" you can go into functions). Limiting the actual memory residency (the amount of memory allocated at one instant in time) with the `-M` flag only gives guarantees about the memory residency, but doesn't tell you anything about the way in which the code is executed. Instead of using the `-M` flag to limit memory residency, you could just look at the output of the profiling information (in your case: `76,758,856 bytes maximum residency (12 sample(s))`) to determine how much memory is used. So I think that's why the `-M` flag is not covered in profiling guides (but again I don't know a lot about this topic and most of what I posted in this thread is just my personal interpretation of information I found on the internet). One other advice I can give you when searching for space leaks ([from my own experience of searching for a space leak in Yi](https://github.com/yi-editor/yi/issues/105)) is to use the heap profiler: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html#profiling-memory-usage.
No worries at all :) Thanks for sticking by it while I've ironed out the kinks. I was going to do the exercises as a second pass so that I had time to really polish the process, but I got excited and went on a blitz with them at the last minute - I'm glad I haven't scared too many people away :) 
I just tried haskell-do for the first time. The UI didn't work -- apparently the "static" directory must also be in my $PATH -- can it be named `haskell-do-static` or something? It's polluting the `$PATH` and I won't remember what this `static` thing is later. Also: I can't seem to open projects, I get silent failures when I try to open a project, no error message, and later attempts also fail to navigate the directory tree, and after 5 minutes of frantically trying to click things with no feedback I gave up :-( EDIT: Apparently it can only open empty directories.
`react-hs` also supports this workflow, although it uses `ghcjs-base-stub` instead of `jsaddle`.
&gt; You're setting yourself up for failure if you are assuming that the entire team, across the entire stack, can adopt Haskell. It seems totally nonsensical to say this then follow up with suggesting PureScript. Particularly since Haskell actually has an easier learning curve than PureScript, largely due to how much more learning material there is. How could it possibly be easier to get half the team to learn Haskell and half purescript rather than just getting the whole team to learn Haskell.
What does GHC infer if you give partial type signatures? broken :: _ loop :: _
If you want to keep all of the Haskell GHC extensions, but also have some structure (like the Elm arch.), I recommend you check out [miso](https://haskell-miso.org). It is built on GHCJS, you can use nix / stack / cabal and it's easy to get started with. There are also lots of examples for learning too. https://github.com/haskell-miso/miso. Basically, it abstracts all of the dirty details of DOM manipulation, and you just focus on mutating your application state and declarative templating. 
I wrote following code to read configuration from environment variables: #!/usr/bin/env stack -- stack --install-ghc runghc {-# LANGUAGE OverloadedStrings #-} import System.Environment (lookupEnv) data Config = Config {name:: String, age:: String } deriving (Show) getConfig :: IO (Maybe Config) getConfig = do maybeName &lt;- lookupEnv "NAME" maybeAge &lt;- lookupEnv "AGE" return (Config &lt;$&gt; maybeName &lt;*&gt; maybeAge) main = do maybeConf &lt;- getConfig putStrLn (show maybeConf) But something is bothering me with this implementation of getConfig: lookupEnv function returns *IO Maybe String*. Since I read multiple configuration params, I have collection of *IO Maybe String* values in my hand and I want to convert multiple *IO Maybe String*s into one *IO Maybe Config* type variable. My implementation opens up all IO's and puts results into a Config type and wraps it up with IO at the end. But it feels like this can be done without opening up the IO results. Is there a better way to implement getConfig function here? Thanks.
Whatever I have loop typed as is what it gave. (That's how I got that type, cause I couldn't remember the params to the Window type) I will double check broken when I get back home but I believe it typed as Num t =&gt; t. EDIT: broken :: _ GHC gives Integer as the type (not Num t =&gt; t, which I initially thought I remembered it being). Manually giving an Integer type annotation, however, causes the program to compile, whereas leaving the annotation off causes it to fail to compile... despite the annotation being the same type GHC thinks it is.
It's the first time I see "DSEL" instead of "EDSL". Since your example is clearly not using Haskell syntax, I'm guessing that while the "E" in "EDSL" usually means "embedded", the "E" in "DSEL" means "external"? What a confusing pair of acronyms!
Even though it's not on your list I would recommend NOT using Fay. Fay is nice on its own but sharing code, even though possible, has proven being a nightmare.
Rust solves problems related to memory management using the type system. Java, Haskell, and other languages solve that problem using garbage collection, which trades some amount of performance for memory safety. Rust has a lot of Haskell inspired feature which give it many of the same safety guarantees as Haskell, likely more than Java. 
Interesting. Here's a minimized test case (that doesn't need an extension). I think it has to do with the monomorphism restriction, but I'm not sure of the details, nor how `TypeFamilies` interferes with defaulting. import Control.Monad.ST main = runST loop loop = fixST broken broken = return 
If by *opening up all IO's* you mean binding their results to names, you could instead write import Control.Applicative (liftA2) getConfig :: IO (Maybe Config) getConfig = liftA2 Config &lt;$&gt; lookupEnv "NAME" &lt;*&gt; lookupEnv "AGE" where `liftA2` is defined as liftA2 :: Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c liftA2 f x y = f &lt;$&gt; x &lt;*&gt; y 
Interesting... (also, I'm presuming w = loop and y = broken?) Not sure why yours always breaks, but mine only breaks if I throw in that reference to the global definition (which, presumable, doesn't change the type of loop)
Rust is what you get when you design a language to have the same safety features as ML or Haskell, but insist on needing no garbage collection or any other fixed runtime 'framework', and on enabling compilation to efficient C-like code. Rust safety features are indeed subtly different, but in principle nothing stops Rust from protecting against the very same logic bugs as Haskell does. However, achieving that in practice requires quite a bit of careful design work, if the language is to avoid dependence on a single, large runtime.
That sounds awfully like an explanation of do notation, not monads.
It's actually *embedded*, but the E was not intended. :D
Oops, I tried to change the variables back to make it look like yours.
Do notation is just some syntactic sugar to make interacting with monads less clunky. It's not much different than having, say, `s1 + s2` translate to `s1.__add__(s2)` in Python. The _problem_ with do-notation is that it hides a bit too much because you can't see the bind operator, hence why mentioning ';' is useful, even if ';' is more akin to '&gt;&gt;' than '&gt;&gt;='. The idea is to show that monads hide in plain sight.
Gotchya, thanks for elaborating. Unfortunately, I think I disagree with pretty much all your points=( Part of the disagreement is based on something that you just mentioned here: " ';' is more akin to '&gt;&gt;' than '&gt;&gt;=' ". If the programmable semicolon analogy is so broad it can cover both monoids and monads, then how much is it really explaining to the reader? They may think they know monads after they feel like they get the analogy, but what they've really learned is typeclasses under a different name. When they actually run into a problem that requires understanding monads specifically they'll realize they still don't get it. &gt; It gets my goat that people bog down what's a relatively simple concept ('imagine if the semicolon in C were an operator, just like the arithmetic, &amp;c., operators, and you could determine how it behaved based on the type of the expression, just like how '+' can mean addition or concatenation depending on the types') with stupid bloody burrito and spacesuit metaphors. I disagree that monads are a simple concept. In addition I think "programmable semicolons" is just as obfuscating as burritos and spaceships -- all are 'tricks' meant to shortcut the following, essential work involved in learning every typeclass: getting familiar with a few different concrete instantiations of that typeclass and then working through how the typeclass generalizes the commonality between them. I'm open to being wrong about the usefulness of the programming semicolon analogy so I may end up taking all of this back, but that's how it looks to me at the moment.
&gt;I haven't fully wrapped my mind around these different kinds of safety. I just sense that they're there. It may even be the case that Haskell safety encompasses Rust safety in some ways, although I'm not certain. Yes and no. Haskell is safer than Rust, but Haskell does not give you fine-grained control of memory management the way Rust does. The big thing in Haskell is that you can separate pure code and impure code pretty easily. That allows you to write pure code, reason about it, property test it, and still have a useful program at the end of the day. Rust has *no* ways of separating pure and impure computations, so you lose many of those guarantees.
&gt;Rust safety features are indeed subtly different, but in principle nothing stops Rust from protecting against the very same logic bugs as Haskell does. Haskell offers purity guarantees that Rust does not.
&gt;Rust has a lot of Haskell inspired feature which give it many of the same safety guarantees as Haskell, likely more than Java. This is true, but I think it understates the difference.
Do you have an example of bugs that are prevented by separating pure vs impure code, that wouldn't otherwise be caught by a basic linter? 
I'm sure the OP understood my point. My comment had enough context for him/her to overlook minor inconsistencies and still get the gist. I'll pass on this unnecessary bait. 
I'm sorry I disagree. I just hopped over to Reflex's Github issues just to validate if I was wrong and if, Reflex, in fact had everything under control, but nope. Here it is https://github.com/reflex-frp/reflex-dom/issues/105 - just a random memory spike being faced by someone in the wild due to TH not working well with GHCJS. (I hope I don't get follow-up comments how "I stay away from TH because it is anyways a bad idea") Unless GHCJS is given first-class citizen status and adopted by the core team, it will remain high-risk. Edit: * https://github.com/reflex-frp/reflex/issues/63 - another one, but I'm not sure if it's a GHCJS issue or a Reflex issue * https://github.com/reflex-frp/reflex-dom-contrib/issues/42
&gt;Do you have an example of bugs that are prevented by separating pure vs impure code Not off the top of my head. I will say that Haskell has forced me to organize errors in my applications much, much better. The overhead for error types in Rust is such that you just want to do `eprintln!("Error"); exit(0x001)` sometimes. In Haskell I just use a `(MonadError ErrorType m) =&gt; m a`. It's not that Haskell code is inherently better, it's that Haskell makes doing the right thing easiest, so you don't end up introducing sloppy solutions into the codebase. In my day-to-day work I use purity most obviously for better tests. You can test a lot more by passing data structures and checking those instead of saying "okay at this step, this variable should be that, and _that_ variable should be _this_". Debugging becomes massively easier too - any error in a pure function is trivial to reproduce. Pure functions won't ever do things like [start dwartf fortress during system upgrades](https://askubuntu.com/questions/938606/dwarf-fortress-starting-during-apt-get-upgrade). Again, this is partly a question of Haskell encouraging you to think a certain way by making it the easiest thing to do, not Rust being inexpressive. I'm working on a compiler right now so I don't know how much this generalizes. What I *do* know is that complex manipulations of data structures without purity guarantees would be a disaster. Depending on your application, things might work out anyhow. &gt; that wouldn't otherwise be caught by a basic linter? Linters don't accomplish the same thing as a type system. They are weaker. No linter I have used for Rust or Python code accomplishes what you have described.
I'm not sure if react-hs is a good example in the context of production usage (reference: "Project Status" section in the Readme) * https://github.com/liqula/react-hs/issues/52 * https://github.com/liqula/react-hs/blob/master/README.md#project-status
If your risk profile allows you to go for GHCJS, I would also recommend Miso. By far the easiest framework to get productive with. 
The pure vs impure code is a red herring imho. In haskell, there's no such thing as impure code: everything, _including_ IO, is 100% pure (referentially transparent). Because of this, we can reason about Haskell code using a simple substitution model, and without having to use non-local reasoning. In other words, pure code is context insensitive, which implies greater composability (building big things by assembling smaller things), and compositionality (the behaviour of something is the sum of the behaviour of its components, without hidden interactions). I think less bugs come more from the ease of reasoning that referential transparency grants us, than from separating functions that return `IO` from functions that return any other type (which is still good practice, ofc). No linter can give you that.
Haha I know about the Project Status thing, they added that in response to one of my pull requests lol. And i'll look into that issue later, but I think it's just outdated at this point, there isn't a `react-hs-examples` module anymore.
Literally the only point made against GHCJS (at least in comparison to PureScript) in that entire paragraph is that single argument about having Haskell on both the front and the back end. So it's not just a minor inconsistency, your entire damn point against GHCJS for big teams is not valid. It wasn't bait, I am genuinely confused at to what exactly "the gist of your argument" IS.
This is a very abstract post... very little actionable content. &gt; But Rust code still has a lot of room for the type of bugs that Contorer alluded to. Can you elaborate on this? This is the most substantive statement in your entire post, but then you don't explain which bugs you _think_ Rust has room for, or anything else.
At the same time, Rust effectively disallows global, mutable state, unless you use either a synchronization mechanism or an `unsafe { }` block, so functions are naturally going to be pure, much to the chagrin of C and C++ users trying to get used to Rust, where they just want to throw stuff in a global.
For composing functors, there's [`Data.Functor.Compose`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Compose.html), as seen [here](https://www.reddit.com/r/haskelltil/comments/31exq0/datafunctorcompose_helps_with_nested_functors/), and there's an Applicative instance for composed Applicatives. import Data.Functor.Compose getConfig = getCompose $ Config &lt;$&gt; name &lt;*&gt; age where name = Compose $ lookup "NAME" age = Compose $ lookup "AGE" Edit: It looks like you don't need the `applicative-extras` to do this, as I originally wrote.
Okay, I bite. May I ask if you know of any large team using GHCJS and GHC to build a large end-customer facing product (not internal tools)? I'll happily change my opinion once I hear some success stories at scale. And if you didn't really understand, my poins is, that both, the GHC and the GHCJS ecosystem aren't "well packaged". The latter being worse off. For large teams, who have a lower risk profile, this means struggling with various issues at both ends (front-end and backend). This is unacceptable in most low risk environments. If the team is constantly running into issues that they otherwise don't have to face in any other popular language, the chances of acceptability are much lower. It is imperative to demonstrate small successes, fast. To be able to do that, I recommend foregoing Haskell on the frontend, because it has greater issues than Haskell in the backend.
I mean GHC on the backend is relatively irrelevant, since you suggested GHC+PureScript as a replacement for GHC+GHCJS. And I have personally not had any packaging issues with GHC, deploying to production does not really seem difficult. So in order for your argument to be valid PureScript needs to be "packaged" much better than GHCJS, and thus much lower risk, is that even true?
I am not in the habit of arguing for the sake of arguing. I'll stop now. I trust you have built large critical systems with GHCJS to defend it so vehemently and to attack anyone who says anything remotely negative about it. Do share you success story and disprove the naysayers. 
&gt;(MVar is faster than STM). It's worth benchmarking this. Sometimes it isn't: https://mail.haskell.org/pipermail/haskell-cafe/2016-January/122785.html https://www.reddit.com/r/haskell/comments/6irosh/the_haskell_concurrency_primitive_shootout/
The first snippet of code doesn't work. You can't divide `Int`s.
Nice catch! Fixed.
&gt; For that you do not need a compile-to-JS language. :) Since JS is something I can already 'relate' to, with it being a typical programming language that's highly associated with web development (not that I know it well), and with node's large ecosystem, I should be able to come up with project ideas more easily as I learn pure functional programming. 
TL;DR Rust is what you get when you design a systems language without putting your head in the sand.
So you are going to not refute my point, and make a ridiculous requirement of me to disprove the point you have not made? Cmon man, you're being very silly. I trust you have built large critical systems with PureScript to attack GHCJS so vehemently and to flee when anyone points out you really don't have a concrete reason to pick it over GHCJS. Do share your success story and disprove the naysayers.
I've worked with GHCJS and Reflex as my day job for over a year now, and that's some serious cherry picking you've done. &gt; I just hopped over to Reflex's Github issues just to validate if I was wrong and if, Reflex, in fact had everything under control, but nope. Here it is https://github.com/reflex-frp/reflex-dom/issues/105 If you have to go back almost a whole year to find an issue worth mentioning, I'd say that's fairly good. You can't just link to a bug and claim it's out of control, as though no other projects out there have bugs. Plus, it's not even a serious issue. TH in GHCJS is slow, but I've never experienced show-stopping problems with it. The issue you linked is an outlier. The second issue you linked is definitely a Reflex issue, not a GHCJS issue; and it's over a year old, so it's probably out of date and potentially no longer true. The last one is (I believe) merely a version bounds issue which hasn't been resolved because `reflex-dom-contrib` is an experimental library that is not actively maintained and should be considered significantly less stable than Reflex. Finally, you have not addressed your parent commenter's point, which is that you can just use GHC to get an environment with great tooling. I will agree with you that using GHCJS has some problems, but I would not say that those problems have been anywhere near significant enough to justify not using it. And once you start using the GHC workflow, it becomes a really fantastic development cycle.
Applicatives are lax monoidal functors with strength. [They're also monoids in the category of endofunctors under Day. Or lax closed.](https://bartoszmilewski.com/2017/02/06/applicative-functors/) Or, like, a bunch of other definitions. All these definitions mean different things, and are incompatible in the general case, but converge to the same thing when you restrict them to `Set`/`Hask`. "Applicative functor" was designed in terms of `Hask`, and it has been retrofitted to category theory, where there is still no consensus on a precise definition. "Lax monoidal functor with strength" is a kind of awkward, oddly specific definition. The `Day` definition is probably the most satisfying one though. Point being, I would definitely not say that Applicative was an advance in categorical logic; it was an engineering advancement that then needed to be explained by existing categorical logic.
I mean assume you can interact with standard input and output anywhere, and access the internet and random number generators and so on. Not to mention mutating variables that are passed in still isn't pure, even if it is nicer to work with than global mutables. So I'm not sure I fully agree unless I am missing something.
`-XTypeFamilies` [implies](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#ghc-flag--XTypeFamilies) `-XMonoLocalBinds`. (So does `-XGADTs`, by the way.) You can disable it with `-XNoMonoLocalBinds`, if you want. That should fix the type errors that you're getting, but at the cost of making type inference [less predictable](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#ghc-flag--XMonoLocalBinds).
Haha, shall I go get some issues off Angular's repo? :P I'm sure I'll find some. GHCJS is actually being used in production for large-scale systems. And not a few! (Takt, Obsidian, and I know of others). Now...I'm not going to claim that GHCJS is *low-risk* in the same way that Java is *low-risk*. There are plenty of solutions that have a lot more miles. But I'm just saying that GHCJS is not a bomb planted in your app. It's pretty darn good for being "high" risk.
I'm convinced that *almost* every use of a mutable global, ever, has a bug lying around somewhere. Purity forces you to bump into mutation very fast and deal with it. But "global" is just the largest scope in a program. You can take this principle and start moving the scope down and you will still find bugs (for example, a mutable field on an object).
All concurrency bugs, I think. I have a complicated system with a lot of threads, but since the majority of it is pure, there should be no concurrency bugs in there, and there never have been. Meanwhile, a similarly multithreaded Java program I worked on was just full of deadlocks, and of course a complicated and ever-changing locking scheme. Also all exception-unsafe bugs. I can freely modify StateT style state, and when an exception happens the changes never happen, so it's like transactional memory for free. Of course the Java program would frequently get in a weird state after an exception. Also unexpected dependency bugs. The Java program I worked on had a tendency to make innocuous looking functions load a config file over the network which would invariably fail in the middle of the night and who knew that thing was even there. And don't forget about all the brittle initialization order hacks. And it's not just IO vs. not IO distinctions, in Haskell I can ensure GUI calls happen in one thread because they are in a newtype wrapper around IO, and only the GUI thread can unwrap it. Naturally the Java program can't make any such distinction, so once more lots of deadlocks due to doing things on the wrong thread. Rust would surely do better than Java, but I assume it would fundamentally would have the same problems, unless you worked hard and had a strong design from the beginning that predicted and avoided those problems. In Haskell I just did things the "natural" way and avoided them before I had enough experience to predict them. Anyway, the list is endless, since it's the same as the "why referential transparency matters" list.
Is that because unfoldr is the anamorphism for lists? Are you sure you can't define unfoldr in terms of iterate?
&gt; Haha, shall I go get some issues off Angular's repo? :P I'm sure I'll find some. Most definitely you will! I can pick out some of my favourite ones if you'd like :P &gt; GHCJS is actually being used in production for large-scale systems. And not a few! (Takt, Obsidian, and I know of others). Thank you for countering with data. I am not against GHCJS, I am rooting for it. But I will not fawn over it till I'm confident about it. Our life would've become so much easier if GHCJS would've _felt_ like a safe choice. I have been searching for large-scale success stories but haven't found any. **If you are a GHCJS contributor, may I suggest having a page on the GHCJS repo which talks about success stories?** I just looked again, but didn't find any -- and I"m not saying this in a snarky way. Please link to it from the project README. It's important to make this information easily discoverable. Wrt Takt, do you know if one could play around with the product they've built with GHCJS? Is it publicly accessible under a freemium model, by any chance? Wrt [Obsidian](https://obsidian.systems), I'm _assuming_ you are referring to the work listed in the "Our work" section on their website [1]. I have tried tracking those three projects down (in the past), but have been able to find only one of them: * Redline - couldn't find online footprint * Prasava - couldn't find online footprint. * [Sephora reservations](https://www.sephora.com/reservations) -- the website currently isn't loading properly. It was done as part of [skedge.me](http://skedge.me) * [Bed bath &amp; beyond reservation widget](https://bedbath.skedge.me/widget2/?action=book&amp;storeId=1194&amp;preselectedServiceRef=1&amp;regFN=&amp;regLN=&amp;coregFN=&amp;coregLN=&amp;email=&amp;contactNum=&amp;registryId=&amp;eventDate=&amp;theme=1) and the related [JS file](https://bedbath.skedge.me/widget2/a1c8307ddbe923d2d81403940588d947caec3392b1541f535f102d1fd1851d81.js) Can you tell if this has been written in GHCJS? It _seems_ like hand-written JS to me, but I may be wrong. Would anyone at skedge.me be willing to confirm? [1] I'm sorry, but pages on their website are not directly linkable. This is **not a good sign** for someone being given as a reference for GHCJS adoption. 
I was just looking for something like this to build a portfolio tracking utility, thanks!
&gt; I've worked with GHCJS and Reflex as my day job for over a year now, and that's some serious cherry picking you've done. &gt; If you have to go back almost a whole year to find an issue worth mentioning, I'd say that's fairly good. It's unfortunate that it looks like I was cherry-picking to make GHCJS look bad. I just hopped down to their issue tracker and this issue was there on the [first page itself](https://www.dropbox.com/s/yq0g3oh7dq9t2h4/Screenshot%202017-09-26%2012.34.12.png?dl=0). I have not spent any time trying to cherry-pick stuff to prove my argument. In fact, I'm not gaining anything in trying to win this argument. I **want** GHCJS to get better. Much better. But it starts first with acknowledging the issues. &gt; Finally, you have not addressed your parent commenter's point, which is that you can just use GHC to get an environment with great tooling. I did not want to get sucked down that rabbit-hole. GHC tooling is not what I'd call _great_. It's barely _passable_, and GHCJS is worse (because it does not get as much attention as GHC). And we can agree to disagree here. I'm used to far better tooling than what is available in GHC currently. &gt; I've worked with GHCJS and Reflex as my day job for over a year now May I suggest please sharing your experience, along with what links/screenshots/videos of what you have built using GHCJS+Reflex? It may not look relevant to you, but seeing a well-polished product written in a less-popular tech-stack is a helluva confidence booster.
&gt; So you are going to not refute my point I am not going to engage with you. You are free to read other comment on this thread to get a sense of what I'm saying. If it makes you happy - you won, I lost. Please enjoy your meaningless victory in a Reddit argument.
Great to hear! Don't hesitate to reach out for any feedback I actually wrote this to do a similar thing myself :D
I despise refactoring in a non-pure language because I don't *know* if I'll introduce a bug. Perhaps my tests will catch it. Perhaps they won't. With Haskell, I can trivially reason about the refactoring and *prove*, informally and formally, to myself that the refactoring won't introduce a bug. (It might introduce a space leak. :p)
I don't want to win. I just want to figure out what your point is. Is PureScript more "packaged" than GHCJS, or is it somehow easier to learn and thus sustain half a team on it? Like what exactly makes PureScript significantly better than GHCJS on the frontend.
Depends on what you mean by 'in terms of iterate'. You can't do `unfoldr (const Nothing) () = []` without the help of any other functions.
Any thoughts about why the foldMap approach is slower that the naive `average xs = sum xs / length` ?
&gt; I'm used to far better tooling than what is available in GHC currently. Really? Most languages I've used have worse tooling than GHC, with the exception of the mega-languages like Java, C#, and Swift; and the latter two actually have worse package management than Haskell now that we have Stackage and/or Nix. GHCJS is only marginally worse, the main detractor being the lack of --interactive (which is made up for by the GHC environment). &gt; I want GHCJS to get better. Much better. But it starts first with acknowledging the issues. Three links to Reflex issues that range from trivial to irrelevant don't count as deal-breakers to GHCJS in general. You would be getting a lot more support in this thread if you actually acknowledged actual issues (of which there are plenty). However, I would still argue that the state of more typical frontend development environments is so bad in 2017, that even the GHCJS tooling solves significantly more problems. Other languages have worse package management, worse type systems, less library maturity/stability (EDIT: less mature/stable wrt non-dom libraries; browser code is all a little bit unstable atm with GHCJS, though this should be ironing out soon), awful concurrency models, terrible development workflows, and laughable backend &lt;-&gt; frontend integration. &gt; May I suggest please sharing your experience, along with what links/screenshots/videos of what you have built using GHCJS+Reflex? I cannot share proprietary information like that, but I do plan on writing some blog posts on the advantages I've experienced and the design patterns I've developed whenever I have the time.
I mean I guess building a real world app if you haven't already is a good idea. Perhaps a website backend and maybe frontend.
&gt; Really? Most languages I've used have worse tooling than GHC, with the exception of the mega-languages like Java, C#, and Swift; and the latter two actually have worse package management than Haskell now that we have Stackage and/or Nix I concede partially wrt stack. Hands down a winner across anything else that I have used. Please try TS development in VS code. It's a much younger language compared to GHC. Please cross over to the dark side once and _feel_ the difference. Else it's just a Mars vs venus conversation. 
I personally do 4 space indents, except for `where` which I indent 2 spaces.
I really wanted to link you to Beautiful Folds by Gabriel Gonzales, until I saw the link at the bottom ;D
Since 8.2, GHC can display constraints for typed holes: GHCi, version 8.2.1: http://www.haskell.org/ghc/ :? for help λ&gt; :set -fshow-hole-constraints λ&gt; :{ Prelude| pleaseShow :: Show a =&gt; Bool -&gt; a -&gt; Maybe String Prelude| pleaseShow False _ = Nothing Prelude| pleaseShow True a = Just (show _a) Prelude| :} &lt;interactive&gt;:5:32: error: • Found hole: _a :: () Or perhaps ‘_a’ is mis-spelled, or not in scope • In the first argument of ‘show’, namely ‘_a’ In the first argument of ‘Just’, namely ‘(show _a)’ In the expression: Just (show _a) • Relevant bindings include a :: a (bound at &lt;interactive&gt;:5:17) pleaseShow :: Bool -&gt; a -&gt; Maybe String (bound at &lt;interactive&gt;:4:1) Constraints include Show a (from &lt;interactive&gt;:3:1-49) λ&gt; (edit: formatting)
I think those two are unrelated (project ideas and JS)
I’ll concede hat TS has a big leg up on other typical front end stacks, but it still loses in a few very important ways to me. The type system (and general pure programming paradigm), the package management, and the concurrency model, to name a few. The main considerable advantages in my view are the stability of some of the dom frameworks, the editor tooling, and the dev iterations. I’m fairly willing to pay the cost of keeping up with Reflex, since in my experience I can build frontend code way way faster with it. The editor tooling hasn’t been a problem since I got dante working, and since I can always just fallback on ghcid whenever I need something good and simple. And the dev iterations with the warp app with GHC are very time-efficient and compete fairly well.
Yeah I haven't done anything real world yet, something like that would be good. I'm sure the learning curve is pretty steep but I guess we can always post in the Haskell irc if we get super stuck
For an easy start go with Scotty / blaze HTML and generate a static site. Later on check out servant and perhaps reflex-dom. There is also react-hs which I personally use in prod but it is admittedly somewhat experimental, I'm not too afraid since the source is short enough and understandable enough to hack on if needed. 
I skimmed the comments but I didn't see anyone mention Idris yet. I believe it can be used as an alt-js roughly like ghcjs but with leaner generated code.
That's a great bug report. The example output from GHC HEAD is horrible but looks quite possible to improve. GHC HEAD says that `a` is a 'relevant binding' but to my human eye it is obviously _the answer_. I guess that valid substitutions that are locally bound are usually the answer and that this is why the error message puts them first. Maybe changing the wording to "Likely substitutions" and "Other valid substitutions" would help. Well, that and culling class constraints into account.
&gt; All concurrency bugs, I think. One of the Rust evangelism strike force favorite catchphrase is "fearless concurrency". The borrow checker does help here.
One thing that differs at least is that the getAverage version requires fractional for the input … It seems to get better when you compile it though. When I test in ghci, `Data Average` is &gt;2x slower whether it's with `foldMap` or `Data.List.foldl'`, while compiled, the `Data.List.foldl'` version with `data Average` is 2x faster (I tried a very simple http://sprunge.us/UecZ giving results http://sprunge.us/fhaC). But then the simple example from `Control.Foldl`'s readme is 5x faster than that again. Maybe the post should have an example of how to do this kinda stuff with `Control.Foldl` (and include that in the benchmark graph) ;-) 
&gt; The pure vs impure code is a red herring imho. In haskell, there's no such thing as impure code: everything, including IO, is 100% pure (referentially transparent). Meh, this is just a case of arguing definitions. Your definition is perfectly good. A definition in which `print "Hello"` is impure but `return ()` is pure is also perfectly good. People talk past each other too much by assuming others are using the same definitions.
Anything where you update an `IORef`. Anything where you call a function you expected to not effect the outside world but it writes to a file, POSTs to an HTTP server, .... In fact it's so easy to come up with examples I'm afraid I've misunderstood your question. Are my answers the kind of thing you were looking for or where you after something else?
Hi, I stumbled upon the [`Tidings`](https://hackage.haskell.org/package/threepenny-gui-0.8.2.0/docs/Graphics-UI-Threepenny-Widgets.html#t:Tidings) type in [`threepenny-gui`](https://hackage.haskell.org/package/threepenny-gui). It is a pair of a `Behavior` and an `Event`, just like a `Dynamic`, but I don't know if it serves the same purpose.
&gt; Anything where you call a function you expected to not effect the outside world but it writes to a file, POSTs to an HTTP server, .... I was looking for something "deeper" than these kind of answers. The canonical examples for purity involve a function that adds two integers, but also launches a nuclear missile or wipes your hard-disk. Really? The more I work with code where you _need_ some side-effects, you almost always end up with multiple different monad-transformer stacks, and have to play type-jigsaw just to get things to work. (Take a look at any web-library - servant, yesod, scotty, all of them suffer from this explosion of needless type jugglery). I wonder if it really is worth the pain. Monads cannot prevent **malicious** side-effects, they can only prevent **accidental** side-effects (hello, `unsafePerformIO`). And there are a number of times when you want to have a _benign_ side-effect without introducing the complexity of a monad transformer stack - eg. logging - and you can no longer do that. Until I hit some epiphany, it seems that most accidental bugs are prevented by immutability itself, not purity. I'm still searching for examples to convince me otherwise.
&gt; Rust is what you get when you design a language to have [...] no garbage collection or any other fixed runtime 'framework' Rust wants to be as close to the metal (CPU) as possible, every processor tick counts. All of this while having modern language features (as long as they do not bite with the prerequisite). Haskell is currently one of the more high-level languages available. It allows the programmer to sit on very interesting levels of abstraction that allow him/her to create elegant solutions in minimal time. Now Haskell is also pretty fast, especially compared to the VM-based languages (Java, C#, Py, Ruby, etc.). 
&gt; A definition in which print "Hello" is impure but return () is pure is also perfectly good By this definition, Haskell cannot distinguish between pure and "impure" code, since both the expressions above will have the same type, `IO ()` (monomorphising `return`, ofc). It's also confusing because you hear that Haskell is a purely functional language, and then you read that it contains impure code. I actually think it's a pretty poor definition. Conversely, referential transparency is a lot more precise, allows you to account for a language that can interact with the outside world whilst remaining pure, and has clearly explainable benefits, which I've outlined above 
About the `static` folder: I know it's really weird, and I'm working on fixing that, right now it is not possible to rename it. About opening projects, do you have `stack` installed and in your `PATH`? It would be great to continue this conversation on [gitter](https://gitter.im/theam/haskell-do) \^_\^
Cool, let me know if you have any questions or run into trouble (:
That Dwarf Fortress problem was not a purity problem. It was a problem of namespacing: the Dwarf Fortress binary had the same name as a system utility, and came before `df` in the system path search order for some reason or another. This kind of problem isn't solved by purity, it's solved by clearer and stricter namespacing rules to make sure unrelated names don't shadow each other.
&gt; I was looking for something "deeper" I don't think there is anything "deeper". Those are silly, trivial bugs that occur in programs people write, especially when IO is not tagged in the type system. It's not "deep". It's just useful. &gt; you almost always end up with multiple different monad-transformer stacks, and have to play type-jigsaw just to get things to work I don't use any of the libraries you mention but I'm sure it would be worthwhile writing an expository article on the problems you've encountered so that future libraries can avoid imposing "type-jigsaw". &gt; I wonder if it really is worth the pain. I think each individual or team has to decide this for themselves. I'm actually quite astounded you're still considering using Haskell if you don't see a massive benefit from it! &gt; Monads cannot prevent malicious side-effects, they can only prevent accidental side-effects Correct. (Note that "immutability" cannot prevent malicious mutation either.) &gt; it seems that most accidental bugs are prevented by immutability itself, not purity. I'm still searching for examples to convince me otherwise. Have you never had a bug caused by writing to a database when you shouldn't? I can't see why the benefits of IO tagged in the type system are not obvious (even if some people may *also* think it has drawbacks).
It's unfortunate that people fixate on the terminology "pure" when it doesn't have a formal definition. On the other hand "referential transparency" does have a formal definition. If you're saying that it's a better term to use then I agree. [EDIT: Corrected to "referential". Thanks /u/protestor.]
Rust has similar features as Haskell in this regard. It eliminates data races and makes it easier to safely share data between threads. It also makes it easy to define data types that can never be used outside its own thread.
Rust only actually protects against data races. While that does cover a lot of bugs, it doesn't include synchronization-related issues.
That sounds like a good idea to be honest. I guess it's moving from less stateful stuff like sudoku or chess type problems to more real world applications that require state that's the big jump in Haskell. Although I feel kind of naiive saying that since I've seen monadic ways of doing those types of problems and half the time I have no idea how they thought of it.
&gt; Have you never had a bug caused by writing to a database when you shouldn't? I can't see why the benefits of IO tagged in the type system are not obvious (even if some people may also think it has drawbacks). *Accidentally write* to the DB? Are you *sure* that's what you meant? On the other hand, I've faced quite a number of perf issues by implicitly reading from the DB (thanks to Rails magic). But, just by not making the db connection available to functions that *shouldn't* be reading from the DB that gets solved. Basically, immutable FP is solving that problem for me, I guess, not purity.
Thanks for pointing that out. It looks like it's related, but it doesn't look it's constructed in a way that keeps the `Event` and `Behavior` synchronized. The pattern does show up [elsewhere](https://github.com/HeinrichApfelmus/reactive-banana/blob/master/reactive-banana/doc/examples/SlotMachine.hs#L105) in `reactive-banana` code.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [HeinrichApfelmus/reactive-banana/.../**SlotMachine.hs#L105** (master → 7b120be)](https://github.com/HeinrichApfelmus/reactive-banana/blob/7b120be3667488cfb46f48cef32bf91c5fbbb366/reactive-banana/doc/examples/SlotMachine.hs#L105) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dnixt0k.)^.
The `foldMap` version is a lazy right fold. These can provide better opportunity for optimization in many circumstances, since GHC utilizes foldr fusion, but our situation is not one. Since we are crushing to one value with a strict accumulator we want to force the accumulator on every iteration to achieve a tight loop. The lazy right fold instead produces a thunk for every iteration. When we force it those thunks get evaluated. This means higher memory consumption, more garbage, essentially two passes over the data and no tight loop. The naive version utilizes two strict left folds which is why we see it being twice as slow as when we have 1 tight loop. 
I suggest to work through https://www.codewars.com/?language=haskell or similar to get familiar with Haskell syntax and way of problem-solving. One thing I like about this kind of task is it's comprised of small tasks with clear goals and quick feedback. After that, learn mtl then off you go to build apps that do IO (HTTP / DB / etc)
Hmm, I'm curious if GHC is unable to unpack the Average Monoid, but can unpack values in the Foldl example. I'll have to dig in to the core.
Rust does prevent you from using mutable globals in most cases.
Sure, but Rust HAD functions that were marked `pure` before and it was decided they were not necessary. Can you make a case for adding them back in?
The thing is, if you can modify them, nobody else can. That's how ownership works. It's equivalent to passing data to a function and having a function return modified data. Except you can skip heap-allocating the new variable and just reuse the memory of the previous data. Instead of having it be a clever compiler/allocator trick it's just actually specified by the programmer.
Agreed. This example is what I had in mind.
&gt; which trades some amount of performance for memory safety. Rust is probably safer since it makes more interesting guarantees. Interestingly, due to some of those guarantees Rust is also probably theoretically slower - at least compared to Java. Comparing safe Rust to pure Haskell is more awkward, since pure Haskell can't really do anything and once you hit the escape hatch in either you can do anything. 
In your *group by* examples, you use several times the *second* function. Where does it come from?
As the wiki suggests, `NoMonoLocalBinds` should be accompanied by `-fwarn-missing-local-sigs`. The latter should help you fill in the signatures you need to remove `NoMonoLocalBinds`.
`MaybeT` is another way. The difference from `Compose` is that `MaybeT` short-circuits at the first `Nothing`, without looking up remaining environment variables, while `Compose` reads them all first and then tries to put them together. ... import Control.Monad.Trans.MaybeT lookupEnv' :: MaybeT IO Config lookupEnv' = MaybeT lookupEnv getConfig :: IO (Maybe Config) getConfig = runMaybeT $ Config &lt;$&gt; lookupEnv "NAME" &lt;*&gt; lookupEnv "AGE"
`Data.Bifunctor`! You could also use `fmap`, but I find `second` clearer.
Rust also solves other resource problems via raii. So you could statically avoid the safety issues when using file handles or sockets directly by encoding their state machines into the type system which isn't realistically possible in haskell. The linear types that haskell might get can do this but it has a couple usability issues. Mostly because it doesn't deal with multiple exit locations very well. Say you want one branch where a resource is freed and another where it is returned. If you layer exception handling on top of that you need to have mutable state to track which resources were freed before the exception was thrown. If you add additional exit paths like closing a stream early this either forbids useful programs or becomes incredibly verbose without compiler support.
Rust does through linear types and preventing aliasing. You can't change anything unless you mutably borrow it or use unsafe. So 99% of the time unless you see mut you know things aren't gonna change. In the case of other things like Arc or Rc well they're not much different than State or Stm in their purpose. Managed mutable access to data. Of course the same is true is Haskell. There are ways to mess shit even inside pure functions. UnsafePerformIO and others. So it's possible for function that looks pure from it's signature to mess things up unexpectedly.
Haskell is faster yhan java and c#? 
Oh yes. Channels work great.
I very much doubt that rust is slower than java, since it is designed to operate in similar niches to c++
Referential transparency. Only one thing can have mutable access at a time...
Yeah. Creating errors in rust is too verbose unless you use a third party crate. And that third party crate should go in std...
And space explosions in rust don't happen because you used the wrong kind of fold on a lazy data structure. Serde-rs is a phenomenal library. Zero copy fast parsing of dozens of formats. Meanwhile about once a month some in Haskell is trying to fix a space explosion from trying to read and parse a file and trying to figure out which string library is best for the task or still maintained. Something that should be easy seems fraught with danger in haskell even for relatively small files because of laziness and the poor space performance of many string libs. Every language has trade offs.
&gt; So you could statically avoid the safety &gt; issues when using file handles or sockets &gt; directly by encoding their state machines &gt; into the type system which isn't &gt; realistically possible in haskell. Perhaps it's in the definition of "realistically", but I thought you could encode these state machines in Haskell with indexed monads?
&gt; It might introduce a space leak. Something worth keeping in mind. Refactoring in Haskell can be risky too, even if it type checks!
Looking at Contorer's example, the room for bugs that came to my mind was having to think about order of execution of a bunch of different statements, some of them mutating things created in others.
The problem is that we would need dependent types for this. If we read from a socket we get an `Either () String` out. If the value is `Left ()` the socket is closed, otherwise it stays open. So the out index has to be a function on the result value. Haskell doesn't have proper dependent types yet, though, so that would mean singletons all over the place. Whether that is realistic probably depends on ones resistance to gross type signatures and boiler plate, I guess.
How on earth is "lax monoidal functor with a strength" awkward or oddly specific? As I just said lax monoidal functor is a very natural thing and "with a strength" is basically equivalent to "internal to/enriched in Hask". I agree with you that it was not an advance in categorical logic, but it is an example of PL people using category theory to clarify their ideas, and I don't think there's anything strange about the definition.
It seems completely logical to me, since Rust is immutable by default; I actually expected a mut keyword for functions rather than a pure keyword and was a bit puzzled when I didn't see it.
I wouldn't say "faster than"; it can be, but it's often around 2-4x C speeds for some of its faster things, around Java speeds for "most code" and can hit C speeds every now and then if you tweak the crap out of your code
I was completely hinting at Beautiful Folds. Though I think attribution for the name goes to http://squing.blogspot.com/2008/11/beautiful-folding.html
Not quite; in Rust if the inputs to a function are not mutable, then your function is pure, except if the input has so-called interior mutability. Most code patterns do not require interior mutability, so you're usually fine. (Of course, this does not apply if you mutate global state or use unsafe, both of which are somewhat frowned upon (mutating global state is also rather tricky, requiring some weird incantations)).
Rust doesn't use channels (I think?). The type system just makes sure only one thread can ever mutate a struct at any given time, so you can't get data races.
I'd prefer not to have to audit my dependencies to see if they are impure. I'd rather let the type system do that for me
Calling a function that unexpectedly modifies my filesystem
That helps a little but doesn't quite give the full notion of referential transparency. `bar = foo(x); baz = bar + bar` and `baz = foo(x) + foo(x)` aren't equivalent even ignoring interacting with the outside world, if x is mutable at all. And the fact that these functions can just print and interact with DBs and the network and such at any time really seems to kill any purity they might somewhat have had.
I didn't say "accidentally write". I said "write when you shouldn't". But sure, reading from the DB is an even better example. &gt; Basically, immutable FP is solving that problem for me, I guess, not purity. Why do you guess that? In Clojure, for example, a language with immutability and without purity you can easily have a function that read- from the DB without that being reflected in its type (since it doesn't have one).
Depends. I'd say on average its in the same league (except memory usage, then Haskell is probably on average faster)
AFAIK Scotty does not do any type level magic. It's essentially a Sinatra port that uses strings all over the place. Also from your various (not entirely unjustified) complaints in this and other threads I think you'd really like the OCaml/BuckleScript/Reason stack. There's a real focus on tooling in that community vs. abstraction. And the jump should be pretty easy since you already know Haskell. 
&gt; AFAIK Scotty does not do any type level magic. Try getting a newbie to integrate Scotty's monad stack with a DB pool. I know. [I was that newbie](https://github.com/scotty-web/scotty/issues/198) &gt; Also from your various (not entirely justified) complaints in this and other threads I think you'd really like the OCaml/BuckleScript/Reason stack. Thanks for the tip. I will definitely take a look, but not till we've pushed a large feature developed in Haskell to production. I'm not giving up so easily!
&gt; Why do you guess that? In Clojure, for example, a language with immutability and without purity you can easily have a function that read- from the DB without that being reflected in its type (since it doesn't have one). You don't pass-in the db-connection to the function. For example, a hypothetical `render-html` function, which needs to be "pure", in Clojure: ``` (defn render-html [user-profile] (...)) ``` Unless you go out of your way to read a config file, parse out the DB creds, and then make a DB connection (to which any reasonable response is going to be -- "WTF?"), isn't this a _good-enough_ way to disallow functions from randomly reading from the DB? Also, this doesn't tie your hands behind your back, if you just need to log the time it took to render the HTML. It's a _benign_ side-effect -- you know it, and you don't need to prove it to the compiler. 
&gt; So the out index has to be a function on the result value. Haskell doesn't have proper dependent types yet, though, so that would mean singletons all over the place. I thought Oleg tackled this years ago with monadic regions: http://okmij.org/ftp/Haskell/regions.html#light-weight But it again depends on what you mean by "realistically".
I'm afraid puzzling out monad transformers is never going to go away, too much of the ecosystem uses them. Best we can offer the newcomer are (1) resources to learn them from basics and (2) framework specific copy-paste-tweak recipes so they can get going in the meantime. Since you keep running into issues with (2) if you haven't already you might consider starting a recipe repo.
&gt; isn't this a good-enough way to disallow functions from randomly reading from the DB? I don't think it's good enough. What if you make a closure that contains the DB connection? Haskell prevents you from doing that. Clojure wouldn't. I don't know Clojure syntax, but in an impure but immutable Haskell renderHTML:: DBConnection -&gt; UserProfile -&gt; HTML renderHTML = ... ... foo :: UserProfile -&gt; HTML foo = renderHTML myDBConnection ... `foo` doesn't require the DB connection to be passed in, yet it interacts with the DB. `foo`'s consumer can't know that from the type signature. In Haskell on the other hand `foo :: UserProfile -&gt; IO HTML` and it's much easier to see it's doing something to be cautious of.
I just found https://hackage.haskell.org/package/fn. No monads. But also no db integration but that's something you can maybe contribute.
Well, a pure function calling an impure function would be a type error. Impurity is an effect.
&gt; in Rust if the inputs to a function are not mutable, then your function is pure I think we have different definitions of pure. To me purity means referentially transparent, ie that the same function called with the same arguments will always return the same result. This isn't the case in rust, I can easily have a function of type () -&gt; T, for example env::vars(), which returns an iterator over the process's environment variables. If I call that once, then set an environment variable, then call it again I'll get a different result
I think you mean referential transparency.
True. No null values _alone_ eliminate maybe 1/4 of the bugs I've seen in Java code.
Thanks, edited.
I totally agree with you that GHCJS has very poor marketing and PR. That's something I myself would love to work on when I can.
While you are correct, my question would be, are you *trying* to hide the DB connection here? Is this really accidental? And how would the `render-html` function, which is being proposed as a "sneaky" closure, become part of the global namespace, so that it can be called by other components? For a large number of function that are pure, we also have monadic variants to allow side-effects. eg. `map, mapM, fold, foldM`, etc. So, side effects are a fact of life. I just keep wondering if it would be better to have a language which had purity as _opt-in_ instead of _opt-out_. You get something to work first without being bogged down by the details that purity forces upon you, and finally, the top-level function that needs to be pure, is annotated as "EnsurePure". The compiler inlines (or does some magic) to all the function calls down from that call-site onwards and ensures that they are pure. But yes, you are correct. Any function that is taking a lambda as an argument, can be passed a closure that breaks purity guarantees.
So my initial suspicions about unpacking were wrong. The actual problem is that your benchmarks were apples to oranges. If we benchmark `Float` vs `Float`, then we actually see that the `Average` `Monoid` via `foldl'` is fast. https://gist.github.com/eborden/6eefd88ef234c4a8540724f8dfa5a170 chart https://user-images.githubusercontent.com/545655/30868828-b0455512-a2ad-11e7-8bae-32b58a514c31.png 
Among other things, purity ensures that given the same inputs, the function will always produce the same outputs. This is important any time you may have to apply the function more than once. The case I'm most familiar with personally is in distributed systems, where you may want to replicate an operation across many different nodes. If the code that implements your operation uses, say `getCurrentTime`, then it isn't going to produce the same results across the different nodes, meaning you will not have faithfully replicated the operation, which is a bug that purity can prove does not exist. 
So you use 4 spaces in front of a `case` and 8 spaces in front of the blocks under the `case`?
I'd be curious to see if /u/tekmo sees any reason why this isn't optimal.
You can still perform IO, generate random values, etc.
&gt; While you are correct, my question would be, are you trying to hide the DB connection here? Is this really accidental? And how would the render-html function, which is being proposed as a "sneaky" closure, become part of the global namespace, so that it can be called by other components? Higher-order programming happens in functional languages all the time. If you give people a convenient way of doing something they'll do it. If that's writing an impure closure that other people expect to be pure, they'll do that. It doesn't need to be part of "global namespace". It'll be passed around and used. &gt; it would be better to have a language which had purity as opt-in instead of opt-out Yes it would. But Haskellers don't love monads because they love category theory. That's mixing cause and effect. They love category theory because they love monads because they're the only known general way of handling effects in the presence of higher-order functions. I've thought a lot about what a language with opt-out purity could look like. I think it's impossible. We didn't choose this way just for fun! There's no other choice (as far as I can see -- if you can see any better you'll get a free PhD). 
&gt; And space explosions in rust don't happen because you used the wrong kind of fold on a lazy data structure. You can use recursion schemes to avoid this. &gt;Meanwhile about once a month some in Haskell is trying to fix a space explosion from trying to read and parse a file and trying to figure out which string library is best for the task or still maintained. None of this is true, and in fact parsing is one of Haskell's strong suits. `text` and `bytestring` are both maintained. &gt;Every language has trade offs. Rust and Haskell are both good languages but in general this is not true.
&gt;So it's possible for function that looks pure from it's signature to mess things up unexpectedly. Certainly. But I have never run into this in practice. &gt;You can't change anything unless you mutably borrow it or use unsafe. Randomness is a good example of impure code that doesn't involve mutability. It isn't just state that has implications about reasoning about code and testing it.
&gt; I just keep wondering if it would be better to have a language which had purity as opt-in instead of opt-out. You mean like with a `Pure` comonad or something? IIRC, it's been tried a couple of times, and always seems to leak. It's much easier to syntaxically exclude certain constructs than it so to verify an ADT doesn't contain specific constructs. (Correct by construction is easier than later verification.)
Once you monomorphize the type of `return`, yes. But `forall m. Monad m =&gt; m ()` expresses purity!
On the other hand, in Haskell I can stream data out of my deserialiser without rewriting the library nor my application. Thanks to laziness it Just Works. Indeed it takes a while to internalise laziness, but once you have done so the benefits far outweigh the costs IMHO. But yes, trade-offs abound.
&gt;Impurity is an effect. Impurity is irreproducibility. Random number generation is impure.
Rust has bi-directional channels that are used to send data between threads. https://rustbyexample.com/std_misc/channels.html
&gt;Sure, but Rust HAD functions that were marked pure before and it was decided they were not necessary. Rust isn't a functional programming language. I'm not interested in debating whether purity is a good thing (it transparently is), nor am I interested in second-guessing the choices of the Rust compiler team. I'd guess it didn't fit the language well, but I don't really know why they made the decision.
&gt;Indeed it takes a while to internalise laziness, but once you have done so the benefits far outweigh the costs IMHO. I agree completely! It's also worth mentioning that strictness can introduce bugs that would not exist in lazy languages. I've run into stack overflows in Idris because I tried to port Haskell code naively. In general, laziness allows for more expressive control structures. It's not just an approach to computing data (if it were, it would be a bad one).
&gt; This kind of problem isn't solved by purity, No, it does not, but it does confine potentially problematic parts of your program. 
I mean for `case` I do: foo x y = case bar x y y of Just z -&gt; z ^ 2 Nothing -&gt; x I guess in situations where I break up a statement more than usual such as putting a case statement on its own new line, then I would consider a half indent / 2 spaces.
&gt; GHC tooling is not what I'd call great. It's barely passable, Aside languages popular enough for commercial IDEs, Haskell does well.
Have you read these paper [The Linearity Monad](https://www.cis.upenn.edu/~jpaykin/papers/pz_linearity_monad_2017.pdf) and [Embedding Session Types in Haskell](http://homepages.inf.ed.ac.uk/slindley/papers/gvhs.pdf)? To me the power of Haskell is its declarative type system as it allows for techniques such as the above to be implemented as an algebra. I'm not at familiar with Rust but if it has a similar declarative type system (e.g. SML, OCaml) as Haskell than it would be of interest to me.
FWIW those "pure" functions were quite misleadingly named and very different from what Haskell means by it -- IIRC it meant something like the function not being able to mutate things in memory, but said nothing about external I/O or anything like that.
It's happening because they aren't in scope for the cubicS function... Only `q` and `r` are defined in `cubicS`.
It's happening because, well, those variables aren't in scope. cubicS takes two arguments, q and r, and then for reasons that are beyond me redefines them in the `where` block, but the variables a, b, c and d that it tries to use do not exist here, only inside cubicQ and cubicR. I get the impression that you have incorrect ideas about how variables, function definitions, and scope work in Haskell, so I suggest you re-read whatever learning materials you have been using and adjust your mental model. Or maybe you are just confused about how where clauses work; it's hard to tell.
[Creating a Desktop Application with Threepenny-GUI and CEF3](https://maxow.github.io/posts/creating-a-desktop-application-with-threepenny-gui-and-cef3.html)
It's meaningless to compare safety in languages without mentioning what sorts of safety we're talking about. Haskell achieves memory safety via garbage collection (with escape hatches for memory that isn't managed by GC), while Rust achieves memory safety via lifetimes and borrowing rules (with escape hatches via `unsafe` blocks). Haskell has more advanced type features, which allow you to encode more information into the type system. It is possible to make more errors compile-time errors in Haskell. In that sense, Haskell is safer. However, "safety" might mean "maximum latency may not exceed ~10ms" which Haskell can't do and Rust can do. Both languages get the big things right: immutable by default, no pervasive `null`, basic algebraic data types, typeclasses/traits for ad hoc polymorphism. The rest is application-specific gravy.
I tried benchmarking `Control.Foldl.foldMap` compared to `Data.Foldable.foldMap`. `C.F.foldMap` is really weird compared to normal `foldMap`. It was definitely faster than `D.F.foldMap`, but slower than the naive approach.
Idiomatic Haskell is faster than idiomatic Java. Idiomatic Java typically involves a ton of dynamic runtime checking and vtable method lookups, and the lack of unboxed value types can wreck some tight loop performance. The JVM's JIT can do a good job with optimizing these at run time, but GHC can compile the source of typical Haskell programs to much more efficient machine code than `javac` can do statically. Java *can* be very fast, if you don't write monstrously indirected OOP code with it.
You might want to have a look at these: https://gist.github.com/eborden/6eefd88ef234c4a8540724f8dfa5a170
Ruby on Rails has a ton of hooks you can implement in your database models. Pre create, pre validation, pre save, post save, post create, etc. These hooks can be used safely, but they're often abused in ways that interact really poorly with database constraints and the rest of application code. That they're done asynchronously and can sometimes silently/selectively skipped can also bring a lot of pain in debugging problems with them. Most Rails programmers that I know are loathe to use them because of these confusing problems. These can mostly be fixed by requiring that the hooks are pure. Right now, if we want to provide a type to the callbacks, it's something like: type Rails a = StateT Everything IO () after_create :: Rails () Now, if the callbacks were *pure*, then we could type them as: after_create :: Model -&gt; Model This can only do one possible thing -- modify the model. No side-effects means you can't do things that are really unsafe, and these hooks are totally fine and reasonable to use. Since they *can't* do side-effects, you're not tempted to design your system around a feature that's extremely difficult to debug and understand, and you write it in a better way.
Wait, what? `_a :: ()`?
One way would be [wxHaskell](https://hackage.haskell.org/package/wx), and [reactive Banana](https://hackage.haskell.org/package/reactive-banana). The advantage is that it gives a native look and feel. You would need to hook wxHaskell events to reactive banana.
Performance is definitely a big reason why purity is great, and it has less to do with compiler optimizations and more to do with "external systems are extremely slow." If something has a type `IO ()` then I know it's very likely calling to a database or making an HTTP request, and that's almost certainly where my bottlenecks are coming from. Just in terms of structuring programs, purity forces you to be conscious of the decisions you make. In porting a PHP app to Haskell, I was maybe 15 function calls deep when some function that *should have been pure* was actually calling out to the database (yes, "accidentally using the database" is possible, and in fact easy, if you don't audit all of your code!). This gave me a few choices: 1. Thread the result of the database query through the call stack., 2. Introduce the App monad to each of these functions and make the database request there. 3. Refactor to not call the database and retain purity. 4. The pure logic returns an effectful function which takes the final bit of input, performs the database call, and returns the result. 3 was the right answer, but our hands were tied, and we ended up going with 4.
Lets pretend your code makes sense - which implies you can (mentally) evaluate the expressions. If I type `cubicS 1 2` what are the values a, b, and c that are passed to `cubicQ`? What is the value `d` that is passed to `cubicR`?
Mutating variables passed in are exclusive, so in effect it's behaves much like a linear kind of State monad.
You do have thread-local variables though.
&gt; "referential transparency" does have a formal definition does it? Usually it seems to me people mean roughly "full beta" which is a property I can give a formal definition for--namely that the following two equations hold always C[(\x.v) v'] = C[let x = v' in v] C[let x = v' in v] = C[v{v'/x}] where `v{v'/x}` is capture avoiding substitution, `v` and `v'` are any terms, and `C` is any context (even one with binders). "Equation holds" means that the programs should have the same possible behaviors (although not necessarily the same behavior on a particular computer as once you reach into `IO` non extensional things, like how long something takes or how much memory it uses, are observable). I'm not sure though about "referential transparency". I don't think it has a good definition, but I could be wrong. If we just mean full beta though, we should be careful in explaining why it is more important (to us) than any other possible equational law.
Tangential to your question, but: You should investigate some of the available tooling options for Haskell. If you're used to vim or emacs, I recommend spacemacs, but there are also decent integrations for atom, sublime text, and VSCode. I suggest this specifically for using the 'type of identifier' feature that many of the Haskell dev plugins support - This is incredibly useful for sorting out 'What the heck does `p` represent in this context.' More on point, the source code for `Maybe` , `Either`, etc. is a great place to start, as they are the most basic representations of monads in haskell, and their definitions are extremely simple to read and understand. Unfortunately the source for `State` in the `mtl` package is kind of hard to read through as a beginner because it's become wrapped up in monad transformers - It's fairly straightforward after you've got some experience with monad transformers, but it's a lot to swallow all at once if you're new to monads in general. I recommend : http://brandon.si/code/the-state-monad-a-tutorial-for-the-confused/ As a solid introduction to the State monad, as it cleanly separates the code implementing `State` as a monad from the code declaring the `StateT` monad transformer. 
I think, I was imagining a solution like this one. But I have a bigger problem now. I don't understand how that works. I understand that liftA2 lifted Config to an applicative. (String -&gt; String -&gt; Conf ) became (A String -&gt; A String -&gt; A Conf) But what exactly is the applicative here? If it is IO, what happened to Maybe? Is it something like (IO (Maybe a)). If it is how can compiler merge 2 applicatives.
But the fact that `IO` *is* pure (in Haskell) doesn’t make it easier to reason about what an `IO` action *does* (in the RTS). Referential transparency is great and I miss it in other languages, but when it comes to impure code, I think we get more benefit from the *cultural* style of avoiding mutation even in `IO`—which may be just because the language makes it inconvenient. That is, I might use one or two `IORef`s or `MVar`s in a typical Haskell application, whereas in C++ I tend to use many more mutable variables because that’s just how things are done in that country.
&gt; Wrt Takt, do you know if one could play around with the product they've built with GHCJS? Is it publicly accessible under a freemium model, by any chance? Takt's app is not consumer-facing, so no, you're not going to be able to play around with our product. But we do have one of significant complexity (26,000 lines of Haskell being compiled by GHCJS at the moment) and it is being used in production by a large Fortune 500 company.
&gt; If the program's denotational semantics say that it is represented as ⊥ then it changes itself to be () or else ⊥. I don't get what you're trying to say about `evil`. It resembles the proof of the halting problem, but... what exactly is your question about it? &gt; ... if an algorithm assigning semantics would not necessarily halt I'm not sure I understand the question. Just because "not all possible algorithms for assigning semantics would halt", this does not restrict you from selecting specific algorithms that *are* known to halt for all inputs. Due to the halting problem, we of course know that such an algorithm could never be decisive for all possible inputs. Instead it would have to admit uncertainty for some inputs: - this program is known to halt - this program is known to diverge - this program *might* diverge 
&gt; ⊥ == denote programSrcCode forall x. denote (⊥ == x) = ⊥. forall x y. denote (if ⊥ then x else y) = ⊥ So, we know the denotation of `evil` is also ⊥, never `()`.
Looks like ghci's extended defaulting rules to me. 
If you are trying to say that random number generation is reproducible, then you're actually talking about pseudorandom number generation. "Truly random" number generation would be irreproducible.
The point is that `⊥ == …` will (almost) always denote `⊥`. Then, you have `if ⊥ then… else …`, which will denote `⊥` again. So, `eval` will denote `⊥`. This is independent of the halting problem, even.
The difference from Haskell or the difference from Java? I'd say it's quite a decent distance from both.
But is that true referential transparency? I'm pretty sure referential transparency requires that `bar = foo(x); baz = bar + bar` be equivalent to `baz = foo(x) + foo(x)`. Which is untrue in the presence of either real world altering affects such as printing, or just `x` being mutable.
Well, with Rust it would be pretty easy to write the equivalent of Socket Ready -&gt; Maybe SocketAddr -&gt; Port -&gt; IO (Either (Socket Closed) (Socket Bound)) so that the `Socket Ready` is effectively replaced by the resulting socket after the call. I think that would be technically possible with monadic regions but require manual lifting across nested ones? Monadic regions are pretty cool but overlapping regions still feel kind of second class. Please correct me if I am wrong, though, it has been a while since I have played with the library.
"A program to assign such a value might itself not terminate due the halting problem." Yes. But this is not a problem. That's just the way the world works. Denotational semantics isn't about assigning values to expressions algorithmically -- it is about assigning them _mathematically_. We can take a mapping from programs to sets of equations. The fact that there is not a universal procedure to solve such equations is no more mysterious than the lack of a universal procedure to solve any other sorts of mathematical equations.
`liftA2`, in this case, is lifting `Config` to operate on `Maybe` values; the applicative operators used after it are then applying the resulting function onto IO. So you have `liftA2 Config :: (Maybe a) -&gt; (Maybe b) -&gt; (Maybe c)` and the IO operations get treated as `IO (Maybe a)`; that is `Maybe a` is treated as the 'contained' value, and `maybeGetConfig` is just a function on those values. The `Compose` approach I mentioned goes the other way - it wraps up `IO (Maybe a)` as `(IO . Maybe) a` - it treats IO and Maybe together as one functor - an arbitrary producer of an `a`. This approach is nice because it's very general - we're not transforming the applied function, so we don't need to worry about number of arguments. We also can stack calls to Compose for multiple layers, and we're not attached to Maybe - we can swap out lookupEnv for something that can't fail. The Monad Transformer approach Syrak mentioned similarly wraps IO and Maybe up together, but as a monad rather than an applicative. Monads don't compose the way applicatives do - not all compositions of monads are still monads - so to do this here requires specific instructions on how to combine a monad with Maybe features (which is what MaybeT does). It means the monad transformer approach is less general than the applicative-based approaches, but the tradeoff is that you get to keep your monad-specific features - here, it's the short-circuiting of the Maybe monad. This is a better approach if you know that, whatever you swap lookupEnv for, it's always going to be something that should short-circuit on failure.
Oh, my mistake. I corrected my comment.
Denotational semantics don't have to be deterministic or computable. From what I remember of [imprecise exceptions](https://www.microsoft.com/en-us/research/publication/a-semantics-for-imprecise-exceptions/) the following case statement: case (raisesException1) of X -&gt; raisesException2 Y -&gt; raisesException3 for example is defined as the set of the tree exceptions. If you cal getException on them you will get one of the three back but it does not define which one. And you can write a program that gives you a denotational result of one of (Terminating, Non-Terminating, No idea) which is good enough for many issues in practice. Do you have a specific scenario in mind where a result of "No Idea" is a problem? 
That doesn't work in Rust. You need to do `baz = foo(&amp;mut x) + foo(&amp;mut x);` It's OBVIOUS it's not going to be referentially transparent since the syntax tells you it isn't.
Oh, that's kind of stupid then.
Is the `&amp;mut` thing always necessary to mutate a parameter? Even for non-primitive types? If so there is still the whole printing / accessing a db etc. that can happen within `foo`.
It works in Rust: https://play.rust-lang.org/?gist=b4fad58cd381eee32dd7754fa2eeba0e&amp;version=stable fn foo(x: &amp;mut u8) -&gt; u8 { *x = *x + 1; *x } fn main() { let x = &amp;mut 3u8; let y = foo(x) + foo(x); println!("{}", y); } It prints 9.
&gt; You don't pass-in the db-connection to the function. I don't know if this is still true, but when I was using Clojure professionally it was common for things like db connections to represented as `*dynamic-variables*` which could be accessed in the dynamic extent of a `with-bindings` call, without being explicitly passed.
Yeah, that's what I meant. You need `&amp;mut 3u8` or it won't compile.
Several comments. Ugh, Strings everywhere. Many of your `parseJSON` methods could use `withObject` instead. More concise. Also the use of printf seems problematic: how about use a proper URL encoding scheme that escapes things correctly? In general, I find unpaired ToJSON/FromJSON inelegant. Surely it is not needed in this case but I would still appreciate complete ToJSON/FromJSON pairs. If you're just decoding from a response, you don't need an instance. You might want to do session management manually and use the same Manager (Reader style) to save repeated TCP/TLS handshakes.
It depends on the type of the argument. You can declare `x` to be `&amp;mut 10i32` and then use just `x` - you need to remember it's `mut`. Same thing with other types like `Vec` - you need to remember it's owned which means you can do anything you want with it.
In case the OP isn't getting it, try rr = 1 : 0 : 0 : 1 : 0 : zipWith5 myFunc rr (tail rr) (tail (tail rr)) (tail (tail (tail rr))) (tail (tail (tail (tail rr)))) 
`MonadUnliftIO` is my preference between the two, because I strong suspect it is equivalent to some amalgamation of `Representable`, which has very nice semantics. I would like to see something that just uses `Representable` or something to get purity back and remove the constraint on IO
I'm not going to get into a Reddit argument about TCS with /u/philipjf :) But I do offer this for further consideration: https://en.wikipedia.org/wiki/Referential_transparency_%28computer_science%29#History
Really appreciate the feedback! This is my first published library so I definitely want to improve the code and get better at Haskell generally. &gt; Ugh, Strings everywhere This did cross my mind, do you think using Text everywhere is a better choice? Using newtypes for every single different string field from the API seemed a little overkill, but maybe that's the better way? &gt; Also the use of printf seems problematic: how about use a proper URL encoding scheme that escapes things correctly? That's a good call, didn't think of that &gt; In general, I find unpaired ToJSON/FromJSON inelegant. Yeah as you said they're not needed the way the lib is now, but as I polish things I'll add those if time allows. &gt; You might want to do session management manually and use the same Manager (Reader style) to save repeated TCP/TLS handshakes. That's a great idea. I didn't expect users of the library to make _that_ many requests so I wasn't worrying about it first, but I'll add that to the to-do list as I continue the project.
&gt; the constraint on IO While I really like your observation that it's close to `Representable1`, incurring `IO` as the base monad is pretty heavy, and I'm not entirely sure is something I want to propagate from this project.
Right. That’s why I would like to see a variant of `Representable` tuned for lifting arbitrary base monads, so you can use pure bases
But you can call `foo(x) + foo(x)`, and that `&amp;mut 3u8` could be concealed far up at the top of a function. That's all I meant.
C++, in practice, is frequently slower than Java at memory management. It can't even do deallocation work in a separate thread. 
Our marketing site is more than 2 years out of date, so most of the work it shows is from before we even started using GHCJS. It definitely needs an update! It would also be great if there were more documentation on users of GHCJS; hopefully someone will find the time to put that together. I don't think most users of GHCJS are spending the time to publicize their use of it.
Yes, but Haskell types are the same way. You need to follow what type is `x` all the way at the top of the function.
@IronGremlin, thanks for the suggestion. I do use vim. Might take a stab at spacemacs per your suggestion.
Honest question: Do you audit your Haskell dependencies for `unsafePerformIO` ? Or you just assume someone else would have already called them out for using it unreasonably? I do the latter.
I feel obliged to point out that any property of programs (or classes of programs) that we might consider, like safety or correctness, are not meaningful without reference to a specification what it means for a program to have that property. I'm not sure what you are getting at with this post, but the simple answer is that Rust intends to give certain guarantees (modulo compiler bugs and deliberately unsafe code) like all references are valid or there are no data races ([A more complete list](https://doc.rust-lang.org/1.20.0/reference/behavior-considered-undefined.html)). How that relates to your (or anyone else's) idea of safety is up to debate. Rust explicitly doesn't give other kinds of guarantees, like no deadlocks or no memory leaks (I dare you to define what a memory leak is, is it any case where memory isn't freed at the first possible point in time? By that definition almost all programs leak memory..) For more non-guarantees Rust has, see this: https://doc.rust-lang.org/1.20.0/reference/behavior-not-considered-unsafe.html. I like to point to [Rice's theorem](https://en.wikipedia.org/wiki/Rice%27s_theorem) when these discussions come up in this naive fashion. Some people read it like we can't say anything meaningful about programs, but that that is wrong. It just means that there is no free lunch. You can't have a statically (i.e. decidable) recognizable subset of all programs with a certain property and no other programs. Either you have to be unsafe in the sense that some programs you accept do not exhibit the property you are trying to enforce or you have to be incomplete in the sense that some programs which do exhibit the property you are looking for will be rejected. It is not a problem to define safe subsets of programs that exclude arbitrary many classes of common and uncommon bugs. The problem is that those sets are typically rather depleted of any program a human would write. It is not that Haskell is just like Rust but with more guarantees. It is a different programming language with different trade-offs. Safety isn't a one way street, you always loose something, namely correct programs.
**Rice's theorem** In computability theory, Rice's theorem states that all non-trivial, semantic properties of programs are undecidable. A semantic property is one about the program's behavior (for instance, does the program terminate for all inputs), unlike a syntactic property (for instance, does the program contain an if-then-else statement). A property is non-trivial if it is neither true for every program, nor for no program. Rice's theorem can also be put in terms of functions: for any non-trivial property of partial functions, no general and effective method can decide whether an algorithm computes a partial function with that property. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
I see, that makes perfect sense now. Also thanks for explaining Compose and MaybeT as well. It was really helpful to understand difference between those three approaches.
The latter
I'm still finishing off the exercises for this one, which should come out in the next day or two. After this one we'll turn things up a notch...
r/reflexfrp --------------------------------------------- ^(For mobile users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(Downvote to -1 karma to remove) ^| ^(PM for sub to be ignored)
AIUI we're just taking a bit of a conversational shortcut when we refer to "the free monoid". More fully it would be "the free monoid _of a set_". Mac Lane (ch. 4) goes a bit into the links between universal algebra and adjoints, my rough understanding of it is that a free algebra adorns a carrier set with the least constraining algebraic signature required, and an algebra always has a carrier set. The corresponding free construct in category theory is a functor `X -&gt; A` on an underlying object from `X` that is the unique left adjoint to the forgetful functor `A -&gt; X`. So it's fine to talk about "the free monoid" because we know by common convention that we really mean the free monoid of a set, and know that this is unique up to isomorphism (Mac Lane shows a proof of this uniqueness in an earlier chapter). There does exist the free monoid of a semigroup, and it is unique, but it's not as commonly used as the free monoid of a set. `NonEmpty` is (fast and loose) the free semigroup, `Either () (NonEmpty a)` is (fast and loose) isomorphic to `[a]`, there exists a unique functor from `a` to `NonEmpty a`, a unique functor from `NonEmpty a` to `Either () (NonEmpty a)`, an isomorphism between `Either () (NonEmpty a)` and `[a]`, and a unique functor from `a` to `[a]`, so the free monoid of a "set" (to push past what fast and loose lets me get away with and keep going out the other side) is the composition of the free semigroup of that set and the free monoid of a semigroup. edit: the forgetful functors from **Mon** forget various aspects of what it is to be a monoid: they can forget the product, the identity, the axiom of associativity, the axioms of left or right identity, or some combination of those. I'm not sure offhand which of those functors would have a left adjoint, but since most forgetful functors turn out to have a left adjoint I'll go ahead and guess all of them do.
I'm ot sure about clojure, but I can confirm this for common lisp, which I have used extensively. It's a killer feature which makes arguments explicit but doesn't force you to pass them around all the time. It's easier to use than a moan reader and it solves similar problems. 
Thanks will look at those
You might be interested in my semi-abandoned projects [HeX](https://github.com/jgm/HeX) and [grammata](https://github.com/jgm/grammata). 
&gt; May I ask if you know of any large team using GHCJS and GHC to build a large end-customer facing product (not internal tools)? I'll happily change my opinion once I hear some success stories at scale. Yes. As I mentioned above, Takt has a significant client-facing app built with GHCJS that is driving substantial business impact. The code sharing that we get from having Haskell on the frontend and backend has completely eliminated mismatched API version bugs that were quite common when we were using a non-Haskell frontend. We also get a variety of other benefits such as much better developer mobility with the rest of our Haskell team, the full power of Haskell's type system, and the ability to leverage all of hackage in our frontend. There certainly are applications where the size of GHCJS generated JS would be prohibitive but ours isn't one of them, so I think GHCJS has been a clear win for us. UPDATE: At least 13 people from our team have at one time or another touched code in our system that is being built by GHCJS. I don't know what your criteria is for "large team", but that's pretty significant in my book.
I don't understand your second example with list. Why is this quadratic instead of linear: that's just a cons isn't it? The tail of the list is already in normal form due to the strict accumulation, hence the strict cons should be constant time. Or list in dhall are more like vector in haskell? This aside, I loved this post and your conclusion about the future of interpreters.
(Disclaimer: I’m not the author so I might get this wrong) The mistake you’re making is assuming that normalizing a list that’s already in normal form is constant time. However if you look at the implementation of [normalizing list literals](https://github.com/dhall-lang/dhall-haskell/blob/master/src/Dhall/Core.hs#L2009), it has to map over the list so it’s linear time which leads to a quadratic runtime overall.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [dhall-lang/dhall-haskell/.../**Core.hs#L2009** (master → 5d897e5)](https://github.com/dhall-lang/dhall-haskell/blob/5d897e55ea9bc95155b2e3aa99064d52f20fae31/src/Dhall/Core.hs#L2009) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dnkjc8c.)^.
Yeah if you allocate on the heap that's slower than in java. But you don't have to in c++. Allocating on the stack is encouraged and if needed you can use custom allocators. Tbh, if you allocate as much in c++ as in java you're doing it wrong. Of course being slower at allocating is hardly the same as being slower in general 
There's some interesting lessons here on how extra-language formal methods/semantic aware tools can make developers more productive. We usually try to solve these kinds of things "in the type system" - but sometimes the static analysis folks have simpler/easier ways to solve specific problems. Something the PL community doesn't always consider.
oh its 11 year cake day. yikes. 
And regurgitate ugly C syntax for another 40 year trip on the merry-go-round.
Happy cake day Don! cake = fix (Frosting :)
If I wanted to learn either Ocaml or Haskell in order to get a better chance at getting a functional programming job, which one would you recommend?
Former...
It's frosting all the way down!
Very much a beginner, how would you make a function for odd in Haskell without using the "odd 13" etc that's already there? I want a function that tells me if a number is odd
 odd :: Int -&gt; Bool odd x = (x `mod` 2) /= 0 There are back ticks around mod but I can't see them in my reply for some reason. EDIT: Thanks for the fix Jep Here's a point free version if you're bored: odd :: Int -&gt; Bool odd = (/=0) . (flip mod 2)
Thank you! I'm sorry it must have been such a basic question, it's my first time learning Haskell today for my A level
No problem. Below are some of the more useful resources I would recommend: https://github.com/bitemyapp/learnhaskell http://www.cs.nott.ac.uk/~pszgmh/pgp.html http://learnyouahaskell.com/chapters 
I was going to write something clever about bottoms and laziness, but then realized that dhall is total... Nice to see dhall maturing! But I wonder if these ad-hoc hacks will eventually be replaced by a more sophisticated bytecode interpreter Also kind of beside the point, but it really bums me out that paragraphs end without a period
I fixed the lack of periods. Sorry about that! :) What I would like to do in the long run is to use an optimal evaluator in the style of: https://github.com/maiavictor/abstract-algorithm There is a very efficient way to implement that which you can extend with support for built-in operations
Yes, that's correct. There is no tag to mark that a term is already normalized Also, to answer the other question: `List` in Dhall is like `Vector` in Haskell, except in the middle of a `List/build` function where they provide a linked-list-like API for assembling them (usually in conjunction with `Natural/fold` or `List/fold`)
I completed the final assignment from the CIS194 2013 course and I wanted to try out Debug.Trace. successProb :: Battlefield -&gt; Rand StdGen Double successProb b = do traceM "INVADING" bs &lt;- replicateM 1000 $ invade b traceM $ "Invaded " ++ (show $ length bs) ++ "times." traceM $ "FOLDING" let won = foldr (\bf acc -&gt; if defenders bf == 0 then 1 + acc else acc) 0 bs traceM $ "DIVIDING" ++ show won return $ won / 1000 But when I execute the code it only says: "INVADING" and then the result. I don't understand what the [documentation](http://hackage.haskell.org/package/base-4.10.0.0/docs/Debug-Trace.html#v:traceM) is trying to tell me and why their example is supposed to work. Next, the documentation suggests using liftIO . traceIO, and as far as I can tell Rand (from Control.Monad.Random) is an instance of MonadIO, but when I do: liftIO $ traceIO "FOLDING" I get an error-message saying "No instance for (MonadIO Data.Functor.Identity.Identity) arising from a use of 'liftIO'. How can I get those trace-messages?
I saw the first episode on Pandoc. I liked it a ton. Going to watch the rest of them.
&gt; foldr (\bf acc -&gt; if defenders bf == 0 then 1 + acc else acc) 0 bs A filter is clearer to me here: length $ filter (\bf -&gt; defenders bf == 0) bs As for your `traceM` issue... hurm, I'd have to look at how the `Control.Monad.Random.Lazy.Rand` monad is defined and I'm headed to work. For now I suggest you use `Control.Monad.Random.Strict` which seems well behaved.
Nice layer cake, I'll `take 10`
 odd :: Int -&gt; Bool odd x = (x `mod` 2) == 0 Indent code by 4 spaces.
I really appreciate the concluding paragraph: &gt; Many people associate dynamic languages with interpreters, but Dhall is an example of a statically typed interpreter. Dhall's evaluator is not sophisticated at all but can still take advantage of static type information to achieve comparable performance with Python (which is a significantly more mature interpreter). This makes me wonder if the next generation of interpreters will be statically typed in order to enable better optimizations.
Check out also this: https://guide.aelve.com/haskell. My friend (the author of the "Lens over tea" series and one of the authors of the upcoming book "Intermediate Haskell") is working on this. The site is still work in progress, but I think it's already pretty nice.
This is pretty much how Shen operates. It's got the basis of a type system and is interpreted with types intact when evaluated (if typechecking is enabled). An implementation may provide compilation and type erasure but is under no obligation to do so.
&gt; We usually try to solve these kinds of things "in the type system" - but sometimes the static analysis folks have simpler/easier ways to solve specific problems. I don't understand this comment; isn't "the type system" precisely a kind of rigorously specified 'static analysis'? Sure, it happens to be performed by syntactic means in most cases (i.e. by classifying phrases of the source program according to their 'type') but it's not really restricted to such if things like dependent types, liquid types etc. enter the picture.
Pretty cool! I've never seen `mdo` before. It seems like you don't need it in most of the examples. Is there any reason you used it? 
What's the difference? Are you advocating for the additional flexibility and more rapid dev cycle of static analysis tools as opposed to language compilers to address some issues, or is there some other concept or set of concepts at play in this observation? It seems likely that there is a strong point, or set of points, to be made here but I feel as if I'm missing a large portion of your chain of thought.
That's me. /u/m-renaud PM me if you feel like helping! :P
Your choice of rending layer doesn't necessarily dictate how 'functional' your app logic is going to be. It may dictate how much imperative glue you end up writing to support your functional architecture. All told, I was fairly happy with using haskell-gi to make a GTK app on windows 10 - The build process is obnoxious, but it's not impossible, and in my experience fairly comparable with doing other 'not electron' and 'not winforms' GUI things on windows in other languages. If you don't use Glade (which I didn't) you end up writing a fair amount of what essentially amounts to markup, but you can dodge a ton of imperative code by keeping app logic in a separate green thread and using TChans to pass signals around. I had a good time with this method, and my code felt fairly 'haskell-ish' by the time I was done. But, personally, I loathe Electron, as I feel that using an entire browser as a GUI framework is like driving a nail with a jackhammer, and I have similar reservations about GHCJS, so I was willing to put up with a fair amount of imperative schlep and glue code to avoid using those tools. Your mileage may vary.
There are various ideas to deal with this issue: **Parametrize all fields of your record with a functor** data Foo f = Foo { fieldA :: f Int, fieldB :: f String, fieldC :: f (Maybe String } You can then provide `Maybe` for `f` during parsing, then use a type `newtype Full a = Full { unFull :: a }` and write a function `checkBuild :: Foo Maybe -&gt; Either String (Foo Full)` to check if required things are there and build the correct version. There are also other use cases for this, see [ftypes](http://github.com/timjb/ftypes) and [opaleye](https://github.com/tomjaguarpaw/haskell-opaleye) for example. **Use an anonymous record library** Depending on which library you use, you can first parse your user input into an anonymous record and then later convert it into a Haskell type. I have written more information in this in [this blogpost](https://www.athiemann.net/2017/07/02/superrecord.html) **Use Template-Haskell to generate a "Maybe" Data-Type** This is a bit hacky, but given a type you could write a template haskell function that generates a similar type, but packs all fields in `Maybe a`. If think I saw some library for this, but I do not remember the name anymore.
there's also http://haskelliseasy.com
I really like what I see there so far, thanks for the link!
I hadn't seen that one before, thanks for the link! Do you know who the author is?
/u/bitemyapp
It is happening as far as I know. You should shoot an email over to the people in UPenn to check exact dates if you're planning to fly over here.
&gt; If we want to combine actions from two different type classes, we just need to merge the constraints Sure, but then you need to run the program, and that is when the problem begins (see how many instances are needed in mtl).
Think past initial authorship. Constraining the effects allows a fail-safe way to identify to the programmer what a given function may or may not be doing, or what resources it may or may not have access to. Compared to say, a large OO codebase in Java, this is a world of difference. Knowing whether or not the behaviors of a given line of code depends on the outside world in order to 'behave' properly allows you to deterministically trace the possible inputs into your runtime to their sources, which is an incredible aid to decision making while refactoring. Given the degrees of indirection that see fairly common use in large codebases, the difference here is extreme. You ever have the experience of, 2-4 hours into a problem, using Eclipse to 'go to declaration' about 15 times to identify the source of that thing you thought was a constant only to find out that it's buried in some weird ass properties file? That's an experience you don't have in Haskell, because the type signature and the structure of the language conspire to alert you to situations where that might be the case.
And also [the state of the Haskell ecosystem](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md), that not only feature lists of libraries, but also will tell you how Haskell is suited for certain tasks. (Also, shameless self promotion, `attoparsec` is not the fastest if you do not need an incremental parser, [`fastparser`](https://hackage.haskell.org/package/fastparser) is, even though it doesn't ship with many combinators).
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [Gabriel439/post-rfc/.../**sotu.md** (master → de073ca)](https://github.com/Gabriel439/post-rfc/blob/de073ca3c33b5df26716f5eca3b29998413bfa0a/sotu.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dnl51hd.)^.
MTL needs so many instances because it tries to make your life easier by doing lifting for you, i.e. `ReaderT r m` is not only an instance of `MonadReader`, but also an instance of `MonadWriter` on the condition that `m` is an instance of `MonadWriter`, etc. So the number of instances is roughly n^2 where n is the number of effects MTL abstracts (actually more because there are strict and lazy versions of some transformers). One does not have to do that in an application. If I have `MonadTerm` and `MonadLog`, I can run an action with the `(MonadTerm m, MonadLog m)` constraint simply in `IO`, or any concrete stack powerful enough for my purposes. I need only two instances `MonadTerm IO` and `MonadLog IO` for that. Or am I misunderstanding your point?
The [SOTU on native GUIs](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#standalone-gui-applications) rates the current situation as Immature because "there still isn't a Haskell binding to a widget toolkit that doesn't have some sort of setup issues with the toolkit." Given that [my bindings](https://github.com/deech/fltkhs-hello-world#fltkhs-hello-world) require nothing more that `stack build --flag fltkhs:bundled` to go from 0 to an app across platforms can we change that to Mature (or redefine immature)?
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [Gabriel439/post-rfc/.../**sotu.md#standalone-gui-applications** (master → de073ca)](https://github.com/Gabriel439/post-rfc/blob/de073ca3c33b5df26716f5eca3b29998413bfa0a/sotu.md#standalone-gui-applications) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dnl6upi.)^.
It's great you've decided to counterpoint the amount of attention free monads get, Haskell newcomers (like me) could benefit from that. I myself was doing some refactoring lately where I indeed was considering free monads, but it felt like it could became too complex (things like that `class Inject` from your post. - that's just something I don't grasp at my level of proficiency). On the other hand, many people (in the comments to various free monad articles) were countering them with the usual "MTL" approach. So I went with this instead, achieved all I wanted without much headaches. Going with free monads when I wanted to just simply capture couple of "domain specific" notions in an application would be too much. It's worth mentioning that going with such typeclasses differs greatly from what one can find in the mtl, simply because such typeclasses are used differently, there's no need for such big amount of instances.
You can submit a pull request to the repo to change it, see Disclaimer #1 :)
&gt;Comparing safe Rust to pure Haskell is more awkward, since pure Haskell can't really do anything and once you hit the escape hatch in either you can do anything. I think what's good about Haskell is not that you *avoid* pure code, but rather that you can easily *mix* pure code with impure code while still guaranteeing purity at compile time. That is not the case in Rust.
&gt; everything, including IO, is 100% pure (referentially transparent) I don't think this is the case. A function such as `readFile` will *not* yield the same result every time it is run. &gt;pure code is context insensitive, which implies greater composability (building big things by assembling smaller things), and compositionality Actually, composability is guaranteed by monads. You don't need purity in general.
&gt; I think we get more benefit from the cultural style of avoiding mutation even in IO That's not purely cultural. Haskell is immutable by default, and mutating data is painful enough that sticking to immutability in the majority of cases makes sense.
Do you run into space leaks in practice? I find them rare.
_Do you run into_ _Space leaks in practice? I find_ _Them rare in practice._ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^vem_ ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
If you want a record minus one field or record with a hole, the simplest way is a to use a function `missing -&gt; FullRecord`. You can even give it specific instance. I used this trick to parse CSV when for example the processing date is "missing" from the CSV. I could give a ParseCsv instance to my function.when I parse the CSV, I get as a result of list of function, which I can then feed with the current date to get a list of full record.
I've written [metamorphosis](https://github.com/maxigit/Metamorphosis) exactly for the later case. Do some TH to inject a functor in some field of a record (to use with Yesod defined records)
I've never used Yesod, so maybe there is something related to it that I'm missing, but would be wrong with having an internal-only User representation that is backed by Persistent and an external facing User that doesn't contain those fields? This way your types are as specific as possible everywhere. The only downside that jumps out at me is that there is duplication of fields between the models, but depending on your app the models may be different enough where this is OK. Alternatively you could factor out the common stuff into a separate type. 
This is only a problem for library writers. For application developers, you can usually get away with `n` instances for `n` effects based on a custom concrete app monad that everything monomorphizes to in the end.
That's exactly the thing i was kinda worried about, the glue code, but if you say that it's not much of a problem, I guess i will try it. About the build process, what is it about it exactly that makes it obnoxious?
I'll do that but I wanted you to know things have changed for the purposes of your centralized resource. The situation today is far less grim.
I wouldn't go so far as to say "harmful" (there are legitimate reasons to use `Free`). But I do agree with the general premise that `mtl`-style is usually better than `Free`. The obvious reason is performance; `mtl`-style is generally about 4x faster (and the difference only gets more dramatic as the number of effects scales), and if GHC manages to fully specialize and optimize the entire app, I've seen it get up to 30x faster. But also, there's just enough minor things that are impossible with `Free` to be annoying. `ContT` is the most obvious one, but you also can't do `MonadFix`, which comes up occasionally (unless you use some kind of final(?) encoding, but I'm not sure the performance implications). All in all, the only serious cost of `mtl`-style is the n^2 instances problem. But if you're having trouble with all the instances you have to write, just make a top-level application monad and write the instances there. Or just write the instances; it's boilerplate-y, but it's easy and the types usually make it hard to get wrong.
It is used in examples where there is a cyclic dependency. For example when the `Dynamic` text of a button depends on something which depends on the button event.
`mdo` uses `MonadFix` to allow you to reference things defined lower down in the monadic block. Which if you notice is often done in the examples. It allows for monadic mutual recursion which can be really awesome for certain tasks, such as codependent inputs that update each other, or just to allow you to define an output box that appears earlier in the HTML than an input box. 
The issue I ran into was that the windows GHC install had a different version of `zlib1.dll` then is required by, I believe, cairo, and the stack env kept shoving the GHC directory to the front of my path, making it evaluate before the dll for msys2 and tanking the build. At the time, the situation was further complicated by me having multiple msys2 installations on the same box, an obnoxious config setting on my machine that was causing inconsistent interpretations of my home directory, and numerous other tiny issues that conspired to make the build process roughly as enjoyable and straightforward as untangling a burning ball of twine with my bare hands. My general experience with doing anything whatsoever on windows is that everyone's box has some basic flavor of said issues lurking beneath the surface, waiting to tank a given build effort. So I suspect you may not run into that issue, but will probably run into some other, similar one. Also, there's the issue of packaging executables for deployment, which is more obnoxious DLL/resource wrangling. All in all, familiarity with windows as a dev environment will save you a lot of headaches, and familiarity with the specific windows machine as a dev environment will save you the most headache of all. I had some of the former, but very little of the latter, and so had a pretty rough time. 
I'd be interested to hear after the deadline about how many applications you've received and how qualified the applicants were.
Yeah, as I said the title is mostly a click bait. What I think is harmful though is the level of attention and promotion free monad is receiving compared to its actual utility in most cases.
It already exists, though your version might be better (I haven't looked at your code): http://hackage.haskell.org/package/interpolate-0.1.1/docs/Data-String-Interpolate.html
By the way, you might be interested in my [fmt](http://hackage.haskell.org/package/fmt) package for doing formatting. It doesn't provide a quasiquoter, but it still results in pretty succinct code and has lots of custom formatters.
Thanks! Also, looks like I have reinvented "interpolate" .
(In addition to this) What I'd really like to see is (a nice presentation of) the metrics around library usage. - Reverse dependencies a la [http://packdeps.haskellers.com/reverse](http://packdeps.haskellers.com/reverse) - Popularity (such as a graph of downloads and/or page views over time) - Data from hackage and github - package age, age of most recent commit, number of contributors, number of/popularity of dependencies - Maybe some objective information from which to gauge maintainer reliability, like number of libraries thay're listed as contributors on, or some measure of activity - Links to relevant blog posts, articles, papers, reddit threads, and Stack Overflow questions. Might be good to highlight newer posts, so questions get answers. Preferably with some graphs and visual indicators of things like freshness, popularity, and breakage. Popularity certainly isn't everything, but it can be a pretty good signal. It may not be as actionable as a recommendation, but they'd be somewhat objective, the need for maintenance would be pretty minimal, and it would offload the work of doing more subjective recommendations to blogs. We already have some of this with Hackage, the HaskellWiki, etc., but as you've mentioned, it's all pretty scattered, not very well presented, and not always up to date.
That’s why I added “…which may be just because the language makes it inconvenient”. Even though it would be *possible* to use an `IORef` or `STRef` in every case where you would use a mutable variable in another language, Haskell makes inline side effects inconvenient and has no syntactic sugar for common mutating operations. If you have to write `modifyIORef' x (+ 1)` instead of `++x` or `f &lt;$&gt; readIORef x &lt;*&gt; readIORef y` instead of `f(x, y)`, then you’ll probably reconsider whether mutation is necessary. It would be *possible* to add these things to the standard libraries and language itself, but culturally there’s no demand.
I usually prefer xmlhtml to xml for xml processing.
https://www.athiemann.net/2017/07/02/superrecord.html
Could you elaborate on why that is? Ease of use? Documentation?
To talk to X, use the libX. However, I don't think there are Haskell bindings.
Personally, I've mostly only been seeing the "overselling" of Free (and similar) from some Scala quarters. I suspect that it has a lot to do with the fact that it's "trivial" to implement a trampolined Free and thus (mostly) do away with a major problem with monadic programming in Scala[1] in a systematic manner. AFAIK there's no other approach to monads in scala that actually solves the problem of TCO "in the framework" so to speak. (Happy to be corrected, of course.) [1] Which would be the lack of guaranteed TCO.
Thanks for sharing this. 13 is a respectable small team on the brink of becoming a mid size team. Unfortunately, being unable to touch and feel the app, the data point of Takt is not very meaningful for me, perronally. But it is still a success story, of hopefully many others and should be on the home page of GHCJS. Would you be able to share the number of end-users within the company using the Takt product? Desktop or browser? Also, if I'm correct, was it your team facing TH compilation issues with GHCJS? How did you handle them? 
This is neat and all, but I don't get the obsession with printf-like format specifiers. I mean, why not just use string interpolation like, say, `"Hello, {name}"` and a type class for interpolating? (So, in my example "name" would be a binding obeying an imaginary type class `Interpolatable` or whatever.)
Absolutely! I'll make sure that's included.
Wouldn't that require compiler support? I think the point here is to discuss something that we can now do in PureScript _as a library_.
Gah, I meant to add a note about that at the first usage! I actually started planning this version of the series after someone else was confused about RecursiveDo and reflex-dom in a Reddit thread ages ago, and was really keen to write something to try to help with that. I'll take care of that when I get into work this morning. Also: thanks for asking the question - if it wasn't clear to you, it probably wasn't clear to lots of other people who didn't end up asking, and the lack of clarity is definitely my problem to solve rather than something you are meant to struggle through. Edit: I've added a little explanation now, you might need a hard refresh to see it.
Kind of, but one could take a leaf out of Scalac's book here and do a StringContext-type thing to support very general types of interpolation. Essentially the compiler would rewrite `"Hello, {name}"` to a bit of code which could be handled by the surrounding context (imports, etc.) however the programmer wanted. (Actually, you'd want a prefix for that, so `xyz"Hello, {name}"` and `sql"Hello, {name}"`, etc. to support different types of interpolation in the same 'scope'.) Even without the generalization: This is one of those features that's *so* useful and generally practically applicable that I would actually be fine with baking it into the compiler, FWIW.
One of my favorite quotes, due to Andrew Appel (at his PLDI 2004 keynote IIRC) "the only difference between a type system and a static analysis, is that when the type system reports an error, its the programmer's fault" ;-)
We are using this technique without much problem. I thought this was standard practice. Is this up for debate? 
It's much more memory efficient. `xml` nodes are Strings, `xmlhtml` uses Text.
Printf is the hello world of these metaprogramming tools, this is where the obsession comes from. Even scala has a built-in `f` interpolator, which is very similar (implemented as a macro). As /u/paf31 said, the idea here is more to demonstrate a lightweight way of achieving the same goal, without building macros into the compiler (I'm not arguing that macros are not useful!). I agree that your example is more interesting, and it turns out that it can also be implemented using the same technique
Thanks, I've made a note of that.
I think you mean: odd x = x `mod` 2 /= 0 
I find `drop` a little more legible in these situations: rr = 1 : 0 : 0 : 1 : 0 : zipWith5 myFunc (drop 0 rr) (drop 1 rr) (drop 2 rr) (drop 3 rr) (drop 4 rr) `tails` can also work nicely: rr = 1 : 0 : 0 : 1 : 0 : [ myFunc a b c d e | (a:b:c:d:e:_) &lt;- tails rr ]
Wait! There's *another* upcoming Haskell book?! Is Julie involved?? 
TIL, thanks.
I think we mostly agree (you, me and /u/paf31), it's just that I think that all the machinery required to achieve this may be... overkill? The Scala approach to interpolation seems fine to me. (Obviously, it needs to be refined, but I blame that on not having native support for type classes and defaulting to ".toString()", ugh. Can't believe we now have to have *linter* to prevent us from inadvertently calling .toString() in various situations. Anyway, that's a different rant! :) ) Yes, Scala does have `f` interpolation, but I think I've seen it about... once in ~7 years or programming Scala full time. It's a gimmick... and, btw, could also have been implemented as a library. The "idea" of interpolation is built-in, but not the mechanism. Again, don't get me wrong, this is kind of a great demo of the type system, I just don't think it's a good *use case*.
Worth noting that `Free` data Free f a = Pure a | Free (f (Free f a)) can also model other things like binary trees data Tree a = Leaf a | Branch (Tree a) (Tree a) which is `Free` over [2D-vectors](https://hackage.haskell.org/package/linear-1.20.6/docs/Linear-V2.html#t:V2) data V2 a = V2 a a newtype Tree a = Tree (Free V2 a) pattern Leaf :: a -&gt; Tree a pattern Leaf a = Tree (Pure a) pattern Branch :: Tree a -&gt; Tree a -&gt; Tree a pattern Branch left right &lt;- Tree (Free (V2 (Tree -&gt; left) (Tree -&gt; right))) where Branch (Tree left) (Tree right) = Tree (Free (V2 left right)) that lets us derive functionality deriving newtype (Functor, Applicative, Monad, MonadFix, MonadFree V2, Foldable, Foldable1) -- Derivable in the future: (Traversable1, Bind, Plated) deriving stock (Traversable) 
It's the practice that seems to be the most manageable for effectful code. Other people are experimenting with other methods, but as far as I can tell this is the standard.
&gt; This is only a problem for library writers. Sure, but when I use things like free monads, it is for my own DSLs, I am not using someone else's library. For me that is the point of free monads, it is very easy to add "commands" without having to alter all instances.
If you want variable look-back, you can use something like: continue :: [a] -&gt; ([a] -&gt; a) -&gt; [a] continue initial f = initial ++ go (reverse initial) where go as = let a = f as in a : go (a:as) which lets you do things like: &gt;&gt;&gt; take 10 $ continue [] length [0,1,2,3,4,5,6,7,8,9] &gt;&gt;&gt; take 10 $ continue [1] sum [1,1,2,4,8,16,32,64,128,256] &gt;&gt;&gt; take 20 $ continue [0] $ \as@(n:_) -&gt; (as !! n) + 1 [0,1,1,2,2,2,3,3,3,3,4,4,4,4,4,5,5,5,5,5] 
By the way, coincidentally I have just today uploaded some code (https://github.com/mtesseract/th-format) implementing the standard Scala interpolation behavior. The Scala code s"Foo is $foo" translates to [fmt|Foo is $foo|] 
&gt; All in all, the only serious cost of mtl-style is the n2 instances problem. I'm still amazed that this gets brushed aside so regularly. The trouble is not about having to write the instances, the trouble is you *can't* write the instances without introducing orphans. Let's take an example with the effects of * `MonadDb` to connect to some SQL database. Comes with `runDbT` and `DbT`. Defined in a `monad-db` library. * `MonadLog` to do logging. Comes with `runLoggingT` and `LoggingT`. Defined in a `monad-logging` library. Now these two are - out of the box - incompatible. `DbT` does not implement `MonadLog`, and `LoggingT` doesn't implement `DbT`. These effects cannot be combined. So what are our options? One is to explicitly `lift` effects, but the whole point of `mtl` is to avoid explicit lifting. &gt; Just make a top-level application monad and write the instances there. Ok, let's run with this. But what if we want to introduce a scoped effect? `ExceptT` for example is very convenient to drop in for a small chunk of code: ok &lt;- runExceptT $ do a &lt;- queryDatabase log "Done" return a Now we're stuck again! Here `queryDatabase` and `log` are both used with `ExceptT`... but `ExceptT` doesn't have an instance for *either* `MonadLog` or `MonadDb`! One of the real problems is that *most* effects are algebraic, but we don't use a single monad transformer that knows that algebraic effects can be lifted. I wrote https://hackage.haskell.org/package/transformers-eff as one attempt to provide a common transformer, and `simple-effects` has another approach https://hackage.haskell.org/package/simple-effects-0.9.0.1/docs/Control-Effects.html#t:EffectHandler that I think might by what I wanted, but done better.
&gt; AFAIK there's no other approach to monads in scala that actually solves the problem of TCO "in the framework" so to speak. You could use [tail recursive monads](http://functorial.com/stack-safety-for-free/index.pdf) (PDF link), which we use in PureScript, and which have been incorporated into both scalaz and cats. Most everyday monad transformer stacks are tail recursive, with `ContT` being the notable exception.
/u/kaol why are you interested? I want to know in order to give a good answer. At this point I can say that the competition for this position/s has been huge, even with the semi-hard relocation requirement to a Northern country (some see it almost as a benefit - I myself can't see why you want to come here though :D) Although I should not be surprised at all: this would have been my absolute dream job a few year ago. Since no-one had my dream job to offer, I decided I'd create it myself and it just so happened that RELEX Solutions let me do just that - and produce some awesome value to the company in the process as well - that's only one of many reasons I like this company so much :)
Oh, very nice! I hadn't heard of this approach. (I still want ContT, though :) )
You don’t *have* to introduce orphans. When you’ve got two transformers and you want them to exchange instances, *write a newtype*. It’s insanely easy and pretty much always solves the problem.
Julie (and a different Chris) is writing [Joy of Haskell](http://joyofhaskell.com/), which is unrelated to our book. Chris Allen is working on [Haskell Almanac](https://lorepub.com/product/cookbook) afaik.
Well, one solution is newtype MyExceptT e m a = MyExceptT e m a instance MonadLog m =&gt; MonadLog (MyExceptT e m) instance MonadDb m =&gt; MonadDb (MyExceptT e m) but this really means that &gt; But if you're having trouble with all the instances you have to write, just make a top-level application monad and write the instances there. is a bit of a lie, because you also need a `newtype` for every "local" effect that you might need (as demonstrated above).
I wrote a lengthy reply but then my browser crashed :/ Here is a brief summary: * you only need IO, just write in IO, you don't need typeclasses. * you introduce several typeclasses, but they will only ever work together, in the same base monad. You also do not need typeclasses! * you need several typeclasses, and need them to be composable. In that case, you are in the mtl situation. * you need several typeclasses, and several base monads. That is cool, but: * let's hope your effects are not too complex, and do not interact in weird ways (state/catch for example), because their interactions will be modelled in all instances (most likely in Applicative/Monad/Alternative) * let's hope you do not need to add complex effects either, because not only will you have to add complex logic to all instances to make sure the effects do not clash, but you will also have to maintain the Alternative/Monad/Applicative laws To me, you gain most from free monads in complex stacks, because, and I understand this is a matter of taste, it is really easy to write/understand the logic in an interpreter, compared to the `&lt;*&gt;`, `&gt;&gt;=` and `&lt;|&gt;` functions. Also, you get the Monad/Applicative/etc. for free, and nice tools, especially with the free applicatives.
So PureScript can uncons Symbols but Haskell still can't. Interesting.
Except you're blowing out of proportion how often you need an unexpected combination of local effects. It's far from n^2. In fact, it's really quite rare when you control your top-level `newtype`. Not to mention, it's really not the end of the world to sprinkle a `lift` here and there because you have to use an explicit transformer on top that doesn't support this one function call, as long as it's rare (which it is)
That's a really clever idea!
Why not just use `formatting` which is a superb value level implementation? `printf` was a hack to begin with. Why are we so insistent on copying it? EDIT: https://hackage.haskell.org/package/formatting
&gt; On the other hand, many people (in the comments to various free monad articles) were countering them with the usual "MTL" approach. So I went with this instead, achieved all I wanted without much headaches. MTL works fine, it's just a question of avoiding convoluted monad stacks. And having signatures like (MonadError ErrorType m, MonadIO m, MonadState StateType m) =&gt; m () &gt;Going with free monads when I wanted to just simply capture couple of "domain specific" notions in an application would be too much. Free monads as they stand in Haskell are... not my favorite. They're free with respect to a particular functor, not free among all monads. I don't know exactly how the freer monad works, but it might fix some of this.
A comparison: * [This is RWS](https://github.com/bartavelle/stateWriter/blob/3e4602b8309bf3e12749032abf7b672d1909e611/Control/Monad/Trans/RSS/Lazy.hs#L115-L136), as a single typeclass (and W as a state) * [This is reader, state, monaderror, promises, and player interaction](https://github.com/bartavelle/7startups/blob/00490a41ae1695ac97282de80b4fc60bfa3d3915/Startups/Interpreter.hs#L46-L73) as an `operational` interpreter, along with [a pure instance](https://github.com/bartavelle/7startups/blob/00490a41ae1695ac97282de80b4fc60bfa3d3915/Backends/Pure.hs#L13-L25) Unless you are used to writing these instances, it is not immediately obvious what the first example does. In the second example, it is pretty obvious that the state is updated in the `catch` case.
I'm not really sure how demonstrating a problem is blowing something out of proportion.
It is not a problem that can be solved by library writers without every library depending on every other library. All I'm trying to point out is that there are legitimate problems with `mtl` too, even though it is my tool of choice.
This is a pretty common thing to want to do. I've experimented with a variety of ways to approach it, and *by far* the easiest/less error prone way is to copy the definition and use vim macros to make the changes you want. The issue with Template Haskell/generic solutions is that you end up needing to specify fields and identifiers in a somewhat awkward way, along with specifying how you augment or alter them (eg `this field is a Maybe now`). These settings and configuration options are pretty difficult to capture in a way that gives good UX. Simply copying and pasting the data definitions, changing the fields/types, and letting generic/TH Aeson decoding do it's thing is very easy, gives great error messages, etc.
Hasn't been rare in my experience. In fact it was my number one complaint when writing code. "Non-deterministic computation would really simplify this code, I'll just use ListT" and then you use some database function from some `MonadDb` class and off you go implementing all 50 methods of that class for a relatively complicated transformer, neither of which you made. Writing silly glue code that mostly consists of `lift` in various places, or would consist of `lift` if the library author kept in mind that someone else will be writing instances for their class. 
Does `Cofree` have any hidden downsides? I use it all the time.
`simple-effects` made my life so much easier. Though, as I use it, I realize that the major benefit isn't in the machinery it provides, but the fact that you can ONLY write liftable effects with it and that you get an overlappable instance for all your effects. You could easily do this with the simplest mtl approach. I still think overlapping instances are needlessly shunned.
&gt; formatting I think there may be a link here that I'm missing...? EDIT: (After looking it up on Hackage:) Yeah, I agree. I don't understand the worship for printf-like formatting. Though there are a *few* situations where the "quotes-inside-text" approach makes a lot more sense -- in particular, when the majority of your content is 'text'. My go-to example of that would be SQL with interpolated 'bits and pieces which cannot be handled by prepared queries'.
All I meant is that it's not nearly big enough a problem to warrant throwing `mtl`-style out entirely. It's very minor in my experience.
I don't like this standard. I don't like that it doesn't let me use local effects. For a community that puts so much emphasis on composability we sure are quick to give it up in this case.
Writing one instance for one transformer isn't bad. Or at least, it wouldn't be if people used `DefaultSignatures` to make their classes derivable =/
No one's saying you can't write functions like foo :: (CanDoThing m, DoesOtherThing m) =&gt; a -&gt; m b They'll just eventually fold into instance CanDoThing App where doThing = ... instance DoesOtherThing App where doesOtherThing = ... Whatever you end up interpreting `foo` into needs to be able to provide at least those effects.
But you're only considering global effects. What if I want to introduce non-determinism in the middle of my program and contain it there?
DefaultSignatures would go a very long way into fixing this mess. I'd still prefer an overlappable instance though.
Use type classes as normal, and run the special effects afterwards. bar :: (MonadLogic m, CanDoThing m) =&gt; m Int observeT bar :: (CanDoThing m) =&gt; m Int The eventual concrete type of `bar` is `LogicT App Int`; the concrete type of `observeT bar` is `App Int`.
While very nice, this is the reason why I prefer [the operational monad approach][1] (which is Yoneda (or coyoneda, I forgot) on Free). Free monads can encode laws in the Functor (here: distributivity) that, I think, should not be part of the definition of the monad. [1]: https://apfelmus.nfshost.com/articles/operational-monad.html
Right; `Free` is a great way to have a local, small DSL that you want to dynamically generate and have easy testing on it. You can have `runSomeDsl :: Free SomeCommand a -&gt; App a` and have the best of both worlds quite easily :)
In practice, I agree and I would use something like `formatting`. But maybe I want to pass around formatters at the type level for some reason, who knows? Or maybe I'm already working at the type level anyway. As another non-printf example, maybe I could use symbol parsing to concisely specify my type-level routing table in a `servant` app.
&gt; the trouble is you *can't* write the instances without introducing orphans What is wrong with writing an orphan if you don't export it? Seems to me there's no way that an orphan instance could be a problem if you're only declaring it in the module where it is used.
I think if you make the `forall`s explicit, it will be more obvious why this doesn't work. How many inhabitants does `forall a. a^(a * a)` have? The rule doesn't tell us, because this isn't of the form `a^b`, there is a `forall` in the way.
Exactly. And now you have to provide an instance of `CanDoThing` for `LogicT`. This is contrary to only giving instances for your final monad newtype.
You wouldn't be solving an equation for `a`. The universal type of `fst` and `snd` hides a quantifier: `forall a. (a, a) -&gt; a`. The quantifier can be seen as some operator `A :: (Type -&gt; Type) -&gt; Type`, defined by `A f = forall a. f a`, applied to the type function `F a = (a, a) -&gt; a`, so the equation would be in fact variable-free: A F = 2 but it's unclear whether there is a sensible "algebraic" meaning (in the sense of that post) to give to `A`.
Ah, I guess I was assuming we could "distribute" across the `forall` without any real evidence. Is there a rule for `forall`?
Re mtl vs. transformers. These days they are part and parcel of the same thing. `transformers` provides data types and a few Haskell 98'ish typeclasses. `mtl` provides classes that sit atop those data types.
All instances are always exported. But, yeah, orphans are nothing to worry about in application code; they only muck up library code.
&gt; All instances are always exported. Well there's your problem...
Interesting, is there any reason other than historical and backwards compatibility why they couldn't be combined into one? Also, it seems like the mtl approach is what's being championed by the community lately instead of working with concrete transformer stacks, would it be correct to say that in general you should just use mtl (and transformers just becomes an implementation detail)?
It's the only way you can even begin to get coherence. Without coherence it's not "safe" to pass the dictionaries implicitly.
The only difference between '&gt;&gt;' and '&gt;&gt;=' is the result of the expression is discarded. BTW, I'm talking not about 'programmable semicolons' - just that they and their equivalents in non-C-like languages can be thought an operator, no different from any other. There's _zero_ analogy here and that's the point. C and their like have a single monad instance you're effectively stuck with, and the operators are named a little differently, but it's there.
 {-# LANGUAGE ConstraintKinds #-} ... type FatStacks m = (MonadError ErrorType m, MonadIO m, MonadState StateType m) foo :: FatStacks m =&gt; m () foo = ... 
They _were_ originally one package. They were deliberately split apart in part so that folks who didn't want to deal with the classes or who wanted to support other compilers could work with just the data types. If you're just writing instances for the data types involved you can get away with the slightly smaller dependency. (There were some other historical motivations in the split, namely that at the time there was some pressure to try out an API built around type families rather than fundeps, but `monads-fd` never got any traction mainly because of the module name conflicts.) Having it as a separate package means that the cottage industry of different attempts to make different kinds of instances via tagging, etc. can still share common underlying data types as well. You can think of the existence of the `transformers` package as an implementation detail.
I've never seen `Cofree` used in the wild. Do you have any examples that are public?
Makes sense. I was naively thinking is might be possible to add something like a `hide instance Foo IO`, but now I realize that might make type-checking impossible.
There's a whole suite of *really* similar QuasiQuoting string template packages. I wrote http://hackage.haskell.org/package/QuasiText-0.1.2.6/docs/Text-QuasiText.html in 2012 which is still almost certainly not the first one. Mine has a bigger version number than yours though. (na na na na na)
Thanks for the explanation, that makes perfect sense.
It's not about making type-checking impossible. It's that typeclasses are no longer coherent, which can very easily introduce bugs and makes general reasoning about your program harder.
Practical use-case for this, please. (not sure I understand what you mean) 
Why not add a "similar libraries" section to hackage along with user comments and voting? Using UGC might make the process of such curation faster and easier to maintain. 
[TIL Deriving strategies](https://ryanglscott.github.io/2017/04/12/improvements-to-deriving-in-ghc-82/) 
For me, the main premise of these static analysis tools are that it is too hard to make programmers change languages or use new type systems, or that too much code has already been written. I may be missing something. Someone rightfully corrected me in the past that a lot of deep work has been put into furthering the analysis tools, but I still hesitate to accept whether the effort should have been spent there or in other places.
Hi all. I'm working on a NES emulator. It's coming along nicely, but I have some huge performance issues. I need to get 60fps, I currently get about 5 fps. I've played with other emulators written in JS and Go, and they easily get 10x performance (so 600 fps). I refuse to believe this is a Haskell constraint, and more just my poor code: https://github.com/dbousamra/hnes/ Would anyone be able to take a look at the code and see if there's anything obvious. Here's a profile run: https://gist.github.com/dbousamra/37f244de7cfbd76b3e3b9793731556f5 I am not sure what those SDL exceptions are, but even disabling rendering (i.e. just fill a pixel buffer) I receive poor FPS. Most of the time seems to be spent in load and store. 
I asked about this a few years ago and got [an interesting response](https://www.reddit.com/r/haskell/comments/2bj7it/let_me_tell_you_about_the_types_of_data/cj5y701/).
That is indeed very interesting. Thanks!
Are they working on it? https://ghc.haskell.org/trac/ghc/ticket/11342
Sometimes I watch a Computerphile video and wonder why they uploaded it. Reading through the comments it's clear nobody new to the subject understands what the point is. 
Metamorphosis tried to give a nice UX by providing an interface similar to sed (in principle) but using lenses. Something like $(metamorphosis ( return . (fdTConsName .~ "RecordF") . (fdTypes %~ ("f":)) ) [''Record] (const (Just applicativeBCR)) (const []) ) Means , change the record name to `RecordF` and stick `f` in front of every type. The two last lines are default options which could which could be hidden somehow. Where I agree duplicating a record by hand is pretty straight forward and has some advantages over TH (one being that you and your editor, can actually see the class) what metamorphosis gives for free is the conversion function between the two types .
He's probably asking as an applicant to see whether to keep his hopes up :).
That is very cool!
Hi Thorsten! &lt;3 Love: someone who survived you as a PhD supervisor ;)
Eh, not necessarily all of the CP videos are designed as ELI5. Thorsten once did a video on quantum computing that didn't air because THAT didn't make the cut in terms of accessibility, but this is perfectly legitimate for people who've done a smidge of type theory. Hail incoming downvotes!
I don’t know of any Template Haskell solutions for that sort of thing, if that’s what you’re looking for, but [you can use the technique described in this blog post, which uses `DefaultSignatures` and doesn’t require any extra libraries](https://lexi-lambda.github.io/blog/2017/04/28/lifts-for-free-making-mtl-typeclasses-derivable/).
I recently discovered the channel and recommended some of them to my wife. I know for a fact that she'd get nothing out of this even though this is exactly the kind of stuff I'm interested in! Meanwhile there's other videos on convolutions or JPEG encoding which are way more intricate topics that are much clearer.
Would you be open to extending your package such that it also includes an easy to use quasi quoter in addition to +| and |+? I dislike this kind of fragmentation and wonder if it makes sense to focus efforts on one such package.
Types share common algebraic structure with numbers. But they aren't numbers. In particular, types form a semi-ring, and there is a semi-ring homomorphism from the semi-ring of types to the semi-ring of cardinals (up to the first infinite one) which is given by a types "number of inhabitants". However, and this is the big however, it breaks down there: there are countably infinite values of type `Nat` (for natural numbers) but also only countably infinite values of type `Nat -&gt; Nat`. Worse though, there is no known way to interpret `forall` in terms of numbers and I don't expect anyone to come up with one. The problem is that `forall` is a strange and magical beast. One can see this just from you example. One can rather easily prove that, ignoring bottoms, the only terms of type `forall a. (a,a) -&gt; a` are, up to observational equality, `fst` and `snd` by use of the [free theorem](https://people.mpi-sws.org/~dreyer/tor/papers/wadler.pdf). Clearly though, there are many more than two functions of type `(Int,Int) -&gt; Int`, so what makes `forall a.(a,a) -&gt; a` different? In a word: parametricity. Any term of type `forall a.(a,a) -&gt; a` must do the *same thing* for any choice of `a`. And that, "doing the same thing" is what is interesting and isn't captured by numbers. Indeed, models of parametricity tend to require mathematics much more powerful than arithmetic. A fun fact is that while *soundness* of System-F (that the type `forall a.a` is uninhabited) is an easy theorem to show by elementary methods, the fact that every System-F program terminates (which we normally think of as a generalization of soundness) implies the consistency of arithmetic! Also note: with bottoms one gets more possibilities. Like, for instance, nope :: (a,a) -&gt; a nope _ = undefined and with `seq` one gets still more sfst :: (a,a) -&gt; a sfst (a,b) = b `seq` a 
This sounds like a really nice solution! I've attempted this, but I'm wondering if I can improve on the code below by re-using the generic FromJSON instance for User, rather than explicitly naming each field. Even if not, this is nice for a small record type. instance FromJSON (Bool -&gt; User) where parseJSON = withObject "user" $ \v -&gt; do firstName &lt;- v .: "firstName" email &lt;- v .: "email" return $ \verified -&gt; User Nothing email firstName Nothing verified
Both approaches free and mtl fail miserably in terms of composition, and this is mostly, not a failure of them, but something more basic: the definition of `&gt;&gt;=` `&lt;*&gt;`, `&lt;|&gt;`. Since these operators do not consider effects in their signature, there is no way to compose two expressions that perform different effects keeping mathematical laws. This precludes composability for real world haskell components and applications. This problem makes Haskell more rigid and unworkable than other programming languages when it should be the opposite: the language that offers more opportunities for algebraic composition. This is an artificial problem due to the lack of expressivity of the monad classes, in which the effects are not considered. It can be solved by promoting the graded monad: https://github.com/dorchard/effect-monad See this thread: https://www.reddit.com/r/haskell/comments/6mt8i6/do_you_have_to_choose_between_composability_and/ It is socking for me that nobody is conscious of this problem. Opinions?
Thanks, that's really useful. If you're using an anonymous record library, would you even bother with the regular Haskell type? And does it then make sense to define all of your record types using that library? I guess for what I'm working on now, the first two suggestions would mean ditching persistent's TH and writing all the required instances by hand - which may be too great a cost.
I want to run a piece of my code in a `ListT` transformer so I can use non-determinism, but I also want to gather all the results in the middle of the program, not at the top. This forces me to handle this effect which makes a piece of my transformer stack concrete and now I have to write instances for `ListT`. The use cases are the usual ones where you'd want non-determinism.
Thanks, that's promising. However, I only have 1 target monad transformer. so by using this approach, I actually need to write more code than what I have right now. This approach works best if there are many target monad transformer. Do you know if it's possible to generate that with Template Haskell? (I don't know much about TH currently)
A video about programs as proofs and no mention to Curry-Howard? 
If plotted you would probably see a direct correlation between number of monad articles compared to amount of people who still don't understand monads.
They mention Curry-Howard in the preceding video by the same guy.
&gt; free among all monads What would that mean?
I'd like that, but unfortunately your version won't make it into `fmt` because it relies on haskell-src-meta and by extension haskell-src-exts, which is a huge package that takes a ton of memory to compile, i.e. from a tiny formatting library that you can use pretty much everywhere we're going to something that can't even compile on VPSes that lots of people (apparently) use for development :) This said, I'm open to either a) adding a simpler version of the quasiquoter, or b) implementing a tiny expression-only Haskell parser that could be used as a replacement for haskell-src-exts. (It's not that hard actually, Nikita Volkov [did it](http://hackage.haskell.org/package/record-0.3.2/src/library/Record/Parser.hs) for his `record` library and perhaps we could convince him to split off his parser into a separate package.)
Informative post, thanks!
We started doing this for data science/numerics/machine learning over here, and always welcome new material (library reviews/guides etc.) : http://www.datahaskell.org/docs//community/current-environment.html 
In TH you can get the definition of a class, and then generate an instance based on it. I don't know of a library does that but here's [code] (https://gist.github.com/Lysxia/2bb87c4320605048022fcdca9f33c400) Here, `lifts` is a generalization of `lift`, `lift2`, `lift3`. A more elementary alternative may be to look at the type of each method (wildcard in the `SigD` constructor in `autoLift'`) and pass around the right number of arguments before just applying `lift`.
Not helpful atm but I am working on an extension to GHC (with Ryan Scott and Andres Löh) that lets you do this
awesome! do you know when it will be available to use?
How is this worse than just coercing the `Int`s into `WEIRD`?
TU Munich published a [paper](https://www21.in.tum.de/~traytel/papers/haskell14-teaching/index.html) on Haskell teaching which features QuickCheck acceptance. Maybe they provide some detail.
Glad that the author "does not mean to be so categorical". But anyway, the point of this post was to trigger a discussion, and that it did. Taken together, this post, plus the discussion here, plus the various previous posts taking the opposite position and advocating free monads, give me a much clearer picture about when this tool provides the most value. Thanks!
It destroys coherence, we bring a different `NUM Int` dictionary into scope shadowing the existing one
Hopefully next release 
I'd suggest against copying over the old submission; better to create a fresh checkout of the grading harness for every submission. This also lets you parallelize your automation, naturally. The Test Anything Protocol (TAP) is a very convenient textual interchange for test results; it's so simple that a small regex and statemachine system can parse it (I tend to use expect for the job), and there are libraries for just about every language under the sun, including Haskell: https://hackage.haskell.org/package/tasty-tap . It is not a testing framework and not directly relevant to the question itself, but I wrote https://github.com/nwf/grade to help with the act of managing grades, and it might be useful to you.
Sorry to bother, but I had the same problems as the other guy when I tried to use mtl style, but I don't understand why DefaultSignatures would help with this particular problem. Is there an explanation somewhere?
I'm using a "homegrown" Haskell web app for doing assigments with automatic feedback for introductory programming courses at the University of Porto. Initially I made it just for Python (using doctests), but recently added support for Haskell and C (using QuickCheck and the C-&gt;Haskell FFI). The code is publicly available at github: https://github.com/pbv/codex and https://github.com/pbv/codex-quickcheck; unfortunately the documentation on installation and use is rather short at the moment. You can contact me if you want some extra information.
The `xml` library is perhaps standard for casual lightweight parsing of XML. But it is not designed for serious XML work. AFAIK, the two fully-featured XML libraries that are most widely used and best supported are [HaXml](https://hackage.haskell.org/package/HaXml) and [HXT](https://hackage.haskell.org/package/hxt). Another popular library intermediate between those two and xml is [xml-conduit](https://hackage.haskell.org/package/xml-conduit). While it does not have as many features as the other two, it does provide a full XML tree, a SAX-like streaming parser, and a clever representation of XPATH-like search cursors. Regarding the name "xml-conduit": It does have good support for [conduits](https://hackage.haskell.org/package/conduit) if you are using those, but you are not required to use conduits. The conduit library is a dependency though.
If you are translating, printf gives you the whole string, and you can handle languages with cases more easily.
Ok my bad ;)
Hmm, I'm surprised this even works. Wouldn't be surprised if it didn't always pick the shadowing instances.
Could you explain more about this? Your claim is rather vague.
Right. One of the template languages in the [shakespeare](https://hackage.haskell.org) library is a plain text template like this one. Quasi-quoters are provided for both strict and lazy text. However, embedded Haskell expression are only supported in "hamlet" templates for HTML. For plain text templates, it's just variable interpolation. And the syntax is slightly different. The shakespeare library has been around since August 2011, and it's at version 2.0.4.1. It's widely used in the Yesod ecosystem, though it is completely independent of Yesod and can be used in any application.
Here is a simpler variation of the "parametrize" solution, if you only need to vary a small number of fields - make the actual type a parameter, instead of going via a functor: data ProtoFoo verifiedType = Foo { fieldA :: Int , fieldB :: Text , verified :: verifiedType } type Foo = ProtoFoo Bool type FrontendFoo = ProtoFoo () 
This sounds like it should be considered a GHC bug
With printf, the entire atom of translation is together, with some placeholders. You know the whole sentence to be translated, and if there's some grammar that requires information from the end for the beginning, you have both. (unless the end is a placeholder, but then broken up strings doesn't save you) I know I saw a page with some examples of it helping in Russian, I think on the C++ FQA or cat-v, but I can't find it right now. Certainly better i18n tools could get around this, but then you have to parse your whole project and you've basically reimplemented printf in a really messy way.
Hmm... But with `-XIncoherentInstances` you're begging for it.
Ah. I had thought the `IncoherentInstances` was only necessary to allow you to pick the shadowing instance over the visible global `Int` instance. In fact, the extension is necessary to give `representational` roles to type classes at all. type role NUM representational error: • Roles other than ‘nominal’ for class parameters can lead to incoherence. Use IncoherentInstances to allow this; bad role found • while checking a role annotation for ‘NUM’ 
Well, I would be very happy to go all in with anonymous records, BUT: * Currently the compiler can optimize real haskell records better, especially with static knowledge things can be inlined (important if you use records like interfaces with functions etc.) * Compile time suffers a lot due to all the type level magic. I still have not had the time to track this down, but I hope to find and fix this soon (if possible...).
Note that it seems reasonable to wrap that function in a newtype to improve type inference and error messages.
This is probably old news but, both `MonadDb` and `MonadLog` sound like a job for [`RIO`](https://www.fpcomplete.com/blog/2017/07/the-rio-monad). instance HasLogger env =&gt; MonadLog (RIO env) where ... instance HasDbPool env =&gt; MonadDb (RIO env) where ... In some sense, "how to make low-boilerplate `env`" is a bit like "how to make low-boilerplate `Monad`" (extensible records &amp; extensible effects). There is intersting dualism: `Inject` is a prism into a sum of effects, where RIO uses lens to get needed part of a product environment to handle the effect. I argue that `RIO` approach is good enough and simpler than `Free`-based. Also I'm quite sure that about every library defining mtl-like class has instances for monad transformers in `transformers`. In other words `log` and `queryDatabase` will work in `ExceptT e m`. --- For work I'm writing boring programs: there all my business "effects" can be modelled with `RIO` (or `Haxl`, `Haxl` is "free" but not inspectable). I **don't** need inspectability when I `log` or communicate with database. I don't have n^2 problem. For each business effect I write instances for `transformers`, `RIO` and `Haxl`. That's linear in amount of backend effects. Note: these effects commute (in a sense that `LogT (bBT m a) ~ DbT (LogT m a)`). "Interesting" stuff happens, when this is not true. But again I write boring programs. --- The only thing where I can imagine one would need `Free` is a need to write a function - takes any effectful computation which uses effect `E` - returnss an effectful computation with `E` already handled. {-# LANGUAGE FlexibleContexts, ScopedTypeVariables, RankNTypes, ConstraintKinds, TypeApplications #-} import Data.Proxy import Control.Monad.Except import Control.Monad.Reader performExceptT :: (Monad n, c n, c (ExceptT e n)) =&gt; Proxy c -- ^ rest of the effects -&gt; (forall m. (MonadError e m, c m) =&gt; m a) -- ^ 'MonadError' + 'c' computation -&gt; n (Either e a) -- ^ only 'c' computation performExceptT _ action = runExceptT action -- We start with computation declaring use of all effects logic :: (MonadError String m, MonadReader Double m) =&gt; m Int logic = asks truncate -- we can handle MonadError noErrorLogic :: MonadReader Double m =&gt; m Int noErrorLogic = either (const 0) id &lt;$&gt; performExceptT (Proxy @(MonadReader Double)) logic -- | and finally reader. -- -- &gt;&gt;&gt; value -- 3 value :: Int value = noErrorLogic 4.14 I think that with `Free` you'll get nicer (and Haskell98!) type: performError :: Free (Error e :+: f) a -&gt; Free f (Either e a) but I didn't ever needed that kind of functionality. Quoting a meme: If I handle effects (usually down to `IO`), I do them all at once. --- Curiosity: I heard PureScript is deprecating its `Eff`. Will it mean that instead of Eff effects a we will see RIO env a -- and a record (row types ftw) to implement env? 
&gt; This is probably old news but, both MonadDb and MonadLog sound like a job for RIO. Possibly, and that's essentially what `simple-effects` is saying - algebraic effects can simply have their interpretation passed around as a parameter and immediately applied. &gt; Also I'm quite sure that about every library defining mtl-like class has instances for monad transformers in transformers. In other words log and queryDatabase will work in ExceptT e m. We only got `ExceptT` recently, and it's hard to imagine there are other commonly used monads, that might not be common enough to get to `transformers`.
I'm not sure what it would mean for Haskell in practice. In math, it would mean any function that uses a monad could be rewritten to use the free monad (plus some other maps). You can take a look at how the [freer](https://hackage.haskell.org/package/freer-0.2.4.1/docs/Control-Monad-Freer.html) monad works, since I haven't quite grasped it yet.
I'm interested to know about where the industry is going and getting one anecdote about what the candidate supply for the rare Haskell coding job helps. I'm in no position to consider hires. But I would love it if someone hired me to code Haskell. Also, I was an applicant for your position.
I don't think you need to build a whole project for each student. How about having every student submit `Homework.hs`, then for every `$student`, ghc $student/Homework.hs MyTestScript.hs 
Indeed, especially if if the missing field has the same type as other fields, then you know which one you are using.
You might be able , instead of passing verified to the User constructor to actually inject the verified value into Json and then use the generic fromJson instance.
At my University, we're using https://github.com/KITPraktomatTeam/Praktomat. I've only used it as a student, so I'm not sure how easy/hard it is to set up and configure, but the README mentions Haskell support (I've used it for Java). Plus if your students already have a Shibboleth account, they can use it with Praktomat.
Thanks for the info. Do you know where xmlhtml (that another user mentioned) fits in compared to the ones you talked about?
I think this was the answer I was looking for. Although it would be nice if I could do it without shell script and ghc.
Or even just type FatStack m = (MonadError ErrorType m, MonadIO m, MonadState StateType m) =&gt; m () foo :: FatStack m foo = ...
Ah, I think I see what you mean: you can get a Monad `Free F` for every Functor `F`, and you can write interpreters of type `Free F a -&gt; M a` for many Monads `M`s; but if you already have a Monad `M`, you can't necessarily find an `F` and an interpreter of type `Free F a -&gt; M a` for your particular `M`. Hmm, unless... would choosing `F = M` work?
But then it's not polymorphic in the return type.
What's the benefit of doing that at the type level instead of the value level?
I really dig this series of posts. I hope there will eventually be a post on the `vector` library, which supports about the same technique with freezing/unfreezing as `Array` does, but also a lot of other performance goodies. I really think `vector` is perhaps the best performance-oriented Haskell library, in that it provides predictable high performance (although not as much as Repa and Accelerate can do), but is still very simple to use.
Testing will only tell you if the code is (probably) correct; it won't tell you if it's any good or not.
[HaXml](https://hackage.haskell.org/package/HaXml) is another. For simple usages where you don't really care about performance you can use [xml](https://hackage.haskell.org/package/xml).
Love this post newtype Cx e a = Cx { unCx :: a -&gt; Maybe e } is an example of [`Op`](https://hackage.haskell.org/package/contravariant-1.4/docs/Data-Functor-Contravariant.html#t:Op) (flipped function arrow) newtype Cx e a = Cx { unCx :: Op (Maybe e) a } so we can derive `Contravariant`. Now contrast `Divisible (Op m)` with the `Cx e` instance instance Monoid m =&gt; Divisible (Op m) where conquer :: Op m a conquer = Op (const mempty) divide :: (a -&gt; (b, c)) -&gt; (Op m b -&gt; Op m c -&gt; Op m a) divide split (Op left) (Op right) = Op $ \(split -&gt; (b, c)) -&gt; left b `mappend` right c instance Divisible (Cx e) where conquer :: Cx e a conquer = Cx (const Nothing) divide :: (a -&gt; (b, c)) -&gt; (Cx e b -&gt; Cx e c -&gt; Cx e a) divide split (Cx left) (Cx right) = Cx $ \(split -&gt; (b, c)) -&gt; case (left b, right c) of (Just x, _ ) -&gt; Just x (_, Just x ) -&gt; Just x (Nothing, Nothing) -&gt; Nothing there is a strong similarity.. we just need to pick the right monoid which returns the first `Just` it encounters — aka [`First`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Monoid.html#t:First) or [`Alt Maybe`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Monoid.html#v:Alt) instance Monoid (First a) where mempty :: First a mempty = First Nothing mappend :: First a -&gt; First a -&gt; First a First Nothing `mappend` r = r l `mappend` _ = l `Decidable` of `Op m` also looks the same as definition (apart from `lose _ = conquer`) instance Monoid m =&gt; Decidable (Op m) where lose :: (a -&gt; Void) -&gt; Op m a lose f = Op (absurd . f) choose :: (a -&gt; Either b c) -&gt; (Op m b -&gt; Op m c -&gt; Op m a) choose split (Op left) (Op right) = Op (either left right . split) If you are OK with that definition of `lose` those classes can be derived automatically by picking the right "adapters" `Op` and `First` (let's not export `Cx__________________`) newtype Cx e a = Cx__________________ (Op (First e) a) deriving ( Semigroup, Monoid , Contravariant, Divisible, Decidable ) pattern Cx :: (a -&gt; Maybe e) -&gt; Cx e a pattern Cx { unCx } &lt;- (coerce -&gt; unCx) where Cx = coerce
Well, you could put a newtype and its related instances in a separate module and import that as needed.
`FatStack m` seems polymorphic to me...
`m` is the Monad, which I'm not even sure that works without a `Monad m` constraint. I can't check right now. I'm mobile. The return type is `()`, which means `foo` can't return anything.
Yes this came as a surprise to me too, I have been thinking if this lets us play any interesting tricks
 {-# Language Future #-} newtype Cx e a = Cx { unCx :: a -&gt; Maybe e } deriving (Semigroup, Monoid, Contravariant, Divisible, Decidable) via Op (First e) a deriving (Num, Floating, Fractional) via Op (WrappedApplicative First e) a
Similar to the xml library. It is designed for parsing web pages, or fragments of them. It parses both HTML 5 and XML into the same AST. As an XML parser, it explicitly does not fully parse the XML, as stated in the docs. Beyond the parser, there is no support for any of the myriads of other kinds of machinery in the ecosystem of generic XML.
Wow well done, looks really interesting. Had a quick run through. This is kind of a stab in the dark, but I see that in your render loop https://github.com/dbousamra/hnes/blob/ba5978f8484616606ed8b481e94b2888b85abe15/src/Emulator/PPU.hs#L33 you use (!!) twice. (_ !! n) takes O(n) time! Consider using unboxed vectors instead of lists as you do in other places: (!) takes O(1) time. Probably starting with your palette :: [(Word8, Word8, Word8)] would be easiest. Let me know if that helps at all.
In general, I've seen that just (Haskell) consultancies consider hiring for Haskell. Normal software companies (SaaS and other) usually go for boring, mainstream languages that even a 2-year old can learn in a day, because they think (falsely, in my opinion, unless you want to be Google-sized) that it gives for best growth opportunity. (One has to fight for non-mainstream languages in these companies, and still may not succeed. It sure has not been easy for me either.) Reality seems to be that you can easily find 200 JS applicants (per day even) and, if you happen to get lucky, there's one or two good programmers you can hire in there. You can not find 200 Haskell applicants even in a months time, but those that apply, you almost could consider hiring all of them immediately. I'm also hoping that this application (that I had to fight for a bit to get out) serves double purpose as an example that this technological direction would be beneficial to pursue on a higher level as well. There are so many talented programmers that like Haskell, and talent is what all software companies fight for. As a programmer myself, management has so far been the hardest to convince of this. (edit: typo)
`Rand` is not an instance of `MonadIO`. `RandT g m` is an instance, but only when `m` is.
For the same reason you'd do anything at the type level; to evaluate it at compile time and not run time. As I say, &gt; In practice, I agree and I would use something like formatting but you wouldn't use this for the same sort of problems you'd solve with value level string interpolation.
oh, i could totally do something with this
Thanks to /u/mrkkrp for writing ["Free monad considered harmful"](https://markkarpov.com/post/free-monad-considered-harmful.html) and forcing me to write this post sooner than I had planned.
Yep, I came across Curry-Howard a few times while browsing Wikipedia but never understood the point until years later when I started learning Coq.
`Free` really is the correct choice, though; it's just that we don't have the proper infrastructure to support it. You need some way to force reduction immediately at the bind site in order to avoid typeclass dictionaries accumulating combinatorially in your AST, but there's no current way to do this. Supercompilation is the right idea, but the way it would be implemented in Haskell likely wouldn't be right: you need to reduce the free AST the moment you build it; if you let it all pile up and try to compile everything all at once later (the way compilation normally works), you're just going to get explosively bad behavior at compile time instead of runtime, which is really no better. What you need is the instant you've built your free shape, immediately reduce it, then continue writing (or parsing, as it may be) your program. This should allow you to have your cake and eat it too. Unfortunately, this world is an alternate universe which we have no access to. But my point is, `Free` is the correct shape, and if anything, its problems are more a reflection of the weakness of our infrastructure than an issue intrinsic to the `Free` monad itself.
This is a lovely description of the problem :)
From what I've read they use the same principle as I do.
I always come back to this comment on free vs mtl https://www.reddit.com/r/haskell/comments/3yksmn/a_modern_architecture_for_fp/cyh9489/ 
Well, combine with some white-box stuff and you'll get a good guess whether it is or not.
Why is this being down-voted?
I think everyone is conscious of it. I don't want to speak for everyone else, but from my point of view, it's the whole goal of the type system: you can look at a type and instantly see what it allows, and you know that everything else is forbidden. I don't want things which break that property.
&gt; Free allowed us to quickly iterate on an implementation without thinking about transformer stacks, associativity of monads in the stack, how best to decompose our type classes, etc. These are noise when you are prototyping. You likely don’t even understand your semantics at first, you just need to get something built and prove your use case. I don't really see how this is true. Whatever API you want to expose via `Free` can be exposed equally easily via `mtl`-style. One effect functor -&gt; one `mtl` class representing the same operations. Futzing with the transformer stack to implement this class is the same problem as futzing with the interpreter. For instance, the associativity problem is equally present in free: associativity of interpreters. The strength of `free` is when you want to mess with the structure of the computation. If one of your operations only returns `()`, you can always execute the next step in parallel. If it returns `Bool`, you can attempt something like branch prediction and call the continuation with a guess (or both) before the actual effect has returned, as long as you can rewind it. You can even just skip steps entirely. Of course, this same stuff is technically feasible with `mtl`, since free monads can always be translated to regular monads; but it is way harder. The strength of `mtl` is performance, and the non-algebraic effects like `ContT`, `MonadFix`, and `Writer.listen`. The former two simply can't be done in traditional `Free` setups. Non-algebraic effects like `listen` or `catch` can be done with `Free`, but only by using an interpreter inline, which stomps on the goal of being able to abstract interpreters to the top level. And FWIW: You probably could have lowered the cost of `Free` significantly by using `Codensity`, a church encoded free, or Reflection Without Remorse (aka Freer). The article doesn't say which variant was used, but I would wager it was none of these. You probably could have gotten away with `Free` using one of these.
Many effects can be trivially implemented with default implementations. {-# LANGUAGE DefaultSignatures #-} class Monad m =&gt; MonadState s m | m -&gt; s where state :: (s -&gt; (a, s)) -&gt; m a default state :: (m ~ t n, MonadState s n, MonadTrans t) =&gt; (s -&gt; (a, s)) -&gt; m a state = lift . state This lets you write really simple instances instance MonadState s m =&gt; MonadState s (MyT m) -- No instance body required.
That first point is likely subjective. I find prototyping with `Free` to be easier, others may not. Different strokes, different folks. Yes, I wanted to explore using `Freer` et. all, but I also wanted to strike while the iron was hot. Maybe I'll follow up with another post on that implementation and how it fares. In the end we would haved moved to `mtl` anyway, because we didn't end up needing to inspect the `AST`, so any `Free` representation would have been superfluous.
That really would make everything easier.
Still, if it fails, good reason to dock marks. Most of my uni courses had programs that were marked by a 50/50 split of tests and by hand. A high degree of compliance with a spec can be checked easily by tests but can take a lot of work by hand.
Yep, that's a no brainer. I'll fix and post results soon
We're currently teaching a course on Spark/Scala for data analysis. The way I set up automatic testing of student solutions was to set up offlineimap to an email account and made it run a Perl script after each syncronisation. The script checks for unread emails in a certain folder, extracts attachments and runs all scripts in a specified directory on these attachments. If everything goes fine the email is marked as read and a response email containing the contents of stdout from these scripts is put in the email account's drafts directory for inspection before sending back to the student. On top of this basic framework, for Scala/Spark specifically, I had a script start a docker container with the necessary software, isolated from the network and with only read-only access to any exercise datasets. A set of preprocessing scripts and tests to run are picked based on the attachment filename and executed in the container. In the end the student receives a list of messages, one for each test, stating that it passed or giving a hint what they need to fix. The only way the instructor is (supposed!) to have to interact with the system is when inspecting the draft email response. Since the docker container is thrown away after each run you don't have to worry about the testing itself being particularly robust, and since you don't manually have to run things manually the fact that is can be a bit slow doesn't matter too much. 
Played a bit with ways to extract pure code until these exercises are up. I settled on a `collect input -&gt; process input+state -&gt; patch state -&gt; display dynamics` chain after some trial and error. Not entirely sure where I stand on the code volume vs clarity thing yet. On the plus side, the business logic ended up entirely pure: process :: Money -&gt; Action -&gt; StockPatch process m Added = EAdd process m Refunded = ERefund m process m (Purchased s) | sStock s == 0 = EErr ProductOutOfStock | pCost (sProduct s) &gt; m = EErr NotEnoughMoney | otherwise = EPurchase (sProduct s) patchMoney :: StockPatch -&gt; Money -&gt; Maybe Money patchMoney (EPurchase s) m = Just (m - pCost s) patchMoney (ERefund _) _ = Just 0 patchMoney EAdd m = Just (m+1) patchMoney _ m = Nothing patchStock :: StockPatch -&gt; Stock -&gt; Maybe Stock patchStock (EPurchase s) v | pName s == pName (sProduct v) = Just v { sStock = sStock v - 1 } patchStock _ _ = Nothing patchStruct :: (Traversable t, Reflex x, MonadHold x m, MonadFix m) =&gt; t v -&gt; Event x p -&gt; (p -&gt; v -&gt; Maybe v) -&gt; m (t (Dynamic x v)) patchStruct t e f = traverse step t where step z = foldDynMaybe f z e 
(Assuming you read the other responses already) There are three different ways to write value recursive monadic computations. `mfix` which is literally monadic `fix`, `mdo` which is syntax sugar on top of that and `rec` which is a scoped `mdo`. That is, rec is kind of like writing foo = do doSomeStuff (var1, var2, var3) &lt;- mdo doSomeRecursiveStuff return (var1, var2, var3) doSomeMoreStuff But variables in the rec block automatically stay in scope. [Here](https://wiki.haskell.org/MonadFix) are a couple examples. I often prefer the `rec` variant because it forces you to think about the scope and order of recursion which makes it easier to spot problems. For instance: f = mdo ... mapM renderEntry dList ... dList &lt;- holdDyn startingList events hangs forever. The fault lies with mapM, it applies some function to each element and collects the resulting effects so we require all entries before we can continue with the monadic chain. But we can't compute dList until we are done with the mapM call so we essentially deadlock. This can be really hard to spot if the two calls are far apart so it can be super helpful to minimize the recursive scope with rec.
I mean that may be true. But it's pretty hard for a student to some how pass all the tests without having at least very close to correct code, and if their code is correct then of course the tests will pass. So for grading purposes you get very very few false positives and zero false negatives, seems pretty good to me.
I honestly had to do a double take on the title of this post. Never in a million years did I expect a Haskeller to be into streetwear! Juging by your last statement on the blog post though I guess I was right. :)
It's always pointless to ask because the people who downvoted are no longer here to answer. Also it's unwritten reddiquette to never talk about upvotes or downvotes.
That looks like a pretty clean way to do it. I wonder if there's a better way to do `Refinable`. I sort of made it up on the spot, but the other possibility that could avoid a new typeclass was to add a new left-absorbing element `e` to `Cx` and set `lose _ = e`. 
That's very interesting way of doing it :D
It's just so that we can focus on grading code quality and not wondering if there is something wrong in our setup or if solution is indeed incomplete.
I'm actually somewhat curious as to why we even have `Symbol` instead of just using `DataKinds` + `String` / `[Char]`.
There's actually a way to use `findD` on any `Foldable` container: -- This Monoid is cheating because it doesn't reassociate. data Cheating a = Zero | One a | Two (Cheating a) (Cheating a) instance Monoid (Cheating a) where mempty = Zero mappend = Two csplit :: Cheating a -&gt; Either () (Either a (Cheating a, Cheating a)) csplit Zero = Left () csplit (One a) = Right (Left a) csplit (Two x y) = Right (Right (x, y)) findD :: (Decidable f, Foldable t) =&gt; f a -&gt; f (t a) findD f = contramap (foldMap One) fd where fd = choose csplit conquer (chosen f $ divided fd fd) I probably got this from Edward Kmett at some point.
Because the tagging for DataKinds (using, for instance, `'Nothing`) conflicts with `'X'` for characters.
The canonical example of such a class is `Coercible` itself. The major usecase that I have is when I need a class like class Vacuous a instance Vacuous a when talking about category theory in Haskell. Such a class can be made `phantom`.
You can move it around by using the `constraints` package to help shape which instance is selected.
What is wrong with having `Proxy @''a'` be the syntax for type level characters? I don't think that will conflict at all. 
This one introduces some higher-order FRP, including Workflows which are really neat - although there is some even neater stuff just around the corner :) The DOM exercises just went up along with this post. The future exercises might be a bit more free-form / less-granular, and I'll probably delay them until I catch up with the progress of my draft posts. It's also a long weekend here, for the Queen's birthday. I never know what to get her, and I don't think express post would even get it to her on time, but that's my problem. For the people following along, it means that the next post is likely to come out on Tuesday rather than on Monday. Have good weekends!
Is it something different from the derive via newtype thing?
r/reflexfrp --------------------------------------------- ^(For mobile users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/6xrtxg/ignore_list/)
It's not that `stack` is missing anything. Rather, whatever you're doing (I don't think it is `stack build`, but the way) expects you to have installed `intero` first. Thusly: stack install intero Now, what *were* you doing, exactly? When you use `intero` as an editor plugin you never have to install it manually (IIRC)... 
I'm not totally sure what you mean by a rule for `forall`, but you definitely can't distribute it (if I'm understanding you correctly). `forall a. a^(a*a)` is different than something like `(forall a. a)^(forall a. a*a)`. That second one, if it were allowed, would be the same as `(forall a. a)^(forall b. b*b)`. `forall a. ...` works like a restricted type-level lambda. It takes in a type and substitutes it appropriately, essentially like reduction of a lambda application (this application generally happens implicitly though).
This is fantastic! Combine this with some simple ANSI escape sequences to clear the screen, change cursor position, and maybe set colours, and you've got yourself a pretty functional, non-interactive, dashboard-like interface, without wasting the full interactive power of `brick`! How *do you manage* to get *so much* done, Chris!? 
I’ve heard of [boxes](https://hackage.haskell.org/package/boxes) which seems similar. I’ll actually need something like this soon, because I want to [render tables in a terminal](https://stackoverflow.com/questions/41096830/algorithm-for-rendering-dag-as-table), but I was thinking of doing it with [brick](https://hackage.haskell.org/package/brick) since I’ll be using that anyway.
Lucky queen. Already had [2](https://publicholidays.com.au/queens-birthday/) birthdays this year, to my knowledge. She's almost 300 by now.
It's a tough gig though. In normal human years she's only 20. Stressful positions can really accelerate the aging process. Just look at the befores and afters for GW Bush, or Obama, or [Yoda](https://imgur.com/yYo3o6G.jpg).
In first-order logic, `∀x (Px)` is equivalent to `~∃x (~ Px)`. Similarly, `∃x (Px)` is equivalent to `~∀x (~ Px)`. (Although since Haskell's type system corresponds to some fragment of intuitionistic logic, I guess only the second applies but not the first? Someone else should comment on this) I believe that answers 1 and 2 (note that negation appears twice). Maybe it helps intuition by reading in English: "'There is an x that is P' is the same as 'It is not the case that all x are not P'". Also, `~A` is equivalent to `A -&gt; ⊥`. You can see this from the truth-table: for negation it's simple, `A` is true when `~A` is false and `A` is false when `~A` is true. For implication: the only time `A -&gt; B` is false is when `A` is true and `B` is false. Hence, since `⊥` is always false, `A -&gt; ⊥` is true just in case `A` is false, and vice versa. This should also explain 3 and 4; in particular for 4 the negations are there, but they've been turned into implications.
To your third question: Saying ~P is the same as saying that from P you can deduce falsehood, or P -&gt; ⊥. This is exactly how ~ is defined in a language like Coq.
Newbie here, I want to build a web app by composing a small library instead of using a battery-included one. Is there any pointer on how it's done? Is it recommended?
&gt; is ~∃ &lt;=&gt; ∀? ... is "there doesn't exist a flying pig" equivalent to "all pigs fly"? :-P
Awesome. Something like this would be really useful with a quick &amp; dirty dsl parsing command line tool.
1. Not exactly. It's (B) plus the "well-known" identity `~E x. P x` &lt;-&gt; `A x. ~P x`. 1. No, see previous for the identity that's applied. You can pull a negation inside a universal / existential quantifier by changing into the other quantitifer. 1. There are no constructors for ⊥, so the only way you can make a function `a -&gt; ⊥`, is if a also has no constructors / is unhanbited / is a false proposition. It's a pretty common definition of negation in intuitionistic type theory, and it's true anytime `-&gt;` corresponds to implication / entailment. 1. C -&gt; D and D -&gt; E are the same transformation. Just rewriting `~x` -&gt; `x -&gt; ⊥`. 1. B, C, D, and E are all isomorphic / equivalent in intuitionistic logics (/ type systems). But, I *think* A is a bit stronger. `~~~x` &lt;-&gt; `~x` and `x -&gt; ~~x`, but you can't go from `~~x` to `x` in general in an intiutionistic context (you can in a classical one). Maybe you can go from the double negation of an existential to the existential... but I don't *think* so. --- data Void Not : Type -&gt; Type Not x = x -&gt; Void -- dng : {a : Type} -&gt; a -&gt; Not (Not a) dng : {a : Type} -&gt; a -&gt; Not a -&gt; Void dng x contra = contra x neg_2_n3g : {a : Type} -&gt; Not a -&gt; Not (Not (Not a)) -- neg_2_n3g : {a : Type} -&gt; Not a -&gt; Not (Not a) -&gt; Void neg_2_n3g contra = dng contra -- n3g_2_neg : {a : Type} -&gt; Not (Not (Not a)) -&gt; Not a n3g_2_neg : {a : Type} -&gt; Not (Not (Not a)) -&gt; a -&gt; Void n3g_2_neg contra x = contra (dng x) --- Proofs of the quantifier inversion through negation are around as well, with a similar definition of `Void` and `Not`.
Ooh I really like this. Do you have anything written up on what terrible travesties you're planning on doing to deriving to get it to do this? (I recall you mentioning tweaking deriving for classes or something in another thread and was wondering if this was related)
Ok, that makes sense. Thanks for the explanation! By the way, I don't know if you're already planning this, but I'd love to see a post about reflex interacting with a backend REST api. 
I think it's more of how much of a nightmare that sort of thing is to bolt onto the parser than anything. Also, a lot of this sort of thing becomes unnecessary with dependent types from what I've read
That won't be in _this_ reflex series, but I'm planning to cover using `reflex` for XHR and websockets in the future.
I'm still not sold on dependent types, unified type and term level syntax I would love. So that: foo :: Type -&gt; Type foo Int = Bool foo Bool = Int Was a valid type family. And: (&amp;&amp;) :: Bool -&gt; Bool True &amp;&amp; True = True _ &amp;&amp; _ = False Worked both on regular `Bool`s and also on type level `Bool`s of kind `Bool` (the `DataKinds` lifted version of `Bool`). But I don't feel like dealing with a theorem prover for day to day programming. I like Haskell's nice balance between safety and programmer productivity.
Hi BoteboTsebo, Thank you for the response. I was trying to build my project using IntelliJ. I just fixed the errors by installing intero. Now I get this error: Error:Internal error: (java.lang.ClassNotFoundException) scala.Function1 java.lang.ClassNotFoundException: scala.Function1 at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at com.haskforce.jps.model.JpsHaskellModelSerializerExtension.getProjectExtensionSerializers(JpsHaskellModelSerializerExtension.java:83) at org.jetbrains.jps.model.serialization.JpsProjectLoader.loadFromDirectory(JpsProjectLoader.java:123) at org.jetbrains.jps.model.serialization.JpsProjectLoader.loadProject(JpsProjectLoader.java:101) at org.jetbrains.jps.model.serialization.impl.JpsSerializationManagerImpl.loadModel(JpsSerializationManagerImpl.java:41) at org.jetbrains.jps.cmdline.JpsModelLoaderImpl.loadModel(JpsModelLoaderImpl.java:45) at org.jetbrains.jps.cmdline.BuildRunner.load(BuildRunner.java:79) at org.jetbrains.jps.cmdline.BuildSession.runBuild(BuildSession.java:267) at org.jetbrains.jps.cmdline.BuildSession.run(BuildSession.java:125) at org.jetbrains.jps.cmdline.BuildMain$MyMessageHandler.lambda$channelRead0$0(BuildMain.java:236) at org.jetbrains.jps.service.impl.SharedThreadPoolImpl.lambda$executeOnPooledThread$0(SharedThreadPoolImpl.java:42) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) I set SCALA_HOME=/usr/share/java, but it did not solve the problem. Can you help me out? 
Thank you so much for writing a balanced article that **puts the use-case first** and weighs **wholesome pros and cons** of the solution. One nitpick, did you try inlining the fmap/bind for your Free monad. I might be talking out of my ass here, but I have discovered a [2-10x slowdown](https://github.com/chrisdone/lucid/issues/63) in Lucid for this exact reason, which was fixed pretty quickly. Would love a follow-up post talking about how exactly you're testing your app with the RDBMS. We are in the process of doing the same thing and could use some tips and tricks that have worked well for others will large, complicated webapps. 
Use Docker containers. Prepare an image with the test suite and a script that add the student homework and run the test suite then upload the result to a web/ftp site or a cloud storage or even mount a disk volume in the container.
You can generate arbitrary code and declarations with TH, so it should definitely be feasible. (Unless TH happens to be missing AST definitions for something you'll need -- I think that would be extremely unlikely.)
Lol no. Can't help you there, m8. This is not the right place to ask this question. **But** could you at least tell us what IntelliJ plugin you were using (to enable Haskell support), and more details as to what you're doing and what your setup is? Maybe someone who knows Scala and IntelliJ better might know what's up, but right now this is certainly not enough information to glean any kind of insight. EDIT: Although, I can hazard a guess and say that `/usr/share/java` is **not** where you have installed your Scala distribution (jar files)... 
I was using the plug-in HaskForce to create a project with almost no line of code and build it to see if it worked. I run Debian 9.1, stack 1.5.1, IntelliJ 2017.2.5, HaskForce 0.3.39. By the way, the folder /usr/share/java contains a lot of jar files including scala*.jar.
That won't be possible without changing the type of cubicS if I understand you correctly. 
&gt; Although functional programming discourages mutable states, sometimes mutable variables are needed. Um. Nope. You don't need no stinking mutes round these here parts! At least, not in this code. In fact, as a beginner Haskeller you very quickly realise that you almost never ever need mutable variables (or even `StateT`, for that matter). In particular, `main` only has two functions -- one creates the state, and the second one mutates it in a (forked) loop. No other part seems to access this state (as far as I can see, you're only spawning a single thread). **Your `loop` does not need to use `IORefs` to change the state.** Since the function is recursive, just make it take the state as a parameter. Something like this (I haven't tested it, by the way): data GlobalState = GlobalState { tasks :: TaskMap } initialize :: IO GlobalState initialize = do titles &lt;- getPages testURLs return $ GlobalState titles startTimer :: GlobalState -&gt; IO () startTimer (GlobalState tasks) = do threadId &lt;- forkIO (loop tasks) return () where loop tasks = do threadDelay $ seconds 1 newTasks &lt;- updatePages tasks print $ getDiffs tasks newTasks loop newTasks `loop` is tail recursive, so there is no danger of stack overflows from abandoned local variables (`tasks` and `newTasks`) as GHC will convert it into a simple, well, (iterative) loop, and will perform garbage collection when things get out of hand. I don't promise that the above code will compile, or that it won't launch any missiles (Trump will probably do that for us, gladly)...
I've started working on something similar, it's in a very rudimentary state, but you can check it here: http://haskelltools.com The code is available here: https://github.com/diogob/haskell-tools. The api is also open: http://api.haskelltools.com
The performance issue with `fmap`/`&gt;&gt;=` in free is a well known asymptotic problem. You may be able to stem the bleeding with inlining, but unless you can convince GHC to reduce the entire structure and eliminate `Free` entirely, you're still doomed. The problem is that left associated usages of bind have `n^2` complexity. So `do { a &lt;- bigFunction; ... }` has to pay in at least the size of `bigFunction` just for the bind. There are well known fixes though. `Codensity` magically reassociates the binds to remove the asymptotic problem, and there are plenty of other tricks you can do.
That's the one: [original blog post](https://gist.github.com/Icelandjack/d258b88a0e0b3be2c0b3711fdd833045) + [reddit thread](https://www.reddit.com/r/haskell/comments/6ksr76/rfc_part_1_deriving_instances_of/)
good job EDSL
Using an `IORef` instead of proper recursion to carry a state is a really common error from people trying to rewrite an imperative style algorithm to a functional one. There is two problems here: - The first that you mentioned, that is not needed and leads to complex code with all the issues with side effect, readability, quality, ... bla bla, we all know - It well be inefficient. If the state is something small (such as an `Int`), ghc can usually unpack it, store it in a register and rewrite the terminal recursion as jump. This is fast. On the other hand, theses optimization are not available (well, less) for `IORef`, resulting in above average performances.
[original blog post](https://gist.github.com/Icelandjack/d258b88a0e0b3be2c0b3711fdd833045) + [reddit thread](https://www.reddit.com/r/haskell/comments/6ksr76/rfc_part_1_deriving_instances_of/) The core idea is as simple as can be, each method is a `coerce`-ion of the same method (at a type that has the same representation) instance Contravariant (Cx e) where contramap :: forall a' a. (a' -&gt; a) -&gt; (Cx e a -&gt; Cx e a') contramap = coerce (contramap @(Op (First e))
I don't understand how you'd find it easier or harder. Is literally the same thing with slightly different syntax.
As context, for beginners and the confused, the above proof basically offers some intuition as to why GHC's `ExistentialQuantification` extension uses the `forall` keyword, even though that's associated with _universal_ quantification. That is, why doesn't GHC have an `exists` keyword? An existential "outside" is sort-of-equivalent to a universal "inside". And indeed, when you use `ExistentialQuantification` in practise you do so by putting `forall` on the things which have "private" information (that is "inside"), instead of putting it at the use sites ("outside"). To wave my hands about even more furiously, the intuition I've developed is that `ExistentialQuantification` is used to model OOP-style polymorphism (dynamic dispatch), which in OOP languages works via virtual-method tables and other primitive, run-time magicks. In Java you would have a list of `Vehicle`s (an interface), where each element may be a different concrete class/implementation, then iterate over the list, calling the `start` method of each object, and have the behaviours vary according to what the concrete class of each object was. You can't do such a thing directly in Haskell, but you can simulate it using `ExistentialQuantification`. 
Did some quick experimentation. I'll be following up on this. https://twitter.com/evanborden/status/913557193772994561
There might be a psychological element here. The influence of "lawful abstraction" is strong when we talk about type classes. So when designing a type class that baggage gets dragged around. There isn't as much of this talk around `Free`. I'm not saying this is a reasonable or correct reason, but it definitely influences one's state of mind.
I plan to write a proper introduction and experience report for `graphula` soon. We've been through a few versions of this idea and we seem to have settled on something with a nice balance.
&gt; Although since Haskell's type system corresponds to some fragment of intuitionistic logic Do you have any sources to read up about that? I would have just assumed that type systems corresponded to classical first-order logic. I'm not well versed in type systems though. 
This is, in part, what got me started on this trail of studies!
Could you expand more, please, how this can be useful not only for displaying but with parsing?
(Edit: oops, I now see that a lot of this is covered by bss03's answer. But maybe hearing the same thing in a couple slightly different ways is still useful, so I'll leave this post here) There is an article on the wiki that may be of interest: https://en.wikibooks.org/wiki/Haskell/The_Curry–Howard_isomorphism The main theorem that holds in classical logic but not in Haskell's type system is double-negation elimination. That is, we can prove that `P =&gt; ~~P` in Haskell as follows (I'm encoding ~P as P -&gt; Void, where Void is defined as a type with no constructors): -- If I can write down a value with this type, I have proved P =&gt; ~~P type DoubleNegateIntro p = p -&gt; ((p -&gt; Void) -&gt; Void) proof1 :: DoubleNegateIntro proof1 p f = f p On the other hand, we *can't* eliminate double negations; unlike classical logic, we can't prove `~~P =&gt; P`: -- If I can write down a value with this type, I have proved ~~P =&gt; P type DoubleNegateElim p = ((p -&gt; Void) -&gt; Void) -&gt; p proof2 :: DoubleNegateElim proof2 f = -- hmm, what to put here? Since I can't make a `p` from a `(p -&gt; Void) -&gt; Void`, I'm stuck. One funny thing is that we still have *triple*-negation elimination! -- Proving ~~~P =&gt; ~P type TripleNegateElim p = (((p -&gt; Void) -&gt; Void) -&gt; Void) -&gt; (p -&gt; Void) proof3 :: TripleNegateElim proof3 f p = f (proof1 p) Anyway, the fact that double-negation elimination doesn't work means we can't be using classical logic. Instead, we end up in a constructive logic! https://en.wikipedia.org/wiki/Intuitionistic_logic
**Intuitionistic logic** Intuitionistic logic, sometimes more generally called constructive logic, refers to systems of symbolic logic that differ from the systems used for classical logic by more closely mirroring the notion of constructive proof. In particular, systems of intuitionistic logic do not include the law of the excluded middle and double negation elimination, which are fundamental inference rules in classical logic. Formalized intuitionistic logic was originally developed by Arend Heyting to provide a formal basis for Brouwer's programme of intuitionism. From a proof-theoretic perspective, Heyting’s calculus is a restriction of classical logic in which the law of excluded middle and double negation elimination have been removed. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Oh nice! That looks very similar actually. 
Interesting! Grammata especially looks very similar to what I was imagining – I'll need to dig into the code to figure out the differences. Why are they semi-abandoned? (Dead end, or lack of time?)
Oh man, this is awesome. It's so simple. Im working on something similar and Ive spent quite amount of time just thinking how should I handle my app's state. And I know im making my code more complicated than it needs to be as Im writing it, but didn't think of updating state that way. Its so simple. Thanks for this. Time to drop those unnecessary TVars and MVars.
&gt; How often is the markup extended? All the time – while you're writing, you'd think up a tag and include it in your document, then go and implement it afterwards. &gt; Are users programmers with access to the source code? Yup. All free and open source. &gt; Can you name other typical markup examples? Well, I'll extend one example: `first-use` might also add the term to the index. `url` might check if the given URL 404's. Allowing this more complex behaviour is why you need more than a simple DSL – though you could combine it with a simple DSL for simple things which only affect the aesthetics. 
I do think Free's data-based approach is conceptually easier to grasp. To drive the point home, let's look at the free Monoid instead of the free Monad, where the contrast is even more striking. Consider the following data-based approach to implementing a sum: myList1 :: [Int] myList1 = [1,2,3] -- | -- &gt;&gt;&gt; mySum1 myList1 -- 6 mySum1 :: [Int] -&gt; Int mySum1 [] = 0 mySum1 (x:xs) = x + mySum1 xs It's quite simple, you can first understand what lists of integers look like, and then come up with a plan to traverse it in a way which produces the result you want. Later on, we may or may not recognize flaws in that initial design, make it use a strict accumulator, replace the recursion with a fold (at which point we can realize that `(+)` is associative so we can pick either `foldl'` or `foldr`), or make it polymorphic over any Num. So, that was the version with the free Monoid. Next, compare the same implementation using type classes: {-# LANGUAGE RankNTypes #-} class Monoid a =&gt; MonoidInt a where monoidInt :: Int -&gt; a newtype MySum2 = MySum2 { getMySum2 :: Int } instance Monoid MySum2 where mempty = MySum2 0 mappend (MySum2 x) (MySum2 y) = MySum2 (x + y) instance MonoidInt MySum2 where monoidInt = MySum2 myList2 :: MonoidInt a =&gt; a myList2 = monoidInt 1 &lt;&gt; monoidInt 2 &lt;&gt; monoidInt 3 -- | -- &gt;&gt;&gt; mySum2 myList2 -- 6 mySum2 :: (forall a. MonoidInt a =&gt; a) -&gt; Int mySum2 list = getMySum2 list This time it's a lot harder to get an intuition for what's going on. Instead of concrete lists, we are constructing reusable computations which are polymorphic over any MonoidInt. Our interpreter is divided among many pieces: a datatype which holds the accumulator, an instance of Monoid explaining what's the default accumulator and how to combine two accumulated values (oh and make sure the default is the identity for that combination function and that this function is associative), an instance of MonoidInt explaining how to accumulate Ints, and a clever rank 2 polymorphism trick justifying that we don't need to worry about `a`s other than Ints wrapped in `monoidInt` because those are the only values which can be constructed if all we have is a MonoidInt instance. We can of course simply write `mySum2 :: MySum2 -&gt; Int`, but the conceptual gap between `MonoidInt a =&gt; a` and the Ints we will sum is still there whether we explain it in the code or not. Notice how we have to face everything at once: dividing the computation into an initial value and an combining function as if for a fold, making sure our combining function is associative, and making our code polymorphic, although interestingly, we're making our inputs polymorphic over structures more complicated than lists, not our computation polymorphic over lists with more general element types than Int. It's also not at all clear how to make sure that this version is using a strict accumulator. I used `newtype MySum2` instead of `data MySum2` is that enough? I don't think so, but I'm not sure. I think it depends on how `myList2` is implemented, not on how my summing machinery is implemented?
Try to implement a functioning `~~A -&gt; A` in Haskell. I'll wait. ;) Every type system I've ever heard of implements some fragment of intuitionistic logic. However, you can recover the power of classical logic by including `call-with-current-continuation` in your language, much as Scheme does. It's not usually done, I think maybe for performance reasons?
1. no 2. no 3. the definition of `~x = x -&gt; ⊥` 4. it isn't distributing, this is using the definition of ~ again. What you're trying to capture with your questions 1 and 2 is actually (in your notation) "~∃ &lt;=&gt; ∀~" does not exist x such that P(x) is equivalent to for all x not P(x). * A -&gt; B relies on `a =&gt; ~~a` * B -&gt; C relies on `~∃ &lt;=&gt; ∀~` (it just applies this law under a ~) * C -&gt; D relies just on the definition of ~ * D -&gt; E relies just on the definition of ~ Note that moving from A -&gt; B is _not_ an isomorphism in an intuitionistic/constructive setting like programming.
 data Void type DoubleNegateElim a = ((a -&gt; Void) -&gt; Void) -&gt; a qed :: DoubleNegateElim a qed = qed ...well, you did say you'll wait :)
Correct me if I'm wrong, but the usage of `haskell-src-meta` is only needed to parse full Haskell expression inside the quasi quotes. But is that really needed? I really like python format string mini language which allows: &gt;&gt;&gt; name = "bob" &gt;&gt;&gt; f"hello {name} a value is {aFloat:.3f}" 'hello bob a value is 12.345' Quickly formatting a string like that, with a bit of support for numerical / padding / somestuff are enough for most of my needs. I don't know any librarie on hackage which provides something simple and convenient. Each time I tried `fmt` or `formatting` I was annoyed by the complexity and the verbosity of "simple tasks".
I'm sorry but It's very hard for me to see how this relates to the Free monad vs the mtl approach.
&gt; `f"hello {name} a value is {aFloat:.3f}"` This looks nice, I might go ahead and implement it in `fmt`. Can you give me a link to Python docs which describe this? &gt; Each time I tried fmt or formatting I was annoyed by the complexity and the verbosity of "simple tasks". If you could give a bunch of sample tasks that you couldn't figure out how to do with `fmt` (or still don't know how to do), I would appreciate that. (Even if the solutions are “obvious” in hindsight, of course)
Circular logic is inconsistent, Haskell allows this because it never pretends to be a valid logic!
We don't have that axiom so we can't implement it... *without* `unsafeCoerce`. data Void doubleNeg :: a -&gt; ((a -&gt; Void) -&gt; Void) doubleNeg = unsafeCoerce doubleNegElim :: ((a -&gt; Void) -&gt; Void) -&gt; a doubleNegElim = unsafeCoerce &gt;&gt;&gt; doubleNegElim (doubleNeg 'a') 'a' -- and for a lark... instance Show Void where show _ = "Void" &gt;&gt;&gt; doubleNeg 'a' undefined Void Yes, this does mean that if we have an `a -&gt; Void` we have a problem in our logic, but: A) Haskell types are not actually logically consistent anyway, there's lots of ways to break it. `a = a :: (a -&gt; b)` for the simplest example. B) We have `Void` anyway via numerous methods. C) In a world where the type logic *was* consistent, we couldn't produce both the `((a -&gt; Void) -&gt; Void)` and an `(a -&gt; Void)` because the former requires an `a` to construct, and if `a` exists then `(a -&gt; Void)` does not.
There are, unfortunately, a lot of really elegant abstractions that seem like they could be made practical by a “sufficiently smart compiler” (fexprs, for example), but that doesn’t mean such a compiler is likely (and it certainly isn’t inevitable). That doesn’t mean it’s impossible, of course, and staged programming might help here. I’m just a little skeptical that this can be done generally enough to be practical anytime soon (but I’d love to be proven wrong!).
My apologies, I attempted to make my comment easier to understand by throwing away the parts of Free and mtl which weren't relevant to my comment, but in the process I made my comment harder to relate to the initial subject. Here is another attempt. I do think Free's data-based approach is conceptually easier to grasp. Consider the following data-based approach to implementing a sum: {-# LANGUAGE DeriveFunctor #-} import Control.Monad.Free data AccumulateF a = AccumulateF Int a deriving Functor accumulate1 :: Int -&gt; Free AccumulateF () accumulate1 x = liftF $ AccumulateF x () myAccumulator1 :: Free AccumulateF () myAccumulator1 = do accumulate1 1 accumulate1 2 accumulate1 3 -- | -- &gt;&gt;&gt; mySum1 myAccumulator1 -- 6 mySum1 :: Free AccumulateF () -&gt; Int mySum1 (Pure ()) = 0 mySum1 (Free (AccumulateF n ns)) = n + mySum1 ns It's quite simple, you can first understand what accumulators look like, namely a strangely-encoded list of integers, and then come up with a plan to traverse it in a way which produces the result you want. Later on, we may or may not recognize flaws in that initial design, make it use a strict accumulator (at which point we can realize that `(+)` is associative so that transformation is valid), replace the recursion with [`Control.Monad.Free.iter`](http://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Free.html#v:iter), or make it polymorphic over any Num. So, that was the version with the free Monad. Next, compare the same implementation using an mtl-style transformer: {-# LANGUAGE RankNTypes #-} import Data.Functor.Identity -- a variant of MonadWriter which works for Int even thought it -- doesn't have a Monoid instance, but only works for Int class Monad m =&gt; MonadAccumulate m where accumulate2 :: Int -&gt; m () -- different implementations can choose different ways to combine -- the Ints, this one sums them (similar to WriterT (Sum Int)) data MySumT m a = MySumT { runMySumT :: m (Int, a) } deriving Functor instance Applicative m =&gt; Applicative (MySumT m) where pure x = MySumT (pure (0, x)) mf &lt;*&gt; mx = MySumT $ (\(m,f) (n,x) -&gt; (m+n, f x)) &lt;$&gt; runMySumT mf &lt;*&gt; runMySumT mx instance Monad m =&gt; Monad (MySumT m) where mx &gt;&gt;= f = MySumT $ do (m, x) &lt;- runMySumT mx (n, y) &lt;- runMySumT (f x) pure (m + n, y) instance Monad m =&gt; MonadAccumulate (MySumT m) where accumulate2 n = MySumT $ pure (n, ()) myAccumulator2 :: MonadAccumulate m =&gt; m () myAccumulator2 = do accumulate2 1 accumulate2 2 accumulate2 3 -- | -- &gt;&gt;&gt; mySum2 myAccumulator2 -- 6 mySum2 :: (forall m. MonadAccumulate m =&gt; m ()) -&gt; Int mySum2 accumulator = let Identity (n, ()) = runMySumT accumulator in n This time it's a lot harder to get an intuition for what's going on. Instead of a concrete data structure, we are constructing reusable computations which are polymorphic over any MonadAccumulate. Our interpreter is divided among many pieces: a datatype which holds the accumulator, instances of Applicative and Monad explaining what's the default accumulator and how to combine two accumulated values (oh and make sure these choices satisfy the relevant laws, which in this case means that 0 and `(+)` must form a Monoid), an instance of MonadAccumulate explaining how to accumulate Ints, and a clever rank 2 polymorphism trick justifying that we don't need to worry about any other actions than Ints wrapped in `accumulate2` because those are the only values which can be constructed if all we have is a MonadAccumulate instance. We can of course simply write `mySum2 :: MySumT Identity () -&gt; Int`, but the conceptual gap between `MonadAccumulate m =&gt; m ()` and the Ints we will sum is still there whether we explain it in the code or not. 
Here's one using exceptions: import Control.Exception import Data.Void import System.IO.Unsafe newtype E a = E { unE :: a } instance Show (E a) where show _ = "E" instance Typeable a =&gt; Exception (E a) elim :: forall a. Typeable a =&gt; ((a -&gt; Void) -&gt; Void) -&gt; a elim f = either unE absurd . unsafePerformIO . try . evaluate $ f (throw . E) It needs `unsafePerformIO` and a `Typeable` constraint (but GHC will provide that anyway).
It's not so much that I'm lamenting the compiler's naiveté as I am lamenting my lack of control over the reduction process. The compiler doesn't need to be smart here: I know exactly what I want to reduce, I just have no way to communicate to the compiler "please reduce this term in this way before proceeding." Compilation as it is now is largely a black box, where you throw high-level code in and pray the optimizer performs some wizardry to make it fast. My fantasy is a model where I can quickly, immediately inspect terms as I construct them and specify what I want reduced when. I don't want better magic; I want control.
To take it one step further, functions are the only functors. Another way to write an infinite list `[a]` is `Integer -&gt; a`. Because `a` is in the positive position, infinite lists are __covariant functors__. A binary tree also has an index for each node, so an infinite binary tree can be represented as `Integer -&gt; a` as well. Or you can replace `Integer` with a different index type to make it easier to work with. Maybe is also a covariant functor: `Maybe a = Nothing | Just a` turns into `MyMaybe a = Branch0 (Void -&gt; a) | Branch1 (() -&gt; a)`, which turns into `MyMaybe2 a = (Bool, Either Void ()) -&gt; a`. You can flip these around to get contravariant versions. If you had an infinite list of unique primes `Integer -&gt; Prime`, then you could flip it around to get a contravariant `Prime -&gt; Integer`. If you had a binary tree, you could flip it around to associate each element with its index: `a -&gt; Integer`. And if you had a `Maybe a`, you could flip it around to get `a -&gt; (Bool, Either Void ())`. Since you can't actually construct a `Void`, this can be simplified to `a -&gt; ()`, so the inverse of `Maybe a` is a constant that selects the `Just` branch.
Are these rules sufficient to show that ~ propagates out of ∀? Is that even the case? (I don't know as this is unfamiliar territory.) ie. y =&gt; ~~y { given } ∃ x. P x =&gt; ~~(∃ x. P x) { y = ∃ x. P x } ∀ x. ~(P x) =&gt; ~(∀ x. ~(P x)) { ~∃ &lt;=&gt; ∀~ } Interested in learning what mistakes I've made here.
&gt; Maybe is also a covariant functor: Maybe a = Nothing | Just a turns into MyMaybe a = Branch0 (Void -&gt; a) | Branch1 (() -&gt; a), which turns into MyMaybe2 a = (Bool, Either Void ()) -&gt; a. No, that's isomorphic to `Bool -&gt; a`, which is `Either a a`, since `Either Void anything` is just isomorphic to `anything`. You have a mistake when you go from `MyMaybe` to `MyMaybe2`. `Maybe` isn't isomorphic to `(-&gt;) i` for any `i`. If it were, I could construct a value of type `i -&gt; Void` by transporting `Nothing :: Maybe Void` over your isomorphism. So `i` must itself be uninhabited, but then `i -&gt; a` is isormorphic to `()`, and not isomorphic to `Maybe a` for all `a`.
Continuize and curry!
I think you're right, and you can't collapse the sum type fully. I suppose I can weaken my claim to "All functors are sums of functions", then? That doesn't seem like it's saying much, since all ADTs are sums of constructors(i.e. functions).
Yup, and this is basically equivalent to a Taylor expansion of the functor.
Functors you might write using `data` are sums of products. These are the ones we're most familiar with, and are the sort of thing that the `Generic1` class captures. But the `Functor` class is more general. For example, functors like `Cont` or `Coyoneda` are not isomorphic to sums of products in general.
I don't recommend waiting...
Also `[a]` isn't isomorphic to `Natural -&gt; a`. data Stream a = Stream a (Stream a) is. FWIW, we have [`Distributive`](http://hackage.haskell.org/package/distributive-0.5.3/docs/Data-Distributive.html#t:Distributive) class, and not every `Functor` is `Distributive`. The list story is complicated: (Finite) [a] ~ exists (n :: Natural). Fin n -&gt; a where `Fin n` are numbers up-to `n`. One intuition is that [a] ~ a^0 + a^1 + a^2 + a^3 + ... Each sum term is isomorphic to `Fin n -&gt; a`, and the existential `n` let's pick the correct term. I wonder is there are papers on the topic, interested to read. 
Every functor and cofunctor can be seen as a profunctor, with an irrelevant type variable (c.f., `Clown` and `Joker` in the profunctors library). Then if you take their `Product` you get yet another profunctor. Here, that construction is the type of pairs of encoders and decoders. You can then try to compose them together as a single entity, see [product-profunctors](http://hackage.haskell.org/package/product-profunctors) and [codec](http://hackage.haskell.org/package/codec). I like the latter because it just reuses `Applicative` and `Monad`. Aside from encoding/decoding (synonymous with parsing/printing--here's a [codec-style wrapper around attoparsec](https://github.com/lysxia/unparse-attoparsec), I figure the same can be done with other parser combinator libraries), I'm also trying to write [generators and predicates](https://github.com/lysxia/gap) that way. Are there other useful instantiations of these patterns? There is some overhead that seems difficult to get rid of, in code complexity, and encoder performance (especially when it is made monadic, there is something awkward going on with the covariant parameter). 
Hmm, fair enough, though you probably wouldn't invent your own transformer there but use the existing ones. Then you pretty much get rid of all the cruft with deriving. You still get don't really see where the folding happens though, so I get your point.
&gt; Can you give me a link to Python docs which describe this? [python formating string](https://docs.python.org/3/reference/lexical_analysis.html#f-strings). This is the documentation for the formatting string literal of python, but implementing all the specification is not needed ;) The [formating mini language](https://docs.python.org/3/library/string.html#formatspec) tells you what you can put inside `{}` to format your value, [python formating string](https://docs.python.org/3/reference/lexical_analysis.html#f-strings) details the syntax of the `f"..."` interpolated string syntax. &gt; If you could give a bunch of sample tasks that you couldn't figure out how to do with `fmt` [...] Disclaimer, I'm a "old" python and C developer, so printf like syntax is obvious for me. I think I figured anything I need from `fmt`, but my issue with that kind of library is the overhead I have to read or write code with it. (So most of the issues come from my own limitations ;) We can start with this piece of python: &gt;&gt;&gt; name = "guillaume" &gt;&gt;&gt; age = 30 &gt;&gt;&gt; print("hello", name, "your age is", age) hello guillaume your age is 30 Here, `print` calls the `__str__` function for each arguments (it is like our `show` function) and separate all of them with a space. This is highly convenient. With `fmt` I need to do something like: Prelude Fmt Data.Monoid&gt; putStrLn ("hello " +|name|+ " you age is " +| age |+ "") hello guillaume you age is 30 You can be sure that 99% of the time I'm wrong with my usage of the operators, I forgot the trailing `|+ ""` and I forgot a space in the different strings. Apparently I may have options with the different variant of the concatenation operators, but that's too many of them for me. Then, I want to format some floating point number rounded: &gt;&gt;&gt; value = 5.1234 &gt;&gt;&gt; f"the value is {value:.2f}" 'the value is 5.12' I like this syntax because the formating part (i.e. `.2f`) is close to the one used by `printf` and a lot of unix tools I know and it does not clutter the readability of the string. With `fmt`: &gt; putStrLn $ "the value is " +| fixedF 2 value &gt; the value is 5.12 I always forgot the trailing space after "is" and I had to look at the documentation to find `fixedF`, everytime. 99% of my needs for string formatting are limited to theses uses cases. For more complex tasks, such as padding, I found `fmt` easier to use because the formater can be combined (they are function). So `padfLeft 10 '0' (expF 2 value)` is easier to think about and write than `0&gt;10.2e`, but there is still a readability issue: &gt;&gt;&gt; f"the value is {value:0&gt;10.2e}" 'the value is 005.12e+00' &gt; putStrLn $ "the value is " +| padLeftF 10 '0' (exptF 2 value) the value is 00005.12e0 
I came too that conclusion too. Class-based immutable object oriented programming is impredicative data types and F-coalgebra.
Can you write `length` for your existential type? And also, you're missing a `Stream a` term.
Looks like monads are finally cool?
http://okmij.org/ftp/Computation/lem.html#nnA ? (without `Cont`.)
How can digitToInt give me an exception when it doesn't have an exception type in its type signature?
Aren't you only talking about representable functors?
&gt; Idiomatic Java typically involves a ton of dynamic runtime checking Like what? Bounds checks on arrays? Haskell does that too. &gt; and vtable method lookups How do you think type-classes work? Hint: There's a reason the [SPECIALIZE](https://downloads.haskell.org/~ghc/7.0.3/docs/html/users_guide/pragmas.html) pragma exists. Further with the prevalence of the final-by-default idiom in contemporary Java code, a lot of Java method dispatch is static. &gt; and the lack of unboxed value types can wreck some tight loop performance The generational garbage collector does a very good job of small object allocation and management. Moreover Java, unlike Haskell, defaults to unboxed primitives for local variables. Haskell meanwhile _uses_ boxed value-types [by default, and relies on the optimiser to eliminate them in certain cases](https://stackoverflow.com/questions/5132350/how-do-haskell-compilers-decide-whether-to-allocate-on-the-heap-or-the-stack). &gt; Java can be very fast, if you don't write monstrously indirected OOP code with it. Code which isn't monstrously indirected _is_ idiomatic Java code. Honestly, I'm not in the slightest bit persuaded by your claim that _idiomatic_ Java is slower than Haskell. Certainly in the benchmarks game, [Java developers have found it easier to catch up with C than Haskell developers](http://benchmarksgame.alioth.debian.org/u64q/compare.php?lang=java&amp;lang2=ghc), and similarly in the [Techempower Benchmarks](https://www.techempower.com/benchmarks/) the idiomatic Java solutions perform substantially better than the idiomatic Haskell solutions. 
&gt; it doesn't have an exception type in its type signature? What language are you thinking about? Haskell doesn't have explicit exceptions, evaluation of a value of any type can throw an exception. That's why we can have the function `error :: String -&gt; a` universal for all types, `a`. If you are talking about the various exception-like monads such as `Control.Monad.Except.ExceptT` then bear in mind those are not exceptions but values plumbed through a monad that provides an alternative notion of return. 
What do you consider a "small library" vs a "battery-included"? I think of snap+snaplets/yesod/happstack as more batteries included frameworks that encourage rather particular choices of solution strategies while servant, just snap server, scotty etc are lighter weight.
I have a variant of that generators and shrinkers code somewhere on my hard drive. It uses Applicative and Alternative for the generators and Decidable / Divisible from the contravariant package for the shrinkers. The shrinkers in that set up end up with type a -&gt; [b], so you build up everything that you need and then set a ~ b at the last moment. I can't remember if I had Monads rigged up for that. I was going to go back to it and then Hedgehog arrived and I didn't feel a strong urge to continue with it. It did seem like an interesting approach at the time though.
Well, you did enable IncoherentInstances ...
BTW, any help with that hideous design is welcome ;)
It really depends on how much distinct (atomic) effects there are and how expressive your language has to be. Since you don't reveal much, I can't tell. If you decide to use Haskell as host for the DSL (e.g., because you need a lot of functionality and expressivity) you could still add some parts as a library (which prevents recompiling to a degree). This is similar to what *XMonad* does with its config. I think it recompiles, but you could also use GHCi. `TemplateHaskell` might help in associating available functions (of the module) with the string used in the text (either follow Haskell identifier conventions or use a transformation scheme).
[Yup](https://www.reddit.com/r/haskell/comments/6qya9a/the_m_word_is_now_cool/)!
Do them all in one line: $ find students -name Homework.hs -exec ghc {} MyTestScript.hs \; Where `students` is the folder with all the student name folders.
heh.... nice try kid.. monads were Always cool... yeah.......
Good surprised or bad surprised?
Make a configuration record, then return your function as the closure of another function that accepts this record. {-# LANGUAGE RecordWildCards #-} data Configure = CFG { tolerance :: Tolerance -- etc configure (CFG {..}) = maximizer where -- Move your maximizer definition here verbatim -- You want "CFG {..}" exactly the ".." is not an elision -- Make the fields of your record the 'globals' you want Then to use it: maximizer = configure $ CFG { tolerance = -- etc someFunc = maximizer (...) You can now also have multiple maximizers with different configurations going at once, if appropriate.
I'd be happy to take a look if you can find it!
Oh yeah, pstree! https://asciinema.org/a/b0OLa8c0DdF9cyOiODx0jxAns
Derp
Sorry for the really late response. I don't think you're wrong about the sequence computation thing. I mostly use Erlang and Elixir, which translates everything into a begin statement in operational semantics terms where everything is done in turn and only the last evaluation is returned, so the concept of do notation made sense to me immediately. There is a slight difference between the two because Erlang and Elixir are impure, but it is a good way to approach it from my perspective. You are right that the symbols get in the way when first learning the language. I know it took me a while to get to the point when I was comfortable with them. I mean one of the reasons I like ML is the lack of extraneous symbols. But after learning and understanding what the symbols meant, it made sense. There are an abundance of them though and any real Haskell code takes patience and a huge amount of time to work through when you're a beginner, whereas in most other languages you can be reading code within a week.
He means someone could write a parser so you could create these diagrams by only editing a text file and running the program (no need to compile what you wrote)
I like that he defines monads with join instead of bind :D!
This hasn't impacted a whole lot. Do you think i'd be better off ditching ST and just returning a new emulator with state modified, and then doing THAT in st?
So it is meaningful to expect to combine two components that perform different effects to produce a third that uses a combination of these effects using these operators and following the laws of these operators. Then the question is: WHY the ... haskell community do not care to make it possible?
Without realizing it, the discussions are about what of the two approaches are less non-composable, while a fully composable solution, that would end the discussion, is relatively easy. See: https://www.reddit.com/r/haskell/comments/72th9v/free_monad_considered_harmful/dnm9rmq/ 
Now you can specify minimum `node` version &amp; extra arguments for starting `node` :)
You can so something along the lines of {-# LANGUAGE RecordWildCards #-} module Maximizer where data Config = Cfg { tolerance :: Tolerance, maxIterations :: Int, -- other fields } data Maximizer = Maximizer { maximize :: _, -- other functions your module wants to export } makeMaximizer :: Config -&gt; Maximizer makeMaximizer Cfg{..} = Maximizer{..} where maximize = ... -- other functions Using your library would be done like so: {-# LANGUAGE RecordWildCards #-} module Main where import Maximizer (Config(..), makeMaximizer, Maximizer(Maximizer)) myConfig = Cfg { tolerance = blah maxIterations = 42 } myMaximizer = makeMaximizer myConfig -- Optionally: Maximizer{..} = myMaximizer This is also how Parsec's [Token](http://hackage.haskell.org/package/parsec-3.1.11/docs/Text-Parsec-Token.html) module works.
If you could tar up the html sources I'd be happy to generate ePub &amp; mobi files to go alongside the pdf. Or stick them in a github project?
Here's one useful example: http://www.well-typed.com/blog/2015/07/checked-exceptions/
Hmm... I hacked up a tiny scraper script in python that grabbed all the sources, but on the last step saved them into .tex files using pypandoc, so I never actually had the html files saved. Unfortunately, I don't know any LaTeX beyond what I learned these past 2 weeks, but I'm pretty sure you could generate epub/mobi out of the .tex sources?
Maybe! Worth a try. If there's a lot of maths typesetting then the mobi conversion will be awkward. Have to see how it goes...
Yeah, for this I actually kept almost everything verbatim (using monospaced font), so it may work. Quick googling suggests pandoc could work, but it doesn't seem to support the 'subfile' package I've used (to keep chapters in separate files), although there's a PR on the pandoc github that suggests it should be possible...
The [boxes](https://hackage.haskell.org/package/boxes) would benefit of some example output!
It looks promising thanks **Edit**: And it even [uses `type role Throws representational`](https://gist.github.com/edsko/f1f566f77422398fba7d#file-checkedrevisited-hs-L46), [SPJ said](https://ghc.haskell.org/trac/ghc/ticket/14292#comment:7) &gt; I bet that almost no one uses a representational role annotation on a class! Although it can be made stronger with `type role Throws phantom`!
The solution the rust book uses for this is to concatenate all the html into one big file in a Makefile (or cargo file or whatever) before shoving it through pandoc IIRC.
Very nice! Thanks!
[removed]
Just put your global constants(s) at the beginning of your function(s) and alias your function after the import to set the global parameter once in your module. So your using module looks like Import Maximizer(maximizer) myMaximizer = maximizer myParam Know, myMaximizer has one parameter less that maximizer (I feel that you are trying to solve a problem which doesn't exist in Haskell) 
I'm looking to do evil sorcery :) **Edit**: &gt;:)
As suggested in this [post](https://redd.it/72sct8) is it possible or to generate a fromJson of "constructeur with hole" using the generic fromJson instance. For example you have data User = User { name :: String , email :: String , Verified :: Bool } We want to parse a Json which doesn't contains "verified" . On solution is to instanciate `FromJson (Bool -&gt; User)` and the pass the verified value , once the Json has been parsed. Can this instance be written without parsing explicitly each field of the Json but using the `FromJson User` instance instead ?
Beautiful! Might be worthwhile TikZ-ifying some of the diagrams. The piggies, of course, should be left untouched.
That makes a lot of sense, &gt; :i Coercion type role Coercion representational representational are there cases when you need `Vacuous a :- Vacuous b`? `Sub Dict :: Vacuous a :- Vacuous b` works but ~~`coerce :: Dict (Vacuous a) -&gt; Dict (Vacuous b)` fails bizarrely with your `Dict` but works with my `Dict` which feels like a bug~~ **Edit**: This is explained by your `type role Dict nominal` and `type role (:-) nominal nominal` annotations but I'm not sure why they're not marked `representational` like GHC infers
I wonder if this could be done in Bakcpack, by leaving some type definitions of kind `Nat` abstract. (I'm not saying this would be a good use of Backpack.) (Update: I haven't been able to make it work.)
Thank you all who answered: I will try to implement the suggestions of u/WarDaft and u/Solonarv I also agree with u/maxigit: I am probably overthinking this, and just application of composability would work as well.
Yep. I excitedly tried to reproduce this blog post in Haskell as soon as I read through it. Nope, I can't analyse `Symbols` so I can parse them. You can't pattern match on type families ("Illegal type synonym application in instance"), and it wasn't immediately obvious to me how I would use GADTs and other tricks to defunctionalise a simple unconsing operation. And Purescript is a language that doesn't even have type families (thus all the MPTC + FD stuff in the article)... 
I made this into a PDF! You can download it here (with LaTeX sources): https://github.com/hmemcpy/milewski-ctfp-pdf
by far my favorite image of all :) https://i.imgur.com/K51rs8J.png
What else would you like me to reveal? :)
I tried something reminiscent of [*reflection*](https://hackage.haskell.org/package/reflection) type role Mon representational class Mon a where unit :: a mult :: a -&gt; a -&gt; a newtype MonoidAdd = MAdd Int deriving newtype (Num) newtype MonoidMul = MMul Int deriving newtype (Num) instance Mon MonoidAdd where unit = 0 mult = (+) instance Mon MonoidMul where unit = 1 mult = (*) data D c where D :: c =&gt; D c trash :: forall ty. (Coercible ty Int, Mon ty) =&gt; D (Mon Int) trash = coerce (D :: D (Mon ty)) main = do D &lt;- pure (trash @MonoidAdd) print (unit @Int) print (mult @Int 10 20) D &lt;- pure (trash @MonoidMul) print (unit @Int) print (mult @Int 10 20) but it produces &gt; main 0 30 0 30
I'm using GHCI in a terminal window and when refactoring, can be shown a long list of compilation errors. In this scenario I am normally only interested in the first error - I would fix it and then enter :r to recompile. Is there a setting to prevent it from recovering and hence stop listing the compilation errors after the first one? Even though that'd be less verbatim, it would make my use case easier to handle. I haven't found anything like that in the documentation. :) 
So you want a pragma like `SPECIALIZE`. That is an interesting avenue.
Not exactly what you asked for, but `-freverse-errors` will make the first one come out last, so you wouldn't need to scroll up. https://twitter.com/haskelltips/status/884490613474734080 https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/flags.html#warnings
Well, we wanted a hassle-free "stack install" to work. So we install node in nodejs-exec if it isn't new enough.
I needed this. Thanks!
Wow. It'd take a lot of work to make sure the logic to download &amp; install a node.js works on all platforms..perhaps instructing the user to provide the right `node`/`npm` on `PATH` is not that much a hassle?
By using a genericly defined `FromJson User`? No, you can't do that since the parse will fail if `verified` is not in the JSON.
If you want to stick the Tex files in a GitHub repo that could also be useful as well. I wouldn't mind going through and seeing if I could clean up the LaTeX code as well.
Sure! Everything is there under the src directory. Each chapter is in its own subdirectory under 'content'! Any help appreciated! 
I was thinking of inject temporaryly the verified in the local json, but I'm not sure how to escape from the parser monad.
Doh, I missed that the PDF was already inside a GitHub repo. I'll check it out, thanks!
My immediate thought was to parse the JSON into a generic map representation, insert the field, render as JSON, and then reparse. Of course, that's not great for performance. TBH, I thought of using lens operators to do this, but I bet my first description is what they do under the hood.
(Ok, so this isn't quite a beginner question. I still want to ask.) Is there a good way to use a term constructed in finally tagless style with two different interpreters? Context: http://okmij.org/ftp/tagless-final/course/Serialize.hs search for "What happened? We lost polymorphism!" It describes a non-solution: using higher ranked existential types brings back polymorphism but kills extensibility. The solution it gives, writing a `duplicate` interpreter for every type class, is unsatisfying for two reasons: boilerplate and (admittedly unbenchmarked) performance. Is there a better way that's been developed or made possible by new extensions since these notes came out?
[Turns out](https://stackoverflow.com/a/45851399/849891), *at its core* it's just about the fact that when nested folds fuse, their reducing functions nest -- by replacing a `cons` operation used by the inner fold's reducer with the outer fold's reducer. In Scheme, (foldl + 0 (foldl (lambda (x acc) (cons (sqr x) acc)) '() (foldl (lambda (x acc) (if (even? x) (cons x acc) acc)) '() xs))) = (define (((mapping sqr) cons) x acc) (cons (sqr x) acc)) (define (((filtering even?) cons) x acc) (if (even? x) (cons x acc) acc)) (foldl + 0 (foldl ((mapping sqr) cons) '() (foldl ((filtering even?) cons) '() xs))) = (foldl ((mapping sqr) +) 0 (foldl ((filtering even?) cons) '() xs)) = (foldl ((filtering even?) ((mapping sqr) +)) 0 ; (lambda (x acc) (if (even? x) (((mapping sqr) +) x acc) acc)) ; (lambda (x acc) (if (even? x) (+ (sqr x) acc) acc)) xs) = (define (transduce-list tdcr op z xs) (foldl (tdcr op) z xs)) (transduce-list (compose (filtering even?) (mapping sqr)) + 0 xs) `foldl` is for consuming from *lists*; and `+` is for producing *an accumulated number*. `(filtering even?)` and `(mapping sqr)` are the two transducers here, taking a reducing function (or "reducer") and producing an augmented one, `tdcr :: reducer -&gt; reducer`. Thus the composition of processing steps is explicit and does not rely nor depend on laziness. 
What you mean by "missing `Stream a` term? Compare to data List a = Cons a (List a) | Nil `Stream` doesn't have `Nil`, so it never ends. As doesn't `Natural` numbers. Here's an evidence that you can convert `Stream a` to `Natural -&gt; a` and back. import Numeric.Natural data Stream a = Stream a (Stream a) -- Essentially an indexing function -- -- &gt;&gt;&gt; takeStream 10 (toStream (fromStream (toStream id))) -- [0,1,2,3,4,5,6,7,8,9] fromStream :: Stream a -&gt; Natural -&gt; a fromStream (Stream x xs) n | n == 0 = x | otherwise = fromStream xs (n - 1) -- &gt;&gt;&gt; takeStream 10 (toStream id) -- [0,1,2,3,4,5,6,7,8,9] toStream :: (Natural -&gt; a) -&gt; Stream a toStream f = Stream (f 0) (toStream (f . succ)) takeStream :: Natural -&gt; Stream a -&gt; [a] takeStream 0 _ = [] takeStream n (Stream x xs) = x : takeStream (n - 1) xs For your other question, Can I write `length` for an existential type? Yes, I can. But I need a language where I can have such type in the first place. For example Agda, which looks a bit like Haskell, but has dependent types. module List where open import Data.Product open import Data.List open import Data.Nat open import Data.Fin -- Agda has dependent products: Σ -- -- that's an interesting question on its own, -- why dependent product is denoted with sum symbol!) -- -- Here we say: -- type Elist a = exists (n :: Nat). Fin n -&gt; a -- Agda looks a bit like Haskell, but different enough EList : Set → Set EList a = Σ ℕ (λ n → Fin n → a) -- First: the length of the EList -- it's trivial, as it's the first element of a pair! elength : {a : Set} → EList a → ℕ elength (n , _) = n -- We can define List → EList function -- -- In empty case we have weird looking `λ ()`. -- That's the term for `Fin 0 → a`. As there aren't natural numbers -- less than zero, Agda doesn't force us to come up with any `a`. -- See `Void` and `absurd` in Haskell -- -- For the cons case, it's about what you'd expect, except that -- Agda doesn't have `case`, but there is `with`. toEList : {a : Set} → List a → EList a toEList [] = zero , (λ ()) toEList (x ∷ xs) with toEList xs toEList {a} (x ∷ xs) | n , f = suc n , g where g : Fin (suc n) → a g zero = x g (suc m) = f m -- Other direction, EList → List is also simple recursive function. -- auxiliary function is to avoid wrapping pair while recursing. -- It's also helps to convince Agda, that this function terminates -- ("n decreases" is more obvious than "fst p decreases"). fromEList : {a : Set} → EList a → List a fromEList (n , g) = aux n g where aux : {a : Set} → (n : ℕ) → (Fin n → a) → List a aux zero _ = [] aux (suc n) f = f zero ∷ aux n (λ m → f (suc m)) -- And because we can proof things in Agda, we'll proof that -- converting List to EList and back gives us the same list open import Relation.Binary.PropositionalEquality -- first an auxiliary lemma, that if heads and tails are equal, -- then the lists are too. ∷-apply : {a : Set} → {x y : a} → {xs ys : List a} → x ≡ y → xs ≡ ys → x ∷ xs ≡ y ∷ ys ∷-apply refl refl = refl -- The proof itself is trivial, as Agda figures out the details. proof1 : {a : Set} → (xs : List a) → xs ≡ fromEList (toEList xs) proof1 [] = refl proof1 (x ∷ xs) = ∷-apply refl (proof1 xs)
Well, we best-effortly clone the node repo, and run its build scripts - then install the resulting executable as a cabal data file.
Excellent! Now I know what I'll read when I'm away from home...
I'm using reactive-banana. I have an `Event Input`, an initial `Vehicle` and a `f :: Input -&gt; Vehicle -&gt; Vehicle`. And I'm looking for an `Event Vehicle`, where each `Vehicle` is based on the current `Input` and the previous `Vehicle`. You can think of the most recent `Vehicle` as current state, and each new `Input` causes a new state using `f`. [An image is clearer](https://i.imgur.com/H12WHjY.jpg)
&gt; What you mean by "missing Stream a term"? Sorry, I was on my phone. I meant that `[a] ~ (a^0 + a^1 + a^2 + a^3 + ...) + Stream a` &gt; -- First: the length of the EList &gt; -- it's trivial, as it's the first element of a pair! &gt; elength : {a : Set} → EList a → ℕ &gt; elength (n , _) = n Ahah, fair enough, a constructive existential comes with its witness.
node website already provides bindists for windows/linux/mac, so perhaps the building step is not required. I'll take some time to see if it's worth implementing...
Does Bartosz really praise Wolfram in the second part of the book? Nothing Wolfram has ever done has been "heroic" :/
Why in Earth would anyone ever read YouTube comments? Regardless of the subject matter, YouTube comments are mindless.
Thanks a lot! And there's a "max errors" right after it, too. :)
It is so very, very tempting to click "Not constructive" on this...
Think I got it: vehicleE :: Vehicle -&gt; Event Input -&gt; Decision -&gt; MomentIO (Event Vehicle) vehicleE vehicle inputE f = accumE vehicle (f &lt;$&gt; inputE) 
First, no matter how good a writer you are, I'm not sure monad tutorials are a good idea. It's easy for the reader to take them as a Haskeller trying to look smart with abstract nonsense no one really needs. Once you've used monads in a language that has good support for them you realize that's totally wrong, but monad tutorials aren't addressed to those people. That said, let's look at this one in particular. We'll start with the first line: &gt; Monads are all about function composition and hiding the tedious part of it. This looks wrong to me. Monads aren't about "hiding" anything at all. Programmers, especially the target audience of Go programmers, are rightfully scared of hiding things. They associate it with Ruby, convention over configuration, magic, and code that breaks in weird ways and is hard to modify. So we're not off to a great start. Just to check that I'm not the only one who sees "hiding" as misleading, take a look at the [top comment](https://www.reddit.com/r/golang/comments/734kp8/monads_for_go_programmers/dnnwp42/) in the Go subreddit: &gt; Am I the only person who doesn't mind the if err returns? Yea, it's a little tedious at times. But it makes it SUPER easy to trace my code when hunting down issues. Monadic error handling (eg through Either) still uses return values. The entire point of monads has been not only missed, but incorrectly understood to be the opposite of what it really is. Instead of "hiding", I would say that monads *generalize* a particular type of function composition. This is both more correct and better PR, a clear win. On to the second section: &gt; Don’t read this &gt; &gt; In Erik Meijer’s Introduction to Functional Programming Course on Edx he asks us to please not write another post on monads, since there are already so many. When you hear, "writing tutorials for X is unhelpful and bad pedagogy" and do it anyway I'm honestly not sure what to say. The rest of the article is fine, but lets look at the conclusion: &gt; Monads hide some of the repeated logic of composing functions with embellished types, This is still wrong. Monads don't hide anything, unless you consider a function that does some explicit thing to be hiding that thing. Let's take this example: `bind :: Either e a -&gt; (a -&gt; Either e b) -&gt; Either e b`. Is this function hiding anything? No, it's just doing something. If that function's not hiding anything, then a generalization of it `bind :: OurClass m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b` is certainly not hiding anything either. I'm sorry if this comes across as mean. I don't intend it that way at all. The author is obviously enthusiastic and wrote an above-average tutorial on a difficult subject. My criticism are aimed at the work, not the writer, and I'm only writing them so strongly because I'm tired of seeing people fall into this same trap over and over, despite clear warnings. (And criticism of my points would be appreciated by the way).
The text also proposes then dismisses another solution: &gt; What are the solutions? &gt; One is to re-write fromTree to have this signature &gt; &gt; fromTree :: String -&gt; Either ErrMsg Wrapped &gt; &gt; where &gt; &gt; newtype Wrapped = Wrapped{unWrap :: forall repr. ExpSYM repr =&gt; repr} &gt; &gt; emulating first-class polymorphism. The successful case analysis &gt; of the parsing result will give us the value of the type Wrapped, &gt; which can be interpreted in many ways, as its type indicates. &gt; Alas, we lose extensibility again: we can no longer enrich our &gt; language because we fixed the constraint `ExpSYM`. When we later &gt; add the multiplication form, we need to add the `MulSYM` constraint. &gt; Thus we have to re-define `Wrapped`. We could not use any of the &gt; existing `fromTree` code (in its compiled form). I don't understand why they want to restrict the type of `fromTree` so that it can only return `Wrapped` values. If you continue to let it return any `ExpSYM` and you give an `ExpSYM` instance for `Wrapped`, you can use `fromTree` with just `eval`, just `view`, or if you really need to run both, then you can pay the cost of the intermediate `Wrapped` form. And if you later define a `WrappedMul` which includes multiplication, you can reuse `fromTree` with that too!
I don't think it worked.
That's the solution I was referring to as "higher rank existential types".
I know :) (The output is far too wide)
I'd be open to relaxing that annotation. At the time the interaction between representational and Constraint was rather poorly understood.
Completely statically linked? EDIT: Because if not, a bindist for "linux" is likely going to be incompatible with the various wild libc's in various distros, for example.
YAGNI. And if it turns out you really needed it, refactoring is a charm in Haskell. 
`ghcid` is the only tool I have found that can reliably handle my Haskell program (around 50k SLOC) without slowing down, or using gargantuan amounts of memory. Intero is good when it works, but often ends up slowing down Emacs unacceptably, and typically uses 8+GiB of memory. I don't know exactly which flags `ghcid` passes to `ghci` to make it fly, but it does it very well. The only thing I miss in `ghcid` is just enough Emacs integration for next/previous error commands to work.
There is reasonably good integration for Neovim and VSCode, so I'm sure something could be built for Emacs if there was any desire. 
I'm a novice in the math side of these things, but I gather from the context that `F = M` wouldn't work, because the free monad would need to be the same for all monads. 
If you want to compose two monadic computations with different effects, just express the effects as typeclass constraints on a polymorphic `m`. While you're at it, you should take a moment to evaluate whether they need to be monadic in the first place. When people say "Haskell has monads, and they're awesome!", that doesn't mean you should always use concrete monads to express everything. Actually, I'm often annoyed by the amount of attention monads receive in general, while `Applicative`, `Traversable` and a gazillion other abstractions and the interplay between them is what makes Haskell so powerful.
I made a paperback version of the PDF on lulu.com: http://www.lulu.com/shop/bartosz-milewski/category-theory-for-programmers/paperback/product-23352094.html
Ah I was curious what tooling you used for this. It's something I'd naturally reach for Python for too, but at the moment I'm trying to force myself to use Haskell for the little one shot scripts and glue stuff. 
&gt; If you want to compose two monadic computations with different effects, just express the effects as typeclass constraints on a polymorphic m. Try yourself: there is no way to compose two computations with different effects using current monadic, applicative or alternative operators, with or without typeclasses. &gt; Actually, I'm often annoyed by the amount of attention monads receive in general, while `Applicative`.... I include `Applicative` The same problem happens for any binary operator that you may think as they are defined now.
Who gets the money from sales?
A PDF edition has been created by /u/hmemcpy. It has a [reddit discussion](https://www.reddit.com/r/haskell/comments/73e7l3/i_made_bartosz_milewskis_book_category_theory_for/) and a [github project](https://github.com/hmemcpy/milewski-ctfp-pdf).
Are you certain ghcid isn't leaking memory for you? I'm on 8.0.2 and every single tool built on top of gchi leaks memory, starting at a gig and landing up at 6-7gig in a few hours. 
Use `getYesod` funtion and pass that to to `appHttpManager`.
I still can't make it work. `getYesod` returns an `m (HandlerSite m)`, and it doesn't typecheck
No, sorry. There was an other error from a previous test that was messing up the error messages :) It seems to work now, thanks!
Not to be “that guy”, since this is a well written article, but it’s a dubious choice to talk about monads in a language without higher-kinded polymorphism, or indeed any parametric polymorphism at all. It’s useful to understand monads as a concept distinct from *particular* monads, and you can make use of that understanding when designing stuff. But it requires the “squinting” and code generation mentioned in the article, and the usefulness is limited if your language doesn’t have a way to talk about these things generically, since you’ll have to reimplement all the utility stuff like `when`, `mapM`, &amp;c.
It might - but if so, it leaks slower than Intero, so it all works out in the end.
Let's look at the free Monoid (lists) instead to get a better intuition. The free Monoid is initial in the sense that we can instantiate it to any concrete Monoid via `mconcat`, and moreover this transformation preserves the Monoid structure of lists. But that doesn't mean there is a single Monoid called `List` which is initial, is it? Instead, we have a family of Monoids, namely `[Int]`, `[String]`, etc., each of which is initial for a subset of all Monoids, that is, `[A]` is initial in a category which only includes the newtypes of `A` which have a Monoid instance. Similarly, I don't think we should expect a single type constructor which is initial in the category of all Monads, but rather a family `Free F` which is initial in the category of newtypes of `F` which have a Monad instance.
Lulu.com gets the money for printing it, and then there is no money left over, which is also why the book is so cheap. 
The simple solution would be just to make the field a `Maybe Bool`. It seems pretty not worth it to use hacks like mucking around with the JSON to get around the fact that your data type doesn't match the one you actually want to decode. I think the best general answer is that if you're developing a web app, this almost certainly won't the be last time the data type you want for your internal logic will deviate from the ones you want to have in your APIs. So you might as well define them as separate types in separate modules and write conversion functions. If that seems like overkill, or neither of those options are satisfactory for whatever reason, then just write a custom FromJSON instance. Its very easy [and well-documented](https://www.stackage.org/haddock/lts-9.6/aeson-1.1.2.0/Data-Aeson-Types.html#t:FromJSON).
Can I request you to please confirm if ghcid is actually leaking memory for you as well (however slowly)? Will help me confirm a possible bug in GHCi.
Even though I agree with you in principle, I'm not the one which need that, I am more interesting if what I am asking is possible in theory, rather than find a workaround.
I am surprised that no one here mentioned `Reader` as an abstraction yet. Define all your functions as `Reader`s and in each module simply define a function that *runs* `Reader`s with a custom configuration record similar to the answer of u/WarDaft and u/Solonarv.
Maybe not the best way, but a fun way to do it if you want to teach networking at all would be to use a servant API and client. The student would write there answers as normal, but in the main function they would parse a config file for network information, connect it to a servant API and query a server that has a set of test data, run their functions against the data and then said it to the server. Then when they submit the their homework you can run it with a slightly different config file, it points to a server/process that has a different set of a data but their homework should be able to return the correct answers.
Around [28:18](https://youtu.be/sT6VJkkhy0o?t=1694) an improvement over the current "layered" approach to concurrency is mentioned. I wonder if it will require some new abstraction or language extension.
What is the relation of this and the work of Conal Elliott on compiling to categories?
Ghcid and Ghci are run in separate processes, with pipe communication only. It should be easy to see which process has lots of active memory. If it's Ghci report it to GHC HQ, if Ghcid then to me. 
Just tried it and it is great! Thanks for the suggestion.
sweet http://neilmitchell.blogspot.com/2017/08/ghcid-and-vs-code.html
ATM haxl does breadth-first processing of the graph. `&lt;*&gt;` makes graph wider, and `&gt;&gt;=` makes the graph deeper. My bet comes from: &gt; Breadth-first search can be viewed as a special-case of Dijkstra's algorithm on unweighted graphs, Currently all nodes are equal (= unweighted graph), but if you make an API to assign weights to various `Haxl a`, nodes, than you have more information to do scheduling decisions (e.g. perform high-cost operations right away, don't wait to the round be full). *TL;DR*, I don't think new language extensions will be required, but the API will grow: more complex api is a trade-off for better performance.
It doesn't need any new extensions, and the only difference in the API is to add a `BackgroundFetch` alternative to the way that datasources can return results. There's a prototype implementation on [this branch](https://github.com/simonmar/Haxl/tree/new-monad3) and a [toy implementation](https://github.com/simonmar/toy-haxl) to demonstrate how it works. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [simonmar/Haxl/.../**06613e696cbd465abd84bbf69cce19d5ab658e06** (new-monad3 → 06613e6)](https://github.com/simonmar/Haxl/tree/06613e696cbd465abd84bbf69cce19d5ab658e06) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dnrh4f5.)^.
I want the hard copy! Is there a company yet that will let you "kickstart" a print, by merely raising the money for a minimum order quantity?
Well that was the fastest impulse purchase I've ever made.
thanks
&gt; Try yourself: Let's try; if I have someComputation :: (SomeEffect m, Monad m) =&gt; a -&gt; m b and also anotherComputation :: (AnotherEffect m, Monad m) =&gt; b -&gt; m c I can just use `someComputation &gt;&gt;= anotherComputation` to obtain a computation of type `(SomeEffect m, AnotherEffect m, Monad m) =&gt; a -&gt; m c` &gt; I include `Applicative` The same problem happens for any binary operator that you may think as they are defined now. Sorry, I guess I should've stressed that my mini-rant wasn't aimed at your comment in particular.
Ok. Does ghcid use the equivalent of the `:r` command while communicating with GHCi? 
It doesn't just use the equivalent of :r, it absolutely literally uses :reload (the fully non ambiguous :r). 
I wish somebody asked about the fact that `Haxl` violates the `Monad` laws in the strict sense, while they're preserved if you assume that the side effects commute. I guess that's the whole point behind the `ApplicativeDo` extension. Without this relaxation of the `Monad` laws, the utility of the extension would be greatly reduced (to those things that are not Monads, but could still benefit from the nice syntax). Well I wish somebody asked so that we could hear his thoughts (and I secretly wish to hear your thoughts as well :) )
Yes, this is a great tool, however there is still a couple of small issues: - Thing need to be restarted manually when `.cabal` file is changed, like new module is added or smth like that. - It's hard to figure out how to auto-run `hspec`-based tests, since there is nothing to load in `ghci` if they're being discovered via `hspec-discover`. `ghcid -c 'cabal new-test'` just exited unexpectedly. So, as it mentioned in `README`, `steeloverseer` could be a better option, however the feedback is slightly slower.
Sure, that would work: -- | -- &gt;&gt;&gt; fmap ($ False) $ parseMaybe (parseJSON :: Value -&gt; Parser (Bool -&gt; User)) $ object ["name" .= "George", "email" .= "george@example.com"] -- Just (User {name = "George", email = "george@example.com", verified = False}) instance FromJSON (Bool -&gt; User) where parseJSON input = (\u b -&gt; u {verified = b}) &lt;$&gt; parseJSON input' where exampleValue :: Bool exampleValue = True input' :: Value input' = case input of Object o -&gt; Object $ Object.insert "verified" (toJSON exampleValue) o _ -&gt; input I didn't have to escape from the parser monad because I modified the json before entering that monad. One big downside of this approach is that we need to have an `exampleValue`, which is unfortunate because that value is only used for filling in a valid json encoding, which is in turn only used to prevent User's instance from failing. If I was working in a codebase in which this style of instance was common, I would create a variant of aeson which recognized a special `Value` named `Undefined`, and would successfully parse that as `undefined` regardless of what the `FromJSON` instance says. Then, we can use `Undefined` instead of `toJSON exampleValue`, the `FromJSON Bool` instance will get skipped, and then `u {verified = b}` will overwrite that `undefined`.
&gt; Now Haskell is also pretty fast, especially compared to the VM-based languages (Java, C#, Py, Ruby, etc.). I wish we witnessed this claim in benhchmarks (like web framework benchmarks, etc.) as well :(
Do you know what happened to that GHCi IDE protocol (ala Idris or LSP) project? My memory is foggy but I believe it was Duncan.
Don't mix-up things: - `ApplicativeDo` doesn't re-arrange computations. - `Haxl` violates `ap = &lt;*&gt;`, which is different From ApplicativeDo paper &gt; Reordering the statements is only valid in a commutative monad, where the order of effects is not observable. The Haxl monad is not commutative, because it supports effects in the form of exceptions, so reordering statements can change which exceptions are reported. In our design, we therefore never reorder computations. We leave for future work the possibility of allowing reordering for commutative monads. (not sure if this is true anymore though).
Description: &gt; Lux is a new programming language in the making. It's meant to be a functional, statically-typed Lisp that will run on several platforms, such as the Java Virtual Machine and JavaScript interpreters. 
There have been many. They always do a bit, somewhat work a bit, but don't become robust enough. I hope one day to use one, since it would be much better than GHCiD. 
Yeah Idris has one from the get go because of interactive theorem tooling. Necessity and all that... Neil, if you ever build one, give a thought to the idea of using a tight, safe and efficient wire format and only supporting the common compatibility wart of JSON as an addon format. Kinda like people speak local languages and English to non-locals. I suspect you have experience with that from all the banking stuff you had to integrate with.
Great ! Can we not use `b` instead of `exampleValue` ? That was my initial idea but where I needed I think to escape the parses Monad.
I have no plan to build one yet! If I did, I'd do it as a Haskell in-process API, then define a wire format for that. 
http://www.lulu.com/shop/bartosz-milewski/category-theory-for-programmers/paperback/product-23352094.html
I don't understand. `ApplicativeDo` is in GHC starting from 8.0.
That's also a reasonable design which I can get behind. Needs extra care and API design to make sure there are no instances of incompatible chatter between different versions.
Oops, I've been out of the loop for a year. I should have watched the video...
The abstract for Compiling to Categories says this: &gt; It is well-known that the simply typed lambda-calculus is modeled by any cartesian closed category (CCC). This correspondence suggests giving typed functional programs a variety of interpretations, each corresponding to a different category. A convenient way to realize this idea is as a collection of meaning-preserving transformations added to an existing compiler, such as GHC for Haskell. This paper describes such an implementation and demonstrates its use for a variety of interpretations including hardware circuits, automatic differentiation, incremental computation, and interval analysis. Each such interpretation is a category easily defined in Haskell (outside of the compiler). The general technique appears to provide a compelling alternative to deeply embedded domain-specific languages. (from http://conal.net/papers/compiling-to-categories/) The approach here, on the other hand, is to try to provide a single embedded DSL which can be used with any CCC. I don't think you would necessarily use these for the same things though. Using the GHC core means you have access to the entire GHC front end, whereas with the EDSL approach, you need to add all of that functionality to your DSL manually. If you want something as simple as pattern matching, you'd be out of luck. On the other hand, using a DSL is nice because everything is contained in one module, all in Haskell. So, this probably has its applications, but I would probably only use this for writing relatively simple DSLs and terms. I was mostly just curious to see if I could write it. Compiling to Categories also deals with CCCs where there is some constraint on the types of the objects themselves, which I haven't done here. I suspect it's possible though, at which point you should be able to use this with all of the examples listed there.
Wrong sub?
No, that would be like writing a `Maybe (Bool -&gt; User)` which uses the Bool to determine whether to return a Nothing or a Just. edit: `Maybe (Bool -&gt; User)` is very different from `Bool -&gt; Maybe User`, and the same difference applies to `Parser (Bool -&gt; User)` and `Bool -&gt; Parser User`.
I see, That's why I get stuck ;-) However, in our case, we don't use the Bool to determine the result of the parsing. Maybe that is why some people don't like Monad. In this case, it seems to stop us something that we feel is doable (and it is doable,as your previous code shown).
The discussion isn't about composability. It is about efficiency. I don't see how graded monads would address this.
This is a much code transformation than is necessary here.
I don't think that the discussion is about efficiency alone. Moreover composability is, or should be, the top issue in any functional programming problem and in general in any programming problem, since it includes theoretical (mathematical) and practical (reusability, testability, usability) aspects
 someComputation &gt;&gt;= anotherComputation At first sight, you can not use `&gt;&gt;=` with these definitions. That does not type match &gt; Sorry, I guess I should've stressed that my mini-rant wasn't aimed at your comment in particular. Don't worry ;)
The monadic abstraction has nothing to do with it! If you had an object-oriented program containing: * a User class * a UserParser object with a `parse` method which expects its input json to contain all the User fields including `validation` and either produces a User or throws a parsing exception * a UserWithoutValidation object with an addValidation method which takes a Bool and returns a User, but definitely doesn't throw a parsing exception * a UserWithoutValidationParser object with a yet-unimplemented parse method which expects its input json to contain all the `User` fields except validation and either produces a UserWithoutValidation or throws a parsing exception You couldn't implement that method by adding the Bool received by `addValidation` to its input json and delegating to the `UserParser`. You obviously don't have a `UserWithoutValidation` object yet, so you definitely don't have access to that Bool yet! The sensible thing to do, in both cases, is either to implement User in terms of UserWithoutValidation instead of the other way around, or to implement a separate UserWithoutValidation parser which doesn't try to artificially add fields it doesn't need.
Also worth mentioning that it's pure.
In that case, it's going to leak memory. If not ghcid, then the paired GHCi process. I'll confirm yet again and report back. Do you have any hypothesis on why this could be happening?
Taking another shot at your question. If you are using Servant, your HTTP handlers are going to be very close to your underlying domain API - actually almost identical. In our use-case we started off with having another layer on top of the domain API, which was supposed to be the web layer, but all it turned out to be, were one liners that looking like: getOrder uid oid = runForUid uid $ Domain.getOrder oid shipOrder uid oid shippingDetails = runForUid $ Domain.getOrder oid shippingDetails So, the extra web layer was just turning out to be boilerplate. We got rid of it and put these one liners directly in the servant route definitions. 
No idea why it's happening, but I can confirm that using GHCid extensively for a large project has definitely been a memory leak for me. Though I've known it was GHCi's fault for quite some time. Just never gathered enough information to form a proper bug report.
Thank you.
Followup thread about the plugin language: https://www.reddit.com/r/haskell/comments/73r0zy/followup_to_my_hobby_editor_port_project/ 
Given the historical Cardano's penchant for gambling, I think it is a great name for a cryptocurrency platform!
One thing that has always bothered me about cryptocurrencies/blockchains is how the mining works. The huge amounts of hardware and electricity used to solve useless problems just feels so wasteful. It seems like [Cardano is using a better solution] (https://cardanodocs.com/introduction/#what-makes-cardano-sl-special) for this. I dont know enough to state if this solution is safe or not though.
Well, there's an issue with [laziness](https://ghc.haskell.org/trac/ghc/ticket/13875).
That would indeed be awesome!
I would have done something like (in pseudo pseudo code) UserWithouValidationParser.parse(json) = UserWithoutValidation.new(json) UserWithoutValidation.addValidation(b) = UserParser.parse(this-&gt;json.inject(b)) The `UserWithoutValidation` would work slightly differently from what you suggested, as there it's specific to our json problem. This should work fine when there is no problem, but indeed throw the exception at the wrong time. The exception will be raise when calling `addValidation` instead of when calling the parser. However, in practice, nobody will never notice as the exception would be seen a thrown in my big function `parse this big json and return a me a list of user`. I seems this is a good example of the compiler even though technically right, just getting in the way. (I'm not saying I agree with this, only that's it can be seen as good point from people in favour of dynamic typing). Anyway, you are absolutely right and I understand my mistake, and you hit the nail on the head by pointing about the difference between `Maybe (Bool -&gt; User)` and `Bool -&gt; Maybe User`. I was seeing a `Parser User` as a `s -&gt; User` (or at best `s -&gt; (s, User)`). In that case `s -&gt; (Bool -&gt; User)` and `Bool -&gt; (s -&gt; User)` are isomorphic. I think the reason why I "ignored" the possibility of failure and what frustrates me in this examle is that the actual failure doesn't depend on the value of the Bool. The generic FromJSON instance is actually applicative not monadic, so we should be able to feed it with external value (and it has to do with Monad). Maybe we are just missing the correct abstraction (A type of traversal, restricted to applicative).
There *was* an issue with laziness, the bug is now fixed.
What's Haxl's license and patent about? Can somebody eli5 if Haxl's licence is more or less permissive/restrictive than ordinary BSD licenses?
Haxl is technically is not law-abiding. But if you use it as it is intended to be used, with I/O that you don't mind re-ordering and overlapping, then the violated law won't affect you. Remember that the `IO` monad technically doesn't satisfy the monad laws, for different reasons, and mostly we've made our peace with that - I think this is another case where we have good reasons to stretch the laws a little. Yes the ice is thin, and you could argue that this is an abuse of Applicative. I'd be happy to have that discussion with anyone, but I strongly believe it is a discussion better had over a beer :)
Yes, Cardano is based on Ouroboros, which is [provably secure](https://eprint.iacr.org/2016/889.pdf) proof of stake blockchain (as opposed to "proof of work", like bitcoin, which -- as you very rightly point out -- waste huge amounts of energy).
Quite a few cryptocurrencies use [proof of stake](https://en.wikipedia.org/wiki/Proof-of-stake) as their consensus algorithm, which does not require miners to solve useless problems. Ethereum (the second biggest cryptocurrency by market capitalisation) is moving to proof of stake in the next couple of years.
**Proof-of-stake** Proof-of-stake (PoS) is a type of algorithm by which a cryptocurrency blockchain network aims to achieve distributed consensus. Unlike proof-of-work (PoW) based cryptocurrencies (such as bitcoin), where the algorithm rewards participants who solve complicated cryptographical puzzles in order to validate transactions and create new blocks (i.e. mining), in PoS-based cryptocurrencies the creator of the next block is chosen in a deterministic (pseudo-random) way, and the chance that an account is chosen depends on its wealth (i.e. the stake). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
**Proof-of-stake** Proof-of-stake (PoS) is a type of algorithm by which a cryptocurrency blockchain network aims to achieve distributed consensus. Unlike proof-of-work (PoW) based cryptocurrencies (such as bitcoin), where the algorithm rewards participants who solve complicated cryptographical puzzles in order to validate transactions and create new blocks (i.e. mining), in PoS-based cryptocurrencies the creator of the next block is chosen in a deterministic (pseudo-random) way, and the chance that an account is chosen depends on its wealth (i.e. the stake). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
&gt; The huge amounts of hardware and electricity used to solve useless problems just feels so wasteful. Definitely agree. From a global perspective, mining is contributing nothing to the system; it's producing waste in an attempt to make local wealth shift in your favor. If there's ways to do decentralized cryptocurrency without mining (I was not aware of this), I'm all for it!
He mentioned using Haxl to make a build system. Where can I learn more about that?
There's some code from Simon's Haskell eXchange 2015 talk: https://github.com/simonmar/haskell-eXchange-2015/tree/3ae0e34a051201eb77721bee2e940ec1f764a0df/build
&gt; Remember that the IO monad technically doesn't satisfy the monad laws I didn't actually know that. Could you clarify?
[Nothing is Cheaper than Proof of Work](http://www.truthcoin.info/blog/pow-cheapest/). It is not wasteful, it secures the network. 
What you wrote looks fine. I don't get a parse error.
I'm a bit skeptical about with this "lazy" error reporting behavior. Are you using `Debug.Trace` or `unsafePerformIO`? Outside of those constructs, laziness should not affect IO semantics.
One clear advantage of PoW? The [Satoshi paper](https://bitcoin.org/bitcoin.pdf) is 9 pages long, while [this one](https://eprint.iacr.org/2016/889.pdf) is 54. That's a lot more stuff to go through, and, from a quick browse, looks like there's a fair few more assumptions about prior knowledge to boot. I'm pretty sure I could give somebody a working knowledge of how PoW functions in about as much text as it takes the Cardano team to describe the differences between their implementation and the paper.
Didn't Moscow ML have first-class modules?
I understand nothing about blockchain or economics in general, but let me ask. &gt; The entire process is called mining. Mining is very energy consuming, and the amount of energy needed is constantly increasing, which can lead to unsound competition. &gt; The core idea of proof of stake is that instead of wasting electricity on cracking computationally heavy problems, a node is selected to mint a new block, with a probability proportional to the amount of coins this node has. How is this a sound competition? People, who have more, receive even more, for nothing. And we know who these people are -- those who created the blockchain and retained a big amount of coins for themselves. And isn't it the case that if they've just granted themselves with some coins, then even though I have the same amount of coins I had before, they're are of less value, because the total amount of coins has increased? So the worth of my money essentially was taken by people who already have quite enough money. And since there is now no proof-of-work which prevents you from cheating (you at least have to spend some money to buy equipment and some time and energy to mine coins, and the more coins you mine, the more time and energy you have to spend -- you can't grant yourself with any amount of coins you wish), people who rule the network can invent whatever rules they like and your money are not warranted by anything. Or am I perhaps misunderstanding something?
I understand nothing about blockchain or economics in general, but let me answer. &gt; People, who have more, receive even more, for nothing. It seems that people receive more proportionally to what they have. Therefore each individual's fraction of all existing currency remains the same.
typo on [the slide at 17:07](https://www.youtube.com/watch?v=qZ4O-1VYv4c#t=17m07s) ([slide 21](http://jelv.is/talks/probability-monad/slides.html#/slide-orga17ef8a))- you've got `uniform xs = Dist (zip xs [1..])` but it should be `uniform xs = Dist (zip xs (repeat 1))`
I see where you are coming from and it is indeed a curious behavior when thinking about exceptions as side effects. The thing is that my lookup functions are pure and I had a thunk put into a named datatype reference - there was no reason to evaluate it yet at that point besides of possibly throwing an exception from that pure code using Control.Exception.throw. That thunk was only evaluated when needed, which was much later in the IO monad controlling the compilation at statement level. The code in the monad itself uses Control.Exception.throwIO. I could lift the pure code into the IO monad too and use throwIO there too, but for the semantics I have, it made more sense to have that as normal code that's not in the IO monad. :)
That would probably work if everyone receives coins simultaneously. But if some account with a big number of coins has just created a coin, then it now has a better chance to generate yet another coin than it should actually have to preserve the fairness of the network. At least, wikipedia says that naive proof-of-stake is biased and I guess "coin age based selection" exists for a reason.
While there may be some truth to this, to put it in perspective, there are 16.5M bitcoins and 12.5 are generated each block. So a currency with a similar rate of inflation would give the account that last found a block an extra one in a million chance of finding the next block (though obviously the effect would be much greater at an earlier point when there are fewer coins).
Thanks for answering! Exception-throwing as values is pretty much the equivalent of null references in other languages. The problem is mostly solved by ADTs: use `Either` as the content type of these references instead of hiding the exception with thunks, with pattern-matching instead of `seq` to propagate errors.
Hm right, I shall consider that, maybe with a Maybe, Either, or Except monad. 
I would do something like this: https://gist.github.com/eckyputrady/4cfa4eb0eb8370cb50c7ffaa92d33e2c Basically: * Domain doesn't know about persistence * Persistence knows about domain, but doesn't know Routing * Routing knows about domain, but doesn't know Persistence * Main know all of them and tie them all together If you want to expose the application via command line, you can just create a "CommandLine.hs" that basically parse command line args into domain types. Domain + Persistence need not to change.
I wrote a thing for Emacs: https://gist.github.com/WraithM/732c532a6dc7e6c4b58ec76a549debf6 This makes a lot of assumptions about what's running on the system, and it doesn't work that well, but it's a start.
I'm sure there's a better reference for this, but my googling only turned up a SO answer: https://stackoverflow.com/questions/12617664/a-simple-example-showing-that-io-doesnt-satisfy-the-monad-laws
Any chance you want to send a PR for ghcid? I can imagine it being generally useful!
Oh well I’m not sure there are many monads that don’t have that specific problem =P
What's the motivation for profunctor optics over the van Laarhoven ones? Is it just the simpler formulation?
It looks like it is just an ordinary BSD license, but there is also the included PATENTS file, which contains some additional legalese around patents. It looks like the PATENTS file grants permission for any patents Facebook might have that would restrict the usage of Haxl, but this permission is revoked if you ever use patents offensively against Facebook. I don't think these things have ever been tested in court. I'm not a lawyer, so I'm not sure what all the subtleties might be.
If I remember correctly, in the last GHC release rudimentary unboxed-sums support was released, but it wasn't performing as expected (?) due to missing demand-analysis tweaks. Is there any progress? I haven't yet tried unboxed sums out, but I am super excited for this feature.
On the plus side you get away with one fewer type variable. (You can also merge some combinators that artificially split by the current lens formulation, like `from` and `re`.) On the minus side you lose compatibility with everything just working using the Prelude. They are also a bit more verbose to construct. For a fresh start like the implementation in purescript, its much easier to get away with profunctor optics for everything, which is why I lobbied for them to switch over in the first place. For Haskell its a much harder sell, because a large part of the adoption of lens has to do with the fact that third-party libraries can write lenses without incurring a dependency on the package.
`IO` only violates the monad laws in the presence of `seq`, as many monads do. Hell half of Haskell's various typeclass laws can be violated via `seq`. E.g `bimap id = id` when dealing with tuples. `Haxl` goes quite a bit beyond that.
So to be clear it is strictly more permissive than BSD? Since BSD doesn't give you any implied rights to patents?
What I not get is whether revoking "perpetual, worldwide, royalty-free, non-exclusive, irrevocable (subject to the termination provision below) license under any Necessary Claims, to make, have made, use, sell, offer to sell, import, and otherwise transfer the Software." will also terminate the permissions given by BSD license.
&gt; Why do foldl and foldr behave the same if we use an associative operator like * or + and the same "identity" element (like zero or the empty list) An example: &gt; foldl (+) 0 [1,2,3] evaluates to something like &gt; (((0 + 1) + 2) + 3) and &gt; foldr (+) 0 [1,2,3] to something like &gt; (1 + (2 + (3 + 0))) Since 0 is the identity element, we can eliminte it in both places, ending up with `(1 + 2) + 3` versus `1 + (2 + 3)`. And you can get from one to the other using the associative property of `+`.
1. Because `foldr (+) 0 [1, 2, 3, 4]` expands to `1 + (2 + (3 + (4 + 0))))`, and `foldl (+) 0 [1, 2, 3, 4]` to `(((0 + 1) + 2) + 3) + 4`. By the law of associativity they are the same. 2. Don't forget the `Ord a` constraint in the signature of (&gt;)! `foldr` takes a function of type `a -&gt; b -&gt; b` as argument. Note that the second parameter and return type are the same. So if you give it the function `(&gt;) :: a -&gt; a -&gt; Bool`, it will try to make the second parameter the same as the the return value. So `a` becomes `Bool`, and the whole type becomes `Foldable t =&gt; [Bool] -&gt; t Bool -&gt; [Bool]`
1) foldr (+) 0 [1,2,3] = (1 + (2 + (3 + 0))) = 6 foldl (+) 0 [1,2,3] = (((0 + 1) + 2) + 3) = 6 So it's basically because (+) and (*) are associative. EDIT: thanks /u/dnkndnts So it's not only associativity, but also the commutativity that makes the two expressions evaluate to the same result.
And you guys forgot to mention that we are going to write the next version using liquid haskell and also stated formal verification work using Psi Calculus 
The Second paramter of (&gt;)? But if foldr only makes the second paramter of the function Bool we would've a fuction a -&gt; Bool -&gt; Bool, which would fit the type of foldr and the whole type would become the same as I mentioned above 
2) Because the only way that can work is if we're comparing the order of `Bool`s. Let's say we have a list `[a,b,c]` with an unspecified type. `foldr (&lt;) False [a,b,c]` = `a &lt; (b &lt; (c &lt; False))`. Since `c` is being compared with `False`, and `(&lt;)` can only be used to compare two things of the same type, `c` must be also be a `Bool`. Haskell's lists are homogeneous, so the type of `[a,b,c]` must then be `[Bool]`.
I think your `foldl` should put the `0` on the left. For the zero element, it doesn't change the normalized result, but for any other element, it will, e.g., `foldl (++) "a" ["b","c"] != foldr (++) "a" ["b","c"]`.
The `a` in the second argument of foldr `a -&gt; b -&gt; b`, and the `a` in `(&gt;) :: Ord a =&gt; a -&gt; a -&gt; Bool` don't refer to the same variable! Let's use for the first `b -&gt; c -&gt; c` to be clear. Then we get `c == Bool` and `c == a`, and therefor `a == Bool`. Substitute Bool for a in the second and you get: `Bool -&gt; Bool -&gt; Bool`
I think so. In the worst case, you might use a patent offensively against Facebook, and that would result in all grants in the PATENTS file being revoked. At that point, you would only have the permissions granted in the BSD license, which is all you'd normally have in most open source software anyway. Again, I'm not a lawyer. Also, lest all my talk about patents make people think this is good and normal, I really wish we'd completely do away with software patents, it doesn't have to be this way.
It should work even without commutativity provided 0 is the identity.
That is true, but the 0 is just an example in this case. The second argument of fold is not necessarily the identity of the first argument.
HsSyn is now based on "Trees that grow". Sweet.
For more (dis-)information about this, they just changed the license for React and some other projects recently, which was heavily discussed on hacker news. I had the "this is strictly more permissive" understanding as well, but a lot of people and some big projects lobbied for it to be removed, so I don't really know what to think.
There seem to be some misunderstandings here. I did not mean "correct" when I wrote "any good". Correctness is only the first step.
I'm currently investigating why it leaks. I can reproduce it with ghci only.
Certainly, but neither is the first argument necessarily commutative. My point was precisely that you get equivalence (of output, not how you got there) in either case. (If we have to pick one, it should be identity, given the text of the question, I think...)
I mean that's fair. But for that you can always make it so that periodically you have projects where code quality matters and then just use tests (with many of them hidden or randomly generated) for the rest of the coding problems. That should strike a balance between teaching good coding practice and limiting how many TA's and such you need. Also for later / harder classes code quality just might not be a priority, and the class might be about just understanding and implementing complex algorithms. 
Powered by [Yesod](http://yesodweb.com) and [Zoomin Software](http://zoominsoftware.com). Actually it's been live for a few years, but only available to DellEMC employees and partners. This is the first publicly available release.
don't try to use ~~foldr (&gt;)~~ because it relies on the fact that Bool is, in Haskell, an instance of Ord, which is... technical.
`class Semigroup a =&gt; Monoid a where` Oh, nice! **UPD**: nope, only in 8.4 
&gt; A function such as readFile will not yield the same result every time it is run. Actually, it will. The result of the readFile function is an `IO String`, an IO action which produces a String. This action is a pure value, and you can manipulate it with `fmap`, `&lt;*&gt;`, `&gt;&gt;=`, and so on without forfeiting referential transparency. The impurity occurs when the action is executed by the runtime, but that is outside the scope of the Haskell program. A Haskell program (barring obvious constructs like unsafePerformIO) consists entirely of pure code computing IO actions as input to the impure runtime system. &gt; Actually, composability is guaranteed by monads. You don't need purity in general. On the contrary, the monad laws only hold if `fmap`, `pure`, `&lt;*&gt;`, and `&gt;&gt;=`/`join` are all pure. Without purity you don't have a monad in the first place. Monads are not the apex of composability, either. They are better in that regard than unrestricted impure code, but monads can only be combined in certain ways. Monads are neither inherently distributive nor commutative, and not every monad can be used as a monad transformer. Functors are more composable, as there are at least three ways any two functors can be combined to yield a new functor: sums (`data Sum f g a = InL (f a) | InR (g a)`), products (`data Product f g a = Pair (f a) (g a)`), and composition (`data Compose f g a = Compose (f (g a))`). In all three cases if both of the original functors are applicative then the combination will also be applicative. However, because monads are less composable, the result is not necessarily a monad even if both of the original functors are monads.
I've been using [boxes](https://hackage.haskell.org/package/boxes) for the last couple of years to produce text reports. It works quite well.
[I think this is the ticket so I'd guess not](https://ghc.haskell.org/trac/ghc/ticket/14259). Manual unpack pragmas probably should be fine, though? Not sure how much worker/wrapper for sum types would do without some sort of nested cpr anyway.
 foldr f a [b, c] Is essentially the same as (a `f` (b `f` c)) This means that b `f` c Must have the same type as `c` as both of these are used as the second argument for f. Or put differently: f :: a -&gt; b -&gt; b (For some a and b) If we look at the signature for `&gt;` we can see that the first and second argument must be of the same type (&gt;) :: Ord a =&gt; a -&gt; a -&gt; Bool Combining these pieces of information tells us that all three arguments must be the same. It also tells us that `Bool` has to be an instance of `Ord`. foldr (&gt;) True [False, True] == (True &gt; (False &gt; True))
Looks like we're in a pickle for now: * Generics make exes big * TemplateHaskell makes recompilation slow
Whoa. That does not sound like a `_._.x` level change. I know this change made its way into `master` recently, but I thought it was for 8.4. Was it intentional that it made it into this release?
It seems that in all asset transferral schemes you have to pay with _something_ to get some safety: * In Proof-of-Work based systems like Bitcoin you pay CPU power * In Proof-of-Stake based systems like Cardano you pay for providing guaranteed uptime (or you pay with transaction stalls when you wait for down shareholders) * In centralised or consortium schemes (like VISA) you pay with the risk of being dependent on the goodwill and honesty of a single party or small group * In bank transfers you're paying with your anonymity
Generics are pretty bloatey when they don't inline. aeson has trouble with that at the moment, I've been looking into it recently, but I'm still not sure about whether the fault lies in aeson or in GHC internals, or both. I don't know whether cereal has the same issue. Can you find Generic code in the simplifier output (e.g., look for the `:*:` constructor)? (Does GHCJS have simplifier output?) Do you have large types? Sums or products, or both?
I do have a few large (machine generated) sum types.
Not a comment on the validity of the instances, but I did find it somewhat amusing that your examples used `mapM` (rather than `traverse`/`for`) which historically incurred a `Monad` constraint. :)
Should we tell them about using [`foldMap`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Foldable.html#v:foldMap) if they are working with `Monoids`?
Wow! Somehow I've been ignorant of the monoidal profunctor constraint, its associated laws, and how it solves the problem of characterizing traversals. I always figured there was something like that that would work, but this is the first time I've seen it presented.
/u/edwardkmett are the Monoidal class methods equivalent to the pureP and apP methods you gave me [before](https://r6research.livejournal.com/27476.html?thread=19028#t19028)? I suspect it is in perfect analogy with the lax monoidal functor and applicative functor presentation of the same idea.
There is nothing wrong with `foldr (&gt;)`. `instance Ord Bool` is a well accepted and uncontroversial fact.
I mean `foldl (+) 0` is more concise then `getSum . foldMap Sum`, and isn't `foldMap` going to be slow in such a scenario due to the fact that it is a right fold?
It's the NFP -- Nuking Freeways Proposal! 
One could argue that requiring `Monoidal` is too strong. For example, if I define a class like this: class (Strong p, Choice p) =&gt; Traversing p where traversing :: (forall f. Applicative f =&gt; (a -&gt; f b) -&gt; s -&gt; f t) -&gt; p a b -&gt; p s t You can define `traversing` using `Monoidal`, but not vice versa. In particular, if you have a type newtype Static f p a b = Static (f (p a b)) you have an instance `(Functor p, Traversing p) =&gt; Traversing (Static f p)`, but you would need `(Applicative p, Monoidal p) =&gt; Monoidal (Static f p)`. However, this argument assumes that such generality is useful, which is not clear.
Theorem 4 from [Profunctor Optics: Modular Data Accessors](http://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/poptics.pdf) claims that `forall p. (Strong p, Choice p, Monoidal p) =&gt; p A B -&gt; p S T` and [`S -&gt; FunList A B T`](https://twanvl.nl/blog/haskell/non-regular1) are isomorphic. Your `forall p. (Traversing p) =&gt; p A B -&gt; p S T` is "clearly" also isomorphic to [`S -&gt; FunList A B T`](https://twanvl.nl/blog/haskell/non-regular1). Therefore `forall p. (Strong p, Choice p, Monoidal p) =&gt; p A B -&gt; p S T` must be isomorphic to `forall p. (Traversing p) =&gt; p A B -&gt; p S T`. I guess you mean to say that the while both constraints produce equivalent optics, the constraints themselves are not identical; which I find surprising, but not unbelievable.
Woohoo! Congrats!
I love the box diagrams. They make things nice and clear! Part 1 of 3 could definitely benefit from some more realistic use cases. Lenses are very compelling by themselves. Many programming languages have independently grappled with the concept, and it's clear to most programmers that this represents a fundamental idea that comes up over and over again. Traversal is possible to motivate well. But the remaining optics discussed there are not well-motivated. There's nothing to indicate why I ought to care about prisms or affines. To me, that's a big part of the question of how much to care about the profunctor formulation. Prisms are profunctors even in lens, so they argue for the profunctor solution. But if I only care about lenses and traversals, they at least are quite elegant and much simpler (though still not entirely simple!) using van Laarhoven lenses.
Is this on a GHCJS issue or also a GHC issue?
Excellent idea! I just put up the printed book for sale on pothi.com too for folks from India. - https://pothi.com/pothi/book/bartosz-milewski-category-theory-programmers. The price is the lowest allowable, i.e. printing costs + shipping, all going to Pothi.
So this is what Phil Wadler was hired to work on? https://bitcoinmagazine.com/articles/iohk-launches-cardano-blockchain-ada-now-trading-bittrex/
Is `Monoidal` a thing with a module on Hackage? I have [`ProductProfunctor`](https://hackage.haskell.org/package/product-profunctors-0.8.0.3/docs/Data-Profunctor-Product.html) for the same thing, but it would be good to unify.
I concur. In my case (servant) I had started with handlers living in my monad stack, code littered with `runDb` calls (persistent), throwing (to handle error cases with proper HTTP status) around, just to make it work. Then I refactored it out (mtl-style), and, as it turned out, my handlers were purified of anything that does not belong to the domain. I still have them in the very same module I have my endpoint types declared, but that's just due to 'historical' reasons.
I live 10 miles from Manchester and am currently going through the Haskell book too. Definitely interested in meeting up with like-minded people!
Excellent! Hopefully we can find a day that'll suit everyone.
The Satoshi paper contains no proofs at all; who knows what it's properties are? Well, actually, I know the answer to that one: the same researchers that came up with Ouroboros know :) Also a [44 page paper](https://eprint.iacr.org/2014/765.pdf). The vast bulk of the Ouroboros paper is proofs. You are however absolutely right that this drowns the description of the actual algorithm. It's not however very difficult, and a concise description is certainly possible. In fact, we (I) am working on that as we speak :)
Are you compiling with any optimisations on? If not, have you tried that?
My aeson instances got a lot faster when I switched from Generic-based instances to TH based ones.
1. The base case of induction is no less important than the recursive one. 2. "It's unfair, but just a little bit" doesn't sound too convincing to me. 3. My main concerns are those regarding cheating.
Simon: The applicative-do extension is great for parallelizing monadic code, and applicatives have a weird syntax that non-haskellers would find confusing. But applicatives are an opportunity for creating algebras for the domain problem. In the case of Haxl, when fetching and intersecting data, there is a relational algebra lurking behind the problem that may be made explicit, and the users can find it much more natural than using naked applicative and monadic code: I got the idea from [this presentation of transient](https://docs.google.com/presentation/d/1u8xC-0X9j-zes24j287r-r2GoeWR7FXOVWb6wWn33ZM/edit#slide=id.g1772aa8118_0_0) from @agocorona: "Applicative is an half-cooked algebra" Numeric: mx + my= (+) &lt;$&gt; mx &lt;*&gt; my … Relational: query1 `and` query2= someintersect &lt;$&gt; query1 &lt;*&gt; query2 query1 `or` query2= someconcat &lt;$&gt; query1 &lt;*&gt; query2 Of course, more sophisticated operators can be defined, combined with lambdas to express complex queries. The terms of the operation will be executed in parallel using Haxl, and the user would use a higher level DSL more readable and maintainable (hopefully)
Is the changelog already available? I see people commenting on features I thought were for 8.4, but I'm struggling to see where they got the info.
[removed]
I too had the same question as you a close friend of mine explained it in this way. In POW, you need hardware to become competitive, the more money you have the more hashing power you can buy, and therefore how much you have influences how much money you can make off the network. POS is build around the same idea, now instead of buying hardware with your money, you now use spend your money by holding it. So if you have more money you have the ability to get more money off the network. I guess one can argue that with POS you have the chance to get rewarded for being smart by designing a faster miner or a more energy efficient miner etc etc. But then again, a lot of money and resources will be needed to do this same thing. At the end of the day what I feel is these pos systems make sense since you are doing the same thing without wasting so much power. With climate change being a huge issue and the fact that all this electricity we waste can be used for something else, I support the POS idea more than the POW idea. 
&gt; Semantically, `parseq :: a -&gt; b -&gt; b` is identical I think you mean "denotationally". "Operationally" refers to semantics too! And I think you probably also want "unless `a` is `_|_`".
&gt; Is the changelog already available? [Looks like it is](https://git.haskell.org/ghc.git/blob/HEAD:/libraries/base/changelog.md)
I had a vaguely similar project that I haven't been working on for a while. https://github.com/mrkgnao/ariadne One thing in common is the "rich edges" feature, which I [implement](https://github.com/mrkgnao/ariadne/blob/master/library/ariadne/Ariadne/Models/Path.hs#L71) by making every edge refer to a node which holds information about it. Outside of that, it uses Postgres as a data store and doesn't really do anything with the graph structure. 
Not for 8.2.2 though
I am working with /u/RyanGlScott to implement this
I have been out of Manchester for a couple of years. https://madlab.org.uk/ used to host FP Manchester but can't see it listed anymore. You can try asking on the XP Manchester forum who also meet at Madlab https://groups.google.com/forum/#!forum/xp-manchester, they where a good bunch. Looking at the Madlab site they also host a Scala group where you may find some people interested in purer FP. Unless it's changed from around 2014 there wasn't much of a functional programming scene in Manchester, most of the interest was in Javascript/PHP with the design agencies turning out Wordpress/Drupal sites or Java/.net on the other side.
But that's HEAD, A.K.A 8.4 no?
TIL
&gt; However, in practice, nobody will never notice as the exception would be seen a thrown in my big function `parse this big json and return a me a list of user`. Ah, I see, "people don't like monads" in the sense of "people don't like precisely-tracked effects", not "people don't like their effects to use a common API". Indeed, the difficulty is that the type `Parser (Bool -&gt; User)` promises to determine whether the json is correctly-formatted or not before it produces a `Bool -&gt; User`, whereas the type `Bool -&gt; User` promises to always produce a User without complaining about spurious problems such as json formatting errors. Since you’re saying that in an OO language you would throw the error from the `Bool -&gt; User` function anyway, perhaps you would be interested in a solution which does the same in Haskell? -- | -- &gt;&gt;&gt; fmap ($ False) $ parseMaybe (parseJSON :: Value -&gt; Parser (Bool -&gt; User)) $ object ["name" .= "George", "email" .= "george@example.com"] -- Just (User {name = "George", email = "george@example.com", verified = False}) instance FromJSON (Bool -&gt; User) where parseJSON input = pure $ \b -&gt; let input' = extendedInput b in case parseEither parseJSON input' of Left err -&gt; error err Right u -&gt; u where extendedInput :: Bool -&gt; Value extendedInput b = case input of Object o -&gt; Object $ Object.insert "verified" (toJSON b) o _ -&gt; input If the json is invalid, the error will be raised as an exception instead of a Parser failure. Note that while this is no worse than your OO approach of throwing the parsing exception from the `addValidation` method, Haskellers tend to like precisely-tracked exceptions, so while in an OO language your collaborators may or may not be happy to let you throw a parse exception from a method which doesn’t seem like it should throw one, in Haskell there is no way the above code would make it through code review. &gt; The generic FromJSON instance is actually applicative not monadic, so we should be able to feed it with external value I have no idea what you’re talking about. Are you perhaps noticing that `Parser (Bool -&gt; User)` matches the `f (a -&gt; b`) in Applicative’s `(&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b`?
What does "got faster" mean? Decoding? Encoding? Both? It would be interesting to know more, esp. how much faster, and what optimization settings were used.
Note that GHC.Generics doesn't have to be bad. https://github.com/glguy/generic-traverse demonstrates how you can get GHC.Generics to entirely optimise away. /u/Syrak, this might be relevant to your studies at trying to improve `aeson`'s Generic story. /u/glguy's study here is also just insanely fun, and worth a read regardless. Not convinced? Here's some clickbait: GHC.Generics, free structures, lenses, codensity transformation, compositional data structures, optimisation, type classes. liftBoggle = Boggle . liftPureK . liftApK1 . liftMapK1 . liftApWrap
Sorry, I meant compile-times got faster.
I’m looking for feedback on this little series I’m doing. It’s about using CLaSH to create CPU hardware designs with Haskell. Part 1 covers a simple serial state machine CPU. 
I guess you mean http://zoominsoftware.com/
Nice! Integrating an external database is on the DWT issue tracker. I forked Ariadne and started to check out Edible, the portion that handles the external database ... but it's 16,000 lines of code ...
It was a bit fiddly, but I got it working with the Except monad. :)
Hurray!
Unpack pragmas also don't work, only manual unboxed sums via `(# a | b #)` syntax work currently
And neither does `Reader`, and possibly lots of other ordinary monads, once you consider ⊥, ⊥ &gt;&gt;= return = \r -&gt; return (⊥ r) r = \r -&gt; const (⊥ r) r = \r -&gt; ⊥ r = \_ -&gt; ⊥ ≠ ⊥
I have a question How is this "newtype Ptr = Ptr (Unsigned 64) deriving (Show)" a valid haskell syntax? How does it work?
There's been a bit of interest, and for a study group I guess small is fine :) I've been pointed in the direction of a few meetup groups with similar interests so that's helping to get the word out, thanks! 
Thanks for the corrections! ;)
I think this is neat. It reminds me of a assignment we had at university's functional programming course and I was never able to solve it. It's facinating to see it implemented so eloquently.
64 in this case is a value promoted to type level: http://hackage.haskell.org/package/clash-prelude-0.11.2/docs/CLaSH-Sized-Unsigned.html#t:Unsigned using `GHC.TypeLits`
Fixed, thanks!
Awesome, thanks!
Generics can be fast when inlining occurs and the optimizer gets a wack at it. Unsurprisingly, they are much slower than typical hand-written code if not: https://michaeldadams.org/papers/tyb/tyb-2012-haskell.pdf
NP! for future reference you can use Hayoo http://hayoo.fh-wedel.de/ (just like I did now) to look up types you don't know . Very handy..
I know 1ML does / don't recall if Moscow ML does.
This is awesome! Looking forward to part 2.
Hello. I'm using Haskell in the Atom text editor on Debian, and I'm quite new to all three of those :) I'm trying to follow along Derek Banas' Haskell tutorial on YouTube and I have the following simple program that he has written: import System.IO main = do putStrLn "What's your name?" name &lt;- getLine putStrLn("Hello, " ++ name) When I run it in REPL, it prints the "What's your name?" and then the environment becomes unresponsive. I type in some text but nothing happens. I can interrupt the current computation to get control back, but I can't get it to make it all the way through to printing "Hello, name". Any suggestions?? Thanks.
I don't know much about ghc-internals, but I would image the pragmas to be the least amount of work.
well, yes but it also consumes enormous amounts of electricity. Since there are other ways, like PoS, that achieve the same level of security without consuming so much electricity, than PoW is wasteful compared to the alternatives. Now, if the Ouroboros algorithm's proof of security is actually correct, that's HUGE!
Those should be equivalent in a cartesian setting. &gt; I suspect it is in perfect analogy with the lax monoidal functor and applicative functor presentation of the same idea. It is. =)
And as the folks who participate in the economy with smaller pots exchange their assets, spreading the number of stakeholders, the individuals who come out ahead and aggregate stake have an inherent advantage over those that do not. So, essentially, the winning move with this mechanism is to get a large chunk of stake and sit on it. That's not really a great attribute for a currency to have.
You can test the number of sparks queued on the current execution context. -- Marlow gave me `numSparks` a few years back for [`speculation`](http://hackage.haskell.org/package/speculation), so I could implement [`specSTM`](https://github.com/ekmett/speculation/blob/354a3accdb1eebc3a8c99e1c5e3f58ebad8104a2/src/Control/Concurrent/Speculation.hs#L143). So you should be able to write parseq with a little bit of (unsafe) ST code locally that basically just does the lookup you are worried about in #2. Writing it blind (I don't have a compiler handy at the moment): parseq :: a -&gt; b -&gt; b parseq a0 b0 = runST $ go a0 b0 &lt;$&gt; unsafeIOToST numSparks where go a b 0 = par a b go a b n = seq a b You can replace the 0 test in 'go' with any sort of thresholding you want -- in `speculation` I allowed you to accumulate a number of sparks equal to the total number of capabilities before I cut you off.
I spent some time looking at it. The benefits are not clearly articulated other than "done by academics and peer-reviewed". I was surprised seeing on coinmarketcap.com the marketcap of Cardano (ADA) of $564,027,863 (16. position). Then I dug out that the value comes basically from Bitcoin contributed by people in ICO. Of course, the original value of the BTC people contributed was much smaller because the value of BTC was smaller at the time. Not sure what happened to that BTC received in exchange for ADA. There is the "genesis block" but it would be good to see exactly how did ADA came to be and where does it's value come from. Also, no binary download for Linux wallet but that shouldn't be a problem for Haskellers :-)
That's surprising. On the face of it `p a a` and `b -&gt; p a b` (or equivalently `p () ()`) seem like quite different things.
Thank! I'll try that. My worry is that if parseq isn't tied closely enough to the metal, it may have too much overhead to sprinkle it across the code when 99% of the time it would just turn into a simple seq/pseq. Also thank you for confirming my recent realization that "Edward Kmett" is an actual person. It is listed as author on so many useful hackage packages that for a while I thought it must be either a default value for authors too lazy to fill in their own name or perhaps a collective of expert Haskell hackers ala Bourbaki.
&gt; "Edward Kmett" is an actual person You can see the man himself in this video in which he demonstrates his l33t vim skillz https://www.youtube.com/watch?v=T88TDS7L5DY
I think the version I gave /u/roconnor back in the day was busted. (The downside of writing it in a thread.) The former part is easy as having `p a a` is the same as offering `p () ()` if you have `first'`. `p () ()` becomes `p (a,()) p (a,())` under `first` then you can put on and take off the `()` with lambda from the monoidal category. The second part of what I gave him there isn't quite what we wound up with when we started using this stuff in earnest, though, so I suspect I made a mistake there.
The encoding in `profunctors` is https://github.com/ekmett/profunctors/blob/master/src/Data/Profunctor/Traversing.hs#L105 I suppose I should offer the combinators from Monoidal as extra combinators in that module if nothing else.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/profunctors/.../**Traversing.hs#L105** (master → 48bd532)](https://github.com/ekmett/profunctors/blob/48bd5329b265c2b4d584a46ab5f486d8309d569a/src/Data/Profunctor/Traversing.hs#L105) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dnuuf15.)^.
This should produce code that is pretty close to the metal. The `numSparks` primOp is just grabbing an Int out of the current HEC. That is literally doing a single `mov` instruction. All the ST and unsafeIOToST overhead is just symbol pushing in the compiler. The only overhead here is that the `numSparks` primop might not inline so there may be a call that gets an Int# out -- i don't know if the compiler has been taught enough to inline it at a c-- level. Everything else is basically free.
In absence of other information, this link might be useful. It's the commits that occurred between 8.2.1 and this RC assuming the tags are correct. https://github.com/ghc/ghc/compare/ghc-8.2.1-release...ghc-8.2.2-rc1?expand=1
Using Haskell (and Clash) to make circuits is something I've been wanting to do for quite a while now, thanks for this! &gt; Then, for Halt, we have to go through LoadingInstruction, ExecutingInstruction, and then finally to Halt. So this should take a total of 3 _instructions_. &gt; &gt; The total, then, is 17 _instructions_ for this program to execute. On the 17th _instruction_, the halt bit should be True, and it should stay that way. I assume these are supposed to be cycle(s)?
&gt; Was it intentional that it made it into this release? [It didn't](https://downloads.haskell.org/~ghc/8.2.2-rc1/docs/html/libraries/base-4.10.0.0/Data-Monoid.html#t:Monoid).
This seems pretty cool! We use CλaSH at the company I work for, and I've been having great fun using it.
Are you familiar with radix sort? That's the first step along the way. 
the new base already got GHC.Generics.packageName and COMPLETE pragmas? brb gtg update some packages. 
Yup. Or, you could use mutual recursion between events and behaviors: let vehicleE :: Event Vehicle vehicleE = (\v i -&gt; f i v) &lt;$&gt; vehicleB &lt;@&gt; inputE vehicleB :: Behavior Vehicle &lt;- stepper vehicle vehicleE (Note `vehicleE` and `vehicleB` are defined in terms of each other)
EDIT: Just read [this](https://www.reddit.com/r/haskell/comments/742b55/discrimination_package_sorts_in_linear_time_but/dnvhl3d/) comment.
Nice :) Edit: that is so cool
i recently came across [monad-journal](https://hackage.haskell.org/package/monad-journal-0.7.2) which reminded me of this post. personally i use `monad-logger`, which you could definitely leverage with `monad-journal`.
&gt; I guess you mean to say that the while both constraints produce equivalent optics, the constraints themselves are not identical; which I find surprising, but not unbelievable. That is indeed what I mean. There is a similar situation when defining profunctor lenses, where you could use: type SLens s t a b = forall p. Strong p =&gt; p a b -&gt; p s t type RLens s t a b = forall p. Representable p =&gt; p a b -&gt; p s t It happens that `s -&gt; Store a b t` is isomorphic to both `SLens s t a b` and `RLens s t a b`, even though `Representable` is a stronger constraint. (Arrows, for example, are strong but not generally representable.) On the other hand, there are some lenses that can be defined naturally with `RLens` that can't be defined in `SLens` without converting via `Store`.
There is an instance for `String`. Both of the following instances exist: instance Grouping Char instance Grouping a =&gt; Grouping [a]
Yeah, it works because it's actually an American flag sort, not a radix sort. But radix sort is much better documented online, and still captures the correct idea. 
You're hitting return?
Ah ok, edited. Now that is not going to be linear time with respect to the number of lists / strings, but I think it will be linear time or better with respect to the input size.
I'm hitting shift+enter in REPL since that submits a command (enter just makes another line). The text I've written disappears but nothing happens, like it's processing something else. After I interrupt, everything I typed appears but nothing was done with the input.
Just to clarify, I'm not saying I WOULD write and use my OO version. I just meant that my intuition was that this would work in OO and what I had in mind, translate indeed to the exception thrown at the wrong time, but I didn't realized until I actually wrote it (my brain is not a good type checker, which is why I need the help of computer for this task). Also, I don't need this feature. I'm just interested on the theoretical aspect of it. Are we able to write one instance reusing the more general one. My intuition was we could but you brilliantly demonstrated that we can't. The next question is then, + are we happy with this result : computer says no and its right because there is moral reason to do so + or do we think it should be possible and the typechecker needs to be relaxed somehow. Afterall, some haskell extensions (like FlexibleParameters) started because somethings didn't typechecked but someone wanted them too. What still make me think it should be possible is, that your argument is based on the fact we can create a `Maybe (Bool -&gt; User)` from a `Bool -&gt; Maybe User`, because we don't know until we inject the bool to return a Nothing or not. However, in our case, `Bool -&gt; Maybe User` **doesn't the use value of the bool** to decide it. That is what I mean when I say the parser is applicative but not monadic. When you write something like readUser :: e -&gt; Maybe User readUser e = User &lt;$&gt; getName e &lt;*&gt; getEmail e &lt;*&gt; getVerified e You know that even though you can that it can fail if one of `getName e`, `getEmail e`, or `getVerified e` fails, but it won't fail if none of them fail and the failures are all indenpendent of each other. Therefore transforming `e` to force a valid `getVerified e` won't change the failure status. I could write something like readVerifiedUser :: e -&gt; Maybe (Bool -&gt; User) readVerifiedUser e = let e' = &lt;modify e to force getVerified to return `Just undefined` instead of Nothing in case readUser e' of Nothing -&gt; Nothing Just u -&gt; Just ( \b -&gt; u { verified = b } This is equivalent to your first example using an exampleValue. This code is ugly but only because we can NOT prove it. In fact we can prove it manually, but the typechecker can't verify it. We know this code works because `readUser` has a `&lt;$&gt; ... &lt;*&gt; ... &lt;*&gt;` shape but we can't stop someone to modify it do readUser e = do name &lt;- getName e email &lt;- getEmail e verified &lt;- getVerified e if name != show verified then Nothing else return (User name email verified) which will break our `readVerifiedUser` function. I feel we are just a missing a way to tell the typechecker that all failure are independents of each others (and a way to use it). 
How about memory footprint then? Can it do in-place sorting since it's similar to AFS?
Whoops, good catch! Will fix when I get home, thanks! 
I mean the input list is immutable, so `O(n)` auxiliary space for sure.
 *Main&gt; main What's your name? tom-md&lt;enter&gt; Hello, tom-md Works for me using `[enter]`. I'm not clear on why you are using `shift+enter` - could you elaborate? What does to mean to "submit a command"? Is that specific to your terminal? Are you on an unusual platform?
Sweet, what’s the company? I talked about doing some contract work with a company that was using CLaSH for rendering and robotics stuff. Sounded super cool but I ended up not having the time, sadly. Glad to see that it’s being used in the wild. 
I'm not sure what else you would call it. Giving input to the program? For example, I can define the following: factorial 1 = 1 factorial n = n * factorial (n-1) and when I type "factorial 4" and hit shift+enter it outputs 24. I'm running Atom on a Debian operating system, but am not very familiar with it. Edit: I guess it's something wrong with Atom. When I compile in the terminal with ghc and run the program from the terminal, everything seems to work.
&gt; I suppose I should offer the combinators from Monoidal as extra combinators in that module if nothing else. That would be nice.
[Myrtle Software](https://www.myrtlesoftware.com/)! We're based in Cambridge, UK, and compile neural networks to FPGAs!
Another tool based on ghc-exactprint :)
Ed gave at least one talk about it: https://www.youtube.com/watch?v=cB8DapKQz-I
If there are good defaults, you often see naming along the lines of: `defaultMaximizerOptions :: MaximizerOptions`, `maximizer = maximizerWith defaultMaximizerOptions`
Thank you very much for your work on this! :D
I've been using [hasktags](https://hackage.haskell.org/package/hasktags) for this purpose.
&gt; Does it really work better than say quicksort in practice? I believe that is very much dependent on the size of the list being sorted.
Definitely O1 (cabal default), probably O2. Not sure if ghcjs may be failing to optimize in some situations though.
Its a GHCJS issue, but I wonder if GHC might be similarly affected. My many constructor types also take a very long time to compile with ghc, and the object file size is large (about 14M.) And the compilers are very closely related.
Cool! I found another related talk with such a bad audio quality. This one sounds much better. Thanks.
Read the article I linked. It also goes into PoS. PoS is also not as secure as PoW. For example, you can't trustlessly bootstrap from zero.
I just realized that I should verify cabal's handling of optimization flags for ghcjs.
Right, but what if the input is mutable? Is that essentially an in-place sorting algorithm?
What is WAI and Warp and is there any difference between the two?
this is the same for comparing sort algorithms though. for strings it's O(kn log n)
Not a lawyer, but, my understanding: Less, sort of. BSD grants an implied patent license. By including separate documentation, Facebook grants an explicit patent license. Because they explicitly grant the patent license, it is theirs to revoke (at least, so they claim in said document). They key point of contention is that Facebook's language is non-specific - They revoke -all- rights to any patents granted under these terms that you have if you pursue patent litigation against them for any reason, not just for reasons related to the software in question. So if Facebook decides to rip off your grand idea , and you pursue legal action against them, they drink your milkshake on any and all patent claims you may currently hold that are using libraries with these terms. I don't think that's really the intent of the patents.md file - but, that's what the community was objecting to, at least. 
&gt;Why do foldl and foldr behave the same if we use an associative operator like * or + and the same "identity" element (like zero or the empty list), for example: What matters here is that it be *commutative* not associative. This just means the operation works the same left-to-right or right-to-left. &gt;I don't understand how the type of the fold functions behave if we use functions that don't have exactly the same type as the type of the function in type signature of fold. Note that the signature of `foldr` is in fact Foldable (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b rather than Foldable (a -&gt; a -&gt; b) -&gt; b -&gt; t a -&gt; b In fact, the latter would not work! Looking at the type of `(&lt;)`, viz. (&lt;) :: (Ord a) =&gt; a -&gt; a -&gt; Bool we see that for `(a -&gt; a -&gt; Bool)` to match `(a -&gt; b -&gt; b)` we must have that both `a` and `b` are of type `Bool`. 
That's a fair point. It definitely does seem like a really cool library, it would be great if there were some benchmarks.
I mean you'd need totally different type signatures, as for whether the underlying algorithm can be remade so that it is done in place, I'm not sure.
## The Good There are a couple of bounds involved here that may be informative: If we define LCP(S) to be the length of the longest common prefixes in a set of strings, then every single sort must pay at least Ω(LCP(S)) as a lower bound, otherwise you could swap around stuff that it didn't visit and it'd give a wrong answer. Next, when you look at string sorts, not sorts where we presume we can compare keys in O(1), but rather where the size of the key becomes an issue, there are symbolwise comparison sorts like multikey quicksort, multikey mergesort, multikey heapsort. Every symbolwise comparison sort must also hit a lower bound of Ω(n log n). Combined you get a bound for symbolwise comparison sorts which is met by multikey mergesort and multikey heapsort: Θ(LCP(S) + n log n). Note: this bound can be considerably smaller than |S|, "the size of the input" which has been bandied about elsewhere in this thread. On the other hand, radix sorts pay an Ω(n log σ) lower bound where σ is the number of buckets used, _not_ the word size -- e.g. you can fix σ = 2 if you like. But this bound isn't attainable. We determined above that we still have to pay at least LCP(S). So the best we can hope to do is Θ(LCP(S) + n log σ) and this is the actual bound obtained by discrimination. Note, this is _not_ the same as the radix sort you probably know and love. The usual radix sort starts at the least-significant digits and works to the most significant digits. This replaces the LCP(S) with |S|, "the size of the input" which can be considerably larger! Normally, what happens is folks try to mash these two contributing bounds together into one bound and to do so σ typically gets replaced with a word-size, but if we carefully separate out the LCP(S) bound, then σ remains merely the number of buckets used in each stage and we can tease apart the difference. So to summarize: * Θ(LCP(S) + n log n) is the bound for multikey mergesort/multikey heapsort * Θ(|S| + n log σ) is the time for classic bottom up radix sort. * Θ(LCP(S) + n log σ) is the bound for discrimination/top down radix sort and provides the best combination of both of those complexities. ## The Bad MSD radix sort also uses more intermediate space than a LSD radix sort (because it has a stack depth based on the length of the longest common prefix.) Another downside of the current discrimination design is that it isn't cache optimal. It has good asymptotics on a RAM machine, but we don't use RAM machines in practice. We have caches all over the place; some memory is cheaper than other memory. ## The Ugly -- Er.. The Not Yet Implemented To get better locality it really should have its guts replaced with something closer to a [burstsort](https://en.wikipedia.org/wiki/Burstsort). That is to say, it should build a trie, but instead of going all the way to the leaves it should bag up a few thousand keys until the node gets too full, then 'burst' to distribute the keys down lower in the trie. Using a burst-sort style trie would fix the bad cache locality of MSD radix sorting. Also there are some constant factors that can be thrown open, like replacing the machinery used in the productive stable unordered discrimination code for hash-mapping with a flat growable linear probing hashtable with a simple tabulation hash. This would require building a library for simple tabulation hashing, though. Hashable isn't good enough.
Technically it is an out-of-place MSD radix sort. The classical American flag sort is also an MSD radix sort, but while it is in-place, it is un-stable.
Yeah, that would be super useful for the type `64`.
I’m wondering how it deals with definitions generated by TH (it probably doesn’t?) and those hidden behind CPP macros (probably also doesn’t?).
(somebody please correct me if I'm wrong) WAI is a spec/api and Warp is an implementation. I'm not sure if Haskell has any other WAI implementation. In the python world, wsgi would be the equivalent of wai and flask/pyramid/etc would be the equivalent of Warp. 
You can search for Unsigned, and you can see the KnownNat constraint on n in most of the instances, which will take you in the right direction. 
If you want to test GHC 8.2.2 RC1 with Stack, use this `stack.yaml`: https://gist.github.com/tfausak/640a1b27252bfce33f1f2229d39781f5
What is Haskell good for?
Sorry, I meant composing then with `&gt;=&gt;`, which satisfies your requirement, doesn't it? I think the key observation is to avoid monomorphising your Monad (stack) until the last moment (which can even allow you to dodge the n² problem.)
Can this parse incomplete/syntactically invalid files? (e.g. WIP modules with some working definitions but not complete) EDIT: Just tried it, it doesn't. I'm wondering if this could be improved. Is this a limitation of `ghc-exactprint`?
Edible is just forks of `tisch`, `opaleye`, and `postgres-simple`, so if you're familiar with those then you don't need to look inside. I forked them because there was lots of API-wrapping and hiding going on so I decided I could just "inline" a lot of code (and I did). There aren't any new features there (yet).
ghc-exactprint works on the output of the GHC parser, and so needs that to succeed at a minimum.
Shouldn't this use Text.Regex.PCRE.Light? I think that might help your throughput.
Except that wrt TH on GHCJS, you're shafted! I hope this is not a consumer facing app which is going to have a negative impact due to JS file-size. Do share your solution, once you find it.
Template Haskell makes recompilation more ugly, but complexity is linear with respect to number of fields. Generics give complexity worse than quadratic.
But that is not what I want. In your example, both terms should have the same type and therefore, should execute the same effects. Imagine that I have two databases and I want o perform a combined query: query &lt;- combine &lt;$&gt; queryDB1 &lt;*&gt; queryDB2 DB1 has been developed by company A. DB2 developed by company B. And both uses completely different effects since they use different architectures and different stack and so on.
&gt; Θ(LCP(S) + n log σ) The naive interpretation of your bound is that it's measured in word operations, with LCP(S) also measured in words, and with the constant factor in Θ independent of word size. But then it seems wrong, because you could take σ=2, feed your algorithm single-character strings (so LCP(S) is at most n), and end up with O(n) sorting of word-sized integers, with the constant factor independent of word size. That's faster than anything ever published, even if all word operations take constant time, so it's probably not what you meant. (Note that the Wikipedia page on [integer sorting](https://en.wikipedia.org/wiki/Integer_sorting) doesn't give any algorithm that's O(n) unconditionally, in particular radix sort doesn't achieve that.) Maybe you meant to measure LCP(S) in bits, or in log σ sized chunks?
https://github.com/Gabriel439/post-rfc/blob/master/sotu.md
It may be worth having a line by line version if parsing fails
I also wrote a processor based on the Sprockell example of CLaSH and had it run on a DE-II FPGA. https://github.com/Alaya-in-Matrix/CLaSH-Processor