To be fair, if GHC and friends had been monetized all these years none of this would matter because Haskell would have died and faded away before Caesar charged the Taj Mahal in 1812. 
Everybody reading this should take some time this weekend to contribute to the Haskell project or Haskell infrastructure that you feel is most instrumental to Haskell's success.
Right, length compiles down to code that expects to see a two-argument constructor (:) or a no-argument constructor [], and doesn't care about the first arguments. A BashList is a totally different type with two one-argument constructors, whose values happen to be lists. If you need to convince yourself that the runtime representations of [] and BashList are not compatible, try it with unsafeCoerce# Prelude&gt; data BashVar = BashString String | BashArray [String] Prelude&gt; import GHC.Prim Prelude GHC.Prim&gt; :set -XMagicHash Prelude GHC.Prim&gt; length (unsafeCoerce# (BashString "hello")) 0 Prelude GHC.Prim&gt; length (unsafeCoerce# (BashList [BashString "hello"])) 0
Please stop the bullshit. You obviously believe that you and your partners can get a return on your investment of time and material. But the other shit about FOSS components, the serve your needs for now. Once they don't, you'll change your behavior like any other rational corporation. There's absolutely nothing wrong with that. But to say otherwise is, well, bullshit.
Btw i'll buy your IDE in a heartbeat...when you release a local version. My company is not going to put its own source code on someone else's cloud. We use a lot of proprietary software for development, including such auxiliary products like jira, confluence, SmartGit etc. 
That doesn't really answer the question, though, since he's asking for a conditional length on a non-parameterized sum type.
But which laws? Also, what happens when we concatenate a BashString to a BashArray?
I left a comment on the gist.
Shows errors and warnings by colorizing the lines. Massive improvement IMHO
Yeah, or better yet, have an interrupt mechanism that gives up on evaluating the thunk if it's taking longer than some threshold value.
sure, but derive is also able to make functions compatible with both inner types work on the newly defined data type, so it’s not difficult to dream up a scenario where we can tell the compiler “BashVar contains a generic list in any case, so make functions which don’t care about the list type work on them”. or make all list functions work on them and make the return type of methods ad-hoc polymorphic, e.g. the resulting list when concatenating two bashvars is only of type `(Eq a, Show a) =&gt; [a]`
Which laws? Also, the type you wrote probably doesn't mean what you think it means. The only "value of that type" (using it loosely here, technically it is not a type) would be an empty list that can be specialized to a list of any showable eq-able type. Essentially, there's no clear way to define a `ListLike` typeclass, because it's not clear what would be wanted in general.
&gt; I'm not worried about people writing proprietary software in Haskell. But I'm worried that if FP Complete becomes the Haskell editor, then more Haskell tooling will begin to eschew the FOSS philosophy. ...what might be adding to that worrying is if FP Complete starts to hire away key developers contributing to competing IDE-like tools.
Are you certain? If you write FOSS, then you are not responsible for the people or companies that use it, no? I think it mostly matters that you do not write this on your employers time (which can be verified somewhat by checking the commit timestamps (though this does not stop you from only commiting after hours -- even if that would not be the smart thing to do)). 
I think the way to benefit from FOSS tools and software is to offer services. There are plenty of companies that did this, though they might mostly be playing in a different league compared to a programming language ecosystem. I'm thinking Drupal and the numerous shops that build websites. I'm thinking Acquia, they provide professional Drupal services, hosting, and whatnot, but they do contribute back to the community. I do not see why this could not be the case for FP Complete. One example could be a tool (I've no idea if it exists) that keeps track of the versions of the libs you are using in your application/library and that keeps running tests to make sure that a range of versions still works with your product as opposed to the single versions in the FP Complete toolchain. IMO, that might be a cool tool and useful for the community. It would mean that if you are in need of a latest feature from some library, the tool would resolve the different versions of the other libraries that are forming your toolchain such that they all play nicely together and together with the compiler. It would avoid people having to check, look up and try different setsm even if cabal helps out quite a bit there.
Although chrisdoner already replied regarding the terms of use, I'd like to join him in correcting this incorrect comment about the terms of use. In fact perhaps you would consider editing your comment as it is profoundly misleading. Code you write and keep in your own private space is not ours. We're not crazy people. You are thinking of the terms of use for *content that people choose to publish on our site.* Meanwhile (if I may informally summarize the sense of the license agreement) private is private, and open source is open source.
Great. We plan to release a local version several months from now. We decided to do the SaaS version first, since it's the easiest to update and works across many different client types.
Obviously you are free to be as skeptical as you choose, though your tone is frankly uncalled for and I wish you would moderate it. Consider that it is in our rational self interest to be good citizens of the FOSS ecosystem on which we and all of Haskell depend. I cannot see that changing, nor how you can imagine it would. 
Actually, 1. It's not about monetization, it's about social contracts. The purpose of money is to allow people to work together and to divide labor. Money is a social contract -- it's an inherently useless piece of paper that people nevertheless agreed to trade goods or work for. The open source community also has a mechanism for diving labour, but the social contract is different: my code is open to you, but then you agree to make your code open as well. It works differently from money, but the purpose is the same. 2. FP Complete *is* in the business of monetizing open source projects, whether they want it or not. The drawback of the open source contract is that it is not supply/demand. People would like to have an IDE, but nobody feels the urge to supply one. Another drawback is that the contract only works in the open source world: you can't go to a baker and get bread for your code, he will be more keen on the money thing. The drawback of money is that it is supply/demand. I am free to write open source code that no one demands (my poorly-made asteroids-mario hyrbid), but I can still use source code that is supplied already and that everybody demands (for example GHC). Not so with money: if no one gives you money for your code, you can't buy the code you want, even if it has already been supplied. You suddenly have to think about whether the code you write is in demand. FP Complete is trying to bridge both social contracts: they supply the demand for an IDE, but they can't do it by the open source contract, so they use money instead. However, trying to bridge both contracts has several implications: 1. The open source contract relies on people passing it on. If you're not perceived to fulfill your end of the bargain, people are going to be miffed, and that's exactly what you can see in this thread. The main problem with this is that this can potentially affect the open source contract *itself* negatively, a sentiment that for example [Neil expresses](http://www.reddit.com/r/haskell/comments/1mcj19/fp_complete_to_release_a_personal_edition_of/cc835l5). For instance, I am suddenly thinking about making some of my libraries GPL (to enforce the contract). 2. FP Complete has to spend a considerable amount of work on figuring out what the demand exactly is -- demand in the sense that people give money for that. They *are* in the business of figuring out how to make money in an open source ecosystem. On the flip side, nobody really cares if they fail, this issue has become their sole responsibilty. I can volunteer to contribute to an open source project because I care about it and don't want to see it fail, but I'm not going to give money to FP Complete when I don't have a demand for their products -- even if I would otherwise contribute to their code if it were open source. 
&gt; we actually chose it after getting direct feedback from a lot of people telling us what we should charge for this edition. Do you have experience with these kind of statistics? You'll find that people are a bit generous when it comes to predicting how much they'll pay. Especially with luxe products they're enthusiastic about.
&gt; Although chrisdoner already replied regarding the terms of use, I'd like to join him in correcting this incorrect comment about the terms of use. Thanks for the clarification, but could you simply make the terms of use more clear? I have been confused throughouly by the fact that in addition to the [terms of use][2], there is now also a [license agreement][1] and it is not clear to me what applies to which (the product to which the contract applies is not named) and why they are in two different places. I think that eliminating the "License" link on the footer making the "Terms of Use" link on the footer point to a landing page with two links "FP School of Haskell Terms of Use" and "FP Haskell Center License" would help a lot. [1]: https://www.fpcomplete.com/business/fp-haskell-center/license-agreement/ [2]: https://www.fpcomplete.com/business/fp-haskell-center/terms-of-use/
If you have a record that has all fields accessible by lenses, using the MonadReader instance with your record as the environment can ease the pain, as you can then just use 'view' and you don't have to thread the parameter around. Unfortunately, you still have to 'view' everything though, and you have to work in a monad/applicative functor.
At one point 12+ million people were paying $15 a month for a game, World of Warcraft. Many more pay similar amounts for other games. I think $10 a month for a hobby is not prohibitive or even that much. If anyone does not want to invest $10 a month in a hobby then they probably should focus more on their day job so they can afford such "luxuries". BTW - I am in no way affiliated with FP Complete. I just feel like this is being blown way out of proportion. What they have here is a very nice tool, and charging for it is very reasonable.
I would also prefer a split along open source (presumably not-for-profit) versus closed source (presumably for-profit) lines, with the former gratis. Similar to what Trolltech did with Qt before Nokia made it LGPL (though not the same, because Qt is a library and FP Haskell Center is an IDEaaS). Then the implied contract is "either you give back by giving everyone your code, or you give back by giving us money to improve the product", which I think is agreeable. (As far as I'm concerned, they could also restrict "open source" to "LGPL or BSD", to avoid the ironic situation where someone uses FP Complete's IDE to write GPL code which FP Complete can't use for their IDE.) Of course, I have no idea about the business side of things. And my personal interest contributes to my preference: an open source edition is something I might use, whereas I'm unlikely to pay for the personal edition. But it does also feel more fair. In the current system, someone who develops Haskell tools or libraries which FP Complete relies on to be able to provide their services, would have to pay FP Complete money for the privilege of using those services to further develop the tools or libraries, which feels a little perverse. (To enforce the open source requirement, they could simply require choosing an open license at the outset, and then force the code to be public. After that whether they also push/pull to/from closed repositories is no longer relevant.)
I think we need an all-internet meeting about the meaning of the word "robust."
&gt; This could be said of any commercial product, without using a paid-for IDE, so I don't know how it's relevant. Ah, but my take is not on commercial products in general. I have no issue with FP Complete selling something, for instance an IDE. I have no issue with FOSS being used to make commercial products either. What I am not fond of is the idea of paying *in order* to develop FOSS software. (Of course, nobody forces me to pay anything, I don't have to buy the IDE. And I think a one-time fee would be perfectly fine. But I appreciate services and products like [GitHub][4], [Travis][5] or [SmartGit][6] that pass on the FOSS spirit and provide resources specifically for shared development.) I feel that the general understanding the Haskell community is that libraries and development tools should be as free as possible in order to facilitate sharing and use, both for commercial and for non-commercial activities. I appreciate that and chose to continue the tradition and license my libraries under BSD as well. But just because the social understanding is not legally enforceable does not mean that it's not there. (Though implicit social understandings are prone to -- heh, misunderstandings, perhaps on my part.) My understanding is that I get to use all these cool development tools like Haskell itself for free, so I pass it on. ----- Concerning the School of Haskell: the [Haskell wikibook][1] is much older and just as good. I have made substantial contributions to it, but I have no intention to write for the Haskell School, because it does not use the same social contract as Wikibooks -- Wikibooks only wants to be granted a creative commons license and encourages sharing, whereas FP Complete reserves the right to reuse my writing in any way they like. Cool for them, but not my cup of tea. [1]: http://en.wikibooks.org/wiki/Haskell [4]: https://github.com/ [5]: https://travis-ci.org/ [6]: http://www.syntevo.com/smartgithg/ 
I assume it should just work if you use the color values from the preference store. Like, e.g., [the java editor does](http://grepcode.com/file/repository.grepcode.com/java/eclipse.org/3.4.2/org.eclipse.jdt/ui/3.4.2/org/eclipse/jdt/internal/ui/javaeditor/JavaSourceViewer.java#JavaSourceViewer.initializeViewerColors%28%29). [More pertinent link.](http://grepcode.com/file/repository.grepcode.com/java/eclipse.org/3.4.2/org.eclipse.jdt/ui/3.4.2/org/eclipse/jdt/internal/ui/javaeditor/SemanticHighlighting.java)
Check out FP Complete's tutorials: https://www.fpcomplete.com/school/ide-tutorials/buiding-a-file-hosting-service-in-yesod
Go to Citrix in the Science Park. And there's of course also Microsoft Research, where the Simons write ghc itself.
Not true. Rational /= optimizing profits. It depends on the goals of the company.
This is a luxury problem. If fpcomplete hires all good developers then it must have a large market for other haskell developers. I.e. highly unlikely scenario *long term*.
Well they're things we never _sought_ to monetize, so there's a difference!
$10 a month is very little, honestly. The monthly cost for a one-year subscription to The Economist, for example, is roughly 30% more. Its certainly cheaper than a data plan for a smartphone, as another example. I'm pretty happy with my current editor setup. But if I found FP Complete's approach worthwhile, price would be the last thing I'd consider an obstacle here.
I’m interested to know this as well. Implementing a concatenative language with local variables, I looked into abstraction elimination algorithms and ultimately came to the conclusion that an elegant combinatorial basis is not generally worth the effort of optimising the O(n²) algorithms down to O(n).
Tutorials are cool! But for this project, I'm not looking to learn as much as to just drop and go. Trying to take advantage of the "composability" that Haskell/yesod advertise so much.
So I'm a professional programmer and I have a doctorate in an technical field (physics). I have a good handle on the idioms of functional programming in a run-time typed language. This stuff, however, is very nearly greek to me. I can't even really see how I could develop enough experience with this kind of programming to move from "interested voyeur" to "somewhat competent type-level programmer". If any of you have been in this position, how did you manage? Any other suggestions for how to get a foot in the door of this universe? I almost daily feel the need for stronger type checking in my actual work, but I have no idea how to gradually migrate there. 
&gt;You want to mark every function with an Ord constraint as INLINABLE Are there any other rules of thumb on when to use INLINABLE?
Yes they are. What's the problem?
Pay lots of attention to /u/edwardkmett.
&gt; This stuff, however, is very nearly greek to me. Funny, because your expertise deals with a lot of Greek. ;-) You said you have a good handle on functional programming in a dynamically typed language, but have trouble understanding type-level programming. Advice depends specifically on what you mean by 'type-level programming'. If you just mean you need more experience using types in programming, then just take some algorithm you've done a hundred times in a dynamically typed language, and naively translate it to a statically typed language. Then progressively refine it to enforce more and more static guarantees, ie. adding phantom types, [static array bounds checks](http://lambda-the-ultimate.org/node/1635), etc. If you meant 'polytypic' programming, ie. programming over the structure of types, like generic traversals and the like, then you might already be familiar with it in the form of macros (although polytipsm is more rigourous and narrower in scope than full macros). I think a good way to get started here is to start thinking of how every data structure can be traversed by a polymorphic fold across HLists.
derive is just a collection of ways to automatically write type instances of a few particular type classes. In particular it doesn't do anything you couldn't do by writing code yourself, so what code would you like it to write?
My problem was with: &gt; What happens when you do length [1,2,3] and length "abc" is that length :: [a] -&gt; Int, so when it's applied to a String, the compiler infers that the type we want is [Char] -&gt; Int. ... which I think is a bit more correct than I'd originally read it, but sounded like there was some conversion/inference step involved in transitioning between the two.
&gt; &gt; What I am not fond of is the idea of paying in order to develop FOSS software. &gt; &gt; Yes, but we pay fixed and variable costs all the time in order to write software, in our life, our hardware, our tool selection. Yes, there's always a cost. I should have been more precise, I specifically mean monetary compensation for the the toolchain, infrastructure and basic libraries for the purpose of developing FOSS. For instance, I'm currently working on a library called threepenny-gui which comes with a [couple of examples][1] and it would be very nice to deploy them live on a public server so that other people can try them out without having to download and install anything. FP Haskell Center would be great for that, but am I going to spend $9.99/month out of my own pocket just to do that? No. I think that my library might be a very nice addition to the community, so in this case, I would say that the fact that FP Haskell Center has a price attached to non-commerical use is a small loss, or rather a missed gain for the community. [1]: https://github.com/HeinrichApfelmus/threepenny-gui#examples &gt; &gt; Concerning the School of Haskell: the Haskell wikibook is much older and just as good. &gt; &gt; Does that mean SoH is instantly better in every way ever? No in fact - that denotational semantics article is quite wonderful for example. But I think you're going to have a *very* hard time convincing me that wikibook is "just as good" for any of the tasks I mentioned compared to SoH. Ah, yes, I didn't mean in terms of content written, more in that the wikibook is a very similar service for uploading tutorials. Why don't authors flock to the Wikibook? Well, the FP Haskell Center is currently en vogue, and you can write live Haskell code, but for authors, there is not really much of a difference. (And some content is written by FP Complete employees.) Actually, there is one important difference, namely the license. For instance, I could port the Wikibook article about denotational semantics to the SoH -- I wrote it. But I won't do so because if I write it at my own expense, then I want to be sure that only the community benefits from it. I don't want to give FP Complete a perpetual, sublicensable etc. license -- if they have a use case not covered by creative commons, they can always drop me a line and ask. It's not just this wikibook article, I have other half-finished articles on my hard drive that might be a nice addition but have met a similar fate. Again, these examples are another small missed gain for the community. &gt; &gt; But just because the social understanding is not legally enforceable does not mean that it's not there. &gt; &gt; [..] No, I do not think this is a free ticket to instantly jump over people for trying something different in an area that is - to say the least - quite pathetic in our community. I would like to state again that I have no issue with selling IDEs or making other commercial Haskell offerings, and I very much agree that the open source model has its limits and needs to be complemented by monetary incentives. It's just that, at the moment, I have the impression that FP Complete has a slight tendency to monetize *on* the community, rather than monetize *with the help* of the community. 
Conal wrote three Haskell-related blog posts in the past couple of days. It doesn't seem inappropriate to post them to our Haskell subreddit.
That's a good question, but all this stuff can look pretty greek even if you are familiar with typed programming, and I don't think this particular post is actually to strongly related to types anyway. The particular choice of combinators is motivated by types and math, but once you've made that choice, this post seems to be ordinary equational reasoning that would work in an untyped but pure language. Take the generalized version of the first example apply ∘ (const g △ f) we could define the operators in Scheme as (define (compose f g) (lambda (x) (f (g x)))) (define (apply p) ((car p) (cdr p))) (define (pair f g) (lambda (x) (cons (f x) (g x)))) (define (const g) (lambda (x) g)) Then the example expression simplifies like (compose apply (pair (const g) f)) == (lambda (x) (apply ((pair (const g) f) x))) == (lambda (x) (apply (cons ((const g) x) (f x)))) == (lambda (x) ((car (cons ((const g) x) (f x))) (cdr (cons ((const g) x) (f x))))) == (lambda (x) (((const g) x) (f x))) == (lambda (x) (g (f x))) == (compose g f)
My first suggestion would be to not get intimidated by the kinds of things that dominate this sub-reddit. Remember when you first started learning how to program, how much code could you fit in your head at once? When I began using Haskell, I thought foldl had a complicated type signature. Follow your nose, read the blog posts that keep your interest, and practice some code golf. Sometimes I'm learning new type tricks and they get stuck in my brain. I know I'm over-using them, but I just have to get them out of my system. It's fun.
Hey everyone! I am the author of this package so let me know if you have any questions or comments. :-) The article links an FAQ; there is also [a tutorial](https://github.com/gcross/LogicGrowsOnTrees/blob/master/TUTORIAL.md), a [user's guide](https://github.com/gcross/LogicGrowsOnTrees/blob/master/USERS_GUIDE.md) (which provides a more detailed overview of how the package works), and the [haddock documentation](http://hackage.haskell.org/package/LogicGrowsOnTrees).
Yeah, that would be simpler: length = Data.Foldable.foldl' (\x _ -&gt; x + 1) 0
It's about the same as unary debruijn notation. [| Env, x |- x |] -&gt; exr [| Env, y |- x |] -&gt; [| Env |- x |] o exl [| Env |- f g |] -&gt; apply o pair [| Env |- f |] [| Env |- g |] [| Env |- lam x . b |] -&gt; curry [| Env, x |- b |]
in reality, my comment had little to do with your article, so hopefully no offense was taken. basically, the comment that started all this was one i don't like seeing in any field, and that's a feeling of incompetence due to proponents of that field. what ended up coming out is frustration that has been building in my learning about haskell, it's libraries, and the community. in fact, my apologies are needed since i effectively hijacked your article post from the most popular comment (but that comment being the most popular was part of the problem). &gt; Your statement makes it sound like there is a zero sum game; that either we can do web apps, or we can have beautiful theory. actually, not at all, and i'm not for sure what gave that impression. in fact, it's my contention the haskell community leans towards beautiful theory. i care very little about "web apps", and feel like that's a normalizing technique or message due to the general contempt that web app developers seem to be held in. i haven't mentioned anything about web apps, but i do indeed care about producing something that is usable outside of the command line. beautiful theory is fine and certainly required in any field, as i've spent a large portion of my time studying just that (outside of haskell), but as i've pointed out in another thread, there are as far as i know very little to none examples of applications written in haskell that target non-research, non-high tech, and/or non-software engineering users. even abstractions found in mathematics scream for applications, and my frustrations with haskell apply to the mathematical community at large as well. so, it's not about web apps. sage was one of the examples from python (which seems to be about as old as haskell). although, some might consider sage a web app in some of its use cases. &gt; I write Haskell precisely because it lets me think thoughts I can't think in other languages. this is exactly why i want to write in haskell. but like i've alluded to, when it comes down to implementing my idea, a UI is ABSOLUTELY necessary. if haskell makes this portion so difficult, then i must look elsewhere, even if that means giving up the niceness of haskell. or i just use haskell under the hood. but even then, many of the libraries i've come across are simply not developed on anymore or are just experimental or don't work on windows. i'm not tied to windows, but all these issues are red flags to me. &gt; My time horizon is just a little bit longer than yours, and my sights are set on something a bit more ambitious than on replicating the status quo that exists elsewhere. i take some offense to this comment, as you have little idea as to what my ambitions are (how could you really). in fact, the very ideas i have for haskell, even being the novice i am, are extremely ambitious and forward thinking. but as i've mentioned, a full-featured UI is eventually necessary. do i just start writing the backend in haskell hoping that the community (or myself) gets a usable UI framework working in the meantime when there is little confidence or evidence that this is something the community is concerned about? i simply lack the skill and experience to do this myself right now in haskell, so i must weigh that in my evaluation. just because i am frustrated at the lack of immediate and useful libraries, particularly in the UI realm, doesn't mean that i'm concerned only with webapps and short-term, status quo, and shortsighted goals. i am about as anti-status quo in nearly every side project i have. i don't like just being outside the box, i like to destroy the box where there isn't any inside or outside, and i've been trying very hard recently to bring these projects to life. i'm only a haskell novice at this point, so i am trying to work up to being able to contribute in answer to my own criticism. again, i am sorry for brining my criticisms up in your thread, as they don't directly apply to your article. but, i do think my criticisms are valid, are what many feel, and i've seen many times that these criticisms (in other threads, forums, etc.) have been marginalized by haskell proponents. as i mentioned somewhere else, i'm sold on haskell and very excited about it, but there needs to be a call to action within the community (myself included) that focuses on bringing abstractions, libraries, etc. to a user friendly state, when it is quite frankly seriously lacking in these areas.
the code i wrote, of course! e.g.: length:: BashVar -&gt; Int length (BashString s) = length s length (BashArray xs) = length xs
are there not any *serious* attempts at native haskell GUI frameworks? it seems bindings are the initial easy way, but my impression is that they cause lots of problems and limitations in the long run. i am excited to see where the threepenny-gui framework goes.
i agree with that assessment.
I know one person who is interested in a native GUI framework: Jonathan Fischoff. His username on reddit and IRC is `jfischoff`.
&gt;It sounds as if your mind might already be made up, in fact. if i had made up my mind, i wouldn't be here, and i wouldn't care. it's as simple as that. i think you're focusing too much on the specifics of the examples i gave (e.g., labview, spaceX, windows, etc.). i'm not just concerned with windows as i do development on both windows and mac os x, but if haskell says it's cross-platform and it isn't and no one seems concerned about it, that *is* a problem, is it not? telling a novice user to "just fix it" is hardly a community response, and is that expected? i'll certainly do what i can as i improve, but if i'm trying out some haskell code during my lunch period at work on my windows development machine, i guess i'm out of luck and a complainer. so maybe it's a communication problem. if haskell was toted as cross-platform with serious caveats (which is true), then that would be fine and accurate. something shouldn't be sold with fervor while ignoring inaccuracies. &gt; I don't know. If LHC and CERN are your only metrics for "useful" - a nebulous term itself - I'm not sure many programming languages qualify or can even come close. Haskell has been used in many industries from HDLs for automotive hardware design, to writing theorem provers for cryptographic software like Cryptol, which can be used to prove correctness of software implementations (or hardware implementations.) And static analysis tools. And financial tools. And used at banks. And used for quantitative analysis. And signal processing. And microkernel verification (and the NICTA se.L4 project has been commercialized, FYI.) lol. where did i mention these are the *only* metrics? i guess i should list off the technological benefits of labVIEW and ALL its applications and use cases then. it's not a game haskell should play at this point, even though it is a much more abstracted and thought-out language, sans development tools. now, in regards to your examples, do you have any interesting links to articles or posts describing this beyond simply listing these? i would be very interested in these and willing to read them. again, this area might also be a communication problem in that the haskell community doesn't do a good job of communicating uses of haskell in end-to-end use cases. i see very few articles and such relating to this in haskell, though i do know that haskell is used seriously at galois and at a few financial companies, but it seems that's it.
Use it for overloaded functions. That's about it.
that's well noted about the experimental tag, so i'll keep that in mind. &gt; You actually had a few good responses to your UI question, so I don't understand the frustration. absolutely i did. i'm very grateful for those, but one can honestly say there wasn't a great solution to the problem. in fact, it wasn't only until i attempted to install both the diagrams and threepenny-gui libraries on windows did i become frustrated about haskell and windows. actually, it's unclear (to me at least) if the threepenny-gui install may be my fault, and i need to look into that more, but the diagrams library has a bug that prevents it from being installed in windows. from what i've learned since hearing about the threepenny-gui library, i agree with you on turning to interfaces running within a browser. this approach seems similar to something like what adobe air does. i could see this being a useful approach, so i'm going to try and put some time into learning threepenny. thank you for the useful response!
thank you! i appreciate it.
You're welcome!
don't get me wrong, just because i'm not intimidated doesn't mean i don't get overwhelmed or am knowledgeable. there's a fine line there, but it's there. i am still very much a novice. i also agree with it not being a zero-sum game. see my response to that comment. i also meant no disrespect to his article or his work, as my original comment just grew out of frustration at the time, not from himself or his work. &gt; I subscribe to SJP's story about the two different paths toward language nirvana. yes, i do to. that video where he is talking about haskell being useless and a language like C# being useful is spot on in the context he was speaking within. basically, i would argue that haskell is a ladder with rungs missing from the middle two fourths and those at the top are saying "just climb". i've taken a look at the links you posted in the other thread, and will try to respond soon. thanks for the comments and links by the way.
&gt; haskell says it's cross-platform and it isn't and no one seems concerned about it, that is a problem, is it not? So, we have every major component working. GHC. Cabal. The Haskell Platform. Many libraries will work. C code will work, and we'll deal with it for you. Lots of Haskell programs work fine. We're not getting rid of Windows support anytime soon. But the sad reality is as I said earlier: few people actually step up to the plate to proactively address and fix problems and actually write things for Windows programmers, by Windows programmers. This is obviously quite a negative feedback loop as time goes on. I apologize if I came off as harsh, and my intention isn't to scare off people who want to write Haskell on their lunch break. But IMO, there is not really much about about our language or toolkit that inhibits using it in a cross platform manner, but we have no community support for it. Does Haskell not support Windows, or do Windows users just not want to support Haskell? I can assure you people are concerned about it, and nobody *actively wants* Windows users to suffer or anything. But the numbers and history just have never really been in its favor, from a "help build a community" standpoint. The GUI story is sort of in a similar boat. GUI interfacing is not glamorous work, and many people often use alternatives that will get results faster - like the browser, as opposed to GTK. This is actually relatively popular outside Haskell too, and for similar reasons: easier to write, cross platform, less fuss. &gt; lol. where did i mention these are the only metrics? i guess i should list off the technological benefits of labVIEW and ALL its applications and use cases then. Let's just be honest with ourselves: the original question is very, very loaded and there's just about no way to respond without the goal post moving. If you're going to list things like SpaceX as a direct, concrete example, and also ask the question "does haskell do stuff *this useful*" in the same breath - what am I to do but draw upon a list of examples to show you 'useful' things exist? I can't possibly know what your definition of "useful" is, but I can show you the work people have done. Do you want me to say nothing? That would be easier for me and save me lots of time, but nobody really benefits from that, and I see you are interested. &gt; now, in regards to your examples, do you have any interesting links to articles or posts describing this beyond simply listing these? i would be very interested in these and willing to read them. again, this area might also be a communication problem in that the haskell community doesn't do a good job of communicating uses of haskell in end-to-end use cases. i see very few articles and such relating to this in haskell, though i do know that haskell is used seriously at galois and at a few financial companies, but it seems that's it. Just start here, there are some particularly interesting case studies (including molecular biology, cryptography, language toolchains, finance, DSLs, semiconductor design, etc etc.) Not all of them have links. http://www.haskell.org/haskellwiki/Haskell_in_industry There are case studies here, and you can also find reports at CUFP which dates back quite a bit (although CUFP is a mash up of many language communities, and thus Haskell has variable amounts of representation): https://www.fpcomplete.com/page/case-studies http://cufp.org/conference/schedule/2013 These are off the top of my head, not including many paper-written experience reports, etc. For many commercial projects it's sometimes difficult to find concrete research about them and their results (companies are cagey like that no matter what the tech,) and those lists are fairly incomplete as well. But they should give you a good starting point to look around.
If you're OK at working through textbooks and doing the exercises, I think this might be a potential path towards that kind of thing: * http://www.seas.upenn.edu/~cis194/lectures.html and the associated recommended reading (if you're new to Haskell) * How To Prove It (if it's been a while since you've done proofs) * Introduction to Functional Programming with Haskell * Algebra of Programming * Algebraic and Coalgebraic Methods in the Mathematics of Program Construction * The various papers from the "Algebra of Programming" group at Oxford That's pretty theory heavy, and aimed squarely at the equational reasoning / algebra of programming aspects of the post, although there's a lot of awesome in amongst there. There are a lot of other potential paths that would add more category theory or type theory into the mix as well. For day to day statically typed programming, I'd recommend * the lecture notes from the start of the last list * write lots of code * Real World Haskell * write lots of code * Introduction to Functional Programming with Haskell (possibly preceded by How To Prove It) * write lots of code I'm working on getting together a set of PLT learning resources. There's two parts to it - the list of resources (kind of like a BibTeX file) as nodes in a graph and a separate per-user set of tags and edges through the graph suggesting various paths through the material. So that might be helpful in a month or two when I finally get it all done and see if people want to contribute to it. It's kind of predicated on the interested folks also being the kind of people who read textbooks from cover to cover while doing all the exercises, so it might not be for everyone. 
Just wondering, have you tried it and was he right about it only working on top level stuff? :)
IDEs are great for doing the boilerplate - it's hell to use Java without an IDE, for example. But Haskell doesn't really suffer this problem :p
Yes, it DOES work on local definitions, simply because default auto-complete comes with currently open file as one of the sources. :) 
Ah right thanks :)
That code doesn't compile. So, I guess the question now is how you propose to change the standard libraries and/or the language to make that legal.
We're still figuring that out -- right now it looks like a few months due to other items in the work queue.
Did you make sure to install the extra packages that the plug-ins need? The github page for the plug-in should have installation instructions. Granted, I don't own a mac, so it could be another issue. 
If we had a tactic based proof assistant written in Haskell (most of the major ones are in OCaml), this would make a good tool for writing tactics.
One way you can improve your post is to move this to the top: &gt; In future posts, I'll talk about how this can be used to derive an efficient revision control monad and I've been using the same skew binary arithmetic to derive an efficient cache-oblivious unboxable version of the venerable Data.Map. I didn't even know what lowest common ancestor search was typically used for until I read that.
:-(
&gt; There are a lot of applications for LCA. &gt;Computing dominators in flow graphs &gt;Three-Way merge algorithms &gt;Finding common word roots/suffixes &gt;Range-Min Query (RMQ) problems &gt;Computing distances in a tree This was under the first picture (so admittedly not at the *very* top)
comparing WoW to a Haskell IDE is like comparing crystal meth to a hammer... :p
I suppose the haskell unit of productivity will also need to include algorithmic analysis of the resulting code as well. ComputationalTime saved/timetoprogram, All library code in other projects efficiency gets credited to original author, then maybe an Edward of productivity does tend to infinity. http://contemplatecode.blogspot.ca/2013/02/haskell-weekly-news-issue-260.html?m=1 however I do believe algorithms will always advance, my reference being some Scott Aaronson post somewhere, think it was an offhand comment at the waterloo lectures. http://www.scottaaronson.com/democritus/lec5.html Blum speedup theorem
Idris has one and is written in Haskell.
Flyspell support, kinda useful, IMO. Try 'C-x C-s' to activate it, then use M-p and M-n to navigate between errors
I had originally posted this as a "Knol" on the eponymous site, before Google took the site down. I may go through and draft similar documents for the other recursion schemes if there is enough interest.
It's still down really ):
.. aaaaand thoughtpolice just brought it back up :)
I don't think it's necessarily wrong to say that Monoid is the "list" type class. Monoid is the most apparent generalisation of a list (seeing as lists are free monoids), so that sounds reasonable to me. What the author is after isn't the monoid structure of a list, which is only concerned with constructions, what the author wants is essentially `Foldable` - catamorphisms, not initial algebras.
Oh, I know that feeling. Visual Studio *is* spoiling. They really put a lot of work into the F# support. Actually, with type providers, I think that the support for F# in VS is better than the support for C# :) I personally use Emacs, the nice thing about it that you can control it fully from the keyboard, but it takes some time to set it up. 
ghc-mod solves *some* of this problems, but I have to agree.
I don't think the scalability plot shown on the main page means a lot without a comparison to a good sequential implementation, so that we get an idea of the overhead incurred by enabling parallelization. Could you add to this plot the performance of a good sequential implementation of N-Queens? Making it a constant line is visually nice as it allows to get an idea of where the lines cross. Of course it's unclear what "good" means in this context; I think a fair comparison would be an efficient (but non-parallelizing) search monad implementation. It would also be interesting to compare with a constraint-solving library, but the same comment you had on SAT/SMT solvers probably applies (it's more efficient but possibly less expressive).
How can outF be used in cata when it's just a unbound variable? Or, is it just the same as writing InF? Also, anybody know when you learn about Category Theory in high school? Will I encounter it if I take all math courses?
&gt; It can require more than 3 days of setup though. Possibly more than 3 months. I've surprised myself a bit by starting to use emacs recently. Only for Org-mode, though. Maybe I'll start using it more for other things - it's unlikely I'll just stop using Notepad++ any time soon, but having alternative editors available is an old habit of mine too. Vim annoyed me because (1) it started messing with my code, doing quite large-scale reformatting by default, destroying formatting that was there for a reason, (2) the configuration file was stored in the Program Files folder which I shouldn't need to look at, and where I needed admin privileges to edit it, and (3) when I ran the uninstaller, it failed. Emacs turns out not to be so toxic, but it's still clearly designed to maximise post-decisional dissonance - I'm convinced a major part of the advocacy is because once you've spent the time to learn it and configure it, it's too painful to admit that was a mistake so you have to tell everyone how wonderful it is to convince yourself. That said, modern emacs isn't as newbie-unfriendly as its reputation suggests (and as I remember it once being). So far, it hasn't vandalized any of my files. It has a toolbar by default, so you shouldn't find yourself not knowing how to save or exit, for example. There's even an option (in the GUI menu, no less) to enable CUA keys such as the standard cut, copy and paste. That said, my theory is that C-x is such a common first key for emacs commands that if you set that, effectively it's not emacs any more anyway - e.g. you can't use all the tutorials. Configuring emacs also seems a little easier - there's an easy-to-access mode for configuration that hides the lisp syntax and turns it into a form filling exercise, not so different to originally-designed-for-GUI editors. So - I haven't got a clue about all that stuff OP wants - I'm not that much of a fan of IDEs - I find Android development annoying because you're strongly pushed into using Eclipse, for example, and I only use Visual Studio for C++ when I need the debugger. But at least for newbie friendliness I recommend emacs over vim. Possibly for org-mode too, though I think vim supports that as well. And I should point out - the issues I had with vim are meant to be Windows-specific, including the installed-by-default configuration file that enables that reformatting/vandalism option. 
Leaving Haskell aside, the idea of having a central "IDE server" for an organization is interesting. Makes recommended settings and guidelines easier to enforce.
`outF` is a field accessor that comes from: newtype Mu f = InF { outF :: f (Mu f) } Regarding category theory in high school, you're lucky if you encounter it at all even in college, even as a pure math major. Worse, even if you read all the category theory you could get your mitts on, you probably wouldn't encounter catamorphisms, as they are more of a product of a certain corner of the constructive algorithmics community.
Thank you for the great article! &gt; data Skew = Two !Int Digit | One !Int Digit | Zero I can not find the definition of Digit and from the context I infer that you probably mean data Skew = Two !Int Skew | One !Int Skew | Zero 
If you write data Foo a = Foo { quux :: a, quaffle :: Int } you are defining: Foo :: a -&gt; Int -&gt; Foo a quux :: Foo a -&gt; a quaffle :: Foo a -&gt; Int Here, I'm writing newtype Mu f = InF { outF :: f (Mu f) } which is defining for me InF :: f (Mu f) -&gt; Mu f outF :: Mu f -&gt; f (Mu f) which can be seen as `InF` packing an extra `f` into the `Mu f`, while `outF` peels one off. This is the same as if I'd written. newtype Mu f = InF (f (Mu f)) outF :: Mu f -&gt; f (Mu f) outF (InF f) = f 
As for where to pick up this stuff, it is a bit of a mixed bag. My personal recommendation, especially if you are coming at it without a strong math background is to start with something like [Lawvere and Schanuel](http://www.amazon.com/Conceptual-Mathematics-First-Introduction-Categories/dp/052171916X), before moving up to something like [Awodey](http://www.amazon.com/Category-Theory-Oxford-Logic-Guides/dp/0199237182) or Barr and Wells' ESSLLI [summer school notes](http://www.ling.ohio-state.edu/~plummer/courses/winter09/ling681/barrwells.pdf). But, really, if you just want to get a "sense" for category theory without making the commitment to buying a dead-tree book, and need some idea of the importance of different areas of it, I highly recommend reading or at least skimming through ["Physics, Topology, Logic and Computation: A Rosetta Stone"](http://math.ucr.edu/home/baez/rosetta.pdf) by John Baez and Mike Stay before much anything else. It manages to keep a remarkably conversational tone throughout, and it introduces the reader to a huge set of examples and gives you a sense for how you can use category theory to move results between examples and how you can use it to peek right at the fundamental nature of each such example.
You are correct. Fixed.
Fair. As a compromise, I've taken the `UNPACK` pragmas off the actual documentation, but I kept the !'s. I tend to keep the three forms `{-# UNPACK #-} !Foo`, `!Foo`, and `Foo` separated quite rigorously in my head as they have rather distinct operational characteristics, so I normally go out of my way to be very clear which one I mean. One of the things that folks view as "hard" about Haskell is knowing when you should actually put those annotations in, so as a rule I try to ensure that I use them appropriately at all times. It reads like the difference between `int` and `Int` to a java programmer to me. However, to the vast majority of users, the difference between `!` and `{-# UNPACK #-} !` is all but non-existent, so I can at least compromise that far.
This doesn't appear to be changed in the first instance, so perhaps you didn't publish it with changes? Edit: Also I note that the first succ definition doesn't seem right. succ $ succ $ succ $ succ $ succ Zero == One 7 Zero I think it should be something like: succ :: Skew -&gt; Skew succ (Two x xs) = One (x*2+1) xs succ (One x (Two y Zero)) | x == (y*2+1) = Two x Zero succ (One 1 Zero) = Two 1 Zero succ (One x xs) = One x $ succ xs succ Zero = One 1 Zero succ $ succ $ succ $ succ $ succ Zero == One 3 (Two 1 Zero)
The technique that gives SAT solvers a big advantage over naive search like this is clause learning. Every time a SAT solver backtracks it computes a minimal set of variable assignments that caused the failure, which will in general be smaller than the set of all variable assignments at the time of failure. Then it adds the negation of that assignment as a new constraint in the constraint store. The advantage of this is that the new constraint can participate in constraint propagation in the future, which means that the same failure will not happen again since that variable assignment will be eliminated by constraint propagation before it is ever reached. Because the assignment that is put into the constraint store is a subset of the current assignment at failure, a SAT solver essentially eliminates a whole subspace of solutions on every failure. The bad news is that this causes problems for parallelization. Two alternative choices for a variable can no longer be explored in parallel, because a constraint added to the store in exploring the first choice will have an impact on the search process for the second choice. So either you lose parallelization or you lose clause learning. What parallel SAT solvers generally do is perform the two search processes in parallel anyway, but periodically synchronize the constraint stores of the two search processes so that newly learned constraints for each process are added to the constraint stores of the other processes. For N-queens this probably doesn't matter since the problem is so easy that clause learning likely doesn't have a big impact. However for difficult constraint solving problems clause learning is a much bigger win than parallelization, since the latter can only speed up the search by a factor of 8 or so, while the former often speeds up the search by a factor of a billion.
&gt; If any of you have been in this position, how did you manage? I think we've all been in this position. Personally I managed in the same way I managed to learn anything hard: bit by bit.
I have to agree about the placement of bangs and pragmas. It's helpful to see good examples of where they are appropriately used, possibly *more* helpful than the algorithm itself if we're talking about maximizing educational value and utility across all readers.
Yes. It seems you need to store paths root-down, not node-up. Reversing them will kill your `O(log h)` complexity though, so hopefully they can built that way (i.e. starting at the node and cons'ing up to the root).
Here's a way to let the type system enforce the invariants of the skew binary random access list by using nested data types which is Haskell 98 compatible: https://gist.github.com/sjoerdvisscher/6570858
Yeah, I missed that. Usually I only read the introduction, code, and conclusions.
Took me a while to understand it myself, sorry about that. The reason why $10 is way too steep for me is the same why the following offer is outrageously expensive to you: "You pay $10/month in exchange for apfelmus being able to use the online Haskell IDE (while you get nothing)." It has no value to you, but surely, you can afford that? ;-) My editor setup works fine, I have no use for the IDE. However, here's the odd thing, and the reason why I'm saying something at all: while the value is zero to me personally, it is non-zero to the community at large. For instance, I'm currently working on a library called threepenny-gui which comes with a [couple of examples][1] and it would be very nice to deploy them live on a public server so that other people can try them out without installing the library. I have no interest in a public server myself, but I'd happy to take a small step to make life easier for others, given the tools. Recently, someone sent a pull request concerning [Travis][1], a service which automatically builds and tests packages. I don't care about it personally -- the package compiles fine on my machine. But I'm happy to adapt a little so that it becomes more useful to others. [1]: http://travis-ci.org
So what will happen to all the wonderful articles on School of Haskell, once someone at FP Complete realizes that no one is using their IDE (due to in reality most of Haskell dev taking place on Vim/Emacs) and decides to tank things? I hope FP Complete isn't actually dependent upon this IDE to sustain their business model. What industry needs is a set of tools for handling Haskell in existing editors. I would pay a premium to reliably refactor Haskell in Vim/Emacs through common core binaries similar to ghc-mod, but apparently that isn't as sexy as having a "cloud" based IDE.
It is very useful in certain situations. For example a trivial web path router. In this case the easiest think is a regex on the url, e.g. /foo/1 or /foo/2 to get the id of the foo resource. Sometimes it's overkill to use parser combinators for a simple string like that. You can mix both approaches still. I use regex for my own ghcjs single-page application to route url fragments to a page structure.
Woops. I meant to write that with 'keep' which means I need to `drop (n - k)`. Thanks. That is what I get for not checking the code I had to write to get to the implementation I already had. 
That winds up with Two's deeper in the list that at the top and recursively invokes `succ`. Instead of: succ :: Skew -&gt; Skew succ (Two x xs) = One (x*2+1) xs succ (One x xs) = Two x xs succ Zero = One 1 Zero it should have ben: succ :: Skew -&gt; Skew succ (Two x xs) = One (x*2+1) xs succ (One 1 xs) = Two 1 xs succ xs = One 1 xs but I screwed up reciting it from memory.
They can I just used the wrong trimming operation. =)
Thanks for the report. We're looking at it :-(
You don't need the Haskell platform to install cabal-install. I always just install GHC binaries and cabal-install for tarballs and everything works fine, better in many ways than using the outdated Haskell platform.
Thought I'd read a rant of why Bas van Dijk left haskell-cafe. Everything went better than expected!
Am I understanding correctly that the "secret sauce" here is random-access lists with O(1) cons, and O(log n) drop, and any implementation of such lists would do the trick? If that's the case, I think it would make sense to add a sentence about it somewhere near the top. Or maybe even add a typeclass for such lists.
Almost, but not quite. Take for instance a [finger tree](http://www.soi.city.ac.uk/~ross/papers/FingerTree.html). That has _O(1)_ `cons` and _O(log n)_ `drop` but it does not have a unique representation, so it doesn't let you walk down the spine in lock-step in the last part. That would be sufficient to tackle the level ancestor problem, but not the lowest common ancestor problem. So we need _O(1)_ `cons`, _O(log h)_ `drop`, monotone search and a unique representation. That has pretty much limited us to normal or zeroless skew k-ary.
Thanks!
Each editor has its specific preferences that you can of course change, but themes work across all of them. I've submitted a pull request to the color themes plugin to add support for the Haskell editor, so it should arrive soon.
Thanks! Your explanation makes a lot of sense and when I see those pragma marks I'll start trying to think of them as you described. My thought process here was that typically, and granted I'm still relatively new to Haskell, I appreciate that most Haskell code is all about "what" and not about "how". As soon as I start seeing annotations on the code, my brain switches modes and starts thinking about "how" things are executing. Perhaps that's just an unfortunate necessity of real world code.
&lt;- Interested. Having not seen the knol version. I like the short format and the curated list of references - very useful in constraining a later deep dive, and b/c I don't get TOC e-mails for e.g. Nordic Journal of Computing
So if I have newtype Foo = Foo { unFoo :: Int } instance Ord Foo where compare = flip (comparing unFoo) and build up a `Set Foo`, which I then `coerce` into a `Set Int`, will my resultant `Set` be broken?
At last check you couldn't use them there, because they'd only work like that if the constructors to Set were available to you in scope or if the Role was left alone or something. What I've been talking to Richard Eisenberg about adding would be a method to the Functor class like: fmapCoerce :: (Functor f, Coercible a b) =&gt; f a -&gt; f b fmapCoerce = fmap coerce but which can be automatically instantiated to a use of coerce whenever the Roles say that it is safe when using `DeriveFunctor`. This will let us use them inside of `lens` for performance, and should let me re-enable a high performance `vacuous` in `void`. It should also let us define something stronger than an Iso but weaker than Equality that we can compose witnesses of using `lens`, that we can upgrade the instances of Wrapped to.
I'm quite happy to see this merged in time for the 7.8 feature window. Thank you again Joachim!
The constructor check for the type constructor to coerce below was removed in non-Haskell-Safe-Mode. So you do need to change the role to keep Set abstract.
Lawvere and Schanuel is a great intro! Highly recommended.
seems that you are looking to share code. Of much more importance than composability is the size of the sophisticated user base (those who would write re-usable code) and tools to easilty distribute code. cabal just got sandboxes, I am becoming hopeful that people will have a much easier time installing stuff, which was one of the largest preventers of code re-use in the past. Let us know how we can make it easier for you to share code.
Yay! I'm much happier with that outcome.
You can use that kind of deBruijn based conversion for SKI too, by the way. However, it has much higher constant overhead than the direct way, so doesn't give a win for small expressions.
Compilers do some heavy lifting during compilation. I'd say it's normal. I haven't had a look at GHCs CPU usage but I've witnessed similar figures with GCC during kernel compilation.
Assuming you weren't actively building something, no, that's not normal. Could you give a few more details about what you did before noticing this behavior? Did you note the full command line of the process in question?
How would you manage to coerce Set Int to Set Foo, when Set wouldn't have any coercion instance nor a functor instance?
No. As explained on the #haskell-infrastructure IRC channel, they just switched the mailing lists to a new server. Apparently mailman got restored from an old configuration.
Yeah, I installed all the packages, got all the tools on my path, `cabal build` works fine from the terminal. SublimeHaskell just never finishes the "Cabal: Building &lt;ProjectName&gt;..." bit. With GHC 7.6.3 and cabal 1.18.0.1.
Strange, I haven't seen that problem before. If you cared enough, you could submit a bug report to the developer. I'm sure he'd be willing to help figure out what was wrong.
I'd recommend Pierce's textbook [*Types and Programming Languages*](http://www.cis.upenn.edu/~bcpierce/tapl/), which is a standard entry-level introduction to type systems theory. Asking questions on Stack Overflow often also nets useful results.
FWIW, that happens to be implemented at [Data.Number.CalkinWilf](http://hackage.haskell.org/packages/archive/prelude-safeenum/0.1.0.1/doc/html/Data-Number-CalkinWilf.html) in [prelude-safeenum](http://hackage.haskell.org/package/prelude-safeenum). The whole point, of course, is that you *don't* need the whole tree :)
Loving what you're doing here Alex, it's really coming together. Please continue letting us know how things go!
If the other comments aren't on target, is it possible you started a thread in ghci that's in a busy loop, and proceeded to leave ghci running?
If you are just making test compile, may try -O0 to make it much faster.
This should be fixed now. Somehow the configuration got corrupted and truncated. We've gone through and replaced it with what looks to be a good, very recent backup. I suspect the corruption has to do with the recent downtime, etc. We're contacting our hosting provider to do a more thorough investigation.
Thank you for the insightful comment! You are absolutely correct that the approach used in LogicGrowsOnTrees limits the kinds of algorithms that you can use it with. Having said that, it does provide a monad transformer, so you can carry around state that lets you transmit information from lower parts of the tree to upper parts of the tree. You do have to be careful when doing this because your algorithm needs to have the property that it is agnostic about which parts of the tree are being explored by a particular worker or how often a part of it has been explored by the full system, but if all you use the store for is to prune branches that never were going to result in solutions anyway then it should be perfectly safe. If you just use the monad transformer with state for the store then it will be local to each worker (which should still be enough to get a significant speedup). However, you could also in principle create a system that lets the workers communicate with each other and synchronize their stores; in fact, a future version of this package could contain functionality for this if I can figure out a good way to implement it.
Well, [this is what classy-prelude does](http://hackage.haskell.org/packages/archive/classy-prelude/0.5.10/doc/html/ClassyPrelude-Classes.html#t:CanFilter). 
What's non-free about an FC coercion?
That type class is called [ListLike](http://hackage.haskell.org/packages/archive/ListLike/4.0.0/doc/html/Data-ListLike-Base.html#v:filter). That being said, I recently attended a presentation by Mario Blažević about how his [monoid-subclasses](http://hackage.haskell.org/package/monoid-subclasses) package is theoretically superior to ListLike. In his hierarchy, the typeclass in which `filter` would belong is [FactorialMonoid](https://github.com/blamario/monoid-subclasses/blob/master/Data/Monoid/Factorial.hs#L70), an extension of Monoid which basically says that in addition to being able to concatenate pieces together, there is also a canonical way to divide any element into atomic pieces. Thus: monoidFilter :: FactorialMonoid m =&gt; (m -&gt; Bool) -&gt; m -&gt; m monoidFilter p = mconcat . filter p . factors One major difference between the APIs of ListLike and FactorialMonoid is that ListLike uses a different type for the container and for the element, while FactorialMonoid represents compound and atomic elements using the same type. In the case of lists, atomic elements would be singletons.
I'd be potentially interesting in contributing, but could not take lead right now.
Isn't `mfilter` good enough? mfilter :: (MonadPlus m) =&gt; (a -&gt; Bool) -&gt; m a -&gt; m a Most of the types that you'd like to filter already implement `MonadPlus`, or if they don't it is because they have some constraint that doesn't play well with the `Monad` type class.
First step: what is the type system of APL?
Do you mean isn't done automatically?
The price of getting types to enforce the invariant here is that `cons_2` is now recursive. So you've lost the _O(1)_ `cons` guarantee, which makes the claim of no preprocessing cost fail to hold. cons_2 x (Z_2 xs) = zero (cons_2 x xs) 
I feel like not enough people are aware of Snoyman's [Keter](https://github.com/snoyberg/keter) project.
Wow, I didn't know about this. I agree that for yesod projects, it makes more sense to use keter.
Interesting that that message was sent 1.5 years ago, but it arrived today.
&gt; or if they don't it is because they have some constraint that doesn't play well with the Monad type class. The notably absent type here is `Set`.
It's dynamically typed. But dynamic typing can be embedded in Haskell in many ways, so there's no problem here apart from the cultural one.
The coercion is free at runtime, but it costs me effort when writing the code.
Yes, thanks!
Not to mention ByteString and Text.
The email fell behind a box and was stuck there for a year.
Try turning off idle-GC.
Sometimes, I have to `pkill ghc` to ask back my memory.
Are you certain your new `succ` is correct? When I run it, I get: *Main&gt; iterate succ Zero !! 14 Two 1 (One 3 (One 3 (One 3 (One 3 Zero)))) But would expect: Two 7 Zero The following code gives the answers I would expect, unless I misunderstood something: succ' :: Skew -&gt; Skew succ' (Two x (One y ys)) | x * 2 + 1 == y = Two y ys succ' (Two x xs) = One (x*2+1) xs succ' (One 1 xs) = Two 1 xs succ' xs = One 1 xs Which gives: *Main&gt; map (iterate succ' Zero !!) [0..14] [Zero, One 1 Zero, Two 1 Zero, One 3 Zero, One 1 (One 3 Zero), Two 1 (One 3 Zero), Two 3 Zero, One 7 Zero, One 1 (One 7 Zero), Two 1 (One 7 Zero), One 3 (One 7 Zero), One 1 (One 3 (One 7 Zero)), Two 1 (One 3 (One 7 Zero)), Two 3 (One 7 Zero), Two 7 Zero] As expected.
I'm not sure what you mean. My first thought was that if you're complaining about the overhead of `MkFoo` and `unFoo` in newtype Foo = MkFoo {unFoo :: Int} then you could just use type Foo = Int instead, and what's the problem? My second thought is that you want to be forced to used `MkFoo` explicitly, but that the compiler should implicitly insert `unFoo` where necessary (or visa versa). My third thought is that perhaps you want something else entirely. I'm not sure.
SPJ must know a thing or two about lazy evaluation!
I think FPC wants to change this, so it is a chicken-and-egg issue. If they undercharge at this point, it might be impossible for them to raise their price in the future, and that could put the company in financial trouble in the long term. I am one of those that would be a potential commercial user, except that I am funding my own startup, so I am super-tight with money :-). All in all, I think FPC should be allowed to *not* have something for the whole community. They might need to keep this away from some part of the user-base in order to ensure that the company survives long term. It is a tough balance between exploiting and exploring this market.
Isn't this too granular? 
Actually, close. It was held up in moderation due to 'too many recipients.' When we were fixing the other issue with -cafe, it was noticed hanging around there, and finally approved.
I don't understand. "Why not" what?
Can you clarify what aspect of this you consider to be completely wrong? I agree that there are problems, but I want to be sure we're seeing the same ones. Namely: * You shouldn't be running arbitrary commands from some website. While true, any way you cut it you're about to be installing some software from some website, and probably running it as root. So while some paranoia makes sense, this would be a bit misplaced. * You should manage tools like Keter via `apt-get` or equivalent. I don't disagree at all, and would certainly recommend something along those lines. When we set up deployment machines at FP Complete, we certainly don't just run this bash script. There's a reason this is under the heading "quick start guide", it's meant to be low barrier to entry. * You shouldn't install the GHC toolchain on your production server. Agreed, see previous point. Point being: this isn't intended to be the final solution for proper server setup. Perhaps I should have been more clear on that point, but I thought that was pretty self-evident. But is there some more insidious problem that I seemed to have missed here?
Yours is the correct version. ;) This is why I always use a single constructor. I can never get this version right. =)
Oh! I get it now, after re-reading that section. But I still think that any random-access list would do: to find the "agreement point" in lists As, Bs (suppose len(As) &gt; len(Bs) and lists have the root at the head rather than tail), we can do a binary search over the index "i" for i in 0 .. len(Bs): [1, 2, 3, 4, 5] [1, 2, 3, 6] We can do a binary search over the first 4 elements and find the last position that has a match. This can be done in log(h) with a fingertree which has subtree size in each node.
What about: let x = undefined c = {- too many thunks -} in (c, x)
I may take the opportunity to use that email to pre announce my own stuff getting released shortly. 
Those could be fixed in theory by implementing them in terms of `Vector`, which is a `Monad`, but not even I would recommend that. However, I still don't think corner cases like these justify a new type class. Just use monomorphic filters for the types that don't work for either efficiency reasons or because of the constraint limitation.
I'm not sure it is a good idea to add a method to the `Functor` class. If this proposal requires invasive modifications like that then it sounds like a flawed proposal.
The method would have a default definition, just like (&lt;$) does now. Like with (&lt;$), the vast majority of users don't need to even know it is there, but it can make an asymptotic difference for those who do.
Yes, but is that the only change that would need to be made or will we start needing coerce variations for every other type class?
It'd really just be Functor and Contravariant. It lets me kill the hackish versions I have in Profunctor.
I think it should be possible to implement this kind of binary search on a fingertree using amortized O(1) time per read. But I guess I should try to implement it :)
The problem isn't in binary searching one tree it is that you have to simultaneously search two, and log n of your levels may be misaligned so you can't just let the fingertree pick your cuts like normal. If you can prove this can also be log n, then I have a whole host of applications.
Forty years ago, APL was my first functional programming language. I got to use a terminal; it competed (easily) for my affections with FORTAN on punched cards. For a professor, I reduced hundreds of lines of BASIC (translated 1:1 into APL) into a ten line program which ran much faster. I left his code in the workspace; two years later he called me in to ask how my code called his code. (It didn't.) APL's strength was multidimensional array handling, so we ran everything through that food mill. Dozens of languages later, Haskell finally recovered the thrill I first felt coding in my youth. But I see no need whatsoever to recreate APL in Haskell. Just code idiomatically in Haskell, and be glad that APL existed.
I only said «cabal-install comes with Haskell platform» not «cabal-install can only be installed by installing Haskell platform»
You claimed that unless you compiled GHC from source cabal-install came with the Haskell Platform which is simply wrong. The Haskell Platform is like downloading e.g. the DVD version of Debian while GHC binary plus cabal-install is like the minmal network install CD, the resulting system is identical if you download the exact same versions (this post is actually about doing just that without the HP installer) but generally speaking compiling the large, one-time download takes more work so it happens less frequently.
Is Unfoldable even necessary? If you represent a collection xs by its fold function, then you can filter like this: type Collection a = (a -&gt; s -&gt; s) -&gt; s -&gt; s filter : (a -&gt; Bool) -&gt; Collection a -&gt; Collection a filter p xs = \f z -&gt; xs (\x s -&gt; if p x then f x s else s) z
&gt; cabal-install comes with Haskell Platform, not with ghc. That was the nutshell of my claim. The «Not otherwise» part in front might have you claim that I «claimed that unless you compiled GHC from source cabal-install came with the Haskell Platform which is simply wrong». And your claim is true in the sense that claiming what you claim I claimed is wrong. But I intended to claim something else so both of our claims are both true and false :D
I'm complaining that I can't use `fmap unFoo` which is the natural way to convert a functor, but instead I have to use `coerce` sometimes.
Here's the problem. Let's newtype the idea of "represent a collection xs by its fold function" data AFoldable a = AFoldable { foldWith :: forall b. (a -&gt; b -&gt; b) -&gt; b -&gt; b } instance Foldable AFoldable where foldr c z foldable = foldWith foldable c z We can convert any collection to this representation easily. toAFoldable :: Foldable f =&gt; f a -&gt; AFoldable a toAFoldable foldable = AFoldable (\c z -&gt; foldr c z foldable) We cannot easily convert back. fromAFoldable :: Foldable f =&gt; AFoldable a -&gt; f a fromAFoldable = ??? It requires extra information. Specifically, the `Unfold` information I specified above: `Monoid` and `singleton`. Abusing Applicative for `pure = singleton`, we could write: fromAFoldable :: (Monoid (f a), Applicative f, Foldable f) =&gt; AFoldable a -&gt; f a fromAFoldable = foldr (mappend . pure) mempty [edit] Or, using MonadPlus: fromAFoldable (MonadPlus f, Foldable f) =&gt; AFoldable a -&gt; f a fromAFoldable = foldr (mplus . return) mzero 
Yes there is a bit of a trade-off here. There are two different kinds of filter you can define: `(Foldable f, Unfoldable f a) =&gt; (a -&gt; Bool) -&gt; f a -&gt; f a`, and `Foldable f =&gt; (a -&gt; Bool) -&gt; f a -&gt; AFoldable a`. The advantage of the latter is that you don't need to Unfoldable, the disadvantage is that you get an `AFoldable` rather than an `f`.
So, it's a matter of optimization, then? You want any runtime occurrence of `fmap unFoo` to be optimized as `coerce` whenever `unFoo` is a newtype unwrapper?
&gt; When I began using Haskell, I thought foldl had a complicated type signature. I remember those days...
I don't fully understand what you are saying. Are you saying that because someone in the community might want to use your library in the FP Complete IDE that you must provide some special version of it and thus pay for a subscription yourself? A.) Doesn't cabal already solve this problem? I was under the impression you could use any library from cabal within the IDE? B.) I am not sure why this responsibility falls on you. Couldn't the community member in question take it upon themselves to bring the library into the FP Complete universe and provide it to others? Why are you responsible? I'd say as long as Hackage remains the distribution method for our community this is simply not an issue.
I think this is too granular and you're just setting each of these subreddits up for failure. As it stands neither of them will have a constant flux of several posts per day, you may have filled them up now but you had backlog to start with. There's really no reason not to keep it together, people familiar enough with the concepts will be able to tell things apart, and as for everyone else, they either won't care about the distinction or they'll learn in time. As for multi-reddits I don't think that's the use case behind them. A multi-reddit of several weak subreddits won't make a good, subscribable source, it will just delineate a district of ghost towns. I have one for all dev-related stuff, and I'd like to just integrate one more subreddit into it concerning flow/reactive programming, not three.
&gt; filter f (filter f c) = filter f c Or, more generally: filter f . filter g = filter (g &amp; f) where (g &amp; f) x = g x &amp;&amp; f x Note that the swap in order for the composed predicate is required in order to preserve the operational characteristics (i.e., we only test `f` after `g` has succeeded).
I think his problem is that you used wget instead of curl. Scandalous! =p
No, that's all fine, the library perfectly useable with cabal and the FP Complete IDE. What I'm trying to say is something entirely different: if I had access to a cloud server (which is included in the FP Complete IDE), I would use it to improve the documentation of my library (by hosting examples on that cloud server). But that is of interest only to the community. Personally, I don't care whether I have access to a cloud server or whether my library has better documentation or not. The commercial model assumes that I have a personal interest in the product. But I don't, it's worth $0 to me. I only have a "community interest" in the product -- which is why I expected it to be free for me, just like Github or Travis.
See my other comment.
Note that this is a functor law if you replace `(a -&gt; Bool)` with the `(a -&gt; All)` monoid: mempty = \_ -&gt; True mappend f1 f2 = \a -&gt; f1 a &amp;&amp; f2 a filter mempty = id filter (predicate1 &lt;&gt; predicate2) = filter predicate1 . filter predicate2
Why is the author of the GHC patch in question getting a nontrivial (not explained by reddit's perturbations) number of downvotes?
I don't see the need for a goto function at all: label = callCC $ \k -&gt; let m = k m in return m Now this code does what I expect: do x &lt;- label lift $ print "Forever!" x
Very much off topic, but check out: http://hackage.haskell.org/package/vector-bytestring
Why don't you ask your homework questions somewhere else.
That function composition is a novel trick.
If you're in high school, you probably don't have the math background to jump straight into category theory. Start with some abstract algebra -- a formalization of the algebra they teach you in high school (but way more fun and interesting). And whatever approach you take, make sure you do the proofs! Learning to read and develop your own proofs is crucial for mathematical understanding, and it's something they don't teach at all in high school (geometric proofs and the like are a good start, but I've never seen a high school curriculum that taught even that properly). Ask a lot of questions and try to really understand what's going on before you reach for the next topic. Once you have a feel for algebra, category theory will make a lot more sense. But don't expect that to come easily -- many professional mathematicians avoid the subject. It's fascinating, but far from easily approachable.
Honestly the tools are pretty good, I'm just wondering what code there is being shared. 
Man, I wish my school offers functional programming course that teaches haskell...
I don't expect cartesian *closedness* to be solved for circuits. I would probably instead look into making a symmetric monoidal traced category, which, through the GoI construction, can be turned into a symmetric monoidal compact closed category, which is a model of linear type theory. The problem is that with closedness, there is 'no' upper bound on the amount of information in an object, which doesn't fit nicely with something that doesn't cheat (such as dynamic allocation and garbage collection).
So there's gonna be even more packages you can't simultaneously use because they depend on different point-releases of something, in the future?
it's your first bullet point. i agree that one might run your script as root anyway, the problem is, that it encourages running a script, live, from the internet, without checking anything. using the package manager is nice, but it's hard to get a deployment tool for up-to-date-things into debian stable or other "stable" distros, so i don't complain about point two. thirdly, it would be nice if there were (maybe there is one i don't know) a way to automatically build cabal managed projects as `.deb` (a la cabal-rpm), but people interested in a kepler-like tool usually are not that much into using their package manager. i have absolutely no idea why though. so i'm fine with three as well.
even though i prefer curl to wget (when not recursively downloading, that is) and the command is shorter, wget is installed by default in most linux distros, curl is not. so that's a plus.
No, this is meant for reproducible builds, not to release to hackage. 
`data BinaryTree a = Nothing | Node a (BinaryTree a) (BinaryTree a)` Stupid question, stupid answer. (sorry I just wanted to say that :D)
In France, it’s not uncommon to be taught OCaml. It’s not Haskell (I far away prefer Haskell), but it’s a good start in the FPL world IMHO :)
Because then it's not “Goto in Haskell”. :-)
you might include a note at the top of the documentation that notes that it is indeed easily done with `ReaderT`.
Are symmetric monoidal compact closed category a model for linear type theory without exponentials?
That's right.
Yes. They are models for multiplicative linear type theory. I have no idea if the GoI method would yield something useful, but it seems like a better way than a biCCC, since with that, we would eventually end up with a hardware garbage collector, and that would change this entire exercise from completely brilliant to kinda stupid.
I don't see how Haskell can be easily translated to anything with linear types. But maybe that's just me being negative. I don't think Haskell is a good language to translate to hardware in general. Unless you plan to execute most of the Haskell at compile time.
Me neither. I'm not entirely sure why you would want to compile Haskell to circuits either, so I can only write from the perspective of wanting to read an interesting series of blog posts. In that case, it seems to me that compiling a linearly typed lambda calculus *directly* to hardware would be more interesting than what would happen if we attempted to compile a biCCC to hardware. But I can't be sure, of course.
You can't unupload things. I think sometimes people will simply upload new version of package, remove all modules from within and mark it as deprecated in few places (stating reasons for it).
Kick ass!
You can always mark the package as obsolete. If you e-mail Ross and ask him he will mark it for you. You can even add a message saying whether to find equivalent functionality.
Have you tried the -keep-tmp-files flag?
GoI = Geometry of Interaction?
One didn't track types so much as shapes: the number of dimensions, and those dimensions. Typical polymorphism would be to replace min, max for plus, times in matrix multiplication. Like many people, this was my first encounter with polymorphism. LISP at the time was a blank slate where one could of course do far more if one knew to do so. APL was a greased set of tracks that told you to engage in this sort of polymorphism. To an extent, one could manipulate functions as values. Everyone tries once to write a function that deletes itself from the workspace. (It should work but it doesn't.) As a student, I had lunch once with APL author Ken Iverson, and I tried to share with him my insights on how the language could be improved. Haven't we all been on both sides of conversations like that? I laugh now, of course it didn't go anywhere... I then worked for a summer as a commercial APL programmer. The firm rarely hired students, but they were desperate. APL has an odd keyboard filled with funny operator symbols. The key "establish the pecking order" interview questions were "dead key" problems: How do you code this, if the following keys are out? Summing as decoding in base one, that sort of thing.
Hey this was written by the dude who sold me his furniture for in my dorm room! What a coincidence. 
Its an expression for: I see your point, I am not sure if it will work well, but give it a try.
If I imagine APL surviving in Haskell (as ancient languages survive in modern spoken languages) it is as a generalization of lists, not arrays. Lists are fundamental to Haskell; arrays less so. Consider everything we can do with lists; it would sometimes (but not often) be handy to have that flexibility with the multidimensional objects that were APL's currency. So what does this mean? Do something, but not slavishly faithful. Rather, capture the spirit of APL in Haskell. Requiring uniform dimensions was also APL's weakness; this didn't always fit the problem. Our zip functions, for example, forgive length mismatches. APL is all about shapes of arrays, and never allows this. We call our pool shots on types, but allow unregulated lists. APL calls its pool shots on array dimensions. These are different points of view. Taking the head of an empty list is still the easiest way to generate a runtime error in Haskell.
relative to popular frameworks in other languages, not a lot. You can peruse the Yesod wiki and hackage.
As far as I understand it, they compile a linearly typed lambda calculus to hardware over here: http://www.veritygos.org
&gt; Are symmetric monoidal compact closed category a model for linear type theory without exponentials? I guess I understand now how other people feel about Haskell.
The first set of winners have been announced. https://www.fpcomplete.com/business/blog/august-competition-winners/
But C is not C++. C++ is like, complicated messy C.
Yes.
I haven't looked too much into this, but that looks exactly like what I'm proposing. Edit: looked a bit more at it. I have no idea if it's what I'm proposing.
Well, let's break it down. I'm going to assume that you know basic category theory (category, functor, etc). A monoidal category is a category C equipped with a functor `x : C*C -&gt; C` such that this functor is associative and has an identity (up to natural isomorphism). An example of such a functor is a product functor. A monoidal category is symmetric if `x` is symmetric (up to natural isomorphism) in a sufficiently natural way. An example of a choice for `x` that is symmetric is the Cartesian product functor. Let's call `x` 'multiplication'. A symmetric monoidal category is compact closed if you can, in a sense, divide. Division then turns out to be a sort of internal hom (basically higher-order functions). You can't divide if `x` is a Cartesian product functor, except for trivial cases. So now we've got higher order functions and a kind of product/pairing functor, but there is no way to duplicate values. This is essentially the definition of linear type theory. Exponentials in linear type theory refers to the type constructors making values duplicable. Those operators do not necessarily exist in a compact closed category, which is why compact closed categories only model linear type theory *without* exponentials.
For those not familiar with the GoI construction, this is pretty neat: http://semantic-domain.blogspot.com/2012/11/in-this-post-ill-show-how-to-turn.html
Personally I find that the unicode symbols on this blog make the code very hard to read.
Also, a symmetric monoidal category is traced if it has an operator called a trace, that sends a morphism `f : AxT -&gt; BxT` to a morphism `f* : A -&gt; B` in a sufficiently natural way. The trace is usually some kind of looping if x is a coproduct functor.
Thanks! I see now it actually makes sense, even though looks like zyglohistomorphic prepomorphisms to an unguided eye.
Arrow notation can be generalised to (pre)monoidal categories, see for example http://www.cs.berkeley.edu/~megacz/garrows/. He even has a [paper]( http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.225.4210) about circuit design using this. Also, a lot of the concepts used in Haskell (monads, applicative functors, coends, Yoneda), which are usually interpreted as constructions on some cartesian closed category, generalise to monoidal categories, and some even to enriched categories.
By sharing this info via reddit and the cafe list, I hope enough people are aware of the issues that anyone who hits them can be quickly helped by someone in the know!
thanks!
I am using the haskell platform 2013.2 supplied by MacPorts, which runs fine in parallel with XCode 5. I think ghc from MacPorts is configured, by default, to use gcc instead of clang.
Brilliant idea! It looks a little like a private version of http://ifttt.com/
Ah, interesting! However, you're still limited to languages where the monoidal structure is the same as the cartesian structure of Haskell. You can't represent substructural languages, or even cartesian categories where the product is not given by Haskell tuples (as in the case of Pipes).
Thanks! It actually ends up being a private version of https://deadmanssnitch.com If you are willing to run the server yourself it will safe you $19 a month.
It's hard to do monads when your polymorphism is limited to function arguments. Without reading the entry, that's what I'd say is blocking. You can have `(do (return 1))` notation, but which monad instance? Who knows. You have to pass the instance around or specify it up front which is boring and doesn't feel like a cool abstraction.
Not quite the model analysis I expected, but a really interesting and fun use of Haskell. I'd love to see the technique abstracted and applied to other visual collections.
What are products in Pipes? I'm not sure I've heard of them, or if I have, I didn't realise they were products!
I haven't thought about this in some time, but I think the natural monoidal structure of the category of pipes is actually cartesian. Here is a detailed description: http://paolocapriotti.com/blog/2012/02/04/monoidal-instances-for-pipes/. The post uses a weird `Comultiplicative` typeclass, but IIRC all that is doing is proving that the monoidal structure comes from a product (I just didn't realise this at the time).
I was able to implement do sugar successfully in scheme by having the do macro return a function from the monad dictionary to the monadic value, and having it plumb the dictionary to the arguments. You can view this as reader monad transforming all of your monadic values with the dictionary as the environment. You can even shoe-horn in Applicative, MonadState, etc. by adding other mixin traits to the resulting dictionary and you can define monad transformers by building up more interesting dictionaries from primitive parts. For a while this is how we implemented them in Ermine as well. However, this approach doesn't scale very well. Once you start working with things like Traversable, the plumbing gets quite complicated and you often lose valuable value-sharing, since you only get sharing of these reader-transformed functions.
There are somewhat compelling security reasons to not have compilers on your webserver.
I use https://deadmanssnitch.com and it's free to sign up. Plus, you get one free snitch for every email referral so it's pretty easy (and still free) to get snitches. Rumor has it they're changing around their pricing models soon, adding an Iphone app and creating an API. As you can tell i'm a big fan of Dead Mans Snitch :D
The article does point out this limitation, and decides to write multiple versions of return with different names. This prevents him from writing expressions which work with all monads, but that's not what he tries to do. Instead, he implements the Maybe and State monads and gives one example using each. He conclusion is that in a language which supports mutation, the State monad is not that useful (and also that the syntax of Common Lisp makes monadic code uglier than it could be). Overall, not the insightful explanation of Why Lisps don't Need Monads that I expected.
Right, sorry some of that was unclear. `helper` is actually provided by optparse-applicative. It's a parser that always fails so that you will see the help/banner message. http://hackage.haskell.org/packages/archive/optparse-applicative/0.5.2.1/doc/html/Options-Applicative-Extra.html Glad you found it was helpful.
Nice. I wrote something similar at work to monitor an application. Although since the app I was monitoring was already using redis, I used the redis pub/sub mechanism to handle the keep alives. Different modules emit messages on different channels and the monitor keeps track of when it saw each signal last. It's configurable so that for each signal you can specify how long it should wait for before calling a shell script that takes some sort of corrective action.
 $ roller -v n2 2d10+2 shouldn't that be $ roller -vn 2 2d10+2 
Fixed. Thank you!
yes. that's why i stumbled on it as well. there is a hint that it's about fashion week though.
Dead Man's Snitch is a good choice if you want a web UI for managing watches and don't want to set up a server to do the job. If that convenience is worth $19/month to you then it may be a better choice. That said, vigilance has a few additional features like multiple notifications per watch, HTTP POST hooks on failures, and the ability to test notifications for a watch that you've set up. I'm also considering creating a second package in the future that will provide a nice web UI for creating/managing watches, however that will change the semantics somewhat (i.e. config file based watches don't make much sense if you're going to use a web UI to create/delete them instead).
What do you think of Mario Blažević solution in ["Adding Structure to Monoids"(PDF)](https://github.com/blamario/monoid-subclasses/wiki/Files/HaskellSymposium2013.pdf)?
I'm not a big fan of it. In Haskell, type classes require massive buy-in, so I'm always extremely conservative about adding new ones to Haskell's type class hierarchy unless I'm convinced that it is the best possible abstraction for the job. The other thing that bugs me about it is that the justification for each one of the type classes is: "There is a mathematical name for this" and there is no deep discussion of the intrinsic merits of these abstractions. The type class that bothers me the most is his `FactorialMonoid` type class, which tries to expose the underlying structure of the monoid. This seems to me like the antithesis of the spirit of the monoid class. I've always viewed the "purpose" of the monoid class as combining things so that they no longer leave a seam behind when you combine them, and this `FactorialMonoid` class is promoting the exact opposite behavior: preserving the seams as faithfully as possible.
 type Exp a = Rec Exp a is that supposed to be type Exp a = Rec ExpF a ??
Watch Pfenning's [OPLSS 2012](https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html) lectures. Then watch Bob Harper's. Then read some intros to dependent types (e.g. some of the Agda intros), maybe read a bunch of Types and Programming Languages. Come to the #haskell channel on freenode before doing any dependent type stuff tho. I'm augur on there, but there are lots of people who can help guide you. You should be in the channel anyway.
I don't get what you want to say.
If you're willing to buy a book or two, Types and Programming Languages by Pierce is probably a good starting point for your first two avenues (and maybe the third). If you're rusty on proofs I'd recommend How To Prove It by Velleman before that (and it might help with Software Foundations as well).
We used optparse-applicative at my last job for its Bash completion support. The API is nice but doesn’t generalise well: it was difficult to do anything nontrivial (such as subcommands), and the generated help had issues (listing the same options and values multiple times).
Read TAPL before HoTT or even Software Foundations.
Homotopy Type Theory is very cool (I've just started working through it myself along with a reading group at Penn), but I would say that learning some practical dependent type theory (via Software Foundations, or playing with Agda or Idris or whatever) is pretty much a prerequisite for tackling the HoTT book, so I would start there if I were you.
The paper says that &gt; syntactic errors in the quotations and antiquotations as well as in their &gt; composition are flagged at compile time; i.e., we can be sure that the &gt; generated code is syntactically correct if we can compile our backend
 quickCheck haskellRocksYourSocks +++ OK, passed 100 tests.
I definitely thought this was going to be about some kind of huge parallel monad stack.
Awesome work. I'll have to give Sodium a spin. Something that I'm even more interested in though, how was your experience with cross compiling?
I’m so happy to see more talk about the application design side of FRP, as opposed to the principles and implementations of various libraries. For me, the more interesting open questions come from this space.
I think it's a good primer for someone exposed to a bit of type theory before (think lambda calculi, system F variants etc.), but for those who had never seen it, it would require a fair bit of mathematical maturity to wade through. Of course, I read that chapter already having learnt that stuff, so it's hard for me to gauge its difficulty.
13000 tests*
I agree. Developing a reactive library was not trivial, so only now am I able to escape my tunnel vision. I think reactive programming is a foundation for a whole lot of techniques we are yet to develop, so I'm looking forward to the exploration. "Reactive programming" is the marketing department's name. :) I use it because I think it's a better term than FRP for selling the idea to the mainstream.
woohoo Im about to try out xmonad on this ole lappertopper
As of now, the following classes which contain mutually recursive default method implementations (and therefore have non-trivial "minimal complete defintions") were augmented by `{-# MINIMAL #-}` annotations: * Bits * Eq * Foldable * Fractional * Num * MonadZip * Ord * Read * Show * Storable * Traversable If you know of any other classes which are bundled with GHC and might benefit from a `{-# MINIMAL #-}` annotation, please let us now! As an example, here's the annotation for [`class Bits`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Bits.html#t:Bits): {-# MINIMAL (.&amp;.), (.|.), xor, complement, (shift | (shiftL, shiftR)), (rotate | (rotateL, rotateR)), bitSize, bitSizeMaybe, isSigned, testBit, bit, popCount #-} 
CL can add dependent types trivially. Haskell turned into agda to do that...
Stand up for what you believe in man. If FP adds an awesome tool that's great. But if they are expecting normal programmers who are trying to learn Haskell to pay for basic information... they can go fuck themselves. If they can't realize the importance of being the watering hole enough to make it free they don't deserve the chance. 
I am so happy to see this! Great work everyone :)
I'd be curious to see dependent types in CL, do you have any link?
Ok, well, I’ll ask him then on IRC when I can. Thank you all.
He presumably means Qi, which is CL's Agda. * Qi is written in Common Lisp. Agda is written in Haskell. * Qi compiled to Common Lisp. Agda compiled to Haskell. * Qi could call back into Common Lisp. Agda can call back into Haskell. * Qi has dependent types. Agda has dependent types. * Qi has ML-like syntax. Agda has ML-like syntax. * Shen, Qi's successor, can compile to various backends. Agda can compile to various backends. Common Lispers mention Qi as a reason why Common Lisp is _so powerful you could add dependent types to it_, which seems misguided because it's not adding anything to Common Lisp, it's building something using CL as a compiler language and a target language. Nothing special about that. I'd link the source but it's proprietary.
I was just linked [this](http://blog.sigfpe.com/2008/12/mother-of-all-monads.html), using built-in continuation support to get monads in Racket, because the continuation is the mother of all monads. =)
At first I thought that number of reddit subscribers might be quite an interesting metrics of language popularity (relative to "number of google queries", for example), but then I saw /r/java and /r/scala are so small.
Subcommands work fine, in my experience. Can't say I have noticed any problem with the generated help either.
Nice. However, it would be cleaner if you had a type for your dice expressions with an instance of `read`. This way you could use `auto` instead of `string` and have optparse handle failure.
Can someone ELI5?
I'm in the same situation as the OP and am working through TaPL right now. Highly recommended.
It would be cool to see a chart of subscribers over time.
This adds a pragma that allows you to define what a "minimal complete definition" is for a typeclass. For example, if you had a typeclass class MyClass a where func1 :: a -&gt; a func2 :: a -&gt; a func3 :: a -&gt; (a, a) func3 a = (func1 a, func2 a) Then you could specify with this pragma that in order to make an instance of this typeclass, you only need to define `func1` and `func2`, since `func3` can be defined in terms of `func1` and `func2`.
In addition to the other explanations, I'm happy to see that finally, this code: instance Eq MyType will not silently build successfully and cause a runtime loop. Beginners I know have been bit by this and had a hard time debugging it. Also, I think I've had an incorrect definition of Ord that bit me at least once, and also resulted in a runtime infinite loop, without any compile-time warning or error.
Compiler pragmas are becoming too pervasive. They're becoming a necessary part of using the language, which is unfortunate. I predict they'll make the language ugly to read, much like overuse of c preprocessor statements can make c ugly to read. Not that I have a better idea in mind.
This is not a good motivating example for the new `{-# MINIMAL #-}` pragma, as GHC was already able to infer itself that `func1` and `func2` have no default implementation, and thus need to be defined explicitly. Here's what GHC 7.6.3 would tell you, if you defined a mere `instance MyClass MyType`: &lt;interactive&gt;:10:10: Warning: No explicit method or default declaration for `func1' In the instance declaration for `MyClass MyType' &lt;interactive&gt;:10:10: Warning: No explicit method or default declaration for `func2' In the instance declaration for `MyClass MyType' 
Just delete pragmas before reading the code.
This pragma is entirely to generate compilation warnings, and does not change the language in any way. Call it a GHC convenience feature. `MINIMAL` definitions are only part of typeclasses, so they don't clutter the code any more than say having a type signature. The other three "big" pragmas are INLINE, RULES and SCC, and they are used somewhat sparingly (the last one only in debugging scenarios). Therefore, I don't agree that the inter-code pragmas make code more hard to read.
That's a great idea. I'll have to give that a shot.
Excellent. Thank you!
Sometimes I wish CS had something like the Oxford English Dictionary as an official source of definitions. In my experience, the term "Reactive Programming" has too many meanings. * The Reactive Manifesto uses it as a soon-to-be methodology. * It could just mean a subset of dataflow where data is pushed and not pulled * Some use it to mean, programming with the .Net Reactive Extensions * And we also have "Functional Reactive Programming" All of dataflow is like a tower of babble with multiple terms meaning the one thing and one thing having multiple terms. I think this makes it difficult for newcomers to learn about dataflow. You have to understand the dataflow model before you can know if two terms are really the same or not. I agree that "Reactive Programming" is the best name for us to give it for the masses to not dismiss it as some old, "already done that", technology. 
This is correct, I was merely trying to simplify it down to what the pragma is to be used for at a fundamental level, even though for simple cases it isn't needed. 
Ross doesn't IRC. You'd need to email him.
I really liked this book. I've actually read it twice and highly recommend it to others interested in learning Haskell.
Shen at least doesn't have dependent types by default. Encoding dependent types using the sequent-calculus based type system makes type checking pretty god-damn slow. Like, unusable-beyond-toy-problems slow. Not to mention it's easy to render inconsistent, and lacks termination checking.
The wiki can be updated now when cabal 1.18 is out!
You can upgrade your example by just defining func1 = fst . func3 func2 = snd . func3 and then the `{-# MINIMAL #-}` pragma offers something the default warning about unimplemented methods doesn't.
&gt; necessary That's a pretty strong word.
Most nontrivial Haskell programs I write require several language extensions.
University of Waterloo has advanced intro-level courses that use Haskell instead of pseudocode, and use Racket as the main programming language.
can you provide some links of where to find this?
It's also a great shame that the halting problem can't be solved automatically. It'd be really useful!
Sure, you can implement things with bind and return-like functions, but the reason monads are of note is because of the general abstraction (whether ad-hoc or with type-classes).
Don't forget the one so large it's just part of the landscape now, LANGUAGE.
Edited my comment with a link.
But pure code can throw exceptions too, so how does this help? Or are there actually two types of exceptions?
It is impossible to state that [useful] code cannot throw an exception. Even if you try it will not be 100%
He means that it would not throw synchronous exceptions (i.e. `throwIO`).
I strongly suspect that his motivation behind this is that he wants to check all exceptions in something like `EitherT`. In other words: try' :: IO a -&gt; SafeIO (Either SomeException a) check :: IO a -&gt; EitherT SomeException SafeIO a check = EitherT . try' Currently, the only available solution is: check :: IO a -&gt; EitherT SomeException IO a check = EitherT . try ... but that doesn't prove in the types that all the exceptions have been checked, whereas `SafeIO` does.
UnexceptionalIO
But his code can't guarantee that, can it? `return (throwError SomeException) :: SafeIO a` Isn't this just a newtype to say "pretty-please, don't throw any exceptions here"?
See [my other comment](http://www.reddit.com/r/haskell/comments/1mpsvj/a_type_for_io_actions_that_do_not_throw_exceptions/ccbhq9u). This is a really useful way to prove that exceptions have been checked using `try`.
What's the use case for this? If you could guarantee that *no* exceptions would arrive in a block that's useful, but not what this does (since async exceptions aren't handled). I don't see how it's useful for most exceptions to be handled but not async exceptions (and if you're building something like a thread pool, `syncIO` is not a good way to do so regardless).
Thank you :D
That's an error, which GHC models as an exception, but it's not a normal IO-exception.
I bet GHC's existing (conservative) analysis for creating minimal recursive chunks of bindings would be enough to cover almost all cases.
Yes, and moreover, if you write code that then handles any possible exceptions, you get a value in just `SafeIO`, which will definitely not fail.
All the exception-throwing code is in `IO`, so this wouldn't work. You can still "throw" `error` and asynchronous exceptions will still happen, and there's a terrible hack in `Control.Exception` to allow exceptions from pure code, but those are seperate sorts of things.
This is a bit long, but I am considering it.
Either I'm missing something (very possible!) or we're working with different understandings of what "prove in the types" means. If you hand me a function of type "EitherT SomeException SafeIO a", I still can't be sure that all the (synchronous) exceptions are in the `Left` branch. Bottom is a member of every type, so it's a member of `SafeIO` too. I have to trust you to always use `check`, etc. (Or check the implementation). But if I trust you to do that, why not just trust you to use `catch`, `bracket`, and so on appropriately? 
Ooh, now let's move "join" into Monad with default impls for bind and join.
+1 for better comments.
Having no bottoms is a much stronger requirement than having no exceptions of the sort that represent an action failure and need to be handled (such as network error).
Dead link?
The correct link is http://source.mozillaopennews.org/en-US/articles/model-analysis/
Some other names I've brainstormed: * CaughtIO * CleanIO
You might be interrested in [this package](http://hackage.haskell.org/package/control-monad-exception-0.10.3.1/docs/Control-Monad-Exception.html), which also provides checked exceptions in haskell. 
Hm.... I would say no, I'd think that filtering preserves structure. I can't really think of anything that could be filtered except for maybe and list though.
that will probably be part of the post AMP definition of Monad (Haskell 2014). I would worry about doing it now because we don't really want to make `liftM` part of the monad class since that function is redundant anyways, but once we have a definition of `fmap` already this won't be an issue.
The mods should have access to that. They can even make it public if they want.
True. But it pays its way with puns ;).
As a relative newbie, can you explain why this is equivalent to the Halting Problem?
Thanks so much to Greg and the Docmunch team for their great work on this critical issue for users of Haskell in industry.
Yes, but that's unrelated to code clutter.
Oh, so the point isn't "no exceptions will be raised in the execution of this code" (as needed for critical sections etc) as it is "You can assume that all necessary exception handling has been done in this code"? That's reasonable enough. I still think you might run into some issues with `syncIO`. It re-throws `ArithException`, which I think in many cases you'd prefer to handle directly (depending on the needs of the program of course). Probably not too difficult to handle that where necessary however.
This is fantastic; thanks! Next step: Haddock should report MINIMAL pragmas in human readable form automatically, so we can finally get rid of all those manual "minimal complete definition" comments. EDIT: I now see that Twan himself - the author of this patch - suggests this Haddock feature in his comments in the Trac issue.
Or just `IO` and design for qualified imports.
You can't write IO actions that never throw an exception unless they always ignore any arguments they might take. Let's say you have a function `myaction :: Foo -&gt; SafeIO Bar`, then I can make it throw an exception by writing `myaction undefined`.
Read the rest of the comments. This has been answered.
fromList isn't "necessary", I was just trying to point out that, given `Monoid (f a)`, you can provide an implementation for "singleton" OR "fromList" OR unfold, and an implementation for the other two can be derived easily. Notice how `fromList` and `singleton` are defined in terms of each other? That means that when you write an instance of Unfoldable, you need to override the default definition for at least one of these. import qualified Data.Set as S instance Unfoldable [] a where fromList xs = xs instance (Ord a) =&gt; Unfoldable S.Set a where singleton = S.singleton fromList = S.fromList Also, I was trying to highlight the "duality" between Foldable (toList, fold) and Unfoldable (fromList, unfold). In theory, not all Foldables are Unfoldable, and vice versa, but in practice, a lot of useful data types *are*.
Are becoming? They've been pervasive for the whole 3 years I've been doing Haskell. *shrug*. Don't forget, though, that Haskell was born in academia, and GHC still serves as a playground for experimenting with new language extensions.
But language pragmas don't make the code uglier: they are once at the head of the file and you don't see them ever again.
RULES are used quite pervasively, at least for the kinds of datastructures I design...
Unexceptional is a bit long-winded. How about OrdinaryIO? ;)
Suppose I have a class: class C a where c1 :: ... -&gt; ... c1 = ...c2... c2 :: ... -&gt; ... c2 = ...c1... To determine that the cycle must be broken via a minimal definition overriding one or the other default implementaton, we'd have to determine whether `c1 x` halts for all `x` (or: whether `c2 y` halts for all `y`). If it does halt, then we're all good and so the minimal implementation is to do nothing. If it does not halt, then clearly one or both implementations must be overridden if halting behavior is desired.
Thanks for that. I took a brief look at .Net Reactive stuff (assuming it was similar to our definition) but couldn't make any sense out of it. There is a wikipedia page for reactive programming that talks about our definition, so that's a plus. Reactive-banana and sodium are almost identical at their core, even though they were developed independently. I think that proves that at least *functional* reactive programming is a clearly identifiable thing.
The code is somewhat large, so I will have to distill a testcase for you. /u/strager would know more about it, as well.
`ManagedIO`, `HandledIO`?
&gt; This pragma is entirely to generate compilation warnings I feel like they should give a compiler *error*, and that such a feature should be part of the language. Maybe I'm just overly worrisome but I really don't feel safe writing typeclasses with cyclic defaults unless the compiler will refuse to compile any incomplete definitions of instances.
Ooh. `HandledIO` sounds kind of neat
A nice explanation by a C++ expert on why he switched to Haskell.
Been using Haskell on and off for years and this one "safety" thing has been bugging the heck out of me the whole time. unsafePerformIO... It's bad, but people reach for it, and without a source audit and a compiler, how can you know your system is free of it? http://www.reddit.com/r/haskell/comments/15ur0k/a_question_regarding_unsafeperformio/ How can the Haskell community claim it is a safe language with such a back-door? 
How about grep -R unsafePerformIO * if you're worried about it (I'm not. It's used by a vanishingly small amount of code, usually written by experts)? 
The problem is it really isn't, as the actual choice of 'r' varies for each of the monads you want to work with. The usual answer from the 'mother of all monads' point of view is just not to use the wrong choices for 'r', which is rather unsatisfying and crash-prone. There are also monads that don't embed into the CPS formalism properly, Lazy writer in CPS form can't be used to build up infinite results.
I understood 0% of this post. Time to hit the books...
What about with shared libraries? Can it be detected there? Security in software is never about implicitly trusting people to do the right thing :-)
I use `unsafePerformIO` to do observable sharing inside the reverse mode of [ad](http://hackage.haskell.org/package/ad) to make it both asymptotically faster and faster by a constant factor than you can write naïvely. I use `unsafePerformIO` inside of [`lens`](http://hackage.haskell.org/package/lens) in `Data.Data.Lens` to make the generic programming traversals both asymptotically faster and faster by a constant factor than you can write naively. As I perform a Traversal I update a "hit map" of what types can occur inside another type, and use that to accelerate future traversals by skipping portions of the structure that can't contain something of a given type. This improves sharing, and drastically speeds up these traversals. I use `unsafeCoerce` inside of `lens` and `profunctor-extras` to avoid accumulating unnecessary eta-expansion wrappers that can lead to asymptotic slowdown in part due to ghc ticket [#7436](http://ghc.haskell.org/trac/ghc/ticket/7436), but which can happen otherwise due to the fact that `Foo . f` isn't the same as `f` when `Foo` is a newtype, but rather expands to the eta-expansion of `f`: `\x -&gt; f x` which has subtly different semantics. I use `unsafePerformIO` inside [`ersatz`](http://hackage.haskell.org/package/ersatz) to observe sharing on the structure of a `Bit` expression to generate smaller SAT models and provide a much nicer API. The use is not visible to the user, but that insight requires knowledge of how SAT solvers work. I use `unsafePerformIO` inside [`bytes`](http://hackage.haskell.org/package/bytes) to enable me to use a `Storable` instance in a pure setting to get the serialization of the data. This is provably safe and could be written in mere ST if there was an ST annotated ForeignPtr analogue. I use `unsafeCoerce` in [`reflection`](http://hackage.haskell.org/package/reflection) to build a dictionary out of a value in local scope, which is necessary to .. you guessed it, give you the right asymptotics when you need to work with something like a modulus for modular arithmetic defined at runtime, or work with a monoid of the tabulations of a particular DFA. The "slow path" in this package uses `unsafePerformIO` to carefully sequence operations that let me attain the same result by reifying a `StablePtr` into a type temporarily, achieving the same results by different means. I use `unsafeCoerce` inside of my revision control monad to work with a heterogeneous map inside of my environment. There is no way to write that code otherwise. Andrea Vezzosi has proven that you can build a version of that `Map` in a restricted universe using dependent types in Agda, but Haskell is not Agda. My `intern` package uses `unsafePerformIO` to initialize and secretly update a hash-cons table behind the scenes, enabling me to improve the asymptotics of comparisons to `O(1)` and get better sharing. I have other examples, and you are free not to use any of these packages if this makes you uncomfortable, but I've needed every single one of them.
Yes, with Safe Haskell.
&gt; due to the fact that Foo . f isn't the same as f when Foo is a newtype, but rather expands to the eta-expansion of f: \x -&gt; f x which has subtly different semantics. I'd like to know more about this. AFAIU `f` and `\x -&gt; f x` are only denotationally distuguishable by using seq. Perhaps you are talking about some sort of subtly different operational semantics.
edward has lots of fancy somewhat internal-to-haskell examples below. but the original motivation is simpler and more obvious, and explains why `unsafePerformIO` came to Haskell initially as part of the foreign function interface. You can find its use explained both here: https://en.wikibooks.org/wiki/Haskell/FFI and here: http://book.realworldhaskell.org/read/interfacing-with-c-the-ffi.html
I have an interesting case in my [star semi-ring post](http://r6.ca/blog/20110808T035622Z.html): class Semiring a =&gt; StarSemiring a where star :: a -&gt; a star a = one &lt;+&gt; plus a plus :: a -&gt; a plus a = a &lt;.&gt; star a Although I don't mention in in the post, the definition has been contrived so that if `&lt;+&gt;` and `&lt;.&gt;` are suitably lazy in their second argument, then the default recursive definitions of `star` and `plus` work. I'm not sure, but presumably I shouldn't require any `MINIMAL` pragma here, even though the recursive definitions don't always work. P.S. (`some` and `many` should be moved to their own typeclass).
That is part of the issue, but as shown by [#7436](http://ghc.haskell.org/trac/ghc/ticket/7436) accumulating a lot of them is quite bad. Folks don't factor the cost of the wrapper in because "they are the same" but if I accrete `n` of them to go `n` levels deep in a structure, then all of a sudden `fmap` becomes _O(n^2 )_ =/ For the rest of `lens` it is mostly just so we can say: over mapped f = fmap f rather than the less satisfying over mapped f = fmap (\x -&gt; f x) which is as you noted otherwise distinguishable only via `seq`.
&gt; shown by #7436 accumulating a lot of them is quite bad &gt; You might wonder why GHC doesn't eta-reduce (\e -&gt; s e) to s. The reason is that doing so is unsound if s is bottom; then eta-reduction might turn a terminating program into a non-terminating one. FUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU... 
So worth it! XMonad+xmobar+xfce4-power-manager+nm-applet+stalonetray is pretty awesome.
You really can't get a better endorsement to switch from C++ to Haskel than from Bartosz Milewski (except maybe Herb Sutter). I've learned so much about C++ from him, and now I'm learning a bunch of Haskell from him. ~~FYI, he's working for [FPComplete](https://www.fpcomplete.com/user/bartosz) right now.~~
Awesome :) Scotty is my go-to Haskell web-framework these days unless I know I'm going to have special requirements. It's the only one I've used that has consistently managed to avoid creating Cabal-hell. That, and the dead-simple Sinatra-inspired API make it a joy to use.
Actually Bartosz is no longer working for FPComplete, but he was a little earlier.
Ah, I wasn't sure. He job hops a lot.
Awesome! :D Scotty is probably the best web framework for quickly building websites(when used together with blaze-html and acid-state).
This was admittedly not my most accessible post. The gist is that one of the ways of dealing with substitution and name capture is called PHOAS, and if we split it apart a bit, we get something we can recognize as a free monad. It is mostly targeted at people who already use something like PHOAS in their work. If I get a chance, maybe I'll go back through and do a follow up post with some examples like cps transformation or a pretty printer. Using the approach given in the post, you can implement a pretty printer using something like: pretty :: (forall x. Exp x x) -&gt; String pretty b = cata phi b vars 0 "" where phi (App f g) xs d = showParen (d &gt; 10) $ f xs 11 . showChar ' ' . g xs 10 phi (Lam h) (x:xs) d = showParen (d &gt; -1) $ showString "\\ " . showString x . showString " -&gt; " . h (\ _ _ -&gt; showString x) xs (-1) vars = return &lt;$&gt; ['a'..'z']
It's not called unsafePerformIO any more. It's called smuggle now :-)
If C++ is Edward Scissorhands then Python is Dr Zoidberg.
Well, you download the code to compile it, so yes.
That's only on Talk Like A Pirate Day, no?
This is my goal, but it seems that it is not shared by every GHC dev: &gt; Finally, the very existence of Coercible is going to eliminate most of these (map Age) calls anyhow (http://ghc.haskell.org/trac/ghc/ticket/2110#comment:39). Raise your support in the ticket if you care about it! ;-)
The thing is, Haskell with `unsafePerformIO` is not the same as your average impure language. It's not as if when I need to add some updating of complicated mutable global state, I can just take my big pure function and hide `unsafePerformIO (modifyIORef x ...)` in it, because that most likely won't do what I want it to, even if my goal is to hack around my original bad design. Lazy evaluation makes it such that it usually doesn't actually even _make sense_ to try to throw side effects into your pure function, because you don't know when they're going to happen. So unlike Scala or OCaml or Scheme or whatever other language that lets me just throw an effect in here and there, `unsafePerformIO` is not a viable approach to do anything much that's nonsensical in pure functions. Sure, some brilliant programmer might figure out a heinous scheme that involves lazy mutation that actually affects the runtime of the program in an intended manner, but it's a lot less likely than e.g., a Scala programmer adding a `var` to their `object` and twiddling it with every invocation of the function. And that's not even to mention the fact that GHC can and will rearrange your program in crazy ways that will break your use of `unsafePerformIO` unless you respect its contract, and there's no way around that. __tl;dr__: `unsafePerformIO` doesn't make Haskell into your average programming language. We avoid it primarily because it doesn't usually make any sense to be using it, not because it has a scary-sounding name or because of cultural pressure.
&gt; I use unsafeCoerce inside of my revision control monad to work with a heterogeneous map inside of my environment. It's actually possible to do that with `unsafePerformIO` -- IORefs are great for circumventing the type system. I've made a package for heterogenous maps: [vault][]. [vault]: http://hackage.haskell.org/package/vault
You should alter you cabal file to add a line like this : ghc-prof-options: -auto-all -caf-all in the executable part. The reason you can't profile is that all your code is in a single cost center : MAIN.
I'm just lazily waiting for it to catch on!
Surely you mean: smuggle = unsafePerformIO . (print "Arrrrr! Avast Ye!!" &gt;&gt;)
Laziness strikes again!
Actually - scratch that this isn't about safe IO.
&gt; You might wonder why GHC doesn't eta-reduce (\e -&gt; s e) to s. The reason is that doing so is unsound if s is bottom; then eta-reduction might turn a terminating program into a non-terminating one. Ah, but isn't it always safe to eta-reduce `\e -&gt; (\e' -&gt; s e') e` to `\e -&gt; s e`? Seem like there could be a useful optimisation in here.
Ah maybe it's because it has *two* lambdas at the front that it can't be done by normal strictness analysis. 
i was under that impression as well, though I also was under the impression that FP Complete seems to have several founders.
Nah, that was uncalled for. Do note that the FP School of Haskell is free for readers and will stay that way, because (as of recently) the texts are licensed under a creative commons license to the general public. There's just a subtle mismatch of incentives (personal vs. "community") that makes me not want to contribute as an author, even though I'd like to.
`edwardk` what smooth sales pitch justifies this use of `unsafePerformIO` ?http://hackage.haskell.org/packages/archive/hyphenation/0.3/doc/html/src/Text-Hyphenation-Language.html The same problem is handled by other libraries without this sort of thing; see for example highlighting kate which perhaps goes too far in the other direction. The example bugs me because I was searching Hackage for uses of `unsafePerformIO` to show `leimy` that it is mostly used as it should be -- a pleasantly large proportion of uses of the word "unsafePerformIO" are in preening comments "... without unsafePerformIO!". `hyphenation` stood out in the google search as the first example of an 'ordinary' library using `unsafePerformIO` to get over a few simple hurdles, not some deep dark construction of primitives and the like. I don't see that the uses in `lens` have been adequately defended at all. It is quite different from `hyphenate` but not the sort of library from which we expect this (`unsafeCoerce` is a different subject); its few uses of `unsafePerformIO` should be open and discussed in the documentation.
Though obviously an overloaded term, I like `LinearIO` to emphasize the reduced amount of non local jumping.
I'm fairly sure the quote relies on using `seq`.
It seems like he defended its use in `lens` pretty well; if you take issue with the points he brought up, why not address them directly (or bring up other specific criticisms)? I do agree that the `unsafePerformIO` usage in the hyphenation library seems unnecessary, but perhaps it has to do with lazy IO (laziness being the double-edged sword that it is).
The problem with this is that Haddock does not qualify types in type signatures, so you wouldn't be able to tell at a glance which `IO` a function used until you hovered your mouse over the hyperlink. This is the same problem you get with strict `ByteString` verus lazy `ByteString`s in types.
Not self-defining. If you don't already know what it means, the name doesn't give any useful hints.
All of them are.
The monad very explicitly needs to run outside of `IO` using sparks or you wind up having to explicitly collect unused tasks, which renders the API hideous to use in practice, as was noted by Daan Leijen on his version, which _is_ written that way. Being able to run as sparks matters because it means I can safely garbage collect a spawned Task once it is clear that I'm not going to run it. Living in the spark queue means I can't run on `IO`. Sadly, due to the recursive nature of the environments I can't use `ST s` there, either.
SafeHaskell is written so you can run Haskell in an environment where you absolutely need to know the user absolutely can't subvert the type system and segfault you. E.g. an IRC bot that lets anyone run arbitrary code for 3 seconds, like lambdabot! Generally we work in a slightly more liberal manner. We're willing to provide you with `unsafeRead` operations in an API that remove the bounds check and move the obligation to the user, because some subset of users will need the extra performance gain they get from having a branchless code path, and can prove their function locally won't step out of bounds. I use them fairly liberally after spending a lot of time proving things about my bounds, because I wind up writing a lot of performance sensitive code, but the use of such a function isn't "Safe". You can climb on a moral high horse that one shouldn't write that code and that one should make the compiler smarter until you don't need to, but I'm not going to stop writing software until one exists.
Ask yourself: Safe against _what_? Code can't just be abstractly "safe". In this case, it's [safe against this](http://www.haskell.org/ghc/docs/7.4.2/html/users_guide/safe-haskell.html): &gt; It allows for unsafe code to be securely included in a trusted code base by restricting the features of GHC Haskell the code is allowed to use. Put simply, it makes the types of programs trustable. Safe Haskell is aimed to be as minimal as possible while still providing strong enough guarantees about compiled Haskell code for more advance secure systems to be built on top of it. In this case, the reason why Safe Haskell does not permit unsafePerformIO is not actually the same "safe" as in the function name itself. The problem isn't that unsafePerformIO may violate abstractions or generally crash things, the problem is that a user may use it to violate the security of the sandbox. Which may happen to include violating abstractions or crashing things, but includes a lot of other things as well.
As for `lens`. I invite you to supply a version of `Data.Data.Lens` that performs as well, but doesn't use `unsafePerformIO`. I also look forward to finding out how one can make up a `Handler` from an arbitrary using the existing API for `Control.Exception`. We use it in both of these places as an escape hatch to write code that can't be written otherwise that isn't obvious to write, and expose it with a simple wrapper for easy end user consumption. Much of lens is predicated on us going off and writing gory systems code so that the user doesn't have to, and then generalizing our combinators so they work in as many situations as possible. The general eta-expansion wrapper concern is of a different flavor, but we spent over a month chewing it over before we took this as the fix, fed the issue to GHC HQ, got patches into mainline GHC to fix the Foldable/Traversable, etc. regressions that were affecting everyone, got SPJ to fix it so that strict function composition can optimize away, and perhaps some day when 7.8 is the lowest thing on our support window, we can eliminate more of them. I'm also hopeful that the new `Coercible` machinery will eventually let us eliminate more of them, but we've agreed to support a version of lens that provides support for the last 3 major versions of GHC. Currently that is `lens 3.7.x` which supports GHC 7.0.4+ and `lens 3.9` for `7.4+`, so even as these things get fixed in GHC HEAD, we've got 2-3 years of support left in us before we can uniformly adopt them.
I will. =) Wren Thornton made a good argument for `seq` on functions for loop-invariant code motion back when we ran a workshop on domain specific languages up at McMaster University in his talk on [Probability Smoothing](http://www.youtube.com/watch?v=O-M1q4tX4do). He showed that the algorithms most people cite for doing probability smoothing have worse asymptotics if written as described. He lifts invariants out of inner loops by using this. The issue of loop invariant code motion comes up at [28m](http://www.youtube.com/watch?v=O-M1q4tX4do&amp;t=28m) and in the second part he uses it around [6m](http://www.youtube.com/watch?v=KHZgXkwJXAQ&amp;t=6m) to shave log factors.
Got it. Then I don't really object, I guess, although I'm not sure it's fair to blame eta reduction for the weirdness introduced by `seq`.
True. ;) Now to figure out where the heck those files went. =)
Well, I haven't seen another statement quite as succinct, and I really don't think you've answered it satisfactorily. The problem with Haskell's exception mechanism is that exceptions can be transported around in thunks and ultimately express themselves in locations where nothing throws an exception. And that's not just an implementation detail.
`{-# RULES "HAI; CAN HAS STDIO?" id = unsafePerformIO (system "killall -9 breathingMachine &amp;&amp; xeyes &amp;" &gt;&gt; return id) #-}`
I have yet to see a haskell library that does not come with source code :) 
At first I was skeptical of the point of Scotty, because the hello world example is as short as in Snap. But its dependency footprint is quite light.
You're still talking about "exceptions" as including everything from `Control.Exception`, including `error` calls, exceptions from pure code, etc. Exceptions thrown using `throwIO` or produced using the classic `Haskell98` IO exception system live in `IO` and are sequenced in `IO` and are handled in `IO`. If you handle all such IO-bound exceptions on an IO action (using `try` or `catch` or whatever) then you are left with an IO action that will not produce any IO-bound exceptions. There may be asynchronous exceptions in the future, but no IO-bound exceptions. The type, however, still says `IO` which means upstream code has no way of knowing that you already handled the exceptions (or what exceptions there might have been, etc). The idea with this library is to have a way to make two things explicit (a) when all IO-bound exceptions have been handled and (b) what exceptions there are in an action. It would actually be possible to implement a Haskell with something like: type IO = EitherT SomeException UnexceptionalIO Which is the relationship that my `runEitherIO` function depicts. If you know you are using, say, `openFile` and that it only throws `IOException`, you could even use `fromIO'` to make and `openFile'` in `EitherT IOException UnexceptionalIO`
Things like ByteString builders are pretty compelling to me. Perhaps it would be nice to have better safety rails when doing that sort of thing, but the Haskell community thankfully takes performance very seriously. Overall, I think `unsafePerformIO` has actually been instrumental to Haskell's adoption due to the way it makes it possible to have FFI pieces that aren't unduly punished for not being written in Haskell. 
Everybody loves Zoidberg
TameIO? TransparentIO? ResolvedIO? InceptionIO? bwwaaaaaa
I liked the intent here, but agree that LinearIO is a non-starter. And I don't see other options either. StackIO? StraightLineIO? TameIO is the best idea I had, I think.
Ack! Is this causing problems for you? I stopped putting upper bounds on deps because they tend to cause headaches for me, as the package maintainer, (I have to make minor releases just to bump dependency bounds) and for users who are inexperienced with cabal, when scotty tries to force an older version to be installed. That said, if the lack of upper bounds is causing a problem, I can certainly address it. Given the choice of headaches for me and headaches for you, I'll gladly self-inflict. :-) I should point out that scotty is not alone in this. There was a thread a while back on the topic, but I'm not sure if any consensus was reached. There were downsides both ways, but the downsides to not having upper bounds seemed more rare.
There are a couple of things I couldn't figure out from this tutorial when comparing Groundhog to my use of mysql-simple in a couple of small Haskell applications. 1) Does Groundhog support mapping arbitrary results to data types (e.g. the results of a join or a subset of columns in the table) and in general mapping of the same table column to more than one data type? 2) Does Groundhog support arbitrarily complex SQL queries or only a subset of SQLs power?
I meant to add that it was *the compiler* that told me `farsi` was undefined when I tried to write a preprocessor to follow something like the highlighting-kate path. `unsafePerformIO` permits us, among other things, to finesse the existence of a genuine binding. An admittedly hideous file like this http://sprunge.us/FFch (the unparsed 'data' for English) takes about 35kb, compiled; all of them together take about 2.5 mb (the text files you are storing are 3.0mb). By contrast, the files in `dist/build/Text/Highlighting/Kate/Syntax` seem to add up to 64 mb, since there are almost a hundred of them, and each contains a fairly complicated little parser (with the same dependence on someone else's files). I'm not sure what would happen if the actual hyphenator was defined in a separate module for each language.
Ultimately GHC is pretty bad at splitting up object files. Linking a library brings in the whole beast, then since these things wind up getting mashed into hashmaps and things the whole result winds up pingponging around in the garbage collector unnecessarily. I'll happily take the lump that I screwed up an invariant. I'm not sure I'm willing to extrapolate from there that it was a bad idea to implement this this way. ;)
That gives me the infinitely more helpful http://i.joelt.io/auto-all.txt
Yeah, it's a general problem that most Haskell source files are more readable and navigatable than the generated haddock.
ftp://ftp.cs.utexas.edu/pub/boyer/diss/akers.pdf I don't know if anyone uses it, but that paper helped pave the way for ACL2. What's with the downvotes and airplane food? Calm your nips people.
 This is an essential read, though you safely skip the few chapters on subtyping. Pierce seems to really overstate the importance of subtyping since he was writing TaPL at the height of the Java era.
the problem comes in that if you do something like fmap f (Node a as) = Node (f a) (fmap (fmap (\x -&gt; f x)) as) then you accumulate a non-trivial number of those wrappers. This is what the GHC bug was with DeriveFunctor, DeriveFoldable and DerivezTraversable, as I presume the eta-expansion was to try to provide additional opportunities for specialization and inlining to fire, but the administrative eta-expansion wrappers we usually ignore started having asymptotic impact. If we build a strict composition we can work around this semantically, but not practically. Currently GHC makes code using strict composition about 10x slower. In response to this issue SPJ put in code that let us optimize down strict composition `Newtype #. f` to `f`, but that won't help lens for another couple of years when it is ubiquitous enough to matter.
Great work, reading through the source is a joy. It's so concise and minimalistic and I really like that no TemplateHaskell is required.
&gt; Ack! Is this causing problems for you? Not yet, but the Cabal solver isn't designed for packages telling lies about their version constraints by being overly lax. For instance, should you ever release a new version of `scotty` with stricter version constraints than its predecessor, Cabal will happily backtrack to an older broken version with weaker version constraints. Moreover, you're shifting the burden of validating newer versions of the build-deps on your users, so the time you save yourself by not having to keep your version constraints up-to-date is now occurring downstream multiplied by the number of your active users. AFAIK there was no real consensus, but it's a big problem if the two conventions clash on Hackage, as both camps lose. Both sides have good arguments, they might even be on par, but I'd follow what the infrastructure, i.e. Cabal, was designed for, and be honest about the build dependencies.
 so much nice much explanation wow much switched so haskell 
1) You can cast raw SQL query to any datatype. It is unsafe, but still gives automatic unmarshaling. http://github.com/lykahb/groundhog/blob/master/examples/rawQueries.hs The preferred solution is to use safe functions for queries from PersistBackend class and do raw SQL for more complex stuff. 2) Groundhog supports only simple SQL queries like projecting several columns from one table and complex expressions (arithmetics, PostgreSQL arrays operators, string manipulation, etc).
Nobody loves Zoidberg
StraightIO is nice
I'm aware that the proof was in the other direction; but there the question about whether there exists an algorithm that requires mutation or runs a log factor slower in a *lazy* language without mutation is an open problem. You are either purporting to have solved this problem, or you don't know if there may be Haskell solutions that avoid unsafePerformIO and maintain the asymptotic performance. I hope it is the later.
Perhaps it shouldn't be referred to as "relational-mapping", given that joins are the only way to express a relational model in SQL databases. If I can't use joins, then I am just mapping from data types to maps.
Is it actually much smaller than happstack or snap when you count up the dependencies recursively? (Is there a simple way to find that info without actually installing them all?)
As far as i remember, the rift between conduit and pipes was along performance+convenience vs correctness line. With recent sweeping changes to pipes and growing ecosystem of useful libraries around it, what is the current status quo on the divide between conduit and pipes? Would haskell community benefit from coalescing around one "blessed" streaming library? 
More like `seq` strikes again!
Can you explain that to me?
As far as I'm aware, all that the Report says is that `seq x y` is `_|_` if x is `_|_` and y otherwise. The tricky bit is when x is a function. If we want `const _|_` to be `_|_` then, the implementation of seq needs to be able to decide generically whether `x v = _|_` for every v, and an arbitrary function x. That's not decidable. So the presence of seq means that we need to consider `_|_` and `const _|_` as distinct values.
I think a simple solution in the short term would be to release an adapter library to convert between them. A couple of people that I've talked to have been doing this internally in their projects and they said that it worked well for them because the two libraries are similar.
It is a different thing here. I'm saying there is stuff you can't write idiomatically in Haskell as a `Num` instance without observable sharing without suffering that slowdown. I'm not using mutation for those speedups. I'm saying that if you are given a directed graph and can only see it as a tree, then you can't write code that hits the same asymptotics as otherwise. Explicit lets work and provide one solution, but they also prevent you from leveraging existing code like `(^)` on the resulting types. These are _Much_ weaker claims. Observing sharing isn't mutation. It just lets me know reference equality.
`seq` was introduced to fix space leaks, and was originally tucked behind a class. This is how we got datatype contexts back in the day because it used to be needed for strictness annotations on the data type. This was simplified away, and every data type became `seq`'able, because plumbing all the instances of it into the right places turned out to be a nightmare in practice. This has the unfortunate consequence that you can distinguish many things in Haskell now based on their strictness properties that would otherwise be canonically the same. `seq` is the villain in this story, not laziness per se.
Reminds me of the [recent Simon Marlow's talk](https://github.com/meiersi/HaskellerZ/blob/master/meetups/20130829-FPAfternoon_The_Haxl_Project_at_Facebook/The%20Haxl%20Project%20at%20Facebook.pdf?raw=true), where he used the free monad for something similar. See slides 37-38. Of course, as he notes, it's an old idea.
No idea, I only know because he comes to the Seattle HUG. He never went into detail about his decision process.
Fortunately this looks like it will no longer be necessary in Hackage 2; you can adjust dependencies after the fact.
Thanks, but I don't think that responds to my questions. My question is whether `(\x -&gt; _|_) x` and `_|_` could be considered "the same" and still be a valid interpretation of Haskell. Specifically I am imagining a Haskell compiler that compiles to super-combinators and maps both `(\x -&gt; f x)` and `f` to the same value (though I'm not certain such a compiler would do this). Edit: I wrote my question incorrectly. See below.
^ mangled :) 
Yes, if you have a halting oracle? `_|_` and `(\x -&gt; _|_)` produce the same result when applied to any value, so if it weren't for seq, it would make sense to treat them as equal. It's just that if you start treating all functions which produce `_|_` when applied to any argument as being equal to `_|_`, then seq's specification is much harder (i.e. impossible) to implement. So a semantics for Haskell with seq has to distinguish them in order to be practical to implement. I suppose you could make an arbitrary distinction and special case `(\x -&gt; _|_)` differently from things like `(\x -&gt; if someArbitraryCondition then () else _|_)`, but I suspect that's going in the opposite direction from where you'd like to go.
 | Abs (Exp a) (Exp a) is that supposed to be | App (Exp a) (Exp a) ??
Sorry I miswrote what I wanted to ask. I want to know whether `(\x -&gt; _|_ x)` and `_|_` could be considered "the same". Edit: I see now that cgibbard did answer my question because `(\x -&gt; _|_ x)` and `(\x -&gt; _|_)` must be equal owing to the fact that `_|_ x` and `_|_` must be equal.
Supprisingly, I've got something like 70% of a post, though I do not use PHOAS in my work, nor do extensive category theory studying. That's because I've read a dozen of ekmett (and others) articles till now, and I'm slowly getting in into his language, much like watching Brazilian soap operas a lot makes you learn portuguese. Though I'd like to request more Abstraction -&gt; Concrete or Concrete -&gt; Abstraction articles with accent on Concrete, and not only Abstraction -&gt; Deeper abstraction.
Yep
Agreed. I've been on a "dragging theory into practice" kick lately and prefer those posts. This was more something I thought of that I wanted to try, that worked, so I figured I should write it up before it faded into a forgotten project folder never to surface again.
I don't think I believe this explanation. From what I recall when I went and looked, the asymptotic wins came from lifting computations past certain binding sites. That computes them asymptotically fewer times. I.E. writing: f x = let z = e[x] in \y -&gt; ... instead of: f x y = let z = e[x] in ... (An optimization that GHC won't do, by the way; it doesn't break up arg lists.) `seq` might get you better constant factors there, but I wasn't convinced it was necessary for better asymptotics.
The seq is for constant factors, whereas the lifting gets you asymptotics, yes. The seq lets iteration skip worrying about seqing the hoisted values. That is what I meant, if perhaps not what I said. :)
Several people are already working on both http clients and servers based on `pipes`. Join the [mailing list](https://groups.google.com/forum/m/#!forum/haskell-pipes) if you want to contribute.
Subscribed :)
I would say a personal project uploaded to hackage is essential. 
K thank you, for your work. O/t I would like tags on hackage Eg safe haskell, haskell 98,
 goldbachConjecture = any checkGoldachConjecture [m | m &lt;- [2,4..]] c1 = if goldbachConjecture then c2 else return True All of a sudden, to know with 100% accuracy whether `c1` calls `c2` or not we must solve the Goldbach conjecture. This is the problem with spots where you have general computation: you can *never* tell what it's going to do. But I agree with you that a conservative analysis is enough for almost all use cases (and said so in my comment above).
&gt; how to lift a function of type ByteString -&gt; ByteString (both lazy bytestrings) to Producer ByteString m r -&gt; Producer ByteString m r Isn't that just: map :: (Monad m) =&gt; (ByteString -&gt; ByteString) -&gt; Producer ByteString m r -&gt; Producer ByteString m r map f producer = for producer (yield . f)
Obviously, Haskell experience will be a big plus. **That being said**, there are so few good, experienced haskell programmers available, that any company using Haskell probably expects to have to train you. If I were hiring I would look for people who dabble with a lot of languages and have been exposed to some diffrent paradigms.
&gt; I don't know of a good way to automatically lift list functions to FreeT. I don't think you can. The only way to do it would be to stream out to `[]` and then come back, which you can do for a particular functor, but not in general (since it requires "extracting" the data from the effects). Moreover, even on a particular functor, this will result in a loss of laziness, unless you use a particular base monad as well and use `unsafeInterleaveIO` or similar.
The function `tel` had in mind had a different meaning. The function was only supposed to be applied once to the entire contents of the `Producer` as a whole, not to each chunk.
Surely C++ is a more common language in finance/banking than Haskell?
As someone who's been looking to hire a haskeller over the last couple of weeks, I would say its completely opposite to this in London. What you have here is a pool of self selected excellent developers (flagged by knowing haskell) who would love to do haskell for a job, but hardly anyone hiring for it. Also the learning curve is such that I wouldn't consider training someone in haskell from scratch - that's very much something I would expect who wants a job to have done themselves.
It is for now, however I can say for sure that has I been allowed to build the bits of software I wrote for the last hedge fund I worked at in haskell it would have been "better" that what I wrote in python. I think more organisations will start to realise this over the coming years.
Just having a decent level of haskell on your CV can be enough to put you on the top of the stack, even if its not haskell that your going to be doing.
https://www.fpcomplete.com/business/about/leadership/
Aye, maybe so, but the OP listed their dream job as "working in the electronic trading world" not "developing Haskell". Learning Haskell - a language used in finance but currently very much a minority one - to achieve the goal of working in electronic trading doesn't seem like an optimal path.
Plug: a "type class mechanism" would not be the right approach anyway. What is needed is a more fine-grained annotated type system, as in http://dx.doi.org/10.1007/s00236-011-0136-9
&gt; I will. =) Would your concern go away if it weren't an all-or-nothing situation (either having seq on all function types as currently in Haskell, or outlawing seq on functions once and for all), but instead something more fine-grained like the annotated type system I link to above? (BTW, author copy at http://www.iai.uni-bonn.de/~jv/SV11a.html) 
So it would fold the producer down to a single bytestring, apply the function, and then have a new producer that just emits that single bytestring? Yeah, I can see how you might lose laziness again :)
Out of curiosity, what's the demand for Haskell programmers like?
I'm very curious if that could be made a practical part of Haskell some day. I'm not sure if the notational costs are worth it, but that it is definitely an interesting point in the design space. The pragmatist in me tends to think it'd never happen to Haskell =/, though for a new language it strikes me as a good direction to explore for this problem. I wonder if we should look into it for Ermine.
Yes. I was hired for my last job with (in retrospect) very limited Haskell experience, to work on a compiler and (later) a build system. In general, it seems domain knowledge is most important, because any competent programmer can pick up missing technologies as they go, but domain expertise takes real effort and a significant time investment to develop. Would you believe that before being hired, I hadn’t ever written my own monad, or even used a monad transformer? But I got the job anyway! Now these things are second nature. If you try hard and immerse yourself, you will learn by necessity. (Or get fired, but let’s be positive.) Of course, it can’t hurt to learn a few things about statically typed functional programming on your own time as you go around applying for jobs. Haskell, OCaml, Scala, and F# all have a good amount of inter-transferable knowledge. 
I don't know what the job market is like for individual roles, but I am seeing more and more haskell trading boutiques cropping up like Tsuru. A few people are `getting` the combination of trading and haskell but the market as a whole is still besotted with python. Who knows whether that turns into a flood. I would definitely look out for opportunities in C++ trading environments with the possibility of talking them towards haskell. I don't think you can dabble in haskell - you really can't go it alone and that means you need to jump right into the community to give it a real go. I don't think you need a hackage personal project or anything, and it's probably way too soon. As a core library for our project, I try and help out with the pipes project where ever I can, but am getting a lot more from the pipes tribe than I can give back. From doing this, I'm getting to know other hackers in my zone of interest, and some of them might even be hiring one day ;) A few of us are learning haskell (myself from scratch) by coding up a trading runtime (and some of us used to be C++ professionals even before we drunk too much of the koolaid). We lost one of our team to an investment bank C++/R development environment a few weeks ago and he reports being a much better programmer having had a few months haskell thinking time. 
When are you going to start blogging again Tony?
I don't think the market is poised for "average" or starting out Haskell programmers yet. So few people are using it that even with the small pool of Haskell talent companies can take their pick of the litter. Just being a Haskell shop is such a progressive move at this point that such companies are probably pretty elite and innovative, and wouldn't spare much expense to bring in top talent. However, if/when a Haskell critical mass occurs, the market is going to look a lot different for Haskell and other functional languages. There will be room for the Haskell-competent as well as the top hackage contributor. By that time I've gone from Haskeller wanna-be to decent-but-no-Simon-Peyton and I blow this popsicle stand! Nice knowing you, C++/C#/Java!! Hahah! Or, something like that.
If hackage didn't make package metadata such as version ranges for dependencies a part of the package then it could be updated separately (potentially by different people). Information such as which versions are deprecated, which is the current recommended stable version etc could also be stored outside the package itself and used by cabal. It does seem silly to require new releases just because other libraries have released new (but compatible) versions.
&gt; As someone who's been looking to hire a haskeller over the last couple of weeks Where have you been looking? I haven't seen this opportunity on Haskell-Cafe or haskellers.com. Or was it just so easy to find someone that you didn't have to bother advertising?
I think this is also related to provide a function `([a] -&gt; [b]) -&gt; Producer a m r -&gt; Producer b m r`, which should be equally difficult to implement with the expected laziness properties. But have you considered adding more utility functions to `pipes-parse`? Like `lengthFree`, `mapFree`, `filterFree` and the various other folds?
It's not *literally* essential because I got a Haskell contract without any public Haskell code. I did however have a long track record of software development, and domain experience relating to the contract in question, as well as private Haskell code and public code in Python.
Thanks!
I rather think the point of SafeHaskell is to turn existing abstractions into such sandboxes. SafeHaskell alone doesn't forbid IO; it just gives you confidence that code can't violate abstractions like IO.
Hurray for new ScottyT! Seriously pushes it in the land of HappStack and Snap.
Misses a comparison to any older system...
It doesn't have to be hackage, but if you have Haskell code that you are willing to share that is not on hackage, then shame on you. 
Getting a package onto Hackage shows that you can deliver. Helps weed out those who can't finish projects.
SafeHaskell is per-module, which Hackage displays in the Haddock documentation. As for Haskell98 the convention is (again, per-module) to use the "portability" module header: -- | -- Portability: Haskell98 module ... This is displayed by Haddock in the same place as the SafeHaskell status; i.e. in the top right corner of each module page. It would be nice though if Hackage would display the Cabal fields that are purely for documentation, like `other-extensions`. And that people would write that field more often. Or that Hackage could scan the sources and add to that list, just like `cabal init` already does.
I posted a comment. It’s interesting, but I’ve been noticing a special pattern in such talks for a while. Every times, people talking about languages that way – exposing drawbacks of a A language using B language’s advantages – are into that B language and don’t fully understand the A language – read here: Python / Haskell. I think it makes the talk less relevant that it could be.
While this is certainly true, other factors may of course still be overriding. If you have knowledge in some particular domain, and no haskeller can be found with the required competence in that area, they may still opt for training you in haskell. If you have both, that's ideal of course.
Except that not all Haskell written is a library or even of common interest. I have a couple of programs that would just be spam to others if I published them somewhere.
"Some languages are easier for newbies to understand than other languages. Python and Java are easy to understand even if you don't know them. [..]" I disagree. I'm most familiar with Haskell, and I don't find it easy to read Python or Java. I think this is that you are more used to OOP, therefor understand languages in that paradigm much easier. I who is most familiar with FP have easier time understanding Lisp code for example, even if I don't know Lisp that well. 
It's apparent. Check out all the usual places like LinkedIn and there are also sites like cufp.org.
Visual Haskell was a research project back in 2005 or so, and as far as I know was never maintained.
"It seems like all my Python friends are playing around with Haskell lately" I read somewhere that a good number of newcomers to Go have Python backgrounds instead of C++ ones. Given that Go and Haskell are actually [substitute goods](http://en.wikipedia.org/wiki/Substitute_good) in a sense (they are both compiled, garbage-collected languages with good support for concurrency) perhaps Haskell could tap into that current of interest.
Not a chance. Haskell jobs are rare and highly in demand because most people who use Haskell, really like Haskell. This means that any company that posts a job using Haskell gets many well-qualified applicants. If you want a job using Haskell, you need to make yourself stand out.
Pretty much :-) went to a haskell meetup anounced it and got a round of applause!
Wait, in the vault library, the only operation that is visibly `IO` is the generation of a new typed key to access the heterogenous environment. If you use `unsafePerformIO` on that, everything will be pure. I do that in my reactive-banana library for the purpose of observable sharing. (Of course, this `unsafePerformIO` is inherently dangerous due to polymorphism, but you knew that already when you wrote your existing code.)
Two major issues: 1.) Key allocation is faster in the revision control monad via `concurrent-supply` than with `Data.Unique` under high contention. I touch a common IORef every thousand or so allocations, due to pooling in the supply, while Data.Unique is forced to play `STM` games every time. 2.) I also need the ability to both intersect and union my maps with custom merge strategies that I tie to my keys. I am creating many of these maps in a tree, and using my online LCA algorithm to find the lowest common ancestor, monoidally merging together all of the maps since that ancestor, finding all the edits in each, and where they intersect I'm using the specified 3-way merge strategy. Shoe-horning that into `vault` means working with the merge strategy 'outside of the existential', which practically puts me back where I started. Vault doesn't and probably shouldn't expose enough of the internals to support my use case.
&gt; seq was introduced to fix space leaks Are you sure about that? I understand that `seq` is *used* to fix space leaks, but it actually does a pretty crappy job of it. As I understand, for `seq y x`, the compiler can, and often does, first evaluate `x`, and then go back and evaluate `y`. Hence `pseq` was invented. This leads me to believe that either `seq` was invented for some other reason, or is a big mistake.
Yes, I have considered adding more utility functions to `pipes-parse`. I just wanted to discuss these with Edward first to see if he was interested in adding them to the `free` package. One of the functions you mentioned is already there: `mapFree`, except that it's called `transFreeT`.
Yeah, that's the problem.
Heading back to [The History of Haskell](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf) it appears I slightly misremembered the details. &gt; In 1996, Haskell 1.3 introduced two features that give the programmer better control over evaluation order: &gt; • the standard function seq, which evaluates its ﬁrst argument, and then returns its second [....] &gt; • strictness annotations in data deﬁnitions [...] So the original intention was to use it to control evaluation order. That role is as you noted now subsumed by `pseq`, to give `seq` more flexibility. &gt; Although seq was not introduced into Haskell primarily to ﬁx space leaks, Hughes and Runciman were by this time well aware of its importance for this purpose. Runciman had spent a sabbatical at Chalmers in 1993, when he was working on his heap proﬁler and Hughes had a program with particularly stubborn space leaks—the two spent much time working together to track them down. This program was in LML, which already had seq, and time and again a carefully placed seq proved critical to plugging a leak. Hughes was very concerned that Haskell’s version of seq should support space debugging well. Later on pain by students of Hughes was the ammunition led to the removal the class constraint. &gt; However, the limitations of this solution soon became apparent. Inspired by the Fox project at CMU, two of Hughes’s students implemented a TCP/IP stack in Haskell, making heavy use of polymorphism in the different layers. Their code turned out to contain serious space leaks, which they attempted to ﬁx using seq. But whenever they inserted a call of seq on a type variable, the type signature of the enclosing function changed to require an Eval instance for that variable—just as the designers of Haskell 1.3 intended. But often, the type signatures of very many functions changed as a consequence of a single `seq`.
As cgibbard points out, VisualHaskell is gone. You might instead look at these other offerings: * http://stackoverflow.com/questions/2610767/visual-haskell-2008-2010 * http://stackoverflow.com/questions/68504/what-are-my-ide-editor-choices-for-haskell Ultimately, I use emacs or vim with a browser tab open to [hackage](http://hackage.haskell.org/packages/archive/pkg-list.html) and [hoogle](http://www.haskell.org/hoogle/)/[hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html).
Fair enough. Vault doesn't make any considerations to special case performance.
I'm not sure what role `seq` plays today. When is it appropriate to use `seq` over `pseq`?
The second issue is the one that really keeps me away. I have no way to find all the values that intersect and mash them together appropriately. The other is just needed to win on benchmarks. ;)
Why need conduit (or pipes) at all along with the other library? Aren't they just different implementations of the same idea? 
Well it's like demand for ninja assassins. You cannot possibly know unless you are in business ;) 
 foo a b c d | a `seq` b `seq` c `seq` d `seq` False = undefined 
 $ cabal sandbox init $ cabal install --dry-run scotty | tail -n+3 | wc -l 57 $ cabal install --dry-run snap | tail -n+3 | wc -l 81
Sort of. There are a lot of nuances where the libraries choose a different way to solve the same problem. Not just different implementation, but different API, different composability implications. The main example is parsing.
This is really a tautology. You find it easy to understand Lisp because you're familiar with FP, and others find it easier to understand Python/Java because they're familiar with imperative programming.
What exactly do you mean with merging? Something like `Data.Map.unionWith`? That seems tricky, as you would need type information to combine values, but it should be possible. If we can figure out a generic function that helps in your case, I'd be happy to add it.
Er. yes. =)
Basically I wind up with a combinator like newVar :: (a -&gt; a -&gt; a -&gt; a) -&gt; a -&gt; Rev s (Var s a) where my key now holds the 3 way merge function it should use and the default value. Then I can spawn fork :: Rev s a -&gt; Rev s (Task s a) join :: Task s a -&gt; Rev s a and edit variables readVar :: Var s a -&gt; Rev s a writeVar :: Var s a -&gt; a -&gt; Rev s () When both the current thread and the one I join with have edited then we rummage back through time and ensure the merge function gets applied to both new copies and their least common ancestor in the tree of fork/joins. Doing this correctly requires lots of monoidal munging of these maps, tracking contents in the keys and is just generally a different abstraction than vault, though it is close enough to tantalize.
Storing the default in the key rather than the current path in the Rev monad lets me avoid leaking newVar's that are never edited, but is an optimization, not the core of the problem.
Very good.
My initial reaction to this was negative. But it’s worth considering his point of view, because it is undoubtedly shared by many similar programmers. &gt; Some languages are easier for newbies to understand than other languages. This is true. However, his other arguments about what makes a language difficult or easy are largely unfounded. When you’re a blank slate, you’ll be more willing to accept any fact of a language which, in isolation or in a different context, is absurd. This applies to every language. What do you mean, variables can change? That’s not like math. What do you mean, statements are ordered? That’s not like math. What do you mean, `while` doesn’t mean “while”? That’s not like English. And so on. &gt; `TVar`s are so incredibly interesting, but I certainly wish it had a more newbie-friendly name! This is a point worth paying attention to. Certain parts of the Haskell *ecosystem* are quite friendly to noobs: GHCi is nice, Hoogle is incredible, and Cabal is doing great under the circumstances. But in large part the *language* is not, because of how the code is written. We write `x:xs` because it’s terse and lets us “zoom out” to get a broader view of our program than if everything would be cluttered up with names like `first` and `remainder`. That is an *expert-friendly* thing to do. &gt; Haskell and APL have very high symbol / line ratios. It’s because of the naming and the compositionality. When you use many verbose names, your expressions get really wide and contain few tokens. When you use short names (in generic code, anyway) and composition instead of sequencing, your code gets a lot denser. Consider the difference in *shape* between these equivalent snippets, and how alien the former must seem when you’re accustomed to the latter: bothTo = liftA2 bothTo combiner function1 function2 parameter = do { let result1 = function1(parameter); let result2 = function2(parameter); combiner(result1)(result2); } &gt; As soon as someone mentions Category Theory to me, my eyes start to glaze over, because when it comes to programming, I am a linguist, not a mathematician. We all know you don’t need any category theory to use Haskell. You sometimes need it to write interesting libraries, or understand the guts of interesting libraries written by folks like Edward Kmett and Conal Elliott. But the fact is that *people think you need category theory to use Haskell*. That is a myth that we are not helping to dispel with our many theory-oriented blog posts. It is worth considering the practical concerns of outsiders, which is why FP Complete is handing out money for practical articles—there is a dearth of them. &gt; When I learn a new language, the questions that come to my mind are what's the syntax and what does the syntax do? This is an effective way to learn programming languages when all the languages you know of are essentially the same, *which is the case for most programmers*. They learn Java and C and Python and Ruby, and they assume they know what programming is. The thing about people is that, on average, they hate having their assumptions challenged. They especially hate returning to the level of sheer “I don’t get this” from when they first started to learn how to program. The solution is of course to give a better rounded CS education from the start, which is easy to assert but tremendously difficult to implement. 
Great and relevant post! :D The funny fact: as a guitar player – when I have spare time… – I can tell how easy is a D chord to make :D Love your comment though.
which was her/his point all along.
It was the post author's point all along.
You might be approaching Paul Graham's [blub paradox](http://c2.com/cgi/wiki?BlubParadox). Imagine some average language called Blub. Then: &gt; As long as our hypothetical Blub programmer is looking down the power continuum, he knows he's looking down. Languages less powerful than Blub are obviously less powerful, because they're missing some feature he's used to. But when our hypothetical Blub programmer looks in the other direction, up the power continuum, he doesn't realize he's looking up. What he sees are merely weird languages. He probably considers them about equivalent in power to Blub, but with all this other hairy stuff thrown in as well. Blub is good enough for him, because he thinks in Blub.
This is maybe veering off-topic, but the annoyance I regularly run into is that if you try to use Google to search for any haskell function or library, the top results are almost invariably a very old, obsolete version. It would be awesome if hackage and haddock were set up so that generated documentation always had a link back to the hackage page, so I can easily navigate to the latest version.
Do companies use Haskell??
How come? 
&gt; In Haskell, a . b . c . d basically means a(b(c(d()))). However, monads aren't quite as trivially composable. `a &lt;=&lt; b &lt;=&lt; c &lt;=&lt; d`.
Yeah, maybe. It’s kinda the thing I said. It’s impressive how people defend the language they’re used to and afraid of new stuff! – hopefully not everyone.
Huh, I didn't realize that's what that links to. That is what I want, though perhaps the interface could be a little more obvious. For instance, I would expect that clicking on the package name in the upper left might take me to the hackage page, but that isn't actually a link.
Can't you do an O(n) translation to the SKI calculus by writing a lambda calculus interpreter in SKI (constant size) and then encode a list of binary digits using SKI with a linear encoding and then just write the lambda program in binary (size O(n))?
inurl:latest more or less solves this problem
I think people trying to hire Haskell programmers in places that aren't tech hubs are more flexible. I am looking for a programmer in Oklahoma, it is hard just finding people who know what functional programming is. 
clojure - lisp, immutable, lazy, practical.
C#. It's not a functional language but when you use a lot of LINQ and Lambda expressions it gets fairly close. Besides that it's a fairly versatile language that works nicely with a lot of paradigms. 
I have found Go very pleasant. I have so many complaints about this language that I could go into, but I find in spite of them I have a very good time using it. My fondness for Python and Go drives me to find a very regular style in my Haskell that is unimpeded by too many choices. Lately I agree with vagif tho :).
Scala. It's like haskell and sml had a very dirty lovechild. With concurrency. 
I like Scala because it has consumed the parts of my work that would have been in Java or Node.js, but I struggle to find other reasons :P.
Scheme. Simple and beautiful. (Sadly too simple—the community has failed to achieve any consensus around a record and a module system...)
Where's Ermine????
Oh, man, so many. C, because you never forget your first love. And it gives me lower-level language that I can use to extend scripting languages, or to understand what's going on behind the scenes in terms of the memory. Ruby, because it really clicked for me when I was ready to move on from C. I'd already been burned by Java, and coming to ruby felt like OO done right. Having faked closures in C manually, blocks were awesome! And then I fell in love with metaprogramming. [Dwemthy's Array](http://mislav.uniqpath.com/poignant-guide/dwemthy/) blew my mind. I love Javascript, clunky syntax, prototypal inheritance and all, because Greasemonkey made me feel empowered to fix all the sucky internal webpages at my company, and help the rest of the users do their jobs right. Plus, it was another nudge towards functional programming with everything I could do with explicit currying. I love Scala, because it's trying to do by the JVM right, and do a lot of things that Java had failed to do. I love Erlang because it broke my brain for a little while with its let it crash philosophy. I still think doing a couple months in Erlang could help my concurrent thinking more. I love Vimscript for giving me absolute power over my editor. I love bourne-shell languages for giving me power over my tools, and letting me mix and match them (and pipe them) with ease. Make, for making it really easy to express a dependency DAG, and for demonstrating the value of idempotence. Even the ones I dislike or don't use, I'm glad I've taken the time to learn. They all teach me to think a little different, put another tool in my hand.
SQL: a few basic building blocks let you mash up your data almost every which way. Python: easy to set up everywhere and comes with almost _everything._
I like scala, but when I tried to use it at work I had a lot of problems, mostly with the underlying java libraries I had to use. :( The actual language itself is really nice.
Have you tried F#? It's OCaml-ish but it's also a first-class .NET language.
C. It's like the ultimate libertarian language (without being assembly obviously), anything I want the computer to do I have to explicitly tell it to do, and if I tell it to blow up, well, it'll blow up but I can also get it to work extremely efficiently for me with a minimal syntax and I know exactly what it's doing.
Javascript - it's a bit of a love/hate relationship, but it's so flexible and it's the one programming language pretty much every user has an interpreter for already. But mostly I love it for its massive range of libraries, particularly jQuery, Angular, and D3. And for the lengths people go to to _avoid_ Javascript (see CoffeeScript). C - it's so damn simple. I quite enjoy some light assembly programming for the same reason. It's that feeling of being in total control, rather like what I imagine motor enthusiasts must feel like when they get down and dirty with the insides of a car.
https://github.com/ermine-language/ermine
Lazy?
http://en.wikipedia.org/wiki/Lazy_evaluation
Yes I know what laziness is. What I meant was, how is clojure lazy?
C. It interfaces with every language and is extremely portable. Edit: Rust! I almost forgot about this until I read `illisius`'s comment. Rust is the closest thing to low-level Haskell.
Yes, Yes they do. 
Agda would be pretty painful for that (though it can certainly be done). Idris would be great for what you describe though, and has actual compilation and execution as more than an afterthought. I use Agda because most of what I am doing is specifying type theories and natural language calculi—and Agda is delightfully suited for this kind of work.
Pascal, it's better than C for all the things C is used for, IMO. Agda, it's my favorite language to formalise thnigs. Idris, I haven't used it much but it looks great for DSLs. SML, the module system kicks ass but some of the coding conventions don't. Other languages that I have gripes with but don't mind too much: Scala, Clojure, OCaml, F#, C, Mercury, Scheme. You can pretty safely assume I really don't like any well-known language not on this list.
Ur/web would work for the dependently typed web angle, though, no? (Well, row types, but still.)
Self, for when Eiffel is too lax with its object model. Agda/Idris, because dependent types are the secret webscale sauce. Ur(/web), because row types. Prolog, except I wish it had an actual type system. Factor's pretty cool, too, now that I think of it.
Tcl - homoiconic, coroutines, introspection, lovely event model.
This is the first time I'm seeing the chunked data problem start to be addressed in `pipes`, so this is getting closer to something that could be used in places where I use `conduit` currently. In `conduit`, this issue is addressed via leftovers. IIUC correctly, the technique `pipes` is using is to have the user explicitly pass leftovers around by using `next` to return the remainder of the stream. The technique seems solid, but also seems like it will lead to much more verbose code than built-in leftover support. I'd have to see more to understand this better. I also know that it's a common theme that conduit is "not correct," or at least pipes is more correct. I really do disagree with this assertion. conduit doesn't follow category laws, but has also never made any claims that it does. It instead gives other guarantees, such as prompt resource finalization, and no data loss due to chunking behavior. At least in the past, `pipes` was not able to give some of those guarantees. So I don't consider this correct vs incorrect, but rather which properties are most important to guarantee.
Lua -- very simple(maybe even simpler than Scheme) yet powerful and widely used. It's very easy to embed it into your C/C++/Haskell/your-language programs(see hslua package for embedding into Haskell programs).
Java. The JVM is simply an amazing piece of technology. The profiling tools available for it are great. Great backward compatibility, both binary and source (unlike Scala). Multitude of libraries. Platform independent, good support for concurrency. Good IDE support for autocompletion, refactoring, and so on (Scala is not yet there). Try-with-resources, generics type inference, and the upcoming addition of lambdas alleviate a few of the existing pain points. Javascript. A versatile language which offers a multitude of highly composable libraries. SQL. The most successful functional language ever in the history of mankind. COBOL. A successful example of a language designed by comitee (there is another example, but I can't remember it right now). In a way, it's like a DSL for business processing and it is well suited for its intended task. Just don't try to write an evented web server with it. Quite efficient, too, and also readable, avoiding the single-letter identifiers that plague other languages. Great for its time.
Prolog - logic programming is fun and great for writing grammars and parsers for natural language processing. C - simple, low-level, easy to use as a foreign language with practically any other language. Java - not so much for the language, but for the great tooling (IntelliJ, Maven), enormous number of high-quality libraries, and easy instrumentation of the JVM. I tried to love Scala, but it is a bloated language.
ruby! I feel like you get the best of object-oriented programming and with list operations, closures and functional styles that we love from Lisp. The dynamic typing also works much better after coming from the extreme weak typing of the javascript world.
I've already tried the chunked approach and taken it to its logical conclusion. It doesn't work for the following reasons (I may have forgotten some, but these are the ones fresh on my mind): 1) Chunk boundaries only solves the case of lists of logical units and does not let you encode richer structures without sacrificing type safety. The `FreeT`-based approach is a special case of a more general approach which allows you to encode data types of arbitrary structure. To see an example of this, check out `Pipes.ByteString.span`, which produces a "pair" of values (i.e. a `Producer` returning a `Producer`), then think about how you would implement something like that `span`. For a more sophisticated example, consider the following function that I have planned for the `pipes-text` package: -- decodes as far as possible and returns the residue that it couldn't decode utf8 :: Producer ByteString m r -&gt; Producer Text m (Producer ByteString m r) How would you implement that using separators? You'd have to throw in an `Either` and then dynamically guarantee that all the chunks in the first block were all `Text` chunks. Those are just the simple examples. To see a really sophisticated example that lays bare all the problems with linearizing your data types, just read Oleg's incremental pretty printing paper, which is forced to flatten a richly nested data type to a more weakly typed linearized representation. 2) You can't intercept end of stream this way. This means that your `lines` function will not produce a final `EndOfChunk` and therefore `unlines` will not correctly add a final newline. 3) Having the power that a `Pipe` affords is a misfeature when parsing. I'm aware of the things that a "function of producers" does not let you do compared to the equivalent pipe. However, in this case the very things that the pipe is uniquely capable of doing are precisely the behaviors that are broken. For example, one thing you can do with a pipe that you can't do with a function of producers is that you can sequence something after it. However, that behavior would be broken in this case since if your parsing pipe intercepts end of stream then you should not be allowed to draw anything else after reaching the end of the stream. Similarly, another thing a pipe lets you do is transform consumers, but that's also broken for the exact same reason: you could then sequence another consumer after the one you just transformed using your parsing pipe and break protocol. There's one thing I want to clarify. I view `pipes-parse` and `FreeT`-based grouping as being a fundamentally separate idiom from `pipes` that really could be implemented using `FreeT` from the ground up. In principle, the `Producer ByteString m r` should really be replaced with `FreeT ((,) Bytestring) m r` to be totally consistent. I just implement the very lowest level using `Producer a m r` instead of `FreeT ((,) a) m r` to allow re-use of `pipes` code.
It's easier to understand if you start from a simpler function like `Pipes.ByteString.span`. Notice that its type is: span :: (Monad m) =&gt; (Word8 -&gt; Bool) -&gt; Producer ByteString m r -&gt; Producer ByteString m (Producer ByteString m r) Compare that to the original `Data.ByteString.Lazy.span`, which is: span :: (Word8 -&gt; Bool) -&gt; ByteString -&gt; (ByteString, ByteString) The key idea is that when you lift things to this higher-order representation, the product of two values gets lifted to the first value returning the second value. Therefore, when you lift a list this way (i.e. each successive element of the list is embedded within the return value of the previous element), you get a `Free`/`FreeT`. Technically, even the `Producer a m r` itself could be replaced by `FreeT ((,) a) m r`. I only use `Producer` as the base layer for interop with `pipes`.
Smalltalk, cause it forces you to write the name of each parameter (e.g. doWithNumber: 123 andString: "foo") and it's very elegant. Vala, cause desktop development+async programming done with ease.
Apparently I'm the weird one: C++ (not C!) because I like zero overhead abstractions. And therefore also Rust. 
Thank you for giving me reason, to look into agda. I was going to reinvent the wheel, but now it looks like I can use agda for my project.
Though, to be fair, I'd be perfectly willing to adjust the type signatures to capture the necessary properties. While the signature: foo :: blah -&gt; blarg -&gt; X -&gt; Prob looks nice; there's nothing entirely wrong with using: foo :: blah -&gt; blarg -&gt; Box (X -&gt; Prob) ...provided that `Box` is some built-in modal operator (or similar), rather than a newtype which requires unwrapped before being applied to its argument. Even a newtype wouldn't be too bad if only we had featherweight syntax for using idioms. FWIW, this sort of thing ---where we make type-explicit when/where closures will be constructed--- is also something that's important in languages like BitC which want to make allocation (and it's associated costs) more transparent to users. In itself, it strikes me as an interesting problem, and one the type theory community hasn't really considered.
I love C for its simplicity. That's what makes it beautiful in its very own way.
Try reinstalling the Haskell Platform.
I have reinstalled and also added export PATH="$HOME/Library/Haskell/bin:$PATH" to my .bash_profile which I didn't have previously ... that took some googling to figure out. Ugh. Still no good
You may want to try ghc-pkg unregister packagename Then deleting the package from ~/.cabal and reinstalling it. 
You should probably just backup your settings and such, wipe the .cabal dir and reinstall the haskell platform from scratch. That's what I would do, anyway, but I'm probably not the most adept at fixing cabal debacles. 
@exDM69: can you please post a link to the company in Helsinki, I may become interested in that in the foreseeable future. Thanks!
As a novice haskeller, and (I like to believe) a more experienced C hacker, I enjoy both languages immensely, as I feel they're both the all or nothing ends of the spectrum of programming languages. If I want to do something tight, mathematical and theoretically complex, I choose Haskell. It lets me abstract away from the machine until I reach the core problem underneath. At other times, if I want to do something very..."computery", eg networking, or simulation code, I'll choose C, for the power, and freedom to control the machine how I want to. For the times in between, I also like python. It feels like haskell, but with enough C thrown in that I can get some control when I really need it.
Do you really mean Pascal, or Object Pascal/Delphi? Pascal was one of the first few languages I learned (maybe no. 5 or 6), and I really mean standard Pascal. But Wirth went on to design more languages for good reasons, and Borland extended Pascal for its Turbo compilers fairly early (before the OOP in version 5.5) for similarly good reasons. Personally, I have fond memories of Modula 2, and Ada (not by Wirth, but clearly in the same style except rejecting the minimalism) is OK. The languages I'd particularly mention here, though, are Icon (not for the language as a whole, but for particular ideas - especially generators and alternation) and Python (which implements at least generators in an overall-better language). Also Prolog, not so much for being a predicate logic language as for the specific mechanism of unification. Also, a domain-specific language - Ragel - not so much for what it does as for the ideas the underlying theory gave me. I've tried other functional languages, and once I'd have mentioned Scheme, ML or Miranda in the same way I mention Icon above, but Haskell is the first I've really spent time using. Actually, probably not Miranda - I only used it for a few days about 20 years ago and I thought it was pointless. I might have said something about it trying to cram a spreadsheet into a single cell. Not entirely untrue, but clearly a failure of imagination. 
[Rust][1]. Basically, it's what Go should have been. I hope it'll eventually displace C++ in many places. There's so much good stuff. Static type- and memory safety, compiles to machine code similar to C++, type classes and ADTs, destructors and RAII, Scheme-inspired macros, a disciplined approach to mutability, ownership, and aliasing... and linear/affine types, which are wicked cool. [Dynamically sized types][2] are also a really cool idea. [1]: http://www.rust-lang.org/ [2]: http://smallcultfollowing.com/babysteps/blog/2013/04/30/dynamically-sized-types/
Pyhon - write general programs in little time.
F#, for the libraries, IDEs, and ecosystem. E: also Go, as it Gets Shit Done Fast.
well not much will be different besides a faster processor, exc..
Java, simply being the OO language I know the best. OO can be beautiful and elegant, when it works. Of course, we all know too well the times when the abstraction has to be beaten, stretched, and finally broken to get anywhere. 
Note that the less verbose way to do this is to use `pipes-parse`. The basic idea is that you use `StateT` to thread the `Producer` you are reading from as the state. Leftovers are implemented by just pushing elements back onto this producer and you implement connect-and-resume using `input` from `pipes-parse` (which is a `Producer` that reads from the persisted `Producer`). In the source code for `pipes-bytestring` I just implemented everything using raw `next` calls and hand-threading of state because it was easier for me to do it that way.
That's why I prefer Ruby or JRuby, because Ruby's more introspective, allowing you to monkeypatch the language itself in order to achieve your goals.
Erlang—BEAM is a joy.
It's a good language within the limits of what it was designed for, but "all the things C is used for" seems an extreme claim. Standard Pascal was designed for teaching. C may not have a real module system, but it at least supports the use of separately-compiled libraries. Even back in the days of Pascal vs. C wars, I don't think many people were advocating standard Pascal. That said, I don't think many were advocating standard C either - especially as there wasn't a formal standard until 1989 (vs. 1983 for Pascal). 
I don't think any Pascal in popular use is completely standard.
So it's specifically the OOP extensions that you dislike? I can understand that. 
Yeah, I think there's a certain elegance to more traditional pascal. It's as though any Pascal program is a refinement of some very elegant program in a dijkstra-style abstract language. You can even write code in that abstract language and do the refinement into executable code, preserving your hoare triples ({} are comments in Pascal, which they aren't in Modula 2, and it's one of the things that irritates me about Modula 2, although on the whole I like Modula 2 as well). Throw object oriented programming in the mix, and all that austere reasoning gets tumbled into a world of mixed metaphors and confused design patterns. 
You are not alone...C++ with Qt is my answer to almost everything in my personal projects...
&gt; Try-with-resources Still not C++’s RAII, but, well. At least it’s a step forward.
Hmm, you must be having VERY TRICKY constraints, because most of what usual web and enterprise developers are working on are not nearly as complex :) 
Besides Haskell, OCaml and Python are my go-to languages.
It sounds like a differnet thing than `Vault`, but I wonder how far we can take it. I think it boils down to two things: 1. What you actually store is not a value of type `a`, but the value together with some extra data `(Extra a, a)`, along the lines of type Var a = Key (Extra a, a) date Extra a = Extra { merge3 :: a -&gt; a -&gt; a -&gt; a, default :: a } A completely generic store like `Vault` has no way of knowing that all entries look like this, but merging requires this information. 2. The previous problem seems to require a custom map data structure, but at least we could outsource the type safety issues. There's a [`Locker`][1] type for exactly that. However, in order to merge keys from different maps, one would still need to be able to equate types when the keys are found to be equal, along the lines of data Eq a b where Eq :: Eq a a equal :: Key a -&gt; Key b -&gt; Maybe (Eq a b) I'm not sure how to do that in a `unsafePerformIO` + `IORef` way, without using or writing `unsafeCoerce`. [1]: http://hackage.haskell.org/packages/archive/vault/0.3.0.2/doc/html/Data-Vault-Lazy.html#t:Locker
Are you now working? The first thing you should do in these circumstances is find out what your system thinks is actually installed by ghc-pkg list | grep -i random. If you have just installed the Haskell Platform and the random package you really shouldn't need to anything fancy. You don't say what os you are running on. Also if you could supply the command you used and the output you got that would be helpful. 
While Go and Haskell are not similar in design they do compete for a similar niche: servers and daemons. They both have different priorities: Go prioritizes compile times and Haskell prioritizes correctness. They also have different factors that drive their design: Go is driven by Google's needs while Haskell is driven by the Haskell community's needs since it's almost entirely a volunteer effort at this point.
Monad transformers are how you compose monads in the sense that you are thinking of.
Racket, since it's easy to write programs in it, although not as trivial to make working programs. 
It should all be a library! Standalone binaries are so last century.
Various assembly languages, since typically there are only two other steps between them and the bare metal.
Clojure's sequence-manipulation functions are mostly lazy.
1 is more or less I actually more or less wind up doing that whenever i do an actual insertion into one of these maps, so that I can have access to the merge function. Taking a whack at it while I sit in a talk, I wonder if one could do newtype Val a s = Val { getVal :: a } data Var a where Var :: Reifies s (a -&gt; a -&gt; a -&gt; a, a) =&gt; Key (Val a s) -&gt; Var a Then we make a key with: newVar :: forall a. (a -&gt; a -&gt; a -&gt; a) -&gt; a -&gt; IO (Var a) newVar f z = reify (f, z) $ \(Proxy :: Proxy s) -&gt; Var &lt;$&gt; newKey (Val a :: Val a s) That lets us know we have the merge, but the final attempt to merge runs into the same problem as part 2.
With C++14 we'll be getting pseudo typeclasses, and there's an ongoing effort to essentially replace the template metaprogramming syntax with Haskell.
Did you run the command it suggests in the output of ghc-pkg list?
You can actually just write `ghc-pkg list random`, unless you want to find every package with random in the name.
Are you using Xcode 5?
I wouldn't say it have failed. It's used a lot in the class rooms and dialects of it like Racket are actually used in industry.
Take these comments as genuine interest, and as actually trying to explore the solution space. I find it very interesting each time `conduit` or `pipes` makes any moves in the other's direction. It seems like `StateT` with a wrapped `Producer` is a limited form of `Conduit`. Like `Conduit`, it has full leftover support. But unlike `Conduit`, it lacks easy composability with existing machinery and doesn't seem to have finalizer support. And unless I'm mistaken, working in this fashion really sacrifices the category laws that Proxy tries so hard to obtain. For that matter, it seems like once we're ready to work in this manner, we could simplify down to the far simpler expression of the concept: newtype Producer m a = Producer { runProducer :: m (Maybe (a, Producer m a)) } Since we're never actually dealing with a `Producer` as a proper streaming data type, but instead just as a dumb data source, it seems like we're pulling in a lot of needless complexity. This is actually part of my more general concern: by leaving out functionality in the core library since it doesn't fulfill category laws, users will be forced to implement the same functionality in each of their codebases. In fact, as part of a request I got from Kazu regarding http-conduit a few months back, I started work on rewriting some of the code base using a style similar to this, and while it can be done, the code is much harder to follow and less easy to reason about. I'm concerned that that will be the result of pushing so much complexity into user code. (I'm trying to balance the expected comment length in Reddit versus the necessary verbosity to make my claim. I've been sitting on these thoughts for a while, perhaps a blog post would be a better way to elaborate, in case this comment isn't really comprehensible.)
4.6.3 -- I had to download it when I downloaded the Haskell Platform. Do you think I need to get 5?
No, I was just thinking that Xcode 5 might be the issue since it apparently doesn't play nice with 7.6. If you've got 4, that's not the issue.
yes it tells me this: `ghc-pkg recache ghc-pkg: /Library/Frameworks/GHC.framework/Versions/7.6.3-x86_64/usr/lib/ghc-7.6.3/package.conf.d/package.cache: you don't have permission to modify this file`
Looks like you need to run that as an administrative user.
In what sense is that a translation?
Yeah it's weird though, my account is the only admin 
Maybe you've run cabal install with sudo? (seeing in another comment of yours that you are not privileged). Under Linux, I'd remove both ~/.cabal and ~/.ghc to start fresh, but I don't think the same is applicable on a mac. What else you could try, if this package is meant as part of a application you're writting use a cabal sandbox instead, this way you will "detach" yourself from the global environment.
I did indeed use sudo. Sorry for withholding relevant information. I didn't even know what it meant when I did it. :)
I'm not terribly familiar with macs. You'd probably get better help with this sort of thing on #haskell (freenode, irc) where the feedback is faster. This sort of troubleshooting thread is a bit off-topic for /r/haskell...
Then with `sudo ghci` the import should work. Also you could chown the folders in question to your user/group and it should work as it would be normally expected. You're looking for the ghc and cabal folders in your installation. Don't use sudo when using cabal/ghc/ghci/runhaskell/etc. edits: chown not chmod
Yes, I have considered a completely `Producer` centric style like you just described. Along these lines I suggested exactly the type you just wrote out for inclusion in `transformers` to the libraries mailing list some time ago. You can read it here: http://www.haskell.org/pipermail/libraries/2013-July/020107.html You can see that the only difference from what you just suggested is that I call this restricted type `ListT` (because that's basically what it is). The idea I had is that you could do most things using `ListT` and `Producer`s (in the `pipes` sense). That proposal got a lukewarm reception, but it lives on as the `ListT` type within the `pipes` library: http://hackage.haskell.org/packages/archive/pipes/4.0.0/doc/html/Pipes.html#g:5 This type is basically identical to what you wrote, except I chose to implement it in terms of `Producer` internally for two reasons: * Faster conversion between `ListT` and `Producer`s, since it is just a newtype addition or removal both ways * It's easier to explain how `ListT` operations works in terms of the underlying `Producer` (`(&gt;&gt;=)` is `for`, `return` is `yield`, and `(&lt;|&gt;)` is `(&gt;&gt;)`). What I ended up deciding going into the `pipes-4.0.0` release is that there really are two separate ecosystems at play: * The `conduit`/`pipes`-like ecosystem where the major abstractions are sources, pipes, and sinks * A "Python-like" ecosystem where the major abstractions are generators, `ListT` and functions between them. I call this second ecosystem "Python-like" because that's basically how Python does everything (generators and functions between them), except with less power. For example, you can't push an element back onto a Python generator, whereas with a `Producer` it's easy (i.e. `cons a = (yield a &gt;&gt;)`) and with `ListT` it's easy, too (i.e. `cons a = (pure a &lt;|&gt;)`). With `pipes-4.0.0` I decided that both ecosystems have various advantages and focused on making them as compatible with each other as possible. There are some circumstances where one or the other ecosystem is not the best fit. For example, we both know the obvious problem with the simple functions of `Producer`s approach: these are only composable in exactly one sense (i.e. composition of functions), whereas pipes are composable in multiple ways (i.e. sequencing, pre-composition, post-composition, for loops, feed loops). However, functions of `Producer`s also have some unique advantages of their own which solve a lot of the problem that plague `pipes`. They can detect end of input, they permit sophisticated logical grouping (ala "pairs" and "lists") and they provide a very simple leftovers and connect-and-resume story exactly analogous to classical monadic parsers (a `StateT` over a textual stream, except this time the stream is effectful). In fact, I noticed that in many cases the narrow composability of functions of `Producer`s is actually a feature in precisely the scenarios that it best addresses (parsing and stream grouping). For example, you don't want a function of `Producer`s to be "sequenceable" because if you handle end of input you don't want to draw any input after that. Similarly, you typically don't want to "pre-compose" such functions of `Producer`s behind a `Consumer` because you can't guarantee that the `Consumer` you pre-apply it to will have exclusive access to the target `Producer`. So `pipes-parse` is fully embedded in the "Python-like" ecosystem, whereas `pipes-bytestring` supports both ways of doing things (defaulting to the pipe if both are possible, since the pipe is more composable). My intuition is that this "two ecosystems" approach is going to be the real solution in the long run rather than trying to shoehorn both classes of problems into one or the other ecosystem.
I'm really excited for Rust 1.0. It's already starting to stabilize a lot, but a major version is going to make me very confident that it's ready for serious work. I actually need to catch up; I haven't stayed up to speed on what's new in 0.7.
I'm just waiting for a "GHC 8: Now with dependent types!" announcement.
&gt; They especially hate returning to the level of sheer “I don’t get this” from when they first started to learn how to program. I don't think this is implausible, necessarily, but do you have evidence for it? There's two components: (a) that for some people, learning to program was a frustrating “I don’t get this” experience, and (b) that these people don't like Haskell, because for them, trying to learn it brings back the same kind of experiences. Learning Haskell did bring *me* back the experience of first learning to program: exhiliration.
I'd need a lot of convincing to believe that having two completely separate ecosystems sitting parallel to each other is the best approach. To me, it seems like a bit of a cop-out: we couldn't come up with an abstraction which handles the entire problem properly, so we've come up with two abstractions, and forced users to switch between the two of them. My experience (both with conduit and enumerator) says that chunking is a very common case, since `ByteString` and `Text` are such natural values to be streaming. And being forced to use the secondary interface for those common use cases doesn't seem right to me. As a simple example, when poking at the http-conduit codebase, I stumbled across [chunkedConduit](https://github.com/snoyberg/http-conduit/blob/1f3eacba361b7a78f3807a91fbee49a6a914b5cd/Network/HTTP/Conduit/Chunk.hs#L23). This does a nice job of abstracting a few different things: * We need to isolate part of the body that the inner Conduit can read to that which is part of the chunked body. * We need to transform the chunked body into unchunked data. * Any leftovers from the inner Conduit need to be retained and passed on to the next inner component. * Any leftovers from the chunking procedure need to be passed on to the next external component. I have trouble wrapping my head around what this would look like in the two ecosystem world, whereas in the unified ecosystem world the type is trivial: `Conduit ByteString m ByteString`. Perhaps I'm not seeing the benefit here, since the advantages you listed don't seem to apply to conduit. conduit has always allowed detecting end of input (I'd be happy to have that discussion with you again, I've definitely prefer the conduit approach here), and leftovers and connect-and-resume have been first class citizens for a while. I don't really know what specific issue you're referring to with pairs and lists, so I can't comment there. To re-summarize my previous points and these: I'm concerned that by oversimplying the core abstraction, `pipes` is heading in a direction which creates more complication overall, resulting in a unnecessarily complex API, and user code which will be harder to reason about.
Go, because: * It has a combination of minimalism, strictness and modularity that makes it easy to create lasting buildingblocks and to use the buildingblocks of others. * It is practical, you can get stuff done with it. * It is well suited for a world where the internet and multicore processors exists (good standard library, wonderful channels and goroutines). * It has been designed to be easy to analyze and can be parsed without a symbol table. Because of this, it compiles blazingly fast to native. Python, because: * The syntax and standard libraries makes it easy to write programs straight from brain to code. (This is a double-edged sword. A language like Haskell, that forces you to think about what you are actually doing, may be better in many cases). * Combined with C, you get a wonderful combination of raw speed and sweet expressiveness. * When there is work to be done, and the problems aren't too hard / in need of careful structuring, it makes it possible to grind out code and be amazingly productive. * It's fun and makes it possible to code in a more exploratory way (using the interpreter, dir() and completion, for instance). C, because: * It is available on pretty much every system under the sun. * You can write libraries that can be used by a wide range of programming languages. * There is a high degree of control. * It's low level enough to write an OS, a driver or a kernel module. * It's high level enough to write a game, desktop application or almost anything. (Languages like Erlang covers a few scenarios that C isn't great at, like creating servers that run forever and are patched while running). Now for the offtopic part that is sure to make me downvoted to oblivion: * C++ is ugly and unsuited for teams. Everybody programs in their own way, even with good code standards and are barely able to comprehend the code that C++ programmers with a different style than themselves write. * Rust keeps changing the syntax and standard libraries. The tutorials and examples online are already obsolete. * Assembly is unecessary low level and makes you stuck to a specific processor. * Prolog, Scheme, Ocaml, Lisp and Haskell are academical wankery. People only write full applications in these languages to prove a point. * Erlang feels like a cold wind from the 80's. Building Erlang from source is a nightmare. The syntax is somewhat akward. * C# is bound to .NET and to Windows. It just almost/mostly/some times work on other platforms, but C# programmers ignorantly like to think that it works great everywhere. * Objective-C and Java are overly verbose. * Ruby is not just slow but uselessly slow and the followers are religious fanatics. Languages that seems promising and that I want to learn: * Clojure - modern Lisp * Scala - less verbose Java * Self - just sounds interesting * Lua - widely used, useful * Rust - once it stabilizes Disclaimer: All of the above is IMO, of course
Eww, I misread the original questions and ult down the things I use rather than like.. :(
I think a "one-size fits all approach" is doomed to fail. Here's a trivial example which `conduit` does not get correct. Consider a byte stream that we want to decode to the following stream protocol, expressed using regular expression syntax: A B* C D* `A`, `B`, `C`, `D` would be primitive elements of our protocol (`A` and `C` might be headers, for example, and `B` and `D` might be data streams). Using `pipes` you would express the decoder as: Producer ByteString m r -&gt; m (A, Producer B m (C, Producer D m r)) If you try to do the same with `conduit`, you might try to "linearize" the stream by using the following intermediate linearized type: data Intermediate = A' | B' | C' | D' Then you would have something like: Conduit ByteString m Intermediate Now you have four problems: * You have to define this intermediate type * You have to define new parsers for this intermediate type that are not reusable * You can't enforce that `A` and `C` are only transmitted once * You can't enforce that only `B`s are transmitted between the `A` and `C`, or that only `D`s are transmitted after the `C`. You've converted your protocol to a more weakly typed representation when you linearize it.
These are ones I'm keeping tabs on; [kernel](http://web.cs.wpi.edu/~jshutt/kernel.html) derivations like [wat](http://axisofeval.blogspot.ca/2013/05/a-new-low-in-programming-language.html), just to keep my mind open. 
&gt; Assembly is unecessary low level and makes you stuck to a specific processor But sometimes still the best way to do small parts. Some heavy lifting code that can make good use of special instructions can often use some inline assembly in what is otherwise C code. Using the pre-processor you can keep plattform-independent code in general and provide optimizations for architectures which can benefit from the assembly code. &gt; C++ is ugly Yes, I agree. &gt; Prolog [..] Haskell [..] academical wankery I agree to disagree without providing specific arguments. Prolog is very interesting on its own, though limited in its usecases and Haskell.. well, there's a reason why I'm subscribed to this sub. To sum it up, it's the language that makes me the most productive by a very comfortable margin.
Is this a joke, or is there really work going on in that direction? 
Well, multiplication is associative, so it could be evaluated that way or `(((1)*2)*3)` or any other way, without changing the result. The point of curried functions is that you can have partial function application. `multiThree 1` can be interpreted in two different ways: it can be considered a function that takes two numbers and returns their product, or it can be considered a function that takes one number and returns a function that takes a second number and returns the product of the two. Similarly, `multiThree 1 2` is a function that takes one number and doubles it. These can be passed in to other functions in turn. For instance, if I wanted to have a function that could take a list of numbers and double every element in it, I could write doubleElements :: (Num a) =&gt; [a] -&gt; [a] doubleElements = map (multiThree 1 2) Does that make sense?
On the contrary, this is the kind of thing that conduit's built in leftover support and monadic/Applicative interface make trivial. Implementing this would be something like: parser :: Sink ByteString m Foo parser = Foo &lt;$&gt; parseA &lt;*&gt; parseBs &lt;*&gt; parseC &lt;*&gt; parseDs
TBH: I don't know if this is gonna be evaluated as 1 \* (2 \* 3) or as (1\*2)\*3 but I guess it will be evaluated the same as 1\*2\*3 ... This has nothing to do with currying ... maybe the more interesting question would concern the evaluation of something like (multiThree 1 2) 3 (?)
You are right. I may have been a bit too harsh with Prolog and Haskell and your points about Assembly are valid.
I love C myself, but don't get carried away; it sometimes helps to remind yourself that there's still an awful lot going on between C and actual execution.
You misunderstand the question. I want to preserve the original streaming of `B`s and `D`s, not load their entire contents into memory. In other words, the final result must still be a stream.
It depends on whether `*` is left-associative or right-associative. An easy way to find out is to ask `ghci`: Prelude&gt; :i (*) class Num a where ... (*) :: a -&gt; a -&gt; a ... -- Defined in `GHC.Num' infixl 7 * `infixl` means that it is left-associative, so it will evaluate as: (1 * 2) * 3
Maybe it helps to have the step-by-step, OP: multThree 1 2 3 = (\x -&gt; \y -&gt; \z -&gt; x * y * z) 1 2 3 = (\y -&gt; z -&gt; 1 * y * z) 2 3 = (\z -&gt; 1 * 2 * z) 3 = 1 * 2 * 3 Notice that how they are associated doesn't have anything to do with currying.
If you install the Haskell platform with a .pkg file, the global libraries are installed under /Library/Haskell which requires superuser authentication to modify. In the terminal write sudo ghc-pkg recache and enter your password. Then it should work.
Ermine is a Haskell-like language, originally implemented in Scala, but with a currently-under-development compiler in Haskell, which generates a portable core language. The main runtime will be in scala, but I think some others are planned. You can see some [examples of code in the standard library](https://github.com/ermine-language/ermine-legacy/tree/master/src/main/resources/modules), in the ermine-legacy project on github. It has a good FFI to the JVM (see e.g. [Native.Stream](https://github.com/ermine-language/ermine-legacy/blob/master/src/main/resources/modules/Native/Stream.e)) and row-types (mostly used to type database calls - see [Relation](https://github.com/ermine-language/ermine-legacy/blob/master/src/main/resources/modules/Relation.e)). It's been used internally for a edsl for generating reports which can be rendered to Excel, PDF, a web page, and a Java-FX based gui.
It would also be worthwhile to nuke your .cabal and .ghc directories. sudo rm -rf .{ghc,cabal} &lt; /usr/bin/yes
You're welcome!
Thinking about it, basically none: all of them suck in different and increasingly creative ways. Honestly, out of all the other languages I've used recently, the only ones to make me happy were OCaml and JavaScript. OCaml's all right except for being at least a little bit worse than Haskell in almost every way. There are even a few features (polymorphic variants!) that I envy. I *do* like JavaScript, but not because it's a good language. It isn't! It's crap. But, unlike so many others, I think it has its heart in the right place. And the browser is the same way: lots of practical problems, but the fundamental design (HTML for structure and content, CSS for styling) is very good. Racket is good in theory, but I found it doesn't even scale to a medium-sized research project, much less something large. Still, metaprogramming that doesn't suck is not to be underrated. Python is too arbitrary in its design and has some *really* annoying features, *really* annoying shortcomings and a rather annoying philosophy. It's also too verbose. In my arrogance, I think it a good language for *other people*: a lowest common denominator. Java is too verbose and inexpressive to use. Everything takes too much code, which makes it hard to read and hard to write. The JVM is pretty froody though--I should really check out Scala. I think Go is my generation's Java. Or at least that's what I fear. It certainly has a big company behind it and is threatening to become popular despite the language's design. The language and syntax are very arbitrary and full of random corner-cases. It's very inelegant! Things like error-handling are built right in when you could make it simultaneously simpler *and* more general by adding sum-types (for example). I think C is woefully overrated. It again has the problem of a rather arbitrary, inelegant design, slaved to the whims of hardware. I like consistency and good semantics, neither of which C has even heard of. It's one of the languages that's successful *despite* its design, largely because there are so few alternatives in its space. (And most of those somehow manage to suck even more!) Out of the languages I haven't really used, the main ones that seem interesting are Scala and Rust. Both show promise, but I haven't had time to try them out properly. I'm also excited by dependent types: Agda, Coq, Idris and Disciple. They seem to combine elegance, simplicity (*real* simplicity, unlike Go or C) and power, but also don't quite seem ready for large-scale programming.
I see, this part makes sense now. The part that does not, is how / when 1,2,3 or x,y,z are actually multiplied. Where do the parameters go when returning a function? What does that function look like? Thanks again! 
I'm not sure how it all gets implemented. One idea would be to store a tuple of (function_pointer, table_of_values_for_already_bound_parameters, number_of_parameters_yet_to_be_bound). When we've bound all the different parameters, it could call the function and look up values for all the different parameters in the (now completed) table, and if we haven't yet bound all the different parameters, it could append the next value to the table and decrement the count of parameters yet to come. but I'm just making this up; presumably the compiler folks have a faster, more memory efficient, cleverer way of doing things. Also keep in mind that Haskell is lazy. Given the chance to put off doing work, it will. So, asking when the multiplication happens gets a bit tricky, since it happens the first time the result is needed to have a side effect (such as printing the answer on the screen). Before then, it's just stored as a [thunk](https://en.wikipedia.org/wiki/Thunk_%28functional_programming%29), which is like a promise to perform this computation in the future if it becomes important.
It works like pretty much any other language. Until you have all three values, nothing happens. Currying has nothing to do with multiplication. 
My only evidence is anecdotal, from tutoring computer science. Until the moment of epiphany, *every* student is awash in “I don’t get this”. But it isn’t necessarily a negative sentiment, simply a fact which you can love or hate. Like you, I’m thrilled by discovery; but remember: not everyone actually enjoys learning. 
So I'm definitely over thinking it. Thanks a bunch.
Since `(*)` is a curried function already, let's get rid of that. Let's say we have some primop called `mult#`. Now, this primop requires exactly two arguments which must be supplied at the same time. Because of this, let's use a different syntax for applying primops than for applying functions; e.g., "`mult#{x,y}`". Because primops are these weird things with weird application syntax, we'd like to generalize over them to make them into normal functions. We have two ways we could do this: mult1 :: Int -&gt; Int -&gt; Int mult1 x y = mult#{x,y} -- == mult1 = \x -&gt; \y -&gt; mult#{x,y} mult2 :: (Int,Int) -&gt; Int mult2 (x,y) = mult#{x,y} -- == mult2 = \p -&gt; case p of (x,y) -&gt; mult#{x,y} Now we can use both `mult1` and `mult2` as normal functions. First consider `mult2`. When we apply `mult2` to some pair, it's going to unpack the pair in order to extract the components and pass them along to the primop. This lets us get away from the weird application syntax, but it still means we need to have both values in hand at once. Now, let's consider `mult1`. When we apply `mult1` to its first argument, we're going to construct a closure to keep track of that value: mult1 42 ==&gt; (\x -&gt; \y -&gt; mult#{x,y}) 42 ==&gt; (\y -&gt; let x = 42 in mult#{x,y}) Note, I'm making the construction of a closure more explicit by using let-binding instead of performing the substitution directly. If you prefer, you can directly substitute 42 in for `x`. Either way, we still haven't done anything with the primop, we've just saved the value of `x` so that we can use it in the future once we get our hands on `y`. Eventually we'll get our hands on some `y`, and then we will have both values in hand in order to pass them off to the primop. This business about primops is just to try to make things clearer; it has nothing to do with actual primops in GHC. Getting back to your original question, we can consider the expression `x*y*z` to be a "primop" `mults#{x,y,z}`. The ideas above carry through; since Haskell doesn't perform partial *evaluation*, partial application just means building up closures until you have everything you need in hand; then you can evaluate the expression just like you would when it's not hidden under a lambda.
One the other hand, the when part of your question is potentially much harder to determine. Haskell performs lazy evaluation, so the actual multiplication won't happen until the final value from the function is needed. If nothing needs that value, it won't be evaluated at all (unless you otherwise force it).
[I think the first two chapters of this book will help you out](http://research.microsoft.com/en-us/um/people/simonpj/papers/pj-lester-book/). It's fun to work through too. [Push/Enter vs Eval/Apply](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.9782&amp;rep=rep1&amp;type=pdf) would also get you there and it's just a paper, obviously shorter than a book.
Sweet! It seems that the book has some good examples to mess around with, thanks.
Okay, so in some ways thinking about partial application then, thanks.
Thank you for clearing the terminology up for me. I sort of just combined all three into "currying".
This is an outstanding explanation, thank you very much. After reading all these responses I believe I have Currying and partial application pretty much figured out. I think I had convinced myself that there was some recursion going on when really currying does not have anything (I hope) to do with it. "Time is an illusion. Lunchtime doubly so" 
I'm pretty much in agreement with you. C, Go, Java, Python, Racket, C#, C++, JS, Ruby, Perl, PHP -- all languages I dislike for one reason or another. I even get frustrated at Scala, F# and OCaml to the point where I don't really want to use them if I have any better choices. Of all the languages I like, the least experimental are probably SML and Haskell. Experimental languages: Disciple, Idris, Agda, are all nice. Not a fan of Coq/gallina.
I also like Agda for its record system, which is &gt; than Haskell's.
Haskell and SML? Neither of those languages have OOP. Are you sure you didn't forget to throw Java in there?
Another great example of currying and partial application. It's all starting to make sense now, thank you.
Yep, cabal is still Haskell's main, major weak point. Package handling sucks. So weird how a shitty language like JavaScript gets the best package manager, NPM.
Right now, your specification for a solution is only given by your type signature. Given that, yes, your solution is the only right one. (And for the record, `conduit` could implement this style of solution as well.) If I was to implement what I'd consider the optimal user-facing solution, I'd just have the user call the `parseA` and so on functions directly and do whatever he/she wants with the data in that manner. The downside to that approach is that it doesn't force the user to call the functions in the correct order, or to exhaustively consume all of the data provided. I'd consider that a good tradeoff. However, if you really want to force that to be the requirement, it's easily enough implementable: just provide a function that takes callbacks. I've put together a [complete demonstration](https://www.fpcomplete.com/user/snoyberg/random-code-snippets/simple-conduit-parser) just to elucidate the point. Notice that: * All leftovers are handled for the user automatically, avoiding a potential source of user-written bugs. * The "library code" is very simple to implement, and requires no low-level hacks at all. `parser` is written entirely with monadic and conduit composition, plus the sinkNull function. * The user cannot parse too much of the incoming `Text`, skip the `A` or `C`, etc. * All `B`s and `D`s are fully consumed, despite the user ignoring them. To experiment with that, try modifying `sink` to not call `CL.mapM_`. __Edit__: Copying in the main component of the solution for clarity: parser :: MonadThrow m =&gt; (A -&gt; m res1) -&gt; (res1 -&gt; Sink B m res2) -&gt; (res2 -&gt; C -&gt; m res3) -&gt; (res3 -&gt; Sink D m res4) -&gt; Sink T.Text m res4 parser getA sinkB getC sinkD = do res1 &lt;- parseA &gt;&gt;= lift . getA res2 &lt;- parseBs =$ consumeAll (sinkB res1) res3 &lt;- parseC &gt;&gt;= lift . getC res2 parseDs =$ consumeAll (sinkD res3) where consumeAll sink = do res &lt;- sink CL.sinkNull return res sink :: Sink T.Text IO Int sink = parser (\A -&gt; putStrLn "Got an A" &gt;&gt; return 1) (\i -&gt; CL.mapM_ (\B -&gt; putStrLn "Got a B") &gt;&gt; return (i + 1)) (\i C -&gt; putStrLn "Got a C" &gt;&gt; return (i + 1)) (\i -&gt; CL.mapM_ (\D -&gt; putStrLn "Got a D") &gt;&gt; return (i + 1))
I think /u/winterkoninkje's answers are the best in this thread since they explicitly distinguish the (mostly unrelated) concept of *partial evaluation*. This is probably what was causing your confusion. A sufficiently smart compiler might notice that once it sees the expression multThree 3 4 it could immediately perform this multiplication (3 * 4) and form, instead of (conceptually) the function \z -&gt; 3 * 4 * z , the function \z -&gt; 12 * z . (Actually, GHC tries to avoid creating these intermediate closures unless it needs them.) If things happened this way, your original intuition would be right: the multiplication would happen left-to-right, regardless of the way the multiplication operator associates (which happens to also be left-to-right - see /u/Tekmo's comment). You could of course implement partial evaluation yourself in the way the parent comment indicates - indeed, that's one way a compiler might do it. So why don't real compilers like GHC do this automatically? Well, actually noticing an opportunity for useful partial evaluation is not always easy (to a machine). Partial evaluation could actually make performance worse, either by evaluating too much or holding on to too much memory. Also, what if multiplication associated the other way (or you used some other operator like subtraction?)? The compiler would need to know something about the laws governing arithmetic in order to determine whether the simplification was valid. In general, compilers aren't good enough at predicting resource usage or understanding programs to make these kinds of simplifications automatically. There's a nice chapter on this in [Software Foundations](http://www.cis.upenn.edu/~bcpierce/sf/), and you can also check out [MetaOcaml](http://www.cs.rice.edu/~taha/MetaOCaml/), which tries to give the programmer additional power to partially evaluate (among other things) via metaprogramming. (Lisp programmers have used metaprogramming - in the form of macros - to implement partial evaluation for ages.)
When a programmer wants to learn python or ruby or even scala he downloads some web framework to try out. But with haskell i see this weird insistence to try out GUI applications. Why not Happstack, Snap, Yesod or Scotty? You'd be up and running with latest haskell platform in minutes. Yes, haskell's desktop GUI story sucks. But do not let it ruin your experience with haskell.
This is the internal iteration approach so I will recite the standard problem with internal iteration: what happens if the user only wants to take the `A` and `B`s but stop before consuming the `C` and `D`s (Edit: I just noticed that you claim this is a feature). With the version I gave the user can externally iterate over it at their leisure and stop when they don't need any more input. Your interface forces the user to consume it in its entirety. Similarly, consider the case where I had two heterogeneous protocols (i.e. the one I just proposed and some other hypothetical protocol). With the external iterator approach I can interleave accesses to the two, but with the internal iteration approach I cannot (well, I can, but I'd have to hack around it using concurrency and shared state). Edit: Also, pipes-parse provides automatic machinery for leftovers to avoid user-written bugs and is also similarly easy to implement. Just study the library API to see what I mean. Edit #2: I want to further clarify. I'm not saying that internal iteration is bad. It's useful when you want to enforce a particular access pattern. What I'm saying is that external iteration is strictly more general: you can always downgrade it to internal iteration, but you can't (easily) upgrade internal iteration to external iteration.
you've experienced "cabal hell" as they call it, and there's many in depth and heated debates on the topic. My suggestion don't force reinstalls. Also there is reactive banana, which has a wx hook and its own web based hook built on top of snap/happstack/yesod(i forgot which), which is an actively maintained FRP library.
Yes, I do think I was confusing partial evaluation is partial application along with what currying is. Thanks for those resources.
Just to make it very clear, the associativity of function application is INDEPENDENT of the associativity of multiplication. You could write the following functions: mult3a x y z = x*(y*z) mult3b x y z = (x*y)*z Both of these can be curried. You can do (mult3a 3 4) to have 3*(4*z) or you could have (mult3b 12 22) to get (12*22)*z.
&gt; But with haskell i see this weird insistence to try out GUI applications. It's probably the people who didn't choose GUI libraries didn't get slapped in the face by cabal. If OP wants FRP GUIs then [reactive banana and three penny](http://apfelmus.nfshost.com/blog/2013/08/02-reactive-banana-threepenny.html) might be what he wants, if he's okay with HTML.
I've tried to install GUI packages on Haskell for quite a while, and I've wasted many, many hours on it. My guess is that you're on Windows, so here's my advice: install a virtual machine with Mint or Ubuntu, and use that for all your coding needs. It's what I did. You'll see mostly all your issues with building packages go away. It sucks, but unfortunately Windows is a second class citizen in the coding world. (If you're not on Windows, I apologize. I've had many of the same problems you described, so I thought it was safe to assume.) I did get gtk2hs installed after hours of wasted custom builds by going to #haskell and asking for help, though. You could try that if you're set on it. I really would not recommend trying to get started with Haskell by doing anything with a GUI. Most GUI frameworks are very imperative in coding style, and that certainly won't help you learn Haskell. FRP is promising, though. 
Scala's OO feels strange, it's not really OO so much as function/closure decorators. 
While I don't know of a package that can do that, I remember [this](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers1/) series of blog posts on writing numbers as words. It's gets a bit dense after this first one, though.
You think FRP/HTML or some Webframework are a good entry to **learn** Haskell? Do not get me wrong but I think the TH stuff alone just gets in your face. And I can understand the OP very well - normaly to get some small value out of something you went and implemented some simple desktop tool and yes of course on Windows if this is your main UI. Instead of telling folks how *weird* they are in trying to build a desktop app - shouldn't the Haskell community not try and get a working solution out there? TBH this is one of the major showstoppers for me why I just cannot go Haskell - Scala, Clojure, F# and co. do not have such issues at all - maybe it's time to spend some more time in real-life instead of some ebony-tower category theory construct (sorry - no offense meant) and get together to produce some value for people like brokoli_ out there!
Care to go into detail about the "what Go should have been" part? This is the second time I've heard someone mention Rust when discussing alleged "faults of Go". This is coming from someone who doesn't code in either language.
The haskell reimplementation is the poorly named [Pure](https://github.com/splinterofchaos/Pure). I'm not finding any good links, but the C++14 feature Concepts lite is a weaker version of typeclasses, being essentially interfaces for template metaprogramming. There have also been some fun toy implementations of functional programming using constexpr.
What is "TP" in this context?
Good point, it's a terrible place to start. I was on auto-pilot when I recommended another FRP library. I think Parsec is a good library to play with after learning the basic concepts. It introduces motivating uses for applicatives and monads, it is well documented in Real World Haskell and it has a high power to weight ratio. As for calling people stupid, I don't think I did any of that. When I said people get slapped in the face by cabal that's not trying to imply that they did anything wrong so much as to say cabal is inappropriately painful for what from the user's perspective are very simple tasks. Regarding producing value for people like brokoli_... that's a complicated issue. Newcomers are welcome (encouraged even!) to use Haskell for any problem of their choosing but Haskell is a diverse community driven language. The only people who *should* produce some value for dissatisfied new comers are those trying to evangelize to them. I don't believe that the rest of the community is under an obligation to stop their research, business or hobbies to service others.
sorry my bad ... wanted to type TemplateHaskell (TH) ... :(
Sorry I did not try to imply that you did say anything like this - was more geared torwards vagif - if someone asks for help with a GUI problem tell him that they have a "weird insistence"? (I guess the word was wrong as well - sorry for that again - I will change it ASAP) For the other part: of course nobody has to produce ANY value in a community - but if/you want to bring Haskell along it's IMHO wrong to only aim at the research level - most programming is still done for simple things like your data-entry formular. As much as I enjoy (and try to learn) the more *advanced* stuff - I just cannot find a single example where it would be a good idea to choose Haskell over another languague and am kindof sad about this. Of course this might be because I am one of those lowly basic-programmers without the need for advanced math stuff in my day-to-day work :( 
Note that you can still use pipe composition to build up the transformation, too. That's no different than conduit. For many of these functions I also export the equivalent parser. I just consider the function easier to use than the parser in most cases, which is why I feature them more prominently in the API. For examples of this, see the `pipes-binary` library, which exports both the parser (i.e. `decode`) and the transformation (`decodeMany`). This lets the user pick which ecosystem they prefer to work in.
YOU WIN THE PRIZE! Thanks man!!
I agree "weird insistence" could be interpreted multiple ways. :) When it comes to helping others I think I probably should have distinguished between individual obligations and community obligations. No individual is obliged to help but I certainly agree that if the community wishes to thrive then it (or at least one person inside of it) needs to be welcoming to beginners. Unfortunately I'm in the same boat as you with using Haskell in the real world. I may gleefully use it for once off scripts or playing with concepts in a deliberately restricted manner (I hate the feeling dynamic typing leaves me with) but I've not yet used it for anything long living. Some people can deal with toolchain gaps by filling them in but I'm not that confident in myself yet. 
The designers of Go are adamently against adding two features that many in the functional language community think would make it much better. * parametric polymorphism * sum types Rust has both of these things. However, I think the two languages have very different aims, and it is a bit weird to consider them as competing. Go is trying to be a fun, friendly, simple language with omnipresent garbage collection and very big opinions on how you should code. Rust, on the other hand, avoids garbage collection, offering syntax and semantics for pointers that allows static guarantees about safe mutability at the cost of complicating the type-system. Rust follows the C++ philosophy of not making you pay for what you do not use. In rust, there is not one "right way" to do anything. 
I'd be very interested to know which version of gtk2hs ended up getting installed on your system. The latest version on Hackage, 0.12.4, is definitely installable using ghc-7.6.* with newer versions of cabal-install (I haven't tried with cabal-install-1.18 though).
&gt; let toEnglish n = take n $ iterate "one" 
Afaik cabal isn't a package manager. 
Perhaps [Elm](http://elm-lang.org) is a good alternative?
I haven't tried Python og Ruby, but Scala inherits GUI from Java, and (IIRC) has its own GUI library. I don't think it's strange to want to write GUI applications.
I like assembly, though obviously, I don't write much asm these days. Erlang seems to be quite fun based on my limited experience. C and Python I can tolerate.
What kind of detail? It's more about some peoples' expectations than anything to do with Go directly. Obviously Google had no obligation to design a language with any particular characteristics, or any language at all. But given that it was pitched as an alternative to C/C++ and it was coming from Google (edit: also Rob Pike, etc.), many people were expecting it to advance the state of the art for that domain, and to have... well, the kind of things that Rust has and Go doesn't. The name Rust is an allusion to the fact that a principle of its development is "no new ideas", i.e. to try to stick to things that have been tried and tested, or at least researched. What irks some people about Go is that for the most part, it could have used the same tagline [in 1970][1]. [1]: http://cowlark.com/2009-11-15-go/
I'm generally not a fan of screeds of unintelligible tactic invocations. If I'm doing theorem proving, I want a proof that I can read without machine assistance. I think if a proof becomes intractably large with that approach, that means a better abstraction or design is needed. Also, Gallina as a language inherits a lot of the syntactic ugliness of all those proof-general style theorem provers. 
I think that really depends on how you use it. 1-1 translation from Java is straightforward and a lot of people use it in that manner.
I am intrigued by this as cabal install defaults to --user. Probably not worth bothering with now you are working. As someone has noted below reddit is not the ideal place for this sort of question. There is also the haskell libraries mailing list and #hackage.
A haddock example in a module or a package description would be great. (on a related note: kudos to /u/Tekmo for *.Tutorial modules)
Perhaps you could re-export that Data.CSV types and operator to reduce import boilerpate, at least for trivial (the most-used) cases.
There is no reason for me to use Haskell for web development, on the other hand for backend services and desktop clients it is. To be honest with you, when I heard of Yesod and all the other Haskell web frameworks I laughed, and still laughing when thinking of.
&gt; Then I come across the numerals package. It works for many languages, but not for too large numbers. Umm... here is one of the examples shown in the main package description for numerals on hackage: &gt;&gt;&gt; EN.us_cardinal (10^50 + 42) :: Maybe String Just "one hundred quindecillion forty-two" Exactly how large do you need the numbers to be?
Not sure if type error or I don't understand the reference. :(
With Haskell, this is kind of a more difficult call to make; there's a lot of movement still on quite basic things, and you'll miss out on a lot of exciting and powerful stuff - lens, conduit, newer versions of the "big" web frameworks...
I'm the opposite. Long ago I stopped trusting my package manager with anything related to Haskell, as it's out of date most of the time. You can get a generic Linux binary of GHC, install that, and then grab the tarball of cabal-install from Hackage, and run the bootstrap.sh script from there and be up and running pretty quickly. Everything other than GHC itself, I install as user only with cabal-install (or in a sandbox using cabal-dev).
Good idea, I've re-exported some common types and operators: https://github.com/jb55/pipes-csv/commit/935b951c410667f501738873cb6eeed77e8e9405
Yeah I only have the small example in the readme right now. I plan on porting it to haddock soon.
For any given number. Theoretically, it is possible, provided in the given link.
Why do you laugh? Yesod is fantastic.
&gt; Instead of telling folks how weird they are in trying to build a desktop app - shouldn't the Haskell community not try and get a working solution out there? Well, I for one am working on it: [threepenny-gui][1] and I'm happy if people join in with feedback, suggestions, additional example code, tutorials, patches, etc. &gt; maybe it's time to spend some more time in real-life instead of some ebony-tower category theory construct Time for whom? Different strokes for different folks. Open source and scratch an itch. Supply and demand. This may be a good opportunity to mention that I have a [flattr](https://flattr.com/) button on my [website][2]. ;-) Also [gittip](https://www.gittip.com/HeinrichApfelmus/). [2]: http://apfelmus.nfshost.com/blog/2013/09/07-threepenny-gui-0-3.html [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
As others have mentioned: 1. Haskell is not your average imperative language, so trying out GUIs first may not be the right place to start learning Haskell. 2. The Haskell ecosystem doesn't feature many GUI libraries. WxHaskell and Gtk2Hs can be made to work, but they are still difficult to install. However, things are changing on point 2. I would like to advertise my own [threepenny-gui][1] library. (It also has FRP.) I'm not sure whether it's suitable for a beginner, though, mainly because it is still in flux. An absolute beginner may have a more stable experience with wxHaskell. [1]: http://haskell.org/haskellwiki/Threepenny-gui
I'm on my phone right now so I'm not sure but maybe not-gloss (http://hackage.haskell.org/package/not-gloss-0.4.0) will suit your needs?
I have a [branch](https://github.com/bgamari/gtk2hs/tree/cabal-1.18) containing some fixes for Cabal 1.18. Hopefully these will get upstream soon although unfortunately progress on the gtk2hs mailing lists is rather slow.
(Author of reactive-banana here.) The hook was on top of my [threepenny-gui][1] library, which, unfortunately, is not a web-based thing. It does use the browser, but it's more of a desktop GUI library -- I wouldn't use it to write a web application due to latency and session state. I'm currently working on integrating FRP right into the GUI library, so the hook is no longer needed. [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
Unary counting!
You can use Java's GUI stuff (Swing, etc.) with Scala, and there's scala-swing, which is a wrapper for the Swing API, but if I remember correctly it's not all that great and has received little maintenance over the past few years.
your project sure looks interesting - but there isn't much documention around (yet) - so it's really hard for novice haskellers to get anything done And the approach to run something in the browser with a local "server" that handles all the stuff seems somewhat "hacky" to me - while beeing very intersting, I still have to see some heavy UI build with it to jump the train. Right now I want a native solution or build a normal webstack.
&gt; Helm Ooh, that's the first I've ever heard of Helm. Cool! Can't wait to fiddle with it.
I am in Windows, but the thing is, I could code in Linux, since I have configured a dual boot already for this very purpose. But really, "don't code in Windows" is not a solution. I'm doing this exercise mostly as a learning experience, and starting off the bat by limiting on what platforms I can code / build in is not compatible with that. In fact, that's the whole point of starting to code a program in Windows: since it is a "second class citizen", it tends to place more limitations on what you can do, and for that reason it's much easier to get a program that's working on Windows to work on Linux rather than the opposite. To compare things a bit, we can code cross-platform in C++ with very little added effort these days thanks to tools like cmake for instance, and it's C++ !! How can Haskell not be at least as good in that regard?
SML, Simple, applicative, and excellent for teaching.
The author seemed to think that haskell is by definition hard, and python by definition easy. That is not at all the same as saying "languages close to the ones you already know are easy".
http://lambdacube3d.wordpress.com/ This project is literally all about writing shaders in haskell, without the boilerplate, and with extra correctness guarantees. Their blog goes through some very efficient worked examples.
Yes, but I thought it would be something like `toEnglish = (flip replicate) "one"` or `toEnglish n = take n $ repeat "one"`. `iterate :: (a -&gt; a) -&gt; a -&gt; [a]` and applying with `"one"` made my brain explode.
I'm sorry, my post was misleading, I did play a tiny little bit with Parsec already a few years ago, and I have studied pure Category theory quite a lot, so I can see the motivation and uses behind Applicatives and Monads, and understand Monads quite well already. The problem is translating that into a more complete application, that's what I'm currently really interested in.
&gt;I just cannot find a single example where it would be a good idea to choose Haskell over another languague and am kindof sad about this. You are in the same boat with the rest of us. Just like you we struggle to find useful libraries that would allow us to use haskell in real work. Just like you we either have to build those libraries ourselves or wait until someone will do it for us. I had to wait several years since i learned haskell, until it acquired enough useful libraries for me to use it in production. Mostly i use it to write web services and a few internal web applications with html/javascript frontend. Yes, there are a lot of specific tasks for which i would not use haskell myself today. And one of them is building GUI applications for windows. But the interesting thing is i could say exactly the same thing about java, python and ruby. Yet it did not prevent me from productively using those languages for years. I think you are exaggerating about not being able to find any use for haskell. I can assure you, today it can be effectively applied to a wide range of tasks. Mostly server side services, web applications and command line utilities. 
I’d rather say “stacking” them up, but well, we all agree to the same thing around here :)
Check out the [gallery of applications built with it][1] and the [examples][2] for learning. One of the main advantages of the browser approach is that it is very easy to install. It may feel "hacky", but it's perfect if you just want a cheap GUI and you want it right now. That said, the library *is* in a very early state, I will likely change the API several times over. [1]: http://www.haskell.org/haskellwiki/Threepenny-gui#Gallery [2]: https://github.com/HeinrichApfelmus/threepenny-gui#examples
Elm is cool, but it's not mature enough.
&gt; How can Haskell not be at least as good in that regard? Because until very recently, there was no commercial/profitable reason for it to be. Also, comparing tooling ecosystem between a research project and the most widely deployed environment in the world is a bit... incomparable?
It is unusual (and some say discouraged), but not wrong to have 'an' in front of a voiced _h_. E.g. an hotel.
Ah, 'k - no worries.
Yup, that's certainly the other possibility! Heck it could even be a typo, who knows. Just curious... it's possible, in some vernacular, the 'h' in 'haddock' goes unvoiced.
In order for something to work on windows, it requires developer time. Windows developers don't care to help code haskell so haskell doesn't work well on windows. It is improving gradually, but certainly for graphical libraries there is a non trivial amount of effort involved, and no one is willing to put forth that effort.
Your contradict your own requirements. There no adequate words in English to represent ANY number :)) 
Once you take a serious look, you would not laught.
I've seen that be the case for android development. As someone who learned Java after learning Lisp, Haskell, and a bunch of other higher-level languages, I tend to view Scala's objects as closures.
If you want to stay with something that is mostly OpenGL, but cuts down the boilerplate, I, unsurprisingly, recommend some of my own libraries as described [here](http://www.arcadianvisions.com/blog/?p=388). This approach just lets you get to the GLSL code more quickly, rather than abstract everything. I find it useful to stay close enough to traditional OpenGL to benefit from all the existing code out there, but if you want to stay entirely in Haskell, then you may want to look elsewhere.
Rust 0.8 will be out in a week or so.
I've worked at Tsuru, and didn't get any Haskell training there. They hire people who can already do Haskell and will be able to code "immediately" (I put quotes, because it takes time to understand a system before being able to hack it, of course). I did learn some Haskell stuff I didn't know, but more as part of casual discussions with the colleagues than training.
I don't know how to do it, but I think it would be beneficial to rename that SO question and added some tags. Can we ping an SO moderator to do that? Also, I provided an answer with a more detailed explanation. Please let me know if you have any further questions.
Have a look at [GPipe](http://hackage.haskell.org/package/GPipe). With it you can write OpenGl programs that utilizes the programmable pipeline entirely in Haskell, giving you purety, type safety and modularity that you wouldnt get from OpenGl otherwise.
Please stop trying to talk over the guest.
It is surprisingly difficult because it's so dissimilar from normal conversation patterns. Or maybe I'm just a jerk who talks over other people. Either way, I am working on it!
Oops. That was really a typo due to last-second phrase/wording adjustment (was: an example) (:
Oh thank god, mystery solved! That's good, 'cuz I wasn't gonna be able to sleep tonight, and I'm starting a roadtrip tomorrow... ;)
and those who do should be judged on who has the coolest t-shirts. your move steve.
&gt; `iterate :: (a -&gt; a) -&gt; a -&gt; [a]` and applying with `"one"` made my brain explode. `-XOverloadedStrings`? :)
By the way, don’t be alarmed by the age of the last post; the project is very much alive behind the scenes. At the moment we’re working on a system to decouple LambdaCube and OpenGL, and it’s coming along nicely.
I wonder if you guys would benefit from using a colour coding system with the webcams. If someone is holding a green card then they are talking and have more to say, and if not then they are done? That, or maybe just holding up something means you want to talk. This seems like it might provide the physical cues you get in in-person conversations over the slightly more disconnected conversation you get over the internets.
&gt; But really, "don't code in Windows" is not a solution You're trying to use a made-for-X11 GUI toolkit. It's been ported to Windows, but that doesn't make Windows a native environment for it. This is unrelated to the Haskell, and would also be true of Gtk and C. It can be done (I've done it on Windows for both Haskell and C during the dark times), but it's certainly not as easy as it is on the native platform.
next time they should get jon stewart, would be a lot funnier
threepenny is web-based, though, so not really a competitor to GUI libraries like `Gtk2Hs`.
I don't even own a single core computer any more (not even my phone) so parallel builds should be pretty useful. Looks like cabal still needs teaching how to use of it.
Nice to see 7 year old tickets still have a chance to succeed :-) Are there any concequences for non-manual compilations? Cabal also has parallelized package-level builds; since `ghc -j` was merged later, does Cabal make use of it at all (in order to increase the granularity of package builds)?
NPM is one of the first package managers to support installing and loading code from multiple versions of packages simultaneously, and does so in a way that doesn't require any extra work from the user. I've never experienced dependency hell in NPM, as I have with many package managers. `vi package.json &amp;&amp; npm publish` is also one of the easiest ways I've ever seen to create and publish libraries. The amount of metadata boilerplate is minimal, and quickly graspable. Cabal isn't too far off from these traits, especially in recent versions. But it still has a ways to go. For example, getting a Hackage account setup takes longer than getting an NPM or RubyGems account setup.
Agreed. I felt it was worth mentioning anyway.
The reason we don't like Go is because it doesn't have generics or algebraic data types. It doesn't surprise me at all that Go attracts more mindshare when it's backed by Google.
You can always invoke cabal with --ghc-options="xxx yyy" and pass additional flags to ghc.
As per [[1]](http://www.haskell.org/haskellwiki/Mac_OS_X) (first google result for "haskell os x"), add the following line to your ~/.profile file: export PATH=$HOME/Library/Haskell/bin:$PATH (By the way, ~ means your home directory.) Now you should be able to say "ghc" in a terminal and have the compiler start. You compile code by saying "ghc foo.hs -o foo" (or just "ghc foo.hs --make"), and the interpreter is "ghci". You can access your terminal by opening Spotlight (Command + Space bar by default) and typing "Terminal". It is also located at /Applications/Utilities/Terminal.app. To learn how to write Haskell code, you can take a look at the tutorial/book [Learn You a Haskell for Great Good](http://learnyouahaskell.com/). If you have any questions you can also ask here or in #haskell at FreeNode IRC :)
I've really liked both of these, I'm glad you guys are doing them. And it was interesting to hear @dons talk about how they're using Haskell.
Please don't use the `ghc-options` field in the `.cabal` file for this. `cabal-install` will support `--make -j` natively soon, so you'll be able to just say `cabal build -j` (or `cabal install -j` or set `jobs:` to `$ncpus` in `~/.cabal/config`) and have it use all available parallelism automatically.
Indeed. Ideally cabal's behavior will be upgraded to utilize ghc's -j option in an intelligent way.
Can someone ELI5 the technique used in GHC now to performs parallel builds?
Right now there is no such technique.
Hey, no need to downvote constructive criticism...
No this patch is precisely for that, there is such a technique!
This blog post is a response to Gabriel's post on perfect streaming, and the [ensuing discussion](http://www.reddit.com/r/haskell/comments/1msinm/perfect_streaming_using_pipesbytestring/).
Alternatively: (++ "zero") . concat . flip replicate "the successor of " Utterly pointless.
Next time you should do more research before you say "this style of coding seems to be out of reach for pipes": {-# LANGUAGE RankNTypes #-} import Control.Monad (forever) import qualified Data.ByteString as B import Pipes import Pipes.Lift import Pipes.Parse import Pipes.ByteString hiding (span) foldLines :: (Monad m) =&gt; (a -&gt; Consumer (Maybe ByteString) (StateT (Producer ByteString m r) m) a) -&gt; a -&gt; Consumer (Maybe ByteString) (StateT (Producer ByteString m r) m) a foldLines f = start where start a = lift peek &gt;&gt;= either (\_ -&gt; return a) (const $ loop $ f a) loop consumer = do a &lt;- wrap (takeBytesWhile (/= 10)) &gt;-&gt; do a &lt;- consumer for cat discard return a lift drawByte start a wrap p = do for p (yield . Just) forever $ yield Nothing takeBytesWhile :: (Monad m) =&gt; (Word8 -&gt; Bool) -&gt; Producer' ByteString (StateT (Producer ByteString m r) m) () takeBytesWhile p = loop where loop = lift draw &gt;&gt;= either (\_ -&gt; return ()) go go t = case B.span p t of (x, y) | B.null y -&gt; yield x &gt;&gt; loop | otherwise -&gt; lift (unDraw y) &gt;&gt; yield x I structured that to resemble your code as accurately as possible, other than the fact that it uses `ByteString`s because `Text` support is not out yet. The only difference is the use of `wrap`, but it's otherwise identical. Of course the idiomatic `pipes` way of doing this is much simpler than the `conduit`-based solution. You would just fold the `FreeT` that the `lines` generates using `iterT` from `Control.Monad.Trans.Free`.
I stand by the claim. You've created something which is superficially similar, but inherently different. The conduit solution is just another simple data transformer that can be composed with all other data transformers and consumers using all of the normal composition afforded by conduit. Your solution of embedding a mutable state which tracks an updated `Producer` is not the same. I'll grant that this is more similar to my solution than the `FreeT` solution, but it's certainly *not* the same. &gt; Of course the idiomatic pipes way of doing this is much simpler than the conduit-based solution. I don't buy this as a self-evident fact. __Edit__ Typos
for the sake of the argument (and me learning pipes-4), can you provide the simpler solution? (and i understand that there is some tension between both of your projects, but you might like to keep it nice. you are both amazing.) 
Right, but _right now_, in released GHC, there is no such technique.
Then help mature it. 
Yes, that's [what I said](http://www.reddit.com/r/haskell/comments/1msinm/perfect_streaming_using_pipesbytestring/ccdaslz). A more complete quote is: &gt; It seems like StateT with a wrapped Producer is a limited form of Conduit. Like Conduit, it has full leftover support. But unlike Conduit, it lacks easy composability with existing machinery and doesn't seem to have finalizer support. And unless I'm mistaken, working in this fashion really sacrifices the category laws that Proxy tries so hard to obtain. Are you trying to imply that I've contradicted myself? "Not the same" and "a limited form of missing a bunch of functionality" doesn't seem like a contradiction to me.
Hmm, that's interesting. Are you saying you'd want something that looks like this: lines :: Conduit Text m (Source m Text) There are issues that would have to be worked out, such as how to ensure that you couldn't write something like: (Just line1, Just line2) &lt;- lines =$= (,) &lt;$&gt; await &lt;*&gt; await and then use `line2` before `line1`. If you have concrete suggestions, please bring them up, I think there's definitely room for improvement here.
It is a problem with new cabal 1.18. You should use cabal 1.16 to build and install gtk2hs. gtk2hs uses internal api of cabal for installation process but the api changed from 1.16 to 1.18 so this has brocken installation in 1.18.
Parallel install will use an OS-level semaphore for limiting parallelism.
I'm not trying to imply any argument. I'm just wondering if you can elaborate, seeing as if StateT with a wrapped producer is a limited form of Conduit, aren't these solutions essentially equivalent, rather than fundamentally different as you claim?
If that is the intended scope, you might want to look into XULRunner. Where you could have a tutorial for your library users how to embed the application inside a webframe, acting more like a desktop application. 
Yes, I can elaborate, though I haven't fully formulated the issue yet (thus the reference in the blog post to a separate blog post I'm working on). Saying that you can achieve the same functionality with two solutions doesn't make the equivalent. Taking the argument to the extreme, Haskell and Assembly are equivalent, since anything you can write in Haskell can also be written in Assembly. Obviously, when talking about a tool, we're not just talking about its raw ability to do something, but to do something elegantly/efficiently/*insert desired property here*. So yes, by inverting the streaming paradigm and instead pulling data manually out of a `Producer`, and then making this more palatable by abstracting the boilerplate via `StateT`, you can achieve the same capabilities. But for that matter, I could achieve the same approach via a generator function and an mutable reference to store the leftovers. If so, why did we go through all of this work with enumerator/pipes/conduit in the first place? Because that solution was lacking in elegance. Code written that way is harder to understand. There are problems of deterministic resource handling. And a host of other reasons which we can get into, but which I think you already know. I believe that by forcing this inversion of streaming to achieve basic functionality, we'd essentially be losing out on many of the benefits we set out to achieve in the first place. As I've made clear in the past, I value hard examples much more than hand-wavy arguments, and everything I said above is pretty hand-wavy, so I don't expect you to take my word for it. I haven't finished analyzing things yet, but to give you an idea of the kinds of complications I'm worried about, check out the following conduit code: import Data.Conduit import Data.Conduit.List (drop, isolate, fold) import Prelude hiding (drop) main :: IO () main = do res &lt;- mapM_ yield [1..20] $$ do drop 5 res1 &lt;- isolate 5 =$ fold (+) 0 drop 5 res2 &lt;- isolate 5 =$ fold (+) 0 return (res1, res2) print res It is roughly equivalent to: main = do let list0 = [1..20] list1 = drop 5 list0 (list2, list3) = splitAt 5 list1 list4 = drop 5 list3 list5 = take 5 list4 print (sum list2, sum list5) With the inversion of streaming in pipes's fold function, I'm not sure how you would implement something like that.
Actually, this may be the kind of thing you're getting at: withLine :: Monad m =&gt; Sink T.Text m a -&gt; Sink T.Text m (Maybe a) The idea is that you provide it with the consumer you want. The wrapper ensures that all trailing input is dropped and that the final newline character is dropped. It returns `Nothing` if no input was available, though alternatively it could just feed a null stream of data into the provided `Sink`. Example usage: {-# LANGUAGE OverloadedStrings #-} import Data.Conduit import qualified Data.Conduit.Text as CT import qualified Data.Conduit.List as CL import Data.Functor.Identity main = print $ runIdentity $ yield "foo\nbar\nbaz\n" $$ do w &lt;- CT.withLine $ CT.take 2 =$ CL.consume x &lt;- CT.withLine $ CT.drop 1 &gt;&gt; CL.consume y &lt;- CT.withLine $ return () z &lt;- CT.withLine $ CL.consume return (w, x, y, z) which results in: (Just ["fo"],Just ["ar"],Just (),Nothing)
Don talked a bit about adding instrumentation to GHC, dtrace and a webserver. Does anyone have any more information about this?
It's very little in the first world. 
Sorry about the mean response. :( I was really stressed out and tired last night when I wrote it. Here are some examples using the `FreeT`-based solution: http://www.reddit.com/r/haskell/comments/1n0i29/folding_lines_in_conduit/cceg4wp
I don't have any opinion on pipes vs conduit, but I do know that liking things and especially understanding things is almost always a result of familiarity with the matter, and almost never some intrinsic property of the material itself.
Possibly - I've always been interested and often played with Conduits and Pipes in their various versions, I liked the idea. To be honest I found both of them a little tricky, then pipes 4 came along and I immediately found it quite intuitive and I've since been using it extensively in my code. 
No worries, I didn't take it as anything negative.
I'm not really goog enough yet to contribute, but I'm trying. ATM I send bog reports and try to use the language to do things. The point was that if he'd starting, using elm may not be a good idea.
Oh, thanks for pointing me to XULRunner, I'll have a look. Never seriously considered it before, but it makes more sense now that I've accepted the "HTML for desktop apps" idea.
Every tool in your toolbox can be used as a hammer, thus they are all the same. Languages can express things in an elegant manner or an ugly manner. Most are elegant is a few cases. Few are elegant in most cases.
I'm not tekmo, but I think I can answer (1). If you don't drain the producer you won't have any action to return (`result`) so you'll have to return something else, causing an early termination of the fold (see `alwaysZero`).
 Consumer (Maybe ByteString) (StateT (Producer ByteString m r) m) a Goodness, what an eyesore. Maybe Pipes.Parse should provide a synonym to sweeten up the use of `draw`, `peek`, and friends.
"So I can use whatever one I want?"
Nothing, it's nonsense.
That's cool. I cannot really judge as I not exactly stirring up a coding frenzy and every little bits. However, I do not know if the project honors [semantic versioning](http://semver.org/), but 0.9, Elm's current version would mean it is nearing the first version deemed stable by the developer. Secondly, the platform is written Haskell, utilizing some very mature Haskell libraries, and [is well over one year old according to Ohloh](https://www.ohloh.net/p/elm-lang_org/commits?sort=oldest). With an active community, I would say mature is relative. Active is better than mature from where I sit, but I know one size does not fit all.
Or, even better, [LaTeX](https://en.wikipedia.org/wiki/TeX#Typesetting_system).
Tell them to program in brainfuck for a week.
kamatsu's answer to (1) is correct. You either have to drain the producer to access the accumulation for the remainder of the list or provide a suitable substitute for the accumulator (like in `alwaysZero` or `head`). You should check out Edward's `folds` library which is a generalized version of the `foldl` library. He includes terminating folds. Right now I'd recommend his library over mine. In this case `iterT` is analogous to `foldr`, in many ways: * It is lazy and can terminate early * It is not strict in the accumulator * It is the "canonical recursion scheme" for the data structure (I think) I think you can write a strict left fold for `FreeT` but there isn't one in the `free` package, yet.
Ah. I had misunderstood, then.
"Go write brainfuck then."
Well, I don't think that type deserves a synonym just because it's very rare to fold things using `Consumer`s using pipes. `pipes` treats `Consumer`s very different philosophically from iteratees or conduits as a consequence of enforcing symmetry between `Producer`s and `Consumer`s. In iteratees or conduits, the consumer can always guarantee that it regains control after every `await`, so a consumer can be used as a fold. In pipes it is completely different: any `await` might never return. This is symmetric in behavior to `yield` which also might never return. This is why folds in `pipes` are functions of `Producer`s and instead of being `Consumer`s. So you might wonder what a `Consumer` is good for, then, if it's not useful as a fold. The role of a `Consumer` in the `pipes` ecosystem is an "exhaustible" sink (i.e. a sink that might stop accepting new values). An example of this is `stdoutLn` from `Pipes.Prelude`, which will terminate if it detects a broken output pipe. Another excellent example is `toOutput` from `Pipes.Concurrent` which will terminate if the `Output` is dead. One of the benefits of this symmetric approach is that you can safely single-step both `Producer`s and `Consumer`s, because neither one makes any guarantees of being driven to completion.
Using the `input` idiom from `pipes-parse`, this can be accomplished easily, although it is not often the style of programming encouraged by `pipes` people so it may not be very "easy" to discover. {-# LANGUAGE Rank2Types #-} {-# LANGUAGE LambdaCase #-} import Pipes import qualified Pipes.Prelude as P import qualified Pipes.Parse as PP import Control.Monad.Trans.State.Strict as S type SuperConsumer a b m = S.StateT (Producer a m b) m drain :: (Monad m) =&gt; Int -&gt; SuperConsumer a r m (Maybe r) drain n0 = if n0 &lt; 0 then error "drain needs arg &gt;= 0" else go n0 where go 0 = return Nothing go n = do p &lt;- S.get lift (next p) &gt;&gt;= \case Left r -&gt; return (Just r) Right (_, p') -&gt; S.put p' &gt;&gt; go (n - 1) isolate :: (Monad m) =&gt; Int -&gt; Producer' a (SuperConsumer a () m) () isolate n = PP.input &gt;-&gt; P.take n ($$) :: (Monad m) =&gt; Producer a m b -&gt; SuperConsumer a b m r -&gt; m r p $$ c = S.evalStateT c p main = each [1 .. 20] $$ do _ &lt;- drain 5 res1 &lt;- P.fold (+) 0 id (isolate 5) _ &lt;- drain 5 res2 &lt;- P.fold (+) 0 id (isolate 5) return (res1, res2) One could easily imagine packaging up `SuperConsumer` with `drain`, `isolate`, and `$$` into its own Conduit-like abstraction, with the implementation details hidden from the library user.
I completely agree that `Consumer` represents a useful limitation, but I still think it might be helpful to have a synonym. See the comment I just wrote in response to snoyberg, where I called it `SuperConsumer`, and defined conduit-esque operations on it. http://www.reddit.com/r/haskell/comments/1n0i29/folding_lines_in_conduit/ccek3me
 element of than wow such great so system so mind element of scala so much assume so language 
PostScript
If you want a really close analogy to conduit, you could define: type ConduitM i o m r = Producer o (StateT (Producer i m x) m) r That is the pipe type that most closely matches the equivalent conduit type. So `input` from `pipes-parse` would be the closest analog to the identity conduit: input :: (Monad m) =&gt; ConduitM a a m r
Write every program in css, and see if that sentiment sticks. I think libraries probably matter more than the language though (that said Java still makes me feel ill). 
Funny you'd ask, :) I have just recently picked this up again and is working with the next incarnation of GPipe. Some of the features Im adding support for are: * Mutable buffers and textures * Vertex array instancing * FBOs with multiple render targets * Geometry shaders * Removing dependency on GLUT 
From a semantic point of view? Yes they're the same. Pragmatics, though, is the next level of language. Some are better suited to different tasks. Also, when you go beyond a Turing model and bring in io different models work differently. This is why haskell handles concurrency so well. 
Css isn't a programming language.
Thanks! However, one thing I don't quite understand is how to run a XUL application in conjunction with a local web server. I guess I just have to write a wrapper program that forks a web server and a call to `system "XULRunner"`? And then I probably have to request the web server URL via `document.href=` in the XUL code. 
In the bug thread the author says he is only posting a minimal subset of his GSoC work. What do the remaining patches contain?
This. Simply run this in your terminal: `&gt; ruby -e "$(curl -fsSL https://raw.github.com/mxcl/homebrew/go)"` `&gt; brew install haskell-platform` Done.
If you're doing this mostly as a learning experience, then why is it so important to code using a particular platform? Haskell is a deep and rich language; learning it - in any form - will be a significant undertaking completely independent of any platform idiosyncrasies. Of course, having to install a whole new operating system for one language might be a lot to ask. Side note; it's ironic that Windows is such a red-headed stepchild in regards to Haskell, seeing as Microsoft has been so instrumental in Haskell's development over the last few years.
Something of sorts. You would actually have the localhost:port hand coded in the browser tag src attribute (in the XUL file). So your application would start up, and in a separate process execute a `system XULRunner`, which you could also monitor for the point when it returns (maybe an MVar), which would mean that the interface has been closed and the webserver is safe to shut down. Kinda hackish though. There must be a way to "orchestrate" this stuff, but I haven't had a look how the application bootstraping takes place in other applications that use XULRunner.
It's pretty much entirely a joke. If you weren't cheating it'd be far less funny.
Nice! I had to add `text` and `path-pieces` to build-depends. There's a bug in the first example: it should be main = run 3000 $ \\_ -&gt; string ... GHC can't infer the type of the handlers in Application.hs. Since it's for new(ish) people and Cabal also has new features, maybe add steps to create a new sandbox and `cabal install --only-dependencies`? When would I use WAI over full Yesod or one of the lighter frameworks? Also, does `yesod-routes` even compile on a platform that doesn't support TH? If not, there's not much point in sticking to Haskell98 and eschewing use of TH to generate routes, which (if portability concerns are moot anyway due to yesod-routes dependencies) I prefer over putting generated code into (non-Haskell) files (that the compiler doesn't track) since there's less risk of stale code causing hard-to-debug errors (with a Makefile, this is less likely but possible). If you continue to go this way, maybe add a note about your choice?
It's not mature because bugs in the compiler are still too common and certain features are missing(most notably typeclasses or something similar: without them Set and Dict are weird to work with). It's getting there, though: 0.9 was a big step forward.
This is what `input` from `pipes-parse` is for. It "reinverts" the inversion of control, letting you stream from the stored producer as if you were directly connecting to it, except that unused input is saved for later. Check out the [documentation for it](http://hackage.haskell.org/packages/archive/pipes-parse/2.0.0/doc/html/Pipes-Parse.html#g:5) which shows something very similar to the example you gave. I'll also translate your specific example, too: import Pipes import Pipes.Parse import qualified Pipes.Prelude as P main = do res &lt;- (`evalStateT` (each [1..20])) $ do runEffect $ for (input &gt;-&gt; P.take 5) discard res1 &lt;- P.sum (input &gt;-&gt; P.take 5) runEffect $ for (input &gt;-&gt; P.take 5) discard res2 &lt;- P.sum (input &gt;-&gt; P.take 5) return (res1, res2) print res -- prints: (40, 90) 
I replied to Michael's comment and you can find the answer here: http://www.reddit.com/r/haskell/comments/1n0i29/folding_lines_in_conduit/ccep9ek
I just noticed that we both provided almost the exact same answer. The only difference is that you can simplify `drain` to: drain n = for (input &gt;-&gt; P.take n) discard ... and you can also use `Pipes.Prelude.sum` instead of `P.fold (+) 0`, but it's otherwise identical to what you have.
&gt; I had to add text and path-pieces to build-depends. &gt; &gt; There's a bug in the first example: it should be main = run 3000 $ \_ -&gt; string ... Thanks! Fixed those. &gt; GHC can't infer the type of the handlers in Application.hs. What error are you getting? It compiles for me. &gt; Since it's for new(ish) people and Cabal also has new features, maybe add steps to create a new sandbox and cabal install --only-dependencies? I don't use sandboxes with cabal, though of course that would work here for people who do :) &gt; When would I use WAI over full Yesod or one of the lighter frameworks? You can consider this post a description of "one of the lighter frameworks". I use techniques like ones in this post for all of my web coding in Haskell. &gt; Also, does yesod-routes even compile on a platform that doesn't support TH? The relevant module does, and the module that doesn't could easily be split out into a seperate package. &gt; If not, there's not much point in sticking to Haskell98 and eschewing use of TH to generate routes `route-generator` is not *just* about staying H98 (though that's obviously a big motivation of mine), but also about being able to more easily see / debug the generated code than one often can with TH.
&gt; Something of sorts. You would actually have the localhost:port hand coded in the browser tag src attribute (in the XUL file). Ah, the `src` attribute of the `browser` tag, nice. I do wonder if it is possible to have XULRunner load the whole .xul file directly from localhost. This would be very useful, as we could add elements that go beyond HTML, like global menu bars or file open / file save dialogs. (But the answer is probably yes, or at least I can serve a minimal .xul file and create the other elements dynamically via the `JavaScript &lt;-&gt; WebSocket &lt;-&gt; Server` link that Threepenny uses anyway.) Concerning, the monitoring, /u/snoyberg [has pointed me][1] to a similar (and very simple) setup with qtWebKit, so I think we're covered on that front. [1]: https://github.com/HeinrichApfelmus/threepenny-gui/issues/52#issuecomment-24048032
&gt; I don't use sandboxes with cabal, though of course that would work here for people who do :) That's why you didn't know your build was broken :) &gt; What error are you getting? It compiles for me. The DMR strikes again: Routes.hs:14:57: No instance for (Monad m0) arising from a use of `homePage' The type variable `m0' is ambiguous Possible fix: add a type signature that fixes these type variable(s) In the first argument of `return', namely `(homePage)' In the expression: return (homePage) In the `rhDispatch' field of a record 
Your `withLine` is not what I'm suggesting - it is what I am saying is essentially what you are doing in your article. This is what I am suggesting: data Chunked a = InChunk a | EndOfChunk lines :: Conduit Text m (Chunked Text) Essentially, `lines` would take in bits of text and just yield them out (wrapped in `InChunk`) until a newline was found. Then `lines` would emit all before the newline, emit an `EndOfChunk`, and continue. Essentially, you treat a stream of `Chunked a` as a stream of `[a]`, with list boundaries delimited by `EndOfChunk`. Gabriel brought up three objections to this approach: * The difficulty of detecting end of input. I don't see this as a problem - it is built in in `conduit` and there is a simple `Maybe`-based protocol to do it in `pipes`. * The problem of implementing functions such as `span` or a parser that doesn't consume all the input. In Gabriel's solution, he uses `span f :: Producer ByteString m r -&gt; Producer' ByteString m (Producer ByteString m r)`. My solution is to just not consume all of the input (i.e. only export `takeWhile`) - this requires proper leftover support, but should work fine. If you want to use the rest of the input, simply sequence another pipe after `takeWhile`. Admittedly, Gabriel's solution is more general, but he doesn't use that generality anywhere in `pipes-bytestring`. * The fact that parsers that parse the whole input shouldn't be sequenced. This is solved by using parsers that don't grab the whole input, as mentioned in my second point.
&gt; I do wonder if it is possible to have XULRunner load the whole .xul file directly from localhost Apparently there is a [way](https://developer.mozilla.org/en-US/docs/Web/API/document.loadOverlay), but there is not enough context. I assume you just provide a simple overlay (which is the term used inside the XUL documentation for a piece of UI), that has a single script tag with just that call. Edit: also a nice thing if you use XUL, you can also ["intermix" HTML](https://developer.mozilla.org/en-US/docs/XUL/Tutorial/Adding_HTML_Elements), maybe that can be helpful for the foundation you already have. 
Heh, I never tried to build the code (most of it is just snippets anyway) -- if I had, sandboxes or no, I would have found the errors :) Ok, so it was the routes that were failing to build for you. I've added a type signature to one of the actions, which in tests locally is enough make it happy.
Cool, that fixes it (after you add `wai` to build-depends).
Forth is on my list to learn because it's so different. Especially with these weird little multi-computers coming out! http://www.greenarraychips.com You can get a 10 pack of multi-computers for 200 bucks! They look fascinating for stream processing. Actually any concatenative language. You can do a lot in haskell's point-free style too. 
With a slight modification you can have it return `Just r` if it drained the whole `Producer`: drain n = runEffect $ (Just &lt;$&gt; input) &gt;-&gt; (Nothing &lt;$ P.take n)
You should probably read this answer I gave to one of Michael's questions: http://www.reddit.com/r/haskell/comments/1n0i29/folding_lines_in_conduit/ccep9ek It shows how you can use `pipes-parse` to program in a style very idiomatic to `conduit`. Note that I didn't include `StateT`-based parsers in `pipes-bytestring` just yet because I was testing to see if the function of `Producer`s approach was sufficient, but now after discussing this with you and Michael I feel that I should also add the parsers, too, so that people who find conduit idioms more familiar will be able to make the transition more easily.
Nice!
Well, I guess I should retract my original and follow-up then. I watched the the video on InfoQ from the creator and played with the demos on my laptop and was under the impression it was more polished than it is. I stand corrected. That said, I fell in love with the methodology and implementation when watching the video. "Easier" server-side Haskell is obviously very appealing to me.
That's a cool little idiom, allows you to see which pipe was the one which returned! 
In my experience, language advocates really tend to hype the benefits of their language. No, languages are not all the same. Some are better at some things than others. 
If you haven't read it already, you can learn more idioms like this by reading the [pipes tutorial](http://hackage.haskell.org/packages/archive/pipes/4.0.0/doc/html/Pipes-Tutorial.html), especially the [Tips and tricks](http://hackage.haskell.org/packages/archive/pipes/4.0.0/doc/html/Pipes-Tutorial.html#g:7) section.
We had great fun blocking the print queues at uni with postscript Mandlebrot and Sierpinski triangle generators.
A small note the parens aren't necessary on the flip replicate above.
It's more of a principle thing: if I'm learning a language, I should make sure that what I'm building works at least on both Windows and Linux, so if I started on Windows, and found an obstacle, I must fix it or find a way around it. Better do it sooner than later. As for why I started on Windows, it's simply because I was using it before starting this. Also, I had this misconception that since Haskell code is very high-level in abstraction, Haskell is a modern language, and you can use the same compiler in both platforms, then there should be fewer problems for cross-compatibility. This apparently is not true, since the build processes and the low-level libraries may require a specific platform. In any case, I definitely shouldn't expect this of GUI libraries in general.
You should study the source code for `xmonad`. Not small, but very clean and readable.
Thanks for the suggestion! I did a google search and came across http://www.haskell.org/haskellwiki/Xmonad/Guided_tour_of_the_xmonad_source
Yeah, that's an excellent starting point.
[hledger-lib](http://hackage.haskell.org/package/hledger-lib) plus [hledger](http://hackage.haskell.org/package/hledger) is about 7k LOC. While probably nothing like xmonad, I think (and once heard) it's quite readable, because I wrote much of it as a naive haskeller, and I write a lot of haddock comments. I'd be interested to hear how you find it.
You know that I am (even if I may not produce much value .. do you?) part of said community. And if I had enough resources (knowledge, time, energy, ...) I would gladly do such a thing (you know of some group that do and are willing to introduce a novice to help them on their work in that field? I am willing to help if I can). You are right in one aspect: **YOU** do not owe me anything - not even a very harsh and rude comment on something you saw on the internet - thank you. BTW: I really do not want to argue anything here - so can we call it a day pls? If you need it let's say you *won* the argument ...
&gt;YOU do not owe me anything That was not what you said earlier: &gt; shouldn't the Haskell community not try and get a working solution out there? You see i am TOO part of said community. &gt;And if I had enough resources You do not need to explain yourself. No one asks you to do anything.
IMO, a solution which relies on the user having to remember and perform some invariant in his/her code in order to get proper behavior isn't a good solution. That's why I've structured all of my solutions to do the draining for the user. __Edit__: I missed kamatsu's answer somehow, and so misunderstood what you meant by "have to." I get it now: the types will require you to do the draining, it's not an unspoken contract. Fair enough, that makes sense. More generally, this issue has been tickling me recently, since conduit doesn't actually allow a transformer to provide such guarantees, since once downstream completes, upstream can perform no more actions on the stream. I started putting together some POC code to take a different approach to that, if anyone's interested I'll write up an explanation and post it. I'll look at folds. And thanks for your explanation of why you called it a `foldr`, I understand what you meant now.
I think my [comment above](http://www.reddit.com/r/haskell/comments/1n0i29/folding_lines_in_conduit/ccez8pu) applies: you can do this, and it works, but you've now lost the ability to do full composition in the `do`-block.
If any mods are listening, it would be nice if this bot was banned.
will this make it into 7.8.1?
Befunge, Malbolge, COBOL, INTERCAL ...
I wrote that package, but I don't think it is the right tool for SrPeixinho. Not-gloss is high-level, and it sounds like SrPeixinho wants low-level control, minus the boilerplate. Maybe that is lambdacube, or maybe something more traditional like GLUT or GLFW or SDL.
What does `Vector()` do that `Vector` doesn't?
Yeah! +1 for Vim :)
I am interested in that too.
For dealing with 3d (and 2d and 4d) vectors for graphics, [vect](http://hackage.haskell.org/package/vect) is pretty convenient and probably more feature complete than any other low-dim vector package on Hackage. There is also [vect-opengl](http://hackage.haskell.org/package/vect) to use it with OpenGL. For graphics I just use OpenGL, though some ad-hoc wrapper code accumulated during the years.
How about [unordered-containers](http://hackage.haskell.org/package/unordered-containers-0.2.1.0)?
&gt; I assume you just provide a simple overlay (which is the term used inside the XUL documentation for a piece of UI), that has a single script tag with just that call. Actually, I only need to add a [`script` tag][1] to the minimal .xul file. This is similar to how Threepenny works at the moment, where the server sends a minimal .html file and then populates it via JavaScript calls. There is no need to have the server send the minimal .xul folder, it's fine if XULRunner picks it up from the resources folder. &gt; also a nice thing if you use XUL, you can also "intermix" HTML, maybe that can be helpful for the foundation you already have. Yup, it looks like I can just output the same HTML that Threepenny would output anyway. For compatibility, I should add a `div` element to serve as "document body", though. [1]: https://developer.mozilla.org/en-US/docs/XUL/Tutorial/Adding_Event_Handlers 
I answered a [similar StackOverflow question](http://stackoverflow.com/questions/4369962/what-are-some-good-example-haskell-projects/4370489#4370489) awhile back.
It imports none of `Vector`'s constructors. They should have the same behavior, since `Vector` doesn't export any of its constructors anyway.
imports the type, but not the constructors, im guessing.
I didn't edit to much, and the link is [here](https://gist.github.com/eccstartup/6699353). It looks like this. I don't know how equal signs "=" ans things like "-&gt;" be modified. I works with haskell-mode. http://i.imgur.com/8JIO3tc.png
I remember yesod helping me in my understanding of haskell quite a bit. Admittedly that was a few years ago and it is probably a lot more complicated now. It is broken into lots of modules so you can sort of break it down a little.
We don't anticipate it going OOC for read-only access, it should just be read-only for a few hours. Fingers crossed, but backup options are good. Another option will be the old server which we will keep around for a while. It's at http://old.hackage.haskell.org/packages/hackage.html but we don't quite have the redirects in place yet to use it with cabal-install (we're working on that).
Does anyone actually say that, who knows even the smallest amount about programming?
I've written a small genetic algorithms library. https://github.com/mcandre/genetics Feel free to copy the project, and modify `hellogenetics.hs` to experiment.
Jekor on youtube made some videos about xmonad and other haskell projects, maybe they are usefull for you too! channel: [Jekor](https://www.youtube.com/user/jekor) Videos: [xmonad (Haskell) on Code Deconstructed](https://www.youtube.com/watch?v=63MpfyZUcrU) [Redoing Make - Haskell from Scratch #1](https://www.youtube.com/watch?v=zZ_nI9E9g0I) 
Yes.
&gt; There is no need to have the server send the minimal .xul folder, it's fine if XULRunner picks it up from the resources folder. I was assuming that the layout is generated on the fly from your libraries DSL, or that that would be the idea. Because if it is in the resources folder, then there would be an additional build step prior to launching the application. &gt; Yup, it looks like I can just output the same HTML that Threepenny would output anyway. For compatibility, I should add a div element to serve as "document body", though. What I was actually referring here was in the idea of preserving what you already have, and prefix the generation with html, as in &lt;html:div&gt;, which is valid XUL. As such you could build the interface with HTML, which would remain static (toolbar,statusbar,sidepanel,etc), and the "main body" would be the browser tag that would serve the main functionality. Either way, there are many, many ways you can do the integration; nevertheless I myself am excited for any variant you may choose.
Real world Haskell - no, don't want to jump books atm. Own project - way too early. Other two links look viable. Thanks.
No. In fact I own a copy. 
there are 4 links :D ... project euler is where most used to go next but I think hacker earth got some nice problems - the first couple of exercisms problems where kindof dull IMHO (I droped at the 99bottlesofbeer chalange) - but all of those are not exactly what you will do in your normal dev-life (well not in mine anyway) so your own little toy project might not be that bad - but take your time and enjoy yourself and haskell ... there is always more to learn :D
Not bad. Too dark/high contrast for my liking, but I do like the colors.
You should avoid `B.unpack` whenever possible. You can avoid it here by using the following function, whose implementation is hidden in the guts of [bytestring-lexing 0.4.3](http://hackage.haskell.org/packages/archive/bytestring-lexing/0.4.3/doc/html/src/Data-ByteString-Lex-Integral.html). foldIO :: (a -&gt; Word8 -&gt; IO a) -&gt; a -&gt; ByteString -&gt; IO a Yes, this is restricted to `IO`, but that's because we're inside a `FFI.withForeignPtr` block due to how `ByteString` is implemented. The only `IO` we actually do is `peek` and whatever your folding function does. Also, it's fairly easy (albeit unsafe) to coerce between `ST` and `IO`. Also, your `writeWord` function should either pass the five arguments independently, or else use a custom record type to ensure that each of the components are strict and (where possible) unboxed. This saves a lot of overhead, and is well within the spirit of bit-twiddling Haskell. Similarly, you should define a custom strict/unpacked tuple type for the elements of `table`; or alternatively, replace the table of pairs with a pair of tables (and use `Unboxed.Vector`).
I can program in any language you want, as long as it's OCaml. Sorry guys ;-)
Ok, so after the switch it will still be possible to use: remote-repo: old.hackage.haskell.org:http://old.hackage.haskell.org/ 
FIY: we're several hours into the process. The import script is running (which takes a while). So far so good.
Ah, I was wondering why the upload page had suddenly gone 404. Guess I won't be making a new release of Shake tonight :(
&gt; Either way, there are many, many ways you can do the integration; nevertheless I myself am excited for any variant you may choose. Thanks again for pointing me to XUL, I think it's exactly what I need for Threepenny. Otherwise, I would have ended up spending a lot of time on implementing something similar.
Have you considered cross-posting this to /r/programming?
Hi flebron, Thank you for setting me up with this. I really didn't understand the documentation on the Haskell site, but running "ghc" in the terminal was enough to get me started. Is there an IDE which might be good for developing scripts in? [I do have the book, and have read quite a bit before I managed to get this to work, and I'm loving it!]
Hi FPguy, Thanks for your reply. I had a look at this, but I'm hoping to avoid commercial solutions for the time being (I don't have access to an academic licence either). Do you have any IDEs you can recommend instead please?
&gt; So they say that Haskell is obviously good for functional applications, but their current applications are best implemented with objects, so Haskell isn't relevant. Well, yes, OOP is obviously good for dysfunctional applications :-b *scnr*
Impossible to respond without knowing what "our work" entails. It may be that Haskell isn't the best fit.
This is the moral equivalent of "English is the best language because it's the one I grew up speaking."
Aww, poor you; bad gateway. Hope you get that fixed. Courtesy upvote. Research is a pain, I'm in it too.
That's cool :) I should seriously read more about LH, since I've also fallen in love with DTP.
I'm doing the Matasano Crypto Challenge for a similar purpose now. I find it a good way to explore the language and the basic libraries.
Good suggestion, I'll do that. Thanks!
[These](http://www.seas.upenn.edu/~cis194/lectures.html) lectures from Brent Yorgey have fantastic exercises, and there's a good amount of overlap between the material covered and LYAH. The lectures are usually quite a bit shorter than the homework exercises, which are fantastic, and they reference the relevant sections of LYAH / RWH / Typeclassopedia as recommended reading.
I would certainly hope my applications were functional. All jokes aside, functional programming is fantastic for general purpose programming. Most of the stigma surrounding practicality of Haskell programming arises from poorly maintained bindings to GUI tool kits on Mac or Windows.
Your approach breaks the category laws. Your pull-based composition has no downstream identity and your push-based composition has no upstream identity. Also, you assume that the only way to connect or use pipes is push or pull composition. Have you considered how your solution would work in conjunction with `next` or `for`? The key function you need to write is the `unlines` function. That is the one that will give your approach the most trouble because you don't have a way to intercept end of input.
Dude I am so excited about everything to do with liquid haskell. I'm glad you're showing it off, too!
Who, Rob Pike? Ken Thompson?
Welp, Haskell(ghc) is no good for Windows, system maintenance, plugin-based, interpreted or dynamic environments, anything that needs to be compiled fast, real-time, interactive or embedded systems, scripts, perl-territory programs which can't be tested or proved, etc, etc. Haskell is most visibly used in processing data and parallel I/O on Linux mainframes, which is where you can expect it to be robust. For anything else, treat ghc ecosystem as experimental at best; betting on it will incur losses.
I use Haskell for scripting and system maintenance all the time. I also wrote a Haskell-based search engine for proteins that is realtime and interactive (code will be released very soon). Therefore I must conclude that the real impediment to Haskell programming is programmers with a lack of imagination who cannot progress beyond what they were taught in college.
Is there any progress on that front? Last I was aware there were a bunch of FRP attempts that everyone kept citing but no one seemed to like.
C++, Java, php....
We don't really need FRP so much as a well-maintained binding to a GUI toolkit that installs painlessly on multiple platforms.
A separate appropriate response is that their environment is already 100% OOP, and a rewrite would cost tens of millions of dollars.
Still not up? Is the import script sending data using ip-over-ipod?
Out of curiosity, why not this skeleton? f _ [] = X f [] (_:_) = Y f (_:_) _ = Z If `Y` needs to refer to `ys`, you can replace the second line with f [] ys@(_:_) = Y
See also "[Conal Elliott » From Haskell to hardware via cartesian closed categories](http://www.reddit.com/r/ReactiveProgramming/comments/1mm59j/conal_elliott_from_haskell_to_hardware_via/)", and please note the spelling of my last name.
&gt; good GUI toolkits Last I checked, there basically were none of those.
&gt; progress beyond what they were taught in college. Which, IME, is not much of anything.
You could also check out [hacker rank](https://www.hackerrank.com/) or [Rosalind](http://rosalind.info/problems/list-view/).
&gt; Therefore I must conclude that the real impediment to Haskell programming is programmers with a lack of imagination who cannot progress beyond what they were taught in college. There are also man-years of work to make ghc do what other platforms do since forever. Are you going to waste months or years to port ghc to a mobile platform, make an I/O manager for Windows, add thread priorities, make a GUI-binding, or implement whatever other feature that might be needed for your project, but for which no one in the academy cares because no papers can be made out of it? You're confusing lack of imagination with a lack of disregard for ones own time.
Well, if you are going to move the goal post then so will I. Some of us believe in creating a better future for everybody rather than becoming victims of circumstance.
Ask them to give up their beloved LINQ :)
&gt;I would certainly hope my applications were functional. LOL, but that's actually the response i got from one of the applicants when i asked him if he is aware about functional programming :) Also on the question if he heard about haskell he said "Oh yes of course. Pascell is the language used to teach programming." 
I wish there was a solution to C++ FFI :( 
That would still be better than using that horrible mess known as bash.
s/best/easiest/g for extra obliviousness
There's an [app](http://hackage.haskell.org/packages/archive/shelly/0.9.2/doc/html/Shelly.html) for that.
I didn't move the goal post, you simply jumped too early to conclusions; my post was a criticism of ghc infrastructure from the beginning. &gt; Some of us believe in creating a better future for everybody rather than becoming victims of circumstance. Rolling a rock up an infinite hill creates better future for no one.
I believe hackage2 is running dcoutts laptop and the Hilton WiFi isn't so hot. 
Just as a sanity check, if you consider making a library with a typeclass for it (since I don't know any, outside of the various `mapMaybe`s), it's a functor from the Kleisli category of Maybe (could be any monad, though) to Hask, it should follow these laws: pmap return = id pmap (f &lt;=&lt; g) = pmap f . pmap g I'm pretty sure the implementation of `pmap` for your type follows these laws, too. Sorry I couldn't be of more help.
Pfft, noob main = mapM_ rawSystem [blah, blah, blah, blah] 
Shelly and friends are nice DSLs, but they really don't match the *terseness[?]* of advanced shell programming. I'm looking at command substitution, here docs, etc. Plus I don't feel like Haskell adds anything in such an environment; everything is untyped text streams anyway.
you can get there with factorialMonoid I guess, but that might be too strong... http://hackage.haskell.org/packages/archive/monoid-subclasses/0.3.2/doc/html/Data-Monoid-Factorial.html I agree that a `filter` or `catMaybe` typeclass that gives `f (Maybe a) -&gt; f a` would be quite useful in general, and it would be fun to think through the implications and laws.
How would that work?
Seems pretty much bang on. I'll take a look today. I've just read through the first lecture and taken a look at the homework assignment. This is exactly what I wanted. Thank you. And let me offer a more general thank you to the other commenters for the other leads.
I use shakespeare-text for here docs. command substitution can be done with monad operators. I haven't found shell scripts or Ruby scripts to be shorter than Shelly once you add in error handling code. Even though the added type safety of Shelly is not that extensive by Haskell standards it can be worth a lot because shell scripts are usually difficult to test and can leave a system in a state that takes manual effort to recover.
I think I understand the frustration here or at least I know it well. As a contractor who moves from project to project I have run across a few different programming styles/methodologies/beliefs so I may have some insight from the perspective of someone who loves Haskell. Often I see Ruby/Javascript programmers waste 20 - 30 minutes (sometimes more) on something that would have been a simple type error in a modern statically typed language like Haskell (I've seen it today, I'll see it tomorrow. Its a fact of life). These programmers waste a great deal of time trying to remember the structure of their hashes (and often get it wrong when moving fast). Unit tests often fail with error messages that proclaim that some object is nil when it is supposed to be otherwise. Data structures are thrown around quite sloppily which of course (as a lot of literature, functional programming or otherwise, would suggest) is the root of many rough points in code and lots of wasted time. Variables are assigned one type of value and then another (on a regular basis) which makes code hard to understand. To top all of this off, these are the same programmers who would have me believe that they do not need a type system like Haskell's and rarely encounter problems that types systems solve -- or blog very stupid things that show a complete lack of understanding. http://www.jayway.com/2010/04/14/static-typing-is-the-root-of-all-evil/ That is only half the story. In Java, which has an OK type system, one has to deal with mountains of boiler plate that only 500$ IDE's can save you from (although the good work that is Eclipse has really improved as of late) . Also, modern Java uses a plethora of techniques and technologies on top of the base language (like annotations) that really obscure the code and ones ability to reason about what the hell is going on. Not to mention, some Java (and C# so on) programmers tend to get slap happy with patterns to the detriment of the simplicity and understandability of the code. However (and not to cause a flame war) C#/Java Programmers seem to be to be the more thoughtful and discerning of the non-functional modern programmers that I have encountered. All that said, Haskell (The community really) is not off the hook. I love Haskell, I can move very fast in Haskell and I write good code in the same amount of time that I would be able to just throw something together in another language. What I mean by good code is code that is simple, easy to understand, and easy to refactor. I believe that this is because the designers of Haskell are familiar with a vast body of studies that describe why programs go wrong and find reasonable solutions to root cause problems (rather than just study techniques to make the status quo easier to achieve). However, and like OOP programmer going crazy with patterns, some of us Haskellers like to go crazy with types and theory. Not that theory and types are bad its just that its scary and can appear to be impractical to the general programming public. So I think the fear of Haskell may have a bit more to do with all the theoretical names and mathematical chest-beating, that is all too prevalent when one types the word Haskell into Google, and less to do with the quality of GUI toolkits. I just love a good Haskell tutorial that shows a normal everyday programmer how to get something fun up and running. The Happstack crash course is a good example of a nice practical Haskell tutorial. Parallel and Concurrent Programming in Haskell is also very nice and readable. Implicit programming in PH is too quite clean and clear. We can probably ease down on the "How game of life is Commonadic" tutorials (although the article I'm referring to is actually pretty damn good IMHO) because even though it is interesting it shrinks the community by a great deal and makes it hard for normal Haskellers like me to make a case for using the language. That's just my two cents.
&gt; I would certainly hope my applications were functional. I hope the double entendre is deliberate.
&gt; "Oh yes of course. Pascal is the language used to teach programming." Same response from one of my engineering lecturers :(.
I would be more likely to accept the argument that "different programming languages are good for _different teams_, and Haskell isn't a good fit for our team." Retraining an entire team would be a lengthy and expensive process, I imagine! Not that I don't think it'd be worth it in the end, but it would be difficult to justify.
Thanks for the kind words. The server went down when I was asleep, I made a temp workaround, see the edit.
Cool! My road to Haskell was simply that I've been trying to get programming for a while. C, Scheme, Lua, Python, GML and Haskell. All those languages I had basic understanding of and could remember most of the essential functions in my head, still I could not design any program. Then a guy invited me to a private(?) IRC network, and there was a guy who was going a Haskell course. The course[1] material was freely available and new slides were uploaded with a week or two in between. I followed the slides, stuff I did not get I just looked up in LYAH and RWH and after a while I usually manage to make make some sense out of it. After the course I continued and read WYSASin48h. It was the first time I realized how I could break up a problem in several smaller pieces, as everyone had told me before. After that I wrote some small ugly programs, then I started learning about FRP and pipes and lenses. I manage to write a game using the Helm[2] game engine. I also contributed a little bit of code to Helm. Hope to hear how more people got into Haskell! :) (sorry for typos wrote this on my phone) [1] http://shuklan.com/haskell/ [2] http://helm-engine.org
* R * julia
Your `PartialFunctor` is rather exotic, it takes a partial function but produces a total morphism. This is possible, but not that common I think. I usually use a functor in a custom category (a bit like `anvsdt` explained): {-# LANGUAGE FlexibleInstances, MultiParamTypeClasses #-} import Control.Category import Control.Arrow import Prelude hiding ((.)) import Safe (readMay) class FunctorC cat f where fmapC :: cat a b -&gt; cat (f a) (f b) data F a = F { unF :: a } instance FunctorC (-&gt;) F where fmapC f = F . f . unF instance FunctorC (Kleisli Maybe) F where fmapC f = arr F . f . arr unF total :: F Int total = fmapC (*2) (F 1) partial :: Maybe (F Int) partial = runKleisli (fmapC (Kleisli readMay)) (F "10") (edit: Damn, I miss the old GHC's TypeOperators)
Even more interesting using the `newtype` package: import Control.Newtype (under) fmapK = under Kleisli fmapC partial :: Maybe (F Int) partial = fmapK readMay (F "10") 
Cocoa is good (I just don't like Objective-C). 
Is it essentially a functor from the Kleisli category for `Maybe` to Hask? Or does it have some sort of extra requirement, such as discarding exactly the positions where the input functions return `Nothing`?
I for one am working on it: [threepenny-gui][1]. [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
I had no good reason. My code looked more like this: f :: [a] -&gt; [a] -&gt; R f _ [] = X f [] ys = case ys of (U x:ys') -&gt; fu x ys' (V x:ys') -&gt; fv x ys' (W x:ys') -&gt; fw x ys' f (_:_) _ = Z until I figured this was more convenient/descriptive of what I was doing: f :: [a] -&gt; [a] -&gt; R f _ [] = X f [] (y:ys) = case y of U x -&gt; fu x ys V x -&gt; fv x ys W x -&gt; fw x ys f (_:_) _ = Z It just irked me that GHC wouldn't pick up the exhaustivity, and wondered how hard it would be to actually do so. The problem is still there, though. I'm working with a tabular worklist, and it's hard to make GHC believe that the worklist inside `y` (and the `ys`) has the same length as `xs`. I might want to implement this in Agda instead... later.
Really wish I had known this was on!
We wanted to reach out to the wider community, but the numbers filled up so fast that doing so would have just created a huge waiting list. There's definitely a demand for further events like this in the future :)
You are not allowed to post your own language :o
I like it! Is it also possible to use this as a plain web service, i.e. for evaluating diagrams without creating a new paste first? Somewhat related: Would it be possible to render diagrams from arbitrary URIs? This way, I could render code from a public [etherpad][1]. [1]: http://etherpad.org/ 
I don' believe the reason is with the lack of GUI solutions. The bigger problem is that in Haskell, there are still no standard ways of doing things and you have to do a lot of research for fairly minor problems. For example, for a beginner it is not obvious that the lens package solves many problems, and even then, finding out how to use it is non-trivial. See for example [my post from a year ago](http://www.reddit.com/r/haskell/comments/nhpt8/question_what_is_a_data_structure_to_use_for/). The ecosystem is okay, but is missing many essential things. Mostly practical tutorials, **understandable** documentation, etc. Being able to quickly find out how a particular, commonly occurring problem is supposed to be solved in Haskell.
I am trying ;-) http://github.com/wavewave/fficxx
So that is why Hackage 2 took so long? They built an entire OS into it? (alternatively you missed an "on" there)
That's not a counterpoint, that's the same point.
Discarding the Nothings would be the only way to implement it, I think. That's certainly what mapMaybe does.
Yes, turning a partial function into a total mapping is sort of the whole point, otherwise I could use a normal Functor instance.
Being a fairly new Haskeller still (I've only been around a couple years), I can say my experience has been the opposite of this.
That's very cool.
Well, more of a counter to a misuse of the point: it is just as possible to treat FP as the only solution as it is to treat OOP as the only solution.
How are all of the other languages able to keep up? There are lots of languages out there with QT bindings, no?
&gt; otherwise I could use a normal Functor instance How would you do that?
&gt; data Validation a = Includes [(a, Text)] | Check (Text -&gt; Maybe a) I don't know the intended implementation, but it's possible to write such a functor for this type that's different, right? For `Includes` you might choose to discard elements of the list when `a |-&gt; Nothing`. It lets you create an instance without `mempty` anyway.
Not necessarily in this case, but an obvious candidate would be lists, which (seem to) have two implementations.
All the other languages aren't. Popular languages are, because they have lots of people to draw from for the pool of potential suckers to do all that work.
Hi all. I was the mentor of this project. I had a lot of fun doing this project, discussing things with Dan, seeing it develop from a slow start (Not his fault, just lots of admin work to get the whole sandbox system up and running) and then really taking off, with several libraries spun off and released on hackage already, with Dan taking more and more initiative to improve things, even if it was not required for the project. I'm looking forward to helping polish the result, in particular performance and user interface for interactive results (if performance is slow for those, that's entirely my fault, fortunately it's also entirely fixable). After that I think we can extend the application beyond just pastes, I really like the idea of having interactive code with rich web-based results for teaching and explaining things, so extending the reach to for example compute results for weblogs, a wiki or documentation pages is definitely one of my goals. We'll need to see how the server holds up though, perhaps upgrading it. I'd like to apologise to Dan for forcing him to work with unfinished technology (GHC HEAD, but GHCJS in particular), but I hope in the end it was worth it, GHCJS in particular has benefited hugely. With the GHCJS patches having been merged in GHC HEAD in time for a GHC 7.8 release I think we're shaping up to make a good first release in a few weeks, although there's still lots of work to be done (Much of it Cabal related, but also finishing the port of the non-concurrent runtime to the new (Gen2) architecture, for people preferring smaller code size over full Haskell support, and some prettyprinter tweaks to make closure compiler minification working again)
Well I mean all of the language that *are* able, but your point about the unwashed masses fighting bit rot is well taken. As unwashed as we are, I don't think we can be called the masses yet.
Anything that can be locally pruned. List-like things, trees with elements at the leaves, things with a built-in notion of nil.
&gt; lists, which (seem to) have two implementations Which two would that be? There's `mapMaybe`... what else?
That's the one I was thinking of, but I got thrown by tailcalled's mention of `someSortOfEmptiness`... which I can't now see how it would work.
 g :: a -&gt; Maybe b fmap g :: f a -&gt; f (Maybe b) The whole idea is to collapse that `Maybe` down.
I get horrified looks from people when I mention that I program in Haskell. I always come to find out that they thought I said Pascal. Really, guys. HAS-kul, pas-CAL.
They don't have showers in the ivory tower! :D
Yes, you don't want the maybe inside the `f`. But you could still pick between: :: (a -&gt; Maybe b) -&gt; f a -&gt; Maybe (f b) or :: (a -&gt; Maybe b) -&gt; f a -&gt; f b ? The first one (my `fmapC`) being a bit more general. Maybe to general for your likings.
&gt; There are also man-years of work to make ghc do what other platforms do since forever. Did those man-years not go into gcc, then? Were all the venerable C compilers let loose on our world fully formed with support for mobile platforms that didn't exist, yet?
Whoops, got the order of `.` messed up. `maybe [] id . traverse f`
We would want a law that excludes `maybe [] id . traverse f` from being valid, correct?
In general, mostly they work more easily because the languages deeply share a C core, so sharing bindings is not as foreign as it is to Haskell. QT is actually much more difficult for being C++, but people also put more effort into it, and bridging Python and C++ is still not that difficult as both are mutable-state OO languages. They don't see eye-to-eye, but they aren't philosophically incompatible enough to break the system either. As I was hinting above, you can still get a long ways by mapping C++ classes to "something", slots to "something", and a few other of the main concepts to "something". It's been done, I believe, at least twice now if you Google around for Haskell and QT, but also as I said, the remaining work once that is done is still quite substantial. And the QT bindings still take quite a lot of effort, even in the philosophically-compatible languages; PyQT is maintained by a hydrid open-source/commercial project, not just an open source one, for instance. And then, yes, QT has bindings only for the really big ones. Haskell's community is still not the size of Python or Perl, for instance, though growing quite quickly.
If it can't be locally pruned, it can't be a member of `PartialFunctor`, pretty sure.
http://www.reddit.com/r/haskell/comments/1n5ess/partial_functors/ccfsh5y
Pfft, noob main = replicateM_ 4 $ rawSystem blah
That depends on your instance, you can choose what to do with failure. If you can nicely embed your failure (`Nothing`) in your type (let's say the empty list), you can just return `Just []`. If you cannot get rid of the failure your can fail altogether returning `Nothing`. That's why the top one is more general and that's why I like that approach. 
What I'm trying to say is that it isn't obvious what laws are necessary to prevent `maybe [] id . traverse f` as a possible implementation.
But I don't want to return `Just []`: pmap (\x -&gt; if x &gt; 2 then Nothing else Just x) [1,2,3,4] === [1,2] The semantics of `mapMaybe` mean you don't pancake the whole set just because one item fails, you just drop that item.
Well, it can be if you can think of a global failure mode like tailcalled is doing with `maybe [] id . traverse f`.