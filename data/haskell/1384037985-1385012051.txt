Mandatory static checking *is* an impediment to pedagogy. It makes the system as a whole more complicated. This may well be a worthwile trade-off, but with a more complicated system there is simply more to learn. Optional static checking, where some kind of theorem prover tries to prove or disprove the annotations statically, and when it can't decide either way just does run-time checks, yea that wouldn't be an impediment to pedagogy.
I don't understand what your point is? Parsing a language syntax is about the easiest thing you can do in haskell. Easier than in most other languages in my opinion. If it is text it is easy, or if it is in xml or json it is just as easy.
This comment isn't very descriptive. What isn't keyboard friendly about it?
PS: Did you actually try it out? I'm kind of worried that people won't figure out that the link at the top of my page links to the IDE. Furthermore, my access logs show no requests to the actual visual IDE since I posted to reddit.
I didn't even realize the link was there until I read your comment.
Is this better?
haha! pretty clear indeed!
I'd actually recommend two improvements: * Don't link it until you've properly introduced it. If you put it at the beginning the reader doesn't yet understand why they would want to click the link. * Perhaps use a button to link to the editor. Buttons are more visually striking than underlined text.
Great paper, but I think it is far from game developing, more like a toy program instead of an industry game, so I donnot think it would be attractive for game developers other than language geeks.
This looks to be a step in the right direction, but if they’re not basing the design on actual programming language usability research, then their claims that it’s suitable for teaching are unfounded.
Very cool. I think the application would benefit from a simple step by step walk through with how to get a simple string to display in elm with `plainText`. I figured it out but mostly through trail and error. Switching modes on mac laptops is uncomfortable since you need to hold down the function key, bottom left of keyboard, and a function key at the top of the keyboard. Even after turning off other functionality for control-arrow the text fields are still getting some keyboard input from control-up or control-down which leaves extraneous characters in the code, parent, name fields if I try to traverse the structure while in those modes. This means I have to switch to explorer mode before traversing making it hard to to look at how code flows through the program. Alternative keybindings mimicking vim might be a boon. `j` to move to a child node, `k` to move to a parent node, `h` and `l` to move between siblings. Vim/vi uses `i` to move into insert mode something similar could be used for both editing code and/or name. You have implemented a modal editor for graphical elm and vim is the widest used modal editor that I know of so mimicking its key bindings might be a good idea. Is there any method for viewing the code in a node other then traversing to the node and switching to code insert mode. It would be nice to move through the tree structure and see the code as you do so, or a method of switching between displaying all of the node names and the code in the node would be helpful.
I wouldn't call this adding back recursion, because it relies on the type level recursion to encode the value level recursion. Remove recursive types, and it won't work. 
I actually presented this today at the Elm conference at Prezi, and it was my first time using a mac(I use linux, but only had a VGA port, apparently the world has moved up to HTMI...). I didn't expect these keyboard problems... The vim bindings are certainly planned. The editor is written in Elm itself. The main trouble is that Elm has yet to expose a way to figure out which widgets are focused. The keybinding was very limited by just generally having to live inside the browser. You'd be shocked how many Ctrl+&lt;somekey&gt; combos are pre bound :/ I'll deffinitely work on this though. I *just* implemented that(on your request). Try CodeView mode. 
I did read your comment. I am thinking about it. However, it's a surprisingly complex decission for me, and I should be asleep right now.
What I want can't be done in a dynamically typed language anyway. :)
Unless you mean deferring type errors I don't really see what you mean. But yeah, I suck at Haskell without -fdefer-type-errors. I need to experiment with my functions to know what's wrong. 
It seems to be a response to "why calculating is better than scheming"
Dependent types ho!
Yeah, I'm a Haskell enthusiast, but that takes things way, way too far.
&gt; Haskell is the purest functional programming language. I have yet to learn either of these languages, but wouldn't lisp programmers take issue with this idea? Haskell certainly seems to have a paradigmatic integrity, but from what I understand does not Lisp also sport a similar conceptual purity?
Hi, yes this is very true. That's why I specified recursive functions at the top of the post, I simply forgot to add it at the bottom. This of course relies on type level recursion. (I'm the author)
Pure in this sense is normally mean something along the lines of functions actually act like mathematical functions, one input corresponds to one output. The lisp/scheme I have read and wrote does not typically focus on this style and at least the scheme specifications I have read do not focus on it either.
How is blowing up with a runtime type error pedagogically better than an error at compile time? You've just delayed the real problem.
To be fair, assignment ought to be considered extremely low level and all of those languages have it.
&gt;lisp programmers take issue with this idea? No. Lisp has mutation all over the place. The purest dialect of Lisp in common use is probably Clojure and it still lets you do unrestrained mutation via Java calls.
I'll bet someone could make a case that python is closer to assembly than to haskell...
The problem is not when stuff blows up. It is that to learn haskell, you need to learn **two** languages. The one for types and another for values. Not a big problem for us, but for first year students, who cannot yet even match parens...
Thanks for your reference 
The obvious solution is to use dependent types - same language for types and values :D
Thanks for commenting... But I design this.
RankNTypes is one way.
Python lambdas in particular can let you code in essentially the same way as you would in Haskell. The difference is the lack of monads and greediness, not things a novice is likely to need anyway.
For reference, this huffpost article is just a repost of Aaron's recent [article in Dr. Dobbs](http://www.drdobbs.com/architecture-and-design/in-praise-of-haskell/240163246/). Here is the [reddit thread](http://www.reddit.com/r/haskell/comments/1pvjij/in_praise_of_haskell_dr_dobbs_journal_long_article/) about it.
Framework code looks correct to me. I think it should draw if you add "SDL.flip surf" below SDL.blitSurface (although I think your blit is wrong for what you want; try it with a more interesting image than just a red square to see what I mean)
Pyret is strict and has side-effects that aren't tracked in the type system. So if it was a simplified X, it would rather be simplified ML. I don't think any of those characterization holds though, as it has a distinct contract-checked character that probably corresponds most closely to Racket. 
Yes, but they are ["confusing" people](http://www.artima.com/weblogs/viewpost.jsp?thread=98196) :-D
The code example is pretty disingenuous. The problem is "Create a list of all even numbers up to 100, and another list omitting the first five of them". They give a complicated seven-line Java solution, to contrast with the two-line Haskell solution let a = [2,4..100] let b = drop 5 a But the same thing in Python is a = range(2,101,2) b = a[5:] or in R a = seq(2,100,by=2) b = a[6:length(a)] or even in Matlab a = 2:2:100 b = a(6:end)
A follow up to this thread, alexander_b any chance of you releasing this in another format being e-reader friendly? Something readers can re-flow as they see fit or automatically? Regardless, great work and thank you!
Every example comparing one language to another is almost always total bullshit. I fucking love Haskell but I hate how lame many of the comparisons to other languages are. The benefits of Haskell aren't immediately apparent and are completely lost on most people until they learn it. If you don't know it, learn it and have your mind blown. It's that simple. It doesn't help you write one liners better. It helps you write correct, fast, and beautiful code better.
&gt; b = a[6:length(a)] Can be `b = a[-(1:5)]`, negative indices correspond to deletion in R. (FWIW.)
To be honest the type inference system is so good unless you're going out of your way to do something weird you will never have to declare a type when you're learning if you don't want to.
You can create types at runtime in various kinds of dynamically-typed languages, from Common Lisp to JavaScript. The idea though is that in Haskell, types are there to check the validity of a program before even running it; and in this context, runtime types don't make much sense.
You might not have to declare types, but you do need to start understanding the type system fairly quickly. Or, that's been my experience any who.
Not "pure" as in "scrupulously respects the principles". "Pure" as in "does not let you do side-effects".
Because if you have *mandatory* static checking, you now need to understand the peculiarities of that checker to write a program. The kind of static checkers that can check Pyret's assertions statically are *not* simple but they are approaching full blown theorem provers and due to the halting problem can never be complete. So to write programs the programmer will need to understand in which case something will be accepted, in which case something will be rejected because it is an actual error, and in which case something will be rejected because the checker isn't smart enough. That is, unless you want to teach programmers cargo cult programming: just do things kinda like the things that worked in the past without any real understanding of why the static checker accepts them.
You could ensure that all possibilities would typecheck if they existed.
or even: List&lt;Integer&gt; a = ListUtil.fromToBy(2,100,2); List&lt;Integer&gt; b = ListUtil.drop(5,a); Nothing stopping you doing this in Java..
This article is a definite improvement over the Dr. Dobbs one. That one stretches the truth by making stupid statements like: &gt; If the code compiles, it works (almost all the time)! In that article, he also implied that Repa enables you to somehow automatically parallelise your code without changing a single line of it, which is totally false. He's fixed those issues in this article, but he still makes the dishonest comparison to Java with that stupid code snippet, and broadly misrepresents imperative programming.
May I also suggest: When you launch the editor, launch it with a small program already loaded so that people can get an idea of what a program looks like without having to write one or download the example file at the bottom of the page.
I am not talking about dynamic typing. Because that just fails at the moment you attempt an invalid dispatch. Rather, I am thinking that it could be possible to incrementally type-check or re-type check entire program parts when new types emerge. Just a thought...
Isn't that exactly what IDEs do?
Just make sure you pull the version from github and not hackage. They are currently doing a rewrite of some stuff so they have postponed pushing the latest version to hackage iirc. and the one on their now has problems with drawing lines etc.
Ha, that was the problem. :)
I never really got why say "Hoge" instead of hello. I noticed explicit use of forkOS which of course binds to a kernel thread, but is there any support for forkIO? According to https://github.com/ajhc/ajhc/blob/0be313fea254039803d444e503c1c44bcf9b7dbd/lib/haskell-extras/Control/Concurrent.hs `forkIO = forkOS` So that's a no, in terms of efficient user threads.
That's a good step to take though, for a teaching language. I strongly considered teaching a class once with Bootstrap (http://www.bootstrapworld.org/), but was dissuaded by not wanting to waste too much time teaching a new syntax for addition. If this is a version of that, but lets people write basic math operations in the normal style, that seemingly shallow point makes a huge difference in early programming education. I'm interested in finding out more about functions and effects, though. I'd be hesitant to reinforce the notion that functions are verbs. I wonder if that's something that could be effectively glossed over by building a purely functional naive FRP system like Bootstrap or Gloss, without leaving trap doors that students would be likely to encounter.
In short, FRP is a way of composing things. I wrote an admittedly hand-wavy blog post on [my thoughts about FRP](http://just-bottom.blogspot.fi/2013/09/a-few-reasons-why-functional-reactive.html) recently. The bottom line is that you can think of signals as light-weight objects (at least as little pieces of encapsulated state), and FRP gives you a way of organising them into full applications in a disciplined way.
I second this. We use xml-conduit extensively in production and it works great. About those cursors though - they are a functional approach to DOM-style XML parsing, and there is definitely a learning curve to it. But that's actually orthogonal to xml-conduit. You can use cursors for any XML library; the types are generic. And conversely, you don't need to use cursors in xml-conduit - the initial parse of an XML file produces a library-independent XML representation using [xml-types](http://hackage.haskell.org/package/xml-types), either structural (DOM-like) or event-based (SAX-like), just like any XML does (or should).
&gt; it still lets you do unrestrained mutation via Java calls. How does this differ to the Haskell FFI?
You have the IO monad to at least give you a heads-up. In Clojure you can stick Java calls anywhere; it's like an implied unsafePerformIO.
This looks dangerous: passwdOverwrite = do overwrite &lt;- readfile "passwd.config" --leave this a lazy overwrite for now. writefile "/etc/passwd" overwrite Looking at the docs, writeFile truncates the existing file. So, if writeFile begins but something happens that causes it to not finish its write (e.g. a power outage), you'll end up with an empty or invalid /etc/passwd. Instead, write your new /etc/passwd to /etc/(some unique name) and then move it to /etc/passwd. mv uses rename(), which is atomic on *nix.
Eh, I don't mean to hijack the thread but I dunno about using high-order functions (or signals) for FRP. It becomes impossible to save the state of the network and load it again. Doing stuff like OP's post and Elm's general direction where you step a model is a lot easier to save If I'm wrong on this, I'd like to know because it's been a kind of terrible thought since I've been planning to actually use FRP in an application, with state saving and hot swapping.
More interesting would be the ability to specify which transliteration/transcription system to use: Yale, McCune–Reischauer, ISO 11941, Revised, etc. 
Who said it was? My claim is that teaching students to deal with runtime failures when we have a tractable method for teaching them to avoid many of these failures in the first place is a backwards way of teaching. Types are not an impediment to pedagogy.
Now instead of learning X you have to learn X+Y. As I already said way back at the beginning that may well be a good thing, but it *will* be more difficult than teaching a system where static checking is optional, so you can start with X and later teach Y. X = debugging, Y = making use of the static checking
I agree. I think that the philosophy of reactive programming in general is to relieve the burden of plumbing networks of interrelated values. A wonderfully productive example of this is the way in which a spreadsheet keeps values in cells up to date. The pure FRP libraries available for Haskell are much more exploratory than pragmatic. 
I find this page impossible to read effectively and I am unable to figure out how to use the editor. Consider not using F-keys as I have to hold down an awkward key combination to use them on my laptop.
That library is based on [Elm](http://elm-lang.org/), which has [tons of resources on learning FRP](http://elm-lang.org/Learn.elm), from ["what is it?"](http://elm-lang.org/learn/What-is-FRP.elm) to [usage examples](http://elm-lang.org/Examples.elm). It's true that the same sorts of things can be expressed in an imperative way, but I think FRP in Elm ends up *enforcing* much cleaner structure in interactive programs. 
No. types are a semantic notion. Of course you can construct types at runtime--even in haskell. You just have to wrap it in an existential... {-# LANGUAGE DataKinds, GADTs, TypeOperators #-} data Nat = Z | S Nat data SNat n where Zero :: SNat Z Succ :: SNat n -&gt; SNat (S n) data SomeNat where SomeNat :: SNat n -&gt; SomeNat data (n :: Nat) :==: (m :: Nat) where Refl :: n :==: n areSame :: SNat n -&gt; SNat m -&gt; Maybe (n :==: m) areSame Zero Zero = return Refl areSame (Succ n) (Succ m) = do {Refl &lt;- areSame n m; return Refl} areSame _ _ = Nothing nextSN :: SomeNat -&gt; SomeNat nextSN (SomeNat n) = SomeNat (Succ n) natify :: Integer -&gt; SomeNat natify 0 = SomeNat Zero natify x | x &gt;= 0 = nextSN . natify $ x - 1 we can create types at runtime with this trivially main = do putStrLn "enter a number" n' &lt;- readLn putStrLn "enter another number" m' &lt;- readLn SomeNat n &lt;- return . natify $ n' SomeNat m &lt;- return . natify $ m' Just Refl &lt;- return $ areSame n m putStrLn "if the program hasn't terminated in an exception you entered the same number twice" the syntactic type of `n` in this code is existentially quantified, but in reduction it is replaced with some concrete type and that type both * is not known ahead of time * could be one of an infinite number of possibilities the reduction rule for existentials is something like unpack (pack t in x) as x' with t' in y ==&gt; y[x' &lt;- x,t' &lt;- t] which seems like a pretty clear instance of types being created at runtime. Honestly, this thread is evidence that the haskell community does not understand what "all types are static types" means. There is nothing about the notion of "type" that says the type of something can't **depend** on something dynamic. The point that dynamically typed languages are uni-typed does not change this fact.
&gt; There's no such thing as types unknown before runtime yes there is. and they happen in haskell. All the time. what is the type of `x` in `id x = x`? The answer "it is a type variable" doesn't cut it. Types are a semantic notion. This purely static view only captures a shallow syntactic part of the notion of type. EDIT: this has been downvoted--so I will extend the explanation slightly. I'm sorry if I come across as pithy or rude, a lot of nonsence is being spewed on this thread and that disapoints me because normally r/haskell is a pretty intelligent place. The type of `x` is variable in the same sense that the value of `x` is. In system f we would have $\Lambda \tau : \star.\lambda x : \tau.x : \forall \tau : \star. \tau \to \tau$. At a particular instatiation a $type value$ for x is chosen. But, and this is the crucial thing, when and where instatiations happen is a runtime property of the system--not a purely static one. In the presence of Rank2 or existentials types can not be "specialized away" since the selection is dynamic. Thus, polymorphism entails "runtime type selection". I advocate for "type free semantics" in the sense that types don't determine reduction rules. But, these don't change the fact that everything is typed, and the type of things can depend on dynamics. Types of code are static and syntactic. Types of the semantic artifacts that exist dynamically are semantic and dynamic.
That's a contrived example. Thx to haskell's innate polymorphism a function can return its own argument unchanged. But in order to do something (anything) with data you MUST know its type. Otherwise you won't be able to do anything with it, besides of course simply returning it as is. 
Wow, I'd never heard of Helm until today. I was hoping to write a game for the next Ludum Dare in Haskell, and this might just do the trick... I know it's not the point of the post, but thanks for opening my eyes to the library! I think Elm is super awesome but I don't generally do any web programming, so this looks like a nice alternative for pure Haskell.
This is a bit off topic but I just installed helm using cabal and I can't compile any of the examples because it states 'startup' is not in scope. Any suggestions or directions as to where to go to get this fixed?
What version? Hackage package should work, the git version should not(recently got updated). EDIT: Oppsite I mean, the exampkes require you to use 0.6(the version in git).
&gt; The only price we have to pay is a linear use of extra space (arising from the use of scanr1), which is cleverly avoided in Michael Kozakov’s imperative solution. The difference is not due to functional vs imperative though, only the lack of random access to the input data. Using a (pure) array, the functional version would also use constant space.
There is also the option to trade space for time and use scanl1 max . reverse instead of scanr1
This requires two passes over the data via scanr/scanl though. I wonder how this can be unified to one only.
Of course, but how do you serialize higher-order function (signals) state?
&gt; For instance, you have to be able to present a coherent view of the system’s state to be able to render a frame. If you can do that, you can do the same for serialisation as well. Well, no, because when rendering a new frame is triggered by signal changes in your FRP "network", than you most likely don't also want to serialize it. If a lot of your application state is hidden inside of your FRP "network", which is one of the selling points of FRP, that you have separate, abstract modules and you just connect them and they just work, than you just can't get at the whole application state at once to serialize it. 
You are bringing in a huge amount of machinery behind the scenes, so the web app you are describing is far from "tiny". Setting up a working dev environment for a full-blown professional web framework in a compiled language is complex. It's a much bigger requirement than just an environment where you can code, compile, and debug using a basic set of libraries, which is what HP is designed for. If that's what you need, maybe try out FPComplete's new web development environment. It's designed for that. It should work right out of the box for what you want to do.
The best I could come up for a single-pass, constant space solution is this primitive recursive version: import Data.Array waterVolume :: Array Int Int -&gt; Int waterVolume arr = go 0 minB maxB where (minB,maxB) = bounds arr go !acc lpos rpos | lpos &gt;= rpos = acc | leftHeight &lt; rightHeight = segment leftHeight 1 acc lpos contLeft | otherwise = segment rightHeight (-1) acc rpos contRight where leftHeight = arr ! lpos rightHeight = arr ! rpos contLeft acc' pos' = go acc' pos' rpos contRight acc' pos' = go acc' lpos pos' segment limit move !acc' !pos cont | delta &lt;= 0 = cont acc' pos' | otherwise = segment limit move (acc' + delta) pos' cont where delta = limit - arr ! pos' pos' = pos + move But that's horribly unreadable compared to the scan-version and prone to off-by-one errors. I wonder if there's a more beautiful solution with the same space and performance characteristics.
How does the solution to this problem apply to Twitter's day-to-day coding or algorithm needs? Or is this just a "interesting" problem unto itself? I greatly dislike the latter class. 
Efficient and elegant algorithms and data structures ought ot be of enough relevance.
Because why not, I tried to write a quickcheck test for this problem (ignoring for a moment that I needed the function I was trying to test to find bugs in the implementation of my reference function); I ended up writing something that simulates the islands and water and has water flow off the islands until fixpoint. Code [here](https://gist.github.com/rpglover64/7412884) (apologies to sensitive eyes).
&gt; the existence of a function from compile :: String -&gt; Maybe a can be an assumption of your system without major problems Wouldn't such a function have to be `const Nothing`?
"hoge" means "foo" in Japan. :) Ajhc does not have user thread yet. 
The time traveler's solution: import Control.Monad import Control.Monad.Tardis water :: [Int] -&gt; Int water = flip evalTardis (minBound, minBound) . foldM go 0 where go total height = do modifyForwards $ max height leftmax &lt;- getPast rightmax &lt;- getFuture modifyBackwards $ max height return $ total + min leftmax rightmax - height
Ha, that's beautiful. Sometimes the Tardis is quite useful!
&gt; only the lack of random access to the input data. Using a (pure) array, the functional version would also use constant space. You don't need random access or an array, a bidirectional range of immutable numbers is enough (warning: D language code adapted from a Java solution by Michael Kozakov): http://dpaste.dzfl.pl/af9612af8
I'm sure it doesn't. Usually these questions are geared to generate discussion. Stuff like How do you react when you are faced with a new problem? How do you act when you get stuck? Can you come up with at least one solution? If the solution is sub optimal can you at least explain why it is? Can you brainstorm alternatives? Etc etc The point being that I'm sure twitter doesn't spend all day solving water problems, but the interviewer can glean a whole lot out of asking it. Also getting the right answer is never really the ultimate goal. You'd never get hired if you just sat tere silent for 20 minutes then wrote out the most perfect answer ever, since nobody can work with someone who can't communicate. Anyways, it's a multi faceted question
Have you looked at this four part step by step tutorial in the School of Haskell on creating a file service? https://www.fpcomplete.com/school/advanced-haskell-1/building-a-file-hosting-service-in-yesod There is also a project template that you will be open in a free community edition or FP Haskell Center(TM), which will be released in a few days. 
Yeah, it's a pretty terrible problem to use for both comparison's sake and practicality's sake. It's kind of a weirdly simple, bizarre thing to even write. There are better examples of Haskell's conciseness and clarity, e.g several sorting algorithms. They should have just used one of those!
Which, on the one hand, is still uglier than the others, but then that's not really what Java's built for.
I started typing "Overloading on return type, which is a huge part of why monads play so nice in Haskell, is..." and was going to tie it to static typing. And then I started wondering. Strictly, it should be *possible* in a dynamically typed language, right? 
Ya, I got that, I would just change your statement that "it is probably better to just use `fromList`" to "it is *definitely* better". :) Not only for correctness, avoiding silent corruption, etc, but also because as you say, it is not even appreciably faster to use `fromDistinctAscList`.
Fair enough. I'll take the deserved abuse for slipping in a weasel word. =)
Sorry, but just I don't think silently returning a corrupt results is an okay thing for a function to do. In my opinion, functions should return either a valid result, or bomb with an error, return `Nothing`, etc. Are you seriously suggesting that it's okay to introduce _undefined behavior_ into an API to save a few percentage points speed or a log factor? Also, it's quite easy with `Map` to screw things up. Imagine you've persisted a Map on disk, and you then change the `Ord` instance for the keys. Be prepared for nondeterministic bugs that the typechecker won't alert you to! Or imagine you've got a client and server, running different versions (perhaps unbeknownst to you), one of which has a different `Ord` instance for the keys. Once again, prepare for nondeterministic bugs! These are entirely reasonable situations that occur all the time in real systems, and your response of "don't do that" is just silly.
There's a Control.Monad.Tardis? Yes, yes there is. *Laughs hysterically*
As much as I would love this, all I've ever done with Haskell is hobby level work and I still haven't really rounded the bend on that deep understanding of its concepts. There needs to be more entry-level positions for Haskell! I'm basically stuck in Python land because its so much more sell-able. 
Lucky you, I'm in php-land right now.
I guess its all relative. Thanks for some perspective.
Hm, looks like fun, and pretty close to where I live too! Shame I'm a bit busy at the moment. :S
If I'm going to change the behavior of a serialized data structure, I'm going to version it. In fact, I'm *not going to trust the serialization library* to do these checks for me. Edit: To be clear, I'm not saying we shouldn't make `binary` and `cereal` safer. I'm saying it's foolish trust them to be so safe either way. The kinds of issues that can arise are very difficult to foresee.
Actually it requires 4 passes, scanl1, scanr1 and two zipWith, you can do loop fusion to get rid of most of those though. If you used arrays or a doubly-linked list or something similar that can be traversed from both directions you could do it in one pass.
Not exactly sure why, but the generated source code made me chuckle, for example: dpi_wizard_flaming.design_virtual = soft.wavelength_uddi_mouse(kindle( unfriendBar, barebonesHard));
Hot swapping would be taking the current state, reloading the pure code and then restoring the state. That's easy to do if you have lots of stepping functions without state in themselves, but things like signal networks which rely on previous versions of itself... Eh, I don't know. Elm does it at a language level, which is kind of hard to do in Haskell.
You've avoided answering my very simple question. :) &gt; Are you seriously suggesting that it's okay to introduce undefined behavior into an API to save a few percentage points speed or a log factor? Look, it's very simple. Silently returning corrupt results is bad. I'm sure you would agree with that, as would any sane developer. :) That there are (error prone) ways of avoiding this corruption (like remembering to increment a version number when you make another change elsewhere) is irrelevant. We should just avoid this possibility entirely if its easy to do so. In the case of `Map`, it's a total slam dunk, OF COURSE you deserialize using `fromList`. Also, I don't really agree that "the kind of issues that can arise are very difficult to foresee". All you have to do when defining a serialization format is _not use any magic internal functions in the deserializer_. It's actually quite simple! If you use only public functions, which establish invariants or bomb with an error, then you are guaranteed to get a valid result or an error on deserialization. If you use additional non-public information in the format, then yes, you are hosed. Don't do that!
It sort of sounds like an entry level haskell position - proven experience in a functional language, not necessarily haskell. In other words deep haskell expertise is not required. 
i feel the same way!
To be fair, I did answer your question in my edit. I think it's good to make it safer to use. The difficult to foresee issues are things like deserializing with the wrong type. You may even get a valid value as a result, but it still almost certainly won't make any logical sense. This is a direct result of losing type information when you serialize. (It is for this reason that I would personally favor using (a checked version of) `fromAscList`, not `fromList`, since the former would be more likely to catch problems like this. In general, structures with normal forms are going to be much easier to validate.)
I do have some untracked files, yes. This is all unbelievably helpful though, so thank you. As for the mv command, it's temporarily moving /home/ into a unique filename so I can make a new /home/ for mounting NFS. This was specified in the project directions when we did all this manually as a way to prevent accidental fuckups and to keep the root account a truly local user but if you have another solution I will use it. When passwd is changed it's actually to make sysadmin's home be located in home.computername instead of home.
Isn't an internal state actually just a delayed loop in the signal network? That abstraction could do the trick.
You don’t, just as you normally don’t serialise run-time object hierarchies directly. You simply expose all the data necessary to be able to reconstruct it later.
&gt; To be clear, I'm not saying we shouldn't make binary and cereal safer. The double negation makes it unclear what you actually think. You're _not_ saying we _shouldn't_? If you'd said - "you're right, we _should_ make binary and cereal safer, silent corruption is clearly bad", then that would have been more clear. :) &gt; (It is for this reason that I would personally favor using (a checked version of) fromAscList, not fromList, since the former would be more likely to catch problems like this. Okay, I see what you're saying here, though I view that as an orthogonal concern. The purpose of a deserializer is, first and foremost, to produce a well-formed, valid value. Separate from that concern, one may wish to ensure that the producer and the consumer are 'in sync' about what the meaning of those bytes actually is. The problem is that this is a somewhat subjective question, and very application specific, to what extent we require them to be in sync. So I don't think I favor using a checked `fromAscList`, but I can see what you're saying. My feeling is that this sort of concern should be addressed in a more cohesive way, with self describing formats. This is something that could be built atop binary or cereal.
What's the complexity of that?
That's a bit of a vague statement on their part unfortunately. Python allows for the use of many functional paradigms, but I'd never accuse it of being a functional language. I've done some hobby work in Clojure as well, but while it is purely functional, its also dynamically typed. The difference of paradigms between Clojure and Haskell is quite large just because of that. More interestingly how many purely functional languages are really seeing that much production use? I mean Scala is probably the closest to a widely used "functional" language. But even then, Scala is still a much different bird than Haskell is. The target for "proven experience in a functional language" is a huge target depending on how you define it. And I got the sense they were looking for someone closer to the bulls-eye than the rim. 
You forgot the link?
Rather than debugging the whole thing in one giant program, start small and work your way up. Does `monoSubst` work as expected? What about `monoSubst1`? Try writing some unit tests for the small pieces, and by the time you've gotten through the last of them, you'll probably have figured out what the issue was.
Heh I'm sure they'd love a serious haskell veteran, but how many of those are there? If none of them send in a resume they might be forced to pull from the dilettante pile. What's the percentage of english speakers out there I wonder?
Yes, I've been mostly debugging in GHCi and testing small things like that. That's why I have a few smaller tests at the bottom of the file, all of which produce their expected results. I did notice something strange when I tried testing a function "generateAll" which tries to generate all Exprs fairly. Namely, the computation would apparently diverge or produce lots of results based on the order of the arguments in a fair disjunction, which contradicts my expectation. I might expect the order to affect speed with which results were generated, but not divergence.
Yep :) 
Amsterdam has two universities with comp sci departments. I went to one of them, and we had to take an introductory FP course using Haskell in our very first semester, so I suppose there are worse places for snagging up FP geeks.
why do people keep thinking that FRP __is__ simple? Yes, there is a narrow domain where FRP-based solution is simple enough to be understood by a novice, but it is nothing compared to "real-world" problems, where newbies think they can use that FRP-fu.
Yet millions of non programmers successfully use Excel in "real-world" :) 
yeah, that is a "narrow domain". But I thought we are talking about programming...
I do not know anything about e-readers. The source is LaTeX, so if you (or anyone else) have any ideas on what we can do to make it e-reader friendly, we will do our best.
When I wrote my description, I wrote it with the assumption that the readers understood Elm signal graphs. I know this is a bad assumption, but it is the assumption that I made.
I'm a newbie here, but as I understand it, signals (time-varying values) map particularly well to event-driven programming such as creating responsive user interfaces. Signals can also be referentially transparent so they can be used in purely functional contexts. I don't think that event-driven programming is a narrow domain, although I'm also not insinuating that FRP can easily be understood by a novice. I am insinuating that I do not know any better or simpler approach to event-driven programming in a purely functional context. If this is true, then I would say that FRP is the most suitable solution for a wide domain of problems for novices (or anyone), from lack of better alternative. Perhaps you could enlighten me.
Not Haskell, but very similar, you might want to look at elm-lang.org
There are a lot of narrow yet very useful and practical domains for programmers that can be made very simple and approachable to layman programmers without going deep into theory. The most obvious one: 2way-data binding GUIs. What AngualrJS and EmberJS are now pushing on the browser front ends, and of course the same can be done for Qt/GTK/dotnet/java GUIs. 
I actually did find this library but none of the documentation is available on hackage for the latest version so I was skeptical of using it.
software tends to grow from simple to complex and even complicated. I hardly believe that "complex FRP approach" will be simpler to maintain than "complex event-driven approach".
I'm pretty sure there are far more experienced haskellers (for reasonable values of "experienced") than that there are Haskell positions. This is Well-Typed on their experience hiring: &gt; We decided to make an initial "longlist" by restricting our attention to people with three or more years of Haskell experience. That combined with a little more discussion and comparing notes between Ian and myself gave us a longlist of 19 people. This was quite a spectacularly talented group of people, everyone with some mixture of Haskell programming and other commercial experience. It included 5 people with 10 or more years professional programming experience, 7 people with masters degrees, 3 people with PhDs and 7 people who have used Haskell in a commercial context. 
check an example https://github.com/plaimi/bweakfwu =)
as for documentation - that can be Hackage bug, because 1.8.0.1 docs are redered right.
We are looking for both a Haskell developer to work on our backend, and a JavaScript developer to helps on our frontend. Both codebase are similar in size and both contain some big challenges. There is also a lot of glue code that touches both sides of our stack. Most of us currently work on both parts, switching between Haskell and JavaScript quite frequently. So if you're not (yet) a very experienced Haskell programmer, but have the relevant experience in client-side application development, and you are eager to learn Haskell, Silk might still be the right place for you. :) All FP enthusiasts please apply!
But the point of FRP applied to event driven development is to hide the complexity, to take care of it in the framework instead of forcing "last-mile" developers re-implement same things again and again. Look at GUI frameworks like Flex, Angular, Ember. They actually do simplify event driven programming by removing a lot of boilerplate. Yes, ghc compiler is much more complex than pascal compiler. But haskell programs are not much more complex than pascal programs. 
I'm the author of Sodium and I am up for some kind of colaboration - even a book. Email me on sodium.stephen@blacksapphire.com. I don't think "not simple" is a fair description of FRP. It's different, yes, and it therefore has a learning curve, but it is in no way complex. The primitives are simple, and the whole point of it is that it doesn't have compounding emergent complexity like the observer pattern has. FRP can be seen as a drop-in replacement for the observer pattern. Where's the complexity?
I think in this case the property require is weaker than convexity, though. Basically you want the function to be unimodal.
http://vimeo.com/13228822 nikkie and the robots is written in Haskell; maybe you can learn from their approach.
[Ye gods! the dashes](https://github.com/jaspervdj/lorem-markdownum/blob/master/src/LoremMarkdownum/Markov.hs).
hey, i have a lot of free time these days. if you want help or commits or architectural ideas, shoot me a pm (only if you're interested). I'm an ex-game dev looking to get into indie dev who happens to be a haskell junkie.
I use sprites in my game which is implemented using gloss. Might be a relevant example for you. http://hackage.haskell.org/package/Ninjas
I think GLFW-b and drawing-combinators is reasonable
You can always run two commands to generate the haddock documentation yourself. cabal unpack Gloss cabal haddock -- in the dir you unpacked the package to
&gt; Engineering program in reactive manner is a new kind of thinking, basically due to lack of appropriate books. Please, explain this sentence :) 
Just started [something similar](https://code.google.com/p/smartlight-haskell/) in the very same stage :) 
Excellent talk! I'm constantly inspired by your breadth of knowledge and clear explanations. I found these lectures on succinct data structures from MIT if anyone is interested. [lecture 1](http://www.youtube.com/watch?v=actIwwQWwjU) [lecture 2](http://www.youtube.com/watch?v=KyWEUVoSDqU) EDIT: Someone should post this to /r/programming for a wider audience. The talk would give insight even to those who don't know Haskell.
Something like this would be pretty useful to FRP newbies. I did a project &amp; paper on FRP for school (https://github.com/jcurbo/functionally-reactive-solar-system) and I chose reactive-banana because it looked like it had the best GUI support, but I didn't do a deep comparison across all the available alternatives and I was able to get r-b up and running pretty quickly so I didn't have to try any of the alternatives. There's a lot of differences in approach and semantics across them that would be nice to be able to explain without having to give a history of FRP back to Fran :)
I understand elm signal graphs, but the writing is very incoherent and the interface is impossible to use effectively. Take a course in UI design or something.
Hey, it would be nice if you added a screenshot to the readme on GitHub!
How is it possible with dynamic types? If I say `read "SomeStructure"` in a dynamically typed language, there's no way to determine what structure I want to get out.
I take it you are a Mac person, given that you don't have working function keys. Mac uses a very good model for dealing with the problem of UI discoverablility, they have educational ads. Rather than simply telling their users WHAT their users can do in their ads, they SHOW their users HOW to do it. I don't have video on my page, and I do not show you how to do things. However, I try to make my user interface discoverable. I provide text describing each function. I did not, however optimize for discoverablility. I am very aware of the "need" for discoverability, because people who become frustrated by a UI, will stop using it before they understand it. VIM is a classic example of a non-discoverable UI. Many people laugh at the idea that others "still use" vim. However, for an editor, discoverablility is not very important. The key is productivity. We make things keyboard only, so that users can be productive. That is the one and only important thing. It does not matter if it takes you an hour or two to get used to the interface, the important thing is that once you do, you can work with it quickly and fluently. As for F keys, I bind only three of them. F4, F9 and F2. If Elm was more flexible, however, I would bind only one. As for the copy pasting of code out of the editor. This is obviously also due to a limitation in Elm. I do not have the ability to generate a new file and serve it to the user. Obviously, this wasn't a design choice, but a "hack" to get things to work at all. Right now, it is impossible to tell when a feild is focused. If it was possible, then I would not bind F9 at all. I would simply submit the focused field. In Elm, feilds also require keyboard events to update. Simply pasting code into a field will not produce a result. This is a bug in Elm. If this bug did not exist, one would not have to press "Home" then F2 to load code. As for the F4 key, I was quite surprised that the mac does not have proper suport for the control key nor for the F keys. I did not design my UI on the mac. On my computer, F4 is a good key to use, because it does not require a key combination. I do not make software for the mac. Next time I release a peice of elm software, I will look at the user agent, and if it is mac I will show a message telling people to turn on the function keys and turn off the Ctrl-arrows stuff. I cannot do anything else. I already have too few possibilities for binding hot-keys in the browser. Look at your keyboard, and think to yourself, what Ctrl-+ some letter does. I beleive almost every key on the top row of letters is bound by firefox. If I was not writing for the browser, I would still use function keys. It allows the user to press one key to do a common task, rather than a key combination. That is the point of function keys, and it is quite usefull. Obviously, when I made this UI, I thought that it would be fast. I did not realise, that in Elm, even changing the color of text is slow. Now. I'd like you to stop, and take a look at your comment. Read it aloud. How many visual programming languages have you used? Can you direct me to a visual programming language which has an editor which is more usable than mine? Obviously, you are used to editing text. This is fine, but my goal here, is not to edit plain text. Pleantly of people have tried that. My goal here, is to see what is possible when doing visual programming. I have read my work over again, since I read your complaints. I did not edit it when I first wrote it out. Indeed I wrote it rather quickly. I do not think it is that bad. Compare it to Hudak. Hudak writes books that are very difficult to read. They are both incoherent, and require a lot of background knowlege in logic and mathematics. You would not be so quick to insult him however. Afterall, he uses many big words and that makes it likely, that the reason you don't understand him, is your fault and not his. I write simply, despite my assumptions about your knowledge of Elm signal graphs. You are not afraid to insult my writing, because you understood every word (though perhaps not every sentence or every paragraph). If my writing is not coherent, I will work on that. I DO want you to understand my product and share my facination with visual programming. But for now, fuck off!
Yes, the 'state' is just a delayed loop. The other problem is that you can't swap out signal functions as the state is relied on the implementation itself, rather than a set of data.
Or if you don't want to be limited to Bitmap, use [Juicy.Pixels](http://hackage.haskell.org/package/JuicyPixels) with the [gloss-juicy](http://hackage.haskell.org/package/gloss-juicy) wrapper to load png/jpg into gloss.
Don't forget Yampa :(.
nice video, too bad ustream butchers it with ads.
&gt; Instead, purely functional languages try to reimplement the universe in functions by passing the universe in and out. I'm going to assume this is about Haskell's `RealWorld` type that is hidden somewhere inside the `IO` type. While the name suggests what the sentence says, this is not at all the case: `RealWorld` is an implementation detail, and it is not "the universe passed in and out". [Here's an insightful post by Shachaf about the idea behind `IO`.](http://stackoverflow.com/questions/13536761/what-other-ways-can-state-be-handled-in-a-pure-functional-language-besides-with/13538351#13538351) &gt; Being purely functional is basically ignoring the capabilities provided by the physical substrate So does any other language that is not processor instruction codes (assuming "capabilities" is meant as "explicitly flip transistors", as the interpretation "do calculations" wouldn't make any sense). The point of programming languages is ignoring the physical substrate to the point where a human can understand the processes going on easily. I don't see physical substrate in Python any more or less than in Scheme or Haskell or Scala. &gt; If you look deep into them, monads make programs complicated and hard to write No. (Glad he didn't provide any evidence so I don't have to.) Verdict: Article written with all the OOP and FP buzzwords, backing technical accusations up with nothing but opinion.
for all (rightly, i guess. i did not notice them ;-)) annoyed about ads. try youtube-dl http://www.ustream.tv/recorded/40685818 it will download a flv-containered h264 video without ads.
Well said. I came here to basically make these same arguments.
I completely agree with you, of course. But a non-Haskeller would respond that we are not interested in mathematical problems at all. We need to solve real world problems, like getting our factory machinery to do the right thing at the right time, or getting our video games to work right. I think we still haven't done a good enough job explaining to the world why real world computing problems really are mathematical problems, and why thinking about them that way is the best way to design software systems.
I wrote a long comment on the blog (and posted it on HN too). I'd be curious as to what people think about my current views on functional programming: (Also, I might want to turn this into a blog post for that blog I keep on meaning to start... :P) Haskell’s “purity” is not about getting rid of side-effects but about *controlling* side-effects. It’s making side-effects *first-class citizens*: now you can write code that talks about having or not having them! You can still have side-effects however you want, you just have to be explicit about it. To some extent, the fact that this uses monads is incidental: all that’s important is that there is *some* IO type, *some* ST type and so on–the fact that they all form monads is almost an implementation detail. That’s why some of the most exciting Haskell features–STM, the IO manager and so on–are all about effects. Clearly, Haskell more than acknowledges effects, so the entire diatribe about ignoring the existence of side-effects is attacking a straw man. Besides, you can’t simply replace a first-class system for managing effects with static analysis, without essentially reproducing the same restrictions. How would you do something like Haskell’s deterministic parallelism model or reliable STM or the very aggressive loop fusion (and general rewriting) Haskell uses? There’s a reason you don’t see these things done nearly as well in any other languages: all of these fall apart as soon as you introduce side-effects, so you need some way to help the programmer ensure things like this are only used safely. And this is exactly how types like IO and ST help make code safer. Sure, *within* an ST block, you have stateful code that’s just as hard to analyze. But you can guarantee that this does not leak outside the block. Similarly, functions can rely on their inputs not doing anything untoward however they’re used. This allows you to explicitly state the assumptions about your inputs: how is this a bad thing? In turn, this makes writing code that takes advantage of these properties much easier: you can ask that your inputs do not cause side-effects in a way that’s self-documenting and easy to verify. Then you’re free to re-evaluate your inputs or call functions however many times you want, as concurrently as you want. At the extreme, this can even be used for security purposes: see Safe Haskell. Sure you can write pure functions in any language. And you can write side-effecting procedures in Haskell too. But the difference is that Haskell lets you be explicit about whether you want side-effects or not–it’s just another part of your interface. And this is how types like IO and ST help make your code easier to think about: any code that is *not* in a type like IO or ST can only depend on its arguments, making all the dependencies more explicit. (Note, again, how this is all independent of “monads”–it’s all about effects, and the types just happen to form monads.) This does not make static analysis too much easier, but that was never the point–the goal is to make the code easier to think about, and knowing that there are no hidden state dependencies certainly does that. A static analyzer can follow data flow easily, but it requires quite a bit of thinking for the programmer to do the same! The core motivation for managing effects à la Haskell is not “mathematical purity”: it’s software engineering. We want code that is easier to think about, has better guarantees and is more modular. The goal is to make code less complex by removing hidden dependencies between distant parts of your program (mutable state) and the effect of evaluation on the meaning of your program (side-effects in general). You can refactor and move around most Haskell code without worrying about breaking its surroundings because any dependencies are explicit. You can extract something into a function, consolidate multiple function calls into one or split one into multiple and generally change your code up quite a bit without worry–these actions cannot change the code’s correctness because effects are managed for you. Ultimately, functional programming like Haskell is not just normal programming with side-effects outlawed. Instead, it’s a different *basis* for programming which allows you to manage side-effects explicitly. In this light, papers like “solved problem but with monads” are entirely reasonable: they’re about bringing things over to this new basis. And this goes the other way too: there’s a reason why you don’t see good STM, deterministic parallelism, stream fusion (and rewrite-rule optimizations in general), anything like DPH and anything like Safe Haskell in other languages.
Verdict: Article written with all the OOP and FP buzzwords, backing technical accusations up with nothing but opinion. --quchen
I read “Java design-patterns”. I threw up.
(or a site that works *with* add, for that matter?)
I've reposted the video to youtube. http://youtu.be/uA0Z7_4J7u8 Hopefully that will result in a less advertisement-riddled presentation.
This is really cool. I can sort of figure out how to do stuff in diagrams, while TikZ is greek to me. I will definitely try this out next time I have to draw something i LaTeX.
&gt; Conclusions… For small ranges and dense data (relatively few counts of zero), use `Data.Vector`, the mutable kind. For large ranges and/or sparse data, use `Data.Judy`. For cases where this kind of optimization is needed - a huge range and/or a huge number of items, and concrete evidence that this is your performance bottleneck. (And thanks for the great work for that case!) But in most cases, the simple `Data.Map` or `Data.IntMap` way is just fine, or even good old `map (head &amp;&amp;&amp; length) . group . sort`.
Now I see, but that gives another question: Can you guarantee that moving home directories and modifying `/etc/passwd` to reflect the changes will be done atomically? I mean bad things will happen if someone tries to access `$HOME` between these two actions, or when the script does not manage to execute the second action for any reasons. This is probably not an issue most of the time, and probably totally unnecessary for your project (it is a project for a course I assume), but that is something to think about.
&gt; I think we still haven't done a good enough job explaining to the world why real world computing problems really are mathematical problems, and why thinking about them that way is the best way to design software systems. Well, I had hoped that the CS courses in university have convinced them already: programming language semantics, compiler writing, ... Recently, I have been doing a little PHP development (I am so ashamed). Many programmers probably see their language as a universal means to write programs, but I merely see it as a way to express particular programs -- it's not a good fit for other problem domains. For instance, PHP was originally designed as a templating language, which means that it has some good points there and that I should pay attention to the `&lt;?php ?&gt;` syntax snippets. They are a syntactically pleasant way of expressing HTML templates, which means that I'm going to organize my code around this idea. In contrast, the `echo "&lt;div&gt;";` style is not such a good way to do HTML templating, so I try to avoid it.
That's good to hear, now I have to ask the obligatory question since you announced this to an international audience. I'm not sure how visas work in Amsterdam, are you interested in foreign applicants?
You really should start a blog. I've had good experiences with Blogger and it is very easy to set up. If you're really lazy I will even set it up for you. Or you can guest blog on my blog, if you prefer.
Of course, visa-wise we have a strong preference for EU applicants at the moment. (possibly plus other Schengen countries) Working visas for non-EU citizens should be applied for by the employer, not sure how difficult this is. I think in the EU this is mostly only done for specialist positions and probably requires some minimum salary. It might be a burdensome and risky thing to do both for the employer and applicant. We don't have a strict policy on this and need to check on this on a case-by-case basis.
I like to think Haskell more as a language that focuses on composability, with a powerful type system layered onto it to ensure that things can only be composed in the correct way.
Thanks for the offer. I actually have hakyll set up most of the way, so that's not an issue. What I need to do is actually finish some articles. I have a bunch that are half-written, but none that I'm quite happy with.
I'll be happy to review any of them.
Why?
I don't understand it
Exactly! There are so many different frp libs and absolutely no information what caused their proliferation. Why so many? What's the difference? At least with iterators (pipes, conduit) there's a discussion about differences and merits. 
I should be changing the script to ensure that's done atomically, yes. Honestly I don't plan on stopping at the course boundaries and intend to implement more as a personal project when finals are over so any and all advice is appreciated. I'll have to find a way to safeguard against /etc/passwd getting borked during that step. 
Seems like flow based programming?
OK, here is example of hidden complexity in FRP. https://github.com/snowmantw/Frag/blob/master/src/ObjectBehavior.hs The example is very old and outdated (2004), but it is one of several real-world examples (though I'm not sure, if this code is really used in the game). But it shows that nothing comes for free, and going from event driven to reactive does not mean going from complex to simple. PS. I know nothing about frameworks "Flex, Angular, Ember". I don't know what they are for, so I cannot currently do more precise analysis on this topic.
Writing FRP programs is very close to making electronics. Or like LabVIEW [1]. You cannot do good electronic schemes without appropriate intuition, and that requires long studying (however you may be a genius). [1] One of complicated examples http://img.thedailywtf.com/images/201104/labview.jpg
That’s actually preprocessor output, not the original source. When Frag was written, support for the arrow syntax was not built into GHC yet.
Can you give an example of that? I'm not sure what the difference is. A link to a blog or whatever is fine.
Good question. As far as I know, this is the version that has been floating around for many years. I have faint memories of having seen the original long ago, but I couldn’t locate it since then.
This makes me wonder: how is the Haskell community around Amsterdam as far as meetups are concerned, any experiences? If there's anything good going on, I'd love to pop around and get to know people. I did come across [NL FP Day 2014](http://staff.science.uva.nl/~grelck/nl-fp-day-2014.html) on the Dutch HUG mailing list, but it looks a bit... annual.
There was the Dutch HUG, but it's kind of dormant (although people are having drinks this Monday, it seems). If you're into something a bit broader, [Cross Functional Amsterdam](http://www.meetup.com/funadam/) is active and the meetings I've been to have been great.
It's a bit vague, but if in doubt, just shoot us an email. We'd love for people experienced in Closure, ML, Scala etc. to apply. As for production use, you'd be surprised. I've been to some events like FP Exchange and FP days in the UK, and met lots of people using F#, Scala, Haskell or Closure commercially.
This is from May, but I had not seen it before, and I can't find it on reddit. I would prefer build to have the type: build :: (forall m. Monoid m =&gt; (a -&gt; m) -&gt; m) -&gt; f a with the fusion rule `foldMap f (build g) = g f`. And this then generalises nicely to non-empty structures by replacing `Monoid` with `Semigroup`.
&gt; Set is not traversable, because it is not a Functor on all of Hask, only that subcategory subject to the Ord constraint. Set isn't even functorial on that. Nothing ensures `Ord` is structural, and the class doesn't claim to be.
there are sparse pieces of arrow code in his thesis - http://www.cse.unsw.edu.au/~pls/thesis/munc-thesis.pdf
I don't know what you mean by 'structural' in this context? Or why that's relevant?
i was hoping for a more elaborate comment of yours (and of other people). i am more in the more typeclasses are good camp myself, so i like that proposal. what's your take on it?
Thanks for posting it. I worked on it for a while, then decided to sleep on it, and then started to feel very perfectionist about a few things that aren't fully sorted out yet, and eventually didn't really keep pushing it. I'm still very keen on the basic idea, and think a `Buildable` library would potentially be very useful. On `build`, I plucked out a signature that most obviously generalizes `build` as used in fusion in base. I suspect your rule is actually better though, since it can take advantage of treelike fold-structures more easily, like that given by the `foldMap` for `Data.Map`.
MFlow seems to be featureful, I wonder if a lot of people tried it? I certainly don't hear much about it
SDL-* is pretty much complete for implementing a game. You can load images in a wide range of formats and draw lines and cubes, play audio etc. Check them out http://hackage.haskell.org/packages/search?terms=sdl
An `Eq` instance can quotient over observationally distinct values.
Aha, I can now see where a perverse use of a 'valid' Eq/Ord instance can violate the functor laws. data Haha a = Haha {unHaha :: a} instance Eq (Haha a) where _ == _ = True instance Ord (Haha a) where... now `fmap (unHaha . Haha) =/= fmap unHaha . fmap Haha`. Whoops! I guess one can say that Set is functoral only on that subcategory subject to an ord constraint that doesn't quotient over observationally distinct values :-)
I want a sticker myself :(
This is really excellent. I only have one quibble: &gt; In this sense, any language that deals with IO this way must clearly be non-strict in its IO executions Strictness is a property of the evaluation model, but as you eloquently described Haskell separates evaluation from execution. Even a fully strict Haskell would still do the right thing, because as you noted immediately after: &gt; evaluating an IO statement must not actually “execute” anything Once you separate the execution model from the evaluation model, the only thing that changes as a result of adding strictness is now that you have more bottoms everywhere, but it doesn't change execution order. Of course, I wouldn't advocate for a fully strict Haskell, but I'm just playing devil's advocate.
Implementing miniKanren does seem to be difficult in Haskell. Here's a related link: http://www.infoq.com/presentations/molog
&gt; I'd be curious as to what people think about my current views on functional programming I'd say your post here is spot on, and more eloquent than I think I could have put it. Do write a blog :)
I think `Buildable` should have `buildMap`, `buildr` and `buildl`, with 3 fusion rules. I was hoping that the `foldr/buildr` rule would follow from the `foldMap/buildMap` rule and their default implementations, but I don't see how. I get: foldr f z (buildr h) = appEndo (h (\a (Endo g) -&gt; Endo (f a . g))) (Endo id)) z 
This also works nicely with the `Free Monoid` from my `free-functors` package: instance Buildable (Free Monoid) where build = Free
I would say that Set is a functor from Ord with order-preserving maps (could be made slower but more general saying equality-preserving maps) to Hask.
I am not sure in what sense you think newtonian mechanics is free of state. Usually you would say, that you have a point in phase space (positions and momentum) i.e. in some symplectic manifold, together with a time evolution, that you get by integrating a hamiltonian vector field on the phase space. It's not that there is no state, you just usually understand it very well. 
Excellent article, and I love your blog. I'll definitely be checking out the code!
Done!
Let's think about what an 'object' is, in non-FRP stuff. It's usually a record containing state which is stepped each frame. You change the stepping code, it's fine. It's just logic that returns a new state. In FRP, the network returns 'visible' state (object positions, etc) but internally keeps things like state and object velocities. It does this by returning new signals each step, and composing them to make a new network each 'step'. Objects are just high order functions. So if you want to hot swap signals, there's no way to get that internal state out and replace the network with new code. Since the internal state is functions themselves. Elm has hot swapping, but it doesn't have signal networks, and contains state in records. So it's kind of a hybrid.
The type of `first_n_fibs` should be `Int -&gt; [Int]` not `Int -&gt; Int`.
Excellent article. You touch an interresting point: why an IO is not printable (instance of Show)? If that was the case, it would be much more clear from a pedagogical point of view. I suspect that when you work in the IO monad what you really do is building a structure representing your computation, but why not making this structure showable? This stackflow question also helped me a lot understanding IO in Haskell: http://stackoverflow.com/questions/17002119/haskell-pre-monadic-i-o 
`IO` is a `Functor`, so arbitrary computations can be a part of your `IO a` value. Since we can't show arbitrary functions, we can't show `IO a` values. (e.g. think about how to show `fmap (+1) readLine`)
Thanks for the comments! IO being not printable actually kind of makes sense from a practicality standpoint. Even something like `getLine` is composed of several layers of primitives, and "ideally", when two monads are composed, you are supposed to "lose" the layers of compositions and get just the full primitives (like for example, State). Also there isn't too meaningful of a way to print out all of the branching either. But I have always been fascinated in finding a way to print an arbitrary function, in the same vein. I mean, Mathematica, another highly functional language, for example, has no problem with it. What gives, Haskell? Thank you for the reference too; I actually was inspired greatly by a Stack Overflow answer very much like that one. It's similar to another blog entry (that i linked) [by chris taylor](http://chris-taylor.github.io/blog/2013/02/09/io-is-not-a-side-effect/). However I felt like this approach to IO as a monad, and the details of its composition, is a well-worn tale and I wanted to write something that I could maybe fill in a space where less was before :)
Thanks for this. I actually originally wrote a much stronger statement, "All pure languages must be non-strict". But #haskell was quick to point out that that was clearly a misguided statement. I still feel like I can stand by my re-write, though: "evaluating" the IO execution, whether strictly or lazily, must not also execute it. Perhaps "must clearly be" should really be "is by definition"?
You are 100% right about how evaluation does not relate to execution. What I meant is that the term strict refers to the evaluation model and not the execution model. Since Haskell separates evaluation from execution we are free to choose either strict or non-strict evaluation since neither of them affects the execution model.
Yeah actually, you're right. The idea of strict/non-strict really doesn't actually come into play here at all. It makes me wonder how much of my article is unrelated to IO at all and is more about strictness/non-strictness. Should I still keep them in, or throw them out? The more I think about it the less I can justify them being there.
No, don't change the rest! It was otherwise spot on!
Made some modifications to the section that used to declare infinite lists and stuff and removed the non-strict paragraph at the end altogether; hopefully it presents a more cohesive/focused whole!
This is a good point; I've moved all actual content out of the comments haha. Good to know for the future too. And thanks for the comments, I am surprised and grateful
Thanks! This is positive feedback I did not expect. The article source itself is available on github, and the link is in the article header on mouseover. But it might be good to allow the individual examples themselves to be downloadable in a gist of some sort...I'll have to make it work with my current workflow of just uploading raw markdown with no frills.
Oops, thanks.
Ah yeah, thanks :) added that in.
You can deal with IO just fine in the Haskell way in a strict language; there's no contradiction.
I think pattern matching is an easy one to explain and demonstrate and it's usually an aha moment when the person gets it.
I agree with this, but it's not quite that simple. Good type checking gets rid of many bugs, and beginning programmers have a very hard time doing even simple debugging, so anything that helps with that is very valuable.
Very good. Unfiltered reactions follow. &gt; there is no inherent ordering in any of these statements I am concerned that people will interpret this to mean that Haskell patterns are unordered, which (alas) is not so. I like that you used the term “action” to refer to monadic values. That seems to be the most accessible term for the programming community at large. &gt; Out of all of the IO objects you can return/represent, the Haskell compiler chooses one of them to be the one it actually compiles into computer-readable code. The compiler compiles all the things. The runtime *evaluates* one of them. Your explanation of `&gt;&gt;=` would perhaps be clearer if you started with `=&lt;&lt;` and explained it as “function application with side effects allowed”. &gt; This distinction between evaluation and execution is what makes Haskell unique. Not unique as such, but it’s the only language with this characteristic that Joe Programmer has heard of. 
Ping me on IRC at some point.
I'm not averse to giving a longer winded reply, I am just traveling at the moment and have had little bandwidth, in both sense of the word.
&gt; Buildable generalized Pointed strictly -- everything Buildable is Pointed, and everything Pointed is Buildable (to get the latter, just always build from a one element list). The latter half of that statement makes me highly uncomfortable.
e.g. IO is Pointed, but I don't see that it's Buildable.
BTW, where you mention random number generation as inherently impure, you could link to this xkcd cartoon if you wanted: (http://xkcd.com/221/)
[Image](http://imgs.xkcd.com/comics/random_number.png) **Title:** Random Number **Alt-text:** RFC 1149.5 specifies 4 as the standard IEEE-vetted random number. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=221#Explanation)
I would love one too, if you drop by France at some point (which I'm sure you'll do). :)
I'll be there in a few months, but pretty far from Paris, IIRC.
rename the file to `Main.hs` maybe?
i tried but it didnt work. i think the file name has to be same as the module anyway
Hey, thanks, sounds gezellig! I'm actually narrowing down to Haskell more and more after a couple of years of faffing about in Scala, but I should probably check out Cross Functional one of these days.
I'm pretty happy about most of this. As a heavy WAI user and a pattern-matching lover, it upsets me that I'll have to import a second 'Internal' module in every app now, but we've already had that discussion. I'd also suggest that pre-pulled-out header optomisations be given their own record, and then then Request just get 'headers' :: SomeHeaders'
This is amazing. Please write more!
So if 'return' actually *returns* a result (that is used later with mplus), how do you 'lift' stuff into computational expressions? 
Perfect, I'm quite far from Paris myself. Would you happen to visit the south east? I surely owe you a beer or two anyway.
Very nice! From the looks of things, if I write something that depends upon the `http-client` base package and want to use `tls`, I simply need to expose `ManagerSettings` customization and use the `-tls` package, correct? What happens if `http-conduit` tries to send a `secure = True` request without SSL-enabled `ManagerSettings`?
&gt; Of COURSE you should do that. I don't think I can agree with you on this one. [(k, v)] should not be silently converted to Map k v. Those are two different types! Certainly it's possible for the implementation of the serialization library to use fromList, but you should *never* rely on that as it's an implementation detail. The best workaround in this case would be to throw a runtime exception declaring the type mismatch, but that's not possible in case of 'binary' anyway.
To clarify, it seems like `fmap (+1) readLine` is a reasonable way to show that value; but (1+) and (+1) are the same function, so referential transparency requires that they are shown in the same way; to take another example, `readLine &gt;&gt; readLine` and `fmap last $ replicateM 2 readLine` must also show the same way. To determine if two programs compute the same function / construct the same IO action is undecidable.
I stopped working last week. I'm reading about FRP right now, working with the [Helm](http://hackage.haskell.org/package/helm) library. It's a very interesting approach and I'm considering to contribute to that library instead of making a new one. 
&gt; Naively that is done by discretizing time and computing x{n + 1} and v{n+1} from x_n and v_n plus some additional data, which very naturally is mapped to an imperative program. But my point is that Newtonian mechanics are not formulated as an imperative program. Sure, you can map the equations of motion to an imperative program (von-Neumann machine) (that calculates an approximation), but the same can be said for pure Haskell code. The fact that you can map one language to another language has no bearing on the expressive properties of the language itself. The point I want to make is that the description of Newtonian mechanics in a physics textbook is purely functional. &gt; Rather there is a prescription that given an initial state (a point in phase space (also called state space), for example an initial position and momentum), will produce such a phase space trajectory, by integrating a differential equation. Actually, no. It follows from the equation of motion that the trajectory `x(t)` is unique once you specify `x(0)` and `v(0)`, but you can also consider trajectories with, say, `x(0)` and `x(T)` as boundary conditions. Or if you want to simulate molecule vibrations, you can ask for eigenvectors, etc. (Also, just because the phase space can be called "state" space doesn't mean that it has something to do with imperative programs.) 
Yep, that's the idea. For your second question, you can [see the relevant code](https://github.com/snoyberg/http-client/blob/master/http-client/src/Network/HTTP/Client/Manager.hs#L84): throwIO TlsNotSupported
However, pattern matching is far from unique to Haskell. And due to the fact that nobody seems to want `Eq` restraints on pattern matching, you can't even do as many cool things with it as you can in e.g. Erlang.
I appreciate that you have a library that does this, but would you mind actually explaining it as opposed to blatantly advertising your code? Suppose I'm writing my answer on an exam (as the article mentions) or, because of some other constraint, I can't import Pipes. How do I implement your solution in plain Haskell?
&gt; distance,velocity, aceleration being states? I am quite sure they are described as functions of time.
I wouldn't call posting about how your own library solves the problem well *blatant* advertising; having a look at `pipes` is highly recommended. For an exam `sum &lt;$&gt; replicateM n oneRandInt` will do, but that article is about optimization and performance pitfalls and not about exams, although it briefly mentions it.
To answer your first question (getting an intuition for why it works), the easiest way to understand this is to read the [`pipes` tutorial](http://hackage.haskell.org/package/pipes-4.0.0/docs/Pipes-Tutorial.html) through the first three sections until you get how `for` and `yield` work. Once you understand their behavior, then the `Monad` instance of `ListT` is summarized succinctly by its documentation: &gt; `return` corresponds to `yield`, yielding a single value &gt; &gt; `(&gt;&gt;=)` corresponds to `for`, calling the second computation once for each time the first computation yields. Then read the [section of the tutorial on `ListT`](http://hackage.haskell.org/package/pipes-4.0.0/docs/Pipes-Tutorial.html#g:6). To answer your second question, here's how you implement `ListT` without a `pipes` dependency: import Control.Monad (MonadPlus(..)) import Control.Monad.Trans.Class (MonadTrans(..)) data Step m a = Nil | Cons a (ListT m a) data ListT m a = ListT { next :: m (Step m a) } instance (Monad m) =&gt; Monad (ListT m) where return a = ListT (return (Cons a mzero)) m &gt;&gt;= f = ListT $ do x &lt;- next m case x of Nil -&gt; return Nil Cons a m' -&gt; next $ mplus (f a) (m' &gt;&gt;= f) instance MonadTrans ListT where lift m = ListT $ do r &lt;- m next (return r) instance (Monad m) =&gt; MonadPlus (ListT m) where mzero = ListT (return Nil) mplus l1 l2 = ListT $ do x &lt;- next l1 case x of Nil -&gt; next l2 Cons a l1' -&gt; return (Cons a (mplus l1' l2)) The easiest way to understand those instances is to compare them to the corresponding instances for lists. The `mzero` and `mplus` for `ListT` are analogous to `[]` and `(++)` for normal lists. The `Monad` instance for `ListT` is analogous to the instance for `[]`: `return` is the singleton `ListT` and `(&gt;&gt;=)` behaves like `concatMap`. Another way to explain the `ListT` type is that each successive element of the `ListT` is guarded behind a call in the base monad. This is how it generates them lazily.
Also, note that `Pipes.Prelude` has a `replicateM`, too, so you can actually just write: P.sum (P.replicateM n oneRandInt) ... to give identical behavior, except without stack overflows. The only reason I didn't mention `replicateM` is that I didn't want the answer to be too magical.
Don't get me wrong -- I'm a big fan of Pipes. It just seemed out of place in a thread about implementations. As for ListT, it took me a bit to work through the Monad instance because of the MonadPlus functions, but your analogy makes it much clearer. Thank you for explaining.
My pleasure! I'm glad it helped.
The compiler compiles everything (unless there's some option for tree shaking). Libraries don't even have a `main` ;) 
in python you can even do : a,b = range(2,101,2), range(2,101,2)[5:] 
And of course there's [FunGEn](http://hackage.haskell.org/package/FunGEn).
Interesting. I would expect libraries to only compile things accessible from the exported functions/types. Is there any particular reason why it is done this way? Why waste binary space on things that will never be callable?
I'd even like to see discussion around having both `ListT-done-right` and `LogicT` in `transformers` as they specialize the two notions of list-as-context and provide high-quality implementations of what one might expect and want out of a `[]`-based transformer stack. Perhaps `C.M.T.List` could default to something like `C.M.T.List.Commutative` to emphasize that while it's a commonly used data type, it has a severe, unchecked restriction on its parameters. Then `C.M.T.List.Streaming` and `C.M.T.List.Search` could provide the `pipes`-like and `logict`-like transformers.
have you seem http://noflojs.org is pretty cool
Very interested the idea of unit testing/behavior testing inline with the function syntactically. Are there any other languages that have these features?
This is a short one, but I thought it was really cool because it brings together a few of my favorite Haskell features. Let me know if anything is off, or if you have questions!
I've been trying to test this, but I have been receiving results to the contrary: https://gist.github.com/mstksg/7456610
http://www.cse.chalmers.se/edu/course/TDA452/FPLectures/Vid/ is a good lecture series that starts from the very beginning and covers some decently advanced topics.
Yes, I saw that. It is "traditional" visual programming with the lines between the nodes. I made graphical elm to show that the lines between nodes aren't necessary and they add clutter. With noflojs it seems that you use the mouse to drag things around. With graphical elm you do everything with the keyboard. I hope that will be more efficient for editing.
SubExpr seems like a weird (and wrong?) way to handle parenthesis in expressions?
Trying to install through cabal &gt; :: cabal install haskell-pdf-presenter ~ &gt; Resolving dependencies... &gt; &gt; /tmp/poppler-0.12.2.2-25035/poppler-0.12.2.2/Gtk2HsSetup.hs:25:2: &gt; warning: #warning Setup.hs is guessing the version of Cabal. If compilation of Setup.hs fails use -DCABAL_VERSION_MINOR=x for Cabal version 1.x.0 when building (prefixed by --ghc-option= when using the 'cabal' command) [-Wcpp] &gt; #warning Setup.hs is guessing the version of Cabal. If compilation of Setup.hs fails use -DCABAL_VERSION_MINOR=x for Cabal version 1.x.0 when building (prefixed by --ghc-option= when using the 'cabal' command) &gt; ^ &gt; [1 of 2] Compiling Gtk2HsSetup ( /tmp/poppler-0.12.2.2-25035/poppler-0.12.2.2/Gtk2HsSetup.hs, /tmp/poppler-0.12.2.2-25035/poppler-0.12.2.2/dist/setup/Gtk2HsSetup.o ) &gt; &gt; /tmp/poppler-0.12.2.2-25035/poppler-0.12.2.2/Gtk2HsSetup.hs:182:35: &gt; `libraryConfig' is not a (visible) field of constructor `LocalBuildInfo' &gt; &gt; /tmp/poppler-0.12.2.2-25035/poppler-0.12.2.2/Gtk2HsSetup.hs:295:69: &gt; Not in scope: `libraryConfig' &gt; Failed to install poppler-0.12.2.2 &gt; cabal: Error: some packages failed to install: &gt; haskell-pdf-presenter-0.2.2 depends on poppler-0.12.2.2 which failed to &gt; install. &gt; poppler-0.12.2.2 failed during the configure step. The exception was: &gt; ExitFailure 1 &gt; Is there something wrong with poppler? Or is my cabal just messed up
In `not`, you remove the SubExpr wrapper, so if you rely on it to print correct expressions, parenthesis-wise, then it will become wrong.
s/The correct/A correct/r
Well, you happily ignore the cost of putting together the *reversed list*, which will also have quadratic cost when the list is used afterwards in any straightforward implementation with Pipes. I suppose they purposefully chose append rather than prepend. Then again, List is a type for control structures, not data. Sequence would be more fitting. It's a post on real world Haskell after all.
I was referring to the types in isolation (i.e. in the absence of structure).
My prediction is that the biggest driver of language adoption in structural bioinformatics for the near future will be molecular graphics systems. If you really want to spread Haskell adoption in this area you have to do two things: A) Write a molecular graphics system in Haskell with a plugin system B) Get it to work in the browser
ever consider offering an http api a la http://loripsum.net/? :)
https://gist.github.com/leroux/6395804#talksvideospodcasts
I gotta use that answer some time. Gallus. 
Well, that works, what also worked for me was just explicit typing. I'm just wondering why on earth it infers the first and last expression to the more general solution but not the middle one.
The [monomorphism restriction](http://www.haskell.org/haskellwiki/Monomorphism_restriction) is a common source of confusion.
Ah yes, it seems prima facie about as silly and arbitrary as the text first calls it. 
The motivation -- notice that the middle expression (f) is in some sense not a "function" -- it does not take a parameter and return something at runtime. In fact, it always evaluates to the same value -- (map . (*)). This means that GHC can try to "pre-compile" it at compile-time, because f is basically a constant --- it will always refer to the same function, and is known at compile-time. For your third line, g x, g x will evaluate to something different for every x. GHC can't "pre-compile" it. But it can "pre-compile" f, which will always be the same function every single time it is retrieved. f has no parameters, it always returns the same thing. Because it is a "constant", and it *can* be "pre-compiled", GHC has a very strong temptation to do so, winning a very strong performance boost in the process. But doing so will require that it fixes the type. If you specify the type explicitly, then GHC knows that it can't pre-compile, so it won't try. A less confusing example would be :t 1 vs let n = 1; :t n. You'll see that the first is Num a =&gt; a, and the second is Integer. This is because GHC again sees that n is a constant, and won't change no matter what, and wants to pre-compile the 1 to an Integer so it won't have to re-compute it every single time. The compiler flag NoMonomorphismRestriction will tell GHC to never "pre-compile", even when it can. For a rather infuriating example of this restriction, try let s = show;, and calling s on different showable things.
I'd say, any project where you want something engineered to last, and to work correctly, is a great candidate for Haskell. For one-off statistical tests and making charts for your slideshow, it may be harder to make the pitch that learning Haskell is worth it. If you have a behavioral assay where you've been making your research scientists hand-score how many times a rat spins counterclockwise or something, do it in Haskell instead. Or if a machine is streaming out fluorescence readings from a high-throughput assay of some kind and you want to data-mine, or to build lists of instructions for which plates to pass to the next processing stage: Haskell. The sense I got from my year at Wyeth was that most lab-heads are interested in adopting new technology, but they don't think of themselves as technical people doing their own building. My bosses, for example, wouldn't want to learn the scripting language for some new piece of equipment they would buy; but they _would_ choose a piece of equipment based on whether or not _I_ was interested in learning to script it - and I was just an intern. If that attitude generalizes across Pharma companies, I think that this kind of characterization of Haskell would resonate with lab-head-level management: "Haskell is something that can slip into parts of your drug discovery process without damaging the parts that already work; and by choosing it will win you some really enthusiastic interns" Wyeth loved it if they could develop a new assay and publish a paper about it. There was real enthusiasm for doing whatever it took to build something interesting &amp; profitable, and they were very happy to hire people to do tech development. If the person they hired said, "I really think that LabView is the future for this frog-egg-recording system I'm building," they'd buy him a bunch of LabView licenses (true story). Speaking to Haskell's relationship with matlab - there are a handful of problems that I've had to solve once in matlab, and once in Haskell, so I can count as a small data point there. My Haskell solutions are an order of magnitude less buggy, and much quicker to write. And I'm _happy_ when I'm writing them. Debugging matlab is painfully slow. I'm angry when I write Matlab code. Many of the errors are silent - you never _really_ know when you're done debugging. The superiority of my Haskell implementations isn't just due to the fact that Haskell is usually the second-round implementation and I've already had practice with the problem. In a few rare cases, I solved the same problem three times: once in matlab, later in Haskell, and then later still a second time in matlab (because I really needed a working matlab version and I didn't like my first implementation). My second-round Haskell implementations beat my third-round Matlab implementations, too! The pain-point of Haskell is that it might be hard to get people who are acquainted with matlab to make the investment to switch, if a Pharma company already has its office of "matlab gurus" established. I don't believe it's actually hard to learn Haskell, but it can definitely _feel_ hard! It's not the kind of mental exercise that Matlab people are used to doing. For me, the motivation to learn came from falling in love with the syntax and the functional-purity philosophy, and from repeatedly running into the "complexity wall" with matlab and c. Haskell is alien and I wonder if someone who isn't magnetically drawn to it from the beginning would stick with it long enough to reap the (guaranteed) benefits. A few things don't work straight-out-of-the-box in haskell the way they do in matlab. Plotting requires a little work instead of being a one-liner. Image processing isn't a one-liner out of the box. Matlab folks get really annoyed that you can't just start up Haskell, load an image with one command, and plot it with a second command. Making a filter and doing some digital signal processing is pretty easy in Matlab (IF your data all fit in memory). But of course there are some things that work out-of-the-box for Haskell and are very awkward in Matlab, too. Not just language features but library stuff. The Matlab Way encourages use of .mat binary files for storing data. Essential things like reading and writing JSON data are hacked together by the community (mostly scientists... and we scientists are known to write pretty shoddy code). Concurrency in Matlab is gross - in Haskell concurrency is a beautiful revelation :) Streaming big files through a processing pipeline - wonderful in Haskell, not really a thing in Matlab (to my knowledge - maybe someone who does matlab big data would know better). Even parsing CSV data in Matlab is broken because of the way it treats numeric and string data differently. I don't know any R, and I haven't used BioHaskell (I don't do any molecular stuff), so I can't comment much on those. But this message is probably too long anyway.
Posted this in the comment thread on the site but thought it might be better here. While your way works fine, I think, from what I've seen, the "canonical" approach from literature would be to have a layer of expr for every tier of priority. In the case of OR &lt; AND &lt; NOT, and left-associativity, (and this isn't actual parsec syntax) expr = or_expr or_expr = or_expr " OR " and_expr || and_expr and_expr = and_expr " AND " not_expr || not_expr not_expr = "NOT " var || parens or_expr || var In the case of OR == AND &lt; NOT expr = and_or_expr and_or_expr = and_or_expr " OR " not_expr || and_or_expr " AND " not_expr || not_expr not_expr = "NOT " var || parens and_or_expr || var these layers can be parameterized by a list of some sort of course to prevent code repetition...this is really what parsec does best :). and in this way you get rid of SubExpr. For Showing, you can nest your pattern matches: instance Show Expr where show (Not e@(And _ _)) = "NOT (" ++ show e ++")" show (Not e@(Or _ _)) = "NOT (" ++ show e ++")" show (Not e@(Var _)) = "NOT " ++ show e show (And e1 e2) = show e1 ++ " AND " ++ show e2 show (Or e1 e2) = show e1 ++ " OR " ++ show e2 show (Var c) = [c]
Thanks a bunch! I responded on the blog. :)
It's a very subtle distinction. f x is going to evaluate to something different, but if you say let f = ..., ghc starts looking at f as if it were a constant, and not a function. It treats it like a "constant" that returns a function. Where if you say let g x = ..., ghc does not consider it a constant. But yes it really doesn't make any sense from any logical standpoint.
Well, technically, in a real program, you will probably actually at one point in time *use* the function, which allows GHC to infer the type. It won't just blindly chose Integer every time. In "most" real programs, it usually works out fine, as GHC picks to statically pre-compile/fix the type that you use the function on first. However, if you use the function on more than one Num type, that's when you run into trouble. You get the same problem if you define a constant n = 1 and try to use n as both an int and an integer, too. But it's typically rare that you will use it as both. Of course even if the DMR is off, you can *always* force GHC to fix the type and allow it to pre-compile by simply adding an explicit concrete type signature. So really the only issue here is which behavior is "default". GHC people probably decided that most people never actually run into problems with this in real life, and benefit strongly from the optimizations of DMR without ever knowing about it, so it's worth setting it as the default behavior. If it wasn't, they figured that there would be so many cases where it would be optimizable that people never think to optimize by adding the explicit signature.
This issue came up [elsewhere in this thread](http://www.reddit.com/r/haskell/comments/1q4r3b/mindbending_behavior_for_deserialization_in/cdc132v).
heatsink in the IRC channel gave a good example of how this matters when type classes are involved (namely that with type classes you could get different implementations, so you need to re-evaluate for each type class). I'm wondering tho, does this have any observable consequence with mere polymorphism, or do you need class constraints as well for the monomorphism restriction to show up in interesting ways?
Hey mstksg. I've got no difficulty solving this kind of thing with Parser-Combinators. What I'm interested in is if there is a library for evaluating arithmetic expressions.
It works using an API, which you can hit like this: GET jaspervdj.be/lorem-markdownum/markdown.txt?allow-headers=true&amp;allow-code=true&amp;allow-quotes=true&amp;allow-lists=true&amp;inline-links=true&amp;hash-headers=true&amp;asterisk-em=true&amp;asterisk-strong=true I should probably document it a little on the websit.
&gt; GHC is making an assumption to optimize it can't prove. Not exactly. The monomorphism restriction is part of the Haskell standard, so GHC is just following the required behavior for a standard compliant compiler. When the standard was written, it was believed that values with no explicit parameters having implicit parameters would be confusing, so they created the monomorphism restriction. In hindsight, it turns out that the monomorphism restriction causes more problems than it avoids and was probably a bad idea, so GHC provides an extension to standard Haskell to let you turn off the restriction.
Besides scriptability, what's the difference between this and, say, libre office? http://linuxnorth.files.wordpress.com/2012/02/presenter_console.png?w=640 http://img769.ph.126.net/JK58_Zuo2kE52Bnka77Uug==/1065382786851020852.png
True, fair enough, it is technically defined behaviour yes.
The MR specifically only applies to type variables that are constrained by type classes.
Aha, that makes things clearer!
probably the most urgent improvement :)
Once you parse your input into a proper datatype, evaluation should be very easy: data Term = X | Y | Val Int | Add Term Term | Mul Term Term eval :: Term -&gt; Int -&gt; Int -&gt; Int eval X x y = x eval Y x y = y eval (Val v) x y = v eval (Add u v) x y = eval u x y + eval v x y eval (Mul u v) x y = eval u x y * eval v x y If you still think you need a library, you might want to search for "symbolic manipulation", that is, tools to simplify equations while leaving variables unknown, ex. "x + x" =&gt; "2x". I think you're it's more likely to find a complicated library like that, which does a lot more than what you need, than you are to find a tiny library which does just what you need and nothing else.
So, this looks silly to you because your example is so simple. Also, the people talking about 'pre-compiling' are wrong; GHC's "precompiling" is called inlining, and that's mostly unrelated to the monomorphism restriction. Here's a program that might make it make more sense. -- just your every day expensive function fib :: Num a =&gt; a -&gt; a fib 0 = 1 fib 1 = 1 fib n = fib (n-1) + fib (n-2) f = let x = fib 40 in (+x) What type should we give `f`? If we give it the type `Num a =&gt; a -&gt; a`, then *every single time you access f*, `fib 40` will be recomputed--this takes a really long time . On the other hand, if you give `f` the type `Integer -&gt; Integer`, then the first time you call `f`, fib 40 will be computed, and the function `(+ someBigNumber)` will be returned. If you call `f` again, `x` has already been evaluated and the result will be computed much more quickly. The point is, the definition of `f` looks like a value, not a function, so the language makes sure it can actually be computed as a value unless you explicitly tell it not to. Here's another example. x1 = 1 x2 = x1 + x1 x3 = x2 + x2 x4 = x3 + x3 -- etc. Without the monomorphism restriction, xN takes time proportial to 2^N to evaluate (unless the compiler does common subexpression elimination, but that's only relevant because this example is so contrived).
Well…Not everyone in Academia/Industry uses the same tooling, that is uses either: PowerPoint or KeyNote or kPesenter or Openffice or LibreOffice or Slidy ~~of~~ or S5 or (insert other JS presentation tools); or LaTeX Beamer or (insert other LaTeX presentation classes). PDF slides are a decent intermediary format that levels the playing field.~~;~~ No more fiddling about with different software and your slides not rendering properly as the *proper tools* (i.e. LibreOffice and MS Office and KeyNote) still do not know how to handle styles from the ODT and ~~the~~ OOXML formats. The main downside, AFAIK, is that there has never really been a brilliant tool that facilitates an proper environment for PDF presentations that you see with this ~~proper tools~~ i.e. ~~specifically~~ a presenter console. This tool does that. I believe it was designed primarily for academics by an academic. **edit:** grammar
You should have mentioned that it was in London in the title. That said, it seems pretty cool.
The new Cabal has a function whose name conflicts with a GTK function. If you tell cabal-install to use an older Cabal library, then it will build successfully. Try cabal install --cabal-lib-version=1.16.0 poppler That works for me, anyway.
it works for me now as well. Now the trick is to get a laptop to use this with when I go away...
Just a naïve question: what's wrong with mapM (const getInput) [1..10000] ?
(Author of tasty here) Thanks for sharing your experience! We definitely need more blog posts and tutorials about tasty. I'll submit tasty for inclusion in stackage shortly — maybe today or tomorrow. I share the ideas behind stackage, although I never got around to building all of it. But last time I asked, Michael was ok with it. It would be cool if FP Complete IDE could show test results nicely. Perhaps through the JUnit XML files (support for which /u/ocharles has written and will release soon). Or even integrate them directly. I designed tasty with extensibility in mind, so adding special support for IDEs can be done easily as an add-on package. Such direct integration could allow the results to appear in real-time, rather than after the whole test suite has completed. Also, options for running tests (pattern to select the tests, number of genetated tests etc.) could be shown as a web form.
I'll definitely let you know when I'm a bit closer to heading into the area a bit more fidelity about where and when I'll be in France. =)
I suspect that Edward Kmett is some kind of anamorphism consisting of many folded layers of Edward Kmetts... How can I prove this (aside from the observable evidence of his prodigious output)?
I think he wrote a Haskell program to generate HCAR report entries. However, I don't understand why he didn't include a report on this program itself.
Why not ST?
Great report and great works for everyone! BTW, how can I contribute to this report next time? I want to add some material about my works: hoodle, fficxx, HROOT (library and application section may be fit. )
There is actually a reason (note: I didn't specify it's necessarily a good one) for the DMR. Imagine this code: foo = map expensiveFunction [1..] I might expect this to memoise indexing of foo (i.e. once I access an element it stays computed, letting me access the same element without recomputing expensiveFunction). But if the return of expensiveFunction is polymorphic, then "foo" in it's entirety is polymorphic. This means every single access to foo has to recompute expensiveFunction, even if it was previously computed. This is because it's unreasonable to store all versions of all types every time you evaluate it. The DMR is intended to stop unintentional recomputation like this by forcing it to be a single type (i.e. properly memoised) or making you add a type annotation (i.e. explicitly telling GHC it should recompute every time). And while DMR confuses newbies occasionally, I've similarly seen newbies blindly disable the DMR and run into this exact problem. Both outcomes are undesirable, but for historical reasons the DMR is currently in the standard, so it's just a matter of having to educate people about it.
Now I saw how to contact the editor and sent my interest to them.
Make the second number larger: Prelude&gt; mapM return [1..1000000000] &lt;interactive&gt;: out of memory It does not return a single result until it computes all of the results (i.e. the entire list), which is what causes the space leak.
It's not about the data, but the language in which the plugins are written in. Chimera is Python-based, so all plugins must be written in Python. PyMOL also requires that all plugins are also in Python. This isn't a problem if the plug-in is a thin client that communicates to a Haskell-based server (that's what [I did](http://www.haskellforall.com/2013/10/an-all-atom-protein-search-engine.html)), but that's more difficult to deploy and less desirable in the long run for two reasons: A) As computer power improves people will want more functionality to be local and avoid costly network round trips B) Drug companies don't want data to leave their intranet
Well, that's isomorphisms in Hask, but morphisms in Hask are usually called functions, are they not? Really, I may be nitpicking too much.
Well, I guess you could say they are isomorphic in that sense, but that's a free theorem.
I really like Jekor's video series, especially his earlier one "Code Deconstructed". You can find them here: http://www.youtube.com/user/jekor
Really? Huh, I didn't know that was a free theorem.
I meant the first of your sentences. The simplest free theorems are that generic functions are natural transformations.
However using Data.Vector.Unboxed and V.replicateM 1000000000 getLine &gt;&gt;= print . V.sum seems to keep the stack usage to a minimum
TIL about *loeb*: really nifty.
That's because `vector` is fusing away the intermediate vector. It's intermediate stream fusion representation is very similar to a `ListT`. See here: https://github.com/haskell/vector/blob/master/Data/Vector/Fusion/Stream/Monadic.hs#L126 That's basically the coalgebra representation of `ListT` (with some minor variations).
Throwing in threadDelays to fix a test never sounds good. Don't you have some synchronization points lying around you can hook into? Such points often prove useful in non-test code. What do the clocks do with threads exactly?
I agree, and I plan to change it today some time. (It was late last night) The clock threads start a STM transaction where it acquires an explicit ownership of the clock resource and begins to "tick" the delta clock. Which involves only sleeping for calculated amounts of time, getting the current time, and updating STM TVars. A delta clock is often used in real-time systems(RTS). However my use is not for RTSs, but for waking up STM transactions which have aborted, but have asked to be woken up at rough intervals to perform some function that takes a (rough) relative time. The use case for this would be to drop values from a list/map/set that has 'expired' and is referenced by a TVar. ---- I made a `sleepOnSTM` [function](https://github.com/Cordite-Studios/solar-wind/blob/master/Solar/Utility/Wait.hs#L19) which only lets the transaction complete if the variable _has_ changed. This will probably be used to ensure that there's no need for `threadDelay` anymore. Note that `sleepOnSTM` has time-out functionality. 
This is great and thorough answer. I totally agree with your agreement re: Haskell that it's this intimidating beauty that requires real motivation or natural attraction to get to know. Can you describe what apps or processes through the drug discovery process that Haskell's advantages you described in the last para. are best suited for. Example: concurrency. Thanks much.
What about current pain points in bioinformatics? Can't believe all the issues there are solved.
&gt; Yeah, but this is just weird logic, the entire idea of first class functions means that let x = &lt;exp&gt; is no guarantee that x is a function. (I assume you mean 'x is not a function') That's the whole point, actually. You don't care whether `x` is actually a function. What you care about is that `x` is a *thunk*, which means you can evaluate it once and get a result. Whether that result is another function or not doesn't come into the logic on the compiler side--functions are treated just like every other value! The monomorphism restriction guarantees that *definitions* are thunks, so any expensive calculation they do can be performed only once. This is a pretty powerful guarantee! And if you don't like that decision, it's trivial to override by adding an explicit type signature. The spec is generally OK with constant factor slowdowns due to not writing your code in the 'right' way, but there are lots of cases where code could be exponentially slower or worse in the absence of monomorphism. And I suspect that anyone whose spent any significant time writing Haskell has relied on monomorphism at least once without realizing it. Side note: Keep in mind that `x :: Num a =&gt; a` is no guarantee that `x` *isn't* a function, either! Functions are first class types, and there's many valid definitions of `Num` for function types.
For interview purposes, I'd still prefer the original functional solution since it's something that you can explain easily within the confines of an interview. There's no point having an answer if your interviewer doesn't understand it.
I love that solution! I think they're similar in that they're both some sort of corecursion. The `modifyForwards` is your regular State monad modify, `modifyBackwards` changes the way state was read in the past. The left/right ideas are corresponding. I've not used Tardis, though, so I'm not sure to what extent it differs from loeb.
&gt; `snake_case` this is just stylistic, but you'll find that Haskell is a `camelCase` language.
Well, for paedogogical purposes you can always do something like, instance Show (IO a) where show _ = "&lt;IO ACTION&gt;" I wouldn't be surprised if this were a bad idea in production.
I wish haskell would let "-" in names by requiring spaces around the operators. I miss lisp-style-names. 
ST s is a PrimMonad anyways, so I'm not sure what you're getting at. The "unsafe" part of `unsafeFreeze` is that you're not supposed to modify the vector after freezing it, since `unsafeFreeze` produces a supposedly pure value as its result without doing a copy. That's equally the case whether you are using ST or IO.
I tested using Criterion the defer-version and the original directly manipulating IVars (and also a linear version), on a beefed up parallel fibs-like test. Results: linear - 550 ms - (baseline) 8 cores - 70 ms - (87%) 32 cores - 27 ms - (95%) The defer and the IVar version were head to head, the defer being a few percents slower.
That's what I suspected. Makes for a nice one-liner for the OP though.
What's the benefit of "-" over "_"?
You're not alone! I suspect a majority of Haskellers hate the dreaded MR.
Come'on, if you were truly lazy you would not hesitate about using Haskell ;-)
There's HMatrix (note: GPL) and Diagrams. It's not Matlab easy to begin playing with this stuff in Haskell, but also not super difficult.
Isn't this what the ST monad was made for? I haven't used hmatrix or repa yet but plan to soon.
I hacked out some basic DFT stuff thats purely functional using vector a few months back, though its on the back burner while i work on my numerical array / matrix lib. https://bitbucket.org/carter/signals/src/c3fa6f1d32d0d4195abd8b2def5b65f909d45bd4/src/Math/Signals/Transforms/Discrete/DFT.hs?at=master has the dft code. Ian ross wrote some neat example code more recently (and that is pretty darn similar) along with a expository blog post. http://skybluetrades.net/blog/posts/2013/11/13/data-analysis-fft-1.html apparently he's quite a few more posts pending in that series. 
yes, but the current numerical libs out there don't have a pleasant support for such.
Is the ongoing BSD-ifying hmatrix-base going to be released? What does the base cover exactly (or, what parts remain GPL)?
I am making myself a promise to be lazy in the future :)
Thanks, is there a canonical source for documetation? or a good tutorial?
Hyphens are actually used in human languages. E.g. e-mail, Church-Turing thesis, to-morrow (archaic).
[This is](http://dis.um.es/profesores/alberto/material/hmatrix.pdf) the best resource that I know of. HMatrix is a solid library though finding examples of usage is a little hard in my experience. I've also had some good experience with [yarr](http://hackage.haskell.org/package/yarr).
The `hmatrix` tutorial itself is pretty good. [link](http://dis.um.es/profesores/alberto/material/hmatrix.pdf)
Not invented here! :) I jest (a little). There are a couple of reasons. First, I'm not fond of the `List` type class. It is too arbitrary for my tastes. I think type classes need to be used more judiciously. For example, notice how the `List` package has to provide monomorphic versions of certain functions to work around ambiguous type errors. I also don't like the fact that it requires type families. Second, the `pipes` `ListT` affords some efficiency conveniences since it is implemented internally with an optional bind in the base monad. This won't benefit you if you consume it using something like `next` (where the bind must be made mandatory to satisfy the monad transformer laws), but it will benefit you if you convert it to a `Producer` and then use a `for` loop or attach a `Consumer` downstream. Lastly, it's not in `transformers`, and I didn't want to incur yet another dependency, especially when it was only a tiny amount of code (all the heavy lifting was already done by `Producer` and `for`). This is why I proposed adding `ListT` to `transformers` rather than splitting it off into another sub-library.
I was trying to ask why he had to reach for tools labeled "unsafe" instead of keeping his entire computation inside ST. Thanks for explaining what `unsafeFreeze` does, I had never heard of it.
the people who wanted hmatrix to bsd have moved on because they were frustrated about things. Also hmatrix doesn't have a terribly extensible design, so no evolution path within the framework of hmatrix
Open unqualified imports strike again? 
I hope that's not the case. We need some access to basic numerical things, and hmatrix is the best we have for now. I had thought the BSD split was in the repo and just waiting to push to hackage? What went wrong?
In case you are [interested](https://github.com/Cordite-Studios/solar-wind/commit/7b6df9c89a69d6525ad9264b9756b555ab988571), I solved the problem by reusing my `sleepOnSTM` and also splitting the functionality of it so that `splitOnSTM'` returns an `IO Bool` for if it timed out or not. My tests now pass without `threadDelay`s. Thanks for giving me that nudge. :)
You can eta-expand the definition using `cloneLens` to remove the need to use `NoMonomorphismRestriction` (see [the docs for Lens.Family.Clone](http://hackage.haskell.org/package/lens-family-core-1.0.0/docs/Lens-Family-Clone.html)). You can also instead use `RankNPolymorphism` and maybe some type signatures to remove the need to use `cloneLens` at all.
Hi, thank you very much for your comments. 1, Sorry, I should've provided a link to the repo which has a main file - https://github.com/kc1212/enigma-hs 2, My initial thought was using the type synonyms will give the functions a bit more meaning. For example if I wrote plugboard :: Direction -&gt; String -&gt; Char -&gt; Char it may be difficult to figure out what the second argument means. But you do have a point. 3, Noted 4, Noted 5, Yes I was thinking about the same thing. The rotor and rotate_rotor function are both a bit ugly. I will look for a way to improve them.
Is there package for finite element method?
In the [comments](http://blog.sigfpe.com/2007/02/comonads-and-reading-from-future.html?showComment=1171056660000#c2284986681058924897) to the follow up to the loeb article they discuss that while loeb gives you absolute reference, comonad fix (called cfix in the article and wfix in Control.Comonad) give you relative reference. Since you only need your neighbors for the problem, I though I'd take a stab at it. import Control.Comonad import Control.Lens import Data.List.PointedList (PointedList) import qualified Data.List.PointedList as PE import Data.Maybe instance Comonad PointedList where extend = PE.contextMap extract = PE._focus water :: [Int] -&gt; Int water = view _2 . wfix . fmap go . fromMaybe (PE.singleton 0) . PE.fromList where go height context = (lMax, total, rMax) where get f = maybe (height, 0, height) PE._focus $ f context (prevLMax, _, _) = get PE.previous (_ , prevTotal, prevRMax) = get PE.next lMax = max height prevLMax rMax = max height prevRMax total = prevTotal + min lMax rMax - height Edit: cleaned up code
&gt; You can eta-expand the definition using cloneLens to remove the need to use NoMonomorphismRestriction (see the docs for Lens.Family.Clone[1] ). Ohh! That works. Thanks, I'll update the post. &gt; You can also instead use RankNPolymorphism and maybe some type signatures to remove the need to use cloneLens at all. By that I guess you meant `RankNTypes`, which works, but it probably requires more knowledge of the lens library to shrink this down: edge :: (forall f s t a b p . (Functor f, Field1 s t a b, Field2 s t a b, Indexable Int p) =&gt; p a (f b) -&gt; s -&gt; f t) -&gt; (Int,Int,Int) edge l = set l (view l (col i x xs)) (x,x,0)
Go for it. The similarity that [ueberbobo](http://www.reddit.com/r/haskell/comments/1qm1zo/twitter_waterflow_problem_and_loeb/cdecku8) noted earlier between your solution and my Tardis solution probably has to do with the fact that loeb is related to wfix, and Control.Monad.Tardis is based on mfix.
As I mentioned to Chris elsewhere in the thread, you can pull a similar trick to loeb using comonadic fixed points (wfix), while Control.Monad.Tardis is based on monadic fixed points (mfix), so there certainly does seem to be a connection.
The people who wanted to help had to wait a year befor the license change happened. Hence a distrust in contributing. I'm aiming to get my basic array API and some blas bindings out soonish though. 
Another reason we should be [thinking in terms of semigroups, not monoids](http://blog.tmorris.net/posts/applicative-do/). Same goes for `Maybe`. Both are semi-comonads.
I added it to [my post](http://chrisdone.com/posts/twitter-problem-loeb#fastest), it clocks in a little faster with Data.Vector. =)
Please continue your work! I'm hungry to use it when I get the chance. Do you happen to know what license you plan to release it under?
I might see things differently here but i prefer todo the prototyping in R, Matlab or Octave then have a production code in Haskell to run on the embedded platform
Since you brought up "hint" suggests you thought about using Haskell's runtime to evaluate your expressions which is probably overkill for simple arithmetic expressions. Like gelisham recommends, encode your expressions as a simple ADT and write an "interpreter" function that evaluates their numeric quantities. 
For (5), how about something inspired by the "if it's stupid and it works, it ain't stupid" brand of thinking? data Rotor a = Rotor { cur, size :: Int, vals :: IntMap a } fromList vs = Rotor minBound (length vs) (IntMap.fromList (zip [minBound..] vs)) lookup k (Rotor c n vs) = IntMap.lookup (c+k) vs set k v (Rotor c n vs) = Rotor c n (IntMap.insert (c+k) v vs) cycle (Rotor c n vs) = Rotor (c+1) n (IntMap.insert (c+n) v vs') where ((_, v), vs') = IntMap.deleteFindMin vs I think that covers all the operations he used in his implementation. It probably even works by accident if you do enough `cycle`s for `cur` to hit `maxBound`. =)
Aside from the few Warnings you could remove this looks better than anything I would have produced on my first Haskell/FP steps - congrats. Indeed I have almost nothing to criticise here - seems fine. The only things I can think of: * You should write down the type-definitions of your functions. * You do a bit of your logic in the IO monad (counting guesses, deciding if win/loss) - I would refactor this into it's own pure function (more testable, seperation of concerns, etc.) * If you draw characters out of a word I would go with `c` for the identifier instead of `x` (rings a bell with most developers - ok maybe just with me) * And you do not need the "flush" if you do this: hSetBuffering stdout NoBuffering but I guess this is a matter of taste here. Really like it! - Keep going I annotated most if it [here](http://lpaste.net/1786436774269026304) - you should try to refactor `makeGuess` and `play` into pure functions (for example return a ADT indicating win/loss/remaining guesses from `play`)
Good job for your first program! I only got two minor suggestions. First I suggest you add type declarations, it's common practice in Haskell and makes it more clear for the reader what your code means. Second thought is that in the play function you repeat yourself several times, consider writing a function in a where clause or let decleration. For example, at end of the function write this: where pWord = display word If you wonder why you get warnings it is since lpaste runs hlint on the snippets. That's a progran that helps with stylistic errors. For example you use ($) quite a lot even if you don't need it. ($) is for removing paratheses. Welcome to the community and good luck with your time with Haskell! 
Thanks, I implemented the type definitions for each function and I will use c when using the list comprehension on strings, since it makes more sense. I'll try to abstract more logic away from the IO monad as well which shouldn't be too difficult.
Looks pretty clean and well-done. Nice job! Some minor comments: 1. Hooray, somebody who isn't afraid to use tabs for indentation! But... **BOOOO** somebody who uses tabs for alignment. 2. You have redundant parentheses in a few places: `(toUpper letter)`, `(map toUpper word)`, `(show guesses)`. 3. I'm not so sure about `elem x ['a'..'z']`. I might prefer something like `x &gt;= 'a' &amp;&amp; x &lt;= 'z'` or `inRange x ('a', 'z')` for speed; but `isLower x` is even better for clarity. 4. Try to avoid partial functions like `head` unless you can prove that it's okay (in the case of `head`, that means proving you're calling it on a list with at least one element). Since you're calling it on a user-inputted string here, you can't guarantee that. Instead, use a `case` statement and decide what should happen when the user screws up, e.g. userGuess &lt;- getLine case userGuess of c:_ -&gt; makeGuess word c guesses _ -&gt; putStrLn "Oy!" &gt;&gt; play word guesses
whichever one i please. you'll find out and be pleased too. so don't ask leading questions or i may decide to troll :) [edit: moreover, i'm overall viewing it as "how can i help create a nice collection of libraries that will be widely adopted, while also supporting myself"] so realistically i'll aim to get a lot of basic stuff available to the community under a bsd style license. There may be certain things I wont, remains to be seen.
Thanks, I've been following a book from my University's library and referencing stackoverflow and the [Learn you a Haskell](http://learnyouahaskell.com/) tutorial series as I go along. So far the only monad I've really operated with is the IO monad so I know I've got a lot to learn there still.
Thanks, that case function is amazing. Thanks for introducing me to it! As far as using tabs for alignment, it's just a bad habit I've got myself into due to working on a project that uses that style.
The second post is now there: http://www.skybluetrades.net/blog/posts/2013/11/15/data-analysis-fft-2.html The goal is eventually to produce a mixed-radix FFT algorithm with some sort of compile-time empirical optimisation (using Criterion to benchmark different execution plans). We'll see how well that goes...
I Also use 'n' for regular integers
Yeah, `case` (and pattern-matching in general) is bloody awesome! Most languages in the ML family have it, and for good reason. I think it may very nearly make my list of top five secret ingredients that makes Haskell great (along with being pure, functional, lazy, and having an actually well-done static type system).
What the hell? My first FP program sucked ass. Good job!
hehe, neat! I've some ideas myself in that space, but you're doing it right :) I kinda want to take a different approach, more from the fftw/spiral side. namely: do the "factorization tree" (mixed radix) exploration, and use llvm-general to do code gen at runtime for the different "plans", and then benchmark them at runtime, with some standard "fast but good defaults" baked in. Though if we can get good shuffle / simd support in GHC 7.10, could be totally doable to have a really really good FFT written in straight GHC haskell with no FFI C / LLVM runtime code gen. Won't be till 7.10 at the earliest. Releatedly: I'm really wanting to find 1-2 people who want to help hack on adding shuffle / general simd support to all the various GHC backends (theres currently some basic SIMD on the LLVM backend only that Geoff Mainland did, but for FFT and friends, we need a bit more and have it on all the backends!)
reading this more and its kinda cool. Relatedly, have you seen the cyclotomic integers package on hackage? http://hackage.haskell.org/package/cyclotomic its handy for roots of unity, BUT GPLV3 so you may want to just borrow ideas from it. Neat lib though
[Adventures in Haskell](http://www.youtube.com/watch?v=9AllRc64pVE) is a youtube series that takes you through developing a calculator with Parsec. It shows you how to parse numerics of different formats, and how to create a representation of expressions, complete with operator precedence and whitespace indifference.
It looks like you're off to a flying start. If you're looking for some additional learning material with solid exercises, [this](http://www.seas.upenn.edu/~cis194/lectures.html) is fantastic.
And k, and m, and j, and all the other nice traditional math notations.
Your game is not a game at all! You should use vocabulary inside, not enter a guessable word from keyboard. I'm not criticizing, just suggesting a new exercise =)
I get that starting with Haskell can be a challenge in itself but why if c `elem` ['a'..'z'] when there is Unicode? if isAlpha c and us Europeans can play
think about all the africans, asians, greek and cyrillic speaking people as well. they have even more to worry with `['a'..'z']`.
Why not just use Hakyll? There is some extra complexity, but also extra control. For simple sites, you'll write your main file once and then never modify it again anyway.
Enzor, well done. I think building games is one of the best ways to learn Haskell. I attempted to recreate your Hangman game w/ some other features in Haskell like monad transformers, lenses, applicatives, and annotated some of it, if interested you could take a look and tell me what you think. The game looks and acts the exact same as yours, the only thing I changed was that if someone has already guessed a letter correctly then re-guessing it wouldn't cause a penalty. I also used a Set as the underlying data structure for guesses and added a log of what was guessed, wether or not it was guessed correctly, and the time it was guessed. I should probably refactor the game loop, but that's what you get for code written in under 30 mins. UPDATE: refactored game loop https://github.com/dmjio/Hangman/blob/master/Hangman.hs
It's possible to to reduce amount of copying. It compiler could prove that vector/matrix won't be used it's free to use in place. Another variation of deforestation
Variety is the spice of life.
Awesome, I am searching for something like this for a long time. Which kind of plotting library do you use? I searched a bit and most libraries were wrappers to gnuplot. Is there something nicer?
I agree with your words although I don't know what your code does.
Which part do you not understand? I can explain it to you.
FYI- There is a `#ermine` channel on `irc.freenode.net`.
/u/Enzor if you don't understand this at all, don't worry. This is pretty advanced Haskell. It'd be something to work towards understanding over the next year, year and a half or so.
I know it's too early for New Year resolutions, but... Next year i will work hard to make something worthy of HCAR.
It seems that `Reducer` is actually a `ProductProfunctor` instance class Profunctor p =&gt; ProductProfunctor p where empty :: p () () (***!) :: p a b -&gt; p a' b' -&gt; p (a, a') (b, b') from which you can derive the `Applicative` instance for free. 
Code review and pull requests are welcome
I notice that ekmett is the [10th most active](https://gist.github.com/paulmillr/2657075) github user in the last year. I don't think it's just HCAR entries, it generates code commits too.
There's the chart lib, which now has a diagrams backend thanks to this summers GSOC. THough it's not really a light weight API overall. There's also a few other nascent experiments to have nice plotting libs built on top of diagrams. But nothing mature yet. There's some which I'm pretty excited about. But nothing worth publicizing yet. 
You have the time? Woot! I think there's a lot of low hanging fruit in the ghc code gens. Soooooo many opportunities to make things nicer. 
Thanks, I was wondering what I should program next so this gives me some ideas.
Thanks, this is quite inspiring (although as 5outh said below, it's quite a bit over my understanding level currently.) I tried to compile this but I appear to be missing a lot of the packages required. I installed Data.Default manually using "cabal install data-default-0.5.3.tar" but now it says I'm missing Control.Lens. Is there a way I can update my Haskell with all the packages required easily? EDIT: Nevermind, I used cabal install -v lens and now it works fine.
I would like to introduce HROOT (http://ianwookim.org/HROOT) here. ROOT is a large framework for numerical data analysis/statistical analysis. This is a mandatory tool for High Energy Physics experiments. I made a (yet incomplete) haskell binding to ROOT. I hope that this may be interesting to you.
"Copious spare time" is a British expression that means "barely have spare time to wipe my butt". I have two current things going on and once they're done (6 weeks? I'm doing some MOOCs: Mike Genesereth's generalised game playing course and Jeff Ullman's automata theory thing) I won't add anything else to my plate. *Then* I'll be ready to wreak some SIMD havoc.
Curious to know if Ermine extends HMF with type classes or does it use row-polymorphism to do something else to that end?
for postgres-integration one would need a c ffi. that's why i am asking: how tightly is ermine coupled to java?
Does anyone have links to resources on row types? I'm not very familiar with the concept. Or even terms to google, because "row types" is not helpful. EDIT: "row-polymorphism" works
I have been looking for something like this for years. I just presented a bunch of slides yesterday and I wish I had seen this earlier. I am looking forward to my next presentation!
amazing, especially given the relative terseness of haskell!
Well done! My suggestion is about methodology. You use lowercase to denote unguessed characters, and uppercase to denote correctly guessed characters. You've demonstrated that this is a valid solution, but I suggest making the distinction more clear by expressing it as a datatype, with something like: data Letter = Guessed Char | NotGuessed Char Then the state of a hangman session is just a pair of [Letter] and turn count. You can define a type synonym and some functions like type Game = ([Letter], Int) newGame :: String -&gt; Int -&gt; Game makeGuess :: Game -&gt; Char -&gt; Game gameIsWon :: Game -&gt; Bool gameIsLost :: Game -&gt; Bool showGame :: Game -&gt; IO () For the last one, you may want to make a Show instance: instance Show Letter where show (Guessed c) = c show (NotGuessed c) = "_" 
While ermine compiles to lambda calculus, much of the relational calculus isn't implemented in ermine. In particular, some functions are just injected into ermine in [Lib.scala](https://github.com/ermine-language/ermine-legacy/blob/master/src/main/scala/com/clarifi/reporting/ermine/session/Lib.scala) (mostly primitives and things involved with the relational calculus), and some other libraries, like [Vector](https://github.com/ermine-language/ermine-legacy/blob/master/src/main/resources/modules/Vector.e) are essentially wrappers around Java or Scala code. It's not actually bound to the JVM, but it would be a fairly significant amount of work to reimplement all the Scala code ermine exercises via the FFI in C. 
&gt; Hopefully this makes more sense if you go back to the slides of Ed: instead of doing this approach, Ermine actually has a notion of disjointness where you partition a record into two disjoint sets. Slight correction: you can partition a record into *n* disjoint sets.
http://hackage.haskell.org/package/nano-md5 ? http://hackage.haskell.org/package/pureMD5 ?
Fixed, thanks!
Part of the rewrite in Haskell is adding proper support for typeclasses. There's a bit of hardcoded typeclass stuff in ermine-legacy for e.g. numbers, but most uses of what should use typeclasses involves explicit dictionary passing at the moment. 
Can you explain a bit more about what you want to accomplish? Delaying the execution limits the amount of parallelism you can get, and I'm not able to see how it helps address the challenge problem.
The easy way would be to grab a word from /usr/share/dict/words
Thanks for the excellent introduction! I'm definitely going to dig deeper to see if I can relate this to some problem areas I see at work.
Thanks for fleshing out the example! I think that time in the ermine repl (if that is what you are representing with "&gt;&gt;") will be fruitful for me.
&gt; the ermine repl (if that is what you are representing with "&gt;&gt;") Yep, that's the case.
How are you installing this?
&gt; This radical thinking comes from following the idea of immutable data toward its logical conclusion: Immutable data means that we can avoid calling functions more than once, which means that we can write infinite recursion and let the compiler figure out where to end it. Isn't this more a result of lazy evaluation?
On general principles, I assume this will blow up horribly unless you make *absolutely* sure that the f (Matrix x y v') thunk is evaluated enough to completely remove all references to v' before you change v again.
Thank you, I'll keep that in mind.
Something it does that test-framework doesn't is have good documentation (so far....) and have in-the-box golden file testing.
It's great :) Others have mentioned useful structuring issues, like type synonyms, etc. I can offer some non-functional suggestions, just for "idiomaticness". For one, people usually don't use list comprehensions. I don't know why. But really they can always be replaced with a combination of filters and maps and joins (because that's all they really are). Instead of [if c `elem` ['a'..'z'] then '_' else c | c &lt;- word] try map obscure word where obscure c = if c `elem` ['a'..'z'] then '_' else c or even where obscure c | c `elem` ['a'..'z'] = '_' | otherwise = c and instead of [if letter == c then toUpper letter else c | c &lt;- word] try map reveal word where reveal c = if c == letter then toUpper letter else c or where reveal c | c == letter = toUpper letter | otherwise = c it's slightly more verbose but it makes it a little clearer what you are doing.
Great read, definitely worth it.
Check out the paper [Generalizing Generalized Tries](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.223&amp;rep=rep1&amp;type=pdf). See also this: http://hackage.haskell.org/package/TrieMap
An intriguingly simple workflow. It just spits out all of the html files for a static site. Now I challenge someone to take a traditional Haskell web framework, and add the following features on top: * blog author authentication * authors can write and save markup via a web page * the blog posts are generated via HBlog and served statically
Cool! I think I've seen that paper before, actually -- cryptomnesia at work again ...
Also the video lectures on implementing dependent types from OPLSS 2013 are quite good as well: https://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html
spawn(P) returns `Par (IVar a)`, so if I don't wrap with `defer`, the variables on the lefts of the `&lt;-` are `IVar`s, just like in the Monad.Par module docs. But then I have to use `get :: IVar a -&gt; Par a` on the ivars if I want to pass multiple args to a function in applicative style, where the `get` would block, so to prevent blocking the whole monadic computation I would have to use `spawn`. So basically by changing to `defer` instead of `spawn` I can spare the `get` applications at each call site (they are implicitly applied). This doesn't change anything computationally, just nicer to read.
Btw I guess with Applicative-comprehensions for `do` blocks the default syntax could be made better - the IVar stuff is introduced just to avoid blocking the monadic computation, isn't it? Didn't think this completely through though.
Or.. does `defer` defer too much, and the process is not even spawned until the result is forced?
In theory the CUFP talk was recorded. I suppose I should start chasing down what happened to that recording.
Not really. Since we have a static guarantee that a function will have the same result give the same input, we can deduce that there's no need to compute it twice. If you throw in that your data is immutable and persistent, then you can just go ahead and reuse the previous results. That said, the infinite data structure would require laziness if you want your program to terminate. 
Interesting. Could you perhaps tell me why/how? Or point me to places where I could read about why/how? It seems to me like doing so would require evaluating the input to the function completely at compile time (in order to then ascertain what the return type would be), which could be done I guess, but seems like saying the program can be statically checked by statically running the program to see if it is well-typed. For instance, in the example dependently typed lambda calculus in the article, 3 of the typing rules involve evaluating (or running) expressions (i.e. parts of the program) in order to check that something is well-typed.
please, someone answer this (to the positive)
Type checking does indeed involve running expressions at compile time. The way this is done is by keeping function arguments abstract. For instance if you do a `case` analysis on such an argument, then instead of running just one of the cases, the type checker runs *all* of the cases because the value is still unknown. This is a bit like partial evaluation: you run the code as much as you can, but because some values aren't known at compile time you can't fully evaluate everything. For type checking you just need to be able to evaluate it far enough to see that the types are valid. You can write code in such a way that the type checker cannot evaluate the code far enough to prove that the types are valid, and in that case the type checker simply signals a type error (in this way it may reject some programs that are intuitively well typed).
cabal install gtk https://gist.github.com/eccstartup/7494338 I think the problem is the incompatibility with the **new** cabal-install.
I peaked at 5th somewhere around the middle of the summer, but then I slacked off and started giving more talks. ;)
julesjacobs wrote almost exactly what I would have. The point is that you do have to do evalutation while typechecking. You do not have to do type checking with evaluating. The point is that "well typed programs don't go wrong" so once you know something is well typed you can run it without worrying about what the types are. Dependent types are just a way of making more programs that "don't go wrong" type check.
Hehe. Sound like fun! No problem. I won't have time to hack on ghc things till around January the way my fall / end of the year is going. 
my attempts usually end with a working gtk2hs demo app, but some kind of c-library level error in larger apps, that I am never finally able to work around.
Or just allow both st and io and pure code and make it easy for the programmer. 
I struggled to find the right abstractions. The three design aspects that I really wonder about: 1. Was a typeclass the right decision here? I felt that I got good reuse, as I had hoped. But it obviously has the drawback of not being as obvious as functions that correspond directly to API calls. Overall I'm happy with this decision, but am interested in others' input. (See Network/Campfire/Request.hs) 2. The use of unsafePerformIO to get an MVar outside of IO for user credentials. (See Network/Campfire/Request.hs) **Motivation: This was to prevent carrying around the API token an subdomain ** 3. Related to 2, should I have instead gone with a more concrete monadic interface? I could have provided a monad transformer and made instances (MonadIO, MonadState, MonadError, etc.), but I wanted to avoid providing any fixed monadic interface. Thanks in advance! EDIT: Add motivation for 2.
I had a working command-line client, at least until I changed the interface here. I'm going to get that working again, clean it up, and push that to github as well.
I wrote a toy FEA code a couple of years ago when I was first learning Haskell. I used cholmod as the sparse solver. I put an interface to cholmod on github (https://github.com/tdox/hcholmod).
Thanks for the feedback. I do agree that these are hacky global variables. And I definitely agree that as a design, this leaves a lot to be desired. A couple of questions. - Could you elaborate about how this is lying to the compiler about referential transparency? I agree that it is a hack, but given that MVars can only be manipulated in the IO monad anyways, what are the problems that could crop up (besides an error due to not calling functions, which I agree sounds pretty terrible)? It doesn't seem like evaluation order particularly matobably have aters here. - Would the type signature for requestToResponse become someting like.... requestToResponse :: (MonadReader m ClientState, MonadIO m, Requestable req resp) =&gt; req -&gt; m (CompletedRequest resp) data ClientState = ClientState { apiToken :: S.ByteString, baseUrlRef :: String } What I was concerned about was providing an inflexible interface, but this seems flexible and I think it would work. Or maybe I could make my own CampfireT that's really just a ReaderT ClientState. Or is there a better way than these? EDIT: forgot MonadIO constraint in requestToResponse
Here's mine. I'm a n00b so it's probably not great. I haven't implemented conversions to and from bytestrings yet. I'd be interested in critiques. https://github.com/echatav/elliptic
I am preparing for a Gtk2Hs release [here](https://github.com/gtk2hs/gtk2hs). However the biggest problem on OS X is that there is no perfect way to install GTK+. If you don't need WebKit then [gtk-osx](http://www.gtk.org/download/macos.php) is very good (install it like [this](https://github.com/leksah/leksah/blob/master/osx/gtk.sh) to make it work with the official GHC binaries). If you don't care if your app uses X11 then homebrew might be good (I have not tried it because I don't like X11 apps on OS X). Macports works well, but requires that you use it to install GHC too (no way to tell macports to use the native libiconv and libgmp).
GTK on macs is just a nightmare, with or without Haskell.
Hmm. The reason I thought it was marginally better to use a typeclass was only having to specify some threshold of methods. Room.hs: instance Requestable LockRoom () where parseResp = noResponse httpMethod _ = POST reqToUrl (LockRoom rId) = roomUrlFn (show rId ++ "/lock") Search.hs: instance Requestable Search [Message] where respObjName _ = "messages" reqToUrl (Search t) = "/search?q=" ++ unpack t ++ "&amp;format=json" where parseResp has a default based on respObjName and httpMethod has the default of (const GET). I do agree, however, that it's not a conventional API. So it should probably have a better justification, or stick to convention.
What about Qt
Primitives and stuff at: http://hackage.haskell.org/package/hecc
sweet! I'll look at it later. Looks like some decent FFI code. One thing i'm spending a lot of time mulling over is making it easy to support lots of different sparse and dense layouts. supporting both in an extensible and low pain way is actually kinda tricky, but I think I have a decent, flexible solution I think folks will like. Mind you, i'm still playing api whack a mole, but I think i have a nice way to add support for sparse layouts! :) 
Macs have poor support for GTK? Or GTK has poor support for Macs?
Just use `type CampfireT = ReaderT ClientState`. If the user requires a different stack, they can just use `runReaderT` or anything in the [mmorph](http://hackage.haskell.org/package/mmorph) package. Using the `MonadReader` class doesn't really add much, and leaks the fact that the code uses `ClientState`. With the concrete transformer, you can at least hide it later using a `newtype`. You can't do that with `mtl` classes. On the use of global variables: if the user wants to run two requests concurrently, using different access tokens, then that'll fail if the token is global.
Maybe you should read [this quick guide](http://stackoverflow.com/questions/17100036/should-i-use-typeclasses-or-not/17100055#17100055) I wrote for deciding whether or not to use a type class.
We did not cover the things between 1NF and BCNF. Although I know how to decompose a relation to BCNF. However, in the process of decomposition to BCNF, some FD's may be lost, which.. at least in a database environment seems to result in some things sneaking in once you do a natural join with all the sub-relations which are in BCNF. I'd think that this problem of functional dependency preserving will either result in a compiler problem, or it does compile, but it isn't the exact type you expected. This also reminds me of `UndecidableInstances`, but I can't quite connect things in my head right now.
Note that using a concrete monadic interface is totally okay. You can always change it after the fact using [my `mmorph` library](http://hackage.haskell.org/package/mmorph-1.0.0/docs/Control-Monad-Morph.html). See the [Tutorial section](http://hackage.haskell.org/package/mmorph-1.0.0/docs/Control-Monad-Morph.html#g:3) for details and examples.
Thanks for the feedback! To the point regarding globals--that is valid and crossed my mind. Because this is a chat client, however, multiple connections in a single program seemed sufficiently unimportant that I ignored that use case. To the point about leaking the use of ClientState with MonadReader. I think that, or something isomorphic is inevitable. Somewhere the user must specify as much information as is ClientState. So why not just expose the actual data type? To the suggestion of making a transformer, I think that I run into another problem when I try to provide a transformer which is that I need access to IO AND exceptions from IO. Unfortunately I forgot that constraint (MonadIO) in requestToResponse above, but have since edited that. So, I roughly have two options as I see them (please feel free to correct me). 1. Add a MonadError constraint. But this suffers from the fact that MonadError uses functional dependencies, so then there couldn't be any ErrorT between me and the IO monad for it to work. Because then the monad stack wouldn't uniquely determine the error. Which makes me feel like CampfireT would be a pretty lousy transformer. ** Not certain about this one. 2. Provide a concrete Campfire monad using generalized newtype deriving to get the Monad* instances. Then It could just be a reader on top of IO. This would work, but I wanted to avoid this because it's inflexible for what feels like such a silly reason.
How did I miss that? Thanks, just what I was looking for! 
Wow! That's really great. That definitely makes me have to rethink monadic interfaces.
Awesome. I was not aware of that. Thanks!
As a related question, would this be more palatable if the type class didn't show up in the API, even if it remained in the implementation?
actually, non termination isn't really that much of a problem from a type theory perspective--it just means your compiler might get caught in an infinite loop. Divergence can already happen in many type systems (Scala, C++, Haskell with the right extensions). Totality though is important when what you want is a theorem prover.
I wanted to compile ThreadScope and it's doable with Homebrew on Mavericks (unfortunately, Apple seems to revamp the whole toolchain so often that this probably won't work soon): * Install XQuartz (don't forget to log in again). * brew tap homebrew/versions * brew install gcc48 * brew install gtk+ * export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/opt/X11/lib/pkgconfig * cabal install gtk2hs-buildtools * cabal install gtk --with-gcc=/usr/local/bin/gcc-4.8 Unfortunately, compilation with clang did not work.
There is also a [pending pull request](https://github.com/vincenthz/hs-crypto-pubkey/pull/9) adding support for ECDSA in `crypto-pubkey`, but it's still slow and not secure for signing. For just Secp256k1, you might want to consider writing a Haskell wrapper for https://github.com/sipa/secp256k1. If I remember correctly, that library is meant to eventually replace the ECDSA OpenSSL bindings in the official Bitcoin client.
I'm really pleased to see the that pull request. I would much prefer to use an implementation that is part of a well maintained library like `crypto-pubkey`. However, the commit comments suggest this PR is only for secp160r1 and sect163k1. I might have to jump in and get my hands dirty. A Haskell wrapper to that C library is a much less attractive solution as it makes compiling on Windows a pain. 
I have never ever managed to make gtk or gtk2hs work on a mac, despite related attempts. 
I know arrows, so that's an option -- especially because I finally want to do something productive with them.
If you don't care about speed, it's relatively simple to implement it in pure Haskell. On the other hand, if you actually care about speed, you will need to go down to C or even asm for at least the bottleneck routines (mod p arithmetic, especially inversion/division), and then it becomes much more fragile and unsafe...
Here is a [generic implementation](http://hackage.haskell.org/package/fixplate-0.1.5/docs/Data-Generics-Fixplate-Trie.html) of generalized tries for any tree-like data type.
I suppose you could consider a type class `MultiParamClass a1 a2 ... an` as a table whose rows are the types `(a1, a2, ..., an)` with an instance of `MultiParamClass`. Since there's supposed to be only one such instance for any given "row", we hope this table is in at least 1NF. I've never considered splitting up multiparameter type classes in order to take advantage of normal forms. It's a neat idea. It may not be possible in general, given the requirement that only type variables mentioned in the head can be used in the instance definition. It's worth thinking about though.
This, together with the iOS cross compiler project (supporting GHC 7.8) could be the beginning of a very interesting move towards Haskell for mobile developers.
It would fix the problem of users needing to know what instances do. However, you will probably still find that even if you use it internally that you may have to turn on a lot of extensions.
Do i have to FFI each NDK function used my project or there are some [generated] bindings for the stuff?
I know that I made this mistake a lot when I was learning Haskell. I couldn't for the life of me understand how to do exercise #8 in chapter 3 of Real World Haskell (the one about counting a tree's height) because I didn't understand the difference between the type and constructors. I ended up trying all permutations until one passed the syntax checker and then called it a day.
Yeah, my previous reply was a little glib - shouldn't post about type theory so late at night :). Let me try to clarify a little. There are two distinct but related reasons to be concerned about the termination behavior of terms that can appear in types: 1) We typically want our languages to enjoy decidable type checking (that is, the compiler can always tell you whether a program is well-typed, with no chance of entering into an infinite loop). In languages with very expressive type systems, this often involves comparing two terms at compile-time, and the standard naive strategy for comparing terms is to run them to completion and see if they wind up at the same value. 2) In the case of dependently typed languages that we want to prove are sound as logics, we need to be able to give a semantics for types. This turns out to be very, very hard to do if we allow non-terminating expressions in types. This is a separate (but related) problem from non-termination in general - see for example F*, which allows non-terminating expressions but restricts types so that they only mention values. As you noted, not everyone cares about decidable type checking. GHC, with enough extensions turned on, may go into an infinite loop at compile time. There is also a long tradition of extensional type theories which are consistent as logics but do not enjoy decidable type checking.
You need to provide it, https://github.com/ajhc/demo-android-ndk
I think this usage of `unsafePerformIO` for creating global variables is perfectly safe (personally I do it all the time). It's so convenient that JHC even had this as a language extension (top-level IO bindings), as far as I remember. Now whether using global variables is a good idea or not is another question, especially in a library (one obvious drawback is that you can only use one copy of the library in an application), but I would say they have legitimate usage in *some* circumstances. 
A Haskell `String` is a list of Unicode characters, and thus it does not have a canonical byte sequence representation, and thus it does not have a canonical md5sum. You should probably use `ByteString` here. Now, to answer the question, it depends what you mean by "cheapest"? If cheap to implement, then probably something like the pureMD5 package mentioned above (Hackage is down so I cannot check it). If cheap as in computational cost, then find a liberally licensed C md5 code and write an FFI binding - or find a library on Hackage which does the same.
&gt; It's a long-existing convention, probably we shouldn't do such a radical change now, even if it is not perfect. But it should be rethought at some point. What's `Control` and what's `Data`? When is something a `Codec`, when is it part of `Text`? What's `Language`, exactly?
`Language` is pretty clean at the moment. `Data` is overstuffed for sure. `Control` and `Data` is somewhat intermixed. `Text` again does not seem too horrible. In any case, we have a large existing ecosystem. It's most probably not worth the pain throwing all this out.
`PackageImports` doesn't solve the point 2), i.e. the inconsistency of namespaces chosen by authors across multiple packages. Excuse me for sarcasm, but I want to point out the invalidity of an argument about something existing for a long time: inquisition was a long-lasting convention too, way longer in fact, slavary too. Finally, what I propose is not a radical change. It'll still be up to package authors to decide whether to follow it. 
Ok, I slighly misread 2). &gt; Finally, what I propose is not a radical change. It'll still be up to package authors to decide whether to follow it. But if all the existing packages do not change their namespaces, how does this proposal solves anything? For example I usually try to use your proposed convention (though not with a top-level root, but with root embedded into the present convention. Eg. `Data.Generics.MyRoot.XXX.YYY`). But of course my actions do not change to general situation. 
By the way, with the new shiny Hackage2, how could one set up a local Hackage mirror? Is there a tutorial for that?
I've started doing this, yes. My `formatting` package was `Text.Formatting`, now it's just `Formatting`. `fay` was `Language.Fay`, now it's just `Fay`. The taxonomy stuff emulating Java and C# is pointless, waste of reading and typing, and has never actually been helpful.
Btw, the existing naming convention was pretty much an *official policy* when I got into Haskell (around 2006-2007). So yeah, it has disadvantages, but there was probably some thinking and consensus behind it (and I guess it has advantages, too). People adhering a non-optimal convention is probably still better than half the people using one convention and the other half the other one.
Personally I always disliked the `newtype X = X { runX :: ... }` or `newtype X = X { unX :: ... }` style. It seems rather odd to have a *field* with such a name. Instead I typically do `newtype X = X x; runX (X x) = x`. There are very many examples of the former in standard Haskell libraries though, so I think lots of developers must like it.
very interesting thanks for sharing
I deliberately put `lens` under `Control.Lens` both to avoid conflicts with `Data.Lens` and to promote the notion that I now view a lens as a control structure rather than a simple data type.
When we taught a Haskell Workshop, we did the same thing: use MkT for the constructor of type T. The difference between types and values isn't quite clear for all beginners, so it's best to be explicit. I think the type variable/data variable convention is pretty standard in the Haskell community. I'd be surprised if there were tutorials doing it differently. Have you encountered any?
We all know that `Text` is best for text, but by comparison to `String` it's a recent invention, so there's plenty of things that haven't migrated yet. Plus `String` is really convenient to program with.
I kind of wish records weren't tied to ADTs. Having ADTs with multiple constructors mixed with record syntax is weird, and it stinks that conflating them seems to encourage this odd way of thinking about records. It would be nice if we just had anonymous records. In addition to being able to use them more conveniently, we could also wrap them in newtypes and use them as fields in ADTs or other records, without conflating them, and it would all just kind of make sense. We could even use some variant on the anonymous record syntax to optimize for records with only one field, similarly to newtype, which would give us something like labeled arguments without having to define a newtype first.
&gt; there is {-# LANGUAGE PackageImports #-}, which is a very low-cost and pretty clean solution This doesn't remotely count as a solution. It's just adding another layer of namespace hackery to the language as a band-aid.
I mostly agree with this, though I have no strong feelings on - becoming . in some cases, especially with related packages (such as pipes-binary) where the author controls the root namespace package as well.
That's a very good point about the flexibility of "instances" as values. I've actually used a solution like this before and found it significantly better than my initial type class implementation. The problem then was that my instances had to be parameterized by an MVar (FFI code). Thanks for those links. I've done a cursory read, but will have to dig into your implementations of StateT, etc. later. Second to last question: one nice aspect of multiparameter type classes (with functional dependencies) was that the types worked out without needing one huge datatype. And I could express the type-level relation between request types and response types explicitly. So `requestToResponse` looked something like: requestToResponse :: Requestable req resp =&gt; req -&gt; IO (Maybe resp) Using a record instead, however, I guess I would have to use something like phantom types? (Otherwise I think I'd need a big sum type, which was one of the reasons I thought to use type classes instead initially). Is there another way to do this? requestToResponse :: Request resp -&gt; IO (Maybe resp) This doesn't seem to capture the type-level relation quite as obviously, though, and only works if my values correspond correctly to my types, whereas before I need only look at the type signature to see that it made sense. My last question could potentially have cropped up in the type class implementation as well. Suppose I wanted my own equality "class" with default implementations. data MyEq = MyEq { mEq :: a -&gt; a -&gt; Bool , mNotEq :: a -&gt; a -&gt; Bool } defaultMyEq = MyEq { mEq = mNotEq defaultMyEq , mNotEq = mEq defaultMyEq } class MyEq a where mEq :: a -&gt; a -&gt; Bool mEq = mNotEq mNoteq :: a -&gt; a -&gt; Bool mNotEq = mEq Are there any compiler optimizations (in either case), e.g. inlining, that could get me into trouble here? Thanks for the great feedback. I really appreciate that you explained *why* this style is bad.
It's the `Writer` layer that is the problem. The `transformers` `WriterT` (both lazy and strict) is essentially broken because it always space leaks and this causes a huge performance penalty. I'd link you to a libraries mailing list discussion about this but haskell.org is down right now. I can summarize it like this: you can dramatically speed up `WriterT` by simulating it using `StateT` and making `tell` strict in its argument (assuming that your accumulator has no room for internal thunks, which you can solve by unboxing all of its fields and subfields). The basic rules of thumb are: * Fast: `ReaderT`, `StateT`, and the fixed `WriterT` * Medium: `EitherT`/`ErrorT` * Slow: Broken `WriterT` There is nothing intrinsically slow about monad transformers *per se*. It's just specific types or implementations of monad transformers that have bad performance properties.
Yeah, it might take some getting used to, but the beauty of this is that the description of the whole convention fits in a single sentence and no exceptions means no ambiguity.
I understand, and in that regard it makes sense to add a "control" tag to the "categories" field of the ".cabal" file, but I don't see why this must be reflected in the namespace. The only reason I see is to reduce the possible namespacing conflicts, and that is what the current proposal approaches anyway.
&gt; The taxonomy stuff emulating Java and C# is pointless waste of reading and typing, and has never actually been helpful. Exactly!
i think this confusion is reinforced by syntax highlighters, that paint both terms and types by the same color. data Thing a = MkThing a Thing and MkThing should be colored differently, but all editors that I've seen don't do that. 
You don't have to use phantom types. Just add ordinary type parameters to your record: data Requestable req resp = Requestable { blah :: req -&gt; IO () , blah2 :: IO resp } Then an example function would be: requestToRespond :: Request req resp -&gt; req -&gt; IO resp requestToRespond (Request {..}) r = do blah r blah2 The compiler will still infer types and type parameters correctly. For your second question, the way you implemented it is not correct. Your circular references are immutable, meaning that they won't update in response to each other "changing" (because neither can change). Your code is functionally equivalent to: defaultMyEq = MyEq { mEq = _|_ , mNotEq = _|_ } To better understand why, consider this simpler example: ones = 1:ones That creates the list `[1, 1, 1, 1..]`, but you can't "mutate" the first element, so there is no way to "update" the rest of the list. This is the same reason why your default `MyEq` doesn't work, because modifying the `myEq` field in your record is not really mutation. It's just creating a new `MyEq` constructor with a new `myEq` field in place of the old one and reusing the old `mNotEq`. The old `mNotEq` still points to the old `myEq`, which still points back to `mNoEq`, so it's still undefined. The correct way to implement your default is to create two "smart constructors", one for each default implementation: myEqFromEq :: (a -&gt; a -&gt; Bool) -&gt; MyEq myEq (==) = MyEq (==) (\a b -&gt; not (a == b)) myEqFromNotEq :: (a -&gt; a -&gt; Bool) -&gt; MyEq myEq (/=) = MyEq (\a b -&gt; not (a /= b)) (/=)
Today i installed a fedora system on laptop specifically for Haskell development. Normally I'm using macos and all the package management with port does not run smoothly. I was pretty disappointed when cabal did not do anything and canceled with a timeout. So thank you for providing the mirror!
Different colors for value-level and type-level expressions. I like it.
To the first, that makes sense. To the second, I should have actually tried to run that example, because it's obviously wrong. Smart constructors make sense. Thanks a lot for all of the feedback.
Apart from the big "WARNING! May be out of date" on the top of the page :) yes, I guess it helps, thanks!
&gt; Also, there are too many cases when people see the weaknesses of this approach and, finding no alternative, they invent their own, which in its turn forms a quite dangerous tendency of turning this problem into a complete chaos. That's exactly what you are doing right now :)
I don't know it wasn't really a problem for me, the difference between the constructor(s) and the type can be easily presented by considerng an ADT with multiple constructors. On the other hand I like how the ML distinguishes between types and constructors by forcing the user to use lowercase for the former and uppercase for the later (albeit the type signatures looks uglier that way, in my book)
Actually I haven't looked into your library (sorry for that!), I was just commenting on this particular usage of `unsafePerformIO` you also happen to use.
It's not because they are more explicit that they should be taught to beginners by default: it's because they are more general. Intuition for only algebraic data types is uninteresting: much better if an intuition can be got from the start for inductive families.
I'm comfortable with type+data constructors being capitalised and type+data variables being lower case. If data constructors were lower case, that would make pattern matching look awkward, IMO. What annoys me is type synonyms being capitalised. (I realise this is what allows `forall`s to be optional: language design is a series of trade offs.)
Also, it is unfortunate that if you decide to replace a data constructor with a "smart constructor", you must change the name. After all, they are both just functions, with the same signature even. I remember being confused while reading Real World Haskell, because it uses the `State` data constructor which was subsequently replaced with the `state` function.
&gt; the difference between the constructor(s) and the type can be easily presented by considerng an ADT with multiple constructors. I strongly disagree with this, since the contrary is shown in the SO question I linked. The data type was data Tree a = Leaf a | Node [Tree a] 
I believe it is also perfectly safe in a multi-threaded environment (I definitely use this a lot in multi-threaded environments). I think the actual semantics (with GHC at least) will be that when you first try to use the MVar, that is the moment the MVar will be created (since we are lazy). Since the MVar itself is just a reference, it is fully referentially transparent, and MVar-s are by definition handling multi-threaded access nicely (that's the whole point). However, it is only safe on the **top-level**; you cannot write a function which creates MVars on-the-fly this way.
&gt; Btw, the existing naming convention Is there one of those? People seem to use `Data` and `Control` for half of everything, and pseudo-randomly.
The traditional ADT syntax is much more compact, though it certainly takes some training to learn how to read it. That's part of the reason I use GADT syntax when presenting ADTs to non-Haskell / non-expert audiences. Once you explain type sigs, GADT notation is straightforward.
Don't forget: 4. A case-eliminator pattern. Unfortunately, Haskell doesn't offer a way to distinguish the availability of #2 and #4, but they should not be confused for being the same thing.
Yeah, try just `ReaderT` + `StateT`. Or try merging them into one monad transformer (like you originally proposed). You do pay a price for `WriterT` even if you don't use it, since with each step you are constantly creating new `mempty`s that get lazily mappended into your accumulator, which builds up thunks. Also, if you have two `StateT` layers you can merge them into one by just combining their states. You might think that this would complicate your code (working with a pair of states), but it won't if you use lenses, specifically the `zoom` combinator. `mtl` is just a type class interface on top of `transformers`, so they should perform identically. Laziness can sometimes improve speed because it gives the compiler more opportunities to reorganize code and avoid unnecessary computation.
I chose to reflect it in the namespace as a matter of taste and fitting into the existing ecosystem, to avoid getting yelled at by cabal for uploading to a non-standard top level domain, and generally because I don't feel nearly as strongly about this issue as you do. ;) I could reduce namespace conflicts much farther by using `Com.Comonad.Lens` as well. ;) I may eventually move `lens` to a top-level namespace, but there is a counter argument about separating provenance from provisioning that the other side of this debate has that carries some weight with me. I've moved a _lot_ of modules between packages. Your proposal makes that a much more disruptive change.
I've thought about that. Borrowing notation from GHC core, you could use `!` to signal saturated construction and deconstruction.
Delete (actually rename, in case you want to undo) the ~/.cabal/ and possibly ~/.ghc/ directories. You might need to keep some files in ~/.cabal/bin/ (try without, add if necessary). Unfortunately I need to do this from time to time myself. 
I really appreciate the feedback. Otherwise I would have maintained my very narrow view of the design space. I was using default implementations for implementing other methods: -- | Typeclass representing a request 'req' that can be transformed into -- a response 'resp' by making an HTTP request and parsing the result. class FromJSON resp =&gt; Requestable req resp | req -&gt; resp where request :: Monad m =&gt; req -&gt; IO (Request m) request req = mkRequest (httpMethod req) (reqBody req) (reqToUrl req) reqBody :: req -&gt; L.ByteString reqBody _ = "" httpMethod :: req -&gt; StdMethod httpMethod _ = GET parseResp :: req -&gt; L.ByteString -&gt; Either String resp parseResp r s = eitherDecode s &gt;&gt;= parseEither (\(o :: Object) -&gt; o .: (respObjName r)) respObjName :: req -&gt; T.Text respObjName _ = "" reqToUrl :: req -&gt; String `reqToUrl` must be specified, obviously. For parsing the response, only`respObjName` needs to be specified in the simple case, but `parseResp` could be "overridden" in more complicated cases. (I don't think I had to do this?) Similarly, for actually constructing a request there are three "tiers". GET requests (the common case) don't ever have to touch `httpMethod`, `reqBody` or `request`. Most POST requests (slightly less common) need to "override" `httpMethod` and `reqBody`, but not `request`. And uploads are the odd-man out, and need to "override" `request` directly because they construct a different type of HTTP request. So overall I liked that simple requests could be two-line instances, yet there was still enough power exposed for nasty instances like file uploads. Again, thanks for the great feedback!
&gt; I could reduce namespace conflicts much farther by using Com.Comonad.Lens as well. ;) That would be redundant. The package name is already secured to be unique. So the only thing it would do is burden us all with more typing. &gt; I may eventually move lens to a top-level namespace, but there is a counter argument about separating provenance from provisioning that the other side of this debate has that carries some weight with me. One thing that might not be exactly evident from what I've said previously is that the package author can still use the same old "categorization" conventions, if it's up to hist taste, just in a safer way, e.g.: package name: `some-package` modules: `SomePackage.Control.Monad.SomeMonad` `SomePackage.Data.SomeVector` 
Just in case you don't have / know about it already, the latest version of cabal has a feature called sandboxes and can build packages in parallel, both hopefully helping you spend less time with package stuff.
&gt; I'd link you to a libraries mailing list discussion about this but haskell.org is down right now. Are you referring to * [Stricter WriterT monad transformer](http://comments.gmane.org/gmane.comp.lang.haskell.libraries/18040) * [Stricter WriterT (Part II)](http://comments.gmane.org/gmane.comp.lang.haskell.libraries/18980) ?
If you are going to throw out the existing practice and start from scratch, then you shouldn't jump straight to a new solution. We don't even have agreement on what the requirements are, so that's a better place to start.
Ok great, I'll try getting rid of the writer then! I haven't gotten to learning lens yet, so it's a bit hard to imagine how I would I would keep, for instance, the state from the main application code and the UI layout/drawing code separate. So much to learn...
Thanks, I'll definitively read that!
Is there anyway to get a mirror of Hoogle? Googling finds [this](https://pay.reddit.com/r/haskell/comments/z1aax/hoogle_mirrors/), but trying to follow these steps leads to it trying to fetch http://www.haskell.org/haskellwiki/Keywords which obviously fails.
[This](https://www.fpcomplete.com/hoogle)?
Thanks!
Good luck with the servers and the host!
Yes, that's the one!
To use hdiff instead of hackage with cabal (and cabal-dev), change the remote-repo property in '~/.cabal/config': remote-repo: hdiff.luite.com:http://hdiff.luite.com/packages/archive edit: And I'm not sure about it's completeness but fpcomplete (hah..) seems to host a hackage mirror at http://haddocks.fpcomplete.com/ edit2: [And here is again the fpcomplete hoogle mirror](https://www.fpcomplete.com/hoogle) (from [below](http://www.reddit.com/r/haskell/comments/1qrldv/haskellorg_servers_down_at_the_moment/cdfwila))
Again. This introduces issues of conflating provenance with purpose. To steal a [comment from Wren Thornton](http://osdir.com/ml/libraries@haskell.org/2009-06/msg00197.html) from one of the dozen or so previous times a similar proposal was offered: &gt; To make things more concrete, it's the provenance issue. We don't want to encode package versions in the within-Haskell module names, for obvious reasons. Similarly, we don't want to encode the package names. Over time packages have a tendency to grow and split into multiple packages, and we don't want code that was valid at minus-epsilon from the change to break at plus-epsilon when no actual code has changed. And there are similar issues with merging packages or migrating modules from one package to another. I regularly move things between `SomePackageExtras` and `SomePackage`. I don't want to just randomly start breaking one set of users' code when I do so to satisfy your particular nervous tick about organization, and I feel little need to break everyone's code to do it in the first place, when there is a reasonable argument not to, and a lot of pain to make the transition.
The namespace serves two purposes, but I'm not going to say that it does either perfectly. 1.) The hierarchy of module names serves as a rudimentary ontological classification of the purpose of the code involved. It does a pretty crappy job of distinguishing `Control` and `Data` as the line between these two categories is particularly blurry, and too many people shove everything into `Data` by default, but elsewhere it does a passable job. 2.) The separation of the module names from the package name permits code reorganization across package boundaries. The is a consequence of the current separation of provenance and purpose. _e.g._ Many, many things are no longer in `base` that used to be. More come in and leave on a regular basis. Under your proposal instead of `Data.Array.IO` remaining stable across that transition, we'd have gone from `Base.Array.IO` to `Array.Array.IO` or `Array.IO`, breaking all code using it, rather than the status quo, where I can write code that works clear back to `base 2.0` and `hugs` if I'm careful. We're going to keep spinning things in and out of `base` over the life of the Haskell ecosystem. I don't want to make those transitions any more needlessly painful than they already are. I don't care much about the former issue, except by convention. I do, however, feel rather deeply about the latter.
[This paper](http://web.cecs.pdx.edu/~mpj/pubs/fundeps-design.html), referenced by the [HASP project](http://hasp.cs.pdx.edu/), talks about this a bit. Edit: Apologies - it's already mentioned in the HaskellWiki article above.
Dayum, was hoping to set up [xmonad](http://xmonad.org/documentation.html) how I liked :( (docs link to haskell.org)
the relational algebra would need to be different for in-postgres ermine anyway i assume. i guess i will experiment with pl/haskell first and get to ermine later, when the haskell port is closer to being finished.
What I often do to avoid the need for zooming is use the `HasS1` and `HasS2` classes I can generate with `makeClassy`. Then you can just work with total :: (HasS1 s, HasS2 s, MonadState s m) =&gt; m C without having to commit to an actual concrete monad or `zoom`. This facilitates easier mocking and piecemeal testing. Later on you can just make: data S3 = S3 { _myS1 :: S1, _myS2 :: S2 } makeLenses ''S3 instance HasS1 S3 where s1 = myS1 instance HasS2 S3 where s2 = myS2 
I would relax the proposal to this: "Don't try to stuff your namespace under Control / Data if possible". Otherwise freedom to authors.
You also may want to retain `~/.cabal/config`.
Something like [this](http://www.reddit.com/r/haskell/comments/1mzrmz/how_do_i_install_get_haskell_running_on_a_mac/).
I just reread those emails (I had only skimmed them when they were new). I notice that at no point did you mention this representation: newtype WriterT w m a = WriterT (forall r. (w -&gt; a -&gt; m r) -&gt; m r) This is the most straightforward (to me) translation from the original to a CPS transformed version. Did you consider/benchmark this representation, and if so, why did you reject it? One reason I can think of is that this is not Haskell98, which would be necessary to get it into transformers, IIRC.
I don't remember if I tried it or not, but I'm guessing that I didn't. Can you try it out and see if that works, too?
Here's how you get the nice laziness: &gt; takeWhile (&lt; 10) [1..] [1,2,3,4,5,6,7,8,9] For list comprehensions, the comparison desugars to `filter`, not `takeWhile`, which is why you didn't get the lazy behavior. 
&gt; In this case, it seems more sense to go back to languages like C and Fortran.. Although you have to write out more steps, at least you are certain about how they will function. This is a strange argument. As a newcomer to Haskell, you should not expect to understand the language completely. By continuing to express yourself in Haskell, you will learn about its intricacies. You've already learnt that [x | x &lt;- [1..], x &lt; 10] will evaluate the entire list [1..], and you know why, as well: filtering a list against a predicate requires checking every element. Continue to learn about Haskell, and soon enough you *will* be certain about the semantics of your programs. You wouldn't stop learning a foreign natural language simply because you don't understand everything that the native speakers say. &gt; I decided to see whether or not the programmers of Haskell programmed the "Laziness". Should I really let go of my optimization techniques, and assume the compiler will know exactly what to do all the time? Of course not, but you shouldn't assume that optimization techniques from a strict language apply to a lazy one. Laziness is not about performance optimization, it's about semantics. Sometimes laziness helps us, but sometimes it hurts us. Indeed, there are many cases in which laziness slows Haskell down and leads to space leaks; just take a look at [this recent post](http://www.reddit.com/r/haskell/comments/1qrp8l/improving_performance_of_complex_monad/) and do a search for the lazy I/O problem and some of its solutions (iteratee, conduit, and pipes).
&gt; not only does the program never finish, but I never get any results back while it is running I guess that's a weakness of FP Complete's implementation. In GHCi (and indeed in your compiled Haskell program) you *will* get the partial results: Prelude&gt; [ x | x &lt;- [1..], x &lt; 10 ] [1,2,3,4,5,6,7,8,9^CInterrupted. Haskell is in a sense far easier to optimize by hand than C or Fortran, because there are far more semantics-preserving transformations available to the programmer.
I tried it. Sadly, it does not take constant space. I haven't looked into why yet.
I tend to disagree. Indeed C# and Java have a very poor implementation of this however. For a good example I suggest looking at Perl (I can't believe I'm saying that). For example, the net prefix: https://metacpan.org/search?q=Net- Much better organized than in Python I think.
This sentence: [x | x &lt;- [1..], x &lt; 10] contains all required information for a compiler to reduce it to a simple 9 element list. If a program given this information never returns this means there is a bug its type system. 
I used to think as you do, but these days I'm not so sure. I prefer to say which data the types contain, rather than what types the constructors have. On the one hand, the former is a better fit with bidirectional type checking. On the other hand, the latter delivers an inductive definition of equality, when it might be better if our notion of datatype were agnostic about how equality is defined. The GADT notation also fails to express the opportunity to inspect the indices of a type computationally in the process of figuring out the choice of constructors available at that index, in that it presents the indices of each constructor's return type as a posterior notion, computed from the constructor's arguments, when they should be a prior notion, as the indices for given data are present already in the type being checked. Of course, I'm not really thinking of Haskell here, which has already institutionalised a rather peculiar equality, capturing the questionable idea that types are inherently first order syntactic things. But there's a divide between traditional Haskell datatype syntax (being a variation on the ML syntax, also used in Agda 1) and GADT syntax (being a variation on the inductive family syntax used in Coq, Epigram, Agda 2). I now prefer the former, despite having implemented one of the latter, although I agree with others in this discussion that Haskell's tendency to put types where values go is perplexing. I want to start with the type on the left-hand side and say which values exist on the right. The twist is that I want to be able to pattern match on the left, just as if I were writing a program. So (made up notation alert) vectors might be data Vec (n :: Nat)(a :: *) where Vec Zero a :&gt; Nil Vec (Suc n) a :&gt; Cons (x :: a) (xs :: Vec n a) noting that the `a` in both cases and the `n` for `Cons` are *not* existential, but rather obtained from the type; simply typed lambda terms might be data Tm (g :: [Ty])(t :: Ty) where Tm g t :&gt; Var (i :: Elem t g) | (f :: Tm g (s :-&gt; t)) :$ (a :: Tm g s) Tm g (s :-&gt; t) :&gt; Lam (b :: Tm (s : g) t) so that every object language type allows variables and application, but function types also support lambda. This :&gt; is a crude ascii rendering of the reverse epsilon, \ni, meaning "has elements". Note that giving the arguments names as well as types helps to emphasize that the argument is a thing of the given type, not the type itself. The same convention also facilitates type dependency (earlier names can be used in later types) and interactive editing (names used in the type declaration can establish the naming scheme to be used when case-splitting interactively, as Agda does badly and Epigram (see also the cabal package shplit) did well). In choosing a notation for Haskell, it might not be necessary to worry about the nuances of what it also makes sense to choose in dependently typed languages which don't yet exist. On the other hand, given that the effective design philosophy for a dependently typed language is "mimic Haskell until something forces you to think about what type dependency makes you do instead", perhaps we should start by fixing Haskell, so that there is a default design choice to adopt in the future which happens to be appropriate.
I still think basic ADTs have their place precisely *because* they're less general and more implicit. Relatively simple and concise syntax for the common case, saving the pedantry for where it's needed. Neither you nor vladley said otherwise, of course. But if both have a place, I'm not convinced it's right to throw beginners in at the deep end. It could put people off, convincing them Haskell is more verbose than it really is. That said, I did struggle with some problems that needed GADTs because I didn't know about them, then later because I hadn't understood the point of them, so learning about them earlier would have been better. 
In Java, this is equivalent to: public List&lt;Integer&gt; filterLessThan10() { List&lt;Integer&gt; xs = new LinkedList&lt;Integer&gt;(); for(int x = 0; true; x++) { if (x &lt; 10) { xs.add(i); } } return xs; } The programmer has defined a function that is bottom (never finishes its computation by definition). If the compiler optimizes that behavior away, it has changed the behavior of the program in a fundamental way that directly conflicts with the explicit behavior defined by the programmer. That would be a bug in the optimizer. Alternatively, even if the optimization you propose was valid, a type system alone isn't sufficient to derive this optimization, so it wouldn't be a bug in the type system. When you start deriving robust mathematical properties and constraints, you start moving into symbolic execution territory (which, granted, many optimizations are mini versions of).
/u/kalcytriol is a troll, so don't pay too much attention to what he says (WTF this has to do with the type system), But can't be this achieved somehow, maybe a rewrite rule? AFAIU, in this case we got an enumeration (+1) that satisfies ordering ( &lt; ), so it's safe to do change filter (&lt;10) $ enumFrom 1 -&gt; takewhile (&lt;10) $ enumFrom 1 Could this intrinsic relationship beetween Enum and Ord be signaled? This changes a non-terminating program to a terminating one, but IDK if this is perverse.
If the compiler knows math. And this is a very big If, I bet no modern compiler does understand basic math associated with logical reasoning.
OOP confounds a specific type with a hierarchy of that type and its subtypes, using the same name for both. Haskell can confound a type with a constructor, using the same name for both. For someone coming from OOP, thinking the constructor represents a kind of subtype is an understandable confusion. It's a shame variant record types aren't popular in OOP languages, as they're basically the same as ADTs, so the constructor equates to a tag/discriminant - not a subtype. Personally, because I used languages that had variant records many years ago, I recognized ADTs as closely related immediately when learning Haskell. The surprise for me was discovering that using constructors to construct values doesn't just *look* like function application, those constructors *are* functions. 
[This StackOverflow post](http://stackoverflow.com/questions/17749756/idiomatic-haskell-type-inequality/17794490) may be helpful. FWIW, I'm working on a lib to produce XML documents that conform to a particular schema, and I've found it useful to be able to add constraints, e.g. `NotX p =&gt; p`, to enforce nesting rules.
For a wise a language. It should regard the Enum as increasing or decreasing.
OT question to litehacker: are you trying to troll us here?
Then this would solve neither of the declared current problems.
Unlike in other languages, the meaning of (&lt;) is not cut in stone. One could write this: numbers = [ x | x &lt;- [1..], x &lt; 10 ] where a &lt; b = True and this re-binding could be more complex so that the compiler could never decide whether some iterator really means some primitive operation on basic types. 
That's really elegant, guess I'll stop messing around with EKG and criterion and learn lens! ;-) I was always frustrated that there are not more of such features in the state monad itself, but it seems I can easily do all of this using lenses. btw, I now tried replacing the RWST with a StateT + ReaderT, and it unfortunately does not make a difference in my benchmark. Running the StateT in the UI code on top of plain IO vs StateT + ReaderT gives a significant speedup. Also, inlining all the UI code does about the same thing, while INLINE + the Monad m to IO change only results in a tiny further speedup. From this I'd conjecture that the issue here is not the bind overhead from RWST, but simply the difference between running in a polymorphic Monad m / MonadIO m / Applicative m vs running in a concrete IO monad. Inlining would allow GHC to specialize all the UI code to concrete types, removing all the typeclass overhead / inlining barriers, just like changing to plain IO does. So the answer here is applying INLINE / SPECIALIZE pragmas to let the GHC optimizer do its magic?
&gt; Many, many things are no longer in base that used to be. More come in and leave on a regular basis. Under your proposal instead of Data.Array.IO remaining stable across that transition, we'd have gone from Base.Array.IO to Array.Array.IO or Array.IO, breaking all code using it, rather than the status quo, where I can write code that works clear back to base 2.0 and hugs if I'm careful. Let's not forget that when you move something from one package to another it inevitably requires the user to update the project's dependencies to make it compilable. You make it sound like it's something unbearably difficult, but with the proposed convention, all the user will have to do on top of that is just perform two replacements across all files, e.g.: * replace `import Data.Array.IO` with `import Array.IO` * replace `import qualified Data.Array.IO` with `import qualified Array.IO` Besides, something moving from one package to another is not something that happens on a daily basis. It's a case of a radical change to a package and, the chances are, that comes with a set of other not quite as trivial compatibility breaking changes.
&gt; I regularly move things between SomePackageExtras and SomePackage. I don't want to just randomly start breaking one set of users' code Let's not forget that changing packages breaks projects too: the user has to update the dependencies, but generally, yes, this convention might require modifying certain approaches to packaging. But then IMO it's a price very much worth paying. I explain why in detail [in this comment](http://www.reddit.com/r/haskell/comments/1qrilm/packages_and_namespaces_naming_convention/cdg9zwd). &gt; satisfy your particular nervous tick about organization I can't help but find this offensive. Let's stay constructive here. You've already acknowledged some problems of current practices yourself in this thread. 
Do you have ulimits (or something similar) in place to keep the system from trashing to death when there is a space leak? That has saved my butt a few times. The OOM killer is usually too slow to respond before everything grinds to a halt.
&gt; If you are going to throw out the existing practice and start from scratch, then you shouldn't jump straight to a new solution. As stated in the first statement of the subject message, it's an open discussion. I expect the community to come up with a collective truth thru both criticism and enhancements. &gt; We don't even have agreement on what the requirements are, so that's a better place to start. Okay. Start.
Yes, sorry, I should have formulated my statement differently. That how I personally got the ADTs thingy. I don't know if there is a universal method for teaching ADTs
Wow, thank you. Cheers.
All I can contribute to this discussion (other than my meta-analysis of the way it was being conducted) is to say that I have never been affected by the problem described, and so I have no particular interest in spending time trying to solve it.
It is possible and useful, but letting a type checker use implicit negative information can easily lead to horrible performance and error messages. If you only use positive information, then type inference is monotonic: additional constraints can only make types more specific. This is really important for predictable type inference. The main place that negative information can arise in Haskell is via GADTs (a missing pattern tells you what the GADT indices *cannot* be), and the point of the OutsideIn algorithm is basically to forbid propagating negative info as much as possible.
I guess you won't get significantly faster than [nano-md5](http://hackage.haskell.org/package/nano-md5). (on a side-note, I prefer these type of libraries to have the C source included, it makes so much more painless to install them...) 
I haven't checked the specifics honestly, but it's a good idea. At the moment, all the VMs are comfortably capped well below the total RAM of the host machine - the 3 VMs each get about 4GB, and a few cores at best out of 32GB and 8 cores (quad core Ivy bridge.) The rest of the host RAM is all disk cache. Thanks for bringing this up, though!
Tekmo, perhaps with your knowledge, you might know the following: I want to calculate [Apéry's constant](https://en.wikipedia.org/wiki/Ap%C3%A9ry%27s_constant). The way to do this is by infinitely adding 1 / x ^ 3. I would need to have the computer to keep adding until the decimal point limit is reached and the number efficiently "converges". In other words, adding another 1 / x ^ 3 does not change the number up to the limits of the current decimal. I've come up with: sum [ 1 / x ^ 3 | x &lt;- [1..] ] Is there some function that allows you to continue adding until the result stays the same?
You can write a function that takes a converging infinite list of numbers and returns the number where the next number in the series is "equal" to it: converge :: Eq a =&gt; [a] -&gt; a converge (x:y:xs) = if x == y then x else converge xs Then: apery = converge $ scanl1 (+) [1/x^3 | x &lt;- [1..]]
FWIW, I have an LRUCache implementation that uses a strict hashmap and a doubly linked list implemented with STM. I get 300K lookups/sec with the single threaded runtime, but only 25K lookups/sec with the threaded runtime. So, if you're looking for performance, STM may not be the best way to go. Thankfully, I care more about correctness.
I think PipesBinary looks pretty ugly. I would say that `-` gets translated to `.`, because pipes-binary is only usable together with Pipes. 
If I move something from a package to a 'superpackage of it. I can do so with zero user breakage, so long as I ship an updated version of the subpackage that depends properly on the superpackage and which exports the right contents when used with older versions. I currently ship several shim packages that provide newer functionality for users of older versions of a package, e.g. transformers-shim provides a couple of data types from `transformers-3` for users of platforms that only have `transformers-0.2`. Things like this are a necessary component of how I deal with support windows longer than a couple of packages old without having to litter my code with hundreds or thousands of lines of CPP. Adopting a convention that makes my life more difficult in that regard is almost a non-starter. I have and can continue to scale my existing practices, but as it is package boundaries are the single largest impediment to the scaling of my work. Making them stronger, and making the work of moving code across them harder has enough of a negative effect on my workflow that it isn't a tenable solution for me personally. I can only speak from my experience. If the rest of the ecosystem chose to follow your lead, I might eventually be forced to jump, but I'm not in a hurry to lead a charge that makes my life more difficult in aggregate.
Whereas now they change a cabal file. I maintain many many packages with very long support windows. The kind of breakage you propose isn't acceptable to me. Users can deal with most of the changes I make with a simple patch to a cabal file to deal with overly restrictive upper bounds, but the code itself doesn't become littered with CPP pragmas. I've endured a great deal of pain shuffling things around, so I tend to stick to the existing convention which is actually pretty good at mitigating that pain. You obviously have had different experiences shaping your wants and needs.
I'm not very knowledgeable about alpha beta pruning, but... I think the 'minleq' function could be more compositional for a start. Assuming I'm reading it right, it is supposed to test whether the minimum element of some list is less than a particular value. This is the same as testing whether any element is below that value, which could be done like this: minleq l pot = getAny $ foldMap (Any . (&lt;= pot)) l This requires that the list type is made an instance of the foldable class of course, but I assume you've probably done this anyway. For the rest of it. I'm not quite sure what is going on exactly. I think you might have type errors in the `omit` function for a start. Type signatures for the functions would help here.
Definitely remove/rename `~/.ghc` at a minimum, that's where the package configuration files are stored so removing `~/.ghc` amounts to uninstalling all your user packages. However "packages are causing problems" is not terribly helpful and you would get better advice if you asked a more specific question, including what you are trying to do and the cabal output and such.
I am late to commenting here, but it would be great to hear more about "full control over error messages"
Try using `INLINABLE` instead of `INLINE`/`SPECiALIZE`. The basic idea is that `ghc` by default does not always export a function's source code (in order to keep the size of the `*.hi` interface file smaller). It uses heuristics to decide which functions to export and which ones not to export. For example, if your module has a lot of functions, then it will start dropping some of them from the interface file. If it decides not to export the source code to a function and the function is polymorphic then pay a performance price, as you just observed. The `INLINABLE` pragma basically says "always export the source for this function" so that it can be specialized at the point of usage. `INLINE` also exports the source code, too, but the difference between `INLINABLE` and `INLINE` is that `INLINABLE` does not change ghc's optimization heuristics, whereas `INLINE` does. `INLINE` does not always improve performance, so in the absence of benchmarks I usually pick `INLINABLE` by default. Generally, `INLINABLE` is always better than `SPECIALIZE` since `ghc` can use the exported source code to specialize at the point of usage. This is why I always mark functions in my libraries `INLINABLE` by default and only change them to `INLINE` if benchmarks show an improvement. The only disadvantage to using the `INLINABLE` pragma is increasing the size of the interface files, but nobody seems to care about that.
The idea that the constructors occur on the rhs of pattern matching on the type is interesting, but there is still some conceptual leap that makes this different from normal pattern matching. Take your `Tm` example. If all you told me is that it works like the pattern matching I already know and love, I would find that the first pattern always matches. The second pattern would be essentially dead code.
I think having some sort of classifcation (like Text or Network) is good, but Control and Data should probably be split into more specific categories, like `Monad.`. 
I don't think there is type error, go check the paper here: http://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf 
I would look for areas in which Haskell's strengths are going to lead to naturally better solutions. Two areas come to mind: * **Computational chemistry** (Not my area of speciality, so take this for what it's worth.): I've long thought Haskell's expressively as enabled by EDSLs would be very powerful when it comes to the definition and automated exploration of chemical properties as is done in computational chemistry. I suspect this could be coupled with Haskell's laziness à la [Composing contracts](http://www.lexifi.com/downloads/MLFiPaper.pdf) and [The countdown problem](http://www.cs.nott.ac.uk/~gmh/countdown.pdf) to make the process much more efficient that implementations in other languages. * **Workflow/job coordination**: I used to have to "pipeline" a lot of different tools together to accomplish various tasks. I was never satisfied with any of the tools available for this task (circa 2005): they were just barely better than having to manage the parallelization, and error detection yourself, and adding new tools that the workflow tool's author did not anticipate or was aware of was always a pain. I suspect there's probably still a place for a good workflow/job coordination tool that makes robust and parallel assembly of different bioinformatics tools easy.
Yes. The trick is to generate a lazy list of successive approximations. Each element of the list contains the next approximation and also the difference from the previous approximation: aperys :: [(Double, Double)] aperys = scanl step (1, 1) [2..] where step (a, _) b = let a' = a + 1 / b^3 in (a', a' - a) For example, here are the first 5 approximations and their difference from the previous value: &gt;&gt;&gt; take 5 aperys [(1.0,1.0),(1.125,0.125),(1.162037037037037,3.703703703703698e-2),(1.177662037037037,1.5625e-2),(1.185662037037037,8.000000000000007e-3)] Now let's say that you want to keep approximating until the difference falls below some point. That requires dropping elements as long as the error is above the maximum acceptable error, then taking the first element after that: apery :: Double -&gt; Double apery maxErr = fst . head . dropWhile (\(_, err) -&gt; err &gt; maxErr) $ aperys For example, if I want to compute it to 13 decimal places: &gt;&gt;&gt; apery 0.0000000000001 1.2020569020822955
Yeah. I'm pretty blank on virtualization, but would recon that VMs are prevented from taking down the host os by overloading it with IOPS when they start swapping. Still, stuffing: # set ulimits * hard as 3000000 (or whatever 3-3.5G /1024 turns out to be) * hard data 3000000 * hard rss 3000000 into /etc/security/limits.d/ulimit.conf on every vm couldn't hurt?
Good question. I don't know.
Well, this maximize = max maximize' minimize = min minimize' is definitely not right. The paper has maximize = max . maximize' 
I realise that you have mostly transcribed the code. However, even once you correct the typo 'otherwiese', there is a type error in the function - it contains an infinite type (at least when I try compiling it). I think this might be due to using a different definition of `min` - the standard definition has `min :: Ord a =&gt; a -&gt; a`, whereas it appears to be used as `min :: Ord a =&gt; List a -&gt; a`, although this is presumably partial.
Yes, well this isn't actually Haskell, it's Miranda. .
You can't pattern match on a smart constructor, so incompatibility is guaranteed. If you don't care about that, because pattern matching is not supposed to be used anyway, then only expose the smart constructor in the first place.
&gt; I think the alpha-beta pruning is not as elegant as prune n We have maximize (Node n Nil) = n maximize (Node n sub) = max (map minimize sub) and maximize' (Node n Nil) = Cons n Nil maximize' (Node n l) = mapmin (map minimize' l) That's a slight generalisation. Hardly "less elegant" from my point of view. &gt; isn't the idea of functional programming not to mess up with your old function? If you need generalised functionality, you'll have to generalise your old function. What's the alternative? &gt; Can the alpha-beta pruning be defined in compositional way just like the prune n? I don't see what's non-compositional about this. Could you explain further? &gt; And in omit min nums seems to be calculated inefficiently because each time it has to go through the list. In C or python it would be easy, just give a variable the min value of nums and keep updating it while iterating. Are you talking about this: Cons (min nums) (omit (min nums) rest) I don't know why that wouldn't be `let min' = min nums in Cons min' (omit min' rest)`. That would certainly be more efficient (without having to rely on CSE). 
&gt; The problem with pure FP is: there exists things that are not pure. FP wouldn't be useful if there wasn't.
oh.. thx
I'm going to have some fun and guess the contents of the article before reading it. * It's written by an “outsider”, i.e. someone who hasn't written their first couple packages. * “Insiders” already know the issues with Haskell, so they don't need to make blog posts about it. * Talking about why imperative/OO is “real-world” and why functions don't represent “the machine”. * Talking about how monads are academic wankery/a crutch/kludge so that Haskell can do IO/the reason why Haskell will never take off/etc. * Talking about how most programs are 90% IO and so you don't get to write much pure code anyway. * Talking about how X_other_language (Ruby, Python, JavaScript, Lisp) has closures and so that's a totally fine functional language. * Because you can write functional code in those languages if you want. They're _multi-paradigm!_ * Haskell has `unsafePerformIO` which is _their escape hatch_. See, even Haskellers want arbitrary IO! I ran out of guesses. Now I'll list some criticisms that are real problems that Haskellers generally agree about, and which are probably **not** listed in the article: * Laziness carries a performance trade-off. Intuition about strictness/laziness has a learning curve. * Laziness also makes things like exception handling tricky. * Haskell hasn't got a great debugger or backtraces. * It's a bit of a craft to be able to look at Haskell code and imagine the Core, the STG, the C--, the assembly, that would result from it. * Prelude needs a reshuffling of functions/class hierarchies. * Prelude should probably move away from lazy-by-default IO to some. Lazy module or whatever. * Generally we need to shift away from uses of [Char] for Text. * The record system is sucky. * The record field namespacing problem is sucky. * The module system is limited. * The package system is not ideal (using versions as the only means of determination). * Cabal hell in general. * The type system doesn't have $my_pet_type_system_feature. * GHC is slow at code generating. Compile times are a PITA. * Template Haskell is a good start, but feels incomplete in many aspects. Work is of course being done on _all_ these fronts. But you don't see them mentioned on “outsider” blogs much. Okay, now I'll look at the article.
yes, totally agree
[Same guy from this](http://www.reddit.com/r/haskell/comments/1qg22x/whats_wrong_with_oop_and_fp/), he got vastly criticized on HN too. I could go on and say how full of bullshit he is, but reading the comments on the reddit thread should be enough. &gt; The problem with pure FP is: there exists things that are not pure. There exist things in nature that uses go-to. Should we drop every single well behaved abstraction in order to stick to reality?
I think the code was intended to show elegance and how composing the desired operations functionally and lazily instead of via nested loops opens up opportunities for algorithmic optimization. It's not trying to be the very most efficient or general implementation as those may make the code murkier for small gains. In particular, this code is nice because it uses equational reasoning, association of composition, and functor laws to expose the crucial optimization opportunity in AB-pruned minimax. This also relies heavily on laziness intermingling data structures and control structures. Equational reasoning helped to prove that changing `minimize` and `maximize` to return lists of optimal values instead of the "best seen so far" does not change the result. Then by shifting some functions around in a value-preserving way we expose the `mapmin` function which has sufficient information to judiciously prune the tree. -- ignoring the base cases for a bit more clarity getSub (Node n sub) = sub maximize minimize = = max . (map minimize . getSub) min . (map maximize . getSub) maximize minimize = = max . maximize' min . minimize' maximize' = minimize' = map (min . minimize') . getSub map (max . maximize') . getSub maximize' = minimize' = mapmin (map minimize') . getSub mapmax (map maximize') . getSub -- lots of information available for optimizing this particular search step! mapmin, mapmax :: Ord a =&gt; [[a]] -&gt; [a] That's the key point and the algorithmic gain. Small optimizations like lifting the `min nums` bit into a let binding probably don't dramatically change the speed of the algorithm but would muddy the code somewhat.
Yet surprisingly accurate.
yes, I am talking about that line. and what i want is something like this evaluateAB = maximize . maptree static . pruneAB . prune 8 . gametree instead of evaluateAB = max . maxmize' . maptree static . prune 8 . gametree u talk about generation, I think u are right,some times the only way to code in fp is to add some generation. 
Laziness isn't magic. It has specific semantics that say that `takeWhile (&lt; 10) [1..]` is a finite list and `filter (&lt; 10) [1..]` is not. Haskell will never go around randomly trying to prove number theoretic facts about your program to turn a nonterminating computation into a terminating one. Laziness is just a tool giving you more ways to break down your problem into pieces. http://augustss.blogspot.com/2011/05/more-points-for-lazy-evaluation-in.html is a good blog post on the topic. It doesn't mean you can stop thinking about what your pieces actually do.
Let's see how inappropriate and prejudice my comment was: 1. ☑ It's written by an “outsider”, i.e. someone who hasn't written their first couple packages. (Granted, the guy is no stranger to FP, type systems, etc., and he anticipates this observation in the post. I didn't see any actual Haskell projects written by him, though. Personally, I think that's important.) 2. ☑ Talking about why imperative/OO is “real-world” and why functions don't represent “the machine”. (See the “Side-effects are very real” section.) 3. ☑ Talking about how monads are academic wankery/a crutch/kludge so that Haskell can do IO/the reason why Haskell will never take off/etc. (See the “Monads are design patterns” section.) 4. ☑ Talking about how most programs are 90% IO and so you don't get to write much pure code anyway. (Littered throughout the article.) 5. ☑ Talking about how X_other_language (Ruby, Python, JavaScript, Lisp) has closures and so that's a totally fine functional language. (See the “What is functional programming, really?” section.) 6. ☑ Because you can write functional code in those languages if you want. They're _multi-paradigm!_ (See the “You can write pure functions in any language” section.) 7. ☐ Haskell has `unsafePerformIO` which is _their escape hatch_. See, even Haskellers want arbitrary IO! 6 out of 7 is pretty good. There are a lot of such blog posts. **UPDATE**: https://github.com/yihuang != https://github.com/yinwang0
The intermingling of the maximisation and pruning isn't due to non-compositionality of the implementation (or the language) it's the inherent non-compositionality of alpha-beta pruning itself! You *have to* do the pruning and maximisation at the same time!
Ah, that explains it then. My bad, I had assumed it was Haskell as it looked so much like it.
pruneAB relies on the evaluation of game states so it certainly cannot come before `maptree static`. Furthermore, it requires changing the way that the tree is traversed away from the first `maximize`'s naive method. It may be possible to write it as something like evaluateAB = max . genMaximize abPrune . maptree static . prune 8 . gametree For a `genMaximize` function parameterized on its traversal strategy, but instead of looking to generalize tree traversals, the author use this algorithm as an opportunity to show off equational reasoning. Take a look at my answer elsewhere in this thread for my attempt to show that more clearly. 
I think it would be possible to write a lazy solution where the pruning and maximization are done separately, perhaps the pruning could be written as a hylomorphism which eliminates parts of the game tree letting some other optimization algorithm traverse the smaller tree. But I don't think it'd be as efficient or elegant for this demonstration at all.
&gt; Talking about how most programs are 90% IO and so you don't get to write much pure code anyway. "Side-effects are real". &gt; Talking about how monads are academic wankery/a crutch/kludge so that Haskell can do IO/the reason why Haskell will never take off/etc. "Monads are design patterns". &gt; Because you can write functional code in those languages if you want. They're multi-paradigm! "You can write pure functions in any language". --- Seem like you hit the nail on the head. My favorite part of the article was where he demonstrates that you can write a pure function in C, even though it's an "impure" language, saying that static analysis can easily figure that out. What if I call a closed source library function? Can I be sure that no IO is performed? Can I be sure that it doesn't mutate any of the values I pass into it? I'm not so sure that static analysis can do that. But Haskell's type system can, and with SafeHaskell you can be sure that `unsafePerformIO` is never called.
I feel like I'm in danger of [this](http://xkcd.com/386/).
so maybe mapmax is a generation of maximize ?
I think the problem is the concept "side-effects" like they're these odd things that sort of hang off the side of functions. A much better concept is the concept of "actions", which are things that "do" something, resulting in some value. In fact, I wish *IO* had been named *Action*. Actions have nothing to do with functions, but there are particular ways in which it makes sense to create and combine them. Given you're ultimately trying to write a program that "does" something, your "main" is an action that you construct in this way. The problem with imperative languages such as ML is that they don't have functions, only these weird function/action hybrid things.
/r/programming ***** ^This ^is ^an [^automated ^bot](http://github.com/WinneonSword/LinkFixerBotSnr)^. ^For ^reporting ^problems, ^contact ^/u/WinneonSword.
Click on "other discussions".
I think type-system vs static-analysis is an unnecessary dichotomy.
yihuang (I think that is his github name) has contributed good patches to Haskell projects even if he isn't an original author. He has studied &amp; imiplemented FP compilers &amp; type systems and tried to surround himself with academics that encourage pure FP. To be honest I think your approach here of judging before reading is very arrogant and makes the Haskell community look bad, and it is the only reason you checked box 1. Rather than going for a whole-safe refusal, I read the article, and summarized it as having 2 valid points * mutating state makes certatin types of programs and data structures simpler to write, and in some cases they are very difficult to write purely * the overhead of working with monads does not always justify their benefits If we can admit to the difficulties of Haskell we will get taken a lot more seriously.
It is a shame that /r/haskell is just the programming version of /r/circlejerk because the language itself is quite nice.
&gt; mutating state makes certatin types of programs and data structures simpler to write, and in some cases they are very difficult to write purely This isn't a criticism of purely functional programming. It's a criticism of Haskell or, more generally, purely-functional languages lacking sub-structural typing. 
/r/programmingcirclejerk 
&gt; yihuang (I think that is his github name) has contributed good patches to Haskell projects even if he isn't an original author. He has studied &amp; imiplemented FP compilers &amp; type systems and tried to surround himself with academics that encourage pure FP. I updated my comment. **UPDATE**: They're not the same person. I re-updated my comment. This is the blog author: https://github.com/yinwang0 &gt; To be honest I think your approach here of judging before reading is very arrogant I think the fact people say the same things over and over again is worth pointing out. If I was proven wrong I'd be equally happy. Reading the same old blog post covering the same old tired topics is boring. Pointing that out isn't arrogant. &gt; and makes the Haskell community look bad, and it is the only reason you checked box 1. I did actually go ahead and read the article and hunted around the guy's blog for evidence of actually having used Haskell for something. I don't know everything. &gt; Rather than going for a whole-safe refusal Hmm, not whole-sale refusal. The game was “guess all the common clichés that are going to be re-iterated.” &gt; mutating state makes certatin types of programs and data structures simpler to write, and in some cases they are very difficult to write purely Yeah, fair point and one that any Haskeller would agree with. I wouldn't class stating the obvious amongst a wall of cliché as being notable, but fair enough. &gt; the overhead of working with monads does not always justify their benefits Fair point. 
Are you sure that https://github.com/yihuang and http://yinwang0.wordpress.com/author/yinwang0/ are the same person? It doesn't look like it to me. EDIT: the blog author is https://github.com/yinwang0, and it seems he has no Haskell repos.
In [another recent article](http://yinwang0.wordpress.com/2013/11/09/oop-fp/), he links to [this](https://github.com/yinwang0) github profile.
You're right, they're not: This is the blog author: https://github.com/yinwang0
Good point, I should have clarified that "static analysis" in this case meant "static analysis of C code". Obviously the type system in Haskell is a subset of static analysis, as is any type system. The point I was trying to make is that static analysis in C to determine if a function is pure is much more difficult than in Haskell, probably even being worthy of a research project.
It's a specialization of `map min` which discards some answers if based on what we know about the game tree at this point they aren't going to be our winners.
Great explanation, I can see the rationale for defaulting to INLINEABLE. Works also just fine in my specific case, thanks! A good use case for SPECIALIZE seems to be when you want to get rid of polymorphism overhead, but do not want to pay the compile time / executable size overhead of having the inlined code present in many different modules. 
The author is a bit prone to making absolute statements, but they do make one important point which I agree with: Ricing your program with sweet monad transformer stacks is not always the best solution. There are other ways, and sometimes they are better.
&gt; The problem with imperative languages such as ML is that they don't have functions, only these weird function/action hybrid things. I'm not sure how seriously one can take such extreme positions. Impure languages like Haskell allow non-termination, and yet we accept to give them a pass because, just as pure functions in imperative languages, we can reason about them in a "morally correct" way.
Although he may have experience (as you say), at the same time he doesn't seem (*seem*! I don't talk about facts -- dunno what he's written) to have ever written or worked at... * A larger project in a non-functional language or even OO language. Else he'd know about how bad side effects may be, e.g. in class constructors/destructors etc. There may be some pain coming with purely functional programming but it's less pain than writing OO code. * A multi-threaded/concurrent project. Else he'd love how Erlang avoids inter-process side effects and wouldn't label side effects as the greatest invention since whatnot. Threads in conventional languages are just terrible. I also don't see why he doesn't appreciate at all that a Haskell compiler finds most of the bugs before even running a program. All in all the article is neither logical nor actually worth reading. Sorry!
While you hit almost everything I as a "outsider" have some issue with the "insider"/"outsider" thing ... granted it will take me a long time to get quasi-insider status according to your definition (and maybe even longer to feel like having earnded that badge) - but does this community think that this "elitist" behaviour brings haskell forward? I really love this languague but sometimes it feels so "unwelcoming". Why can't a guy write about some of his felt observations without having the "pack" tear him down? Why not point him to sources/opinions to make him rethink this opinion instead? TL;DR: why not make the rants more constructive (would even help some bystander like me)?
How can I tell cabal sandbox (or cabal-dev) not to use packages installed system-wide? I want it to get everything from scratch.
I think I'd add, in the list of generally accepted problems, that many applications of Haskell tend to lead us to stacks of monad transformers, which perform badly and are fragile and difficult to work with. Do you think I'm wrong about that? At least, I've always assumed it's a commonly acknowledged issue. I don't know a good answer, and I am convinced that it's a reasonable cost of a less sloppy modeling of the problem. But I'm not ready to dismiss it as an issue. It's also very close to what the article is about.
In my current use case I have two levels of LRU caches, first going from disk/network into system memory, second one going from system memory to the GPU. Fortunately, the second level only needs to be accessed by a single thread and services like 99% of the lookups, so using STM for the first one is totally fine. If your cache is read only, you might also get a speedup from using a second thread-local cache. At this point my options for how to proceed would be to either write my fully custom data structure based on a mutable array, or to only update the LRU time on every 10th lookup or so (the retiring order is still perfectly good then). I'm just curious to learn more about pure functional data structures, and would like to stick with a immutable / functional data struture, if possible. 
I agree with [eegreg's comment](http://www.reddit.com/user/eegreg) below. I think you have a fair point about blog posts against pure functional programming being a bit repetitive, but the form of your comment is still not respectful enough of the author. First for commenting on an article without reading it (which is always problematic), second for making a comment on the identity of the author (which can always be interpreted as an attack, even though you did not say anything insulting). If want to make a point about some kind of blog posts, feel free to write something (a blog post?) about them. Not quoting any of them directly would probably be best -- I'm uneasy about thinking of the author of a reasonable blog post reading your comment as a reply to him or her specifically. I also think your characterization of the points of his article is stereotyped and excessive (of course, without reading it...), to the point of being unfair. Some examples below. I don't think the author claims that monads are "an academic wankery" (or any of the possibly less pejorative characterizations you suggest); the "Monads are design patterns" section is actually surprisingly respectful and interesting (as it uses an actual anecdote about actual code). I'm not convinced by your suggestion that the author claims that "programs are 90% IO and you don't get to write pure code anyway". I didn't see that anywhere in the article. I don't think your point about "X_other_language (Ruby, Python, Javascript, Lisp) has closures so that's a fine functional programming language" is fair. He acutally mentions "Lisp, Scheme, ML", and everyone sensible would recognize that at least the latter two are pretty damn fine functional programming languages. True, there is the mention than languages with untyped effects should merit the name "functional", but there is a qualitative difference between claiming that Python is functional and Scheme is functional. Besides, no points about multi-paradigmatic languages were made. Finally, you missed (both before and after reading the article) to mention anything about the "Equational reasoning hype" section, which is rather accurate and interesting -- even though I think the author is wrong about equational reasoning not scaling up to complex and interesting problems. Let me summarize: I think you're mostly wrong about the article and, even if funny, the "commenting before reading" exercise would have been better left as a never-submitted comment in your browser's history.
Yes, Haskell doesn't have functions either! It has these weird function/non-termination-bottom hybrid things. In fact, because of its laziness, it doesn't even have a boolean type.
Indeed the author article has `yinwang0` as a github account, with not significant Haskell software on it. You're right, but I think that's also not a point worth debating. We should handle comments about Haskell indifferently of who they come from -- not saying that you ever said or did otherwise.
Chris, everytime I see such blog posts I think instead to La Fontaine's fable: http://en.wikipedia.org/wiki/The_Fox_and_the_Grapes
Yeah, I think monad stacks are a commonly cited problem. Forgot that one. I personally haven't been bitten by it much so it didn't come to mind. &gt; Monad transformers are in essence a hack to get around monads’ limitations — they are not principled ways of composing monads I think Gabriel Gonzalez would argue quite strongly that monads transformers are principled and reasoned. Whether they're the one true way to compose monads is another story. I'm not sure stacks were what the author was arguing against, but monads in general, “this story tells me something about monads: they make things unnecessarily complicated.”
&gt; imperative languages such as ML When a category, such as "functional programming languages," shrinks to a population of ~1, you have to wonder how useful it is as a concept.
I first phrased my comment this way, mirroring your own (Haskell doesn't have functions, etc.), but I felt that was actually not quite right, in the sense that `f x` is always well-defined in Haskell (and you can reason equationally about it), it is only when you force it that you can observe non-termination. So the problem applies to any Haskell expression (there is no obvious notion of "value"), and is not intrisically related to functions. On the contrary, in strict languages there is a clear notion of (pure) values, and you can indeed pinpoint all effects to the evaluation of a function call. In Haskell that would rather be a property of forcing contexts. (You can add strict datatypes in Haskell, or lazy datatypes in ML, to regain better product/sums; and everyone does that. Looking at the difference between the "good" and the "idiomatic" constructs is an enlightening way to observe the semantic issues at hand, but it cannot in itself pinpoint the problem with, say, non-termination.)
It's worse than that. It doesn't even have a unit type! Personally I think it's OK to say that "ML doesn't have functions" when pretty much every other language has the same function/action hybrid things, and we're comparing to something like Haskell. This is informal and non-rigorous language, and I think it's OK for a forum like this even though it wouldn't be for a mathematical paper. If, on the other hand, *almost every* language was like Haskell, it would be OK for Agda enthusiasts to say "Haskell doesn't have functions, only weird function/non-termination-bottom hybrid things". 
Yeah, that's right. `SPECIALIZE` can lower the size if you can anticipate the specializations in advance.
Heh, nothing stopping you from breaking out a disassembler and manually piecing together functions to determine that they are, in fact, "pure" from an outside perspective.
I disagree with the premise that monad transformers are intrinsically slow. See [my comment](http://www.reddit.com/r/haskell/comments/1qrp8l/improving_performance_of_complex_monad/cdfrzna) in another thread which discusses the nuances of this issue. Also, monad transformers are composable in a rigorous sense. Their composition operator [is `ComposeT`](https://github.com/Gabriel439/Haskell-MMorph-Library/blob/master/Control/Monad/Trans/Compose.hs#L23) and their identity is `IdentityT`. I can also answer most criticisms regarding the use of `lift`.
:-) ! My research is based, in part, on pointedly ignoring this dichotomy!
&gt; It's worse than that. It doesn't even have a unit type! Is that actually true? Haskell 2010 allows this: data Empty Empty is a terminal object in **Hask** since there's no way of distinguishing bottoms, i.e. the morphisms (functions) of type "a -&gt; Empty" f _ = error "f" g _ = error "g" are the same. At least, this is a reasonable way of defining **Hask**. So it's reasonable to consider Empty as the unit type.
Ha, interesting. Looks like you're right about that!
What's your research?
&gt; Should we drop every single well behaved abstraction in order to stick to reality? No, you should deny reality to preserve every single well-behaved abstraction.
&gt; Personally I think it's OK to say that "ML doesn't have functions" when pretty much every other language has the same function/action hybrid things, and we're comparing to something like Haskell. This is informal and non-rigorous language, and I think it's OK for a forum like this even though it wouldn't be for a mathematical paper. That's an interesting distinction. I'm personally not fond of such sweeping statements -- except as a joke when I'm among people who share a common understanding of how dubious the statement is. They have the look and feel of scientific truth (in fact they are true in a very precise sense), but as a tool of informal argumentation they overlook a large number of nuances that a scientific argument would be forced to acknowledge. I think that if we decide to advance our communities using scientific arguments (and I think that's a good way to do it), we should use all their features (being zealous in citing related work, highlighting the limitations of the promoted approaches, etc.), not only the "speaking demonstrably true sentences" part.
Always a pleasure to read your comments! I've struggled with inductive families for exactly this reason. They seemed 100% magic to me until I figured out that it is a kind of function inversion / logic programming on patterns that are on the right hand side. Even now they still seem quite magic. There is somehow some kind of nominative typing going on with the constructor names. Another magical bit is that free variables are turned into "implicit existentials" where the value that is hidden inside the existential implicitly comes out again when you pattern match. This is probably in some way dual to what happens with free variables in types elsewhere that are turned into functions with an implicit argument, but I can't quite put my finger on it. The kind of pattern matching you can do is quite general, but you can't invert arbitrary functions. What are the limits on what can be done? (i.e. what are the limits on what you use as the patterns? in the examples you gave there are only data constructors `Zero`, `Succ n`, `(s :-&gt; t)`, but apparently you can put general expressions there). Is there an article that dispels the magic by desugaring it into into ordinary dependent types? Another point that I have trouble understanding is when this pattern matching is done on types. `Vec Zero` and `Vec (Suc n)` is fine but what about: data Expr a where ConstInt :: Int -&gt; Expr Int ConstBool :: Bool -&gt; Expr Bool Add :: Expr Int -&gt; Expr Int -&gt; Expr Int Eq :: Expr Int -&gt; Expr Int -&gt; Expr Bool Now the pattern matching happens on types. Aren't types supposed to be opaque, so isn't this changing the whole notion of what it means to be a type? Somehow `Expr Int = ConstInt Int | Add (Expr Int) (Expr Int)` and `Expr Bool = ConstBool Bool | Eq (Expr Int) (Expr Int)`.
I'm going to try my best to steer away from the unproductive comments that this thread it doomed to have in favor of talking about what *actually* sucks about Haskell, although mostly I'm just going over your list and pushing my useless two cents on what I think. If I don't mention one of your points, I agree with it wholeheartedly. &gt; Laziness also makes things like exception handling tricky. True, but I would restate this issue as "Haskell supports throwing (a form of) exceptions in pure expressions." &gt; It's a bit of a craft to be able to look at Haskell code and imagine the Core, the STG, the C++, the assembly, that would result from it. Did you mean C--? &gt; The type system doesn't have $my_pet_type_system_feature. I'm not sure I understand what you mean here. I will also add the following criticisms, but I do not expect all of them to be embraced by other Haskellers: * There are compiler-/platform-dependent pure values in base. For many practical definitions of purity, this doesn't fit. * The prelude is too monomorphic. Generality and parametricity are underutilized. * The standard library has too many partial functions, and there is no way to statically enforce totality. * Haskell is generally difficult to learn and tempting to dismiss (as evidenced by this submission) if you are used to other languages. * The type system is quite complicated, at least when including common extensions, and GHC error messages are sometimes difficult to understand. * General criticisms of GHC, like the stop-the-world GC, which means that unless nearly everything is in unboxed arrays, you can't have both a large heap and low latency.
He makes several good points. I love Haskell, but it seems to take to some challenges beautifully, and contorts itself and sacrifices elegance to meet others. I sometimes wonder if the problem isn't that it is or isn't a pure world, but that it's not a describable world--there is no notion with which to ever truly capture it. Maybe no programming language will ever capture everything it needs to capture. We'll just be patchworking disparate solutions together as we always have until the end of time. I for one look forward to the Spring Haskell-on-Rails .NET framework...hahah..oh...
Is the [strict RWST](http://hackage.haskell.org/package/mtl-1.1.0.2/docs/Control-Monad-RWS-Strict.html) using a broken `WriterT`?
Oops, yeah, I wrote Cmm first and then corrected it to C++. Force of habbit. &gt; The type system doesn't have $my_pet_type_system_feature. Well, e.g. some people might want type-level nats with arithmetic or type-level lambdas, or built-in row polymorphism, impredicative types, better singletons, etc. Depends on the user. 
Well, it's still static, it all happens at type-check-time.
Yes. So both the "lazy" and "strict" `WriterT`s in transformers are broken. I put the terms in quotes because neither of them is truly strict. The correct implementation is linked to [by this comment](http://www.reddit.com/r/haskell/comments/1qrp8l/improving_performance_of_complex_monad/cdfv6pn). The lazy RWST uses the lazy WriterT and the strict RWST uses the (not really) "strict" WriterT, so both RWSTs are broken.
http://pl.cs.jhu.edu/big-bang/ (and a potentially illuminating LtU [discussion](http://lambda-the-ultimate.org/node/4808)) We're constructing a language from scratch intended to be both strongly typed and a scripting language, and the way we're typing it is basically a flow analysis. Future work involves leveraging this for an effect system (e.g. make it possible to require that a parameter to a function never read or write to or from any ref which it did not, itself, allocate).
Effect systems! Any opinions on Disciple?
Not any qualified ones. It seems like a good idea, and I'd like to see where it goes. Anything which pushes in the direction of bringing strong types to systems programming is IMO important research.
I don't quite understand. Negative type information still makes the type more specific, does it not? Because you know more about the type than you did before: you know some things that it is not.
It would be great to include dash as valid character in `import` statements and show syntax errors when trying to use something like import qualified data-lens.Lazy because dash must not be used outside import clauses.
This is made unfeasible by infix operators without spaces, e.g. (a-b).
Is it on hackage? Is there a repo I could check out? That 12x slowdown looks suspicious.
I don't agree that this is some kind of big problem related to setting up a complex web framework. It's plain old dynamic linking! If cabal/GHC/HP could do exactly what they already do today, but with dynamically linked libraries instead, the issue would be solved. It seems GHC/Cabal already have all the required functionality in there, but a few last issues prevent it all from coming together smoothly. Too bad, let's hope once there's a HP based on 7.8 we have working setup out-of-the-box! The FPComplete IDE is neat, but I don't actually do any web development, just consuming data from a few REST APIs here. Plus, I'm kinda attached to Vim + Terminal ;-)
Wish I knew, so far I have just been doing a --reinstall --force-reinstall for HP packages I want to override in the sandbox.
It seems that it would benefit all users if the internals of the RWST implementations were changed. Has there been any history on any efforts for this? The gmane posts are from a year ago.. 
Unfortunately, you are going about it incorrectly. But don't fear! This gives you a chance to learn something new about Haskell. If you were looking for the analog of class inheritance in Haskell, you really want typeclasses, not algebraic data types (what `Base` is in your code). An example would be class Base a where step :: a -&gt; Int -&gt; a display :: a -&gt; String data Foo = Foo Int data Bar = Bar String instance Base Foo where step (Foo x) delta = Foo $ x + delta display (Foo x) = "Foo: " ++ show x instance Base Bar where step (Bar s) delta = Bar $ s ++ show delta display (Bar s) = "Bar: " ++ s stepAll :: Base a =&gt; [a] -&gt; Int -&gt; [a] stepAll bs i = map (flip step i) bs displayAll :: Base a =&gt; [a] -&gt; IO () displayAll bs = putStrLn $ concat $ intersperse "\n" $ map display bs This won't work exactly like it's OOP counterpart, since in C++ you can have a list of `Base` that can be composed of `Foo`s, `Bar`s, or both, but in Haskell, the signature `Base a =&gt; [a]` means that `a` has to implement `Base`, and all elements of the list have to be the same `a`, so you can't have the list `[Foo 1, Bar "2"]`.
This is a question for those that are more experienced: What is the performance overhead? I recently saw something like a [state machine]( https://raw.github.com/NicolasT/kontiki/master/src/Network/Kontiki/Raft.hs) when reading a Haskell implementation of Raft. How does this compare to what is done here? What implications does having your own data structure with functions in it have when it comes to testing, correctness, and so on?
If you want homogeneous lists, you'll want to look into [existential types](http://www.haskell.org/haskellwiki/Existential_type). Here's some code excerpted from the link: class Shape_ a where perimeter :: a -&gt; Double area :: a -&gt; Double data Shape = forall a. Shape_ a =&gt; Shape a type Radius = Double type Side = Double data Circle = Circle Radius data Rectangle = Rectangle Side Side data Square = Square Side instance Shape_ Circle where perimeter (Circle r) = 2 * pi * r area (Circle r) = pi * r * r instance Shape_ Rectangle where perimeter (Rectangle x y) = 2*(x + y) area (Rectangle x y) = x * y instance Shape_ Square where perimeter (Square s) = 4*s area (Square s) = s*s instance Shape_ Shape where perimeter (Shape shape) = perimeter shape area (Shape shape) = area shape -- -- Smart constructor -- circle :: Radius -&gt; Shape circle r = Shape (Circle r) rectangle :: Side -&gt; Side -&gt; Shape rectangle x y = Shape (Rectangle x y) square :: Side -&gt; Shape square s = Shape (Square s) shapes :: [Shape] shapes = [circle 2.4, rectangle 3.1 4.4, square 2.1]
I was the one who wrote those posts. The main barrier is that Ross Paterson is very conservative about changing `transformers`. I've been contemplating releasing a `transformers-plus` package that would contain: * The fixed `WriterT` and `RWST` * `ListT` done right * `FreeT` * `Codensity` * `EitherT` However, I have to coordinate that with Edward because he is the current maintainer of `FreeT` and `EitherT`.
Your approach makes it difficult to have a heterogenous list of foos and bars. The approach outlined by the OP is better in that regard. See [Haskell Antipattern: Existential Typeclass](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/).
In OOP you can use the visitor pattern for "*Rarely gets new types and often new functions*" situations. In Haskell you can actually **solve** the expression problem (*easily add types and easily add functions*) using type classes. [relevant lecture](http://userpages.uni-koblenz.de/~laemmel/TheEagle/movies/xproblem1.mov).
Exactly. It'd be a custom `IntMap`, `HashMap` or `Map`.
Oh, thanks. This looks very interesting. I will definitely read it. In my article I tried to come up with a solution that also works in [Elm](http://elm-lang.org/). (I just added this remark to the article.)
The issue here is that you can't have a list of mixed types. Even with type classes, the problem isn't solved. For example, you can't have a list of `(Num a) =&gt; [a]` where `a` is both a Double and an Int even though they both are instances of Num.
Wow, that sounds interesting (*downloading*). Thanks. :) I also added a remark to the visitor pattern to the article.
The [typed tagless final] (http://okmij.org/ftp/tagless-final/course/index.html#lecture) approach is pretty awesome for dealing with the expression problem. Your second solution Haskell solution appears to be a final encoding of the problem, so those notes might be of interest. The [homework for week 5](http://www.seas.upenn.edu/~cis194/lectures.html) in Brent Yorgey's lecture notes point in that direction, if you're looking for a smaller introduction.
Yes, heterogenous lists are impossible with this approach, and this can be remedied with existential types, but I tend to steer clear of that kind of solution if possible.
Thank you for this bit of lazy iteration wizardry. 
Why bother with an existential typeclass there instead of just a record of values?
http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/
You're welcome!
I wonder if a tool like Hlint could provide reminders in useful places to help avoid common space leaks. Eg. suggesting strictness on accumulators in a tail recursive functions. I've only been putzing with Haskell for about 6 months, and just started using Hlint today, so take this suggestion with a grain of salt. 
I think I understand your other arguments, but could you explain the problem with "too many partial functions"?
I'm talking about functions like head which are not defined for some inputs. 
I personally don't experience any problems with fragility with large monad transformer stacks: * Glue your multiple states and environments together with "classy" lenses. * Glue your writer outputs together with prisms the same way. * Never pick a monad transformer until the actual site at the end that runs everything and use the monad transformer classes to do the rest. * Don't typically throw `MaybeT` or `EitherT` into the stack or you'll usually be forced to lift. Done. `mtl` speed is one of those things where a few bad apples (ಠ\_ಠ `Writer`) gives the whole thing a bad rap. Could `transformers` stand to have a few strategic `INLINE` pragmas sprinkled about? Perhaps. But if you follow the advice above, you aren't locked into any particular _instance_ of the transformer classes and so you can roll your own composite über-monad once you profile and determine it really is the culprit, not just the scapegoat.
Wait, isn't this OO in desguise?
I get the feeling that this would impact negatively on the system package documentation index, which due to the current package naming convention, is a very convenient way to explore packages on your system as a tree. IMO: The suggested module names aren't pretty and don't convey similar properties/namespaces at a glance because of the lack of dot alignment.
Existentials tend to be an antipattern, and sum types close you off to extension.
You might want to think that one through. Or take a spaceship back to your twin Earth in the center of the universe.
The sum type solution is inflexible but what other solution could you use assuming you don't access to the superclass and don't want to use something even worse like Dynamic?
no one would doubt that Agda, Coq, or Clean are "functional programming languages." It is just that haskell is much more popular. 
The expression problem feels to me more and more to be a consequence of the data/codata divide. If you treat an ADT as data it's easy to define new functions which take it as an argument and destruct it inductively; you build algebras like `f a -&gt; a` for the various constructors enumerated by `f`. data Base = Foo Int | Bar String step :: Base -&gt; Int -&gt; Base step (Foo x) i = Foo (x + i) step (Bar x) i = Bar (x ++ show i) display :: Base -&gt; String display (Foo x) = show x display (Bar x) = x -- then twoTimes :: Base -&gt; Base twoTimes (Foo x) = Foo (x * 2) twoTimes (Bar x) = Bar (x ++ x) If you treat an ADT as codata then it's easy to define new implementations as they just need to be coinductively observed over some set of patterns; you build coalgebras like `a -&gt; f a` and just need to define the fixed set of observations. {-# LANGUAGE ExistentialQuantification #-} data Base = forall x . Base x (x -&gt; Int -&gt; x) (x -&gt; String) step :: Base -&gt; Int -&gt; Base step (Base x s f) i = Base (s x i) s f display :: Base -&gt; String display (Base x _ f) = f x bar :: String -&gt; Base bar s0 = Base s0 (\s int -&gt; s ++ show int) id foo :: Int -&gt; Base foo i0 = Base i0 (\i int -&gt; i + int) show -- then baz :: () -&gt; Base baz () = Base () const show
independent of purity, ML lacks functions/exponentials in the same sense the Haskell lacks sums. Haskell lacks both products and functions though because of `seq`, so this is not really a good point for Haskell.
Yup. Records full of functions are awesome.
I have two general questions if anyone is willing to answer them: 1. Something makes me feel uneasy about having to construct entirely new `Foo` and `Bar` each time we want to `step`. It makes the implementation feel somewhat convoluted because `step` seems to have to worry about `display`. What happens when we want to add more functionality to `Base`? I'm not really sure how to fix this, however, because I think the point is that `step` has to close over all of the state for `Base` to work without existentials. Is there something that I am missing here? 2. This vaguely reminds me of [Scrap your Boilerplate](http://research.microsoft.com/en-us/um/people/simonpj/papers/hmap/), but I am having a hard time making the actual connection. Is there any?
When I learned ADTs I thought the concept and syntax was incredibly simple and natural. Honestly, I was never confused about ADTs until I read that SO question. I know the code is wrong but it looks logical enough, so it made my head spin for a minute. Maybe I'm unusual though. I also despise the Mk thing
Well, because of laziness+nontermination, at least.
One advantage you have is that you can extend the instances with specialized functions. For instance, you can add `diameter (Circle r) = 2 * r`. But you can't "downcast" if you get the value from a `[Shape]`; there's no pattern matching at the type level (if you try you'll get `Could not deduce (a ~ Circle) from the context (Shape_ a)`). At least you can't do it with regular Haskell, maybe with some extension?
Wait, you're the pipes guy? I recognize "`ListT` done right" statement. 
That's why he said he wishes Haskell would require spaces around operators.
Yeah, that's me :) However, the phrase "ListT done right" precedes `pipes`. It originates [from this](http://www.haskell.org/haskellwiki/ListT_done_right).
&gt; If you want homogeneous lists Heterogeneous lists, you mean?
&lt;/troll&gt; Three words: component entity system Components are just records (structs in C) Entities are just sets of components. Systems are just entities + functions. &lt;troll&gt; Inheritence is the lamest thing invented ever. 
Thanks. By happenstance, I saw the [Haskell wiki page on partial functions](http://www.haskell.org/haskellwiki/Partial_functions) just after I posted that. My original interpretation was that you were referring to partial application. Now I understand.
&lt;/troll&gt;... &lt;troll&gt;. Reverse troll? Am I missing something? 
If you want to be really hardass, even Agda doesn't have a function type. For instance, there is no Agda value of type "Nat → Nat" that represents the busy beaver function. Instead it has a "computable function" type.
While I'm not entirely sure I deserve an opinion (I'm incredibly new to Haskell, and quite new to programming in general), as a newbie, I don't really agree with the article. I personally think it's quite nice and elegant to have a bunch of pure functions, and keep things like IO and state encapsulated in a few monadic functions. Every language makes certain things trivial, and certain things difficult, there's always a trade-off. In Haskell operating on lists is trivial, any sort of data analysis is trivial, compiling correct programs is trivial. It makes certain things difficult, but IMO Haskell presents a nice set of compromises. It certainly hasn't stopped people from building cool things with Haskell. And while the case could possibly be made for Haskell being 'too functional' or 'too pure' (I wouldn''t make that case, but many would), it does seem as though every new language or update to a language has more functional features than those before it (Clojure, Scala, Ceylon, Rust, Ruby, Pure, etc...). In my newbie-adventures (doing some Hackerrank challenges, doing some data-analysis type things, and building a small game for fun) I've found with Haskell I use state and impure functions alot less than I would in say, Ruby or Javascript. And it's not a bad thing. 
Yep.
their usual mode is trolling, so that was a partly not trolling post.
Ah, thanks.
It took me a while to understand how this scales up. The part that I kept struggling with was updating state, so adding new "mutators" meant you had to look at everything that might use that part of the state and update it as well. But if you update all fields every time, no such problem. I've included (in mostly Haskell syntax) [how I think that it scales](http://lpaste.net/95842). Please feel free to correct me if this is not what you had in mind. * Or if it does not work! Thanks for a thought-provoking post! EDIT: * EDIT2: Typos
E.g. classes give you overridable default method definitions. [Example](http://hackage.haskell.org/package/haskell-packages-0.2.3.1/docs/src/Distribution-HaskellSuite-Compiler.html#Is)
I've had success with macports gtk, and normal hackage gtk2hs edit: I might have had to install some X11 thing, but I think that was already on there for some other reason.
&gt; I'd basically have to implement my own Map data structure as none of the standard Haskell containers allows amending their internal tree data structures, correct? Well, you could use a Monoid tree.
So the idea is to make things harder to find? I know right now that if I'm looking for language parsers or pretty-printers there's a good chance they're in `Language.*`. Similarly with text processing in `Text.*` and so on. This alternative means I have to know the "brand" of the package rather than its purpose in order to browse it and install it, which is entirely the wrong way round.
It's not open-source, but watch this space.
Although it should be noted that with floating point numbers, you probably don't want an exact comparison but rather checking whether the number is within some acceptable epsilon.
Yes, both the record of functions pattern and the existential typeclass antipattern are OO in disguise. Sometimes OO is the best approach to a problem.
As Tekmo has pointed out, Haskell's laziness is great for numeric methods, see this example for instance: http://stackoverflow.com/questions/19720457/recursive-haskell-newtons-method-why-doesnt-this-converge/19730017#19730017. There are many more, Legendre polynomials spring to mind
".cabal"-files have a property `category`, in which the author lists the tags, which this package may be found by on the following page: http://hackage.haskell.org/packages/ . I'd say that what's entirely wrong is using namespacing for that instead, that is besides being redundant in the light of what I just mentioned.
I think that whether it's "pretty" is only determined by habits. So I'd say "unusual", yes. But then it's only a question of getting used to. For instance, the current Haskell's practice is what seems unusual to me. E.g., in the whole JVM languages family, the proposed practice is a de facto standard. Sometimes they extend it to `domain.package...` scheme but in our case that would be redundant, since we only have a single repository (Hackage) and the package names are guaranteed to be unique already.
You're right to raise these issues. What's a pattern? Which information is around to be matched upon when? What's the difference in the situations of dependently typed languages (which usually avoid typecase) and Haskell (where types and other type level things are carefully kept first-order)? I guess the first thing to observe is that existentials and equality (whatever that is) let you encode a lot. My examples use pattern matching just for those situations where I'm drawing distinctions based on datatype constructors, but we could do data Vec :: (n :: Nat)(a :: *) where Vec n a :&gt; (n ~ Zero) =&gt; Nil | (n ~ Suc m) =&gt; Cons (x :: a) (xs :: Vec m a) which is just a noisier (and older) notation for what Haskell does at the moment. When you write the type of a constructor in a GADT, that's not the type the constructor gets. Instead, it gets the type after this "Henry Ford" translation (e.g. any length you like as long as it's Zero). What's good about doing it this way is that it keeps separate the ideas of what it means to be a datatype and what it means to be equal. Agda 2, Coq, etc, let you use datatypes to define a rather intensional notion of equality, which we're then stuck with. Whilst Ford equations allow you to restrict or remove constructors, based on index values (a bit like greying out a menu option that's currently unavailable), the pattern matching version computes the whole structure of the type from the index (so that the list of menu options, not just their greyness, varies with circumstances). I would like both to be available and to know which I am using when. In that way, I can use pattern matching to make the values of my type represent just the extra information that isn't already computable from the indices. The patterns you can match correspond to the distinctions you can compute, so datatype constructors are good. In "The View from the Left", James McKinna and I show that you can establish the computability of more exciting patterns exactly by reducing them to constructor distinctions, so data Trich (m :: Nat)(n :: Nat) where Trich m n :&gt; (n ~ m + Suc k) =&gt; Lt | (n ~ m) =&gt; Eq | (m ~ n + Suc k) =&gt; Gt expresses the analysis of two numbers in terms of +, and we can show admit that analysis by implementing trich :: (m :: Nat) -&gt; (n :: Nat) -&gt; Trich m n There is no fancy function inversion going on. The funny +-patterns for m and n arise by an ordinary constructor analysis of a value in Trich m n. But now we arrive at the more subtle question: *when* does the pattern matching happen? If we are using pattern matching to determine the constructor choice of a datatype, then that's a type level computation, which has access to a bunch of stuff that will be erased at run time. Type checking must also ensure that any distinctions which must be made at run time depend only on run time information: if a datatype has been constructed in a way which exploits information from its indices, that index information may need to survive type erasure. Haskell's kind * is essentially an open first-order datatype for type-level-only information. If we coded your `Expr` so that the same runtime bit position was used to separate `ConstInt` from `Add` as was used for `ConstBool` vs `Eq`, we might need to ensure that an existentially quantified `Expr` carried a bit to distinguish `Int` from `Bool`. The usual method is to build some `TypeRep :: * -&gt; *`, essentially a singleton copy of the relevant chunk of *. Given that Haskell's * really is a first-order open datatype, we might do well to consider cutting out the singleton encoding and admitting types at run time. I can already hear the bleating about type erasure. My point, however, is that the type-level-only-versus-run-time distinction no longer aligns with the type-value distinction, so we should stop trying to figure out what to erase on the basis of what sort of stuff we're dealing with. De facto, we already have type-level values and run-time types. Which brings me to my current campaign which has two complementary components: to persuade Haskell to add a non-parametric quantifier, Pi, which is distinct from forall, whose lambdas and applications exist at run-time, and to equip dependently typed programming with a usefully parametric forall, distinct from Pi, expressing run-time irrelevance (as opposed to typechecking irrelevance, as in the current Agda). It's time to split where-and-when from what.
I think you only need to delete ~/.ghc. This saves you from downloading all the package sources again. 
Beware: the following might be a bit off topic: I speak of OCaml, not Haskell. In OCaml we have support for open sum types through [polymorpic variants](http://caml.inria.fr/pub/docs/manual-ocaml-4.00/manual006.html#toc36). I don't think there's such a think in Haskell, but you might find the approach interesting nonetheless. You'll probably want to have a look at [this paper](http://www.math.nagoya-u.ac.jp/~garrigue/papers/fose2000.html) which show (by example) how they work and how they tackle the expression problem. This is probably a more digest lecture than OCaml's manual. Cheers. PS: On the same topic, [gasche](http://www.reddit.com/user/gasche) wrote (a few years ago) a gentle introduction to the expression problem as well as an overview of the available "solutions" in OCaml. He never finished the series of articles, but the last would have talked (I believe) about polymorphic variants, the previous link can probably be used as a replacement. The series is in french (sorry) but if people are interested I think he'll see no objection to having it translated. * [Article 1](http://blog.huoc.org/expression-problem-1.html) * [Article 2](http://blog.huoc.org/expression-problem-2.html) PS2: Polymorphic variants are not the only way to have "open sum types". See for example: http://www.cl.cam.ac.uk/projects/ocamllabs/tasks/compiler.html#Open types
And there are plenty of them in various unix kernels.
I am surprised that nobody mention the old "haskell overlooked object system" from Oleg et al. It solves expression problem using heterogeneous lists and mimic the C++ behaviour, even better, with better type safety: http://arxiv.org/abs/cs/0509027 The only problem IMHO is the syntax, since there is no syntactic sugar for such a powerful object oriented abstraction. Or I´m talking about a different problem?. It may be since I passed trough this thread too fast
&gt; [relevant lecture](http://userpages.uni-koblenz.de/~laemmel/TheEagle/movies/xproblem1.mov). Slides and more resources here: [Exploring typed language design in Haskell](http://userpages.uni-koblenz.de/~laemmel/TheEagle) 
Great write up! I stumbled on this problem too. My solution was existential types but it seemed inelegant, and turned me off from Haskell for larger projects somewhat. I like this solution much better. Thanks for sharing. 
That isn't what is used when I type :browse into GHCi. 
OO is just a sometimes useful technique, not something to base your entire paradigm/language around :) Also, some parts of OO (implementation inheritance) are IME never useful, as alternate solutions are always better. But maybe I've not seen the right example yet.
&gt; I'm not really sure how to fix this, however, because I think the point is that step has to close over all of the state for Base to work without existentials. Is there something that I am missing here? Well, you can close all the data for `Foo` in a data record `FooData`. Then you can use punning and wildcards or lenses to modify and access that data more easily. 
If you want to keep the stepper away from the display then you need to use the pattern-matching version instead of the OOP version. Its basically the Expression Problem stuff all over again. At this point, the version with the "dispatcher" is essentially as good as it gets. If you really want, you can use an interface similar to the fold functions to abstract it just a bit: make_stepper :: (Int-&gt;Int-&gt;Int) -&gt; (String-&gt;Int-&gt;String) -&gt; Base -&gt; Int -&gt; Base make_stepper onFoo onBar = step where step (Foo intVal) = Foo $ onFoo intVal delta step (Bar strVal) = Bar $ onBar strVal delta This has the advantage that the individual stepper functions don't have to know about each other and that you only need to bring them together under the `Base` datatype at the end. However, its still just as hard to extend `Base` with new cases, if you want that to be easier 2. I think SYB is more about when you have tons of separate datatypes and complex, boilerplaty traversals: data Comp = Comp [Department] data Dept = Dept Person [Person] --manager, employees data Person = Person Name Int --name salary -- get salaries -- getXSals:: X -&gt; [Int] getCompSals (Comp ds) = concatMap getDeptSals ds getDeptSals (Dept p es) = getPersonSals ++ concatMap getPersonSals es getPersonSals (Person name sal) = [sal] Lenses (datatype accdessors/mutators) might be closer to what you want but they still don't solve the issues related to the expression problem. 
Nah, `diameter (Shape (Circle r)) = 2 * r` is what gives that error. Since Shape is existential over Shape_, the compiler can't deduce that it may contain a Circle. Maybe you can force it, I don't know. 
I don't think you can. GHC itself comes with some packages you can not install manually via cabal so you can not use only packages in the sandbox.
&gt; [W]e only have a single repository (Hackage) and the package names are guaranteed to be unique already. Cabal and hackage already provide a lot of the necessary package managent features (of course, there's still some gaps). Therefore, as Haskell's industry utilization increases, a single Hackage repository is likely to be less and less the case. I setup the private Hackage server that my employer uses to manage our proprietary Haskell libraries, and using Hackage for this purpose works quite well. I think we must consider the reality of multiple Hackage servers as we consider how to evolve the Haskell ecosystem. For example, perhaps it's time to consider some form of package aliasing scheme.
Yes, you are right of course. I now edited the article to make this more clear.
I'm fine with those packages, but during development I only want to specify the packages without a version bound and have cabal install the newest version by default. It skips old packages I have lying around from the platform etc. 
I _think_ you could use singletons to get a something like a static version of Dynamic; it's more extensible than the sum type approach, but it tends to be worth only theoretically. Typically, when considering the expression problem, we assume you're the one designing the initial interface. In order to be a solution to the expression problem, though, you need to support both new constructors and new functions, so typeclasses (by default) fail to solve it (so do OO classes, though). The two common solutions to the expression problem in Haskell are [DTalC](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf) and [Finally Tagless](http://okmij.org/ftp/tagless-final/course/index.html). The first encodes open unions using recursive types, and the second inverts the normal use of typeclasses, using them to define constructors of your datatype.
Where I ran into the expression problem was trying to write a nanopass compiler (okay, a nanopass translator/desugarer, but it's the same issue). I want each pass to have preconditions and postconditions (say, one pass converts `if`s to `case`s, and I want to know that the output has no `if`s). This requires enough machinery in Haskell that it's not worth the effort. &gt; [I]n practice, I don't think this is a real bottleneck. While I'm leaning toward agreeing with you, it's hard to know how much this inextensibility constrains programming style in unnecessary ways; most of the time you don't need it, but if you think you do you might come back to Python, which doesn't impose this restriction on you. &gt; I do think it would be interesting to have a type system with anonymous union types with the obvious subtyping relationship. MLPolyR is such an experiment. The [research language](http://pl.cs.jhu.edu/big-bang/) I'm working on is another, significantly less mature one.
I misunderstood what you were trying to do. The "pattern-matching on the type-level" you were looking for is basically Typeable, isn't it?
Isn't that what Haskell always does if you just mutate part of data structure (which you can't actually do because purity)? You'll have the same problem if you're using existentials. Just to be clear, what I was suggesting is data FooData = FooData {fooInt :: Int, uselessNewField :: String} foo :: FooData -&gt; Base foo fd = Base (stepFoo fd) (displayFoo fd) -- uselessNewField gets copied implicitly. stepFoo :: FooData -&gt; Int -&gt; Base stepFoo (FooData {fooInt,..}) delta = foo $ FooData {fooInt = fooInt + delta, ..} displayFoo :: FooData -&gt; String displayFoo (FooData {fooInt,..}) = show fooInt Or something prettier like that with lenses. You can add fields to FooData (e.g. for new functions that you add to Base) without having to mind the functions that don't use the new fields.
yes! indeed. The paper is full of gems understanding oop in terms of haskell data type. I followed the approach of sec 3.5 for fficxx. In the case, I do not need an implementation for haskell data type with record but I can use C++ opaque foreign pointer as a haskell datatype. Although this approach is regarded as rather anti-pattern in the discussion here, it is still the most straightforward clean way to implement multiple inheritance using typeclass. In my case, boilerplate was dealt with automatic code generation. Of course, that method was not a main theme of the paper and the main and other methods discussed in the paper are very illuminating. 
That's because the "Worse is Better" approach is always more popular.
Oh that works. I haven't looked into Typeable before. It feels a bit dirty, though. diameter :: Shape -&gt; Maybe Double diameter (Shape a) = fmap (\ (Circle r) -&gt; 2 * r) (cast a) and Typeable constraints everywhere does the trick. Still no pattern matching but I guess it works for selective application (e.g. `map diameter shapes = [Just 4.8,Nothing,Nothing]`). Edit: *sigh* anonymous sum types would be great. 
Building on this answer, you can build an exact LRU with one trie if you take a regular trie and annotate each node with the least 'access time' of any of its descendants. This allows you to do an insertion of a new element and the removal of the oldest in two root to leaf to root traversals. Or a lookup and access time update in one root to leaf to root traversal.
Both are amazing speakers. A talk by either of them is practically guaranteed to contain several great ideas &amp; perspectives.
Not on reddit. /r/haskell : &gt; 13,000 redditors. /r/scala : &lt; 5,000 redditors. 
I should probably mention that this is the result of a long discussion with Chris Done in #haskell in response to [his article solving the Twitter waterflow problem with `loeb`](http://chrisdone.com/posts/twitter-problem-loeb). Thanks for that!
Sorry I'm a bit late to the party. I have a working set up for running a hackage mirror as a few docker instances working together that encapsulates some of the (rather under documented) steps you need to take to get hackage set up internally. This mirror is only available internally, but if there's any interest I could tidy it up with some documentation and a how to guide?
You're probably right; I only have cursory knowledge of row types. We're attempting it, at least if I understand you correctly. In the following Python-esque pseudocode with braces denoting records rather than dictionaries if someCondition: x = {foo:4} else: x = {bar:4} `x` has an anonymous union type `{foo:int} | {bar:int}`, right?
&gt; -- [m]oeb = multi-loeb :-) Or Möbius, since you're dealing with strange loops? (-:
You are excellent. I will read this later! A side note: I've been meaning to write an interpreter for FLooP and BLooP from GEB for a while because it seems like fun, but my friend has been borrowing my book for a long time. :(
It will eventually have static type checking.
You seem to be saying "laziness can be unuseful in this certain situation...therefore, it makes sense to abandon Haskell." In any case, when approaching Haskell, it might help if you try to detach your idea of a program (a structuring of ideas) and an implementation/compilation (a translation to bytecode). When you think about things, say things, you do not worry about how they will physically look when you write them down on paper. The ideas, to some extent, exist independently of the actual compilation. Haskell isn't GHC. Haskell isn't an implementation. It is a system of ideas. And while you can never completely divorce yourself from the bytecode or javascript code it compiles to, you gain a lot from freeing this one-to-one correspondence that languages like C and Fortran force you into thinking. If you reason about the ideas, and not the implementation, you open the door to powerful things.
This is an interesting point of view. Forgive me for not being knowledgeable of all the terminology, but could you point out something in the literature to better appreciate how you came to the conclusions? i.e. algebraic manipulation of types, how does existential quantification gets implemented on haskell's type system (System F + ?). I recently bought TAPL and its 'advanced topics' sequel, so maybe I'll have my answers within them?
&gt; Isn't that what Haskell always does if you just mutate part of data structure (which you can't actually do because purity)? In the case that you only need to update one field of a record, *I think* that you need to allocate a new record, but *I think* that you can reuse the unchanged fields of the previous record. Sorry, what I meant was that you always have to construct a new `Base` and all of its fields. I.e. you always have to do foo :: FooData -&gt; Base foo fd = Base (stepFoo fd) (displayFoo fd) and never just update a an existing `Base`, because you may end up with different views of the state in different fields/mutators otherwise. Well, technically you could just update the ones that care about this part of the state, but that seems like it would make the code unmaintanable. So I guess that I was just surprised that every mutator has to call `foo`, which in turn reconstructs all of the `Base` fields. And reconstructing all of the base fields is just partially applying a bunch of functions to `FooState`. It seemed like a weird coupling at first, but I also initially thought there had to be more than one definition of `foo`. I now realize that if you always update every `Base` field, you only need one `foo` per "subtype". Please feel free to correct anything that might be wrong here. This is largely based on how I think that it probably works.
&gt; or if this is even sensible It's not a very big extension, but you bet it's going to lead to people using it like function overloading. 
I think I understood it for a second :) This spreadheet-style behavior is one of the nice things about the humble Data.Array, which I think a lot of people aren't aware of.
That looks correct. `foo` is just a codata constructor. &gt; but I think that you can reuse the unchanged fields of the previous record. No, not in the language model, at least. GHC might do some pointer magic behind the scenes, I'm not sure. 
&gt; No, not in the language model, at least. GHC might do some pointer magic behind the scenes, I'm not sure. Ah, ok. Thanks for clearing that up.
There are some other issues though. INLINEABLE and INLINE can interact in unfortunate ways. I've been meaning to write up an article on the gotchas inherent in the current system once I can pin down the whys and wherefores for it all.
A lot of things are generic data structures or control functions.
Checked that. There was, I deleted it, it continued making files with a ? tacked onto the end.
Are you sure it's a question mark and not the placeholder for an undisplayable unicode character?
try to print to console `computername` byte by byte, check it length. Be sure, the problem is not on `readfile` stage Or do it for `T.append "/home." computername`
It could be, but if I try to CD into home.computername? I can.
The spreadsheet behaviour is really just a consequence of Array's Functor instance. Vector+Ix (for 2-dimensional indexing) would have worked just as well, the same goes for [[a]].
This was a really interesting read. I like your writing style, too; it made things pretty clear to me.
http://sourceforge.net/mailarchive/message.php?msg_id=31649613 Try out version in github (clean out the "dist" directories before you build it) and let us know if it works on your system. I have just emailed the hackage admins to get access to upload candidate packages.
Glad to hear that. I'm always a bit worried because I tend to write things so that I specifically can understand them, which may not be so good for other people. I think everything in my articles respository was written for myself before I thought about publishing it.
`($ go)` is a function which applies a function to `go`. It is equivalent to `\f -&gt; f go`
None of the functions you show here will allocate any `Tree` nodes because none of them use the `Empty` or `Node` data constructors as values (they only appear in pattern matches).
Thanks!
OI(X) doesn't deal with negative information so much as positive information that is only true in a certain scope.
This is pretty cool! I don't know if this is a defect or not, but: I can't move stacks of valid cards onto another valid stack. So, if I have alternating 10, 9, 8, with 8 being red, and in a different stack I have alternating 7, 6, 5, 4, 3, with 7 being black, I can't move the 7-stack over to the 10-stack.
Brilliant work! One thing I particularly like is that unlike most of the recent (mainly Emscripten) browser demos this one doesn’t make my CPU fan go crazy. Unfortunately, I also encountered the disappearing card bug, and I think it was correlated with stutters. Maybe some event is dropped when the GC kicks in?
Thanks for your help there - there are definitely a few things to fix. The game will sometimes not load at all and it has also been known to freeze up. It performs OK for me on Firefox (no more than ~300 ms pause) and well on Chrome. It could be a lack of WebGL hardware acceleration.
From boring lazy recursive self-definition to loeb in a few easy steps: 1) let xs0 = 1 xs1 = succ xs0 xs2 = succ xs1 xs3 = succ xs2 xs = [ xs0 , xs1 , xs2 , xs3 ] in xs 2) let xs1 = succ (xs !! 0) xs2 = succ xs1 xs3 = succ xs2 xs = [ 1 , xs1 , xs2 , xs3 ] in xs 3) let xs2 = succ (xs !! 1) xs3 = succ xs2 xs = [ 1 , succ (xs !! 0) , xs2 , xs3 ] in xs 4) let xs3 = succ (xs !! 2) xs = [ 1 , succ (xs !! 0) , succ (xs !! 1) , xs3 ] in xs 5) let xs = [ 1 , succ (xs !! 0) , succ (xs !! 1) , succ (xs !! 2) ] in xs 6) let xs = [ const 1 $ xs , succ . (!! 0) $ xs , succ . (!! 1) $ xs , succ . (!! 2) $ xs ] in xs 7) let fs = [ const 1 , succ . (!! 0) , succ . (!! 1) , succ . (!! 2) ] xs = map ($ xs) fs in xs 8) Rinse and repeat for your favorite functor.
It does use some CPU time when idle because I've written the engine for animated games (even though this game isn't very animated). The logic of the game absolutely requires every 'mouse down' to have a corresponding 'mouse up' before the next 'mouse down'. I think that's why it's going wrong. I'll clean up the input.
If webgl isn't working, it'll give an alert saying it couldn't initialize WebGL. Also try http://get.webgl.org/ Firefox is on the edge of acceptability on my Ubuntu/Intel-NVidia/1.2 GHz i7 system, so it wouldn't take much to push it over the edge. I think the generational GC in FF will help a lot, since Haskell relies heavily on an efficient memory allocator: [Here's mozilla's ticket for it.] (https://bugzilla.mozilla.org/show_bug.cgi?id=619558) Chrome already has a generational GC
Not quite sure what you mean. I was referring to something like: `let a = listArray (0,2) [a!1 - 1, a!2 - 1 , 3] in a`
Have you seen mmorph? http://hackage.haskell.org/package/mmorph-1.0.0/docs/Control-Monad-Morph.html I think you want something like `hoist (hoist atomically)`
I think the right way to build an intuition for this is that `fs` is a list a list of accessors -- it's type `[a] -&gt; a` tells you, in some sense, how to access an `a` out of a `[a]` (maybe by computing something interesting). What `loeb` for lists is doing is letting you define the list in terms of the accessors for each cell -- some cells are trivial accessors that just give you a value without accessing anything at all, but other cells have to reference the other cells in the list (i.e. they're non-trivial accessors). The generalized `loeb` then just does this for any functor whatsoever. The version in (6) I think makes this clearest: if you ignore the `$ xs` part, its just a value, successor of the 0th cell, successor of the 1th cell, and successor of the 2nd cell. Oh and by the way, its cells of this very list, `$ xs`. In natural language this would be something like Let's defining a list. The 0th element is 1, the 1th element is the successor of the 0th element of *this very list*, ... etc. The self-referentiality is relative trivial in that sense, then. Factoring out the accessors from the `loeb` structure lets you generalize to arbitrary functors, but ultimately its essentially the same game for all of the containers -- first element, vs. m-by-n-th element for 2d matrices, vs. paths into a tree, it's all just defining one piece of a structure by reference to another piece. The real trick, no doubt, comes from groking what this means for things like, say, functions. After all, `(-&gt;) e` is a functor. But that's not *too* bad, because function types are kind of like generalized lists indexed by the domain, rather than by the natural numbers. Maybe fancier things loose this intuition, but probably not. I don't even want to attempt to grok moeb right now. :x
Yes, that was my inspiration for this. However if I understand correctly, hoist is for mapping a monad morphism over a monad functor (such as StateT S). That would be useful if I wanted to run my monad in a StateT or ReaderT, but not if I wanted to use the MonadState and MonadReader classes.
Right.
`($)` and it's flipped form `cont` are some of the most sublime things you will ever encounter in Haskell.
And don't forget the reduceron papers ([1](https://www.doc.ic.ac.uk/~wl/icprojects/papers/reduceron08.pdf) [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.2486&amp;rep=rep1&amp;type=pdf)) since he mentioned an interest in hardware.
You are the (co-)author of Sodium, right? I was just studying Sodium sources yesterday, and I couldn't understand the purpose of the Context abstraction. The only Context instance is Plain, and it wasn't clear what other instances you could have and what exactly you're abstracting over. Could you explain?
I never had any problems with ADTs at all when I first encountered them. They were very intuitive for me. GADTs though took *months* before I understood properly and I still feel a bit uncertain when working with them. :-\ 
that does not tell you anything, if you are not typing the `?`. apart from that, unix says you _shall_ use a newline in the end of every text file (i can try to find the reference), so most (unix) editors automatically insert a newline. you can of course construct the file without newline using gnu's `echo -n` or `printf '%s'`, but i would fix the input logic instead and use `head . lines` or something along the lines.
Too difficult. Is there a **nööb** function?
It's a placeholder for something I have done in the C++ version but not the Haskell version. The idea is that all the types get an extra type parameter which is a phantom type identifying the 'partition'. Normally the FRP code has a big mutex around it in order to provide transactional consistency, which is a parallelism killer. Partitions give you effectively separate FRP systems in one process, so they can run in parallel with each other. Because the consistency isn't there, we use the type system to protect you against this lack of consistency. Crossing from one partition to another is an explicit operation. I hope that made sense.
The optimization you want depends critically on the predicate `(&lt; 10)` having the following property: `not (x &lt; 10)` implies `not (succ x &lt; 10)` - in other words, if it's False for a value, then it won't become True for the next value. In this case, it may be possible to hardwire this into the compiler somehow, but where do you stop? You might want `[ x | x &lt;- [1..], x*x &lt; 10]` to return `[1,2,3]` - after all, x^2 is monotonically increasing on the positive numbers - but if you use this expression in a context where the type is inferred to be `[Int]`, the result will be a lot longer :-) Even if you limit yourself to the mathematically well-behaved `Integer` type, I'm sure it's possible to construct a predicate with a fifth-order (or higher) polynomial in there that would require adding the capability to numerically solve equations to GHC to get the 'obvious' behaviour. 
So it's needed for concurrency/parallelism. Got it.
I came across a bug where I had a black 8, red 7, black 6 on one column and a red 9 else where. When I tried to drag the 8 and down onto the 9 it wouldn't accept it. There was a black 8 elsewhere that I dragged onto the 9 separately which worked then I moved the red 7 and the black 6 together onto the other 8 and it worked. Unfortunately I don't know how to replicate. Otherwise looks good. Although zooming has the opposite effect as well, which is weird.
Ah...
I'm getting "Package file was not signed correctly" on a Galaxy Nexus.
Are? I will check it.
Umm... My android device can launch the app...
Oh, you mean even without `loeb` you can do spreadsheet-like things. That's correct, but also not limited to `Array`. Here's the same code with `Array`, `Vector` and `List`: import qualified Data.Array as A import qualified Data.Vector as V import qualified Data.List as L a = A.listArray (0,2) [a A.! 1 - 1, a A.! 2 - 1 , 3] v = V.fromList [v V.! 1 - 1, v V.! 2 - 1 , 3] l = [l L.!! 1 - 1, l L.!! 2 - 1 , 3] main = print a &gt;&gt; print v &gt;&gt; print l -- Output: array (0,2) [(0,1),(1,2),(2,3)] fromList [1,2,3] [1,2,3]
The issue is that my device cannot install it from the play store. It does not get as far as being able to launch.
I got it. If "Unknown sources" check box is off, you will see the message. I try to fix it. Thank's.
The way I came up with `moeb` is really just looking at `loeb`'s definition and thinking about how `Lens` parameterizes functions over `traverse` and so on. So given loeb x = go where go = fmap ($ go) x I simply added a "fmap" parameter to loeb loeb' fmap x = go where go = fmap ($ go) x But now that's silly, because the `fmap` could be anything and not just `Functor.fmap` (shadowing is evil), so I renamed it to `f` and `loeb'` to `moeb`.
This looks like a no-op to me, is it being used to change the order of evaluation?
It's disappointing that the function is named `loeb` instead of `löb`.
Noop.
Anyone else notice that moeb has type `((not not a) -&gt; c -&gt; a) -&gt; c -&gt; a` ? `moeb` essentially says that, if you can give me a function that can extract an a from an a-continuation and a `c`, and a `c`, I can get an `a` out of that. Now reread @psygnisfive's answer: `moeb` is your (generalized!) accessor function which finds `a`'s buried in `c`, up to arbitrary amounts of intermediate computations. Edit: better formatting.
English is ambiguous. The computer science thought on ambiguity is that a language is ambiguous if there is more than one parse tree for a string in that language. (also that the problem of deciding if a random language is ambiguous undecideable) And the idea that numbers are the only type in mathematics ignores set theory, or more importantly for functional languages, lambda calculus and category theory. These are examples of math that do not focus on numbers so much. 
This is a really great way of presenting it.
Hi, I tried the darcs version a couple of weeks ago, and it worked with latest cabal. What is the official development repository ?
Reminds me a little of my own approach: http://productivedetour.blogspot.com.es/2013/11/my-version-of-twitter-waterflow-problem.html I too used a stack to store already seen heights.
 extract :: Monoid w =&gt; (Writer w (r a) -&gt; Writer w (r b)) -&gt; r a -&gt; r b extract f1 = fst . runWriter . f1 . return converter :: Monoid w =&gt; ((r a -&gt; r b) -&gt; r (a -&gt; b)) -&gt; (Writer w (r a) -&gt; Writer w (r b)) -&gt; Writer w (r (a -&gt; b)) converter myfunc = return . myfunc . extract 
Yes, I found that one too. But test it, and you'll see it has a deep flaw: it throws away all the 'writing' that f1 does. `myfunc . extract` is 2/3 of the way there, but you can't just start from an empty Writer (which is what `return` does in `converter`), you have to make sure that what `f1` writes is not thrown out.
To be honest I don't really know how to test it as I just don't know why you would like to write this :)
It's definitely one of those things that you have no trouble with once you get used to it, but in the beginning it's like wha?? I remember being thoroughly confused by *Write Yourself a Scheme in 48 Hours* which does things like `data Value = Bool Bool | String String | ...`, leading to code which is very hard for a beginner to grok. Ironically, that book claims to be accessible even to those without a computer science background, which I find laughable, as it completely glosses over concepts which are difficult for anyone not fluent in certain Haskell concepts to grasp, let alone someone who's never heard of variable assignment, recursion, constructors, types, etc... (it is a good book, just don't assume that you're dumb if you can't understand it right away).
I totally agree. Of course, implementing this would require sophisticated algorithms for parsing and analyzing code, so it's not surprising that it hasn't made it into things like Sublime Text.
nice observation. but to my point: it would be so much easier to read if you'd use some style hints. e.g. enclose something in backticks to make it appear like code: `a`
I have the checkbox allow installs from unknown sources enabled. And i still get the error message. No go.
Good point about the formatting: done.
Also, check out http://www.urbit.org I don't know much about it but it appears to be an operating system (Arvo) written in a functional language (Hoon) running its own VM (Nock).
There is a significant amount of lag when carrying a card around.
Could you be more specific about what this particular function is supposed to do? What are you logging? What behavior do you actually want it to have?
I'm really excited about this release! We just launched this free version of the IDE, check [the page](https://www.fpcomplete.com/) for screenshots. See [here](https://www.fpcomplete.com/business/blog/fp-complete-launches-free-community-edition-fp-haskell-center-announces-feature-upgrades/) for the announcement. Think: * Demonstrations. * Pastes/getting help/reproducing (lpaste.org now has a “Clone in IDE” link that you can use to clone any paste straight into the IDE under your account). * Learning Haskell/going through books with a zero setup IDE with really useful features (e.g. see the sub-expression type info, goto-definition, hoogle, etc. all great learning tools) and pre-installed package set (Stackage). It's like Try Haskell/School of Haskell on steroids. (Though I plan to also make * Just writing code. * **EDIT**: Forgot to mention, the SoH tutorials, you can hit “Open in IDE” and it'll clone that snippet as a project. There's [a tutorial on sharing projects](https://www.fpcomplete.com/school/using-fphc/sharing-projects-with-fp-haskell-center) with others which is _itself_ a project with a tutorial in it that will get autocloned to your account. The trial for pro edition is still available, but this community edition is now the starting point for any user. Everything else (Git/amazon deployment) is upgrades. Let me know what you think. :-)
&gt; We just launched this free version of the IDE Are you working for FP Complete? I didn't realise.
Yep! As someone who spends most of his time working on Haskell tooling anyway, it seemed the perfect place for me. =)
Interesting post. I'll probably stick with the following approach to this particular kind of problem though: `grep "latermuse.com" access* | awk '{print $1}' | sort | uniq | wc -l`
In the latest version of [`layers`](https://github.com/duairc/layers), which I haven't released yet, you could do this: hoistB atomically :: (MonadInnerFunctor IO n STM m, MonadBase STM m) =&gt; m a -&gt; n a `hoistB` is like `hoist`, except it hoists from the base of a transformer stack instead of the monad just beneath the top of the transformer stack. It is a special case of `hoistI`, which hoists from any inner monad of the stack. Also, you almost never have to define instances of any of these classes (`MonadInner`, `MonadInnerFunctor` and so on) manually, instances are automatically defined using `OverlappingInstances` for arbitrary transformer stacks (as long as the transformers in the stack are instances of `MonadTrans` in the case of `MonadInner`, and `MFunctor` in the case of `MonadInnerFunctor`).
It's not quite a few lines of Haskell! It's three lines, and it could have been one line if he'd written them all on the same line :)
Ahhh I get it and now I feel like an idiot...I had misread your post as home.computername???? leading to the confusion. My apologies.
No problem. I hope it was helpful!
I've already benefited from the School of Haskell's new ability to open an active code sample inside the full IDE. Great work!
The point was that replacing awk '{print $1}' | sort -u | wc -l with that Haskell is not a good example for the power of Haskell when compared to Shell or other common system scripting languages. And by my count his example is 6 lines since neither the module nor the import lines are optional and each of them is about the length of the shell one liner. There are many tasks shell is bad at but scripts of this complexity it usually handles just fine (and not that differently from a functional language I might add).
I'm a bit puzzled by the pricing page. Towards the top there's this line, which is checked in the "free" column. &gt; Open Projects from Git, FPHC, or Web Then a bit farther down there's this line which is not checked in the "free" column. &gt; Push Projects to Git and GitHub Does this mean I can import and edit Haskell but I can't get my code back out without paying? (Or I suppose, copy and paste file by file :p)
I am logging access to all parts of an AST which affect control flow. Think of a typical trace like `[EnterFunction, EnterThenBranch, EnterFix, EnterFunction, EnterFunction]`. Typical example: the trace through a factorial function (except for things like + and * and lifting integers). 
Exactly! Git integration is in the personal edition. So if you want to export your project you can either (1) upgrade to personal and then just push it to some Git repo, (2) start the one-month trial and do the same, or (3) copy and paste the files manually.
I wonder how hard it would be to write a small wrapper for `runhaskell` that looked for imports in a config file and prepended all that bs to the real meat of the program. It's made harder by the fact that `runhaskell` needs a file as input...
Just in case anyone has the same idea runhaskell &lt;(echo 'main = putStrLn $ show "Hello World"') won't work, the error message is this: *** Exception: /proc/self/fd/11: hFileSize: inappropriate type (not a regular file) In general it should be easy to make a small shell script to generate a temp file with a constant header and calls runhaskell on it though. Anything more complex (like dynamic imports depending on the used symbols) would probably not be worth it.
adding to the point. i even have no google play account acociated with my android phone, so i couldn't buy it anyway.
&gt; The real trick, no doubt, comes from groking what this means for things like, say, functions. After all, (-&gt;) e is a functor. Specializing the type of `loeb` gives: `(e -&gt; ((e -&gt; a) -&gt; a)) -&gt; (e -&gt; a)` It squashes a function which produces a CPS-esque supsended computation into a regular function. We can write a stylized while loop with it: `loeb (\str@(c:cs) -&gt; if length str &lt;= 3 then const str else ($ cs)) "Strange"` I guess it fits your intuition about self-referentiality, though I'm not sure of how to state that clearly.
I feel `Action` is more general than the IO monad. Which I wish had been named `RTS` for Runtime System :)
Let us know when it is fixed. And how :)
The focus is definitely on web apps, online and offline services, data processing, stuff like that. That's quite a large design space. I think “classical” desktop GUI apps are much rarer these days (whereas the mobile market is huge and it would be neat to compile to Android and iOS once that support in GHC stabilizes). I think with the (upcoming) standalone version a GUI (e.g. with wxWidgets/Gtk/Cocoa/Win32) could be done, but it's not an avenue we're _currently_ focusing on. Although, threepenny-gui is something that would work fine.
Maybe not the best choice of a priced feature, the GitHub part at least (as opposed to general Git repos). Pushing to GitHub should be encouraged, as that way there's more code for everyone to leverage.
It took like 10 minutes of random fumbling for me to find the "create new project page" please tell me I am not the only one.
What steps did you take from the home page?
I was looking to create a new blank project so here is roughly what I did * clicked "build" https://www.fpcomplete.com/page/project-build - missed the link at the bottom * back to home page * products =&gt; haskell center, read page, no joy * clicked community edition, landed on a pricing page, nothing there * back to home page * back to build tried a the monte carlo template just to play * after messing around for a bit landed at https://www.fpcomplete.com/user/lab-notes * found the new project button also after posting the initial comment I spotted the llnk at the bottom of the project-build page
Better I think would be to have a personalized prelude for the shell type stuff you're actually wanting to use it for, and just auto-import that.
A shell-specific Prelude is a nice idea.
I agree, they could make it more obvious. EDIT: Well, it's kind of obvious in the start screen I just realized, but maybe put "create" in the menu too? Just suggestions... :P
looks great :) should be a wonderful resource
Where can I do that? I created a temporary project for just trying the editor out, but now I don't know how to delete it. 
it seems like a reasonable differentiator between the free and personal editions, though. "learning edition" might have been a better name for it than "community edition"; the most compelling use case seems to be following along with haskell tutorials while building and running code.
Also: $ echo 'import Data.List;main=readFile "access.log"&gt;&gt;=print.length.nub.map (head.words).lines' | runhaskell 1099 Which reminds me, I should really start using GHCi/GHCi-derived as my main shell and stop using bash. I'll start now and report back findings.
Surely you don't mean "copy and paste the files", right? I assume the editor still saves files normally, so you would just need to use git on the command line to manage it. **EDIT**: Oh, a *web* IDE. I completely misread that. Sorry, please disregard my comment.
For some use cases, bash is very hard to beat in brevity. It still feeds of a somewhat similar concept as Haskell does though. I like to think of pipes as a way to compose things in bash much like I would using (.) in Haskell.
Cheers. Is [this](https://www.fpcomplete.com) and [this](https://www.fpcomplete.com/page/project-build) clearer? 
I agree with Taladar that this is not a very good example for the power of Haskell. I think that Haskells true strengths are better seen in more complex examples where you can really admire the modularity and conciseness of functional code. Almost trivial log parsing is a task that we threw at bash (or other shell implementations) for decades now and we've got a rather powerful set of tools that we can arbitrarily compose using pipes. It's also not the kind of task you usually reason about for a long time. In short: It's not a very hard problem. But those hard problems are exactly where Haskell shines and hence I think we should rather use them to demonstrate its power. Then again, I don't say it's a bad idea to use Haskell for this sort of task. After all, it's very good at it as well. At that point it's probably about what you're most comfortable with.
Yep! My eye jumped there immediately, as did my colleague's.
Yeah much clearer for me, I would have found what I was looking for much much faster.
Can't möb just be written as moeb f = fix (f . flip id) The properties like id being fix make a lot more sense here too. And it's obvious that f is being applied as f ($ f($ f ...)))
Cheers! This'll help future users. :-)
Thanks for helping!
isn't that why pipes is called pipes?
Thinking about it more, it might be doable just with regular expressions. Seeing as the only place type literals appear is in signatures, and those always have the format of `&lt;something that starts with a lowercase&gt; :: &lt;one or more identifiers separated by '-&gt;'&gt; &lt;newline&gt;`, you could at least go a long way by just capturing that second group, and coloring everything that starts with a capital letter to be whatever a type should be. Maybe that would be it. Oh I guess you'd need to do `data` declarations too, but that would be similarly easy. I haven't written a syntax highlighter either, although I have experience with parsers. Maybe I'll take a look at the SublimeHaskell source some time and see if I can make that modification.
I prefer to think of it as a big product, like I said.
I don't understand exactly but wouldn't a "trace" be something that depends on the specific argument value provided? `Writer w` can't capture that since it's just a pair of `w` and some other value.
Yes, it will depend on the input value. And so you are correct: Writer is not enough. Seems like I have no choice but to go to State for this.
Very cool. I see in the latest release notes support for vim keybindings but I can't figure out how to enable them. I think more documentation and UI improvements could go a really long way, would love to finally replace my horrible "vim-only" Haskell development experience.
&gt; I should really start using GHCi/GHCi-derived as my main shell Is that plausible? How would it work? Are there Haskell ls, cd, mv etc?
Just define a custom Prelude with your ls, cd, etc commands?
Derp.
That can certainly be done, but the trick is getting all the options that you want to play nicely with Haskell syntax.
I'd imagine you can do that by defining some weird Num instances :) (or hiding (-) from Prelude) and defining the variables a..z.
I use hdevtools, syntastic, ctrl+p, tagbar, neocomplcache, nerdcommenter, etc. It's still horrible because it lacks hoogle/build/debugger integration, automatic module imports, jump-to-definition... For large projects, I prefer a clickable collapsible file hierarchy, as opposed to something like nerd tree. I do love vim key bindings, though.
I tried out [some basics](http://lpaste.net/2383274054015516672) and [here is a demo](http://lpaste.net/3159158122216423424) and it's actually not bad, and with ghci's editline functionality it has filename completion like a normal shell, which is pretty cool. Of course, that only works in one directory, because you have to use `:cd` in GHCi to actually cd. But with some expansion and maybe a wrapper around ghci to support cd + editline properly and to put the current dir in the prompt, I can see myself using this for real. And I know I'm stubborn enough to make it work for myself. There's some previous vapourware work on Haskell shells but they're all overengineered/designitus. Will report on further results some time in the future.
I agree. I think if one were to apply the lessons learned from functional programming to OS API design, one could come up with something pretty interesting. I think the right approach is that, rather than just thinking about what FP features could be applied to an OS, think about what are the things that make modern operating systems painful to use, and can they be fixed? For instance, modern OSes don't generally have any sort of undo feature. If you delete a bit of text in a word processor, you can hit undo and everything's fine. If you move a bunch of files around and then realize you shouldn't have done that, you're out of luck unless you have a backup. Modern SSDs don't re-write data in-place anyways, so it's kind of silly we don't use persistent data structures for the FS and keep a reference to the previous versions around. (There are probably FSs out there that can do this, I haven't really looked into it, but it's not something you can take for granted on a modern system.) Lack of undo also makes shell scripts hard to write. Most of the time, you want a script to either succeed or, if it fails, you don't want to have any side effects. Actually accomplishing this is hard and error-prone -- for every possible error, you have to be able to go back and manually undo everything you did. Guaranteeing mutual exclusion is hard, too; any script or sysadmin task can race with any other, and that's considered normal. I'd love to see some sort of STM-like system, where scripts run inside of atomic transactions and the filesystem is treated like any other shared datastructure. If several transactions race, we just back out all the changes automatically, and re-run them sequentially. There's a lot of cool things you could do by giving programs and files explicit types, known to the OS. For instance, suppose you have a bunch of conversion tools for things like pdf, ps, dvi, etc.. Trying to run, say, ps2pdf on a dvi file could be caught by the shell as an error, instead of requiring the program to do all the input validation. Or, let's say you have a dvi file you want to convert to ps, but don't remember what the command is called. Maybe you could stick a wildcard in the command line, and the OS could select a program with the appropriate type conversion. &gt; cat foo.dvi | ??? &gt; foo.ps could be automatically promoted to &gt; cat foo.dvi | dvi2ps &gt; foo.ps or even prompt you to install the appropriate utility, if it's not locally installed. A truly paranoid OS might not even let you write out a file as a particular type unless the contents can pass a validation test. 
Also, with respect to making life simpler for Haskell programs, specifically: It seems like if the OS were able to distinguish between programs with side effects and programs that act like pure functions (read from stdin, write to stdout), one could conceivably write Haskell programs with no IO monad at all. Or, more alternatively; one could use any pure haskell function as a standalone program, assuming the OS knew how to do the right stuff behind the scenes to make it work. For instance, there seems to be a correspondence between commonly used haskell functions and commonly used shell commands: cat = id grep = filter echo = const sort = sort and so on. I'd sure love to be able to use the whole prelude on the command line.
Such a shame to use `awk` only for extracting columns.
I don't know. To me, it comes off as a power grab on GitHub's widely successful execution of project forking, and seems a rather great way to encourage community *fragmentation*. 
you can put #!/usr/bin/env runhaskell at the top of your haskell programs (now you can call them scripts i guess...), but don't forget to chmod +x
Any plans for a self-hosted version? I can't put proprietary code owned by my employer into your environment.
Success on my phone. I love this! Smooth animation. Feature request: gloss-styrene with the phone's gyroscope linked to the angle of the viewport :)
Does that mean that haskell-mode is abandoned? :( 
Come back when `awk` supports the regular expression set/syntax that `grep -P` has :P
That sounds really interesting. I'll admit that I'm comfortable with bash so I'd probably not be one of the earliest adopters but I could see myself reading any blog posts about this you cared to share.
Security would be real nice
Yes
This. Some codebases are under legal restrictions that make it very hard to use cloud services. For example, for US federal contractors anything "OUO" (which covers a lot) have special restrictions that make cloud based tool impractical unless they have self hosting options. You can charge (a lot actually) for self hosting, but a self hosting option is important.
OK. I try it with opensource!
I retry to upload the application. My device factory-resetted can get the application.
Now we accept your bitcoin donation at http://ajhc.metasepi.org/.
It's coming: http://ghc.haskell.org/trac/ghc/wiki/PatternSynonyms
But not publish to github to take advantage of its forking and issue tracking. Not to mention the visibility and openess of Github means that your project can get a lot more eyes on it. I can't see your project without logging in. This is a big problem. On Github and almost any other repo hosting community I don't need an account. People will not join the community without first seeing a good reason to invest their time in such a community. Community engagement is **very** important when it comes to programming, and even moreso in an as tightly knit community as the Haskell one. I would evaluate and rethink how having a closed community like what you've built will impact your visibility and viability as a business. If you have killer features but I can't see them how do I know I want to invest the time/money in setting up my, already well established, project on your system? I understand you're trying to run a business, but this kind of attitude will ostracise a lot of people who would want to try out the IDE and are already heavily invested with their project on Github.
&gt; Modern SSDs don't re-write data in-place anyways, so it's kind of silly we don't use persistent data structures for the FS and keep a reference to the previous versions around. (There are probably FSs out there that can do this, I haven't really looked into it, but it's not something you can take for granted on a modern system.) You should have a look at ZFS, a copy-on-write filesystem that implements "snapshots", simply by keeping a pointer to the current state of the filesystem. &gt;I'd love to see some sort of STM-like system, where scripts run inside of atomic transactions and the filesystem is treated like any other shared datastructure. If several transactions race, we just back out all the changes automatically, and re-run them sequentially. Would you then disallow networking for that script? or disallow changing of kernel configurations? STM only works because it limits you to non-IO operations, such that effects can't leave the STM until they are committed. 
Did you have two free cells? You can only move a stack one card at a time. The game lets you drag a stack of N if you have N-1 free cells. I got stumped by this too but realized i'd forgotten the rules :) 
it can also shine when a parser combinator library is useful. whenever you need to really parse e.g. the access log, it might work out. (but even then i have a script that tab-delimits an access.log so i can use cut without problems.) edit: the problem is that you have `"` enclosed fields in the access log, so you can't get away with using cut/awk with space delimiters.
you are doing great work btw. very appreciated.
because then you have 2 problems.
Self-hosted/offline version is in the works presently, aimed for next release. 
The opposite, haskell-mode will benefit too. I'm currently working on support from Emacs. And I'll announce structured-haskell-mode soonish.
Yeah, the editor keybindings should be moved into the IDE, they're still in the old place, your [account](https://www.fpcomplete.com/account) page. Go there for Vim keybindings.
Hey there, In London, there's two major Haskell groups - http://www.meetup.com/London-Haskell/ which meet regularly at Skills Matter/City University and http://www.meetup.com/hoodlums/ that meet in Canary Wharf. These are both evening based. At Skills Matter we also run a Haskell eXchange in October, but also do a lot around other Functional languages (F#, Clojure, Scala) and play host to the Functional Programming eXchange in March which might be of interest too. http://skillsmatter.com/event/scala/functional-programming-exchange-1819 Theo
It's actually a nontrivial part of the design to avoid dealing with negative information. Suppose you case-analyze a vector (a length-indexed list) whose length index is an existential -- i.e., a term of type `Vec ?n A`. Then, if the case statement has two patterns, `[]` and `x : y : zs`, you may conclude that the `?n` is not equal to 1, even outside the case statement. However, supporting such constraints moves you from unification to disunification, and that's an exponential jump in complexity, because equational disunification algorithms must backtrack. This is why OI is not complete with respect to its specification. 
ronpaulitshappening.gif
One of the standard lines about IO-like monads is that it's not just about isolating the IO and its impurity - it's finer grained than that. STM is already one of those finer grains, but what if it were a bit finer grained still. A file that you have an exclusive lock on is effectively a block of (slow) memory. Even using system memory to store values has side-effects that we ignore (the abstraction of pure computation leaks a little), and some of that "system memory" will actually be virtual memory that has been swapped out to disk. So having an STM-like monad that supports a kind of memory-mapped file access makes some sense - it's not so different from virtual memory. And don't forget - the idea of transactional storage was originally developed for databases. Basically, it may well make sense to extend the transactional model to support some things that are currently considered I/O - but not the full generality, and not just because we still don't have a way to rollback the world. 
The [Shell Prelude](http://chenected.aiche.org/wp-content/uploads/2011/06/2011-06-06_1653-shell-prelude-flng-ship-3.png) is apparently a very large tanker. I'm not sure what can be done with this information.
How will this work? We don't want to tie the open source emacs-mode to e.g. FP complete servers.
That might have been the case, I'm not that familiar with the rules of free cell. Solitaire for life!
That seems like more of a problem with the IO libs than haskell itself. 
It was a bit hard to tell from my peek at the Website, but I assume you do not mean free as in freedom? In which case I have zero interest in this.
Free as in free beer that is, although they do contribute quite a lot to the community as I've understood.
You might be interested by [DragonflyBSD](http://www.dragonflybsd.org). It's an offshoot of FreeBSD that embraces erlang-ish message passing and is built around hammerfs which is similar to the sort of filesystem you're describing.
unsafeCoerce#
I've found that view patterns and matching with `cast -&gt; Circle r` works.
Great stuff! I've been intending to do something similar, though I was hoping to give it a go using FRP since I write too much imperative UI code at work already. :P
Very beginner-hackable code. Maybe you should write some posts about it... &gt; Item Potion "Homeopathic Potion" 100 0 0 500 (Healing 0) 3 Please add "Cure Blindness" scroll (8
Great game! Have you considered splitting it up into modules?
You would probably be doing your future self a favor if you split the code into modules in order to maintain some semblance of order, and ease maintenance.
[The ubuntu grep manpage](http://manpages.ubuntu.com/manpages/hardy/man1/grep.1.html) (includes -P) [Differences between PCRE and posix regexes in the PHP manual](http://php.net/manual/en/reference.pcre.pattern.posix.php) 
Hell yeah! This is great, keep doing it!
That's much cooler than my [left4deadrl](https://github.com/mcandre/left4deadrl).
[Ariadne](https://github.com/feuerbach/ariadne) is providing go-to-definition for haskell in vim. Though I admit in its current state of only working within one file, is not going to do very much for you yet. Personally, for hoogle/build etc. I'm happy using a separate pane in tmux, though I would like better support for import management. 
You would have to lift expressions to accept desired type and perform the lookup, and set things up to thread that through as part of argument passing. In essence, read "SomeStructure" would be something like: \ t -&gt; (M.lookup (ReadFn, t) functions) (flip const "SomeStructure" String) Which obviously wouldn't type check in haskell, but should hopefully convey the idea... I'm still not *sure* it's doable, much less practical or desirable, but I'm not convinced it's *not*...
I don't think it's an attempt to form a separate community from github, rather to strongly encourage anyone using it for more than playing around to pay for the personal edition. To me, there's nothing per se wrong with the free version being limited, but the name community edition is confusing.
Im not so familiar with emacs work flow, but I know that there are some nice plugins for "haskell mode" on emacs that you might want to look into.
It's an additional module. Currently, it's fpco-api.el. You `(require 'fpco-api)` and then add a save hook and then choose the `fpco` driver for `autocomplete.el` and for `flycheck.el`, change your `haskell-mode` `M-.` keybinding to `fpco-goto-definition`, etc. Similar to ghc-mod or hdevtools.
[Free as in Github.](http://www.reddit.com/r/haskell/comments/1p03cw/my_first_haskell_game/) :-)
&gt; can't see your project without logging in. This is a big problem. On Github and almost any other repo hosting community I don't need an account. Yeah, an anonymous version is coming in the next release to remove this requirement. It's not an evil “hehe let's make a required login” move, it's a “we need to come up with a scalable way to support anonymous use of a full-featured IDE.” We _want_ people to use the IDE, with as low as an impediment as possible. Hence this community version which is completely free, no time limit, no ads, and you get in there within a few clicks. Once the anonymous version is stable and ready, it will be like Gist. &gt; If you have killer features but I can't see them how do I know I want to invest the time/money in setting up my, already well established, project on your system? You mean you don't have 5 minutes to register and create a blank project to try it, or a second to look at the screenshots? &gt; I understand you're trying to run a business, but this kind of attitude will ostracise a lot of people who would want to try out the IDE and are already heavily invested with their project on Github. What attitude?
Even cooler: grep "latermuse.com" access* | awk '{print $1}' | sort | uniq -c Get the counts of each unique IP. You could possibly dump awk for "cut -f 1" but I didn't look at the output to try.
You don't have to use the proprietary JavaScript on GitHub to host your project there, so it works fine. Or do you mean that the IDE is SaaSS? If so, then I care even less. I prefer doing my computing on my own computer.
Found them, thanks. However it seems like once I'm in insert mode, I'm stuck, since "escape" doesn't do anything
Still same error message. I tried again just now. Sony xperia v, nothing special (not rooted etc).
I've been thinking about how to do a type-based filesystem lately, actually. You could even have file-typeclasses for stuff like plain text files, image files, so on. I don't think there'd be a simple way to integrate it with an impure, unsafe ecosystem like the one we have today, though.
If you're using OSX and are having trouble trouble installing hfov-1.0.1 (a hoodie dependency), try using the latest version at github: https://github.com/nornagon/hfov 
I'm sure [CEDET](http://cedet.sourceforge.net) has whatever you need for that.
Our meetup tonight will be over Google Hangouts, so I figured I'd post it here as well in case anyone was interested. 
I’m in love with static types, but couldn’t name a dynamically typed language I would rather use than Perl. Explicit typing means data tends not to get away from you, and the library ecosystem is unparalleled. Also, I miss right-associative function application when writing in Haskell; `.` and `$` are fine but not ideal.
is there anything useful that can be done with moeb (=&lt;&lt;)?
Would be nice if you could have completion based on the buffer content too. I wrote something like: longFunctionName pattern1 = .... longF&lt;crtl+space&gt; but it didn't complete it into longFunctionName. I would have been happy with simple completion based on the buffer (*à la* meta-/).
It depends on the defaulting rules. No "conversion" is going on. Expressions like `sin` and `3` are polymorphic, meaning they have multiple types. Typeclasses like `Floating` and `Num` allow polymorphic definitions to be "non parametric"--that is, to do (hopefully only slightly) different things depending on what types you specialize them to. Types are members of typeclassess. To understand how this works in practice sin 3 we know sin :: Floating a =&gt; a -&gt; a 3 :: Num b =&gt; b unifcation gives the constraint set a = b, Floating a, Num b which leads to the type sin 3 :: Num a, Floating a =&gt; a but because `Floating` is a subclass of `Num` that is just sin 3 :: Floating a =&gt; a Finally, when you enter this expression into Hugs or GHCI it needs to pick some particular `a`. Special rules in Haskell are used for this "defaulting" for numeric types. A `defualt` declaration in scope lists types in the order they should be tried for resolving constraint sets that include certain basic classes (such as `Num`). It is from this that your implementation picks `Double` or `Float`, but that can easily be changed.
No transformations were necessary, since 3 already has the right type. If you lookup [Floating](http://hackage.haskell.org/package/base-4.6.0.1/docs/Prelude.html#t:Floating) and [Fractional](http://hackage.haskell.org/package/base-4.6.0.1/docs/Prelude.html#t:Fractional) on [Hoogle](http://www.haskell.org/hoogle/), you will see that `Num` is a superclass of `Floating`: class Num a =&gt; Fractional a where ... class Fractional a =&gt; Floating a where ... Therefore, since the type of 3 says that it can produce a value of any type `a` satisfying `Num a`, in particular it can also produce a value of any type `a` satisfying `Floating a`. Now, let's figure out which version is used. &gt; sin 3 :: Float 0.14112 &gt; sin 3 :: Double 0.1411200080598672 &gt; sin 3 0.1411200080598672 Looks like it's `primSinDouble` which is being used. [This StackOverflow answer](http://stackoverflow.com/questions/6165049/why-does-haskell-appear-to-default-to-reading-int-when-reading-num) explains why, but it's not a very important part of Haskell.
Check out the [section on defaulting](http://www.haskell.org/onlinereport/decls.html#default-decls) in the Haskell report. The short version is that if you have an ambiguous type signature for a variable with a `Num` constraint, then Haskell will try to default it first to `Integer` and then to `Double` if that fails. So here, it's defaulting the ambiguous type to `Double` (`Integer` wouldn't work as it isn't `Floating`). You can change the list of types it will try by writing at top level something like `default (Int, Float)` or `default ()` to disable defaulting entirely. If you do this, your example will fail as the type is ambiguous. ---- As to how the 3 becomes some number, Haskell inserts a call to `fromInteger` around each integer literal, or `fromRational` for a numeric literal with a fractional part. This is why `3 :: Num a =&gt; a`. (This is [described in section 3.2](http://www.haskell.org/onlinereport/exps.html#sect3.2).)
Hey this looks very neat! I actually think you should start a project on github for this, I would contribute
5 minutes to register and try something is approximately 5 minutes longer than I'm willing to spend. And this is true of the majority of the population at large, too.
I read a bit of their site. Hoon may be functional, but is much different than haskell or ml. Actually, I'm tempted to think it is all a very elaborate joke.
This seems backwards to me, it can pick any type which is in floating because all types in Floating must also be in Num. From my reading you're implying that anything which is in Num can also be in Floating, but I feel this is more an ambiguity problem than an understanding problem on your behalf.