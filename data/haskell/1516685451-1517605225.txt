I definitely agree about imputing bad intentions to others. At the same time, it would be nice if there were a clear statement somewhere about what the intentions are behind this proposal. If it were really just about immutable packages, then that's wonderful news, because this isn't a hard problem to solve. The version bound overrides are not actually changing the package description, so it shouldn't be too hard at all to just grab the original package description and not pay attention to the modification. There have been some claims (which I can't attest to personally) that there was already an agreement to do precisely this, which silently disappeared and was replaced by this one instead, all under the same name. If that happened, then clearly there are other intentions involved. I could guess what those are. I imagine it's frustrating for Stackage maintainers to have had to accommodate a few Hackage changes that were designed to support cabal-install features, if they don't care about that tool. I could believe there are package authors who only care about Stack users, and don't like getting bug reports about version bounds. But these would be guesses on my part, because I cannot find anywhere that these reasons are discussed.
I was wrong about some details. It looks like at least one instance of this involved tightening a version bound. There was some reasoning that supposedly established it was safe to do so, but that reasoning assumed everyone was using a dependency solver (a la cabal-install) and specifying bounds according to PVP assumptions, so that their builds would automatically switch to the newer version of the dependency. Unfortunately, there was an old Stackage version involved, so it became unbuildable. It seems clear this was a mistake, not an intentional breakage. But from the perspective of a Stackage maintainer, it makes sense that it was a particularly annoying change. On the other hand, my understanding is that Stackage has fixed already this by including hackage revisions in their versions when they freeze a snapshot. It's not clear why it's still a problem.
I'm confused. Are you implying that if Hackage didn't recommend the PVP, you'd be inclined to fork off an alternative package repository that did? Whatever one's feelings about the PVP, I can't imagine anyone believing the question of a versioning recommendation on the Hackage web site is *that* important to anyone. (Yes, that means I can't imagine that being the real reason in the other direction, either.)
Note that despite the recent ruckus, the situation has now gotten much better with the ability to specify cabal version dependencies directly in cabal files. New versions of the parser will be able to bail early and clearly when they see that the version specification is newer than what they can parse.
&gt; How the hell did we get to the point of threatening to fork Hackage? Exactly, where does that come from?
Does it bail if the version specification is wrong? I mean if new syntax is used in something marked as compatible with an old version? Otherwise I do not see that helping much.
Thank you for the great write up! It's important to not just hear the positives, but the pain points too. As others here have mentioned: please do point out places where the documentation needs improvement. Such feedback is a contribution that helps everyone going forward. Welcome to the Haskell world, I hope you enjoy your stay 😀
The overheard of microservices is generally far higher than expected.
thanks alot, this did the trick to be able to install intero.
thanks for the link, unfortunately the directions didn't work for me and am just using intero for the time being since it was the easiest to get running thanks to /u/roelofwobben. 
Hmm someone could honestly just do it on their own with no need for Stackage’s blessing. It would take effort, but specifying each Stackage LTS in an ever-growing Nix repositories seems valuable. 
Well, the stackage _compatibilities_ are curated. The platform was supposed to also be curated in the sense of providing a common core of functionality so you'd have "the standard library" for json, "the standard library" for html generation, "the standard library" for regular expressions, etc. I still think we would benefit from something like that, but given other issues to work out don't see where we can get there anytime soon...
&gt; If hackage weren't so pushy about the PVP then there would be pretty much no reason for people to seek other package repositories. I have to admit that I didn't follow the discussion close enough. I always thought that the PVP is good for me, because it allows me to figure out what versions my packages should depend on, and how I should version my own packages. In what circumstances is the PVP a bad idea?
&gt; I said (well, implied) that some people don't like/want forced adherence to the PVP. So let me get this straight. Does this come down to a handful of [SemVer fans](https://www.reddit.com/r/haskell/comments/5kt1l0/problematic_versioning_policy/) [lead by one angry guy](https://groups.google.com/forum/#!msg/haskell-core-libraries/xkJT8HEh7tM/tL-DWZ75AAAJ) who are unable to suck it up for the greater good that now everyone else needs to suffer because they didn't get their way?
I just use Atom with VIM bindings. I get lots of other plugins because I am not limited to VIM ecosystem (git-blame, minimaps, split-diff, term3, language-haskell, FiraCode ligatures). However I still don't use ide/haskell/ghcmod in Atom, it was too fragile last time I tried.
&gt; It is pretty impressive how a group of people infiltrate, subvert and divide the community, and then portray themselves as victims and the half of the community that didn't follow them as the bad guys. "We tried to take over haskell and you didn't let us omg you guys are so childish and terrible for not compromising!". It's the classic S. Maneuver: 1. find a popular piece of Haskell infrastructure you do not control 2. Identify a few minor flaws in the existing infrastructure 3. Hack something up that addresses those specific issues 4. Write a blog post which exaggerates the minor flaws in the old thing, claiming that the new thing is the only possible way to address the minor flaws 5. Attack the character and motivations of people supporting the old thing, insisting that they are unreasonable and corrupt people and there was no way to work with them 6. Deny there are any significant flaws in the new thing 7. Claim you are being unfairly attacked Repeat until you have control of enough of the Haskell infrastructure that you can usurp the rest. 
&gt; I genuinely believe Stackage maintainers have the community's best interests at heart. &gt; I don't believe these actions are made in malice. It's not one or the other, and I don't believe either is accurate: the former is naively charitable, and the latter is too pessimistic. They're just respecting their own interests, nothing more or less. The way to effect change is to figure out a way to make this fork backfire and have big costs; if it becomes apparent that that's what would happen, the fork won't happen, simple as that. &gt; I don't think it's fair to expect me to be quiet on the issue. By all means, speak on! This fork is not in my interest, either, so I'm against it.
For [similar reasons](https://github.com/snoyberg/snoyman.com-content/blob/a796432a685907c8017eb84c9062c20cf139367b/posts/haskell-org-evil-cabal.md) they forked http://haskell.org to http://haskell-lang.org : To have something they control and can freely implement their quite opinionated vision for Haskell.
From what I've seen, the imputation of bad intentions is much stronger the other way. Stackage maintainers pretty frequently launch attacks on Hackage maintainers with direct accusations of malice. Stackage maintainers are not a protected class. They're not free from criticism about bad decisions.
While naming and ignores can certainly be abused, I find that one-letter variable names and ignores actually are documentation that gives me important information. to your example: * single letter function name - should only be used locally (not top level) to name a function where the implementation is simpler than the name * ignores in the first definition - only the third argument is relevant. this is probably something we pass to a higher order function where we don't need it's full power * c - the implementation doesn't take advantage of any specific properties or values of what is passed. A bit odd considering the signature is Int instead of *, this must be a specialization for performance. * definitions 2 and 3 - poor examples since they will never be reached. note we can easily tell this from glancing at definition 1 * g a b c = - this would be a better example and is probably the most likely to have been abused. This should say that the implementation does not care about any properties or values of a, b, or c, and does not care about the order they are passed. For example a+b+c or a*b*c are okay definitions but a*b+c is not. If any of the above are not true then the code has a bug in either the implementation or the naming.
IMHO Scala is like C# done right (_much_ less features, but more powerful ones), except with a way more functional orientation. The language has its complexities, but for a functional programmer to dismiss it over C# would be a total absurdity.
Kotlin has an ersatz of pattern matching (much less powerful), dumbed down not to scare Java developers. Quoting [the forum](https://discuss.kotlinlang.org/t/destructuring-in-when/2391/2): &gt; You basically want `when` to do the full blown pattern matching that Scala offers, but that is one of the features that JB decided was too complex (and probably really hard to implement).
What overhead? The overhead of changing a single line of code in hackage is *apparently* gigantic. The proof being that nobody wants to add features to hackage, but lots of work is being done to *rearchitect* it? Why? That's not getting us anywhere compared to using real REST (i.e. a server that responds with links to *other resources not necessarily on hackage*) and microservices. How many years of refactoring will we need until hackage can list reverse dependencies, add comments, or list which stackage versions are being used? Those are 10-line fixes in javascript.
No information whatsoever on the web page about how coins are distributed, how the ICO was setup? Scam!
&gt; * Dry(ing) ecosystem: &gt; * Non-existant library ecosystem and tooling How do you reconcile this with the fact it has [more active Github repositories than Haskell, and is growing faster](http://githut.info/)? &gt; * Chaotic: &gt; * No referential transparency (i.e. arbitrary effects anywhere) Haskell (and derivatives) is the only semi-mainstream language to track all effects, so you're going to have a hard time finding something that does not have this bullet point. &gt; * High dev cognitive overhead (i.e. you can't trust APIs nor the JVM) This I more or less agree with (but not really for the stated reasons). &gt; * Unstable runtime What does that mean? The JVM is probably the most stable managed runtime out there. &gt; * Typically code length is 3x longer than haskell Probably accurate. Scala does not shine on this point. &gt; * Poor FP ergonomics with no currying / partially applied functions Currying is awkward (and looks different for methods), but it's there. &gt; val f: Int =&gt; String =&gt; String = n =&gt; str =&gt; str * n f: Int =&gt; String =&gt; String = &lt;function1&gt; &gt; val g = f(3) g: String =&gt; String = &lt;function1&gt; &gt; def f(n:Int)(str:String) = str * n defined function f &gt; f(3) _ res3: String =&gt; String = &lt;function1&gt; &gt; * Incorrect core philosophy: I guess this is more on the subjective side.
I don't want to enter a protracted discussion on this but 1. My scepticism about microservices is unrelated to whether or not I actually like Hackage as it stands. 2. I don't accept your comparison between something that actually exists and an idealised version of something that might conceivably exist after a lot of effort. 
Helping to write documentation is super useful! I'm hoping to finish up a proposal for a conference which is a call to action to help document more :) 
I think what you are describing exists! Attribute Grammars. I don't have time to talk more about it now but essentially it is exactly what you are looking for. Have a look at this: https://www.semanticscholar.org/paper/Typing-Haskell-with-an-Attribute-Grammar-Dijkstra-Swierstra/30ec129dda28ce1bffeeddedb8e102b3bba723c7 And it exists: http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarSystem
Haskell has a proper module system. It doesn't need the microservice workaround just to enforce programmer discipline on modularity.
Will the formalization inspires how to improve Cardno performance in transactions per seconds?
I think this should work: waitAnyN :: Int -&gt; [Async a] -&gt; IO [(Async a, a)] waitAnyN = go [] where go :: [(Async a, a)] -&gt; Int -&gt; [Async a] -&gt; IO [(Async a, a)] go acc m xs | m &lt;= length acc = return acc | otherwise = do res@(match, _) &lt;- waitAny xs go (res:acc) (m - 1) (delete match xs) 
Why is the information on www.stackage.org not included in the hackage server?
It can make sense, because it is not a function (data constructor) application, ie. `{setter}` expression is not a value.
Or, even better, let's put all packages into a blockchain! 
I don't think ghc-mod supports GHC 8.2 (which LTS 10 uses) yet, but looks like it is very close! https://github.com/DanielG/ghc-mod/issues/900#issuecomment-359293054 As a shameless self-plug, I would suggest checking out [SpaceNeovim](https://github.com/Tehnix/spaceneovim), which has a [haskell-layer](https://github.com/Tehnix/spaceneovim-layers/tree/master/layers/%2Blang/haskell) that sets up a nice enough Haskell environment. You can also choose between a couple of backends, I'd suggest running `both`, which tries to mix together ghc-mod and intero. Otherwise, I personally, other than using SpaceNeovim, also use VSCode heavily with [hie](https://github.com/haskell/haskell-ide-engine), which works great!
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Tehnix/spaceneovim-layers/.../**haskell** (master → 91ac1a3)](https://github.com/Tehnix/spaceneovim-layers/tree/91ac1a3adec8738c053a0e228f759a8676cebf84/layers/%2Blang/haskell) ---- 
I'd say "exhaustive instance checking" is more accurate, since it does not actually have "pattern matching."
Thanks. I fogot that `Async` a is Ord and Eq. XD
I really think these are nice, and I've already linked them to a few people. Here is a nitpick though: I can really hear some "mouth sounds" whenever you talk and they are(to me at least) really offputting. 
In general - not in the context of Yesod - it seems to me that often more traditional-style documentation is often needed only because people coming from other languages aren't yet accustomed to the wealth of information you get from a Haskell type signature and a few short and strategic haddock comments. In Yesod and similar libraries, the traditional documentation is more crucial. The types are often very abstract and obtuse, so it takes a lot more work to get useful information out of them. And the use of TH sometimes hides some of the types you really want to see. I'm not complaining about those things at all - both the abstract types and the TH are there for very good reasons, and they contribute to the greatness of Yesod as a web framework. I'm just pointing out that the issues raised by OP are more pronounced because of that.
Because `v { x = a, ... }` is a single expression. You can't split it further down so 
Personally, I always thinking that formalization is about _correctness_, not _performance_.
&gt; Why is the information on www.stackage.org not included in the hackage server? Because of the conflict this whole post is about. There is no technical reason.
The PVP itself is alright, it is the pokes for "fixing" the dependencies of your packages that are downright annoying (especially for very minor packages). It is a lot of work to get this right too. 
(Also the PVP is not easy to get right. Everything is leaky as hell, so it is really hard to know when you should do a major version bump.)
&gt; Yes, that means I can't imagine that being the real reason in the other direction, either. It is not just a recommendation. My personal experience : having issues opened on github for "buildmatrix" build failures, for really minor packages. These issues were *not* opened by a user of these packages!
Kotlin does have pattern matching (e.g. on integers): when(10) { 0 -&gt; "too low" in 1..9 -&gt; "one digit" in 10..99 -&gt; "two digit" in 100..999 -&gt; "three digit" else -&gt; "too high" } The only difference to Haskell/Scala is, that Kotlin offers only basic pattern matching.
With "pokes", you mean a Hackage trustee making either a minor revision of your package and/or leaving a pull request on your repository? I have to admit that I did not find that annoying, the minor revision did not create any work for me, and the pull request was easy enough to incorporate.
Now I see it. Thank you for telling me!
Vim LSP with HIE and enhanced Syntax Highlighting together with tmux and repl in other window... its the best!
I had, adding a mapping from an executable name to a system package, which fixed things on my computer. The problem was with a complex (but reliable) custom setup that was (artificially?) failing when build-tool / extra-libraries were explicitly specified (in the .cabal). thanks, i'll try modifying the haskell overrides.
You're right the UI isn't much different, in that it is still not designed to be very pretty, but rather to be functional. But there is a _lot_ more information -- including rendering of readmes, links to changelogs, build status information, improved haddock rendering, browse-within-source, a system of maintainership, a more flag-aware way of rendering most metadata, etc. The main browsing interface is also now table-based instead of the big list, etc. The main issue impeding much visual change is that it is not clear that there is a good list of things to add to any given page that people can agree would be unilaterally better to have. Suggestions are welcome! On your part I see one request here -- which is that you want to be able to browse a _specific_ lts release and see all packages in it, correct? That seems reasonable enough to file a ticket for. As the `distros` mechanism isn't widely used, there hasn't been much UI gloss put on it. As far as REST, hackage provides a much more extensive API than most people know (even if some bits are underdocumented). In fact basically any page you can access as html also has an associated json representation: http://hackage.haskell.org/api . So it is very easy to add microservices on top of hackage if anyone wants to write them.
If you go to a page for a package that is in stackage and follow the `stackage` link, you literally arrive at the haddocks you're asking for. If you're asking why that's not the main way to get haddocks -- well, there are vastly more packages that _aren't_ in stackage and _don't_ have haddocks, so that mechanism isn't very universal, for one...
With pleasure.
Can somebody please explain in what way hackage artifacts are "mutable". If referring to the fact that revision metadata can added, I think calling it "mutability" is misleading. Also isn't that a red herring as stack already uses frozen revision metadata? Excuse my ignorance, I would just like to understand why this division is necessary as I'm fairly new to haskell and find it pretty disheartening.
[Here is an example](https://github.com/bartavelle/compactmap/issues/1), and [another](https://github.com/bartavelle/pcre-utils/issues/2). I tried to be helpful and fix the "problems", but: * These are not real users problems, but made-up problems. They might have been important to /u/phadej at the time, but I felt fixing them was busywork. * I couldn't care less about supporting GHC 7.4 in 2016. * One of my packages was altered, and I was not even told. * The tone was wrong (*Please specify lower and upper bounds to comply with PVP* sounds a lot like [this](https://www.youtube.com/watch?v=Hzlt7IbTp6M)). I am pretty sure there is a third interaction of the same kind (much better, I remember a PR), but I cannot find it now. I understand this is for the common good, but at the time it felt overkill, and really brushed me the wrong way.
No, you're pretty much right. Hackage *isn't* immutable. It's just that `Cabal`-the-library sort of treats it as immutable by applying revisions on its own. This previously caused Stack some pain, but has long since been totally resolved by Stackage pinning revisions.
`nix-overrides.nix` doesn't exist. Do you mean `configuration-nix.nix`? Because that is decidedly *not* where you should put stuff like this. That file is exclusively for fixing up assumptions that Nix alone breaks; e.g. patching LFH paths. It is not for mutating the dependency semantics of packages.
Why not just press the "close" button on the issue? I mean, it's not exactly an inalienable right to post something on Hackage; what's wrong with saying that, as a matter of policy, package maintainers on Hackage are to deal with (and by "deal with" I mean "receive a GitHub notification and press the close button on the issue") the occasional build matrix failure-related GitHub issue? Many times I've daydreamed about the feasibility of making backwards-incompatible changes to languages/tools by writing a bot that automatically detects, fixes, and PRs every GitHub repository using code the old way. Your seemingly fragile attitude towards easily ignored issues/PRs makes me wonder how many people would feel angry at such a bot. I mean, I guess I can understand being annoyed if you're getting _tons_ of these issues, but in your other comment you only listed two! I'm genuinely confused by the feelings you've expressed on this matter, and would be interested to hear you elaborate on why you are so seemingly disturbed by something so inconsequential.
You can solve that by allowing the user to specify a SHA256 hash of the downloaded Git repo (and if you ignore SHA1 collisions, then it suffices to allow the user to specify the desired git revision hash).
`intero` (and `ghci`, too) use a ton of memory as the codebase gets larger. Not a problem on smaller projects. 
thanks! So, I shouldn't add my override to: https://github.com/NixOS/nixpkgs/blob/ec0c4802ae89dccfbff1060fd411b46113e87671/pkgs/development/haskell-modules/configuration-nix.nix right? To be clear, it's not something nix breaks. It's a workaround until someone with more experience in nix and/or cabal and/or autotools can fix this: https://github.com/deech/fltkhs/issues/16 my custom nix file just adds: setupHaskellDepends = [ autoconf c2hs ... ]; libraryToolDepends = [ autoconf c2hs ]; which can be done as an override with `addBuild&lt;etc&gt;`. Do you know where in nixpkgs I can/should put those modifications? 
&gt; I'm genuinely confused by the feelings you've expressed on this matter Your seemingly fragile attitude towards easily ignored Reddit posts makes me wonder how many people would feel compelled telling me I should not feel (especially when I already did not). But don't worry, you do not have to feel my feelings, let's hope you will not be confused for too long! More seriously, I suppose you are referring to [this comment](https://www.reddit.com/r/haskell/comments/7s58id/slurp_a_single_liberal_unified_registry_of/dt4bgr1/?context=3). I am not a native English speaker, but I do not believe that being polite while "being brushed the wrong way" qualifies as being "so seemingly disturbed". &gt; what's wrong with saying that, as a matter of policy, package maintainers on Hackage are to deal with [..] the occasional build matrix failure-related GitHub issue? Nothing is wrong with that, because this is a straw-man. Being noticed about built failures was not my complain.
It seems like y'all are jumping to some pretty conclusions about even what such a "fork" would look like. Now this is my personal take on it, not acting as a representative of others who work on stack. Here are the points I think separate storage for stack would have: 1. Superset of what's on hackage, updated with hackage changes as fast as possible. 2. You wouldn't be able to add a package to the stack package set without also adding it to hackage. This might not be a firm rule if hackage's restrictions on uploads get stricter. As is, I see nothing wrong with having this rule. So yeah, the fork I imagine does not even making a new namespace. People are freaking out for no good reason...
This is my POC crafted during the reading A La Carte paper: https://github.com/nlinker/playground/tree/master/haskell/a-la-carte HTH
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [nlinker/playground/.../**a-la-carte** (master → d93f2a5)](https://github.com/nlinker/playground/tree/d93f2a5b21a3739c2ada9c7f7f230c981a4f5ad8/haskell/a-la-carte) ---- 
hmm, one of the reasons listed in the configuration.nix is &gt; passing native libraries that are not detected correctly by cabal2nix and (my sibling comment) half-counts, so I'll try it.
Thanks to /u/spirosboosalis the [newest version of FLTKHS](https://hackage.haskell.org/package/fltkhs) has basic [Nix support](https://github.com/deech/fltkhs/blob/master/default.nix).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [deech/fltkhs/.../**default.nix** (master → cdc29e2)](https://github.com/deech/fltkhs/blob/cdc29e2b0227d95628ec55dd864948d73a0ed71c/default.nix) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dt4jl9d.)
I did not mean to impugn the politeness of your behavior in the GitHub issues, but both of the comments you made prior to mine held an IMO exaggerated and emotional tone that surprised me given how inconsequential the subject is. Personally I'm of the opinion that it would be desirable for things like GitHub issues to be written in the straightforward technical way phadej wrote his, without constantly sprinkling in words for politeness (and keep in mind that I'm _not_ saying that it's okay to be abusive or mean in a GitHub issue). At the end of the day it's waste of both the reader and writer's time to include these extra words, and given that I have a finite number of seconds in my life, I consider it a waste of my time to get worked up over such unimportant things. When _I_ write GitHub issues on other projects, I try to be as polite as possible, because I want to maximize the probability that the maintainer deals with my problem, but I would never dream of getting mad at someone for not returning the favor. &gt; It is not just a recommendation. My personal experience : having issues opened on github for "buildmatrix" build failures, for really minor packages. These issues were not opened by a user of these packages! &gt; Nothing is wrong with that, because this is a straw-man. Being noticed about built failures was not my complain. These two quotes seem quite inconsistent with one another.
It has indeed been a problem in the past that rev-changes haven't always been communicated well, although since then there are policies to try to make things better. But an ever better thing than policies is automation. There is a 90% done PR to hackage server for _automating_ such notifications. https://github.com/haskell/hackage-server/pull/622 It needs a UI for opt-in/out and it would be great if someone jumped in to help take it over the finish line.
&gt; We could have instances of Aeson's FromJSON and ToJSON typeclasses for a generic record type^1 like Rec from vinyl. This is useful because most of the time, when you wrap an API that uses JSON, you want two different "levels" of types; one level is a straightforward translation of the JSON format described in the API documentation (which requires comparatively little effort to completely wrap the API), and the second level is a more high-level Haskell-appropriate translation of those types (which is almost never an up-to-date complete description of the API). Exactly. That was my yesterday. And further, I wanted to parse the low level loosely typed json, just to explore the data (and run folds like min/max, the set of unique chars, etc), to know enough about it to pick the right types (like Double or Natural or an Enum). `composite-aeson`, and `schematic` (which also exports `json-schema`), are decent, but no library can match records as a language feature. https://hackage.haskell.org/package/composite-aeson https://hackage.haskell.org/package/schematic http://json-schema.org 
&gt; People are freaking out for no good reason... People are freaking out because we had none of this information. We have no idea what this fork entails. How is it surprising that people assumed the worst? People are unwilling to judge Slurp until we have the ability to judge the fork. That's totally fair.
I just installed haskell-ide-engine. It seems to be working so far. call plug#begin('~/.vim/plugged') Plug 'prabirshrestha/async.vim' Plug 'prabirshrestha/vim-lsp' Plug 'prabirshrestha/asyncomplete.vim' Plug 'prabirshrestha/asyncomplete-lsp.vim' call plug#end() " Autocomplete settings inoremap &lt;expr&gt; &lt;Tab&gt; pumvisible() ? "\&lt;C-n&gt;" : "\&lt;Tab&gt;" inoremap &lt;expr&gt; &lt;S-Tab&gt; pumvisible() ? "\&lt;C-p&gt;" : "\&lt;S-Tab&gt;" inoremap &lt;expr&gt; &lt;cr&gt; pumvisible() ? "\&lt;C-y&gt;" : "\&lt;cr&gt;" imap &lt;c-space&gt; &lt;Plug&gt;(asyncomplete_force_refresh) if executable('hie') au User lsp_setup call lsp#register_server({ \ 'name': 'hie', \ 'cmd': {server_info-&gt;['hie', '--lsp']}, \ 'whitelist': ['haskell'], \ }) endif
&gt; curated tested sets of git revisions You can't `push -f` over the same revision, unless you find a [sha1 collision](https://github.com/blog/2338-sha-1-collision-detection-on-github-com) :)
What about scotty or ysond?
Go to university and learn hard core mathematics such as Logic, type theory, category theory, algebra, linear algebra, etc.
From where I was sitting I can imagine I have a very different view of how the cassava thing played out. To me it looked like: * there was a change made in cassava * the change adversely effected stack * I couldn't see any attempts at understanding why the change was made * I saw a lot of accusations of the change being a "bad change" made for "no good reason" and that it was aimed at breaking stack, along with plenty of internet drama * somewhere in there someone forked cassava but with different capitalisation It left me pretty glad that I don't have anything upstream of stack. Imagine I was a package author and one of my changes broke something for two downstream users. I know I'm personally going to respond differently to the user who tries to understand where the change came from and is looking for ways we can both get what we want and to the user who goes on the offensive against me. In that scenario there are probably also a few users who didn't contact me at all but instead just implemented workarounds. Maybe some of them keep an eye on the repos for their upstream dependencies and had a fix ready before I even made my release. That little imagined scenario captures most of what I've experienced with open source development and the relationship between upstream and downstream. Maybe I've been living a sheltered life for all of this time (although I've had a couple of full time gigs doing open source over the years). To me it looked like there was a lack of engagement and a "do what I say!" attitude in the approach to the cassava issue. It's ironic that seems to have precipitated something similar at the package set level over on the linked github issue, where anonymous parties are threatening a fork if they don't get what they want but are also refusing to be part of the discussion. Just thought I'd share my perspective.
In the medium-long term Hackage and Stackage are doomed. They need centralization and central administration because they don't leave to the author the responsibility and don't give to them a location resolution for distributed resources with strong names, like URL's. IPFS would make this even more trivial to distribute and more strong naming
&gt; in that it is still not designed to be very pretty, but rather to be functional. sorry but I don't think it's functional.. - It will not remember who I am, unlike 99.999% of websites on the internet. using a cookie (yes I know, it's great for privacy, but ...) - It has no clue what set of packages I usually visit. - There's no autocomplete in any field AFAIK. - There's no "what's new" relevant *to me* (because .. no cookies). - There's no modern login (oauth, a simple auth0 redirect would suffice). Case in point, I've been using hackage for at least 10 years, but couldn't bother to create an account. IT'S JUST TOO ANNOYING. I'm not proud of this, but that's just the way it is.. I'll probably get an account after writing this comment, but... you know. - There's no efficient way to navigate using the keyboard. Whyyyyyy???? Why do I have to *click on the f%@&amp;ing search window to search?* - No "see also" or useful way for me to guage quality (I know it's being worked on, but without modern login - what's the % that will use this?). Take any random "bootstrap" site made in your favorite javascript framework and they will have all of the above. Either out-of-the-box or with *very minimal effort*. You list a lot of things I didn't know. I hadn't actually seen the table-based navigation - the tags just loaded too slowly and it's been useless for years so I've mostly learned to avoid it at all costs. Same with the search functionality (which isn't such a bit issue as using google is fine). The fact that everything is available as an API is *very good*. I might consider retracting parts of what I said given this. I should probably sit down and write a SPA in ghcjs that is usable (of course - will never happen - it's easier to be angry at the world from my armchair).
&gt; The fact is, that right now stackage is dependent on the good will of the authors of the packages that constitute it. Yea. That's how having dependencies fucking works. Why do you think GHC tries so hard to avoid external dependencies? They won't even depend on LLVM, which is probably far more well-supported than cassava. When an upstream package breaks your work, you are not their priority, and you are not entitled to the fix you want. Your options are: 1) Cooperate (which neither side did a good job of, but the nature of "upstream" puts the burden more on you) 2) Don't use that thing anymore. Preferably, cooperate, to improve the state of the art. Causing a silly capitalization clash was not one of the options.
If you had those requirements, it's not clear to my why Nix wouldn't solve the problem. Granted, Nix could do with some UX improvements, more documentation, and I can't wait for a type system. The will is certainly there, but they seem like they could use more resources. If I had enough time to write / run / maintain a new package repository, I'd probably have a long think about how much I could improve Nix with those resources. Maybe I've just got a bit of a crush on Nix :) 
Note that "Replacing Hackage" is not part of the actual title. And u/snoyjerk is not u/snoyberg. 
Can we please use the actual title instead of trying to editorialize it in an attempt invoke negative reactions?
I think it would be, this issue is already heated enough as it is, we don't need the extra fuel on the fire.
This is another really unfortunate interaction between Haskell's numeric hierarchy and the realities of IEEE floating point. Everything that converts floats to integers (or Rational, for that matter) behaves a bit oddly around infinities and NaNs.
&gt; Building with something like Nix will guarantee determinism in the first two bullets. And the latter two, if you use NixOS and NixOPs, respectively. As for the general premise, yea hashing is pretty critical to Nix. I think it'd be great if Stackage resolvers included hashes for all the sources they make available, and if stack supported letting the user provide a hash for the resolver as a whole. &gt; In the next post, we'll discuss how to create a storage system that can provide downloads of packages, package metadata, and snapshot definitions. Stay tuned! Why does this need a special system? Nix gets by without such a system just fine, and it's not even that complicated. Content addressable storage would be a nice improvement, but it's not necessary, and it's literally never gotten in my way with Nix + Haskell.
Yea I'd be in favor of replacing the post, with OP's consent.
I think the /r/Haskell moderators should do this.
Yea I'd be in favor of replacing the post. Would be nice to have OP's consent and let him be the one to repost it, just to avoid the precedent of stealing posts from users. But that's not critical.
As the author of the fork you mentioned, I feel obligated to respond. I did attempt to understand why Herbert changed the flag name in Cassava. If you look at [my PR for reverting the flag name](https://github.com/haskell-hvr/cassava/pull/155), you'll see links to many different issues. You'll also see this exchange: - Me: Why is it important that the flag name be like this? - Herbert: Let me turn the burden of proof around, and ask why does it take so long for Stack to make a point release [...] - Me: Regardless, that doesn't explain why this flag name has to be the way it is. What was wrong with the old flag (or my proposed flag)? - Herbert: [no response] For more detail on this whole Cassava flag name ordeal, check out [this other comment thread](https://www.reddit.com/r/haskell/comments/7kt36c/lts_stackage_with_ghc822_released_today/drhiwvg/?context=3). 
I reposted this because the [original discussion](https://www.reddit.com/r/haskell/comments/7shikr/replacing_hackage_hash_based_package_downloads/) had an unnecessarily inflammatory title. cc u/mutantmell_ u/Athas u/ElvishJerricco u/snoyjerk
I think a PR to change our auth mechanisms would be very welcome. We've stuck with http-auth because it is simple, and it does remember you within the lifetime of the run of a browser. However, it does make some things more difficult. We've actually just been discussing oauth! That said, autocomplete or relevance-based what's new would be great. The former seems sort of straightforward if you could spec out what you wanted. The latter seems hard in that I don't know how to figure that out. But the recent rss feed gives something that can be hooked into! (http://hackage.haskell.org/packages/recent.rss) You can navigate with tab, just like in any website. In my experience, three tabs in a row will bring your cursor to the search window. I wouldn't necessarily object to some JS to add shortcuts, but I might actually find it sort of confusing myself. Search is also way better than you remember (as is tag based browsing -- e.g. https://hackage.haskell.org/packages/tag/bioinformatics) -- give it a try!
This constant mischaracterisation of package revisions is tiresome and can no longer be classified as ignorance. 
This post hardly mentions revisions. It's as much about immutable snapshots as it is about immutable packages. 
That is actually true. Thanks for picking me up.
If you ignore the namespace stuff, and just let stackage have a patch overlay on hackage for tweaking things to work together, I don't think anyone would have a problem with that! As has been pointed out elsewhere, this happens to haskell packages when they go into distros like `debian` or whatever, and there's also prior art in terms of head.hackage and mobilehaskell overlays, etc. It seems the ability to do this sort of overlay is really part of the design of things from the get go...
I did not realize those were trustees. I thought their role was to help claiming abandoned packages. That clears things up. 
At [ocharles.org.uk](https://ocharles.org.uk/blog/ ) there are several 24-days-of guides on different aspects of Haskell which are excellent reads. [http://www.parsonsmatt.org] had a number of interesting articles too. A sort of professional Haskell by Example guide is given by [What I wish I knew when Learning Haskell](http://dev.stephendiehl.com/hask/) which goes beyond the basics into practicalities like records, logging, concurrency etc
It's no big deal.
Yeah. Maybe next time just don't editorialize the title. 
Here's a sneak peek of /r/ProgrammerCirclejerk using the [top posts](https://np.reddit.com/r/ProgrammerCirclejerk/top/?sort=top&amp;t=all) of all time! \#1: [Hello world! xpost from /r/ProgrammerHumor](https://i.redd.it/ur9t0ag8jw6y.png) | [2 comments](https://np.reddit.com/r/ProgrammerCirclejerk/comments/5laws7/hello_world_xpost_from_rprogrammerhumor/) \#2: [DAE think JavaScript is the worst???](https://np.reddit.com/r/ProgrammerHumor/comments/5mlyer/we_hired_a_new_developer_this_week/) | [2 comments](https://np.reddit.com/r/ProgrammerCirclejerk/comments/5mo855/dae_think_javascript_is_the_worst/) \#3: [Can we all take a moment to hate on PHP?](https://np.reddit.com/r/ProgrammerCirclejerk/comments/5ldnwv/can_we_all_take_a_moment_to_hate_on_php/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/7o7jnj/blacklist/)
IPFS is certainly interesting, I enjoyed Gabriel's use of it in Dhal. I think a distributed content addressable storage service like that is a really attractive idea for Haskell. Of course, content addressable storage means that there isn't one source of truth but any number of mirrors (HTTPS or otherwise) is fine. So a plain old list of S3 HTTPS servers *or* IPFS both work nicely. From a regulation stand-point this is good, but this is becoming essential to me from a day to day stand-point. I don't have the patience anymore to deal with musical chairs software.
Putting them in ipfs seems pretty good to me, actually.
Is this homework? There's a simpler way to achieve what you're trying to do: take a look at the [`filter`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#v:filter) and try to combine that with the `length` function. you might also want to multiply `a`,`b`,and `c` by 2 before comparing it to `m = a + b + c`. Please keep asking if you get stuck, we won't think poorly of you! 
It would have to either be arbitrary, or throw an exception, right? There's no sensible value for it to give.
... Username checks out, I guess...
&gt; Facing the wrath of Stackage maintainers would very easily convince me to stop contributing to Haskell. I'm aware of some pretty long time members of the community distancing themselves because of this very issue.
A lambda introduces a new variable in the context. If above the binder, the type of variables is `a`, then under the binder, it is data S a = S a | Z The `Lam` constructor might then look like: Lam :: StaticTree x y (S a) -&gt; StaticTree x (w -&gt; y) a A typed term `t :: StaticTree x y z` can be read as "in the context `z`, `t` represents a term of type `y`". With that way of reading that GADT, the type constructor `S` should probably carry the type of variables as well.
this is called "Persistence"
NB: This could be a terrible idea: Shower thought: Has anyone proposed using DNS as an index for packages? What if the root was, say, "pkg.haskell.org", and when a person registers a package they will get, "bytestring.pkg.haskell.org". Package metadata would be stored in TXT records. Hell, you could even have an A record point to the user's website or github repository. Maybe you would have something like deps.bytestring.haskell.org return multiple TXT records: one for each dependency. Just spit-balling here.
What have you experienced that has caused you to use the word, "wrath"?
&gt; Why? If you need a new kind of proof, you are proving a new logical property - i.e. a new type. What's wrong with it is that there's a bunch of code that doesn't care about the new property. I can do all sorts of random operations (let's choose something simple but non-trivial, such as computing the list of all partial sums up to the point where the sum is at least 100) on any list of numbers, regardless of whether it's infinite, finite, known to be non-empty, known to have even length, or whatever property you like. The result may or may not have some or all of those properties itself. It may even depend on the element type! If the list is of natural numbers (the original math kind, excluding 0), then the result is finite regardless of the finiteness of the input; for most other types, that's not true. Now, what does this look like when you've got five or six varieties of list type depending on which properties you want to structurally guarantee? As far as I can tell, it looks messy. (If you see a clean way to implement this function, I'd love to see it!) And this wasn't even a complicated example.
I was under the impression that /u/snoyberg and /u/snoyjerk were the same person. I don't actually know why I believe this, though.
Please just ban snoyjerk. He only exists to antagonize the sub.
Important clarification - thanks!
Because he posts everything that snoyberg writes.
I really wish Haskell hadn't tried to fit floating points into the parts of the numeric hierarchy they don't belong. Addition, multiplication, division, subtraction, and various conversions to and from other types do not obey many laws. So IMO the numeric tower should be based around integers and rationals and matrices and vertices and so on. Floats should then have their own class or two for various floating point operations and not be a member of most of the main classes (e.g. Not `Enum` or `Ord` or `Semiring`).
I've said it before and I'll say it again: it was a terrible mistake to make `Double` and `Float` instances of `Num`, it was a terrible mistake to make `RealFloat` a subclass of *anything*, and as everyone already knows, the design of `RealFloat` has several weaknesses of its own.
Yes, exactly this.
I've reported you for gaslighting
&gt; If you had those requirements, it's not clear to my why Nix wouldn't solve the problem. The actual requirement is that they need the Haskell infrastructure to be an advertisement for their company.
You might be able to get some inspiration from this snippet I wrote up a couple years ago: [Typed embedding of STLC into Haskell](https://gergo.erdi.hu/blog/2015-02-05-typed_embedding_of_stlc_into_haskell/)
Ah, that's why they're called persistent!
I'm pretty sure that, on a /r/programmingcirclejerk thread, I said that.
Some of our customers _do_ use Nix to get these guarantees. I'm definitely not trying to imply otherwise. In fact, I tried to point out that Nix excels at this, _and_ tries to give stronger guarantees (byte-for-byte identical build results). But there will be build systems besides Nix, and even for Nix the principles I'm laying out here apply. It just so happens that Nix has already implemented such a system and has nothing at all to learn from what I've written.
&gt; I just don't have the patience anymore to deal with musical chairs software. Oh boo hoo. So you have to play nice with he other kids in the sandbox. Welcome to open source. Thanks for pissing in the sand.
You've not addressed the fact that clearly the way you introduced the discussion was really bad, and at least partially responsible for the response you got.
Ideally I think code would not be written hardcoding a specific numeric type anyway -- you should just say `(Field a) =&gt; ...` and then you can plug in whatever implementation (law-abiding or IEEE) later. Having a more sensibly defined numeric hierarchy would be nice, too. Part of the difficulty tho is that Haskell (unlike, say, Rust) does *not* guarantee inlining of typeclass dictionaries, which means that you very well could end up paying a cost for using `Field a` instead of hardcoding `Float32`.
Sure. I'm saying that the amount of time it would take me to do this in Haskell today is far, far greater than the amount of time it would take me to implement the function once for `[a]`, and then verify a number of uses. In Haskell, some partial functions are justified by there not being an alternative with a reasonable ratio of safety to complexity. (I believe this is also true of Coq; but there, the answer is typically to choose not to use Coq for the project.)
I might have to try that out sometime soon. I use spacemacs currently and it makes me sad how lsp is lacking in emacs currently (something I thought it would adopt very quickly and eagerly...)
I think people mean many things when talking about immutability. What I tend to mean is the massive reduction in complexity you get by not having the semantics attach an extra arbitrary term representing the effects of time passing to the result of each evaluation. Those extra terms destroy algebraic/equational reasoning, because two different expressions can never *really* be equal. Copying and compiler optimizations are important, but they are implementation details, so that puts them strictly lower on my list of priorities.
&gt; FYI, between writing this blog post and this comment objecting to the change, I already merged the changes into the 1.6 branch based on general support for the change. I'm hoping my explanations below will set aside some of your concerns. Ah. I still have concerns, but I don't understand all of the details yet. Maybe you can help clarify. &gt; &gt; Just that there would be more of a learning curve for Yesod users. They'd need to remember what HandlerFor does in addition to HandlerT (Is there an additional type alias for subsite? I didn't see this in the diff, but I image there would be something like type SubHandler sub master a = ReaderT (SubsiteData sub master) (HandlerFor master) a). &gt; &gt; That's not the case. Users who write subsites will need to be aware of something else, in theory. In practice though, even those users can just program to the MonadSubHandler typeclass and avoid the concrete types, much the way all of the functions today in yesod-core use MonadHandler. I've started looking at the diff in more detail. It looks like `HandlerFor` is a customized `ReaderT (HandlerData app) IO` while subsites are actual `ReaderT`s of `SubsiteData`. Is there any reason for this asymmetry? Why not have them both be `ReaderT`s? They could both be newtype wrapped and many instances like Monad, MonadIO, etc could automatically be derived. Is the main improvement that subsites no longer need to duplicate everything in `HandlerData` in their `ReaderT`s? &gt; &gt; If these functions need to interact with an IORef, maybe a MonadIO instance requirement is sufficient? &gt; &gt; If your goal is to write funnyLift :: MonadIO m =&gt; HandlerT app IO a -&gt; HandlerT app m a, it can definitely be done. I think it goes something like: &gt; &gt; funnyLift (HandlerT f) = HandlerT $ liftIO . f &gt; In the post-HandlerFor world, you'd still be able to implement a transformer stack and create a MonadHandler instance for it, but I don't know what the motivation is for having the generalized base monad. My goal was to write handlers with type `HandlerT app LIO a`. I previously couldn't get this working, but maybe something like funnyLift would've made it possible. I ended up rewriting `LIO` as a monad transformer (`LMonadT`), but this allows cheating since the handler is not required to run in `LMonadT` (you end up having to trust that each handler will run in `LMonadT`). Either way, this use case probably doesn't apply for most users. &gt; If you point out the code implementing your subsite, I'd be happy to take a look and see what changing it to the HandlerFor approach would look like. I'll grant you access to my repositories if you send me your bitbucket username. I doubt I'll get around to making them public for a while. Ps. I created a [PR](https://github.com/yesodweb/yesod/pull/1478) implementing a slight breaking change. Do you think this can make it into 1.6?
Is there a link to this proposal? This is the first I've heard of SLURP.
What's the process for amending the policy on retroactive changes to version constraints, flags, etc on released packages? Someone writes up the amendments and the community votes on accepting or rejecting it? It seems like metadata revisions is the main point of contention and a community wide resolution would hopefully put this behind us. 
&gt; -now I also using haskell-vim-now. It works perfectly for me on Debian, FreeBSD and NixOS. Regarding Installation on NixOS I faced an issue: https://github.com/begriffs/haskell-vim-now/issues/284
Here's the [previous thread](https://www.reddit.com/r/haskell/comments/7s58id/slurp_a_single_liberal_unified_registry_of/).
The framing of recent discussion made me feel like something irreversible was at risk of happening very soon. Doesn't actually look that way. That's great news in and of itself.
Bored, much?
I'm very sorry that my "notifications" sounded like automatic pre-recorded messages. It's not too far from the truth. And yes, I'm a Hackage trustee, I'll make that clear in my future interactions &gt; (Also the PVP is not easy to get right. Everything is leaky as hell, so it is really hard to know when you should do a major version bump.) &gt; *Breaking change.* If any entity was removed, or the types of any entities or the definitions of datatypes or classes were changed, or orphan instances were added or any instances were removed, then the new A.B **MUST** be greater than the previous A.B. Note that modifying imports or depending on a newer version of another package may cause extra orphan instances to be exported and thus force a major version change There's also an entry in PVP FAQ: https://pvp.haskell.org/faq/#what-implications-does-the-pvp-have-when-re-exporting-api-elements TL;DR use explicit export list and don't re-export modules from other packages. Then it's impossible (or at least very hard) to accidentally change your package API when your dependencies' API changes. If something is still unclear, I'll happy to discuss further. (FWIW: I'd like to relax *orphan* instances note, there's actually a ticket https://github.com/haskell/pvp/issues/3) &gt; It is not just a recommendation. My personal experience : having issues opened on github for "buildmatrix" build failures, for really minor packages. These issues were not opened by a user of these packages! I think that I run into `pcre-utils` and `compactmap` when you (or someone else) added them to Stackage. There are no minor package on Hackage as far as Hackage Trustee can tell, we care about all 12000+ packages (with their 100000+ versions!). &gt; These are not real users problems, but made-up problems. They might have been important to /u/phadej at the time, but I felt fixing them was busywork. I hope that comment was due a misunderstanding of me acting as a Trustee. And I was proactive as I noticed the problems. &gt; I couldn't care less about supporting GHC 7.4 in 2016. There is a way to communicate that: specify lower bounds (on `base`). If you don't test with GHC-7.8 nowadays, add `base &gt;= 4.8`. See https://ghc.haskell.org/trac/ghc/wiki/Commentary/Libraries/VersionHistory
Flags cannot be changed. https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md And specifically: https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md#will-these-rules-for-what-can-and-cant-be-revised-ever-change
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-infra/hackage-trustees/.../**revisions-information.md#will-these-rules-for-what-can-and-cant-be-revised-ever-change** (master → 5a30f96)](https://github.com/haskell-infra/hackage-trustees/blob/5a30f96e1bb1bdb1d71fba137cea149b1c197810/revisions-information.md#will-these-rules-for-what-can-and-cant-be-revised-ever-change) * [haskell-infra/hackage-trustees/.../**revisions-information.md** (master → 5a30f96)](https://github.com/haskell-infra/hackage-trustees/blob/5a30f96e1bb1bdb1d71fba137cea149b1c197810/revisions-information.md) ---- 
IEEE floating point does not constitute a field. Among other things, it lacks associativity of operations, and a multiplicative inverse. It's close, though, but it would not really be a lawful instance.
Thanks for this. I think having patched repos is a fine idea, as long as there is a clear purpose other than "avoid the Hackage trustees." In this case, such a purpose could be "curate changes specific to mass version pinning." This isn't unheard of; nixpkgs makes heavy use of `jailbreak-cabal` to patch packages at build time to support the package set better. And in fact, there are even [mobile](https://github.com/mobilehaskell/hackage-overlay) and [HEAD](https://github.com/hvr/head.hackage) Hackage *overlays*, which allow us to curate Hackage for the specific use cases of mobile development, and GHC HEAD development. So what's being proposed here isn't unheard of, and is actually quite accepted in the Hackage world. However, I think it's critical that this should use the same overlay style as mobile and HEAD. This overlaying infrastructure is something I'd like to see advanced; preferably to the point that Hackage+revisions is an overlay on the often proposed "Hackage-raw," and potentially to the point that nixpkgs patches are hosted in an overlay instead of nixpkgs. Producing a disjoint infrastructure for accomplishing the same thing would be very frustrating and would lead to more problems than it's worth. I'm very glad that there is no intention of producing a potentially disjoint package set. Ensuring that Stackage is always a subset of Hackage, modulo curations, is really important. Without this property, I think all the fear and panic in the previous threads was completely justified. So it's really good to actually be told that this property will be preserved. All that said, I will say that this whole issue has been disastrously handled up until now. There should NOT have been a SLURP proposal if *even the forker* wasn't interested in it; there *should* have been discussion about the fork before anything else; there *should not* be so much important communication occurring behind closed doors. So I really appreciate the forwardness and level-headed-ness of this response. Thanks guys. This really put my mind at ease.
Without jumping to any conclusions about who is responsible for the comments, I agree that this definitely needs a strong response. Everyone I've seen talk about the matter has been clear that Hackage recommends the PVP, but does not require it. It looks possible that we need some community education about that fact. Clear statements on the web and my figures in leadership could help make sure everyone is aware of the state of affairs.
&gt; The authors showed me the proposal before it was published, and I told them at that time I would not support it. I've also told them that, out of respect to them, I would hold back on commenting on SLURP. SLURP is approaching Windows Vista® levels of popularity. 😛
I wish Michael had come on the thread to say this two days ago. It would have saved a lot of redundant argument and suspicion. &gt; The authors showed me the proposal before it was published, and I told them at that time I would not support it. What on earth ...? It becomes even more incomprehensible to me why this proposal was published. &gt; many people rebelled against this public broadcast methodology, and I've switched to quieter communication channels. I think this is unfortunate, and I'd much rather talk openly and loudly about ecosystem plans Although it may initially seem to be disruptive I think that private communication on these key issues has proved to cause even *more* bad feeling. &gt; At least some PVP advocates have requested (or demanded) that package authors who will not follow the PVP do not upload their packages to Hackage. This must not happen. &gt; I've been told to stop using Hackage, full stop. This must not happen. &gt; I requested a change in official policy to guarantee that my usage of Hackage is allowed. I think this would be a very good idea. Many very eminent Haskellers do not want to follow the PVP and our infrustructure should respect and support that. (FWIW I *do* follow the PVP and think it's a good thing.) &gt; Am I holding a gun to someone's head? I think it's unfortunate such evocative language was used. &gt; To those who were truly terrified I was going to do something nefarious: I'm sorry to keep you waiting two days in an explanation. Thank you.
Sure, I just mean that you can write your logic in one place, then decide whether you want to run it in a strictly law-abiding way (Rational) or a fast, loose way (IEEE). Although with this approach you still end up at another difficulty with trig operations, since they're not actually computable, I don't think you can even have a law-abiding implementation. And dealing with them in the mathematically-rigorous (limits of sequences) way requires huge amounts of work that's just completely orthogonal to what programmers are trying to accomplish when using these functions. But at least in the non-trig cases (e.g., Field), I don't think there's too much impedance between the math and what programmers usually do.
From [here](http://hackage.haskell.org/upload). &gt; In order to ensure the integrity and well-functioning of the Hackage/Cabal ecosystem, all packages should follow Haskell's Package Versioning Policy (PVP). If someone lived under a rock in the past 20 years there is even a handy tootip hint upon placing the mouse cursor over *should*, which reads: &gt; [RFC2119] The word 'should' is intended to denote that there may exist valid reasons in particular circumstances to ignore a particular item, but the full implications MUST be understood and carefully weighed before choosing a different course. How clearer than this can you get?
[removed]
Ah, and I should have read the actual page instead of taken everyone's word for it. Implying the standards meaning of "should" is almost surely the wrong level of expectation. Someone who implements a standard (at least one that matters for compatibility) and chooses to ignore a "should" without a documented reason generally expects to be challenged or called out on it. But this clearly can NOT be the standard we apply to Hackage, if we truly want it to be a universal place for finding available Haskell packages. (And I think it's clear that we DO want that.) So now I think the first step is obviously (to me, anyway) to change the web site to say something like "Most Haskell packages follow the PVP, and we recommend you do. However, it's not a requirement to use Hackage." That would be a lot clearer.
&gt; &gt; Am I holding a gun to someone's head? &gt; I think it's unfortunate such evocative language was used. FWIW, I think it was inevitable given the completely information blackout we were in, and the implications present in the original proposal.
&gt; Then it's impossible (or at least very hard) to accidentally change your package API when your dependencies' API changes. &gt; &gt; If something is still unclear, I'll happy to discuss further. I thought about a dependency adding a new exception, which shouldn't cause a major version increase (but perhaps it does?), and now my code can crash where it did not. This is probably a very edge case, but I am sure one can find other cases like that if ones mind is put to this. &gt; There is a way to communicate that: specify lower bounds (on base). If you don't test with GHC-7.8 nowadays, add base &gt;= 4.8 ... how did I not think of that? &gt; I hope that comment was due a misunderstanding of me acting as a Trustee Yes. I found that weird, and a bit annoying, that there was a "PVP militia". But I guess I am OK with a "PVP police" ;p
But using the functions on Float you will rightly expect them to work like Floats should. NAN propagation and not arbitrary values from floor. And Field based functions won't give you that since they use Field semantics. 
&gt; I wish Michael had come on the thread to say this two days ago. I hope I don't end up in a position where I am expected to write immediate responses to sudden proposals! This is one reason why controversial GitHub issues are so tedious... You get this extremely verbose onslaught of increasingly hostile confusion by everyone remotely interested...
Oh dear the psyche police are here!
Simple and elegant solution.
Here is the problem. Forking cassava on hackage and using that fork would require forking everything that depends on it. No-one wants to see a fork storm. I should be able to publish my own version of cassava to hackage.
&gt; I hope I don't end up in a position where I am expected to write immediate responses to sudden proposals! You can avoid that by not becoming the figurehead of a programming language subcommunity.
Other than cassava and integer-gmp when has this a problem? There is a long history with HVR that goes beyond these issues so there is that extra context... That history informed the tone of those ugly discussions. The whole point of a "fork" from my perspective is to precisely avoid facing a situation where one individual can break everyone's builds. If we can patch things then cooperation would no longer be necessary.
&gt; There should NOT have been a SLURP proposal if even the forker wasn't interested in it I think it's probably best not to use that word. As has been made clear, there is no "forker".
If "pkg-1.0" means one thing in day and a different thing the next, that's a form of mutability.
https://www.reddit.com/r/programmingcirclejerk/comments/6sbt21/destroy_imperative_programming_aka_the_root_of/dlcegjt/
The most important language features for me are: * Immutability * (Exhaustive) pattern matching * Higher order &amp; first class functions supporting basic FP constructs * Typeclasses Contrary to the sentiment of some other comments on this thread, I think if you can't use Haskell, Scala will probably provide the best experience in terms of what you're looking for. For what it's worth, Haskell was my first foray into FP but I've been working professionally in Scala for 2.5 years now. There are definitely things I wish Scala has, but there are also many things I miss when using Haskell (Mainly on the tooling and ecosystem side). Our backend all Scala, and I can't recall NPEs or concurrency bugs during development nor prod. We use libraries like Cats, Monix, Circe, Doobie and tend to avoid the more java-esque side of the Scala ecosystem (though we do use a lot of Java libraries). 
I just tried it in OCaml, they have `int_of_float nan = 0` `floor` itself returns a float, so you just get `nan` back.
It seems unfortunate to see this ongoing conflict within the Haskell community. I see this as a potential issue that could actually force people to leave or not adopt Haskell at all. I is difficult to understand how this situation came about since so many smart people working on the Haskell ecosystem. I wish there was a more unified perspective and vision for the future of Haskell or Haskell will not have any future at all.
I think that's really overstating things. The vigorous hashing out of technical solutions won't push anyone away from Haskell. On the contrary it will lead to a better ecosystem for all. On the other hand a lot of this seems to have become very personal and that's unfortunate. We should work on healing those wounds and make our community a more pleasant place for all.
I think what is clear by now is that Hackage needs to be a build-tool independent package database. A source of packages which can be used by any packaging system. It is then reasonable that these systems augment the single package set with whatever changes are necessary to make the package fit into their ecosystem. As people came to realise that is what was meant by this whole proposal, they pointed out that this is what every distribution already does (see debian, gentoo, nix, cabal). Having specific patch files or additional git dependencies is possible and normal but usually a hackage upload is preferred to reduce the maintenance burden. As such, the name "hackage trustee" is somewhat of a misnomer. A hackage trustee's job is to ensure that a package is able to be built by cabal and the cabal ecosystem and so a better name would perhaps be a cabal trustee. It is a consequence of history that hackage is quite tightly intertwined with cabal as for a long time it was the dominant build system. Now there are at least three viable options it is not surprising that there is some friction. Those who say they dislike package revisions are effectively saying they don't want packages to work as well with cabal. They don't have to consume the revisions if they don't wish to. However, Michael Snoyman needs to stop consistently playing the victim. His leadership is responsible for aggression ([a classic example](https://www.reddit.com/r/haskell/comments/4zzmoa/haskellorg_and_the_evil_cabal/) but there are many others) and known for an unwillingness to compromise. Whatever his perspective, there are several sentences in this post which are contentiously worded and can be interpreted differently by different parties. For example, I am assuming his "public broadcast methodology" is a reference to the "evil cabal" post referenced above. Likewise, Hackage and Cabal contributors need to accept that Herbert is also culpable in the situation. There has been needless aggravation of third-parties due to build matrix failures being assessed as the maintainers fault. They are right that they do not care about such obscure build plans failing but Herbert is also right to care about the issue. It is normal practice for distributions to modify packages so that they work in their ecosystem, so patching cabal files is no big deal. As far as I can see, Herbert is also somewhat resistant to radical quick changes and errs too firmly on the side of correctness and conservative changes. This leads to very slow progress as he controls a lot of the hackage and cabal ecosystems. So to sum up. I agree with the sentiment of Michael's post but it still doesn't resolve the internal frictions which have developed. There are now three complementary build systems for haskell packages which all work in slightly different ways and the internal cogs of the ecosystem need to adapt in order to account for this. It does however, say a lot that we got to this situation over a rather simple proposal. I hope all parties see this post for what is is intended to be, constructive, rather than accusatory. 
The post title requires a lot of context. The linked package's description doesn't help much (other than providing links). If I understood correctly what this is about, this might had been an appropriate title: "A package for using the skylighting syntax highlighting library along with the brick terminal UI library"
Just in case you misinterpreted that sentence, [here is what I meant.](https://en.wikipedia.org/wiki/Musical_chairs#As_metaphor) I'd prefer to configure my project's build system once and have it work indefinitely, than to have it randomly break in n months to n years because something somewhere has moved or been altered. Content-addressible means you locate things by its content's hash, so that prevents the altering and the moving. The only thing left is availability of the storage medium, which can be addressed by things like IPFS and mirrors. I have *no idea* what you thought I meant.
Good news! You can use it today by switching to Nix :)
proposal to update hackage in order to avoid fork between cabal and stack packages IIUC
If you want to turn your syntax tree into a monad, you may want to check out Bound. The `examples/` directory has stuff you could use, including this: https://github.com/ekmett/bound/blob/master/examples/Overkill.hs After building a syntax tree of type `StaticTree x y a`, you can apply the `closed` function to it (if you can write a `Traversable (StaticTree x y)` instance), which can give you a `Maybe (StaticTree x y Void)`. ----- Alternatively, there is also an analogous construction of a "strongly typed Bound", where instead of a monad, you have a higher-kinded analog with an operation (&gt;&gt;-) :: HMonad t =&gt; t f a -&gt; (f ~&gt; t g) -&gt; t g a where (~&gt;) is a natural transformation: `type (~&gt;) f g = forall xx. f xx -&gt; g xx`. This can get you pretty far. The original strongly-typed Bound demo by /u/edwardkmett was a simple language with HOAS: http://lpaste.net/79582 /u/gelisam wrote a simplified version (that I haven't looked at properly myself) that specialises things a bit and cuts down on the machinery: https://github.com/gelisam/strongly-typed-bound Lastly, I have an example of using strongly-typed Bound to track more information than just a type. For my requirements, I have to distinguish multiple kinds of data: a `Term` with free `a`-variables (`Tm` in the source), a `Coercion` (`Coer` or `Co`), a `Constraint` (`Ct`), so I have things like `data Tm a = Tm a` that I use as type indices. https://github.com/mrkgnao/system-dc/blob/master/src/Types.hs#L66 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mrkgnao/system-dc/.../**Types.hs#L66** (master → fd16326)](https://github.com/mrkgnao/system-dc/blob/fd1632675acce1940fa0532eb6762f920bf6fbb4/src/Types.hs#L66) * [ekmett/bound/.../**Overkill.hs** (master → 60e39ba)](https://github.com/ekmett/bound/blob/60e39ba8f798b3fa8ed36ee875077c91a9ebcdb7/examples/Overkill.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dt5uzrk.)
That is not an accurate way to describe it.
Yes it might be a bit overstated but the potential of this issue is more far reaching than it might seem at the moment. If things escalate further this will impact the future of Haskell in a big way.
Well, the [PVP][1] states quite clearly that &gt; When publishing a Cabal package, you SHALL ensure that your dependencies in the build-depends field are accurate. This means specifying not only lower bounds, but also upper bounds on every dependency. &gt; &gt; At some point in the future, Hackage may refuse to accept packages that do not follow this convention. The aim is that before this happens, we will put in place tool support that makes it easier to follow the convention and less painful when dependencies are updated. In other words, if you publish a package with dependencies, then you should make sure that your specification of dependencies is accurate, which means that your package should compile if given these dependencies. This is actually a MUST in the sense of [RFC2119][]. The only reason why the Hackage homepage says "should" is that this rule is not enforced yet, with the understanding that it will be one day, when tool support is available. [RFC2119]: https://tools.ietf.org/html/rfc2119 [1]: https://pvp.haskell.org/#dependencies-in-cabal
Thanks for sharing your perspective. I didn't follow what happened very closely when it was happening. Certainly the way the issue was raised probably wasn't so healthy, but it seems even more unhealthy that this was used as a justification to not fix this infuriatingly simple issue. I caught up on it a couple weeks later and it really pissed me off that it still wasn't resolved. A couple different perspectives one could have: 1. Maintainers can do whatever they want 2. It is the responsibility of maintainers to make reasonable efforts to enable people to use their code Both are legitimate perspectives I think. However, I can see (1) being a stronger rule than (2). After all, many people may make many different simple demands, we can't expect maintainers to satisfy them all. So, what can we do? The solution is to make it so that no demands are necessary in order to fix builds for users. In order to facilitate that, we need a package storage system that allows for publishing versions of packages that differ from those in hackage. I believe this is one of the primary motivations for dicussion of a fork.
If I understand the [PVP][1] correctly, /u/ndmitchell may have to upload with upper bounds in the future (when tool support is present to make this less work for him). [1]: https://pvp.haskell.org/#dependencies-in-cabal
I see a dotted line. https://i.imgur.com/iqItbsa.png
Oh. Apparently, I do not: https://i.imgur.com/vKnKG1a.png .
I feel pretty safe, based on the last few days of discussion, that there is far from a consensus that Hackage will one day enforce the PVP. In fact, quite the opposite! Comments from several parties indicate a general shared expectation that this is not a requirement. Good to know that there's documentation that conflicts with that. Either the documentation or the shared understanding should clearly be fixed. Personally, I'd be disappointed to see a big part of Haskell's library ecosystem told to stop uploading to Hackage. The costs to shared package discovery and documentation are too great. So I very much hope things get resolved in the way that has everyone using Hackage.
Well, strictly speaking, I'm not sure if it's possible to remove a package once it has been uploaded... :-) But I hear you. On a conceptual level, I don't think that it's a dumb rule to only allow packages whose dependencies are specified correctly. Unfortunately, on a practical level, things break all the time, even if dependency versions are specified correctly. It appears that "Painless dependency specification in the presence of errors" is still an open research topic. Human experiments on Haskellers are currently underway…
There are two types of upper bounds. I _known_ my package doesn't work with foo-1.0 or above, and I _haven't tested_ if my package works with foo-1.0 or above. The former is valuable. The second is not - you can infer that it was tested with the version available on the date I released (and probably should as part of the upload check - I have tools that do so), but that value shouldn't be updated every single release of any dependency. Automation with the PVP is just a bad way of predicting the future. As a tool for humans, it works well.
&gt; I don't think that it's a dumb rule to only allow packages whose dependencies are specified correctly. Agreed. However, the question is how to define "correctly". In my (and others) opinion, adding upper bounds for package versions that do not yet exist is not necessary for "correctness", but the PVP disagrees. The Hackage maintainers are free to define and enforce their own rules, of course, but it might result in people moving elsewhere.
I would be behind this change. It would ease a lot of tension. And in practice, everyone would keep doing what they're already doing.
Flame wars are caused by responses that are too fast. I'm glad that Michael took the time to think carefully about his response. It could be it was the right thing to do even if he already knew exactly what he was going to say. For the good of our community, this whole situation needs to be cooled down. Once we achieve that, it's clear that this can be solved in a way that will allow all of us to keep moving forward successfully.
But you *do* know how you are using this particular dependency - one tiny detail from it, or the entire API, or certain parts of it that haven't changed for 10 years, or certain parts that are unstable and change all the time, etc. You as a package author should have the opportunity to share this knowledge with your users. Both PVP automation and neglecting bounds are equally bad in this respect.
As an onlooker, the ongoing strife in the Haskell community definitely looks terrible. I'm not touching Haskell for hobby projects anymore, and I'll be bringing up stuff like the cassava double-dash flag controversy if my workplace ever discusses adopting Haskell.
Are you taking issue with the wording in my PR ([haskell-hvr/cassava#155](https://github.com/haskell-hvr/cassava/pull/155))? What could I have done better? 
My argument is you shouldn't be _forced_ to have upper bounds, and I think we're in violent agreement here. I use them whenever I think a dependency is going to break my code in the next version - I always depend against template-haskell and haskell-src-exts with upper bounds.
No I agree with you that you shouldn't be forced. I am proposing that the semantics be relaxed to maximize the useful information that can easily be expressed by the bounds, to make it easier for authors to use them that way.
Sounds like a good compromise but I don't understand the rationale for 1. What's the benefit of not requiring authors to bump at least the second component when making a breaking change?
I would *guess*, then, that you're in a middle ground where you're not sold enough on Haskell to really want to use it (or you wouldn't much care whether there are Stack vs Cabal arguments) nor are you sufficiently isolated from the community that you don't even notice the arguments. Again I would *guess* that that puts you in an extremely small minority. (I could be wrong on both guesses.)
Even if I accidentally slide into a position of any importance, I think I might adopt a habit of a two day delay in all my communications, just to preempt the expectation that I am synchronously available. Now that I think about it, I subconsciously have a similar policy already...
This is *sort of* accurate, though I would surmise that this middle ground is more unstable than you'd expect. On one hand, I would be strongly inclined to adopt Haskell for personal projects if there were signs of an enduring consensus towards Slack, or some third alternative. (I'm treating cabal as a no-go for entirely superficial reasons, including that it's been an awful experience every time I've tried to use it.) If, on top of that, there were signs that the community was coming together, that yesteryear's bitter feuds ended in mutually agreeable terms, and that the perceived aggressors received suitable retribution, then it would be a powerful sign that the Haskell community is healthy and has long-term growth potential. Then all that's missing is an IntelliJ plug-in and I'd be pushing your shit at work. The Scala community has also experienced shit like this, leading to some luminaries quitting entirely. Contrast with the F#, OCaml, and Rust communities, none of which get quite so bitter.
&gt; The second is not - you can infer that it was tested with the version available on the date I released (and probably should as part of the upload check - I have tools that do so) I don't believe that inference is reliable. Hackage has no idea what dependencies you built and tested your package with.
I'd be very happy to tell it - I have the information to hand. It could also do its own build and test to get that information (which it does, at least the build part)
I don't think it's a good look from the outside. To a casual observer (such as myself) this looks like a culture that leads to constant bickering and upheaval, for some reason. Maybe a somewhat less vigorous hashing out would work out better for Haskell adoption.
In practice there are respected people who intentionally don't do it. They have their reasons. I think it would be great if everyone did this, but I don't think anyone should be banned or ostracized if they don't.
Really? I know there are respected people who don't specify upper bounds. They have good reasons not to do that, even though I *do* do it on my own packages. But who doesn't indicate breaking changes with a second component bump? I would be interested to see example packages where that happened.
That makes two of you then. Perhaps I was wrong.
Some people believe that the entire concept of semantic versioning doesn't make sense in a statically typed language, where you can discover that the API changed when the new version doesn't compile. That's _not_ my viewpoint BTW, just trying to explain it.
Hadoop didn't have much in the way of credible competitors back then. Its market share has been getting chipped at by cloud providers (both public and hosted), and Spark has been competing with MapReduce/YARN, but it's still one-of-a-kind in the FOSS + in-house big data space. Haskell is also one-of-a-kind, but it's competing on every side with nearly a dozen languages. And transition costs for languages might be lower than for cloud platforms.
Similar, but not exactly: * The new strong syntax stands as is. * No new syntax (yet). * The old syntax is now interpreted as lax, because that is how most people use it, and that makes it easier to use. * Lax syntax does **not** mean "cautious and follow PVP". It means "cautious and letting you know my estimation of how long this is likely to keep working, whether or not that complies with the original PVP".
I'm sympathetic to that line of argument, in fact I'd rather pretty much *all* bounds were automatically generated, but I really have never heard on anyone not versioning their own package according to the PVP versioning scheme.
&gt; I'd be very happy to tell it It seems to me that version bounds and the PVP are the method that currently exists for telling it.
RemindMe! 2 days "Reply to my adoring fan /u/yitz"
I will be messaging you on [**2018-01-26 14:52:29 UTC**](http://www.wolframalpha.com/input/?i=2018-01-26 14:52:29 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/7sl5ut/michael_snoymans_comments_on_slurp/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/7sl5ut/michael_snoymans_comments_on_slurp/]%0A%0ARemindMe! 2 days ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I liked u/int_index's [proposal](https://github.com/haskell/ecosystem-proposals/pull/4#issuecomment-359693651) very much. Mostly for the build-plans part and the reasoning behind it.
No, this is a drastic misreading. Since the PVP itself is under a _should_, any _shall_ under the _should_ cannot possibly strengthen the entire thing. In haskellspeak, `Maybe Id a ~= Maybe a`.
how about: a solution nobody wanted to a problem that wasn't the actual problem anybody had :-P
That's not a *defining* property ...
I'm still confused. Sorry if it spoils the joke, but in the present times I would rather know if it's you having fun, or somebody being an ass.
It's definitely _not_ me. I have no idea who it is. I have absolutely no sock puppet accounts for discussing programming topics. I used to use alternate accounts for other interests, like lifting, but I got rid of those to avoid any confusion. To my current recollection, every social media account I use is called `snoyberg`.
Thanks for clarifying. I definitely was confused. snoyjerk now goes on my automatic downvote list, and I support this [request to ban it](https://www.reddit.com/r/haskell/comments/7shikr/replacing_hackage_hash_based_package_downloads/dt5eluy/).
Yes please do. Such behaviour is harming our community and we should deal with it. [Context](https://www.reddit.com/r/haskell/comments/7shvxo/hash_based_package_downloads_part_1_of_2/dt5alxh/)
I'll increment my internal counter for counter-examples from 0 to 1 ;)
Can you please explain what you mean in more many words?
&gt; Either the documentation or the shared understanding should clearly be fixed. Agreed! :D After all, for someone not involved in this whole thing (me), learning the shared understanding is only possible through documentation… 
Yes, I get that. But the PVP does mention Hackage explicitly, so if you take the PVP as the normative description, then the *shall* is not actually under a *should*. The documentation is conflicting, to say the least.
I must say I like what the site is trying to offer. I tried out a few songs and some of them worked not bad. However I can't seem to find any indication of a "suspected chord detection quality" or somesuch, which might lead beginners - which I believe the site is targeted at? - to unknowingly pick up things that are not quite correct. I might be blowing this out of proportion though, maybe an Ukulele transcription of Rachmaninov's 3rd Piano Concerto was just wishful thinking.
I'm not such a newcomer and even I can't figure out what's going on. I'm even using Haskell (and stack) for a serious production piece of software right now.
&gt; In my (and others) opinion, adding upper bounds for package versions that do not yet exist is not necessary for "correctness". Well, there is one reasonable definition of "correct", namely the idea that "cabal file = logical expression that tells me which dependencies I need to make this software compile". If I want to make this happen, then I have to include upper bounds on my dependencies, otherwise there might, one day, come a dependency which satisfies the logical expression in the cabal file, but does not make my package compile. I don't know any other definition of "correct" that only depends on data given in the cabal file itself. 
[removed]
Just to share the knowledge: cabal new-build has really shored up most of its issues, so I could easily see it being used in production (in conjunction with freeze files for reproducibility, which I believe it supports). However, a lot of people use Nix instead of either, as it offers all the same power as Stack and a whole lot more, at the cost of a higher learning curve.
I found the title amusing. It was as if he was shouting it in a medieval market. "Help, please, guards!"
I’ve been told the same thing before: https://www.reddit.com/r/haskell/comments/5kt1l0/comment/dbqf1om &gt; If the author doesn't wan't to follow the norms set by Hackage, perhaps he shouldn't publish his packages there.
I version my packages with SemVer. It’s not the PVP, but it still signals changes. 
Sure, it wasn’t official in any capacity. But it has a lot of upvotes. And nobody disagreed with them. 
I can point to a specific phrase that has the tone that u/dalaing was getting at: &gt; There are no apparent downsides to using a more compatible flag name. But overall, I'm guessing that by 'you', u/amonadaday means the group wanting the flag to change, not you (u/taylorfausak) in particular. It gave the whole thing an adversarial tone from the beginning, and your PR was tainted by association. u/amonadaday, does that sound correct with your intentions in your previous post?
Well, I suppose improving the algorithms would be part of the job. As a musician and haskell programmer It'd be nice to give this a shot.
&gt;Now that I think about it, I subconsciously have a similar policy already... I'm going to start calling my procrastination a policy from now on lol 
/u/taylorfausak uses SemVer https://www.reddit.com/r/haskell/comments/7sl5ut/michael_snoymans_comments_on_slurp/dt6ab2s/
Hmmm really interesting ! I work in Utrecht now, and as a (ex) professional musician this seems like a really cool thing! And yes, it needs improvement. A quick listen told me the chords are off and as a music teacher I'd say, it needs to be a bit better for aspiring musicians to actually be able to use it. Also, who know some extra features like theory of harmony could be integrated quite easily, as long as the listen-and-interpret functionality is ready. Now I am not really experienced in Haskell. I have just started in fact. But who knows? Most of my experiences is in automated testing and small cli apps and stuff like that, written in , java, node, some python. u/JzB_ do you know of anybody I could talk to ? I'd like to learn more about this company and product! 
&gt; Well, there is one reasonable definition of "correct", namely the idea that "cabal file = logical expression that tells me which dependencies I need to make this software compile". I have another reasonable definition: "cabal file = logical expression that tells me which dependencies I need to make this software compile *in the context/time during which this Cabal file was written*". Different people might prefer one of these definitions for various reasons. I certainly think yours is simple and more elegant, but I have not had much luck with it, whereas mine (plus some scaffolding/snapshotting) has served me well. I don't think the Stack/Stackage/cabal-install/Hackage conflict can be solved simply by figuring out what is *correct* - clearly the problem is too complicated for any singular notion of truth or correctness.
&gt; It's a variant of the classic S. Maneuver: I almost always stay out of this sort of social drama, but the blog post from yesterday was 4-7 basically.
There is a form at the bottom of the page they linked to: https://chordify.homerun.co/
The barrier for using nix with haskell seems really high at the moment. 
FWIW, I'd say it's more than taste. Nix objectively provides a lot more power than stack. The cost is learning curve.
I see you have an office in Austin. Are there any positions available there?
I think that’s right. If people put constraints on my packages assuming that they’re PVP, worst case scenario they won’t get new features (which update `y`). You’re correct about the `x == 0` scenario. For my packages, I treat `0.y.z` as if it was `y.z.0`. That is, changing `y` is a breaking change and changing `z` is not. However I also try to move my packages out of `0.y.z` as fast as possible. 
Yeah I saw that indeed, and might fill it in. Was just wondering if I could get some more info and share some ideas. It seems I do not fit the description perse, but that never means there are no opportunities :)
Your understanding is right - but I happen to think that your recommended title contains just as much information as mine, plus the package description. Yes, by posting only a link I assumed that only folks who knew what Brick or Skylighting was might find this of interest, and that anyone wanting to know more would click around and learn. I'm glad you figured it out. :)
/u/snoyberg how crazy of an idea would it be to suggest that stack be rewritten to be based off nix?
&gt; When writing functions dealing with floats you have to use different reasoning than when dealing with integers or other ring-like things. I think I'm just doing something odd then, because honestly this is how I write my code and I've never had any problems. There was some lengthy discussion on the Agda dev issues a while back where we discussed lots of the same things - Floats aren't law abiding, they have weird crap like NaN (in fact, multiple different NaNs!) and +- 0 and Infinity, but if you're writing your logic on rationals, none of that stuff exists anyway so there's no need to worry about it. I know the algebraic properties don't technically hold, but they don't hold in such practically insignificant ways that it usually doesn't matter. I mean my computer isn't technically a turing machine, either, but I have no difficulty pretending it is. And honestly this is what we should expect - I mean this is literally the reason Floats exist! If they weren't a useful approximation for rationals, then what's the point of them?
ah, right. :( Next crazy idea: Write a windows version of nix that only builds Haskell packages and run a hydra server for prebuilt non-Haskell packages.
I've been using `ghc-mod` on 8.2 for a month now :) There are a bunch of people who've managed to get it running near the end of the thread. It's actually pretty stable!
From what I can tell you are in compliance with the PVP.
u/sclv I've kinda taken up some work for doing this, from a H2020/ i guess libraries perspective one fixup ive planned out exploring is having nans -&gt; sigfpe -&gt; have the RTS throw a floating point exception :), at least as an RTS flag option. Need to carve out some time to get this in shape though 
&gt; their quite opinionated vision for Haskell What do you mean by this? Can you elaborate?
I'm a fan of this blog posts series, recently re-read all of it and was wishing for another one, thanks!
Haha you're such a piece of shit mate.
Thank you very much! I've read this and referred to [this video](https://www.youtube.com/watch?v=eKkxmVFcd74) for more information on Free and Cofree, however - I'm not sure what is the value we get from using this kind of structure over a plain data type with an annotation type variable. Is it the generic recursion algorithms which are the selling point in your opinion?
This is an awesome blog post series. What are some of the possible advantages of recursion schemes? One of the use cases of caching historical computation(dynamic programming) was shown in one of the previous blog post using histomorphisms. I was wondering what other benefits do we gain from expressing our computations using recursion schemes. (NB: I am aware of the entire Origami Programming paper and the argument of explicit recursion is the 'goto' of functional programming. I am asking for real-life use cases outside that paper.) 
Ah cool, thanks!
Thank you for the kind words!
That's a good question! Some examples where I've used recursion schemes in production code: * traversing a syntax tree top-down to convert it into A-normal form * ensuring that nested data I received from the network maintained its internal invariants (using an error monad to terminate early with a report) * visualizing nested structures bottom-up with the 'boxes' package * annotating structures parsed from a file with their position information In a sentence, given a recursive computation (which are, needless to say, common beyond words in Haskell), recursion schemes allow us to separate the boilerplate associated with recursion from the actual computations we're performing. This has enormous benefits in the real world.
After briefly using `fgl` and having some complaints, I looked around r/haskell and found a comment wishing somebody would modernize it. So that's what I set out to do. I ended in a somewhat different place than expected, but still think it fulfills that request. There is definitely some polishing to be done, but overall I think it is in a reasonable place. Removing `fgl`'s id/label split means I get to simplify many of the types. There is no longer an `LNode` vs `Node` difference for example. This implementation is a wrapper around a `HashMap` at its core, so I get to lean on the inherent speed of `unordered-containers`. But I also have done and will continue to do tuning to make sure it stays fast. I'm open to requests/critiques if people have them!
I may also like this lib: https://github.com/yesodweb/Shelly.hs
If there are any questions, just comment here or DM me.
You want your function to do two different (albeit related) things. Why not use two functions?
Also the second definition can be written in terms of the first. dist2 x y = dist1 (x-y)
"should" SHOULD have been be capitalized, like they do in specs ;)
fwiw, Haskell (language, ecosystem, and community) has hundreds of conflicts in philosophies (backwards compatibility, licensing, abstractions, tooling, etc), very much not "one right way to do things", even more than most languages; but this one, while important, is the only one where it's become somewhat personal, imo. 
lol, reading less drama, and writing more haskell or interacting with random maintainers on github, is probably a more accurate perspective on the community. 
And the first in terms of the second: dist1 = dist2 Point(0,0)
I think part of the mixup is that formally the first distance is actually length of a *vector*. While the second is the distance between *a pair of points*. This may sound like nitpicking but being clear could make this problem disappear. One could make a data sum type that would cover both cases.
iirc, among other non-determinisms, module import/compilation isn't semantically order-dependent, but ghc's generation of the unique identifiers is. (again, iirc). 
Unless I'm misunderstanding, I think you *do* need a split between the id of a node and its label. Labels are annotations on a graph that don't have to be unique—for example, what if I want to talk about graph coloring and label every node in a graph with one of four colors? The ids, on the other hand, are part of the representation of the *structure* of the graph, not annotations at each node.
&gt; will haskell get actual records with syntax yes, but it will take years, For Better or Worse. &gt; dooming Haskell haskell isn't dying. it's 30yo, cabal hadn't existed for years, etc. and despite having an ancient compiler, it's development is much more flexible than most newer languages. (fwiw, records are extremely important to me, and i'd be happy with something that came sooner.) 
purescript? 
when I tried new-build in a nix-shell (with vanilla cabal2nix files), it tries reinstalling packages, probably because nix provisions them in the "old-"build location. Does your project, like, override haskell packages to move/rename the artifacts? if so, is there a isolated nix file i can drop into to my non-reflex-platform projects?
Someone else can probably comment with something more thorough, but here's some things you can check out (for moving really fast), - use `undefined` for stubbing functions until you need them - [typed Holes](https://wiki.haskell.org/GHC/Typed_holes) are nice when you are unsure about the type [1] - pick up some common design patterns that you can quickly reach for, personally I like the [ReaderT Design Pattern](https://www.fpcomplete.com/blog/2017/06/readert-design-pattern) and `Freer` (I would say to check out [freer-simple](https://hackage.haskell.org/package/freer-simple) and read up on `Free`, since most concepts from `Free`-style apply to `Freer`/extensible-effects) And some nice blog posts :) - [Exploratory Haskell](www.parsonsmatt.org/2015/12/09/exploratory_haskell.html) - [Debugging Types: A Stream of Thought](www.parsonsmatt.org/2017/09/13/debugging_types_a_stream_of_thought.html) - [Rapidly prototyping scripts in Haskell](blog.ezyang.com/2010/10/rapid-prototyping-in-haskell/)
I've also thought about redesigning fgl :). I think the best solution is to make the node id entirely abstract—it's a unique identifier of a node, *but that's all the user sees*. All you can do is check whether two node ids are the same and check whether a node is in a graph or not.
My goal in this series is to understand the recursion-schemes ecosystem well enough to be able to document the package. :)
I think there‘s a misunderstanding going on. i ment vector in the sense euclidian vector - and not Data.Vector - mostly because in the question was talking about points. I fully agree that one of the functions should be defined in terms of the other. I was more worried by the errors one can make by mistaking points with vectors. 
Why would hash maps be faster than int maps intuitively? I would actually expect the opposite.
The answer to "can I do" is "no you can't". I explained why in my first post.
Great! I'm looking forward to it!
In my programming language Coconut you can only declare variables, with any type you want. You cannot mutate them or print them. It's a purely functional, massively-concurrent language with absolute safety. It's (been) used by (one guy at) the Commerce Bank of Addis Ababa to do quant stuff.
It isn't different except in the syntax of the namespace protocol: `org.haskellstack/wai` using a slash to delimit the package name from the namespace would be easier to parse by both machines and humans.
Hm, I'll see what I can do :P Thanks for your thought, I'll be back!
Fair enough. Leveraging Haskell's type system to give semantic meaning is a strength of the language.
Regarding speed differences, I'm being more aggressive with inlining and optimizations than `fgl`. For instance, my graph isn't a class, which saves some dictionary passing. So, for now at least, comparing to `fgl` is not equivalent to comparing to raw `IntMap` performance. As for why I chose `HashMap`s, I would like to eliminate the need for explicitly handling node ids that aren't related to the program's data. Take an edge from `fgl` for example: `edge :: (Int,Int)`. That edge is effectively meaningless by itself. With hashing, we could easily make that `edge :: (String,String)` or `edge :: (Foo,Foo)` covering many basic use cases. As pointed out in the discussion under my first post though, this isn't a perfectly accurate representation. If I do completely internalize the node ids, then switching to `IntMap` may or may not make sense. The dictionary benchmarks should help with that :)
Like others have said, this is poor style. But, here's how you could do it (no GHC around but I think this works): minus (Point x y) (Point x' y') = Point (x-x') (y-y') mag (Point x y) = sqrt $ x^2 + y^2 class Distance a where distance :: a instance Distance (Point -&gt; Float) where distance = mag instance Distance (Point -&gt; Point -&gt; Float) where distance x y = mag (x `minus` y)
[For those who didn't get the joke.](http://www.telegraph.co.uk/comedy/comedians/monty-python-s-25-funniest-quotes/the-spanish-inquisition/)
I think of the sized int types in terms of modular arithmetic, so wrapping around is actually perfectly sensible.
Can this technically replace the traditional law-lacking foldable?
I've put in a request to JGM to consider a more permissive license for Skylighting - we'll see what he says!
This will be useful for anyone debating what computer to buy for Haskell development. One more data point for you... Answering the question for myself, what kind of computer do I need for a smooth Haskell development environment. Answer: a MacBook Air or Mini is just fine, outside of some edge cases. I also do Haskell development on an 11" MacBook Air i5 w/ 4G RAM. Works well enough when I really need mobility.
I'm sure stack is here to stay, these arguments come about because of personal differences between stack developers and the cabal/hackage maintainers. FWIW. I've never had a problem with cabal sandboxes (and now cabal new-build), but those times I've reached out to stack (mainly in order to use ghcjs) have always ended in frustration because whichever stackage 'version' ends up with packages incompatible with the ones I was previously using. Granted this is probably more an issue with the ghcjs builds only existing for certain older stackage versions. That combined with the sour taste I get from these ructions in the community disinclines me to stack in a large way.
How do you express versions and dependencies in a "build tool independent" way? Do you have packages on Hackage which can only be built with certain build tools? 
Although I do like this article, I think anything geared towards beginners should not use unicode syntax and operators.
That question is probably above my pay grade. My instinct, however, is yes: if the folded structure abides by the Functor laws, the behavior of `cata` and friends is predictable. However, the docs for Foldable indicate that there are laws that Foldable instances must satisfy, so I may be missing something w/r/t your question.
I'm not sure if `cata` actually obeys any laws, unless you restrict the type to use the generic *a la carte* derivation that the blog post uses. Types like `Vector` or `data Replicate a = Replicate Int a` have `Foldable` definitions that differ from their generic representation (`Vector` by being an actual contiguous array, and `Replicate n x` by pretending it's a list created with `replicate n x`).
It's a good question. I think: Practice. Building the kinds of things you've built many times before. A personal collection of utility functions and go-to libraries. stack project templates. ghcid, ghci, and Debug.Trace.
I'm pretty sure that is a monospaced font with ligatures, not unicode operators (the function arrow appears two cells wide, for example). Probably http://larsenwork.com/monoid/, from the look of bind.
Ah, yes, the early-failure case that the Haskell `Maybe` monad handles well. The early-success case has been harder for me. Here's a more challenging example, inspired by some code in my [IMCS](http://github.com/BartMassey/imcs) server: let val = 5 let answer = case case1 val of Just result -&gt; result Nothing -&gt; case case2 val of Just result -&gt; result Nothing -&gt; case case3 val of Just result -&gt; result Nothing -&gt; 17 Partial credit for stuffing all the calls and the default value in a list and saying `head $ catMaybes`, because ugh. No credit for a bunch of `where` clauses and `isOk` guards, because double ugh. Would sincerely like to know what folks do with this.
Still makes it hard for beginners to see the syntax of what's going on, though.
You might be able to do that by abusing `Either`. Not tested, but: let val = 5 either id id (do maybe (pure ()) Left (case1 val) maybe (pure ()) Left (case2 val) maybe (pure ()) Left (case3 val) pure 17)
Yes, thanks for posting this. I was about to write this too when reading everyone else saying that it is not possible. It is. It just may not be a good idea. Let's say how it is possible and what are the pros and cons of writing this. I can think of these: - (+) allows for more conciseness - (-) messes with type inference, sometimes makes it necessary to provide explicit type signatures - (+) This can be alleviated to some extent with `{-# language ExtendedDefaultRules #-}` and `default (...)` declarations. - (-) even still, type errors, when they occur, are more difficult to understand when this technique is used, especially for beginners.
Is that 1 hour and 50 minutes for 32 cores? Also, would you be willing to include comparison results for `-j={4,6,8,16,32}`? I'm wondering if I want to buy a new CPU and I'm curious to see how package level parallelization contrasts.
Username checks out
Yeah, I know it's an unpopular idea. I just worry when we start getting more strongly typed things and we start having to do `x Vect.++ y` to append length-indexed vectors, where unqualified usage isn't actually ambiguous, and qualified usage is rather noisy. Plus the whole record accessors issue. A lot of the time I want to be able to make the trade-off between explicit vs. unambiguously implicit that makes the best sense. Sometimes type classes are the right solution, but I don't think exclusively qualified names are *always* the right answer when type classes aren't.
It's not really part of the library itself, but it might interest you anyway: if you wrap Algebra in datatypes and use the same trick in the [foldl](https://hackage.haskell.org/package/foldl) library you can give them Functor/Applicative instances. I'm not sure how to generalize the Coalgebra based types though. Also really interested in your post :) I wasn't super familiar past with the library past cata/ana/para, so seeing the others in action is really cool.
&gt; The asum is just a glorified head $ catMaybes I think. Nah, they're different. --- `catMaybes :: [Maybe a] -&gt; [a]` `catMaybes` will give you a list of all the things inside `Just`s. `head . catMaybes` will error on a list full of `Nothings`. --- `asum :: [Maybe a] -&gt; Maybe a` `asum` will give you the first `Maybe` value that is a `Just`. `asum` will give you a `Nothing` when you give it a list full of `Nothing`s
You're wrong that they can't do it, and I wonder why you say something so wrong so confidently. In case you don't see why you're wrong, consider another commenter saying &gt; You cant write a function that has type both `String -&gt; Int`, and `String -&gt; Char`, because what would `read "123"` return?
Yeah, I spent too much time trying to write something similar to printf and QuickCheck before realizing that this more simple use of classes works. 
&gt; Is that 1 hour and 50 minutes for 32 cores? Yes. &gt; Also, would you be willing to include comparison results for `-j={4,6,8,16,32}`? Yea package parallelization is often much better for the first parts, then mediocre for the latter parts. For the `stack` test case, with `-j 32 --cores 1`, the load average was 5.46, and the time was 17:28.01. About the first quarter packages (out of ~300) manage to use all 32 cores, then drops down to about 10 for the next quarter packages, then it drops again to 7 and then steadily tapers down to 1 by the end. If I manually curate it (that is, adjust to `--cores 6` manually mid-build, once the initial burst is through), the build time is almost identical: 17:36.12. But the load average is higher, at 8.91. It spent much more time in the 20+ cores range, and the rest was fairly similar. My conclusion is that it's very hard to get GHC to use multiple cores effectively...
Haskell is sweet for prototyping. And the future you will says thanks down the track. A few thoughts: - create a tight compile loop running in the background for an always compiling instance of the solution to date. Don't drift into a broken, almost-working version - type errors scale quadratically. Look at stack's --file-watch or ghcid - open several ghci's. One for holding state, one for type discovery and others for experiments. As experiments prove themselves, copy to state and uptohere solution. copy simple results - whatever convinces you you're on the right track - to doctests for bonus points and prevent degradation. - let ghc tell you all your types rather than type them yourself. I do this thru flychecking via intero. - whatever the task, assume you will be inventing an Algebraic Data Type, then working out some transformations from said ADT to other ADTs, and then writing a compiler/interpreter to handle effects. When you don't know what to do, just start writing out types, capitalizing the buzz words of the problem domain. This attitude also automatically separates pure code from IO and other monads. - don't introduce new concepts or libraries you are unfamiliar with. Hackathons are not for learning about Template Haskell or GHCJS. My goto list of well thumbed libraries: lens, foldl, mwc, streaming, managed, containers, vector, formatting - use lhs (oi!). Put all your notes in the one lhs file, mix in results, code snippets, links, ideas, output and everything. `stack new hey-ho-lets-go readme-lhs` is a template of mine. 
For enum I agree - even the enumeration on rationals is more of a mathematical curiosity than anything useful. For orderings, I'm not sold. Again, if you're only using logic defined on rationals (e.g., no NaN, no +-Inf, etc.), the Ord laws aren't that bad. The vast majority of the violations come from nonsensical cases anyway (e.g., divide by zero). And we really do use ordering logic for floats - for example, one of the most common things that comes up with floating point is determining if/how two regions overlap (hit-testing). My question is: am I going to implement this logic differently because of Float-specific characteristics, or am I going to re-implement the same algorithm I would working with `Ord a`? With `Ord Float`, this says "I'm using the same algorithm, but just do it fast and sloppy with Floats", without `Ord Float`, it says "I'm doing something different here because of Float's pathological characteristics."
Too far for a daily commute but there's [Checkpad MED](http://cpmed.de/jobs/) in Freiburg im Breisgau
Thank you :)
http://trofi.github.io/posts/193-scaling-ghc-make.html
Why aren't we all just using Nix, or a cabal/stack-esque wrapper around it? Serious question: if it's just the UI that needs improving, and presumably an equivalent to `cabal`/`stack upload`. You and the Reflex/GHCJS folk at a minimum are already using it for Great Good with the WASM and JS stuff respectively.
From my past experience, in hackathons language matters much less than the easily available libraries. As /u/tonyday567 wrote, there is no time to learn new ones - especially since so many of the good libraries are really hard to learn if your knowledge of CT is below the library's and there are no comments. So come prepared. Collect and learn some useful libraries for the hackathon you expect. Maybe write some utilities or templates.
Hi, [Tweag I/O](https://www.tweag.io/) has a presence in Zurich :)
Digital Asset are in Zurich.
That was a helluva read. Unfortunately, I don't think it's going to be easy to upstream non-default perf flags for GHC to Nixpkgs. Supposedly, they're the default for a reason
Three big reasons in my mind: - **Invasive.** It puts something your door directory, uses a daemon, takes a ton of storage, and symlinks a bunch of crap all over the place. - **Learning curve.** It's much much much too complicated for me to recommend it as a newcomer's default. This is incredibly difficult to fix (try as I might), especially considering the crazy state of nixpkgs. - **Windows.** As much as I hate to admit it, we can't just ignore Windows. Our default tools need to work on it. It does work in the WSL, but this isn't good enough.
Well, when I hear Hash\* I always think of a `HashTable` (which is another thing, I know)... That may well be faster than an IntMap.
Thank you, that's really useful. I've updated the snippets to include GHC_OPTIONS https://github.com/louispan/esoteric-haskell/blob/master/Library/Application%20Support/Code/User/snippets/haskell.json
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [louispan/esoteric-haskell/.../**haskell.json** (master → 9282cb9)](https://github.com/louispan/esoteric-haskell/blob/9282cb9bd06819a8628396f7ebad05c86e7a2bff/Library/Application%20Support/Code/User/snippets/haskell.json) ---- 
I’d be interested but don’t have a good idea atm. If someone wants to team up and see if we can come up with something, feel free to PM me
You can also come to the Haskell meetup tonight and make friends. You might find someone who knows some more companies.
&gt; So if I can create an abstraction that is pleasant to deal with, to write, without redundancies and repetitions, without clutter and minutia, than that's a win in my book. Well, I agree with you. Otherwise I wouldn't have written the package. That being said, you do pay a pretty big price with overloaded functions. Type inference gets unreliable (but `overload` makes sure it's the best you can hope for), error messages get horrible, type signatures become nearly useless, etc. &gt; Also, some counterexamples to Haskellers not liking "syntactic" abstractions: QuasiQuotes, TemplateHaskell, Generics, EDSLs, custom preprocessors. &gt; Edit: also, even in the base language there are many built-in syntactic sugar constructs: list comprehensions, do notation, etc... That's not what I mean when I say "syntactic abstractions". Say you write a function in Java or something that has 3 overloads taking different types of arguments. It's not like you can now talk about this set of 3 types that work with this function. You can't use the function to make further abstractions because the type of it's parameter must be known at each call site. It's purely a syntactical convenience. While some of the things you name are just syntax sugar, they don't claim to abstract anything. Overloaded functions, on the other hand, do kind of pretend that they can take different arguments from this special set of types even though this set can't be manipulated or talked about in any way.
Early success is `First`! foldMap First [case1 val, case2 val, case3 val]
Fair, you can do it with typeclasses. If OP really wants to make a distance typeclass and add manual type annotations whenever type inference gets confused, he can. However, for the situation that OP is asking about, the answer is "don't do that, here's why". Writing a distance typeclass in this scenario will not improve OP's life.
I'd rather recommend using `\case` and other nice things to write the stuff out to make a habit for correctness early. It's not *that* difficult.
There are a lot of tutorials online that explain the state monad. [Learn you a Haskell](http://learnyouahaskell.com/for-a-few-monads-more#state) is a classic. There is /r/haskellquestions for questions.
&gt; I have a feeling there's a lot more GHC could parallelize than just separate modules. Indeed. It seems that GHC can't (yet) parallelise the compilation of functions within a single module, even if the functions and data types within the module are embarrassingly parallelisable. Our experiments show that we can obtain **huge** speed-ups at compilation time by breaking up larger modules into smaller modules. As an extreme example, I recently had to create Haskell bindings for a REST API defined with a Swagger specification. To start with, I used an auto-generator, and this generated a single module with a Servant-based specification, and a single module with all the record-based data types. Without any changes, it took a 16-core machine around 10 minutes to build (looking at htop revealed that the compiler was mostly just using one core). But after we split up the data types into separate modules (one-module-per-datatype), the compile time went down to around 45 seconds (looking at htop now shows that nearly all cores are at 100% for the entire build).
I am leaning a bit out of the window so I may as well build a dream castle. But this specific problem seems to have a little more structure than just allowing different number of arguments. Assuming the definitions from [euclidean vector](https://en.wikipedia.org/wiki/Euclidean_vector) it would make the first definition `distance: Vector -&gt; Length` and the second function `distance: Point -&gt; Point -&gt; Length`. However Vector is itself: `data Vector = Point -&gt; Point`. Now for me it looks there is a pattern going on. More generally written `f1: (a -&gt; a) -&gt; b` and `f2: a -&gt; a -&gt; b`. This looks general enough that there might be theory (that I don't know). Also euclidean vectors form a Monoid and have some other properties.
Make sure to substitute a definition for `State` in all of the various `Monad` functions you're thinking about. It's a bit easier to understand when you see `(&gt;&gt;=) :: (a,b) -&gt; (a -&gt; (c,b)) -&gt; (c,b)`.
kann you add the code you tried? It's a bit hard to help you short of giving a complete tutorial otherwise
You aren't really doing anything "wrong", but GHC can't determine the type 't' from just 'EmptyMessage'. In order to show your message type it must determine that a show instance for 't' is available, and if it doesn't know 't', it can't. If you supply a type signature such as `print (EmptyMessage :: Message ())` it will work. If you were to type `print EmptyMessage` into GHCI it would work by defaulting the type 't' to '()': ghci &gt; print EmptyMessage &lt;interactive&gt;:7:1: warning: [-Wtype-defaults] • Defaulting the following constraint to type ‘()’ Show t0 arising from a use of ‘print’ • In the first argument of ‘GHC.GHCi.ghciStepIO :: forall a. IO a -&gt; IO a’, namely ‘print EmptyMessage’ In a stmt of an interactive GHCi command: it &lt;- GHC.GHCi.ghciStepIO :: forall a. IO a -&gt; IO a (print EmptyMessage) 
The problem is that in your second case, the compiler can't infer that the type parameter to your type is an instance of `Show`. If you do something like `print (EmptyMessage :: Message String)` then the compiler knows that the type parameter is an instance of `Show`. In a real world scenario this most likely won't be a problem, since you'd most likely be receiving it from a function which would be annotated with the correct type. You could also create smart constructors with a show constraint so that the example works. ```haskell emptyMessage :: Show a =&gt; Message a emptyMessage = EmptyMessage ```
note that, from a certain point of view, your example code in the other language isn't defining one function that take different inputs, but rather two different functions that happen to just have the same name.
Not really a fair comparison - Linux running on that huge hairy thing. What will you get for the same price range for Linux on a laptop or mini?
why can't the compiler infer that `EmptyMessage` is of type `Message` which is an instance of `Show`?
Wow, that's surprising. I'm planning to buy one of the new Ryzen processors coming up in April/March, and I was hoping to get a good boost in compile speed. At least for Haskell, it seems that's unlikely to happen. Still, at least we know there's plenty of room for improvement. It would be much worse if we were already doing the smart thing and the slowness was inherent to the problem we're solving.
Correct me if I am wrong, but here's what I understand: * `data Message t = ...` is a type constructor * ` = EmptyMessage | ...` is a concrete type * ` | Message t` is a value constructor that returns a value/term If this is true, then can't GHC determine the type `t` from `EmptyMessage`?
Assuming you have read up on Monads in general, that is you googled the popular tutorials or are working your way through a good book (I recommend *Haskell Programming from First Principles*), and now you want to get your hands dirty, does this help? {-# LANGUAGE InstanceSigs #-} {-# LANGUAGE ScopedTypeVariables #-} module State where -- What should a "stateful computation" look like? -- I would expect a function that computes a sum over -- a list of Ints to look like this. stateSum :: [Int] -&gt; State Int Int stateSum = undefined -- =&gt; That is, we have a "sum variable" of type Int, and -- we "return" a result, also of type Int. -- In a pure context (outside of a "State" computation), -- what would you expect a function that -- -- 1. takes some "State computation", -- 2. provides you a way to initialize all variables, -- 3. gives you back all the results of interest (above: both Ints), -- -- to look like? -- -- In the case of a moreGeneral :: State s a moreGeneral = undefined -- -- we need to -- 1. take an initial State s a, -- 2. initialize s (for instance, we would initialize the -- sum variable in stateSum to zero), -- 3. return the final value for s, as well as the result of type a. runState :: State s a -&gt; s -&gt; (a, s) -- (s, a) would be fine, too. runState = todo -- Note that the (s -&gt; (a, s)) part is a State computation in of itself. -- The important thing to remember here is that for all the common monads, -- like Writer, Reader, etc., the "run function" is the *inverse* of the -- value constructor. -- That is, we have newtype State s a = State { todo :: s -&gt; (a, s) } newtype Writer w a = Writer { runWriter :: (a, w) } -- Writer-only state needs not be initialized. newtype Reader r a = Reader { runReader :: r -&gt; a } -- Remember that these "getter" functions are actually of type actualType :: Writer w a -&gt; (a, w) actualType = runWriter -- Now to implement Functor, Applicative, Monad. -- It's helpful to make compiler errors more concrete, by giving -- it some additional information on how we would like things to be named. -- That is, using the above language extensions: instance Functor (State s) where -- class Functor (f :: * -&gt; *) =&gt; the 'a' is free. fmap :: forall a b. (a -&gt; b) -&gt; State s a -&gt; State s b fmap = undefined instance Applicative (State s) where pure :: forall a. a -&gt; State s a pure = undefined (&lt;*&gt;) :: forall a b. State s (a -&gt; b) -&gt; State s a -&gt; State s b (&lt;*&gt;) = undefined -- This is the interesting one for now. -- Here's "how I think" when implementing it step-by-step. instance Monad (State s) where (&gt;&gt;=) :: forall a b. State s a -&gt; (a -&gt; State s b) -&gt; State s b -- (&gt;&gt;=) = undefined -- { = Inserting arguments } -- s &gt;&gt;= k = undefined -- "k" is a commonly chosen name for a "continuation". -- { = How do "States come to be"? By applying "State" to an argument. } -- (State f) &gt;&gt;= k = undefined -- { = This applies to the return value too! } -- State f &gt;&gt;= k = State $ undefined -- { = We can make use of "type holes" to make GHC tell us -- the type of the missing value. } -- State f &gt;&gt;= k = State $ _ -- &lt;- We need a (s -&gt; (b, s)) here. -- { = How do "functions come to be"? By introducing a lambda (for instance). } -- State f &gt;&gt;= k = State $ \s -&gt; _ -- &lt;- We need a (b, s) here. -- { = Now we have stated all that we know so far. -- Let's add some type signatures to make things explicit, -- and try to plug things together. -- Remember that the "_" can be plugged not only with a value, -- but also an *expression* of the same type. -- A "let ... in ..." is one such expression.} State (f :: s -&gt; (a, s)) &gt;&gt;= (k :: a -&gt; State s b) = State $ \s -&gt; -- We have an f, but also an s. If it fits, I applies. -- let y = f s -- { = We know we have a pair, so we can pattern match on it. } let (a, s') = f s -- =&gt; Now we have an 'a' we can shove down the 'k'. -- ssb = k a -- { = And again, we can pattern match. } State (g :: _) = k a -- =&gt; Can you see that you are almost done now? -- What is the type of this 'g' we just got? in undefined -- If you want to implement Functor and Applicative, you can work in a similar manner. -- Actually, you can *make use of your knowledge* that you have a Monad. -- That is, you can use do-notation. If you do, you will basically re-implement -- =&gt; You can define fmap = liftM above. liftM :: Monad m =&gt; (a -&gt; b) -&gt; m a -&gt; m b liftM f ma = undefined -- =&gt; Likewise, (&lt;*&gt;) = ap. ap :: Monad m =&gt; m (a -&gt; b) -&gt; m a -&gt; m b ap mf ma = undefined I tried to write down some notions on how I would do it step-by-step. I haven't implemented it all the way though, and probably it's too much already. You definitely should do the work yourself, and for other monad instances as well.
 pls :: source pls = show $ implementation where implementation = your code 
If you go into ghci and do `:type EmptyMessage` you will get `EmptyMessage :: Message t`. There is no `Show` constraint on the type variable `t`. When you say `deriving (Show)` you're saying: If the type parameter `t` I use for `EmptyMessage` is an instance of `Show`, then `EmptyMessage t` is also an instance of `Show`. It doesn't put a constraint on the data constructor to only allow type parameters which are instances of `Show`. You can do that -- put a `Show` constraint on the data constructors -- using GADTs, but even if you did, you would run into an error which essentially says that even though the compiler can infer that all `EmptyMessage`s are showable, it doesn't know which type parameter `t` to pick so that it can choose a `Show` instance with which to display it.
Actually, the GADT solution seems to work fine, the `t` for the empty message case doesn't need a `Show` constraint: {-# LANGUAGE GADTs #-} data Msg t where MkMsg :: (Show t) =&gt; t -&gt; Msg t Empty :: Msg t instance Show (Msg t) where show (MkMsg t) = "MkMsg " ++ show t show Empty = "Empty" main = do print (MkMsg "a") print Empty but yeah, this is probably unnecessary complexity for little gain.
We have been updating our chord extraction and beat tracking algorithms regularly, and currently we are on par with the state-of-the-art as published in the scientific community. However, it's difficult to pin down a single number. One reason is that for a simple pop song we might score 100% correct, but for a free jazz tune, we do rather poorly. Another issue is that even expert musicians do not always agree on what the right chords are: a novice guitar player likely needs different chords then an advanced piano player. We're working hard to handle these cases accordingly, and serve the best chords for every user, and we could use some help with this. Hence the 'we're hiring' shout out. 
Actually, for the Haskell position we require an experienced programmer. However, we also have a front-end developer position available that has different requirements. We're always happy to talk!
If you're in Utrecht, you should definitely drop by for a coffee. We're happy to show you around, and explain in more detail what we do and how we work. 
Haskell doesn't have parameters-based function overloading (it does type-based overloading , via type classes). Though your example feels like two very different functions. I would be worried about mixing them up.
We do indeed, but it's a small one. We mainly do customer support in Austin. So, if you're interested in programming, Austin is not the ideal place for Chordify, I'm afraid.
You're not using these words correctly. Some of the confusion is that two distinct items are sharing the same name, so let me change your names to make this clearer: data Message a = EmptyMessage | SomeMessage a Here, `Message` is a *type constructor*, `Message a` is a *type*, `EmptyMessage` and `SomeMessage` are *value constructors*. Now, when you write `print $ SomeMessage "hello"`, the compiler can see that you passed a `String` to `SomeMessage`, so the only possible type of the expression is `Message a`. But when you write `print $ EmptyMessage`, there is no information about which `a` to use in the type `Message a` - any type could be valid here, and there's no way to know which one you wanted, so the compiler throws an error.
Nice article thx!
Allright sure enough! I'll send you a message so we can set up a time!
I see. So even if I want an "empty" value, I need to provide the type. let m1 = (EmptyMessage :: Message Bool) let m2 = Message True :t m1 -- m1 :: Message Bool :t m2 -- m2 :: Message Bool print m1 -- EmptyMessage print m2 -- Message True 
In my experience, the most confusing thing about `State` is the name, which puts you on a bit of a wrong track. `State` doesn't actually implement state in the usual sense, nothing is mutated anywhere. Rather, the `State` monad abstracts over argument passing. That is, suppose you have a function `f :: a -&gt; b`, and another function `g :: b -&gt; c`, and you compose them into a pipeline `p = g . f` - this is fairly standard and straightforward. But now you also want to pass a *second* value through that pipeline, so your function signatures turn into `f :: s -&gt; a -&gt; (s, b)`, and `g :: s -&gt; b -&gt; (s, c)`. Oh dear, you can't compose these anymore, so you have to write ugly code like this: p s x = let (s', y) = f s x in g s' y You can abstract that into a compose-with-extra-value function, like so: hyperCompose f g s x = let (s', y) = f s x in g s' y And then you may want to write a bunch of utility functions that match the `s -&gt; (s, something)` pattern. Well, it turns out that this kind of function type follows the `Monad` pattern, so it makes sense to actually make a `Monad` instance for it. We'll add some bookkeeping like slap a newtype around it to make that possible, and we write some helper functions to convert between that newtype and "normal" code, so that we don't have to unleash the ugliness on the inside to code on the outside, and, well, that's basically `State`.
Maybe I'm missing something obvious, but could you expand on what exactly you mean by that? Thanks.
state monad is not state in redux, it’s the reducer function actually .
&gt;- use `undefined` for stubbing functions until you need them &gt;- [typed Holes](https://wiki.haskell.org/GHC/Typed_holes) are nice when you are unsure about the type Don't use undefined for stubbing, you can unintentionally leave them lying around, you can just as easily use typed holes as a stub (which I always do) instead, why limit yourself to using typed holes for querying types?
You can try /u/xalyama's reply below using GADTs, but I don't like that solution because it requires you to build your `Show` constraint into the data type itself, rather than letting the data just exist and specifying the show constraint at the use site. I recommend just doing `print (EmptyMessage :: Message ())`.
You don't need to provide type when compiler can infer it. For example, print [EmptyMessage, Message "hello"] works, becaus compiler can deduce than type of `[EmptyMessage, Message "hello"]` is `[Message [Char]]`, thus `EmptyMessage` must be `Message [Char]` here. Of course, there's also `TypeApplications` extension: {-# language TypeApplications #-} data Message t = EmptyMessage | Message t deriving Show main = do print $ Message "hello" print (EmptyMessage@Bool) 
Well, see one of my other comments here. The compile times do get a ton better when you parallelize at the package level. And apparently, you can get the same effect for an individual package by using `ghc -M` to produce makefiles so that `make` can manage the parallelism. I think it'd be interesting to add this style of parallelism to Cabal if it becomes too hard to fix it in GHC
I just wanted to say: Kudos for a spectacularly comprehensive answer. It's people like you that make the Haskell community the assistive and friendly one that it is. Wish I had some gold to give.
Leaving a space after the `@` in `@ Bool` really freaks me out!
You're thinking of the Reader monad, really. State has get and put which correspond roughly to reading/writing to scope variables. It definitely abstracts stateful computations.
Have you tried changing nursery size? Something like `ghc-options: +RTS -A128m -n2m -RTS`?
I haven't really tried to optimize it at all. For one thing, there's no way I'll be able to upstream that into nixpkgs, so it wouldn't be that useful anyway. But also, it seems like if compiler flags would help so much, we should make those the default.
&gt; Likewise unless you amend floats (deviate from IEEE) to make them totally(/partially) ordered they should not be members of Ord(/POrd). Any classes where floats don't even try to obey the laws they should stay away from. I think that's not so much a deviation from IEEE, as it is a legitimate alternative IEEE operation. The IEEE 2008 standard defines a total order predicate. I've never used it, or even looked at it all that closely, but I believe the associated equality operation is bitwise equality. So, if a total ordering is required, it is already there. That is not intended as an argument to get rid of the IEEE unordered comparisons, but there are other choices for floating point Ord instances. I agree about Enum, even though I'm a numerical analyst and usually like to see floating point well integrated into a language. It's very hard to get something like that to behave reasonably for floating point types in all cases. The current implementation compounds the intrinsic difficulties by adding more rounding than necessary and a strange termination criterion. One thing that doesn't seem to be mentioned often when there is discussion of removing floating point instances is that moving operations to one or two special IEEE classes would not work any better than lumping all Integer operations into one or two classes. There are still valid reasons to want operations for real and complex scalars, vectors, matrices, quaternions, polynomial rings, etc. If you exclude floating point from the standard type classes, you almost need a hierarchy of parallel floating point type classes like IEEENum, IEEEEq, IEEEOrd, IEEEFractional, IEEERealFrac.
I was trying to represent an abstract syntax tree for a programming language implementation. I wanted each node in the tree to carry some metadata, for example a reference to the span of source code it's referring to. Different phases of compilation are going to require different metadata payloads. So you want your AST type to be generic over the metadata you're associating with it. And maybe some desugaring phases are going to remove certain constructs entirely, so the mapping of metadata types to AST types is many to many. By this point you're starting to complicate your AST type. The solution using `recursion-schemes` was simple and clean. Use one functor to express the structure of your AST, another to express the metadata payload. Compose them, and take the fixed point; voilà, you have your tree-with-payloads. Both the AST and metadata types are independently swappable.
Well, yeah.. looks odd. Good point.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [NixOS/nixpkgs/.../**default.nix** (master → e401af5)](https://github.com/NixOS/nixpkgs/blob/e401af5f980f38d72e130a75ee55c3d01a627996/pkgs/development/haskell-modules/default.nix) ---- 
I'll try to be there, thank you.
Any progress on dependent types? It was mentioned that it could ship in 8.4: https://typesandkinds.wordpress.com/
Wait -- I think I'm not clear on exactly what's going on: So for the initial part when you download and install all the deps, you can use `cabal/stack -j` and get nice benefits, right? But once that's done and you're making changes and rebuilding your project, that's where issues show up with very suboptimal parallelization?
Try this: {-# language ExtendedDefaultRules #-} default (()) data Message t = EmptyMessage | Message t deriving Show main = do print $ Message "hello" print EmptyMessage This way `EmptyMessage` just by itself will be interpreted as having type `Message ()`, which is entirely correct. At the place of `default (())` you can also try other things like `default (Int, Bool)`.
Well, if it helps, I'd explain it like this: there are (many) people who are not actively participating in a language community, but are still using the language. Keeping up with a language community takes time, and is sometimes actively boring, not everyone is going to do it. For someone like that this kind of internal strife is a pain to unpack. Is it technical? Is it just people not getting along? Is it going to be a problem? These are some questions you can't answer, when you don't actually know who the people disagreeing are, and what role they play in the community. That's my point of view, as part of a silent (likely) majority who just wants to get on with things, and not have to figure out why are people fighting.
Is this compiler open source? I'd love to take a look. Also, w.r.t metadata, are you using/familiar with the Trees That Grow approach using type families? If yes, how do the two apporaches compare (the one you're following vs TTG)? 
Yeah. Parallel cabal works great. I was surprised to see this conversation since I was under the impression that work on parallel GHC had mostly fizzled in favor of parallel cabal. But these days I get antsy when any package has more than a dozen source files, so your mileage may vary.
Everyone has given you some great info, just going to throw in my 0.02. It would seem that Haskell should just be able to know that `EmptyMessage` is always going to be the string "EmptyMessage" and it could then not care about the concrete type and just output "Empty Message". BUT with something like `FlexibleInstances` you could actually have different messages depending on the type, though I don't recommend it AT ALL. Example: #!/usr/bin/env stack -- stack --resolver lts-10.3 script {-# LANGUAGE FlexibleInstances #-} data Message t = EmptyMessage | SomeMessage t instance Show (Message String) where show EmptyMessage = "This is an empty message where a string should be!" show (SomeMessage str) = str instance Show (Message Int) where show EmptyMessage = "What? no Int!" show (SomeMessage i) = show i i :: Int i = 42 s :: String s = "What is the meaning of life?" main :: IO () main = do putStrLn $ show $ case Nothing of Nothing -&gt; EmptyMessage Just _ -&gt; SomeMessage i putStrLn $ show $ case Nothing of Nothing -&gt; EmptyMessage Just _ -&gt; SomeMessage s putStrLn $ show (EmptyMessage :: Message Int) putStrLn $ show (EmptyMessage :: Message String) 
Read again. Reader abstracts over context that only goes into functions, but not out: `s -&gt; a -&gt; b`. State abstracts over context that is passed into functions and comes back out, potentially modified: `s -&gt; a -&gt; (s, b)`. State implements "state" in the wider sense, but usually, when people talk about state, they mean *destructively mutable state*, which is what things like `IORef`, `MVar` or `TVar` provide. State doesn't abstract over this kind of mutable state, it abstracts over state that is modeled as plain old immutable data that gets passed around, and the "mutations" are conceptually just functions of type `s -&gt; s`, nothing is mutated. The point being, understanding `State` is easier when you consider it not an abstraction over state, but an abstraction over passing additional values through functions.
I’m currently going through the same learning process and I agree that examples for some concepts are sparse and/or outdated. I’m working on a rhythm-game using Yampa / sdl2 / OpenGL, with mouse and keyboard input. I’d be happy to share my source with you, but I can’t make it public yet as it’s a Uni Project. I’m planning on releasing the source once I’ve submitted; would people be interested in an blog post? (I’ve always wanted to try my hand at writing one on Haskell)
That's interesting. I'm surprised that people "who are not actively participating in a language community, but are still using the language" even *know* about this issue.
Add a bookmark to `https://www.stackage.org/lts/hoogle?q=%s` with keyword `h`. Then you can type `h map` into your address bar to do a Google search.
I could think of defining a rose tree à la `newtype IntMap a = SmallArray (IntMap a)` and have the invariant that leafs have length 0, while internal nodes have length 4. That should get you the best of both worlds: The constructor tag is implicit in the SmallArray's length and the elements follow [directly after](https://github.com/ghc/ghc/blob/f00ddea96cc856654ac90fcf7d29556a758d6648/includes/rts/storage/Closures.h#L151). Basically how an unboxed sum type would work.
&gt;The *typed holes as stub* approach will barf warnings at you and quickly clutter your console or your editor's error buffer, making it progressively harder to filter out the warnings or errors you are actually interested in. Yes, which is why I implemented -fdefer-typed-holes and -fno-warn-typed-holes two years ago to make sure my editor doesn't show me all these uninteresting hole warnings ;) 
Seconding this. It's a full time job to both engineer an application and onboard new folks to a new language. You will not have time to be an engineering manager *and* take these responsibilities on. Another note: definitely stick with the simpler end of Haskell. Use libraries that are really well documented and have great error messages. This means avoiding stuff like Servant, lens, opaleye, etc. The extra fancy you get from these libraries just adds weeks or months onto your novice Haskeller's ramp to productivity.
Fair enough, I think we're just disagreeing on a matter of semantics. My point was that, though State has no "true" mutability, it does model it: You can effectively simulate imperative programming as a chain of transformations from `State S ()` onto itself, for some type S that represents your mutable scope.
&gt; pick up some common design patterns that you can quickly reach for, personally I like the ReaderT Design Pattern and Freer (I would say to check out freer-simple and read up on Free, since most concepts from Free-style apply to Freer/extensible-effects) Thank you for these pointers! They seem like the "Less of a hazzle than mtl, but much more sane that doing unsafe stuff and still mantainable enough for transition into a more serious codebase without too much change"-way. Is the ReaderT Design Pattern a viable choice for middle-sized projects?
Sorry I put those as another way to show that Haskell will infer the type then forgot. That is something I didn’t know though so I did learn something ;)
A habit for correctness is not the issue - I found hacks in my code aesthetically insulting. But I too think that there is a balance to strike. While some thing may be a bit faster to type, they may impose to much dirtyness on my code. Where that balance lies, I will have to figure out with time and practise. 
A habit for correctness is not the issue - I found hacks in my code aesthetically insulting. But I too think that there is a balance to strike. While some thing may be a bit faster to type, they may impose to much dirtyness on my code. Where that balance lies, I will have to figure out with time and practise. 
I still think the current Ord for Float is pretty dangerous. Sorting floats and putting them into ordered data structures can go wrong in very strange ways if a `nan` shows up, potentially destroying a variety of desired invariants. If we declared that floats were morally `Maybe NonNaNFloat` and then made `nan` the smallest element, creating a true total ordering, I would honestly probably be ok with that. But the current Ord instance is just blatantly lying in ways that go far beyond rounding errors. 
I found a trivial example using key press events. Maybe you are interested: https://ocharles.org.uk/blog/posts/2015-09-07-announcing-sdl2.html
&gt; It's the same as any other language, but with the compiler catching some of your mistakes. Not same same tho - If one has just three hours to code a basic webapp, the (horribly unsafe) possibility to access a database, print out a string and send an email at whatever place is quite easy to do in LISP and others - while it requires more effort in Java.
nice. I really like your approach. For a beginner like me, it just "works".
You are basically my personal hero right now. Thank you.
While it might get you a bit more familiar with GADTs, I wouldn't really recommend this approach for this use case. It makes you unable to create messages of types which have no `Show` instance. A prominent examples is function types: `a -&gt; b`, which seems a very useful thing to have. The approach seems a bit in violation of [separation of concerns](https://en.wikipedia.org/wiki/Separation_of_concerns), which was mentioned in this thread earlier. Personally, I think you ultimately you have to bite the bullet and assign it a type in situations like `print EmptyMessage`.
As I'm writing this my Mac Book Pro with 16GB of RAM is building two GHCJS projects and one GHC project at once and it's consuming ~10GB RES. So I would definitely recommend 16GB of RAM. (Note that my MBP is running NixOS natively). 
Keep in mind that 'participation' and 'consumption' are two different things. Lots of people read avidly but don't feel they have meaningful input, or feel that someone else already said it, and so stay silent. Similarly, it is not uncommon for newcomers to test the waters of a community by lurking for an extended period, and such lurkers do not often wish to engage in forum archaeology to unpack 'hot' issues like the one we're discussing. Community discussion provides lots of value to people who may not be actively participating in the discussion - That's true of most groups of humans past a certain size, regardless of being 'online' or not.
Really nice, keep up the good work. Thanks a lot!
Don't settle on ugly. There are tools to "move fast and make broken things unrepresentable".
At this point, the right people to talk to are the haskell website working group: http://neilmitchell.blogspot.com/2016/11/the-haskellorg-website-working-group.html
`foldr (&lt;|&gt;) Nothing $ zipWith ($) [case1, case2, case3] (repeat val)`
I think /u/ryantrinkle could coordinate having someone give them a look and acting on it at this point?
I use [dante](https://github.com/jyp/dante) and seeing how [lsp-mode](https://github.com/emacs-lsp/lsp-mode) + [lsp-haskell](https://github.com/emacs-lsp/lsp-haskell) work alongside dante.
Maybe the letter of the PVP, but definitely not the spirit :) 
Yeah I've thought about that. It would allow for a lawful `Ord` instance right? Sounds good. And you could make a good `Num` with errors on inexact. Not as nice.
No, it is not acceptable to throw up my hands and declare the problem to be so complicated as to be "beyond truth". Now, I don't want to make the case that PVPs definition of "correctness" is practical for maintainers. What I do want to say is that precise definitions are important, because only those can be checked automatically by a machine. The definition you offer, at least in the way you write it, has the big drawback that is not checkable by machine without a precise definition of "context". I mean, ultimately, the goal is to make sure that a package compiles and works. The PVPs definition of "correct" has the benefit of implying that the package will compile. If this is not practical, fine; it's just a definition. Perhaps we are talking past each other a bit, when I say "correct", I don't mean that it's the thing we should do, only that it adheres to a definition. Whether "we should do it this way" is a different question — but informed by the definitions and their consequences! (In the case of Hackage, the consequence of the PVP definition of "correctness" is that the plans found by cabal-install will compile. If we omit upper bounds and upload packages that are "not correct", that is no longer the case. Again, whether we should do it that way is another matter.)
If you want to thread the error result, you can use `catchError`: answer = either (const 17) id $ case1 val `catchError` \e1 -&gt; case2 val `catchError` \e2 -&gt; case3 val You can turn this into do notation using the continuation monad: runFail = flip runContT Left contError = ContT . catchError answer = either (const 17) id $ runFail $ do e1 &lt;- contError $ case1 val e2 &lt;- contError $ case2 val contError $ case3 val Probably the added complexity is not worth it, but it's an interesting tool.
I upgraded my laptop from 8Gib to 16GiB of RAM because I otherwise could not load my project (~50k SLOC) into ghci without swapping. I think the only reason I need more than 8GiB is because Happy-generated parsers require weirdly large amounts of memory in ghci. In general, GHC is pretty memory-hungry, so that's probably the main thing to worry about.
Aha! This is exactly the sort of example that I've been envisioning. At the moment, I would say that your example is somewhat supported. You could easily define nodes to be of type `String` and edges of type `SomeExchangeRate`, but this would lead to some duplication of information because `SomeExchangeRate` already stores the endpoints. At the very least, your path would be upgraded from `[Int]` to `[NodeType]`, where `NodeType` can be anything that is hashable. On the other hand, `SomeExchangeRate` is effectively an Edge type by itself. I.e. it holds two end points and a label between them. I don't currently support arbitrary edge representations, but I think I fairly easily could. I think all it would require is a class with methods `toHead` and `toTail`, which splits the edge. These make perfect sense in this case as you would end up with the half edge `From "USD" 1.25` attached to the node `"GBP"`. And then recombining them into `SomeExchangeRate` is straightforward. I haven't personally used the type-level stuff happening in `ExchangeRate`, so that would take some research for me to answer. So I would say this example is possible, but not as good as it should be right now. My goal was to support uses just like this, so I will definitely work to make this nicer. These examples are immensely helpful, thank you!
You can convert from a TF based API to a fundep based one with one or two lines of code. class (Recursive t, Base t ~ f) =&gt; RecursiveFD f t | t -&gt; f instance (Recursive t, Base t ~ f) =&gt; RecursiveFD f t | t -&gt; f or even type RecorsiiveFD f t = (Recursive t, Base t ~ f) but you can't come back without type family instance for every line. Those functionally dependent arguments can leak into types and become permanent fixtures you can't remove to get simpler types. Consider, the rather contrived example forgetting :: MonadState s m =&gt; m a -&gt; m a forgetting m = do x &lt;- get result &lt;- m put x pure result Notice how the type signature of forgetting has to mention `s` even though it doesn't care about it? This is fine for simple examples, but when you start picking up, say, a [CCC](https://hackage.haskell.org/package/category-extras-0.53.5/docs/Control-Category-Cartesian-Closed.html) class where you now have a hom type that acts like (-&gt;), a product functor that acts like (,), an internal hom e also (-&gt;) in this case, but different for other categories, a unit for the product like (), etc. and they are all FDs things devolve into an unholy mess of parameters `CCC k p e i` you have to pass around to every operation and that you can never eliminate, and if you build data types in a way that is generic in the category you can't just parameterize it on a single type `k` that determines all of those others. You have to pass all of them all the time, even the parts you aren't using. If it wouldn't break everyone everywhere and break third-party Haskell compilers, it'd be slightly more powerful if the `mtl` types were built around TFs and then exported a shim that converted them down to the FD API that we all know and love. The fact that the `reflection` API is fundep based rather than TF based is occasionally a pain point as well. **tl;dr** With FDs, the definition gets a little shorter in exchange for type arguments you can never ever eliminate. With TFs you can build the fundep-based API as a one-time shim on top. 
You can use `Monoid t` instead of `Typeable t` and write the whole thing in Report Haskell: instance (Show t, Monoid t) =&gt; Show (Message t) where show t@EmptyMessage = "EmptyMessage: "++ show (pull t) where pull :: Monoid u =&gt; Message u -&gt; u pull _ = mempty
Trig functions are technically computable, btw.
 Python 3.6.3 &gt;&gt;&gt; round(float('nan')) Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; ValueError: cannot convert float NaN to integer 
I had the exact same question (Redux background, not understanding State monad) and finally felt like I really understood this recently. One of the key things is that State is much more accurate to a State Machine than Redux. A reducer is an efficient way to update a state, but the State Monad models an actual state machine. The difference is that a State Machine has outputs. This is not to be confused with your view layer (React) in Redux, that’s a mapping of your state. An output is kind of like an event in response to the input. Like Vending machine returning your money or dispensing change. That’s not something you want do do based on the state, because you want to make sure it happens exactly once. If you’ve used Redux loop, effects are basically outputs. Outputs can be used to connect state machines to make them work together. Now to the state monad. The Random Monad is what helped me understand the State Monad. The “state” is the current seed, and the “output” is the random number. You can then connect this output to another machine to make it more useful. Theoretically, the output of the Random Monad could _probably_ just map the state to generated value, but the State Monad is more flexible.
I've got that notification starred in my inbox and will be looking at it in due course. Sorry for the delay!
Nix is not going to be the solution to this problem, likely ever - A language ecosystem should probably not 'only function' in a *nix like environment. Nix leadership has show exactly 0 interest in trying to get their project up and running on any system that isn't Linux or MacOS. Even setting aside Windows (which is not actually tenable to do for about a million reasons), nobody should assume a non-portable ecosystem for their language from the ground up. It's just a bad plan. History is littered with mostly dead languages that made this mistake in the past, and it's hubris to assume Linux is immune to the future.
Yeah 16GB definitely seems like the minimum. Any experience on how that behaves with intero? And that's cool you're running NixOS natively on an MBP. I've been tempted to try that, just a bit nervous on running into issues with nix. 
I will definitely give that a try! Yeah it seems it's well known that intero is pretty heavy, so an alternative could be useful. I'll try and get that working and see how it goes
Yeah, definitely becoming clear to me that 8GB just doesn't cut it if you want decent performance on large haskell codebases
I would do it like this: pick Python and let your team write code as functional as possible, that means they should use list comprehension, unpacking, itertools, lambdas, avoid mutuable variables wherever they can, and so on. You can even validate the input of functions using `isinstance`. When they finally got that you can bring the project to the next level and introduce Haskell. Transporting the code from Python to Haskell should be relatively easy as it is already functional. 
well, it's on twitter. I don't follow the reddit, but if a controversy makes it as far as Twitter, I'm likely going to see it. 
Did you try `ghcid`? It has a VS Code plugin and it works great for me. I tried `intero` and never really got it to work reasonably (it only occasionally provided a type error etc and mostly said "Loading...").
It's strict, but it has a reasonably full set of type classes. It is generally compiled to javascript and most of the ecosystem is focused on browser stuff, but it runs on node just fine and there is a c++ backend if you really want a binary for whatever reason. Overall, it seems pretty nice to me.
It turns out that Skylighting is GPL'd because it uses XML sytnax definitions from a GPL'd project and generates parsers from them, so relicensing Skylighting isn't possible.
Well, then say why!? Nan would be bottom.
I added a demo program screenshot: https://github.com/jtdaugherty/brick-skylighting/blob/master/README.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [jtdaugherty/brick-skylighting/.../**README.md** (master → 0a130b6)](https://github.com/jtdaugherty/brick-skylighting/blob/0a130b6d4d7dd8715808531061bdb6a3dbb5e9e1/README.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dt8nzt5.)
Definitely! In fact, that's the whole motivation of the author, to give a nice design pattern one can reach for, for large scale real world applications. From the analysis part at the end, &gt; While the technique here is certainly a bit heavy-handed, for any large-scale application or library development that cost will be amortized. I've found the benefits of working in this style to far outweigh the costs in many real world projects. Also, if you weren't familiar with them, FPComplete is a consulting company doing Haskell, and Snoyberg (the author) is a very prolific Haskeller with lots of good libraries and frameworks, like conduit and Yesod to name a few :)
Floating point "equality" is funny stuff. It's approximate!
Not at all. Addition is. Multiplication is. Comparison is not. &gt;&gt; 3.999999999999999 == (4.0 :: Float) False
thanks, it worked!!! btw, I couldn't get an emulator working: https://github.com/sboosali/cards/blob/master/emulate.nix https://github.com/sboosali/cards/blob/master/notes.txt#L105 but that's probably just my ignorance about Android development, which I've never done. I just manually copied the APK onto my Android, which installed successfully. Still somewhat incredulous that all this "transpilation" and cross-compilation actually worked. Thanks again, for all the work you, Ryan, and others have done on this. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [sboosali/cards/.../**notes.txt#L105** (master → 87efe05)](https://github.com/sboosali/cards/blob/87efe05a17919f84a271c3cccbb8daacf3b1e148/notes.txt#L105) * [sboosali/cards/.../**emulate.nix** (master → 87efe05)](https://github.com/sboosali/cards/blob/87efe05a17919f84a271c3cccbb8daacf3b1e148/emulate.nix) ---- 
It doesn't really explain the state monad well. You can stare at the code for a while and try grok it, but there aren't enough intuitive examples of any of the state/reader/writer monads IMO.
These three I'm comparing are kind of fair in the sense that they're the most usable dev computers I could find, with no price limit. I bought two used Thinkpads to try as Linux desktops, and I'll be selling them: https://www.reddit.com/r/linuxhardware/comments/7pt7k3/ama_linux_on_thinkpad_x1_carbon_2nd_gen_450_ebay/ I bought a fully maxed out MacBook Pro 13" Touchbar (over $3,000), and returned it. I read every review I could find of the [System76 Galago Pro](https://system76.com/laptops/galago). Seems decent enough, but I'm a little burned by buying laptops that I can't test in person first! And I'm super picky. The only downsides to the Galago Pro that appears in the reviews is short battery time. Those definitely are in the price range, FYI. As far as the Mini-style form factor w/ Linux, there are really cool options: See the [NUCs](https://www.newegg.com/Product/ProductList.aspx?Description=intel%20nuc&amp;Submit=ENE) or the [System 76 Meerkat](https://system76.com/desktops/meerkat).
I can comment a bit on RAM. I used to develop with 8GB on Linux. Was definitely constraining. Here is my current setup: I have 24GB and run 2 operating systems at the same time, Windows as host, and Linux in a VM as guest booted from a partition with raw disk access. Linux gets about 10.6 GB, Windows gets about 13.4 GB. I almost always develop Hs on the Linux side, and the RAM-wise upside is that I run almost nothing else in the VM. When I work on a single project that ~11 GB is mostly enough, but when I start to leave a ghci open here, a ghcid open there, hopping from codebase to codebase, it can get insufficient. I too am wondering if I would want to perhaps upgrade to 32GB or 40GB and give more to the VM. 
Right, if we’re going to allow things like NaN I can make no defense of any of this. I never even consider supporting it in any program I’ve ever written. It makes no sense to tell other players to meet you at (NaN,NaN) coordinates or to check overlapping boxes with NaN vertices, etc. It’s sort of like how String in Java is actually either String or Null, but the only sensible thing you can do is simply say “that’s a crazy language decision, I’m not supporting that, I’m going to write in a self-consistent subworld where Strings aren’t null.”
I'm personally a huge fan of TikZ but the learning curve was extremely steep for me.
As is, it's a viable competitor to GHCJS, and you can use it for server-side stuff if you are ok with node.js and an immature ecosystem. With enough work, it has the potential to compete with Haskell in general (the ecosystem stuff will get fixed if enough people write good libraries, and there's no inherent reason why the c++ backend has to be worse than haskell overall). Server side purescript may never actually get established enough to be a viable haskell competitor, and you'll have to use explicit thunks and lazy lists when you need lazyness, but you'll also have an easier time reasoning about space complexity in general. And you get sane record types.
Yes-ish. The base Functor needs to be law-abiding, plus `project . embed` = `id` = `embed . project`. Everything else is basically a free theorem. :)
Euclidian `Vector`s are *not* the same thing as `Point -&gt; Point`. A vector is equivalent to a translation of points, which is a very small subset of all possible `Point -&gt; Point` transformations.
Dante uses GHCi which merged in the features from a ghci fork I wrote and later I renamed to Intero. They’re basically the same code and collect type information for type querying and go to definition. There’s nothing heavy about Intero the executable. The Emacs Lisp mode is a larger file because it has more features than Dante. This is the meaning of lightweight for Dante. 
`doctest` is really awesome! I've already using in one of the projects I'm working on. And experience so far was great. Thanks for your feature regarding pretty-printing. This might be useful :) But I really miss this feature :( It blocks usage of `doctest` in another of projects I'm working one... * https://github.com/sol/doctest/issues/154
I just use a different prompt in those cases. 
I have 16gb RAM with fastest i7 6th gen and NixOS native. I could not work smoothly with GHCJS + GHC simultaneously on same machine. The chrome browser had lag, and there was swap usage. So I moved the ghc part to a another machine, now things are good again.
Hi thank you! I didn't expect to get this much help from the thread, Haskell community is awesome! Thanks for keeping me motivated to learn Haskell, I will sit down and read everything properly this weeked
Wow. That should close this issue and be recommended as solution for this problem.
But the issue is that you still have to decide how you want to handle `nan`; from what you are saying I'm assuming you'd want `nan` to be `_|_` when inspected. Which in some contexts is fine (but it does make `+` and `*` partial which is unfortunate), but in many others it is not practical, as we want to actually catch `nan` or ignore it, which you should not do with `_|_`. I am just definitely not OK with `nan` breaking various invariants through unlawful instances. Crashing things or making them `Nothing` is much more OK, but invariants are crucial for reasoning about behavior. 
Yeah I remember when that was happening. But wasn't that argument more about trying to get haskell.org to present Stack as the only solution to getting Haskell? My changes don't really favor one or the other, they just present all options in a much more concise way.
Great! Thank you.
On the subject of custom GHCi stuff, is it possible to make GHCi use a different monad than `IO`? I don't care if I have to write a custom program round the `ghci` library; I'm interested in writing a library for writing shell scripts in Haskell, and I want to focus on usability from the repl. `-interactive-print` lets me elide boilerplate in the case that they're just running a command, but doesn't let them bind results: `foo &lt;- bar`
I think you'd do well to benchmark against other data structures. HashMaps are frequently not the right choice in Haskell because they come with a lot more copying than, say, `Map`. Like Vector, their main performance advantage is in fast indexing, often compromising in most other areas
 -- | Haddock comments look like this, -- with a single pipe at the top. They are meant to be processed by the haddock tool ([docs](https://haskell-haddock.readthedocs.io/en/latest/index.html))
I'll address this in two parts. First part, how to use `fromIntegral`. Let's say you have a function `log` which is of type `Double -&gt; Double` but you want to take the log of `a :: Integer`. If you write `log (fromIntegral a)`, that will let you do what you want. For the second part, you say you want to compute `log a + log (a+1) + log (a+2) + ... + log (b-1) + log b`; it's not quite clear to me how the a magically turns into a b, but that detail isn't super relevant so I'll pass it up for now and assume you can figure out how that works :) However, your recursive function does not calculate anything close to that, unfortunately. What your function does is recursively vary `a` and `b` and then eventually executes `log` once (or returns 0). It sounds to me from your description that you want something very different. Sort of like the difference between `sum [1..10]` and sumNums :: [Integer] sumNums [] = 0 sumNums [x] = x sumNums (x:xs) = sumNums xs
pkg-1.0 itself doesn't change though, right?. The list of transitive dependencies that cabal-install installs for it by default might change (if I understand correctly). And I really haven't heard anybody explain why that is an issue given that it's trivial to freeze your dependencies. How is this not a red herring wrt the question of "forking" hackage?
Yeah sorry, I tried to only include the important parts but full assignment description is here: &gt; Problem 3: Write a function called logSum that takes two Int parameters a and b. Its value must be the sums of the natural logarithms of all the numbers between a and b inclusive. In other words, the value must be log a + log (a+1) + log (a+2) + log (a+3) + … + log (b-1) + log b. If a == b, the value should be log a. If a &gt; b, the value should be zero (since there are no integers between a and b in this case). The result of logSum should be a Double. To compute natural logarithms, use the Haskell log function. I'm not allowed to use lists or strings in this assignment. I understand how to use fromIntegral now though thanks to you and am trying to solve the recursive part (recursively adding all the sums together)
Yeah, i finally understand how to implement fromIntegral and am now trying to figure out adding all the sums recursively
&gt; Happy-generated parsers require weirdly large amounts of memory https://ghc.haskell.org/trac/ghc/ticket/14683
Try the sum first without the log. sum :: Int -&gt; Int -&gt; Int sum a b | a == b = a | a &gt; b = 0 | otherwise = -- a + sum of the rest of the interval 
Why do you think diagrams doesn't have all these properties? It's pretty extensive.
Hi, I am an experienced haskell programmer, and also a master in music (piano). I have a bachelor in music theory (counterpoint and fugue). Designing new algorithm would definitely be my cup of tea :-) However if I am required to listen to pop-songs all day, that would be a bit of a turn-down. I"d be mostly interested in the algorithms and how to turn them into haskell.
One have to use of fork for it to work?
Finally figured it out, thanks for your help!
Finally figured it out, thanks for your help!
&gt; (\(_, a) (_, b) -&gt; compare b a) Unpacking this, it looks like you want to sort based on comparing the second value of the tuple, in reverse order. You may consider rewriting this as `comparing (Down . snd)`, where `comparing` and `Down` are as defined in [`Data.Ord`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Ord.html). `Down` is pretty neat, as it wraps any value and provides an `Ord` instance where comparisons are flipped. Going to GHCI just to verify: &gt; Prelude Data.Ord&gt; import Data.Ord &gt; Prelude Data.Ord&gt; :t (\(_, a) (_, b) -&gt; compare b a) &gt; (\(_, a) (_, b) -&gt; compare b a) &gt; :: Ord a1 =&gt; (a2, a1) -&gt; (a3, a1) -&gt; Ordering &gt; Prelude Data.Ord&gt; :t comparing (Down . snd) &gt; comparing (Down . snd) &gt; :: Ord a1 =&gt; (a2, a1) -&gt; (a2, a1) -&gt; Ordering 
Last time I checked my layout cannot depend on the size of text because some backends can't provide it ahead of time.
From your profile you should be able to get a job in my team at Google Zürich: https://careers.google.com/jobs#!t=jo&amp;jid=/google/technical-solutions-engineer-google-google-building-110-brandschenkestrasse-2944290094&amp; We do not yet use Haskell in our team, but with more people we can reach critical mass to at least use Haskell for our internal tooling. Also the job pays good enough to also live from 80% or less and thus you have a lot of free time for your projects. Make sure to apply through me and not through the web site! thk at google punkt com.
Is there a typo in your type signatures? Your lambda one isn't identical to the point free one (the point free one is missing the a3). Unless it's always true that a3 ~ a2 is valid in this case?
I have something in the pipeline. :)
This is why we should have `safeHead :: [a] -&gt; Maybe a`. Using this, they would once again be equivalent.
You could use the ["success monad"](https://hackage.haskell.org/package/errors-2.2.2/docs/Data-EitherR.html#t:EitherR) or locally [swap](https://hackage.haskell.org/package/either-5/docs/Data-Either-Combinators.html#v:swapEither) success and error cases.
Question for a beginner like yourself: Where would you have hoped to find this kind of information, short of asking for it on forums? i.e. where did you look before coming here? I think it'd be really good if we put excellent answers like these in the places that beginners go to first.
Docker is native on OS X these days.
 `on` from Data.Functions can be useful in situations where there isn't the equivalent of comparing: comparing `on` snd
Reproducing these examples sounds like a great part 5, or, 6, or.... ;) Just saying, if you get stuck for material!
If you consider this cheating you shouldn't do unsupervised assignments at all. Because I can guarantee you half your students cooperate on such assignments one way or another and there is absolutely no way to prevent this.
Fair 'nough. I only mention mania because I have personal experience, I know what it looks like and feels like, and happily I am no longer afflicted. I might have benefited from someone calling me out on it at the time. It is true, probably wouldn't respond well to an opponent in an argument.
Changes to the cabal metadata could have many effects - https://hackage.haskell.org/package/acme-mutable-package There are multiple concerns that could be solved by having an alternative way to store cabal packages. This is just one, so it is not a "red herring". In stackage we can have the precision of specifying the exact cabal metadata SHAs. However, we cannot expect that people will do this in the `extra-deps` section of their stack.yaml. So the mutability of `pkg-1.0` can affect reproducibility of stack's build. Usually the change is just version constraints. Maybe we should default to only warning about violated version constraints, but then when using stack there'd be little incentive to update version constraints such that cabal could do the same build plan.
&gt; Now since one of the main things people like `Enum` for is the ability to do `[a .. b]` and `[a, b .. c]`. For that we could just have convenience functions that do the same thing for non-enum types, such as based on `Group + Ord` `(\a b c -&gt; takeWhile (&lt;= c) $ iterate (+ (b - a)) a)`. I agree on the general idea of convenience functions. But you wouldn't want that implementation for floating point. A similar use of iterate is actually one of the rounding problems with the current `Enum`. Usually in numerical applications you want to partition an interval. In some sense, `[a,b .. c]` is the wrong interface for that. You're better off with some more specific function that takes the end points and the number of subintervals. There is no ambiguity about where you start and stop. &gt; I could perhaps expose a single Float class that gives you `+.`, `*.`, `**.`, `isNaN`, etc. This seems to transfer one of the bad properties of `Num` over to your hypothetical `Float` class: It starts well with various ring-like operations `+.` and `*.`, but then adds more operations that can't be sensibly defined for every approximate ring. If you want to approximate the ring of polynomials over the reals with a type `Polynomial Double` then you would want `+.` and `*.` defined, but `**.` has no reasonable definition. Same thing for a `Quaternion Double` type. I'm not sure if you meant `/.` to be covered in `Float`, but it makes sense for `Double`, `Complex Double`, and `Quaternion Double`, but not `Polynomial Double`. Most of the algebraic distinctions that make people want to split type classes along algebraic lines apply to various sets defined over the reals, which then have corresponding approximations using `Double`. That's before even getting into vector spaces, matrices, and matrix groups. Even if floats are in `Ord` and `Eq` based on a total order, then you probably still need to define the IEEE unordered comparisons somewhere else for compatibility. Unordered equality would need to be defined for `Double` and `Complex Double`, but the unordered `&lt;=` wouldn't have an instance for `Complex Double`. So those couldn't all live in the same class. The classes proliferate quickly if you want to work with a variety of numerical types. Lumping all this together into a single catch-all Float class for things that are seen as too lawless to go anywhere else would be very painful for anyone who wants to write polymorphic numerical code. You would would have instances with things like `_ **. _ = error "(**.) not defined for whatever type."`. With the available type system, having the compiler point out that I ran `(**.)` on a `Polynomial Double` instead of a `Double` seems like it should be an achievable goal. 
I [don't see any flags that would help with that as of 8.2.2](https://downloads.haskell.org/~ghc/8.2.2/docs/html/users_guide/flags.html#interactive-mode-options).
Yea I'm guessing that it would require making a custom program around the `ghci` library. Don't know if that library exposes enough internals to do it or not...
Arguably `comparing` shouldn't exist because using `on` is so easy. Perhaps we can define `ing = on` and write compare `ing` snd :)
 flip (compare `on` snd) also reads nicely 
A euclidian vector is an equivalence class^(*) of point-pairs. The most natural section for this quotient set maps each equivalence class `c` to `(O, p) ∈ c` where `O` is the origin. Obviously, such a pair `(O, p)` is equivalent to `p` alone, so each vector uniquely corresponds to a point in the space. ^(*) two point-pairs `(x1, y1)` and `(x2, y2)` are deemed equivalent if one point-pair can be turned into the other with a simple translation. **TL;DR** a `Vector` is a `Point`.
Yea, I tried Haskero (Intero) with VSCode recently too, and I was surprised how slow it was even for displaying type sigs. Is that a known issue? Is it quicker with vim/emacs for some reason?
The length only matters insofar as it gives them enough time to accomplish something useful. I don't think I've ever actually cared how many months of experience an intern candidate has had.
Being able to answer a design question in an interview is good, but being able to explain how you designed a prior project that you took ownership of is even better.
Hi kuribas, the job we are currently offering is mainly a Haskell job and not an music researcher job, I'm afraid. We already have some excellent researchers on staff. What we really need is someone that can port their prototypes to efficient Haskell code, and maintain the remainder of the Haskell infrastructure. 
Fantastic, this is exactly the sort of thing I was looking for. Thanks!
Derp. Thanks!
If rebindable syntax works for GHCi's `&lt;-` I can probably just make it actually do what I want it to do. Good suggestion, thanks
Thank you! * I did consider writing `sequencify` with `scanr` but I didn't really like doing `dropLast`, isn't dropping from the end of a list (or any list-end operation) less efficient? * +1 on `countMapToSortedList`, thanks! * Great, I was actually unsure whether pattern matching or `fst`, `snd` etc. were the more idiomatic way of handling tuples. As a general question on pointfree style, another thing I was unsure about was functions like `sequencesToCountMap`: `sequencesToCountMap = foldr (\s m -&gt; insertWith (+) s 1 m) empty` The `foldr` accumulator can be made pointfree -- `m` can be straight up removed, while `flip` can be used to remove `s`. But is this even worthwhile? In this situation, does the extra work done in making the function pointfree counteract the benefits?
Right, good catch! Down looks super neat; I'm starting to really enjoy seeing all of these clever uses of data types that have certain behavior embedded in them.
I would be interesting to see what is the minimal support a monad needs to provide to be usable from GHCi. For example `m a -&gt; IO a` is obviously enough but it feels way too limiting.
Have you tried the RTS flags to limit memory usage when running ghci? I use neovim and have created a simple [python3 script](https://github.com/sras/ghci-remote) that wraps a ghci instance. I can get reload and errors on file save, navigating errors in editor, get output of arbitrary ghci commands and a couple of other nice things. I have added a memory tracker in its gui so that I can easily track the memory usage and kill the ghc process if it exceeds a limit. It accept all the ghci cmd line options and passes it to the wrapped ghci instance, so I can limit the memory using the same RTS options. It communicates with the editor via a network socket, so the editor performance is not affected. I have 16gb mem, and without RTS memory limiting flags, I routinely exhaust all of it. I have a stack ghci instance, an instance created by the python script so 2 ghci instances. As far as I have seen, it will happily eat all your RAM if you allow it. 
&gt; fish operator Kleisli arrow
*Never, never and never again shall it be that this beautiful homepage will again experience the oppression of one by another.*
Could you add that as a possible solution to the issue comments?
Seems a little harsh. Clearly, OP is trying to learn the material. And isn't that what counts? Not how they learned it?
Could you add that as a possible solution to the issue comments?
I mean HyperKit instead of VirtualBox
It works! λ :set -XRebindableSyntax λ import qualified Prelude λ :{ ⋮ (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b ⋮ m &gt;&gt;= f = putStrLn "rebound!" Prelude.&gt;&gt; (m Prelude.&gt;&gt;= f) ⋮ :} λ x &lt;- return () rebound! 
I think [this article](https://www.schoolofhaskell.com/user/edwardk/phoas) is relevant.
done
The secret is not to use ghc-mod directly but to use haskell-ide-engine.
Because some usage of dynamic libraries are safe, while some are not. If you follow the rule that every change in any dependencies must trigger a rebuild, then you'll be fine! For a project that's ok, but system wide, that's very annoying. I think, but correct me if i'm wrong that in Arch, like in most distributions (nixos excepted), most libraries can be updated without updating apps depending on them. Like updating libssl or libcurl without having to rebuild and update everything that depend on them. But there are libraries like libc for which everything is rebuild. With GHC, rebuilding is the norm. It doesn't make shared libraries unsound! It just forces you to rebuild every packages that depend on them on update.
An inappropriate resource? This is just arrogant gate-keeping. You assume that seeking knowledge through some other means is somehow 'cheating' - That is an absurd and fundamentally flawed premise. This student was struggling to understand an assignment, they went, found help, and gained understanding. The point of academia should not be following some arbitrary set of absurdist rules to prove that you can jump through hoops. It should be to gain a set of knowledge. If an academic organization is so profoundly and fundamentally incompetent that it doesn't have some means of ensuring that it's students have actually gained set knowledge aside from locking them in an empty room on every homework assignment, it does not deserve to exist. Get over yourself.
There's a pull request that may fix this problem. https://github.com/begriffs/haskell-vim-now/pull/276
Unfortunately it doesn't seem to work if you try to use a different Monad in the first argument of `&gt;&gt;=`
Reporting the issue to haskell-cafe@haskell.org could be helpful.
I guess it's related to project size. So if you have a very small project or are checking it out on a toy program it probably works great, and that's why people are recommending it.
Do you know of a good write up on HIE and vim/neovim?
Not exactly what you are looking for, but I work in a pretty large codebase and I've recently bound `hoogle` search to `K` in `vim`. It takes some upfront cost to build a hoogle database, but once you have it lookups are lightening fast. It is much better than any IDE situation I've used before.
I've played a bit with [`hint`](https://hackage.haskell.org/package/hint) recently, and I've had some good experiences with being able to inspect the type of the entered line, and dispatch custom logic based on that: forever $ do (lineNotYetWrapped, curs) &lt;- io $ fmap (_1%~cs) $ takeMVar lineToEval let line = "\\ curs -&gt; " &lt;&gt; parens lineNotYetWrapped getType = typeChecksWithDetails typeResultIO &lt;- getType (parens lineNotYetWrapped &lt;&gt; " :: IO ()") typeResultShowable &lt;- getType ("show " &lt;&gt; parens lineNotYetWrapped) typeResultFunction &lt;- getType ("(undefined :: a -&gt; b) `asTypeOf` " &lt;&gt; parens lineNotYetWrapped) case (typeResultIO, typeResultShowable, typeResultFunction) of (Left _, Left showError, Left _) -&gt; do io $ putStrLn "Nothing matched, returning error" io $ replResTrig $ Left $ Right showError (Right ioSuccess, _, _) -&gt; do io $ putStrLn $ "IO type matches: " &lt;&gt; show ioSuccess lineResult &lt;- interpret line (as :: GCCurs -&gt; IO ()) io $ replResTrig $ Right $ IOInferred $ lineResult curs (_, Right _, _) -&gt; do Right showSuccess &lt;- getType lineNotYetWrapped lineResult &lt;- interpret ("show . " &lt;&gt; parens line) (as :: GCCurs -&gt; String) --eval line io $ replResTrig $ Right $ ShowInferred (cs $ lineResult curs) (cs showSuccess) (_, _, Right _) -&gt; do Right showSuccess &lt;- getType lineNotYetWrapped io $ replResTrig $ Right $ ShowInferred "&lt;function&gt;" (cs showSuccess) a -&gt; do io $ putStrLn "Strange, unknown error:" io $ putStrLn $ cs $ pShow a io $ replResTrig $ Left $ Left $ tshow a The above does a few things. For instance, it emulates what GHCi does: - tries to execute `IO` values - it tries to `show` pure values - and does one thing extra that I've been missing from GHCi (albeit more so as a beginner, but still I think it's nice to have); upon the result being a function, instead of exploding with &gt; id &lt;interactive&gt;:2:1: No instance for (Show (a0 -&gt; a0)) (maybe you haven't applied enough arguments to a function?) arising from a use of ‘print’ In a stmt of an interactive GHCi command: print it It attempts to be friendlier and says &gt; id &lt;function&gt; :: a -&gt; a I especially like that it even shows type class constraints: &gt; even &lt;interactive&gt;:3:1: No instance for (Show (a0 -&gt; Bool)) (maybe you haven't applied enough arguments to a function?) arising from a use of ‘print’ In a stmt of an interactive GHCi command: print it -- vs &gt; even &lt;function&gt; :: Integral a =&gt; a -&gt; Bool 
I’m interested in helping out there.
A snippet I usually add to any editor I'm using is this: ``` "Double import": { "prefix": "impq", "body": [ "import ${1:Data.Text} ($2)", "import qualified ${1:Data.Text} as ${3:Text}" ], "description": "Import a module qualified and unqualified" }, ```
&gt; isn't dropping from the end of a list (or any list-end operation) less efficient? Correct. &gt; The foldr accumulator can be made pointfree -- m can be straight up removed, while flip can be used to remove s. But is this even worthwhile? In this situation, does the extra work done in making the function pointfree counteract the benefits? You can mostly always eta-reduce (i.e. remove the `m` from the end) but I think many people would think using `flip` to achieve more pointfreeness is sort of crossing a line in that it really obscures things (the most common use of it I can think of is things like newtype MyMonad a = MyMonad { unwrapMyMonad :: StateT Something (ReaderT SomethingElse Identity) a } deriving (...) runMyMonad = flip runReaderT initialEnv . flip runStateT initialState . unwrapMyMonad where it's practically idiomatic at this point).
The funny thing is, looking at this PR alone, you wouldn't guess that this is a subject of much contention. Funny how far a little good faith goes... (That's a dig at *both* sides in the last debate; to be clear).
I feel like this was the least ambitious proposal from the slurp discussion. More ambitious, and more useful, was the solution proposed: remove dependency versioning from cabal files. Possibly add some "trusted build config", with trees of dependencies that are proved to work, and allow consumers to extrapolate version constraints from these trusted build configs. Stackage could be a baseline, ie packages could say "I work with the dependency trees found in LTS 10.0, LTS 9.8".
! How do I use this‽
I hadn't heard of the term eta-reduce, thanks for that! And thanks for the overall advice.
How would you like to use it? As a full GHCi replacement? Or some other way?
Pretty tired right know but that looks like hoist hoist :: Monad m =&gt; (forall a. m a -&gt; n a) -&gt; t m b -&gt; t n b https://hackage.haskell.org/package/mmorph-1.1.0/docs/Control-Monad-Morph.html
Yea I'd be interested in seeing how good kleisli arrows are for teaching monads. Seems easier to me to talk about functions `a -&gt; m b` and then just say monads are a way to compose such functions, where each instance defines how that actually works.
Please stop the witch hunt on HVR. I'm on your side in almost every technical issue, but the shitty way you guys treat people who disagree with you makes it feel really shitty. I don't give a fuck that HVR acts shitty too, that doesn't mean being childish in response helps anyone. He should also stop being shitty in these issue, but let's not pretend he's breaking "everyone's builds." He's fixing some cabal-install builds and breaking Stackage nightlies that no one should be depending on. The tension between you guys is manufactured and makes you both look like complete and total jackasses. But more importantly, it makes an absolute mockery of this community, and has apparently been actively driving people away
What about [Rank2](https://hackage.haskell.org/package/rank2classes-1.0.1/docs/Rank2.html)?
Ah: that's actually it! Specifically the `Traversable` class in Rank2.
I kind of want to refer back to OP's question, because you're right, but in context it's a little confusing. &gt; when people talk about state, they mean destructively mutable state... state doesn't abstract over this kind of mutable state, it abstracts over state that is modeled as plain old immutable data that gets passed around Redux, the JavaScript state library in question, doesn't deal with destructively mutable state either. JS of course doesn't natively offer the sort of guarantees of immutability that Haskell does, so it's up to the programmer to ensure that Redux's state is always changed via pure reducer functions, not mutated. (Whether that's through following convention, introducing libraries like Immutable.js that enforce immutability, or a combination is up to the developer.) Redux itself doesn't really care if you mutate its state - you're not supposed to, but it won't break anything. But Redux is usually used to manage state with the React view library, and React cares quite a lot about immutability, as parts of it rely upon referential equality to decide whether to update the view. Very broadly speaking I guess you could say that React/Redux and the `State` monad are similar, in the sense that you could conceptually view a React/Redux program as a function `AppState -&gt; Input -&gt; (AppState, View)`. I'm a shitty Haskell developer, so please let me know if this is totally wrong... but I think the main difference between Redux and the `State` monad is level of abstraction. Redux is a relatively high-level library that orchestrates state management for an entire application, while the `State` monad can be a lower level abstraction, used to thread a second value through a functional pipeline, much more tightly scoped. So Redux is more like, I dunno, Yampa or another FRP library that handles the behind-the-scenes wiring for you so you can focus on business logic. Redux is far simpler than Yampa, but I think it's a morally fair comparison.
Got it. This is a technique I used before, and indeed saves a lot of time! I was confused at first because it seemed to me that you where getting at some kind of cache in order to not repeat some time-intensive form of querying and number-crunching. (Ad hoc, I would likely generate this state one time and then serialize it to JSON and read it it next time.) Will try out your approach for sure!
&gt; "move fast and make broken things unrepresentable" Hahaha. You work at Savebook?
I'm nicely surprised to see more large codebases running in Haskell, and that people is taking the tooling just like other big guys do :-) ... That said, my experience everywhere with code-intelligence tools in all languages is that "seamlessly working fast" is more the exception than the rule :-( . 
There is a more direct encoding of some dependent pairs `(x : A) * B` using a GADT that mimicks the values of A but is indexed to return `B`. Technically, this is `A2 t` such that `exist t. A2 t` is isomorphic to `A`, and the embedding of any value `v:A` has type `A2 B(x)`. In the post example: ``` data Tag t where True' :: Tag Int False' :: Tag Bool data Pair where mk :: Tag t -&gt; t -&gt; Pair ```
&gt; I will concede that my float class wouldn't be ideal, I guess it's just my distaste for floats that makes me prioritize them less, when compared to law abiding numeric types. I'm probably at the very opposite extreme. I'm a numerical analyst whose main area is stability of algorithms in numerical linear algebra. Very nearly the only code I write is numerical, with some intent to test or prove stability of an algorithm. If Haskell standardizes on something that casts out floating point types and relegates operations to a large float class, I'd be shopping for an alternative prelude. Or defining my own parallel set of classes. Or wrapping `Double` in some `newtype` and defining my own illegal instances for the standard classes. I'd stop at writing orphans, but I'd need to do something. &gt; I would still personally be ok with relegating certain IEEE operations like the standard non-reflexive == to a single Float class. Going with the IEEE total order and the corresponding equality wouldn't bug me too much, but it might trip up some people. Arguments for non-reflexive `==` are mostly historical. In most languages for many years it was the only way to test for `NaN`. At this point it's expected and many people implement numerical algorithms blindly without understanding what they are doing. In theory everyone who sees `x /= x` should know to reach for `isNaN`. In practice, I expect most people who are implementing pseudocode they found in a book won't know that. Possibly this would still cause fewer problems than the nonreflexivity. And I suppose it can be argued that people who implement algorithms they don't understand deserve what they get. `NaN` is truly an error value. I can see the appeal of using `Nothing`. The way `NaN` propagates even strikes me as loosely analogous to the `Maybe` monad. But it would be a killer for performance and I still wouldn't want to use it. In contrast `Inf` and `-0` are often useful or even desired values. The standard example for signed `0` is in dealing with branch cuts of complex functions. For `Inf`, it's not uncommon to arrange specific computations so they do the right thing even in the presence of overflow if the arithmetic rules for `Inf` hold. People use these things to prove correctness of numerical code. IEEE isn't so much lawless as it is following a set of laws designed to facilitate proofs of numerical stability. 
A few years ago I had the idea to not put any bounds into my cabal files, upload them as such to hackage, run a separate, offline process which checks build plans against stackage snapshots, extrapolates version bounds from that, and writes those back into hackage as cabal revisions. The only reason to put explicit bounds into my cabal file is in case there is a known bug with a dependency for which one can't write a test.
No Windows support, can’t run it on more than half of my development machines :(
Are you referring to the [R+ proposal](https://github.com/haskell/ecosystem-proposals/pull/4#issuecomment-359693651)? SLURP did not propose removing version bounds from cabal files, it only suggested a mechanism which would allow some hosting services to mutate those bounds and other services not choose not to mutate them. (It was a bad idea, since nobody wants to _mutate_ those bounds. Hackage publishes new immutable revisions but leaves the original revision unchanged.)
I also had similar ideas, and probably many more. Did anyone actually implement that?
Couple of options that come to mind here: * We could do what Scala does [here](https://www.scala-lang.org/download/) and add a single line synopsis of each one using a similar arrow graphic. * Keep it the way it is and assume the user will click into each to learn more.
I gave [a talk on this at LambdaConf](https://www.youtube.com/watch?v=PNkoUv74JQU) last year, based on the wonderful [exinst library](https://hackage.haskell.org/package/exinst), which also lifts typeclasses across the existential boundary. 
That's right. SLURP did not propose making any changes to .cabal files. To be clear, mutation wasn't the point though - only federation of control over package publishing. Any hosting service, whether Hackage or something else, should be pretty serious about immutability, just as Hackage already is with package tarballs.
Something like this is absolutely the solution to our current dilemma. If upper bounds are metadata, then they shouldn't be required to be included with the original data. The PVP upper bounds policy can be removed. Revisions should be renamed to "Solver Overlays" and shown separately from original package info in the Hackage docs. (This is doubly important because for BSD3 packages replacing original package info with revision info looks awfully like a license violation.) In order to keep the information needed for the solver (which is used by both cabal-install and stack) we could require library authors to upload a Solver Overlay with each version of their package. This way the solver still works, but when `text` makes a minor change we don't have to do 3,500 package releases, instead the Solver Overlays can be changed, hopefully mostly by an automatic process.
Yeah this is an important point. My experience has been that lots of these work well with small codebases, but once they get large you start getting big perf issues
I have not, but I will definitely give it a try :)
Update: I got irritated enough to try this. The mods (two responded) aren't convinced. So.. u/snoyjerk, how about it ol' friend ? I think like all of us you're interested in Haskell and would like to see it grow healthier, bigger and better. Would you consider just renaming/recreating your account with your own name, and moving on from the "snoyjerk" ? That would be much appreciated, and your posts would become more valuable. 
PS, in case it sways you at all, here's what I wrote to the mods. Hope you'll think about it. :) &gt; "snoyjerk"'s choice of username is both insulting, and similar enough to Michael's ("snoyberg") account to be confusing. "snoyjerk" often posts things that are fairly plausible - even useful - but in a deceptive way, as in a recent post purporting to be Michael Snoyman. I thought for a long time that "snoyjerk" was Michael Snoyman letting off steam. Even once you know these accounts have different owners, it's very easy to misread the posts of one as being by the other. &gt; &gt; Ordinarily, we might say "it's the internet" and ignore it, or downvote problem posts. But "snoyjerk" muddies the waters and makes it harder for the community to know what's what. In the present climate this creates more space for the more extreme trolls, lowers the general level of discourse, hurts our ability to communicate clearly and function well as a community, etc. &gt; &gt; Given how long this has gone on and how much bad discourse we already have in our community, not to mention frequent low-quality attack posts aimed in the general direction of Michael Snoyman, I'm requesting that the reddit moderators take action. The votes at https://www.reddit.com/r/haskell/comments/7shvxo/hash_based_package_downloads_part_1_of_2/dt5alxh/ and https://www.reddit.com/r/haskell/comments/7shikr/replacing_hackage_hash_based_package_downloads/dt4sxpq/ suggest I'm not the only one to feel this way.
&gt; I also had similar ideas, and probably many more So did Michael Snoyman in "[The true root of the PVP debate](https://www.yesodweb.com/blog/2015/09/true-root-pvp-debate)". The blog post says this is an "official" proposal, but clearly it isn't _really_ official until it's a PR on `ecosystem-proposals`. Let's write one!
There was also https://github.com/haskell/ecosystem-proposals/pull/1
Well, let's just say that http://matrix.hackage.haskell.org (or also the [`^&gt;=` operator](http://cabal.readthedocs.io/en/latest/developing-packages.html#pkg-field-build-depends)) provide glimpses of a much larger &amp; ambitious project I've been working on whose goal is to tackle this very issue... ;-)
`sequencify` is inefficiently written (it's quadratic in the lenght). You typically don't want to use length in the loop of a function. For example: sequencify n lst | length lft &lt; n = [] | otherwise = lft: sequencify n rt where (lft, rt) = splitAt n lst This also uses length, but only once for every element.
If `text` gets major bump, it's most likely because it changes internals. For example changing internals to use UTF8. That change may not cause compilation failures, but still break downstream. `text` is bad example to mention where you can omit bounds. It's unluckily to get major bump often, but when it gets one it will break a lot of downstream
Ah, completely forgot that `length` isn't `O(1)` here. Good call, thank you.
As a full GHCi replacement would be nice, but I'm not sure how to use it as is. When I try to run the following, GHC complains about `GCCurs` module Main where import Language.Haskell.Interpreter import Control.Lens main = forever $ do (lineNotYetWrapped, curs) &lt;- io $ fmap (_1%~cs) $ takeMVar lineToEval let line = "\\ curs -&gt; " &lt;&gt; parens lineNotYetWrapped getType = typeChecksWithDetails typeResultIO &lt;- getType (parens lineNotYetWrapped &lt;&gt; " :: IO ()") typeResultShowable &lt;- getType ("show " &lt;&gt; parens lineNotYetWrapped) typeResultFunction &lt;- getType ("(undefined :: a -&gt; b) `asTypeOf` " &lt;&gt; parens lineNotYetWrapped) case (typeResultIO, typeResultShowable, typeResultFunction) of (Left _, Left showError, Left _) -&gt; do io $ putStrLn "Nothing matched, returning error" io $ replResTrig $ Left $ Right showError (Right ioSuccess, _, _) -&gt; do io $ putStrLn $ "IO type matches: " &lt;&gt; show ioSuccess lineResult &lt;- interpret line (as :: GCCurs -&gt; IO ()) io $ replResTrig $ Right $ IOInferred $ lineResult curs (_, Right _, _) -&gt; do Right showSuccess &lt;- getType lineNotYetWrapped lineResult &lt;- interpret ("show . " &lt;&gt; parens line) (as :: GCCurs -&gt; String) --eval line io $ replResTrig $ Right $ ShowInferred (cs $ lineResult curs) (cs showSuccess) (_, _, Right _) -&gt; do Right showSuccess &lt;- getType lineNotYetWrapped io $ replResTrig $ Right $ ShowInferred "&lt;function&gt;" (cs showSuccess) a -&gt; do io $ putStrLn "Strange, unknown error:" io $ putStrLn $ cs $ pShow a io $ replResTrig $ Left $ Left $ tshow a 
To make sure we're on the same page -- major refers to both A or B in the A.B.C.D scheme right? Shouldn't there be a way for even foundational libraries to improve their APIs without causing thousands of cascading pacakges to have to be released? (assuming the change is small and doesn't affect most of them) &gt; It's unluckily to get major bump often, but when it gets one it will break a lot of downstream This line makes me think I may be miscommunicating. Breaking builds is *terrible*, I certainly wouldn't want to do anything that would let that happen. I'm saying we should track the same info we do now to prevent breakage, but do that tracking purely in revisions (or whatever revisions become, eg Solver Overlays) instead of in the source code of each library.
See package `cabal-debian`, it contains tools which generates all for the package from the cabal package.
Thanks for this feedback. Although I love Monoid's ligatures, I turned them off to make the syntax more clear.
&gt; (This is doubly important because for BSD3 packages replacing original package info with revision info looks awfully like a license violation) This is not correct. There is no issue with licenses, because the package info is not replaced -- the original tarball is left intact. C.f. https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md Further, take a look at the conditions on licenses at http://hackage.haskell.org/upload There it says expressly that &gt; The Hackage operators do not need and are not asking for any rights beyond those granted by the open source license you choose to use. All normal open source licenses grant enough rights to be able to operate this service. &gt; &gt; In particular, we expect as a consequence of the license that: &gt; we have the right to distribute what you have uploaded to other people &gt; we have the right to distribute certain derivatives and format conversions, including but not limited to: &gt; documentation derived from the package &gt; alternative presentations and formats of code (e.g. html markup) &gt; excerpts and presentation of package metadata &gt; modified versions of package metadata These things are all things that are performed by e.g. every major linux distro.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-infra/hackage-trustees/.../**revisions-information.md** (master → 5a30f96)](https://github.com/haskell-infra/hackage-trustees/blob/5a30f96e1bb1bdb1d71fba137cea149b1c197810/revisions-information.md) ---- 
I think you can adjust the value of `h$gcInterval` using JS FFI at runtime. You can see where it is used in the RTS in [thread.js](https://github.com/ghcjs/shims/blob/978ff1783c29ae39a36a668ced6b4d74161ea904/src/thread.js#L438).
To get a space in LaTeX math you need to write `\;` (or `\,` or `\:` for narrower ones), rather than `~`.
Oh, lol, I did not see this was posted. Blog author here :) Thanks for the comments. I've published it to cabal and have requested it to be included in stack as well. As for windows support, yes that is because of the vty dependency. It would be nice if that got ported one day. Hopefully a few windows haskell devs can get together and see what is required? :) 
I *believe* there's actually a compiler flag for this, though I couldn't tell you what it's called.
I use Intero on VSCode, 12 MB RAM quite decent codebase in size, with five medium size projects. Acceptable performance. Third attempt to install haskell-ide-engine. fingers crossed
That clears things up a bit! So meaning the State is not automatically 'instantiated' just by defining it, right?
Here's key handling and mouse handling on one screen. Frontend saves the input to a channel, from which the backend reads it. There is some noise, because it implements an API shared by SDL2, HTML5, GTK, vty, curses and stdin frontends and that's the only file that know which library is used. https://github.com/LambdaHack/LambdaHack/blob/12d67b6ee218ad0a923e327cf8a85152cc80bb60/Game/LambdaHack/Client/UI/Frontend/Sdl.hs#L191 
I always start with reading up Learn You a Haskell and then google for Haskellers' blogs and read up posts that maybe related to what I need to know. I don't really mind googling a bit to find answers, but to be brutally honest, it's very hard to find simple working answers -&gt; I like to learn from a working code and understand the theory as I go along rather than the other way round. As to where I would have hoped to find more information, ideally it would be https://www.haskell.org/documentation . Right now the official docs page is just curated list of resources and it can be confusing as to where to start or knowing what I expect to learn by clicking the links. I think it would be nice to have it like these documentations below: * Elixir - https://hexdocs.pm/elixir/Kernel.html * Redux - https://redux.js.org/ * Vue.js - https://vuejs.org/v2/guide/ Plenty of examples on how it might be used in a project, combined with explanations for motivation behind the design choices. Being a general-purpose research language, it might be counter-intuitive for Haskell to focus on just a certain array of fields, but I would love to know which fields in computing where writing code in Haskell would be beneficial so that it is easier to know where to start. Elixir for example, is very easy to see that it's well-suited for a web-server that deals with parallel tasks which are further enhanced with functional programming. Redux makes managing data on the client a breeze with functional programming concepts, etc etc
Unfortunately, Scotty isn't necessarily simple, though it can probably be used without fully understanding what it's doing. `ScottyM` is a type alias for `ScottyT Text IO`, which itself wraps a `State (ScottyState Text IO)` value, and the `ScottyState Text IO` type wraps the middlewares, routes, and a handler. But all that is probably machinery you can largely ignore. Instead, you can work in the `ScottyM` monad. Working in a monad essentially means entering a `do` block and writing a sequence of expressions that evaluate to a value like `ScottyM a` for any type `a`. In particular, the type `ScottyM ()` crops up a lot in function signatures. Let's consider `get :: RoutePattern -&gt; ActionM () -&gt; ScottyM ()`. This function takes a `RoutePattern`, an `ActionM ()`, and produces a `ScottyM ()`. We'll invoke a bit more machinery and enable the `OverloadedStrings` pragma in a comment: {-# LANGUAGE OverloadedStrings #-} Then we can short-cut a few things: :t get "/foo" get "/foo" :: ActionM () -&gt; ScottyM () :t text "hello, world" text "hello, world" :: ActionM () :t get "/foo" (text "hello, world") get "/foo" (text "hello, world") :: ScottyM () So, we have an expression of type `ScottyM ()`. We can run a `ScottyM ()` with `scotty :: Port -&gt; ScottyM () -&gt; IO ()`: scotty 9000 (get "/foo" (text "hello, world")) … and a web server will be serving "hello, world" on http://localhost:9000/foo Under the hood, `get` is adding a router to the state value stored by Scotty. We can do several `get`s in sequence, and at the end of it there's a state value that describes the web server to run: scotty 9000 $ do get "/foo" $ text "hello, world" get "/bar" $ text "goodbye, world" 
&gt; Is there any simple working example which I could get started with (I tried googling but couldn't find one)? A simple state like sum of numbers that shows how the monad is being used in a working code Sure thing, but do note that this is not exactly how we do things "in working code". Anyhow, here goes: module State where import Control.Monad.Trans.State -- Let's define some type synonyms to be a little more descriptive. type SumVar = Int type SumRes = Int -- Disclaimer: This function is *very* unidiomatic! -- We pattern match on the list constructor to check -- if we're done. stateSum :: [Int] -&gt; State SumVar SumRes -- The empty case first. We know we have "nothing left to sum". -- However, we can't just "return 0" either, since we probably hit -- the inductive case a few time before we got here. So currently, -- 'SumVar' contains our result. We need to use -- -- get :: State s s, -- -- that is, a State computation which simply returns the current state. stateSum [] = -- do cur &lt;- get -- return cur -- { = Monad law: get &gt;&gt;= \cur -&gt; return cur = get &gt;&gt;= return = get } get -- The inductive case. Aside from 'get', we need to use -- -- put :: s -&gt; State s (), -- -- that is, "put s" results in a State computation which has a new 's' -- as current state. This update operation doesn't return anything -- meaningful. But it has to return *something*, so it returns unit (()). stateSum (x:xs) = do -- cur &lt;- get -- let newState = cur + x -- put newState -- { = We can use a shorthand for this: modify } modify (+ x) -- And now we loop. stateSum xs -- Note: In the inductive case, we could have written the one-liner -- stateSum (x:xs) = modify (+ x) &gt;&gt; stateSum xs -- or rather, -- stateSum (x:xs) = modify (+ x) *&gt; stateSum xs, -- with (*&gt;) :: Applicative f =&gt; f a -&gt; f b -&gt; f b. -- Meaning that we don't need the whole algebraic structure of a Monad -- after all. -- | &gt;&gt;&gt; doIt -- (55,55) doIt :: (SumRes, SumVar) doIt = let theStateComputation :: State SumVar SumRes theStateComputation = stateSum [1..10] thePureFunction :: SumVar -&gt; (SumRes, SumVar) thePureFunction = runState theStateComputation in thePureFunction 0 -- Note that if we unpack things a little, we can see -- that using State does not give us much benefit here. -- For instance, the base case expands to this: -- stateSum' [] = get -- { = definition of get } -- stateSum' [] = State $ \cur -&gt; (cur, cur) -- (Actually, I lied. The true definition is more like -- stateSum' [] = StateT $ \cur -&gt; Identity (cur, cur), -- since State is actually a monad transformer.) -- That is, we could have just as well "cut the middleman" and -- written this instead: pureSum :: [Int] -&gt; SumVar -&gt; SumRes -- pureSum [] = \cur -&gt; cur pureSum [] cur = cur -- pureSum (x:xs) = \cur -&gt; pureSum xs (cur + x) pureSum (x:xs) cur = pureSum xs (cur + x) -- This corresponds to a common recursion pattern which is colloquially -- called "go function" or "worker-wrapper". It's often written like this: pureGo :: [Int] -&gt; SumRes pureGo xs = go xs 0 where go [] cur = cur go (y:ys) cur = go ys (cur + y) -- This kind of "pure tail-recursion" can be squeezed into a left-fold: pureFold :: [Int] -&gt; SumRes pureFold = foldl step 0 where step :: SumRes -&gt; Int -&gt; SumRes -- step cur x = cur + x -- { = Use prefix notation } -- step cur x = (+) cur x -- { = "eta reduce" } step = (+) -- =&gt; You should use Data.List.foldl' (&lt;- note the prime) instead. Regarding `Scotty`, I would recommend to try to get more familiar with these basics, and "Monad Transformers". Monad Transformers are a simple concept *once you get what they are and what problem they are trying to solve*. But like Monads, or some of the other algebraic structures we use, they take some getting used to. One further note: In the above example I used `foldl`. `foldl` sucks. Everybody that looks like they are using `foldl`, sucks. Just use the strict variant `foldl'`.
I agree with you but my only diagreement is "laziness". I can't see how laziness contributes to safety. Would you be able to elaborate?
A lot of people get "bitten" by Learn you a Haskell when they notice that pretty pictures and a little playing around in the REPL doesn't teach you anything. Like I already said, [Haskell Programming from First Principlces](http://haskellbook.com/) is very good, and it even has quite a few `Scotty` examples as well. If you would like a free alternative, you might want to consider something like [Real World Haskell](http://book.realworldhaskell.org/). While outdated in some aspects (read the comments on the page), it covers the basics well. Let's not make this a recommendation thread. But seriously, I would reconsider LYAH as your primary resource.
&gt; If upper bounds are metadata, then they shouldn't be required to be included with the original data. Hear hear! To me this is one of the crucial flaws with the current system for constraints.
You could try Spacemacs (Emacs, with vim key bindings (and more)).
You could also always just write in a normal hs file and then convert it into lhs somehow. However, given emacs excellent support for org mode and things like that, I would be surprised if there wasn't a nice way to get lhs somehow. Although I do confess I haven't looked very hard yet :)
I'll give it a try, thanks.
I know a lot of people do that but the workflow seems tedious to me...
/u/alan_zimm linked to [his proposal](https://github.com/haskell/ecosystem-proposals/pull/1), which IMO is very close to that blog post I wrote. I truly believe that implementing Alan's proposal would result in a better status quo for _all_ parties in the Haskell ecosystem today.
Does this solve the use case of constructing an entity based on runtime data? 
&gt; The aim of this proposal is to separate these two purposes, by allowing authors to distinguish if they wish to opt-out of following the PVP and the attendant curation process that helps to maintain correct dependency information. One has to separate the PVP into two parts: * The how-to-version-your-libraries part, which is consensus and uncontroversial. And like you mentioned it does help to maintain correct dependency information. * The you-must-set-upper-bounds-a-certain-way part of the PVP. This is the controversial part which caused a lot of breakage and long periods of time where Haskell software was un-buildable unless you manually patch packages you download from hackage to relax their upper bounds.
Did you try reading this ? https://en.wikibooks.org/wiki/Haskell/Understanding_monads/State I'll admit it's a long time since I've read it myself.
This is something that C needs pragmas on too. Should a function allow register leaks? Especially on MMX.
The C version is missing an n
Good question! Yeah, it does -- now to create/update an entity you need to provide a `EntWorld 'SetterOf`, which you can build at runtime via an `Endo (EntWorld 'SetterOf)`.
Hello Chris. Just wanted to check if you had a chance to look into this matter. Working MS Sql Server library would be a boon to haskell adoption in small and middle size enterprises. 
And here I am trying to make a functional language that throws this out because I can't handle a garbage collector.
There’s no O3 in ghc. O3 just is an alias for O2 and or any On for n&gt;2 is converted to O2. I forget which and I’m on a plane atm :)
More programs terminate under call-by-need than by call-by-value. (All the CbV programs terminate and then some more.) Unexpected lack of termination is commonly considered a bug. =) Mind you, the cost is that you still have to be careful about reasoning about space behavior as the patterns for avoiding space leaks are different between strict and non-strict evaluation, at least from a surface language perspective. There is also another factor, which is that lazy algorithms compose better as black boxes: I can write `sort` once and package it away, then I can write `take` and package it away, then I can compose `take 10 . sort` and get better asymptotics than the two parts in isolation. But in a strict language I always have to pay full price. This in and of itself isn't a safety property, but the fact that I have to pay to fully sort the list in a strict language, means that I wind up manually fusing those two algorithms together into some custom one-off quick-select rather than reusing code that has had far more eyeballs on it.
I checked and I've been given the go ahead to clean up and release it as open source! I'll announce on reddit with a blog post when it's available.
Consultants hate him!
Could you specify version bounds for the dependencies? I wanted to try it, but it doesn't build with 'cabal install' on my machine
Did you do any benchmarks to compare the performance of each?
I've done something a bit close to what you are asking once. I've mainly used `System.Process`; some function from there that allows controlling the created process's raw stdin and stdout handles. Perhaps I used `createProcess`. Now, I think I only went as far as trying to specify unit tests, and didn't reach as far as attempting property based testing. I suspect QuickCheck, hedgehog or similar might allow you to define and test monadic/IO properties, but I'm not sure. Maybe someone else can chime in with some relevant experience on that front. And if the second is also possible, you can just put these two together and, if I understand your question correctly, you can have what you are wishing for.
The `ActionM` type is an alias for `ActionT Text IO`, where `Text` is the format for errors and `IO` is the "base" monad. There's no room in Scotty's types for `State` or `StateT` to provide shared state between two actions. The type `State s a` is effectively the same as `s -&gt; (s, a)` - that is, executing an action in the State monad takes the current state as input and returns the new state and the computation result as output. The State monad works by sequencing computations such that the output state of one computation is the input state of the next. This ordering of actions is (IMO) the heart of monadic computation: monads impose an order (see footnote 1) on evaluation that Haskell's lazy evaluation model otherwise lacks. But in the context of a web server, this poses a potential problem: you probably don't want to serialise your web request handlers such that each runs fully to completion before the next starts. I don't know the specifics for Scotty, but I would _assume_ that handlers can execute concurrently. When we're dealing with concurrent execution and shared mutable state, there are two main options to consider: `IORef` and `TVar`. An `IORef` type is a mutable reference with atomic access semantics, including a `atomicModifyIORef :: IORef a -&gt; (a -&gt; (a, b)) -&gt; IO b` atomic modification function (but use `atomicModifyIORef'`, see footnote 2 for more). Code using this model would look like: main = do counter &lt;- newIORef 0 scotty 9000 $ do get "/foo" $ do liftIO $ atomicModifyIORef' counter $ \c -&gt; (c + 1, () ) text "incremented." get "/bar" $ do count &lt;- liftIO $ readIORef counter text $ T.pack $ show count The `counter` IORef is created outside the `scotty` call, because the `ScottyM` monad is not a monad transformer and can't wrap IO actions. Inside the `ActionM` blocks, the IO actions are executed with `liftIO` from `Control.Monad.IO.Class`, which "lifts" the IO action from `IO a` to `m (IO a)` as long as `m` is a member of the `MonadIO` class, which `ActionM` is. Monad transformers are a bit out of scope for this post, but in a nutshell they're how we can layer monadic effects such that we can have, eg, state and IO together. The other option is the one I prefer, using software transactional memory. If you recall your computer science lessons, STM is optimistic concurrency, while IORef is naive locking. An IORef will lock for every access. An STM access will assume it's running without contention, and if any interference occurs, the operation will fail and retry. In low contention scenarios, such as many readers and few writers, STM is significantly more performant. counter &lt;- atomically $ newTVar 0 scotty 9000 $ do get "/foo" $ do liftIO $ atomically $ modifyTVar counter (+1) text "incremented." get "/bar" $ do count &lt;- liftIO $ atomically $ readTVar counter text $ T.pack $ show count This looks almost the same as the IORef variant, with the introduction of the `atomically` function - this is the core of STM which takes an action in the `STM` monad and executes it atomically, returning a result in the `IO` monad. An `STM` action may do multiple operations on multiple `TVar`s, and `atomically` will ensure that these operations are not interfered with by another thread, retrying as necessary until the entire set of operations succeeds together. TL;DR, don't use `State`, use a concurrent shared mutable state abstraction instead. ^1 `do { x &lt;- a; b x}` is syntactic sugar for `a &gt;&gt;= \x -&gt; b x`. `&gt;&gt;=` is the "bind" operation of a monad, which binds the result of one monadic computation to the input of the next. The superclass of a monad is an applicative functor, which has an operation like bind except it does not feed the output of one expression as the input to the next. Applicative functors may parallelise computations, monads may not. ^2 The statement `atomicModifyIORef ioref (+1)` replaces the reference in `ioref`, call it `x`, with the unevaluated thunk `(+1) x`. Successive calls to `modifyIORef` will continue to stack `(+1)`s in front of the old expression, and nothing will be evaluated until some other expression requires the value to be determined. This can cause a space leak in your program. The variant `modifyIORef'` is _strict_ - it will evaluate the result of your function immediately and store the value. This is usually what you want to happen. `atomicModifyIORef` and `atomicModifyIORef'` are similar, but the non-atomic variant is easier to write examples for :-)
Your Plots are dead (view limit exceeded) :(
Been a while since I read up on haskell performance stuff but I would be astonished if ghc didn't do a worker wrapper transform for ackermann since it is obviously strict in both arguments. And iirc a case on primitive types compiles to primops? If that's true then the problem probably is with the recursive call and not the internals of the function?
Take a look at the type of the first case you have. 
I believe you're referring to the "odd" case. My thought process after reviewing this was to add fromIntegral () around the text following the "=", but this flipped it so the function only operates on a list of ints now, and not floating points. I'm having trouble understanding why I can only accept one or the other 
It might be helpful to give an explicit type signature for fromMedian. This will probably help you see why your function doesn't make too much sense. What type do you want fromMedian to have?
I guess I'm looking for it to return with the same type as the list input. So if I give it a list of floats, I would want to receive a floating point value after running findMedian
Thanks. I just tried this solution and ran into an issue [here](https://github.com/ddssff/cabal-debian/issues/63).
It is `wip/T2893` in the [GHC repository](https://github.com/ghc/ghc/tree/wip/T2893).
Two questions for you: what should `findMedian [1, 2, 3, 4]` return? Also, what type does each of your cases return when they receive a list of Integers?
That input should return 3, using (2+3)/2. The function does not currently run for a list of integers, but returns a floating point when given a list of floats
Things you can do data Exists :: (k -&gt; Type) -&gt; Type where Exists :: f xx -&gt; Exists f instance (forall xx. Show (f xx)) =&gt; Show (Exists f) where show :: Exists f -&gt; String show (Exists xs) = "Exists " ++ show xs dup :: (forall xx. Semigroup (f xx)) =&gt; Exists f -&gt; Exists f dup (Exists xs) = Exists (xs &lt;&gt; xs)
They just don't really bother with consistent version bounds. You should survey how Maven, Ivy, Cargo, etc. all handle it. Maven and Ivy have different ways of picking "winners", one uses proximity-to-desired-version, one uses "newest wins." It's pretty interesting how far they get by just allowing some slack in the system.
The two biggest reasons we have more problems in Haskell are that we tend towards having both: * lots of dependencies * lots of breaking changes I've also seen arguments that we're simply _detecting_ the breakage better than other languages due to strong, static typing, but I don't know if I buy it. I've also heard that other languages don't have it nearly as good as we think, but I don't fully buy that one either.
Unfortunately, there isn't a clean way to write a function that lets you do (2+3)/2 and get 3 while also letting you do (2.0+3.0)/2.0 and get 2.5. From the haskell point of view, those are fundamentally different operations, and haskell makes it somewhat awkward to express "this argument needs to either support this operation or that operation". Options: the simplest is to use separate functions. Maybe `findMedian` always returns the exact median as a fractional (Float/Double/Ratio), regardless of the argument type, and `findMedianInt` operates on integrals (Int/Integer/...) and returns the median rounded to the nearest int. Getting back the exact median as a double from a list of ints is a plausibly useful operation, so the extra flexibility you get from this isn't even a waste. You could even just define `findMedian` and use `round . findMedian` when you want to get an Int back. If you do take this option, look at your first case. You need a function that can take either an Int or a Double and returns a Double. You are actually already using a function like this in your second case, so you should be able to figure this out. Your next option is to use a sum type. Something like data ListOfNums = Ints [Integer] | Doubles [Double] Then, `findMedian` branch based on the type of list that you got. That said, this isn't much better than the two function/one function + `round` option -- you'll essentially be defining two functions anyway, and `findMedian . Ints` isn't really much better than `round . findMedian`. Overall, this can work, but I wouldn't suggest it. Finally, you can use typeclasses. Something like class Averagable x where average :: x -&gt; x -&gt; x Then, define an instance for all of the data types that you care about. Make `findMedian` use `average` instead of calculating the average directly, and you are all set. This is probably the cleanest route from a caller's perspective, but you do need to create a typeclass and some instances.
I've only written trivial code to this point but Vim and Tabularize plugin is working out fine. 
Also, style note: people usually omit parentheses when possible. In haskell, `f(x)` is completely equivalent to `f x` -- space is function application, not parens. You need parentheses if you want `f (g x)`, since `f g x` is equivalent to `(f g) x` (ie `f` is a "two argument" function). Even then, you can do stuff like `f $ g x` to avoid the parens. You definitely can use parens if you want to (as long as you don't try to call a two argument function with `f(x,y)`), but it looks weird to a more experienced haskeller's eye.
This is awesome. So now I can write instances for `ReaderT` based `mtl` classes like: instance (forall r. MonadReader r m, HasSomeThing r) =&gt; MonadSomeEffect m where doSomeEffect = view someThing &gt;&gt;= doEffectWithThing ?
Thanks for the help. I made a mistake in answering your previous question, if given a list of integers, I would want to return a float if division is necessary. Sorry, it's been a long day. Not sure if this changes your response at all. Separate functions is not an option at the moment unfortunately, and I don't believe I'm supposed to be using typeclasses at the moment.
I've noticed that in a lot of examples that I've reviewed. I guess I'm used to using parenthesis from other programming languages so it just feels natural to use them
No worries. In that case, you don't even need separate functions -- that makes things far easier. From here, look at the odd case. If you get a list of ints, what type does it return? Similarly, if you get a list of doubles, what type does it return? Now, look at the second case. If you get a list of ints, what type does it return? Similarly, if you get a list of doubles, what type does it return? See the mismatch? This is what the compiler is complaining about. What you need is a function that can take either a double or an int and will convert it to a double. You are already using a function like that in your even case to convert the numerator of your division to a double. Use that in your odd case and you should be all set.
If `cabal-debian` works for then I would use it. That said it's not actually that hard to make your own. You will have to learn a bit our how it all works but it shouldn't take too long. To make a basic package the steps you need to go through are: * `stack clean --full` or use what ever you use to clean the project^* * dh_make --createorig -s -p &lt;package name&gt;_&lt;version&gt; This will create all the files required. You will then need to remove the files you don't need and edit the others. The files you will almost certainly need to modify are: `debian/rules, `debian/&lt;package-name&gt;.dirs &amp; `debian/&lt;package-name&gt;.install. For an example of something I did recently see: https://gitlab.com/filterfish/prometheus-matrix/. When you need to modify the changelog use `dch` it will add a new entry in the correct format—don't do it by hand you **will** get it wrong! I should point out that I'm not a Debian developer but we do deploy all our code using Debian packages and the above process, whilst being slightly tedious, works really well. [*] `dh_make` creates an initial tarball and if you don't clean the project first you will end up with a massive tarball containing all you previous build artifacts.
So / is defined as float division. When you divide two integers, the result isn't necessarily an integer, so / isn't defined on integers (you need div or quot instead). Now, back to your issue. If you have a list of ints, what is the type of `(xs !! mid) + (xs !! (mid + 1))`? And yet, somehow, you are able to divide it using /, and you are getting a float back. You are clearly doing something right there. Figure out what that is, and you should be able to handle the odd case.
I use markdown for my lhs, which is much more convenient. https://hackage.haskell.org/package/markdown-unlit
I just figured it out, thanks so much for helping me out with that
lhs is good enough in emacs that I was wondering why I don't see more of it.
Where's the link to the in-depth post about how GHC does compilation?
You seem to not have ghc in PATH. You should install it, and all dependencies, by the `apt`. Quite often you have to debianize some of dependencies as well.
The PDF has been updated several times since your post was made. Is the paperback version in sync with the latest PDF?
Dang, it's still a draft as of now :) [Link is here](https://github.com/bollu/pixeldruid/blob/master/content/blog/laziness-for-c-programmers.md), but it needs to be re-written way better IMO. I'm sorry, but college really does suck up most of my time :(
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [bollu/pixeldruid/.../**laziness-for-c-programmers.md** (master → 2f88c28)](https://github.com/bollu/pixeldruid/blob/2f88c28efc54fb16463e2d9e0ac63f28620e223b/content/blog/laziness-for-c-programmers.md) ---- 
One trivial solution is to use the simple `Map.fromListWith`: toEntry :: CountryCityPopulation -&gt; (Text, [CityPopulation]) toEntry (CountryCityPopulation a b c) = (a, [CityPopulation b c]) Map.fromListWith (++) (map toEntry l) If your list is already sorted, you can use `Map.fromAscListWith`. One trick for optimization may be to avoid `(++)` and instead uses unpack + cons, something like `(\[x] xs -&gt; x:xs)`. I don't know how it will perform, this need to be benchmarked. Your key being `Text`, you will have a big performance win by using `HashMap` (from `unordered-container`) instead of `Map`. Same API, but more efficient (in general) and highly more efficient for data type like `Text` where the comparison function cost a lot (again: benchmark). I wonder if it is efficient to unpack / repack the `CountryCityPopulation` to a `CityPopulation`. The unpack / repack will generate a lot of intermediate data structure, when storing a `Map Text [CityPopulation]` will eventually leads to a final higher memory footprint, but with really few pressure on the GC during the creation of the data structure. It also depends on the origin of your initial `[CountryCityPopulation]` and if it can be consumed lazyly or not and if it stays in memory after the map creation (in this case, sharing between the map and the list is important). Once you tried all of that (and probably more), come back with your benchmark results and a more detailed definition of "memory-efficient" and we'll try something else ;)
Any practical things we can do with this new extension?
Holy shit u/taktoa
&gt; view limit exceeded It was on the HN frontpage :)
As I understand it, no. You'd need `exists` in place of `forall` for that.
It sound to me that you should aim for incremental build rather than moving your binaries everywhere, not an expert but things like: - https://halcyon.sh/#halcyon - https://buckbuild.com (which has support for Haskell) - Nix integration with Stack : https://docs.haskellstack.org/en/stable/nix_integration/ but I have never tried it. also many people do not use Stack but Nix &amp;&amp; Cabal. I might have not understood your problem, but it seems to me, that you should rely on the dependency resolver to do its job rather than eating to get everything offline. 
I use spacemacs with the Haskell layer : http://spacemacs.org/layers/+lang/haskell/README.html Also a lot of people are using Vim, an example here : http://www.stephendiehl.com/posts/vim_2016.html
In fact I'd say it's pretty likely that this instance would loop the typechecker since most functors indeed implement `Eq` as `Eq a =&gt; Eq (F a)`. In this case this creates a `Eq (Rose f a) =&gt; Eq (f (Rose f a)) =&gt; Eq (Rose f a)` loop.
Note that "large" amounts of live data may result in rather heavy performance hits for gc https://stackoverflow.com/questions/36772017/reducing-garbage-collection-pause-time-in-a-haskell-program Just something to be aware of. :o)
Did you try or consider using plain ol' functors instead of type families - that is, `data EntWorld (f :: * -&gt; *)`? Seems like it would work straightforwardly if you don't have `'Unique` - then you just instantiate `f` with either `Maybe`, `Update`, or `IntMap`. (I don't know what the significance of `'Unique` is, of course.) Do you think this might have any notable advantages or disadvantages? (Like - better type inference? My intuition is that the simpler solution should have *some* benefit, but I dunno.) Also, I'm vaguely surprised types whose fields include type family applications can have `deriving Generic` . How does that work?
As much as I love SmallCheck (and its brother [leancheck](https://github.com/rudymatela/leancheck)), [Hedgehog](https://hackage.haskell.org/package/hedgehog) is probably the most modern "ready for production" version. Separating the Arbitrary/Serial/Listable typeclasses into generator functions in a nice API is a big win for large projects -- the QuickCheck approach of bundling generators/shrinkers with types was probably a mistake that's stuck.
Isn't that the case for all GC'd languages?
That's just mutual recursion then. That's a pretty common thing to want to do.
Sure I guess. It's just non-terminating.
&gt; That definitely isn't right. Map.fromListWith makes no guarantee that it doesn't apply your function in a tree-like shape. Good remark, I had never thought about that actually. The documentation for `insertWith` states that `If the key does exist, the function will insert the pair (key, f new_value old_value)`, which imply (I think, am I right ?) that the first argument will always be the new value (i.e. the singleton list) to be inserted. I blindly assumed that this hold for `fromListWith`. Apparently it is true (from a quick look at the source, which uses `insertWith`, but again, am I right?), but there is no guarantee that this will remain in future revision. &gt; I'm not so sure about this. HashMap isn't just generally faster. Poor wording, "most of the time" may have been a better choice of word. Sorry about that. &gt; the common case short circuits on length differences (constant time) `Text` length is `O(n)` https://hackage.haskell.org/package/text-1.2.3.0/docs/Data-Text.html#v:length but the `Eq` implementation has a constant time comparaison on the length of the internal buffer: https://hackage.haskell.org/package/text-1.2.3.0/docs/src/Data-Text.html#line-340 Thank you, I learned something.
Following your comment gave haskell-ide-engine a spin. Certainly much better experience than I had with intero, but still, looks like one time trying to "jump to definition" of a lens created using Template Haskell causes everything to stop working at that point. Compared to my normal setup using hasktags - there it just fails to know these symbols but everything keeps working (jumping to other symbols or getting compilation errors). So I think I'll go back to using `ghcid` for now.
The integrated shrinking is [not without its drawbacks](https://www.reddit.com/r/haskell/comments/646k3d/ann_hedgehog_property_testing/dg1485c/), though for the most part, it does do what you want.
Would that mean that forall a. Monoid (m a) becomes a legal constraint?
Huh. I thought `Text` length was constant. Didn't realize it was only the internal buffer length that was cached. We both learned something :P
I trust -XTransformListComp, it hasn't failed me. I shortened constructors here, and assumed your CCPs were not necessarily unique, though if they are you can omit the second group by. groupCCPs ccps = Map.fromList [ (the country, CP (the city) (sum population)) | CCP country city population &lt;- ccps , then group by country using groupWith , then group by city using groupWith ]
Man I really wish they hadn't written that article... It's extremely misinformed. I always have to link back [a great response explaining how it's not all roses](https://blog.plan99.net/modern-garbage-collection-911ef4f8bd8e). Go's garbage collector may have lower pause times, but ends up spending a lot more time in the GC in total. GHC's GC is neither unique nor bad in the fact that it stops the world; it's Go that's unique (and bad) about *not* doing that.
Yes!!!
Yes, that is exactly what this means.
Smallcheck is great in the cases where it works. However, there are cases where it doesn't work. Consider an algorithm begins introducing optimizations for values over a certain size. If that size is too high, smallcheck is going to want to check everything smaller than it before it checks it. This can result in an unacceptably large number of tests being run.
Well, my point is not that GHC makes no such tradeoffs. Again, it's that GHC is not unique nor bad in this regard. GHC's GC is fairly typical w.r.t. throughput and pause times. I'm just pointing out that it is in fact the case for all GC'd languages (including Go) that large working sets yield proportional GC performance hits.
I think that would be written more like: instance (forall r. HasSomeThing r =&gt; MonadReader r m) =&gt; MonadSomeEffect m where ... Not sure if that's possible.
Stephen Diehl's setup doesn't mention literate Haskell ([neither he pushed lhs files on github](https://github.com/search?utf8=✓&amp;q=user%3Asdiehl+language%3Alhs&amp;type=Repositories)). A quick look at [his plugin bundle](https://github.com/sdiehl/haskell-vim-proto) tells me it doesn't support lhs. Jasper VDJ [wrote blog posts in lhs](https://raw.githubusercontent.com/jaspervdj/jaspervdj/master/posts/2017-01-17-lazy-io-graphs.lhs). I think he's using vim, I wonder how he manages with indentation.
Good point thanks !
Yes, running `sudo apt-get install ghc` fixed the error; however, now I'm getting `cabal-debian: Prelude.read: no parse` in case you have any suggestions : ]
I understand that different gc systems have different characteristics. In my initial reply to the OP, I simply tried to make him/her aware of the fact that GHC gc will pause for "potentially too long" when dealing with large sets of live data. This is not something I made up. It comes directly from Simon Marlow, who I suspect knows a lot more about these things than I do. https://en.wikipedia.org/wiki/Simon_Marlow His reply in the initial SO thread: https://stackoverflow.com/questions/36772017/reducing-garbage-collection-pause-time-in-a-haskell-program/36779227#36779227 &gt; Your assumption is correct, the major GC pause time is directly proportional to the amount of live data, and unfortunately there's no way around that with GHC as it stands. We experimented with incremental GC in the past, but it was a research project and didn't reach the level of maturity needed to fold it into the released GHC. I meant no harm with my "I don't think so". I'm sorry if it came out like that. I intended to convey what Simon writes in the SO link I initially posted. I'm sorry for utterly failing at that. Lets not hijack this thread any further.
**Simon Marlow** Simon Marlow is a British computer programmer, author, and co-developer of the Glasgow Haskell Compiler (GHC). He and Simon Peyton Jones won the SIGPLAN Programming Languages Software Award in 2011 for their work on GHC. Marlow's book Parallel and Concurrent Programming in Haskell was published in August 2013. Formerly of Microsoft Research, Marlow has worked at Facebook since March 2013. The "noted Haskell guru" is part of the team behind Facebook's open source Haxl project, a Haskell library that simplifies access to remote data. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Oh I attributed no harm to your "I don't think so" :) But do you think that Simon is calling out GHC as being unique in this way? He makes no such claim, and it's in fact fairly typical amongst GCs, including Go's in a different way
I haven't been able to build it even on Linux (ghc 8.2.2): $ stack install Downloaded nightly-2018-01-15 build plan. AesonException "Error in $.packages.cassava.constraints.flags['bytestring--lt-0_10_4']: Invalid flag name: \"bytestring--lt-0_10_4\"" 
I think it's pretty clear that all gc suffer in one way or another. There's no such thing as free cookies. Sadly. :D I read his reply (and the blog post from pusher) as "GHC gc is optimized for throughput, which means it might suffer some latency issues under special circumstances". These "special circumstances" seems to come from having very large live sets of data and a requirement for low latency, or short gc stops. So no, I don't think Simon (or anybody else for that matter) is calling out GHC gc as unique. It's just how things are with GHC gc. It is optimized for certain use cases, which may or may not make it good or bad at some other use cases. Allow me some final hijacking.... I initially replied to the OP because I've been having thoughts about a giant map myself. I currently have a system written in Dart, where I have a collection of documents stored in PostgreSQL. There are some issues with this system, so I thought I'd try a re-write in Haskell, and since I can fit the entire collection of documents in memory, I thought about dumping it all in a map of sorts and just query against that. I have about 2 million documents, of varying size. Access is 99.5% reads and 0.5% writes. After having read about the gc pauses, I decided against the map (without testing it), as what I read made total sense. Instead I settled on SQLite. So far this seems to work very very very well, but I actually have no idea whether the giant map would've worked. I just decided that it didn't, based on the blog post and Simons reply. I guess I'm like the Sand People: I scare easily. 
I've written a program that held abou 2GB in `Data.Map`. Didn't really try to measure the performance of the GC, but the program ran reasonably fast without any special effort. Within 3x of an equivalent Java program I had written previously, even though the Java program used mutable maps. If you've got the time, give it a shot :)
What version of stack are you using? I had to do a `stack upgrade` to something newer than 1.6 (I think?) See [here](https://github.com/fpco/stackage/issues/2759#issuecomment-353373881)
(iirc /u/acow does org/Haskell stuff)
Thank you for the detailed suggestions. I'll investigate and benchmark. And at this point "memory-efficient" just means a program that doesn't cause thrashing :)
With really few writes compared to the number of read, I wonder if storing everything in a compact region which is rebuilt at each update can be a solution. We can even play with `appendCompact` to store the new root of the data structure, if the changes are small compared to the compact region, the memory overhead will be acceptable. 
Is there a paper or description of Hedgehog's shrinking algorithm, or any kind of integrated shrinking algorithm really? QuickCheck had a number of papers published so I'm looking for something comparable to get a feel for the internals. Some googling didn't turn anything up.
If any windows user gets this to work, please post a PR to the hie readme, so others can benefit.
Just take extra care of the internal type used in your map and its strictness. `Data.Map.Strict` guarantee that values are stored in weak head normal form, which *may* not be enough.
That worked, thanks!
&gt; This is a case where I'd use `Data.Sequence`, due to its vastly superior appending performance over both `[]` and `Vector` I was unaware that `Data.Sequence` had logarithmic-time concatenation - will give it a shot. Thanks for your suggestions and insightful comments elsewhere on this post.
I don't know. Maybe it's out of date vs some metadata format in cabal, or some dependency has some unexpected name.
It was enough of a problem that some people would use extensions that woulds squawk and warn if your dependencies weren't consistent. Cf. https://github.com/xeqi/lein-pedantic
Certainly, will do. I've totally broken my local cabal setup though. Between that and some other commitments it may take a few days, but I'll let you know when its sorted.
I'm interested too, I'll PM you in a minute.
&gt; It seems like a reasonable assumption, but what if you want to play a cut-scene? Or how about if you don’t want to always have control over the player? Maybe you’ve just been hit by something big that should exert some acceleration on you, and you don’t want to be able to just press the opposite direction on the control stick to negate it. I'm not sure if I'm not getting the point of the post, but I don't understand why these two scenarios introduce the need for the ECS. In scenario 1 (cut-scene): The simplest solution is probably introducing a cut-scene flag somewhere and then `updateUniverse` checks this flag to see what it is supposed to be updating. I think this flag needs to be introduced in the ECS case as well, because we need to decide how the list of `Position`s needs to be updated exactly. In scenario 2 (forced acceleration): Here again, we can introduce a flag on an `Actor`, which determines what `moveActor` will do exactly: let player run freely, or force movement. And again, in the ECS we also need a flag to decide how the `Position` is updated.
I totally didn't think about that. My original formulation was a little trickier than this stuff and I didn't reconsider getting rid of that type family when I did. I see no reason it couldn't work -- I might keep that in mind for later revisions of the library. Thanks for pointing it out! One advantage of the approach in general (though not in this instance) is if you want to instantiate `f ~ Identity` -- with a type family you can store `a` in this case rather than `Identity a` which makes downstream users of your types much happier. As for `Generic`, my understanding is that the deriving rules don't do the usual `Generic a =&gt; Generic (Foo a)` thing, since what they're describing is the shape of the data structure `Foo` itself. 
The problem is that we need to submit a zip archive of our code and dependencies. Ultimately the problem comes down to submitting a set of files that can be compiled without internet access.
It has to be Zip ? No Containers allowed ? I am thinking of Stack + Docker intégration ? Otherwise Nix might do the trick but can’t confirm this yet
Is the collection immutable, by chance? Perhaps it would be worth it to put the map in a [compact region](http://hackage.haskell.org/package/compact).
Run `stack build --verbose`, copy the last compilation command(s) to a shell script, leave only the needed files, should bd much less than your gigabytes
I updated the post with two promising solutions. Could you test them?
This is a classic windows problem. Normally recommended is to put the application at the root path at like C:\hie and hope. Or get the application itself (somewhere deep in the Haskell standard library?) to use UNC paths (which can be long).
Plus you can tell vim to use haskell highlighting for fenced code blocks with " highlight haskell blocks in markdown files let g:markdown_fenced_languages=['haskell']
It has to be Zip ? No Containers allowed ? I am thinking of Stack + Docker integration ? Otherwise Nix might do the trick but can’t confirm this yet. To compile all in static seems to be a custom set up I think. You might want to pass by GHC to create what you need. 
Yeah -- I'm not saying they introduce the need for ECS, but everything gets a lot easier when you stop trying to think of these things as cohesive functions rather than a big dumb bag of possible behaviors. For the cutscene thing, imagine if you have two components: `hasFocus :: Unique ()` and `isAvatar :: Unique ()`. `hasFocus` indicates to the camera what it should be watching, and `isAvatar` describes where the player's input should be directed. If you unset `isAvatar` from the main character, you no longer need a "cutscene" flag in order to disable movement. Now just by driving who has `hasFocus` you can easily script a cutscene without any explicit support for them from the engine. Likewise, in the forced acceleration situation, you can model this with a `forcedAcc :: V2` component, and have your update code be emap $ do with hasAvatar -- only run this on the avatar without forcedAcc -- if he doesn't have forced accel keyboard &lt;- getKeyboardInputs -- move based on keyboard I'm not saying it buys you anything you can't do otherwise, but this kind of thing is much easier on my brain than trying to thread a million different flags through a giant update function.
I'm the author of Brick and the maintainer of Vty. This is right; Vty doesn't support Windows because Vty is built for Unix terminals. However, there is a dormant pull request on Vty by the prior Vty maintainer Corey O'Connor that anyone interested could consult for context: https://github.com/jtdaugherty/vty/pull/1 I'm more than happy to work with anyone who wants to contribute along these lines, but bear in mind that I don't know what Windows support would mean in Vty or how it should work, so we'd need to work that out together.
The [state machine testing support](http://teh.id.au/posts/2017/07/15/state-machine-testing/) is *very* cool. It's a bit rough around the edges and underdocumented but it finds bugs and it's way easier and more effective than replicating something like that with QuickCheck.
If you do consider it, you may find my library [rank2classes](https://hackage.haskell.org/package/rank2classes) useful. There's also [conkin](http://hackage.haskell.org/package/conkin) and a very nice writeup at https://www.benjamin.pizza/posts/2017-12-15-functor-functors.html 
This is cool, I've wanted this before.
So can you do: newtype Free c a = Free { runFree :: forall b. c b =&gt; (a -&gt; b) -&gt; b } instance (forall a. Monoid a =&gt; c a) =&gt; Foldable (Free c) where foldMap = flip runFree Such that `Free Monoid` and `Free Semigroup` (by the time `Semigroup =&gt; Monoid`) are both made `Foldable` in the same declaration?
Fantastic!
Oh. That issue.
I don't think it will be tactful. They have hundreds issues to deal with, I would keep spam to a minimum.
I've gotten some small PRs merged to Stack before, though nothing on the scale of adding a new command. I think the approach you need to take is the same as you'd use for contributing to any other large project: * build the project (this is pretty trivial for Stack, but can be involved for other projects) * start by identifying the code you're interested in. In this case, it would be the `build` command * find a way to focus in on the specific place you're interested in. In this case, you want to find out where GHC is being executed. * if we delete (or rename) the GHC executable itself, the error we get might give us a clue where the source is located. In particular, grepping for parts of the error message can show us where it's being handled. * just running `find` and skimming through the output can give you a good feel for the layout of a project. e.g. there's a file called `src/System/Process/Run.hs` that is likely used to run GHC. If you grep for usages of the methods in it, you'll likely find the code you're interested in. Ultimately, the thing that makes contributing to a large project hard is the layers of complexity and abstraction that exist. These are different for every project, and learning how to navigate them is a key part of becoming a software developer. Good luck! :) 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [frontrowed/graphula/.../**graphula-core** (master → e2d4f11)](https://github.com/frontrowed/graphula/tree/e2d4f11f8b3dbd873b96f938f29ebe947559d174/graphula-core) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dtfp2xh.)
The `stack` team is super welcoming and friendly to new contributors. I guarantee that you won't annoy them, and they'll likely be happy to help with guiding an implementation.
Here's more past discussion at [vty issue #1](https://github.com/jtdaugherty/vty/pull/1#issuecomment-297143444), where I gathered notes and started a bounty drive.
These may be relevant and interesting: - https://en.wikipedia.org/wiki/ANSI_escape_code#Windows_and_DOS - &gt; In 2016 with Windows 10 "Threshold 2"[1] Microsoft unexpectedly started supporting ANSI escape sequences in the console app, making the porting of software from (or remote access to) Unix much easier. - http://www.nivot.org/blog/post/2016/02/04/Windows-10-TH2-(v1511)-Console-Host-Enhancements
https://github.com/lpsmith/postgresql-simple
Try `maxBound :: Integer`! ;)
Thanks!
https://hackage.haskell.org/package/relational-record
https://github.com/morphismtech/squeal
[Experience report: types for a relational algebra library](https://dl.acm.org/citation.cfm?id=2976016)
https://github.com/ocharles/rel8
It is.
So many choices! It must be hard to pick one. Maybe someone should build an sql-zoo similar to my [frp-zoo](https://github.com/gelisam/frp-zoo#readme)!
and other sugar on top of Opaleye: https://github.com/k0001/tisch https://github.com/silkapp/girella
Postgresql-typed is a continuation of an older library. It's not my favorite in the space, but it is a different design point. I like the ones that dive deep into the relational algebra and rethink how to integrate with it.
A CPS transformation combined with unboxed types gives only a 3x slower Haskell program. ackermannCPS :: Int# -&gt; Int# -&gt; (Int# -&gt; Int#) -&gt; Int# ackermannCPS 0# n f = f (n +# 1#) ackermannCPS m 0# f = ackermannCPS (m -# 1#) 1# f ackermannCPS m n f = ackermannCPS m (n -# 1#) (\result -&gt; ackermannCPS (m -# 1#) result f) although I only checked on 8.0.
Your professor needs to come up with more realistic rules
Haskell Lovesql. &amp;nbsp; ^^^^^^I'll ^^^^^^show ^^^^^^myself ^^^^^^out.
There shouldn’t be anything deeply Windows-Specific about it, although I have no experience with Coq on Windows. What problems are you facing?
Test based bounds sound great. If something is declared to be working but actually isn't you just need more tests, not arbitrary limits that won't be updated with more human work.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [thormick/HaskRel/.../**HaskRel** (master → 1fa1b6b)](https://github.com/thormick/HaskRel/tree/1fa1b6bd3a4e367553fe5f7edbc639423a297b66/HaskRel) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dtg95j2.)
[acid-state](https://hackage.haskell.org/package/acid-state).
Really cool project: https://github.com/agentm/project-m36
[Project-M36](https://github.com/agentm/project-m36) is a Haskell-based RDBMS that properly adheres to the relational algebra. I've used it in a few small projects and it's pretty cool.
0.1.2.4 is on cabal with lower bounds. It builds fine for me on a clean VM (happy is required though). Its also a bit smarter about trying to find a default hoogle DB. Let me know if there are any issues
It could be not stack based if there is something better.
It has to be a zip.
I think it should happily ignore *upper* bounds on the fourth version component, *probably* ignore bounds on the third, and *probably not* ignore bounds on the two major components (though I could be convinced otherwise). It should never ignore lower bounds, as those are more often about preventing some old buggy behavior.
OK, crazy and dumb idea: what if package versions had a special additional version tag/field/thing to signify *additional* APIs? like: 1. `foo-1.0.0` released 2. `foo-1.1.0` makes some breaking API removal or modification 3. `foo-1.1.0+1` adds entirely new APIs then: * `foo-1.1.0 &gt;= 1.1` * `foo-1.1.0+1 &gt;= 1.1` and `foo-1.1.0+1 &gt;= 1.1+1` * BUT: `foo-1.1.0+1 &lt; 1.2` Previously, an author would release `foo-1.2.0` even if it *only* new APIs, because that's the point of the major version fields in PVP. But if a new package version `foo-1.2.0` only *adds* APIs, surely all packages that previously built with `foo-1.1.0` will also build with `foo-1.2.0`. This would also have the benefit of making package versions look objectively cooler. 
This looks awesome! Definitely hadn't seen this one yet, somehow. I'll be keeping an eye on it for sure.
and there's still lots of room, by no means exhausted
https://github.com/helium/postgresql-transactional
That is very cool. Did you see why this affects the core / STG / CMM? 
Do you have an example of some of those large queries? I tend to be suspicious of libraries that try to wrap SQL since in my experience, they often break down once you start using more complex features provided by your DB. Your experience sounds like I might have to give Beam a shot!
Since the lower bounds are almost always mean breaking, we probably would not need ^&gt;= but =&lt;^ Alternatively we could simply add ! at the line where upper bounds need to be respected. 
Who even _has_ bounds on the minor components of packages!? I mean, does that come up? :-)
Don't "more tests" also require more human work?
Oh I see. But that's...confusing. It is a change in the symbol that is not related to the upper bound, yet carries the information about the upper bound, which may not even be present. Would't adding ! after the number be much more clear in its meaning allowing also use that symbol for both upper and lower bounds? Example: foo &gt;= 4 &amp;&amp; &lt; 5.2.1! 
Honestly, insistence that Hackage Cabal files include arbitrary soft upper bounds for dependencies was one of several things that drove me away from posting packages to Hackage. How do I know what future versions of a package will break my particular usage of that package? So I have to be conservative and follow PVP and assume that even though I only use the most stable part of some package's API a "breaking" version bump will cause a problem. These soft upper bounds, as Snoyman points out, are pure noise that obscure the hard upper bounds that really should be enforced because there are known problems. It's too late now, though. Everybody's Hackage Cabal files, including mine, are full of made-up upper bounds. I don't see any way to put that genie back in the bottle short of editing all the Cabal files in the world. Information has been lost, and we aren't getting it back. I guess one answer is to introduce an alternate dependency field with hard semantics: dependencies should only exclude versions that are known to be a problem. Package managers could prefer the hard-dependency field to the old dependency field when it is present, allowing a gradual transition that could eventually allow the soft-upper-bound dependencies to be deprecated away. Really, though, the whole idea behind the PVP — reducing the complex problem of package-version interdependencies to a single dotted-integer scalar — is hopelessly flawed. The only possible maybe right answer I can see is what some of the rest of the software development community seems to be up to: * Explicit contracts need to be specified as documentation for each function in an API. * The only changes allowed to the implementation of a function are to fix bugs, specifically deviations of the implementation from the contract. * The only way to change the behavior of an API is to provide a new function name with a new contract and (if needed) deprecate or remove the old one. * Now no programmer-defined explicit dependencies are needed: the package manager can try to select a set of package versions that provide each package with the APIs it needs to operate. In the absence of some real solution, Stackage should do whatever it wants with Cabal dependencies. It's not like there's going to be any right answer anyhow.
From the few information I could gather I would say JVM is faster. If we're talking about just pure server HTTP requests, we see that Warp is generally not as efficient as HTTP-Kit: https://github.com/jakubkulhan/hit-server-bench This benchmark is old. If you plan to use a web framework, then you might want to take a look here: https://www.techempower.com/benchmarks/#section=data-r14&amp;hw=ph&amp;test=json As you see Haskell framework are quite low in the table. I tend to believe the main reason is that Haskell framework lose more time to check more things than the other framework. Still, if performance is your goal, Haskell doesn't appear to be the best. That being said, I think the performance are still quite reasonable and I'm ok to pay that small price for all the benefit Haskell provide. Also depending on your specific needs, warp can be the best, for example in term of throughput (req/s), warp appear to be way better than nginx: http://www.aosabook.org/en/posa/warp.html
https://github.com/paul-rouse/mysql-simple I've used it a few times, and even made contributions to it.
even better with https://hackage.haskell.org/package/postgresql-query
As long as it respects the lower bounds, I would welcome this change very much
:)
Steen acid-state?
That's imo a confusing description of `^&gt;=`; that's not the intended meaning of `^&gt;=` as I envisioned it. Right now, the [Cabal user's guide](http://cabal.readthedocs.io/en/latest/developing-packages.html#pkg-field-build-depends) likely provides the best *public* description of what `^&gt;=` is for. It's part of a bigger larger framework that's still in the design phase, but as far as build-tools interpreting the `.cabal` files are concerned, the documentation in the cabal user's guide is all they need to know -- that meaning is part of the `cabal-version:2.0` specification and is not going to change retroactively (and most importantly, it's *not* a "soft bound", it's a different concept! It's quite the opposite, it's a hard fact documenting that a specific version is guaranteed to be semantically compatible, and from that the semantically safe version range according to the PVP contract is implied).
&gt; I guess one answer is to introduce an alternate dependency field with hard semantics: dependencies should only exclude versions that are known to be a problem. Suffice to say this doesn't work either. (It *might* work in the Stackage world where every dependency is effectively pinned. It won't work in the constantly changing non-snapshot world of Hackage+dependency-solver.) I *do* think there are problems with the PVP and they stem largely from over-constraining (to e.g. minor versions). It's *extremely* rare IME for a minor version bump (which e.g. adds a new function) to break anything -- it's only really a problem if one is using un-qualified imports galore. People who publish to Hacakge also don't tend to bump versions willy-nilly, so as long a no *unexpected* incomoatibilities turn up, it's usually fine to assume that a bound like "x &gt;= 0.2.3.2 &amp;&amp; x &lt; 0.3" will basically and keep working until "x" version 0.3 is released. (I do tend to avoid depending on rapidly evolving packages, so YMMV, but I've basically never had this type of version bound blow up in my face.)
Yes, and there's good reasons for needing that. But fortunately those cases can be avoided by using defensive `import` styles and avoiding orphan instances most of the time, so don't represent a majority use-case. Also, breakages introduced by minor versions are "harmless" in most cases (assuming the recommendation in the previous sentence is heeded and the PVP contract is held up) and can't result in silent failures/incorrectness.
You'd be stuck writing infinitely many negative tests, i.e. tests that "things don't not work" which is different from "things work" in that -- absent proof via e.g. parametricity -- there are inifitely many ways things could "not work". (Just consider calling any function in IO in a dependency.)
By the way, there's state machine testing support for QuickCheck [also](https://github.com/advancedtelematic/quickcheck-state-machine).
Your question gets another dimension considering many Haskell programs can run on the JVM using [Eta](eta-lang.org)
Thanks for this! Also thanks for the link to Yang's post about design patterns. I was listening to a Haskell podcast (which was a couple years old) in which one of the guests made a comment to the effect that "design patterns are what you need when your language isn't expressive enough to have those things in the standard library". That was thought provoking, and I think you could find examples to support it, but I think it unfairly dismisses a set of useful abstractions that would be VERY useful to someone like myself who is coming to Haskell from a strong OO background. I'm really looking forward to the future posts in this series.
That was excellent! :D
This is a good point. People like to point out "Types don't replace tests!" but rarely point out "Tests don't replace types!" i.e. Tests are extremely flawed as well. They're both important, but even combined, I wouldn't trust them to verify something as breakable version compatibility without *a lot* of rigor.
Are you sure? Am I missing something? I just tried and syntax highlighting is ok but it does not manage automatic indentation.
&gt; does have any lower bound semantics at all? ...have you read the linked cabal user's guide section? It contains a Note about that. Also note that `^&gt;=` has been designed with the PVP contract in mind which gives you more guarantees than Maven can rely upon, and there'll be additional machinery to complement that externally to improve that first order approximation. I'm fully aware that the description in the cabal user's guide is a bit terse, but I can't disclose more at this point.
&gt; Why, thus, I was told so many times such a thing do not exist? ¯\\\_(ツ)\_/¯
&gt; have you read the linked cabal user's guide section? Yea, let me clarify my question. It lays out some rules about what `^&gt;=` means, but these rules don't seem to describe the intended semantics; rather they describe a conservative way to satisfy those semantics. W.r.t. lower bounds, it currently conservatively assumes that it shouldn't ever relax the lower bound. But the doc also vaguely mentions that the *intended semantics* might allow the lower bound to be relaxed. So although it's clear what the operator means, it is not clear what the operator allows the tool to do. Anyway, my point about Maven was that it will see positive knowledge as sort of a suggestion. If all the dependencies in my graph used only positive knowledge, and two of them were in "conflict" about a common dependency, Maven would choose the newer version of the common dependency, as the dependent does not explicitly state that it *doesn't* work with that version. Point being: As far as the intended goals of `^&gt;=` go, it seems that tools should be allowed to treat it as a good suggestion. So it seems to me like the rules in the cabal guide should be relaxed.
The thing that confuses me about the lower-bound semantics is that it only covers a single (potentially breaking) version bump. How should I replace &gt;=1.2 &amp;&amp; &lt;1.4 using ^&gt;= ?
Thanks, I'm glad it was helpful! I've certainly heard variations on that quote before, but I agree that it is not entirely fair. Some of the *encoding* of design patterns is clearly making up for missing language features (e.g. one-method objects where you'd rather use a lambda). But the *motivating idea* behind many patterns seem general enough to serve as a rough translation guide between languages. I haven't decided which pattern to do next, any suggestions?
I am surprised to see that you and u/sclv, who is also a Cabal developer, disagree about the semantics of `^&gt;=`! If y'all don't agree on what it means, I think it will be hard for the community to understand it. In fact, this isn't the first time we've had a Reddit thread trying to figure out `^&gt;=`: https://np.reddit.com/r/haskell/comments/7i4ukq/stacks_nightly_breakage/dqw7idp/
Yes, that's because `^&gt;=` was the smallest incremental extension to the grammar to support this new idiom. And single major versions are currently the easiest to manage dependency specifications if you take into account the combinatorics involved, and for that `^&gt;=` already helps a lot cleaning up the dependency specifcations, see e.g. [`hackage-server.cabal`](https://github.com/haskell/hackage-server/blob/master/hackage-server.cabal#L316-L369) for a non-trivial real-world example where `^&gt;=` significantly improves the readability and reduces the error-proneness of the `&gt;= &amp;&amp; &lt;` combination. But note that `^&gt;=` doesn't fit all use-cases; one very important one for which I'm still working on a good solution is handling the case where you combine the PVP with additional guarantees based on an inverted contract based on a closed world of API consumers (c.f. Ed Kmett's versioning style). That being said, currently you'd have to use `||` as the union operator to join multiple "compatibility neighborhoods" You can e.g. see an [example here](https://github.com/haskell-hvr/text-containers/blob/master/text-containers.cabal#L47-L54), Another way you layout it could be build-depends: base ^&gt;= 4.8.0.0 || ^&gt;= 4.9.0.0 || ^&gt;= 4.10.0.0 And there's already some ideas for how to make this kind of data-point specification more convenient, by e.g. introducing a set-like syntax which would make `^&gt;=` act a bit like a element-of operator, i.e. build-depends: base ^&gt;= { 4.8.0.0, 4.9.0.0, 4.10.0.0 } Which is a more compact way to say the same as w/ the `||` joins, i.e. "this packages is declared to be *known* to be semantically compatible with either 4.8.0.0, 4.9.0.0, or 4.10.0.0". It's also noteworthy that tools like [`staversion`](https://github.com/debug-ito/staversion) have already added support for the `^&gt;=` syntax early on, and make it more convenient for those who subscribe to Stackage based workflows to generate the meta-data for your `.cabal` files, which also does some compaction of contigous ranges, e.g. $ staversion --format-version cabal-caret --aggregate pvp -r lts-6 -r lts-7 -r lts-8 -r lts-9 -r lts-10 rfc.cabal ------ lts-6 (lts-6.35), lts-7 (lts-7.24), lts-8 (lts-8.24), lts-9 (lts-9.21), lts-10 (lts-10.4) -- rfc.cabal - library base &gt;=4.8.2 &amp;&amp; &lt;4.10 || ^&gt;=4.10.1, aeson ^&gt;=0.11.3 || ^&gt;=1.0.2.1 || ^&gt;=1.1.2 || ^&gt;=1.2.3, servant ^&gt;=0.7.1 || ^&gt;=0.8.1 || ^&gt;=0.9.1.1 || ^&gt;=0.11, classy-prelude ^&gt;=0.12.8 || ^&gt;=1.0.2 || ^&gt;=1.2.0.1 || ^&gt;=1.3.1, uuid-types ^&gt;=1.0.3, lens &gt;=4.13 &amp;&amp; &lt;4.15 || ^&gt;=4.15.1, http-api-data ^&gt;=0.2.4 || ^&gt;=0.3.7.1, text ^&gt;=1.2.2.1, servant-server ^&gt;=0.7.1 || ^&gt;=0.8.1 || ^&gt;=0.9.1.1 || ^&gt;=0.11.0.1, ...
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-hvr/text-containers/.../**text-containers.cabal#L47-L54** (master → 989b693)](https://github.com/haskell-hvr/text-containers/blob/989b6931a07948998f92340035e21dac4c366196/text-containers.cabal#L47-L54) * [haskell/hackage-server/.../**hackage-server.cabal#L316-L369** (master → 2e70ec3)](https://github.com/haskell/hackage-server/blob/2e70ec313379d75ede6c70ec211b4551777ec80b/hackage-server.cabal#L316-L369) ---- 
The first thing that comes to mind is there may not be just one generalization. For example, `($) :: (a -&gt; b) -&gt; a -&gt; b` generalizes to `ap`: ap :: Monad m =&gt; m (a -&gt; b) -&gt; m a -&gt; m b ap mf mx = do f &lt;- mf x &lt;- mx return (f x) but it *also* generalizes to ap' :: Monad m =&gt; m (a -&gt; b) -&gt; m a -&gt; m b ap' mf mx = do x &lt;- mx f &lt;- mf return (f x) 
I definitely agree with that quote, with the caveat that it doesn't quite tell the whole story. The other reason design patterns are used are to structure code in predictable ways at larger levels. Going from "I want to iterate through a container" =&gt; for loop to "I want to iterate through all of my objects" =&gt; visitor pattern; well, it can be helpful to people who are still figuring out how to structure code. Unfortunately Haskell doesn't seem to have a lot of resources on API design, architecture at the module level, how to design a function to be as useful as it can be, and overall architecture of an application. Tradeoffs and different approaches would need to be given since Haskell has too many ways to do any one thing (a blessing and a curse of it's expressiveness) So, while design patterns work around weaknesses in a language's expressiveness, they can also be used to teach architecture guidelines and principles. I think Haskell could use a bit more of the second :)
The top answer to this question is excellent. Also, for those interested in more reasoning about `seq`, I asked a [tangentially related question](https://stackoverflow.com/questions/45043009/strictness-of-datatotag-argument) some time ago that was met with an another informative answer.
I've never considered that before, honestly.
Right now, when trying to compile Enum.v, I'm getting the error Compiled library mathcomp.ssreflect.ssreflect (in file C:\Coq\lib\user-contrib\mathcomp\ssreflect\ssreflect.vo) makes inconsistent assumptions over library Coq.Init.Notations I've tried reinstalling ssreflect with no success.
They're not disagreeing about the current implementation. What /u/sclv is perfectly compatible with what /u/hvr_ has written in the Cabal user guide. It's just seems that /u/hvr_ has some future plans that might expand the implementation in the future. The precise semantics are just "I know this one version is good; do with that as you will."
I think it's just fairly often obvious when it's overkill. Like using `Control.Category/Arrow` instead of `(-&gt;)`; it's more powerful, but just not usually necessary.
That was a great read! I'm looking forward to the next articles. It might be worth mentioning that this pattern exists in the standard library, e.g. [Data.List](https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-List.html#g:22)'s `sortBy` / `deleteBy` / etc.
I'm not expecting negative testing, only positive testing that the important properties required of interactions with dependencies hold.
&gt; it's highly likely that the above bounds could all be collapsed into a single version range Sure, but you need additional evidence to justify that; once you have it, they collapse.
Right, but it is typical to only put an upper bound at the next major version, and the pvp is fine with this...
Correct. He’s being careful to specify only the current desugaring. I’m gesturing towards the future and therefore potentially suggesting things that may turn out differently (because they’re in the future). 
Did you try it with hFlush? It seems to be a thread buffering or locking problem to me.
I think the best way would be to sort the list, then use group to clump all of them together. Sort that comparing length and if you use a stable sort, the first element will be the most common. Typing this by phone so no examples sorry. 
Also maybe look into that Ubuntu in Windows thing, WSL, https://docs.microsoft.com/en-us/windows/wsl/install-win10
Thanks!
Maybe represent them using `ImplicitParams`?
[Here is one of the servant author's experiences](http://reddit.com/r/haskell/comments/6b3dlt/techempower_benchmarks_14_released_with_servant/dhl500h). And [here are the results that the techempower servant benchmark author got](https://turingjump.com/blog/tech-empower/), which are *far* better than the officially posted results.
Just because I love pushing `-XTransformListComp`: mostCommon xs = maximumBy snd [(the x, length x) | x &lt;- xs , the group by x using groupWith]
Unfortunately rather awkward to use in practice, and in fact *so* awkward that I preferred to use the untyped relations.
 data A a = RetA a | B (B a) deriving Eq data B a = RetB a | A (A a) deriving Eq Compiles flawlessly with GHC today.
Oh, cool! Someone's open sourced such a thing. Every shop I've worked at has had its own flavor of this wrapper.
I did not know maximum behaves like that with tuples, thanks a lot.
Using group is a nice idea, thank you.
Good point.
Ah. In that case I think you're right, sounds like socket magic rather than Haskell magic to me.
It may be my question was too convoluted. Now the question about flushing is answered I should probably create a new one for the issue with sockets.
I made this for myself but you can have it too. Sources: http://starcolon.com/IMG/ARTWORK/haskell-logo.png https://cdn-images-1.medium.com/max/2000/1*WTT1eEsZZeqJ86Itt9SAHQ.jpeg
Anytime. Sorry I couldn't figure it out
I've used `persistent` and `esqueleto` and have contributed to both. A wonderful point in the design space -- the types that `persistent` is able to impose upon the database have sussed out a ton of bugs, and the easily extensible nature of the library makes it easy to take advantage of database specific operators. When you have to drop to raw SQL, the libraries get right out of your way.
Selda is nice for a small project I did. I just needed something to quickly define tables, and seed a DB. It was nice and easy to use. Selda can be used with SQLite and PostgreSQL. The small project that I just mentioned was using SQLite, but when I tried to create a bigger project, I ran into some hiccups with Selda and PostgreSQL. Selda would have issues converting the information from PostgreSQL into the types in my application, and it became a headache. It could have possibly been something that I was doing wrong, but I would commonly get these errors telling me that the data from my DB couldn't be applied to my type constructors, but when I would do something similar with my other project where I was using SQLite, I wouldn't get these errors.
I'm using Opaleye and I generally really like it. I love the basic idea of having an arrow-based DSL providing a haskelly interface while keeping you within the confines of valid queries. I do wish the type errors were better. In particular I don't need the generality of `Default QueryRunner`, I'd prefer to have a 1-to-1 relationship between the Haskell and the DB types. There's also occasionally some functionality or convenience missing that I want. Overall though, do recommend.
I see now. Thanks for posting it. I had seen the former but not the latter blogpost.
&gt; I think it should happily ignore upper bounds on the fourth version component, probably ignore upper bounds on the third, and probably not ignore upper bounds on the two major components (though I could be convinced otherwise). I think the opposite. The vast vast majority of version constraints are on the second major component. And it is definitely these that we are talking about ignoring, since 90% of outgoing communication from Stackage curators to package maintainers is to let them know that they should relax their upper bounds because a new "major" version of one of their dependencies has been released. For many (but not all) packages, this can be addressed merely with a revision to the upper bounds. When people put constraints on the third (minor) component, this is either a) very intentionally avoiding a known broken build, b) a package that is intended to be versioned in lock step with the dependency in question, or occasionally c) unintentional or misguided.
As toonnolten noted, "adds [only] entirely new APIs" is a minor version bump, not major. `foo-1.1.0` -&gt; `foo-1.1.1`.
I see... Then honestly, it sounds like version numbers do not convey nearly enough information to adequately create constraints, since people constrain on components in a way that seems opposite to the intention of those components. I'm starting to think we have two real options: 1. Follow Maven's lead, and encourage people to just use `^&gt;=`, having the build tool break ties by choosing the newer version. This is sort of ad-hoc and unfortunate, but seems like it would get the job done in practice. 2. We could *fix* the fact that versions are conveying the wrong information. When we find that a minor version bump breaks another package, rather than constraining against that version, we should deprecate that version, re-releasing it with a major version bump. This sort of implies that the current approach of fixing the depending package with revisions is the wrong way around, in that it's the *dependency package* which should be fixed with a proper release system.
Wow, Beam looks great! I wonder why it's not on Hackage?
`beam-core` and `beam-sqlite` are on Hackage as of very recently. `beam-postgresql` is only on GitHub but that's what I've been using as my backend. https://hackage.haskell.org/package/beam-core
Author here :) And the answer is that I'm working on putting the newest version on hackage. `beam-postgres` should be up by the end of the week.
/u/jkarni, can you comment on why your results were so much better than the officially published results? I wonder if it's just GHC producing flaky performance in the particular environment they used.
Hah, I wrote one recently, too.
Whoops, I see. My bad, I must have mis-analysed it then. Hm, I'm not entire sure what is causing the huge delta in that case. Do you?
I see. So, you are saying that this would not occur on a "larger" benchmark? 
No but I imagine it is due to constructing an implicit stack using CPS which i higher performance than the usual one (for some reason.) Note that this is slower if boxed types are used which suggests that the stack can be be represented more efficiently using unboxed types. Next I want to try using an explicit stack along the lines of: data Stack = Nil | Cons !Int# !Stack | Ack !Stack I couldn't really get that to work with good performance though. I could use an actual Array type but I don't think Haskell can optimize through imperative code that reads and writes to an in memory array like that.
Wait, no, it further extends the core language with descriptions... there is also a confusing notion of equality... I'm not sure this is that simple, is it?
Not sure I would say it has to do with a benchmark being "large". It shouldn't make a big difference for code that doesn't require such a deep stack. As I understand it the issue is two fold: * The RTS increases stack in fixed increments so you trigger a GC for every X levels of recursion. * As the stack size grows each GC also takes longer as it traces a larger stack. This might cause the nonlinear behavior. For C code it doesn't come up because the default size is a few MB and IF you run out of stack it will just crash. Haskell is usually heap heavy so a smaller stack default makes sense. (Default is 1k iirc) Also it doesn't crash if it outgrows the stack and just triggers GC instead. So using a smaller default isn't as much of a risk.
OK, so I guess there are a few different cases: - The base 4.{8,9,10} example which is clear-cut either way - Bounds automatically inferred from snapshots like the `staversion` output. Those will always be ugly and TBH I don't care much about them. - Bounds manually maintained by those of us trying to hold the PVP line ;-) If there are multiple major versions supported then old version range syntax will always be shorter and clearer than the ^&gt;= syntax.
Hi. I haven't had time to go much further, but I created a branch where I did a first pass at cleaning up some of the things to get closer to the way I would write them. You can see the changes in this [pull request](https://github.com/lgastako/twitter-api-hs/pull/1). A lot of the changes are cosmetic just because it makes it easier for me to work with the code (eg the import formatting, etc) so those are obviously a matter of taste, but you'll find some that are more substantial, eg. eliminating IO from type signatures where it's unnecessary. The next thing I was going to focus on was removing usages of `fromJust` and other partial functions. If I get some time I'll do another pass, but I wanted to share what I had with you so far. Let me know if you have any questions about the motivations or reasoning behind any of the changes. Hope this helps.
Or better on Stackage (then it get build-tested along with all other packages in a set).
You could write `specialised = sortBy (comparing mostCommon)` and avoid writing `specialCompare`
[I take a liking to vcache](https://www.reddit.com/r/haskell/comments/7is4m5/what_popular_databases_are_written_in_haskell/dr1nwui/?context=10000). cc /u/jared--w 
That could be the problem, but I’m not sure – sounds like a more general Coq issue. However, we don’t support Coq 8.7 yet (and have no particular upgrade plans), so to get `hs-to-coq` to work you’ll have to use Coq 8.6.x regardless.
Hi, and welcome to /r/haskell! :) I'm not sure, I've seen one of the current hosts here on reddit, but I remember having seen one of the former hosts, /u/reinh, here on occasion. I've dropped Jekor a mail linking to this post. Maybe he has time to comment on future plans regarding the Haskell Cast.
I'm very intrigued by your library. Almost enough to spend the time to swap our growing code base over. What would one have to do when their records use `Data.Tagged` for the majority of their types? ``` data UserIdTag type UserId = Tagged UserIdTag Int32 mkUserId :: Int32 -&gt; UserId mkUserId = Tagged data EmailAddressTag type EmailAddress = Tagged EmailAddressTag Text mkEmailAddress :: Text -&gt; EmailAddress mkEmailAddress = Tagged data User = User { _urId :: Columnar f UserId , _urEmailAddress :: Columnar f (Maybe EmailAddress) } deriving Generic ```
We started using Tisch when we dove in last year. It works great and erases a lot of the Opaleye boilerplate we wanted to avoid. However, k0001 made it sound like they were either going to rewrite it without the Opaleye dependency at some point, and there hasn't been much in the way of updates for the last 8 months which has me a little nervous when it comes to using a well supported database library.
Where there are lots of Haskell programmers in one company, there will be more likely to be entry-level positions. I would try to identify the places on this list with large number of Haskellers and contact them (https://github.com/erkmos/haskell-companies)
I kinda feel like an idiot now. I was just recently introduced to these comparing functions so I was not sure how they work exactly. Once more, thank you.
I don't think you need the comparison wrapper, the Ord for (,) will handle the comparison: λ&gt; let common = foldr max (-1, -1) . map (\x -&gt; (length x, head x)) . group . sort λ&gt; :t common common :: (Ord t, Num t) =&gt; [t] -&gt; (Int, t) λ&gt; let xs = [1, 2, 9, 3, 4, 4, 4, 5, 9, 6, 7, 7, 7, 7, 5, 3, 9, 5, 8, 9, 8, 8] λ&gt; common xs (4,9)
Don't worry, it happens to all of us. I think Haskell is such a well thought out language that it makes all of us feel stupid!
I've been reading through your incredible documentation all morning. Between that, your detailed reply, and some discussion with my peers it sounds like the rest of my day will be spent experimenting with beam. Thanks!
We have a build reports upload api in hackage, though its only tied to the docbuilder at the moment. The matrix builder is also a source of such data.
The sandbox solution you have sounds like it should work just fine. Note you can also “vendor” dependencies through cabal.project files and new-build.
Does beam support transactions (at least on postgres)? It looks really nice, but transaction support is something that I need/want in a SQL lib.
Or far off in the past - https://github.com/haskell/ecosystem-proposals/pull/1 - https://www.yesodweb.com/blog/2015/09/true-root-pvp-debate I truly believe we know much better ways to solve our problems than the status quo.
Beam doesn't deal with the backend at that level. You just pass it a connection. You can start and end transactions at will and run beam queries on those connections. For postgres, you can just use `postgres-simple`'s `withTransaction` function. I wrote a simple monad for transactions that's not yet a completed package: https://github.com/3noch/beam-postgres-transaction-monad
Why does that type have a `MonadIO` instance? I don't want arbitrary IO mid-transaction. What if it aborts and I've sent out emails or something?
While this might be true, it's the kind of clever knowledge that makes understanding the code harder than it should be. 
Thanks a lot for the detailed answer! That’s quite helpful.
you can test this yourself if you use for example `trace` from the `Debug.Trace` module to output some string when your expression is evaluated
As a case, it takes a patch of 13 changes to 5 different modules to add a command line / stack.yaml option that does nothing. It does show up in --help automagically, but that's thanks to optparse-applicative.
Hedgehog is neat. Foundation’s check is still WIP but promising.
Yep, it might be worth aliasing it to `lastMaximumBy` or somehting like that.
Yeah, I do hope they make more episodes. I enjoyed each of them thoroughly and they're still on my regular playlist for repeats.
This is probably completely unrelated, but isn't it strange to use a float for your iteration variable?
Thanks for letting us know how much you love the podcast. It helps us to hear from you. We don't have any more guests scheduled currently due to lack of time on both my and Alp's (/u/AlpMestan) part. The main bottleneck is the time it takes to edit and upload each episode (usually something like 6 hours spread over 2 days). We haven't given up though, and have experimented with a faster editing workflow on episode 14 (although due to inexperience and particularly bad mic audio, it led to poor audio quality).
Beam is for dealing with sending SQL commands -- queries, data manipulation, and data definition (with `beam-migrate`) -- and receiving SQL output. This is a small (but significant) part of the DBMS experience. Each DBMS is so unique that it's a disservice to everyone to force a standard transaction model on them. The best place for this kind ofcode is backend libraries tailored for the particular DBMS you use. Beam backends use regular haskell database interface libraries. There is no magic. `beam-postgres` uses `postgresql-simple`, `beam-sqlite` uses `sqlite-simple`, `beam-mysql` uses `mysql-simple`. It does not take them over, though. You are free to use whatever functionality those libraries offer to run transactions.
Aside from base, you don’t have to put upper bounds on packages on Hackage. And for base you can just put &lt; 99. I do this now for about a dozen of my packages and there’s no issue.
The `evaluate` version creates a thunk each iteration which results in the increased number of garbage collections you observed. `evaluate` boils down to the `seq#` primop. After simplification you roughly have something like this: ```haskell case seq# (f x) s of ... ``` CorePrep ANFises the primop application to something like this: ``` let t = f x in case seq# t s of ... ``` So the huge number of GCs can be explained by unwanted thunk allocations here.
Are you sure? How does that explain my floated value experiment in my comment?
Summer of Haskell will probably be your best bet. https://summer.haskell.org/ Getting starting exploring one of those ideas now would put your proposal in a very strong position come application time. 
Decorator
Yes. What follows is the STG output for the different variations: original `evaluate` $wgo = \r [w_s4fE w1_s4fF ww_s4fG w2_s4fH] case ==## [ww_s4fG 0.0##] of sat_s4fI { __DEFAULT -&gt; case tagToEnum# [sat_s4fI] of { False -&gt; let { sat_s4fK = \u [] w_s4fE w1_s4fF; } in case seq# [sat_s4fK w2_s4fH] of { (#,#) ipv_s4fM _ -&gt; case -## [ww_s4fG 1.0##] of sat_s4fO { __DEFAULT -&gt; $wgo w_s4fE w1_s4fF sat_s4fO ipv_s4fM; }; }; True -&gt; (#,#) [w2_s4fH ()]; }; }; `sat_s4fK` here is the thunk I mentioned earlier. original `seq` $wgo = \r [w_s4e4 w1_s4e5 ww_s4e6 w2_s4e7] case ==## [ww_s4e6 0.0##] of sat_s4e8 { __DEFAULT -&gt; case tagToEnum# [sat_s4e8] of { False -&gt; case w_s4e4 w1_s4e5 of { __DEFAULT -&gt; case -## [ww_s4e6 1.0##] of sat_s4eb { __DEFAULT -&gt; $wgo w_s4e4 w1_s4e5 sat_s4eb w2_s4e7; }; }; True -&gt; (#,#) [w2_s4e7 ()]; }; }; No thunk here. GHC enters f to x and enters but forget its result. No allocation happens here. `evaluate` with `y = f0 x0` floated out $wgo = \r [ww_s4fi w_s4fj] case ==## [ww_s4fi 0.0##] of sat_s4fk { __DEFAULT -&gt; case tagToEnum# [sat_s4fk] of { False -&gt; case -## [ww_s4fi 1.0##] of sat_s4fm { __DEFAULT -&gt; $wgo sat_s4fm w_s4fj; }; True -&gt; (#,#) [w_s4fj ()]; }; }; GHC is able to drop `y` as it sees its result will never be used. (No allocation either) `seq` with `y = f0 x0` floated out $wgo = \r [ww_s4dQ w_s4dR] case ==## [ww_s4dQ 0.0##] of sat_s4dS { __DEFAULT -&gt; case tagToEnum# [sat_s4dS] of { False -&gt; case -## [ww_s4dQ 1.0##] of sat_s4dU { __DEFAULT -&gt; $wgo sat_s4dU w_s4dR; }; True -&gt; (#,#) [w_s4dR ()]; }; }; Same as above, `y` will never be used and so is dropped. 
And just to be really sure. Here is the dumped CMM code for the initial `evaluate` variant: c4iD: I64[Hp - 24] = stg_ap_2_upd_info; P64[Hp - 8] = _s4fF::P64; P64[Hp] = _s4fG::P64; I64[Sp - 32] = block_c4iB_info; R1 = Hp - 24; P64[Sp - 24] = _s4fF::P64; P64[Sp - 16] = _s4fG::P64; F64[Sp - 8] = _s4fH::F64; Sp = Sp - 32; if (R1 &amp; 7 != 0) goto c4iB; else goto c4iE; c4iE: call (I64[R1])(R1) returns to c4iB, args: 8, res: 8, upd: 8; c4iB: We can see GHC allocates an updatable STG_AP_2 thunk which is filled with `f` and `x`. Strangely we can see there is a redundant evaluatedness check (`if (R1 &amp; 7 != 0) ...`) which will of course always be false. So it will always enter our thunk in `R1` (block `c4iE`). 
&gt; classy lenses, free monads, parser combinators, lazy-on-demand api's How are these patterns exactly? 
Thank you for replying and letting me know. Listening to your casts has been a major learning experience for me and I hope you guys keep making more.
PM'd -- get in touch!
Great, thank you for that list! I've been looking for companies that use Haskell, but didn't know about that list; having them all there in one place is very useful. I'll investigate the companies on that list and get in touch with the ones that look like they might have suitable positions.
Thanks; I'll start exploring the projects and thinking of my own ideas. This sounds like a great program -- I did GSoC last year; it was very interesting and I learned a lot from it, so I'll almost certainly apply for the Summer of Haskell if I can find or think of a project that's suitable for me.
Good points! You can also rephrase let m = mean xs in sum [ (m - realToFrac x) ^ 2 | x &lt;- xs ] slightly more idiomatically as sum [ (m - realToFrac x) ^ 2 | let m = mean xs, x &lt;- xs ] 
Oh... I was totally wrong about this. Sorry for my lack of knowledge. Thanks for answering my question.
I did it 
Also known as if-then-else
Thank you very much for your help and comments. It helps a lot for sure. Thank you again
Yeah, those sound like pretty big issues. For now, it might be worth looking at a Windows 3rd party terminal or two that support ansi things and escape sequences (if there are any) and then putting links to them in the readme and testing that they work. It won't be a full real solution, but it'll at least be something Windows users can use to run the apps :)
There has been a thread on [haskell cafe](https://mail.haskell.org/pipermail/haskell-cafe/2016-January/122745.html) which discusses the idea. 
I think this is a valid question, but you may wish to read up a bit more about it. If you haven't read this yet, it deals specifically with VN and FP: http://wwwusers.di.uniroma1.it/~lpara/LETTURE/backus.pdf It is an old paper, but an important one. One could argue that current massively parallel architectures like GPGPUs and Hadoop are more attuned to the functional style, because pure functions are more easily parallelisable. Hence MapReduce, an allusion to the FP concepts in Google's compute architecture. There is one new architecture which has been introduced recently, called the Mill Architecture, which operates on a concept of streams of data, rather than mutation of registers. This also seems to have more of an affinity with FP than VN. There haven't been many updates on that recently to my knowledge, however.
Modern computer architecture is based around caching, instruction reordering, parallel execution, and aggressive pipelining. These techniques are incredible performance boosters and simply aren't feasible to implement in logic circuitry with a "functional programming" architecture. Modern CPUs are based around RISC ISAs (even x86 has microcode), which look pretty much like a declarative 3-op SSA with conditional jumps. More bulky ISAs with FP abstractions would require way more hardware for decoding, and probably would be slower anyway because they'd be harder to pipeline. With all that said, it turns out that one of the boons of functional programming languages is that certain kinds of static analysis are easier, enabling much better code generation. Naive (i.e. stack-based) recursion is still kinda slow, but in a language like Haskell many things get optimized to eliminate that performance overhead -- whereas in C you must manually make that optimization, introducing more complexity, more places for errors, and less expressive code.
What is your definition of a pattern?
&gt; That being said, I don't see any particular reason that would mean our approaches would be necessarily mutually exclusive. Having explicit mutability would actually improve some of the ecstasy APIs along the boundary. ... I suspect there's a solution here that would be optimized away by GHC however. Interesting, I'm curious to see what you come up with &gt; If you're interested, I'd love to chat more about this and see if we could find a way to combine our efforts. Of course! 
A convention that tells you how to structure code in your project.
ok, well, some might be better fit under that than others, but certainly those items are conventions, which either structure code or help solve a problem that allows you to structure code a certain way (e.g. without them, you must choose some other organizing principle for your code)
I appreciate the idea of self directed learning, but would you mind elaborating on this for the slow kids in class? I see this idea thrown around a lot, and I understand that this is forcing GHC to identify that the two separate ops are equivalent and therefor re-use the result and not re-run the computation, but I don't understand why GHC can't recognize the equivalence given context. 
I will also leave [this excellent explanation here.](https://stackoverflow.com/a/9814654/389837)
Also hard to test if you rely on services (eg. a database). Even if you use travis it's not that simple, you have to start the DB in the background, wait a few seconds to let it start up, run your test, then tear everything down. Tests can be flakey when they depend on external services (eg. a package running queries against google or github HTTP endpoints). Also some packages rely on native libraries. Getting all the tests to run reliably is a huge undertaking.
Okay, good. I was about to worry, because I use `evaluate` to make sure that a value is evaluated in a `catch` block so it can do `MVar` cleanup if it fails.
Yes, for example sequential addition instructions can be often done in parallel, if they are arranged the right way. Moderns CPUs have pipelining stages which run in parallel.
Could this be an issue in GHC? Why is the function call not inlined into the `case`? It's not used elsewhere. 
Just wanted to add that I love the podcast and constantly watch for any new episodes! Keep up the great work!
Some people did work on a [specialized architecture](https://www.cs.york.ac.uk/fp/reduceron/) to run Haskell code fast, though it used very different ideas than yours.
This makes me wonder: when people ask about Windows support in terminal applications, what do they *expect*? What is it that we think actual Windows users are likely to be able to (or want to) do in order to use terminal-style applications? I really don't know any Windows users who would want to do this, so I don't know what's likely to come up. But I mention this because I think it's only worth going to the trouble of implementing Windows support in a way that is going to work realistically for users, and I don't know any of them. :)
Think about what would happen if you would eval a long xs for the first - would you want it to remain in memory for the second part
GHC can recognize the equivalence, but performing the "optimization" would result in a pretty large space leak. See: https://mail.haskell.org/pipermail/haskell-cafe/2003-June/004484.html
To put this another way, when a Windows user encounters a terminal application and wants to run it but is unable, then the questions I'd want them to answer are "If you could run this, how would you expect that to work? Do you run other terminal applications successfully? If so, how do you run them?"
Complete example with minor corrections: {-# LANGUAGE TransformListComp #-} import Data.Ord (comparing) import GHC.Exts (groupWith, the) mostCommon xs = maximumBy (comparing snd) --------------- [ (the x, length x) , x &lt;- xs , then group by x using groupWith ---- ] 
This is great help, some good ideas here, thank you! I'm not sure if racing `recv` and `threadDelay` will work. Currently `recv` seems to not block and immediately returns no data instead, then `forever` loops furiously, running every core in the CPU at 100%. If it were blocking, the process should sleep (I think). Maybe closing the socket is part of the problem? Perhaps (after the first loop, when the socket is activated) `getActivatedSockets` is returning `IO (Just [])`. Is that a case I should be accounting for? I'm trying to think of reasons why it might be looping as fast as possible without blocking and waiting for data.
I wonder if this could be contributed to Hackage instead? Just swapping out the file storage system seems like it could be a relatively uncontroversial change, since it wouldn't require any changes to the concepts of versions or revisions as Hackage sees them. It would basically just be a storage optimization trick that also gives us hash based downloads for things that *aren't* on Stackage.
I guess exponential time is less of an issue when the output is exponentially long to begin with
My employer, [Digital Asset](https://www.digitalasset.com/), has a bunch of folks in Zürich, and a significant cross-section of the work we do is in Haskell with a cryptographic and financial focus. It could be a decent fit. https://www.digitalasset.com/careers
It's hard to tell from context, but I am fairly sure `recv` blocking would be continuously polling the socket for content. BUT Yes, your exit condition is `Nothing`, it should be `IO (Just [])` That sounds like a more likely culprit. See the docs: [getActivatedSockets](https://hackage.haskell.org/package/socket-activation-0.1.0.2/docs/Network-Socket-Activation.html#v:getActivatedSockets) `Nothing` is representing another case entirely. Probably good to solve both, because `recv` should probably have a safety timeout anyway. 
Thank you so much, this is very helpful. I now have a way forward and some things to try out.
There is others point of interest in your code related to performance. First of all, `sum` is known for generating space leak. On big list, in means that instead of storing the accumulated sum of your list, it stores the accumulated list of operations needed to compute the sum of your list. It is usually advisable to replace `sum` by `foldl' (+) 0` which does not suffer from this issue. This can result in an important memory usage and time reduction. Finally, your variance needs three traversals of the input list, which means that the input list will be realized in memory and take space. I suppose that your `sumOfSquares` is part of a variance computation, for which it exists [iterative functions](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data) hence you will be able to compute your sum in constant space. In some case, it can even totally remove the allocations on the input list items and compile to an optimized loop. 
The easiest way seems likely to be a priority search queue. Use the values as keys and the frequencies as priorities. Once you've built the PSQ, you can pull out all the keys with the highest priority and compute their maximum.
&gt; I wonder if this could be contributed to Hackage instead? Funny that you mention that... you may be interested to know that I had been drafting an extension to the Hackage repo/security protocol (possibly to be implemented by a GSOC student) when it became obvious last year when working on http://head.hackage.haskell.org &amp; http://cand.hackage.haskell.org prototypes that mutating source tarballs require extensions to the protocol in order to have a more complete story, including (but not limited to) the ability of exposing a download endpoint that allows content-indexing via sha256 hashes. So it's quite likely Hackage will provide something in this spirit in order to fully support a candidate release package workflow.
I've quite a few internship ideas for people willing to cope with Paris, be it in summer (spoiler: it's awesome) or not. I'll let you find me on the Internet.
Nice! Perhaps you and /u/snoyberg could coordinate to avoid duplication of work? I see no reason for Stackage to implement a custom solution when it seems everyone is intent on solving the same problem.
IPFS can do the work. If you do that, please do a littlw extra work to make it also compatible with IPFS. there is a general gateway `https://ipfs.io/ipfs/&lt;hash&gt;` that would load the package from any repository where it is (even from many of them at the same time). IPFS uses multihash this means that in the future an special hashing can be defined to hash individual functions if the hash normalize the variable names and the layout. IPFS is quite fast. IPNS, the mutable counterpart is not so much when used with the public gateway, but the latter is not necessary in this usage case. If a local IPFS daemon is installed, the retrieval would be even faster and it can instantly publish directly from the local computer disk.
Dunno about the others, but I'd call free monad a pattern, in that it defines the structure of your code. Actually, I guess I often think of architecture when I think of design patterns, so I'd put mtl-style, RIO-style, free/freer/interpreter-style in there.
[removed]
Yeah I agree.
Yup, I also thought of using IPFS when talking with snoyberg about these ideas a while back. Quite possible that it will be used, or at least be an optional backend!
Interesting read comparing computer architectures for implementing graph reduction machines, proposes efficiency improvements for graph reduction on standard architectures: http://users.ece.cmu.edu/~koopman/tigre/index.html
Using IPFS-the-network also has the advantage of making it really easy to work offline, and also making it easy to mirror data on your own servers (Just `ipfs pin add $rootblock`).
Developers who understand that a shell is good, but are stuck on windows at work, is a statistically significant portion of the population. Source: I am in that category, and I am statistically significant. =P In seriousness, it's not a case of expectations so much as it is unfulfilled desire. I understand and sympathize that the task of attempting to wrap an API around the 'shared' functionality between a linux terminal and a windows console is obscenely difficult, but this does nothing to mitigate my desire for a magical solution to the problem. Interesting point of fact: `cmd.exe` does actually present an instance of an interactive, "full screen" terminal application, in the form of the `more` command. Do not confuse this with the *nix more, it is a different, and far more limited beast. Try it out sometime the next time you're on a windows machine for kicks. 
You don't really explain why an ISA with "FP abstractions", whatever you have in mind by that, would require way more hardware for decoding and would be harder to pipeline, or why they would be "bulky". This also disagrees with your later claim that that certain kinds of static analysis are easier. Optimizations like pipelining are a type of static analysis.
You know what also would be cool. A wall paper in which the two *legs* of the haskell logo branch into two smaller haskell logos, ad infinitum (c.f. [Binary tiling](https://en.wikipedia.org/wiki/Binary_tiling)).
How would I read a file into a streamly Stream?
Others have already addressed your question, but as a side note to: &gt; computer architectures based on functional programing(such as LISP Machine) i think it's fair to say that Lisps don't *inherently*/*necessarily* involve FP in the way that Haskell does, even though there are some Lisps (such as Clojure) that do, and even though, more generally, one can program in an FP style in many (most?) Lisps. Of course, this is all setting aside the issue of what constitutes "a Lisp" (cf. whether or not Scheme is a Lisp), and what consitutes 'functional programming'[1] or a 'functional programming language'[2]. [1] cf. e.g. [this page on the HaskellWiki](https://wiki.haskell.org/Functional_programming), or [this Quora discussion](https://www.quora.com/What-is-functional-programming?share=1). [2] cf. e.g. "[[Common] Lisp is not functional](https://letoverlambda.com/index.cl/guest/chap5.html#sec_1)", or "[What is a Functional Language?](https://existentialtype.wordpress.com/2011/03/16/what-is-a-functional-language/)" by Robert Harper.
Just provide a cleaner function and suggest your user to obey the bracket pattern, preferably provide an example in the document. Exceptions in haskell are messy, async ones make them even messier. DEAL THEM WITH CARE!!!
How about the spineless tagless G-machine?
If either of the show's producers is reading this, please do an episode on Nix. 
Your withPairing function needs work. Take a look at the type of withFile from base withFile :: FilePath -&gt; IOMode -&gt; (Handle -&gt; IO r) -&gt; IO r withFile name mode = bracket (openFile name mode) hClose You should aim for an API with a similar style using bracket. 
&gt; faster than C using this approach I’m not sure what you mean. Do you have a link to docs or data?
Has there been any noise about graph reduction since the mid 90s, though? I don't recall seeing a lot of research about it and iirc even Haskell's compiler doesn't do a lot in that area (due to some technical debt and complexity of implementation...? Memory is fizzy about this)
I don't understand. Which lambda?
Is there a toy compiler based on this idea we can play with/contribute?
I've seen Peter Kogge's *The Architecture of Symbolic Computers* [mentioned](https://carlo-hamalainen.net/2012/08/15/kogges-the-architecture-of-symbolic-computers-1991/) several times. It's from 1991 though and I imagine computer architecture concerns have evolved quite a bit since then.
OK, sure, but *somewhere* along the pipeline an optimisation opportunity has been missed, hasn't it.
&gt; Problem one is ANSI control codes. This isn't a problem at all, if your library is properly abstracted (and I haven't looked at Vty at all) then all you'd need to do is call the right Win32 API instead of just emitting ANSI control sequences. That is if you want compatibility with older OSes. If you're only aiming for Windows 10 and higher it supports most if not all VT100 sequences fine https://docs.microsoft.com/en-us/windows/console/console-virtual-terminal-sequences If the library isn't then you can just re-interpret the control sequences back to Win32 api calls, It's not that hard at all, to the point where lots of compilers just do this. See projects like ANSICON that have existed for years https://github.com/adoxa/ansicon. &gt; Namely, instead of sending an escape sequence, ALT+{Key} escapes the console host entirely and talks to the OS instead, and you get similar behaviors with some CNTRL+{Key} combos. I am not sure if there is any way at all to bypass this behavior. If you want low level controls, you need to look at the low level console apis instead of the high level ones https://docs.microsoft.com/en-us/windows/console/low-level-console-i-o Particularly, the only signals I am aware of that the console host actually routes to the OS by default are `Ctrl+C` and `Break`. And if you don't this you can just turn off console input processing https://docs.microsoft.com/en-us/windows/console/low-level-console-modes (this is how you get mouse events in the input buffer as well). Lastly you can even get access to the full underlying screen buffer and manipulate the entire buffer as a char* which allows you to do things not supported by the APIs if you want. (I've used this before to do things like screen scraping of console programs which use TUI like functionality so std redirection didn't work.). In short, there is no technical reason why Vty shouldn't be able to achieve full feature parity with unix like OSes. The API is more than capable, it's just different, which is why historically you didn't have things like an ncurses port. It would essentially be a rewrite because it's not abstracted correctly. &gt; Basically, MS threw CMD.EXE in the dustbin about 20 years ago and only recently started to try pulling it out again - CMD is just a layer on top of conhost which is the actual console host. The limitations of CMD in no way reflect the limitations of window's console host. ConEmu etc all use conhost because it's the only user facing console interface on Windows. https://blogs.technet.microsoft.com/askperf/2009/10/05/windows-7-windows-server-2008-r2-console-host/ explains the architecture behind it. and if you're interested in learning more https://www.amazon.co.uk/Inside-Windows-Debugging-Practical-Strategies/dp/0735662789
I would recommend writing an instance for `Exists Expr` where data Exists f where ExI :: f a -&gt; Exists f
Maybe you can provide another FFI call that clean resources in the C side. this should be called by the haskell exception handler. Simon experience reports at Facebook are invaluable.
May need to tag the value so that he can recover `a`. For example using `singletons`. OP maybe be interested in *dependent pair*.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [snoyberg/conduit/.../**resourcet#readme** (master → 1c21ff9)](https://github.com/snoyberg/conduit/tree/1c21ff911d24317ebf87e52c49ac361ff810716a/resourcet#readme) ---- 
GADTs let you specify type constraints for each constructor as well, e.g. in `Cond` you'd be allowed to say `Binary a =&gt; Expr a -&gt; ... `. Does this answer your question?
This is somewhat puzzling since your type is not inhabited by any finite values. I think you may need to give a more realistic example! 
(Not the OP) I think this will not help much with deserialization.
That's interesting - can you explain further why this is a good architecture for FP?
The proposal here has an optimization which allows more incremental fetching of packages. You associate a package version with a list of file SHAs, and then only fetch the ones you don't already have. Not strictly necessary, but should be more efficient, particularly if the response or parts of the response get gzipped. Often a new version of a package will only change a handful of files out of many
This. It's exactly what OP is asking for - ignore the return type, and create a serialization that depends only on the structure of the expression.
Is it really more idiomatic? I usually use a let inside a list comprehension to bind values I've generated there (for example binding an expression involving x). When the value is available outside, I would typically use a where clause: sum [ (m - realToFrac x) ^ 2 | x &lt;- xs ] where m = mean xs
Fair enough. I would probably still vote for treating packages as single-file archives just for simplicity.
Ah I see. If you want that to work without specifying the type, then it requires some kind of dependent typing, yes. I don't agree that it "feels wrong" to specify the types for this application though. If we had some simple way of doing dependent types, it would be cute here, but it's really not essential. Our current singletons-based DT is a huge hammer for this tiny nail.
I think they might've hidden the literals under et cetera.
Even if it is a single tarball file, I understand that IPFS partition the file in chunks so that if a file is changed, only the chunks modified are added to the storage. The tarball hash changes too of course. Anyway it would be nice to publish packages as folders as well as tarballs
`ipfs pin --help` mentions recursive pinning
&gt;one of the guests made a comment to the effect that "design patterns are what you need when your language isn't expressive enough to have those things in the standard library Well, I think a clarification needs to be made. There are different kinds of patterns, there are language deficiency patterns (e.g. everything in the GoF book), there are project organization patterns and there are architectural patterns (e.g. 3-teir, REST, message bus, etc.). I think Haskell tries not to have that 3rd category but of course it has the first 2. It should be kept in mind that not even all OO languages need all the first kind of patterns. Recall in the GoF book that half of the patterns were for C++ and the Smalltalk section said something like "not needed in Smalltalk, just use X".
Ah, Another nice thing that may be very useful: if the package is exported within a folder it is possible to have very meanuingful urls which includes the hash and the package identifier: folder "any" content: hash/package-name-1.2.3 &gt; ipfs add any The the url of the package would be ....ipfs/hash/package-name-1.2.3
Not to mention that both C and Haskell are very unsuited for the GPU. I'm not sure what kind of language would be best for the GPU, maybe something like K?
I think John Wiegley uses something like this, he speaks about it in an episode of haskell cast http://www.haskellcast.com/episode/013-john-wiegley-on-categories-and-compilers. You can find further links there
I've [answered your question here](https://stackoverflow.com/questions/48492025/why-does-forever-mean-this-code-doesnt-read-from-a-socket-or-print-to-stdout/48564350#48564350). I think your problem was a misunderstanding about how `getActivatedSockets` works.
There was a well-typed [post](https://www.well-typed.com/blog/2017/06/rtti/) for exactly this problem! You don't need a type family (good guess, though); you need existentials (as /u/kamatsu [recommends](https://www.reddit.com/r/haskell/comments/7uhhz0/help_with_gadts_and_type_families/dtkby4e/)) and singletons (the concept, not the library) (as /u/jd823592 [mentions](https://www.reddit.com/r/haskell/comments/7uhhz0/help_with_gadts_and_type_families/dtkcb6d/)).
There's [Futhark](https://futhark-lang.org), which is an FP language developed for GPU programming.
You would need more hardware because the number of combinations of operations would be much higher. If you want a feel for the kind of ISA that's easy to decode, look at RISC-V. Every instruction is simple and does a very clear thing to all the parts of the architecture. The rewrite-based nature of lambda calculus makes the fundamental performance optimization of computer architecture (grabbing the "next thing" right before you need it) way harder. Wrt pipelining-as-static-analysis: I'm specifically talking about pipelining in the microarchitecture, which is a kind of optimization that functional programming doesn't help a huge amount with. The kind of static analysis I'm talking about is usually not computable in constant-size, bounded-depth, memoryless circuits, and thus is mostly infeasible for a processor to do in hardware. I'm thinking about stuff like free theorems, explicit macro-level dependencies and some semantics-from-types things you get in FP, all of which allow a compiler to do things like aggressive reordering and stream fusion. 
Oh that's perfect, that was pretty much like reading my train of thought before they got to the solution. Thanks
This post is incomplete without a sleeping sort implementation.
I had considered existentials (though not enough to actually make anything), but as far as I know, I would have to store the type information for every expression since I'd be inverting who has control over the type. I didn't really like that
I guess it's time for me to learn what singletons actually are. That's been another one of those things I've intentionally avoided
It's the only solution, unfortunately. [I gave a talk](https://www.youtube.com/watch?v=PNkoUv74JQU) on how to derive these concepts from first principles, if you're interested. The tl;dw is to use [exinst](https://hackage.haskell.org/package/exinst).
relatedly, `nix` makes a great case for laziness (with afaik unique use case, package management). 
&gt; In seriousness, it's not a case of expectations so much as it is unfulfilled desire. It really is a case of *me wanting to know what Windows users expect*, because I don't know what's possible on Windows and it really would be helpful to get that information. &gt; I understand and sympathize that the task of attempting to wrap an API around the 'shared' functionality between a linux terminal and a windows console is obscenely difficult, but this does nothing to mitigate my desire for a magical solution to the problem. Yeah, it helps to have that acknowledged; I am increasingly concerned that comparing Windows terminals to Unix ones is comparing apples to oranges, and at least in Vty's case, the design is built around more than just whether ANSI escapes are emitted, so even if there is a Win32 API, I strongly suspect a separate library would be a better approach than modifying Vty. If that were to succeed, then perhaps *then* it would be clear whether there was sufficient overlap to abstract. At present I'm not convinced yet.
Thanks for this info - I'll pass it on to the folks who expressed interest to me in investigating this.
Yes, sorry. I just didn't want to dump a giant block of code in my post
&gt; You would need more hardware because the number of combinations of operations would be much higher. If you want a feel for the kind of ISA that's easy to decode, look at RISC-V. I'm familiar with ISAs from my electrical engineering background, but I still don't understand specifically what insurmountable difficulties you foresee for some dramatic execution or resource use improvements. Clearly functional languages can run fairly efficiently on stock hardware, but perhaps you're simply forgetting how many sacrifices are made to achieve this efficiency and maintain compatibility with other languages. The original post didn't posit any compatibility requirements, so there's a lot of flexibility here to push functional languages up to and beyond C performance in various ways. For instance, prefetching instructions can dramatically accelerate some GC algorithms on stock hardware, as can parallelism during the marking phase. These both seem very amenable to custom hardware, ie. hardware could perform all ref counting and/or tracing for you for on a hardware-provided universal representation. This a) eliminates all boxing and unboxing for heavily polymorphic code, possibly without the typical code explosion for unboxed types, and b) probably eliminates a good 30% GC marking overhead across the board, not to mention the potential latency improvements if GC takes almost zero time. As another possibility, the hardware has to deal with all sorts of memory protection, but MMU overhead can increase a program's runtime by over 50% last I checked. A hardware realization of the pure STG-machine wouldn't need any of that hardware because memory protection wouldn't be a thing. Protection is enforced by the language not the hardware, ala Singularity. Virtual dispatch cost in languages with closures and heavy use of objects, like C++, also adds upwards of 50% overhead, so reifying closures could also provide some improvements in execution performance via speculative execution. Reified closures are an example of a simplifying assumption, in that, [k-CFA in the lambda calculus is EXPTIME, but only PTIME in OO languages](https://arxiv.org/pdf/1311.4231.pdf). These are all off-the-cuff, low-level approaches granted, so they're not on the level of free theorems and fusion, but I don't think it would be too controversial to say that custom hardware could improve runtime and latency characteristics of functional languages by an order of magnitude. That's still significant. &gt; The kind of static analysis I'm talking about is usually not computable in constant-size, bounded-depth, memoryless circuits Our computers don't have unbounded memory either, so all currently implemented algorithms are effectively bounded by our available memory. We'd have to actually quantify the cost of any given analysis to conclude whether or not it's feasible to implement fully, partially via some simplifying assumptions (like k-CFA above), or not at all. Any optimization that currently requires a few gigabytes of memory for even simple programs will probably be difficult to achieve here, but given some of the above hardware level changes or some simplifying assumptions, some might actually become partially tractable.
That's totally understandable (and a good thing!). I think tomejaguar wasn't reading carefully and missed the "et cetera" by accident.
&gt; [...] but this does nothing to mitigate my desire for a magical solution to the problem. So, for you, is the problem "we don't have a unix shell on windows" or is the problem "`cmd.exe` sucks balls" or is the problem "none of my tools take advantage of the potential that `cmd` actually offers"? I think this question is actually the most important one because it sort of determines how we go about approaching this. * If the problem is "I want to do unix things on windows" then a unix shell on windows, with properly ported system calls, is sufficient and `cmd.exe` doesn't need to be considered much. It would be enough for vty to declare "official-ish" support for some 3rd party shells, maybe the linux subsystem, and so no. * If the problem is "`cmd.exe` sucks", then that's sort of an insurmountable issue that could be resolved by looking at whether or not powershell can do what you want, or if `cmd.exe` will receive enough enhancements in the future to be worth using, or so on. In that case, it might be sufficient to support powershell (assuming it's possible to support) or to go with the solution in the first bullet point. * If the problem is that `cmd.exe` is fully capable but that nothing supports it... Well, if it's fully capable, I would expect it and a unix shell to actually be able to support a mid/high-ish level API identically well. (Draw stuff on the screen, respond to keypresses, display color, be able to manipulate text in certain ways, etc.). This, of course, assumes there's no fundamental limitation on `cmd.exe` which, from what I've seen, doesn't quite appear to be the case :)
I was more worried about the need to download all of the cabal files for a snapshot than downloading all the files in a package. Downloading a tarball is something we're already doing for the latter. For the former: our current approach is to download the 01-index.tar.gz file from Hackage, but: 1. This downloads a lot of cabal files unnecessary for snapshot-based workflows, and some users are suffering from slow downloads 2. There's no obvious way to add support for tweaks to cabal files we want to host (in accordance with the plans mention to allow more direct curation of packages in Stackage)
Please check /u/Phyx 's response to my comments above. He has some much better knowledge than I have, and challenged our core assumptions here. Apparently the lower level conhost bindings allow for much more feature complete control, and that would be an experience that was agnostic between powershell / cmd.exe. I am not convinced `vty` would be a sane place to start, but you could definitely write some kind of library that replicated most of `vty` API.
&gt; Another interesting way in which this plays out is list fusion. … I would not call this list fusion: List fusion is when intermediate lists are completely removed, and would work equally well in a pure, strict setting. What the authors is describing here is plain lazy evaluation, where the intermediate lists are still present, albeit only one `(::)` at a time.
Vty makes some assumptions about the use of a Unix terminal (in particular, in its output diffing functionality) that will cause an attempt at direct Windows integration to be potentially very painful. Even if Vty is a good place to do this work, that may need to be abstracted, so any attempt at this should probably *start* as an independent work and then we can proceed to integrate with Vty as appropriate later on.
if the user has a local IPFS daemon and stack connect it via `http://localhost/5000/ipfs/....` the damon will download from all the repositories available everything not included in his own cache in parallel. After the first snapshoot is read, it will never should need to download complete snapshoots. if there is no local IPFS daemon, stack can rely on a ipfs gateway like http://ipfs.io but it will fall back to HTTP: there would not be parallel download neither local caching. If it is necessary to tweak a cabal file, then IPNS can be used. IPNS is a single identifier generated and managed with the ipfs command which is a mutable pointer to a IPNS object that can be changed. to access a IPNS object via gateway: https://ipfs.io/ipns/identifier IPFS and IPNS usage cases and access methods are the similar, except mutability.
I would make the argument that being able to fuse a (somewhat) arbitrary chain of operations against a list that 'doesn't exist yet' is a sufficiently unique pro to be worth bragging about, but I do agree that the way this is worded seems to imply that fusion is unique to laziness, which is certainly not true.
Throw away and forget the names of all those tools. Use emacs + [intero](https://github.com/commercialhaskell/intero). I expect that you are on linux. If not, switch to linux. 
`ghc-mod` doesn't work with the latest ghc yet, use an older stackage snapshot. That's what all the errors have been saying, they're all version out-of-bounds messages.
what's probably happening is that your stackage snapshot (lts-10.4) works with ghc 8.2.2 (you have for example base-4.10.1.0 and Cabal-2.0.1.0) and your same is true for you cabal and ghc packages, but ghc-mod does not yet support ghc 8.2.2. One option is to change your global stackage solver to lts-9.21 which works with ghc 8.2.2, but a better option in my opinion is not using ghc-mod at all and use, for example, [dante](https://github.com/jyp/dante) instead. I've made a simple emacs configuration for haskell programming based on dante and friends. it might help you create a configuration to your liking. you can find it [here](https://github.com/soupi/minimal-haskell-emacs).
I'm actually using dante. Thanks for sharing the config! Here's what I have. ;; Haskell with Dante (use-package haskell-mode :ensure t :defer t :init (add-hook 'haskell-mode-hook 'interactive-haskell-mode) (add-hook 'haskell-mode-hook 'company-mode) (add-hook 'haskell-mode-hook 'flycheck-mode) (require 'flycheck-color-mode-line) (use-package flycheck-pos-tip :ensure t :init (with-eval-after-load 'flycheck (flycheck-pos-tip-mode)))) ; Dante (use-package dante :ensure t :after haskell-mode :commands 'dante-mode :init (global-eldoc-mode -1) (add-hook 'haskell-mode-hook 'dante-mode) (add-hook 'haskell-mode-hook 'haskell-tab-indent-mode) (add-hook 'dante-mode-hook 'company-mode) (add-to-list 'company-backends 'company-dabbrev-code) (add-to-list 'company-backends 'company-capf) (add-to-list 'company-backends 'company-dict) (add-to-list 'company-backends 'company-cabal) ; hlint with Dante checker (add-hook 'dante-mode-hook '(lambda () (flycheck-add-next-checker 'haskell-dante '(warning . haskell-hlint))))) 
The talk Jacob Stanley gave on the reasons for rethinking QuickCheck is really good: https://www.youtube.com/watch?v=AIv_9T0xKEo (having Phillip Wadler was in the audience made it even more fun).
Because sometimes you do want that - not having it is arbitrarily restrictive for the many cases where a little IO is safe (or the unsafely is tolerable in your app)
A new way that I can get blamed for build failures, but now one that I can't do anything at all about? Please, no.
When summing the numbers from 0 to n, why would you generate a list and sum them up? Why would you not just use the formula of n(n+1)/2? It's simple, it's fast, it's takes the same amount of time for n=1 as n=10000000, it takes no storage space, etc. Yes, there are problems were you can't just do this, in which case, why not use one of those for your example? I'm not against laziness as a whole, but in general, it's often slower than evaluating everything anyways.
It is nice having declarative memory management without building every single package in &lt;nixpkgs&gt;. :P Sup, fellow Nix user! :D
If you're like me and have no idea what this library does, or what " OTP style supervision trees" are, this [library's book](https://www.gitbook.com/book/romanandreg/capataz/details) says: &gt; This library is intended to be a drop-in replacement to forkIO invocations throughout your codebase, the difference being, you'll need to do a bit more of setup specifying supervision rules, and also pass along a reference of a supervisor for every thread you fork.
Am I correct to say that list fusion may totally remove allocations when lazy consumption may be O(1) in memory (bounded by GC first generation size) but it will still allocate ?
That's correct as I understand it.
&gt; This library is intended to be a drop-in replacement to forkIO invocations throughout your codebase I started to wonder why we need this library then if we have `async` and `slave-thread` packages which solve `forkIO` problem quite good: * https://hackage.haskell.org/package/async * https://hackage.haskell.org/package/slave-thread
Thanks for the feedback, this comments tell me I need to be more _to the point_ in the documentation, I'll try to organize the information in a TOC form so readers can figure out quickly where to look for the differences with other solutions, the _why_, _what_ and _how_ 
I used `async` and had my feet shot quite a few times. Moved to `distributed-process` since then. Now considering `capataz` to rule them all.
&gt; Now considering capataz to rule them all. This is great to hear! Keep in mind though, Capataz does not (and probably never will) offer and out-of-the-box solution to remote call procedures and distributed nodes, it only serves to manage local thread errors and restart strategies for long living threads using supervision trees.
&gt; remote call procedures and distributed nodes And this is precisely what I *don't* do or want. Lucky me!
What's the current state of affairs with zero-downtime deployment of new versions of long-running services? E.g. web servers with worker threads.
🙌 🙌 🙌
I went through [The Monad Challenges](https://mightybyte.github.io/monad-challenges/) and liked it a lot.
Achieving the Haskell mindset. As a Haskell neophyte I have read the almost mandatory series of beginner books - LYAHFGG, YAHT, and Real World Haskell. I realize that one reading will not suffice and have begun again. In the interest of learning by doing I attempt the in chapter exercises. I also have attempted to solve problems from http::/wiki.haskell.org/99_questions. My results from the wiki and exercises in books are disappointing. I can solve roughly a third of the problems without having to eventually get hints or merely give up and study the solution. The solutions I do provide correctly may work but to say they are naive solutions is being nice. The correct solutions always seem obvious once studied which makes one feel rather inadequate. The provided solutions are so beautiful and idiomatic. My solutions when correct seem rather clunky and inefficient. I'm almost always thinking when seeing the provided solution - why didn't I think of that. Granted at times It's clear I attempted a problem beyond my skill set but I move past that easily. Do I just keep at it? Is there some FP for dummies I should read? I'm frustrated. 
Author here. Could you expand on what you mean by &gt; where the intermediate lists are still present, albeit only one (::) at a time. My understanding is, given the following: fmap f (fmap g [x,y]) Forcing the head should evaluate to: f (g x) :: (fmap f (fmap g [y])) [y] looks a bit like an intermediate list, but it's not because it's just the tail. No allocations have occurred. So the upshot is that I avoid the intermediate list: fmap f ([g x, g y]) Is it incorrect to call this fusion? I'm basing this off the following passage from FP with Scala (p 65) &gt; This view makes it clear how the calls to map and filter each perform their own traversal of the input and allocate lists for the output. Wouldn’t it be nice if we could somehow fuse sequences of transformations like this into a single pass and avoid creating temporary data structures? We could rewrite the code into a while loop by hand, but ideally we’d like to have this done automatically while retaining the same highlevel compositional style. We want to compose our programs using higher-order functions like map and filter instead of writing monolithic loops. &gt; It turns out that we can accomplish this kind of automatic loop fusion through the use of non-strictness (or, less formally, laziness). 
So far, I'm struggling to see how this is any better than Haskell Programming from First Principles. Aside from a guide on editor integration, it seems like it's just strictly less useful. Also, it lists Nix/NixOS as a way to install Haskell, then immediately discounts it as insane -_- This reeks of someone who didn't even try to learn anything about Nix.
The best way to preserve invariants of a datatype is to not expose any function that can break them. Map itself is actually a good example of this: it's defined as (roughly) `data Map k a = Leaf | Bin (Map k a) k a (Map k a)`. In order to work properly, the map keys must be in ascending order; thus, `Data.Map` does not expose the Map type's constructors (since those would allow you to break the invariant), and only exposes functions which either preserve sortedness or (in the case of e.g. `fromAscList`) explicitly warn you that you'll get a broken map if you don't respect the precondition. You can then build up more complex functions from your simple primitives. Since the larger functions only use your safe primitives to interact with your data structure, they can't break any invariants.
Thank you! I will update the article when I have a minute. 
1. Don't think of it as getting hints. Working through problems with people here or on IRC in a mostly real-time interactive way is a healthy learning process that you shouldn't seek to avoid. 2. Set out to complete a task, such as a simple program that you can enjoy building on for more than one exercise. 3. Most beginner solutions are clunky - that's expected. 4. Keep at it.
ditch silly exercises, go straight to building software that does interesting stuff. [This might help](https://gilmi.me/post/2015/02/25/after-lyah).
[The second paragraph of the book](https://www.vacationlabs.com/haskell/introduction.html#yet-another-book-really) address your first concern directly: &gt; More recently, new members in our team at Vacation Labs tried learning Haskell with the help of The Haskell Book and CIS 194 course. The feedback on the former was that it was too verbose (it’s 1,200+ pages!) and the feedback on the latter was that it was too fast-paced. Also, in my own words, I thoroughly enjoy Haskell Programming from First Principles, but it is also nice to have a free, online resource to point people to. Can you point me to the Nix comment you saw? I don't see "Nix" or "insane" in the linked blog post. The book mentions Nix as a way to [install Haskell](https://www.vacationlabs.com/haskell/environment-setup.html#installing-haskell) but doesn't comment on it at all. Perhaps you're talking about this comment: "only Stack is recommended for maintaining your sanity"? That's exactly what I would recommend too. I think Nix is wonderful, but throwing a new person at both Haskell *and* Nix at the same time seems like a recipe for disaster. 
I think that simple file/folder manipulations (using `directory` for example) should force you to interleave IO and pure functions enough to give you the hang of it.
Unfortunate that your first words of the main book are "don't use Haskell on Windows". True, but unfortunate.
Would they show if the modules (whose names should be known ahead of time), were listed under `autogen-modules`? 
Working on it. Slow going... GHC is hard, and wasm is weird :P
First off: good on you for doing this. I'm genuinely happy that more people are writing Haskell texts. Some notes: "Don't use Haskell on Windows" is probably a very good point, as is "use Stack and don't bother with anything else". Nix is excellent, but having people learn that at the same time is indeed likely a bad idea. I think you're also going to cover real-world library choices ("what works and what doesn't", essentially), which sounds exciting. (Especially if you talk about integrating different libraries, incompatibilities, and so on. Most of that knowledge is either nonexistent in written form or spread across hard-to-find blog posts.) Count me in as someone who disapproves of the connotation *in the title* (!) that the "theory" is superfluous, although, if this is what it takes to suddenly get tons of people into Haskell, so be it. The community-ish aspect sounds good; it does give me the feeling that a collaborative Coursera-like thing could be pulled of here. Have you thought about an associated forum of some sort? "Mathematical thinking" is very common, but "tangents about category theory"? At least from RWH and LYAH, which you named: neither even defined a category IIRC, and this is just tying into the not-even-wrong retorts about "type theory PhDs", imho. But I think you did mean mathematical thinking, and wanted to de-emphasise it in favour of more mainstream software-engineering concerns. Equational reasoning is a large part of what makes Haskell useful for the things it's good at, and I fear this is just going to strengthen misconceptions people have before they come to the language. As I said before, perhaps this will work where others haven't. This isn't a knock on you, more of a lament of sorts, and I hope you're successful.
Besides me there's only like 2 other windows Haskell users, so I can't imagine you got too many down-votes :3
&gt; Haskell beginners commonly make the mistake of trying to learn as much of the language as possible before writing their first program and overengineering the first draft. This will quickly burn you out. [...] You will accelerate your learning process if you get dirty and make mistakes. Write really ugly and embarrassing code and then iteratively refine your implementation. There is no royal road to learning Haskell. http://www.haskellforall.com/2017/10/advice-for-haskell-beginners.html
What is a *combinator library*?
What are some well known mobile apps (Android/ iOS) running mostly on Haskell?
There are dozens of us. Dozens!
There are people who work hard to support haskell on windows. I'm not going to name the names but we all know who they are. They get very offended when someone mentions how pitiful is the haskell on windows experience, especially when linux as an alternative is suggested. And then they lose their shit. 
&gt; …lack of discourse on the design of Haskell programs… Indeed! I'm writing my first larger Haskell program, and it's getting so entangled. I can't find discussions or advice on how to structure a Haskell application, however, so I applaud you writing on this! If any one does know of good writings on this subject, I would gladly hear it.
"[Magic Cookies](http://keera.co.uk/blog/)" is as famous as you're gonna get. Gotta start somewhere!
If there's no specific reason why newtypes wouldn't work, just go with newtypes. No reason to make it more complicated.
I would say don't try and compare against other things that might be similar - just get straight on with what you do. :)
The article has been updated. Hopefully it addresses the concerns. Thanks again for taking the time to point out the error!
Well, six more people like me and you'll be over a ton.
Do you actually have experience specific to Windows Haskell development that makes you say this?
Don't tell new people not to use Haskell on Windows. Just be honest and tell people that you don't want to spend time documenting the steps to set up the environment. Windows users are used to this anyway.
If by that you're asking, "do you program Haskell on a windows machine and have you had problems before?" Hell yeah.
✓
&gt; as allocations in Haskell are literally as cheap as incrementing a pointer. I have seen that sentence a lot, and I think it is a bit misleading. From what I understood, the allocation can be as cheap as incrementing a pointer (and checking the bound of the allocation space to see if the RTS must trigger a GC, so perhaps an increment and a test at minimum). However the allocated value must be copied in memory, so there is a memory write overhead, and the GC will still have an overhead during its trace to dispose of the allocated cells. That overhead can be huge compared to the simple cost of a register operation. (I'm no GC expert, so perhaps I totally missed an important point, feel free to correct me). For example, I tested the following piece of code, with too implementations of `sum [0..10^n]`: {-# LANGUAGE BangPatterns #-} module Main where import System.Environment (getArgs) import Data.Foldable (foldl') -- Sum, this fonction is supposed to be as strict / fast / low in allocation as possible strictSum :: [Int] -&gt; Int strictSum l = go 0 l where go !acc [] = acc go !acc (x:xs) = go (acc + x) xs slowFoo :: Int -&gt; Int slowFoo n = strictSum [0 .. n] fastFoo :: Int -&gt; Int fastFoo n = foldl' (+) (0 :: Int) [0 .. n] main = do args &lt;- getArgs let e = read (args !! 0) n = 10 ^ e useFastFoo = read (args !! 1) print $ if useFastFoo then fastFoo n else slowFoo n If I'm not wrong, the `fastFoo` implementation use list fusion, so it is `O(1)` in memory and without any allocation, everything is supposed to be packed in a tight loop. On the other hand `slowFoo` uses a lazy generator `[0..n]` and a lazy consumer `strictSum`. This is still `O(1)` in memory, but with allocation: all allocated (:) will be deleted at the next GC collection. The differences cannot be considered "negligible": (Gcc 8.2.2, compiled with -O2, this is a crappy benchmark so nothing else matters): The "fused" version, `sum [0..10 ^ 10]`: λ skolem ~ → ./TestPerf 10 True +RTS -s -5340232216128654848 62,432 bytes allocated in the heap 3,408 bytes copied during GC 44,504 bytes maximum residency (1 sample(s)) 25,128 bytes maximum slop 2 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 0 colls, 0 par 0.000s 0.000s 0.0000s 0.0000s Gen 1 1 colls, 0 par 0.000s 0.000s 0.0000s 0.0000s INIT time 0.000s ( 0.000s elapsed) MUT time 2.787s ( 2.787s elapsed) GC time 0.000s ( 0.000s elapsed) EXIT time 0.000s ( 0.000s elapsed) Total time 2.787s ( 2.787s elapsed) %GC time 0.0% (0.0% elapsed) Alloc rate 22,404 bytes per MUT second Productivity 100.0% of total user, 100.0% of total elapsed So, 2 MB total memory in use, 2.7s, 0 GC collection and virtually nothing allocated / copied by the GC. On the other hand, the "slow" version for 10^9 items: λ skolem ~ → ./TestPerf 9 False +RTS -s ~ 500000000500000000 80,000,062,264 bytes allocated in the heap 2,465,240 bytes copied during GC 44,504 bytes maximum residency (2 sample(s)) 29,224 bytes maximum slop 2 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 76592 colls, 0 par 0.149s 0.141s 0.0000s 0.0000s Gen 1 2 colls, 0 par 0.000s 0.000s 0.0000s 0.0000s INIT time 0.000s ( 0.000s elapsed) MUT time 7.295s ( 7.302s elapsed) GC time 0.149s ( 0.141s elapsed) EXIT time 0.000s ( 0.000s elapsed) Total time 7.443s ( 7.443s elapsed) %GC time 2.0% (1.9% elapsed) Alloc rate 10,967,120,476 bytes per MUT second Productivity 98.0% of total user, 98.1% of total elapsed For ten times less items, it takes 3x more time, so the overhead is roughly 30x! The total memory in use is still really low, so I think this benchmark confirm that this implementation only allocate short lived object. However the number of bytes allocated changes from 60 Kb, to 80 Gb. However, I don't understand the 2 Gb copied during GC.
+1 on this. If code haven't been updated for last 6 years then probably there's no much sense in resurrecting this project while there exist maintained alternatives and this project doesn't reall have some outstanding ideas.
What I am really trying to establish here is whether or not this is a question of expectations. Folks who use windows as a dev environment on a regular or semi-regular basis are more or less used to the idea that they're on their own when it comes to a very large category of common issues, at least when compared to solving the same problem on an OS in the Unix family tree. Yes, developing on windows can be a pain, but there are some languages and ecosystems that are much worse than others, and Haskell generally isn't one of the bad ones - Honestly, it's been fairly painless in my experience, with only a few notable exceptions. Frankly I run into problems with Node.js and Python libraries WAY more often than I run into issues with Haskell dev, although I will readily admit that I'm much better about stearing clear of certain trouble spots when writing haskell than I am with JS or Python. In any case, continuing to advise prospective Haskellers away from windows dev without saying why isn't very constructive. If there are solvable problems with developing Haskell on windows, lets talk about them, and try and get them fixed, instead of posting cryptic 'here be dragons' comments.
What are the problems?
Haskell on Windows has more problems than the fact that the author doesn't know how to set up a development environment. In fact, setting up stack on Windows is fairly straightforward - the painful parts come later, when you run into issues such as: - ...packages being written with *nix in mind, and never tested on Windows, breaking in confusing ways because they make assumptions that don't hold on Windows - ...bugs in GHC's Windows linker (the most blatant ones have recently been fixed, but the 8.2 release that has them is fairly fresh still) - ...packages that rely on C libraries means you will have to install those libraries, and suddenly you're fighting an insanely complex mess between stack, cabal, GHC, MSYS, and the way Windows "manages" DLLs - ...tooling being built with *nix-style processes in mind, which leads to inconvenient behavior like processes hanging because a subprocess needs input but doesn't receive any because pipes are connected wrong So, unfortunately, if you want to get stuff done on Windows as an absolute fresh beginner in Haskell, "install Linux in a VM" is sound advice.
Haskell works just fine on windows with Stack (and ghcid gives you a robust mini-ide). Like if you only have a Windows machine, which I did for a year. 
Appreciate the effort, we need more material in this particular corner of the book space. So, kudos for sticking your head out here and doing the work. That said, I think this could use some strategic refinement. The book is very opinionated, which, I believe, serves the goal well, but there is no need to go over the top and use sensationalist language. For example, recommending stack is a choice that makes sense, because stack, unlike nix or cabal in its current state, provides the "just make it work, don't make me think" experience that keeps a beginner's brain free for learning the stuff that really matters. But there is no need for trash-talking, sensationalist language, or stern warnings - just present the recommended procedure in great detail, and merely hint at the existence of the alternatives. That is, not "do yourself a favor and use stack, because anything else will make you insane", but rather just "here's how you install stack, step 1, step 2, etc.; Haskell can also be installed in other ways, but we recommend against them." In other words, the text oozes a certain anger and frustration, and they are very understandable and relatable to a Haskell veteran, but in someone new to the community, they will only lead to confusion.