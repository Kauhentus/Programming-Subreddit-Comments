I suspect the author means "I am mentally crippled by only being familiar with C."
I'm so happy you didn't say XML.
What's amusing is that they say xmonad is crippled by not being in c, and then go and start this project using CVS as the revision control system. Whoever wrote this clearly does not like new things.
It wouldn't really. The guy (and other people) aren't complaining of having to have GHC installed to reconfigure XMonad; they're complaining about needing the entire Haskell toolchain to compile XMonad in the first place and it being written in a language they don't know.
&lt;Something about Coq&gt;
&gt;has Coq proofs of its correctness... What, really?
http://www.cs.nott.ac.uk/~wss/repos/StackSet/StackSet.v
Stop calling me Shirley!
Sounds like a great opportunity to test beliefs about Haskell helping us avoid bugs, get code out faster, and so forth. I wonder how concretely one could go about doing it, if for example, you could adapt xmonad's test suite to scrotwm.
OpenBSD developer - what do you expect.
is it generated or translated by hand?
What environment were you running under where you didn't have that stuff? Embedded work possibly?
The configuration is written in haskell, so it wouldn't help much. You would still need a compiler to change the configuation.
Brains.
The solutions are sinister.
While this situation is a challenge, we do have a solution, too. Currently, in Nix, a package database is built on the fly for all packages that you have installed for the correct GHC version in your user profile. In Nix, you can not only have different versions of GHC installed at the same time, but also the same version of a Haskell package compiled for different versions of GHC, and all sorts of other combinations ...
That's no longer entirely correct. Eelco Dolstra, the lead Nix developer, used to be at Utrecht, but is now at Delft. Also, while Nix is certainly influenced by Utrecht functional programming ideas, it is itself not implemented in a functional language.
I read that in the voice of the undead.
Wanted to post the exact same observation. [If this is him](http://nl-nl.facebook.com/marco.peereboom), it doesn't surprise me. I know code can smell, but can it also have old man smell? And if so, would that make his UI manager's default theme Aqua Velva?
It's "platform-dependent C", and it looks like no C any human would ever write. You can bootstrap GHC using it, if you run a ./configure on the target machine, but using it to port individual programs will probably not work :)
Sarcasm OS
SarcOSm
[No](http://braincrater.wordpress.com/2008/08/28/announcing-xmonad-light/).
I'm the author of xmonad-light. It's a little bitrotten, since I wrote it to prove it would would work and to make a point, and haven't been maintaining it actively. It would be easy to add new input formats for xmonad-light, as dons suggests in another comment. JSON, YAML, whatever. But that doesn't escape the fundamental problem of the language not being Turing-complete. There is a limited set of xmonad features that xmonad-light knows how to use, and a limited number of combinations. More features could be added, of course, but that's just delaying the problem in my mind. I always envisioned someone would start with xmonad-light and then inevitably want a feature that necessitated upgrading to a full xmonad.hs config. That's why xmonad-light can output an xmonad.hs equivalent to your xmonad-light configuration, to make that far less painful a transition. I never got word of anyone actually using xmonad-light anyway. I suspect it exists in a no-man's-land: the GHC dependency doesn't really hold people back. Either they think like the FA author and would never use xmonad anyway, or they don't sweat the dependency. Using jhc might be an option, when (if?) it can compile xmonad.
Contrary to what others are saying, I wasn't being sarcastic (there goes all my upvotes!). Perhaps you could say it was sarcastic if you thought I was implying C is portable. I've recently joined some folks at Portland State who are known for House and other such wonders - in other words they are using Haskell for systems programming (i.e. the kernel). Fighting with GHC to make it not assume things about the environment actually isn't very fun - can you believe it? Edit: From the standpoint of the language reference this isn't much of an issue though - the IO parts are fairly self contained and one could imagine simple flags that would cause the compiler to omit those parts of the language much like gcc has the -fstandalone flag.
I find it amusing that he complains that xmonad has not invented here syndrome. What is he proving by writing his own xmonad-alike then?
JÃ¶rg's [fxtbook](http://www.jjj.de/fxt/fxtbook.pdf) [PDF, page 57] has a lovely bit-twiddling Hilbert curve generator.
how very in-depth. wonderful, thanks!
The [benchmarks](http://repetae.net/computer/jhc/results.html) on this are quite interesting. It seems to be several times faster than GHC (although the comparison is a year old. Are there more up-to-date benchmarks?
Lemmih posted some today, including the latest jhc release: http://darcs.haskell.org/~lemmih/nobench/x86_64/results.html
Ah, nice. It seems it breaks on more tests than before, though.
looks like nobench could use a few updates in the "real" section. (mainly like it is failing on haskell98 namespace).
Wow, that's really is a sad sight!
Pretty neat. I like how it is built on a VCS backing store so that essentially already has a robust and strong api. Why did it never occur to me that a wiki is really nothing more than a web front end for a VCS?
Perhaps because you're mostly familiar with large wikis such as Wikipedia, where the wiki=repo formula could never work due to performance reasons? But it might also be because it turns out to be quite a bit of work to interoperate with the VCS binary to get all the functionality you need - it's only obvious if you already have a bunch of files in a format (like John had in Markdown) and are thinking 'how can I turn these into a nice website?'
How does the data get into Hackage? Where does the code live?
And if you don't use arch or debian?
I actually just recently switched from Arch to a Debian derivative. To answer your question though: First get GHC installed, then just use Hackage, after you jump through all the dependency hoops of getting 'cabal-install' installed. As for the Haskell Platform, I've seen nothing yet that gives me any confidence in its usefulness. Will it mandate that packages are updated in a way to avoid the diamond dependency problem? I'm not sure, because I've never found anything highlighting the specific benefits of using the Haskell Platform. How is it to be determined what goes into the Haskell Platform? Is it the packages with the largest number of dependent packages or the packages that get the most popular votes from the web? Every developer will need to install additional packages outside the Platform. What are the implications then? How can it not be less robust to have an extra layer of distribution packages on top of Hackage and then add to your system additional Hackage packages that you have to build yourself? I've always wondered why cabal-install didn't just come bundled with GHC? Do people really install GHC these days and not use 'cabal install'?
In my opinion, probably the biggest single benefit of the platform will be that it is "GHC + cabal-install". The other immediate and less user-visible benefit is that it decouples the release cycle of the "extralibs" that used to come with GHC from GHC itself. Other than these things, we will have to see how it will evolve. The goal with distribution packages (with or without the platform) is to provide a way of installing hackage packages that is properly integrated with the underlying distro, which is particularly beneficial for users of applications that don't care what language they are implemented in. If the distro packaging is done right, there's no reason that installing foo-x.y.z via the distro shouldn't produce the same result as installing it via cabal, so there shouldn't be any problems with then installing other packages on top by whatever mechanism. The platform won't really solve the diamond dependency problem in itself, although by providing a consistent set of versions for each package in each release it may help mitigate it by reducing the number of ways in which other packages depend on packages that are in the platform.
And what if you use Arch but not i686? 
Currently it is the wrong way round, but we'll fix it in the new server. Currently the "clients" (ie distros) publish the data in an agreed format and Hackage pulls it in via a cron job. In the new hackage-server implementation we'll have it the right way round where the clients just http PUT resources and the sever acts more like a dumb database.
Could somebody explain me why would anybody use the distro packages instead of cabal-install? The only possible advantage I can imagine that the distro packages could specify the non-haskell dependencies, but as far as I understand, they are auto-generated from the .cabal files...
&gt; As for the Haskell Platform, I've seen nothing yet that gives me any confidence in its usefulness. Will it mandate that packages are updated in a way to avoid the diamond dependency problem? Packages in a release of the platform are known to work together. This complements the "latest and greatest" from Hackage. &gt; I'm not sure, because I've never found anything highlighting the specific benefits of using the Haskell Platform. [Paper](http://www.cse.unsw.edu.au/~dons/papers/CPJS08.html) and [Slides](http://blog.well-typed.com/2008/11/haskell-platform-talk-at-the-london-haskell-users-group/) &gt; How is it to be determined what goes into the Haskell Platform? Is it the packages with the largest number of dependent packages or the packages that get the most popular votes from the web? We [posted our recommended procedure](http://www.reddit.com/r/haskell/comments/9cajv/draft_recommendation_for_the_procedure_to_add/) the other day. &gt; Every developer will need to install additional packages outside the Platform. What are the implications then? That's fine. It is expected. That is why we include cabal-install. There is still a benefit in getting the community to synchronise on the versions of a bunch of core packages. It means you know what to target and users know what to install. &gt; How can it not be less robust to have an extra layer of distribution packages on top of Hackage and then add to your system additional Hackage packages that you have to build yourself? &gt; I've always wondered why cabal-install didn't just come bundled with GHC? Do people really install GHC these days and not use 'cabal install'? It's part of the platform, that's the point. It does not scale to have the GHC hackers maintain and ship a whole development platform. That is the new job of the Haskell Platform. We want the GHC hackers to be able to focus on their compiler. 
&gt; why would anybody use the distro packages instead of cabal-install For one, distros provide binary packages, so installing (and removing) of stuff is so fast you can do it speculatively. Also, upgrades are completely managed for you.
Replying here so you see it - could you make it print the package name in Debian and not the version? At least in Debian, the installation workflow we prefer to promote is apt-get/aptitude/etc, not downloading .debs from packages.debian.org which linking in the current manner suggests. Thanks.
Are the binaries produced by different ghc versions compatible? Also, I don't think that automatic upgrades are a good thing (and it would totally ruin my setup anyway).
For Debian (at least Ubuntu) it would be nice to have apt:// links: browsing to apt://package-name under Ubuntu launches the package installer and offers to install the named package.
Another single big benefit (imho bigger than instant cabal-install) is that it makes the life of windows users infinitely easier. Installing autoconf-based packages (like the old OpenGL binding) on windows is a trip to hell... 
Well, this is just turning into a general distro-packages-versus-installing-development-stuff-yourself argument, which is a little cliched in 2009. Horses for courses.
So, why doesn't the community put together an apt repository with Hackage packages? We could work around the slow moving Ubuntu/Debian repositories that way.
Besides shipping binaries, it also resolves dependencies on C libraries.
&gt; seen nothing yet that gives me any confidence in its usefulness Please look at the extensive material we've written on why having a single, standardised set of packages that work together, across platforms, is so much better than the ad hoc mess we have now in the distros.
Thank you for the reference links, and also for all your thoughtful work on Haskell packaging. In enumerating the potential users of the Haskell Platform, I'm thinking mainly of hard-core developers and also of users who would just be using Haskell as a specification language. Both of these types of users will need various extra Hackage packages apart from the Platform. Now, granted it isn't feasable for the Platform to be concerned about these extra packages installed with Cabal, aside from making cabal-install available. However, isn't there a need for cabal-install to somehow be made Platform aware, so that an upgrade of the Platform doesn't leave all the extra packages high and dry without an easy upgrade fix? 
It's great to see this open sourced! How big a task would it be to hook this up to the [gitit wiki](http://www.reddit.com/r/haskell/comments/9e0dp/gitit_061_gitdarcsbased_wiki_running_in_happstack/) instead of pmwiki?
Hard to say. It really depends on how it uses PmWiki. It could be very easy (since PmWiki by default stores articles as files on disk, like Gitit usually does) or very hard.
Wow, this is like one of those good-news/bad-news jokes: the good news is that you can easily call some Haskell from Ruby! But the bad news is only that Haskell JHC can compile...
Heh. JHC is getting easier to install, stop being cynical :) The long term plan is to get it working with GHC as well. I haven't tested the dynamic lib support for a week or two, it might be better now (was horribly broken on Mac for a while for lack of love, i think)
Oh, it's not the install I'm complaining about. It's the fact that it can compile basically 0 of the libraries and apps I actually use that bothers me more.
But JHC deserves more loving.
This makes me very happy, these are my two favourite languages right now.
I only compile things myself that I really care about, and let the distro manage the rest and let things fall as they may. I like this philosophy. So, I doubt that I personally would ever use distro packages with regard to Haskell stuff, and it sounds like you aren't really a candidate for that either. But if you are merely an xmonad user, and don't care that much about Haskell, then the packages might make sense.
&gt; However, isn't there a need for cabal-install to somehow be made Platform aware, so that an upgrade of the Platform doesn't leave all the extra packages high and dry without an easy upgrade fix? I don't think it needs to be "platform aware" to make it easy to upgrade packages. See [this feature request ticket](http://hackage.haskell.org/trac/hackage/ticket/199). It's got an associated patch though we've not yet merged it.
How does that work? I had the impression that the packages are auto-generated, and the .cabal files do not contain enough information to resolve the dependencies on the C libraries...
As long as there's a predictable pattern for page names, it shouldn't be hard at all. 
The native distro packaging tool has more smarts.
Use the x86_64 package. For some reason Arch doesn't provide a single project page that links to the different binary builds, as far as I can see. 
is this like monad based sandboxing?
IHG is paying for shared library support. Hammer on it, and write bug reports if it isn't working.
I think you're probably better just thinking of it as "type-based sandboxing" that happens to involve the monad type, and even more coincidentally happens to involve IO. Locking down what a passed-in closure can do by the type it can have is a generally-applicable technique, not just limited to wrapping the IO Monad; it's one of the primary uses of a good strong-typed language.
The IO monad is a major special case, though. It's a dumping grounds for all FFI-imported code. Securing it completely would be really difficult--not significantly easier than doing the same thing in another language. If what is currently called IO was broken up into a number of finer-grained monads, it would be much easier. Each kind of "IO" monad would provide a specific set of services through its type class interface, so creating a mocked or jailed version would be a simple matter of implementing a type class instance with the right semantics.
Program has been fixed.
Yes, there are far better ways of separating concerns regarding IO functionality. But this requires the entire IO system to be redesigned from scratch. My approach here is a bit more simple, wrap up a part of the original (dumping ground) and shield of the internals. I hope to see more restricted IOs in the future. For example, having a read-only IO would also be nice.
Even if you wrap System.IO functionality, you still have to worry about foreign code that can affect the file system directly and indirectly. It's so hard to provide a guaranteed jail under such circumstances that my recommendation is to use OS-level file system jailing if you really care about security. I guess it depends on your use case. If you just want to run an expression (a la lambdabot) then what you're doing might be fine. If you want to be able to jail entire modules, then you open up a big can of worms.
It looks to me like there's no ability to use any ffi code in the IO monad -- this lib provides a *different* monad, confusingly also named IO (I assume to keep things simple) and only the provided wrapped functionality can be run in it. So if there was ffi code that was supposedly pure and *not* in IO, but which under the covers did impure things, then that would be exposed.
Your money can go in, but can never come out unless you transfer it to another Credit Union. Perfect plan.
This would be more appropriately implemented in a shitty language. HDR is fail 95% of the time.
Follows Sturgeon's Law then. I see no problem.
Also, there are scientific applications for tonemapping, not just aesthetic applications (whether you question the aesthetic or not)
I 'spose.
Also, the Haskelly guy (Alex McLean) is talking at London HUG soon too... http://www.londonhug.net/
I see, the name overload was what confused me. Thanks for the clarification!
People that use HDR are fail 95% of the time. It's not HDR's fault.
Watch the end.. the *algorithmic dancing* bit. WTF?
One of the pictures has syntax features of Haskell, Ruby and JavaScript. \filter -&gt; {|son| var a,b,c,d,e,f,g,h,i,j; son }; What is this? 
can't watch that video:/ 
[SuperCollider](http://supercollider.sourceforge.net/) **Edit:** I'll explain the code. All this is an anonymous function that has something referenced to it `-&gt;` means "references" like in a dictionary. Everything between the two pipes is an argument. `son` is the sole function argument. The variables are just variables and the `son` at the end just returns the passed value. This code is *sort of* the python equivalent of `{"filter":lambda i: i}` There's probably more to his code. The variables inside that function just kind of look like they're there for no reason, or maybe it's there just because it used to be something else.
Almost all of my favorite programming languages in one video! Pd, SuperColllider, and Haskell. Yay!
Impromptu is a lisp visual and audio environment. It REALLY needs to be integrated with emacs though http://www.google.com/url?sa=t&amp;source=web&amp;ct=res&amp;cd=1&amp;url=http%3A%2F%2Fimpromptu.moso.com.au%2F&amp;ei=vheYSuzGKoOusgPWuuyFAg&amp;usg=AFQjCNEGANbWR_xbxHqEroHdX9rhGMxPiA&amp;sig2=JzI0gAhqILd9UFCpVs28sQ
They don't show much or say much about it, and neither Google nor Wikipedia have much insight about it, so it's _very_ difficult to answer your question.
My question was rhetorical :)
Also, the language is apparently based on Smalltalk.
Instead of outputting music, they were using live coding to output movement instructions to the dancer. "clap"... "turn around"... "sip a little beer"... etc
&gt;There's probably more to his code. He adds to it in the video.
Fluxus is another one http://www.pawfal.org/fluxus/
These are great slides.
What if I want to remove said packages? As far as I know, uninstallation is currently not possible with cabal-install. Better to just package the binaries.
OMG Haskell, drop EVERYTHING. I mean Alex McLean's work is neat and all, but 3 seconds of video of a few lines of Haskell isn't terribly exciting.
I think what you're talking about is when he he adds `* SinOsc(200)`: `\filter -&gt; {|son|` `var a,b,c,d,e,f,g,h,i,j;` ` ` `son * SinOsc(200);` ` ` `};` All that does is modulate the volume of whatever is being passed to son via a sine oscillator at 200 Hz. There is other code that isn't being shown that processes the input to his microphone and passes that processed audio to that function. 
Argh. http://en.wikibooks.org/wiki/Haskell/StephensArrowTutorial
You can either submit a link or text, not both. :(
He should probably mention that his construction only gives an extension _field_ when the polynomial is irreducible. When you're constructing the splitting field of a polynomial (the minimal field extension in which the polynomial splits into linear factors, i.e. the smallest extension which contains all the roots), you cannot mod out by the whole polynomial in one go; you need to break it into irreducible factors (a good opportunity to talk about the Berlekamp or Cantor-Zassenhaus algorithm) and mod out by them successively in whatever order. It might also be worth making a historical connection. Formally computing with complex numbers by considering them as 'artificial numbers' obeying all the usual rules in addition to the relation i^2 = -1 is how it was done historically by Cardano and later mathematicians, until Gauss put complex numbers on a firm geometrical footing. The quotient construction is a precise formalization of this process. That quotient rings are well-defined expresses the fact that you can reduce an initial expression incrementally by relations such as i^2 = -1 in any order and you will always end up with the same normal form. This is a kind of Church-Rosser theorem. You guys are up for a treat when he will presumably talk about prime subfields in the next issue. The first time I saw the dimension argument for classifying finite fields, it struck me like a sledgehammer. That one argument is one of the best justifications for teaching linear algebra over general fields.
Use newtype for Circuit instead of "data". The "pure" circuits can be less space-leaky: id = circuit where circuit = Circuit $ \a -&gt; (circuit, a) arr f = circuit where circuit = Circuit $ \a -&gt; (circuit, f a) 
Changed - thanks.
Yeah, i'm putting together a demo project on github so i have something to point to with a sad face.
&gt; Any Haskell function can behave as an arrow, because there is an Arrow instance for **the function application operator (-&gt;)**. `(-&gt;)` is the *type constructor* for functions, which just happens to be an infix operator. I'd at least recommend against using "function application operator" as people might think of `($)` instead.
(-&gt;) is also a *constructor*, and application is, in a way, the destruction operation on functions.
This is awesome. I've only been coding Haskell for a few months. I wish I was further along in "School of Expression" so I could participate.
This is a really interesting list, it would be great to integrate this into the Hackage main-page. It is somewhat interesting that half of the top 20 applications are basically applications to help you "do" Haskell more/better (cabal, haddock, hlint, etc). Are there some popular applications not on the list that people find useful for non-Haskell tasks?
Do you happen to know the rationale for this limitation? I often find it annoying.
In the past we only had links or self posts. The text was added later for the self posts and not for the links. That's really all there is to it. I wish the links could get text too, though.
The -lglut thing is because of a bug that (I think) I reported: One of the libs/headers (libXmu I think) is missing while the Haskell GLUT bindings are being built, causing the build to fail to put the "-lglut" option into the package description, rather than failing the configuration step. Try to edit your package.conf file and adding "-lglut" to the ld options of the GLUT package. Alternatively, install libXmu and everything the "configure" script of the GLUT bindings depends on and reinstalling glut.
There's a slight mistake in this post: formlets do not require Text.XHtml.Strict. The core of formlets is independent of the output format, Text.XHtml.Strict is just one output format with default functions.
Nice! Thanks for the example.
I know very little about networking, but why would you use UDP for a chat client?
My guess: to get a feel for how to do UDP before using it for something real.
My assumption is that he'd get more input if he fixed the bug that prevents people from submitting comments. I would like to email him about it, but I can't find his address. Initial ideas: * Profile to find out what is slow (there are lots of articles about doing this for haskell) * Make sure those numbers are being typed as Int not Integer, the difference is probably significant * Avoid using lists (especially the Data.List lists) * Make sure laziness is being properly controlled (profile and check, if not just some of the strictness tools) * Consider trying to rewrite as a fold (or series of folds/unfolds), things are much more predictable then and you might be able to use fusion to your advantage.
Ya totaly makes sense if it is just to show how to do UDP in haskell. Just wondering if there was a real reason for that in a chat client, since as far as I can tell tcp/ip seems more approriate.
Well, in this case, it started as a chat client. I'm going to springboard this small platform into a description of doing STUN style NAT traversal. You need UDP for that.
Well, UDP is meant for fast communication, where each message is trivial and losable. So it fits perfectly.
Why not use [libev](http://hackage.haskell.org/package/hlibev)? It abstracts away epoll, kqueue and various other such mechanisms?
I thought timsort was a good way to minimize the amount of comparisons (as comparisons in Python are pretty expensive). This is useful in Haskell too, when your comparisons are expensive. But if comparisons are cheap, perhaps other sorting algorithms are better?
I think seeing code as an "AST" is only seeing half the picture. The code is actually a graph, not a tree. This graph is serialized into a tree with cross-references to other sections of the tree by using namespace rules and names. This tree is the AST, which is then serialized into text using the language's syntax. I think we really ought to be using structure editors that edit the ASG, not the AST, and have names be "scoped comments" only, rather than "scoped comments" + "graph links".
 inLines f = unlines . f . lines to: inLines = (unlines.) . (.lines) or: inLines = result unlines . argument lines where result and argument are defined as (.) and (flip (.)) respectively. or: inLines = lines ~&gt; unlines where (~&gt;) is defined as: infixr 2 ~&gt; f ~&gt; g = result g . argument f 
Why are people upmodding this? It contains almost zero content.
**Abstract** Functional logic programming and probabilistic programming have demonstrated the broad beneï¬ts of combining laziness (non-strict evaluation with sharing of the results) with non-determinism. Yet these beneï¬ts are seldom enjoyed in functional programming, because the existing features for non-strictness, sharing, and non- determinism in functional languages are tricky to combine. We present a practical way to write purely functional lazy non-deterministic programs that are efï¬cient and perspicuous. We achieve this goal by embedding the programs into existing languages (such as Haskell, SML, and OCaml) with high-quality implementations, by making choices lazily and representing data with non-deterministic components, by working with custom monadic data types and search strategies, and by providing equational laws for the programmer to reason about their code. **Categories and Subject Descriptors:** D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.1.6 [Programming Techniques]: Logic Programming; F.3.3 [Logics and Meanings of Programs]: Studies of Program ConstructsâType structure **General Terms:** Design, Languages **Keywords:** Monads, side effects, continuations, call-time choice 
Lol... That's my blog. I implemented a little expect-like sublanguage in Haskell, and I'd love to post the code, but it's company IP unfortunately. I wish I could have put more detail in there, but I think the take away from the post is "these things are not scary" and "I used to think they were" + you don't have to write a monad from scratch to get this functionality, and finally, and most importantly. Haskell is definitely for the win. :-)
Sorry so late... I should actually login to reddit more often. I work for a company called Verari Systems, and I write Haskell and Erlang code for control systems for our FOREST product line. Basically we do some monitoring of temperatures and power, and provide an SNMP interface to the infrastructure of a rack as well as some basic server management via Erlang and Haskell. There's a lot more to it than that, but going into great detail about it would probably get me into some trouble, so I'll just leave it at that.
and Oleg!
This solution did not work for me.
Worked fine for me.
try running alias ghc="ghc -optl -m32 -optc -m32" Are you sure you're not using some other install of ghc? does anything else show up when you `which -a ghc` ?
That didn't work either. I completely reinstalled, followed the instructions on the site, tried your method. Nothing worked. Errors remain unchanged.
Ah, thanks. Sometimes I can't see the forest for the trees. I'm still not sure why this didn't work in /usr/bin/ghc
Did you end up going with bytestring-trie, or did one of the other approaches win out?
Believe it or not, I totally ended up rewriting it using a simple association list, because the number of items was usually about 4 or 5 items long at most. So a linear search was not prohibitively expensive for what I'm doing.
Watch out for setting aliases, sudo might not see the alias declaration. Editing the ghc script itself is more robust.
I hate to bring up the color of the bike shed... but anyone who thinks that logo should change has *wrong* opinions and should be avoided. Reinterpreting the green reflection to something like Bruce Banner transforming into the Incredible Hulk might also be useful.
Shows an empty page on opera for some reason.
Looks great! Anyone know of an inexpensive haskell-friendly shared host? Although I guess with the price of VPS these days you don't even need to go shared...
Very interesting paper -- I wish I could do such fun things in my day job. :-) However, it looks like much was sacrificed in order to keep the proofs reasonable and the translation to C fairly direct. They avoided in the Haskell code: * laziness * mutual recursion and other complex recursion schemes * most higher order functions (special ones like mapM_ were translated into C loops) * function composition that relies on laziness for efficiency * type classes (except three specific Monad instances hard-coded into their proof system) * ghc extensions (duh) And in C they allowed no address-of operator (&amp;) and no function pointers (hence minmal higher-order functions). Programming in Haskell without the above seems scarcely like programming in Haskell. :-/ Still sounds more fun than C, though.
\*giggle*
6.10.4 you mean? 
Congratulations on getting such an excellent project up and kicking!
That's what's easy to verify. Complex language features make for complex proofs.
10.10.4 incorporates the super awesome "translate from C" function. Also, it can now target a toaster.
10.10.4 includes the new TimeSpaceT monad stack. Also it has effect types.
The TimeSpaceT monad provides the Lorentz transformation. Also allows time travel when combined with continuations.
yes, of course. sorry about that.
so *that's* what you were giggling about. :)
but when will we see a macports build on snow leopard! bah! I mean... congrats :)
When I try signing up, it says: &gt; * You do not seem human enough. If you're sure you are human, try turning off form auto-completion in your browser. So I tried typing everything very slowly and it makes no difference. I look forward to being able to use this site some day.
You probably shouldn't use *this* site as your wiki. It is a gitit demo. Try installing gitit instead :-)
How do you stop unsafePerformIO?
Not a shared host, but I've been happy with [Slicehost](http://slicehost.com).
If I could, I would prefer to read the draft and see what he has to say himself, but a quick google didn't give me a pdf.
welcome to the future
As I see, that page has link to the pdf.
They need to turn off gzip compression. There's some bug in Happstack that makes it break with Opera and some other browsers. (Opera Turbo uses a proxy, which is why that works.)
Oh.
Indeed it does. Thanks for pointing it out.
main = putStrLn "How toasty do you want your toast?" &gt;&gt; fmap (\x -&gt; read x :: Int) getLine &gt;&gt;= toastify
Heh. And here I thought you were aiming for larger sets. Shows I've been spending too much time in the land of industrial SAT solvers and other gigascale domains, I suppose.
Read the paper: ftp://ftp.cs.york.ac.uk/pub/malcolm/icfp99.html
Does it work in Snow Leopard as well?
Recordings? Slides?
From just the summary, I am curious about a couple of things (that I imagine were made clear in the talk itself): 1) If he was going to default to Java anyway (being an experienced Java coder etc.), why was Scala being on the JVM instead of native such a big deal? 2) I'm a bit curious about the "non-coders can read Haskell" part. Has this been anyone else's experience? I'm wondering if, for example, XMonad users ever get weirded out when they see things like $ in their config file. I got weirded out by it when I was first learning Haskell, and I *am* a coder :) 3) He points out some problems with laziness but then concludes that Haskell is the ultimate language. Does this mean he didn't actually consider those issues a big deal, or perhaps wrote them off to his inexperience with a lazy language?
Video!
What's the status of the Haskell Platform on FreeBSD?
As far as I know (as a user) the current status appears to be "What's the Haskell Platform?". FreeBSD hasn't had a GHC 6.10 up until today, because of a nasty memory bug on 64 bit builds. There are OpenGL and GLUT ports for the Platform's required versions, and (on a first look) I think they're the only "could-be-nasty" packages. So, maybe it's a good status. Edit: happy, alex and haddock have packages for the right versions, too. 
Link via [LtU](http://lambda-the-ultimate.org/node/3596).
&gt; 1) If he was going to default to Java anyway (being an experienced Java coder etc.), why was Scala being on the JVM instead of native such a big deal? This was not covered in depth, but it seems like it was decided early on that interfacing with existing Java code would not be needed. At this point, the programmer prefer to have a native code application. &gt; 2) I'm a bit curious about the "non-coders can read Haskell" part. Has this been anyone else's experience? I'm wondering if, for example, XMonad users ever get weirded out when they see things like $ in their config file. I got weirded out by it when I was first learning Haskell, and I am a coder :) I found this curious as well when I read the summary, but the article explains that the parts that would be consulted and possibly modified in the program were very mathematical functions. I guess Haskell isn't worse than another language in that respect. &gt; 3) He points out some problems with laziness but then concludes that Haskell is the ultimate language. Does this mean he didn't actually consider those issues a big deal, or perhaps wrote them off to his inexperience with a lazy language? He says that it's a problem, that he wishes more tools existed to alleviate the problems, but that it's not worse than the big problems of other languages he was used to. I imagine that he figures as he gains experience, he'll be better at spotting possible memory leak problems.
This is a good post running into the long (USA) weekend.
I can't contact this server from New Zealand, but it works from my server in the USA. Is this one of these geopolitical things? It is very annoying. Is anyone else having this problem? Can it be fixed, please?
Awesome that this is all available so quickly. Really hope there's video of the keynotes as well.
I was reading the awkward squad paper, and noticed that the author used unsafePerformIO rather more freely than I've ever seen (i.e. reading config files and such). Is that really ok to do?
Yes, I'm quite interested to see Guy Steele's talk.
Down from Denmark as well.
[Google Cache!](http://74.125.113.132/search?q=cache:4ctgqHp-oG4J:research.microsoft.com/pubs/80976/ghc-parallel-tuning2.pdf)
Not anymore, I suspect. Note that the awkward squad paper was written in 2001. Haskell and GHC have come a long way since then. Someone more authoritative than I should probably answer your question though...
These look *excellent*! Can't wait to read them.
Most toasters I know are more non-deterministic, not to say completely random, than that! :-)
I remember seeing slides of a similar talk by Steele. Ah, found it: [The Future Is Parallel: What's a Programmer to Do?](http://groups.csail.mit.edu/mac/users/gjs/6.945/readings/MITApril2009Steele.pdf)
And freely available, too! *impulse download*
The natural extension of this is things like `fromString` and `fromInteger` and (maybe someday) a way to overload `:` and `[]`.
[Nearly Free Speech](https://www.nearlyfreespeech.net/) is nice and very cheap. Although their GHC version is kinda old (6.8.3). 
It would be nice to be able to use the `[]` and `:` constructors. What I would really like to have is a way to do normal pattern matching, as well as support all the build-in syntax like `if then else`. I guess that could be implemented as some sort of source-to-source transformation.
Rather delightful slides. *Motivating*, even. As for that program, that blog post.... I'm working on it :-)
does Haskell automatically exploit the chuch-rosser theorem yet? http://en.wikipedia.org/wiki/Church%E2%80%93Rosser_theorem
Will there be videos available later for people who couldn't go?
Not that I know anything about GHC internals, but what implementation benefits do you expect to come out of the Church-Rosser theorem? It seems a bit like asking whether a programming language automatically exploits the commutativity of addition.
it was tongue in cheek, bro
Well, it sort of always has, in that it uses a non-strict evaluation order. The Church-Rosser theorem assures us that we won't get strange different results by evaluating in a different order. Along with another theorem that says that if any evaluation order terminates, then the normal order (outermost first) reduction terminates, this assures us that if some order of evaluation will terminate, lazy evaluation (which is outermost first) will, and will produce the same answer as any other order of evaluation. Real implementations like GHC don't just use lazy evaluation though, but do analysis to determine where it's okay to do some strict evaluation. If it weren't for the Church-Rosser theorem, things like that wouldn't be acceptable.
This feels like a cover of all haskell news I've seen. It shows a fantastically overcomplicated and difficult to understand solution to a simple problem. It's this kind of thing that pushed me away from haskell, along with the "Haskell is all things to all men" attitude. It's interesting, sure, but it seems to appeal more to mathematicians and theorists than engineers.
While I'm not sure I can come up with a convincing example off the top of my head of exploiting the commutativity of addition, exploiting associativity can turn linear-space algorithms into constant space ones. For example, consider a program like: sum [] = 0 sum (x:xs) = x + sum xs If we apply it to a list: sum [1,2,3,4] = 1 + sum [2,3,4] = 1 + (2 + sum [3,4]) = 1 + (2 + (3 + sum [4])) = 1 + (2 + (3 + (4 + sum []))) = 1 + (2 + (3 + (4 + 0))) = 1 + (2 + (3 + 4)) = 1 + (2 + 7) = 1 + 9 = 10 You can see that the amount of space used to store intermediate expressions grows proportionally with the size of the input list. If our runtime system were to make use of a rule employing the associativity of addition, the reduction might look something like this: sum [1,2,3,4] = 1 + sum [2,3,4] = 1 + (2 + sum [3,4]) = (1 + 2) + sum [3,4] -- associativity = 3 + sum [3,4] = 3 + (3 + sum [4]) = (3 + 3) + sum [4] -- associativity = 6 + sum [4] = 6 + (4 + sum []) = (6 + 4) + sum [] -- associativity = 10 + sum [] = 10 + 0 = 10 Now the space usage won't grow like it did before, and the right fold structure gets flipped over into a left fold structure dynamically.
This is all kinds of awesome! I love the fact that they can be composed as a category.
it looks like something Oleg would write (the biggest compliment there is!)
Yes!
That was really never OK, in my opinion.
And they're paying for it now. How long did we spend discussion how hard it is to parallelize ghc for this very reason??
It seems to me that the fundep is flipped around on the Boolean typeclass.
Brian, your slides and message are downright beautiful.
Tries are very interesting structures, but for the sorts of stuff I was doing, it just didn't make a ton of sense to have to import an external dependency of any kind.
Thanks Edward, looking into your category-extras packages has helped us giving things proper names. Like the `dimap' function, which still feels a bit strange to me.
Thanks a lot!
What is the difference with the venerable data-accessor package?
I keep writing these auto-accessors for all my records with a big comment "TODO: Autogenerate these". Thanks for solving this one for me :-) However, I do think I'd prefer it to generate the type signatures too. I don't think record accessors need documentation, why not attach the documentation to the record fields themselves?
Not that much... but neater syntax that reads more like functions, composition using Category, Applicative instance for arbitrary views (these are very useful), less than 150 lines of code. Which all make venerability a relative concept. ;-) But admitted, for the ones that are currently happily using data-accessor, a more concise syntax might not be worth it.
Yay, feedback! :-) &gt; It shows a fantastically overcomplicated and difficult to understand solution to a simple problem. My intention was to apply fundamental concepts like binary trees, stack based languages and shortcut fusion to find a sequence of ever shorter (and faster) solutions; explaining these concepts in the process. They sure are overkill if you just want to solve the problem, but the problem makes a marvelous toy if you want to study the concepts. Hm, maybe I should make that clear in the article, what do you think? &gt; It's interesting, sure, but it seems to appeal more to mathematicians and theorists than engineers. Yes in that it is but idle "fun", no in that the insights gained while playing are incredibly useful for serious engineering problems. At least, that's my belief and that's why I wrote the article. 
Nice article! &gt; Do you have an explicit type signature for the top-level? Is it correct? What does Haskell infer if you leave it out? As far as fixing type errors is concerned, adding your own type signature (assuming that it is correct) has the added benefit of helping GHC produce much more accurate and informative error messages. 
 data Then = Then data Else = Else if_ b Then t Else e = if b then t else e -- or whatever main = print $ if_ (9&gt;10) Then "foo" Else "bar" 
these look particularly pleasant and general. the iso is a nice touch.
It seems that the extra features can be tacked on data-accessor without any problem. I'm not that convinced that the syntax is all that more concise :) My main concern as a user is the multiplication of libraries with almost exactly the same functionality. It's hard to separate the wheat from the chaff.
Do some data structures. Search trees, heaps, random access lists. Maybe some graph algorithms.
My problem with Data.Accessor is that the guts of it make horrible reading via haddock due to Henning's insistence on pretending that Haskell is ML.
NIce idea, I was thinking on something similar to [Fun with Morse code](http://apfelmus.nfshost.com/fun-with-morse-code.html)
Yeah, that's nice, but a little advanced for beginners I think. Should you talk about lambda calculus make sure to mention [Alligator Eggs](http://worrydream.com/AlligatorEggs/)
I'm not so versed in ML, so I have difficulties interpreting your comment. Assuming it's related to the module system: the top level module doc seems ok to me, as I read it from Hackage.
Parallelism!! Multicore!! Here are some slides: http://donsbot.wordpress.com/2009/09/05/defun-2009-multicore-programming-in-haskell-now/ Stuff thta people really want to learn how to do. And no other language comes close to.
Make sure to cover Monad Transformers, learning how to use and stack them took longer than learning Monads themselves for me, and they are used in pretty much every serious project. And concurrency, especially STM should definitly get some spotlight, since that's one area where Haskell really blows the competition away.
I want to, but I'm not sure yet if the lab is the one with dual core machines (I'm getting resources through the local CS Alumni Society). If it is, and time allows, I'll sneak something for sure.
This is a nice philosophical issue that also came during the Haskell Symposium in Edinburgh last week. What to do with packages that expose more or less the same functionality. This package has been written before I ever came in contact with the data-accessor package and is exactly what I (we) needed and liked. Will it benefit the community when this package gets uploaded or will it only increase the burden of choice? I personally think not uploading will at least not benefit anyone and sharing might at least be helpful to some. I share your concern, but really think that keeping the semi-duplicates to ourselves will most certainly slow down any form of progress. One of the supposed solutions for this seemingly problematic superfluous amount of packages on Hackage at the Haskell Symposium was to have (reddit style) up and down voting of packages. Or having user-comments attached to the Hackage page. 
[PLT Redex](http://redex.plt-scheme.org/index.html) might be a more mature (less fun?) way to [visualize](http://redex.plt-scheme.org/traces.png) reductions.
You can demonstrate all the multithreaded and parallel code even on a single-core. It just won't actually run any faster :-)
If not tree zippers, then at least list zippers, since the concept is so neat.
I've not really looked into haskell much, how does the parallelism compare to erlang?
Read the above tutorial :-) Erlang does distributed concurrency well. Haskell does parallel shared memory well.
One of the first things I do with a type error is start adding type signatures to figure out where I went wrong keeping track of types. 
Nice little article. The one that I still need to work with is error handling. It's so different from other languages that most of the time I've managed to just work around it. Fortunately I haven't had to program anything for external use yet, so we can deal with compiler-generated errors.
I appreciate your sharing the package, and even more your open discussion of the issues I raise. This is ultimately what leads to a better set of libraries.
Could you elaborate on that? I'm new to using fundeps, but this way seems to me, at least intuitively, correct. In the example below, the type of the `Bool` instance (`f`), uniquiley identifies the type to which a `Bool` can be destructed (`r`). If I flip or remove the fundep, I can't write down `t = true :: Js JsBool`, because that's too ambiguous. class Bool f r | f -&gt; r where bool :: r -&gt; r -&gt; f -&gt; r false :: f true :: f instance Bool (Js JsBool) (Js r) where bool f t p = Prim "(function (p, t, f) p ? t : f)" `App` p `App` t `App` f true = Prim "true" false = Prim "false" data JsBool data Js a where Prim :: P.String -&gt; Js a App :: Js (a -&gt; b) -&gt; Js a -&gt; Js b 
Thank you, was interesting.
If possible, record it and make it available to the general public. Other than that, monad transformers can be tricky.
sounds pretty good, you could go into some other operations in data.list that are a bit more complicated, but spending alot of time on the typed IO system and how it tries to preserve purity should probably take up alot of time (and it'd be well spent)
Hadn't spotted it was you Dons :) I normally skip anything with scribd since it often gives me trouble. Looks interesting, erlang currently suits my needs better (and mentally clicks better) than haskell, but it's certainly worth watching. I certainly need to look into shared memory more. Cheers :) 
You should split out the bool function from the Bool class. Then you can get rid of the fundep if you want.
Wow, that's an awesome improvement (in most of the benchmarks). Lots of Kudos to the people behind this.
He's talking about the fact that HT calls every class he writes C and every type T. If you need to distinguish the two, you need to import everything he writes qualified. Also, try to figure out the documentation on [Data.Algebra.Vector](http://hackage.haskell.org/packages/archive/numeric-prelude/0.1.2/doc/html/Algebra-Vector.html) for example. You'll see things like: linearComb :: (C a, C v) =&gt; [a] -&gt; [v a] -&gt; v a Where the only way to see what class constraints those are is to hover over the link and look at the URL. Sure, this is officially Haddock's problem, but I'm not sure many people are keen enough on his approach to fix it for him.
Erlang has a complete concurrency/distribution story. And it is here today. Haskell has all the ingredients for an even better story. But it is not yet all here. And as the auther says, haskell's learning curve is a definite problem.
Wondering the same thing.
Haskell basically has no story for distribution currently, which I see as a major drawback.
That's very impressive!
That's funny. I actually found Haskell far easier to learn than Erlang. Erlang's syntax is just plain baroque most of the time to me.
I think that I read about a library that implements Erlang's distribution protocol, so that it is a legitimate Erlang node--an Erlang node doesn't have to be written in Erlang, it just needs to implement that protocol--and does so faster. I don't know much about Erlang, though.
I wonder when this (or something similar) will be available in official GHC distribution.
It would be nicer if it was explained how the "word counting" program would be supercompiled, because my hunch is that they're wrong about it being slow solely because of the intermediate list of words. It's probably also slow because String is not a particularly space efficient data-structure, in contrast to C's arrays. It would be nice if he would give an example of C that encoded "space/not" so we could see the difference in C code. I suppose that might be a little out of scope, but it would be nice to see the feasibility of speeding up the C program first hand.
Perhaps you should take a look at the paper http://community.haskell.org/~ndm/downloads/paper-supero_making_haskell_faster-27_sep_2007.pdf ? I would assume that the supercompiled program doesnt include any strings.
The [erlang](http://hackage.haskell.org/package/erlang) package?
That would be nice, but I don't know how. The `bool` function is the "primitive destruction" function for each instance of the type class. It replaces pattern matching. I don't see a way to generically write it in terms of the `Bool` type class.
I'd like to make a minor quibble with one point. &gt; It should noted that while Haskell can produce fast results, its lazyness can also be unpredictable in the optimizations (both speed and memory) that it performs. In my experience it is only the memory usage that is difficult to predict. The speed is bounded above by the speed of whatever algorithm you are using (and in some cases might be [much faster than you expect](http://www.reddit.com/r/programming/comments/8mmcu/i_tried_to_translate_the_ironpython_code_to/)). I believe that any unexpected slowness is entirely due to memory allocation (or possibly due to you not having any idea what algorithm you are actually using (which can easily happen in a declarative langauge)).
IIRC there's a guy from the Timber group doing an internship at MSR on supercompilation.. SPJ mentioned it in an interview a while back, but I haven't seen any actual information. 
Distribution should be transparent, or close to it. Using the erlang package isn't.
Something like this (it's been done many times before) class Boolean bool where false :: ... true :: ... (&amp;&amp;) :: ... not :: ... class (Boolean Bool) =&gt; Conditional bool a where cond :: bool -&gt; a -&gt; a -&gt; a You have to make an instance declaration for each. 
It's on the wiki: http://hackage.haskell.org/trac/ghc/wiki/Supercompilation
Couldn't C be supercompiled too?
Oh right, now I get what you mean, thanks! I didn't quite realize what it meant, the first time I read you [saying that](http://www.mail-archive.com/haskell-cafe@haskell.org/msg60177.html). Separating the constructors and destructors to separate type classes is interesting. I'll try to see if I can do this for different data types as well.
Thanks for that - both the haskell and the C are using C's getchar. The string elimination is manual. "space/not" is also more fully explained - it is just as non-trivial as promised.
People like to bash Erlang's syntax, but I really don't think it's as bad as people like to say it is. It definitely has some niceties too. Probably my favorite is it's single-assignment flavor, that you don't have to use let or where to declare a variable, and you can write things like if condition -&gt; Var = exp1; else True -&gt; Var = exp2 end, exp3(Var) But don't get me wrong, I acknowledge it can be a little overly quirky in a few respects, probably the most annoying being statement delimiters.
Sometimes I consider this pair of isomorphisms: aâ(bÃc) â (aâb)Ã(aâc) (a+b)âc â (aâc)Ã(bâc) and at some level I'm like "of course arrow is contravariant" and at another "huh? why is a sum isomorphic to a product?". I can't decide if it's trivial in a deep way or the other way around.
&gt; Haskell propaganda: polymorphism at higher kinds is a much more substantial advantage over ML than, say, laziness Amen.
we should all hurry up to vote *against* Haskell being included.
Yes, but it's more difficult.
How does this differ from Parsec 3? it seems to support all the "in addition" items listed for parsimony. I'm curious what prompted the name change.
If only the page loaded, I could better understand my objection to this book.
What a stupid UI selectricity uses -- in Safari, at least, you have to drag and drop from the bottom of the list to the top in stages, as the window doesn't autoscroll and the text is stupidly big.
The triviality/deepness is a lot more apparent if you consider coexponentials instead of just exponentials. [Filinski](http://www.springerlink.com/content/m2105282ru426654/) is a good place to start.
It didn't even record my votes correctly. In the "Your vote has been recorded with the following ranked preferences" screen, my selections from 5-10 were not included in the order I selected and some weren't even in the top 10.
Same here, in my case it started getting my votes wrong after the 3rd choice.
:) It nodes ill for this thing if a) the page never loads, and b) when it does people complain of wonky over-complicated UI.
Learning a language's syntax is probably 10% of the effort of learning to use the language properly. I would say that Erlang is a bit nicer for the last 90%. Besides, Erlang's syntax ain't that bad. Commas, periods, semicolons work roughly similar to English really :)
Woops, time to go refactor my own "Parsimony" library. That is what I get for not publishing it to hackage, yet.
What? As far as I can tell the string elimination is *not* manual, that's kind of the point. They use getchar, sure, but they still pipe that into "words" etc.
Fun stuff! When will we see it in GHC? Or is this stuck in SHE-land for the forseeable future?
:) It is a good article, please don't get me wrong. It's perhaps more personal gripes with haskell :) Many things I've seen revolve around "You can solve it in this way, which is fun". That's great, and I do enjoy it, but there are rarely more reasons. Since I've had such a bad time with haskell, I think it's harder for me to get into without a good reason. &gt; Hm, maybe I should make that clear in the article, what do you think? I think you already do. I'd be very interested to see a little section about each algorithm or method with a sort of pros/cons or "sure it's overkill *here* but what if you had 10000 ..." explaining why this other algorithm suddenly becomes useful. Perhaps they run with a much nicer complexity, or are better for running while an input streams in (that's something I thought of when seeing the tree based approaches). My other query is how hard would the algorithms be to extend to properly decoding morse code, with spaces between words? Which methods would be amenable to this kind of change and which wouldn't? &gt; At least, that's my belief and that's why I wrote the article. I do agree, although since I have much less understanding of these solutions I don't really get where they would be used in the real world. This is a problem with my lack of knowledge though! Sorry for the barrage of questions &amp; thanks for the reply!
The paper is behind a paywall. :'( Got a free version somewhere?
Comparing at the two entries in hackage, I note the following differences. Please correct me if I'm wrong. * Same authors. * Parsec 3 was last updated about 18 mos. ago. * Parsec 3 does not depend on any utf8 or other encodings package, so it does not seem to support character encodings. Its bytestring support seems to treat each byte as a Char, except limited to 8 bits. * Parsimony does not depend on mtl, so it does not appear to be a state transformer. In fact, judging by the API, it doesn't seem to support user-defined state at all, at least directly. Its "State" type seems to be some kind of internal parser state. 
Yes, you're right, I'm not sure why I wrote that last night...
It's simple if you look at it like high school algebra, and remember that (-&gt;) is (flip (^)): (a+b) -&gt; c = c^(a+b) = c^a * c^b = (a -&gt; c) * (b -&gt; c) a -&gt; (b*c) = (b*c)^a = b^a * c^a = (a-&gt;b) * (a-&gt;c) 
Is there a pdf?
Scribd is awful -- nearly unuseable. Why not just a PDF hosted somewhere.
Guys, guys. This is a .pptx file I just got from Simon. There's a link to a converted .pdf on the page (check the abstract). Rather than wait for Simon to embed the viewer in a blog post, I've posted it here, since the talk is *very important for ghc 6.12*. So quit the whinging - or recommend a better online viewer for .pptx files.
http://www.galois.com/~dons/talks/simon-marlow-new-io-layer.pdf
The problem that you have is that Js JsBool doesn't determine one type on the right it determines a whole mess of them (different instantiations for r). So your fundep is a pleasant fiction. You can flip it around with some work by factoring your class as suggested by Lennart. class Boolean b where false :: b true :: b ... class Conditional b a where cond :: b -&gt; a -&gt; a -&gt; a but now the type system is going to generally be a pain in the neck, because it may have a hard time inferring which b you meant in any given use of cond. So you'd have a lot of type annotations. On the other hand, for your particular Js example we know that no matter what the a's will have the same shape, so we can use them to get an unambiguous instance selection mechanism that will tell cond which Boolean instance to use: class Boolean b =&gt; Conditional b a | a -&gt; b where cond :: b -&gt; a -&gt; a -&gt; a instance Conditional (Js Bool) (Js r) where ... Now when the compiler encounters an expression of the form 'cond foo bar baz' -- it can use the types of bar, baz and the result, to figure out which Boolean to select. Moreover, you no longer have an ill formed instance declaration, because every type of the form: (Js r) generates a distinct type through the fundep: (Js JsBool) The fundep is both a blessing and a curse. Unrestrained, you could do something like also provide instance Conditional Bool a where cond True a _ = a cond False _ b = b which could then perhaps be applied to your javascript EDSL in certain circumstances, to select based on compile-time known constants, but this comes at the cost that almost every use of cond would have to have a type annotation, and in general having the fundep and disambiguating with newtype wrappers when you want other cases is usually less work.
To be fair the bytestring support in Parsec 3 is more of a sample of what you could do with ByteStreams as a source. ByteStrings are much more naturally a Word8 Stream than a Char Stream. The existing Char Stream is a sop to people wanting to use the same combinators. It is fairly trivial to layer a Char Stream over UTF8 ByteString from the utf8-strings package though. Since you can basically just pass along the call to uncons. As I currently use the extra Parsec 3 user state it might be an interesting transition for me. There are a couple of things I would have liked to see changed in Parsec 3, (GenLanguageDef exists, but all the languages use the monomorphic LanguageDef needlessly), but Derek Elkins seems to have vanished lately, so I'll have to take a look.
Hrmm. I think this means the only decent parser combinator library name left is "Parsely" or "Parsley". ;) (It's mine! You can't have this one!)
I think we're converging on at least a first step for the GHC version. SHE's been really useful for trying out examples and falling into funny little traps. I still think we need more thought about how to manage the business of run-time witnesses to type-level values (singletons, classes, case analysis principles, ...). I resent the apparent need for more than one kind of skullduggery to beat this into Haskell. The question is not whether, but exactly what and exactly when. My best guess, early 2010. Types lifted to kinds will be in, but pi types (as illustrated in my HIW talk) may need some more thinking through.
Upload! Quick!
Thanks for your explanation! Your `Conditional` type class is the same as `IfB` in Conal Elliots [Boolean](http://hackage.haskell.org/package/Boolean) package. I guess that's the way to go. I'm definitely going to try to do this for all types in the Prelude. As well as write some more realistic JavaScript examples, to see if this "data type -&gt; type class" transformation works out in practice.
Parsnip! Parsley is already taken by Conor McBride in SHE ;)
If you have a link to the PDF, please submit it directly. Linking to scribd just introduces an annoying "find the PDF" link to view the presentation.
I'm curious what the motivation might be for using CHP over any of the other concurrency primitives. What is it best used for?
I know some Haskell, but looking at things like this make me feel like the Haskell masters are really aliens. How do you learn stuff like this (not a rhetorical question)? Do they teach this in universities in Great Britain or something? I get on Wikipedia occasionally to try to learn about category theory and the impenetrable wall of mathematical jargon and notation leaves me sad and less educated than I'd like to be. I've taken Discrete Mathematics, but I feel like I'm missing several layers of knowledge to even begin to approach this stuff.
(-..-) is funny because your username is pigworker. Such a cute sleepy pig!
I am scared of Tom...
FWIW, I think Scribd's interface is fine for presentations. It sucks for papers but that's another matter entirely.
&gt; Since I've had such a bad time with haskell :'( &gt; I'd be very interested to see a little section about each algorithm or method with a sort of pros/cons Let me try: Dichotomic search (aka binary search) is the mechanism behind any finite map, be it balanced trees like `Data.Map` or tries like `Data.IntMap`. Association lists are O(n) and slow whereas binary search is O(log n) and very fast. Of course, libraries implement that one for you. The most prominent examples of reverse polish notation are probably [Forth](http://en.wikipedia.org/wiki/Forth_%28programming_language%29) and [PostScript](http://en.wikipedia.org/wiki/PostScript). It's very easy to parse yet very expressive and thus very useful in C or Java when you don't want to write a proper parser with tools like YACC or JavaCC. Of course, parser combinators make parsing a piece of cake in Haskell. Deforestation is a great optimization technique for reducing constant factors in Haskell. For instance, [stream fusion](http://www.cse.unsw.edu.au/~dons/papers/CLS07.html) achieves C like speeds on code like wordCount = length . words by automatically transforming it into a tight loop. Are these answers satisfactory, anything you'd want to know more about? &gt; How hard would the algorithms be to extend to properly decoding morse code, with spaces between words? This is not very difficult, you just need to use a replacement for the overly simplistic `words`. Here is one possibility: morseWords :: String -&gt; [[MorseCode]] morseWords cs = case space of [] -&gt; [[code]] " " -&gt; (code:w):ws _ -&gt; [code]:(w:ws) where (code ,cs') = break isSpace cs (space,cs'') = span isSpace cs' (w:ws) = morseWords cs'' decode' = concat . intersperse " " . map (map decodeLetter) . morseWords The main function `decodeLetter` decodes just a single letter from a stream of dots and dashes without whitespace. &gt; Sorry for the barrage of questions &amp; thanks for the reply! No, thank you for the feedback. :-)
Thanks :-)
&gt; No, thank you for the feedback. :-) Thanks for the reply :) It's something that I really love about the programming community. A complete stranger to you made what might be interpreted as a nasty comment, and after a quick back and forth you've posted a lot of detailed information to help out! &gt; It's very easy to parse yet very expressive and thus very useful in C Now this will certainly be useful. &gt; Dichotomic search (aka binary search) is the mechanism behind any finite map, be it balanced trees like Data.Map or tries like Data.IntMap. Association lists are O(n) and slow whereas binary search is O(log n) and very fast. Of course, libraries implement that one for you. Ah yes, but hasmaps generally get O(1). Still, I suppose practically this all comes down to the size of the set and the time to run the hashing function. &gt; This is not very difficult, you just need to use a replacement for the overly simplistic words. Here is one possibility: I'm afraid at the moment I'll have to take your word for that :), haskell syntax makes by brain melt. I'm learning erlang at the moment, so maybe when FP concepts are more common to me I'll be able to pick up haskell. &gt; :'( It's a shame considering how much people who use it seem to love it. I mainly drifted away because of the (seemingly) constant "Your program can never crash!!" statements. The only two non-trivial apps I used were incredibly buggy. I certainly like many of the concepts, particularly laziness. Though I've not replied to each section, all of this is very useful and interesting, so thanks again :)
 * Has anyone investigated how to use this with GHC 6.12's dynlinking yet? * The small community characterization is a little unfair. We're the biggest FP community. But small compared to Ruby.
The chap has managed to mess up all his installed packages, so it's not entirely surprising that now things are not working well. While ghc-pkg check can tell if files are missing, cabal cannot, so cabal thinks all the registered packages are actually installed. The fix in this sorry state is to unregister all the broken packages and then installing new things via cabal will work fine.
Don - it _almost_ works. I'm having some issues with bus errors at the moment, but I'm pretty close, I think. Having it work with GHC is a pretty high priority. And the small community thing was entirely unfair and inserted mostly to get a laugh.
&gt; Ah yes, but hashmaps generally get O(1) Ah, not quite, hash tables are O(size of key). And if you have n items with a unique key each, the key size must be *log n* bits, so it amounts to the same. Nevertheless, hash tables have a smaller constant factor, mainly because calculating the hash won't access RAM. I've alluded to that in the section [comparing to C](http://apfelmus.nfshost.com/fun-with-morse-code.html#comparing-to-c). &gt; haskell syntax makes by brain melt Fabricando fit faber. ;-) The most confusing thing is probably the lack of parenthesis around function arguments. Other than that, it's pretty close to mathematical notation: *f : R â R* *f(x) = x^3 + ax + b* translates to f :: Double -&gt; Double f x = x^3 + a * x + b
Thanks Mark. This is great stuff. And right in time for a broader push to use GHC + dynlibs to integrate Haskell into Erlang, Ruby, Python, ... Lead on!
He does look a little scary in that photo.
As a ruby programmer by trade and aspiring Haskell dev, the original post on infoq made me hopeful at sliding Haskell in to one of my projects at work. Glad to see the interest is continuing
You forget parson!
&gt; Ah, not quite, hash tables are O(size of key). Hmm, interesting, although I'm not sure I quite agree. This would surely be dependent on the hash function used. Is this a minimum complexity for a perfect hashing function? You could design a (bad) function which performed in O(1), for example H(message)-&gt;message &amp; 0xFF, for 8 bit keys. [edit - changing this to more bits would not change the complexity... or would it? Does the increase in bits required change things?] &gt; Fabricando fit faber. ;-) :p true! I'll have some spare time soon, once my thesis is finished. Maybe someday I'll even be able to mentally parse the primes (') in variable names without looking for the closing one :) 
Hrmm, thinking phonetically a bit I get 'Parcel', which for a monoidal parser is actually pretty good, or dropping the initial 'pars' requirement: 'Sparse'. I might go with Parcel, actually.
I cannot speak for the masters, as I am just a workaday C++ programmer with a hobbyist interest in haskell and mathematics. My formal education was in electrical engineering, which was pretty far removed from this stuff. I have been piecing together some of this categorical/algebraic material by asking in the #haskell IRC channel and in the comment sections of relevant blog posts. Many haskellers seem happy to help explain the concepts or point you towards the more accessible academic papers/resources. I have printed off and read lots of papers, often multiple times in the last few years because I didn't really grok them at first. ;-) The [haskell wikibooks](http://en.wikibooks.org/wiki/Haskell) are fairly accessible and, while incomplete, seem to be getting steadily better. I also found this [beginner's intro on type algebra](http://blog.lab49.com/archives/3011) pretty sensible, as it was aimed at "regular" programmers.
Wow. That beginner's intro on type algebra is incredibly mindblowing. Well done, sir.
I said it once, I'll say it again. Haskell is the Katamari Damacy of programming languages. We just roll over and absorb you into our functional, type systemy goodness. :)
I'm pretty sure this is trivial by having a RealUser type, and then User and Superuser as direct newtypes over RealUser; then an empty typeclass with a function `a -&gt; RealUser` so you can write functions working for both.
It would be nice if the author would put these up on github (or similar) where they could be browsed and would likely stay up longer. Right now he links to a .rar on a file sharing service.
Right. If you can get the current GHC HEAD going, it should pass all the tests bar one, modulo header paths for ruby etc. Probably next step is to make installation a bit less fraught.
Reading mistake #1: Mathematics and Copulation
care to try it out and tell me what went wrong? :)
I'll have a look later on after work.
&gt; encoding the distinction between a user and a superuser[1] directly into the type system? User capability separation is actually pretty straightforward. Some people at Galois have done this for projects in the past. Essentially your authentication mechanism becomes a smart constructor for the User or SuperUser types.
&gt; &gt; Ah, not quite, hash tables are O(size of key). &gt; Hmm, interesting, although I'm not sure I quite agree. This would surely be dependent on the hash function used. Is this a minimum complexity for a perfect hashing function? Yes, any proper hash function has to "touch" every single bit of the input, so it needs at least O(size of input in bits) time. (The input is the dictionary key.) This is the same argument that finding the maximum of a collection of n numbers is O(n) because the function has to "touch" every number. (Otherwise, it might miss the maximum). Of course, if the key is a 64 bit integer, then it's 64 bits long and the hash function runs in O(size of key) = O(64) = O(1), that is constant time.
Well done for avoiding the obvious title, "call/cc considered over-rated". :-)
Across the board on OS X or Linux, I use a package manager whenever I'd be happy with a version of a package from the last year or two. I have friends who adopt packages, and they tell me I should too, if I want to expect better than this for any package I care about. This has nothing to do with GHC, it's the norm. There's a tiny issue with Snow Leopard; see http://hackage.haskell.org/trac/ghc/ticket/3400 and links for details. Intall GHC using the package "GHC-6.10.4-i386.pkg" found directly on their site: http://haskell.org/ghc/download_ghc_6_10_4.html Then edit (as superuser, e.g. "sudo nano /usr/bin/ghc") the file /usr/bin/ghc to change "-dynload" to "-optc-m32 -opta-m32 -optl-m32 -dynload". This forces GHC to issue 32 bit code; it can't yet manage 64 bit code for OS X.
Ah, thanks. I think my problem was that in reality, any hashing function will run with O( ceil(n / k)) where k is the size of each processing chunk. For all n &lt; k, this is O(1). Therefore you can always define a hashing function which will run in O(1). Ahh, it's nice to think through problems again. Next up: fault tolerant job server in erlang with c nodes "in the cloud" :). Mmmmm, programming. Thanks for this. I'll have to keep an eye out for your articles in the future :)
&gt; To show the other direction, we traverse a term, inserting return before each constant or variable and replacing applications with flip bind, aka (=&lt;&lt;). Thus, if a proof term for the proposition (NOTNOT a) isn't already in the form (forall m. Monad m =&gt; ((a -&gt; m F) -&gt; m F)), we can always make it so. I didn't follow this step. Can someone do this transformation to \f -&gt; f (\s -&gt; s (ex_falso_quodlibet . (f . const))) for me to illustrate? I mean, I can write \f -&gt; f (\s -&gt; s (ex_falso_quodlibet &lt;=&lt; (f . const))) :: (Monad m) =&gt; ((((a -&gt; m b) -&gt; a) -&gt; a) -&gt; m F) -&gt; m F But I end up with `(m b)` instead of `b` in the type. Now I cannot apply this to h: *LEM Control.Monad&gt; :type h (\f -&gt; f (\s -&gt; s (ex_falso_quodlibet &lt;=&lt; (f . const)))) &lt;interactive&gt;:1:0: Inferred type is less polymorphic than expected Quantified type variable `m' escapes In the first argument of `h', namely `(\ f -&gt; f (\ s -&gt; s (ex_falso_quodlibet &lt;=&lt; (f . const))))' 
It is pretty straight forward to encode security policies at the type level, so you can work with different classifications of data in the same program. I have a small package I never packaged up nicely for hackage that does this: http://comonad.com/haskell/security-policy/ which provides a SecurityPolicy http://comonad.com/haskell/security-policy/src/Control/Monad/Security.hs which can be used as a monad in your code. You just need to supply an appropriate security lattice: http://comonad.com/haskell/security-policy/test/Policy.hs and then code can just use the right security levels. http://comonad.com/haskell/security-policy/test/Main.hs And nothing that doesn't go through your Policy can extract the code (without using unsafePerformIO/unsafeCoerce magic). The reason I never packaged it up is that I started building a set of type level lattice combinators, so you could express chains and antichains, and more complicated policy lattices without incurring RSI in your wrists, but I never made them simple enough for people to easily understand.
Yay dynamic library support! Now I feel an urge to write a plugin for something, but I'm not sure what yet...
You may not be able to encode that. It uses `f` non-linearly, like his `t3` example, which is also disallowed. If I'm understanding things right, his encoding only works for propositions which are computational, so I guess it's not surprising that Peirce's law wouldn't be admitted (since we're not working in a wacky classical type theory). It doesn't allow you to prove any propositions that you wouldn't normally be able to prove constructively, it just lets you use proof-by-contradiction as an additional technique for (some of?) those propositions that would have proofs by more typical constructive means.
Not to steal Oleg's answer, but perhaps trying to encode Peirce's law is meant to fail? If it did go through there would be no point in claiming that this version of LEM is weaker than full classical logic. A type error like this is what I'd expect from something weaker, but still useful.
I will just be happy to reclaim something like 60 megs of space (for the 4 or so executables using the GHC API and linking in GHC statically).
I'm troubled by Oleg's claim that forall m. (Monad m) =&gt; (a -&gt; m F) -&gt; m F and (a -&gt; F) -&gt; F are isomorphic when ((((a -&gt; b) -&gt; a) -&gt; a) -&gt; F) -&gt; F is inhabited while forall m. (Monad m) =&gt; ((((a -&gt; b) -&gt; a) -&gt; a) -&gt; m F) -&gt; m F appears to be uninhabited. Edit: I'm not even convinced there is an isomorphism between types such as forall m. (Monad m) =&gt; ((((a -&gt; m b) -&gt; a) -&gt; a) -&gt; m F) -&gt; m F and ((((a -&gt; b) -&gt; a) -&gt; a) -&gt; F) -&gt; F although I haven't yet been able to show otherwise yet. Values of the monadic type appear to have extra sequencing information that you might be able to exhibit by running it with more interesting monads than the Identity monad. Oleg's syntactic transformation appears to simply make up one possible sequencing based on the syntactic form of the term.
Done http://github.com/snkkid/LazyFooHaskell/tree/master
I don't know of a free version of the CMU address, though his thesis (mentioned by applicative) covers the same in more detail. *edit:* The short version of the story is that the direct side of the universe has values, products of values, and the natural transformations, curry : C^(A*B) -&gt; (C^B)^A apply : (C^B)*B -&gt; C phi : A*(B_C) -&gt; (A*B)_C Whereas the dual side of the universe has continuations, coproducts of continuations, and the natural transformations, cocurry : C_(A+B) -&gt; (C_B)_A coapply : C -&gt; (C_B)+B theta : (A+B)^C -&gt; A+(B^C) And each function can be viewed as either a value-transformer or a continuation-transformer, that is each morphism (f : A -&gt; B) can be objectified as either (Exp f : B^A) or (CoExp f : A_B).
Wouldn't the isomorphic monadicization of ((((a -&gt; b) -&gt; a) -&gt; a) -&gt; F) -&gt; F be forall m. Monad m =&gt; ((((a -&gt; m b) -&gt; m a) -&gt; m a) -&gt; m F) -&gt; m F (aka convert all "-&gt;" to "-&gt; m") ? That looks more like the Kleisli lifting than the other versions you proposed.
dons: stop with the scribd already.
Possibly, since I don't understand the isomorphism that Oleg is suggesting here. But means that makes it even less likely for `h` to apply to the transformation and it should be easier to exhibit non-isomorphic behaviour. 
dons: don't stop with the scribd, it works fine for me.
Why? It works fine for me in every browser and OS I've tried it with.
Are there any performance results?
See also [accelerate: An embedded language for accelerated array processing](http://hackage.haskell.org/package/accelerate/).
Because it is inferior in every respect to just a PDF; and the company isn't one I wish to support, nor is it long-term trustworthy.
The very first link on the page is to the PDF. It's not like he's denying you the PDF goodness.
Neat, thank you for this! Waiting for Missile Command :)
Yes, and I'm glad he included it, although I had to read the page once or twice to figure out what link was what (I don't expect the PDF link to be the first thing on the page, snuggled under the comments link). Not all scribd users are so considerate.
very cool. btw, **:s/\s\\+$//** ;-)
30% faster Data.List.map, wait for the paper though for full results.
Does it matter what goes in the type class? How about a continuation passing style 'case' function: class AdaptList a where data List a empty :: List a cons :: a -&gt; List a -&gt; List a caseList :: b -&gt; (a -&gt; List a -&gt; b) -&gt; List a -&gt; b xxs ++ ys = caseList xs (\x xs -&gt; x `cons` xs ++ ys) xxs That will save some case statements compared to head and tail, but ghc is probably quite good at getting rid of those by itself.
might just do that. Thanks for the idea.
Yeah, i know, can be annoying if your editor wraps lines. Mine doesn't so I never notice.
So far as I understand it, Oleg is just suggesting that there's an isomorphism between the direct implicit monad and a universally quantified explicit monad (i.e. id ~ return, ($) ~ (=&lt;&lt;), (.) ~ (&lt;=&lt;) ...though the direct monad doesn't require us to explicitly create values with id nor explicitly use ($) for application since these can fade into the whitespace). By universality we can treat the rank-2 quantified monad as the Identity monad, whence its the same isomorphsim as the one between direct and Identity. I haven't sat down and thought about it yet, but I'd imagine that we don't even need the full power of a universal monad; a universal applicative functor should be sufficient. *edit:* That is, it's an isomorphism between the pure category, **Hask**, and the pure subcategory of any Kleisli category generated by a monad. The morphisms of a Kleisli category all have the form (_ -&gt; M _) and by universality we can show that every morphism of the subcategory has the form (return . f) for some morphism f in the original category (since there is no other way provided by every monad for generating an M X) and thus the subcategory is isomorphic to **Hask**.
Thanks! I especially like the pace these go at.
The `GameObject` type class is never really used in a polymorphic function. Maybe it doesn't really need to exist?
Why are you using Î² in the class declaration for `GameObject`? Couldn't you have used a simple 'b'? That aside, cool stuff. Keep sharing!
It might not *need* to exist, but I personally rather liked it as a way of organizing the code.
The code would be organized exactly the same way without it though. There just would have been a few more identifiers.
What does Int# mean?
What I meant by "organizing the code" included the choice of identifiers -- as a matter of taste or bikeshed color. Perhaps I am just inexperienced, but I found that the class made the code slightly easier to follow. Your original point, that there is currently no function ``:: (GameObject g) =&gt; ...`` is of course quite true.
It is an unboxed machine integer (in Ghc). A normal `Int` is stored as a pointer to an object holding the actual value or an unevaluated thunk that returns an `Int` value.
thanks!
My problem is that \f x y -&gt; x &gt;&gt;= \a -&gt; y &gt;&gt;= \b -&gt; f a b and \f x y -&gt; y &gt;&gt;= \a -&gt; x &gt;&gt;= \b -&gt; flip f a b are identical when run with the identity monad, but are distinguishable when run with other monads. Since running with identity maps two distinct functions to the same image, it cannot be part of an isomorphism.
Nice!
On slide 17, if you're trying to refer to bitdata annotations, you might want to cite Diatchki, Jones and Leslie, High-level Views on Low-level Representations, or Diatchki, High-Level Abstractions for Low-Level Programming. The Habit report is neither public nor the original source of these ideas.
solving the "Show" problem: http://www.hpaste.org/fastcgi/hpaste.fcgi/view?id=9314
I have funky editor plugins that I felt like trying out.
This is true, however I use typeclasses as a way of grouping a set of types that share common traits (like this one). That way, I can just call update on any Game Object and whatever updating will be done. I think it's a nice way to structure it, but it is true that it isn't necessary.
&gt; Thanks for this. I'll have to keep an eye out for your articles in the future :) Anytime. You know were to find them. ;-)
Cool. It is like Parenscript but for haskell?
The package in question is http://hackage.haskell.org/package/jsmw Is it like Parenscript? Dunno.
From the post title, I thought that someone had automatically produced a javascript file from idiomatic haskell source. But this seems to be a program written in a haskell DSL that, when run, prints javascript code to stdout. It's still quite cool (I upvoted), but I would not call this 'compiling'.
Why wouldn't that be compilation? ASM to machine code is compilation, and the transformations involved in turning a Haskell EDSL into JS would seem to be more complex.
Gwern, I see your point. Perhaps I should have said "I would not call this 'compiling *haskell*'". This is translating a particular, javascript-flavored haskell data structure into JS code, not invoking a compiler on general haskell code to a JS target (``ghc --make -fjavascript foo.hs``) as the title suggests (to me at least). I seem to recall some experiments using one of the more experimental haskell compilers to target javascript. But other than my reservation about the title, I found this very interesting and want to go play with JSMW now. :-) 
It doesn't work very well on my firefox configuration :-(
Nathanic, Thanks for your interest to JSMW. Please take into consideration that a lot of changes have been made (and keep being made) into the JSMW toolchain since the package referred here was released. You might consider darcs get http://code.haskell.org/yc2js/ instead. You will also need HSFFIG darcs get http://code.haskell.org/hsffig/ If there are any questions, e-mail me &lt;golubovsky at gmail dot com&gt;. 
Paraphrasing the definition for Parenscript: JSMW is a translator from a small Haskelly language to Javascript. JSMW cannot unlike Parenscript run identically on both browser and server. It might be easier to achieve if it was based on a monad transformer, but currently it is not. Generally JSMW is a monadic interface to WebBits Javascript code generator http://hackage.haskell.org/package/WebBits. It also incorporates automatically derived bindings for DOM interfaces, based on published W3C specifications (WebIDL). That's why my examples do not always work well with Microsoft Explorer ;) 
Please forgive a bit of the weirdness w/ this particular episode, I'm still working out the kinks in both the software (it can be a little touchy) and the scheduling. They'll (hopefully) be a more normal looking one next week, and after that it'll probably get weird again as I start to work on the utilities associated with compiling the HWN. /Joe
 Resolving dependencies... Downloading cpphs-1.9... cabal: Error: some packages failed to install: cpphs-1.9 failed while downloading the package. That's very strange, never seen such an error before, it is reproducible, and downloading the package manually from Hackage worked without any problem... 
Very cool ! http://jng.imagine27.com/articles/2009-09-12-122605_pong_in_clojure.html
I think the point remains that since the monad is universally quantified, x and y (and f) can only be formed by return (which adds empty structure; or by bind/join which preserve structure). And if they can only be formed by return, they cannot have the side effects which would allow the difference between these functions to be observed. Thus, by restricting the domain, these functions become equal under the universal monad. The isomorphism is only to a *sub*category which is shared by all Kleisli categories, namely the embedding of the pure world. This is part of the reason why I said Applicative should be sufficient. Using Monad imputes a diversity of structure which cannot be taken advantage of (due to universality), so it sets our intuitions on edge. Since applicative functors have less structure, there's less we need to use universality to get rid of and so it better matches our intuitions about the pure embedding.
I tried it about an hour after you posted this and cpphs-1.9 downloaded and installed for me without issue.
bind/join doesn't just preserve structure, it creates new structure, and it does so in a way that doesn't preserve extensional equivalence because two semantically equivalent but syntactically different terms can be sent to two different values. This doesn't sounds like an isomorphism to me.
I still got the same error message, but only for cpphs, the other packages work.
Oh, a print-on-demand version? Last I checked the book was out of print and only obtainable at insane prices, what's going on?
Try again with higher verbosity and see what the issue really is (maybe it's downloading fine) - there have been the occasional problems in cabal-install relating to proxies and network access.
 $ cabal install cpphs --reinstall --verbose Gives basically no extra (useful) information to the above. And it works with other packages, the only problem is with `cpphs`.
OK - and you've checked that if you manually downloaded the tarball and unpacked it, that it will install? If it will, I'd suggest grabbing darcs cabal &amp; darcs cabal-install, and then if cabal-install *still* won't download &amp; install, you may have a bona fide bug on your hands.
Thank my wife, she's something of an internet shoping wizard. I was actually pretty blown away on the price.
Any feedback on the accuracy of my explanation in the article is welcome. I'm just excited to be learning the material.
Yeah, manually downloading and unpacking it and then installing locally with cabal-install worked. Also I installed cpphs in the past with cabal-install, but it does not work now with older versions either. So you suggest updating the whole Cabal framework? This is with cabal-install version 0.6.0 using version 1.6.0.1 of the Cabal library I remember having some issues with cabal-install 0.6.2 (?) and/or Cabal 1.6.0.3, so I am somewhat reluctant to upgrade, but I cannot find the relevant ticket at the moment... 
darcs cabal-install is up to .7
If you ever do get into the industrial-scale, in addition to tries you may be interested in checking out this paper [Dany Breslauer (1995) *Fast Parallel String Prefix-Matching*](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7259)
Do you have an example? My understanding of bind/join is that it preserves both the inner and outer structure by gluing them together in the natural way. Which is to say, I've yet to run into a counterexample to that intuition. It seems to me that if bind/join were to truly create new structure then it would violate one of the monad laws...
Having followed GPU-accelerated Haskell with excitement, I wonder when they'll release some free code at all. I've got some numeric code to speed up.
This crashes GHC for some reason, but GHCi works.
Yes, I'm thinking of `\f a b -&gt; f a b` and `\f a b -&gt; flip f b a`. These two presentations denote the same function, but if you insert binds and joins you get two different functions because the two parameters are sequenced in two different ways. [See above](http://www.reddit.com/r/haskell/comments/9j5c7/constructive_law_of_excluded_middle/c0d1j98).
GHC is trying to do infinite inlining because they think no one uses such an [awkward type](http://r6.ca/blog/20060919T084800Z.html) in real code.
Good lord this thing's a mess to untangle. Fun, though. I've got it down to: newtype M a=R{u::M a-&gt;a} main = print (( (\x-&gt;f ( (u x) x ) ) (R (\x-&gt; f ((u x) x ) )) ) 10) f g 0 = 1 f g 1 = 1 f g n = g (n - 1) + g (n - 2)
Okay, so here's my untangling of the first two lines there: newtype M a=R{u::M a-&gt;a} main = print ( f ( (u x1) x1 ) 10) x1 = R (\x -&gt; f ((u x) x) ) u x1 = (\x -&gt; f ((u x) x) ) (u x1) x1 = f ((u x1) x1) Since we're using the parameter g below, we'll just say ((u x1) x1) = g. Thus g n = f g n f g 0 = 1 f g 1 = 1 f g n = f g (n - 1) + f g (n - 2) And now it turns out that g doesn't matter. So: f 0 = 0 f 1 = 1 f n = f (n - 1) + f (n - 2) main = print (f 10) Edit: I suppose saying g doesn't matter isn't quite right. It just behaves in such a way that we can rewrite as that last bit.
Neil: this is amazing! I was just about to ask if there was a way I could improve my code without bugging the poor people on #haskell :)
Very helpful, thank you!
`(\x-&gt;f ( (u x) x ) ) (R (\x-&gt; f ((u x) x ) ))` is immediately recognizable as `fix f`, at least to those who have been trained to see it.
Ah. Thanks. I was not aware of that function. TIL...
It's just a literal translation from lambda calculus. Not so bad. He could have used the X-Calculus. Also [Always relevant](http://www.willamette.edu/~fruehr/haskell/evolution.html)
I don't understand this post. Maybe it's all the Foos and Bars. Could someone explain it to me?
I will look into X calculus for further reduction, thanks!
It's using the y combinator to recursivize the function over g. So g does matter, but after the application of the combinator it goes away. The y combinator (or fix-point combinator) can be used to make functions like this: f g 0 = 1 f g 1 = 1 f g n = g (n - 1) + g (n - 2) into functions like this: f 0 = 0 f 1 = 1 f n = f (n - 1) + f (n - 2) 
Don't worry. Despite knowing in advance what he was trying to explain, I admit I also found it pretty incomprehensible. The idea is an old recurring one: parametric polymorphism imposes strong restrictions. For a type class C, a function f of type forall a. C a =&gt; a -&gt; a can only use operations defined in C's interface to compute the output from the input. It cannot vary its behavior depending on the specific type except through the type's instance of C. This means that, at least for sufficiently simple C, we can figure out how f does its business for _any_ type. The trick is to cook up a special probe instance of C, feed a value of that type to f, and then read the entrails of the return value. You can do symbolic differentiation for Num-parametric functions in a similar way. You create a Num instance that builds up an expression tree of Num operators, feed it to a function, and then apply the rules of differentiation to the symbolic return value. Here he's using the same philosophy to compute inverses functions. You can imagine trying to compute inverse functions for Num-parametric functions by solving symbolic equations a la Mathematica. That's a lot fancier than his toy example but it's in the same spirit.
see [here](http://people.cs.uu.nl/jeroen/article/combinat/index.html). But I'm not sure whether you can do it in a typed language.
[Also relevant:](http://www.madore.org/~david/programs/unlambda/#what_looks) ```s``s``sii`ki `k.*``s``s`ks ``s`k`s`ks``s``s`ks``s`k`s`kr``s`k`sikk `k``s`ksk [Edits: attempts to put links in &lt;code&gt; in markdown -- is this possible?]
Being a former IOCCC winner (and having perused some past IOCCC entries just this weekend), I certainly enjoy looking at nicely obfuscated code. But this left me feeling a bit uninterested. Obfuscations that greatly increase the amount of code are a dime a dozen and tend to leave me unsatisfied--one can always obfuscate by adding complexity. But obfuscations that reduce code to a surprising level or are pulled off under extreme constraints (such as http://www0.us.ioccc.org/2001/herrmann2.c) are truly beautiful. The unobfuscated Haskell code is already quite small: fib 0 = 0 fib 1 = 1 fib n = fib (n-1) + fib (n-2) I'm not seeing a whole lot of room for obfuscations to reduce this.
Since `u` and `R` are constructor and deconstructors for a `newtype`, one can throw them away to get the same function in the (untyped) lambda calclus (\x-&gt;f ( x x ) ) (\x-&gt; f ( x x ) ) Which is more clearly `Y f`, for those who know about the Y combinator.
Rather than arbitrarily introduce finite geometries as coordinate spaces over finite fields, one can base their definition on the familiar incidence axioms of Euclidean geometry. With that approach it turns out that the condition for a projective geometry to be expressible in coordinates over a field is equivalent to Pappus's theorem. If you already know classical projective geometry, you can get a sense of why that might be so by meditating on the relationship between Pappus's theorem and the projectively invariant cross-ratio. As the name suggests, the cross-ratio involves _ratios_ of linear magnitudes. John Stillwell develops this line of thinking in a chapter of his excellent book The Four Pillars of Geometry.
Remarkably similar to [this](http://www.reddit.com/r/haskell/comments/9jziy/lazy_functional_yak_shaving/) :-)
Have you tried to contact him? http://eivuokko.vuodatus.net/
With a better title :-)
big thanks to Malcolm for getting this online so quickly
I forgot about his blog, I just tried to email him but I got a post failure so I guess this email address isn't valid anymore?
Very awesome.
Thanks for sharing Don.
Hrmm. Do you know if a bug report has been filed? Hyperfunctions aren't that rare. http://hackage.haskell.org/packages/archive/category-extras/latest/doc/html/src/Control-Monad-Hyper.html
I don't know. I'm under the impression they they are aware of the problem but won't fix it until someone shows it being used in a "real" program.
Hey Bryan, I'm really psyched for your benchmarking library, please continue the excellent work!
I have been playing around with this one for a long time, couldn't really convince myself about the "safeness" of it. Anyone wanna give it a shot? :-)
I wonder if there's a place to just get slides? I love videos and demos, but I almost never have time to sit through one.
Not everyone has posted their slides. In fact [only a few](http://www.scribd.com/people/documents/16433628-don-stewart). I don't know if we'll get more, but I'm trying. Agreed though -- slides are easier to digest.
See "On generating unique names", Lennart Augustsson, Rittri and Dan Synek. Journal of Functional Programming 4(1):117-123, 1994. 
probably still locked up in that crappy proprietary license "for integration".
Nope, they fixed that.
oh, good!
I need to sneak into a library to read that one some day.
How about ghc on 64b Snow Leopard before wondering about the iPhone?
Screw the iPhone. Where's GHC for my G1? Oh wait, I can install Debian on it. Nevermind.
If you can install Debian on it, can you install Maemo as well?
All praise Malcolm, this is great stuff. Where else can one get one's fix of fresh SPJ presentations nowadays? :-)
And get me some GHC for 64 bit Windows while you're at it.
Where this question will get interesting is when the Apple touchscreen tablet appears. It's not hard to imagine pervasive touch tablets a generation from now, with everyone rolling their eyes that it took a couple of decades for everyone to die who clung to the notion of a keyboard. Haskell is not far from an ideal language that one could compose using gestures, with a yet-to-be-written specialized editor that understood Haskell and its libraries cold, kept frequently-used personal identifiers handy, etc. During one break from Haskell I figured out a robust system that worked for me, for removing parentheses from Scheme. (I didn't like any system out there; if you didn't either, we agree. They all seemed to have been designed by people who didn't actually write code.) That struck me as the ideal language to enter by gesture; Haskell's syntax makes for denser code (good) but makes gesturing harder. But it would be worth it, and why not be the pioneers at something that is absolutely, without a doubt, inevitable?
**[ghc-iphone-1.0/GHC-iPhone.pdf](http://upcycle.it/~blackh/iphone/ghc-iphone-1.0/GHC-iPhone.pdf)**
about 3.75
Summarising [#2965](http://hackage.haskell.org/trac/ghc/ticket/2965) * 6.10.4 needs a tiny fix to work with Snow Leopard: add -optc-m32 -opta-m32 -optl-m32 to the script /usr/bin/ghc, or wherever ghc lives on your system. If there is a problem with TH, then please make a separate ticket. * The 32-bit OS X distribution of GHC 6.12.1 will work on Snow Leopard without modification. * A 64-bit port is being worked on by various people (see e.g. #3472; we will help with the porting effort) * 64-bit OS X is, for the time being, a Tier-2 platform. That means we expect the community to support it, with guidance from GHC HQ. We don't hold up releases for it. (this is moot since there isn't even a working port at this stage, but still). However, depending on how much extra work is involved and the demand, we'll consider upgrading it to Tier-1 in the future. So the missing piece is: who is the 64 bit Mac OS X maintainer now?
And a cookie
Note that you also have to patch the haddock executable in the same way, if you want to use it.
The main obfuscation is my use of the y combinator, which reduce it to entirely non recursive functions describing a recursive relationship, which is something I would say is pretty nice and difficult to do in other languages.
Nice slides. But, does anyone here like checked exceptions really?
I didn't complain when it was just Scribd embedded in another page along with a link to the PDF, but when it's on Scribd proper I have to sign up to get a download link!
As far as I know, the only person who's successfully made a 64-build of GHC for OSX (unregistered) was Ian Lynagh. After that, Austin Seipp took over the ticket and worked on the fact that Ian's build unfortunately included an odd bug that was introduced a couple of years ago by a CPP check explicitly for OSX64 (not sure how the patch submitter ever tested that code). He then got a job and didn't have time to continue work on the build. Since then, I've tried several times, unsuccessfully (see the ticket #3472 for a full account of my failures). In principle it shouldn't be hard. We already have OSX support and 64-bit support, just separately. But I think at some point, after all the new build system improvements that re-enabled building from .hc, that system inadvertently got broken, so porting no longer works again. I've offered to be a co-maintainer to make it into a tier-1 platform, but at this point I don't have the expertise to be the sole maintainer.
Yeah but it turned into a C++ hatefest somehow :-) Here's my interpretation. 1. ZOMG are you nutz! 2. But I don't understand Haskell (whine whine) so I'll make up some crap FUD to make it sound like it's not suitable for a kernel. 3. Let's talk about C++ since we know how to bash that with FUD instead. *sigh* Makes one wonder why they bother opening their mouth at all about anything they work on when all you get is criticism from the peanut gallery that just "doesn't get it". 
Oh, you noticed that too? The lack of discussion is a little disappointing, but not unexpected. People talk about what they know (or what they are pretending to know) and not _that_ many people know Haskell or the pros and cons of GHC and its RTS.
I thought the people comparing Haskell to Java were the worst...
http://safe-tools.dsic.upv.es/mediawiki/index.php/Jose_Iborra/Papers/Exceptions
Or with less effort, you can use standalone deriving. {-# LANGUAGE StandaloneDeriving, FlexibleContexts, UndecidableInstances #-} data Mu f = Mu (f (Mu f)) deriving instance Show (f (Mu f)) â Show (Mu f) deriving instance Eq (f (Mu f)) â Eq (Mu f) deriving instance Ord (f (Mu f)) â Ord (Mu f)
At least it was getting closer to on topic - the concept of using GCed language for systems programming. I find myself wishing HALVM and GUK would become popular - resulting in exposure and a reasonably even field for comparison.
i guess i thought i'd expect more from the lwn crowd comment wise, but that didnt happen ;(
While true it really focuses on the wrong aspects of Haskell. Haskell is a bad choice for system programming because of GC and laziness but it is a great choice due to the very strong type system and separation between pure and non-pure code. Haskell will probably never be a serious system programming language but it has many features system programming languages could benefit from.
&gt; Haskell is a bad choice for system programming because of GC and laziness That might prove to be true, but it is already used for systems programming in a number of ways. There is House, the H interface, L4 Hank, and HALVM to name a few. &gt; it has many features system programming languages could benefit from. As a result there are at least two Haskell inspired languages that seek to be used for systems work. Timber and Habit.
I know it is used for some system programming already. I was just saying that it has features that improve current system programming and others that might be worse. Rejecting it for those worse ones while ignoring the improvements shows that they probably don't know Haskell well enough to judge its suitability for the task.
It sounded like a lot of people didn't know that much about Haskell, so many of them latched on to the obvious differences from their familiar code, mostly garbage collection. Not much serious pro/con discussion.
I don't think that presentation was very focused towards the biology and bioinformatics crowd. For example why bother with: * history * type system theory * comparison to other languages when you only had 45 mins. I think your presentation would have benefited from a [learn you a haskell](http://learnyouahaskell.com/chapters) type of approach with examples taken from biology.
Haskell really has some strengths when it comes to massively parallel operations. A natural place to take advantage of this is in the GPU. I look forward to using some GPU power in the future and I'm glad to see such progress is being made.
yes, I do. they're the error monad done right.
there is one reason missing from that "not haskell98", unsafeCoerce hidden deep in the bowels of typable exceptions... :(
Mostly just looking for input here. Does anyone have an opinion, or is it too minor an issue to be worth spending time on? Edit: I am already aware that many editors can transparently display a lambda for you. This doesn't mean all editors can do this, or that it's a particularly clean approach. We have unicode syntax for all our other symbols, so why not this one? It's not impossible to parse in such a way that still allows us to use the lambda in entity names if we choose. I see this primarily as an elegance and completeness "feature", rather than a (visually) aesthetic one.
Right! I wasn't looking to contradict or debate. Just converse. :-)
do you like to see lambdas or input lambdas? emacs and leksah can help with 1 and display lambdas (and there's nothing better than lambda char inside string when you want to quote something else:) about 2, I think you will need more help than any software can provide.
No
I released a library for OpenCL not too long ago in Haskell, and I would like to see it used as a target for Data Parallel Haskell. In particular, its strengths are that data parallel code written in OpenCL/C can be executed on CPUs as well as GPUs. There are production drivers for AMD Athlon and nVidia Teslas already as well as Apple's machines shipping by default now with OpenCL on board and connected to the Core2 Duo and nVidia 9600 series chipsets. OpenCL's architecture level control functions (those things that look very much like OpenGL functions) also support and encourage out of order program execution via their command queue control structures. The OpenCL command set is relatively small and the language is relatively complete, moreso than CUDA/C, and better documented. nVidia is also fully on board supporting OpenCL. OpenCL could even be implemented on top of MPI and thus provide a higher level abstraction for issuing data parallel computations than the geriatric CL. In short, I seriously think we should move to this.
Hi Daniel (Austin S. here!) I'm currently the owner of the trac ticket for 64bit compilation on Mac OS X. The current state of play is that while GHC HEAD currently has re-enabled the porting code, it bitrots *very* fast because nobody uses it that much, and since its inclusion there have already been fundamental big changes to the compiler: the linux shared library support, the exorcism of GMP from the compiler, etc. etc.. This complicates the porting process a lot. I've found some time here and there to get my GHC tree up to a recent state and try the porting a little, but I inadvertently get pulled away most of these times (unfortunately.) But I haven't forgotten about it, and I'm very keen on trying to get things up to speed as fast as possible. (It's mainly just that I need to pencil these activities into my schedule on a non ad-hoc basis.) On this note, I have just recently upgraded to Snow Leopard and I think it's quite fantastic. Given the EOL support for previous versions os OS X by Apple and the high adoption rates amongst users, I think it would probably be best to mostly concentrate the efforts there, and I've set up my own 64bit ubuntu box to use as the host in the port. In fact, after an official 64bit port to Snow Leopard (when **we all** do it!) I have plans for other things in GHC-OSX-land, including A) getting shared libraries working on OS X, and B) looking into integrating fun stuff like Grand Central Dispatch/OpenCL into the compiler. Daniel, if you would like to help accelerate GHC and make it Tier-1 on OS X 64bit, I would be very happy to be a co-maintainer with you (two brains are better than one!) If we want to do this though, we should *really* schedule a dedicated time to do work on these things, particularly I think we should: * Find a good time to talk to the GHC Team about our plans and notify that we'd be happy to take OS X 64bit to Tier-1, and maintain it. * Find time to bug GHC HQ a lot ;) * Find some time to hack on code Like I said, I'm confident I have the time to help GHC, it's just that I need to schedule the work into my planner. :) Unfortunately, the window for getting things into the 6.12.1 RC is pretty much closed I think. But I think it should be quite possible to get support for 64bit snow leopard into 6.12.2 or something perhaps - we'll need to talk to GHC HQ about that.
I wouldn't be opposed to being able to make my code unicode pretty, but I've never much cared for unicode input methods, so I probably wouldn't use it much. I guess I'm neutral.
Probably not.
Sounds good to me. Have you checked that ticket I submitted?
It doesn't make any sense to talk about a 32 bit versus a 64 bit OS X maintainer. Mac users have a rather different OS upgrade pattern than other platforms â rate of adoption is **much** faster. Moreover, Snow Leopard (on which 64 bit is the default) is dead cheap. As a result, once GHC works with 64 bit on OS X, there will soon be very little demand for 32 bit. (BTW, Apple has also written PPC off â SL only runs on Intel processors.)
Ok, so who is the Mac OS X maintainer now? :-)
I agree that OpenCL will probably be more interesting than CUDA in the longer term. However, we already have code for a code generator targeting CUDA (and none for OpenCL), so I think it makes sense for us (= the PLS group @ UNSW) to first get a CUDA backend out of the door âso that people can actually use the library to write GPU codeâ and then look at OpenCL. Unless somebody else wants to have a go at OpenCL, that is.
While you may be correct that the adoption rate for OS X upgrades is much faster, there is the need to consider those with PowerPC platforms that will be stuck with 10.5. I suppose they will eventually upgrade, but I wouldn't expect them too just for Snow Leopard and GHC.
Haskell mode for Emacs can do this.
It wouldn't be too great a step toward Church orthodoxy, since you'd be replacing `\x -&gt; x*x` with `Î»x -&gt; x*x` where Church would write `Î»x(x*x)` or `Î»x.x*x `. The faux arrow `-&gt;` is pretty vulgar I would rather follow Church's master, Frege, who used the smooth breathing over greek vowels -- a 'combining sign' as the unicode people put it, rather than an extra symbol, like an umlaut. Church's update of Fregean function abstraction gives a more "Principia Mathematica" look to it all (he claimed to have adopted the Î» by a process of 'eeny meeny mieny moe..") A re-Fregeanized back-to-basics Unicode Haskell would look like so: square = á¼(Îµ*Îµ) I can't quite remember, but some of Frege's actual definitions include something like, false = á¼(Îµ/=Îµ) true = á¼(Îµ=Îµ) -- not sure if these are the right defs. for his two favorite objects "the True" and "the False" ("If I had discovered two new chemical elements I would have been given every scientific prize; as it is I only discovered the True and the False..." -- quotation from memory, probably botched.) Unlike Boole he construed predicates as expressing functions that map arguments to the True and the False, and thus in a lambda-ish language like Haskell, the truth values should of course be called the Fregeans. The only trouble with the smooth breathing in our imaginary unicode Haskell is that there's only a certain number of greek vowels. Also, Frege unfortunately used an operator to express application, and shamelessly puts the argument on the left. So for the example, the theorem of *Grundgesetze* (1893) that expresses the fact that the 'double value range' (i.e. two-variable lambda abstract) curries a function of two arguments is expressed f (a,b) = aâ(bâ á¼á¼(f(Îµ,Î±)) -- here I have the exact phrasing rather than the more Churchy-Haskelly f(a,b) = á¼á¼(f(Îµ,Î±) b a 
I am not saying that we should discontinue support for PowerPC and/or 32 bit, but soon enough (probably while 6.12.x is the current stable branch) the majority of GHC users on OS X will run Snow Leopard â i.e., that's were our focus should be.
According to http://hackage.haskell.org/trac/ghc/wiki/Contributors it's Wolfgang Thaller and myself. I don't think Wolfgang is active anymore. I just upgraded to Snow Leopard; so, I have no choice, but to make sure GHC runs on it (i.e., the build system properly builds a 32 bit binary by default). As for 64 bit, I'll look at this eventually âif nobody else gets it working firstâ but at the moment, I rather fix the most pressing outstanding type family tickets first. (In practice, Roman fixes OS X issues, too, if he get's sufficiently annoyed by them. Maybe he should hence be added to the platform contributors.)
I agree, I was just saying that some people will still use their old PowerPC platforms.
emacs can be programmed to replace input text with other text. It wouldn't be especially difficult to make \ input a lambda in certain contexts.
That seemed so simple that I assumed that the poster had already tried it and found that it didn't work. I guess that standalone deriving allowing you to provide the instance context is the magic?
This is a good thing. The use of `\` for `Î»` is a blight.
I guess you haven't tried agda-mode yet.
Yeah. With standalone deriving you can make the circular dependency, but then that circular dependency requires UndecidableInstances (because it has an instance body that is more complicated than the head, and FlexibleContexts, because the instance body is non-trivial, both of which contribute to why newtype deriving doesn't try to find those instances in the first place.
I think that the backslash is much easier on the eye than a lambda, so in my opinion it's a lucky abstraction (interestingly, there is quite a few examples of *a posteriori* lucky notations/names in mathematics). I also find the Church notation pretty much unreadable, so apart from nostalgia-heavy compsci graduates, I cannot imagine why on earth would anyone want to use the lambda... That said, I like the idea of unicode operators in general (in moderation, with ascii backup solutions, and only when it feels natural, eg. in mathematics-oriented code). EDIT: on a second thought, I really really don't want a (much used) greek letter to be a reserved keyword. When using Maple, I always cry when I want to use gamma and it doesn't allow me to do that, because some stupid engineer thought that Euler's constant is *that* important...
just pushed experimental support for stack traces on exceptions
http://github.com/pepeiborra/control-monad-exception/commit/c7e837d28bcc873eb3cc4bf17afada4c81560273
It isn't impossible, but I think it's a bad idea to complicate lexing for such small payoff. Your suggestion on the linked page to allow use of mathematical lambdas, but not the alphabetic one, is quite likely to lead to confusion. And if you want to provide sane error messages to people who look at code using a math.lambda and try to use alph.lambda in their own code, you still have to complicate lexing -- for even smaller payoff!
A problem with Unicode symbols in source code is that all fonts don't have glyphs for them so you might end up with source code that looks like tofu (those white boxes) to others. Not to mention that I can't easily write them on my keyboard.
Checked exceptions suck in Java because its type system is too weak to express the proper exception types. For example, it's almost impossible to do this in Java: . :: (b -&gt; c throws x) -&gt; (a -&gt; b throws y) -&gt; (a -&gt; c throws x + y) It's possible only if x and y share a common supertype z, then then composition throws z, but this is a kind of hack because nominal typing isn't extensible.
I work with chak at unsw, really awesome guy.
The point in the ticket is that it wouldn't be reserved. It would be unambiguously detected as an anonymous function in one context and as a character in a name in another context. This would complicate lexing/parsing but would add no limitations to the programmer.
Please, no, it's a horrible idea.
Care to elaborate?
Is it too late to ask for a pony?
It would be cool if Haskell adapted some of the ideas of [Fortress](http://projectfortress.sun.com/Projects/Community/wiki/MathSyntaxInFortress) regarding that.
It's *never* too late for a pony.
&gt; It's not impossible to parse in such a way that still allows us to use the lambda in entity names if we choose. So you want to be able to write the identity function as `Î»Î»-&gt;Î»`? Or combined with view patterns: `Î»(Î»-&gt;Î»)Î»-&gt;Î» Î»`.
I don't see why it should be impossible :) for what it's worth, I don't see why we can't make :: forall forall. forall -&gt; forall parse nicely too. Maybe I just don't believe in reserved words :P
Ah, ok. Sorry, somehow I thought this is a self-post, and didn't click on the link. Still, my personal opinion is that the backslash is much easier to read than a lambda.
I dislike the idea of a letter being reinterpreted as punctuation in certain contexts. Imagine if ax was a valid variable name but lx wasn't? Parsing only the "mathematical" lambdas is a much better solution.
&gt; This would complicate lexing/parsing I'll add another vote for alexeyr's position, "it's a bad idea to complicate lexing for such small payoff." I don't see anything wrong with handling this at the presentation level. 
Really glad to hear this dons. I must say that Data.Binary is probably one of my favourite haskell libraries, just because it makes something that's often somewhat difficult trivially easy. I've made great use of it in my TernaryTrees package, which has a very efficient Binary instance, that often allows me to compress things like wordlists, without using any actual compression.
In a way, you're right.. Do you know if the debian userspace has GHC ?
Note you can already use true unicode arrowsââ â in GHC. Prelude&gt; :set -XUnicodeSyntax Prelude&gt; (\x â x*x) 2 4 Not as nice as Î»x.x*x though. Perhaps we need a unicode . for â
There are some things that bother me about Data.Binary: * PutM seems silly to me, why is it a Monad and not a Monoid? Just for symmetry with the "do" syntax of get? * The lack of error handling (Pure exceptions are real bad. Throwing a MaybeT in there could be nicer) And ideally, I'd like a more declarative infrastructure where composing primitive parsers would build a parser, not having to specify both get and put.
Most of the time, having both separated is not a problem, since you can generate the code (using the Derive package)
Very cool, but what about the correct enharmonics? C# major, for instance, is not spelled *C C#* D# *F F#* G# A#.
My knowledge of music theory is basic at best. Concrete suggestions and/or improvements are welcome. :-)
How does he generate those pictures?
Enharmonics are two names for the same note: Câ¯ and Dâ­ both describe the same note. The basic rule for a scale is you don't want to have the same letter in there twice, so enharmonics are chosen to address that. So, Câ¯ major has these notes: Câ¯ Dâ¯ F Fâ¯ Gâ¯ Aâ¯ C But to avoid the duplicate letters, it's usually spelled: Câ¯ Dâ¯ Eâ¯ Fâ¯ Gâ¯ Aâ¯ Bâ¯ (Note that, because there's only a half step between E and F, Eâ¯ and F are enharmonic.
Hehe, whats really funny is that my better half asked me the same question... 
&gt; Just for symmetry with the "do" syntax of get? Yes. For the syntax. &gt; The lack of error handling (Pure exceptions are real bad. Throwing a MaybeT in there could be nicer) Yep. How to do this lazily is an interesting problem. We used ErrorT for a while, but sacrificing stream-based parsing didn't seem worth it. &gt; And ideally, I'd like a more declarative infrastructure where composing primitive parsers would build a parser, not having to specify both get and put. This I don't understand. Are you referring to the Binary class having both encode/decode? You are free to use Get and Put separately.
Ok, so currently chromatic = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"] which is nice because you have a bijection between chromatic and Z mod 12, so given a Note you always know the NoteName. But it would be better for the purposes of representing the scales if you could also represent sharps enharmonically as flats. C# major, for instance, becomes C# D# E# F# G# A# B#. Note (sorry) that since there's a half step between E and F, and between B and C, E#=F and B#=C. Adding flats would also allow you to give a more natural representation of a scale like F major, read as F G A Bb C D E. Instead of a bijection between Notes and NoteNames, you'll have one between Notes and the elements of a partition on NoteNames. I believe the only rules you'll need to determine the correct NoteName for a Note are "don't give two notes the same letter value," and "don't mix sharps and flats within a scale." For some scales, like the harmonic minor, you'll need double sharps or flats. Another thing you might want to implement is ordering the scales by root. Currently the scale finder gives G major as C D E F# G A B. This is really C Lydian, the scale that you get when you start G major on the 4th scale degree. Edit: a bunch of folks have replied in the blog's comment thread with the same points, and better put.
An upvote for you, sir, for introducing me to Shellac.
It really is rather awesome, yes. :-)
Aha, thankyou: lightbulb moment. I hadn't got that you don't want the same letter in there twice. From a performance POV, I can see why that would be the case. Thanks!
&gt; Yep. How to do this lazily is an interesting problem. We used ErrorT for a while, but sacrificing stream-based parsing didn't seem worth it. Can you explain why ErrorT causes a problem here? Does it add extra laziness? Is it possible to have a strict variant of ErrorT or something else to prevent this? &gt; This I don't understand. Are you referring to the Binary class having both encode/decode? You are free to use Get and Put separately. I'm basically wishing for Pickler combinators (ones that allow full control of "on-the-wire" formats).
ErrorT strictifies our lazy code, by forcing a check of the entire stream for failures. So we take the same approach as lazy IO -- return errors asynchronously. We ideally need both points in the design space, however. &gt; basically wishing for Pickler combinators Why aren't Get and Put suitable for on-the-wire formats? That's what they were designed for, and used for.
What is really nice about the picklers from, for example, HXT is that the put and get are always written as once piece. Combining the picklers always means combining both the parser and printer at once, this way it is almost impossible to accidentally produce non-isomorphisms. Using the xpWrap (bidirectional fmap) you can change the output semantics by giving two functions, one from a -&gt; b and one from b -&gt; a. (This is also what fclabels does for labels and lenses.)
Or derive them generically on-the-fly using regular-binary. Thanks Don for this excellent package and for still finding ways to improve the performance!
Thanks, that's what I meant.
Paul Hudak is currently teaching a couse on Computer Music in Haskell at Yale: http://plucky.cs.yale.edu/cs432/index.htm He uses his own book "The Haskell School of Music" (only a draft currently). Which looks like The Haskell School of Expression, just modified to use the Euterpea (the replacement for Haskore) as his running example instead of animations. I'm following a course that runs concurrently with the same topic at the Technical University of Denmark. (DTU). But we aren't really started on the programming yet, we're just looking into the libraries and tools (Euterpea, CSound, PureData, ..)
ocaml mode in emacs does just that. It looks like a lambda when you read the code, but when the cursor is in or the expression it reverts to an ascii '\' .
You can use the Builder monoid. Put is just a writer monad over the Builder monoid.
An upvote for you for using the sharp symbol instead of the octothorp.
It seems weird to me that the Put monad is created merely for syntactical sugar.. do { .. .. } instead of: mconcat [ .., .. ]
Thanks for the pointer to Euterpea. I hadn't seen it, but have had some (small) dealings with Haskore in the past, ie trying to get it to run! ( http://gimbo.org.uk/blog/2007/04/13/installing_haskore_under_freebsd/ ). It seemed a bit bitrotty at the time, but maybe this explains why. :-)
Well, it was a gut reaction, but basically non-ASCII characters in program code are a pain. The riposte is "use a decent editor", but there are just so many programs in the chain involved in editing and displaying program text that I don't want to have to worry about whether they all understand some particular Unicode characters.
&gt; How to do this lazily is an interesting problem. I've come to the conclusion that the best way to mix error handling and lazyness is to put the errors into the returned structure. It does mean the structure has to be able to represent the errors. Lazy parsers have probably also solved this problem.
But we already support unicode syntax and names. The only thing the proposal is suggesting is to add one more unicode character for syntax (as an option, of course).
I think you wanted this instead: f g 0 = 1 f g 1 = 1 f g n = g (n-1) + g (n-2) Then "fix f" would be equivalent to your second definition.
As far as I can see, this page doesn't say who the author of that backend is.
The SVN commits are under the username ``MaxTaldykin``. The most recent commit is from a year ago, and many files have not been touched for over two years.
Dmitry Golubovsky's work has been far [more visible](http://haskell.org/haskellwiki/Haskell_in_web_browser).
Thanks :-)
Thanks, woops.
sounds good. did anyone use it to make a web-site or something?
http://functionaljava.org/ &lt;A, B, C&gt; F&lt;A, Either&lt;C, Either&lt;X, Y&gt;&gt;&gt; z(F&lt;B, Either&lt;C, X&gt;&gt; f, F&lt;A, Either&lt;B, Y&gt;&gt; g) Yes I am mostly joking. 
Solution: don't use visible monad transformer stacks. Monad transformer stacks are good for defining custom monads wrapped in a newtype, not good for application code.
Therein lies one of Haskell's adoption problems. I am an aspiring haskell programmer. I have no idea what you just said :(
He's saying that the approach used by the OP is exposing too many implementation details in the interface, which is why the interface is awkward.
&gt; Therein lies one of Haskell's adoption problems. Every language has its jargon. "Virtual inheritance is problematic because library authors must anticipate diamond inheritance and provide an appropriate virtual base class constructor."
I too am just learning, and I feel this is one of the reasons I'm so interested in it: it's a challenge, something new that requires work to learn and understand... I Love it
Don't worry about it too much. There's this tendency among new Haskellers to think that monads are essential and more, that monads are a good thing. Then they think, if monads are a good thing then monad transformers must be even better!!!11 Then they write horrible code using lift . lift . lift.
That might be so, but having features that are difficullt to use just pose a challenge to your users doesnt seem like a good design strategy.
The thing we're talking about here isn't a language feature, or even a feature of the standard libraries. It's the misuse of an advanced design pattern by someone who does not understand when it's sensible to use.
Using monads and monad transformer to simulate state is definitly a feature. And in my mind a failed feature. Every time some one defends them they sound like they have the stockholm syndrome.
Real World Haskell, nao. You'd understand this roughly after the first half. (Or go through All about Monads, but that's less hands-on imho.)
Wow, I must have been sleeping. This is the first time I heard about CHP and it looks awesome. :)
&gt;Our time complexity is therefore O(N^2) overall, but importantly if you had N processors (and very fast communications), each process would run O(N) operations in parallel, and thus you would have an O(N) sorting algorithm. What if N &gt;&gt; number of processors? Wouldn't you be closer to O(n^2) then? &gt;While our theoretical bound is good, in practice the sort pump is only going to stand a chance of good performance if you have very few items that are very expensive to compare (so that the computation â i.e. the comparisons â outweigh the communications). Why not make the compare operation parallel then? For example, if you are comparing very complex structures why not distribute the compare operations i.e. breakup the structure into smaller parts than can be compared independently.
Monads and monad transformers are not about simulating state. There are some that do, but if you've been told that's what they're for, someone lied. :) They're also not a feature. Well, do-notation is supported at the compiler level, but it's just syntax sugar. You don't need compiler support to write monads. You do need a certain amount of support for polymorphism in order to be able to write code that works in any monad, but that's a more general thing which has nothing to do with supporting monads specifically. From a programmer's perspective, a monad is just a library with a datatype that has a few specific types of functions as part of the API. The reason we pick out such things is because we realised that a lot of useful functions can be written to work in any monad. So by recognising that something is a monad, we get a bunch of libraries of functions (Control.Monad, Data.Traversable, the monad transformer libraries, etc.) to work with it, and not have to write those functions separately for each library. From the viewpoint of someone just trying to get their programs written, this is the *only* reason to recognise that something is a monad, and it is a good one. It saves quite a bit of trouble and code. Monad transformers are an interesting device: they transform these monadic libraries into other monadic libraries in a uniform way. They're useful if you're writing a new library, and realise that there's a monad at the heart of it which decomposes nicely into existing parts. For instance, a monad which is useful for analysing games of chance is a nondeterminism monad (like the list monad, or logic monad), transformed by WriterT (Product Rational), which will attach rational numbers to each of the possibilities, and multiply them when multiple choices are made. The natural way to interpret them then is by treating them as probabilities. You can make a bunch of choices, each of which can depend on the values of choices made previously, and which has probabilities attached to each choice, and then compute some result in the end, and the monad will take care of computing the probability of each possible result. Hopefully that doesn't sound like Stockholm syndrome, but as something you might actually be interested in using. :) Of course, the interesting part is not the probability library itself, but the way in which we constructed it from existing components. The definition of this monad in Haskell is only a handful of lines (maybe ~10, including a few things necessary to make things pretty), and we get the leverage of being able to use hundreds of lines of definitions from the monad libraries for free. Don't get me wrong, we're not always so lucky, and the example I chose is one where things work out easily, but when things work, they tend to work really well. There are more monads than those reachable from monad transformers, and more combinator libraries than just monads, but the more structure you can pick out, the better things go.
[It does.](http://packages.debian.org/sid/ghc6)
The State monad (wich was used in the example) is definitly about simulating state. but yes I should have written "effect" rather than "state". I use monads a lot. Thats why I complain about them. Mistaking all critisism as ignorance really isnt productive.
Some C equivalent of what he just said is "remember to properly align your pointers on the stack to minimize level-1 cache misses" or some other advanced jargon. In other words, something that a beginner doesn't have to worry about too much. (No, sorry, I'm not entirely sure if what I just said about cache misses is right or not. I made it up on the spot).
&gt;I use monads a lot Does not sound like you master them yet. 
Translation into OOP-speak: Use a encapsulation to hide the internals of a complicated part of the code from the rest of program.
Unless you've grown too large to effectively ride the pony without hurting it. Then you should reconsider... it wouldn't be good for the pony.
Ryan fixed that one. It had meant to be removed before it was put out publicly, while they were passing it around internally; they just forgot.
Get a better pony. Ponies tend to be pretty solid animals, you'd have to be badly overweight to faze a Shetland.
The average weight of a Shetland pony is 600-650lbs for a classic type Shetland of 11h-11'2h. (Or 400-450 for the more typical breed available today.) http://wiki.answers.com/Q/What_is_the_average_weight_of_a_full_grown_shetland_pony There is a good rule of thumb for rider weight being no more than 20% of the weight of the animal, which would say 120-130 lbs is the ideal upper bound on it's rider's weight. But heavy trail riding horses might be able to handle 30% so 180-195 is pretty much the upper limit of a strong classical Shetland. http://www.gaitedhorses.net/Articles/HRiderGuide.shtml I on the other hand would be a bit over 40% of its bodyweight and would likely drive the thing into the ground after breaking its back. ;) No O_PONIES for me.
Bravo!
What is this power of the copy-pasted letter Î» which I do not know how to type that one would be wishing to harness?
&gt;Haskell is not far from an ideal language that one could compose using gestures, with a yet-to-be-written specialized editor that understood Haskell and its libraries cold, kept frequently-used personal identifiers handy, etc. The fact is that that is way more complicated to implement and not, I think, superior. Durn kids and yer anti-tactilism.
Very interesting reading on the blog about where the HWN is heading. Should be on Planet Haskell. And I'm happy to see publish.hs is getting a work over after all these years.
Plug: my paper "Bidirectionalization for Free!" (http://wwwtcs.inf.tu-dresden.de/~voigt/popl09-2.pdf) pulls the same trick for bidirectional programming. I hope it is quite accessible both in terms of the way parametric polymorphism gives insight into function behavior, and in terms of how we then can use such knowledge for inversion. 
It's that "not the people in this room" that's important. Go further: not people who have any need or desire to _be like_ the people in this room either. Working programmers don't want to be like Computer Science researchers. We just don't. From my limited investigation Haskell has some nice Software Engineering properties. Someânot all of its properties are nice, it doesn't have all the nice properties. But they are almost impossible to get at through the fog of Computer Science. I found _Real World Haskell_ to be a good book but not a great one, because it still has that smell of evangelism about it. It still wants to make me a better person, or lead me towards enlightenment, or some such. That's a huge turn-off for me. I'd add a target book to O'Sullivan's list: _Choosing from the Haskell Library_. When I've played around with Haskell for anything that's not got a striking resemblance to a algorithm course homework problem (at which it excels) I've wanted to use some library functions to help: this has turned out to be excruciatingly hard because they are not, so far as I can see, described in operational terms that would help one know what they're good for, and what few examples of usage are given are far, far too complicated and clever. For example, I've yet to find an tutorial for using the State monad that doesn't launch into some clever clever thing, usually involving StdGen. Well, yes, fine, nice that we can generate more than one pseudo-random sequence in a pure language. But that's too clever an example for me to use to learn how to use the State monad. What I want is a very simple, plodding example of using the State monad to do a simple, plodding thing. Not even the sort of thing that's _best_ done with the State monad, but a simple, simple thing that's good to learn from. An accumulator, say. How do I write a function that returns how may times it's been called using the State monad to keep it clean? I've tried, and I can't get it type-correct and I can't find an example of anything so trivial to help me. And so I turn away from Haskell, because I only have so much of my life left.
In lieu of a link to that kind of tutorial material - which I agree would be best - I can strongly recommend the #haskell IRC channel on irc.freenode.net. There's also [the Haskell beginners mailing list](http://www.haskell.org/mailman/listinfo/beginners).
http://conal.net/blog/posts/semantic-editor-combinators/
[the haskell weekly news](http://sequence.complete.org/hwn/20090905) has been talking about [lenses](http://hackage.haskell.org/package/lenses-0.1.2). it looks like what I need.
Clearly the System.Random class needs an overhaul. I don't just mean the slow StdGen implementation, but the class too. These new rng implementations do not fit the class interface. There's two reasons for that: * They do not support the split operation * They are impure My preferred fix is to: * Break the random class into two, with split in a subclass. * Bludgeon the new implementations until they are pure. The latter may be hard with the mersenne-randome one because it uses horrible C code with global variables.
Instead of working with the RandomGen class, I find MonadRandom to be a lot cleaner. Perhaps we should promote the monadic (or applicative) style. Different random generators can use different monads: the ST monad, a pure state monad or a reader monad using split. I think a type class like this could work: class Monad m =&gt; MonadRandom a m where random :: m a 
My natural tendency is to think the pure interface is the better primitive and if you want to use that in a monad then that's fine. Note it's possible to have a generator like Mersenne that uses mutable state in the ST monad and generate pure lazy results. See for example Lennart's code: http://www.augustsson.net/Darcs/MT/MersenneTwister.hs
My experience with random numbers in Haskell is with generating random values of, sometimes complex, data structures. I usually end up wrapping Random in a monad anyway. Compare: data Box = Box { x,y :: Int, color :: Color } randomBox :: StdGen -&gt; Box randomBox g = Box a b (random g2) where (g1,g2) = split g (a:b:_) = randoms g1 randomBox' :: Rand Box randomBox' = Box &lt;$&gt; random &lt;*&gt; random &lt;*&gt; random I find `Rand a` especially natural to think about as "a random value of type `a`". This is better than `SomeGen -&gt; a`, which is "a function that generates an `a` using some random number generator".
Hackage has [data-accessor](http://hackage.haskell.org/package/data-accessor-0.2.0.2), and [data-accessor-template](http://hackage.haskell.org/cgi-bin/hackage-scripts/package/data-accessor-template). The latter has Template Haskell to automatically generate the necessary accessors. I've used them on a few projects that had a lot of record data types, they work well. edit: you think I'd know markdown, as a gitit user.
I'm not disputing that it's a nice programming style. I'm talking about what is the right common primitive interface that RNGs should be required to provide. You can of course wrap those in your applicative functor.
Type classes are definitely the most awesome feature of Haskell. I don't know why they don't get enough credit.
I agree, the lack of clear introductory material (and well-documented libraries) is our #1 problem in the Haskell community. Our platform is still in its adolescence and is struggling with cleaning its room and doing its chores on time. HOWEVER, please believe me when I say that there are practical and concrete reasons why so many of us are so psyched about Haskell and are willing to spend our time evangelizing it and working to improve its ecosystem: in a lot of respects it's qualitatively *better* than the alternatives. Haskell programs can be nearly as fast as C and purity/referential transparency + QuickCheck means that Haskell programs are *very* easy to test.
&gt;Our platform is still in its adolescence and is struggling with cleaning its room and doing its chores on time. Your platform is nearly non-existent. I asked SPJ about this at a conference and he just smiled his movie-star smile and said "use emacs". If anyone wants Haskell to be successful with more than the quants in the IBanks then that response and that style of response has to go. Practitioners have moved far, far beyond the state whereby we think of programs as really complicated text files for which we need really good text editors. &gt;please believe me when I say that there are practical and concrete reasons why so many of us are so psyched about Haskell I do believe it, otherwise I wouldn't even bother looking at Haskell. Please believe me when I say that there are practical and concrete reasons why there are so many of us who don't have the time, inclination nor energy to overcome the barriers to learning Haskell. In a lot of respects it's quantitatively far too high an investment to have to make. 
[fclabels](http://hackage.haskell.org/package/fclabels) is another package like data-accessor that helps with this. It also has TH generation, and very useful Category and Applicative instances.
The current class also uses Integer conversions everywhere. So even if you do plug in a fast seeder, the instances are terrible. Also, regarding mersenne-twister, user the pure implementation. It's still very fast.
Related: [Multi-stage programming with functors and monads](http://www.cas.mcmaster.ca/~carette/publications/scp_metamonads.pdf)
Those are pretty much functional references... If (part -&gt; whole -&gt; whole, whole -&gt; part) are the (setter, getter) functional-ref pair, then a lens, as I read, is: (whole -&gt; (part -&gt; whole, part)) which is isomorphic and pretty much the same, up to laziness/sharing considerations...
+1 for lenses
&gt; Your platform is nearly non-existent. I asked SPJ about this at a conference and he just smiled his movie-star smile and said "use emacs". Sorry, I'm a bit lost. The Haskell Platform is a new initiative (which hasn't yet progressed very far) which is about shipping a compiler bundled with some additional standard libraries which have been reviewed and recommended by other developers - "batteries included" if you like. In some other popular systems with which I'm familiar, like Java and .NET, "the platform" refers to the runtime environment and standard libraries. So when you say "your platform", I think "libraries". But given that Simon's answer was "use emacs", I'm not sure I've understood what you mean by "your platform". Did you actually mean libraries and Simon misunderstood your question? Or did you mean something else, like integrated development environments (IDEs)?
As I've said on #haskell, I concur about moving split to a subclass. It's important when it's important, but it's an embarrassing wart for someone just whacking something together without a care. As for mersenne-random, I'm sure that could be bludgeoned into getting rid of C global variables.
I've started using lenses, and it is perfect. The thing I appreciate the most is that it is using only the "." operator. The only thing I find weird is the order of the function arguments. fetch :: data -&gt; lense -&gt; value update :: data -&gt; lense -&gt; value -&gt; data alter :: lense -&gt; (value -&gt; value) -&gt; data -&gt; data I don't know why the author didn't choose the same order for all functions : fetch :: lense -&gt; data -&gt; value update :: lense -&gt; value -&gt; data -&gt; data alter :: lense -&gt; (value -&gt; value) -&gt; data -&gt; data this order allows to compose updates and alters, which is pretty convenient. but in the end, it's only a matter of defining helper functions for fetch and update ...
Are these lenses first class? In other words, can you compose them using the (.) pass them around and then still use the getter/setter/modifier on this composed lens? I cannot really get this from the documentation.
yes, you can compose lenses with (.) and use them with the accessors functions provided by the package. the resulting code is really clean, it's a pleasure to write.
If you are looking for an ide, there is leksah.
I'm also rather partial to the fclabels implementation.
The main thing that prevents me from using Data.Binary more is the instance for lists, which can be a bit of a big when serializing long lists that otherwise could fuse away. I find myself needing to use a newtype wrapper a lot to build a version that chunks up the list or uses a byte per entry to encode the constructor the naive way.
The Haskell platform has nothing to do with an IDE. Furthermore, what's wrong with using Emacs? I know many practitioners that use Emacs or vi. Of course, with bad languages you need and IDE to make them bearable.
I didn't know that "The Haskell Platform" was a name for a particular thing, which won't have helped. By "platform" in this case I mean the entire ecosystem of tools and runtime and integration and so forth. 
new semester starts next week (at least here), so those heathe^H^H^H^H normal people try to brace themeselves for many months of those crazy academics ramming lambdas down their impure throats.
The standard deviation of subscriptions per day is definitely up, but is the mean all that much bigger? Hard to say from the look of that graph
 let x = foo `fetch` (a . b . c) in foo `update` (a . b . c) (x + 1) or alter (a . b . c) (+1) foo I think they are to be used infix. `alter` doesn't make much sense infix since it really needs another argument along with the lense.
It plays especially nicely if you're using the State monad and MonadState.
&gt; ramming lambdas down their impure throats. Sexual Harassment charges. That is all. Actually, that's gonna go in the quote pool for this week... :)
They heard I'm doing the HWN now, so everyone is coming to pick on me when I screw up the line-endings... :)
Something related to post-ICFP activity maybe?
Because we're badass!
You just made my day! I wrote lenses and I'm glad to hear other people are actually finding it useful too! sw17ch was correct that the reason for the inconsistency with the order of function arguments was to make infix use nice. I didn't notice that the infix form killed composeability. I might have to switch the order to the way you suggest. Also, if you find yourself composing a lot of updates and alters you maybe find 'execIn' to be more convenient. (you don't need fetch, et al, in the state monad) If you have any other issues or suggestions, you can reach me via my github repository for the package at: http://github.com/jvranish/Lenses I'm glad your finding it useful! :D
definitely the semester starting
Simple: smart people running away from proggit's downfall of quality links
I suspect generating the x and y coordinates separately would be even cleaner... line = zip [x1..x2] (steps y1 slope)
Neat. Is there any way to synchronize this file between two computers? All it would really take is the ability to specify the location as in my [Dropbox](http://getdropbox.com).
Also, Bresenham's algorithm is a classic example of a coroutine. Last time I implemented it was in Python years ago and I remember using generators. In Haskell I would probably use unfoldr.
Nice idea.
Yes, you just add a symlink into the dropbox as described [here](http://lifehacker.com/5154698/sync-files-and-folders-outside-your-my-dropbox-folder). Probably a ln -s ~/.todo.txt ~/Dropbox is sufficient
Darcs is one option.
No it's not. I can't specify the file location to be within a DVCS repository, and I'm certainly not going to make my entire home folder a DVCS repository. That said, I'm going to see if the Dropbox symlink thing actually works.
Well, we have quite a lot. We have an actively-developed IDE (leksah), interop with various languages, HTTP/HTML/XML/JSON libraries, lexer and parser generators (some of which generate Haskell code from an input file, such as Happy, and some of which are parser combinator libraries, such as Parsec), integration with various widget toolkits and graphics libraries, profiling support, debugging support, code generation support, hundreds of other libraries for various purposes on Hackage... and of course, a Haskell mode for emacs. So no, our platform is not "nearly non-existent"!
I was a little surprised at first, but it sure is: bres run rise | run &lt; 0 = [(-x, y) | (x, y) &lt;- bres (-run) rise] | rise &lt; 0 = [(x, -y) | (x, y) &lt;- bres run (-rise)] | rise &gt; run = [(x, y) | (y, x) &lt;- bres rise run ] | otherwise = zip [0..run] . map fst $ iterate step (0, run `div` 2) where step (y, error) | error' &lt; 0 = (y + 1, error' + run) | otherwise = (y, error') where error' = error - rise line (x1, y1) (x2, y2) = [(x1+x, y1+y) | (x, y) &lt;- bres (x2-x1) (y2-y1)] *Main&gt; bres 9 3 [(0,0),(1,0),(2,1),(3,1),(4,1),(5,2),(6,2),(7,2),(8,3),(9,3)] *Main&gt; line (1,7) (9,4) [(1,7),(2,7),(3,6),(4,6),(5,6),(6,5),(7,5),(8,4),(9,4)] 
&gt;No it's not. I can't specify the file location to be within a DVCS repository, and I'm certainly not going to make my entire home folder a DVCS repository. Behold a slice of my home dir: [vegai@louhikko ~]$ ls -alh|grep darcs/ lrwxrwxrwx 1 vegai users 15 2009-05-18 13:07 org -&gt; darcs/docs/org/ 
Sure, that works fine for folders, but will it work for files? A lot of programs like to write files "atomically", e.g. they write the new contents of the file in a new location, then swap it with the old location, so that way if a problem occurs during the writing it hasn't corrupted the old version of the file. I have no idea of hsgit bothers to do this, but it's certainly possible. In any case, manual synchronization (which is what a DVCS is) is not appropriate for a todo list. If I forget to synchronize my changes while I'm at home, then I don't have access to them when I'm on my laptop. And adding an extra "push" step to every edit of my todo list will virtually guarantee I don't maintain the list.
So, the author wants org-mode without knowing it ;)
&gt; Also, regarding mersenne-twister, use the pure implementation. I've just switched some code I'm working on to use this (mersenne-random-pure64), it's great, thanks! One limitation I found is that afaict, it doesn't support serialization of the generator state, which I rely on. Both System.Random.StdGen and GSL.Random.Gen support that. So, I implemented full Read &amp; Show support for PureMT and MTBlock, which does the trick. Would you consider a patch for that? (Edit: of course, exposing this functionality via something other than Read/Show would also be fine.) One caveat is that I had to use unsafePerformIO in the Read instance for MTBlock, which (based on allocateBlock) seems to need to be allocated in IO. Don't know if there's a way around that, or what the implications might be, if any. 
Cale + ##proggit
Unless you do not want to use emacs
Ok, that's more than I knew of. Thanks for the information.
these videos are great from technical point of view, you can see the speaker and slides (without that two flash plugins approach from another website), and even people asking questions. the sound is great, or people from 3 videos I've seen had great pronunciation (in contrast, I gave up on vidoes from haskell implementers, because I couldn't understand enough to not lose interest) so keep up the good work mister, whoever you are.
From the source it seems like they are just plain old binary trees with elements at the leaves only. Nothing revolutionary in other words, but with a good collection of class instances and helper functions that it could be helpful not having to rewrite all the time.
Your version is certainly neater than the state mutation version. Probably more efficient too, since ghc can optimise this kind of thing better than a load of state monad binds. Next challenge: separate out the Brezenham's stuff from the straight line stuff: create a generalised Brezenham's function that takes some other function as an argument to describe the curve. 
If the point is lists + cheap concatenation then Data.DList is the recommended choice. However this library can still be useful for comparisions purposes.
&gt; you can see the speaker and slides So far I've seen only one video (Conal's talk about FRP). It was nice, except that sometimes it switched back from slide view to presenter view too early.
Is joining on regular lists really that expensive, given that they're non-strict? Why wouldn't a `++` thunk be equivalent to the `Join` constructor here?
I found it slightly amusing that they needed "an alias for (++) that does not cause a name clash with the Prelude" so they decided to call it ``join``.
The usual stuff, but good to see again.
You can do something like: fromFocus focus = \act -&gt; do outer &lt;- State.get let (inner, update) = focus outer (x, inner') &lt;- State.runStateT act inner State.put (update inner') return x to create a "lense" that can take advantage of sharing. I'm still trying to decide whether I like this approach better than `data Lens s a = Lens { focus :: s -&gt; (a, a -&gt; s) }`... It's awesome that you can compose with `Prelude..`, but the type of the resulting expression is dreadful.
List concatenation is expensive on the right `xs ++ [x]`. By contrast a join list is always constant time for concatenation.
What it is missing is parallel reduction. A sequnce as a binary tree is an ideal opportunity to do parallel reduction operations. See [Guy Steele's recent talk at ICFP](http://www.vimeo.com/6624203).
Joining on regular lists is inexpensive only in the sense that all operations are O(1) in Haskell as long as you don't use the result.
I noticed the same thing. I got to the point where when a slide displayed, I paused for a bit to digest the slide before continuing. Mainly, though, excellent quality recordings!
Yes, that's the missing piece. A wide tree with parallel reduction strategies.
The use of the '&lt;&lt;&lt;' notation reminds me of C++. Ugh.
Yeah, so technically the append isn't expensive except in how it affects future operations. For lists, each append adds O(1) time to every element access on the left side. I guess my confusion was because I was thinking about accessing it as a list; breaking it into a head and a tail and recursing. JoinList gets to be faster because maps and folds can break at the join, paying that O(1) once for each append instead of once for each element. If you access it like a list, it wouldn't be any better. It's not the append that's faster -- it's the maps and folds that are faster over appends. I suppose it's just a semantic difference, but it helps me to understand what's going on.
&gt; List concatenation is expensive on the right xs ++ [x]. Thanks. I did already know this. It's sort of effectively mostly true, but not completely entirely true. The append makes uncons slower. JoinLists don't change that. What I missed before was that it allows more efficient decomposition of appends. Mainly, I think, because I don't have sufficient understanding of Data.Foldable.
I don't really see the point of this. It seems import Prelude hiding (.) ; import Control.Category((.)) would be a better way to achieve this, whatever he's trying to achieve here?
He could have used `(.)` instead of `(&lt;&lt;&lt;)`, yes, but I think the point is that the stages of that pipeline, since they are individually wrapped in `arr`, are run concurrently with each other.
This is how I've been trying to understand the point of the article, but I may be WAY off. The concurrency isn't spoken about in great detail here, but it should give you a clue that these things are not a sequential stream of processing, but more of a concurrent stream of processing, using Arrows. I think... Since things are lazily evaluated in normal function composition, you end up with a bit of a coroutine-like implementation of stages of processing (like cobbling together behaviors in a Y combinator in Scheme or Python or Groovy or something :-)). This is like Y but with chunks running concurrently?
Amusing isn't quite the word I'd use...
I think he makes good points. I'm not sure exactly which discussions he's referring to regarding Hackage becoming like Debian, but I agree with him that there shouldn't be any maintenance requirements just for uploading to Hackage. Certainly anything claiming to be part of the Haskell Platform should play nice, though! I'd be thrilled if Hackage had user comments, though. 
Are we likely to see shared libs on Mac and Windows in a 6.12 point release or is the 6.14 timeframe more likely?
gtk2hs is practical - it has great docs (even tutorial!), easy to install and looks just like C version. the only problem is gtk itself, it looks like shit. with themes.
Real World Haskell, chapter 23, [GUI Programming with gtk2hs](http://book.realworldhaskell.org/read/gui-programming-with-gtk-hs.html).
1) 5 years. I used GuiTV to make a little bitty application. 2) How to do UI programming is still an open research problem. 5) I have found GuiTV to be very Haskelly. Presumably gtk2hs and wxHaskell are the most practical.
The IHG are paying for ongoing work for Windows and our aim is to get that included into a later 6.12 release. There's a couple people looking at it for Mac so it's possible that might be included in a later 6.12 release too. The main issues seem to be management ones, where libs go, how linker paths are sorted out etc.
Agreed. gtk2hs is great, but I'm not too fond of gtk
A very good analysis. A minor point he mentions is how do we manage user comments / votes whatever when new versions of a package are uploaded. We've no idea if criticisms are still valid or if problems got resolved.
&gt; I'm not sure exactly which discussions he's referring to regarding Hackage becoming like Debian. He's referring to the "Future of Haskell" discussion at the Haskell Symposium. See the video. It starts off with short presentations and then a general discussion. One of the questions/comments was about managing hackage a bit more like debian.
Just to echo everyone else: gtk2hs. It's actually rather rare that I find I need a program with an interactive interface and even more rare when it has to be graphical.
Why should it? You can use any string of punctuation marks as an operator in Haskell.
1) I started relatively quickly, like after the first month of playing around with it. I wanted to get out of the toy-program stage fairly quickly. I played around with wxHaskell and then moved on to the OpenGL bindings. I haven't implemented any full programs, but have cobbled together some very fun experiments. 2) UI in Haskell comes with one big, forbidding paradigm -- monads -- and it's baked right into just about everything having to do with IO. You need to learn 'em. Fortunately, Haskell's OpenGL library comes with a ton of example code that allowed me to play around with monadic logic even as I'm still trying to wrap my head around it. Learning monads is not as tough as some claim, but it does take some time, and frankly there are a lot of crappy tutorials running around wherein some condescending PhD tries to make the concept sound hip and sexy rather than clear and accessible. [This](http://www.haskell.org/all_about_monads/html/index.html) is the best introduction I've seen so far, both in terms of clarity and accessibility to the non-mathematician. (For the record, I haven't read the Monads chapter in RWH, but dons and company generally do a *great* job at providing a clear and approachable intro to Haskell.) 3) see #2. Monads, learn your monads. 4) There is a veritable ton of sample code in [Hackage](http://hackage.haskell.org/packages/archive/pkg-list.html). If you don't mind switching gears into wxHaskell, [this asteroid-ish game](http://hackage.haskell.org/package/wxAsteroids) is as simple as it gets. 5) I second Paczesiowa's recommendation for gtk2hs, as well as his caveat on how bad gtk itself looks. wxWidgets is a bit better, but the GTK binding comes bundled with the full library stack, which means you can get up and running on it after a quick cabal install, even on Windows.
Ahh so, I now get it, you build state computations that can act as labels. (They are not really `lenses' I think). Too bad they are monadic inside. In fclabels we tried to keep things more pure by having a Label datatype. So no need for the state monad there. Keeps the types far more clean and generic which enables the usage of the Category and Applicative typeclass.
for the love of god, don't learn monads by doing IO the IO monad is a weird and fucked up monad
So I've heard. But the IO monad is also ubiquitous and unavoidable if you're actually *doing* IO. I'll just have to unlearn my bad habits as I progress. But I absolutely thank you for the warning. :)
Nice header graphic.
Seriously nice work, well done to you and everyone else who put in the effort. :-)
Is that because you aren't into programming those sorts of programs, or because you do that sort of programming in a different language?
I wonder if that's still true today. The Judy code is horrifyingly complex. I'd have a hard time stomaching the idea of using it in practice.
Great idea!
The paper is from 2003, so it's not horrifically old yet. Though I'm also curious how well/how long it'll hold up. The complexity of Judy code is one of the big reasons why they're so rarely used, so it'd need to be a robust finding to convince most folks.
[Learn You A Haskell](http://learnyouahaskell.com/) is a nice tutorial. You can find references on the Haskell site. You can also ask for help in the `#haskell` channel on Freenode.
 1. Install [The Haskell Platform](http://haskell.org/platform) 2. Work through [Real World Haskell](http://book.realworldhaskell.org/) or [Learn You a Haskell](http://learnyouahaskell.com/)
Thanks for that, that's a really nice tutorial. You wouldn't happen to be able to recommend any decent textbooks as well would you? I'm trying to learn Haskell too, but I want to brush up on my lambda calculus which seems best learnt from a textbook ;).
Can't say I know of any. I suggest that you try looking in [r/csbooks](http://www.reddit.com/r/csbooks) for one.
A number of further Haskell-related videos from ICFP and co-located events that you might want to add: http://www.vimeo.com/6703480 http://www.vimeo.com/6697688 http://www.vimeo.com/6656645 http://www.vimeo.com/6654930 http://www.vimeo.com/6654722 http://www.vimeo.com/6653485 http://www.vimeo.com/6632347 http://www.vimeo.com/6630020 http://www.vimeo.com/6628930 http://www.vimeo.com/6624581 http://www.vimeo.com/6623087 http://www.vimeo.com/6622658 http://www.vimeo.com/6622272 http://www.vimeo.com/6612724 http://www.vimeo.com/6590617 http://www.vimeo.com/6575114 http://www.vimeo.com/6574677 http://www.vimeo.com/6574557 http://www.vimeo.com/6574069 http://www.vimeo.com/6573440 http://www.vimeo.com/6573255 http://www.vimeo.com/6573128 http://www.vimeo.com/6572966 http://www.vimeo.com/6572504 http://www.vimeo.com/6572153 http://www.vimeo.com/6571975 http://www.vimeo.com/6571637 http://www.vimeo.com/6571321 http://www.vimeo.com/6571053 http://www.vimeo.com/6570764 http://www.vimeo.com/6570515 
Will do, thankyou!
I found the paper "Church's Thesis and Functional Programming" (Turner) gave me pretty much everything I needed to remember about lambda calculus, and I'd recommend reading it before anything else, particularly if you already studied it in uni, it will make Haskell so much easier to understand. Also, read "Why functional programming matters" (Hughes). Unlike most other languages, you'll find yourself reading a lot of papers rather than textbooks with Haskell. Edit: And don't obsess about monads, just practice writing lots of code and they'll come when you need them
I found this one pretty helpful: [Haskell for C Programmers](http://www.haskell.org/~pairwise/intro/intro.html)
Interesting stuff, I'd never heard of these sequences before. I should point out though that most mechanical key entry systems have a 'C' key which you have to press as the first key, which breaks stream attacks. I don't know if electronic ones work the same (never had to use one) but I'd imagine they count the input keys and reject after every N presses if the sequence is invalid.
does that mean I won't have to recompile everything, everytime I update a haskell package on my archlinux box ?
Oh, I thought everyone could to add videos. Turns out only moderators are allowed to. Apparently with "groups" everyone can add videos, but those have a lot more community stuff than the simple list of videos I like from the channels. See [vimeo.com/groups/haskell](http://vimeo.com/groups/haskell)
I wouldn't call using gtk2hs or other such UI toolkits "Functional GUI programming". Its just imperative GUI programming in a functional environment (Which is fine! I love Haskell as an imperative language too!)
So, based on the linked page for the Haskell Platform - is there no 'standard library', such as found in the other languages I'm familiar with? I guess that's not as critical as it is in Ruby/Python, because Haskell is compiled, right? 
We used to have a "standard library" - that was pretty much *de facto* defined as the libraries that were needed by GHC to build itself, plus a few other useful ones that were included with GHC. But now that has now evolved in two complementary directions: 1. The Haskell Platform, which is striving to a achieve a pythonesque "batteries included" single install. 2. [Hackage](http://hackage.haskell.org/packages/archive/pkg-list.html), which provides a central location and infrastructure for publishing libraries and applications, like CPAN and RubyGems. [Cabal](http://hackage.haskell.org/package/Cabal) and [cabal-install](http://hackage.haskell.org/package/cabal-install) provide easy installation and dependency resolution for packages on Hackage.
Agree with dons and Flow. Doing same as we speak. Except I stopped doing RWH at about Chapter 5, and started to dream up a and make stupid modifications to other people's code. So now I have a very unfair version of pong and a gui program that does nothing but show off wxHaskell controls. Will return to the book and it will be easier I suspect. 
Once I'd grasped the basics of Haskell (I'm still quite a novice), I started trying to solve the problems from this page I found: [Ninety-Nine Scala Problems](http://aperiodic.net/phil/scala/s-99/) I got up to number 27 before I got distracted by other stuff.
[Project Euler](http://projecteuler.net/) is great for giving you tiny projects. And it's easy to use ghci or hugs to get answers out before you ever start playing with IO. And I recommend reading through [A Gentle Introduction to Haskell](http://www.haskell.org/tutorial/). One recommendation I have, for when you play with IO, is that you want to write as much of your code as possible as though it had static inputs, and then write an IO function to bind its input and output to files/streams.
While not directly about lambda calculus, Bird's [Introduction to functional programming using Haskell](http://books.google.de/books?id=ypNQAAAAMAAJ) is an excellent introduction to the mindset of functional programming.
Right, that was one of the concerns (if you can really call it that) in my post. The stuff I've done so far has involved a lot of simple sequencing of actions. I was wondering if there's not really a paradigm for doing GUIs more "functionally"? 
Need to run gnome-appearance-properties? The Gtk+ theme I use (glossy) looks awesome though I don't run gnome.
Read [yet another haskell tutorial](http://hal3.name/docs/daume02yaht.pdf) (PDF) and maybe [learn you a haskell](http://learnyouahaskell.com/), then select one of your (non-trivial) personal projects and write it in Haskell. Don't give up. Then write more personal projects in Haskell.
There is a standard library, specified in the [Haskell Library Report](http://www.haskell.org/onlinereport/). The Haskell Platform is a newer project to build a much more comprehensive standard library.
The best monad to learn first, hands down, is `Maybe`, followed by `Either`, followed by `Writer`. All of these monads have very simple definitions for `&gt;&gt;=` and `return`, and working through the monad laws (if you like that sort of thing) is easy enough to do on paper. After `Writer`, proceed to `Reader` and then to `State`. `Reader` is more complex as a monad than as a comonad, unfortunately (as a comonad it's basically just dual to `Writer`) but it segues well into `State`. Understanding `Reader` and `State` basically comes down to understanding currying -- if you really grok currying, then these two beasts are not actually very complex. `IO` is best understood, I think, as a special kind of `State` monad, at least when you are starting out. I would also recommend, especially if you are an imperative programmer, that you eschew `do` notation entirely until you are absolutely comfortable writing code using only `&gt;&gt;=`, `&gt;&gt;`, and `return`. The reason I say this is because `do` notation, while pretty, makes Haskell *look* imperative all while subtly behaving in a manner totally unlike your average imperative language. Things like the difference between a `let` binding and `&lt;-`, which often confuse beginning Haskell programmers from an imperative background, are totally clear if you're used to writing your code with `&gt;&gt;=` and `return`. It's very easy to go from `&gt;&gt;=` and `return` to `do` notation, but the other way around (usually) takes more mental acrobatics for someone with no functional programming experience. Have fun!
I am by no means an expert in haskell or in any programming language for that matter. But I don't think the logical path of programming in any language goes from non interactive to gui programs. From my limited experience gui code tends to be rather cumbersome and uninteresting. Mainly because the major design choice is the gui library itself, everything else just revolves around it and your program logic should be separated anyway. So unless you have a specific need for a gui, I would try avoiding to build one as long as I could.
I know how to switch gtk themes (I'm not that dumb). the problem is, that after browsing whole gnome-look.org I couldn't find one good looking theme, they all stink with that specific smell of gtk. you can theme all you want, but one look at the button and I know it comes from gtk. on the other hand, I like the default theme from kde4 and I've seen a few other nice themes for qt4, so there must be something that depends on toolkit. anyway, like any true linux user, I've just stopped caring and just don't pay attention to all those ugly gtk apps.
I recommend Learn You a Haskell first, then move on to RWH. LYaH is fun and exciting and quickly shows why haskell is fun to play with. RWH is is comprehensive and will really teach you the language, but it was not untli after I found LYaH that I was motivated to learn the language. LYaH won't teach you everything you need, but is a great intro IMO.
Someone else mentioned it, but I thought it's worth repeating: fire up an IRC client and join #haskell on irc.freenode.net. It's very beginner-friendly and asking questions there is often more efficient than looking through the documentation.
Given Church's early involvement with Lambda Calc, that's probably a good starting point. Wiki doesn't explain it too well to a fresh newcomer :). Truth be told, for most languages I've learnt myself it's been about reading APIs and experimenting (as opposed to reading textbooks) which I'm fairly comfortable with; Haskell just seems so alien I was kind of keen to have a massive tome to refer to :p. I think I'm getting the hang of a lot of it though; pattern-matching and guards in particular remind me of Prolog. Thanks for the advice though, I'll grab those papers.
No. That's not related at all.
It's out of print but if you can get a copy of it from a library or 2nd hand, Recursive Programming Techniques by Wm Burge is great. It is pretty thorough on lambda calculus and if you complete some subset of the exercises you'll have a deeper understanding of what's going on when you use a functional language, even though it is about the pure (untyped) lambda calculus.
Hey, thanks for using my library!
An irc channel where it's 'more efficient to ask before reading the dox' sounds way too good to be true :)
Thanks go to you here, it's been very easy to use !
00011101 isn't a De Brujin sequence for 3-bit words. 
The Haskell Platform is a binary distribution of stuff that's mostly already available through Hackage (which is the CPAN/Gems of Haskell). You could manually install GHC and cabal-install and then just get the packages you need, but this is easier. GHC alone comes with most (all?) of the libraries covered in RWH, by the way.
Those sounds made me nervous. Really, when will those damn computations finally come to an end!
I am sure they did all their prototyping in Haskell.
shared libs on Mac basically work, the patches required are very minor. will be official soon with any luck.
Yes it is: * __000__11101 * 0__001__1101 * __0__00111__01__ * 00__011__101 * __00__01110__1__ * 00011__101__ * 0001__110__1 * 000__111__01
This is pretty neat! To scratch an itch, I tried implementing it with STArrays instead of lazy immutable arrays. Also, I used the following idea: rather than tracking the actual bits in the array, let's track which sequences we've seen directly, so that checking a sequence doesn't involve reading the entire array. So, for the bit sequence `01001` we'd look in position `9` in the array. We also track the current bit sequence as its binary representation rather than as a list of bits. {-# LANGUAGE FlexibleContexts #-} import Control.Monad import Control.Monad.ST import Data.Bits import Data.Array.ST import System.Environment type Bit = Bool type BitWidth = Int fromBit :: Num a =&gt; Bit -&gt; a fromBit = fromIntegral . fromEnum append :: Bits a =&gt; BitWidth -&gt; Bit -&gt; a -&gt; a append n b = (fromBit b .|.) . flip clearBit n . flip shiftL 1 -- don't let the type signature scare you, this just returns a new bit sequence representation, along with the bit we chose preferOne :: (MArray (STArray s) Bool (ST s), Ix a, Bits a) =&gt; BitWidth -&gt; a -&gt; STArray s a Bool -&gt; ST s (Maybe (a, Bool)) preferOne n s arr = do missing0 &lt;- get False missing1 &lt;- get True return $ guard (missing0 || missing1) &gt;&gt; Just (val missing1, missing1) where val b = append n b s get = readArray arr . val deBruijn :: BitWidth -&gt; [Bool] deBruijn n = runST (newArray (0 :: Int, bit n - 1) True &gt;&gt;= deBruijn' 0) where deBruijn' :: (MArray (STArray s) Bool (ST s), Ix a, Bits a) =&gt; a -&gt; STArray s a Bool -&gt; ST s [Bool] deBruijn' m arr = do maybeBit &lt;- preferOne n m arr case maybeBit of Nothing -&gt; return [] Just (m', b) -&gt; do writeArray arr m' False bs &lt;- deBruijn' m' arr return (b:bs) main = liftM (read . head) getArgs &gt;&gt;= print . deBruijn The main function will read the first argument on the command line and compute deBruijn sequences for words of that size. I added a similar main function to the code on the page. Compiling with `--make -O2`, I found that the timings for word sizes up to about 8 bits or so are basically just noise, but above that, we get: debruijn_st debruijn_array 0-8 0.006-0.016 0.006-0.009 9 0.006 0.029 10 0.031 0.115 11 0.005 0.429 12 0.004 1.639 13 0.006 6.443 14 0.010 25.813 15 0.035 102.92 16 0.103 412.30 So it looks like using this different algorithm was a pretty big win.
&gt; Thanks. I did already know this. It's sort of effectively mostly true, but not completely entirely true. Ok you're making the distinction about when we reduce (ie lazyness). &gt; The append makes uncons slower. Yes. &gt; JoinLists don't change that. Ah but they do. Append makes uncons quadratically slower. With the join list it is only linearly slower.
&gt; Append makes uncons quadratically slower. Hmm. How does that work? It seems like, with n right-appends, uncons will take O(n) time to descend to the innermost list, then take O(n) time to collapse left-side empty lists and O(n) time to create new append thunks. What am I missing?
There is. conal's FRP. For example, [Phooey](http://www.haskell.org/haskellwiki/Phooey).
Graham Hutton's _Programming in Haskell_ is a fantastic resource. The book is short, sweet and to the point, and it focuses on the language itself instead of trying to explain monads before you understand the fundamentals.
Working on it at the moment at HacPDX. Expect a full release soon.
Can it use a hash function instead of a Hash class?
Does anyone know, if he creates his illustrations with a haskell library, too?
Heh, the original HS Judy bindings were the reason I joined the #haskell channel. Glad to see something robust finally released.
More about the failings of Python than the merits of Haskell. However, it does support Haskell's strengths: short development time, small code base, and easy code reuse.
This is very exciting! Wouldn't it make sense to change the order of the arguments so it's easier to do partial application? That's the issue I have with Data.Map and variants. I think this has come up in numerous list discussions.
Until then, `flip` and infix are your friends. :D I agree though, that Haskell is the first language where I've really started to care about the order of my arguments. Edit: How do you get backticks to show up?
Escape them like this \\\`
I would like to see the original problem, the Python source code and the Haskell one... It's possible that the Haskell success was caused by the earlier work with the problem under python... He already had the algorithm clearly structured inside his head, and everything he needed... Maybe without the previous python work he wouldn't make such a succeed... But well, as noted bi _periodic_, he talked about the haskell strengths...
Nice work, and thank you for the detailed writeup instead of just yet another link to Hackage. Why can't a structure like this have pure lookups, even though it has to use the IO monad for creation, insertion, and deletion. Is it not just a specialized trie in that respect?
They appear to be mutable structures. So, you need to sequence the reads as well as the writes to specify when/what version of the structure should be read. If it were pure, presumably lookup would happen whenever the result of that lookup is demanded, and would use whatever mutated version of the dictionary was around at that time, which probably isn't what you'd want.
Thank you, nice answer. Lazy evaluation still gets me sometimes.
there are other mutables structures with pure interfaces (diff arrays)
Sure, you can keep a record of all changes you've made, and construct duplicates of the old versions whenever you try to reference one. That isn't a recipe for high performance, though, if DiffArray is any indication. :)
some would say that having the python algorithm already structured in his head was the biggest problem with writing haskell version.
Just an idea. Could be a nice way to lure newbies. Or something. Might do work on it but the fact no one's made something like this already makes me think I'm missing something. Hm?
not entirely sure what i'm looking at there to be honest...
seems to be broken, at least on chrome
Is there an easy way to download these videos so I don't have to watch them in a browser?
Is this the same as what is described in [rational trigonometry](http://r6.ca/blog/20060614T204400Z.html)?
xmonad
Haskore is nice.
Two unexpected things I took away from this: 1. The thorough process, excellently described, followed in creating and benchmarking this library. 2. The points made in the comments about why mutable data structures in Haskell are not "anti-haskelly"
no, you're right some people who think perl is a good language get into haskell and start vomiting all over the screen with results like you described
I think one of the bad trends in Haskell is to use bad names. Sometimes it is justified, there just isn't a name for something really general. But sometimes, it is just because. data Accessor w p = Accessor { getter :: w -&gt; p, setter :: p -&gt; w -&gt; w } vs: data Accessor whole part = Accessor { getter :: whole -&gt; part, setter :: part -&gt; whole -&gt; whole } 
well, the general rule should be that the variable size depends on it's scope. A variable like w or p should have a scope of a line or two, while a completely spelled out variable (e.g. "FractionWholePart") would be a global.
I agree that any variable that has a single-letter name should have a tiny scope. I disagree that all such tiny scopes warrant single-letter variable names. Those names are great at letting you see the structure, but unless the structure makes the meaning completely clear, the meaning should probably be spelled out in the variable name. 
I didn't mean to imply that variables with a small scope *must* have small names. The code should be clear, but often in a really small scope one letter is clear enough.
I'm not familiar with that. Looks awesome. Just about to install Ubuntu, so I'll give it a go.
I'd like to see some form of s-box based crypto algorithms written in an elegant fashion. I haven't seen this done yet. I mean, we have them, but they are about as readable as ... well ... they aren't.
If you have a similar mindset to mine, you are in for a treat. Edit: It's not haskell, but once you feel comfortable with XMonad, try [vimperator](http://vimperator.org/trac/wiki/Vimperator)
Very cool, hope I find the time to play with this soon.
I really enjoyed reading the code for the Data.Binary library. It's not a big piece of code but it's very well written. 
vimperator+xmonad+emacs(client) is pretty much where I spend 99% of my working time. I barely ever touch the mouse.
Perl *is* a great language, and there are very well coded Perl projects. Perl is simply too flexible for some programmers, and so they write unreadable crap.
I completely disagree. Imperative programmers always complain about the tendency, found throughout the functional language ecosystem, to use cryptic one-letter variable names. That's because in their programming world, everything is about state and managing state, and therefore when in an imperative mindset the programmer is constantly asking himself what the value of this variable is and what the value of that variable is and importantly what they *do* and what their values *affect*. In languages like Haskell, ML, Miranda, and so on, variables' values do not actually vary. There is no destructive update, typically scope is quite narrow, etc. As a result, the functional programmer tends to deliberately de-emphasize the variables -- they're just place holders, really. This is also why point-free style is so popular, which if you think about it is cryptic variable names taken to the extreme -- no variable names *at all*. Programmers operating in the functional mindset typically would prefer if they could make the variables just go away, in most cases. They give their functions descriptive names, but the variables just *don't matter* in the same way they do in imperative languages. When imperative programmers start with Haskell, they are usually stuck for some time in the imperative mindset, and resist the idea that variables just aren't important. It's too deeply ingrained. But they don't matter. And once you've internalized this, long variable names begin to annoy you, a lot. They're noise. They detract from the meaning. In your `Accessor` data type example, I don't care about `p` or `w` individually (and I'd probably just use `a` and `b`). What I care about is the whole type signature. I understand what sorts of types I might put in `w` from the type signatures of `getter` and `setter`, not from the names of `w` and `p`. Your `whole` and `part` version is much less readable for me, because it makes the whole type signature longer, making it all much more difficult to parse. But then, that's another thing that beginning Haskell programmers struggle with -- type signatures and how important and meaningful they are.
Note that someone has already converted the 99 problems to Haskell: [H-99: Ninety-Nine Haskell Problems](http://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems).
Ah, nice, thanks for the link. Maybe it's time for me to get back to solving them!
I still use the mouse way to (too?) much in vimperator. I'm thinking about just stashing it somewhere in the back of my desk for a few days so I am forced to do without it. What do you mean by *client* for emacs? I've only started to use emacs about a year ago for it's nice support for Haskell and Agda. For most everything else I use vim. Just for fun, [here's a screenshot from my old system](http://imgur.com/oTjdK.png). My current setup is similar, but with a glorious 1920x1080 monitor. I only have [this rather boring screenshot](http://imgur.com/vxZHi.jpg) (I'm at home now).
I guess we can just agree to disagree then. The type signature with "whole" and "part" is clearer to me than with "a" and "b". I am not a Haskell guru, perhaps, but I have been using it for over 15 months, and I like to write my own Monad Transformers and higher-order combinators. But I have long departed from the "imperative mindset". 
Well, if you ever used more lightweight editors like vi, ex, and ed, you probably know that Emacs' Achilles heel for quick and dirty edits is that when you're sitting around at the terminal, firing up "emacs .bashrc" forces you to wait a lot longer than say, "vi .bashrc" or better yet "ed .bashrc". You probably have an emacs running already somewhere, but then you need to switch to that workspace and that window, open it up, etc. That's kind of a pain (which is why many emacs users live in M-x shell instead of using xterm or whatever). Well, for some time now you've been able to put emacs in server mode and connect to that instance of emacs using a sort of thin client called "emacsclient". Basically what it does is it creates a new emacs frame in your terminal or whatever, but the new frame is nonetheless part of your existing emacs. It launches as fast as vi because it doesn't do anything except talk to your existing emacs over a socket, essentially. It doesn't need to parse your .emacs, etc. And you can access existing buffers and such in it, too! It's really cool. And with emacs23, you can actually start emacs in daemon mode (via emacs --daemon) and it sits in the background and waits for emacsclient connections. So you can actually configure your system to automatically start up emacs on boot or on login or whatever your pleasure, so that it's already there, waiting for you to edit something! I have `emacsclient -c` as my editor option in vimperator. So when I'm editing a post (like this one, say) I can just hit Ctrl-i and it'll launch a new emacsclient frame, and then when I finish typing the post I hit C-x # and it transfers the text back into the form field. It's really pretty slick. Apparently there's an extension for Firefox called "It's All Text" that does this without vimperator for emacs users that don't want vimperator installed. Personally I'm fluent in both emacs and vi, and while I prefer emacs generally I still think vi is a great editor, so I'm not bothered by the vi-like keystrokes that vimperator uses by default. Not to mention that vi-like keystrokes are better for browsing anyway, I think.
Ah, cool. Thanks for the detailed explanation. Does it work with the GUI versions of emacs as well? &gt; automatically start up emacs on boot Why don't you just boot directly to emacs.. there's no point in running two operating systems :P
&gt; Does it work with the GUI versions of emacs as well? Yes. If you do emacsclient &lt;file&gt; in a terminal, by default it launches emacs in that terminal. If you do emacsclient -c &lt;file&gt;, then it launches a new graphical frame. Either way, it's all part of the same running emacs instance. Remember that unlike say, vim, emacs is the same application both graphically and as a terminal app (assuming you didn't specifically compile X/GTK support out, of course). &gt; Why don't you just boot directly to emacs.. there's no point in running two operating systems :P Har har.
dons (or someone): could you please share some commentary? I, for one, don't understand at all what this question is all about..
I liked this interview. It's very interesting to see two people who are at different ends of the functional programming community have a dialog and share their perspectives. One of the best parts to me was near the end where they started talking about what they liked in the other language. It really seemed like they both wanted to meet in the middle, but unfortunately a language can really only be defined one way. Each language was written for some very specific reasons, and it's elucidating to hear what those are.
Sheesh, a draft posting and already it's on reddit three times. (There are more comments on the earlier posts of this to reddit.) Anyway, feel free to mail me comments, and note that this draft will disappear in about three weeks, to be replaced by the final version.
Factor would be very similar as well.
I really like the way SPJ described the pull of erlang and the push of haskell
I'm not sure what exactly you mean by "the same". Angle and magnitude are not projective invariants. Regarding your blog post, you mention the analogy with null lines in Minkowski space-time. A rather common model for the conformal geometry of Euclidean n-space is the celestial sphere within a light cone of the Minkowski space-time with n+1 spatial dimensions and 1 temporal dimension. This is also related to the model of Euclidean space as a horosphere embedded in a hyperbolic geometry. The celestial sphere is the one-point conformal compactification of Euclidean n-space; the conformal relationship is realized by stereographic projection from some arbitrarily designated point at infinity. This should carry over to finite fields. I haven't read the book you mention and I can't extract enough from your short summary to see whether this would suffice as a model.
Agda uses a lot of memory when you start doing more involved proofs. In this case, it was using a lot more than one might expect, and I eventually stumbled on a minor change to my code that made it use way less memory, and even the implementors don't seem to be sure why it made a difference. Maybe it's a bug. I'm not sure why dons posted it here, though. It's no secret that Agda is kind of a resource hog. :)
"This channel requires that you have registered and identified yourself with the network's nickname registration services" I'm not going to bother to register the (pred|succ) of my nick.
Oh, I just think it is cool that someone is pushing on Agda enough to make performance complaints -- that is the first sign it is useful.
*Keep writing about Haskell, and sharing good information* We've overtaken a lot of other groups in the last few months: * [Python](http://www.reddit.com/r/python), 9201 * [JavaScript](http://www.reddit.com/r/javascript), 4741 * [Ruby](http://www.reddit.com/r/ruby), 3824 * Haskell, 3000 * [PHP](http://www.reddit.com/r/php), 2850 * [Lisp](http://www.reddit.com/r/lisp), 2722 * [Perl](http://www.reddit.com/r/perl), 2537 * [Java](http://www.reddit.com/r/java), 1545 * [Erlang](http://www.reddit.com/r/erlang), 1527 * [Scheme](http://www.reddit.com/r/scheme), 935 * [Scala](http://www.reddit.com/r/scala), 631 * [OCaml](http://www.reddit.com/r/ocaml), 524 * [Clojure](http://www.reddit.com/r/clojure), 474 * [Lua](http://www.reddit.com/r/lua), 115 
disappointing, I was looking forward to going as vdaeq`lo today
You can join with a registered nick and then change.
I guess I'm ilb\`qsgtq today.
It seems the new me is Upctbo :o
meh, I prefer the first one and I come from the imperative OO world. Spelling out "whole" and "part" does not add much to program comprehension. 
we lost 2 people:-(
Two more to make a round number: * [C++](http://www.reddit.com/r/cpp/), 3376 * [Tcl](http://www.reddit.com/r/tcl/), 133
link is down:-( and there wasn't any readme or docs. too bad, would be great to have something like this edit: up again
I think it helps to see what the Accessor is about at a glance, without having to fully comprehend it first. When you have dozens of these in a file, and you're skimming, it certainly helps.
Heh, but notice it is a performance problem in doing the type checking, not performance of the final code. :-)
Thanks dons for keeping Haskell Reddit full of interesting material :-)
Indeed, I haven't run any of this code. Type checking is the whole thing. Running code is so passÃ©. :)
:) but dons is still the primary contributor. would be interesting to see a histogram of contributors on x axis and number of submissions on y axis. then another including comments. My guess is dons will be the leader in both by a big margin in the former.
A visualization of numerical type class dependencies and uses. The author, Thomas, just got started learning Haskell. He presented an interesting talk on fast propositional logic at the [Dutch HUG](http://haskell.org/haskellwiki/Dutch_HUG).
Wow, I'm a bit surprised a few other languages which I thought were more commonly used are further down the list. I'm also not surprised that Python is at the top, as the site was developed in python and so had some professional python coders since day one. 
Horrah! Good job folks. However, I wonder if we're getting a bit of a reputation for being religious zealots. I get the impression from outside the Haskell community that we are a bit heavy on the evangelism and are a bit out of touch with reality. Of course the appropriate response is to prove them wrong.
Just checked /r/fortran only had 1 member. It now has 2.
 fix $ Build something; measure it; release it. 
How about: build &gt;&gt;= measure &gt;&gt;= postToReddit 
On day one, reddit was written in common lisp.
Another consideration is that people subscribe to subreddits because they are interested in a language not because they use the language. I subscribe to Haskell because I find it very interesting but I don't think of myself as a Haskell programmer. Java and Erlang are at about the same level of subscribers but I would wager a large sum that more redditors use Java than Erlang. Erlang has some unique quirks which get people interested in it and want to read articles about it, even if they don't use it (at least not commonly.)
That's most excellent! So much for "Avoid success..." ?
Out of curiosity, does this really bring much that Haskell doesn't already have? It seems like what may be a brilliant C++ library of some scope and complexity on might be a small, tight Haskell module. Honest question, despite the loading I've done there.
* [C++](http://www.reddit.com/r/cpp), 3377 * [C](http://www.reddit.com/r/c_language/), 769 * [Smalltalk](http://www.reddit.com/r/smalltalk), 334 * [D](http://www.reddit.com/r/d_language/), 322 * [Prolog](http://www.reddit.com/r/prolog), 288 * [Ada](http://www.reddit.com/r/ada), 223 * [C#](http://www.reddit.com/r/csharp), 215 * [Squeak](http://www.reddit.com/r/squeak), 138 * [TCL](http://www.reddit.com/r/tcl), 133 * [Forth](http://www.reddit.com/r/forth), 114 * [Oberon](http://www.reddit.com/r/oberon), 94 * [Awk](http://www.reddit.com/r/awk), 34 * [Factor](http://www.reddit.com/r/factor), 32 * [Cobol](http://www.reddit.com/r/cobol), 12 * [Pascal](http://www.reddit.com/r/pascal), 8 ...still no reddits that I could find for APL, J, K, Modula-2, Eiffel, Basic, or Mercury. 
&gt;One day one, reddit was written in common lisp. I liked the version before the edit better.
Oh, thanks for correcting me. I just remembered some comment by one of the Reddit devs about his love of Python. I'm glad they were influenced by Lisp originally. Any functional language is better than none in my book. :)
The reason for this was that freenode was attacked by some group yesterday, targeting the top 20 channels on freenode. since #haskell is has the 8th highest number of users, action had to be taken. Pity about the timing, it totally ruined IMPS day :(
Oh good, I've been looking for something like this. Thanks!
Few months ago I made [a similar (but less detailed) graph](http://lh4.ggpht.com/_PiUWFeprZSw/Sd72lQjUr3I/AAAAAAAAKfA/4PLE8uFQqUk/hs-nums.png) for myself originally published in my Russian [blog](http://sovety.blogspot.com/2009/04/class-diagram-of-haskell-numerical.html). Concrete classes are in gray frames. Polymorphic classes have rounded corners. Here is its [graphviz source](http://sites.google.com/site/sovetyplus/Home/hs-nums.txt).
&gt; That's most excellent! Keanu? &gt; So much for "Avoid success..." ? Heh, indeed.
That looks like the GNOME toolbar along the top, but I don't recognise the numbers -- presumably they're xmonad workspaces? How do you get them there?
The numbers, layout icon and window title are drawn by dzen, which I just put on top of the Gnome toolbar. [Here is my xmonad config](http://github.com/arnar/xmonad_config). To get xft support in dzen (to get the nice looking fonts) you have to download the latest version and compile it yourself.
Ah! Cunning. I didn't think of having them both together but it makes sense. Cheers!
The Haskell numerical tower makes me cry some times.
It could be worse. What if we had the numeric prelude instead?
Note that org-mode is part of emacs, so the next release of emacs should include this support as well. **EDIT:** No, I was wrong. org-babel is shipped in org-mode's contrib directory - so it is now part of the org distribution, but *not* built-in. The contrib directory isn't included in the version shipped with emacs, so the next release of emacs will not include this support.
 C / \ / \ C C | \ | \ | \ C C / \ / \ / \ / \ | | | | | | | | C C C C :)
Then I'd cry more often, mostly when trying to read haddock.
It's really sad that, while having such a nice type system, the numerical tower is so broken. Of course, this is obvious, but Complex numbers are subclassed from Ord? Ideally there would be an implementation of Magma =&gt; Monoid/Semigroup =&gt; Group, Ring {commutative,factorial,...} =&gt; Field, the concept of an Ordering really has nothing to do with that hierarchy, instead it would probably better to introduce metric/normed spaces, valuations and so on. I am not sure however if the haskell type system is really flexible enough for that and I am not entirely convinced by the numerical prelude either. (And it would be total overkill, if you did something like that for general purpose programs)
I use org-mode everyday, but had no idea it supported scripting. Can you elaborate or provide a link on exactly what that means? Can I write haskell code in org-mode and execute it for some reason?
Here's where I wish I had started with emacs instead of vim... Org-mode is pretty awesome; great to see it get Haskell support.
Does this mean that emacs will be shipping with ghc now?
check it: http://orgmode.org/worg/org-contrib/babel/org-babel.php
The initial sort pump post, which explains things very nicely can be found below: http://www.reddit.com/r/haskell/comments/9lv91/concurrent_pearl_the_sort_pump/ I'm only posting this because I missed it the first time round 
Sorry, seems I forgot to mention the source for the paper: http://www-bucephalus-org.blogspot.com
Out of that number, how many actually make money with Haskell?
Microsoft Research kicks ass.
Microsoft Silverlight? Are you serious?
Are you shortsighted? There are download links for 7 seven different formats.
Actually I am shortsighted.
&gt; Ideally there would be an implementation of Magma While I'd be the last to argue that the number hierarchy in Haskell is broken, are we really such Bourbaki-ites that we would ever use a magma (that is, a set closed under a binary operation, not [MAGMA](http://magma.maths.usyd.edu.au/magma/))? Why not go one step further and have a primitive type `BinOp X Y = (X, Y, X -&gt; X -&gt; Y)`, with `Magma X = BinOp X X`?
This is a good start for novices. The book does a nice job of it too. I'm looking forward to future installments. Silverlight ain't so bad either really. I believe you can target it from any .NET language. I think Mono can even target it now. http://www.mono-project.com/Moonlight
&gt; In addition, projective geometry is in fact more fundamental than affine geometry (at least from the point of view of algebraic geometry, although I'm not sure how you would justify that statement in general). Every projectivity (projective symmetry) is an affinity (affine symmetry) but the reverse is not true. That at least is an argument for why it is more basic without appealing to algebraic geometry. Of course, that characterization will only satisfy someone who has taken the time to appreciate the philosophy of Klein's Erlangen program. These posts of his are cool but I wish he would spend some more time talking about ideas rather than racing through the definitions. For example, here he's talking about enumerating subspaces. In doing geometry over an infinite field, you cannot count individual lines quite the way he does it here with finite fields, but you can still count the number of independent lines in a dimensional sense. The technique that he sketches of enumerating shapes of reduced row echelon forms is absolutely central in geometry and combinatorics. It's closely related to [Schubert calculus](http://en.wikipedia.org/wiki/Schubert_calculus) and the [Bruhat decomposition](http://en.wikipedia.org/wiki/Bruhat_decomposition). These in turn are based on viewing Grassmannians as quotients of algebraic groups by closed subgroups: if you describe a k-subspace of n-space by a k-tuple of linearly independent n-vectors then GL(n) acts on such k-tuples transitively and the isotropy subgroup of every k-tuple is isomorphic to GL(k). A basic theorem on group actions then tells you that the (n,k)-Grassmannian is isomorphic to GL(n)/GL(k) as a principal homogeneous space. But it's a different copy of GL(k) for every subspace, even though all of them are isomorphic, and the different shapes he enumerates in this post correspond to the different ways GL(k) can be embedded inside GL(n) when both are LU factored.
No, that's just Reddit --- you can either enter a long description, or a URL, but not both.
Good stuff, but... His quicksort is "wrong", i.e., it's not the stable version. He also gets history a bit wrong (thus confirming his thesis). Miranda was not the first language to add an ML type system to a lazy functional language. 
&gt; His quicksort is "wrong", i.e., it's not the stable version. But is this relevant for a first lecture? It was an example to show Haskell's declarative expressiveness. (Also, aren't most quicksort implementations not stable?)
Was it LML?
The reason you don't need binop and magma is probably because haskell already has a hardwired notion of a binop and magmas can be enforced through the type system. I just listed bourbaki's hierarchy to express my wish for more mathematical rigor.
Imperative quicksorts are often not stable, but the functional one is easy to make stable. Just change (&lt;=) to (&lt;) and (&gt;) to (&gt;=). It doesn't matter much, but it's a detail you might as well get right.
LML is one of them, there were probably others as well.
Thanks for point this out --- rather embarassingly, I hadn't noticed this.
Good catch, I didn't notice!
Okay, so a bare metal RTS + an embedded architecture (ARM, M68K, etc) would be absolutely fantastic.
I do find it funny that Barrelfish's architecture makes it more suited to their competitors' machines (i.e. the Cell CPU in the PS3, which is effectively already a heterogeneous computing mostly unshared environment with message passing) than to Microsoft's current offerings.
Pattern matching is amazing and one of the things I really miss in most other languages :(
WTF 100% like it!!!!! I liked it too :D
Source for the curious: http://www.barrelfish.org/release_20090914.html Lots of C, a little bit of Haskell. :\
Thanks for posting this. I haven't had time to watch it yet, but the first few minutes alone look good. What a great looking format. On a side note, just wanted to say that one of the things I love about what I've seen in the Haskell community is your general passion. I haven't had as much time as I'd like to work with it, but you guys are inspiring. Thanks!
Many times when you ask on #haskell how to do X, and they point you to Y, and Y happens to also have a link to a paper which was the precursor to the actual package, go read the paper! Brent Yorgey has also done a terrific job of compiling a list of papers for extra reading in his Typeclassopedia http://haskell.org/sitewiki/images/8/85/TMR-Issue13.pdf 
[If you liked scrap your boilerplate, you might enjoy this one from the same author](http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/beautiful.pdf)
I second this. Typeclassopedia has turned out to be one of the most invaluable references on Haskell types.
I'd look out for functional pearl papers if you want accessible. Most of the ones I've read have been pretty straightforward yet interesting. Somewhat unsurprisingly, anything called a "tutorial paper" is also pretty clear :) The tutorial paper on arrows, for example (linked from the arrows homepage) made everything a lot clearer for me. Keep your eyes peeled for Ph.D. theses too. Some of these are extremely obscure in the meat of the work, but contain a great deal of accessible introduction for the first several dozen pages. Here's one paper I enjoyed a lot: http://www.cs.nott.ac.uk/~wss/Publications/DataTypesALaCarte.pdf . Not sure how comfortable you are with types in Haskell so it might be rather scary, but once you're a few months in you should definitely try that paper.
http://haskell.org/haskellwiki/Research_papers has a lot of papers. Most from the 1990-2005 or so. I'd appreciate people maintaining that site.
I would recommend that when you use a module or library, you take a look at its Haddock documentation. Oftentimes there will be a link there to the paper that inspired the library -- this is often a data structure or something similar. For example, when you go to the [Haddock page for Data.FingerTree](http://hackage.haskell.org/packages/archive/fingertree/0.0/doc/html/Data-FingerTree.html) you find a link to a paper detailing the data structure. Read these -- generally, they are written clearly enough that an implementor was able to create the library from the knowledge he gleaned reading the paper in question. It's also always a good idea to peruse the references of papers you read. There will be lots -- you don't need to read them all or even a single one, just skim over the titles. Soon enough you'll notice that there are some papers whose names you've seen referenced in lots of places -- this is either because they're interesting and groundbreaking and worth your time to read, or because they are considered "classics" and therefore must be referenced whether you like it or not. You'll figure out which is which pretty quickly.
I second the notion of hunting for Functional Pearls. Richard Bird gave a wonderful talk at ICFP 06 about the selection process for the Functional Pearls in the Journal of Functional Programming. If the reviewer gets bored or can't follow some complicated twist of logic, they should reject it as a pearl (even if they should encourage it to be submitted as a regular article if it is good). This encourages Pearls to be well motivated and easy to follow.
Related: HWN referenced a [cafe thread](http://thread.gmane.org/gmane.comp.lang.haskell.cafe/64267) on the topic of DSLs with some nice links.
I thought it did own this space.
It's too bad he starts the lecture series with a history lesson. I fear that might put some people off who'd rather have seen some code in action. Apart from that I think this initiative is totally awesome.
...and it isn't a "space" so much as a closet.
You mean we don't already?
TBH, I've seen much nicer DSLs written in ruby. Not that haskell isn't great though.
&gt; I've seen much nicer DSLs written in ruby What does that mean? Nicer than what? Why was it "nicer"?
It's sad that a person like Schneier makes such a misleading comment (regarding scaling this to the Linux and/or Windows codebase). I would have hoped that (a) Schneier understands the difference between a micro-kernel and a monolithic kernel and (b) he understands that the work in question, first of all, targets embedded devices. In other words, practical /= being able to verify the Linux or Windows codebase.
Yes, tutorials are needed. 
Mainly that ruby has a simple syntax for expressing things like hierarchical structure that I haven't seen with Haskell. For example, Shoes (a ruby GUI toolkit) lets you do stuff like: Shoes.app { @push = button "Push me" @note = para "Nothing pushed so far" button "Another button!" @push.click { @note.replace "Aha! Click!" } } I haven't seen such intuitive DSLs come out of the haskell world, and I suspect that it would be difficult to create one. The only DSLs i've used in haskell are things like: testOr3 = do{ try (string "(a"); char ')'; return "(a)" } &lt;|&gt; string "(b)" To me, Haskell's strength isn't so much its ability to produce DSLs but rather its ability to express data structures and transformations upon data structures very naturally, that basically make it well suited to specific problems without developing a whole new structure to represent it.
And of course, the entirely predictable journalistic distortion has been made: "100% bug-free". No. Formal methods does not guarantee that your code is bug-free. Nothing can. What it can give you, at most, is some level of *assurance* (subject to both your formal methods tools and your hardware behaving correctly) that all the possible bugs *you've thought of* cannot occur - assuming you've encoded each statement of the form "bug X cannot occur" correctly in the language used by your tools. It's entirely possible that you've missed a class of bugs. Programmers do this all the time - why would we expect theoreticians not to?
That's great work. Congrats to everyone involved. Let's hope Debian/Ubununtu aren't that far behind.
For me there is also a big difference between proving that a kernel is correct wrt some bugs you don't want to occur and proving that a compiler preserves semantics (like in the compcert project). In the second case, you know that the program will, at least, comply to the specification. In case of an OS, which doesn't output something at the end, or of a plane, etc. you just know what it won't do.
Well, saying that a program is in the set S is logically equivalent to saying that it doesn't live in the complement of S (assuming a finite universe of discourse).
I can imagine a Shoes implementation in Haskell looking like this: app :: Shoes app = do push &lt;- button "Push me" note &lt;- para "Nothing pushed so far" button "Another button!" push `click` (note `replace` "Aha! Click!") What's the difference?
I don't really understand what you're saying. If I prove that a function returns the ordered list of the primes, and that is what the function was meant to do, then as you say, there could be problems with the proof assistant or with the hardware. There could also be an error in my specification. However, if each of these is ok, wouldn't the code be '100% bug free'? How could I have missed a class of bugs?
I thought Schneier was saying that verifying Linux would be impractical. Please would you give more detail about what you disagree with? 
After quoting the original articles from physorg.com, Schneier writes, "Don't expect this to be practical any time soon:" followed by the LOC argument. This pretty much implies that the approach is not practical and justifies that POV by the LOC argument.
Can the example be fixed or explained, w.r.t: \n -&gt; 10+n-n ?
Sounds like someone needs to think of a low level API first, before asking for all that work to be done in the code generator. What are the types? What are the operations? Are they portable or are they tied to the length of vectors in current x86, ppc or sparc chips? It's really not clear what is being asked for.
Static type safety :-)
I think he is saying that the bugs move from the implementation to the specification.
That would be great, but I have never seen a haskell DSL that clear. Perhaps we just need more work put into our DSLs.
I read it as basically saying "please give me intrinsics for specific architectures", initially the various SSE incarnations and maybe AltiVec. I.e. you could use SSE4 intrinsics, say, which would be unportable in some sense, but if you use it on a CPU without SSE4 you just get a software implementation of each instruction. 
Hmm. I'd love to give a tutorial on my little "expect-like" EDSL if I could get permission from my company to do so. Remember, EDSLs are about coming up with a notation that you want to express your solution in within another programming language. To some extent, it seems that the very embedded nature of an EDSL means your'e going to be using the language it was implemented in. So first you need to know just a tiny bit of Haskell, then you can write a lot more of your code in the EDSL. 
Right. Formal methods ensure that the implementation matches the specification. By my definition, this means the software is bug-free.
Since I wrote this, I'll point out that the ideas in it turned out to be largely identical to the existing packages fclabels and data-accessor. Absent a compelling reason otherwise, use them and not my code from this blog. Especially in this case, standardization is more important than any particular detail.
You could put them in separate modules and require that they use `import Your.Module as YM` for disambiguation, but that doesn't sound that great either. When mapping C libraries into any of the languages that have superior namespace support, sometimes you just have to bite the bullet and expose the old enum names anyhow. You really want people to be able to use the original documentation to find out what things are, unless you're going to commit to a complete documentation port as well, and the only way to do that is stay as true to the original names as possible. It sucks, but this is a hard place. (I suppose if there's no documentation for the old library, this doesn't apply.)
So gcc provides arch-specific `__builtin_$arch_$instr` functions (e.g. `__builtin_ia32_addps`). According to the docs there is no software fallback. The builtins are available with the appropriate `-m` flag (e.g. `-msse`) and are not available otherwise (see [gcc x86 builtins](http://gcc.gnu.org/onlinedocs/gcc-4.4.1/gcc/X86-Built_002din-Functions.html#X86-Built_002din-Functions)). In addition, gcc provides limited support for machine independent vector stuff. The [gcc vector extension docs](http://gcc.gnu.org/onlinedocs/gcc-4.4.1/gcc/X86-Built_002din-Functions.html#X86-Built_002din-Functions) gives the example: typedef int v4si __attribute__ ((vector_size (16))); v4si a, b, c; c = a + b; So the question for Haskell is what kind of primitive and fallback support would we need so that we could build useful stuff on top.
It seems like the stopper here in general (not just this proposal particularly) is pattern matching failures, not the rest of the language. Has anyone explored something to put pattern matching into a library, beyond the apparently-inadequate-for-this-task view pattern feature? If Python taught me anything, it is that "special" language capabilities that can only be done by the runtime and can't be hooked by client code are one of those pernicious failures you can't even see in your language because you aren't even looking for it. A lot of Python's concision in an expert's hands comes from judicious overrides of the language syntax features to "do the right thing". If pattern matching could do the same thing, you might be able to fix the records issue cleanly, and offer who-knows-what-else too. (Would be a challenge to do, though, I acknowledge.)
"Well, I've proved this algorithm is correct, except, _oops_, it's O(2^n^n) when I implement it! Guess I better add that property to the proof assistant..." That's a bug. Primes may not be the best example since we have known algorithms for it, but you can see how this could happen in general. Proofs can handle the known knowns and known unknowns, but the unknown unknowns can still bite you.
EDSL is indeed a nice place for haskell to shine. But the people who say Haskell already own this space forget about Scheme or CL.
Actually, I'm fairly convinced that view patterns are, actually, very nearly adequate for the task. The only problem is the rather artificial limitation that a value can only be matched with one pattern at a time.
Works in nicely with the HWN focus on DSLs this week. There was _lots_ of good discussion about them this week. Related note, I've heard of some kind of 'finally tagless' thing related to (E)DSLs, anyone have resources on that? EDIT: I just found the paper on LtU, but methinks (given it's authorage) it would be nice to have a synopsis for the stupid person, (read, me.) to help guide my thinking while I read this. EDIT2: Scratch that, the LtU link is dead. I'll just have to hunt around for it some more. 
Either way, surely any solution would start by having architecture dependent intrinsics (e.g. SSE4), similar to the __builtin stuff. I'd prefer software fallbacks to those as a second iteration, with a generic vector library a distant third.
You should take a peek at the EDSL embedding of BASIC or C done by (IIRC) sigfpe^H^H^H^H^H^H *augustuss*. They're up on hackage. Most (E)DSLs I've seen in haskell look like sfvisser's or the examples above. EDIT: Credit to whom credit is due, Thanks taejo and dons.
I think formal methods work better when developing something from scratch. If you read the papers, they had to adapt their design process, and they made some technical choices up from that made the proof less complex. This would not be the case with legacy code. So it is probably not yet feasible to try to prove correctness of an existing large software system. From what I have read so far, the verification/proving process found more bugs than testing. Most of them were trivial but some of them had big impacts on security/stability and were not detected during traditional testing.
What does the E in EDSL stand for? Are there any introductory tutorials on writing DSLs in Haskell? I've been having trouble finding them.
That's a strange definition of "bug" you've got right there. "I've found a bug! The software crashes!" "No you haven't - not crashing wasn't in the specification, so it's not a bug!"
While you may be technically correct, in practice there is a world of difference. The current status quo relies in trusting systems described by millions of lines of code, probably tens of millions if you include hardware and OS, to be bug free. For a formal specified system one can derive theorems for the properties he is interested in. The description of the definitions that the theorems rely on is orders of magnitudes smaller. For example, a system that provides a 100% type-safe Java implementation only needs to define the logic gateway behavior at one end, the type-safe Java semantics at the other end and the rules of logic, but it is not concerned with the concrete details of/affected by the bugs in processor/memory implementation, OS implementation, optimizing Java JIT implementation. In other words, each layer of abstraction completely hides the details/"specification bugs" of the previous layers. If a layer's specification is "buggy" that does not result in an overall buggy system, but in the impossibility of proving the next layer correct. Essentially the system cannot be proven correct anymore.
That style of imperative EDSL is precisely what I was getting at though. We use do notation, and overloaded strings and numbers to make the DSL look identical to the core language. Credit Suisse's Paradise is like this, as is Lennart's [Basic](http://augustss.blogspot.com/2009/02/more-basic-not-that-anybody-should-care.html) Clearly we need more education. Particularly about deep EDSLs and their optimization. Also LLVM as the target backend. 
One does not encode "bug X cannot occur" properties. One encodes the set of behaviors that can occur (safety, whitelisting) and the set of behaviors that will eventually occur (liveness). I'm not current with the latest formal verification research, but I bet there are already people studying time-bounded liveness properties. Once you've got safety and (time-bounded) liveness, what else do you think is going to miss?
Safety and liveness are basic properties that one needs to prove for a system to be taken seriously. Safety prevents "crashes" and liveness prevents "stuck" bugs to appear.
...and Prolog.
&gt; Once you've got safety and (time-bounded) liveness, what else do you think is going to miss? Detailed requirements on the output data, for example. Corporate and governmental customers are notorious for giving out bad, incomplete requirements.
There are two sources of troubles with systems: A. System does not implement the intended functionality correctly. Bugs. B. System implements the wrong functionality. Improper requirements. Formal specifications and verification can help with A, but not B. Improper requirements produced by a dysfunctional organization do not represent a "new class of bugs", unless we stretch the English language to the limits of its usefulness. That's exactly one of the reasons "bad, incomplete requirements" happen, misuse of language :) I'm not arguing that using formal methods results in useful systems, I only argue that using formal methods results in bug-free systems, i.e. systems that behave exactly as intended.
What is WIP? Work in Progress? Do you perhaps instead mean WIMP (Windows, Icons, Menus and Pointers)?
Work In Progress, yes. The "project" is right now 1 day old :-)
&gt; I think formal methods work better when developing something from scratch. I noticed that proving statements about code results in better code factoring. The cost of proving one thing at a time is high. The cost of proving two things at a time in unbearable. Do one thing and do it right.
So it's a work in progress that just started progressing progressively along a progression to progress? (WIPTJSPPAAP2P?) :)
E=Embedded
I hope you won't mind if I keep WIP in the title :D But anyway, basically, any opinion about the "thing" ? Layout, simplicity, ... ? (as soon as I've something correct, I'll put it on my github repo)
I think it's [this one](http://www.cs.rutgers.edu/~ccshan/tagless/aplas.pdf) (from Shan's site, not Oleg's).
Example code?
This is an awesome idea. I'm going to steal it.
As Don points out, Basic was by Lennart Augustsson. So was his C-like language (http://augustss.blogspot.com/2007/08/programming-in-c-ummm-haskell-heres.html) which wasn't as C-like as his Basic was Basic-like.
I approve, despite being an Ubuntu user myself. I remember having to setup the entire stack myself when I wanted to get Cabal working. This is definitely a step in the right direction.
I am curious about why you decided to develop this when yi and Leksah already exist.
What I think I will do is keep this a single file of enums still while trying to avoid prefixes as much as I can and only use prefixes when I need to disambiguate. I may follow [this style of naming convention](http://www.haskell.org/gtk2hs/docs/devel/Graphics-UI-Gtk-Gdk-Enums.html), what do you guys think?
They don't fit what I expect, simply. A matter of taste.
Am I missing something? [root@mousepad jwc]# yum install -y haskell-platform Loaded plugins: presto, refresh-packagekit Setting up Install Process No package haskell-platform available. Nothing to do
This is on Fedora 11
I concur, but this isn't the only Haskell package on hackage that's lacking examples. ["Project info page (wiki-like?)"](http://www.haskell.org/haskellwiki/Hackage) is on Hackage's wanted features list and might help to get more examples in the packages' documentation. Does anyone know what the progress is on implementing new Hackage features?
Someone help me out here. It looks like an EDSL for shaders, but it doesn't use special types. Rather than defining Num, Fractional, etc. for say ShaderFloat, it uses regular ol' Prelude.Float and those instances. But somehow when you write "1 + 1 :: Float" in the context of a FragmentStream, the Haskell runtime doesn't reduce it to 2 but passes it on to the shader. How? Rewrite rules? It doesn't seem like that would preserve type. Thirding the request for examples.
What is wrong with yi?
I have deciphered the API enough to render a gray background with a red square-ish thing in the upper right corner: http://hpaste.org/fastcgi/hpaste.fcgi/view?id=10451#a10451
`1 + 1 :: Float` will not actually work. GPipe *wraps* `Float` with the `Vertex` type and provides new instances for `Num` and family using that, and similarly for fragment shaders. I already said so elsewhere in this submission's comments, but I have managed to figure out the basics of the API now and have written a tiny example program that renders a red rectangle in the corner with a gray background: http://hpaste.org/fastcgi/hpaste.fcgi/view?id=10451#a10451
Might take a day or so for the yum mirrors to get the new packages.
Should be named "Haskell **Syntax**: The Confusing Parts". Wadler's Law again. Otherwise, a good article.
Are we meant to infer from Wadler's Law that (1) ho, ho here come the idiots again not getting it, what a bunch of losers or (2) that programmers relate to languages in a different way from the way that language theorists thing they should, which makes the language theorists partly mistaken about where their efforts should be directed?
The part of Haskell (syntax) that confused me most the last time I played with it isn't even mentioned. In foo :: Bar baz -&gt; Bar quux what is Bar? The name of a type or a name bound to a constructor for values of some type? (*) I hate markdown so much 
We generally call it a type constructor because it takes type parameters. Not sure it's specified, but the word type seems to refer to things of kind *, and if you have anything above that involving the type-level -&gt; between kinds, then you get a type constructor.
Author here. The scope of the article is hugely biased towards two things: My own private experience learning Haskell, and a handful of questions that newbies ask a lot on IRC. I deliberately left out a lot of stuff that was explained perfectly by RWH. On the other hand, I completely forgot some things that ought to be added, or in some cases just haven't gotten around to writing yet. Anyway, I never struggled at all with a lot of the semantic aspects, so those have sort of been on the back burner.
baz and quux are types? So foo is of a higher-order type? It's when I see definitions of this shape: data OpenFileFlags = OpenFileFlags --etc. that I get really confused. On a subsequent use of 'OpenFileFlags' how does on tell is the type or the constructor is being referred to? This is the sort of thing that confuses me about Haskell. 
With lower-case first letters, baz and quuz are type _variables_. Since you didn't restrict them, that means that they can be any possible type. Normally you'd probably restrict baz and quuz with some typeclass requirements. But the presence of type variables means that your foo function is polymorphic. The OpenFileFlags example demonstrates the fact that the type level and value level have completely independent namespaces. In your example, the OpenFileFlags type has kind *, and the OpenFileFlags data constructor has type --etc -&gt; OpenFileFlags :) It can definitely be confusing at first, yeah :/
Glad you asked! I'll write another section to explain that and some related issues, once I'm satisfied with the indentation section.
Fantastic job. I've been learning Haskell recently and the one thing I've not found documented so far is the '$' operator. I'm so glad I know what it means now, thankyou!
Cool. The article is pretty good overall (and I second nzfrio's comment about $), I was just slightly dismayed that the things that confuse me personally weren't there. I lot of Haskell tutorial material leaves me feeling like a retard for just this reason.
&gt;type level and value level have completely independent namespaces So, here we have "=" acting as a transducer between namespaces? I think this is a good example of clause (2) of [my interpretation of Wadler's Law](http://www.reddit.com/r/haskell/comments/9qucz/haskell_the_confusing_parts/c0e0n0p). It's _cute_ to have two namespaces for very, very closely related things, but probably a mistake wrt programmer comprehension. (*) rephrased 
sounds interesting. Some day, at some time, in the near future I will give it a chance. For now, I just upvoted it on reddit, hoping the day I need it, I will find it with a google search. Thanks.
Why do you think it's a mistake? It seems exactly the right thing to do. If I want a type to represent rectangles, I might call it `Rect`. data Rect = ... Then I think, actually, `Rect` is also a decent constructor name. So why not: data Rect = Rect Int Int If you want you can think of it as a verb and a noun using the same word :-)
Let's say I don't use the --external-sort option. If I have a dataset larger than RAM I will swap like crazy unless a cache-oblivious algorithm was used. But, if a cache-oblivious algorithm is used, then --external-sort is only needed for the times where the sysadmin has sized swap too low. But, the sysadmin set the swap low because that was the limit he wanted to set for temporary disk space. In that case, --external-sort would be going against the syadmin's wishes by using more disk space for a temporary job than he designated. And, unless you use very low-level disk I/O routines, the performance of the operating system's virtual memory subsystem will probably be much better than the performance of this explicitly-managed temporary disk usage. Let's consider a modification to the MySQL package that includes a special "temporary database" script. This script takes a set of filenames as parameters, generates a bunch of CREATE TABLE statements according to the contents of the plain-text files with those filenames, runs some SQL statements against said database, and then deletes said database. MySQL would then have a superset of the functionality of this program. Now, let's consider the --external-sort option again. Let's say the user can say "--external-sort 1GB" to mean that the program will use at maximum 1GB of disk space for sorting. But, why does the program have to use this 1GB of disk space just for sorting? Why not build some (b-tree, hash, or bitmap) indexes instead? What the user is really saying is "I have 1GB of disk space for you to use as you please in order to give me the results quicker." Wouldn't it also be cool if a process could tell the OS "Hey, I'm terminating now, so you can give this memory to other apps if you want. *But*, I am probably going to be recalculating the contents of this memory the next time I'm run, so it would be really cool if you could somehow save the contents of this memory for me, so that I don't have to do those recalculations. Thanks!" Actually, you can more-or-less do that by using mmap on a file in /tmp + an application that deletes unopen files in /tmp when disk space gets low. With this, you could sort/index the data only when the data sources change, and use the sorted (indexed) data across multiple runs when the data sources don't change.
From the looks of it, the Debian/Ubuntu team has a long way to go before the Haskell Platform can be considered complete.
You're not meant to infer anything.
I am pretty confused about haskell, but none of my issues are the ones listed here. My confusion is with types, and the syntax around types (using and creating them.)
The LLVM supports vector instructions in a fairly generic way with multiple backends. It seems like a major pain, and is quite bug ridden. I'd prefer ghc to just get good at what it's doing with scalars before attempting vector instructions.
Am I the only person for whom the page completely crashes Gecko browsers? On OSX, both Firefox 3.5 (`Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.3) Gecko/20090824 Firefox/3.5.3`) and Camino 2.0b4 (`Version 2.0b4 (1.9.0.15pre 2009091516)`) die straight at page opening.
And this is why Hackage needs wiki functionality so the community can add examples to the documentation. :) Thanks for the example. :)
Well... Since it's an early draft, I didn't test it at all under Gecko. Could be the HTML 5, or it could be... something else. No idea what. **EDIT:** It's a CSS problem, apparently. I knew I was bad, but sheesh...
It absolutely isn't an issue with your page (or it might be, but even if it is, that shouldn't crash browsers). The remark wasn't an indictment against you at all, merely a tentative gathering of datapoints (to know if it's an issue specific to my configuration or one more widespread)
&gt; Does anyone know what the progress is on implementing new Hackage features? We're working on a new all-in-Haskell implementation of the server. The limiting factor is volunteer time. darcs get http://code.haskell.org/hackage-server/
Example?
&gt; In any language design, the total time spent discussing a feature in this list is proportional to two raised to the power of its position. &gt; &gt; (0) Semantics &gt; (1) Syntax &gt; (2) Lexical syntax &gt; (3) Lexical syntax of comments &gt; &gt; (That is, twice as much time is spent discussing syntax than semantics, twice as much time is spent discussing lexical syntax than syntax, and twice as much time is spent discussing syntax of comments than lexical syntax.) See also [Wadler's Law on the Haskell wiki](http://www.haskell.org/haskellwiki/Wadlers_Law).
Crashes mine too.
&gt; baz and quux are types? So foo is of a higher-order type? No, foo is a function of type `Bar baz -&gt; Bar quux`. `baz` and `quux` are both types (unspecified I'd guess, so anything goes), and `Bar` would be a type class (a higher order type?) &gt; On a subsequent use of 'OpenFileFlags' how does on tell is the type or the constructor is being referred to? It's non-ambiguous, you can't have the constructor and the type in the same contexts so by looking at the context (a type specification or a value building) the user (and the compiler) know which is which.
&gt; and the one thing I've not found documented so far is the '$' operator. http://haskell.org/ghc/docs/latest/html/libraries/base/Prelude.html#v:$ not good enough? (also important: if you're learning haskell and are using a Gecko-based browser, setup hoogle as a keyword search bookmark)
&gt; That style of imperative EDSL is precisely what I was getting at though. While it may get Haskell some more attention from the imperative crowd, I think this may be exactly the wrong direction to go. Wouldn't it be nicer to encourage *declarative* EDSLs?
for some reason, I always get those two confused... :/ thanks
The fact of it is that they aren't that closely related. Haskell maintains an absolute separation between typeland and value land, and no value-level term may ever appear at the type level, and vice versa. Maybe for a beginner the two names will be confusing, but I've honestly never heard anyone who uses Haskell on a regular basis complain about this issue. I for one have no trouble distinguishing the two cases.
Crashes mine as well Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.3) Gecko/20090824 Firefox/3.5.3 I filed a crash report.
Well apparently it's not *just* my imagination, I'll have to try reducing it and open a ticket.
You're a hero. &lt;3
How does one give an example of something they do not understand?
It is not all gecko browsers. FF 3.5 works just fine for me on linux. Looks like the other redditor who is having issues is also on a mac, mac font issue?
&gt; mac font issue? Safari would break as well. It doesn't. Nor does Opera. It does, however, seem related to Gecko specifically on the Mac.
So, there's an absolute separation between these not-closely-related things that are on either side of _an equals sign?_ That seems like very poorly chosen syntax to me. &gt;I've honestly never heard anyone who uses Haskell on a regular basis complain about this issue Of course not. They have become trained not to see the problem. Just as Lisp programmers (eg myself) have become trained not to see the parentheses. Everyone begins as a beginner, so ease of comprehension for beginners has a very strong influence on uptake. Just saying. 
Sometimes you need an EDSL with variable binding and then the do notation is quite convenient. Also, there's no contradiction in using do notation and being declarative.
try reading: data OpenFileFlags = OpenFileFlags as: *(the data type) "OpenFileFlags" is instantiated by (the data constructor) "OpenFileFlags".* [edited for terminology] 
&gt; programmers relate to languages in a different way from the way that language theorists thing [sic] they should If that "way" is "ignoring the meaning of programs and focusing on all the cool-looking symbols" then yes, they are idiots. I think you're the one underestimating the common programmer here. Perhaps the gap is that, while users and theorists both care about meaning, they don't have common terms in which to talk about it. I also think that Wadler's Law is a special case of the very general fact that people focus on easy problems, where everyone has an answer and an opinion, over hard ones, where you have to think a lot or risk looking dumb. That's human nature and not something one class of people can use to label another class as idiots.
Here's an easy way for you to differentiate them: data Foo a = Bar a You will only ever see `Foo` in a type signature. You will *never* see `Bar` in a type signature, only in code. In that sense, their namespaces are totally different. You use `Bar` for things like pattern matching. So you might see something like: xyzzy :: Foo Integer -&gt; Integer xyzzy (Bar 5) = 12 xyzzy (Bar x) = -x + 5 I think it's pretty common for newbies to be confused when someone writes `data Foo a = Foo a`. I remember when I first started with Haskell I made a point of giving the type constructors and data constructors different names for a few weeks because of that confusion. But after a bit you start to want to pattern match using the same name (i.e. `Foo`) because in a lot of situations it makes the code look cleaner.
You find an example of something you can't explain. Or you give an example of something you think should work, but doesn't, *along with* an explanation of why you think it should work.
dons specifically said "imperative EDSL." Do notation does not necessarily mean imperative.
In this situation, I have had one module or a set of modules for all the FFI stuff, but rather than exposing that I expose other modules at a slightly higher level of abstraction. It would be in those other modules that I remove the prefixes from the C identifiers.
&gt; (the data type) "OpenFileFlags" is instantiated by (the type constructor) "OpenFileFlags". The second `OpenFileFlags` is a data (or value) constructor.
Sorry, I meant an example of one of your issues that you didn't see listed. Which types are confusing to use? What sorts of types would you like to create?
Well, so far everything I code in haskell types to me are a hindrance rather than an aid. They just seem to get in my way. So when writing a function f x y = x+y I know that is f :: a -&gt; a -&gt; a But can not think of how restricting the types to something specific benefits me really. So I constantly run into issues where I am calling functions with inappropriate types and get frustrated. I do have some understanding on how types are supposed to be useful. Such as designating "d2463345" as a "employee id" type rather than just a string so that you could have functions which only accept employee id strings as inputs however * I feel like everything will work if i never specify the types, I am just being sloppy * Feel with all the discussion of types in haskell there must be something deeper which would benefit me by allowing me to do something MORE rather than just be restrictive I am a fairly amateur programmer in general so it is difficult for me to see the big picture. The largest program I have written in ANY language is about 1000 lines so I can see types benefiting much larger projects, so maybe I just don't need them... but I feel like I am probably missing something.
I guess my issue is I do not have a strong enough understanding of types to even ask the correct questions. I see types as a restriction to me where everything I read explains them a powerful tool. Anyway I posted a full response to gbacon below. I am always surprised when I don't understand something in haskell everyone is so helpful, it is nice but strange :-) 
Bar baz, is sort of like Bar&lt;baz&gt; in C++, Java, C# etc. Bar is the "type constructor", i.e. give it enough "type variables" and it will "construct" a type for you. In this case you give it one type variable (baz) and it constructs the type Bar baz.
I checked again last night, and was able to pull the package. Thanks!
I think I understand what you're saying but I might not be able to do this because I'm using c2hs. I'll be defining higher-level functions in c2hs and the tool is going to generate foreign imports where ever I define the higher-level function. I think you and Jerf are right, maybe I should expose them as their raw C name form (which will be generated from c2hs).
You are correct. Thanks for the note.
Or why there is a need for yet another texteditor. One would think that by now that problem is settled. If it uses qt bindings then it might be interesting as an example.
If any language *should* own that space it is common lisp or scheme. The whole point of the last few chapters in sicp is to prove how easy a e(dsl) can be written in scheme, without using more then a few macros. The reason that so many recent research papers are written about ones in haskell is probably because many learn haskell at university rather than scheme. But I might be wrong, some of the haskell dsls are certainly impressive.
They are a restriction of what you can do, and are designed to be. It's almost impossible in general (see for example Rice's theorem) to reason about unrestricted systems, so we restrict them in particular ways (purity, types, effects, depending on your language) to allow the compiler to reason about the program. Many proof languages aren't even Turing complete, by design. I find a type system like Haskell's to be a nice middle ground. Even when writing ruby, which is dynamically typed, I typically put the same kind of stuff (classes) in the same variables. Sometimes I'd have a function return different types depending on the input, but most of my variables were pretty consistent. I'd rarely use metaprogramming techniques and run-time addition of fields. So if you are used to a dynamic language such as ruby or python, ask yourself if you already program in a reasonably consistently typed manner. Do your functions expect their inputs to be of a specific type (or support some specific interface/methods, if duck typing), and fail (either implicitly or explicitly) when you don't pass what they expect? If so, a type system like Haskell's won't restrict you too much. Even if you sometimes return multiple types from a function, you can approximate that using ADTs for sums. If instead you do crazy run-time class creation and other stuff like that, you might even be able to approximate that in Haskell, but you won't get as much benefit out of it as you would if you wrote it more idiomatically. 
No, f's type is Num a =&gt; a -&gt; a -&gt; a. This makes sense. Otherwise, f needs to work on _any_ type a. For example, would it make sense for me to write: f f f The two arguments I pass both have the same type, so it meets the requirements, but how would I apply the (+) function to two other functions? (+) imposes its own restrictions to its parameters, and the type checker figures out that f needs to inherit those restrictions because its parameters are (+)'s parameters. The whole point (as I mentioned in my other reply to you above) is that types _do_ restrict the kind of behavior you can write. Does it make sense to add a 2d vector to a 3d one? Or to pass a unit of mass into a function that expects a distance? Maybe with the appropriate conversion function, but not directly. (There's a Haskell library that allows you to keep your units consistent; I didn't just make up the example). In many languages the 2d + 3d vector example would throw a runtime exception but what if it's buried deep in the logic of the program and is only triggered in an extremely unlikely set of conditions? You can advocate 100% test coverage, but what if the dimensionality of the vectors is determined at runtime (you can encode this situation in a non-trivial manner in Haskell, but all it can possibly do is ensure that you deal explicitly with the case when the dimensions are different)? The point of a type system is to restrict what you can do to the point where automated reasoning obviates the needs for such tests. Everything is a trade-off. Most Haskell advocates will agree that the type system restricts you somewhat, and forces you to think harder about the problem than you otherwise would. So why is that better? We prefer compile-time errors to runtime errors, basically. I also believe that the kind of thinking Haskell forces you into is the good kind, in that it makes you think deeply and laterally about a problem, instead of just increasing the cognitive load for no particular benefit. If you're looking for other benefits of types, take something like QuickCheck or SmallCheck. The beauty of typeclasses allows us to write automatic unit tests for functions, just specifying a property, and letting the type dictate how it should come up with test cases. I can thus use Haskell to automatically show that Float addition is not associative by simply asking QuickCheck to check that \x y z -&gt; x + (y + z) == (x + y) + (z :: Float) The one type annotation is to force all three arguments to be Floats (they might otherwise default to Integer or some other numeric type for which addition is actually associative). QuickCheck could then print out a "counterexample" to your above assertion, which would be in the form of three Floats for which addition is not associative. Beyond security and correctness, types can also significantly help the compiler optimize a function. If a whole-program compiler (i.e., not GHC) sees that you only ever use a polymorphic function on a single type or two, it could generate a specialized version of the function for that type, and maybe even unbox the values of the type so that they fit in machine registers. Using more recent Haskell extensions, the behind-the-scenes representation for certain structures can actually differ enormously. See for example uvector's UArr (associated) type that transparently zips two arrays together as a tuple of arrays in O(1) time, and uses a bunch of other similar ideas to give you the best representation for your compound type. You see this to an even greater degree in data parallel Haskell, where transformations of types can come with significant performance and parallelization benefits. And if nothing else, types are fascinating just from the logic side of things :) There's a whole world of research on type systems and their impact on both languages and logic. ** Edited :) **
You're correct that the purpose of types is to restrict what you can do. The benefit is that a Haskell program which compiles with no errors is much more likely to work than a Python program which loads with no errors. You've eliminated a large class of runtime errors, which might slip past testing and crop up in a production system. You've replaced them with an exhaustive, compile-time *proof* that your program is free of a whole class of undesirable behaviors. This is an incredibly valuable thing.
It's generally a bad idea to recommend that anyone ever use tabs in Haskell source files. Sure, it's possible to make them work, but they're a problem waiting to happen, and if you try to collaborate with other Haskell programmers, you're likely to get an earful about it. :) Personally, I think tabs should be considered an outright lexical error. The golden rule of indentation: If two things are siblings, they begin in the same column, if something is part of something else, then it starts in a strictly deeper column. This is actually a lot stricter than the real indentation rule, but it avoids problems like the confusion with `if`/`then`/`else` (`then` and `else` are siblings, so they are aligned, and they are part of the `if` so they start in a deeper column). The only exception that I make in my own code is the one which almost everyone does, which is not indenting after a `module` declaration.
You're right, he did.
With Haskell you can leverage a highly programmable type checker to make a type system for your EDSL.
&gt; I feel like everything will work if i never specify the types Broadly speaking, if your program is correct then that's true. There are a few exceptions where types are required because they can't be inferred, but you'll know when you hit those (e.g. ambiguous MPTCs or GADTs). The big reason to give type annotations is because often the inferred types are *more* general than you would've thought/desired. Sometimes you mean to be that general, but many times you don't. By restricting the type you can make sure it's never used in an unanticipated way. Moreover, giving types serves as documentation for how the function is intended to be used (e.g. whether you mean for it to be so general)--- which becomes important when maintaining a large project because it gives a contract for what other modules should expect. Also, by having explicit type signatures you tell the compiler what type you intended, so that you know when tinkering with a function definition has changed it's type (e.g. making it less polymorphic than intended, or messing up the argument count in recursive calls). If you have a type signature the compiler will tell you right there that things won't work; if you don't have the type signature then the type error can show up somewhere else and be harder to track down. Thus, by saying what you mean (giving types) you can be sure you mean what you say (the implementation matches what you want), which is essential for rapid programming.
&gt; baz and quux are types? So foo is of a higher-order type? At the type level, anything that begins with a lowercase letter (e.g. baz, quux) is a type variable. Anything which begins with a capital letter (or ":") is an actual type or an actual type constructor. And whenever you see things separated by spaces it's application, just like at the term level. So, if we see foo :: Bar baz -&gt; Bar quux We know that foo has type "Bar baz -&gt; Bar quux" (whatever that is). That type is a function from "Bar baz" to "Bar quux" (whatever they are). And they are applications of the type constructor "Bar" to some type variables "baz" and "quuz". The type variables are for parametric polymorphism. But what is this type constructor Bar? Well it's something defined by one of the following, data Bar x = ... newtype Bar x = ... type Bar x = ... These definitions are similar (syntactically) to term-level function definitions. E.g. bar x = ... Type constructors don't have the full expressive power of type functions (i.e. they can't return an arbitrary type), but the syntax is intentionally similar. For what it's worth, you can consider "-&gt;" to be a type constructor too, it's just an infix operator taking two types (one for the domain, and one for the range).
No. Bar is a type constructor. Typeclass constraints are never written inline, they're always put in "the context" which comes before the main part of the type. Witness f :: (Num a, Num b) =&gt; a -&gt; b The type variables a and b are restricted. They can't be just any type, they have to be some types which have a Num instance. Given proofs of these constraints (i.e. the typeclass dictionaries implicitly passed around for you) then you get the type "a -&gt; b" (for the given a and b). The constraints in the context can be given in any order, and if you only have one of them then the parentheses become optional.
&gt; It's when I see definitions of this shape: [...] that I get really confused. Understandable. The ADT syntax is nicely succinct, but it's a bit strange to learn since it doesn't keep term-level and type-level separated in the usual way (though they are still separate). One way to make sense of it is to use the GADT syntax instead. For example, the type data Expr = Add Expr Expr | Mul Expr Expr | Val Int can be written (with the -XGADTs pragma) as: data Expr where Add :: Expr -&gt; Expr -&gt; Expr Mul :: Expr -&gt; Expr -&gt; Expr Val :: Int -&gt; Expr Which makes it clear that you're defining three constructor functions (Add, Mul, Val) and what their types are. In the traditional ADT syntax, the different 'constructors' are separated by "|"s. For each 'constructor', the first token is the name of the actual constructor and all the other tokens are the types of the arguments for that constructor. The return type of the constructors is always the type named between "data" and "=". (For GADTs that restriction is lifted.) You can also define symbolic infix term-constructors so long as they begin with ":". If you're doing this then you parse the 'constructor' just like you'd expect for an infix operator, e.g. data Tree = Tree :^: Tree | Leaf defines data Tree where (:^:) :: Tree -&gt; Tree -&gt; Tree Leaf :: Tree
I think, the even more important quote from that page is "Essentially every modern programming system that claims to address concurrency provides some FP capabilities." In other words, FP is **inevitable**.
Interesting to have an alternative (and clearer) syntax available. Thanks.
I guess I should clarify. Of course limiting types prevents you from making errors and forces you to say what you mean, that is great. The problem comes when you want to use someone else's code or use a built in function and it either uses types you don't understand or you don't understand why it has chosen to use that type. The only piece of code I have written used functions from Data.List and I was always frustrated at the types the functions would return since I would have no idea how to then use them in my own code. Types are great that they restrict you from making mistakes. That is always good thing, but they require far too much for thought and deep understanding of how everything works to casually use someone else's code. I am sure that is good thing too, but you seem to have to be far more skilled or more intelligent than myself for it to be beneficial.
&gt; I also believe that the kind of thinking Haskell forces you into is the good kind, in that it makes you think deeply and laterally about a problem Maybe it is just that my mind doesn't work that way. Forcing me to think that way doesn't seem to help. I am not a planner, in anything especially code. I will probably never be able to become a developer and work on production code, I do however find haskell interesting and fascinating. But as you say haskell FORCES you to think deeply about the code, and I just cant seem to do that. I want to be able to play with infinite lists, but the things i want to do are functions defined in Data.List. The problem is I can't seem to understand the reasons why the types are what they are all the time. Types are great at preventing me from doing something I didn't mean in my own functions, but they seem to have the effect of making me unable to use existing code. fromJust $ elemIndex =&lt;&lt; minimum $ somelist that is a clip from the first Haskell program I wrote. I didn't want a damn maybe. Fine elmIndex doesn't always find that value, but in my case I knew it was there. I had to spend a few hours learning about monads just to use elemIndex, but I had to force the types to come out correctly for the code to work at all. Yes fully understanding the code is the correct thing to do, but frankly I am just not smart enough to do it. Haskell forces you to think and prevent you from shooting yourself in the foot, but not all of us are *good* at thinking. 
The quote is just spot on, this is how I came to be interested in haskell.
Me too.
Scala is a better gateway drug IMO. It allows you to gradually progress from a java-style OO language to a functional language with static types, parametric polymorphism, monads, typeclasses (kind of) and optional lazyness. 
In which Sun's Hideous Jabbering Head wakes up and mumbles "concurrency, functional programming,.... er, wait .... what... I need ideas, do you have ideas? I like Ruby."