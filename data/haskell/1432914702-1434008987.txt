Thanks for creating this issue! I think it's great to have more discussions on how we can improve hackage with relatively (technically) small "look-and-feel" changes.
I just tried with FF + ABP, it worked... There is another site where you can test the contact form (on the bottom): http://satoshi.bitcells.com/ --&gt; is it the same? If you try without these extensions in Private Window, does it work?
It turns out that stochastic processes can be represented in haskell quite nicely. The typical way is to use a monadic interface to keep track of the random state of the algorithm for you automatically (see e.g. [MonadRandom](http://hackage.haskell.org/package/MonadRandom), but there are others too). This is more robust than the typical solutions in C++ because it is automatically thread safe and you can easily swap out other random number generators to get the right tradeoff between randomness guarantees and speed for your application. At one point I had a decision tree and random forest classifier in HLearn, but I removed it a while back so that I could focus on other things. I would very much welcome another implementation, but creating one that works with HLearn is definitely not a "first haskell project" sort of project. The tree itself would be a good project, but integrating with HLearn would be hard because HLearn has basically no documentation and it is not designed like other haskell libraries. So at this point, I wouldn't recommend you try to do something like that.
Great example! If you're going to go so far as to use three or four letter variable names you may as well go all the way and just write out what they are!
Yeah, this is already a thing :) You should just join fpchat
Is there really much to discuss across Clojure, Elixir, Erlang, Haskell, JavaScript, OCaml, Scala? Once you learn your maps and folds, the problems you run into are very language- and ecosystem-dependent.
Yes, there are Elixir and Erlang channels. They're not very active. 
Since you're asking for the goal, I guess a quote of the [FAQ](https://github.com/ekmett/lens/wiki/FAQ#goals) is appropriate: &gt; Q: What are the goals of this project? Why does this project exist? &gt; A: The lens library exists to provide more composable versions of the abstractions you already know how to use in Haskell. Virtually every Haskell programmer already knows how to work with functors and functions or Foldable and Traversable containers — we simply provide you with a vocabulary for composing them and working with their compositions along with specializations of these ideas to monomorphic “containers” such as Text and ByteString. &gt; One goal of lens has been to provide a consistent vocabulary that lets you access and work with pure data of any sort, while retaining the ability to be able to reason about your code with laws.
If you've got a good suggestion of how to work around the need for OverlappingInstances, I'm all ears. The main cases I know of are: * ToJSON a =&gt; [a] * [Char] * Foldable t =&gt; t a * IntSet
Lens is less solving a particular problem, I feel, and more a discovery of a certain kind of pattern and then an extension of that pattern to its conclusion. This induces nearly a new kind of language entirely and if you like the pattern at the core then you like lens. So what's this pattern? It's the notion that types can be decomposed into pieces and those *decompositions* themselves are first class. As a simple example, it ought to be clear that the tuple `(a, b)` is decomposable into the `a` part and the `b` part. If we could give a name to each of these decompositions we might call them `_1` and `_2` and the properties of these decompositions are that of a `Lens`. _1 :: (a, b) ~&gt; a _2 :: (a, b) ~&gt; b The basic properties are that lenses ought to composable compose _1 _2 :: (a, (b, c)) ~&gt; b that we ought to be able to use them to pull the "focused" pieces out view _1 :: (a, b) -&gt; a and that we ought to be able to replace the focused bit set _1 :: x -&gt; (x, b) -&gt; (x, b) (These types are not accurate and a bit mythical, but are more or less the proper intuition for the `Lens'` and `Lens` types). --- So this is the core of what a `Lens` is. There's another form, `Prism`, which works on sum types (like `Either`) in the same way that `Lens` works on product types (like tuples). Then the rest can be seen as extensions of these ideas more or less as far as they can go.
"Lens, you're a great library. Keep on trucking -- we're behind you 100%."
There's often the idea that GoF patterns in Haskell just become first-class items directly. This is kind of the case here. One could call "first class focusers" a pattern... but in this case, Haskell is expressive enough to just capture this pattern directly as a type and give it a name.
Probably the simplest problem that `lens` solves is updating or setting a deeply nested field. For example, let's say that you have these data types: data Atom = Atom { _element :: Element, _position :: Point } data Point = Point { _x :: Double, _y :: Double } Now, you want to increase an `Atom`'s `x` position by 1. Without `lens` you would have to unpack and repack every constructor along the way: example :: Atom -&gt; Atom example (Atom e (Point x y)) = Atom e (Point (x + 1) y) This gets really tiresome the more deeply nested the field is because you have to keep track of a lot of unrelated fields along the way as you unpack and repack these constructors. With `lens`, you can just write: makeLenses ''Atom makeLenses ''Point example :: Atom -&gt; Atom example = over (position . x) (+ 1) Also, you don't need the full-blown `lens` library to do this. The `lens-family-core`/`lens-family`/`lens-family-th` libraries are much smaller and provide most of the functionality you need. These are acceptable alternatives to people who balk at depending on the really large `lens` library just for getters and setters. Side note: `pipes` is a coroutine library. You would use it in the same places that you would use coroutines in other languages.
Also, w.r.t. `pipes`/`conduit` I wrote up an SO answer a while back on that topic: [What is pipes/conduit trying to solve?](https://stackoverflow.com/questions/22742001/what-is-pipes-conduit-trying-to-solve/22746612#22746612)
More a higher level of category theory but similar in concept.
The discussion went like this: member Matt wrote to me why there are JavaScript and Scala on the list... I said because they are partly functional but he wasn't satisfied. It all went fast from there. Those two were removed and 3 new ones added. Also Matt is an Elixir developer and I showed him this thread and neither to him or other was it obvious we should stop the team in particular because you say you don't have too many Elixir developers. For now the state is that we are going to focus only on real functional languages with special focus on Elixir. Check the updated http://functionalslack.com/ page. What is you philosophy ? If for example we did finish up deleting our team and inviting the members over, would you remove Scala and JS (if you have it)? Not sure about F#, don't know yet enough about it but it probably also won't be included in our team. It's possible that we put a pointer to your group from our page (would have to be discussed) for people that want those missing languages. 
`q`, `z`, and `h` are reserved keywords. 
checkout What I Wish I Knew When Learning Haskell by Stephen Diehl http://dev.stephendiehl.com/hask/
Sounds like we're doing different things. My intention was just to set up a place where people could talk about whatever functional languages they wanted to. Any user can create a channel so I'm not deciding that. All I did was set the place up and the community decided the rest. I don't see the point in shutting down channels..if people don't like them, then they just mute them and that's it. I just wanted to see if the two communities were aiming for the same thing so that it might be efficient to combine them (whatever that means), but it sounds like you guys are doing something different. Good luck with it!
Even though there are already some great answers in this thread, I want to throw mine into the ring. A while back I wrote [a blog post explaining lenses](http://taylor.fausak.me/2014/08/03/lenses-from-the-ground-up/). I tried to stick to one motivating example and show all the ways to solve it. The best one-sentence explanation I can give is that "a lens is both a getter and a setter". 
I didn't mean to suggest that Edward's lens library is harder to use. I was referring more to the larger dependency graph. For application code it's not a big deal, but for more general-purpose libraries it helps to have a smaller dependency.
You're right. UHC builds with no problems on the mac. The thing is that what I'm really looking for is a lightweight Haskell interpreter supporting foreign import and foreign export, suitable for various scripting and embedding tasks with performance and footprint (and use-cases) somewhere between Lua and Python. It seems that UHC is an even heavier-weight tool than GHC if such a thing were possible.
A new version will be out soon. IHaskell used an old version of classy-prelude, which causes trouble in nix. Take a look here: https://github.com/gibiansky/IHaskell/issues/495 It's nearly fixed.
why do you use `~&gt;` for `_1` `_2` and `compose`, and `-&gt;` for `view` and `set`? I think I know what `-&gt;` means but what does `~&gt;` mean?
Here's a key * `s ~&gt; a` becomes `Lens s s a a` or `Lens' s a` * `compose l1 l2` becomes `l2 . l1` then `view`, `set`, and `(-&gt;)` are their standard meaning.
There are some functions in the Prelude that can be rather dangerous. head, tail, read, (!!), foldl1, foldr1, maximum, minimum, toEnum can all throw exceptions at you. zip/zipN, zipWith/zipWithN will not throw any exceptions, but perhaps they should! If you were expecting lists to pair up (and sadly they don't), the results can be even more catastrophic. I've been found guilty of writing bad data into our database :) Do the right thing and ban 'em in your production code, or at least make it so they have to be imported explicitly - using safe defaults is much more appealing when the unsafe ones are a little bit inaccessible. You can find some alternatives from a couple of libraries on hackage (I personally quite enjoy some of the naming conventions used by [safe](http://hackage.haskell.org/package/safe) ).
Can't you do this with having a record of defaults and using update syntax? 
your package has awesome documentation
You might be interested in https://github.com/andrejbauer/marshall/tree/master/etc/haskell
It's like a member ptr, or a relative ptr...
The problem is when only some fields are defaulted but others are not
Any idea how to salvage `:set +m` entry support for `let`-less binders?
Well, now I find it inconvenient to program in anything else :) Haskell has ruined quite a few of my old C codebases for me.
I thought we weren't supposed to use the safe package? I feel so conflicted, although I normally just do as you say and try not to use partial functions.
Here's another one that appears to bug quite a lot of people: Lazy I/O seems really natural and powerful for doing streaming computations, at first. Unfortunately the way that resources (think file handles for example) are acquired and released can become very tricky to manage. It seems to be some kind of pipe dream to have somehow had lazy I/O magically give you good automatic resource management under the hood in the same way that we've come to rely on garbage collectors for automatic memory management.
In what cases would you want `zip` to throw exceptions?
Inconveniences: * Double-think in the community. Evangelist tropes are all over the place. * In particular, regardless of what anyone says, laziness isn't all that fun. Among other issues, it makes memory usage hard to get right. Lazy iterators are A+ though. * Not all monad tutorials are born equal. Haskellers tend to focus on what something *is* rather than that it is *for* and what it enables. The best free monad tutorial I ever saw was a presentation at Scala Days 2014, and I don't even know Scala. * Haskell is bleeding-edge. Learning resources for lazy programming are relatively sparse compared to, say, assembly programming or object-oriented programming. The design patterns are still evolving, spearheaded by /u/edwardkmett, /u/Tekmo, and a bunch of other brainy folks.
And record namespacing. Records in general are a pain point
Well you might not want an exception, you may prefer `Maybe`. I only mean to say that even an exception can be better than the alternative. Imagine that you have a list of ids that you want to use to fetch some records from a REST service in a single batch. However, unbeknownst to you, some of the ids are incorrect and instead of 8 objects the service returns 6 (maybe they're not so great with reporting errors). When you try to merge the 6 records in the response with your own 8 records there's a mismatch. `zip` only gives you 6 results and unfortunately they're not paired up correctly, but you have no way of knowing except by inspecting the results. `zipExact` gives you some peace of mind that at least the lists were the same length so that at a minimum there shouldn't be anything missing. So, an exception will at least tell you that your assumption about the data lining up correctly is flawed and allow you to rewrite that piece code rather than failing silently.
The `Foldable t =&gt; t a` is the only one of those you can't redeem, but it could be turned into a default signature or something. The others are fixable with tricks like we use with Show.
&gt; You mean (, e) -| (e -&gt;), right? Indeed. &gt; [...] what can we pull with Const -| Lim? a natural transformation `Const a ~&gt; f` is isomorphic to a function `a -&gt; Lim f` this is weaker than the usual category theoretic notion of a limit because parametricity is stronger than naturality. This condition means limits here rule out things like constructing products/coproducts, etc. using limits/colimits.
Including the syntax, which prevents Haskell from being a curly-brace-free language. :(
Ah yes, I knew this would come up. I actually thought someone would mention `zipWith f [1..]` as a counter example. I personally feel that in these cases an explicit `import Data.List (zip)` is warranted to let everyone know that you know what you're doing, it only serves as a stumbling block for beginners by being included in the prelude.
It's not very neuanced to claim that "regardless of what anyone says" they're wrong and you're right. :)
Is the Scala Days presentation listed on [this](https://gist.github.com/kevinwright/9505828a0dcc0c4c0d56) page ?
[Composable application architecture with reasonably priced monads](https://dl.dropboxusercontent.com/u/4588997/ReasonablyPriced.pdf)
Technically I show that a transducer is like a fold, and that a transducer is 'inspired' by the notion of a Moore machine where we've switched the internals to use an adjunction. When we switch to the one contravariant adjunction we have on Hask then we get a transducer. This is "different" than the Moore machine. A Moore machine takes one input and gives one output at a time, but can "remember" something. A transducer takes one input and gives several outputs, but is effectively memoryless.
Fascinating. It's the opposite, yet it seems to cause the same effect: more successfully built packages. Thanks for the effort! :-)
Do notation also allows curly braces.
&gt;This doesn't really get rid of the length check The idea is to enforce the length check. If `zipWith` has a signature like zipWith :: (A → B → C) → Vec A n → Vec B n → Vec C n then you're obligated to prove that the two input vectors have the same length. If you're dealing with arbitrary inputs, then you're also obligated to handle mismatched lengths outside of `zipWith`, and `zipWith` doesn't need to handle precondition violations.
I find this really irritating too. When I'm fiddling around, I just import `Lens.Simple` from this primitive package https://github.com/michaelt/lens-family-simple. It does nothing but re-export the `Control.Lens`ish complement of combinators from `lens-family` and `lens-family-th`.
No way to get a decent debugger. And arguments to lack of familiarity are real arguments, whether you like it or not. Haskell doesn't exist in a vacuum, it strongly departs from tradition, and there's a cost associated with that.
Tradition is also what is giving us software projects failing in major ways virtually everywhere on at least one, often several of the budget, time or scope axes. Quite frankly we need a strong departure from tradition to get this industry to a more healthy and predictable state that actually deserves the term engineering. I would even go so far as to argue that the very ease with which some languages allow people to claim that they are programmers despite only being able to do voodoo programming and only testing the happy path is partially to blame for many of the problems in our industry. As for a debugger, the vast majority of languages does not have debuggers and they all work just fine without them using a mixture of print debugging, REPLs, unit (and other automated) tests, type systems, profilers,...
`zip :: [a] -&gt; [b] -&gt; Maybe [(a, b)]` would be a strict function, which is pretty annoying.
Full-fledged dependent types are a **big deal**, you can't just bolt them on an existing language and expect to reap the benefits. In particular, they're not all that useful if you don't observe a separation between data and codata.
It tells you which kinds of languages are so low level and/or badly designed that you can not have a simple mental model but need to step through the execution because every little detail might potentially break and the amount of state that influences how exactly an operation occurs is so large that it is non-obvious which part of it causes the error even once you know how to reproduce the problem reliably (or often enough for testing anyway...and it is not as if debuggers are much help with the kinds of problems that occur only on every 10th run anyway).
Lenses, as a general concept, solve the problem of making record field access first-class. Without lenses, "modifying" a record field looks like this: foo' = foo { someField = performModification (someField foo) } I think we all agree that this is butt-ugly. Lenses provide some sort of mechanism that gives us a first-class thing that allows us to do this instead: foo' = over someFieldL performModification foo `someFieldL`, here, is a lens, an opaque first-class value that can be used to both modify and query its target. Now, that's all fine and dandy, and several lens libraries existed that did more or less this; and then, along came Edward Kmett, took the idea, and ran with it, and now `lens` is this library that gives us not just lenses, but also folds, traversals, and a dozen other somewhat related abstractions. Kind of a "one thing led to another" situation, I believe. Given how these abstractions all fit together in an elegant, natural way, I would say it's a huge success; one caveat is that it depends on everything and the kitchen sink, so much that people have suggested installing just `lens` to pull in all the popular libraries.
You don't need to know category theory to learn Haskell, you need to know Haskell to learn category theory (efficiently). :P
You might be interested in [SPJ's talk on the lens library](https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation) (signup required, but it takes 5s). At the end of the day, the cash value of lenses isn't the theory behind them, but the degree to which they expedite the handling of data. That big-ass diagram on hackage scared me too, at first, but I can't tell you how appreciative I became of the library once I had to deal with data structures like data Game = Game {_cells :: Map CellInd CellData, _edges :: Map. (CellInd, CellInd) EdgeData} data CellData = CD {_items :: Map ItemName Double, _agent :: Maybe Entity, _temperature :: Int} type Entity = Either PC NPC data PC = {_health :: Int, _stamina :: Int, _inventory :: Map ItemName Double, _hunger :: Float, _mind :: PCMind} data PCMind = {_messageQueue :: Queue MindMessage, _memory :: Game, _components :: Component} data MindMessage = AttackedBy EntityName | HealthDecreased Int | ItemGained ItemName Int | ... makeLenses ''Game makeLenses ''CellData ... Now imagine writing, say, a function that goes through all cells that have PCs on them, increases the amount of gold in their inventories by 10%, and decreases their hunger by 0.1. All the packing and unpacking, and the drilling down through the fields; the pattern-matching... egregious, pointless busywork. With lenses: f :: Game -&gt; Game f = cells . mapped . agent . _Just . _Left %~ (inventory . ix Gold *~ 1.1) . (hunger -~ 0.1) What goes on here: * The regular fields have an underscore in front of them (`_agent`), while the lenses made by `makeLenses` lack it. * The expression is split at `%~`. The left part is the path you wish to take through `Game`, the right part the thing you want to do. `%~` itself is the update-operator that applies a function. * `mapped` is a setter that is basically `fmap` and selects all values of the Map as targets. * `_Just` and `_Left` are prisms that go down sum types, while leaving the `Nothing`s and `Right`s untouched, respectively. * `*~` and `-~` are the lens-versions of the `*=` and `-=` operators you know from C. * `ix` is a traversal at a specific key (`Gold`) which does nothing if the key is not present. This example shows how lenses can do much more than mere setting and getting, i.e.: * setting all elements of a structure (`mapped`), * handling sum types (`_Just`, `_Left`), only changing anything if they find the right constructor, * chaining assignments (`(inventory . ix Gold *~ 1.1) . (hunger +~ 0.1)`), * selecting elements at specific indices (`ix`).
 f = cells . mapped . agent . _Just . _Left %~ (inventory . ix Gold *~ 1.1) . (hunger -~ 0.1) Is that ever readable though? o.O
Yes! Most programming languages lie to you and give you a false sense of how difficult programming really is to get right. And then it blows up at runtime. Most software is just not well thought out, and a lot of languages (or compilers) let you get away with that. As /u/michaelochurch put it, "Haskell crushes imprecision of thought."
It's way more readable than the alternatives.
There are really two options: 1. Only allow `zip` to zip lists of the same length 2. Truncate to the length of the shorter list Of the two, the second is much more useful. The docs are pretty clear about the behavior of zip, as well.
Multi-constructor data types can have named fields in each constructor. This is horribly unsafe for fields that only exist for some constructors.
I've been hovering nervously at the edge of `Lens` for months, and holy shit, yes it *is* readable. This example is fantastic for me. Let's see if I can help: first, note that `cells`, `agent`, `inventory` and `hunger` aren't part of the `Lens` library, they're the generated accessors matching up to records in the `data` definitions above. Also, by inference, `Gold` is a constructor for the `ItemName` type. Here's a C# equivalent of that line: // f :: Game -&gt; Game Game f(Game game) { // the Haskell version is point-free, but "game" is implied foreach ( Cell cell in game.GetCells() ) { // cells . mapped PC pc = cell.agent; // . agent if ( pc != null &amp;&amp; pc.entityType == Entity.PC ) // . _Just . _Left { // %~ double gold; if ( pc.inventory.TryGetValue(ItemName.Gold, out gold) ) { // (inventory . ix Gold pc.inventory[ItemName.Gold] = gold * 1.1; // *~ 1.1) } pc.hunger -= 0.1; // . (hunger -~ 0.1) } } // ...in reality this would return void, but for the sake of the example... return game; } 
What a very good answer. But I think it depends on how do you like to think about your programs. Many people want to see the exact behavior of some part of their programs. I prefer better type systems though, because too much time is spent in the discovery of runtime errors.
I mentioned Functor -&gt; Applicative -&gt; Monad in another comment. At the end of [section 4.1] of Typeclassopedia, you'll find that originally there was a Pointed typeclass that sat between Functor and Applicative, and was the home to Applicative's `pure`. I kind of liked the consistency of each thing adding only one thing - Functor has fmap, Pointed has pure, Applicative has `(&lt;*&gt;)`, and if Monad had the Applicative constraint, then `return` - which is exactly the same as `pure` - wouldn't be necessary, so Monad would really just have `(&gt;&gt;=)`. The Applicative debacle also meant that we needed `liftM`, which for monads is just fmap, but the break in dependencies at Applicative meant we couldn't trust that fmap was implemented, so liftM was thrown in to pick up the slack, I guess. I might also say that it's an odd inconsistency to have words for some things - `pure` and `return` - but only infix operators for others - `(&lt;*&gt;)` and `(&gt;&gt;=)`. There's also the oddness of having map for lists, when fmap for the list functor is literally the same thing (defined as `fmap = map`). `map` has only been kept around for new users, so they see error messages about `map`, instead of about functor. It's a kind of inconsistency-from-newb-friendliness argument. It's odd to have a map just for lists, when all the other maps out there are defined via Functor's fmap. Oh, and as for the fate of Pointed, Typeclassopedia links to [this](https://wiki.haskell.org/Why_not_Pointed%3F).
If you really want to talk about inconsistency in Haskell, have a look at Cabal, which is a kind of [inconsistency generator](https://www.google.com/search?q=cabal+hell), mostly used for stochastic reasoning and dependency resolution through simulated annealing. It's sort of a middle-ground between the Monty Hall and Monte Carlo methods. It helps us come up with higher-order questions about our code, such as "Why even code, when there are so many other things?"
There I don't know of a general construction. The problem is ultimately that the adjunction isn't unique. I don't know what category the adjunction you "intended" went through. Lots of adjunctions give the same monad / comonad.
A bunch of things spring to mind. That there are a complete lack of tutorials. Solving new problems / learning new libraries tends to be an academic exercise. The learning curve for every new thing I implement in Haskell is much higher than the same learning curve in other languages. On the other hand, once I've learned something I like being able to reason about the code easier. Makes the pain worth it - but no less painful. Lazy evaluation means that reasoning about logic errors is really difficult. Stepping through code with the debugger was something I gave up on after a handful of attempts. Even using `trace` is problematic because it's behaviour can be inconsistent. Unfortunately I found that fixing bugs is usually best done by starting over. Fortunately Haskell code is really easy to compartmentalise and hence triage. There are performance costs to a lot of abstractions and also reasoning about performance is an entirely different exercise to an imperative language. Unfortunately there isn't a library for absolutely everything like in say Python or Java (or even JS). Lastly - writing imperative code can become a frustrating affair... like occasionally forgetting about mutability by default, thinking what do you mean `X x' = x.set(y);` is an error? How am I suppose to get a new value if `set` returns null? And what's with all the `{ ; }` everywhere? Or working in a Duck Typed language and getting super annoyed that you have no idea what type anything is. Though I have been thinking about giving Rust a go lately. I feel like it could be a good imperative language for a Haskeller to learn. Still not a fan of all the `{ ; }` though.
Go for it! I'm all in favour of improving the ecosystem of preludes and letting market forces do what they will.
&gt; Slack is just so very well thought out and it can do things IRC cannot Can you expand on both what makes Slack very well thought out and what things it can do that IRC cannot? Maybe you can get some of the IRC folk to join instead of just lurking on IRC forever ;)
Two points I said above plus: link expansion, file attachments, history, search.... there is more but these are obvious :) IRC is also useful but I think for these type of things Slack will shine in the future... of course some will always prefer to stick with IRC, some will use both (like myself) until there is still activity on IRC channels 
In short &gt;What problem is lens trying to solve? The update record syntax in Haskell is a pain. To take /us/Tekmo example to do something like `atom.position.x = 1` in an OO language, you'll have to do atom { position = { x = 1 } } which, is still alright but what if you want to do `atome.position.x+=1` ? It becomes atom { position = { x = x ( position ( atom)) + 1 } Things get really complicated really quickly. &gt; How does it succeed By allowing you to write `atom.position.x+=1` as `atom &amp; position.x %~ (+1)`. Ok it's a bit crypting but close enough to the OOP version. Apparently you get use to it. &gt;and how does it fail? The lens library you are talking about takes ages to compile, which wouldn't be a problem on it's own, if people were not avoid cabal hell using sandboxes. Lens+cabal sanboxes means each time you start a new project, you'll have to wait for a while before doing anything. It makes working with nested record to easy, and some people see it as cheating ;-) What the fuss then ? It becames an abstraction (a bit like monad) which in fact occurs to be 1000 times more powerful than just simply solving the initial problem. There are lots and lots of goodies coming with lens (you can for example traverse through collection a like in JQuery but for free and with a unified syntax). 
I remember seeing an argument about conduit being better at freeing resources where pipes didn't... I could be totally wrong about that ever happening though. Does anyone know if this is true or false?
OK :) Good luck with yours too! We think there is space for both... So to clarify we decided to only have purely functional languages to really study the core principles. JavaScript for example is not such language and is probably best discussed in http://fedsonslack.com/ or even your community .. at Becoming Functional (http://functionalslack.com) it is still welcome but not emphasized and there is no special channel... as there isn't for Scala for example... and we want to build strong Elixir presence (that's going well), there has been quite a lot of new members recently and esp. Elixir part is active.. Haskell as well, Clojure not yet so much. Elm hopefully also soon... To learn is to live, keep the λ flying! :) 
&gt; it looks like your applying "fmap (+1)" to "do", which makes no sense. That's precisely why it *should* work. If there are two possible interpretations, of which one doesn't make sense, obviously it should use the one that does make sense.
&gt; As for a debugger, the vast majority of languages does not have debuggers Really? Of all languages I've had the misfortune of using professionally (which spans from C to Python and JavaScript in the abstraction spectrum), exactly 100% have had a good debugger. (I was initially going to say PHP was the exception, but then I looked it up and PHP does have debuggers.) Of course, the languages I've had to use professionally are far from "all languages", but I fail to see how that is relevant in a thread about learning Haskell, which most people do coming from languages of the kind I've used professionally.
No this is exactly that. The focus of conduit is efficient resource finalization. I suggest this : http://www.haskellcast.com/episode/006-gabriel-gonzalez-and-michael-snoyman-on-pipes-and-conduit/
&gt; I actually thought someone would mention zipWith f [1..] as a counter example. I use `zip a (tail a)` all the time.
&gt; You often end up writing more import than code itself. Sounds like a property you crave for in most other language.
&gt; I'm shocked that the community thinks using "id" or "indexes" to represent mutable graphs is acceptable even though it's reinventing pointers with the same level of unsafety Wow, this is not nearly as unsafe. You don't get arbitrary memory writes when inserting into a Map with a key that's under a hostile user control.
Because it ships with the compiler...
What are these for?
https://github.com/haskell-suite/halberd/
And I think it's worth saying: The extra functor requirement on lenses which makes them different from what /u/josuf107 is doing is there for two reasons: 1. It allows you to use lenses in side-effectful contexts, such as IO, and 2. It allows you to *read* from a "setter" function, so you can use the same lens for both reading and writing.
I think the real problem is that Haskell got monoids backwards. Being a monoid is not the property of a type, it's the property of a type T, a function (+): T^2 → T, and a value *&amp;epsilon;*: T, but type classes do not allow us to express this idea.
&gt; I doesn't cost anything to just add now a Monoid instance to Num Yes it does; it costs everyone else the unwanted instance.
Having a vector of things, and reference to their indices looks really like having a heap and pointers. You don't get indeed arbitrary memory writes in the full memory but you do get them in your own local heaps. You can get an index, add 1 to it, and modified the newly refered data. You can also have indexes out of bound. You can remove an object from a vector and by that make all indexes referencing to the wrong object. Yes all of that is manageable, but it's also manageable in C (not more, not less). The only difference is each type of data, has it's own allocated pool of memory but that doesn't really change the problem. What is disappointing is all the solution have seen for this problem are "unsafe" and everybody seems to consider it as normal: "after all, it's Haskell, therefore it is type safe by definition". Am I really the only one seeing this as a problem (even though questions related to mutable graph are recurrent)?
That is certainly true. Instances being global, you can't, in good conscience, prefer one over the other in case of real ambiguity. Ziplists are a novelty in contrast to the natural ([a], ++, []) algebra, but (N, +, 0) isn't really preferable to (N, *, 1).
The problem is code that was previously correctly rejected as nonsense starts to typecheck.
This only works when there is *one* function and *one* argument etc. Clearly functions like `map` are not problematic. It is when the number of variables in scope is large descriptive names are needed. Haskell functions are meaningfully short compared to other languages anymore. Only when comparing to C or C++ is this true, not compared to JS or Scala for example.
https://github.com/dan-t/hsimport
I just wanted to point out that in Haskell you will have a logic error, whereas in C you will have at best a runtime crash, and might end up with an exploitable vulnerability. So I agree with you that if you mean "less bugs in my graph algorithms", there is not much to gain by moving from raw pointers to indexes in immutable maps. But if you mean "less devastating bugs", you are much better off with Haskell. Just to answer the "with the same level of unsafety" part.
I'm not sure what you mean by logic error. Accessing something out of the bound of Vector gives you a Out of bound exception which is similar to runtime crash. Also, having wrong indices and modifying the "wrong" slot, is equivalent to memory corruption and as devastating. I can provide some example code if needed. 
Not just 2: Also, `(max,minBound)`, `(min,maxBound)`... We use one of these to give rise to the tropical semiring, in which max is playing the role of addition and (+) plays the role of multiplication.
Let's classify the inconveniences * Import: Language * "If it compiles it works": Community * Debugging: Tooling * Indexes: Community * Num a =&gt; Monoid a: Language or libraries. I lean toward the former. 
I have `Num` types where the `Monoid` is much better served by being something other than `(+)`. Example: newtype Poly a = Poly [a] `(+)` is one choice of `Monoid`, but if we instead take 'substitution' as the Monoid, then we get some remarkable utility. We don't need to come up with a one off combinator for substitution, logically, `f(g(x))` becomes `(f &lt;&gt; g)(x)` in this representation. The identity for substitution `mempty = x` gives me a name for the `x` in my polynomial.
&gt; I would be happy with just a tool help doing equational reasoning. Have you tried HERMIT?
&gt; Most software is just not well thought out That seems like a gross simplification, and also not true. I would say the majority of software is well thought out - that doesn't prevent all the problems a codebase will have during it's lifetime. New features, budget constraints, time constraints all factor in to how someone decides to write their program. When someone has to make a compromise due to these things it may produce code that appears to you to be low quality, or sub-optimal, but that does not mean it wasn't thought out! GHC isn't going to protect you from annoying customers! :P
What do you mean? Something like Imports.hs: export qualified Control.Applicative export qualified Data.Monoid Main.hs import Imports ... use Control.Applicative.pure ... ... use Data.Monoid.mempty ... 
The problem is that the phrase "if it compiles it works" is ambiguous. It can validly mean "if it compiles than vast swathes of undesirable behaviour cannot appear at runtime". Often it is misunderstood to be a claim that "if it complies then it is guaranteed bug free, perfect, silver bullet software". The latter is so implausible that I can't understand *why* someone would interpret it like that, but when communicating with those outside the community it helps to bear that in mind ... 
Do you mean `instance Num a =&gt; Monoid a`? Doesn't it overlap with every other `Monoid` instance?
Ok, I didn't think of newtypes being a Num and having their own Monoid instance. I totally agree with that and that's the argument I was waiting for :-) However, that still stands for *pod* num : ie `Int`, `Double` etc ... 
The instance for list is also special in that (morally) it is the "largest" monoid in that we can naturally extend any function `Monoid m =&gt; (a -&gt; m)` to `[a] -&gt; m` with `foldMap`.
No, what is it ?
Could you elaborate on the AST methods ?
 instance Monoid a =&gt; Monoid [a] where mempty = repeat mempty mappend = zipWith mappend But I think it would be bad for `s &lt;&gt; t` to mean different things depending on whether they had type `String` or type `Text`.
I meant, general instance for List, ie without class constraints.
Is there a language with a more algebraic type system? 
That's a good question, I don't know any. One could however implement this for Haskell: Give programmers the ability to construct type class instances at runtime by providing a set of functions with the right signatures for a given type.
It depends what you mean by "pointer". That word represents a whole mass of different abstractions and, while most (not all) are implemented using machine addresses, I'll do the captain obvious thing and point out that Haskell implements things with machine addresses anyway - e.g. referencing lazy-evaluation thunks. What matters is the set of rules the compiler enforces. If a hostile user can get control of machine pointers and your raw memory, you're pretty much screwed whatever. Pointers as provided by various languages aren't just arbitrary memory writes, and aren't susceptable to C-style memory curruption bugs. For example there are languages that don't support pointer arithmetic, don't allow pointers/references to be cast to/from other types, and aren't susceptable to dangling pointers because you can't have pointers to local/stack variables (the last point was e.g. true in Ada 83). So yes, in a sense, some uses of `Map` are really just re-inventing a particular class of pointer - potentially inefficiently, though of course an immutable implementation has its own advantages with its own potential efficiency savings depending on the application. Actually, I have a module with a typeclass for references - providing overloaded versions of the usual operations for `IORef` and `STRef`, including an implementation based on `Map`, so you can use this to literally write `newRef`, `readRef`, `writeRef` and `modifyRef` and not know whether the underlying implementation is `IORef`, `STRef` or items allocated in a `Map`. Obviously you also wouldn't know which monad, as that depends on the pointers too (for `Map`-based pointers, usually it would be `State`). Of course you get some limitations - it needs to know (for all pointers) the one type that your pointers reference, obviously items in a map won't get garbage collected etc. However, it still e.g. guarantees no dangling pointers and no "arbitrary memory writes" in much the same way that `ST` does. But clearly when your references can be implemented as either a `Map` or an `IORef` (or even potentially an FFI `Ptr`, with further no-garbage-collection caveats) using the same notation with the same type constraints, there's a very strong sense where neither is safer than the other. 
It's not very pretty but that is possible too. See [this](https://www.fpcomplete.com/user/konn/prove-your-haskell-for-great-safety/dependent-types-in-haskell) and [this](http://stackoverflow.com/questions/8332392/type-safe-matrix-multiplication) for example.
Yes in fact there is. You can use the excellent [data-memocombinators](http://hackage.haskell.org/package/data-memocombinators). So to memoize the collatz function, you would write something like import Data.MemoCombinators collatz :: Integer -&gt; Integer collatz = integral go where go 1 = 1 go n | n `rem` 2 == 0 = 1 + collatz (quot n 2) | otherwise = 1 + collatz (3 * n + 1) Viola! You can also do something clever with laziness since Haskell's got this sort of memoizing like aspect to how it handles computation built in. We can write an expression like `1 + 1` and it gets stored a lazy "thunk" which is computed once and then destructively updated to avoid recomputation. This means if we create say a vector of lazy thunks we can avoid recomputation by only calculating each entry once. For example, here's how we'd build a vector containing all the fibonnacci numbers up to a certain point. import qualified Data.Vector as V fib :: Int -&gt; Integer fib n = V.last vec where vec = V.generate n go go 0 = 0 go 1 = 1 go n = vec V.! (n - 1) + vec V.! (n - 2) 
Diagrams is supported in IHaskell in general, but I'm not sure we'll get it in try.jupyter.org. We are a bit disk-space constrained on the Docker images, I believe. But if we add any more libraries, diagrams is on the top of the list.
&gt;&gt; When someone has to make a compromise due to these things it may produce code that appears to you to be low quality, or sub-optimal, but that does not mean it wasn't thought out! &gt; &gt; Actually that is exactly what such a compromise is. I'm not quite sure I follow. So any code where a compromise has been made is not well thought out? That seems demonstrably untrue. As for your argument on OO: I'm personally not a big fan of it and I would acknowledge that it's not great for code reuse and relies to heavily on mutating state invisibly. However I do not agree that there as much of an issue as you do. For me, OOP is not that great of a model, but that doesn't mean the code written in it is automatically rubbish, short-sighted or doomed to fail. People have written very clean, very modular C but C doesn't have typeclasses or a HM type system or purity. C or Java or C# or any language may have features which lend themselves to code that is more difficult to understand but that doesn't automatically make the majority of the code written in those languages badly thought out. You're going to need a much stronger argument than software is written in OOP languages to convince me the *majority* of software has not been thought out and has been written in a short-sighted way. The claim was very specific to software not being well thought out, not language features.
… those and the monoid laws
&gt; I'm not quite sure I follow. So any code where a compromise has been made is not well thought out? That seems demonstrably untrue. Any compromise made due to budget or time constraints is what I meant. Sorry, I should have included that part in the quote perhaps. If you had the budget or time to think it through and implement it properly you wouldn't have a compromise on your hands. As for the stronger argument, that would be the current percentage of projects that fail, go over budget, under scope, over deadlines,... I mentioned in my earlier post. As well as the fact that we essentially write the same kinds of code over and over again in many projects. If it was well thought out many things (e.g. user login systems or CRUD form generators) would be reusable and written once or twice per language, not once or twice per project.
&gt; Granted, first-class language support would be much more convenient. It's weird that Haskell has no first-class support for that because type classes are basically syntactical sugar for tuples. Instead of sum :: Monoid m =&gt; [m] -&gt; m one could also write: sum :: (m -&gt; m -&gt; m, m) -&gt; [m] -&gt; m but that's not possible.
&gt; I'm shocked that the community thinks using "id" or "indexes" to represent mutable graphs is acceptable even though it's reinventing pointers with the same level of unsafety We do have a tendency to add a veneer of purity on top of imperative solutions and be satisfied with it instead of continuing to hunt for even more functional abstractions. The most famous example of that, as [explained by Conal Elliott](http://www.haskellcast.com/episode/009-conal-elliott-on-frp-and-denotational-design/), is how monadic IO was considered to be such a good solution that most researchers stopped investigating other functional approaches to interaction, such as FRP. (Conal Elliott is the inventor of FRP.) So it might very well be that the way mutable graphs are normally represented in Haskell could be made more functional, and I'd be quite interested in having a conversation about alternatives. Here are all the ways of representing graphs that I can think of, and how nodes are represented in each. 1. As an adjacency matrix. A node is an index, corresponding both to a row and to a column in the matrix. 1. As a set of edges. Each edge is a pair of nodes, which can be represented using whatever representation you want. You probably want at least an `Eq` constraint on them in order to do anything. 1. As a function from a node to a set of neighbours. The representation of the node is whatever the function expects as an argument. Again, you probably want an `Eq` constraint on that. 1. As a map from each node to a set of neighbours. Similar to the previous representation, but makes it easier to reason about resource usage. 1. As a lazily-constructed set of nodes, whose representation includes links to neighbours. Think of an infinite tree in which nodes which should be identical share the same memory. 1. As an imperatively-constructed set of nodes, whose representation includes a mutable reference to a set of neighbouring nodes. 1. Using the [fgl](http://stackoverflow.com/a/30469287/3286185) library. The part about decomposing the graph into a node, its edges into the rest of the graph, and the rest of the graph is pretty interesting. Nodes seem to be represented as integers, but in principle I don't see why we couldn't use any of the other representations and still be able to implement such a decomposition function. They're probably using a clever representation which allows their decomposition function to be fast if you use Int. Only number 6 is a truly mutable representation, for the rest we'd simply (and perhaps inefficiently) construct a new immutable graph by examining the previous immutable graph and replicating most of its structure. Not all of those representations require nodes to be represented as indices, but of course if you are holding a node from the previous immutable graph and you want to find its corresponding node in the new immutable graph, you're going to need something to identify them. Are those the IDs you are objecting to? From your other comments, it sounds like you're objecting to using integers as IDs, but with most of those representations, you don't need to do that. Presumably those nodes in your graph are representing something, like game states, so you can simply use that. Or if they're really abstract entities for which you need to create a separate ID, and you're worried that you'll accidentally add 1 to one of those IDs, you can use [`Data.Unique`](http://hackage.haskell.org/package/base-4.8.0.0/docs/Data-Unique.html), which satisfies `Eq` but not `Num`. Anyway, I'm not trying to say that using IDs is obviously the right way to go, as I said I'm eager to have a conversation about more functional alternatives. This is just me starting off the conversation by explaining why I think IDs are needed.
Maybe it would help to use fewer operators? over (cells . mapped . agent . _Just . _Left) (over (inventory . ix Gold) (* 1.1) . over hunger (subtract 0.1))
I don't understand the downvotes. When I transitioned from C# to F#, this was the most notable and obvious difference. I can't imagine it's any different going from C++ to Haskell. Perhaps it's not the languages specifically to blame, but a feature of the base libraries, however "if it compiles it works" is very real. Even if it isn't always literally true, you are much less likely to be stuck debugging something.
I think the red herring there was focusing on mutability. As you say in your post: &gt; With an OO language, it's pretty easy. You have Boxes, which are located on a shelf, which belongs to a racking etc ... Everything has a name, a position and things are mutable. .i.e I can rename a shelf, turn a box easily. That's how I'd model it in Haskell: data Warehouse = Warehouse {warehouseIsles :: ![Isle]} data Isle = Isle {isleShelves :: ![Shelf]} data Shelf = Shelf {shelfBoxes :: ![Maybe Box]} data Box = Box But without the mutability. Forget about mutating. [Here's an example paste of using the data structures I defined](http://lpaste.net/8783033985165950976). I can add a box to the warehouse and it will pick the first isle and shelf where it can fit from the bottom up. Hence the output: λ&gt; fromJust (foldM (flip addToWarehouse) warehouse (replicate 50 Box)) B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B _ _ _ B B _ B B B B B B B B B B B B B B B B B B _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ If I want to remove the third box from the second shelf on the second isle I can just have a function `removeBox :: Int -&gt; Int -&gt; Int` and do `removeBox 3 2 2`. If I was making a GUI, I wouldn't have to use numbers, I'd just build up the UI so that it knows how to produce the data structure with or without a box in a slot. If I wanted to label the boxes with a globally identifiable barcode, rather than reference them by order, I could have `removeBox :: Barcode -&gt; Warehouse -&gt; Warehouse`, which would search the Warehouse and produce a new warehouse without that box. It could also be `getBox :: Barcode -&gt; Warehouse -&gt; (Warehouse,Box)` which would produce a new warehouse and the box in question. And so on. With the pure data structure you can also trivially go back in time to trace modifications or try out scenarios speculatively (which is partly what I do in `addToWarehouse` with `thisIsle &lt;|&gt; nextIsle`, that tries two possibilities and picks the first one that works). Decoupling everything into some kind of relational database with IDs mapping objects to objects kind of defeats the purpose of modeling this in Haskell.
There are at least two, forward and backward concatenation.
I really enjoyed your [diagrams example](http://nbviewer.ipython.org/github/gibiansky/IHaskell/blob/master/notebooks/IHaskell.ipynb) ("In `[20]`"), I wish `try.jupyter.org` supported it.
`-Wall` with `-Werror` is a bad combination IMO. Some warnings like name shadowing is becoming very annoying very fast. IMO there are some cases where you want to shadow names. For example: fun xs = iter (preprocess xs) -- assume this is more complex and can't be written in point-free style easily where iter [] = ... iter (x : xs) = ... iter xs ... By shadowing `xs` in `iter` you make sure you don't loop forever by passing parent function's `xs` to iter in `iter` body by mistake.
I had a *very* similar idea for a project I'm working on! Would you mind sharing your code?
How is shadowing better than writing `iter (y:ys)` in this example?
You can do `iter (y : ys) = ... iter xs ...` and loop forever.
Should probably specify that this is specific to libraries. &gt; Does your README make the license obvious? I don't think I've ever seen a library that mentions the license in the README outside of a FAQ.
The bug here isn't in `zip`, it's in the incorrect list of ids. Enforcing a length invariant on `zip` won't guarantee anything because the lengths could happen to match up because of errors in calculating *both* lists. This is a case where the problem should be solved at its source rather than allowed to propagate until you lack enough information to provably solve it. `zip` isn't a partial function and shouldn't be lumped in with them. It's just a function with a denotation that is sometimes not what you meant.
I guess I'm used to the idea of memory corruption in C, where some part of your program, if it uses a bad index, can modify anything in the memory space of your program, including code. That, to me, is a worse class of error than writing to the wrong element in a map, or erroring out with an exception. Your haskell program might not work right, or might terminate, but at least it doesn't keep going with compromised security and do hacker stuff on your computer. 
First, I'm really impressed by your example and honored that you took the time to make a working example. However my problem is about a real warehouse and is much much more complicated. First, boxes have different sizes, and there is no slot. Boxes are just chucked wherever there is space and the "empty slots" are the result of the stacking of the current box in the shelf. Moreover, boxes are already there and can't be moved, boxes can be rotated (see picture in red circle) if need to fit in shelf, some are too heavy to be put on a top shelf etc ... . The fact there is no slots means you can't really predict how many boxes of a given size you can put in a shelves without looking at the exact content of the shelf (you can speed up things by keeping for each shelves a list of available rectangles but it's still dynamic and needs to be updated each time a new box is added). The only way is to try "physically" (but virtually ) some box arrangements. There is about 1500 boxes and 100 shelves so trying all the arrangement is not option. To be able to try those arrangement I need a mutable state (I think) because each arrangement correspond to "place" each box one by one. I can't have a different state for each step (placing a box) even though I need the actual "sub state" to know where to place the next box. I attached a [picture](http://i.imgur.com/vHfjaZq.jpg) which is an extract of whole result. White boxes are boxes already in place (and can't be moved). Boxes in green and yellow are the boxes coming in the next delivery. 
For 2, the functor is Constant right? I tried one time to see why you needed rankNTypes for Lens. I thought: why not just make a Lens type parametrized on the functor? You can make a function that returns a lens parametrized on any functor, but once you use it the type of the functor gets stuck on just one type, which is mostly only a problem if you want to pass your lens around or implement a getAndSet method.
I like the view that, say, a monad is a triple, not a data type with some implicit operations. However I don't think that type classes quite express these relationships in a first class fashion (for some meaning of "first class"). In particular it is possible to express that two type classes share a particular data type but not that they share a particular operation which makes it difficult to make inferences between type classes. Type class design is not a mistake it is a deliberate design decision, but it come with certain advantages and limitations.
&gt; [`Data.Unique`](http://hackage.haskell.org/package/base-4.8.0.0/docs/Data-Unique.html) mindblown.gif No, seriously. That is so *ridiculously* useful. And it's in `base`! How did I overlook this? **Edit:** holy crap, it's also `Ord` on creation sequence?
Sure, you have a [packing problem](http://en.wikipedia.org/wiki/Packing_problems) which is [NP hard](http://en.wikipedia.org/wiki/Bin_packing_problem) in the worst case, but it seems like in your case at least you only have to pack a couple objects at a time rather than re-arranging the whole container for the best packing. It is an interesting problem to work on and should be a nice test for Haskell. But my example wasn't to demonstrate an exact implementation of your domain but to show this kind of problem can be modeled purely, despite your natural instinct to think about it imperatively. If you're interested in experimenting further, probably checking out examples of [constraint satisfaction problems](http://en.wikipedia.org/wiki/Constraint_satisfaction_problem) in Haskell could be a good way to get inspiration and confidence that it's doable. [N-queens](http://rosettacode.org/wiki/N-queens_problem#Haskell), [connect-4](https://gist.github.com/jaspervdj/3238203), [Sudoku](http://web.math.unifi.it/~maggesi/haskell_sudoku_solver.html) and [Sudoku on the HaskellWiki](https://wiki.haskell.org/Sudoku), things like that. Just googling or looking on hackage or lpaste there will be a bunch. I implemented a word cloud generator back when I was more newbieish, which is essentially a restricted packing problem. [Here're some of its outputs](https://imagizer.imageshack.us/v2/646x969q90/r/514/algosxh9.png). The code is [here](https://github.com/chrisdone/wordcloud/blob/master/Graphics/WordCloud.hs), but it's so old and probably doesn't build anymore. At any rate the crux of it is [here](https://github.com/chrisdone/wordcloud/blob/master/Graphics/WordCloud.hs#L119-L145) which uses mplus (these days aka &lt;|&gt;), essentially an [amb](http://mitpress.mit.edu/sicp/full-text/sicp/book/node91.html) operator that picks between two branches. Also possibly of interest for motivation might be [this page](http://chrisdone.com/posts/twitter-problem-loeb) in which I list a number of varying (but all pure) solutions to the constraint satisfaction problem posed by Twitter. The best one is probably the fastest vector one, my loeb one is more of a nice curiosity than a serious solution. So in the end you would just replace my function for adding a box with something that doesn't just put in the first available slot but maybe takes a list of boxes to place, and tries various configurations for the most economic fit. I'd imagine it would boil down to generating a list of solutions (e.g. `[Isle]`) and rating them with some rating function (giving `Isle → Rating`) and picking the best rated solution. That's the logic of it (of course you can optimize by having it generate-and-rate as it goes and have it stop generating a solution if at any point it exceeds the rating of previous solutions, etc.). To do this in a pure Haskelly way, it has to be thought about in terms of generating new things instead of changing existing things.
I’m biased, but I think the things CJ East did with Frames and Chart are pretty awesome. It still amazes me that all of those pieces work so smoothly in IHaskell. There’s currently a small hiccup getting Chart updated, and I have to push Frames to Hackage, but I think getting expressively typed data from some CSV file hosted somewhere, top notch performance processing that data, and in-line charting is a really compelling demonstration.
The biggest benefit of a refactor-friendly language is that you can easily change course rapidly if you realize that your initial approach to a problem was incorrect. For example, completing a project is not a binary "working"/"not working" proposition. Often you will complete a project in stages, beginning with a minimally viable prototype and then iterating on that prototype until you have a fully featured solution. If you can't easily refactor, you will often discover along the way that you made some bad choices in your initially minimally viable prototype, but you've sunk too much time and effort into it already and you can't easily change course. In a refactor-friendly language like Haskell you can make significant and sweeping changes while still preserving functionality.
Using the wrong identifier is an easy mistake to make when refactoring. That's why it's nice if the compiler can warn you about dubious choices like name shadowing which might have been entirely accidental.
And a third option: define `iter` at the top level rather than in a where clause. Not always the best idea, but it can be beneficial to have it available to play around with in ghci.
Think of `where` as defining a local declaration instead of a top-level declaration. For example, instead of writing this: positive :: Int -&gt; Bool positive x = x &gt; 0 keepPositives :: [Int] -&gt; [Int] keepPositives xs = filter positive xs ... you could instead write: keepPositives :: [Int] -&gt; [Int] keepPositives xs = filter positive xs where positive x = x &gt; 0 These locally scoped declarations can also reuse locally bound variables. Here's a (contrived) example: pairs :: [a] -&gt; [(a, a)] pairs (x1:x2:xs) = pair:(pairs xs) where pair = (x1, x2) pairs _ = [] Notice how the `pair` declaration has access to the `x1` and `x2` bound variables.
&gt; zip isn't a partial function and shouldn't be lumped in with them It wasn't my intention to lump `zip` in with partial functions (or to insinuate that it's buggy), only to label it as dangerous. I could have added `unsafePerformIO` to the list, there are many dangerous functions that aren't partial. It has been my experience that the worst bugs (data corruption) I've encountered over the past year could have been prevented by using `zipWithExact` instead of `zipWith`. In light of that, I don't think it's too objectionable for me to want to label `zipWithExact` "safer". &gt; This is a case where the problem should be solved at its source rather than allowed to propagate I 100% agree, I did end up fixing each problem I've run into at its source. However, it was only because I replaced `zipWith` with `zipWithExact` that the problems revealed themselves in the first place (after all, I didn't write buggy code on purpose, I had no idea that the issue existed there). I suspect you wouldn't find the same invariant expressed in the type system objectionable: do ids &lt;- readIds records &lt;- readRecords ids return (zip &lt;$&gt; fromList ids &lt;*&gt; fromList records :: Maybe (Vec (Id,Record) n)) Unfortunately I don't have access to anything like that, so I've had to settle for `zipExact` instead: do ids &lt;- readIds records &lt;- readRecords ids return (zipExactMay ids records) 
No state... so no methods on objects at all
Let's say your package depends on `LibX` and you don't have an upper bound tight enough. When a new `LibX` is released it may export one more name and break your package because now it's shadowing that new exported name.
many extensions and accompanying or affected pragmas have fluctuated in the folding of their edge-cases together with each other my brain has elided the details, but i have seen this many times anecdotally
I second this emphatically. Haskell lets the compiler find all stupid mistakes you would have ordinarily discovered at runtime (maybe eventually.) 
For the included documentation: - Ensure that there is a statement of the code's purpose or utility - Ensure that there is an example of how typically to use it, not just an abstract description of the API signature
I think as a profession, software developers have not done a good enough job of selling quality. And accordingly most languages do not have good enough support for building modular, robust, refactorable systems with strong compile-time correctness guarantees, which are important for high-quality software. That is where GHC will protect you--when your annoying customer wants tons of "minor" changes you can be pretty confident that when implementing them you won't subtly break everything else in the process because the compiler won't let you. Meanwhile the language is also high level enough that you can deliver features timely if you know it well.
I'll blame you if anything ill comes of it, but what the heck: http://hackage.haskell.org/package/lens-simple-0.1.0.0 It imports the haddock material from lens-family together with the combinators. Maybe this could be better arranged. 
*image macro
Possibly, but the keys will still be in the map anyway unless some kind of discard-dead-keys scan was included explicitly. The kinds of use I had in mind (theoretically - this has never been used seriously) were rather like using `runST` - you have a single algorithm that allocates and uses some pointers then delivers a result and that's it. Although single items in the map can't be garbage collected, once it's finished with, the map as a whole certainly can be. The real trick would be using this with the FFI `Ptr` type, particularly without leaking memory but there's other issues too. After all, as well as being non-garbage-collected and not having a convenient way to find all relevant pointers for explicit cleanup, `Ptr` *does* support casting, pointer arithmetic etc - after all it's designed for interop. with C. So long as those `Ptr` values can't leak out of the interface to that module (so references can only be used according to the `IORef`-style rules) that's not necessarily a problem, but it also defeats the whole point of using `Ptr`. TBH it was an interesting exercise for probably an hour or so (I seem to remember fighting the types a bit), and then I forgot about it until today. 
`Memo a` is a type synonym for ` forall r. (a -&gt; r) -&gt; a -&gt; r`, so `integral :: (Integer -&gt; r) -&gt; Integer -&gt; r` so we apply it to `go :: Integer -&gt; Integer` and get back the right type.
 type Memo a = forall r. (a -&gt; r) -&gt; a -&gt; r Takes a function `(a -&gt; r)`, which is `go` in this case (type `Integer -&gt; Integer`) and an `a` which is an `Integer` (the first argument to `collatz`) and returns an `r` (which is an `Integer` and the return type for `collatz`).
The invitation form is really intimidating, given the 24hr turnaround and the required field that asks about what you've done with FP. It makes it look like someone is evaluating these requests and only letting people in if they're cool enough. If I had less to show for myself I don't think I'd try to sign up. fpchat.com has a much nicer sign-up process -- no fuss. Though I would prefer to see a CoC on that page, which both encourages politeness/empathy and explicitly indicates inclusiveness.
Funny that my mind got stuck in believing `Memo` was a base type when of course it's just a higher level type. Thanks! The `forall` thing is interesting, what happens without it ? Why don't we need `forall a` too ?
I'm not an expert on C# but I think they have something called Linq which fights traversal code in a way similar to Lens. Advantages of Linq over Lens (again, not an expert): * Use words that are already taken in Haskell, like `all` * No squiggles (%~, *~, ...) just english words * Data access and transformation flows in one direction -- You see the lens access flows from left to right while the transformation goes right-to-left * Linq is carefully designed so you don't need to worry about operator precedence which causes me lots of problems writing Lens traversals -- I usually have to start with many parentheses and see which ones hlint tells me I can omit
&gt; intimi Everyone was invited that sent a request, usually within an hour... soon the form will be automated and there won't be any delay. The intro is sent back to each person who joins in a direct message with proposal that they put it on their Slack profile in the Team. 
Keep at it. As a dilettante computational linguist I'm always happy to see linguists getting involved in Haskell.
Thanks, Bud. I need all the nitpicking I can get!! I'm replying from a phone at the moment, and so can't see some of the code blocks clearly. * Yep, that Aeson TH was a typo. I wasn't intending for any of the code to be runnable. Maybe I should have pointed that out at the beginning. Related to that, I wasn’t even aware that the Aeson TH magic is re-exported by the top level Aeson package! Almost none of the Aeson tutorials I looked at mentioned this splice function. * I prefer to use splices with $. The ugly syntax reminds me that I'm doing something strange. :-) * I'll go over microlens now. Yeah, I still can't wrap my head around why Lens is so big. The sandbox for this package alone is already at 119 MB, and takes a while to build (especially due to all of the magic in nikita-volkov's record library). It doesn't feel "right" to produce so much code and pull in even more libraries just to bootstrap a few hundred lines of very simple Haskell. * Once again, I will look at your newtypes better when I'm not using a phone. This was one of the major problems I had to solve and I'm excited to here that there is a more elegant way of doing it! * I can't get hlint to stop complaining about stupid shit like splice syntax "errors" while ignoring actual issues. It used to work great in Haskell-Vim-Now, but now that I'm using Leksah it seems to miss a lot. Basically, I don't know how to use hlint from the command line, especially since it gets distracted by splices and multiparameter type classes. What's a single command to run hlint and have it take into consideration extensions which introduce new syntax?. * I'm still not sure about what the hell RecordWildcards does. It sounds magical (uses hidden info). I will go back to that code and see how to use the extension. * I overlooked the fact that the "inner" functions could be pure. Thanks for that wonderful rewrite -- IT'S GOING IN THE NEXT PATCH!! It really makes things much clearer. * Where do I find &lt;/&gt;? And thanks so much for the feedback, it's very encouraging. :-D I should have probably used something like BlogLiterately to create the post, as I very quickly learned that source code and WordPress don't go well together...
The main problem with mutability is to represent bi-directional relations. You can easily build them by "tying the knot" like newBox attributes shelf = let shelf' = shelf {boxes = box:boxes } in box = Box attributes shelf The problem is there is no way to update them (how do you update the attributes of box ?). So it's not a problem of performance optimization but theoritical one.
By pointer I mean a vector or map of object somewhere and reference to keys or ids somewhere else.
&gt; Maybe I should have pointed that out at the beginning. Yep! --- &gt; The ugly syntax reminds me that I'm doing something strange. Okay, that's legit. --- &gt; Almost none of the Aeson tutorials I looked at mentioned this splice function. *checks whether [nir tutorial](http://artyom.me/aeson) mentions it* Aha, it does, good. --- &gt; the Aeson TH magic is re-exported by the top level Aeson package! It's all in the aeson package, yep (but not exported by `Data.Aeson`, you need `Data.Aeson.TH` for that). --- &gt; I'll go over microlens now. It's good if you just need basic lens stuff or compatibility with all the lens tutorials out there (and it'll be better once I finish writing the docs and tutorial for it), but don't take it as something “recommended” yet, it's very much not. Just an option if you need a tiny lens package here and now (like I did). --- &gt; I'm still not sure about what the hell RecordWildcards does. 2 things: * When used as a pattern (`f X{..}`), it brings all fields into scope as actual values (instead of accessor functions). * When used as a value (`return X{..}`), it assembles a record from fields that are in scope (this proves to be useful when writing Aeson parsers, among other things). --- &gt; Where do I find &lt;/&gt; [Use the Hoogle](https://www.haskell.org/hoogle/?hoogle=%3C%2F%3E), Luke. (It's in the filepath package, which is pretty standard so there's no harm in using it.) --- &gt; And am thanks so much for the feedback, it's very encouraging. (If you're going to write more Haskell posts, I'm totally willing to serve as a beta-reader – just PM me.)
Funnily enough I just uploaded a package yesterday that had `-Werror` and hackage told me to get rid of it before it would accept the package. Might be worth noting.
We need to "bind" that type variable `r`, the crux of the matter is that memoization only depends on the *argument* and has no impact on the *result* type. It should work for for *all* result types. We could define type Memo a r = (a -&gt; r) -&gt; (a -&gt; r) but the only valid occupants should be of the form foo :: Memo SomeType r so to save some typing we just use that `forall` it encodes in the type itself that `r` must be treated polymorphically.
You are joking, right? This is an area where Haskell really shines. In fact, much of the work on Project Euler itself was actually done in Haskell by Daniel Fischer, the author of the [arithmoi](http://hackage.haskell.org/package/arithmoi) library. Here is Lennart Augustsson's classic quick-and-easy primes generator in Haskell. I haven't measured it, but it's hard for me to believe that the compiled version of this wouldn't beat your python code: primes = 2 : primes' where isPrime (p:ps) n = p*p &gt; n || n `rem` p /= 0 &amp;&amp; isPrime ps n primes' = 3 : filter (isPrime primes') [5, 7 ..] Here's my own slight modification of that, using a "wheel" to make it even faster: primes = 2 : 3 : 5 : primes' where isPrime (p:ps) n = p*p &gt; n || n `rem` p /= 0 &amp;&amp; isPrime ps n primes' = 7 : filter (isPrime primes') (scanl (+) 11 $ cycle [2,4,2,4,6,2,6,4]) If you want to know more about prime generation in Haskell, see the [wiki page](https://wiki.haskell.org/Prime_numbers). Others have posted about collatz elsewhere in this thread.
Look at implicits in Scala and canonical structures in Coq. They solve exactly this problem.
I'm probably wrong but the difference between with or without slot is immense. With slot, the problem is just to find the next free slot and the result can be stored as a list. Without slot, it becomes a 2d problem. As boxes can be of different size, to add a box to a shelf with already some box in it, you need to find if there is any existing gaps between the present box. Trying an arrangement boils down to place the first box in one corner, place the second to first available gap and so on. Finding those gaps is tricky : the boxes can have different size, you need to be able to compute from a list of occupied rectangle, a list of available rectangles. For performance reason, you don't want to recalculate from scratch this list of a available rectangles each time you add a new box, but want to keep a cache and only compute the intersection of the new box with previous list of available rectangles. Ok, maybe I'm doing premature optimisation but my actual program is quite slow. If you have time to waist, I strongly recommend you try this problem, it's an interesting one . About the representation, maybe I again doing premature optimization but if I model my warehouse as you did (which by the way, seems perfectly reasonable), each time I move a box, I need to the whole structure (or at least part of it). I have 1000 boxes, which I think is a big graph, and each time I move a boxe, I have to regenerate a new graph. Trying an arrangement, generates 1000 graphes of 1000 boxes. Then I have to "try" billions of arrangement, so I need each try to be reasonably fast. Once again, maybe I'm wrong and maybe there is no problem. Last but not the least, (maybe it's in fact the main reason why I think I need mutability )is I need to be able to do efficient lookup both way (ie , shelf -&gt; box and box -&gt; shelf), meaning I also need a box to refer to a shelf (or at least I think I need it). This is this bi-directional relation which is my main problem. I can build it by tying the know, but I don't see how you can update a graph build that way. Having said, that I realized writing this, that there is a couple of things I probably could do in a different ways but I literally had a week to do it (the deadline being a container arriving on the monday).
There's a significant difference between the (non-)safety of pointers in C and arbitrary indices into a Haskell Vector object. The term 'safety' in programming language circles is generally defined to be a bit less general than how you're using it. Roughly, it means that the evaluator of the language can't get "stuck", i.e. it can't ever run into a situation where there's a value that isn't fully evaluated but doesn't have an evaluation rule that applies to it. It is important to realize in this context that "throw a runtime exception" is a well-defined evaluation rule. In a safe language, anything that the language accepts as input has an evaluation path that's well-defined (i.e. the language definition knows how to handle it) even if the definition includes run-time errors for some situations that the programmer will need to pay attention to avoid. This is in stark contrast to C, where there are *plenty* of constructs that the language implementation will accept that have no defined evaluation rules. In the C specification, these are noted as "undefined", and if your program contains *any* of these, the specification declares your entire program to be essentially meaningless. An implementation may do *anything at all* with such a program; quite often, pointers to one type of object will end up referencing a *different* type of object, which is exactly what it means in a technical sense to violate type safety. In Haskell, getting the object at an index in a Vector (or some other indexable type class) of A will always either give you an A or a runtime error (assuming the index function doesn't return a Maybe), and will *never* return a value of some other type B. This is the essence of type safety.
 align :: [a] -&gt; [b] -&gt; ([(a,b)], Either [a] [b]) Well, the context in which I brought up `These` is that /u/twistier complained that a `zip :: [a] -&gt; [b] -&gt; Maybe [(a, b)]` would be strict. Your suggested signature here partially shares that property—forcing the `Either` forces the whole spine of the `[(a, b)]`. So maybe something like this then: data Alignment a b = BothEmpty | FirstEmpty (NEList b) | SecondEmpty (NEList a) | Aligned a b (Alignment a b) align :: [a] -&gt; [b] -&gt; Alignment a b align as bs = _ strictAlign :: Alignment a b -&gt; ([(a, b)], Either (NEList a) (NEList b)) strictAlign = _
btw, where is IncoherentInstances used? (in https://hackage.haskell.org/package/aeson-0.9.0.1/src/Data/Aeson/Types/Instances.hs)
`zip &lt;*&gt; tail` But you really meant `zip &lt;*&gt; drop 1`, didn't you? :)
Your prime sieve takes about 5 seconds to generate all the primes up to ~250k. My python example does the first million in less than a second. I went over the wiki page, and it wasn't satisfying. 
I find the more verbosely named one harder to read. i like functions to have long names and locals to have short names. makes it easier to see the dataflow at a glance. (unless the function is long, either split it into multiple, or provide longer names, because the intermediate computations don't often have type annotations, and in forced to lookup the types of random callees). of course, a nice IDE that drew arrows between references and declarations like DrScheme (DrRacket?), and/or gave the type of any subexpression on hover (with selection by dragging or by expansion/contraction), would make both verbose and concise locals easier to read.
&gt; No way to get a decent debugger What features do you feel are missing from Haskell debuggers?
How would you interpret this: f do g $ h Or this: f \x -&gt; g :: Foo Now throw some `let` and `where` clauses into the mix, each with its own indentation relative to the indentation of a `do` block. You can come up with answers for all of that, but they probably won't be consistent with some of the basic rules about how Haskell syntax currently works. The trade-off is that having an intuitively consistent set of syntax rules in all cases is better than adding *ad hoc* rules that are slightly more convenient in certain special cases. But everyone would be thrilled if you can prove me wrong and come up with rules that work in all those cases and are intuitively consistent with existing Haskell syntax.
what's monitoring mean here (for a language runtime), that profiling/debugging doesn't cover?
&gt; Lazy evaluation means that reasoning about logic errors is really difficult. Stepping through code with the debugger was something I gave up on after a handful of attempts. Even using trace is problematic because it's behaviour can be inconsistent. Unfortunately I found that fixing bugs is usually best done by starting over. I think that these are all just part of the fact that getting used to Haskell takes time when your brain has been trained to think imperatively. In my opinion, reasoning about logic errors is actually *easier* in Haskell once you get used to it. Also, debugging logic errors in your own code gets much easier after you learn to express complex login in ways that are more natural in Haskell when you *write* the code. Use more breakpoints when you "step through code" in the debugger. You don't really care about the exact order of all the steps; what care about is what values the function was called with, and where it was called from with those values. To avoid inconsistent behavior with trace, make sure to trace values at a place where they would already be completely eveluated even without the trace.
So what do you use in place of headMay or readMay? I'm not familiar with the type classes you are referring to.
You just update the box after it is returned from new box? Please let me know of this wouldn't work, I feel like I'm missing something.
One of the best parts of the clojure ecosystem is that you can just download lein and get a web site running with one command. I might be missing something, but the process to create a new cabal build is a little intensive, especially if you want to follow best practices. Hein current accepts a single command "new" and the name of a project to create. You can then pass in names like "+scotty" and scotty will be added as a dependency to your cabal file. The code is terrible, as I really don't know haskell all that well, so any feedback would be appreciated. Also, if anyone knows of plugin libraries that work with ghc 7.8/7.10 I'd be grateful for some links or examples :)
Where does boxes come from? I tried to expand your example into working code here, maybe you can help me fix it so I can understand what you are talking about: data Box = Box { boxAttributes :: [String] , shelf :: Shelf } deriving (Eq, Show) data Shelf = Shelf { boxes :: [Box] } deriving (Eq, Show) newBox :: [String] -&gt; Shelf -&gt; Box newBox attributes shelf = let shelf' = shelf { boxes = ([box] ++ (boxes box)) } box = Box attributes shelf in box 
Yep. It's great that you can do amazing things. Just look at all the people who really dive into it and do those amazing things versus just trying to access a field. I'm being sarcastic, of course. But, really, if we could do the equivalent of myrec.field1.field2 = x, or x = myrec.field1.field2, there would be very little motivation. 
Both of those functions are available in base, under different names: `readMay` = `Text.Read.maybeRead` `headMay` = `Data.Maybe.listToMaybe` Examples of type classes from base that are useful with `Maybe` and `Either`: `Functor`, `Applicative`, `Monad`, `Monoid`, `Alternative`, `Traversable` Examples of packages beyond base I would include as dependencies for safe programming: [semigroups](http://hackage.haskell.org/package/semigroups) (see especially the type for non-empty lists), [either](http://haskage.haskell.org/package/either) (a monad transformer for `Either`). Both written by /u/edwardkmett.
I think you can get the requisite safety with just `ST`, can't you?
The words `old` and `new` add zero information if you know what the symbol `-&gt;` means. The word `type` adds zero information if you know what the symbol `::` means. The character `'_'` is useless if you don't have two words anymore. So that may be helpful if your intended audiences is mainly beginners who aren't comfortable yet with the meanings of those symbols. But if this is real life application or library code whose audience is Haskell programmers who are already at least somewhat comfortable with the syntax of the language, that verbosity is only an annoyance that makes the code harder to read.
Oh, yes, that one is bad. Especially when it is shadowing.
I've failed utterly over the course of days, and with help from #haskell to install vty-ui in a sandbox, and no one has yet been able to figure out why. I think Cabal Hell is still a place.
Sorry to take a hard line but I don't just prefer not to use infix, I prefer that you don't either if there's any chance I may read your code. But there's plenty of reasons to avoid squiggles other than making me happy: Why bother with layout when braces and semicolons could do the same job? Line noise is beautiful to parsers but hideous to humans and we've decided in favor of humans. Type `fmap` ten times fast. Now type `&lt;$&gt;` ten times fast; it's not just slower, but also awkward, distracting and even painful. Stretching your pinkie to the shift key will give you RSI sooner or later. You know what `&lt;$&gt;` does, but do you know it's fixity? It's 4 for some reason. `$` is 0 and `+~` is 4 again but `++` is 5. `fmap` and `over` are 10 just like all prefix functions. Sticking to prefix, you don't have to think hard about where parentheses are needed. Ghc is very good at detecting type errors. So good that it will usually report an incomprehensible, page-long type error when what you've really done is missed a parenthesis. Even well-accepted operators like `*&gt;`, `&lt;$` and even very common `++` and `&gt;&gt;=` create chaos. Editor's syntax-highlighters choke on squiggles like they were markdown. Double-click on `++` right now. Your mouse doesn't think its a word. It took a while to notice all the issues, but I now I am firmly against infix squiggles. We should have learned our lesson with regular expressions. Brevity /= Clarity.
Don't look at the [NumberSieves](http://hackage.haskell.org/package/NumberSieves) package, it will spoil a large number of PE problems! (Though, admittedly, this package has since been superceded by the arithmoi package.) As for the second question, open recursion is a powerful technique I probably used several dozen times to solve PE problems. It allows you to directly express a problem and then write some code to efficiently solve it, separately. You might be interested in [Y in Practical Programs](http://wwwusers.di.uniroma1.it/~labella/absMcAdam.ps) as a starting point.
&gt; Do you provide the facilities to extend and add new features on top of the library where possible? This is a bit hard to quantify, but worth thinking about. Especially if you're publishing a parser library. Please expose the underlying bits and not just a top-level "parse everything in the file" function. Even if the interface is messy or unstable, working with it will be better for your users than having to write everything from scratch (or maintain a separate fork). A single extensible Haskell-parsing library would eliminate so much duplication of effort.
I'm flabbergasted about the secrecy here. Why not just come out and say what the project is and who's working on it? Why does it have to be a secret?
There's no secrecy involved, just no public announcement of anything being available (because there *isn't* anything ready yet). However, there have been multiple public calls for people to get involved in the project, e.g.: * https://groups.google.com/d/msg/commercialhaskell/sbMs7WQ9iDc/EYzk6gWHDk4J * https://groups.google.com/d/msg/commercialhaskell/bLbrLp2I11E/9xunXdm56DEJ (which references https://github.com/commercialhaskell/commercialhaskell/blob/master/taskforce/tooling.md) The project is developed by the Commercial Haskell group. Anyone interested in collaborating is still more than welcome to join in the discussion now.
Thanks for those links. Is it fair to assume that basically Stackage is basically this tool? So maybe OP's project could become a `stackage new` plugin, if they were to buy in to the Stackage brand? edit: and it would make a lot of sense for `stackage new` to set up the new project to use Stackage packages (either LTS or Nightly, depending on options passed?).
You're welcome to your opinions about how you should code, and even your opinions about how *I* should code. We all have our opinions and preferences, and I don't really have any problem with people expressing them. What I do have a problem with is people pretending that their personal preferences and experiences are somehow logically grounded in universal fact, and that things outside their preferences couldn't possibly be valid. It's the same general kind of irrational dismissal that makes people reject Haskell and functional programming out-of-hand. Forth, Haskell, Lisp, Smalltalk and APL (as examples I'm familiar with) all have *very* different varieties of syntax. They've all got advocates that repeat variations on the theme you're expressing, i.e. "This way of doing things is the best because people are like this and not like that and *we* have the way that works best for all people!". Yes, even APL was designed as a *human-centric* way of expressing certain kinds of programs, and although I don't know it well and most programs look pretty incomprehensible to me, I believe the people who find it to be a very effective way for them to write programs. I'm not saying that it's all relative and subjective; just that the factors that you're harping on are *largely* subjective and opinion-based in nature. I see people all the time "finding the truth" on some matter or other, completely rejecting their previous opinions for new ones. There could be some underlying factors that make some aspects of syntax and grammar objectively better than others in some ways, but I think we're far from a solid understanding of those factors, your personal anectdotes notwithstanding.
*You* would have very little motivation, perhaps. I would still be strongly motivated. As I've said, I miss lenses almost on a daily basis in other imperative languages that have the OO style record access built-in. Lenses are better for several reasons: 1. They are first class, which means they can be passed around as arguments and returned from functions, 2. They are composable, to a degree "regular OO accessors" are not because they're built into the language, 3. There's more freedom in how you construct them, because they are simply ordinary functions – you don't have to specify them as fields in a class 4. They can have multiple targets 5. They can branch based on value 6. Their default mode of operation is in their immutable form Obviously, if you only use lenses to do simple field updates, you'll never realise all of this, and that's understandable. Just don't mistake your level of experience with all there is to something. :)
The `stackage-cli` work is definitely the basis for what we've been talking about, but the discussions I've been having with others is what the next evolution of that tooling should look like to have the biggest positive impact on everyone's workflow. But broadly speaking, yes, it looks like OP's project would fit in like some kind of `stackage new` plugin.
Ok, I made a typo, it should have been `shelf'`. However, your code may compile but doesn't work. Let's imagine we have a valid shelf with 2 boxes. If you modify the first box, the second box still refers to the previous version of the shelf with the unmodified first box. Without going that far, even create boxes doesn't work. In your example, you expect foo to be equal to `Shelf [Box 99 foo, Box2 foo, Box 1 foo]`, but in fact `foo` equals to `Shelf [Box 99 foo, Box2 shelf 12, Box1 shelf1]`. In other word, box1 thinks it has no siblings, box2 thinks it has one, etc ... 
That's what I mean (however I made a type it should be `box = box attributes shelf'`., the problem there is creating a new box should update all the boxes to reflect the shelf changes. Ok, let's do shelf0 = Shelf [] box1 = newBox "box1" shelf0 First problem, `s0` is different from `shelf box1` but that's ok, things are immutable in Haskell. To add a new box I need to do shelf1 = shelf box1 box2 = newBox "box2" shelf1 shelf2 = shelf box2 Everything is fine ... is it ? The problem is the first box in `shelf2` still referst to `shelf1` and not `shelf2`, so if we do box1' = (head.tail.boxes) shelf2 `shelf box1'` returns `shelf1` (with one box) instead of `shelf2` with 2 boxes. 
A nice overview, but for some reason the dollar signs in operators are doubled up. $$$$ should be $$, $$= should be $=, =$$ should be =$ and =$$= should be =$=.
The problem is , to update a box, you need to recreate a new shelf, embedding the new box. This is fine, but the problem is for everything else refering to this shelf (fox example other boxes on this shelf). You need to also update the sibling to refer to the new version of the shelf containing the new version of the box.
I'd refine that so say that using `-Werror` when building w/ e.g. TravisCI is fine too. Hackage (as noted below) will happily reject any package that has `ghc-options: -Werror` active by default (or more specifically: [not guarded by a default-disabled manual cabal-flag](https://github.com/haskell/cabal/pull/2594)...)
Well, if somebody came up with an implementable proposal to augment [H2010's module import/export specification](https://www.haskell.org/onlinereport/haskell2010/haskellch5.html#x11-1000005.2) it'd increase the chances of getting such a language extension in the foreseeable future (after some obligatory bikeshedding of course...) :-) 
&gt; I think there are things happening in the Haskell community that can and have been perceived as kind of "cabalish", if you'll excuse the pun. It came up with the packaging/security thread, for example. And I think every time people were accused of being too closed in their development/discussion it's really come out as not such a big deal, but I think a little bit of proactive openness can readily dispel this problem. It's been a few times in the past when Michael came out with unconventional ideas (the security thread being a recent one), and a lot of the response has been explanations why he shouldn't do that. This arguing takes a lot of energy and kills enthusiasm.
&gt; What it means is that you get many less run time errors and bugs in a language whose type system is semantically rich, because the compiler is able to catch many more of them at compile time. Yes, I can't argue about that. The compiler catches errors, the question how long would have those errors survived undetected and how many deserves to be called "bug" ? The thing is people do test. Even if it's not through an automatic test suite, people usually test somehow the feature their implementing (through REPL, or just looking at the result of a web page etc ..). Now, there are mainly 3 types of "bugs" + 1 The feature in progress is obviously not working at all + 2 the feature seems to work but there are corner cases which doesn't (and they haven't been tested) + 3 something else is broken outside of the feature. 2 is what I call a real bug. Nobody' seen or though about it. The code will be deployed in production and a user will sooner or later discover it and raise a ticket. There are usually a "semantic" problems and so can't be detected by the compiler. 3 are what we call regression. That's were Haskell shines and will probably detect 99% of them. And that's one of the reason I use Haskell. However, a vast majority of Haskell programmers are just "toying" and most of the Haskell project will never be confronted to that type of mistakes. Also, you normally know when you are creating error of that type. 1 are the stupid mistakes that usually the compiler detects automatically. They have a life span of about 2s, they'll never escape from my machine, do we really call them bug ? Moreover, is the compiler detecting them quicker than me ? Let's take an example. The other day I was going through 99 haskell problem, I had to write a function to reverse a list. My first go (I'm playing dumb). myReverse :: [a] -&gt; [a] myReverse [] = [] myReverse (x:xs) = xs ++ x Doesn't compile, complaining that `x` is not a list. That took a couple of seconds. Dynamic language , compiling is ok, but runtime error `Can't convert Fixnum to Array`. It's nearly the same error message. Took a couple of seconds too. In that case Haskell was faster to find the error, but usually, on bigger project, the compile time, plus understanding what's GHC is spitting out take longer that the runtime error equivalent. This is what I call an error #1. This *error* lived for about 10 seconds, I don't call that a bug, it's just a WIP (Work in progress) ;-). I fix the mistake myReverse :: [a] -&gt; [a] myReverse [] = [] myReverse (x:xs) = xs ++ [x] It compiles, job done. Even don't need to test it. For the dynamic language version, I need to test it , in case there is another error type #1. No runtime error, but the output is strange `reverse("foobar")` =&gt; `"oobarf"`. Doesn't work. Does that mean the Haskell version doesn't work either ? In fact I tested the Haskell version and I know it doesn't work. That's an error type #2 myReverse (x:xs) = let xs' = myReverse xs in xs ++ [x] Compiles but it's still not working, etc ... My point is, error #1 are (nearly) as quickly detected/fixed in the development process (edit code -&gt; compile/test loop) in Haskell or other language. Then, once you are happy with your code (compiles or silly error cleaned up), and ready to commit it, there is no more or less error of type #2, which are the real bugs. Anyway, I'm probably rubbish but I write LOTS of Haskell code which compiles but doesn't work. However, given 30mn I usually get stuff fixed and 99% bug proof (whatever the language) ;-) 
I'm not so sure about this one. JavaScript's WTF-typing does not clearly delimit between numbers and strings which are numbers in quotes -- implicit coersions happen when you least expect them. We all know this, and we know that Haskell is the opposite. I'm not sure whether we want Aeson to silently try to parse strings as numbers, since it's already a bit too magical sometimes, or if the burden should lie on the programmer to explicitely use newtypes. I seem to prefer the latter. It's a bit strange that Bryan O'Sullivan had not seen this before, though. Nothing surprises me about JavaScript nowadays...
&gt; Or maybe even like this: Hmm... There pops up that magical applicative syntax again. What's going on here? Are you (and the Aeson tutorials) treating the Maybe monad as an applicative? This seems like a simple idiom to get used to ("if you need to try alternatives with failure, then do it like this"), but I'm very uncomfortable not understanding just how/why the hell it works...
Yet it sucks that you can't explicitly specify a dictionary for the few cases where typeclass magic is insufficient.
&gt; 1st, you can write just `deriveJSON id ''RecordDeal` when using top-level splices (this is more-or-less personal preference, tho). As a tangential derail, why is this allowed? Seems like a gratuitous inconsistency.
However, I have to admit, that the harder a function is to get to compile , the more chances it has to be correct. This is especially true for polymorphic function which could have been generated by djin (like `fmap`, `bind` etc ...) 
Because TemplateHaskell in general has rather weird syntax. https://downloads.haskell.org/~ghc/7.8.1/docs/html/users_guide/template-haskell.html It annoyed me to no end when I discovered that when writing a QQ you can't put spaces before or after the QQ's name. [stuff| |] is valid, but not [stuff | |] Apparently this particular rule is to prevent confusion between QQ and list comprehensions? Well, I say, why didn't they choose slightly better syntax, rather?
I have no idea what any of that means. :-) Are you saying I can automatically "cast" types without remorse? Can I please see a minimal example? 
This clarifies things greatly. Thanks. I was not aware that Maybe could be treated as a Functor like this, outside of GHC 7.10. Or maybe I'm just confusing Functors and Applicatives (the topic of GHC 7.10's AMP).
See my [response](http://www.reddit.com/r/haskell/comments/37vna8/a_newb_me_wrote_a_haskell_library_and_unwisely/crqq4th) to your post's sibling.
I think the current recommended way of deriving FromJSON/ToJSON instances is using generics, not TH. 
Implicits, at least in Scala, cause me many more headaches in spurious compilation errors than they prevent with the added power of multiple instances. Manual dictionary passing becomes absolutely terrible when you want to e.g. traverse over a Compose [] Maybe, or do anything else like that where you need to specify 3+ instances for a single method. If you can specify a dictionary manually with typeclasses, then you can no longer rely on the coherence of instances. This means, for example, that two sets don't necessarily have the same underlying order, so the best you can do is insert every item from the first into the second (as is done in Scala), instead of using the asymptotically faster hedge merge. While there might be something better than typeclasses + newtypes to specify multiple instances, I haven't seen it yet. 
There seem to be a bunch of hits for this on Google.
&gt; Data.Aeson.TH's deriveJSON just does that with its fieldLabelModifier option. Generics can do the same. (I have an example in my tutorial.)
Brilliant! This makes me appreciate the AMP even more, and goes a long way to helping me understand Parsec magic!
[Ticket.](https://ghc.haskell.org/trac/ghc/ticket/8118) [GHC display of unicode characters](http://stackoverflow.com/questions/21365317/ghc-display-of-unicode-characters) The problem isn't with Text as such, but with GHC's output. While GHCi itself is unicode-compatible, on Windows, putChar as its low-level friends aren't. You can confirm this hypothesis by loading {-# UnicodeSyntax #-} import Data.Monoid.Unicode ∅ -- syntax error because it's an operator If the error message causes the same `invalid character`-exception, you indeed have the encoding problem. Unicode should work under Unix, but if you're on Windows, I recommend giving up. You'll never get Unicode output to work properly in GHCi. I know; I tried for weeks. You can, however, play around with `System.IO.hSetLocale`: &gt; hSetLocale stdout X &gt;&gt; putStrLn "èé∅öäüß" where X is one of {utf8, utf16, char8, localeEncoding}. None of them print the string faithfully for me, but utf and char8 at least don't crash.
I would like to be able when I set a breakpoint, to see what are the argument of the current function. Example maxigit &gt; cat deb.hs f x = let y = x+1 in y maxigit &gt; ghci deb.hs GHCi, version 7.8.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. [1 of 1] Compiling Main ( deb.hs, interpreted ) Ok, modules loaded: Main. *Main &gt; :break f Breakpoint 0 activated at deb.hs:(1,1)-(2,10) *Main &gt; f (5::Int) Stopped at deb.hs:(1,1)-(2,10) _result :: a = _ [deb.hs:(1,1)-(2,10)] *Main &gt; :list vv 1 f x = let y = x+1 2 in y ^^ 3 [deb.hs:(1,1)-(2,10)] *Main &gt; y &lt;interactive&gt;:5:1: Not in scope: ‘y’ [deb.hs:(1,1)-(2,10)] *Main &gt; x &lt;interactive&gt;:6:1: Not in scope: ‘x’ [deb.hs:(1,1)-(2,10)] *Main &gt; I would like to see what `x` is without having to `:step` twice. I'm happy to force the argument if needed. 
Currently, typeclasses are these magic dictionaries that you obtain from somewhere and can then forget about and drop on the floor; they're not carefully threaded through the code. This works because instances are canonical. It doesn't matter how the instance was made or where you got it from; it's always going to be the same instance. What you need to do for this is to tag types with *literally every instance that you might ever want to be the same when working with values of that type*. Additionally, what happens if you call toList . fromList? You forget about all the instances used in the first set, and replace them with random new ones?
&gt; What you need to do for this is to tag types with literally every instance that you might ever want to be the same when working with values of that type. Additionally, what happens if you call toList . fromList? You forget about all the instances used in the first set, and replace them with random new ones? Elements of a list have to be homogenous, thus they all have to have the same dictionary. That would be the difference between type Something a =&gt; [a] and [Something a =&gt; a]
My rule of thumb for what symbols are beginner-friendly is that associative operators are beginner-friendly For example, that includes `(.)`/`(+)`/`(*)`/`(&lt;&gt;)`, but excludes `($)`/`(&lt;$&gt;)`/`(&lt;*&gt;)`/`(^.)` and most other lens operators. This is one reason that I'm highly in favor of the `ApplicativeDo` extension because then we wouldn't need to use the `Applicative` operators.
Also, while it wouldn't help your specific use case, you can derive `FromJSON` and `ToJSON` instances using GHC generics: {-# LANGUAGE DeriveGeneric #-} import GHC.Generics data Coord = Coord { x :: Double, y :: Double } deriving Generic instance FromJSON Coord instance ToJSON Coord It's buried in the documentation of `Data.Aeson`, but it's there.
Thank you for doing this!
Well, the type is `Set s =&gt; s Int` resp. `Set s =&gt; s Integer`, so the instance of `Set` doesn't change or did I get that wrong? The first example would actually be enough to erase the `Set` instance, but then defaulting could happen just as it does with numeric types. For instance, `toInteger . fromInteger` erases the information about what numeric type we are using. The Haskell standard defaults the type in this case.
Hey, fancy seeing you here Tebello. (You should demo this at Lambda Lum!) &gt; I needed some way of dealing with financial numbers without using potentially-error-prone floating point, and eventually settled on the Decimal package Could you use [Scientific](https://hackage.haskell.org/package/scientific-0.3.3.8/docs/Data-Scientific.html) instead? It's already included as a dependency of aeson.
You mentioned that [haskell-vim-now](https://github.com/begriffs/haskell-vim-now) used to be your config of choice but that ghc-mod is having trouble. I know ghc-mod [fails to build](https://github.com/kazu-yamamoto/ghc-mod/issues/437) under GHC 7.10. If you'd like I could precompile it for for Mac and Linux under 7.8 and then have the installer download the binary rather than attempting to build it. Would that work for you, or is there a deeper problem with ghc-mod even after it is compiled? I notice you're involved with this [issue](https://github.com/kazu-yamamoto/ghc-mod/issues/456#issuecomment-107149247) as well.
The point here was that there is no `getRecord` because it's a batch operation. There is only a `getRecords :: [Id] -&gt; IO [Record]`. But I'm happy to concede that matching up to some uniquely identifying information in the record would be preferable if something like that is available. Perhaps it would be interesting to look into some function like zipOn :: (a -&gt; b -&gt; Ordering) -&gt; [a] -&gt; [b] -&gt; [(a,b)] for situations where pairing up is indeed possible but inconvenient.
Oh! Sorry, this design is immutable. You change the graph by processing the binding structure. `NL a -&gt; NL a`.
Great post! I completely agree! That's how we should be treating the wiki -- as a living place that can be edited and curated to reflect all the ideas described and articles published, not just as a repository for long-form single-author content.
Sadly editing the wiki does not stroke one's ego. Editing wikis is a selfless act, good on you for doing so.
&gt; Lastly, floating point numbers still suck. No amount of typeclass twiddling will change that :( [This talk](https://www.youtube.com/watch?v=LJQgYBQFtSE) might interest you (might be a repost). The guy shows two really very clever schemes for doing arithmetic with unbounded precision.
&gt; On Cygwin or MSYS, the solution is actually pretty easy: use hSetEncoding stdout utf8. This will cause Unicode I/O operations such as putStrLn "→" to work as you'd expect them. I've been been able to get some Unicode characters displayed correctly, but not others (∅ doesn't work, accented chars do). The thing is that, in bash/cmd, I can't even *paste* → or ∅. I've only been trying this out in WinGHCi.
Hey, Rehno! I made sure to advertise Lambda Luminaries in my post! Yeah, I don't see why I wouldn't want to present it to the group, so why not? :-) Er, Scientific actually looks perfect. Goddamn! It's going on the TODO list!
How does Wikipedia make it so compelling to contribute?
Maybe you'll even manage to pick up a collaborator, there's probably one or two others there that use bitx ;)
Thanks for the simple intro. I wish this was around when I was struggling with concept. One thing I never really figured out was how to get good performance out of conduits (pipes, machines). The whole reason I want streaming is to choke down data that's larger than memory, but I think other people use it more for resource management and light loads. The best idea I could come up with is to use the conduits to manage resources and do some light framing of the input stream into 8k chunks which get passed to some highly-optimized vector or bytestring-based thing for the ugly, ad-hoc inner loop. The inner loop also has to build up output chunks to send to the sink for final pasting together. It reminds me of anything where you use IPC or messages or queues of any kind: If you want to process individual characters you will die from overhead, so you chunk the data sacrificing the elegance of the idiom.
You'd probably be interested in [this article that I wrote](http://www.haskellforall.com/2013/09/perfect-streaming-using-pipes-bytestring.html) on how to efficiently process streams chunk-wise. The relevant `pipes` libraries are `pipes-group`/`pipes-bytestring`/`pipes-text`.
&gt; The guy seems to despise C++ but he sure knows a lot about it. These go hand in hand. The more you know about C++ the more you know how much is wrong with it. It's a terrible language but for many use cases there's not really any viable alternative.
&gt; Start with a monoid and work your way up. Semigroups are weaker than monoids and relatively common in Haskell.
I know. But other wikis solve this somehow, right? What about a Haskell captcha? E.g. "enter the result of `filter even [1..5]`"
http://www.reddit.com/r/programmerhumor
So Scheme is type safe under your definition...
Link?
Corollary: atoms only do the wave if you show them on the jumbotron
Disclaimer: In this conversation, I am *not* trying to say not to do this. I would love to have it. Just pointing out that a lot more work would have to be done. So, another issue I can think of off the top of my head: You want the rule to be "insert $ before those keywords". But $ itself is not a keyword, it's a function defined much later on in an imported module. During the parsing stage it's not available. Do you want to make $ a keyword? That would be a major change to the grammar whose consequences would need to be investigated. So maybe you want to insert a magical "virtual keyword" that gets translated to $ during desugaring, similar to the way that `do` blocks get translated to an expression involving the non-keyword function &gt;&gt;=. There are many dark corners of Haskell syntax. To put it mildly, we are not talking about Lisp here; it's very complex. The general rule is: any change to the fundamental syntax of Haskell, even a tiny change that seems much smaller than this one, triggers a cascade of serious problems, especially when you are touching infix operators, keywords, and the maximal munch rule. I'd would like to see what one of the Haskell syntax experts has to say about this suggestion.
Some unobserved part of the universe is setting us up for a massive space leak. OMGZ Maybe dark matter is unevaluated thunks!!?!!?!!1one
maybe a -fwarn-record-dum-types would help. or more generally, check that each variant shares the same fields.
I really dislike articles like this (the journalism not the paper being referenced). Quantum mechanics is not lazy (or even particularly weird). Saying that the atom behaves sometimes like a wave and sometimes like a particle is silly and misleading. It always behaves exact how a *wavefunction* should because that is the most useful description. It's like saying sometimes it behaves like a duck and sometimes more like a potato. You may be right but only because ducks, potatos, particles and waves all suck as descriptions of what is going on. Saying stuff like "reality doesn't exist" requires (re)defining reality as something that doesn't exist. /rant
Actually, it would be the opposite: if the universe is already lazy then Haskell's laziness would be redundant.
Point against really. 
Computers are good at that, but spam bot writers probably are not going to spend time customizing bypasses for every site that invents their own custom captcha system.
[Get a Handle to a file with more data than you have memory to contain.](http://hackage.haskell.org/package/base-4.8.0.0/docs/System-IO.html#v:openFile) [Read it into a lazy ByteString](http://hackage.haskell.org/package/bytestring-0.10.6.0/docs/Data-ByteString-Lazy.html#v:hGetContents) Profile it doing something silly like summing the bytes, such that it should hypothetically only need the current byte/word/etc. and the summation so far. [1] [1] NB. This is completely off the top of my head, but it's the sort of situation where I'd reach for pipes-bytestring just to avoid any *possible* grief. That doesn't mean I've established it's necessarily "bad". IME, there's a lot of superstition around this topic. In some respects, that's a good thing (bear with me) because it means we're propagating folklore around how to avoid broken programs effectively. OTOH, it means people are being deprived of the opportunity to learn these lessons for themselves. We should show people what doesn't work before we show them the fix.
Found the lisper...
I have struggled through much the same problem. Just like the options you've explored I simply implemented my own pointer system in the heap. I started with an `IntMap` and when that was working I switched to an `STArray` (tried different arrays for different components and also one with an `(Int,Int)` index as an adjacency matrix. The biggest frustration I found was that I would get (for lack of a better phrase) null pointer exceptions. These were always logic errors that resulted in a node referencing an edge that didn't exist. When I finally got it all to work I still had no guarantees that everything worked. The performance gains of `STArray`s were good but certainly not C-level. But the type overhead and code verbosity made using Haskell for this to be exhausting. So I've been exploring using an imperative language for that part of the project. I'm thinking Rust because I like the type system but also the low-level control. Sometimes I think you need to choose the right tool for the job and it's not always going to be purely functional Haskell. Another option I've heard of outside Haskell is using an in-memory relational data-base. You could combine that with Haskell and perform all your manipulations in a pure functional way and commit transactions to the DB. In that way you isolate the graph relationships in IO. I've never used an in-memory database before, so YMMV.
since you mentioned the IDE situation, I don't think it's too bad. for example, now we even have 3 -- yes that's right, not just one or two, but three -- plugins just for Intellij IDEA: * [HaskForce](https://www.openhub.net/p/haskforce) * [haskell-idea-plugin](https://www.openhub.net/p/haskell-idea-plugin) * [intellij-haskell](https://www.openhub.net/p/intellij-haskell) the pace of development is not earthshaking, but it's being done, and I personally have had a good experience with HaskForce. 
Just follow the existing rules for what stuff ends up parsed inside a do or lambda. Without the context it's do {g $ h} and \x -&gt; (x :: Int) So, without confusing changes to the handling of a layout block in do or the extent of a lambda, your examples have to parse like f (do {g $ h}) and f (\x -&gt; (g :: Foo))
I apologize - this is not a desktop program. This is meant for people using their command-line shell (although now that I think about it, I could probably make a GUI). But you're right, this has NOTHING to do with the clipboard, but it is a solution to the "repeated content" problem - where you manually need to copy content from one file to another. It solves this problem by parameterizing files _themselves_, like a function from math. Then, to populate your parameterized file with content, you just need to _give_ it content to substitute-in (from function application - `foo bar` means "bar gets inserted in foo's first parameter).
From my understanding of quantum physics, observation is basically synonymous with interaction, so this is like a really bad way to say that reality consists of all the things that interact with each other, which is unprovable (trivially, you can't observe things that don't interact with other things, so you can't prove they do or don't exist, not that it actually matters since for all purposes a thing that doesn't interact with anything might as well not exist).
Very cool, love the higher order templates and partial application. Is there an equivalent to "flip" for templates of more than one parameter? Also, is the "\x -&gt; x" syntax limited to the command line?
Here's a [link](https://youtu.be/2-JFkv9-JOQ?t=14m32s) to the part of a talk I gave where I cover it.
&gt; Libraries like pipes/conduits/iteratees are not "fixes to lazy I/O". I agree. That's why my example was specifically about something involving a dataset larger than available memory, a mode of consumption conducive to piecewise work, and the accumulated data is tiny compared to the original data. I was trying to make a strawman example of, "streaming is better than lazy IO". This was, `exists a . a is better in terms of streaming IO than lazy IO` This was not, `forall a . a is better in terms of streaming IO than lazy IO` &gt;The fact that all of these libraries have been compared to lazy I/O in the past is a historical artifact Yeah, but I think this is also partly because people more often want streaming IO rather than lazy IO. Many problems I've tackled and seen others tackle lend themselves better to the former operationally than the latter. That doesn't mean Lazy IO couldn't work, but it does seem like making it work seems to be harder than re-expressing what you're doing in terms of a streaming abstraction (when that is possible to begin with) to get the operational behavior you want. &gt;I also don't think lazy I/O is always broken. Good, I don't either! In fact, I think I said: &gt;IME, there's a lot of superstition around this topic OP didn't give me a lot to go on, so I tried to take a little time to give people something real to talk about and kick around. Perhaps you could come up with an example that demonstrates the opposite so OP (and others) can learn something?
Wikis stagnate because nobody really knows who the maintainer is. Then they don't know if it is ok to edit or delete text and are only comfortable with adding new text, which bloats and confuses the content. Wikipedia does well in this regard because there are maintainers of pages. The fundamental problem with the wiki model is that edits are performed instead of being proposed. This is why I favor just using a github repo as a wiki: pull requests propose changes. This makes people comfortable with editing and facilitates easy maintenance. The downside of this approach is only a lack of page-level notifications. I guess what I am saying is that the best way to improve the wiki would be to switch to a wiki that facilitates proposing changes instead of just making them.
typo: "real word packages" -&gt; real world packages
`cabal-install` is the package which provides the `cabal` executable, a tool for building and installing haskell projects which have a cabal file. `Cabal` is the backing library which `cabal-install` and other Haskell infrastructure is built on. EDIT: You want `cabal-install`. With the `cabal` executable, you can install packages from Hackage and build cabal projects from source.
&gt; With the cabal executable installed How might it be not installed when I've installed cabal-install?
I'm not installing it, reread my question. 
Maybe dark matter is bottoms And what about black holes...you know, they disappear over time
In case you're interested, here's the original [CABAL Specification](https://www.haskell.org/cabal/proposal/pkg-spec.pdf) which describes the idea behind the `Cabal`-library (which you can actually use directly w/o the `cabal` executable -- but it's less convenient, as you'd have you perform cabal install-plan solving and package downloading yourself...)
Actually, that is what [`LensLike`](https://hackage.haskell.org/package/lens-4.11/docs/Control-Lens-Type.html#t:LensLike) is. [`Optic`](https://hackage.haskell.org/package/lens-4.11/docs/Control-Lens-Type.html#t:Optic) is a generalization of this and `Optical` generalizes this further.
If you have an Applicative, you shouldn’t assume anything about the order of effects. If you are defining an Applicative (instance), you are free to perform effects in whatever order you want *because* no client can assume an order. However, if you are planning on writing a Monad instance for the type for which you are writing an Applicative instance, then you need to think carefully. The effects associated with Monads *do* have an order that a client can depend on (due to the possibility of data dependence), and making Applicative and Monad instances for the same type agree on effect ordering is customary. This reasoning wraps back around at this point. If you have a particular Applicative for which you know there is a Monad (such as IO), custom lets you assume the effects will be evaluated left-to-right so as to be compatible with the Monad instance. In summary, you should think of any Applicative you are given as *potentially* evaluating effects in parallel, but if you’re on the other side of the implementation and want effects to *actually* run in parallel, you should reconsider defining a Monad instance for the same type so that clients can reason about effects based on the concrete type. 
Good grief, yes. It burns so bad that I've started to use the length of the dependency list of a package as one of its main fitness criteria. And this is *with* sandboxing.
Is that really the case (which laws enforce this?), or is this just a convention created by the arbitrary choice that has been made for the `Monad m =&gt; Applicative m` instance of the standard library? 
&gt; I've never met an Applicative that didn't have an order to its effects. Purity and the generality of the type signature for &lt;*&gt; kinda implies that a given instance must either pick left-to-right or right-to-left evaluation order and stick to it. Look at the [Ziplist](http://en.wikibooks.org/wiki/Haskell/Applicative_Functors#ZipLists) applicative `[a]` with `&lt;*&gt; = zipWith`, the prototypic example of Applicative that is not a Monad. You have `[f, g, h] &lt;*&gt; [a, b, c] = [f a, g b, h c]`, with no particular left-to-right or right-to-left ordering.
wait, why aren't they composable? Are you talking strictly about performance? I've also wondered about this question and can never seem to get to the core of the issue with Lazy I/O
&gt; Get a Handle to a file with more data than you have memory to contain. &gt; &gt; Read it into a lazy ByteString &gt; &gt; Profile it doing something silly like summing the bytes, such that it should hypothetically only need the current byte/word/etc. and the summation so far. [1] Actually this is a situation where lazy IO works perfectly well (I believe it was *designed* to handle this situation...). For example I just wrote a sha256 checksum script last week with lazy IO and lazy ByteString exactly this way, and it works perfectly well for files larger than than the memory (and the running time was the same as the cygwin sha256sum executable). 
In fact on second reading I see that maybe the text is nuanced on this point: it says that the instances "must" satisfy the other laws given as equations, that they "will" satisfy the functor-compatibility equation (because I think it is implied by the layws), but only that it "should" satisfy this one. So it would be a recommendation of the law, rather than a mandate.
The first bit of code gives compilation errors. Anyone know what's up? Main.hs:12:28: Couldn't match expected type ‘[a0]’ with actual type ‘Int’ In the first argument of ‘(++)’, namely ‘f’ In the first argument of ‘yield’, namely ‘(f ++ 1)’ Main.hs:12:28: Couldn't match expected type ‘Int’ with actual type ‘[a0]’ In the first argument of ‘yield’, namely ‘(f ++ 1)’ In the first argument of ‘(&gt;&gt;)’, namely ‘yield (f ++ 1)’ In the expression: yield (f ++ 1) &gt;&gt; conduit Main.hs:18:16: Couldn't match type ‘()’ with ‘Data.Void.Void’ Expected type: String -&gt; IO Data.Void.Void Actual type: String -&gt; IO () In the first argument of ‘CL.mapM’, namely ‘putStrLn’ In the expression: CL.mapM putStrLn 
&gt; Writing this all out, it feels like a dumb question. I've never met an Applicative that didn't have an order to its effects. Purity and the generality of the type signature for &lt;*&gt; kinda implies that a given instance must either pick left-to-right or right-to-left evaluation order and stick to it. I recently [wrote an article about an applicative](http://jaspervdj.be/posts/2015-05-26-prio-applicative.html) that can have another order than left-to-right or right-to-left.
These are my first "real" contributions to the Haskell ecosystem, so I would be very glad to get feedback. 
You can turn a monad into an applicative in two ways, but you can also turn an applicative into an applicative in two ways: by flipping the order or not flipping the order. It seems analogous to saying that we can turn a group into a monoid in two ways: by defining `mappend a b = a * b` or by defining `mappend a b = b * a`. Yet in math we still have a canonical way to a group into a monoid, and these conventions are useful. It is technically arbitrary but at the same time it makes a lot of sense that we don't pick `mappend a b = b * a`. Technically arbitrary but not humanely arbitrary ;-)
Or you could have this: &gt;&gt;&gt; 2048-12-01 :: UTCTime 2048-12-01 00:00:00 UTC (I'm not saying this is a good idea.)
That's a subtle detail - In order to render a tempate, _all_ text needs to be at the top level - otherwise, you would have a multi-line expression. I'll put more documentation on this later. But you're right, the only way to pass text as an argument is through content in a file.
The website itself was made with ltext, but it only uses some simple substitutions. I'll try and make one where a _theme_ itself is a higher-order template :)
I'm doing that as we speak! benzrf suggested I do that - I just need to make a typeclass to turn an expression data structure into a haskell (n-ary) function, then we're all set :)
I'll try out HaskForce on Ubuntu later today. Thanks. :-)
It's not arbitrary. If we drop these laws there isn't a coherent superclass relationship any more. For `Applicative` to be a superclass of `Monad` there needs to be a canonical way to get an `Applicative` from a `Monad`. This is where the two laws come from, and I think that if you abolish `(&lt;*&gt;) = ap` you also need to abolish `pure = return` for things to make sense. 
The Applicative laws essentially state that any Applicative expression is ultimately equivalent to pure f &lt;*&gt; x &lt;*&gt; y &lt;*&gt; z &lt;*&gt; ... for appropriate values `x`, `y`, `z`, `...`. In this sense, law-abiding implementations of `(&lt;*&gt;)` have *access* to ordering "by law". A clearer way of seeing how this really works however is to explore the isomorphic `Monoidal` typeclass: class Functor f =&gt; Monoidal f where unit :: f () tup :: f a -&gt; f b -&gt; f (a, b) If we ignore the differences between `((a, b), c)` and `(a, (b, c))` this helps suggest that all canonical `Applicative` values are like ex :: Applicative f =&gt; f (a, b, c) or fmap (f :: (a, b, c) -&gt; d) ex :: Applicative f =&gt; f d So the heart of this is that `Applicative` "effects" all get bundled together into a single `f` layer and then the "pure" part can do anything a function from all of the pure arguments to another argument can do. In essence, `Applicative` has access to ordering of both effects and pure arguments, but has no compulsion to use them in any particular order. On the other hand, Monad's `bind` gets us in a bind (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b It essentially is the only way to combine effects and values and it indicates something interesting—the value level and the effect level are *dependent* on one another since when computing `m &gt;&gt;= f`the function `f` has the opportunity to look at the value of type `a` "inside of" `m`before it's compelled to provide the next "step". Furthermore, a monad can't usually just "ignore" the next step because that's its only access to the type `b`. The only way for it to not care about the type `b` is to be phantom in its type parameter. So in what way can a monad rearrange its order? Well, if it is not phantom in its parameter and in a call to `m &gt;&gt;= f`the function `f` is strict in its argument then I feel reasonably confident then it cannot ignore the order—the type of `f` ensures that there's sufficient interleaving between the type and value layers. But in Haskell functions are often lazy and if there isn't an actual data dependency between `a` and `m b` in the implementation of `f` then the actual order of operations may differ. Monads can even take advantage of this to save work.
I disagree with your group/monoid argument. A group is a tuple of a carrier set, a binary and a zero-ary operations, which verifies some equations. The natural way to turn a group into a monoid keeps the same carrier set, binary and zero-ary operations, and only weakens the laws. The fact that the operations are preserved is important, but not canonical. That is, there is a *canonical* forgetful morphism from (G, *, 1) as a group to (G, *, 1) as a monoid, but we have specified the operations on both sides, and it is only with those fixed that the morphism is canonical (that is, uniquely defined). There is a "natural" (in the informal sense) morphism from "any group on the carrier set G" to "some monoid on the carrier set G" that preserves the information, but it is not stricto-sensu "canonical". (That said, reasonable person may disagree with this strong notion of canonicity and use weaker criterions that align with your usage.) In the case of the Monad/Applicative morphism, `ap` is not part of the algebraic structure defining what a monad is. I fully agree that it is natural to take *the same* binary operations when moving from the group structure to the monoid structure. There is no such natural choice when turning a Monad into an Applicative -- `ap` is an arbitrary derived operation, there is no natural reason that it should be used and not its backward version. In any case (whether you consider this canonical or not), my point still stand: this requirement *is* arbitrary while the other *are not* arbitrary. Only the non-arbitrary requirements should have the force of law.
`Applicative` being a superclass of `Monad` is the business of the Haskell type-class ecosystem, it is unrelated to the lawful properties of these algebraic objects that make sense outside the Haskell language. The type-class system of Coq for example has no problem with handling incoherent instances (they are incompatible at the type level and thus users can reliably work in an incoherent setting) and should have the exact same laws for `Applicative` and `Monad` -- except this odd one. &gt; you also need to abolish `pure = return` for things to make sense I don't think so: I think that you could prove that `pure = return` should hold even with a weaker compatibility law between `bind` and `ap`. Coming back to your group/monoid argument, this *is* one instance where the exact same operation is carried from one algebraic structur to another, so it is natural to keep the same one.
Yes exactly. My audience is beginners. Normally I write a and b for this. But the OP is seeking alternatives. 
Once you cross the line into FP, that is where data and behavior are completely separated, a lot of the ideas from the OO world don't work anymore. What were once useful tools, techniques and patterns fall apart and become difficult to manage. Learning a new set of tools, techniques and patterns the FP way does translate across languages very well.
oh, thanks for the pointer: [`src/`](https://github.com/ltext/ltext.github.io/tree/master/src) and [`build.sh`](https://github.com/ltext/ltext.github.io/blob/master/build.sh) indeed is real-life enough for me ;)
Looking at the code, my impression is it could use more types. type OnionAddress = Base32String type Port = Integer type TorControlServerSocket = Socket Of course, newtype might be better for some of these; perhaps TorControlServerSocket could be an entirely opaque type.
+1 This is what flips the developer's way of thinking into a completely different mindset. Once data and behavior are separate, its a whole different ballgame. Elixir, Clojure as well as many other functional languages are not "pure" as they don't abide by some of the more academic definitions such as always having the same inputs always producing the same outputs (reading state or IO is typically where this rule is violated). Haskell by an academic definition is pure. So even within functional languages some are more pure than others.
Hey this is a really cool exercise! Here are some tips from another beginner: * I don't like `parseTimeLine`. Why not think in terms of the things that make a `TimeStamp`? For example, I'd code a `Time` parser, then using the applicative instance, `TimeStamp &lt;$&gt; parseTime &lt;* string "--&gt;" *&gt; parseTime`. I like Applicative :D * use one of haskell time libraries, instead of writing your own. * .srt can have DVD formatting options[1], which would break this parser. * add an option parser, `head args` is too pythonic ;)
Hey, this is exactly the same project I took on when I first started learning Haskell!
A group is a `(G, *, 1, inverse)`, `inverse : G -&gt; G`, so we haven't just forgotten laws, but also operations (like we've forgotten bind when we go from a monad to an applicative). I understand your point that `ap` is not part of Monad, but I don't fully agree. Even before Applicative was a superclass of Monad there was a `&lt;*&gt;` combinator on monads that is/was the same as `ap`. It just happened to not be part of the standard *definition* of monad, but a monad is not its definition. A monad is a conceptual thing that can be defined in many equivalent ways. The fact that `&lt;*&gt;` has a different name than `ap` is a historical accident (but then again the same goes for groups: `*` has a different name than `mappend`). So I still don't see in what sense going from group to monoid is not arbitrary but going from monad to applicative is.
The problem with scala is the runtime, it is awful. The language is awesome but the runtime is deeply flawed. 
:)
&gt; Conversion time -&gt; milliseconds : I could not wrap my head around this and finally wrote my own Could you be more specific on this? &gt; The code in the Left/Right result of the parsing (basically everything after the -&gt; arrow) : How does this work? Either is defined like this: data Either a b = Left a | Right b The pipe `|` essentially means "or", so you can read that a bit like "The type `Either` has two type variables `a` and `b`. It has two data constructors, `Left a` and `Right b`". The value of the expression `parse parseSubtitles "(stdin)" c` is `Either a b`. You don't know ahead of time what value you'll get, so you need to account for getting a `Left` (error) and a `Right` (success). When you use a `case` expression, you're saying "Evaluate this expression, and I'll *pattern match* on the result. Then execute whatever follows the arrow in that case." Pattern matching starts from the top, looks at the data constructor used to make the value, and tries to see if it can make a match. So: case parse parseThing of Left e -&gt; putStrLn "Error!" Right s -&gt; putStrLn "Success!" can be read as "Do `parse parseThing`, and in the `case of` the result being `Left e`, do `putStrLn "Error!"`. In the `case of` the result being `Right s`, do `putStrLn "Success!"`.
The example that I linked elsewhere in this thread goes like this: let's take `readFile` and `writeFile`, which are very nice high-level primitives, and try to imagine composing them with a `showProgress` function that prints a little status output every million bytes or so. How do you do it? Well, the answer is, you can't. You have to rewrite `writeFile` to do it internally; or else you have to switch from using `writeFile` to using something like ```mapM_ (appendFile `andThen` printStatus) . chunksOf 10000000``` or something like that. So these two pieces of code that look perfectly reusable suddenly can't be reused -- you have to implement a new chunk of code that does substantially the same thing, but with a few minor tweaks. Nasty! There are definitely different approaches that *do* allow you to write `readFile` and `writeFile` in a way that lets you stick a progress bar in the middle; so in the compositionality sense at the very least those are preferable.
Okay, it's implemented now under `LText.Internal.Expr.TH.exprToHs` :)
&gt; Writing this all out, it feels like a dumb question. I've never met an Applicative that didn't have an order to its effects. Purity and the generality of the type signature for &lt;*&gt; kinda implies that a given instance must either pick left-to-right or right-to-left evaluation order and stick to it. How about newtype Op f a = Op { unOp :: f a } instance (Functor f) =&gt; Functor (Op f) where -- ... instance (Applicative f) =&gt; Applicative (Op f) where pure = Op . pure Op f &lt;*&gt; Op x = Op $ (flip ($)) &lt;$&gt; x &lt;*&gt; f newtype PProd f g a = PProd { unPProd (f a, g a) } instance (Functor f, Functor g) =&gt; Functor (PProd f g) where -- ... instance (Applicative f, Applicative g) =&gt; Applicative (PProd f g) where -- ... and then things of type `PProd IO (Op IO) a` for some `a`?
I have used it, but just asking your question will probably give you a better answer than asking if somebody might be able to answer a question you have not even told us.
The standard answer is to use newtypes and write a different instance for them.
Actually I have something like this data Term idF idV = F idF [Term idF idV] | V idV deriving (Show, Eq, Ord) and just need (for now) to create a new type for V idV, can I do a "clone" of the Term idF idV type and reassing the type of my actual vars?
Could you elaborate on the quadratic cost thing, or provide a link to more info? I'm very curious. 
Yep, I made that same observation at the pub on Sunday! Oh well, next time :)
`not (equals Brevity Clarity)` sorry, couldn't help it
Hi, Tekmo. I do that quite a bit in the [main types module](https://github.com/tebello-thejane/bitx-haskell/blob/master/src/Network/Bitcoin/BitX/Types.hs#L476). I was not aware that I could also use it with records while taking care of field name clashes, but peargreen [made me aware that it is certainly also possible](http://www.reddit.com/r/haskell/comments/37vna8/a_newb_me_wrote_a_haskell_library_and_unwisely/crqqrar . [His tutorial](http://artyom.me/aeson) is super awesome.
Since we talked a bit over the weekend, there has got to be a shared secret among (just) us. My fingerprint is 73C9 44F6 FC05 E4C0 2665 B580 BAD0 B1A4 F959 052C and our shared secret is that uhmmm you have a beard!
This is how `pipes` was implemented back in [the old days](http://hackage.haskell.org/package/pipes-2.4.0/docs/Control-Proxy-Core.html). The reason I no longer use the `FreeT` type was for a constant-factor performance improvement. I also am the one who came up with the name `FreeT` for what was previously known as `Coroutine` and wrote up [the first implementation of `FreeT` on Hackage](https://hackage.haskell.org/package/transformers-free) which was later merged into the `free` library, so the correspondence to the free monad transformer was not lost on me. I'm also quite aware of the quadratic time complexity for certain operations, but the naive implementation of a free monad has very good constant factors and `pipes` is structured in such a way that you avoid the cases with quadratic time complexity. For example, instead of writing: replicateM 10 foo You instead write: Pipes.Prelude.replicateM 10 foo That gives a right-associative bind and also streams the results. Same thing for `sequence` and `mapM`, which have right-associative streaming versions in `Pipes.Prelude`.
Umm... How do I set the SDK home for HaskForce on Ubuntu? Google turns up a bunch of complaints about this not working automatically, and nothing about how to do it manually.
I don't think it's a wrong law. Making the instances behave very differently is at best confusing to a user, and at worst will lead to horrible bugs. However, I think there's an argument to be had over what the equality means in these laws. For example, you could argue that in a case like [haxl](http://hackage.haskell.org/package/haxl), the eventual returned values are the same, even though the Applicative can do requests in parallel where the Monad can't.
Good work! It looks like the separate sections per `remote-repo` would also be useful for setting separate username/password combinations for, say, an internal and external hackage and being able to upload to both of them.
I feel that `$` is usually mastered before `.` 
I liked the notation a lot, pity the actual type has four parameters otherwise it'd be great notation for lenses. 
Sure. I'll use a simpler monad just to give you the example. data M a = Pure a | Read (Char -&gt; M a) instance Monad M where return = Pure Pure a &gt;&gt;= f = f a Read k &gt;&gt;= f = Read (\c -&gt; k c &gt;&gt;= f) read :: M Char read = Read (\c -&gt; return c) This type represents programs that need some number of characters of input, dependent on the input, before giving a result. For example, a program could read matched pairs of () until the first ( is closed, then report the the maximum depth of () found. Now, consider this function: ignore :: M () ignore = read &gt;&gt;= \_ -&gt; return () ignoreN_1 :: Int -&gt; M () ignoreN_1 0 = return () ignoreN_1 n = ignore &gt;&gt;= \_ -&gt; ignoreN_1 (n-1) ignoreN_2 :: Int -&gt; M () ignoreN_2 0 = return () ignoreN_2 n = ignoreN_2 (n-1) &gt;&gt;= \_ -&gt; ignore These two implementations of ignoreN both skip n characters. Now consider how many times `&gt;&gt;=` has to be evaluated in each of these cases. First, `ignore`: ignore = read &gt;&gt;= \_ -&gt; Pure () -- read ignore = (Read (\c -&gt; Pure c)) &gt;&gt;= \_ -&gt; Pure () -- one evaluation of &gt;&gt;= ignore = (Read (\c -&gt; Pure c &gt;&gt;= \_ -&gt; Pure ())) -- one evaluation of &gt;&gt;= ignore = (Read (\c -&gt; (\_ -&gt; Pure ()) c)) -- simplify ignore = (Read (\c -&gt; Pure ())) So `ignore` evaluates `&gt;&gt;=` twice. Now, `ignoreN_1`. The base case (`return ()`) evaluates `&gt;&gt;=` 0 times. The recursive case: ignoreN_1 n -- evaluate ignore &gt;&gt;= \_ -&gt; ignoreN_1 (n-1) -- 2 evaluations of &gt;&gt;= in ignore (Read (\c -&gt; Pure ())) &gt;&gt;= \_ -&gt; ignoreN_1 (n-1) -- 1 evaluation of &gt;&gt;= Read (\c2 -&gt; (\c -&gt; Pure ()) c2 &gt;&gt;= \_ -&gt; ignoreN_1 (n-1)) -- beta c2 Read (\c2 -&gt; Pure () &gt;&gt;= \_ -&gt; ignoreN_1 (n-1)) -- 1 evaluation of &gt;&gt;= Read (\c2 -&gt; (\_ -&gt; ignoreN_1 (n-1)) ()) -- beta Read (\c2 -&gt; ignoreN_1 (n-1)) So we have this equation for # of `&gt;&gt;=` evaluations: ignoreN_1 0 =&gt; 0 &gt;&gt;= ignoreN_1 n =&gt; 4 + ignoreN_1 (n-1) &gt;&gt;= So `ignoreN_1 n` evaluates `&gt;&gt;=` 4*n times. Now, ignoreN_2. Base case is the same. ignoreN_2 n -- evaluate ignoreN_2 ignoreN_2 (n-1) &gt;&gt;= \_ -&gt; ignore -- ignoreN_2 is going to reduce in some number of &gt;&gt;=s to -- Read (\c -&gt; ... (Read \c -&gt; Pure ())...) (n-1 copies of Read) (Read ... \c -&gt; Pure()) &gt;&gt;= \_ -&gt; ignore -- &gt;&gt;= is evaluated n times to give Read (\c -&gt; ... Read (\c -&gt; (Pure () &gt;&gt;= \_ -&gt; ignore) -- evaluate &gt;&gt;= 1 more time Read (\c -&gt; ... (\c -&gt; ((\_ -&gt; ignore) ())) ...) -- beta Read (\c -&gt; ... (\c -&gt; ignore) ...) -- evaluate ignore (2 &gt;&gt;=s) Read (\c -&gt; ... (\c -&gt; Read (\c -&gt; Pure ())) ...) So `ignoreN_2 n` evaluates `&gt;&gt;=` (n+3) + # of `&gt;&gt;=` evaluated by `ignoreN_2 (n-1)` times. Table: n &gt;&gt;=s 0 0 1 3 2 7 3 13 4 20 This ends up as O(n^2 ).
I'm not sure how you can really avoid the 'quadratic costs', at the very least you're paying a penalty for any abstraction. Any time you write do x &lt;- some_function rest of computation You pay for an extra layer of &gt;&gt;= for all the binds inside of `some_function`. While it's not 'quadratic', since we aren't talking about recursive functions here, it's still a penalty on abstraction that gets worse the more abstraction you use. Is the performance a lot worse using some of the "Free Monads for less" techniques instead of naive Free/FreeT?
See also [demystifying dlist](http://h2.jaguarpaw.co.uk/posts/demystifying-dlist/) that explains a similar problem in the context of list concatenation. The analogue of dlist for monads is [Codensity](https://hackage.haskell.org/package/kan-extensions-4.2.2/docs/Control-Monad-Codensity.html).
Actually I lied :x `let` expressions don't quite work _yet_. They will be available in version 0.0.3! Sorry about that!!
&gt; I only know of one faithful functor from Monads over Ideal Hask to Applicatives over Ideal Hask. It satisfies `(&lt;*&gt;) == ap`. While I haven't gone through this in full detail, I'd be surprised if there was a another and it didn't have this property. (If you know of one, I will immediately come around to your point of view.) A functor from `Monad` to `Applicative` (over some category) can be given by taking `pure = return` and `&lt;*&gt; = ap` for *any* of the two definitions of `ap` mentioned (the right-to-left or the left-to-right one). This was already known in the paper that introduced idioms / applicative functors, [Applicative Programming with Effects](http://staff.city.ac.uk/~ross/papers/Applicative.html), Conor McBride and Ross Paterson, 2008: &gt; The IO monad, and indeed any Monad, can be made Applicative by taking pure = return and (~) = ap. We could alternatively use the variant of ap that performs the computations in the opposite order, but we shall keep to the left-to-right order in this paper. &gt; &gt; [...] &gt; &gt; Thus in categorical terms applicative functors are strong lax monoidal functors. Every strong monad determines two of them, as the definition is symmetrical.
This is not a very serious problem IMO, and would vanish with good enough tools that could sort out your imports for you.
Just a guess, but maybe the question should have read: &gt; What would cause the cabal executable to not be available when I've installed cabal-install To that, I'd say it depends on how cabal-install was installed (which I believe is what /u/creichert was getting at. As an example, I installed cabal-install using my package manager (pacman for Arch Linux), but have `~/.cabal` prepended to my `PATH` so that `cabal install`ing `cabal-install` recognizes the new `cabal` executable. Man that was a mouthful.
In a BST (as far as I know) each node contains a key, and the tree mantains always some kind of ordenation. This tree doesn't. Also, in this implementation a node with only one child is impossible. Although I'm interested in the extension of fold to tree-like structures.
Correct. He's described a general binary tree. Compare chapters 9.1 &amp; 9.3 of Introduction to Functional Programming Systems Using Haskell (which he references).
Yes indeed. Want to send us a patch? :-)
That's really overthinking the problem. Also, it's going to also discourage Haskell newcomers from contributing.
I gave a [talk at ZuriHac](https://github.com/meiersi/HaskellerZ/raw/master/meetups/20150529-ZuriHac2015_Duncan_Coutts-Better_Faster_Binary_Serialization/binary.pdf) with an example where you really do want a deep embedding of composition for performance reasons. It felt a bit odd since I know it's not the recommended practice.
&gt; Is haskell purely functional? It is, and I explained it here: http://www.reddit.com/r/haskell/comments/37q0wr/functional_programming_slack_community/crsep0i?context=3 &gt; It's not total, so can you even say a method is a true function? Totality doesn't decide what is purely functional. If you want to talk about totality then you need to use the word totality, not purely functional. I explained it in the comment I linked above. &gt;You say no state, but what about using ST to manipulate an array for instance? Sounds like mutation to me. ST is pure. I'm not going to elaborate on the assumption that you know that and were bullying the person you were replying to.
&gt;well, maybe. but that's the point, we're arguing definitions so you can't just say I'm wrong with circular reasoning. We're not just "arguing definitions".[1] I'm using what "purely functional" has meant for *decades* for which I've provided evidence dating back to 1965 in the comment I linked. Your distinction is idiosyncratic, the onus is on you to provide evidence for your definition of purely functional. I've provided an old source (also where monads being useful with an LC was first known to be noted). [The wikipedia definition works as well](http://en.wikipedia.org/wiki/Purely_functional). You need to provide a source or justifying reason to totally revise the definition of purely functional CS has operated on for the last 50+ years for me to believe you're serious. [1]: Cf. the absurdity of that throwaway excuse for low-effort, low-familiarity disruptiveness when somebody is asked to reconcile confusing/contradictory use of language, "oh we're just arguing semantics" - except semantics are the actual *content* of what we're communicating so it does nothing to make what is at hand *less* important.
Good points. I think the fact that I'm passing around a `TorControlServerSocket` is kind of a hack; it actually represents the state of the session (only in this case, all state is represented by a single socket). I think it would be best to encapsulate that in a `data Context` or something like that.
not an m4 fan i take it
Have you tried the Reflection Without Remorse approach? It has benchmarked pretty well for me in my OCaml implementation, at least.
I'm not aware of a standard one. I guess that's a blocker. Edit: I should also point out that the fastest version I've written is more like operational than free, but it's not like the operational version you will probably find first. From memory, it's something like this: data Free f a = Return a | forall e. Bind (f e) (TSequence (Kleisli (Free f)) e a)
Heh. I spoke to Ed a little bit about the `Pairing` trick after Lambda Jam, and he mentioned that the pairings aren't necessarily unique (because of something to do with adjunctions). Unfortunately it was loud at the bar, I was tired, and I had questions for Ed based on his talk, so I just mentally bookmarked that point and moved on without thinking about it much further :/ By the end of this series I should have a small handful of useful but boilerplate-y things, so I might have a play around with generics and/or Template Haskell and see how far I can get.
Is video available anywhere?
Even better is detailed type signatures. Lisp docstrings are mainly useful for knowing what sorts of arguments and return values are possible, but Haskell doesn't need that because they're specified, enforced, and exposed by the types. You can rewrite the types using synonyms or newtypes for more specificity. Between that and good naming, documentation is largely unnecessary. But it is sometimes necessary, and for that we have haddock, which isn't as good as docstrings, but is also far less important.
Ah, fair point. With that knowledge, I do agree with what you initially said.
At least one other person ran into the same issue a few years ago while playing around with the Bitstamp bitcoin REST service. http://swizec.com/blog/writing-a-rest-client-in-haskell/swizec/6152
Do machines next!
I think that's probably a good rule of thumb, and I think that "beginner-friendly" is a good way of describing what that rule-of-thumb represents. I don't have any objections to people advocating for beginner-friendly notation, though I don't think that's the only sort of notation that's worthwhile. In other words, I welcome new syntax that makes things beginner-friendly, but time will tell whether such things are *also* friendly to the experienced.
Wait, so you are or are not defending /u/davidhq's definition of purely functional? Because it doesn't sound like your source (Landin) matches up with what he said. I don't thinkt he wikipedia article is valid at all. Since your argument is sound and I read the source and sounds legit, that's fine, we'll go with the definition you provided in the other link. I'm still not sure how we rectify the ST monad (operating on a mutating array for instance) according to your definition though. 
Glad you like it! Helping people to get involved is my goal with those updates.
Nice work! Clearly, we need more ZuriHac events per year... can't wait to see what's next... =)
speaking of language wars... :)
Niiice! Maybe Haskell platform should be renamed to "Zinc" now?
If you have `f -| g -| h` then you can pair `g` with either `f` or `h`. newtype Const a b = Const a newtype Lim f = Lim (forall a. f a) data Colim f where Colim :: f a -&gt; Colim f gives you a threeway adjunction like that: Colim -| Const -| Lim (This is weaker than the corresponding construction in "real" category theory because parametricity is a stronger claim than naturality, so you can't use this version to derive products and coproducts from the adjoints to the diagonal functor.) This is why the `Zap` machinery in older versions of the `adjunctions` package wound up being based on passing a data type instead of a class, figuring that you could compose the structures as you needed. http://hackage.haskell.org/package/adjunctions-0.5.2/docs/Data-Functor-Zap.html
Ah, that makes more sense. I was trying to work out what issues would arise if you went back and forward multiple times through the same pairing.
Exactly. That's like adding "_variable" to every variable name in C or Python. It just duplicates information, doesn't add anything new.
Yi is not dead and currently more actively worked on than ever in the last decade. Just check the git repositories. It has huge potential and time invested there is much more likely to give us the Eclipse equivalent than any Emacs or Vim integration of various tools. You're right that there's not enough documentation or tutorials.
Thanks, fixed.
Thank you for such posts. Firstly, it reminds me to update `haskell-mode`. Secondly, it makes it easier to find new features and fixed stuff. Previously I was reading commits :D And it wasn't cool. So keep posting! **Update after I've read the post** Nice month! I am very happy to see how the mode being improved more and more.
Thanks, awesome explanation! I have a follow up question for the `Right` part. mapM_ print (map (addDelaySubtitleBlock (read (head args) :: Integer)) r) How can I know the type of r? I had thought this to be something like `IO [SubtitleBlock]`, but instead it must be closer to `[SubtitleBlock]` as it compiled when I inserted `map`. I just tried different mapping functions until one worked. With regards to the time. I was looking for a way to parse the time and construct a data type. Additionally, I was looking for a function with this type signature: add :: Time -&gt; Time -&gt; Time which I would then use in the ``addDelay*`` functions. Or I could just use an Integer to hold the milliseconds, mimicking the approach onmach toke.
Unfortunately, type-aligned's "`FastQueue`" is really, really slow. That's why I don't recommend it.
Very nice article! You mention adjunctions in the comments below, but what precisely _is_ the connection between Pairing and adjunctions? (The curry/uncurry example is of course every computer scientist's favourite example of an adjunction :)
You might look into neovim's fancy plugin support. Should be simple enough to write Haskell bindings if they don't already exist.
Ah, I actually meant a library that parses Haskell, like `haskell-src-exts` but extensible.
The curry/uncurry adjunction (isomorphism) is not a free/forgetful/cofree-like adjunction. 
There is still a pairing for that adjunction though: instance Pairing ((-&gt;) a) ((,) a) where pair p f = uncurry (p . f) instance Pairing ((,) a) ((-&gt;) a) where pair p f g = p (snd f) (g (fst f))
Great to see it goes both ways :-)
My understanding is that extensible effects are essentially a fancy way of taking free monads of coproducts of functors, with various type tricks used to make them practical.
You probably want `deriving (Enum,Bounded)`, and then you can use `[minBound..maxBound]` to represent a list of all the data constructors.
You can always get a Pairing from an adjunction - check out `zapAdjunction` in [Data.Functor.Zap](http://hackage.haskell.org/package/adjunctions-0.5.2/docs/Data-Functor-Zap.html) which Ed linked to above. It's time for me to sleep now, otherwise I'd have a go at diving further down that rabbit hole. Hopefully someone who knows more about this stuff will have enlightened us all by the time I wake up :) Edit, post sleep but pre coffee: You can always go from an adjunction to a Pairing, and you can always flip a Pairing to go in the other direction. If you could go from a Pairing to an adjunction, then you could derive g -| f from f -|g, which I don't think is going to fly in the general case. I'm kind of curious about the existence of trivial / specific kinds of adjunctions which would allow that.
Whoops, yeah.
I think I see where you're coming from, now, and I see why the other poster was comparing it to using the monoid with the same ordering of operations as the parent group. In one applicative instance the ordering of effects is the same as the notation and in the other it is different, but only if you privilege the left-to-right version of bind. m1 &lt;*&gt; m2 = m1 &gt;&gt;= \x1 -&gt; m2 &gt;&gt;= \x2 -&gt; return (x1 x2) m1 &lt;*&gt; m2 = m2 &gt;&gt;= \x2 -&gt; m1 &gt;&gt;= \x1 -&gt; return (x1 x2) It's more like the choice of using the "right handed" cross product instead of the "left handed one", or choosing a left or right module action. These vary by taste. I'm relieved you're not arguing that things of the form `instance Monad ZipList` should be allowed. Perhaps the law should be that one of `(&lt;*&gt;) == ap` or `(&lt;*&gt;) == ap'` holds, but I can see how that would be awkward for the Haskell ecosystem. 
Just to clarify the appropriate use of terminology. Here, `X`, `Y` and `Z` are not types. `Direction` is a type, and `X`, `Y` and `Z` are the *values* which inhabit it. Other commenters have explained how to obtain an exhaustive list of such values.
"Dead" or not it's still a very nice project and definitely complete enough to work with. I now use Yi where I used to use Vim for editing Haskell, so far it's been great.
I recently found Spacemacs and must say it is pretty solid. I came from bbatsov's Emacs Prelude.
[It's out now on Hackage.](http://hackage.haskell.org/package/codec)
I think there may be a more general way to formulate that law, that is equivalent in its effect (it gives the same equational theory) but less apparently arbitrary. For example, could we simply require that `pure f &lt;*&gt; m` be equal to `ap (return f) m` and `m &lt;*&gt; pure x` be equal to `ap m (return x)`? The two definitions of `ap` should be equivalent for the purpose of these equations; but I don't know whether they are strong enough to to fully characterize coherence. One question that is related is whether these two forgetful functors are the only possible one. This is suggested in the McBride and Paterson paper, and I think they are, but I wouldn't know how to formulate a proof in categorical term. I think one may prove it by reasoning on all possible well-typed programs of type `Monad m -&gt; Applicative m` (for reified-as-records versions of these clases).
If you need inspiration ;) https://github.com/sebastiaanvisser/subtitles/blob/master/src/Subtitles.hs 
I fail to see the benefit of single letter type variables if you're willing to have the long version as well. What use do you have in mind?
This isn't entirely accurate: import System.IO import System.IO.Unsafe showProgress :: String -&gt; IO String showProgress [] = return [] showProgress input = unsafeInterleaveIO $ do let (block, tail) = splitAt 1024 input blocks &lt;- showProgress tail putChar '.' return (block ++ blocks) main :: IO () main = do hSetBuffering stdout NoBuffering writeFile "b.txt" =&lt;&lt; showProgress =&lt;&lt; readFile "a.txt" Not saying you _should_ do it this way, but you _can_ :)
How would substructural types help here? Other than making it possible to _rule out_ a definition such as `average` -- that doesn't give you an alternative solution though. 
As I see it, laziness can be used wherever one wants, instead of some places predefined in the language.
One of the objections was performance, which is (IIUC) neatly solved with the "Reflection without Remorse" approach. I wonder if any of the other objections have lessened with time and progress or if any new ones have arisen. 
This deserves a cross-post to /r/haskellgamedev
Lazy state and lazy writer cannot be captured by any effect system. Quite simply anything using "free" is going to be pulling effects from the wrong end of an infinitely long queue when you do something like foo = do foo modify (1:) which works fine with Control.Monad.State today. On the other hand, you might be working in a strict language, like Idris, where these things just don't exist, so you haven't lost anything yet. Next, if you use a simple free monad full of effects you have the wrong asymptotics for left associated joins and can't handle `Cont r` effects. On the other hand, you might be in a proof assistant of some sort that requires strictly positive types, and this wasn't in the cards anyways. Another issue with the direct "free" encoding is that left associated (&gt;&gt;=)'s in a normal free monad are potentially very slow, having the wrong asymptotics. If it is available, and you use Codensity applied to such a free monad then you can fix those asymptotics, in exchange for getting the wrong asymptotics for repeatedly accessed structures. Codensity also results in structures that are "too big". The approach in that paper is `Codensity` based, so it holds here. `Codensity ((-&gt;) s) a` is isomorphic to State s a, not `s -&gt; a`. `Codensity m` is much bigger than `m` in general. You can fix that by taking the right Kan extension of your responses along your request types, splitting up your open sum type into two open sum types. One of requests and one of responses: Eff xs a = forall r. (a -&gt; Request xs r) -&gt; Response xs r You get a much uglier monad. Evaluation becomes less efficient as (&gt;&gt;=) no longer comes for free, but now you can handle `Reader` effects without the system constantly worrying you might be secretly working with `State`, while trying to quotient out that concern by just not exposing the methods a user would need to do so`, so that is something. I wrote a library for that back in 2009. http://hackage.haskell.org/package/monad-ran Inside of it I included all of the mtl effects, but my state and writer were (necessarily) strict. Next, there are other effects I tend to 'accrete' multiple times such as the `Scope b` transformer in Bound. It can be encoded as an effect, but without reflection without remorse it means that when you revisit a branch of your syntax tree you pay to re-evaluate the whole thing, from scratch, every time. This doesn't yield a very fast compiler. If you use reflection without remorse you get terrible constant factors, but now you don't have the usual Codensity/Kan extension problems. I don't know how to handle Cont effects under that model, but it may be possible. The version in that paper requires Typeable state. `StateT (State s) (ST s)` is something that can be quite useful, and neither monad involved is `Typeable`. With more modern type system features you can avoid that concern with closed type families or other tricks, but you now have traded a lot of inference for multiple states. YMMV, but I prefer inference to scoped type variables and a ton of manual signatures. There are a lot of reasons to be wary of effect systems as a total replacement for the `mtl`. On the other hand, for the effects that they _can_ handle, using effect handlers remains a reasonable point in the design space. And effects like lazy state, backwards state, the tardis, lazy writer, bound scopes, etc. aren't really in that subset. I perfectly well understand the desire of folks to avoid the mtl classes and the n^2 instances you need to work with them, but, finally, there is one last issue that keeps me from loving effect systems: Handlers lie. You can have a handler for `Cont r` effects and a handler for `Writer e` effects, but when you compose them _you fail to pass the `MonadWriter` laws_!!! In the mtl we can signal that by the lack of a `MonadWriter` instance. With the handler model you get no indicator that your attempt to handle one class of effect caused you to fail to handle another in a law-abiding manner. For basic Lawvere theories you get by fine for the things that fit, but for things like `Cont r` which go beyond that model there are silent law failures! We have to write the "O( n^2 ) instances" in the `mtl` because some of them are very _very_ deliberately missing. This combination of factors make me very leary of effect handlers in a lazy language. I'm much more amenable to the notion of them in a strict language where I have to use strictly positive types such as Idris, however, but you need to implement them carefully (e.g. without remorse) or you get the wrong asymptotics, and you need to concern yourself with handlers that may cause other handlers to lie about passing the laws that are carried along with their effects.
I think folks usually call those “data constructors” or just “constructors”
Reflection without remorse resolved the asymptotics issue amazingly well, but the constant factors are still troubling. I've been struggling to get a "win" out of it in production.
Care to explain? I didn't get that reference :(
&gt; One of the objections was performance, which is (IIUC) neatly solved with the "Reflection without Remorse" approach. No, [it is not](https://ro-che.info/articles/2014-06-14-extensible-effects-failed#free-monads). It is addressed by my [monad-classes](https://github.com/feuerbach/monad-classes), however.
I'm going to rewrite your expression a bit: mapM_ print $ map (addDelaySubtitleBlock (read (head args) :: Integer)) r What's the type of mapM_? [Hoogle says](https://www.haskell.org/hoogle/?hoogle=mapM_)`mapM_ Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m ()`. The underscore is used to indicate we don't care about the result (per Haskell convention). We can specialize `m` to `IO` and `a` to `Subtitle Block` to get the specialized type signature: mapM_ :: (SubtitleBlock -&gt; IO SubtitleBlock) -&gt; [SubtitleBlock] -&gt; IO () When I rewrote the expression, I tried to separate the arguments to the various functions. So `mapM_` takes a function and a list. `print` is your function. Your list is the expression `map $ addDelaySubtitleBlock ... r`, and we already know the type of the list is `[SubtitleBlock]`. The type signature for `map` is `(a -&gt; b) -&gt; [a] -&gt; [b]`. Since we know the final return type, we can specialize `b` to `SubtitleBlock`. The type signature now looks like: `map :: (a -&gt; SubtitleBlock) -&gt; [a] -&gt; [SubtitleBlock]` So that first parameter to `map` is a function which takes an `a` and returns a `SubtitleBlock`. You partially applied `addDelaySubtitleBlock` to an Integer, which makes it a function of type `SubtitleBlock -&gt; SubtitleBlock`. We can further specialize `map`'s type signature now: map :: (SubtitleBlock -&gt; SubtitleBlock) -&gt; [SubtitleBlock] -&gt; [SubtitleBlock] So `r` must be of type `[SubtitleBlock]`. Neat! Alternatively, you could look at the return type of `parse` from the library you imported. [Here's a link to the Hackage documentation](https://hackage.haskell.org/package/parsec-3.1.9/docs/Text-ParserCombinators-Parsec-Prim.html#t:GenParser). The return type is `Either ParseError a`, where `a` was defined by the first argument (of type `Parsec s () a`). What generates a `Parsec` type? It must be `parseSubtitles` since that is passed to `parse`. The type of `parseSubtitles` is `GenParser Char st [SubtitleBlock]`. What's `GenParser` about? Back to the Hackage docs! type GenParser tok st = Parsec [tok] st I'm -- actually not entirely sure what's going on here. The type signature of your code indicates that GenParser takes three arguments, but here it only takes two. Perhaps `Parsec` is being partially applied here? Let's look at that! [Hackage link for Parsec type](https://hackage.haskell.org/package/parsec-3.1.9/docs/Text-Parsec.html#t:Parsec) shows that: type Parsec s u = ParsecT s u Identity Another type synonym! Is ParsecT partially applied here? Is it, perhaps, expecting another argument? data ParsecT s u m a Ah! So it is! The docs on `ParsecT` say that `a` is the 'return type' of the Parser. So now, we can go all the way back to your code, and look at the type signature: `parseSubtitles :: GenParser Char st [SubtitleBlock]` And know that the return type of `parse` must be `Either ErrorText [SubtitleBlock]`, since we have filled in the value for `a`. The types in Haskell will tell you a lot, once you learn what the right questions are :)
I've used Yi. It's certainly a pretty good Vim clone. If you're a vanilla vim user then Yi should feel pretty comfortable to you. Its only deficiency in this aspect is macros IIRC. It definitely seems like yi isn't active lately but I think these things come in cycles. Personally, I think the problem with Yi is that it's aimed at people who would like to write plugins in Haskell. But there's almost no documentation on how to do that. So the people who are interested enough in trying it out don't end up committing because they haven't written any plugins. On the plus side, they've done a lot of good work with dependencies and such. I'm pretty sure the prelude problem is mostly solved now. Making it far more approachable for contributors.
It's fairly complex to do so, from the readme: &gt; But building it for mobile devices ain't that easy. &gt;Apart from requiring a GHC cross-compiler, you must cross-compile various C libraries and then build cross-compiled versions of all the Haskell libraries which, unfortunately, doesn't work out of the box for some libraries when installing them with Cabal. &gt;So, with the aid of Docker I wrote a script to build a fully fledged Android build environment. This builds on earlier work that I did in the docker-build-ghc-android repo. docker-build-ghc-android just builds a GHC 7.8.3 cross-compiler targetting ARMv7, while this repo builds all the C and Haskell libraries required to build Epidemic. &gt;In conjunction with android-build-epidemic-apk you can build an APK for installation on your Android device. 
Nothing afaik, just time to review.
I think you're right - this is an issue of text files being generally untyped. I don't think that security is the right place for this project, though - it would require too many domains of interest to be embedded in the project. I think that ltext is just a _phase_ for processing, much like how a lexer is a phase during compilation. I will look into StringTemplate, though, maybe I can grab some ideas :) Thank you!!
&gt; Wait, so you are or are not defending /u/davidhq's definition of purely functional? Because it doesn't sound like your source (Landin) matches up with what he said. I don't thinkt he wikipedia article is valid at all. Noting you were bullying OP doesn't mean I agree with their definitions or thoughts. (I don't, methods/state have nothing to do with it.) I do think Landin's definition is the right one and the one most representative of what it has been understood to mean for decades. ST doesn't suddenly change Haskell's semantics into having something more than abstraction and application. So you'd have some work to do if you want to assert that somehow makes the language impure. I'd suggest that you'd also need to explain how everybody that worked on ST missed that it invalidated Haskell's purity where IO did not. My guess? You're fetishizing specific features/properties rather than keeping in mind on what "pure functional" means or what it gives us.
Yep, I agree. I wanted to write plugins, I could find a few examples and get a rough idea but beyond that not much. Still confused. Just a guess: perhaps it's because Yi isn't done yet/is changing and they don't want plugins to be written? I haven't seen this said anywhere but it would make sense. Also, on their site they acknowledge plugins being hard to make as a problem. So I guess they might be working on it too.
Is there 'jump to definition' support? I couldn't find it in docs.
I *am* using this package in production. In fact, the needs of my system at work were one of the big drivers behind this development.
What layers do recommend for haskell development?
If someone finds a good tutorial I will give it another shot. Last time I tried Yi I couldn't figure out how to do basic things.
So why *does* the HP do this?
&gt;Emacs has plenty flaws, and ELisp is not fast nor type-safe, and gives me a lot of runtime errors (especially after updates of packages). Er, the messages you see when updating packages are actually *compile-time* and are *warnings*, and they are just there because the elisp byte-compiler is somewhat aggressive about printing warnings - they don't actually correspond to bugs.
[It's a bug](https://github.com/haskell/haskell-platform/pull/137). Basically the directories didn't exist prior to the HP install, so the install script created some erroneous symlinks. The fix is just to `mkdir -p` those directories first.
conduit is actually used in web development though :-)
I got something working, but it would be great to hear about better alternatives - of which I'm sure there are plenty.
Maybe save someone some googling: https://github.com/yi-editor/yi https://wiki.haskell.org/Yi
You are right, ruling out bad code like that is exactly what linear types do. They do allow more efficient implementation in some cases. If you have a data structure that is used linearly then updates can use mutation under the hood. If you don't have linear types then updates have to be implemented with expensive path copying even if the data structure happens to be used linearly. In most cases data structures *are* used linearly. This is the main reason imperative languages have a performance advantage.
Yes, I know what linear types do :) I'm just saying, they don't really offer a solution here. You'd still end up writing `average` in a convoluted way.
The C thing you're imagining probably allows (much) more than ST does. Further, C thing implies adding more to the semantics than [Application, Abstraction] as C is imperative and has a lot more to it than that. How do you explain something like `sizeof` (exclusively) in terms of application and abstraction? You want to mentally contrast ST with IO and think about what it means for something to be guaranteed pure on `runST`. Then consider why IO exists to begin with and how that contrasts with why ST needs to exist.
The last decade?!
Thanks. Let me know if you see anything missing.
Yes, this is basically the same philosophy I took with `hs-nacl`. It's waaaay easier to play with the design space, types, and implementation details when it's all in one package. It also makes it a lot easier to point people to the latest documentation I think, which can be complete (and documentation is no longer is scattered among packages if you don't consolidate it). Sounds like a good win to me!
The issues we have troubleshot tell me that the long livedness isn't the issue. Its that small changes in code can drastically alter the laziness of pieces of code. The compiler knows how lazy the code is and it would be great if it would share that information. For issues with longevity a profile guided optimization like system could dump out thunk lifetimes.
Does pivotcloud actually use all these or is it mostly "for fun"?
Started in 2005 and it's been very active the last two years.
Oh I see. So you can't really write an entire application with the NDK, unless you don't need any of the native interface.
How does lazy evaluation affect timing side channel attacks against encryption algorithms?
Yes, please do. I corresponded with them regularly and they were very helpful.
i don't know what you want for a backport, but you can always build just the package you want from the master branch. alternatively you can use the [nixpkgs-channel](https://github.com/NixOS/nixpkgs-channels) repository to find the commit your local channel can be updated to and try to cherry-pick the changes to the nix-expressions you need from the master branch of the nixpkgs repository. that way you should get the same updated software you want, bot not having to rebuild a bunch of other stuff that has changed as well
That's what I'm hoping to make more possible by releasing this repo. However, as gelisam points out, this won't help you write apps.
There is a mailing list you can subscribe to if you want to beta test at: http://eepurl.com/boW1vz 
More people complaining about dependencies?
pivotcloud is no more, so i imagine its because he likes to work on crypto code :) 
I think that having the nixpkgs-channel repository and then cherry picking changes is exactly the solution I've been looking for. Since I actually rebuilt an environment from master (from unstable) and didn't like having to rebuild libc, gcc, etc... lol. So, thanks!
I thought windows users still benefited from the Haskell Platform
Yeah, I know how to do it (and I read your post), thanks. I was just bitching.
&gt; the consensus Windows HP is still installed by thousands of people every month. Stackage is still a comparatively complicated list of steps you have to do, compared to a one-click install. It's important to think from the point of view of beginners; of students; of people who want to build some Haskell app, but don't care about Haskell. Not everyone is a dev/enthusiast who is willing to execute multiple, independent steps to get a working Haskell env. 
I've never seen Yi as a re-implementation of Emacs, but I take your word for it. Totally agreeing that Emacs is awesome, and hard to beat at it's game. Though I think that if a hacker editor like Emacs will every be knocked of it thrown then it must be by using a language that is able to do so. Haskell might just be that language. The "likely or not" debate aside: a have my hopes up for Yi. :)
I mean that I get incompatibilities after updating packages. Emacs' package system is not mature enough (pinning versions seems to be hard somehow), and there's no type system to ensure compatibility between all parts to some degree.
The "Haskell" one? I found a bug and fixed it with a workaround (when you run into the bug just look for the message in Spacemacs' Github issues)
you can find what pivotcloud was using there: [ac-crypto-prim](https://github.com/alephcloud/hs-ac-crypto-prim) (which we manage to opensource before closing). In a nutshell, we were using most of the crypto packages (hash, aes, ..), along with some cryptonite stuff (chacha/salsa/pbkdf2), along with some extra openssl bindings for ECC / AES. My short term plan is to take those openssl bindings and package it properly as a cryptonite-openssl bolt-in package.
yes, the way cabal / packaging / versioning work currently is not ideal IMO. I like hashes in this context, but a middle ground would be to introduce versioning on module inside a package. Which would let people express much better their dependencies : "I'm depending on API version 4 or 5 of module A.B.C in package foo, but the rest of the modules in the same package do not matter to me"
The zinc is a "*sacrificial* anode" though. Sounds bad for Haskell.
I can't thank you enough for your amazing answers!
The platform gets unfairly blamed for the problem where people want to start a new minimal sandbox, but cabal currently puts everything from the global package db into each new sandbox. We need to be clear about where in our stack our problems are, and then we need to fix them where they are. In this case the problem is with how cabal constructs the sandbox, and the fix is not so hard. [Someone started working on it at ZuriHac this weekend](http://www.well-typed.com/blog/2015/06/cabal-hackage-hacking-at-zurihac/).
&gt; That was a really interesting read. In particular I think the introduction to functional programming for imperative/OO programmers was actually really good. It's one of the better introductions to FP that I've read which explains a lot of functional/type system concepts in terms an imperative programmer can understand. I'm glad you enjoyed this part, it was my major concern about the fact that it would be easily understandable by non-functional programmers. &gt; With regards to the mutable state using Vectors, a while back I stumbled on the vector-strategies package which has a single function for parallelised optimisations for boxed vectors. You might also want to check out repa and yarr which offer parallel arrays. I didn't know about `vector-strategies`. It seems to work on any vector type (not only boxed types). Maybe I would use it in `friday` sometime. I started building `friday` on `repa`, but the framework has nothing for mutable arrays, and these are required to write some image algorithms efficiently. Thus I started to use `vector`s instead. &gt; OpenCV is actually a really good example of where I think Python is so strong - by having the ecosystem to make use of these powerful libraries without having to know C/C++. It's the same for a lot of machine learning / natural language processing libraries. It makes Python a top choice for people who want to learn to code so that they can use these algorithms without having to learn "hard" languages. Yeah, Python is probably the best language to prototype machine learning and compution vision algorithms, because of the huge number of libraries. &gt; With having an order of magnitude performance cost to OpenCV do you think the advantages of the Haskell type system / functional purity make your library (or something similar) an attractive choice? I think purity and functional programming would be very useful in an image processing library, to make more generic and reliable algorithms. But it would be an HUGE amount of work to become comparable with OpenCV, has the library implements an astronomical number of algorithms. Speed is clearly an issue, but keep in mind that OpenCV is an order of magnitude faster because it uses specialized algorithms, with SIMD intrinsics (SSE, MMX, ...) in between. The fact that it's written in C++ instead of Haskell probably only account for a 2x speed improvement. One interesting feature would be to be able to interface with OpenCV, so one could use `friday` except when performances are required in which case OpenCV would be used. 
ah that's a shame. Didn't they exhibit at RSA recently?
What is it like to work with Haskell full time? Aren't there very few Haskell professionals? Do you usually work alone?
Thanks for keeping this up!
I've been interested in Yi for a while, but I've never been able to install it successfully. It doesn't seem to matter if I install in Linux or Windows; I tried most recently in Windows. That was not fun; getting cabal to install just the dependencies was massively frustrating. (charsetdetect-ae, in particular, fails to install and gives some rather mystifying errors without tweaking a C file in libcharsetdetect.) Yi.Types also imports from vty unconditionally, so -f-vty still results in breakage. I really, really want to use Yi. But I can't, because I can't build it.
Before it was the "size of the one package when they only wanted a little piece of it" that was the reason people were avoiding `category-extras`. Pick your poison. I am interested in working on stuff that happens to build on top of that foundation, so I need something to make the interconnections, and orphans are not an option, so we're left with one monolithic package or lots of little ones with dependencies. At least with the mound of smaller packages when something out on the lunatic fringe changes only us lunatics pay for it, with one package, every user pays for every small change. The price of course is that it is harder to think thoughts about changes that cut across the boundaries between packages.
It's ok if you can use -O0 locally and -O2 on production, at least it greatly improved experience for me.
You could use two maps. One mapping boxes to shelves and the other mapping shelves to boxes. You then hide this in a module and export an api that provides the functionality you need while maintaining the consistency between these two maps. The shelf map could contain two values, one being the boxes on it and the other being the space available. Then you could iterate through the shelves seeing if any rotation of the box would fit into the available space. When you find a good shelf you add the box to the list of boxes for the shelf in the Map Shelf ([Box], Space), update the available space, and add the box to the Map Box Shelf. In principle you are producing an entirely new structures when you update these. However the implementation of Map and ghc use sharing and fusion to make updating data structures efficient. There is the idea of programs being a "law" and the compiler is free to use whatever means satisfies that law. So an api can actually be mutating structures internally while exporting a pure interface.
I would absolutely 100% come to these.
Great! start by joining the haskell-platform list (http://projects.haskell.org/cgi-bin/mailman/listinfo/haskell-platform) then email mark and the list to offer your assistance. I imagine the trick is to A) learn the new platform build process and B) teach it to autogenerate html files, then C) redesign it to be in sync with the current haskell.org design. The first few steps don't require any special access anywhere, though they may require guidance from people like Mark that know the platform build process. Once it comes to "making it happen" the haskell-infra admin team is ready to help figure out that last mile. You can always reach it at "admin at haskell.org" or on the #haskell-infrastructure irc channel on freenode.
&gt; Yi.Types also imports from vty unconditionally, so -f-vty still results in breakage. Just fixed that in master, it was probably caused by a merging accident. Thank you for reporting. 
This is why I'm still optimistic about the way Nix-based solutions (well, cabbage) work. I host a binary cache of Stackage, and a user can build against a Stackage release and download binaries. But the user can also build without any additional constraints and still benefit from the binary cache in that whenever the solver would build a package identical to that specified by Stackage, it is pulled from the binary cache. You also build up your own local store of built packages, so everything grows organically without closing the world.
Yes, I tend to think that a nix-like approach will ultimately prove to be the best solution--or at least one of the biggest pieces of the puzzle.
The work to integrate it into cabal is being done as we speak, which looks promising! 
I love this. And I agree with the article that curation *by itself* is insufficient for some people's needs. But why not reap the benefits of curation *and* customization? No need to throw the baby out with the bathwater.
Have you looked at codex? https://hackage.haskell.org/package/codex
I suppose part of the problem is that the tooling isn't ready yet. I use cabbage every day, but I still need to ensure it works smoothly on Ubuntu, and start populating the binary cache for Linux platforms. I've received an offer to provide a Mac build box that will become available in a few weeks, then it's a matter of automating the Linux builds, too. I don't think there's any willingness to piggyback on **Nix as the package manager for the Cabal build system**, but its Windows support is seeing some attention, so there is a tiny glimmer of hope that we could offer binaries for Windows, too. I think the interplay between Stackage curation providing a basis for binary caches and the free-for-all of hackage riding on Nix is really fantastic.
Thanks for your great work on crypto packages! What about tls, and other related packages such as asn1 and x509, not included on the cryptonite list in this post? Together with some of the packages moving to cryptonite, those are a cornerstone of many production web apps in the wild.
A big issue is whether two versions of a single package should ever be used to compile a program. Say package B depends on package A version 1 and package C depends on package A version 2, and you depend on packages B and C. You could compile B with A.1 and C with A.2. However this does not work if B and C expose details of A, for example if one of the functions in B returns a value of a type defined in A, and one of the functions in C takes a value of that type, you cannot safely pass that value from B to C, even if the types still match. Maybe A.1 has a different internal invariant on that type than A.2 has, and passing a value from A.1 to A.2 may cause errors. On the other hand if B and C both depended on the same version of A you would want to allow passing those values from B to C. And if either B or C only used A internally then you would also allow B to depend on A.1 and C on A.2. I *think* a proper module system automatically solves this issue. If some module B only uses A internally, then only the implementation of the module depends on A, and the signature of B does *not* depend on the signature of A. If B exposes details of A then the signature of B *does* depend on the signature of A. So this would give you precisely the right criterion for when it is safe to use multiple versions of the same module even in an arbitrarily complicated network of modules that depend on different versions of other modules.
I thought the backpack work was a prelude to offering the ability to distinguish internal and external dependencies (i.e. a dependency is external if one of its types appears in a downstream package's exposed definitions). This would be inferring a de facto signature rather than an explicit one. It seems like this would also be a boon to tooling that automates PVP compliance to some extent.
&gt; Stackage has been iterating on which hack it uses to peg dependencies (remote-repo, cabal.config, now cabal.sandbox.config) Those three things you mention are all features explicitly programmed into cabal to allow dependency pegging.
There is a difference between dependency pegging and semantically understanding that there is a curated package set. It should be easy to treat a curated package set as a strong preference rather than a hard peg and to be able to suggest to the user how they can go outside that preference in the most conservative way possible to satisfy a build. Theoretically this could be done in the pegging case also, but the tool no longer knows whether you manually changed things in the cabal.config
&gt; It should be easy to treat a curated package set as a strong preference rather than a hard peg and to be able to suggest to the user how they can go outside that preference in the most conservative way possible to satisfy a build. As the author of stackage-sandbox, I absolutely agree that this is a deficiency in the stackage-sandbox approach.
&gt; cabal-install doesn't tell the user where something is pegged (or even explicitly that it is pegged) It does say that it is pegged, it will list the constraint as "global" ``` rejecting: base-4.7.0.1/installed-c64..., [...] (global constraint requires &lt;4) ```
TBH I'm not really aware of closure-based design patterns. Some styles of code depend on some way to access the data, but lexical scope is just one particularly convenient way to access that data. There's plenty of *idioms* that rely on lexical scope, but not on a scale I'd call a design pattern. I guess maybe continuation passing style? But I've never felt the need to use it TBH - since the point is to define control flow, I suspect it's more for the formal semantics crowd and for compiler writing. I was serious about the implicit parameters point, though. Basically, implicit parameters are another way to implement lexical scope - an alternative to closures. [**EDIT** - stupid mistake above - you need a closure to implement partial application too, including those implicit parameters. I let the memory of ye ancient Python along with not thinking about currying etc deceive me. Presumably when Python didn't have lexical scope for lambda, that was so the VM compiler didn't need to determine which extra identifiers to capture.] Closures require an implicit parameter to point to the closure - I guess maybe that's part of the like an OOP this/self parameter and possibly part of your reasoning. But that's still just one way to implement lexical scope. Lexical scope is lexical scope - not one facet of its implementation. The implicit pointer parameter is not what it's for in either case - it's just an implementation detail. If you *really* want to understand the benefits of lexical scope and closures, you could read the famous Scheme "[lambda papers](http://library.readscheme.org/page1.html)". I confess I've personally only read a few little bits a long time ago. 
ALGOL 60 had lexical scope, no? Certainly C, which predated Scheme, but ALGOL at least allowed nested functions.
Amazing: on time and without bugs. Added bonus - it'll also prove to be maintainable over time. 
C doesn't have lexical scope except in the sense that it doesn't have nested function definitions (a GCC extension supports this, but it's not standard) - sure the scope is lexical but only by sidestepping the point. Pascal I think had nested procedures but I'm not sure about lexical scope - it's a long time since I used Pascal. I bought some Algol books a little while back and I've never had much trouble reading Algol on the few occasions I've needed to, but I've not attempted to learn it or its history so I don't really know. I know John McCarthy, original designer of LISP, also influenced the design of ALGOL but I didn't think either of those languages had lexical scope. Of course McCarthy was aware of it from lambda calculus - I assumed it was perceived as too costly at the time. After all, this was a time when the idea of even recursive functions/procedures in programming languages was shiny and new. But I could easily be wrong. 
The user's dependency plays perfectly nice with the rest of the ecosystem. That's why it built with no trouble when not using stackage.
An algebraic data type is a type that consists of sums and products (different constructors and constructord with multiple fields). An abstract data type is a data type that has its implementation abstracted over by an API that is defined... usually with some sort if encapsulation over its implementation. For example, a priority queue is an abstract data type. In memory it might be implemented as an array, or as allocated cells on the heap, or with a `struct` in C, or as a shakespearean sonnet. However, you abstract over it with a well defined interface: push and pop. you can push items into the queue with priority, and you can pop the highest priority item out at any time. Another example is the associative map or dictionary, which can be implemented with hashing, binary search trees, or well trained sea otters. What matters is that you define lookup, insert, and delete operations that abstract over the underlying implementation. So they really talk about fundamentally different things. Some data structures can actually be seen as abstract data types implemented as algebraic data types! Like the commom linked list abstract data type in Prelude, which is implemented as the [] algebraic data type. the interface it offers is consing and unconsing, but it's implemented as multiple constructors with one having multiple fields -- sums and products. 
Closures aren't a semantic feature; they're an implementation technique. I'm not sure if you could finagle some way of saying GHC uses closures, but it certainly doesn't do them the same way languages "with closures" do. So, I just think it's orthogonal. Haskellers don't care one way or another about closures. We care about *functions* and *binding* and *scope*.
Plus you get to humble brag that you're using Haskell for $$$
The reason you would have trouble making a shared counter in Haskell is because of immutability, not some quality of closures. In languages with both unrestricted mutation and closures you can easily rig up a shared counter without global state. In Javascript this is even a common design pattern: var counter = (function() { var c = 0; return { inc: function() { c++; }, read: function() { return c; } }; })(); `counter.inc()` and `counter.read()` work as expected, leveraging closures to store the state, change it, and read it later.
Woah thank you!
An **abstract data type** is a data type that only exposes (to the user of this type) the ways you can manipulate it, but not its definition. This introduces an interface for the programmer to use the data type as its creator intended, reducing complexity by narrowing the usage of said data type only in ways that matter. For example, you could make a *Stack a* an abstract data type, by only exposing the operations "pop : Stack a -&gt; (a, Stack a)", "push : Stack a -&gt; a -&gt; Stack a" and "isEmpty : Stack a -&gt; Bool". I, as a programer using this data type, don't need to know how "Stack a" is defined, I just need to know that I can "pop" it, "push" something into it, and check if it "isEmpty". This reduces complexity, because there are way less concepts and entities that you need to be aware of when programming using "Stack a". You don't need to know how to update linked lists, or how to iterate through arrays (if it was in C for instance). You simplify the usage of a stack only to what, conceptually, a stack is. An **algebraic data type** is a data type that is defined in an algebraic fashion. As wikipedia puts it, *algebra is the study of mathematical symbols and the rules for manipulating these symbols*. An algebraic data type follows the same ideal, you have some symbols, and rules on how to manipulate these symbols. In arithmetics you have symbols like '1'; '2'; '3'; and '+', as you have rules for manipulating these symbols, like knowing that 1+2 = 3. With algebraic data types it's similar. You have symbols, like type variables ('a', 'b', etc), product ('x'), sum ('+'), and some more. But you also have rules that tell you how you can compose these together to create new data types, that follow certain rules. For instance, it tells you that if you have 2 type variables 'a' and 'b', you can create their product type 'a x b'. If you have 2 type variables 'a' and 'b' you can also create their sum type 'a + b'. You can combine these rules to create lots of different types, like "Nat = Unit + Nat", or "List a = Unit + (a x List a)". In Haskell, the notation is different. Instead of having "Nat = Unit + Nat" you have "Nat = Z () | S Nat" ("()" being "Unit" in Haskell). Since "Z ()" is equivalent to a single constructor "Z" in Haskell, we just use "data Nat = Z | S Nat". With a list you can make a similar translation into "data List a = Nil | Cons (a, List a)". Notice how the constructors are additions in Haskell. The main utility of using algebraic data types, is the fact that the same rules apply to ALL algebraic data types, and that they can compose together quite nicely. The fact that the same rules apply to all of algebraic data types means that you can reason about them abstractly, and then specialize that knowledge to any specific data type you have. For example, you could prove that "(a x b) + (a x c)" is isomorphic to "a x (b + c)" (meaning you can transform one into the other without losing information). You could use this seemingly random piece of knowledge to better refactor some haskell code. For example, if you have a type "data TaggedShape = Circle String Int | Rectangle String Int Int" you can safely refactor it into "data Shape = Circle Int | Rectangle Int Int" and "type TaggedShape = (String, Shape)". The fact that algebraic data types compose nicely is also shown in the example above. You can easily define types, and compose them into bigger ones by making products and sums of them. Like mstksg says, these are ortogonal approaches to defining a data type. You can easily have an algebraic data type that also functions as an abstract data type (like a "Stack a" defined as an algebraic linked list in Haskell). 
&gt; Touring around other languages like javascript, some of these functional-ish languages seem to make a big deal of closures as part of the functional paradigm. To me it looks like OOP by another name - passing state and associated method around as a bundle. IMHO, this is a non-issue. What it comes down to is that functional programming has never been all that opinionated about the choice between union types + case analysis vs. first class functions/behaviors + dynamic dispatch. Heck, here's an example that combines both: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map _ [] = [] map f (a:as) = f a : map f as That's basically a switch statement, one of whose branches dispatches to a dynamically supplied behavior. Meh, what's the big deal. The choice between static and dynamic comes down to what behaviors you want to fix at compilation time and which you want to leave open, and that's just that. Easy to learn, hard to master... OOP, on the other hand, has long been extremely judgmental about this, insisting that first class behaviors + dynamic dispatch is The One True Way. "Switch Statements Smell" and all that jazz. So in the pursuit of this vision they have *crippled* their languages, doing things like banning plain old record types (!) in order to force objects on programmers. &gt; All in all closures feel a bit like OOP-in-sheep's clothing. [Closures existed well before the term "object-oriented programming" was invented.](http://en.wikipedia.org/wiki/Closure_%28computer_programming%29#History_and_etymology) And don't forget that one of the biggest influences on Smalltalk was Lisp. Frankly, you're committing one of the common sins of the OOP crowd—crediting OOP for things that it doesn't deserve credit for.
Looks to me there's no deep reason -- probably just trickiness with the interaction of quasiquote parsing and layout...
It is the other way around. We must move to a closed world system to deal with the combinatoric explosion of possible dependencies. Using humans to gauge whether a new source distribution will be compatible with the previous version is an enormous waste of everyone's time. Again and again it is wrong. I will say that it is almost NEVER correct. WHY didn't the author ensure that his package works with the single closed world that everyone uses? 
Algebraic data types let you express sum types and product types; C-style structs only let you express product types.
&gt; cabal-install doesn't [...] suggest any reasonable course of action (or take any automated action). IMO, this (i.e. explaining why the solver couldn't find a solution) is a difficult problem, and certainly does not have a unique answer (somtimes it's just N constraints forming a contradiction when combined, relaxing any single of those constraints can unlock a solution already -- but which of those relaxations is the most desirable one?), since the cabal build-dependency specification language is rather rich with its version ranges and the dynamic cabal conditionals (i.e. automatic cabal flags). Do you have an algorithm/heuristic in mind, by which the cabal solver could compute a suggestion? 
More granular versioning (while still requiring a compound versioning) breaks down if enough packages keep depending on the global compounded package version (which then would inevitably increment faster, as it has to follow each of its modules' version bumps). This is one of the benefits (which ones pays with more overhead managing the inter-package deps) of splitting into individual packages, as package major version increments don't necessarily (unless there's affected inter-package re-exporting going on) require other packages to perform a major version increment as well.
&gt; There are problems with this reasoning. First, a package can't decide for itself what version to use of another package without getting multiple versions of a package. That's bad. &gt; Supporting multiple versions of packages is a great way to build crappy software because buggy package live happily together with same ones. I don't understand how this has anything to do with my comment. &gt; Incidentally if the package had been part of stackage I'm sure someone would have been notified. Yes, but we don't need stackage for this. All we need is a simple monitor that emails maintainers when any of their direct dependencies have a major version bump. But this blog post was about curation and that's not curation.
&gt; By introducing one additional dependency in the project it would be cabal hell. No it wouldn't. You're just speculating and spreading FUD. In fact, the user was introducing two new dependencies and without stackage everything worked fine. No cabal hell. In this situation it was exactly reversed: hackage was heaven and stackage was hell. &gt; I don't see how building a small package is relevant. This wasn't a small package. The successful install plan had more than 90 dependencies. Building large systems is not sufficient. We must build robust systems. The closed world that curation imposes on us is not robust. We need tools that are not encumbered by the limitations of curation.
“Abstract” in plain English means an idea or term considered apart from some material basis or object. Hiding or ignoring the details.
Dr Benedict R. Gaster will be supervising and the it is part of a new group focused on Parallel Computing.
you could probably argue that a C union is a kind of type-unsafe sum type
[Vishal Agrawal](https://github.com/fugyk) is currently working on "Nix-like" features for Cabal as his [Google Summer of Code project](https://gist.github.com/fugyk/37510958b52589737274).
I think Vincent covers that in this answer: https://github.com/vincenthz/cryptonite/issues/4 So they will be upgraded in time.
Hey, could you consider creating Alpine package for it? https://bugs.alpinelinux.org/issues/4257
I'm not saying that closures prevent immutability, but they also don't _enable_ it, which is what /u/klaxion seemed to be implying by saying you could basically get fragile, stateful programming out of closures. As I (briefly) qualified in my original response, when you have mutable references of any kind, closures obviously don't prevent mutating them. The trick you present of avoiding global state is definitely cool. But my point was just that you definitely can't have shared, mutable state from closures alone. In Haskell, a closure would really only have "state" in terms of what its bindings mean in the lexical scopes nested within it (or things that uses that function, etc). You couldn't make a fresh counter in a global scope and then repeatedly call the methods of that counter from different places in the program, which you could easily do with the Javascript example - and obviously could also do in Haskell with some kind of mutable state (like IORef)... but even then, in the Haskell version you would probably be passing around new versions of closures that have different things bound.... whereas in the JS version you can keep using the original closure and manipulate the single, original enclosed value. In JS, you can "leverage closures to store the state" more elegantly, but try writing the same thing without using ++, assignment, or any other operation that fundamentally modifies state.
Looks very useful, but is there emacs integration?
I'm pretty sure if you can work out a patch for how to do it they'd accept it. At the very least, file an issue with the GHC tracker at least to remind folks that this is an issue and subtly prod it along.
Will do! Both the ticket and investigating a patch, when I get the time.
Agreed. Haskell doesn't have closures at all. The Haskell Report defines the language entirely in terms of denotation semantics, not operational semantics. Compilers may or may not use objects, closures, or whatever under the hood in their implementation. But in most day-to-day Haskell programming you don't need to be aware of those low-level details.
One per Haskell programmer.
Is an algebraic data type a monoid?
I hear a /u/pigworker on the warpath ...
&gt; but even then, in the Haskell version you would probably be passing around new versions of closures that have different things bound.. I don't think so: import Data.IORef data Counter = Counter { get :: IO Int, inc :: IO () } newCounter :: IO Counter newCounter = do ref &lt;- newIORef 0 return Counter { get = readIORef ref, inc = modifyIORef ref (+1) } In ghci: *Main&gt; c &lt;- newCounter *Main&gt; get c 0 *Main&gt; inc c *Main&gt; inc c *Main&gt; get c 2 
I'd say yes, up to isomorphism. data (a :+: b) = Left a | Right b data (a :*: b) = Pair a b `:+:` is associative and commutative (well, up to isomorphism) and has a neutral element of type `Void` which has no habitants (think `data Void` without any constructors). The same goes for `:*:` and `() :: ()` or `unit :: Unit`, the type with only one element. They even have a distributive law (again, up to isomorphism): `a :*: (b :+: c) ~= (a :*: b) :+: (a :*: c)` which is what is done with variant record field accessors à la data Shape = Rectangle { bottomLeft :: (Float, Float) } | Circle { center :: (Float, Float) } | Ellipse { center :: (Float, Float) } Notice that `center` is promoted to a partial function from `Shape` to `(Float, Float)`, which is effectively pulling out the accessors into an outer record type.
Which confused me more than just once when beginning to learn Haskell.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 2. [**Definition**](https://en.wikipedia.org/wiki/Ring_%28mathematics%29#Definition) of article [**Ring %28mathematics%29**](https://en.wikipedia.org/wiki/Ring%20%28mathematics%29): [](#sfw) --- &gt; &gt;A __ring__ is a [set](https://en.wikipedia.org/wiki/Set_(mathematics\)) *R* equipped with [binary operations](https://en.wikipedia.org/wiki/Binary_operation) + and __·__ satisfying the following three sets of axioms, called the __ring axioms__ &gt;1. *R* is an [abelian group](https://en.wikipedia.org/wiki/Abelian_group) under addition, meaning that &gt;&gt; &gt; &gt;* (*a* + *b*) + *c* = *a* + (*b* + *c*) for all *a*, *b*, *c* in *R* (+ is [associative](https://en.wikipedia.org/wiki/Associativity)). &gt;* *a* + *b* = *b* + *a* for all *a*, *b* in *R* (+ is [commutative](https://en.wikipedia.org/wiki/Commutativity)). &gt;* There is an element 0 in *R* such that *a* + 0 = *a* for all *a* in *R* (0 is the [additive identity](https://en.wikipedia.org/wiki/Additive_identity)). &gt;* For each *a* in *R* there exists −*a* in *R* such that *a* + (−*a*) = 0 (−*a* is the [additive inverse](https://en.wikipedia.org/wiki/Additive_inverse) of *a*). &gt;2. *R* is a [monoid](https://en.wikipedia.org/wiki/Monoid) under multiplication, meaning that: &gt;&gt; &gt; &gt;* (*a* ⋅ *b*) ⋅ *c* = *a* ⋅ (*b* ⋅ *c*) for all *a*, *b*, *c* in *R* (⋅ is associative). &gt;* There is an element 1 in R such that *a* ⋅ 1 = *a* and 1 ⋅ *a* = *a* for all *a* in *R* (1 is the [multiplicative identity](https://en.wikipedia.org/wiki/Multiplicative_identity)). &gt;3. Multiplication is [distributive](https://en.wikipedia.org/wiki/Distributive_law) with respect to addition: &gt;&gt; &gt; &gt;* *a* ⋅ (*b* + *c*) = (*a* ⋅ *b*) + (*a* ⋅ *c*) for all *a*, *b*, *c* in *R* (left distributivity). &gt;* (*b* + *c*) ⋅ *a* = (*b* ⋅ *a*) + (*c* ⋅ *a*) for all *a*, *b*, *c* in *R* (right distributivity). &gt; &gt; --- ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+crv99ud) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+crv99ud)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Au contraire! I'm very happy to see this thread.
It's the paradigmatic example of Acronym Denotation Tension.
And it is fixed in pre-release - will be in the next release, later this month.
Haskell has closures but doesn't have mutable state captured in them. So closures aren't used to the same effect that they are in, say, Scheme.
Would android dev be a good use case for Frege? Has anyone tried? https://github.com/Frege/frege
To add to that, _lexical scope_ is also called _static scope_ and is contrasted with _dynamic scope_, where variables (effectively) refer to names in a global dictionary. Consider this (python style) code: def foo(): x = 1 def bar(): print(x) return bar baz = foo() x = 2 baz() In a language with lexical scope, this code would print `1`; in a language with dynamic scope (emacs lisp is the only one that comes to mind), this code would print `2`.
Absolutely, [as I keep saying](http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/), the two major solutions to cabal hell are nix-style management and curated collections. We need both. Yes that's partly because you can't always live inside the cozy world of a collection curated by someone else, sometimes you need bleeding edge rather than stable, or just different things from other people. Or perhaps you're the person trying to make a new collection that works with a about-to-be-released ghc version or whatever. And we can do nix-style management and curated collections either integrated in cabal/hackage or on top. There are people actively pursuing both approaches for both solutions.
That's an interesting idea, optionally making it a strong preference rather than a hard constraint. That's something we can do if we properly integrate the feature into cabal/hackage rather than layering it on top. And similarly, we can produce better error messages in the hard constraint approach, because then the solver knows you're using a collection and so can better explain why something isn't possible.
But that option is only available in `cabal install` command. During dev, I'd like to do `cabal build` without optimizations. But, yes, setting it in `cabal.config` is probably better to have a global option. I hope it can be overridden by CLI. EDIT: so, in your project create an empty file `cabal.config`. Add the line `optimization: False`. This will turn the optimization off by default for all cabal commands. But, if you want to override you can execute either `cabal build --ghc-option=-O2` or `cabal install --enable-optimization=2`. What is good about using `cabal.config` in you project, is that the sandbox will also be installed without optimizations. Unfortunately, for my project, no optimizations cuts down install time of the dependencies in the sandbox only from 4min37s to 4min12s, so it's not worth it. 
&gt; Fundamentally cabal-install has no knowledge that curation is going on. Tooling that understood package sets could present a different UI/UX and help users get the packages they need for their build. I agree. This can, should (and is) being added to cabal/hackage, so that we can present a better UI/UX.
In what situations would you say we *need* curated collections? In other words, can you (somewhat) precisely characterize the situations where curated collections provide something not achievable by one of the other solutions you mention?
nice answer
Ahh, good point. I conflated the two as a more concise nod at some of what "purity" means for the benefit of people who might not know.
Perhaps 'need' is too strong, but there are many situations where users would be happy to live in a world of stable packages that are managed by someone else. Yes it takes flexibility and choice away from them, and it cannot work for every obscure or latest package, but it can work for a big enough set of users enough of the time to be very useful. It's almost exactly the same as the distro model. You use debian stable or testing, and in addition to removing the work of building things, it's removed the work (and choice) of which versions of things to use. That doesn't work for everyone, or for all packages, but it works for many users. What I want is for it to be easy to use collections (including adjusting and combining them), and be easy to define and distribute them, and for the cabal ui to help with that, e.g. in the error cases and in making the configuration more explicit rather than having to be an encoding in terms of other more general things. I should also say that I think having accurate bounds on the upstream packages is still required. I'm not suggesting we move to a model where we only use collections and have no accurate bounds in .cabal files. There are still plenty of cases where collections are not a solution and we need to know the more general bounds and find solutions outside of a curated collection.
From [section 1.2 of the Report](https://www.haskell.org/onlinereport/intro.html#sect1.2): &gt; Although the kernel is not formally specified, it is essentially a slightly sugared variant of the lambda calculus with a straightforward denotational semantics. The report largely specifies identities between Haskell surface syntax and this putative kernel calculus. Which induces a denotational semantics on the language (parameterized on the denotational semantics of said calculus), but it certainly doesn't provide that semantics directly. Also, an operational semantics doesn't prescribe how a language is implemented, only how it behaves. An operational semantics for a language defined in terms of substitution can be implemented via closures or vice-versa.
&gt; That's an interesting idea, optionally making it a strong preference rather than a hard constraint. ...so even with collections that's no free pass for package maintainers to skip adding proper upper bounds and instead relying solely on stackage to provide compatible package versions, as the solver would still be allowed to access package versions outside the collections (albeit with some penalty score)?
&gt; But that option is only available in cabal install command. **and** in `cabal configure` (which afaik is honoured by a subsequent `cabal build`) (`cabal install` = `cabal configure` + `cabal build` + `cabal copy` + `cabal register`)
Conor McBride calls this problem Acronym Denotation Tension (ADT).
I would imagine the part /u/klaxion is interested in isn't the implementation method, but rather that closures give you proper first order functions (formally speaking, [a closed](http://ncatlab.org/nlab/show/closed+category) category). Whether this is implemented by closures or whatever-you're-imagining isn't important, as you say, but the fact that you can have lambdas that can access the outside scope is kinda important.
Oh nifty, I may come back to this later. Thanks :)
No no no, you're thinking of Ambiguous Datatype Terminology (ADT). 
On block structuring, good point and +1. I've learned the basics of Perl a few times and hated it - I don't recall this but it might be something I once knew but repressed. On "does not require nested fuctions" - I know, I never really said otherwise, I probably implied it but only because - as I said - I consider that sense of lexical scope trivial. So formally "lexical scope" means what it actually says and you look for the violations, but no-one ever even cared enough to coin the term AFAICT until there *were* nested functions/procedures. That's where the point of lexical scope arises, and that's why the term on its own is misleading. Of course at this point I'm only continuing this to see if I actually turn out to be wrong about that. 
Yeah, `forall` is enough to model exists, but I think it's easier to write abstract types with `exists`.
Lower bound
Still, it's not 'just' a C union at that point.
You would just upload it as normal and if it compiles/tests it would become part of the new sync-state. When people sync they will pick up the compatibility breaking version and it will be ok because they will also pick up the dependencies that work with it and everything will be consistent. If someone is depending on an older version of your software they will notice they are not part of new sync-states and so will update their software and it will become part of the new sync-states. There could also be a history of sync-states so if you want some old package you could ask for the most recent sync-state containing a working version of that package. Unmaintained software preventing new sync-states because they don't work would eventually be booted out to allow new sync-states. They could then become part of a historical archive.
it's a work in progress; I should get the first port by early next week I think.
But Haskell's semicolon is vestigial anyway, who would miss it?
I'm quite sure you just described Stackage. Replace "sync-state" with ["snapshot"](https://www.stackage.org/snapshots). What differs is the granularity, i.e. Stackage produces a snapshot nightly because it takes a few hours to compile and run tests. Doing that for every package upload would require much more computing power. In any case the only things that end up in a snapshot are packages known to compile and pass tests together. Also typically uploading a package breaks *other* packages. E.g. this [doctest upload](https://github.com/fpco/stackage/issues/622) broke the listed packages in the sense that their upper bounds are not satisfied. Each author will now review whether they can indeed bump their dependency and go ahead. Once done, a full build (as opposed to just a version constraint check) will kick off.
Pretty cool, well done!
Good to know. I pretty much splice my way through everything.
Isn't pattern matching just sugar for `case` anyway? Why can't you use `\case`?
&gt; For example, the expression. . . &gt; &gt; let y = 42 in f !(g !(print y) !x) &gt; &gt; . . . is lifted to: &gt; &gt; let y = 42 in do y' &lt;- print y &gt; x' &lt;- x &gt; g' &lt;- g y' x' &gt; f g' I want that so bad! 
Can't you add `_ -&gt; fail` to the end of your cases?
Of course you can, but that clutters my code with the *uninteresting* cases. The point of syntactic sugar is to focus on the business logic and hide the boilerplate.
You can almost do this by defining: (!) = (=&lt;&lt;) ... but it doesn't work for functions of multiple arguments
Other issues aside.. Some of the compilation pain could be eaved by only type checking things and not actually doing any code gen... Of course that means no tests could be run.
I've never known that a pattern match failure inside a `do` wouldn't crash, but rather be accommodated in the monad!.. Is that really so?
if mess in terminology is your hobby, you might like these: * [Against the definition of types, Tomas Petricek](https://www.reddit.com/r/haskell/duplicates/35zzvu/against_the_definition_of_types_by_tomas_petricek/) * [Words Matter, Robert Harper](https://existentialtype.wordpress.com/2012/02/01/words-matter/) 
Not sure what you mean; I think my Haskell implements exactly the semantics of the Javascript above...
in some monads, that accommodation is crashing :(
Were there any particular reasons why you chose DigitalOcean for hosting? (Currently I'm looking for hosting services, and I'd like to hear your opinion on DigitalOcean's services.)
Not nearly as pretty but.... {-# LANGUAGE NoMonomorphismRestriction #-} import Control.Applicative import Control.Monad (%) = (join .) . (&lt;*&gt;) (?) = (&lt;$&gt;) (!) = (=&lt;&lt;) (#) = (&lt;*&gt;) inc = return . (+ 3) a = return 3 gar a b c d = print $ a + b + c + d infixr 6 ! infixl 7 % infixl 8 ?,# *&gt; print ! gar ? a # (inc ! a) # a % a 15 () Some type classing magic might let this work with only a single operator, but that implementation is left as a thought exercise for the reader.
I think both Parsec and AttoParsec expose this information, you could take a look at how they do it.
You can use the [Reader Monad](http://stackoverflow.com/questions/14178889/reader-monad-purpose) for that purpose.
I'm wondering about the overarching abstraction, but maybe you're right, I'll take a look at how they do it!
Only partly related, but I had an eye-opening lens / compiler moment when I had read the [optimizer code for GHCJS](https://github.com/ghcjs/ghcjs/blob/master/src/Gen2/Optimizer.hs). I really like the idea of stating the common traversals up front, as first class values. It's possible that doing that kind of thing was already obvious to everyone else :)
I did deployment manually. I pretty much followed the README I created in my Github repo, except for one small change. To run in 'production', I used upstart/initctl on Ubuntu following [this SO answer](http://stackoverflow.com/a/11304866). If you want your process to be automatically reloaded, I think you can add `respawn` to the configuration script. Since I was running on a droplet with 512mb RAM, I had to create a temporary swap file, described in [this SO answer](http://stackoverflow.com/a/28207691), in order to install cabal and the dependencies. I currently have no health checks in place for my website. It won't automatically respawn, either. I haven't noticed any performance problems (yet). I get about ~400 visitors per day, so I don't see performance being an issue just yet. I'm starting small and scaling as necessary. I hope that answered some questions, but if you want to know something specific, I'll try to answer it.
DigitalOcean is the most straightforward, simplest hosting provider that I've found. You can spin up a new instance (droplet) really quickly. Also, after attending some hackathons in college, I was gifted with DO credit, which made it really easy to choose DO. I've also used Amazon AWS, Heroku, and Google App Engine. None of them are nearly as easy to use as DigitalOcean for what I want to do. I've never had any problems with DigitalOcean, nor have I felt limited, but I have also never hosted a high-traffic application. I'd say just give it a trial run if you can.
dalaing was probably referring to the first-class traversals in complex structures. subStats :: Traversal' JStat JStat subStats f (IfStat e s1 s2) = IfStat e &lt;$&gt; f s1 &lt;*&gt; f s2 subStats f (WhileStat b e s) = WhileStat b e &lt;$&gt; f s ... subStats _ s = pure s And their subsequent reuse localVars = universeOf subStats &gt;=&gt; getLocals removeDeadVars' s = transformOf subStats (removeDead dead) s And chaining removeDeadVars s = s &amp; thisFunction . nestedFuns . _2 %~ removeDeadVars'
I think that `matrix` site needs a front page description...
Very interesting!
It is an improvement, but being totally hands off other people's code is not going to work. Other packages might simply be buggy in how they use your library and you need to be able to patch them. Introducing some invariant checking is good, but there is also a social component around fixing broken code and being able to do large scale refactoring on Hackage.
That does not mean it is wrong. What is needed is conceptually a single repository of code where continuous integration happens, and where large scale, atomic refactoring happens. The lack of large scale refactoring because package distribution is in a sorry state does not mean that continuous integration is a bad idea!
I'll just note that a few hours of computer power on Amazon's most high end machine costs about 60 seconds of ekmett's time. 
See my other comment. One full stackage build &lt;= 1 minute of high paid consulting time. For an incremental build we are taking about seconds. Should we put the alternative cost of Haskell programmers' time at $1 USD per hour? It makes complete sense for any commercial enterprise with only a single employee to sponsor this.
I'd be interested.
Yes. That's why there is a `fail` method in `Monad` currently. Look up "desugaring monad do notation" for more.
A lot of the code we have in the rebuilt compiler for Ermine we've been hacking on is lens-based: https://github.com/ermine-language/ermine https://github.com/ermine-language/ermine/blob/master/src/Ermine/Syntax/Kind.hs#L247 https://github.com/ermine-language/ermine/blob/master/src/Ermine/Syntax.hs etc. We aren't currently using them in our core optimizer, though: https://github.com/ermine-language/ermine/blob/master/src/Ermine/Core/Optimizer.hs
Really interesting! Is there a way to download the video?
It can't all happen automatically. That would assume that everyone, in every package, has 100% automated test coverage, not just of their code but also of all required and all potentially undesirable behaviour.
You would probably have better luck getting people to look at it with a git (or other VCS from that generation at least) mirror of your code.
I'm guessing that when they say &gt; delivered it defect-free into a production environment where it has proven robust they mean that unlike previous projects, they've delivered it with no _known_ flaws, and they're mentioning it because they're more used to _not_ meeting all design goals in version 1. 
&gt; When people sync they will pick up the compatibility breaking version and it will be ok because they will also pick up the dependencies that work with it and everything will be consistent. No they won't, not unless the authors of the dependencies between your package and their package all updated their packages already. It would essentially block anyone from uploading a new version of their package that still depends on the old version of your package and intermediate dependencies.
Dammit - I knew there was gold in those hills, but I was answering during lunch and only checked the optimizer for examples. 
&gt; code may ultimately "crash" Not at all! `fail` just means producing a suitable value in many monads. In `Maybe` that is `Nothing`, in `[]` (list monad) it is `[]` (the empty list). Generally when you are in a `MonadPlus` the "desired" way of failure is `mzero`, because that value won't contribute to the "flattened" result. 
This is awesome! I'm learning Haskell right now and I'm also writing a simple web app with Snap for the first step (though one which I won't put on the internet, just for personal use. Source will be on github, of course)! Thanks for posting this, it encourages me to go on! :-) If you have any tricks or something, please tell me! I'm currently struggeling with how Heist works and how to integrate it properly.
Do not do this in the `Id` monad! It is not a `MonadPlus`, so `fail` will be `error`. Why not use the `let (Just x) = x'` trick you have just described? Please note that here probably you won't ever see a warning because `case` also lets you write partial matches.
Let me tell you, it's not just a "few reverse dependencies" we're talking about... As I'm working on http://matrix.hackage.haskell.org/ I'm currently facing to compute all install-plans which are affected by a new addition (or even just a `.cabal` revision) injected into the Hackage package pool, and that's far from trivial (even though it may appear so, if you have something like http://packdeps.haskellers.com/reverse/ in mind, which is a very oversimplified reverse-dependency report). And a very significant amount of cpu time actually goes into re-solving the install-plans to decide whether a rebuild is actually needed.
Yep, that makes sense. I was just poking a bit at the old 'we have no bugs' as opposed to the more correct 'no known bugs' :)
ooo. No time to look at this in detail now, but this is very relevant to my interests.
I have skimmed only over the README, but it seems like it was a great deal of fun to build, as well as I well-thought piece of software. Nice stuff!
&gt; fail just means producing a suitable value in many monads. I'd strongly argue against that point. The following do not have and can not have a sensible `fail` implementation: - IO - ST s - Identity - Reader r - Writer w - State s - Either e STM *could* have a somewhat well-behaved `fail`, but it does not have it right now. These can fail gracefully: - Maybe - List - Parsers Point is, when you're in code that's polymorphic over the Monad instance, you must not use `fail`.
I don't see this pattern very often, and even if, I don't like the sugar. It makes the first line of do-notation behave in a special way, looking like a simple statement, when it is actually a binding statement. Since the rest of the potentially large do-block is going to be business as usual, this introduces an awkward special case. (PS: Is that you, Gabor?)
Absolutely! I enjoy your posts on it :-) Edit: blog would be great too!
&gt; We were not focused on writing “beautiful code” or interested in solving challenges that weren’t real to our business, we wanted to build something that could make us money as fast as possible. This is what I like to hear. This is what makes businesses choose Haskell. EDIT: &gt; I changed *one* line of code from &gt; `map outputParticipant parts` &gt; to &gt; `map outputParticipant parts `using` parListChunk 10 rdeepseq` [my thoughts (kind of NSFW)](http://www.reactiongifs.us/wp-content/uploads/2014/03/stop_penis_erect_archer.gif)
Sounds a lot like how Erlang shapes things into programming for what is expected and subsequent clarity.
"Move fast and don't break stuff"?
I wouldn't put them together like that. Your type parameter is now neither co- nor contravariant and it's unlikley you'll be able to do anything useful with it. If you write newtype Codec a b = Codec (Decoder a, Coder b) then you have what I call a [`ProductProfunctor`](http://hackage.haskell.org/package/product-profunctors-0.6.1/docs/Data-Profunctor-Product.html) though /u/edwardkmett may have a different name for the same thing. Your type parameters can vary independently of each other, but in general you will write code that consumes or emits a `Codec a a` where the parameters match. [EDIT: Corrected username, thanks /u/rpglover64]
With a little trick you can compose both with `Applicative`, with the use of a helper function for the decomposition (division). I use that for the `Applicative` instance in `fclabels`[0] and for a toy `pickler`[1] (parsing+printing or coding/decoding) library on top of that. Your code would look a bit like this: data User { name, address :: String, age :: Integer } userCodec :: Codec User userCodec = Codec $ User &lt;$&gt; name &gt;- nameCodec &lt;*&gt; address &gt;- addressCodec &lt;*&gt; age &gt;- ageCodec nameCodec, addressCodec :: Codec String ageCodec :: Codec Integer The idea is to have an `Applicative` instance for a possibly diverging type `CodecD a b` and a wrapper type `Codec a = CodecD a a` to close the type again for prettier composition. data CodecD a b = { code :: a -&gt; ByteString , decode :: ByteString -&gt; b } instance Functor (CodedD a b) ... instance Applicative (CodedD a b) ... data Codec a = CodecD a a infix 5 &gt;- (&gt;-) :: (b -&gt; a) -&gt; Codec a -&gt; CodecD a b A bit like what /u/tomejaguar explains. [0] https://github.com/sebastiaanvisser/fclabels/blob/master/src/Data/Label/Poly.hs [1] https://github.com/sebastiaanvisser/pickler/blob/master/src/Pickler.hs
Are funcs A through Y used by other functions? If not, you might want to include them inside the scope of funcZ in a where or let clause, then they will simply have access to firstArg without being passed it, and it will keep your top level namespace nice and clean.
the link is http://player.vimeo.com/external/129301223.hd.mp4?s=7d047e5e32eb41d71cfad82086ee188f
This is a nice approach. I'm using "product profunctors" a lot in Opaleye, but the combination of Applicative plus `&gt;-` operator provides perhaps a more pleasant interface. &gt; data Codec a = CodecD a b I guess you mean `data Codec a = CodecD a a`, as you wrote elsewhere.
Yes thanks, I wish reddit would type check my pseudo code. :)
You mean "/u/edwardkmett". I hear that if you say his username in a thread three times in pitch darkness during the witching hour, he appears and lensifies your current project, but curses you to never understand the `lens` library.
Parallel map is really the dankest shit ever. 
Strongly agree. (assuming the condition you stated)
Shouldn't it be `return (f !a !b !c)` if `f` is a pure function?
But the Haskell version makes the statefulness explicit through the use of IORef (i.e., you are confined to the IO monad), which is arguably better than implicit mutable state of the JS example.
Thanks! This is quite interesting!
&gt; given the adoration for types, I always feel like I'm offending someone when I talk about it No, not at all. All functional programming stems from LC. Have a look at how many modern FP techniques are in this [awesome LC/Church encoding book from 1975](https://books.google.co.uk/books/about/Recursive_programming_techniques.html?id=81kZAQAAIAAJ&amp;hl=en). Whoever has been telling you it's irrelevant doesn't know what they are talking about. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [LambdaCube 3D - purely functional graphics programming language \[x-post from /r/haskell\]](https://np.reddit.com/r/programming/comments/38oysu/lambdacube_3d_purely_functional_graphics/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot)
Probably.
I don't see how this is relevant, though. `breaks-dependencies` will never *cause* a break that doesn't already exist; it only mends false positives (and perhaps rare false negatives) based on different interpretations of (or simple failure to use) the PVP. The *worst case* is identical to the current environment.
Fun fact: I built [build.elastic.co](http://build.elastic.co) which is written in Haskell using [Spock](http://www.spock.li/).
&gt; a) why it is that way and b) why it's important. a) I usually make an example of a `User` model that had an `age` property. This being an obvious mistake we refactored it to `birthdate`. After doing so my compiler (and sometimes the editor/IDE) can help me by guaranteed exhaustingly pointing me where the remaining occurrences of `age` are. In dynamic languages I need use multi-file find for this, without guarantees. (In Haskell I still use multi-file find for occurrences in the documentation I might have overseen). I proceed by explaining how this stretches to the browser side with something like GHCJS. b) Importance is mainly understood by making a business case. Show 'm the money; or in this case, the stacks of money one does **not** need for making refactorings/testing/headachepills/sleepingpills.
&gt; Significant version changes are also frequently used to indicate major new features Those major new features also frequently include breaking changes. So that case is fine as things are. My experience is that this situation you are trying to address is rather rare. It does seem to have happened in the case of text, but that's just one example. Yes, it caused a lot of pain, but once people figure it out they bump their version bounds and the problem is behind us. I'm sure /u/bos will be more careful in the future to make sure he's not needlessly doing major version bumps.
It is relevant because major effort and complexity for fixing a rather small percentage of the cases might not be worth it.
We've tended to use direct HTTP through wreq for the small things we have, but we're not opposed to Bloodhound at all. In fact, because it exists we haven't seen much priority in creating our own. As we build out more stuff on Haskell we'll almost certainly use it and contribute. We need more people though.
Do you think the deeper points (not strictly about a client library) regarding executable/testable/readable specifications will go anywhere? Not having a spec for the Elasticsearch API makes writing a client library for it feel very precarious. As an example (I made this point in the GH issue, I'll repeat it here), there is a *lot* more I could do to enforce type-safety. For example, the document type / request type / reply type are all totally uncoupled and it's up to you to assert what you expect to get back. Ditto schemas &lt;---&gt; queries. I haven't unified it under a GADT/fundep MPTC because I'm worried about API churn from Elasticsearch and because I don't have a concise and precise specification telling me what I can expect, so I'd have to develop all of it empirically and eat the cost/risk of the increased surface area with no guarantees about API component stability. Now, I don't think this significantly tarnishes the utility of Bloodhound. The original problem was type-safe query construction and composition – that problem now has a reasonable answer where I do not think one previously existed but it burns my tail to ignore so many opportunities for improved type-safety because I don't know what ES is going to do now or in the future. That I no longer use Elasticsearch in my day-to-day means I don't even have the benefit of practical work driving me to find bugs/problems, so I have to sit on it and wait for my commercial users of the library to tell me what they want.
(2^8 - 8) / 2^8 = 0.96875
Well data transformations (and validation) with free monads give you a lot of help from the type system. In fact its what I'm doing at my day job right now.
Hey, cool! I'll be at the FP AMS meetup.
Thanks! It was a great deal of fun to build. It's definitely a thrill when you first receive someone's voice - much like when you first build an analog radio.
For a) wouldn't most (any?) other statically compiled language (or at least the ones I'm familiar with, such as C++, Java) find such an error? This specific example does not seem like motivation to choose Haskell over most other statically typed languages. Now if `age` was an `int`, and you changed `age` to `daysEmployed`, also an `int`, then you'd wrap it in a `newtype` to make sure you fixed up all use sites to handle the new meaning. The capabilities afforded by `newtype` is something that sets Haskell apart from the previous mentioned alternatives.
&gt;the person I responded to prefaced their comment by stating they were a hobbyist, which I took to mean these were primarily hobbyist issues, especially since they were the only ones upset. Being a hobbyist doesn't mean their concerns only apply to hobbyists. And what makes you think they are the only person "upset"? &gt;If they're making a move that's bad for time locked hobbyists "If they are making a move that is bad for people with hair...". Look at the actual arguments put forth, not a casual mention of the fact that they are a hobbyist. How is having to learn a whole not "almost Haskell but not really Haskell and missing a bunch of stuff" not a downside for a professional? &gt;this person doesn't seem concerned with what's best for the platform, but what's best for their use of the platform. I don't see anything like that. Nothing in their post suggests that. Obviously their motivation is to their own use of the software, everyone's is. But you seem to be assuming that everyone else on the planet must be in complete disagreement for some reason. All the downsides listed apply to anyone considering using it. I do not want to learn a new language that is missing all sorts of features, tools, etc and can't interoperate cleanly with any other language, just so that the syntax can subjectively be a little nicer.
You're possibly going to lose a lot of auto deriving when using existentials anyway :) But as a trick with GADTs, often `... deriving (...)` clauses fail where `-XStandaloneDeriving` will succeed. I haven't an honest clue why.
Yes, that's what I've been doing. However, in the JSON case, it's the Aeson.Th template haskell derivation that's failing. I guess, there's no other option then manually fix the generated code.
Can someone tell me what this library is about? What is software defined radio? When would I want to use something like this...or does it have any other application? I read the readme on the Github page but couldn't understand much. 
It's not 'some additional time'. It's objectively less useful as a general purpose tool with this approach.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Software-defined radio**](https://en.wikipedia.org/wiki/Software-defined%20radio): [](#sfw) --- &gt; &gt;__Software-defined radio__ (__SDR__) is a [radio](https://en.wikipedia.org/wiki/Radio) [communication](https://en.wikipedia.org/wiki/Telecommunications) system where components that have been typically implemented in hardware (e.g. [mixers](https://en.wikipedia.org/wiki/Frequency_mixer), [filters](https://en.wikipedia.org/wiki/Filter_(signal_processing\)), [amplifiers](https://en.wikipedia.org/wiki/Amplifier), [modulators](https://en.wikipedia.org/wiki/Modulator)/[demodulators](https://en.wikipedia.org/wiki/Demodulator), [detectors](https://en.wikipedia.org/wiki/Detector_(radio\)), etc.) are instead implemented by means of software on a personal computer or [embedded system](https://en.wikipedia.org/wiki/Embedded_system). While the concept of SDR is not new, the rapidly evolving capabilities of digital electronics render practical many processes which used to be only theoretically possible. &gt;==== &gt;[**Image**](https://i.imgur.com/doSl23A.png) [^(i)](https://commons.wikimedia.org/wiki/File:SDR_et_WF.svg) --- ^Interesting: [^Tucson ^Amateur ^Packet ^Radio](https://en.wikipedia.org/wiki/Tucson_Amateur_Packet_Radio) ^| [^3.5G](https://en.wikipedia.org/wiki/3.5G) ^| [^GNU ^Radio](https://en.wikipedia.org/wiki/GNU_Radio) ^| [^Demodulation](https://en.wikipedia.org/wiki/Demodulation) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+crxd6wf) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+crxd6wf)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Hi, I am one of the authors. As I see it is possible that LambdaCube 3D can be fully Haskell98 compatible at some point, so some of the cons could be eliminated. I updated the homepage according to this. Note that we don't want to be GHC compatible which means that the evaluation order can be different (Haskell98 does not fix the evaluation order) and .hi files cannot read by the LambdaCube compiler.
really impressive.
C has newtypes. struct newtype { int contents; }
We’ve lived with the EDSL for years, and believe me, we’ve spent a long time thinking about the issues. The bullet points on the website are admittedly kind of rushed, and they don’t tell the whole story. For us, moving towards an independent language has very important benefits: 1. The language is not tied to Haskell any more. It can be used in combination with any other language. This is an absolute must. 2. We are not limited by Haskell’s type system, so we can create our own that matches the domain much better. With the EDSL we had to jump through elaborate hoops to be able to capture all the constraints we wanted to, and the error messages tended to be quite dreadful as a consequence (I personally got C++ template flashbacks sometimes). We don’t want a compromise where we sacrifice either domain modelling or usability, and a separate language allows us to reach a much sweeter spot. 3. We can get much better compilation performance, because we can inspect assignments and don’t have to do silly things like recover sharing. Speedy compilation absolutely matters to us (it’s really non-negotiable if we are to compete with existing graphics programming tools), and that’s just not going to happen with an EDSL compiled with GHC. 4. 3rd party libraries cannot be used in shader code anyway, so the loss of code reuse opportunities is not as big as it seems.
Good point. I was showing the difference between Haskell and dynamically typed langs. C++/Java do have types, but are also more verbose and often a little more low-level then Haskell and the dynamic langs. You've show already how to change the example, and /u/kamatsu has provided a C++ counter example. I'd change the example into something that uses `Maybe` instead of `null` and of a `-1` value to signify absence of a value. This is an easy to grok example to show clearly how a better type system catches some bugs. Next up would be to explain that in Haskell it is possible reduce the kind of operations to only those that make sense as the type system can limit the possible side-effects.
Your code (although not completely clear for me, because you seem to have two different `foo`) reminded me of a technique I've seen in some of the papers by Oleg Kiselyov and Ken Shan employed to write down "linguistic theories" (some formalization of a grammatical combinatory machinery and a lexicon). That technique/style was really a delightful discovery for me. Being also a theoretical linguist, I have always been unsatisfied with my attempts to express grammatical theories in a language like Haskell. What were my perceived problems?.. (Simplified.) The meaning of lexicon items (words, morphemes) resemble functions. So I'm inclined to define them as functions (by the usual equations like `f x = expr`). But that doesn't capture that the lexicon is thought of as a closed set. (At least, think about the set of morphemes of a language.) I want guarantees that wherever I use a word (or a morpheme) in my code, I use one from the fixed set (and not an arbitrary function). This leads to another design: let the words/morphemes be constructors from an "enum" type representing the lexicon (like `data Lexeme = Wolf | Fox | Big | Small | Go | Me`), and write an `interpret :: Lexeme -&gt; Meaning` function to assign meanings to them. Another related issue with "words as functions" was that apart from the meanings a word (as any sign) must be assigned other values like the pronunciation/spelling etc. This is solved by the data type approach. But the data type approach has its drawbacks. The words should be typed (the types reflecting their combinatory syntactic properties). And the meanings have different types (the same reason), so we can't write an `interpret` function really, such that it would give meanings of different types. Ok, so we should go for GADTs insetad of a simple data type with nullary constructors then. But somehow I didn't like GADTs much (I can't remember now why; perhaps, I never really had a need to implement this). Something that never came to my mind was the design I saw in a paper by Kiselyov and Shan(?). Represent the lexicon as a class, its methods being the words. So, the types of each of the words are fixed. Now, different instances implement different "sides" of the signs (meanings, spellings), or perhaps their interpretations in different grammatical frameworks (different theories). (In the actual code, the class must be heavily parameterized, of course, by the functor-like types of the interpretation framework.) Your suggestion seems to bring together the "lexicon as a class" and "lexicon as a data type" . I have no idea yet whether this could be nice for the things I'm thinking about.
I used to wonder about this as well. I will give you my own conclusions. First of all, though, you have to understand that people use these words in different ways. If you're a category theorist, then "algebra" has a very unambiguous meaning. But in a particular programming language, even one with sound foundations, it could be used very loosely. That said: An **algebra** is a type `X` together with a function `f: F X -&gt; X`, where `F` is a(n endo)functor. The meaning of **datatype** varies depending on the speaker; often it means that `F` is polynomial, which itself usually means that it can be expressed as a sum(-of-products) `F1 X + ... + Fn X` of its input. In that case, `f` can be decomposed into a set of functions `Fi X -&gt; X` called operators (or sometimes operations). It also usually implies that `(X,f)` is initial or, equivalently, a least fixed point of `F`. That implies that `f` is invertible -- which is why the operators behave as **constructors** and you can decompose values of `X` --, and that there is a unique function `X -&gt; Y`for every F-algebra `(g,Y)`, which in FP is usually called a **fold**. As I said, though, depending on context and how loosely you use this term, most or all of these conditions might not be present. For example, I think Rust has something they called algebraic datatypes, but I doubt they are guaranteed to satisfy most of these properties. **Abstract data( )type** is a much broader, catch-all term which originated and found use among programmers and researchers long before the idea of regarding types as algebras caught on. The idea of a **datatype** in this sense is just that you have some type `X` and a bunch of procedures involving `X`. The most significant difference, IMO, is that, whereas in an algebraic datatype the operators all have type `... -&gt; X`, in an abstract datatype, you might have procedures of any type whatsoever, though usually they will involve `X`. Put differently, operators of an algebraic datatype always construct, or at least return, an `X`, while in an abstract datatype they might observe an `X` (as with coalgebraic operators) or transform an `X`. Conversely, if your language doesn't have general recursion, an algebraic datatype has (usually) many constructors but exactly one deconstructor (the fold). In an abstract datatype, there could be many constructors and many deconstructors/observers, and they don't have to satisfy any particular laws. The other major component of an abstract data type, of course, is the "abstract". The idea there is that clients of the type see only a type `X` and some operations that manipulate values of type `X`, but the type is actually implemented by some concrete type `Y`, usually called the representation type, and procedures operating on `Y`. Clients either don't know what `Y` is, or they cannot use the operations of `Y` on values of `X`. (`X` and `Y` could be the same, though; a typical example is cartesian and polar coordinates.) The language mechanisms for getting all this done vary from language to language and are usually very *ad hoc*. You can regard an algebraic datatype as a particular sort of abstract data type where the compiler picks the representation type and implementation. This is a bit subtle, but: from this perspective, the representation type of an algebraic datatype is a fixpoint of `F`, but it doesn't necessarily have to be the fixpoint named in the code. For example, if I write: data Nat = Z | S Nat then `Nat` is certainly a fixpoint of the functor in question (given by `F X = () + X`), but, viewed as an abstract data type, the representation could be any initial fixpoint, for example `[()]`, lists of units. Seen this way, a Haskell datatype is actually a signature or interface which can be assigned a default implementation. One last thing: in my mind, the distinction between a "type" and a "datatype" is that "data" signals initial semantics, so: constructors and fold. More generally, it signals for me the existence of a universal mapping property (in particular, `(A -&gt;)` is a datatype to me), but that is another topic. And, again, probably many people will disagree with my interpretation.
You already have a job. Writing Haskell. Don't be greedy! :-P Looking at the function below that uses ($!!), maybe they forgot a strictness annotation there? If that was the case, the explicit map would make more sense. On the other hand, you can just create a pipe that (WH)NFs everything flowing downstream as well, which is more compositional.
&gt;There's literally zero benefits to anyone Nobody said that. It can be less useful as a general purpose tool, while still having benefits to some people. For example, it would appear that the authors do not wish to use Haskell, so for them it is a benefit that it is not a Haskell module any more. It is easier to use lambdacube3d from C++ if it is not Haskell. But for people on /r/Haskell their interest is likely making 3d applications using Haskell, so this is a step backwards.
Because Google Translate doesn't have a button that lets you download the audio file. You have to know the URL to use.
The concise, purely-functional description of the rendering pipeline -- as seen in the aforementioned online editor -- is mind-blowing!
User is the one model I don't do that for because i don't want to be slinging the password (even if it is encrypted) and other user data around for no reason. In general the approach that I take is to have a custom To/From JSON instance possibly of a newtype wrapper. But the new record approach is valid to.
This is awesome, I'm also working on a servant+persistent-postgresql project. Still trying to figure out a good way to abstract the authentication. Been following their discussion here https://github.com/haskell-servant/servant/issues/70
Ah, that is fantastic! Thanks for pointing it out :D
I have a job writing Haskell? That's news to me!
There you go then. I was waiting for it to go in to add a bunch of prism-based patterns to lens. I suppose it is time to think about their inclusion.
Ah, I see what you mean. Also, in the Idris prelude, `return` is just an alias for `pure`, so `return (f !a !b !c)` should indeed be the same!
Commenting your code may help people understand your intentions, especially when identifiers belong to a domain, but are not specific enough to depict a concept.
 concept :: Num a =&gt; E a -&gt; a -&gt; a concept (E vals) x = sum$ map(\(v,e) -&gt;v*x^e) $ zip vals [0..]
I'm quit curious on answers about this too. I love the concept of extensible effects, but without the ability to nest "safe effectful" computations like writers and readers within in an existing effect stack like one can do with a monad transformer stack I have a hard time wanting to leverage it.
He's not. 
Write something about it. Seems like an equation equality checker? What can and can't it do? Learned anything interesting while making it? 
I think this would be a really nice experiment to try. It's a bit orthogonal to what I'm doing which is trying to get natively compiled Haskell code to run on Android. But I think it would be a cool thing to try!
Yes it is an equation equality checker! I feel like it is hard to write about it. It is not able to do div(/). I like the way it works. You can even use more vars like this x :: E (E Integer) x = malX 1 y :: E (E Integer) y = E [malX 1]
My opinion (sorry but I'm posting on here because I don't like Haskell cafe) +1 for a shorter syntax -1 for it by default (just activate it in your cabal file) -1 for the proposed syntax The problem with `import Data.Map(Map) as M` is everybody understand it as : import Data.Map.Map as M, you get M.Map in scope and nothing else. What about `import Data.Map as M and [qualified] (Map)`?
Why would one disable a change that doesn't break backwards compatibilty by default? Only because haskell 2010 doesn't contain it?
Because it becomes specific to GHC (and a certain version of it) and it's not written anywhere. Give a such file to someone which doesn't use GHC (or an older version) and he'll get the syntax error without explanation or clue how to solve it. If you add a language pragma, at least he should get an nice error message, "this pragma doesn't exist or is not implemented ...." which basically says, get the latest GHC version ;-). 
&gt; ... packages which allow to leverage the type system for effectful programming purposes? This is pretty vague. Do you mean paclages that encode effect restriction (extensibe-effects), or like libraries that encode effect interleaving (pipes/conduit)?
I know you can do some magic with monad coproducts as per [this talk](http://functionaltalks.org/2014/11/23/runar-oli-bjarnason-free-monad/), but there are some caveats with monads that aren't distributive. (Exactly what concrete caveats this implies is lost on me.)
Wait, weren't those pragmas introduced for the very purpose of making it possible to reference syntax extensions relative to the Haskell Report? Or put differently, those language pragmas are what [make Haskell truly modular](https://www.youtube.com/watch?v=MoxqBGaTmWQ)... 
M...mix Fix?
Has there been any work on a simple, closer to first-class, import syntax (that doesn't go as far as a full-fledged module system)?
Here, the ultimate argument for having a proper macro system exposed to the user: http://www.reddit.com/r/haskell/comments/38oy59/lambdacube_3d_status_update/crxskm8 
It is pretty much the syntax the thread started with. I was referring to not liking the `andAs` stuff that started later.
Here is a linear solution: import Data.Ratio hhs n = getsum (iterate hhs' (0, 1, 0, 1) !! (n-1)) % 2^n where hhs' (a, b, c, d) = (a+b+c, d, a+c,b+d) getsum (a,_,c,_) = a+c -- hhs 8 =&gt; 201 % 256
This looks nice, but is `*::types` something that already exists or just a new syntactical construct to import all types? If it's new, I'm unsure if that would be needed at all, I think manually listing the types wouldn't be a problem in (almost?) all of the use cases. Why `import Some.Module (), * as FC` though? That is definitely harder to read and write than `import qualified Some.Module as FC`. Really what I'd just want to see is being able to leave out the "qualified"; why do we need it anyway?
I really don't see how having two ways to write something is somehow less confusing to beginners than having a single way. I also don't see how you could fit that proposed syntax onto a single reasonably short line for anything slightly more complex than a single symbol in the import list.
I would love to have Agda style modules in Haskell as well. Nested modules would also be useful. But parameterized modules make little sense in Haskell. One thing that Haskell does better than Agda is allowing multiple modules with the same name. So you can write import qualified Data.Map as M import qualified MyExtraMapUtils as M
Try it Pow-style. data ICon (shapeGives :: (ix -&gt; *) -&gt; (ox -&gt; *)) (s :: ix -&gt; *) (o :: ox) = forall pos. shapeGives pos o :&lt;: (forall (i :: ix). pos i -&gt; s i)
&gt; I also don't see how you could fit that proposed syntax onto a single reasonably short line for anything slightly more complex than a single symbol in the import list. For me that's what this boils down to. It's not actually shorter when you're importing many things. I also find it confusing.
Maybe we could have the convention of adding a `type T = ...` synonyms to modules which expose only one central type. On the other hand, what do we do for modules which expose more than one type? Also, how would you keep apart `Data.ByteString.T` and `Data.ByteString.Lazy.T`? What module aliases would you chose (as `B.T` and `BL.T` are probably no sensible typenames)?
&gt; Reading ∃ as summation :o :O :◯
You're right, calling curried functions with one argument at a time is not so cheap. This is why GHC generates special code when a function is applied to all the argument it needs (such a function application is called saturated application). See https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution/FunctionCalls
That is a good idea, using `type *` instead of `*::types` would work, and be backwards compatible since that isn't valid syntax atm.
Haskell has good type inference. Languages with dependent types historically do not. We've slowly been extending Haskell towards the world of dependent types while trying to retain that property.
Dependent types mean totality. And total languages are highly impractical. Turing completeness, not having to prove termination of anything, not having to use unary numerics - all of those are benefits of independent types. Edit: type inference, too. All in all, Haskell is a great middle ground between research wackiness and getting stuff done. Thank goodness it isn't dependently typed!
&gt; Haskell has good type inference. Languages with dependent types historically do not. True, but when you're programming in Agda/Idris, do you really miss it? The ability for the compiler to fill in holes is, in my opinion, exponentially more useful than type inference. I sometimes even fantasize about a world where I don't call functions by name, but rather by expressing a type signature. People think this sounds crazy -- how would the compiler know which implementation to pick? How should it know that I want to use + and not *? To which I reply: the same way I know I wanted to use + and not *. There is a distinction, because apparently the difference is important to me; and if the difference is important to me, why not explain to the compiler why it's important? Ok so I think I just came out of the closet as a crazy person. But that is how I think programming should be done.
-1 My sentiments are well expressed by the three posts of Malcolm Wallace on the café thread: [here](https://mail.haskell.org/pipermail/haskell-cafe/2015-June/119983.html), [here](https://mail.haskell.org/pipermail/haskell-cafe/2015-June/120016.html), and [here](https://mail.haskell.org/pipermail/haskell-cafe/2015-June/119985.html).
Of course. I was referring to the basic approach, not to the question of what formal semantics have been explicitly specified. The bottom line is that I agree with /u/tel that closures are an implementation technique. You can implement their functionality in a number of different ways in Haskell, depending on the context and how you define your requirements. And Haskell compilers may or may not use some technique internally which you might identify as being a "closure". But it doesn't make much sense to ask whether Haskell "has" closures or "uses" closures as an inherent part of the language.
False in almost all respects. There are non-total dependently typed languages (and most of the total ones let you cheat). A language can be total and Turing complete: modern dependently typed languages are. It is possible to do binary arithmetic (and even use machine arithmetic, if you trust it) in a language with dependent types: Idris does this. Oh, and you seem to think Haskell isn't dependently typed. It's not "full-spectrum" dependently typed, but the key hallmark of being able to "learn by testing", doing a case split which refines the return type differently in different branches, has been with us for quite some time. The possibility but inconvenience of the singleton type construction, allowing values to cross the ::, is something we can mitigate by doing a better job of being dependently typed.
Please expound.
Certainly. Here are [45 minutes of your life you won't get back](https://www.youtube.com/watch?v=ad4BVmPni7A&amp;feature=youtu.be).
&gt; For example, the status of pattern matching was not clear in TT at that time. There's a long way to go from having a foundational theory to a reasonable language design. I kind of figured that might have been it. My first reaction after reading ML's 1975 paper was "If somebody told me to implement that in a 'normal' programming language... how the hell would I do it?" It's certainly non-trivial, that's for sure. That being said, 20 years is a long time.
&gt; when you're programming in Agda/Idris, do you really miss it? I wind up wanting it more often than I thought I would at least. To further complicate the story, we've started picking up hole-filling in Haskell. =)
Oh really? Wow, I've completely missed that. How do I use it? Does it work in haskell-mode in emacs?
Thanks for the lecture, b-b-but I'm afraid I can't go past &gt; A language can be total and Turing complete Has someone solved [this](http://en.wikipedia.org/wiki/Halting_problem) already? Did I suffer a massive memory loss and spend years in a cryo chamber?
Well, the dependent sum type expressed by Sigma corresponds to the existential quantifier in constructive logic :)
You're right, there's definitely ambiguity. I was referring to HM inference on bindings, and its absence in Idris/Agda being insignificant.
Yeah, I was thinking of the `Fam^2` presentation... The `Pow^2` one is quite brutal... It seems like `Fam Pow` works as well though, using the same trick as in the article to get rid of the `Fam`.
Ok, I just tried it in one of my projects, and it worked perfectly for giving me the type of the hole. I'm so glad you mentioned this. Even without magical code inference, it's really useful.
Perhaps this should begin with the detailed and clear suggestions in the recent thousand plus user survey.
How does it? In [this paper](http://www.jucs.org/jucs_10_7/total_functional_programming/jucs_10_07_0751_0768_turner.pdf) it is explicitly stated that total functional programming languages are not Turing-complete. &gt; There are two obvious disadvantages of total functional programming &gt; 1. Our programming language is no longer Turing complete! In fact, it is said that codata and coinduction only solve the "If all programs terminate, how do we write an operating system?" problem, but not the problem of a lack of Turing completeness. This same distinction of total languages and Turing-complete languages is supported [here](http://stackoverflow.com/questions/145263/what-is-total-functional-programming) and [here](http://lambda.jstolarek.com/2015/03/the-basics-of-coinduction/) &gt; So how is that possible that languages like Agda, Coq or Idris can answer that question? That’s simple: they are not Turing-complete (or at least their terminating subsets are not Turing complete). They prohibit user from using some constructs, probably the most important one being general recursion. So I'm still curious as to what /u/pigworker meant when he said that a total language may be Turing-complete.
A total program indeed can not go into a *really* unresponsive infinite loop - but neither can non-total programs! We never observe an infinite number of Turing-machine steps; someone always pulls the plug or hits Ctrl-c. Total programs have the same *observable* behavior as non-total programs; it's just that "pulling the plug" must be internal to the program, and there is no world-stopping out-of-context event from the inside semantics perspective. Non-total languages can't guarantee totality for actually total functions, much like non-pure languages can't guarantee purity for actually pure functions. In total languages partiality is an *effect*, somewhat like some other effects in Haskell which can be tracked via the type system. 
_ works better than undefined or the ?foo trick now. You get all the things that can be used to fill the hole in local scope.
Since you ask, consider reading [my paper on the subject](https://personal.cis.strath.ac.uk/conor.mcbride/TotallyFree.pdf) in which I respond to that particular remark in Turner's excellent paper. I see David around, and I think I've convinced him that his "disadvantage" is overly pessimistic. Thank you for the other links. I'll be popping up there shortly. The expressive weakness is entirely on the side of the partial languages, which cannot make strong promises about the good behaviour of programs, because the possibility of looping or finite failure is pervasive. Total languages allow us to acknowledge the potential infinitude of potentially infinite processes and run them on a suck-it-and-see basis, just like when you run a partial program. The only thing you can't do is lie. The problem when you insist on being able to lie is that you can't expect people to believe you when you tell the truth. Argue in favour of inexpressivity if you like, but be aware that that is what you are doing.
I don't get it: if it's a wishlist, why do so many entries already have associated packages?
Consider also the monad (ha, appropriate given the original topic) Free (Con (s &lt;| p)) and now write a function foo :: forall i. s i -&gt; Free (Con (s &lt;| p)) (p i) which describes a general recursive partial function of type forall i. s i -/&gt; p i as a call-response tree explaining how one call expands into zero or more recursive calls. Or if you want the more simply typed version, the free Monad which looks like data RecOrac s t x = Ret x | Rec s (t -&gt; x) and write foo :: s -&gt; RecOrac s t t for the description of a partial function in `s -/&gt; t`. Every time you feel the urge to have `foo` make a recursive call, even if you can't justify it structurally, you can always have `foo` emit a `Rec s k`, where `s` is the argument you'd have given to `foo` and `k` is the continuation which says how you'd keep going if the recursive call ever delivered a result. You can *write* any old general recursive program this way, without making any commitment as to in what way you intend to run it. Handle the `Rec` effect any way you like: your implementation of `foo` tells you everything you need to know to make progress on demand. Now you just need to manage demand. Ultimately, you can hand off to a runtime that keeps trying a bit more if you haven't hit ctrl-c. That is, the way way to say what a general recursive program *is*, without being in any way prescriptive about how you *run* it, is given by the (inductive) free monad on the container whose shape and positions are exactly the domain and codomain of your partial function. Saying you can't program general recursion in a total language is just the same kind of wrong as saying you can't program IO in a pure language. [edit: added a little more detail]
Agda supports infixl, infixr, postfix, prefix, and... Whatever if/then/else is: if_then_else_ : ∀{A} → bool → A → A → A if_then_else_ false _ x = x if_then_else_ true x _ = x That's how if-expressions are defined in Agda.
i do not entirely follow. if i am not mistaken, in the haddocks / errors for that module, you would see T bare, but in modules that import it qualified it will be qualified in the errors as well as the haddock do you think it is onerous to have to think of what module you are in when reading an error? maybe so, but i think it is inferred by context. either the error file path or the file you are in where your editor has set errors in the gutter, these disambiguate the problem quite well for me edit: i am less sure about haddoc then ghc errors. investigating now to see if what you say is true
While that's the common style in ML (and one Henning Thielemann has advocated for since forever), I find it quite disgusting. I'd much rather have a single line at the top of my program saying `import Foo.Bar.Baz (Baz)` than to pollute the entire code base with extraneous `.T` suffixes on all my types.
Thanks, this is a really good explanation, and very intuitive given an understanding of how IO effects actually work in Haskell. 
A much better approach —and a radical one— would be to reconsider what type identifiers really identify: namely, a "type" is really a module collecting many disparate things (a name for the type, data constructors, record lenses, pattern destructors, induction principles, typeclass instances,...). Given a first-class module system that can properly combine all these different facets of a "type", we can almost always use our name for the module rather than needing to project out a name of the underlying carrier from the module. The only time we need to refer to the underlying carrier is when we're constructing new type modules from old ones, or ensuring they unify in the right way (e.g., ML-functors, or newtypes). But in these cases, the important thing is to gain access to the *identity* of the carrier, not a *name* for that identity.
This confused me as well.
Check out the yet unpublished [Frames package](https://github.com/acowley/Frames) for data processing / visualization.
I didn't say strict positivity was necessary for the induction principle: I said it was sufficient. It's straightforward to specify and compute with the induction principle for strictly positive types, but there are weaker sufficient conditions than being strictly positive: [this, by Ghani, Johann and Fumex](http://arxiv.org/pdf/1206.0357.pdf) tells the current story. It's certainly the case that induction gets hairier to state, let alone establish, for functors which are not strictly positive. Sure, if you have internalized parametricity you also get the induction principle. Kind of a big ask (but it's happening). I'm not sure I know how to program with your proposed encoding of the lambda terms, since I don't know how to make that F a functor, and the use of a strong function space makes me worry about exotic terms. How do I compute, say, whether the outermost five constructors are all lambdas? How do I avoid the usual nonrecursive program that loops all the same? In any case, I don't find impredicativity a price worth paying for common or garden datatypes. Impredicativity is, by definition, the most complicated way to define anything. It's also not obvious how to define "large" datatypes (like lists of types) impredicatively if you're in the consistency business. Mostly, I'm bemused and, frankly, bored by the widespread determination to do everything, even perfectly simple first-order one-potato-two-potato things, by lambda-calculus alone. Sure, lambda-calculus can do everything, but there's more to life than what things do: there's seeing what they are (as in induction), and there's knowing what they don't do.
Also might be of interest to you: https://haskell-servant.github.io/
I'm sorry but I grimace every time I hear about "teach it to a new Haskeller" argument. How many thousands of time do I have to import qualified Data.Map as M import Data.Map (Map) import qualified Data.Text as T import Data.Text (Text) in almost *every* module I work with. It looks like this should be solved by: https://ghc.haskell.org/trac/ghc/wiki/ModuleReexports which enables reexporting modules with different names. So I'd just make a module Core which reexports a ton of stuff. -- Control.Monad as Monad -- Data.Map as Map -- Data.Traversable as Traversable I sometimes wonder about Haskell module names. What does 'Control' in Control.Monad really tell me? Just more letters to type. And Data.Traversable? Why Data? import Applicative import Monad -- Monad, like he is, everyone sees import Traversable Kudos to package maintainers that ditch the unnecessary prefixes! import Pipes That's nice.
My Haskell wishlist is very selfish. I just want a bigger, faster brain.
I always had assumed the compiler generates different entry points for each arity and called the right one based on the number of args given in the calling statement. 
Isomorphisms I get. I just don't get "`random thing` is `random thing` up to isomorphism."
My favorite implementation of effects systems like this ("algebraic effects handlers") is in Sam Lindley and Conor McBride's [Frank](http://homepages.inf.ed.ac.uk/slindley/papers/frankly-draft-march2014.pdf) language. Essentially, when an effectful primitive is called, one can imagine it as an "exception" being thrown to be handled by the closest enclosing effect handler, before computation resumes. It allows for a more natural style of effectful programming than the monadic style (application is just application rather than `=&lt;&lt;`), and programs can easily mix and match "concrete representations" (i.e., handlers) of effects.
Basically yes. Another option is to transpile the IR to the language you’re using to write your application and compile it as part of your normal build process.
The goal is not to have more ways in the long run. If people adopt the new syntax, the old will be used less and less over time. As to your second paragraph, that is really what this is aimed squarely at. Off the top of my head, people do import one unqualified name for Text, ByteString, Vector, Set, Map, and sometimes a view pattern plus a type name for Sequence. There are many more examples, of course, but these are core types we *want* people to be using regularly because their surrounding functionality has been so well tuned.
It is literally no different if you want to just use long explicit import lists. A fairly common style, though, is to import qualified if you want to use a bunch of names, but still want to establish a source or avoid clashes. This is fine, and how text, bytestring, vector, etc. are often used. The pinch currently felt is that the central type names of these modules *are* unique, so folks do the two-line import dance. But again, the syntax for long import lists without qualified names is not touched.
I may be unique, but I am a counter-example to the assertion that everybody understands `import Data.Map.Map as M` to mean anything in particular. Usually we import modules, not types, so I'd have to check the manual because this form of `import` looks like a type error!
There are a bunch of fairly low-level FFI-flavored functions like `withCString :: String -&gt; (CString -&gt; IO a) -&gt; IO a`, which give you a `CString` and clean it up for you when you're done. What may not be obvious to someone first coming across that is how to get `CString` values for multiple input `String`s at once. Fear not! Stick it in `Cont`, `sequence` (which suggests I'm only using the `Cont` `Applicative` here), and get your `CStrings` all at once :) I can't remember who showed me that several years ago, but it's handy!
FWIW, I think that's a pretty valid request with new or old import syntax.
The [Shake build system](http://shakebuild.com) is based a continuation monad. * http://neilmitchell.blogspot.co.uk/2014/06/optimisation-with-continuations.html - why Shake uses continuations * http://neilmitchell.blogspot.co.uk/2014/08/continuations-and-exceptions.html - how it deals with exceptions in continuations * https://github.com/ndmitchell/shake/blob/master/src/Development/Shake/Monad.hs - the Shake monad implementation. And the eventual Monad is: newtype RAW ro rw a = RAW {fromRAW :: ReaderT (S ro rw) (ContT () IO) a} 
See the [documentation for the `managed` library](https://hackage.haskell.org/package/managed-1.0.0/docs/Control-Monad-Managed.html) which goes into this in more detail.
While Haskell’s type system deals only in unary functions, GHC’s code generation deals in functions of any arity—it produces simple calls for fully applied functions, and only allocates closures in the case of partial application. This is a definite oversimplification; /u/simonmar would probably know more.
There are many examples in Haskell of modules exporting more than one type. Your proposal of exporting one type power module feels quite limiting. 
I guess it's kinda hard to seperate them. If there are no libraries, then it's hard to know if your wish is already fullfilled.
FWIW I haven't even started working on this.
Can't provide a canonical list, but https://jude.bio and https://github.com/pikajude/jude-web
[Yep](https://github.com/yesodweb/yesod/wiki/Powered-by-Yesod).
The previous version of LambdaCube 3D was a GHC Haskell EDSL, the current version is a Haskell-like DSL. Whichever approach is followed (EDSL or DSL), we gain experience about purely functional GPU graphics programming and this experience can be used in the other approach too. I think that in the current phase of development we can do experiments faster with the DSL approach.
Check out https://git.gnu.io/snowdrift/snowdrift source of https://snowdrift.coop
The comments so far (as of this writing) are great, and I agree with all of them. Which actually surprises me. I strongly disagree with what some people often write about the continuation monad. But they haven't yet in this thread. So I'll troll them at the end of this post, because they are very smart people and I'm probably wrong :). What we have so far is that `ContT` is good for: * Transforming IO operations to work in a specific environment, as often happens in FFI. * Re-inversion of control when using a library whose API is based on IO callbacks, such as a GUI library. * Optimization - but in that case, do it the simple way (*without* CPS) unless you are sure that optimization is needed and that CPS is the right optimization. What I disagree with is that `ContT`, and its souped-up version `LogicT`, are the best way to implement complex program flow logic such as multi-way decisions, short-circuiting, and backtracking. They are not. Each of those has a simple, straightforward implementation without introducing CPS: the `Maybe` monad (and `Alternative`) for multi-way decision, the `Either` monad for short-circuiting, and the `StateT []` monad for backtracking. CPS makes code much more complex - harder to understand and harder to maintain. Even if code based on `LogicT` looks simple on the surface, it's important to be able to understand what is going on in the monad behind the scenes. CPS should be avoided unless it is for one of the reasons above.
It feels like there's some overlap with [awesome-haskell](https://github.com/krispo/awesome-haskell). I wonder if instead of listing existing libraries there could be links to sections in awesome-haskell. e.g. something like: See [awesome-haskell#distributed-computing](https://github.com/krispo/awesome-haskell#distributed-computing)
Make sure you post this on Stack Overflow as well :)
I also explain this in a fair amount of detail in my blog post [Understanding the RealWorld](http://www.well-typed.com/blog/2014/06/understanding-the-realworld/). 
In F# this is called [async workflows](https://msdn.microsoft.com/en-us/library/dd233250.aspx). It's used to write code without callback hell.
&gt; But parameterized modules make little sense in Haskell. Personally I think parametrized modules would be very useful. For example, a very simple usage idea would be to specialize types: Write polymorphic library, but import it monomorphically -&gt; better error message, less type annotations, and also possibly better performance (if the compiler can also specialize) 
I have no experience with the platform, but the source for activate-hs seems to look for your ghc installation dir. I installed GHC from a ppa, so mine is `/opt/ghc/7.8.4/`, so I'd have to call the script like: `sudo /usr/local/haskell/ghc-7.8.3-x86-64/bin/activate-hs /opt/ghc/7.8.4/` This is all guessing nevertheless. You can update cabal via `cabal update &amp;&amp; cabal install cabal-install`. Do you really need the platform? GHC and Cabal alone are enough I'd say :P
To update cabal, run: cabal update cabal install cabal-install and look in ~/.cabal/bin for the new cabal binary. If you can't find the new cabal binary, see this SO answer: http://stackoverflow.com/a/30709928/866915 
It's nice to have the invisibility of terms given a name as I hadn't known how to describe it, having done the `read.show` with type annotation equivalent, and used Proxy types to tell the compiler the type I want without having a source-level term for it. Also the forall vs pi quantifiers came up for me recently trying to express a vector with type-encoded natural size in its type but from a user-provided runtime value. This was [trivial in Idris](http://lpaste.net/133630#line40) because the type just depends on the value, but [requires two types in Haskell with singletons](https://gist.github.com/Philonous/3d039b29d980ed881b14#file-main-hs-L62), it seems like "faking" it with a bunch of infrastructure because of this "type-world vs value-world separation" that's been causing me some existential grief, so this video comes at a coincidental time. Is it possible to get anything near as useful as Agda's code generation for Haskell? The most practical thing I've seen so far is a case split, but that's a special case (hur). Djinn has its practical use within a small space but can it be generalized to work on "real" Haskell 2010 code? I.e. involving recursive data types and type classes. I have infinite optimism in this area but no knowledge to support it. Even if a Djinn-like tool generated various possible permutations of implementations of ambiguous types like [a] -&gt; Int or [a] -&gt; [a], it seems like you could drill down to what you wanted in short order without having to actually write code. And backed by hoogle and type info of every expression in your project the completion possibilities seem like they could be substantial. Some funny quotes: &gt; When you work in the old-fashioned mode where you write in an editor and you send your completed file and you get back abuse … and &gt; When you get an error message from GHC, you should think of that as Simon talking to you.
This is exactly what attribute grammars tackle. Read the user documentation of UUAGC for quick introduction to their power.
As I understand this talk, Conor proposes new type checking rules for a dependently typed language with type inference. Does this new approach help in any way with non-dependently typed languages like Haskell?
I don't really know, which is why I was asking, but I imaging things like assignment statements for modules. Instead of import qualified Data.Map as M you could write M = Data.Map
It's not new type checking rules or a new type theory; rather, it's a call for a rethinking of surface language design. In the current Hindley-Milner paradigm, languages tout their ability to hide types from the programmer and refuse to let the programmer play with them, often forcefully insisting that the type checker be allowed to guess the type of an expression rather than let you explicitly specify the type yourself. (e.g., show . read in Haskell, try to call yourFunc&lt;YourType&gt;(yourParam) in Swift, etc.) Conor is saying this is ridiculous, and doubly so in a dependently-typed context: HM has been superseded. In the "new regime", types are explicit, visible, and the central focus of the programmer -- they should not be hidden in the syntax, and direct control over them should always be in the hands of the programmer, and certainly never forcibly guessed by HM.
First off, Haskell Platform *includes* GHC, so there is no need to separately download or install GHC. Further, you can't mix-n-match GHC and Haskell Platform (or any pre-built Haskell package for that matter). To install the platform from the "Generic Linux Binaries" of the page you referenced, you were doing the right thing. Did you perhaps cd into that `bin` dir to run `activate-hs`? If you did, you need to supply the `ghc-root` argument, since it won't be able to figure it out from the path of the script. You can do this like so: sudo /usr/local/haskell/ghc-7.8.3-x86_64/bin/activate-hs /usr/local/haskell/ghc-7.8.3-x86_64 Note the under-bar in `x86_64` the web page has a typo. Your GHC 7.10.1 installation won't be of any use for things built with the Platform, which in that version uses 7.8.3. A new version of the platform, based on the upcoming GHC 7.10.2 is due out this month. Finally, the Platform you installed has Cabal 1.18.1.3.
That's a really good point. One could probably come up with rules for allowing both uses, but they'd be complicated.
Yeah, I've looked at that table a lot. I think import syntax is one of those things where your best bet really is to read the manual, as it were.
Yeah, you're right. Always mix up GitHub's triple \` vs Stackoverflow and Reddit flavored markdown.
&gt; Soon everything will be dependently typed anyway, non dependent types are a legacy technology. Someday. But I don't think it will be so soon. There is still a long way to go. Even in Agda and Idris - and even in Coq - proofs of even simple assertions can be painfully complex to write and understand. And in DT languages you lose the expressiveness and elegance of purity by introducing strictness. But I think we will get there, and I am looking forward to it.
Seems like that page also ought to have a commercial applications section, too. 
What's that then?
Looks like haskell platform causes more confusion than actually helps. There was a big discussion on reddit about the need of haskell platform and most commenters agreed that the only (remotely) viable use case for haskell platform nowadays is windows installation. On linux just install ghc, alex, happy and cabal-install. And you are all set. 
I've always followed the Ubuntu instructions on https://github.com/bitemyapp/learnhaskell , never had trouble with it.
This is such a simple app, but makes my life so much better. The sheer speed of feedback is amazing. I run it in a tmux pane, next to my editor. No fussing with ghc-mod integration in Vim. 
That's how I interpreted it as well.
&gt; And in DT languages you lose the expressiveness and elegance of purity by introducing strictness. ?
I imagine he means being able to write `show . (:: Double) . read` instead of `(show :: Double -&gt; String) . read`. It was [described here](http://augustss.blogspot.it/2014/04/a-small-haskell-extension.html).
So what you've described is usually called an "applicative morphism", e.g. when you have an arrow `phi :: (Applicative f, Applicative g) =&gt; f a -&gt; g a` which has the following properties pure a = phi (pure a) fmap f (phi x) = phi (fmap f x) phi x &lt;*&gt; phi y = phi (x &lt;*&gt; y) These properties imply the "linearity" equations you describe. So what you're looking for is "when are natural transformations exactly Applicative morphisms?" which is kind of a weird question to find an answer to. The set of all natural transformation is not constrained by applicative laws and so there is a high likelihood of them cheating ways that break structure. So I don't know a good answer to your question directly. But there is an interesting related question which has a great answer. In particular, it's possible to find a type construct `K` such that the set of natural transformations `forall a . f a -&gt; g a` correspond exactly to the applicative morphisms `forall a . K f a -&gt; g a` when `g` is an `Applicative`. In this case there's no demand that `f` is `Applicative` at all, `K` will turn it in to one. {-# LANGUAGE GADTs, RankNTypes #-} data K f a where Unit :: a -&gt; K f a Ap :: f x -&gt; (x -&gt; b -&gt; a) -&gt; K f b -&gt; K f a instance Functor (K f) where fmap f (Unit a) = Unit (f a) fmap f (Ap fx xba ffb) = Ap fx (\x -&gt; f . xba x) ffb instance Applicative (K f) where pure = Unit Unit g &lt;*&gt; y = fmap g y Ap fx xba ffb &lt;*&gt; y = Ap fx (\x -&gt; uncurry (xba x)) (liftA2 (,) ffb y) liftK :: f a -&gt; K f a liftK f = Ap f const (Unit ()) lowerK :: Applicative g =&gt; (forall x . f x -&gt; g x) -&gt; (K f a -&gt; g a) lowerK k x = case x of Unit x -&gt; pure x Ap fx xba ffb -&gt; fmap xba (k fx) &lt;*&gt; lowerK k ffb This should satisfy laws like, for any `phi :: forall a . f a -&gt; g a` and applicative `g` pure a = lowerK phi (pure a) fmap f (phi x) = lowerK phi (fmap f (liftK x)) phi x &lt;*&gt; phi y = lowerK phi (liftK x &lt;*&gt; liftK y) Edit: To add, since I left it out this morning, the above construction can be found as the composition of the standard Free Applicative structure and Coyoneda which is the "free Functor". I didn't just pull it out of nowhere, but it does have a sort of nice appealing look, especially if I wrote it more like this data K f a where Unit :: a -&gt; K f a Ap :: (f l, K f r) -&gt; ((l, r) -&gt; o) -&gt; K f o
Thanks! I took a casual look at the subject some time ago but from a distance it looked more like a software engineering topic.
Sugar like `(:: T) ==&gt; (\x -&gt; x :: T)` I think. Or was it `(:: T) ==&gt; (id :: T -&gt; T)`?
&gt; And in DT languages you lose the expressiveness and elegance of purity by introducing strictness. Huh? DT languages can be lazy, you know.
Have you looked at `vinyl`? The `Rec f rs` type is a heterogeneous list of elements with types given by `rs`, with a functor `f` applied over each element. You have an `HList` when `f` is `Identity`, and you have an extensible record when `f` tags each value with a type-level symbol. These definitions have become much simpler with type-level lists.
You could look into it as encoding a left kan extension using a corepresentable profunctor rather than a function space from the positions. data instance Con (s &lt;| p) x = forall i. s i :&lt;: (p i -&gt; x) is exists i. s i * q i x -- for a profunctor q which is corepresented by p, but that looks like a form of profunctor composition, because 's i' is another profunctor in its own right which ignores its input, and that traces a similar lan-style end (which makes sense when you go into profunctor composition as a form of kan extension in its own right).
But are there any corepresentable profunctors that are faster than the ordinary function space?
can you give an example of that? I read the haskellforall posts in free monads. I get "reinterpreting" (like execution versus visualization), but not "validation".
Aren't there big limitations? Can the type checker check a possibly infinite heterogeneous list? 
I already hoped the project would do well for its own sake, but now I have reason to hope it does well for mine :)
[Image](http://imgs.xkcd.com/comics/standards.png) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini-USB. Or is it micro-USB? Shit. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/927#Explanation) **Stats:** This comic has been referenced 1618 times, representing 2.4198% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_crzn6oi)
&gt;DT languages can be lazy, you know. but not useful? edit: What I mean is, how do you give any dependently typed guarantees on values without eager valuation of all thunks? You can't return a lazy sequence that is typed to a particular size right? 
In your first definition of `phi`, do you mean to leave out `g` from the type signature?
I wrote some compiler passes for a simply typed lambda calculus, it's really good for (mutrec) ASTs. The Eventual monad seems to do something similar, but you need to define the passes yourself.
Nope, typo! Thanks, fixed now.
Why not? Dependently typed languages only need a type distinction between finite lists and potentially infinite lists. The latter have to be evaluated lazily of course, but finite lists can be evaluated strictly or lazily. The same is true for lists that have a length that is determined by their type. With some extensions you can even do lists typed to a particular size in Haskell, and Haskell is lazy.
mutrec?
Meta-comment: please don't downvote comments to oblivion merely for being "wrong," *especially* when that wrongness is discussed below! Downvote comments that are *not constructive*.
Wow. That is so cool, thank you for your work.
They can be, but more importantly they can memoize the individual results rather than recompute on each inspection.
Yes, this was, i think, my question. Though I'm lacking in terminology and general knowledge of what I'm talking about. The context is me playing around with linear algebra, using something like the following: data Linear a f g where Diagonal :: f a -&gt; Linear a f f RowMatrix :: g (f a) -&gt; Linear a f g ColMatrix :: f (g a) -&gt; Linear a f g Where a is the scalar type and with some appropriate constraints. feeling a general lack for something like: Projection :: (forall a. f a -&gt; g a) -&gt; Linear a f g Which won't misbehave. Noting (perhaps incorrectly) that Readerish things behave something like this, for example Maybe, ZipList, Map, Fixed-size vectors &amp;c. while Lists does not. 
I'm planning on disallowing any kind of forward reference, at least as a first draft. :)
Look back a few posts, someone asked the same question today or yesterday. There were some good responses there.
Nice. It remembers me the first Pearl of Functional Programming by Richard Bird, but I don't find any link to buy a book, and I googled a little without success. Can anyone tell me what document is the author refering to?
Thanks for your feedback. Which document are you referring to? How to Program It?
Can bottom not exist in a finite, lazy list? How do you ensure that without evaluating the thunks? 
Yes. I didn't find it at Amazon. So many Spaniards like Haskell, I say this because I am one and maybe you too xD. 
Why not? How exactly do you propose to get your hands on a possibly-infinite heterogeneous list? Presumably you'll have to construct it from pieces. And each of those pieces will have some finite description of how they're constructed (you do plan on finishing your code, right?). Thus, they will be as easy/hard to check as any other finite description of a value. Moreover, even if there are infinitely many pieces, there will be some finite description of how many there are and what they look like; so that's no escape either.
You don't need to know the particular length of a finite list in order to know that it has some (finite) length. That's the whole point of dependently typed languages: we can define a function of type `Pi n:nat, forall a:Type, a -&gt; List n a`. When we lift `n` to the type level, we don't —and can't— know what it actually is at compile time; we only know it's a `nat`. But that's sufficient to know the list is finite. When we append lists: `Pi n, m:nat, forall a:Type, List n a -&gt; List m a -&gt; List (n+m) a` we don't need to know what the lengths of the lists are; we only need to know that they are finite and that addition preserves finiteness. The reason for desiring totality is that eventually we'll need to decide equality of types in order to decide whether we're allowed to pass a particular argument to a particular function. But we don't need strictness to guarantee totality. It helps, of course; but isn't necessary.
Not Cont exactly, but I've found ContT useful for doing setup/teardown. This is from my current spare time project: withInputChannel :: ContT r IO (IO (Either Error Input), Output -&gt; IO ()) withInputChannel = do chan &lt;- lift newChan let fork :: IO (Either Error Input) -&gt; ContT r IO () fork io = ContT $ \body -&gt; bracket (forkIO . forever $ writeChan chan =&lt;&lt; io) killThread (const $ body ()) fork $ do threadDelay 1000000 gitStatus &lt;- fmapL (pure . ("gitGetStatus: " &lt;&gt;)) &lt;$&gt; gitGetStatus return $ flip (,) NoEvent . Event &lt;$&gt; gitStatus vty &lt;- ContT $ \body -&gt; bracket (mkVty def) shutdown body fork $ do vtyEvent &lt;- nextEvent vty return $ Right (NoEvent, Event vtyEvent) return (readChan chan, performOutput vty) main :: IO () main = mapM_ T.putStrLn =&lt;&lt; runContT withInputChannel (flip run mainWire) `withInputChannel` is increasingly badly named, and I'm not sure sending two separate events into netwire is a good idea, but ignore all that: it sets up the state the main part of the program needs (the main part of the program here is `flip run mainWire`), and shuts it down again afterwards. The best example of that is the line setting up vty. Sure, you could do all this without ContT, but it would be messier.
&lt;https://personal.cis.strath.ac.uk/conor.mcbride/TotallyFree.pdf&gt; is what is probably being referred to.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Module (mathematics)**](https://en.wikipedia.org/wiki/Module%20%28mathematics%29): [](#sfw) --- &gt; &gt;In [abstract algebra](https://en.wikipedia.org/wiki/Abstract_algebra), the concept of a __module__ over a [ring](https://en.wikipedia.org/wiki/Ring_(mathematics\)) is a generalization of the notion of [vector space](https://en.wikipedia.org/wiki/Vector_space) over a [field](https://en.wikipedia.org/wiki/Field_(mathematics\)), wherein the corresponding [scalars](https://en.wikipedia.org/wiki/Scalar_(mathematics\)) are the elements of an arbitrary given ring (with identity). &gt;Thus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over the addition operation of each parameter and is [compatible](https://en.wikipedia.org/wiki/Semigroup_action) with the ring multiplication. &gt;Modules are very closely related to the [representation theory](https://en.wikipedia.org/wiki/Representation_theory) of [groups](https://en.wikipedia.org/wiki/Group_(mathematics\)). They are also one of the central notions of [commutative algebra](https://en.wikipedia.org/wiki/Commutative_algebra) and [homological algebra](https://en.wikipedia.org/wiki/Homological_algebra), and are used widely in [algebraic geometry](https://en.wikipedia.org/wiki/Algebraic_geometry) and [algebraic topology](https://en.wikipedia.org/wiki/Algebraic_topology). &gt; --- ^Interesting: [^Mathematics ^Mechanization ^and ^Automated ^Reasoning ^Platform](https://en.wikipedia.org/wiki/Mathematics_Mechanization_and_Automated_Reasoning_Platform) ^| [^List ^of ^commutative ^algebra ^topics](https://en.wikipedia.org/wiki/List_of_commutative_algebra_topics) ^| [^List ^of ^abstract ^algebra ^topics](https://en.wikipedia.org/wiki/List_of_abstract_algebra_topics) ^| [^Hodge–Tate ^module](https://en.wikipedia.org/wiki/Hodge%E2%80%93Tate_module) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+crzwuy4) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+crzwuy4)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Say I lazily generate a list of all the natural numbers. Each number is wrapped, and the wrapper's type reflects whether the Collatz conjecture holds for that number. Over this list, I want to map a function which only takes numbers for which it holds. I think you can write that, but the type checker will never finish (if it does Erdos owes you $500). 
I think you have a wire crossed somewhere. What is the corepresentation of Dia? Corepresentability of `p`, means `p a b` is isomorphic to `(Corep p a -&gt; b)`, so yes its equivalent. It is isomorphic. `Kleisli f` is _representable_ by f, not corepresentable in general. * `Representable p` means `p a b` is isomorphic to `(a -&gt; f b)`. * `Corepresentable p` means `p a b` is isomorphic to `(f a -&gt; b)`. The only corepresentable Kleisli cases (in Hask) are isomorphic to `Kleisli ((-&gt;) i)` for some `i`. Kleisli ((-&gt;) i) a b ~ a -&gt; i -&gt; b ~ i -&gt; a -&gt; b ~ (i, a) -&gt; b ~ Cokleisli ((,) i) a b (This includes Identity, composition, etc.) The fact that all of them look like this follows from the [uniqueness and the left adjoint properties](http://en.wikipedia.org/wiki/Representable_functor#Left_adjoint) of representable functors and the definition of adjoints.
Why you stalkin' my class?
I think you missed the point of the type system altogether. A total language guarantees all terms, if you were to fully reduce them, would reach a normal form. But that is a hypothetical. There is no need to reduce them at all if you don't need to. Total languages are even better computationally speaking because not only can you freely choose lazy evaluation, but you generally get evaluation order agnosticsm. You can pick redexes at random if you wanted to, and confluence guarantees your semantics are preserved.
Could you provide some learning materials for this idea? It sounds a lot like FRP in a way.
It's very much like FRP (in principle, at least). A google search should hopefully turn up useful things, but https://www.youtube.com/watch?v=8JKjvY4etTY seems like a relevant talk. I actually came upon the concept from an earlier talk of Greg Young's (after having indepently re-re-re-invented it and thinking I was so clever[1]) and haven't fully watched this one, but he usually argues pretty well for CQRS/ES, so I feel reasonably comfortable referring you to this one :). [1] It's been reinvented several times. WAL (not quite the same, but...), Log-Structured File Systems, (etc.)
In case you overlooked this, the Yesod Book itself contains several toy examples; e.g.: http://www.yesodweb.com/book/visitor-counter
Too bad there aren't videos of the lectures. Looks like a great class.
I have no idea what "wrapper" you mean, nor where you think the typechecker will fail. We can have an infinite stream of natural numbers; that typechecks just fine. And we can define a predicate of natural numbers returning whether the Collatz conjecture holds for that number; that typechecks just fine. You can filter the infinite stream to only include those natural numbers for which it succeeds/fails; I see no reason why that shouldn't typecheck either. The only place I can see running into issues is if you desire to stipulate that the filtered list is finite (or infinite, or has any other property). But there's nothing special about the Collatz conjecture here, this same problem holds for filtering any infinite stream. If you want the type to encode whether the result is in/finite, then you'll need to provide some proof of that fact. The only difference vs, say, filtering the nats by whether they're even or not is that it's easy to provide the proof of infiniteness for the even nats. Coq agrees: Axiom Collatz : nat -&gt; Prop. Axiom Collatz_decidable : forall n, {Collatz n} + {~Collatz n}. CoInductive Stream (A : Set) : Set := | Cons : A -&gt; Stream A -&gt; Stream A | Skip : Stream A -&gt; Stream A . Implicit Arguments Cons [[A]]. Implicit Arguments Skip [[A]]. CoFixpoint natsFrom (n:nat) : Stream nat := Cons n (natsFrom (S n)). CoFixpoint filter {A:Set} {P:A-&gt;Prop} (f : forall a:A, {P a} + {~P a}) (xs : Stream A) : Stream {a:A | P a} := match xs with | Cons x xs' =&gt; match f x with | left p =&gt; Cons (exist _ x p) (filter f xs') | right _ =&gt; Skip (filter f xs') end | Skip xs' =&gt; Skip (filter f xs') end. Definition quux : Stream {n:nat | Collatz n} := filter Collatz_decidable (natsFrom 0).
I mean whatever it is that causes us to use the same word for Haskell modules, Agda modules, ML modules, etc. A module is a particular sort of (dependent) record, a namespace which can be passed around. That many languages only allow one module per file is an unfortunate engineering concern.
I think there's a point missing in the idea of "Compositionality, but at a cost". The cost is that we must factor our programs in a particular way; the benefit is that our programs are factored to assist local reasoning. But local reasoning isn't just a benefit for the compiler, it's a benefit for humans as well. The more we can rely on local reasoning, the easier it is to keep the relevant parts of the program in mind and the easier we can refactor things. This may not mean much for "interaction", but it means a heck of a lot to programmers of all stripes.
&gt; I have no idea what "wrapper" you mean Just a constructor like `(CollNumber 1) : Coll True`. &gt; You can filter the infinite stream to only include those natural numbers for which it succeeds/fails I left this step out. I suppose you can always do that. Or just fake it and assume it's always true, it's a phantom type anyways. 
This is a good one imho: https://github.com/thoughtbot/carnival
You can fix the first problem by restricting the stored functions to be total
These are wonderfully designed and written lectures. The homework problems, you refereed in syllabus, are they at the end of the lectures, or something special accessible only to students?
/r/haskellquestions for completeness.
Finally I fixed it, thank you. I just had to install cabal-install. My ghc version was 7.10.1
Yes. So we need not be mortified by that issue; but perhaps morte-ified would be good? :-P
Do you (or does anyone else here who's done this) have any recommendations for database systems designed for this? This is how we've modeled our data at slant.co, but just directly on PostgreSQL. I'm particularly wondering about any that can take advantage of the commutativity of some events for distribution/parallelizing playback.
Yes. It's sad :(
Great tool. I have something similar I use for pretty much everything. It's been on my "write a blog post about this" for three years now :(. I should really work on that.... onmodify () { TARGET=${1:-.}; shift; CMD="$@"; echo "$TARGET" "$CMD"; ( if [ -f onmodify.sh ]; then . onmodify.sh; fi; while inotifywait --exclude '.git' -qq -r -e close_write,moved_to,move_self $TARGET; do sleep 0.2; if [ "$CMD" ]; then bash -c "$CMD"; else build &amp;&amp; run; fi; echo; done ) } No one ever said bash was pretty. Any way, usage is onmodify ./some/path "thing --args &amp;&amp; otherthing &amp;&amp; this is just a bash cmd that is eval'd" which sets up `inotify` (linux only) to watch everything under `./some/path` and runs the command whenever it changes. For me, it's often something like onmodify . "nosetests --with-id --failed || fart_noise" which runs python tests and plays some randomized rude noises if I break something.
*ba-dum tish*
From the discussion: _"However, the key message of this paper is that the status of generalrecursive definitions is readily negotiable within a total framework. There is no need to give up on the ability either to execute potentially nonterminating computations or to be trustably total. There is no difference between what you can do with a partial language and what you can do with a total language: the difference is in what you can promise, and it is the partial languages which fall short"_
Ah, it's because I looked at the convention used in [`representable-profunctors`](https://hackage.haskell.org/package/representable-profunctors-0.4.2.2/docs/Data-Profunctor-Corepresentable.html) instead of in [`profunctors`](https://hackage.haskell.org/package/profunctors-5.1.1/docs/Data-Profunctor-Rep.html).
But... cabal: Error: some packages failed to install: text-icu-0.7.0.1 failed during the configure step. The exception was: ExitFailure 1 yi-0.12.0 depends on yi-language-0.2.0 which failed to install. yi-language-0.2.0 failed during the building phase. The exception was: ExitFailure 1 yi-rope-0.7.0.1 depends on text-icu-0.7.0.1 which failed to install. Now what?
I'm getting this: cabal: Error: some packages failed to install: text-icu-0.7.0.1 failed during the configure step. The exception was: ExitFailure 1 yi-0.12.0 depends on yi-language-0.2.0 which failed to install. yi-language-0.2.0 failed during the building phase. The exception was: ExitFailure 1 yi-rope-0.7.0.1 depends on text-icu-0.7.0.1 which failed to install.
Seeing as the constructor for such a wrapper would invoke some sort of collatz program, you wouldn't be allowed to lie in the types and say that this collatz program definitely terminates, which would make this example impossible. But not lying in types is a good thing!
Here is the companion, or probably even the prelude to, paper by the same authors: http://referaat.cs.utwente.nl/conference/15/paper/7282/on-the-potential-of-functional-relational-databases.pdf
There's nothing stopping you folding the events into a relational store. Here's roughly what we did at fynder: The Command part of CQRS is a Free Monad that has access to the relational db and produces Events. We used the db constraints as part of our domain model in that we'd open a transaction, try and apply the events that the Free Monad wanted to produce to the database and roll back the transaction before The events produced are appended to the event stream. An example of a Command might be: bookEvent :: BookCmd -&gt; CommandT BookingId bookEvent (BookCmd eventId userId) = ... Events produced might look like: BookingCreated BookingId UserId EventId UserCharged UserId Ammount ... As you rightly say you can now do what you want with the events and they are your golden source. We actually kept the Events in a table and folded the Events *back into* the relational (postgres) database which was also our Query database too. So we had both event semantics and arbitrary query ability. So depending on your schema you might end up with to following SQL getting run: insert into bookings (user_id, class_id, ....) value (userId, classId, ....) returning id .... That was pretty cool as you could join the event stream to the current state of the entities they express which is great for analytics etc. Here's what I'd do now: I'd have a RDBMS in front of the Event stream (which I'm looking pretty hard at Kafka for). I would do the operations on the DB as part of the Command as before, but send the Events to the log (kafaka) and I'd commit the transaction when the events were received instead of aborting. There's a great write up of this idea here: http://blog.confluent.io/2015/05/27/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/
For a while I was using `Representable` for `Functor` to model `f a ~ (x -&gt; a),` and `Corepresentable` for `Contravariant` to model `f a ~ (a -&gt; x)` and when I first packaged profunctors I followed this notation with p a b ~ f a -&gt; b for representable and p a b ~ a -&gt; f b for corepresentable. Unfortunate in each case this is precisely the reverse of the standard convention, so I finally gave in and flipped it all. We can justify saying both of those are "representable" but corepresentable is always applied in literature the covariant case. Also UpStar and DownStar were reversed from their correct convention, so I finally flipped those as well, making them `Star` and `Costar`, (analogous to Kleisli and Cokleisli) so they could follow the "rule of stars".
Here's my first issue already: the URL to the Github issue tracker is incorrect, it should be https://github.com/commercialhaskell/stack/issues :)
There is also: [CMSC-16100](http://cmsc-16100.cs.uchicago.edu/2015/)
Fixed, thanks. ;-) 
When you say "install `shake` locally," you mean run `stack build` inside the `shake` directory? If you do that, then only packages in the same project will have access to that version of shake. If you want to use a locally modified shake in another project, you'd need to reference its source directory in `packages` inside stack.yaml, otherwise it will download it from the package index and use that copy (i.e., whatever's on Hackage).
Thank you to everyone who has contributed code, ideas, feedback, and bug reports to get us to this state. I just wanted to point out some more useful resources for people looking to get started: * The [downloads page](https://github.com/commercialhaskell/stack/wiki/Downloads) has binaries and packages for difference OSes and Linux distros * The [FAQ](https://github.com/commercialhaskell/stack/wiki/FAQ) answers a bunch of random questions * The [transition guide](https://github.com/commercialhaskell/stack/wiki/Transition-guide) is a stub page to help people familiar with other build tools pick up stack * The [architecture page](https://github.com/commercialhaskell/stack/wiki/Architecture) gives some details on how stack works internally, which would be useful for people looking to do corner-case stuff or work on the deep internals Also, there's a [newcomers label](https://github.com/commercialhaskell/stack/issues?q=is%3Aopen+is%3Aissue+label%3Anewcomer) for issues that are easy to get started with, and a [discuss label](https://github.com/commercialhaskell/stack/issues?q=is%3Aopen+is%3Aissue+label%3Adiscuss) for items that feedback would be especially useful on.
Italian here! Glad to see this happening, although a name which doesn't remind me of a [train company](http://www.italotreno.it/IT/Pagine/default.aspx) might have been more effective I think :P
Sometimes it's practical, though. You can already lie. Partial functions are allowed. That, too, has mostly to do with infinity and proofs that go beyond induction.
Thank you! This looks very interesting! Am i correct in understanding that what's essentially going on here is that K delays the "application" until you provide it with a natural transformation to an applicative?
Very, very cool!
Well, ItaloHaskell was the name of an Italian Summer meetup held in 2008: https://wiki.haskell.org/ItaloHaskell The name is inspired by AngloHaskell. Maybe it could be changed into *ItalicoHaskell*, I don't know.
Congratulations /u/bas_van_dijk!
Congratulations! Great work!
This looks really helpful. I'm trying this out right now at work. In thirty more minutes, I will know for sure if it's helpful.\ UPDATE: Now that I've been using this for two hours, I can say that I'm really pleased with it. The close integration with stackage LTS fits my workflow very well, and I'm pretty excited about not having to rebuild the whole sandbox every time I start a new application. For web application development at my office, I consider `stack` a serious contender to `cabal-install`. Can't wait to see the future of it.
We don't currently do binary downloads of packages, but there's an open issue for it (on my phone, sorry for lack of link). Definitely something to aim for in the future. In the meantime, the docker support provides all of Stackage precompiled.
+1 on that
Congratulations! 
I just tried it out on a multi-package project, and it seem to work well. One thing I ran into was that `stack ghci` didn't do what I hoped: it just loaded a single file in the repl, using the stack package database. I was hoping it would at least add one of the packages as the input directory, and ideally all of them. That fits with the way I usually develop. I found [this issue](https://github.com/commercialhaskell/stack/issues/130) which is kind of related. Should I add a comment there, file a new issue, or is this comment enough feedback?
Please add a comment to that issue. As you can tell, we have plans to significantly improve `stack ghci`, we just haven't gotten there yet.
text-icu failed to install, error message was somewhere earlier in the build log. Most probably you don't have icu4c libraries and headers on your system. If you're using OS X then there's additional caveat: you have to tell cabal where to find said libraries and headers, see http://stackoverflow.com/questions/7420514/using-text-icu-library-in-haskell-on-mac-os
Awesome! I study here so that's really cool. Haskell impacting my daily biking life :) parking here in Utrecht is a real problem which I struggle with everyday. Its hard to believe as an outstander just how many bikes are here. If you need help maintaining this in the summer, I'm looking for a summer job! haha.
Good catch, can you open an issue? I think this is a simple change to make actually, IIRC we added this support to http-client a while ago.
Every aspect of this story is fantastic. Congratulations!
Is it fair to say that Halcyon does attempt to provide a point of abstraction where the user doesn't need to use or think about `cabal-install`? In that sense, would it be fair to say `cabal-install` is just an implementation detail of Halcyon?
Are there any changes necessary for emacs haskell-mode to work with this? Or would it just transparently work with cabal / ghci just like it does now with sandboxed projects? 
With Halcyon, I’ve tried to augment the capabilities of `cabal-install`, while remaining compatible with it. My goal has always been to show people a concrete vision of how Cabal could be improved, with the eventual goal of merging these improvements into Cabal. [[1]](http://www.reddit.com/r/haskell/comments/2ogoot/deploy_any_haskell_application_instantly/cmn5mc1?context=3) [[2]](https://twitter.com/mietek/status/541323535046963200) [[3]](https://mail.haskell.org/pipermail/cabal-devel/2014-December/009924.html) [[4]](http://thread.gmane.org/gmane.comp.lang.haskell.general/20499) [[5]](https://mail.haskell.org/pipermail/cabal-devel/2015-March/010053.html) [[6]](https://mail.haskell.org/pipermail/cabal-devel/2015-March/010064.html) [[7]](https://mail.haskell.org/pipermail/cabal-devel/2015-March/010066.html) [[8]](https://mail.haskell.org/pipermail/cabal-devel/2015-March/010073.html) For example, you can use Halcyon to install GHC and `cabal-install` — and stop there. You can then continue using these tools as you’ve always had. Or, you can go one step further, and also use Halcyon to build a Cabal sandbox for your project. Once you’ve done that, you can tell `cabal-install` to use this sandbox, and get back to your regular workflow. Finally, you can also use Halcyon to build your entire project — whether on a development machine, on a continuous integration server, or on the deployment target — as thoughtbot showed in [“Building Haskell Projects with Halcyon”](https://robots.thoughtbot.com/building-haskell-projects-with-halcyon). At all stages of the process, Halcyon makes use of openly-available infrastructure. You are never locked into depending on a service provided by any single company, including my own.
I definitely think that `stack repl` issue is the right one. Whether we merge them into one command or keep them separate, the intention is definitely there to load in a package with its source dirs, extensions, autogen files, etc.
I am sorry, I don't mean to be rude or anything, I really enjoyed your paper, but I don't see how it eliminates the problem (or rather a non-problem IMHO) of Turing-incompleteness. According to the usual terminology (completeness, computability, totality), a total language can not be Turing complete. This a well-known Richard's Paradox -- heck, even you had a posting specifically about it: &lt;https://mail.haskell.org/pipermail/haskell-cafe/2003-May/004343.html&gt;! So, under the terminology that is used by me, people who taught my computability classes, and was used by you at some point, a total language can not be Turing complete. I can only assume that when you claim that the usage of coinductive data can make a language Turing-complete you use some other definitions (e.g. your definition of "semantics" or "completeness" or "total" is different). Maybe defining the aforementioned notions would shine some light on this?
Nice work! Too bad the sensors won't be counting my bike anymore, it recently got stolen right at the spot from the picture on that page. Maybe we can install some Haskell to prevent theft. ;)
I think we can do it. I've been using my binary cache for Nix stuff hosted on S3 between a couple OS X machines, and it works great. Replicating the Nix hashing isn't trivial, but it is clearly possible.
I get an error following the link to the "Parsing and Monads" lecture of the Spring 2014 class. Anybody know if the lecture is roughly equivalent to the 2011 "Monads and More" slides or the "Parsing and Continuations" slides? Or know the correct URL for the 2014 lecture?
161 was my intro to Haskell, and Kurtz is great. It was definitely a whirlwind of a course for me at the time, but very fun. 
Nice work!
He *is* the best lecturer I ever had, hands down.
Awesome work! Looking forward to a deploy near Mannheim.
And we made it the default, since I predicted that if it wasn't then every app would get big reports one after another :)
Good! I also live in italy, in Trento!
Wow, my two main interests on the Internet (bicycle infrastructure and Haskell) converge. Never expected a link to bicycledutch from /r/haskell. Now I need to find that icon I made a while back that adds wheels to the Haskell logo to make a Haskellcycle. 
Found here: https://github.com/nahuel/misc/tree/master/haskell/CS240h Edit: this seems to be exactly like 2011 "Parsing and Continuations"
Looks to be [in progress](https://github.com/commercialhaskell/stack/issues/111).
Treviso here, nice to see a fellow haskeller so close!
Author of the hakyll css garden here. Thanks @AIDS_Pizza for linking them. I haven't spent much time creating new themes lately, didn't know anyone was interested! I'm open to taking requests.
Not haskell-related, but I found [this presentation](http://www.infoq.com/fr/presentations/predicting-balancing-bikeshare-systems) about balancing bikeshare systems very interesting.
^^^Docs?
Some thanks are in order: - Franz Thoma, who implemented the warnings in GHC with me - Herbert V. Riedel for his help with the compiler and writing the proposal - Edward Kmett for preventing us from doing some stupid mistakes (we'll keep the `String`, alright alright) ;-) - ZuriHac and its organizers for giving me the final kick in the butt to actually start implementing/proposing this
Ciao a tutti :) Marco
I also thought that Halcyon was more focused on deployment rather than development. Is [this](https://robots.thoughtbot.com/building-haskell-projects-with-halcyon#IotHha-indicator) how you develop with Halcyon? Can you perhaps detail your development process? I installed Halcyon on my girlfriends mac and went through the tutorial a bit ago. I got stuck when I couldn't figure out how to start a repl while following the [tutorial](https://halcyon.sh/tutorial/). Perhaps you could add a "developing" section like [this](https://robots.thoughtbot.com/building-haskell-projects-with-halcyon#IotHha-indicator) to the tutorial?
Please join our subreddit: http://www.reddit.com/r/Haskell_ITA/
It's absolutely the right thing to do. We write types which will be thrown away in order to distinguish exactly which invisible values will be used at run time. That is progress.
+1 Long overdue! Sincere question, does `fail` in `Monad` have any supporters whatsoever? Everyone I have talked to about this views it as a complete abomination.
Some view it as a sometimes useful abomination, some don't care, and I'm sure there are plenty of other "some"s. If I could turn back time I would simply not add special &gt;&gt;= desugaring for do-notation.
&gt; Now I need to find that icon I made a while back that adds wheels to the Haskell logo to make a Haskellcycle. That would be cool!
That was an interesting presentation. Thanks for sharing! In a while, when we have more data to work with, we are going to start modelling the in- and out-flow of parking facilities. Then we can use these models to predict the number of free parking spots that will be available in the future taking your travel distance into account. I can't wait to start visualising similar behavioural patterns that Raphaël Cherrier showed in his talk and show the bicycle dynamics of Utrecht city!
Given how much everyone hates `fail`, I wonder if there isn't some way to get rid of it completely instead of adding a MonadFail class. That way you don't have to deal with a perpetual increase in complexity, such as the "MonadFail is a bad name if its a subclass of Applicative" problem that is mentioned in the proposal.
What about using `MonadError` instead?
`&lt;-` desugars via do pat &lt;- computation &gt;&gt;&gt; let f pat = more more &gt;&gt;&gt; f _ = fail "..." &gt;&gt;&gt; in computation &gt;&gt;= f
Maybe, maybe not. Worst case is we're paving the way for a deprecated `MonadFail` class here, allowing for a painless long-term transition.
I'm only 99% sure, but the desugaring of them is very similar to do-expressions, so they should get `MonadFail` constraints just the same.
I'm not a supporter of fail (I think GHC should just raise an unconditional error on pattern match failure, just like we do for any other partial function), but I think the proposal overstates the problem of having `fail` in the `Monad` class. We could just pretend it's not there without seemingly suffering any problems. I think most people already do. Introducing `MonadFail` adds some additional complexity to the desugaring and yet another migration (people who promised to help with the BBP migration haven't sent me any patches so far. My libraries still aren't fixed for BBP.)
If we don't give it a superclass we'll end up with a strange `Pointed` class, which seems to be universally hated due to its lack of laws. Putting a superclass constraint on `MonadFail` allows us to assert certain laws when interacting with the super-functions, and narrow down the use case to be pretty precise.
Thanks Simon, without [blaze-react](https://github.com/meiersi/blaze-react) we wouldn't have finished this project in time.
I would have had a post containing this on Monday, but some family stuff came up. If I've come up with a decent progression through the content you should have some more interesting puzzles to gnaw on in the weeks to come :) 
Thanks Edward. BTW [lens](http://hackage.haskell.org/package/lens) and [linear](http://hackage.haskell.org/package/linear) were very helpful in this project. 
Failing is not necessarily an error. See parsing!
I feel like the implementation of `permutations` should probably be considered here for this to have any real meaning :p Also you might mean `sort (permutations [0..9])`
Thanks for the links /u/AIDS_Pizza and for the content /u/katychuang. I actually found that site before, but I was confused because I didn't notice the theme-switcher at the top! (I guess it was a case of "header blindness"). For the record, I've done plenty of CSS/HTML stuff. Enough to know that I'm a pretty bad designer, and also I hate working on it, which is why I like having resources like these :) I am interested in blogs, but I also wonder if anyone has a decent template for documentation and/or books. It'd be absolutely lovely to have a page with browseable thumbnails / categories and a way to download templates without the content of some random other site. Wishful thinking, I know, and I certainly don't have any time to dedicate to such a project! :-(
awesome!
Dang it ._. I'll try and make a vector implementation, to see if it can get better
Hold on a second.. I thought the single most ugly thing in Haskell was `Num` and the rest of the numerical hierarchy. Other than that, I approve.
error in example? do x &lt;- foobar e &lt;- case foobar of Left e' -&gt; e' Right r -&gt; error "Pattern match failed" -- Boooo stuff Shouldn't it be: e &lt;- case x of Otherwise: +1, very cool, and thank you.
Very happy to hear that! =)
&gt; MonadFail is a bad name if its a subclass of Applicative Why not just call it "Partial"?
I'm for this in spirit, but against it in its current form, because the error can only be a String. I'd say that using `MonadThrow`, which has `throwM :: Exception e =&gt; e -&gt; m a`, would be much better, as it allows the throwing of arbitrary errors. The fundamental question is this: what is the error message supposed to signal and who is supposed to consume it? I argue that as far as the program is concerned, it doesn't signal anything specific, because the overwhelming majority of programs won't parse error messages. As far as they are concerned, there's only one way to fail, and the String is only there for fluff, which &gt; fail _ = mzero acknowledges. This begs the question of who is supposed to read it. I'd say "only the programmer". &gt; The String might help error reporting and debugging. It's fine if a programmer sees some error during testing, but as /u/heisenbug said, fail != error. Operations like parsing can fail in non-fatal ways and it's not fair to throw some non-localizable, beep-boop-error message into the end user's face if that happens. If one instead used `ParsingException Line Column ...`, one could present that in a friendly way, and the Show-instance would still be there as a fall-back. Lastly, you could have a `failS :: String -&gt; m a` for the lazy, defined as something like newtype MonadException = MonadException String failS :: MonadThrow m =&gt; String -&gt; m a failS = fail . MonadException You probably thought about this more than I have, but to me, it seems that using Exceptions instead of String wouldn't result in any more breakage, nor in any more complexity (due to failS), but would bring in all the benefits of full-fledged exception-handling.
&gt; You can already lie. Partial functions are allowed. They are? You can't lie and give it a total function type.
I really enjoyed the talk. Looking forward to part 2. I feel it could have been a bit faster paced, and focused more on concrete examples of how X technical decision enables Y useful feature of diagrams. Of course, I come from a graphics background. To use the opening example, I could tell that `aFunction` would draw a Sierpiński triangle, but I had to wait to see the better alternative from diagrams. I think the better sequence would have been “Here’s a pretty diagram. It’s called the Sierpiński triangle. Here’s how you express it in Asymptote: (big, ugly code). Here’s how you express it in diagrams: (small, pretty code).”
For a similar construction, see the free applicative: https://hackage.haskell.org/package/free-4.12.1/docs/Control-Applicative-Free.html
`mapM_ (const f) [1..x]` -&gt; `replicateM_ x f`
The release announcement is up and has probably the best introduction: http://galois.com/blog/2015/06/announcing-software-analysis-workbench/
That does not really make sense for `[]`, does it? Maybe I am missing things...
...is there a top 10 list of most ugly things in Haskell? =)
Thank you very very much. For me this is the killer feature. (Personally I find `add-source` really inelegant.)
I have to agree strings are fine for pattern-matching failures alone, but my comment was motivated by my belief that meaningful exception types would be the right thing to do™, as written [here](https://www.fpcomplete.com/user/commercial/content/exceptions-best-practices). The pattern-match-functionality could easily be subsumed by inserting `throwM PatternMatchException` or `throwM (PatternMatchException "Pattern match failure at...)` instead of `fail "Pattern match failure at...`, but, in addition to that, exceptions would provide a powerful and generally useful failure mechanism. Nothing would be lost and a lot would be gained. I suppose including a portion of mtl into base would be an issue, but if `Data.Traversable` and `Data.Foldable` made it in, we might as well bite the bullet in this case too.
Newbie here. What's ugly about Num?
Here's a direct link to the threaded view of the associated mailing list discussion for reference: ---- - http://thread.gmane.org/gmane.comp.lang.haskell.libraries/24910 ----
I use MonadThrow in IO, [], Maybe, Either, etc. It depends what kind of sense you're looking for.
I wonder why all this effort is not put in cabal-install and ghc-mod. As a Haskell newcomer I suffered a lot from getting up to speed with Haskell it's ecosystem (including ghc-mod) and changes between versions. Of course I can learn yet another tool but it would be nice if the Haskell world would choose one direction. Also with tools for cool IDE support. For example, this issue cost me a lot of time and it is not good for Haskell: https://github.com/kazu-yamamoto/ghc-mod/issues/417 I wrote IntelliJ plugin for Haskell (which depends on cabal (sandbox) and ghc-mod) and encountered a lot of problems. 
Yes, we're hiring again. - 1 in Singapore - 2 in London Work with /u/ndmitchell, Roman Leschinsky, /u/augustss, /u/gergoerdi, /u/malcolmw, Andy, /u/dreixel, George, Chris, me 
Many would prefer to build a more stratified numeric hierarchy atop things like monoid and ring and field. The current system is fairly ad hoc and is based on very informal laws instead of the existing, highly tested numerical hierarchy from abstract algebra. The upshot is that there are a lot of instances floating around of "`Num`-like enough" things which are at best poorly described by the current `Num` class
or Failable ?
It's an interesting suggestion, but you'd lose the ability to locally outlaw failable patterns in `do`-notation. I.e. let's assume that you want to use `throwM` in your `do`-body, but forbid failable patterns. Merging the `MonadFail` concern into the `MonadThrow` class would take away that possibility.
The very big downside of this is you now can't admit handling just this simple String case, you have to worry that they can throw _anything_ they want. You open the barn door wide by broadening scope to an arbitrary exception. With `MonadFail` you could work with something isomorphic to `Either String`. With `MonadThrow` it internally winds up using a GADT, Typeable, an Exception instance, etc. and you get more stuff coming along for the ride.
Which is fast, but pretty un-Haskelly :( It's a real shame that often to get the real speed, we have to start working with mutation.
Why not have class FailInfo i where failInfo :: i -&gt; String instance FailInfo String where failInfo = id class ... =&gt; MonadFail m where fail :: FailInfo i =&gt; i -&gt; m a Probably answering my own question: this only allows us to interpret the info as a string, other methods in `FailInfo` would be needed to extract other aspects.
If the language standard's main purpose is to be old and not keep up with de facto standard Haskell that everybody uses in practice, it seems to have little worth, to me. I understand the desire to adjust a solution to fit into the Haskell 2010 mold but if that mold is kind of crappy (string exceptions? seriously?) and not what people write in industry, I have to wonder: what are we *doing* here? (We're not the Common Lisp community, at least, whose spec hasn't changed since 1994.) But it's better than what's currently in Prelude, so at least there's that. 
&gt; With MonadThrow it internally winds up using a GADT, Typeable, an Exception instance, etc. That is true, but is it really so bad? `throwM` doesn't such much add a burden as a capability: the only way of handling a String is to show it; with arbitrary exceptions, you retain that capability, and gain the ability to do specialized handling of different kinds of errors, if you want. On the other hand, I admittedly don't know what sort of costs are associated with GADTs. Would there be other, unintended side-effects?
&gt; The very big downside of this is you now can't admit handling just this simple String case, you have to worry that they can throw anything they want. A string *is* anything. You're just shifting the meaning of the failure from compile-time to run-time.
While a nice one liner, I don't think that solution is in the spirit of the Project Euler problem... :-)
I think the language standard could stand to evolve faster than it has, but I do think there is merit in having both the advancing wave of current techniques that thrashes all over the place and a steadily advancing standard behind it. Putting something into the language in such a central place that fundamentally gets in the way of that strikes me as a poor trade-off. It cuts off that standard ever being useful and you win nothing or very little in exchange in this particular situation. All of the Foldable/Traversable, AMP, removal of Eq, Show as a superclass of Num, FiniteBits, etc. changes that we've moved into `base` have been made with an eye towards the fact that the little cottage industry of Haskell compiler writers out there can comply with those changes should we get around to reviving the language standards process, and all of them are more "in the wings" of the language than playing with the desugaring of `do`.
And now to define how `throwM` works in a language standard you need to add all of this machinery to the language standard. That is a _lot_ of work. And `fail` will be part of the language standard as it sits front and center in `do`-desugaring. My point here is this, you're requiring a larger type, and we have no way to really standardize the functionality you want. Each one of those would give me pause, but the combination largely kills the suggestion for me.
&gt; should we get around to reviving the language standards process Just for the record, here's a shy attempt (moving slowly) at trying to update the report: https://github.com/hvr/haskell-report/commits/hvr/h201x 
With that every invocation of "fail" in a module with `{-# LANGUAGE OverloadedStrings #-}` turned on blows up. I'd have no objection to adding a more robust method to the `MonadFail` class that included whatever members you wanted about the location of the error, though.
Do you mean that `MonadFail` should receive special handling during the do-desugaring, i.e. if you had a function like f :: (Monad m, MonadFail m) =&gt; m (Maybe Int) -&gt; m Int f x = do (Just x') &lt;- x -- failable pattern if x &lt; 0 then fail "x shouldn't be negative!" else return (x*2) and switched on a {-# LANGUAGE NoFailablePatterns #-} pragma, then the first line would generate an error, while the second one wouldn't? If so, I do see your point. It probably would be odd if `MonadThrow` with its `throwM` in lieu of `fail` received that sort of special treatment too. (Apologies if I understood you completely incorrectly.)
&gt; And now to define how throwM works in a language standard you need to add all of this machinery to the language standard. All right, I now see the seriousness of that issue. I had just naively read &amp; compared the types of `fail` and `throwM`, without considering their respective statuses as part of the language spec and a library function. Thanks for enlightening me :)
That said, I think there is definitely room in this proposal to add another member to the MonadFail class that takes more information about the location of the pattern match failure, and to give that a default implementation in terms of `fail`, so if you want more structure for that one exception it can be done in a nicely standardizable form.
I think it's quite rude to reply to "here's a tool that you can have for free" with "you should have worked on something else and given us all the results for free, instead". You're not being forced to use it. It's not up to anyone other than FPComplete themselves to decide what to work on. As to why a new tool was created instead of working on cabal-install, I would speculate that this may be because cabal-install has been around for a _long_ time and has assumptions baked-in which are no longer appropriate or desirable. For example, sandboxes were only introduced in 1.18. Starting afresh means you have a lot more freedom to design a good user interface without worrying about backwards compatibility.
This is amazing. Congrats! Makes me want to move to the Netherlands. :)
Hey I'm not at all qualified for this job, I was just wondering what would motivate you guys to write your own compiler? I think it's so cool to see a serious team so invested in Haskell, and I'd love to take a look at what kinds of things you guys have produced if that's at all possible! Thanks!
If you're asking "where are usage docs?" check out the readme and the wiki on Github. They can definitely use some expansion, which many of us are working on doing. If you're asking something else, can you clarify?
Sorry, it was not my intention to be rude. It was meant to be a question. Thanks for your reply.
Hopefully Halcyon, Stack and Cabal-install will benefit from each other. 
&gt; How does this compare to "GADTs meet their match They refer to it (p. 2. footnote 2), and it appears in the bibliography. 
&gt; highly tested numerical hierarchy from abstract algebra. Where can I find this ?
I think the main purpose of that complexity is enabling stuff like: catMaybes xs = [x | Just x &lt;- xs] But if we all embrace lens, that kind of use case is solved by `listOf` and folds: catMaybes = toListOf (folded . _Just) (Or you can generalize `_Just` to `folded` and have it work for all foldables. This kind of lens code has become so easy to write on the spot, though, that combinators like `catMaybes` are no longer useful. So perhaps, in a sense, `lens` obviates the main use case for Monad's `fail`!
If I understand how reddit works, the moderators can edit the title but I can not - it is one of the most annying aspects of reddit.
I'm unfortunately heading back to the states on the 16th of June.
Keeping with my tradition of double-announcing: I deleted the last post announcing SAW because I said "open source" when we've actually released it as non-commercial and people don't like surprises (to the point they get repetitive!). Also, this links to Aaron's blog post about it, perhaps giving a better introduction than the raw http://saw.galois.com page.
Sadly: in math books. I think there are a few packages which try to do it in Hackage, but no consensus on how to translate it to Haskell. If you're interested in learning more (from a math book) then I recommend the following one: http://homepages.math.uic.edu/~acamer4/aluffi.pdf
The un-Haskellness of it bothers me too. If we use the immutable type from the package we get something more Haskelly that runs in 0.3s on my laptop: main = print $ elems $ go (permute 10) 999999 where go p 0 = p go p n = go (fromJust $ next p) (n-1) We can also use the ST monad instead of IO and get purity with the same speed of the IO version: main = print $ elems $ runSTPermute $ do p &lt;- newPermute 10 replicateM_ 999999 (setNext p) return p 
Awesome. I really think we should have a Haskell meetup thing going on in NL, or are there any that i dont know about. 
Interesting...thank you.
There is also instance e ~ SomeException =&gt; MonadThrow (Either e) where throwM = Left . toException Weirdly, the ErrorT and and ExceptT transformers do not have a similar instance; they instead prefer throwing exceptions to their inner monad.
Very nice, congrats and thanks!
I amended the post with a console command to show the list of packages that would break because of bad desugaring. Note that this may contain false positives, so it's an upper bound. The cleaned up output is: cabal-src, control-monad-free, dns, doctest, easy-file, fgl, generic-aeson, gitlib-test, haskell-src-exts, heist, hflags, hledger-web, HUnit, jose-jwt, language-java, libgit, mime-mail, MissingH, mongoDB, neat-interpolation, network-multicast, network-simple, pager, pandoc-citeproc, persistent-template, pipes-safe, rethinkdb-client-driver, shell-conduit, singletons, socket-io, Spock, stackage-setup, stackage-update, stackage-upload, streaming-commons, th-desugar, x509-system, xenstore, xmlhtml, yesod-auth-oauth, yesod-gitrepo, yesod-static
[This talk](https://www.youtube.com/watch?v=hgOzYZDrXL0) by /u/augustss might answer some of your questions.
I think the idea was that a separate MonadFail would allow you to remove it from the context of the function containing the do notation to outlaw failable patterns and explicit fail calls without affecting unrelated throwM calls.
Not nearly enough ;-)
Is this ever going to be open-sourced? If you're not making money off it you might as well.
`withA :: (A -&gt; A) -&gt; IO ()` would be the simplest approach. It would Create, call the closure, and Destroy. But, it has remarkably limited utility. `withA :: (A -&gt; IO a) -&gt; IO a` is not what you want, due to the ability to return the `A` -- which could cause use-after-free bugs. `withA :: (A -&gt; IO ()) -&gt; IO ()` is no help either, because while you can't return the `A` directly, it can be stashed in a IORef and again expose you to use-after-free issues. Other methods include tying the life time of an A to be associated with a particular state thread (the parameter of `ST` that is eliminated with runST) and using finalizers to call DestoryA (which is "guaranteed", but not prompt).
Notice Cryptol and saw-core are both BSD so things have been transitioning that way for a while. I can't predict the future, certainly not for a project that isn't mine, so I'll leave everyone else to speculate as to the future of SawScript (the main non-commercial part of this work).
You can find a Haskell implementation of this more complex class hierarchy at: https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra.hs
The issue is more that you're going to incur a bunch of constraints that aren't sufficient constraints for the operation to be fundamental. You can of course 'check' a type that incurs the extra constraints that you use in the meta-theory to justify the canonical nature of a structure, but it'll infer type that is too general. This would happen even if we gave up on typeclasses and went with implicits like everyone else.
Agreed. Now that I moved back from Zurich to the Netherlands I miss my monthly [HaskellerZ](http://www.meetup.com/HaskellerZ/) meetups. Maybe we should revive the [Dutch HUG](http://dutchhug.nl/)...
Is this the right subreddit to ask about non-Haskell books? I almost reported your post as spam because your post looks like a clever way to get us to click on referral links to those two books.
Well, it takes the complexity outside of the core language, at least :-)
I think it is! Thanks so much. I love this sub. &lt;3 EDIT: To be specific it was [this](http://www.reddit.com/r/haskell/comments/2t4lba/haskell_design_patterns_extended_modules/?) discussion that I was remembering.
That's really great to see. Thank you. 
would proposals support integer literals distinguishing between negative and positive and stuff. like being able to safely write (0 :: Natural) would be cool. while (-1 :: Natural) becomes a type error like "no instance for Ring Natural" or something). 
I agree, this one sorts too: module Main where import Data.Permute import Data.List as L import Data.Text (Text) import qualified Data.Text as T main :: IO () main = print . flip (!!) 999999 . L.sort . perms . Just . permute $ 10 perms :: Maybe Permute -&gt; [Text] perms Nothing = [] perms (Just p) = toText p : perms (next p) where toText = T.concat . map (T.pack . show) . elems Takes 6 seconds in my laptop.
Note that you can defer type errors to runtime if you pass the -fdefer-type-errors flag to GHC. This flag turns type errors into exceptions which will be deferred as lazily as possible. This lets you prototype something quickly like you did with PHP.
If this is the major complaint, why not call this class `DeprecatedMonadFail` right away?
Running the tests that do exists for a given set of versions will in all cases give stronger guarantees than just guessing. You're making perfect the enemy of good.
Why?
Nice :-)
Though this kind of "straight jacket" is, as you seem to be implying, a good thing, I feel that I should also point out that this feeling disappears after a while, and then is replaced by exactly the opposite. I feel so ridiculously constrained in languages like PHP that it has become very difficult to relate to the point of view that Haskell is more constraining. 
Can I, using a web interface, connect to a haskell process and get a CPU profile, a heap profile or anything at all? 
Welcome to [Perlism #19](http://en.wikiquote.org/wiki/Alan_Perlis#Epigrams_on_Programming.2C_1982) The straight jacket thing made me think of Capoeira. It started with very movement limited slaves, turning constraints into opportunity for new ideas and possibilities. It would be worth a dedicated website to list this kind of experience. I too learned so much by using non mainstream languages. ADA, sml, haskell, Lisp being my first red pill. They reveal abstractions that you can see and use in other situations. And since most programming languages are close, even if you're back in PHP, you can reuse ideas and concepts. For instances, monads, untyped though, can be written in Lisp, JS, etc etc. Typed thinking is a gem among gem to think about correct logic. I often decorate python return statements with type hints so I can reason. Anyway spread the good words around. Or don't and let's enjoy or own tiny snobby circle :) btw: the "error conditions last" way bugged me so much, I searched for a 'negative' paradigm where you'd design error firsts, then fill in the working case. I think Nassim Taleb wrote about systems with this features. He argues that we're very bad at predicting, and thus we should spend more time trying to manage errors rather than to rely on potential correctness. ps: how did you run into haskell ? a tutorial, a book, and how long did it take for you to start doing something serious in it. I talk a lot but I actually never wrote anything in it. I always toy around, go back to lisps, and write actual code in python because .. libs (ho the sadness).
Me too. But I write the function types and names and get it all type-checked before I actually implement anything. I prefer to code only what I need. 
Word. Particularly, I feel like programming without the guidance of a very strong type system is ridiculously hard now. What did I forget? What might the outputs here be? Etc...
Haskell's constraints will set you free :) At first programming in it was so difficult for me as I had about 8 years of C experience with a few years of C++. Now I find things that Haskell makes principled and easy everywhere. Real world example: I have a friend who maintains a C++ doom port who brought up how difficult it is to ensure a function is reentrant, which is a property needed by the linedef part of the renderer in Doom. It immediately came to me that Haskell already gives us this for free via functional purity(IO shows us the function probably isn't reentrant)! Doesn't exactly help said friend, but I thought it was neat!
Which libs in particular do you have in Python that you don't have in Haskell? I'm still a noob, but I really didn't have problems finding (multiple) appropriate libraries for all parts of the Haskell project I just started.
To take your title to its logical conclusion, is working in PHP like programming while being insane? ;)
Write a parser for C++ :D
Are there any good posts about what this type of workflow is like? I've wanted the ability to say "shut up everything else, I'm working on this part", but I still want the type errors for the parts I'm working on. I try to get around this by running ghci on just the module I'm working with, but if it's part of a Cabal project (which it usually is), I can't do this because it's dependent on other modules from the project. I'd ideally like to be able to run "cabal repl Path/To/Module.hs", but as of cabal-install-1.18.x (which I'm using so that I can work with LTS Haskell), file targets aren't supported. Could anyone chime in on how they like to handle these types of situations? Also, as a bonus, is there any way of telling Cabal to use whatever packages you have installed on your system? When I'm working on a Cabal project, I often want to include/remove libraries while I'm iterating on an idea, but the process of: Add/remove dependency -&gt; cabal configure -&gt; Code; slows me down a lot. Sorry for the overloaded question.
&gt; I have no problem removing the links to the books. Now that you have proved you're a human, you can put the links back, they're useful :) I haven't read either book, so I'm sorry that my only contribution to this conversation will have been so unconstructive.
I'd imagine that this difference is because in python, the sort is working on the list as if it were a list of integers, not a list of lists of integers. Once you do this conversion, the compare operator suddenly becomes significantly more efficient and lots of pointers can get thrown away. when I run the naive approach with Haskell (with O3), I get 8 seconds and 1.5 gigs, and when i run the following, I get 4 seconds and 480mb. main = print $ sort (toInt &lt;$&gt; permutations [0..9]) !! 999999 toInt l = toIntR $ reverse l where toIntR [] = 0 toIntR (a:l) = a + 10 * (toIntR l)
Excellent point, I'm beginning to notice this too. At compile time you wear the straight-jacket of strict type-checking but runtime is a different experience, it's a strong breeze of JustWorks™ which blows away the trepidation I usually feel when trying some PHP code for the first time. PHP also gets worse as the codebase becomes more complex. Drupal (which is what I'm paid to use) has been making significant strides towards adopting OOP; the extra levels of abstraction and indirection in the codebase and lack of typing in PHP can lead to some obscure runtime errors. There are debugging tools to deal with it, but I'd rather not have to.
this implies that "crazy" people become sane when they are put in straight jackets, and that they are insane outside of straightjackets. this seems wrong on many levels, but i will meerly wave at them and leave the exploration to yourself
ghc now has this functionality with _ as any hole. Just fyi!
I'm happy to try to answer any questions. 
That still happens with `undefined`, no?
Question: What's the company's name? ◔ ◡ ◔
Having read Clean Code, I have come to the conclusion that it is a terrible book without any merit and should therefor be avoided in favor of better books. Every chapter is written by a different guy - who in his examples is not following the guidelines for what clean code is set out by the other guys, thereby making the whole book an inconsistent mess. That it's also a very Java specific book focusing on the problems Java creates doesn't really help matters. (Not that the general ideas in some parts of the book are necessarily bad, just not well delivered.)
Whoa, can't believe I didn't see that. I really hope to catch the next one. 
ah, Skedge! Y'all grew quick!
THIS IS AWESOME. Also, I had no idea that pattern matching in monad syntax desugared to that.
Well, since the title already says spoiler... import Data.List perm :: [Int] -&gt; Int -&gt; [Int] perm [] _ = [] perm xs n = let (q, r) = n `quotRem` product [1..length xs - 1] x = xs !! q in x : perm (delete x xs) r main :: IO () main = print $ perm [0..9] 999999 Running it in ghci: λ&gt; :set +s λ&gt; :main [2,7,8,3,9,1,5,4,6,0] (0.00 secs, 518168 bytes) λ&gt;
I'm a little disappointed it isn't actually megacorp.
Programming with a nice strong type system, you can kind of tune out and go through the motions. If you make a bad assumption, you'll get feedback on it.
&gt; ... I also wonder if anyone has a decent template for documentation Here's a long list of documentation examples and generators ([beautiful-docs](https://github.com/PharkMillups/beautiful-docs)), as far as I know, there aren't any haskell based documentation generators. 
I think you are confused about what we mean by curation. What you are describing is the normal semantic versioning and dependency specification process that goes on with most software today. And that of course is a good thing that everyone needs to keep doing. Curation goes beyond that and tests that certain packages work together beyond just the version bounds (for example by building and running the test suites of all versioned packages). This can catch many things that version bounds cannot, for example if one package was relying on buggy behavior of another that got fixed without a type change. Curation necessarily places restrictions on versions because testing all possible combinations of dependencies takes extra resources. At a minimum it takes time to run tests, so the just released dependencies are always going to have some delay before they end up in a curated collection.
Two important things it does are 1. base your packages on stackage LTS (Long Term Support) by default, which means all the package versions are compatible, avoiding "cabal hell" where you can't install packages, and 2. make 3 layers of packages: a few core ones, then the stackage set, then your added ones for a particular project on the top in a "sandbox", preventing them version-clashing with ones from other projects, avoiding "cabal hell". It automates a lot of things for you so you don't have to (learn to and) remember to do all that manually. 
I was asking if they were referring to ZDoom, not what ZDoom was.
Guess I'm the jackass then. Sorry!
That's great! Now if they can get it to work with the Haskell IntelliJ plugin I'll be totally sold!
But that would imply that people using dynamic type systems have lower inherent skill levels than those using strong static type systems. I can't stand behind that implication, which is why I didn't mention that effect in my comment.
Maybe the mapConcurrently introduces potential blocks, races etc that skew the results. Even if not, an IORef call is a dozen nanos or so, and any GC is going to splat comparisons. Removing the concurrent complexity (tweak [here](https://gist.github.com/tonyday567/026e26bfc1623309535c)), it looked a lot more stable, with IORef &lt; MVar &lt; TVar &lt; TMVar (at least with the writes I checked) 
Yes, when you run everything in a single thread, then you will get the result you describe. I personally would never use Concurrency primitives in a single-theaded environment though. The reason `swapMVar` is slow is that it isn't atomic, while the other operations are. If you ignore this one case, the result is consistent with your simplified version. In either case `atomicModifyIORef'` is way slower than `modifyTVar` and `modifyIORef'` is just unsafe.
I believe they are talking about adding types for things that are undefined so that they are strongly typed to the correct type. undefined will check to any type (although only one).
Mostly because you can't get everyone to change their libraries in a short time frame and you can't have a single person do it because of the variety of locations and access rights to source repositories.
FYI, augustss is responsible for implementing the first ever haskell compiler, and the (semi-serious) running joke is that reimplementing haskell compilers is his hobby, so that'd help explain the cost benefit ratio of writing your own compiler ;) 