I haven't watch this myself yet, but you may find the following useful... http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml You may need some prerequisites first. A likely one would be the MIT 18.06 Linear algebra course... http://ocw.mit.edu/courses/#mathematics I watched an earlier version (I think) of this course and it was excellent. Stanford IIRC has released video courses in linear algebra and convex optimisation techniques. I don't know much about this field - do some sanity checking before committing time to these courses, as some (perhaps most) of the content is likely to be irrelevant to compressed sensing.
Personally, I'd like to use the applicative style here: logLine :: Parser LogLine logLine = LogLine &lt;$&gt; (plainValue &lt;?&gt; "IP") &lt;* space &lt;*&gt; (plainValue &lt;?&gt; "Ident") &lt;* space &lt;*&gt; (plainValue &lt;?&gt; "User") &lt;* space &lt;*&gt; (bracketedValue &lt;?&gt; "Date") &lt;* space &lt;*&gt; (quotedValue &lt;?&gt; "Req") &lt;* space &lt;*&gt; (plainValue &lt;?&gt; "Status") &lt;* space &lt;*&gt; (plainValue &lt;?&gt; "Bytes") &lt;* space &lt;*&gt; (quotedValue &lt;?&gt; "Ref") &lt;* space &lt;*&gt; (quotedValue &lt;?&gt; "UA") 
Maybe I'm a little cranky about it, but It seems to me that DrRacket seems to much a Toy. I feel that ghci is more work/profesional-oriented.
I can definitely see the attraction in using the applicative style, but it's also a little bit scarier if you're unused to the symbols used. Do notation is something even the newest haskellers are exposed to early on, and has the added "bonus" of resembling imperative languages (although as I say that, I wonder if it's really a bonus in the long run)
Actually, in the next part I'll show how to do exactly that. It works just fine, done 2GB log files. Parsing one line ;)
You can use attoparsec with do notation
Parsec 3 got support for Bytestrings, but it's almost unusably slower than Parsec 2. However there was a sped-up branch somewhere.
 getIP isn't this a very un-haskellish name? It's only the first part that makes me think you're not approaching the problem from a functional mindset, which is why the code is so ugly.
Or just $ say $number ;-)
readFile is lazy, it only evaluates what is actually used. On first parse it evaluates the head of `lines file`
The idea of MonadThrow would be so that you could "throw" exceptions in pure monads too, by putting them into an Either for example. The version of throwIO in lifted-base only allows you to throw exceptions in IO-like monads. This is annoying, because this should be what the MonadError class from mtl enables you to do, but it is made unusable because of its stupid Error type class which is a hack to work with fail (which is also a hack). Lots of packages implement something like MonadThrow, but all are tied to other parts of the package or have some other quirks about them. I don't think there's a standard solution to this problem.
At the least I will compare them on a large log file, I have just temporarily misplaced the one I used before and I don't remember any numbers.
I'm planning on making ResourceT a separate component in the next release. I've made some comments at length elsewhere about why I don't think regions is the right fit here. Basically: a goal of regions is to statically ensure that a resource is not used after it has been released. As a result, regions cannot release resources as early as possible. ResourceT does not provide that static guarantee, and as a result, we can release resources as soon as possible. There's also the issue of the added type complexity in regions. duairc gave a pretty thorough explanation of the MonadThrow situation.
no computers will be programming themselves in microcode abstractions are for humans we will not be capable of understanding the machine-created code 
The Lisp people write now is very different from the Lisp that people wrote 40 years ago. It's like saying that people still write Algol 50 years later because JavaScript syntax looks vaguely like Algol. I predict that in 100 years, people will still be writing in Algol-syntaxed languages, but we'll think that higher-order functions, combinator libraries, and partial application are as basic building blocks as arithmetic operators and function calls.
Visi is built with Haskell, for anyone not already aware.
 It may be worth mentioning more context: As far as I understand, David Pollak wants to build a system similar to the celebrated HyperCard, but with a sane programming language called Visi (purely functional, type inference, Haskell-like syntax) and updated to the 21th century, i.e. cloudy websites and stuff. I love the choice of language, though I'm a bit sad that it's the GUI stuff probably won't be backported to Haskell. 
I agree with the fact that the Error typeclass is useless, but one can easy workaround it by defining a fake instance, ignoring the String passed to strMsg. It's different for "fail": it's needed when pattern matching fails (but this has already been extensively debated on Haskell-Caf√© \^\^).
I don't know for Haskell, but in 100 years maybe C++ will have at last standardized garbage collection...
Good catch, updated it.
Couple of things: - Visi is not simply HyperCard for the iPad, but development beautifully re-imagined. That means that we're going to take the best of what has been done before and add our own thoughts to make something that enabled 2 orders of magnitude more people program computers. - All of Visi is open source (Visi.Pro, the service will not be open source, but will build on Visi and there will be substantial give-back to the Visi community). So, once I get the ObjC Bridge/GUI stuff running (I'm working on it in concert with the language definition), it'll be part of Visi and open source. However, it's looking like much of the Visi GUI stuff will be written in Visi (a goal of the language is to be self-hosting like Smalltalk). On the other hand, one of the medium-term goals of Visi is to support transforming its own typed lambda calculus into GHC intermediate representation so it's conceivable that Visi code could become Haskell libraries.
yeah... me too. If you've got a better idea, please join the discussion at http://groups.google.com/group/visi-lang
Concerning the GUI stuff, I was thinking of the lower level ObjC Bridge, not necessarily the Visi libraries on top. Are you saying that the ObjC bindings are not written in Haskell, but in Visi directly?
Haskell has seeped into laboratory work here in france, though I don't have a large industrial-strength example to present, but you can see [this](http://www.glyc.dc.uba.ar/intohylo/) for instance.
Are there any plans to support other platforms?
All opinions welcomed. I possibly got a bit carried away here. Does anything like this exist? I know of [Scratch](http://scratch.mit.edu/), which is deeply procedural, and other similar things. I've been wondering how one might go about teaching FP to people as young as possible. It occurred to me that quite young people can understand flowcharts and spreadsheets, and FP is in some respects quite similar to those. So this was an attempt to build on both of those to resemble something like a polymorphically-typed lambda calculus.
Great. I always wanted to program like the masters at Lascaux.
Reminds me a bit of of [this notation](http://www.cs.virginia.edu/~evans/cs655/readings/mockingbird.html) for the lambda calculus (by David C Keenan). He also proposes a way to animate it for demonstrating reduction.
I agree that no interface is automatically obvious, but it's just an easy way to check if people can "grok" it rapidly.
Ooh, this looks fun. I'll give it a read.
Although not exactly a functional approach, after some experience with Max/MSP and PureData, I have to say that I am _not_ convinced that visual programming makes things easier. It is nice for understanding very straight-forward pipelines, but when it comes to more complex structures I find it neither easy to express nor easy to read. You always find yourself spending more time following lines to boxes, trying to figure out what is going on, than just directly writing down what should happen. I can see the appeal for new programmers, as it is definitely a friendlier introduction to programming, but eventually when things get complex, it seems to just make it easier for them write spaghetti. At the very least it takes every bit as much discipline in terms of being careful break off bits of code into abstractions and functions, or before you know it you just have lines everywhere.
...while I do think visually about code, not *that* visually. 
As has been said to GUIs, "it makes simple things easier, and complicated things more difficult."
It seems very similar to Apple Quartz Composer. This is an application where you program by drag and drop filters. Its role is to make animations. One advantage of such an IDE is the ability to type check during the creation of the program ; even before compilation. One limitation is the ability to use recursion directly. In these case, written code is generally clearer. In the Quartz Composer, to achieve this, you could add JavaScript filter. I recommend everybody to give a try to Quartz Composer. It really show how IDE can help to program. One other advantage I see, is it will promote the use of higher order function as they would be easier to use than to write a recursive function. It would certainly be great to see such an IDE applied to a language as powerful as Haskell. 
The API of the numerals package is completely ASCII. I also have no problem with people submitting patches that are plain ASCII. Being able to type non-ASCII characters is not a requirement to use the package or to help develop the package. Do you have a problem reading the code on your system? In that case it may help to install a font with better Unicode support.
Btw, I finally actually tried the projector-per-constructor plate yesterday. It doesn't work quite as well as with regular Multiplate, because you can't strip off the constructor without `mkPlate`. I.e. `expr plate (Add e1 e2) = add plate e1 e2` has to become `expr plate (Add e1 e2) = add plate (Add e1 e2)`. But because a plate is just a function in the version of Multiplate, you don't need a project per constructor, because pattern matching works much better. See f.e. `varPlate` here: https://gist.github.com/1919528.
Hi David, that looks nice and undoubtly useful, so I wish it great success ! The good support for literate programming is, IMO, a must for academic acceptance. One thing that I wonder though is why a new language ? Especially since you seem to be comfortable with haskell combinators. Chapter 2 mostly says that inferior languages are not innovative enough and therefore not up to the job. But you don't mention haskell, which has undoubtedly a steep learning curve upfront, but has a lot of ready-to-use material for spreadsheet-oriented computation (non-strictness, FRP, Enumerators and more). So why did you choose to make a whole new language, instead of providing very high level combinators (and maybe syntactic extensions) ? PS : target page reads "the visi lanUgAge" at the moment :)
Yeah, the pink boxes are definitely the wrong way around. Things on a small scale should flow in the same direction as things on a large scale. Referential transparency helps here, in that you can "zoom" to any level and still largely understand what's going on without having to refer to other functions.
This reminds me of the box calculus developed for Hierarchical Hume by Michaelson and Grov.
This is a pretty active research topic. For a formal graphical notation (i.e. it's a logic, we can reason with it and about it) with the same spirit as your drawings, google Interaction Nets. Interaction Nets are flat (no nesting). See also Robin Milner's [BiGraph](http://www.sciencedirect.com/science/article/pii/S0890540105001203) notation, which includes hierarchy, and is a more general graphical notation for modelling functional calculi -- not just lambda calculus, but also the pi calculus etc.
Hindley-Milner next, plox :)
When hacking on the package, you can't really access the variables without cumbersome copy &amp; paste. I guess if the API is reasonable, it matters less what goes on inside the package.
If you haven't already done so, you should take a look at "dataflow languages". They seem to base on an idea similar to yours.
It is called LabView. (And I hate it). Though it might be better if it was more aware of the semantics of functional programming instead of dressing itself up all the wrong metaphors, like Labview does. 
It compiles without error on my whiteboard ;-)
You did a great job. Thanks for sharing it.
Agreed. The best prototypical example is probably the Redstone machinery in Minecraft. It always goes like this: * It's visual, so you can see how the data flows. * It's straightforward to create simple circuits like switching a light on from afar. * You can make more complicated circuits, but soon you will have to manage spatial layout and draw a lot of connections. * It's hopeless to try large projects, but some people succeed anyway and upload a Youtube video. The most important point is that figuring out how to create complicated logic by arranging primitive building blocks is actually a fun and rewarding activity. The crux, however, is that it rewards the wrong behavior. 
a couple of things: * The ratio symbols/screen_area in very low when compared to text * After you reach the third layer of function calling you have some possibilities: show only the name of the called function, losing the visual representation; show all the functions nested, making the block size explode; show the web of calls, obtaining a mess of wires. * How do you represent macros, quotations and quasiquotations?
I was going to suggest maybe INRIA, but you've already covered that.
&gt; ivoritoweritis Did you mean: _peritonitis_ -- Wikipedia
From what I've heard/read, despite seeing more and more French haskellers on IRC/haskell-cafe etc, there doesn't seem to be much enthusiasm for using Haskell in a professional setting. I would be HIGHLY interested in a Haskell-powered company, but don't see anyone actually willing to start one. And I would be really glad to join one after my studies are over, but it seems unlikely there'll be one, even by then.
I think this approach, notwithstanding your list_case function, doesn't have pattern matching. [The view from the left](http://strictlypositive.org/view-Dec6.ps.gz) says we should use patterns (or more generally views) to refine our variables on the left.
This ordering leaves you with nothing but Agda... so there is no turning back ;o)
I think I pretty much agree with you, but &gt; And yet I'm not even a fan of UML, except as something that a tool like Doxygen can generate for you from the code. And even then, those diagrams can easily get too cluttered and noisy. I don't understand the "even then" there. Of course generated UML is going to be cluttered and noisy, *because* it's generated, and so you didn't have a thinking person making decisions about what to include and emphasize. Generated UML is going to be a failure for the same reason that visual programming is questionable: because diagrams are well-suited to selective presentation, but do not scale to including all relevant detail.
Your homework is to take a non-trivial Haskell module off of Hackage and convert it to your notation, by hand. It doesn't have to be the largest module, but it does need to be more than something like "12 four-symbol functions that ought to be in Prelude". Then tell me if it's actually an improvement. You'll learn more from doing that than any amount of debating here. (And I really mean that, you will learn all sorts of things, no sarcasm.)
&gt; Unfortunately you seem to lose some information in how you handle constructors--it's not visually obvious that one input to `list_case` will be forced in the process of choosing between forcing the other two. Perhaps case selection warrants being displayed differently from other functions? Agreed; I was getting tired by that point. I might [look at the GHC Core](http://hackage.haskell.org/trac/ghc/browser/compiler/coreSyn/CoreSyn.lhs) `Expr` data type, which in addition to abstraction and application has `let` expressions and `case` expressions (and other stuff I might avoid -- type expressions for passing to type-lambdas, casts, and coercions). `let` expressions might be less useful here -- their use is basically "name it to share it", but arrows seem to already have sharing covered. So in short you're right that the important missing piece is the `case` expression.
I would run a few tests with both binaries. With a large dataset, it's possible that the first test was dominated by disk IO, whereas the second test, the data had already been slurped into RAM by the file cache. A couple months ago I reduced a programs startup time from 16 minutes to under 30 seconds by doing a "cat bdbfile &gt;&gt; /dev/null" before the program started.
Oftentimes nor do I to tell the truth. :) I wonder whether that's just because I code in text, or because text is better for thinking in.
&gt; How do you represent macros, quotations and quasiquotations? I wouldn't consider even beginning *trying* to think about that.
So how long did the `cat &gt;&gt;/dev/null` take?
Why `&gt;&gt;` and not `&gt;`?
Or [this](http://www.lix.polytechnique.fr/dedukti) :)
You're right, it doesn't, but I was thinking in most-simple-thing-that-would-work mode, which would be some variant on lambda calculus.
That's pretty much the pep talk i gave our new IT manager when he asked me why do we use haskell.
That's rubbish! Rubbish I say! Not even a termination check in there :)
Yeah, it's possible that this is a good test. This is such an extraordinary speedup though that it would be nice to see better evidence that it wasn't just disk IO.
I guess this makes sense if you are used to it but for those who aren't this is confusing. Each line has way more "symbol noise" (e.g. &lt;?&gt;, &lt;* and &lt;*&gt; compared with just &lt;- per line of do notation). Also what are the words in quotes (e.g. "IP", "Ident" etc) needed for ? They seem to be throwaway. One of my main beefs with Haskell is that, for me, the next step up from the beginner code examples goes straight into harder to read obfuscation of the same ideas. Real World Haskell does this to me again and again.
Presumably, it isn't that 7.4 is doing something incredibly smart but likely the case that 7.0 was doing something incredibly stupid?
&gt; Companies, apart maybe consultancy shops like Well-Typed, don't advertise themselves as being a Haskell company, but more as a company doing product XXX" This is IMO true if those companies doesn't seek to grow, i.e. if they only look for customers and not new developers to hire. (Which is, cynically, the drawback of progress: the easier time you have to do stuff, the less people you need to do it) &gt; And Haskell is not viewed I think as a language to use in a professional setting, it was more meant as a test bed for ideas on programming Do we agree that it's the way mainstream people may think about Haskell and not the way we do? Don't you see Haskell fit for your daily job? &gt; and not wait years and years for Java to get closures As if it makes any kind of difference now that it has... Closures without good combinators and good ways to express other combinators are just marketing arguments, not real assets. Why wait for a language to be *like* Haskell? We *have* Haskell! LISP introduced garbage collection more than 50 years ago, and still it hasn't made its way into C++. We can wait a long time...
Well, at least the &lt;?&gt; part is optional. It just annotates the parser in case something goes wrong and thus gives a meaningful error message. What is compelling about using &lt;*&gt; et al is that they are defined for any Applicative instance and have a certain meaning. You learn them once and can use them all over the place.
To me, it looks just like *regular* fmap... in any case, not bind. 
I WAS HERE
Surely you mean "to *decrease* FF startup time"? ;)
Where and what are you studying ? (just to have an insight of where the french haskellers might come from) Personnaly I just graduated from INSA Rouen.
&gt; We cannot afford QA teams, so we use a sadistic compiler. ...should we rename **GHC** to **SHC**? *Sadistic Haskell Compiler* has a nice ring to it... =)
Fixed!
It's just the correct type for the data you're processing. ByteStrings represent strings of bytes. Text represents strings of unicode characters. A unicode character can be transformed to a string of bytes in many ways, and most characters use up more than one byte. This tends to lead to problems if you do things involving length, but also filtering on characters (which are translated to bytes, which might be part of a multi byte sequence). Note that if you are not treating text as text, but just streaming it, it is fine to use ByteString (and probably a little bit faster). But the first instinct should be to use the right type, and then, if you understand why it's ok, use a different one to speed things up.
Same here, all my big files are stored in /dev/null. I don't know the algorithms behind it, but the compression ratio is truly amazing.
There was a thread a few weeks back about cairo being slow, which was related to ffi IIRC, and which was fixed. Maybe this is the same thing here?
Heh, another problem I solved for myself and then learned was better solved on Hackage already. Haskell workflow used to be "think really hard and have a long conversation with GHC, then compile code, then run, then done." These days, it's "think hard, trawl hackage, cabal install, write a little code, run, done" `(.******)` could be made more compact and easier to count if one would adopt the pattern of using ":" to mean `**`, so `compose7 == (.******) == (.:::)` or: (.:) (.‚àµ) (.::) (.:‚àµ) (.‚àµ‚àµ) or even: (.‚†è) (.‚†ü) (.‚†ø) (.‚°ø) (.‚£ø) Hee hee. 
s/adicity/arity/ ?
I hate to be that guy, but this blog post has gotten monads all wrong. The type of hamon2 (which the author says is equal to bind (&gt;&gt;=)): &gt; hamon2::(Monad m)=&gt;(a-&gt;b)-&gt;m a -&gt; m b is plain wrong. That is the type of fmap (i.e. liftM). Furthermore, this does not seem to be a typo. The author consistently confuses fmap with bind: &gt;I proceeded to say (on the whiteboard) that ‚Äúv &gt;&gt;= f‚Äù is this in a pseudo-code that looks like Python: &gt; if v has something in it: &gt; return f applied to what is in v The above pseudocode is **not** the Monad (bind) instance for Maybe but the Functor (fmap) instance for Maybe. And it is certainly not the pseudocode for all monadic binds (as the author seems to imply). I mean, it's nice people are excited about haskell, but this post looks like someone confused Monads with the Functor instance of Maybe. EDIT: formatting EDIT2,3: &gt;I proceeded to say (on the whiteboard) that ‚Äúv &gt;&gt;= f‚Äù is this in a pseudo-code that looks like Python: &gt; if v has something in it: &gt; return f applied to what is in v Can't represent fmap and can actually be interpreted as a bind for Maybe if f has the type f::a -&gt; Maybe b but this is far from obvious while reading the text. Only to get more confusing with the following implementation of bind (i.e. fmap?): &gt; hamon2 f (Just x) = Just (f x) &gt; hamon2 f Nothing = Nothing
Ya that makes sense, but as I never really worked with it as text it never even occured to me (as in I didn't see it as text, just bytes). Will def take a look at that
Cool. I was just thinking, if it took significantly longer than (16 minutes - 30 seconds) then it'd be worth doing as a general tactic. The fact that cat uses large chunks for reading, whereas most programs don't, is surely a major factor here; especially if the program uses non-linear access. I wonder if there's a good way to quantify when it'd be a win, rather than just testing to see...
See also the [sec](http://hackage.haskell.org/package/sec) Hackage package.
You have to take into account what else the machine is doing. If your ginormous log file parser isn't time critical but your high availability service that does lots of file access is, then you don't want to be trashing your file cache with log data. There is a big tradeoff here similar to swapping. In my case, this was the only program running on the server, so filling up 2GB of file cache to get it started was a big win.
re: maybe Go Maybe Rust too.
Color me unsurprised that unsafeCoerce is id for the type a -&gt; a :3
 undefined :: a undefined = unsafeCoerce unsafeCoerce That's a deficient implementation of undefined, surely. I might write the following and then forget about it when I don't get errors! myUnsafeCoerce :: a -&gt; b myUnsafeCoerce = undefined -- implement later 
Cool. I've never used EclipseFP; perhaps I should give it a try.
what about fixSTM = unsafeCoerce fixIO ?
My gut reaction says no, but Haskell has a good shot at being in the evolutionary tree leading to the languages used in 100 years. The reason I don't think Haskell will be around for a long time has to do with the community: I think we're more loyal to the ideas behind Haskell than the language itself. If someone came along with a new language that does what Haskell does only better, a lot of us would jump ship. (In my case, I've already made a similar jump once from Ocaml to Haskell.) It seems worth throwing a link to Paul Graham's essay into this discussion: http://paulgraham.com/hundred.html
Anything involving dependent types. You see e.g. Agda and Coq scatter `unsafeCoerce` all over the place when generating Haskell. `unsafeCoerce` doesn't help performance, btw, it prevents GHC from doing quite a lot of optimisations, so if you ever do such kind of stuff, please consider not using it *everywhere*.
In what sense is `treeToLists` the right function? import Unsafe.Coerce newtype Fix f = Fix (f (Fix f)) data Tree a = Leaf a | Branch (Tree a) (Tree a) deriving (Eq, Ord, Show, Read) data FixTree = FixTree [FixTree] deriving (Eq, Ord, Show, Read) treeToLists :: Tree a -&gt; Fix [] treeToLists = unsafeCoerce unFix :: Fix [] -&gt; FixTree notSensible :: Tree a -&gt; FixTree unFix (Fix xs) = FixTree (map unFix xs) notSensible = unFix . treeToLists *Main&gt; notSensible (Leaf 3) FixTree [] *Main&gt; notSensible (Branch (Leaf 3) (Leaf 4)) FixTree [] *Main&gt; notSensible (Branch (Branch (Leaf 3) (Leaf 4)) (Leaf 5)) FixTree []
Of course it's a deficient implementation, the whole thing is a joke really.
Yeah you're right, I'd probably use Text if I wrote it today. Thanks =)
No, seriously \^\^ I still can't find what this word means.
&gt; I don't think the order of constructor definitions is in any way an API with any stability guarantees, so just don't do this. I think that it's already accessible from the `Typeable` instance.
I fail to understand why the author calls them futures. [Futures imply concurrency](https://en.wikipedia.org/wiki/Futures_and_promises); and what I've seen is just a syntactic form for continuation passing style.
In fact, there are many cases when this definition will magically do the right thing depending on context! Of course, it might just segfault, instead.
Emacs Lisp (without lexical scope).
&gt; Do we know a language that provides first-class functions without closures? C. They're called "function pointers" but have all the relevant characteristics of first-class functions. C++ as well, for suitable definitions of "C++".
In Scheme you can use (escape) continuations to give you break/continue, like this: (let loop () ...stuff... (loop)) ; this is a simple loop (let/ec break (let loop () ...stuff... (when foo (break)) ; jumps outside a loop, aka break (loop))) (let loop () (let/ec continue (when foo (continue))) ; jumps out of loop body but still inside the loop, aka continue ...stuff... (loop)) Note that "loop", "break" and "continue" are just names; they have no extra meaning in the language. It's not a very common thing to do though, just a possibility. edit: oops, my continue example doesn't actually continue the loop...
`odd` isn't it?
Well, you do need some sort of flow control beyond what a simple fold would give. A simple example would be something like this: data For a = Continue | Break a | Step a for [] z _ = z for (x:xs) z act = case act z x of Continue -&gt; for xs z act Break r -&gt; r Step z' -&gt; for xs z' act foo = for [1..] "" $ \accum iter -&gt; if odd iter then Continue else let i = show iter in if length i &gt; 3 then Break accum else Step (i ++ accum) Note that `for` is very similar to `foldM`, if you treat `For a` as something like `Either a (Maybe a)` and combine the `Maybe`s with `mplus` at each step. This is really the same in spirit as the Scheme example, but with only sum types for escape rather than continuations.
I don't think so. `(* 2)` is shorthand for `(\x -&gt; x * 2)`. `2` is just a constant, it's not taken from an environment.
Mu [a] wouldn't typecheck. Also, Tree has two constructors. And why would Tree be Cofree [] a?
Maybe, because a future contains a value which is not available yet -- makes sense now? Maybe you can elucidate your point, I don't get it.
There are two ways I know to add break and continue: * Write the iterator in continuation-passing-style, and pass the break and continue continuations to the inner function, so it can choose to use them if it wants to. * Add non-local returns or "blocks" to the language. This lets an inner return statement in the loop function break out of the iterator.
But C doesn't allow for nested functions. This takes it a bit towards the "non-first-class" functions side of the spectrum.
But this kind of transformation is less flexible. You can do some pretty complicated stuff with just a few lines inside a for loop (and this is both why it sucks and why it is so powerful)
thanks! 
I'm not sure I'd call them first class -- they can't be created at run time
In Haskell, you can use the MaybeT transformer, and then you get a "break" in a regular "forM". If you want continue you can nest the iteration code in another layer of MaybeT, and breaking out of that is actually a "continue".
Dojo's `.then()` acts as both *fmap* and *bind*. Is Jquery similar in this aspect?
Given that I've done that, I disagree. On the other hand, I did that using some rather grungy assembler slinging, and one could argue that implementing something that perhaps aught to belong in the compiler is ... stretching it. (What I actually is is use a couple of templates of assembly to compose function pointers in certain ways. Today, I would call what I did an attempt to implement currying; but then today I would just do it in Haskell)
Because it seemed easier than doing it in Bash? :)
&gt; If you put the code for those tests in the module and run ghci with -fobject-code you get the result you'd expect. Well... you get a `notSensible` function that isn't constant. I'm not sure I would go as far as saying that its output is what I'd expect, though.
To be.pedantic, certainly (*) comes from a surrounding environment. However, its definition is known statically, so a compiler is still unlikely to generate a closure.
Okay, then you get the results I'd expect. ;)
Right -- closures enable meaningfully different functions to be produced at run-time. Hence Ywen's point.
I'd say a better example of why the for loop can be undesirable is SQL and why user-defined aggregate functions tend to be expressed as left folds and not for loops. For one, this ensures a particular access pattern to the result set, and enables tupling of multiple aggregates so that only one pass is needed. But of course, none of this is particularly related to closures.
nice, but my version is more helpful to the typechecker: *Main&gt; :t (,,) `mcomp` show (,,) `mcomp` show :: (Show c, Show a, Show b) =&gt; a -&gt; b -&gt; c -&gt; [Char] *Main&gt; :t (,,) &lt;$$&gt; show (,,) &lt;$$&gt; show :: (EndFmap (a1 -&gt; String) a (a1 -&gt; z) (b -&gt; c -&gt; (a, b, c)), Show a1) =&gt; a1 -&gt; z
I guess you don't: [http://hpaste.org/64576](http://hpaste.org/64576)
I suppose. Except .then in jQuery returns the original deferred object, not the new one, so you can't chain. e.g.: A.done(f).done(g) will call f and g when A is done, not f when A is done and g when f is done. You have to use pipe instead. It was counter intuitive to me.
A big difference between continuation passing style and promises, as far as my experience with them in Javascript goes, is that promises store their values and let (multiple) callbacks be attached to them *after they have been created*. This is particularly great for stuff like caches and object stores.
I can't put it better than what is provided in the wikipedia page I linked, but the semantics of javascript (and lack of baked in threading) don't let you have implicit or explicit futures. With futures, you have the guarantee of value when you need it, whereas with the async code provided you can't return the value at any point in time you'd like. If they where actually futures you could have something like this just fine: var futureValue = future(ajax(url, success: function(result) { return result; })); if(someValue === futureValue) { // would block up until a result is available }
Potassium is more reactive because its outer electron is further away, hence more likely to escape.
Java has had closures for a very long time, what is being added in Java 8 is a nicer syntax for them (one that doesn't require the creation of an anonymous class and having to declare the closed variables final). Also, the article is from 2007, a lot has changed in Java land since then. 
Try implementing function composition (i.e., taking two functions and returning a function) in C.
[GitHub link in case you went straight to the comments section.](https://github.com/mikeplus64/Level-0) This is my first "full" Haskell project, so sorry for the messiness :).
I can't help feeling that it's quite a lot of words to say that `map (f . g)` may be more efficient than `map f . map g`. Note that this manual optimization could be obliterated by a different representation making it unnecessary -- like Repa turned matrix-transform optimizations into nothing. In the "package it up" section, you could request users to register their element-transformation functions such as `setFoo` in a data structure (list) rather than a function; `$.fn.a` would dynamically compose the functions in this list, and map the whole thing. Then composition of transformations would just be function list concatenation, and the fusing wouldn't need to be done manually. (Of course, in the general case you want to allow several ways to specify transformations, stackable element-transform being only one of them, and you end up with an algebraic datatype and explicit optimization passes on it.) 
As far as syntax is concerned, isn't plain _ ok to use since it doesn't mean anything on the RHS? Personally, I'd go for something like an "anonymous lambda" syntax: "\\-&gt; f _ y _" == "\x z -&gt; f x y z"
I like the idea of anonymous lambda. Perhaps this should be generalized a bit to also work on normal lambdas, `\x -&gt; f x _` == `\x y -&gt; f x y`. Otherwise, it can be confusing what the scope of an `_` is. Or it could just be an error if blanks are used inside a normal lambda. Another minor problem is that right now `\-&gt;` is a valid operator name. Although I don't think people will miss it. But is `\-&gt;` the same as `\ -&gt;`?
Since you're using Racket, you can try ~~some of~~ something analogous to Haskell's features before making the switch. Try Typed Racket and Lazy Racket. 
Well, it's on Hackage.
My two cents: * Pattern Matching * List Comprehensions
You may find some motivation in this paper by Philip Wadler: "A critique of Abelson and Sussman - or - Why calculating is better than scheming" http://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf The paper predates Haskell, it uses Miranda, but Miranda and Haskell are quite close.
I made this transition too. The thing that will be the hardest is dealing with haskell's OCD with regards to the type system. Does it have benefits? Sure. But coming from scheme it's a bit of a system shock.
I've added (.:.), (.::), and friends to the composition package. :)
I've added (‚àò) to the composition package
While the collective has already judged you, I still want to point out that you are making the same mistake a lot of other people are making here, which is asserting various things specific to language implementations as fundamental to the question of first-class functions, when they aren't actually fundamental. The adder in C is a simple function taking two arguments and returning their addition, and that's how it works in C. "Partial function application" fundamentally doesn't exist in C, for instance.
Nitpick: &gt; Lists? They are monads. File handles? Monads. Database query results? Timer countdowns? Fuel tank status? Monads No. A Monad is a *type* with kind * -&gt; * (often called a "type constructor"). Don't say "Lists are monads", say "List is a monad". Values are not monads.
Which may or may not be true, depending on your perspective... the compiler insists that everything type checks, yes... but if you let it infer most types, there are very few situations where things don't just work.
I'd argue that's another instance of projecting something from other implementations onto the question of first-class functions _in C_. In C, that's simply not how functions work, so of course it's impossible. Functions _to the extent functions exist in C_ are first-class functions in all the ways that matter _in C_. _No_ functions have context function-specific, ever. The alternative, if we are to treat "first-classness" as a binary value (which is the real root problem), of claiming that they _aren't_ first class is _more_ wrong than the claim that they are. There are languages that truly do not have anything like first-class functions and binning C with them obscures the way real modern C is frequently used. This is the same sort of thing as claiming that Java isn't _truly_ object oriented because atomic values aren't objects. Well, in Java, that's how the OO works. Perhaps for a certain definition that renders it "not OO", but that's not a very useful definition of OO in the context of Java. C has first class functions and it doesn't have closures, which was the question in the first place if you go back up several posts. You basically just reiterated that it doesn't have closures.
How much of Racket background do you have? Have you used contracts? Haskell enforces purity and has a type checker to enforce it. It may be annoying at first, but it comes with the benefit of helping you to not write stupid code, and catching more errors at compile time. Racket's philosophy is that writing code should be organic: you should have as much freedom as possible to write the program you want to write. Haskell's approach is more heavily guided by the type system. Both strongly support a purely functional approach to problem solving.
I've written extensively about Monads with Racket being one of my primary example languages. I wrote [this](http://planet.plt-scheme.org/package-source/toups/functional.plt/1/1/planet-docs/better-monads-guide/index.html) Racket library and [these](http://dorophone.blogspot.com/2011/05/hyperturtle-monad-makes-pretty-pictures.html) [blog](http://dorophone.blogspot.com/2011/11/understanding-haskell-io-monad.html) [posts](http://dorophone.blogspot.com/2011/09/scheme-syntax-is-monad.html) [about](http://dorophone.blogspot.com/2011/05/monadic-parser-combinators-in-elisp.html) [them](http://dorophone.blogspot.com/2011/04/deep-emacs-part-1.html). I've been told that they are helpful for people trying to wrap their minds around monads from outside the Haskell universe. I can also recommend [these](http://www.intensivesystems.net/tutorials/monads_101.html) articles about them if you can read Clojure. You might also want to look at the ML family of languages as a kind of transition towards Haskell. ML is a lot like Scheme with types, and I found [these course notes](http://lambda.jimpryor.net/monad_transformers/) helpful as I built a deeper understanding of the relationship between monads and data structures. I should say, despite all this, that I'm not much of a Haskell programmer, but its more that I haven't had a good opportunity to use the language. In any case, I think Scheme/Racket is a great platform for understanding Haskell. Good luck!
to be fair, i don't think it will be a trivial transition, but it is worth it anyway. haskell's community is much larger and more vibrant. i like racket too, but i've put it on the back burner because (a) it isn't as powerful as haskell in my opinion, and (b) it isn't popular enough to reward my time investment
Yea, that's exactly it. There are other ways I was thinking of describing it, like 'type fetishism.' =) Someone that's only used strict languages might only need a little adjustment to deal with all of the abstraction that haskell does (e.g. monads) and the way compiler errors are presented. But if you're a scheme/lisp programmer used only to dynamic languages it will seem absolutely baffling at first. 
&gt; While the collective has already judged you, A.k.a. some random people or bots have downvoted me. &gt; which is asserting various things specific to language implementations as fundamental to the question of first-class functions, when they aren't actually fundamental. I'm not talking about anything specific to any particular Haskell implementation. It has the same meaning in any Haskell implementation. In fact it's not specific to Haskell, you could use Scheme or whatever. &gt; The adder in C is a simple function taking two arguments and returning their addition, and that's how it works in C. This is a weird way of saying "we can't write adder in C". 
Before learning Haskell, my favorite "for fun" languages were Python and Scheme. Based on my experience, I'd say that if you're comfortable with avoiding mutable data and with using higher-order functions like `map` and `reduce`, then as far as functional programming goes Haskell shouldn't be too hard to understand. On the other hand, I expect that by far the biggest hurdle you'll face is (predictably enough) Haskell's type system. To me, there seems to be a qualitative difference between it and even the type systems of most ML-style languages that's hard to explain to someone who doesn't already know Haskell. Purity, laziness, and assorted things you can't do in general (such as inspect the type of an arbitrary value) together conspire to create powerful means of reasoning about and understanding code. But that's hard to demonstrate with examples. Straightforward examples of "this is cooler in Haskell than in Scheme" are mostly going to come from laziness-by-default. So you have a list of unknown length and you want to pair each item with its index for later use? Ok, just `zip` it with an infinite list `[0..]`. No need to worry about the length or keep a counter or make sure your loop is tail-recursive or any of that. Laziness is great for making simple, say-what-you-mean functions work.
The point is that it's hard to prove that `map (f . g)` is faster than `map f . map g`. There's no doubt that they're equivalent in functionality, but microoptimizations get complex easily.
One thing that got me when I first started using Haskell after learning Lisp is the repl. Unlike in lisp, you can't just define a new function (e.g. "square x = x \* x") in the repl in the same way you do it in a source file, you need to say "let square x = x \* x". It's a fairly minor thing, but it threw me off for a little while.
Totally. The type system is really awesome once you get used to it.
Why do you want to know the number of arguments a lambda takes? You must already scan the right hand side in order to manually infer the type: \x -&gt; (+) x 
It's "more efficient" in the sense that you "go out and return" from one structure to another just once instead of two (or more) times. This could mean just one boxing/unboxing, array traversal or whatever; actual implementation details are not really about the point gasche was making.
Well, I'm a bit in the middle of this transition too. Haskell will possibly be a bit tough‚Äîthough I think the documentation and learning materials now are much better than when I first looked at Haskell 8 years ago or so. Certainly you should start with [Learn You a Haskell for Great Good](http://learnyouahaskell.com/), then follow it up with [Real World Haskell](http://book.realworldhaskell.org/read/). Some things that are going to trump you: 1. The syntax. Scheme's syntax is really easier; everything is prefix and parenthesized, and there are no operator precedence rules; nearly every operation has a readable English name instead of endled Snoopy-swearing nonalphanumeric operators like `&gt;&gt;=` and `$` and `$$` and `&gt;=&gt;` and `&lt;$&gt;` and `&lt;*&gt;` and please God make it stop; etc. 2. It's going to be tricky to understand IO code initially. At first I would try to emulate examples I saw on the web, but I would make all sorts of mistakes that I couldn't figure out easily (e.g., doing `x &lt;- pureExpr` in a `do` block instead of `let x = pureExpr`). Try to make sure you understand monads soonish (the *Learn You a Haskell* book is good for that). 3. Reading the elaborate compilation errors and figuring out why you got them. They're always telling you that such and such type didn't match, but when you're starting out it's very often because you didn't get some subtle syntactic point right, like operator precedence. 4. There's a lot of more advanced stuff that's just not easy to understand, and it adds high value. Existential types, monad transformers, iteratee/enumerator pattern, etc. 5. Everything is evolving really fast. E.g., the answer on how to do I/O in practical applications is shifting from old style (use handles or lazy I/O with String) to new style (use ByteString and Text with Iteratees, Enumeratees and newer functional I/O patterns that are being invented just as we speak).
That's a ghc thing. There are other Haskell implementations where you can write in the same way at the interactive prompt as at the top level. 
I'm still wondering about the feasibility of creating a Haskell powered start-up in France...
Pattern matching on the left, tho, is something to get used to.
In GHC 7.4 you can at least define data types and the like in the interpreter: ghci&gt; :{ Prelude| data T = C Prelude| :} No functions yet though: ghci&gt; :{ Prelude| f x = x Prelude| :} &lt;interactive&gt;:14:5: parse error on input `=' 
Can you show us the exact error you get? Also, the haskell-cafe mailing list is sometimes really helpful in cases like this :)
Maybe it's just me, but I'd rather click one link to see the content than two.
Somehow I didn't know about pattern matching with records before, thanks.
Slightly different idea, but often I've wanted to be able to write things such as (xs ++ ys ++ zs ++) (. f . g . h) 
A timely post, as I started using lzma-enumerator this evening and am considering moving to lzma-conduit. I was aware of this problem when I independently came up with pipes last year; having an overly-strong synchronization between input and output certainly is a common problem in many iteratee-like stream processing abstractions. This is a step in the right direction for conduit.
Baha, are you taking naumann's course this semester? 
I tried `--extra-include-dirs` and `--extra-lib-dirs`, pointing them at the relevant directories in the Cellar where Homebrew installed opencv. Didn't work. I used Cabal to install c2hs, but Cabal failed to recognized its presence while installing CV.
Oh how often I've wanted to do that!
I never understood why people would call the Haskell syntax weird. It's different to many other languages - true - but it's the most beatiful syntax I know. Point-free style sometimes hurts readability, so use it when appropriate. You can always fall back to named variables - it can even look a lot like Python if you wish: getYear year = let (t, r) = getRows(getTable(parseTags year)) in controversy(sort[getData t x| x &lt;- r]) &gt; As hinted at above, Haskell has built-in operators that work on functions. These are all library functions, not built-in.
If memory serves me, some of the folks at Scrive were relatively new to Haskell at first. I'd be interested to know how they approached some of the idioms touched on in this post--the use of monad transformers, the motivation for `newtype` wrappers, using `ReaderT` to provide an environment that's mostly meaningful in the context of the wrapped monad, &amp;c.--because in some ways this is all pretty standard, but on the other hand it's a larger, architectural sort of concern that's often beyond the scope of basic introductory materials. Did you end up rediscovering some of it, or was it suggested by a more experienced Haskell programmer? Was the motivation behind the design immediately obvious, or a "learn the hard way" sort of thing? While OOP-style "design patterns" get a lot of flak (much of which is deserved, but not all) I don't know of a better term for general architectural idioms like this, and last time I looked there was a distinct lack of good resources describing such Haskell-style "design patterns" in an organized way.
Well, why do you love Scheme? If you like thinking in terms of filtering, mapping, and folding lists, or recursively chomping through them cons'ing elements together, then that translates very well to Haskell. If you like metaprogramming macro aspects of Racket (which I understand is pretty sophisticated - moreso than CL's), then I don't think that's as strong in Haskell land. I got my start in Scheme (6.001 at MIT) and I'm learning Haskell now. A lot is familiar, but the syntax is overwhelming at first as you have a lot more options about how to express something than in Scheme. I do love what I'm learning about types, though, as everyone in here is saying. The compiler can figure a lot out, but I still like to put the type signature at the top of every function: bestName :: [String] -&gt; String I know that's pretty simple Haskell, but I still really like it. It's usually pretty straightforward coming up with what you want the function's type signature to be, so it's neat to easily lay that down and then know the compiler's got your back as you implement it.
Cool, I had it with Naumann, enjoyed it. 
It is my personal opinion that library internals should be as simple as reasonable, but it's OK to complicate library internals as long as 1) the added complexity is well documented and 2) this added complexity of the internals provides a significant benefit to the end user, e.g. easier to use, more efficient, etc. So while it's nice to keep the internal and convenience libraries looking similar, it's better to complicate the internals in order to provide a better experience for "end users" of this library. Writing libraries is for the few, while using libraries is for the many.
ReaderT might not be the correct answer though. For example in our system after current user does something we sometimes need to notify another user about changes. Having user stored in environment led to confusion, it was just better to pass user handle explicitly. I guess before Design Pattern was publish mass rediscovery was happening among OOP people. 
Please polish and release ! We all love pipes !
I'm still undecided whether I should release it separately. Tekmo and I have somewhat different ideas on how to implement exception handling, but it would be nice to avoid having different pipes incarnations around.
Yep!
too much type checking just puts the pain of programming up front, as opposed to late in the project, or worse! in the long term it really is a good trade.
Yes, the lack of synchronization is both a blessing and curse. If you ever contribute to the pipes library you will learn that you have to reason about the type in a different way in order to prove or falsify invariants required by type classes. The fact that pipes are so permissive places a much bigger burden of proof on the library contributors. A good rule of thumb is that if it satisfies some sort of category law then it will be intuitive to use, but if it does not then you will find not behaving the way you would want. So when I first released the library I spent about half my time proving that various proposals submitted to me violated some law before my intuition for the proofs improved.
Does anyone have experience with programs like the [Computer Science Summer School 2012?](http://www-hpc.cea.fr/SummerSchools2012-CS.htm) I'd love to attend and I'm wondering if it will be worth it.
Fess up, who is the jerk that popularized this terrible "string literals have a dark green background in haskell" stylesheet.
Seems that some are toying with lambdabot... \^\^
I do not like it either. Do you have a better css?
what is so wrong with the first example ("procedural")? it is easy to understand...the "progressions" just seem to introduce more voodoo
Yeah, you need (left and right) cancellation as well, right?
Error messages are not "incomprehensible": they describe *one specific aspect* of the type problem, it just takes some getting used to to get that aspect. For instance, forgetting an argument to a function often leads to a message like "(XXX -&gt; Int) has no Num instance". It's just an effect of curryfication and functions being regular types: a message like "String has no Num instance" is immediately clear (you tried to pass a String when some Num was expected), but it has to be the same message for functions (as they're normal types just as String is).
At the end of that same paragraph, in parentheses, "but it just so happens that all finite monoids are groups" is the bit referred to. 
Great article Sjoerd! I have but one nit to pick, though. In all but one place you use "bounded" rather than "bound" -- there is a subtle difference. - "bound" comes from "binding", i.e., a bound variable is a variable that has a binder. - "bounded" comes from "bound" which in turn comes from "boundary/limit"; a "bounded" variable is one that is somehow constrained (e.g., as in "bounded polymorphism") I think you meant to say "bound" in each of your examples. I actually had pretty much the same idea as `multiplate` and was working on a paper for the 2009 Haskell symposium but I then dropped it, because it couldn't deal with bottom up traversals. Bottom up traversals require a monadic version of the traversal function which would have required another generic boilerplate definition. I like Russel's paper a lot, so I'm not sad about it.
More generally, the set of natural numbers `{1,..,n}` with the operation `min`. EDIT: And the neutral element `n`.
&gt; what is being added in Java 8 is a nicer syntax for them (one that doesn't require [‚Ä¶] having to declare the closed variables final) But they still must effectively *be* final, correct? That‚Äôs a sticking point for some people.
That's a great attitude to take towards work like this!
Brilliant! This calls for a t-shirt whose target market is small enough to be almost unmeasurable with modern instruments, I think.
I've thought the same for a good long while. Didn't have the discipline to try to code it up, though. Good on you. I think I'll try it out.
Maybe this is what you did but I think the easiest way to do this (so you can package it up for other people) is probably to take uzbl and add the interface widgets to it that you want.
Interesting Idea. I would say that if you keep the browser always full (with only the page been shown) and make the visual browse tree appears only when you ask (via ctrl+tab for example) that should make it visually more interesting (the only UI that the program will really have will be the tree)
Right, so the currying in (+2) could be implemented as a closure that calls the uncurried function bringing in the first argument from the environment. So that was a closure.
Not to mention that you can feel much more confident about a principled design grounded in algebraic/categoric laws. I'd also be very interested to see whether something interesting can happen here modelled around delimited continuations.
Where is that defined? You can argue like that with any closure. Create an anonymous function taking the closed over variables as extra arguments and use normao function application on that anonymous function. Any compiler can do it. But a non optimizing compiler will not create a separate function for (+2). Another example let x = 2 in map (*x) Here (*2) closes over x. If the implementation does constant propagation and does not implement currying using closures then no closure is created. However a reasonable implementation of currying of the function f :: Int -&gt; Int -&gt; Int is to create the helper function f' x = \y -&gt; f x y f' is used whenever f is curried. f' returns a closure closing over the first argument. Creating a specialized function for (+2) is an optimization.
The shorthand is defined in the [Haskell 98 Report ¬ß3.5](http://www.haskell.org/onlinereport/exps.html) and the [Haskell 2010 Report ¬ß3.5](http://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-300003.5).
undo/redo should naturally be represented as trees as well.
Oh, for the love of god guys, stop making all of this cool shit that I want to work on. I have to be a productive person outside of my office, you know.
I just use [surf](http://surf.suckless.org/), I think the key-bindings suck, but the source is right there, pretty easy to fix and recompile. EDIT: And it's based on Webkit so it should stay up-to-date with the latest W3C fads.
I don't understand why you would create a whole browser just because you dislike the tabbing but I do however think that a browser in haskell is good idea. I'm curious as to what a pure algorithm for parsing all kinds of quirky html would look like.
Er, that "voodoo" is called "abstraction", it tends to be useful when programming. And it's not like the other examples aren't easy to understand.
Great! They say that you know a programming language has gone real when you've implemented a web browser in it. (Granted, Webkit's not in Haskell, but it's a step!)
Could you summarize the question so that people don't have to watch the video to answer it?
Followup: it turned out not to be a category.
You can use `#t=` to link to a particular spot. In this case, http://www.youtube.com/watch?v=PUv66718DII#t=17m
Do you mean something like [java's hotswap](http://en.wikipedia.org/wiki/Hot_swapping#Software)? I've always wanted something like this for python as well. As far as I can tell there aren't any project's that offer this though on [Don Stewart's about page](http://donsbot.wordpress.com/about/) he says he did some PhD work on it at UNSW.
There's a post about exactly this is /r/emacs: http://www.reddit.com/r/emacs/comments/qbtou/inventing_on_principle_great_talk_my_question_is/
Well, it's not that easy. I'm using this Emacs extension myself. Making history a Tree would require another dimension for storing those trees. I cannot think of easy way to express that. I did think about that idea first. You can see that in the beginning the Page datatype also have had pgHistory field - just for this purpose in mind. I have abandonded it as I couldn't find a way to *cleanly* implement history as a tree.
(sorry for the spelling mistakes in my previous entry - typing on a mobile phone is cumbersome) Thanks for the references! I think this is confusing language semantics with an implementation technique, so I think my point still stands. I don't think there can be a requirement that (+2), (+3), etc. are implemented as separate functions in an haskell compiler/interpreter. It makes more sense to share the function +' x = \y -&gt; x + y for both cases.
He might be referring to the (==x) syntax and/or the precedence declarations which are built-in.
I'd argue that the explicit use of the conn param is enough motivation to change the function. It's existence there demands greater understanding of the underlying functions then is strictly necessary, allowing for the possibility of confusion or improper use. I don't it's an issue of "even to the seasoned programmer" as the mental overhead is always overhead, regardless of the programmer's skill. Of course that's only a problem if you want a water tight API, a lot of the time your effort would probably be better spent elsewhere. If that were the case I'd agree with you, as there's nothing inherently wrong with a procedural implementation. Although there is something to be said for the beauty of a water tight abstraction.
This talk was great and the guy is very smart and makes good points. I was particularly impressed with his visual art and game examples. However I don't think his visualization of algorithms is extendible enough. It worked very nicely for the example he gave, binary search in javascript using integers on arrays. But imagine if he'd used an array of objects that couldn't be made an instance of show. Or at any stage in his code he was making a function call. Or even worse, the algorithm didn't provide useful information for datasets small enough to fit on the screen and large datasets took prohibitively long to compute. His principle of providing immediate feedback is a good one which I wholeheartedly agree with but I think this specific example for visualising algorithms is still inferior to just teaching better (composable and modular) coding practices. 
Initial motivation was to get rid of many, many functions that had explicit parameter both in their signature and at every call site. Secondary motivation (that came up later) was that using monads we can ensure proper open-use-use-use-close cycle of database connections. Third motivation was that we could swap out production database monad with a test monad effectively achieving equivalent of object mockups. Hmm, seems I made the same mistake as everybody else: I state how to do things, but did not explain enough WHY do it! Thanks for the inspiring question!
But then he'd somehow get extra karma.
The neutral element is `n`. ;-)
You can write the instance in a more readable way by using the combinators from the [Data.Generics.Uniplate.Direct](http://hackage.haskell.org/packages/archive/uniplate/1.6.6/doc/html/Data-Generics-Uniplate-Direct.html) module. [EDIT: Scratch that, you probably can't due to the wrapping] Where exactly do you lose type safety? Do you mean because the Uniplate instance has some invariants? AFAICS, the problem with this formulation is that at each recursive step your wrapping and unwrapping every subexpression. It would be interesting to see the performance cost of that.
Trust me, when I'm programming in Haskell, I am **not** simulating in my head what each line of code would do on a computer.
That's the diference of functional vs procedural. It makes all the diference, but IMO there still value for haskell.
I agree, his ideas aren't close to replace good practices, but they surely make you gain some time.
I'm pretty sure I saw the exact same idea in a paper from several years ago. That person (a PhD student, I think) implemented an Eclipse plugin to do the same thing for Java. Let me see if I can find the paper. OK, found it. It was actually Jonathan Edwards earlier work: http://subtextual.org/OOPSLA04.pdf
Great work! I have been looking forward to such tutorial for days when your post comes out. It'll bring benefits to others if your work is incorporated into http://www.haskell.org/haskellwiki/Multiplate#Traversing.
Is the directory containing the c2hs binary in your PATH?
If ever there was an example of something that 'doesn't scale', 'simulating in your head what each line of code would do on a computer' would have to be one. All you have to do to see that it is completely insane paradigm is put it into words. It works for his example of a binary search. 
So it starts with this: &gt; So in order to write code like this you have to imagine an array in your head and you essentially have to play computer. You have to simulate in your head what each line of code would do in a computer. And to a certain extent people who we consider to be good software engineers are just those people that are good at playing computer. No! This is what bad programmers do. You shouldn't just rely on an operational interpretation of the code when writing (imperative) programs. Axiomatic semantics are really useful. When writing code you should have first of all, a very clear understanding of what each variable *means*, and second a pretty good idea of *why* your code *preserves* their meaning. And this should be reflected in your code by means of comments. And this way you can make sense of it. The first requirement is even necessary for you to take advantage of your operational interpretation of the code. You can't know if, at a certain point in the evaluation, all is OK when you don't know what you're supposed to have. But programmers do this all the time. They can tell the end result is wrong, but they find it really hard to debug because they really don't know *where* it is going wrong. The second is what gives you confidence that your code is right, or at least that the algorithm you have in mind is correct, without having to simulate every step of it. And the converse, it allows you to sometimes very quickly realise it is most likely wrong. To be clear, I'm not trying to say you shouldn't thinking operationally, or that what he shows isn't useful. Of course that is not the case. I'm simply very opposed to the idea that the operational view is all you should rely on when writing code. *Bonus:* Take the algorithm for binary search and change it so that when the key is not present you return the position where it should be inserted. If you do have a good understanding of what the variables mean, it's really easy. If you don't, it's not. 
I agree with you, that's what make Haskell so consise and guaranteed to run after compilation, but in a very big problem (a game) you can't run away from a big number of states,they'll be happening all the time. You can't resume various entities AI, position, time, etcetera on a few states. If you could follow up the ' states' (how an enemy AI acted), you could do better decisions about your code, IMO. But maybe I didn't rewire my brain deep enough ;p
Yes, hot swaping would be it, or pretty close. Bonus question: LLVM compiles to bytecode, doesn't it? Could this facilitate a feature like hot swap?
you can try implement your own in haskell and using GSL as a reference. the [gsl code](http://bzr.savannah.gnu.org/lh/gsl/trunk/annotate/head:/cdf/gauss.c) is very very simple and readable. 
Indeed. I've been working with flash recently and I can say my jaw dropped on the flash example 
No. When the elves leave Middle Earth they will take it with them.
Your one stop shop for most statistical distributions: http://hackage.haskell.org/package/statistics-0.10.1.0
A nice overview of a slice of the persistence space. There are also in-memory approaches such as pioneered by happstack and provided by acid-state. Also, there's the work on the iteratee approach pioneered by Takusen. There's also the ongoing research vis-a-vis database-supported Haskell, and Ferry. MetaHDBC also provides a quasi-quoted/template haskell approach.
* http://hackage.haskell.org/package/plugins * http://hackage.haskell.org/package/hint
The medium term future of acid-state is likely to *start* with everything in RAM, and then allow the application to smartly control what data gets cached to disk. That is sort of the inverse of what traditional SQL databases do -- where they store everything on disk and then hope that the right stuff gets cached in memory. But, if you look at large scale systems, like reddit, facebook, amazon, ebay, etc, you see an emerging pattern where a majority of the working dataset needs to be in RAM (via memcached) and where database level transactions are disabled, where tables often only have two-columns, and where low priority data still has to be migrated out of SQL databases into other slower, long term storage mechanism, because even without transactions, etc, the database replication can't scale up to thousands of servers. So, they are hacking up joins and fancy queries in *PHP*, with no transactional guarantees, and trying to get memcached to hold the right things, and trying to get the SQL servers to cache the right things, and dealing with other custom storage solutions. acid-state starts from the other end of the problem -- it assumes you want to have everything in RAM, and then (still-in-development) allows you to flush data to disk in a application specific manner if you encounter memory pressure. This gives you much greater control over what is in RAM/disk, because it can reflect the metrics that are specific to your app. And it greatly simplifies the problem because you only have a single system to work with .. you are not trying to cross boundaries between PHP, memcached, mysql, and fights among multiple all-purpose caching algorithms.. Whether this ends up being the best approach in the end remains to be seen.. but it is certainly awesome in the cases where everything does fit in RAM, and there is still a ton of low hanging fruit when it comes to handling memory pressure situations. 
As a temporary hack for large objects, I've been storing filesystem references in acid-state which I then pull and stream from disk when needed. Works pretty well for my current needs. I'm such a huge fan of acid-state, it's really on point for a lot of modern use cases. I would, however, be interested in more general-purpose data structures for use with it - IxSet and HiggsSet are really cool but they seem to lack any sense of relations. I'm currently working on a HiggsSet-derivative that's backed by IntMaps and bytestring-tries for efficient filtering. I hope to extend it to a "jagged table" kind of layout where data is stored completely denormalized and relations are enfored on update.
Hmm, slightly off-topic question: The post mentioned that Persistent is going to get a CouchDB backend - I have two web applications (one public, one for internal use at work) which are currently built with Happstack and Database.CouchDB. Yesod seems to have gained *a lot* of traction lately, would switching be valuable?
The erf package provides a type class and implementations for Float and Double. 
yes yes. this. great comment
Last I checked, wx-0.13 seemed to have a few problems on MacOS X, that's why I've postponed it. I will definitely update the dependencies when I release reactive-banana-0.5.
&gt; A nice overview of a slice of the persistence space. Yes, and it's a pretty important slice. Especially in the context of web programming, which is the context of the post. I think Greg's post gives a pretty complete picture of the current state of affairs. 
I love this video, but this idea is not new. See for example this paper from '95: http://www.cs.ucsb.edu/~urs/oocsb/self/papers/programming-as-experience.html
Don't wonder, criterion it! =)
I enjoyed the article, but let's be real. It only spends a few sentences on anything leading up to Persistent. It's really a history of persistent and persistent-like and derived projects over the last few years. Which is fine! But even in the web space, people are doing lots of things and using lots of things not covered under that umbrella. Which is great! More libraries, more options, more designs explored. And I appreciated the article for what it does. In fact, the article even writes: "I am sure I am missing some other attempts at type-safe querying and automatic data-marshaling: please let me know of them." So I did! 
The proliferation of `&amp;lt;` and `&amp;gt;` is... distracting.
I think the best part about Yesod is compile-time hamlet templates. If you can use that from Happstack and also use the type-safe urls feature you will have a lot of the benefits of Yesod. You should evaluate the CouchDB backend: my understanding is that to fit the Persistent mold it will have to make some assumptions for you: so you might also need to change your schema since you didn't start with it, or you might not like its assumptions
That pattern that emerged according to you isn't a nice pattern at all. I don't want to lose all the SQL just because someone else works with big data where he has to do it. When I saw happstack's wannabe facebook/reddit I just went back to HDBC which turned out to be a lot more powerful and reliable though not very streamlined. There's a lot of work that can be done to make SQL+stored procedures be nicer.
Not sure if its an issue or not, but there is of course the licensing difference between gsl (GPL) and statistics (BSD).
By redundantly do you mean replicating the data across multiple servers? happstack-state had two different working implementations of multimaster replication. Lemmih has been thinking about different options for acid-state as well. The issue here is not so much "is it possible", but more of "which of the many possibilities is best." For example, given the CAP theorem-- which two do we want? Or, can we provide multiple solutions which give you a different two?
One idea would be to explicitly model availability vs consistency in the type system. A in Cap is about instantaneous availability. While interesting in order to get a theoretical result, I do not think that is a very fruitful way to think about this. Imagine acid state distributed across multiple processes on the same os, in the same cluster or intercontinental. Consistency in the three cases mean very different latencies. Availability is broken in the formal CAP for any delay whatsoever, but the delay to reach quorum in a cluster although not technically Available does not necessarily matter in real life. Similar with partitioning. If the transaction time is always larger than the partitioning timeout then you can get the Partitioning tolerance by making the transactions take longer time. In a real application that has data distributed like above it comes down to deciding which transactions need to be consistent and which ones that do not. Then put the data such that the fast consistent transactions have data on a closely connected set of nodes. I think Haskell could do "something" in encoding this in the type system. Example a global system with session state near the connection origin and global state centralized in a cluster. You could do fast transactions on the session state or fast transactions on the global state. A transaction covering both would have to take a long time to overcome partitioning tolerance between the two clusters. I imagine that the entity distribution and the node topology could be encoded in types or somewhere so that cap properties of a given transaction could be analyzed at compile time. Basically - if the topology and placement of entities does not match what was encoded in the types during development then the system will not run. My comment regarding consensus is that something like that is required to get anywhere on the consistency front in the first place.
On the other hand, I don't want to have large, slow, complicated, non-typesafe data structures just because someone, somewhere needs to reserve the possibility to perform arbitrary relational algebra on his data.
Would anyone please think about us, enterprise developers ? Blessing ? Nirvana ? It's really hard to sell yesod to managers and bosses with such talk. How about certifying and cabal-certify ?
It is pretty amazing. patternToPipe :: Pattern r -&gt; Pipe Double (Double, Maybe Double) [] r patternToPipe (Atom e) = Pure e patternToPipe (Cycle ps) = M (map patternToPipe ps) patternToPipe (Signal f) = Await (patternToPipe . f) patternToPipe (Arc pat d md) = Yield ((d,md), patternToPipe pat) pipeToPattern :: Pipe Double (Double, Maybe Double) [] a -&gt; Pattern a pipeToPattern (Pure e) = Atom e pipeToPattern (M xs) = Cycle (map pipeToPattern xs) pipeToPattern (Await f) = Signal (pipeToPattern . f) pipeToPattern (Yield ((d,md), p) ) = Arc (pipeToPattern p) d md instance Functor Pattern where fmap f = pipeToPattern . fmap f . patternToPipe
I don't like the sound of that "cabal" thing, either.
On a modern machine (laptop, 1.5 years old), with the latest Aurora; I currently have 90 tabs and it's still very responsive (though I still need to relaunch it from time to time, once every two day maybe). Or do you mean that this add-on does introduce random delays ?
You'd simply implement the HTML5 algorithm.
Being able to filter by licensing metadata is very important for some (myself) also. If you are implementing multiple tag levels, license levels would be very helpful at the same time.
Well he made it pretty clear that later on he understood the type errors and benefited from them. In fact the type system in Haskell is pretty amazing at catching lots of errors once you get accustomed to it, but for someone that comes from a dynamic language... it can be a bit jarring at first. And despite the improvement of the error messages year after year they can still be a bit obscure as you yourself point in your examples (GHC doesn't tell you that you forgot an argument, it tells you that maybe you forgot to write a Num instance for functions... and Haskell is one of the only languages where that may be the right call from time to time).
Drop cabal ;) 
Oh yeah .. I love Learn You a Haskell .. it's one of my favorite books that I've ever read. I'd rate it alongside the K&amp;R C book. I think it's utterly brilliant and I long for a sequel so that I can finally learn the more advanced stuff like the continuation monad, arrows and maybe frp etc.
Or 'yesod' or Hack-age, or snap!, or gloss or.. Although we can find problems everywhere, nirvana may be a little overboard.
CEFDMTWS
The principle of this visualization is not about testing logic, but to tweak (Having a better looking picture changing some parameters, change some things on your game design so it feels better). Those example are programmed, but you cannot how it 'plays out' by the code (as in, I know this variable is speed, but how much speed is a good one? You need to test it). The point on minute 16 on general programming wouldn't be to substitute how you visualize, because you still are programming the same way, but to have some confirmation bias. 
you could use Persistent with Happstack as well. Personally, I have reservations about an abstraction layer than attempts to work with very different styles of databases. It seems like you would end up with least common denominator library which has difficultly exploiting the strengths of any individual database. 
Wait, do I read this correctly in that an insert takes on the order of 1-2 milliseconds? That doesn't seem right to me...
If I understand the description correctly, this is the exact datastructure that Clojure uses for almost everything. Nice to see innovation leaking from one corner of the FP world to the opposite one.
Yes it is. Scala uses it as well now. Aside: I suspect our implementation is faster than Clojure's, as or old Patricia trie based implementation was almost on par. I should run the numbers some day.
Measuring the amount of time spent collecting garbage, it damn well better should be. Clojure pays a tax to live on the JVM.
I would love to see the benchmark against IntMap
I've updated the post to contain a graph comparing `HashMap` to `Map`.
I follow the [PVP](http://www.haskell.org/haskellwiki/Package_versioning_policy), which defines the first two components to be the major version.
The old implementation is basically `IntMap` (slightly improved.) The old `HashMap Int a` performs almost exactly as `IntMap a`.
Cool, so it's twice faster than IntMap with practically no penalty compared to String keys. Good to know. Not that i would ever hit performance problems with IntMap :)
Just a heads-up: there's an bug in cabal-install that prevents us from being able to use constraints here. So I've released version 0.2 which uses a different approach. A description of what it does is available on the [Hackage page](http://hackage.haskell.org/package/cabal-nirvana).
I'm looking at your implementation of FullList right now. Can you explain to me the meaning of this comment? &gt; The 'FullList' type has [the] benefit [that] it can be unpacked into a data constructor.
A function that works with very large data *can* work with small data too. It's useful to visualize the small case to understand the large case better. Polymorphic functions can usually be made with *some* showable value for concrete presentation cases.
Could you do a run comparing unordered-containers to hashmap?
You can read stdin via: http://www.haskell.org/ghc/docs/latest/html/libraries/bytestring/Data-ByteString.html#v:getContents You can convert the String from your argument to a ByteString via: http://www.haskell.org/ghc/docs/6.6/html/libraries/base/Data-ByteString-Char8.html#v%3Apack To combine two ByteStrings byte-per-byte, you can use: http://www.haskell.org/ghc/docs/latest/html/libraries/bytestring/Data-ByteString.html#v:zipWith You can probably figure out the rest?
You've told us what your program needs to do, but you haven't told us which bits you're having trouble with. What have you got already? What don't you understand? It's usually better to [ask for help on the Haskell StackOverflow](http://stackoverflow.com/questions/tagged/haskell) than here. But again, you need to make it easy for people to help you, and you need to demonstrate that you've put some effort in and aren't just asking for teh c0d3z.
Basically i have no clue what to do for it. I'm not asking anyone to do it for me, just point me in the right direction where i might get help or examples.
Note that he should probably use lazy bytestring instead since he wants an infinite bytestring on one hand (with cycle) and streaming for the I/O.
Yes but reddit isn't the place to ask such a question, reddit is more for story, news, general questions on language evolution... Homework isn't really appropriate. You'll find more help on the Haskell-beginner mailing list, the #haskell channel on irc.freenode.org or the haskell tag on StackOverflow. Globally the Haskell community is very helpful but reddit isn't the right place.
fun to see that RoR drove some innovations on the dependency distribution/mgmt on the Ruby platform (Bundler+Gemfile), and Yesod now seems to be driving the same innovation in Haskell. i really like what Bundler has achieved. the Gemfile and the Gemfile.lock basically do the trick for me, and with Ruby-1.9.3-head they are fast enough to be usable. :)
This is coursework: presumably someone is trying to teach you Haskell, and you have lecture notes you don't understand. You are likely to get the best results in terms of learning by asking your fellow students for help understanding those notes. If after that you still have *no clue* what to do, then your teacher is appalling. Here is a list of substeps your program needs to do: * fetch the key from the command line... * ...but print an error message if there was no key on the command line * read data from standard input... * ...and write the transformed data to standard output * transform the data by transforming each individual character in the data * transform an individual character by xoring it with the corresponding character from the key Do you literally not know how to do any of this?
i was about to comment the same.. this is the main thing that keeps me from truly loving chrome.
What is a xor of characters?
okay
Xoring the characters's codepoints together and interpreting the result as another codepoint. e.g. '&amp;#x2208;' (U+2208) xor 'A' (U+0041) = '&amp;#x2249;' (U+2249)
Yes, I understand that, there are different needs and different solutions. I guess that I'll have to work on SQL+SP side myself. HaskellDB is closer to what I want but there's a lot that's suboptimal in it - quite confusing type features for instance.
hashmap's implementation is the same as the old unordered-containers, with some extra indirection and memory overhead. In presence of lots of collisions hashmap degrades a bit better, by using a map instead of a list for the buckets. I believe we're better of using better hash functions that trying to deal with large number of collisions.
Some sort of automated approval process would probably be best: default to the lowest common denominator, and let the user specify a whitelist or blacklist in order to raise that common denominator to an acceptable level.
BTW, tibbe = Johan Tibell (note the differing number of b's and ll's). :)
my head is spinning trying to keep track of what the preferred data structure packages are in haskell today. we need some sort of rating system in hackage
nice! thank you! installing some things like yesod is brutal under vanilla cabal
It's ok. They had me at management.
I'd just like to issue a small warning. Don't switch without benchmarking. I have a compiler that uses hashmaps extensively for symbol tables and here are some benchmark numbers for allocation and time using different packages: hashmap-1.2.0.1 34.462 Gbyte 43.84s (GC 2.18s) unordered-containers-0.1.4.3 33.206 Gbyte 44.80s (GC 2.43s) unordered-containers-0.2.0.0 35.709 Gbyte 48.47s (GC 3.20s) As you can see, the new unordered-containers is significantly slower, probably because I use fromList a lot. I'll stay with the old hashmap package for now. 
The natural way to me would be backtracking-via-function return.
A rating system would be nice. You should use containers for ordered containers and unordered-containers for unordered containers. These two have lots of testing and benchmarking done to them.
I'll see if I can extract some information about what API calls are used. The keys are small unique ints or the combine of two small unique ints. The most used bulk function is probably fromListWith.
So, more concretely, you mean, just recurse after having zeroed out the the most recently played number, passing the Map as a parameter? My thinking went like this: I'm passing around a Map representing the board. Most of my top level functions need it. So I can add it as a parameter to most of my top level functions, or make it State. Whenever I find myself passing state around as a parameter, I'm now thinking that I should be using a State monad. Did I take a wrong turn somewhere?
You basically want to look at [The GHC commentary](http://hackage.haskell.org/trac/ghc/wiki/Commentary). It should have all the relevant information you need - it may be somewhat out of date in some respects, but overall it's a great resource.
I'll try to get some numbers on `HashSet` as well.
There are codepoints that are [invalid characters](http://en.wikipedia.org/wiki/Mapping_of_Unicode_characters#Noncharacters), and guaranteed to never be assigned.
I *think* that you can use Template Haskell to get the lists you're converting to HashMap/HashSet/whatever with fromList at compile time rather than runtime, but this is only good for HashMaps/HashSets you know the values for at compile time.
[This](http://www.reddit.com/r/haskell/comments/jvvoa/hcas_haskell_computer_algebra_system_pdf/) may interest you. 
Ahhh... thanks.
Well, there are a few tools you have already at your disposal. 'traced' provides Debug.Traced, which gives you more or less what you have already done, but it also supplies tools for exploiting 'observable sharing' to get the common sub-expressions that you'd used originally back into the resulting expression. My 'ad' package for automatic differentiation works particularly well with this, because you can use the combination (with the simplifier provided by traced) to obtain symbolic differentiation, and it can provide gradients, jacobians, hessians, jets (towers of derivatives), etc. from arbitrary haskell functions. The 'numbers' package provides fixed precision and arbitrary precision reals for when you need to do actual evaluation. Beyond that when you start talking about integration the techniques become rather interesting. In particular chebyshev-pade approximation plays a big role.
It's not a static list. It's built by each import statement that is processed. 
Some of [Simon Marlow's papers](http://community.haskell.org/~simonmar/bib/bib.html) touch on this. IIRC Multicore Garbage Collection with Local Heaps talks about the effects of thunks on GC in a pure functional language.
I recommend checking out the books [Computer Algebra and Symbolic Computation: Elementary Algorithms](http://www.amazon.com/Computer-Algebra-Symbolic-Computation-Elementary/dp/1568811586/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1331178197&amp;sr=1-1) and [Computer Algebra and Symbolic Computation: Mathematical Methods](http://www.amazon.com/Computer-Algebra-Symbolic-Computation-Mathematical/dp/1568811594/ref=sr_1_4?s=books&amp;ie=UTF8&amp;qid=1331178197&amp;sr=1-4) by [Joel S. Cohen](http://web.cs.du.edu/~jscohen/) The algorithms in the books are presented as pseudo code. Here's a project which implements them in R6RS Scheme: https://github.com/dharmatech/mpl
What are you interested in? Haskell's just a tool - if you can narrow it down to graphics, AI, networking etc, you'll probably get more responses.
http://i.imgur.com/NYnCA.png http://i.imgur.com/Q6fv8.png these are mathematica screenshots. The data structure it uses is to represent expressions as lists. The first element of the list is called the head, and it is usually an operator or function like + * or Integral. The other items in the list are arguments to the function. Naturally the any part of the list can be an expression itself. In the tree diagram, vertex labels are the heads of the list. All of this is very similar to s-expressions... Note however that to represent just a plain list on its own as one of these expressions, the head has to be a special symbol that has meaning to the algebra system. In this case that special symbol is 'List' Hope this gives you some ideas.
I made a tiny symbolic math tool in Haskell, available on [GitHub](https://github.com/Twinside/Eq) and on [Hackage](http://hackage.haskell.org/package/Eq) (ergl, I made a really crappy cabal package :s). I'm sadly lacking time to continue it, but I use it once in a while and try to keep up with Haskell platform releases. So might want to look at it.
I found the book Modern Computer Algebra very useful. http://www.amazon.com/Modern-Computer-Algebra-Joachim-Gathen/dp/0521826462
Thanks! this helps. I'm still not good at reading Monad instance definitions, so I'm sure it will take me a while to understand what's going on in MonadNondet
I've always liked &gt; powerSet = filterM $ const [True, False]
He mentions &gt; The next version of the definition of Haskell will have a built-in type of natural numbers Is he talking about where you use type fams to inductively define One, Two, ... for type safety in vector lengths or physical dimensions? Or are they just adding UInt? 
It was a lot simpler 12 years ago - we had lists and arrays and Daan Leijan's ddata library. 
I don't know a lot about haskell performance, but isn't garbage collection a "problem" for haskell too?
a classic: &gt; fibs = 1 : 1 : zipWith (+) fibs (tail fibs)
I don't think it's the most clever piece of Haskell code I know (that would have to fall to things like whole libraries (like 'ad'), or Oleg's stuff), but here's a nice small snippet that can be posted on reddit: primes = 2 : 3 : (minus [5, 7 ..] nonprimes) nonprimes = foldr1 f [[p*p, p*p+2*p ..] | p &lt;- tail primes] where f (x:xt) ys = x : (union xt ys) Where 'minus' is just list-set difference, 'union' is list-set merge. All of this is on [here](http://en.literateprograms.org/Sieve_of_Eratosthenes_(Haskell)) edit: Which, in hindsight, looks like a copy and paste affair from [the haskellwiki](http://www.haskell.org/haskellwiki/Prime_numbers#Sieve_of_Eratosthenes)
Not a one-liner, but code that makes use of the laziness properties of haskell's Array are pretty clever http://brandon.si/code/fun-with-lazy-arrays-the-lz77-algorithm/
Since this was from 2007, I think he was just guessing (or looking at HaskellPrime). 
Don't know if anyone caught this, but shouldn't the line in the code for the List_Case be cons x xs -&gt; cons_case x xs instead of cons x xs -&gt; f x xs I am not a haskell programmer, so I had to read all that code very, very carefully. The f comes out of nowhere, and the cons_case is never used, so I think the f is wrong. I like your idea very much, but I noticed that using the list_case function just hid some of the important details. I really think you need a different graphical layout for match/ case/ switch/ if -then-else. Jonathan Edwards had a cool way of doing complicated nested conditionals in not just a graphical representation, but it was an attempt at a graphical IDE. It did functional programming almost exactly like how you did it, he showed how to messy conditional if elseif elseif elseif style code blocks. Sadly finding the link to that old video is tricky. Start at subtextual.org. He has done so many half-projects, I forget which one it was. My personal thoughts, I suggest that the little arrows be drawn differently for different types data types. Maybe change colors, maybe change the thickness of the arrow. If a function is being passed, that should look different from an integer, which would be different again from a single list element (x) which is different from the rest of the list ( xs)
That is true. I can now, finally, checkpoint my structure with lots of sharing when using `acid-state`. Thanks.
It's clearly not nonsense. BTW, a research project is currently building a infrastructure to use arrays operations on GPU : http://hackage.haskell.org/package/accelerate-0.9.0.1 
... which was exactly what I was thinking! So shouldn't a sufficiently lazy iso allow us to use the tricks described in that article on other types?
I love haskell but this perlish attitude about syntax really irks me.
Which attitude about syntax?
What is that doing?
One project, which serves as more of a "launching off point" is Mainland and Morrisett's http://www.eecs.harvard.edu/~mainland/publications/mainland10nikola.pdf In particular they solve a really thorny problem with how to get observable sharing to 'see' function applications in a DSL for GPU programming. Another interesting point in the design space is Conal's 'shady' http://hackage.haskell.org/package/shady-gen I have a pet project I'm exploring that would occupy yet another point in this design space, but I haven't released anything yet.
Where do they actually *define* reifyGraph? e: Ah, they cleverly hid it in the section labeled 'Implementation'. Tricky!
I'll consider this. Thanks!
Another frustration is that HaskellDB makes every join into a subquery, which kills MySQL performance.
Graham's solutions are always a great read. Very simple and easy to understand. Check out his [countdown solver](http://www.cs.nott.ac.uk/~gmh/countdown.hs) too.
 &gt; take 30 $ fix$(&lt;$&gt;)&lt;$&gt;(:)&lt;*&gt;((&lt;$&gt;((:[])&lt;$&gt;))(=&lt;&lt;)&lt;$&gt;(*)&lt;$&gt;(*2))$1 [1,2,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536,131072,262144,524288,1048576,2097152,4194304,8388608,16777216,33554432,67108864,134217728,268435456,536870912] 
This is not what you are asking for, but you might consider another approach where you define a template language that can be type-checked like Haskell but gets translated directly into GPU code without an intermediate Haskell translatiom. If this interests you then google the "finally tagless" paper for some inspiration. I'd link it but I'm on my phone at the moment.
But we're any of them *fast* 12 years ago? 
How to represent things depends a lot on what exactly you want to do with them. For example, if you're primarily concerned with converting things into some sort of normal form, then it often helps to use the free X as your representation for an X (e.g., sequences for representing monoids, sets for representing commutative monoids). If you're primarily interested in evaluation, then sticking with an AST may be more efficient since there's less overhead. If you're interested in integration and differentiation then Ed's *ad* package is definitely the place to start. But the big thing, I think, is to be clear about what exactly you're aiming to do. Are you only interested in real/complex analysis? Are you only interested in ring/group/order/foo theory? Or are you interested in abstract algebra generally? Because each of those projects is going to be very different and is going to alter the way you want to set things up.
Look at the link that raphael gave you. The project homepage has a link to the paper describing accelerate. BTW I'm about to buy a graphics card just so I can play with GPU programming using Haskell (I wouldn't do it if I had to use C++...). I'm interested on doing non-numerical algorithms, for instance string and text processing, using GPUs. I hope you go ahead with Haskell/CUDA-related project, and if so please post your results on this subreddit. ;-) 
Arrays have always been pretty consistently fast, I think, just a pain to use.
The language is wonderful but most haskeller tend to write as if the code is not meant to be read. All the shortcuts (like $) and their too-clever-for-their-own-good compositions are really hurting and reminds me of perl. Yep, I know what they mean. But I don't code all day long in haskell and after 8 hours in another language, I tend to mix them up and make stupid mistakes. If you live and breath Haskell all day long, that's not a problem. But it puts a silly barrier to entry for beginners/new teammember. And I am pretty sure, even for haskellers, mis-reading is common. 
"Could someone please tell me the difference between data structures, without actually telling me anything about data structures?" "Never mind that O(log).. stuff, just tell me which is fastest!" Seriously though: when in doubt, use the "natural" data structure for your language(lists for haskell). If you are not satisfied with the performance of your program, analyse your algorithms and data structures. Of course this means that you have to learn something about algorithms and data structures! 
I skimmed the arxiv paper quickly. It looks like an interesting language idea - I've had that idea for some time, but I never did the thorough work needed to bring it into fruition. The idea of having explicit effects is alluring. I'd probably be interested in playing around with this language!
I'm very pleased to see this and I look forward to reading it in more detail. I'd like to see a move towards algebraic effects in Haskell. Whilst I still want to be able both to write effectful programs which do stuff to get values and to write pure control operators that plumb together computations, I'd cheerfully pay more notation for the latter to make the former cheaper. The separation of effect invocation from effect handling might well be a big help in that respect: it's easy to combine signatures of operations; the interaction of their semantics is a separable concern.
There is also Joel Svensson's [Obsidian](http://www.cse.chalmers.se/~joels) - like Nikola this is a *GPGPU* system. 
What I'm interested in is how does computational effects and handlers interact with, for example, STM? IMHO, STM is the best demonstration of why monads are important for all programming languages, and not just something Haskell is forced to do simply because it's lazy. Monads, and *preventing* side effects, is why implementing STM in Haskell was a long weekend for SPJ (maybe I'm exaggerating a little, but not much) and it succeeded wildly, but Microsoft could dedicate 30 people for 2 years to add STM to C# and fail. 
The [upload list](http://hackage.haskell.org/packages/archive/log) (on the front page) has the entire list. 
This sounds more like a caching strategy to me. You might want to read about write back/through and write-(non-)allocate from computer architecture. Wikipedia features some good articles on CPU caches. :) 
Yeah, I had sort of wondered that. Thanks for the suggestion. I referred to it as VM since it was inspired by Redis's attempt at doing something similar. [http://redis.io/topics/virtual-memory](http://redis.io/topics/virtual-memory). Do you know if something like this is generally done like what I have? Or are the Weak Pointers unnecessary, as the data should get GC'd after it is replaced anyway. I don't fully understand how to predict when things will get GC'd and it seemed like a way to have more control over it.
FYI, I wrote my first blog post recently about Racket: [Restringing a Racket with Haskell](http://warrenharris.blogspot.com/2012/02/restringing-racket-with-haskell.html)
The eff types are like those of ocaml, i.e., they do not carry information about effects. We are planning to equip eff with fancier types that also carry such information, and then what you are asking for will be possible. In any case, even at this point it should be possible to experiment with STM in eff, except that it is not clear to me how much that makes sense without concurrency. If a computation is effectful when it shouldn't be, eff will simply do something strange (perform IO twice when it shouldn't etc). We already have such examples, for example in non-determinism handlers there is a requirement that the computation must be free from effects, and that's not something eff can check at the moment. 
[List of keywords](http://www.haskell.org/haskellwiki/Keywords) might interest you, although there is much more to language than syntax.
Yep. STM is basically only important in the case of multi-(non-cooperative)- threads. But this, I think, is the most important case for functional programming. A "mere" 10x increase in productivity isn't sufficient to force adoption of a new paradigm. This makes sense when you think about from a managers perspective- even assuming that the productivity improvement exists (and it has often been the mirage), you're looking at 2-3 years of significantly diminished productivity as the programmer learns the new paradigm. Given that the average tenure of a programmer at a company isn't more than 3-5 years, and add in the odds that the promised productivity increase isn't as large as promised (or even doesn't exist at all), and it doesn't make financial sense to switch. The reason new paradigms are adopted is because they solve some problem which simply isn't solvable in the old paradigm. OO got adopted because it could do GUIs well, while procedural programming can't. Go program Motif- and remember that Motif was the *best* procedural library for doing GUIs. Multi-threaded programming is, I think, the reason functional programming will be adopted. If types and effects can't support STM (or some variant), then yes, my interest in them drops a lot. They may be interesting from a (type-)theoretical stand point, but from a pragmatic stand point they're pointless. But this isn't to say that they can't support STM (or something that solves the same problem, the problem of thread synchronization in a composable manner). In fact, if I had to bet, I'd say that types &amp; effects did have something interesting to say on the subject, I just don't know what it is.
yup, recently found out about that one.
How does Eff compare to disciple? I'm aware that there are differences in approach, but it sounds like they're both investigating the same space.
Well, disciple has an effect system which we hope eff to get in the future. Eff has handlers which disciple does not. Eff is pure (except for the I/O effect instance std), while disciple has built-in destructive update. In eff you can define computational effects (like monads in haskell) which have the same status as the builtin I/O. Can you define new computational effects in disciple? Disciple uses call-by-value (by default), while eff is "fine-grain call-by-value", although that's a bit of a simplification because handlers also control execution.
Isn't it `(.) . (.)`?
The point wasn't to create *unreadable* clever code, just to make clever code.
There is no computable predicate that tests if a stream contains at least 1 bit.
after glancing a bit at the paper it looks, like it's more formal approach to conditions (beefed-up exceptions from common lisp)
Here's statistics of the operations for the map functions: 378896 ! 1955 elems 58904 fromList 6636 fromListWith 790578 insert 9169856 insertWith 7250582 lookup 15364 toList 
~~So it looks like this won't quite do what I want if used with some a :*: b that does not satisfy HasLoc (a :*: b)~~ EDIT: latest code fixes these problems.
awesome! agree with the docs, could be hugely addictive great work!
great work!
Check [Evolution of a Python programmer](https://gist.github.com/289467) and [Evolution of a C++ programmer](http://synflood.at/blog/index.php?/archives/744-Evolution-of-a-C++-Programmer.html). In every language you can write code in many different ways, including ridiculous ones. No one who seriously wants to compute factorial will reimplement natural numbers or an interpreter from scratch.
As Andrej observes, eff has yet to acquire an effect-tracking type system which would "manage permissions" appropriately. But that's the direction of travel. I also have been thinking a little in this direction, as [these slides from 2007](http://cs.ioc.ee/efftt/mcbride-slides.pdf) might suggest. The idea is that the types of handlers make it clear which effects they locally enable. Locality of effects is something that Haskell currently handles badly (consider, e.g., the usual type of "catch"). This way of working has the potential to be much neater than mucking about with transformer stacks. If I were designing a language (not all that hypothetical, really), I'd be thinking about managing effects in the algebraic style. Meanwhile, Haskell's in need of a new treatment of effects, now that its new treatment of kinds allows us to *index* effect systems. That creates an opportunity to think again...
Thanks, [fixed](https://github.com/pcapriotti/pipes-core/commit/95c0c20b8987785635a77ad827e6360fe35beb31). I'm sure there are other typos, so I'll wait a few days before releasing an updated package.
You should adapt the normalization procedure to use the *normal-order* evaluation strategy. EDIT: also that in `step`.
Any particular reason this is released separately from pipes?
I didn't permit it to go in the main library because it it violates the category laws. I have other objections to his library, but that's the most important one. I'm working on a separate error-handling implementation. He asked if he could release his work as a separate library and I said that was alright, but I just want to make it clear that I don't endorse his library. The reason his error handling violates the category laws is because it permits upstream flow of information in the pipeline. A pipe type that allows bidirectional flow of information can't be a category. Specifically, it will always violate the identity laws (which are essentially the laws that enforce the unidirectionality of pipes). This is not just some abstract objection. Every time he presented me with an implementation that used upstream information flow to handle errors I immediately checked the case for upstream flow against the identity law and showed that it violated it. Every time I demonstrated the counter-example his implementation grew increasingly complex to try to hide the error without fixing the underlying problem (look at the source code if you don't believe me). Most importantly, you don't need bidirectional pipes to correctly finalize pipes upstream of the returning pipe. My error handling approach solves this quite elegantly by just having every pipe pass its finalizer alongside its yielded value so that it can be properly finalized if the downstream pipe never awaits from it again. You can actually show that my error handling approach is a special case of a monoidal fold from upstream to the returning pipe (it folds finalizers from upstream pipes to the current pipe) and a comonoidal unfold from the returning pipe to downstream (it unfolds the exception value to each downstream pipe). When I mentioned that I had other objections to his approach, it's that his is essentially ad-hoc while mine is grounded in theory and is very general. To illustrate this, just look at his "ensure" command, which only makes sense in the context of the IO monad. How would "ensure" make sense if the monad were "List" or "Cont r"? He essentially needlessly specialized the entire library to only be useful for the IO monad and violated the category laws anyway. Edit: I've posted a comment with the proof that it violates the identity laws [here](http://www.reddit.com/r/haskell/comments/qq5p6/pipescore_001_released/c3zpp82)
Yeah, I will. I'm busy this morning but I will post something later today. He added more complexity in the last few days in an attempt to patch my last counterexample so I have to go over it again.
I don't think the bar chart says anything about SPJ's quote, but your post text says you're trying to rile people up.
It's nice, but I still find it disappointing that you had to inject a `Print` constructor to get a printing function. Done this way, it looks like cheating: for which other functions that I would like to implement will I have to escape by hardcoding a constructor? I would rather have a more general `Var` case; I suppose the whole type would be parametrized over its content type, and you would get a nice monadic structure. Do you think that approach would work as well?
You've provided `pipes-conduit`, which can basically take any conduit and transform it into a pipe. Can you also provide the reverse? (transform any pipe into a conduit) If so, then what appeal is there to using `pipes-core` and friends instead of `conduit`? If not, then what additional power does `pipes-core` provide?
He specifically asked for permission to use those names and I didn't object, so no foul. The main reason I didn't object was that I was already planning on keeping utilities in the main package.
Fair enough; still, considering naming conventions like http-{enumerator,conduit}, zlib-{enumerator,conduit}, etc., it's going to be difficult for the two packages to coexist if anyone bases libraries on them.
&gt; My error handling approach solves this quite elegantly by just having every pipe pass its finalizer alongside its yielded value so that it can be properly finalized if the downstream pipe never awaits from it again. What if there is no yielded value? And how do you know when you won't await anymore? Does the programmer have to say so explicitly?
&gt;When one suspects a troll, check their history. This guy has no recent history in haskell reddit, and I'm surprised that some of his recent posts elsewhere haven't been banned. Because disagreeing is so odious.
It were lovely if more of these packages were worked on as of last year at least.
Thanks! This is indeed troubling, and pretty much disqualifies pipes-core until/unless it's fixed. Here's a counterexample for associativity, since I just tried it and lucked out: &gt; runPipe $ (p &gt;+&gt; idP) &gt;+&gt; return () 1 &gt; runPipe $ p &gt;+&gt; (idP &gt;+&gt; return ()) &lt;nothing&gt;
Alright, that's a bona-fide counterexample, I will definitely look into that. I don't think it's a fundamental problem, rather a just mistake in the implementation, because when I do the math on paper to check what the two results should be, they do turn out to be identical. &gt; Like I said before, any attempt to solve error handling by communicating upstream always invariably violates the identity laws. I don't see why that should be true. The original "simple" pipes do communicate upstream in a way (a terminating downstream pipe terminates upstream) and they form a category. &gt; On a side note, his "ensure" command does not actually ensure that the action gets executed, as the first of the two examples shows. That's correct, and intended. Maybe the naming could be improved. 'ensure' is actually a low level primitive, which shouldn't be used directly. The correct combinator to ensure finalization is 'finally'.
Of course, this definitely needs to be fixed. I have a partial proof of the category laws, but the termination cases are still a little bit fudgy, and indeed by Murphy's law they turned out to be wrong. :) 
It means people tend to claim they are when they first start a package, and people often forget or disregard the stability field altogether, so it often stays that way. It doesn't actually say anything at all about the quality of the libraries.
A word of advice: reddit's spam filter hates link shorteners. I recommend using the full URL when submitting links, otherwise it's likely to end up being removed until a moderator rescues it.
Sorry, didn't know. I thought the statistics feature was worth the while. Remorse level 8/10 won't do again.
Some errors I caught: &gt; GHCi treats `min -3 4` as `min (-) 3 4` (section 2.1.1) Actually, it treats it as `min - (3 4)`, a subtraction. &gt; `foo (bar 10)` [...] `foo bar 10 -- this is equivalent to the above` (section 2.1.3) Function application is _left_ associative, so `foo bar 10` is equivalent to `(foo bar) 10`.
Ultimately it's up to the users. However, generally if something violates the laws it tends to not behave the way you expect. A great example is [ListT](http://www.haskell.org/haskellwiki/ListT_done_right), which does unexpected things because it violates the monad laws. Obviously it's out there and people use it, but they often tear their hair out when it breaks. The reason category theory works so well is because it is like a universal language for every field, not just math and programming. This means that if you can formulate your thoughts in whatever concept you are interested in terms of categories, then you can then leverage your intuition to reason about anything that you can naturally transform it into. If you violate the laws, though, then your intuition doesn't match reality and things start behaving unexpectedly. This is why pipes are easier to use and more intuitive than other iteratee implementations, because they are structured along category theory idioms and they strictly obey the laws. A great example of this is how I knew his implementation violated the category laws before I had even proven the counterexample. I've built up a completely non-mathematical intuition for how pipes work, but because there is a categorical translation of my ideas then I can reason about any pipe implementation mathematically by transforming my intuition directly into the mathematical equivalent. This natural transformation of my intuition into the mathematical description is why I know that any implementation that allows bidirectional information transfer will violate the identity laws and I can work backwards from the mathematical statement to generate a constructive proof of the violation (i.e. the counter-example). However, doing so requires defining a natural transformation from his implementation to my implementation and that is what takes up the bulk of the time for my disproofs.
It would be *really* cool to see all (or as many as possible) of the tikz examples [here](http://www.texample.net/tikz/examples) in written in diagrams. I might try some of the simpler ones to get warmed up. cabal installing now.
I also think that unless you need to cover the entry level topics to gather your thoughts you're better off skipping it and going straight for the harder topics with less coverage. Everything from the most basic haskell syntax up until monads has been covered quite extensively already by other resources, it's the topics beyond that that require another book. Unless, like I said, you need to gather your thoughts, which is perfectly fine too.
I think it's very admirable that you're attempting something like this. The tone of the book is very readable, so good luck! If you haven't already, I'd recommend reading [Programming in Haskell by Graham Hutton](http://www.cs.nott.ac.uk/~gmh/book.html) as it's aimed at beginners too. He's great at explaining complex things in a simple way.
No. You're trolling. Either learn the difference, or leave.
Nobody cares about your shitty work. Contribute or fuck off.
`chunk` is from Data.List.Split in the [split](http://hackage.haskell.org/package/split-0.1.1) library.
I greatly look forward to your next pipes release. I have used them enough to know that I really enjoy the basic idea. But, to really use them I need a correctly working implementation with good resource finalization and exception handling. 
ok - so here's a question... how come hoogle didn't find chunk via it's type signature?
Technically it's indexed, but you have to add `+split` to the query to tell it to search in that package, which kind of defeats the purpose.
Sure! And if there are features needed for some of them that diagrams doesn't yet provide, [file a feature request](http://code.google.com/p/diagrams/issues/list). We definitely intend for diagrams to become a viable alternative to TikZ.
do clients care about fp? i can't tell who is a prospective client
As far as I understand, redis (and you) want to keep a reference to a data object. There are two different things to consider here: 1) Your caching strategy (see above) 2) Getting rid of old an unused references (garbage) Since I don't know your background I'd recommend reading up on Garbage Collection (again, Wikipedia :) There are several techniques available for GC and each has its own pros and cons. (Naive) Reference Counting is easy to implement but might leave you with a lot of junk lying around. You'd probably want a generation model, employ a heuristic to figure out when your load is not to high to perform gc, compaction, maybe coloring, etc. That's quite a task. For starters I would try simple reference counting and then tune to my desired behaviour. If you have something working and it's intended for the public, let me know, I'm kind of curious about it. :)
As long as your sales guy has something new and flashy to talk about, he'll make sure they care :) 
I think you can also install Hoogle locally and let it index whatever you want.
I guess that the text output would need to support LaTeX? :-)
Thanks. I was afraid that the style was confusing.
You may want to have a look at Bj√∂rn Buckwalter's awesome [dimensional](http://hackage.haskell.org/package/dimensional) library that solves (almost) all your unit problems. :-) By the way, I'm dabbling with audio programming, [too](https://github.com/HeinrichApfelmus/tomato-rubato), but using SuperCollider.
I don't quite understand what exactly they are selling, or if they are selling anything at all, but I think it's a great website for self-motivation. Whenever a project doesn't compile, I can always read "Our vision is to double the productivity of serious software developers, while unleashing the massive computing power of modern electronic devices." and feel good about *unleashing* da power.
Just to clear things up a bit. This is foldr: f(x1, f(x2, f(x3, f( ... )))) And this is foldl: f(f(f(f( ... ) xn-3) xn-2) xn-1) 
But there are so many potential utilities, see conduits: networking, zlib, xml, specialized networking (http)... Would you put all that in the core package, including all those dependencies?
I think it might be useful to note, perhaps in both packages, that they're similar but different. Especially as pipes-core started out as a fork and even the Cabal descriptions are almost identical. It is very confusing, particularly to beginners, when packages are similar enough to look like either is deprecated. As an example I recall I was confused by mtl vs transformers at some point ...
Saying Haskell requires IQ above 130 serves no purpose. Haskell is difficult for people who are already set in their procedural/OOP programming ways. I do not consider Haskell as complicated. And if it were, and required high IQ to work, I would actually consider it a bad language.
If you mostly want to avoid wasting time, perhaps instead of writing about Haskell you could use it for something. Contribute to an open source project (or start your own), for example. I know that no matter how much I read about cool features of Haskell, I learned much more when I started using them, debugging the type errors, and trying to understand what they were good for.
I took a look at dimensional a bit ago, obviously a problem like this is likely already solved. But it has particular units, and seems to be optimized for SI or decimal or whatever other specific cases. I'm interested in a general unit system where I can easily invent my own units. Seems like a really cool library though. But also it's largely an exercise for me in how to do this sort of thing right. Though it may be worth having another look to see how Bj√∂rn has does things. Looking closely, it looks like Bj√∂rn may be restricting the use of basic arithmetic operations in a way I didn't think possible, it would be awesome if I could implement Num after all. I guess having heard of SuperCollider I don't actually know much about it. From reading your project's description, it seems like it's not just a straightforward wrapper? Is SuperCollider more of a library than its own system?
It's pretty hard to get hard objective evidence, it's not easy to conduct controlled trials and account for all the variations. Certainly there is anecdotal evidence that teams of people using FP can be signficiantly more productive than comparable teams and factors of 2x or more are not unheard of. But was that because of the FP or did the FP team just have smarter more productive people? Could you retrain the Java people and improve their productivity? So no, there's not much hard evidence, but hard evidence in this area is extremely difficult to obtain.
And I'd add that evaluation is outermost first.
Yes. The simplest way to achieve this would be with a TikZ backend. =)
Learning how to write a book is a skill like programming that you can only pick up by doing it. It doesn't matter if people don't think you're qualified, and you are right not to heed their say-so. Go for it.
I'll probably do that, eventually. The book is so much fun to write, though. I was thinking, maybe I'll include a full-length program somewhere at the end. Something like a basic database or text editor. In the summer I'll have more time. For now, I'm a bit tied up in other things, but I still try to work now and then on the book.
Okay, maybe I worded it wrong. I was trying to say something like "you need to either be smart, or work on it". I'll change the wording to sound less offensive. Ha! changed it: ~~An IQ over 130~~ An inclination towards the abstract Notice: changes don't actually show up immediately in the google document; I'm not working directly on it.
Bj√∂rn has a few examples where you can make your own units, I think. In any case, his approach is definitely worth copying. SuperCollider is its own system, but fortunately they have separated it into a server that does sound synthesis and a programming languages that communicates with the server. The latter can be safely discarded in favor of Haskell. Rohan Drape's [hsoc][1] library is a direct wrapper on the server component. Still, the SC server contains a few design decisions that are weird from a Haskell point of view, that's where my library comes in. It present the whole thing in a way where you don't need to know anything about SC anymore. [1]: http://hackage.haskell.org/package/hsc3 
Sure, there's some stuff to do with slick tools like IDEs, debugging and profiling etc, but as Don says, the bigger issues are not in the technology.
This is now [fixed](https://github.com/pcapriotti/pipes-core/commit/68089b9ed11835c1a3934a35db0396cc9127c7e5) in master.
Good point, I'll add an explanation of the fact that pipes-core is an incompatible fork of pipes in my next release.
Cool. Well, I very well may discard my stuff for Dimensions at some point after I learn the lessons there are to be learned here.
I do see that as a danger, to some extent, but I see it as a positive sign that they've decided to start with Haskell. It would be quite hard to turn Haskell into PHP.
In case it's not clear, they're working with existing people and companies, see the [partners page](http://fpcomplete.com/partners/) (disclamer: including Well-Typed).
Sure, PHP was cool in the beggining
I've always read it as "fold-from-the-right" and been terribly confused. Thanks for that simple clarification.
"First, go to the end of the infintie list." :P
I think you're being too apologetic at the beginning. Don't say that the book will serve no purpose, or that you had no reason to write it. You don't need to worry about needing to apologize to connect with the reader as a friend and not sound threatening - everything you have written is in a nice friendly style. Of course, this means that you'll have to decide on some goals you hope the book will accomplish, and state those instead. And then live up to it :). But you can always change it later if things seem to go in a different direction.
Would something like literate Haskell help you to embed a book inside your source and avoid these issues? http://www.haskell.org/haskellwiki/Literate_programming might be a good start.
If there is one thing that needs improvement, it's GUI tools.
That is not foldl.
FWIW, maybe the biggest non-technical weakness with Haskell today is the lack of mid-level documentation? That is, there are many introductory tutorials and a couple of books covering slightly more advanced topics, but their scope is still very limited by professional standards. There are also lots of serious, heavy, scary papers at the bleeding edge, which mostly seem to be of interest to researchers and those working on the Haskell language itself. However, there is (to my knowledge) nothing comprehensive and well-established to bridge that gap, which I suggest is the region where most practising professional programmers who might be interested in adopting Haskell for such work would fall. I'm not familiar enough with the language and community myself yet to know whether this is because no-one qualified to produce such material has had the time/inclination/funding to do so, or whether the community isn't large and collectively experienced enough to have formed a consensus on "best practices" at that kind of level yet that might be captured in that way, or whether such material actually does exist and I just don't know where to find it. Perhaps you have a better idea. In any case, I suspect this plateau in the training material available is at least one major reason that more interested programmers don't get into the language enough to commit to it for "serious" projects. No-one has written the books on the level of *Effective Haskell* and beyond yet, but you can only read so many not-that-great monad tutorials and write so many quick experimental programs before you want to get into something deeper, or give up.
&gt; Why do you think you're qualified to write a book? Quite right. No-one should ever write a book unless they've already written one before. 
Not sure if trolling in good fun or if trolling in criticism.
The argument order is switched, right? I did that deliberately to help anacrolix see what it was doing.
I'm glad to hear you are taking the time to focus on modularity. Trying to install Leksah is always painful and using it is often buggy (last I checked) so refactoring the code base is a feature to me.
cabal install --extra-include-dirs=. gtksourceview2
http://code.google.com/p/leksah/issues/list
My attitude is that a person's time is too valuable to waste (which is why I'm on reddit, 24/7). Trying to build a halting oracle is (probably) a waste of time for most people; not for everyone, since you might learn a lot, or create something which works for all practical purposes, but it's not a good bet. Similarly, writing a book is a big task that may end up being a massive waste of time. Or it might be a good learning experience, even if the book itself isn't particularly useful to others. Or you could end up refining the book later. Or it might be exactly what someone needs. Regardless of whether OP decides to continue with the book (which seems likely) or put efforts and talents elsewhere, those are questions which should be asked. Also, there are many ways to be qualified to write a book on a subject without having ever written a book before. Coming up with the subject, for instance.
Ah, okay; I understand now.
I remembered of a post that showed up here a while ago [How to read Haskell like Python](http://blog.ezyang.com/2011/11/how-to-read-haskell/) I don't think it is exactly what you want, but it might be interesting
The world does not need another Haskell book that has the reader manipulating lists and tuples and typing in arithmetic expressions with GHCi. In three chapters and you haven't even told me how to compile a program with ghc...? This is the fundamental problem with Haskell. I am beginning to think this language simply is not documentable. Sorry to be negative. But I have read almost every book and wiki and tutorial on Haskell and they are all virtually identical --- by which I mean they are virtually content free and ultimately useless. E: Please feel free to go back to the drawing board and try again.
If you want to write things from scratch, it is still recommended that you factor out the task of uppercasing a character, then map that over the string. FYI, the Char module already has a toUpper function: import Data.Char (toUpper) capitalize_string :: String -&gt; String capitalize_string s = map toUpper s -- or just: -- capitalize_string = map toUpper I recommend going to Hoogle to search for the function you want. You can search by type, for example, typing "Char -&gt; Char" will show you the "toUpper" function. http://www.haskell.org/hoogle/
You can think of `map` as taking a function that works on a single item and turning it into a function that works on a list. So if you want to write `capitalize_string` in terms of `map`, you should start with a `capitalize_char`, then give it to `map`, which will make it work on a list of Chars (a String). So based on that, you can say: capitalize_string = map capitalize_char (If you prefer to include the argument, it's just `capitalize_string a = map capitalize_char a`.) Then you can write `capitalize_char`, which is pretty much what your original `capitalize_string` is, minus the `map`s: capitalize_char a | ord a &lt; 97 = a | ord a &gt; 122 = a | otherwise = chr (ord a - 32) There are two other changes here besides just removing the `map`. First, I've changed the formatting to match how Haskell code is usually written (using spaces to apply a function, rather than wrapping the argument in parentheses). Second, and more important: the guard clauses (the code between | and =), have to evaluate to a boolean -- `ord a &lt; 97` is either `True` or `False`, so it can go there, but `map (\x -&gt; ord(x) &lt; 97` doesn't (it's also not valid syntax, but maybe that's from Reddit's formatter mangling your code). And one final side note: `chr` and `ord` are already defined, you don't have to include them yourself. Just put `import Data.Char` at the top of your file. _Edit: typos_
this seemed to be my problem, was driving me nuts
I use Haskell in an enterprise environment with no troubles. cabal-dev is great. I recommend using it for all Haskell development, enterprise or not. I'm not sure what problem this cabal-nirvana is trying to solve.
It is still not clear. What are they providing that the partners, and possibly the IHG, don't provide? It seems they want to act as a central contact point for people who needs FP contractors.
I will cover this extensively, but due to the nature of Haskell, efficient I/O usage is an *advanced topic*. My problem (and I'm sure others') is that, at first I didn't have the basics (manipulating lists and tuples etc.) become second-nature, so I became stuck later on. . The book will have enough detail later on that the reader may only skim the first chapters and go for compiled programs, advanced I/O etc. (i.e. the **useful** stuff - not that basics are not useful)
How about: instance Coapplicative NonEmpty where copure = head cozip (Left x :| zs) = Left $ x :| catLefts zs cozip (Right y :| zs) = Right $ y :| catRights zs 
The following is related: http://sneezy.cs.nott.ac.uk/fplunch/weblog/?p=69
The complicated versions in Haskell are doing a lot more than the complicated ones in Python/C++. It would be and look even more complicated and ridiculous to implement them in Python/C++.
This could also be useful: class (Functor c) =&gt; Cotraversable c where cosequence :: Coapplicative f =&gt; (f a -&gt; b) -&gt; (f (c a) -&gt; c b)
&gt; What are catLefts and catRights? catLefts = foldr (either (:) (const id)) [] catRights = foldr (either (const id) (:)) [] &gt; Are they total functions? Yes.
That's weird - your coapplicative instance for comonads only uses counit. Is that what we would expect?
Ah, those things. They're total on nonempty inductive lists. They're not total on nonempty colists. But there are implementations that would be total for both.
It uses functoriality, but not the cojoin, it's true. It's quite tricky to write functions that do somehow exploit the cojoin of an arbitrary comonad. I wonder if something more fun becomes possible with costrength.
OK. I was just providing an instance for a very concrete and well-known Haskell type: edwardk's non-empty list type, from the [semigroups](http://hackage.haskell.org/package/semigroups) package. It seemed like it would be a far more interesting example than `Id`.
Store is coapplicative: instance Coapplicative Store where copure (Store f s) = f s cozip (Store f s) = case f s of Left a -&gt; Left (Store (either id (const a) . f) s) Right b -&gt; Right (Store (either (const b) id . f) s) And so is Costate: instance Coapplicative Costate where copure (Costate _ a) = a cozip (Costate f (Left a)) = Left (Costate (f . Left ) a) cozip (Costate f (Right a)) = Right (Costate (f . Right) a) 
I was playing with `Cotraversable` this weekend. I got it to do `fmap` with `Identity`, `transpose` with `ZipList`, and `unfold` with `Constant m`, where `m` is `Splittable`: `split :: m -&gt; Maybe (m, m)`. But it seems to need a completely different kind of Coapplicative, and the Cotraversable instance for lists I tried to make for all Comonads, seems to be useless. https://gist.github.com/1975599
There's nothing necessarily contravariant about your `Inapplicative` class above, other than the constraint you've placed on it. Both (√ó, 1) and (+, 0) are valid choices for the monoidal structure of categories that have them (and for Haskell, ignoring some things). They're also valid for the dual category, so you could switch the `Functor f` with `Contravariant f` in the first-order formulation of `Applicative` to get something else. There are multiple axes on which you could 'dualize' `Applicative`. Sum vs. product, covariant vs. contravariant, lax vs. colax.
I already build my projects directly from GitHub using Jenkins. I've also set up build bot for most of http://github.com/haskell
Indeed, it's not necessarily contravariant at all. It's just a straightforward equivalent of `Applicative` for a contravariant functor. The same class instead defined on `Functor` would be something rather different with no clear relationship to `Applicative` specifically, beyond the basic definition of a monoidal functor.
 instance Costrong Stream where costrength s@(Left x :&lt; _) = Left x costrength s@(Right _ :&lt; _) = Right (rights s) where rights (Left x :&lt; t) = rights t rights (Right x :&lt; t) = x :&lt; rights t Does this instance for the stream comonad satisfy the costrength laws?
Nonempty lists may be concrete and well known, but catLefts (repeat (Right 3)) is still bad news. It's quite standard for Haskell libraries to jumble together works-only-for-finite and might-make-infinite operations, with a caveat emptor policy. It's not a standard that makes me happy.
cool, this is the first haskell shell thingie i would consider using kudos to the yesodweb team for pushing practical haskell forward, you guys are doing a great job (and i really appreciate your dedication to good docs)
I don't think `$` is a shortcut. Maybe it's because I come from a CS/Math background, but a super-common and useful piece of notation is 'this expression extends as far to the right as possible' as a way of eliminating parentheses reading problems. I find `f $ g x y z` a lot more readable than `f (g x y z)` especially when x/y/z may also have their own sub-expressions. From an actual piece of Haskell code: allTrees f a = Node a $ map (allTrees f) $ f a compared to allTrees f a = Node a (map (allTrees f) (f a)) Similar, but I find the `$` version more readable. As expressions become more complex that readability helps.
Meanwhile, how about instance Costrong Stream where costrength (Left x :&lt; _) = Left x costrength (Right y :&lt; zs) = Right (y :&lt; smear y zs) where smear y (Left x :&lt; zs) = Right y :&lt; smear y zs smear _ (Right y :&lt; zs) = Right y :&lt; smear y zs This version uses a productive padding strategy rather than a skipping strategy for elements tagged differently from the one in focus.
Thanks. I believe I have corrected it now by replacing `either id counit` with `right counit`. I copied my laws from [wikipedia](http://en.wikipedia.org/wiki/Strong_monad) except I flipped all the arrows around.
Such an instance won't satisfy the `costrength . fmap costrength . cojoin = right cojoin . costrength` law. Given `x = Right 1 :&lt; Left 'a' :&lt; Right 2 :&lt; Left 'b' :&lt; ...` we have right cojoin . costrength $ x = Right (1 :&lt; 1 :&lt; 2 :&lt; 1 ...) :&lt; (1 :&lt; 2 :&lt; 1 ...) :&lt; (2 :&lt; 1 ...) :&lt; ... while costrength . fmap costrength . cojoin $ x = Right (1 :&lt; 1 :&lt; 2 :&lt; 1 ...) :&lt; (1 :&lt; 1 :&lt; 2 :&lt; 1 ...) :&lt; (2 :&lt; 2 ...) :&lt; ...
You're quite right. Even if I remove my superfluous Rights from the rhs of smear, I get something that disobeys that law. Other padding strategies seem similarly troubled.
Major worries for me would be the ghc runtime under 24x7 stress (java is still causing trouble after all these years), managing large code trees (few success stories), and recruiting CS (non math) haskellers (least risk of low productivity). Wrt training - getting up to speed in whatever is the employee's responsibility these days. A company that really knows how to tune the ghc runtime would be a valuable asset to the commercial viability of ghc I think.
&gt; It may also be worth noting that my formulation of Decisive and the OP's Coapplicative are not the same, in that &gt; `nogood :: f Void -&gt; Void` &gt; doesn't give you copure. It does if f is costrong: counit = either id (void . nogood) . costrength . map Left However, there are comonads that aren't costrong in Haskell. But, we don't really need costrength, we need: vdist :: f (Either a Void) -&gt; Either a (f Void) Which is definable for all comonads of course, and is in fact definable on a case-by-case basis for a lot of functors: vdist@comonad = map void . extract vdist@Id = id vdist@(F * G) = vdist@F . fst vdist@(F + G) = [ map Left . vdist@F, map Right . vdist@G ] vdist@(K X) = Right . Const . runConst At least, I think those are accurate. Edit: incidentally, Stream is an example of a comonad that is not costrong (in Haskell). costrength is, "if there are any Lefts, give me the first (or, one, I guess). Otherwise, give me back a stream of all the elements with the rights removed." _But_, in a classical setting, all functors are costrong via: costrength f = callCC (\k -&gt; Right $ map (either (k . Left) id) f)
Cool, I have a hard time seeing the Haskell is a vast improvement in readability, though.
They brought up one important tangential point that I'd like to highlight: &gt; Git is winning as version control only due to one of its technical merits: speed. The rest of the reason is Github. If your repo is on Github I know I can look at the issue tracker and even look at forks. I am not against using darcs for version control, just please also make sure to have an issue tracker. I don't honestly believe that people these days are using git over darcs just for speed. It's all github. Github provides an extremely convenient universal location to 1) host your version-controlled code, 2) track bugs, 3) track forks, 4) mediate "pull requests", 5) *comment* and allow other social interactions upon all of these aforementioned things, and even throws in 6) a wiki. This is killer. Github really got it right. I wish there were a darcs alternative that provided all of these features, because I really believe darcs is the long term way to go, but until that happens, it's git and github for me.
&gt; whenever a project doesn't compile So in Haskell, pretty much anytime you have a bug. :)
Browsers need Haskell scripting support :( alas we are stuck forever with JavaScript.
It would be nice if there was a little radio button: "Prelude only", "base only", "Haskell platform only", "All packages". And then a link for advanced search: only exact matches, search only given packages, etc. When is the last time haskell.org/hoogle was edited, and what are the chances of getting a GSOC project to work on changes like this? Installing your own hoogle is only as good as the packages you've installed, so encouraging people to just use hoogle on their own machine isn't necessarily the ideal solution.
I just enabled it for my crypto library and it found a bug lingering on my end as well (in my hand-written `configure` script, on Linux.) The fact Cabal doesn't track dependencies needed for tests even with `--enable-tests` is unfortunate, but oh well. Otherwise, I'm very happy with this and will likely start using it for all my projects - and it beats having my own Jenkins instance. :)
FP makes sense and has done for decades. No amount of popular, bad ideas like Java or PHP can change this. This robustness is what makes FP so attractive to those who care about "stuff that works proper."
FYI if' is the catamorphism for Bool. That is, if we pass in the two constructors, we get an identity: \x -&gt; if' x True False == x. In order to be more consistent with existing folds (either, maybe, foldr), the value on which to fold appears last in the argument list. Therefore: if' :: a -&gt; a -&gt; Bool -&gt; a 
I think part of it is also that certain high-profile projects use it.
Underscores are better because they are easier to read by those whose first language is not English and, especially, it their first alphabet is not Latin.
This makes me nervous, because then you might have `foo 3` and `foo "3"` do two different (but likely very similar) things, and it starts to feel a bit like JavaScript.
Yeah, I read that sentence as just an awkwardly-worded way of saying "of all the technical ways that git is winning, only speed is a really convincing argument to use git over darcs; all the real arguments are social (and non-technical)".
Well that's sort of a chicken-and-egg issue. The reason these high-profile projects use github is because of all those nice social features. If we had a comparable darcs service, I wouldn't be surprised to see big-name Haskell projects switch over.
[Idris](http://idris-lang.org/) seems to manage just fine with ad-hoc overloading. Sure, you'd have to disambiguate things like bar = baz . foo but only if it shows up on the top level. Otherwise it should be able to be inferred from the context in which it is used. And it is already considered good Haskell style to provide top-level annotations.
Shouldn't costrength be of type `f (Either a b) -&gt; Either (f a) (f b)` since it's inverting the `Either (f a) (f b) -&gt; f (Either a b)` ?
I would like to read this course. To bad that I have start working and have only time to study to the courses I have failed. :(
AFAIK, [strength](http://en.wikipedia.org/wiki/Strong_monad) is always defined as `a ‚äó m b ‚Üí m (a ‚äó b)` so I think my type for costrength is correct. (Although that wikipedia page defines costrength to be `m a ‚äó b ‚Üí m (a ‚äó b)` so I guess I cannot win.)
&gt; But, in a classical setting, all functors are costrong via: &gt; costrength f = callCC (\k -&gt; Right $ map (either (k . Left) id) f) The more Haskelly person can write: costrengthM :: (MonadCont m, Functor m, Traversable f) =&gt; f (Either a b) -&gt; m (Either a (f b)) costrengthM f = callCC (\k -&gt; Right &lt;$&gt; Data.Traversable.mapM (either (k . Left) return) f) noting that this only works for traversable functors. (Are all functors traversable classically?) However, bear in mind this is costrength for a functor, which may not necessarily be costrength for a comonad, even if the functor is a comonad.
I don't think ecosystem is non-technical. You can argue that there's a social component to a technology ecosystem, but extra features available to users (in this case what github provides) aren't purely a social distinction.
&gt; To be honest, I prefer hyphens the best (Racket style) I couldn't agree more :-) &gt; but that's not going to happen anytime soon in Haskell. Well, that's never going to happen as a change to Haskell proper. It's just too big of a change for too small of a benefit. But anybody could write a LANGUAGE extension for GHC that allows hypenated identifiers. We could have hyphenated identifiers tomorrow if somebody who knows enough about GHC decided to implement such a patch. But that does raise the issue of interoperability between libraries that export hyphenated identifiers and code that doesn't use them, which would have to be dealt with before this would have a chance of being accepted into the main line. One approach might be to translate between camelCase and hypenated-identifiers, so that concatMap would (or could) become concat-map in code that uses the extension. And an identifier like "snazzy-sort" that is exported from a library would become snazzySort in code that doesn't use that extension.
Section 3.2.2, on typeclasses, feels more like a reference than a book. I am not sure whether it is the "bullet" form, or the dry iterating over a few "examples". Perhaps that stuff should be in an appendix, and an example of a single typeclass, like Ord or Eq should be explained in detail instead.
Also, isn't saying that a typeclass "includes something" a little wrong? I mean, types can be instances of a typeclass.
"Contribute to an open source project" should be a cliche already. Either give a concrete example of which project to contribute to and why, or don't propose it. I mean there is no reason whatsoever for a person to contribute to an open source project they are not interested in, and the ones they are interested in - most of the time are hard to contribute to because they either have a very high barrier to entry (e.g. any apache based project) or are so stuffed with people already contributing, that one finds it almost impossible to do anything useful (e.g. django).
I think being "negative" is the whole point of his posting his book here, so I think that the author does not mind negative feedback. It will be critique, not praise that will improve the book. The only danger is that the author does not become discouraged and stop the work on the book completely. But, I suppose, that is the obstacle that everyone must overcome, and what separates the finished books from the unfinished ones.
Off topic: that is a cool project. Looks similar to TikZ for Latex...
Why the downvotes? I think it is a valid point. It is up to the author to make the final decision.
Let the compiler do the const-folding, instead of you. capitalize_char a | inRange ('a','z') a = chr (ord a + (ord 'A' - ord 'a')) | otherwise = a 
how I feel about all of the bash scripts I've written: http://wwwdelivery.superstock.com/WI/223/4259/PreviewComp/SuperStock_4259-27859.jpg
As an alternative to hyphens, one can use single quotes: `apt'get`.
I can see a case for that, although well-chosen names make it much less important. Note that hyphens, being in the middle of the line rather than the bottom, do not suffer from the same visual problem (imho): foo_ bar (baz biz-baz) bar-bar bar foo 
&gt; An open source project can't just have a code repo whether it is using darcs, git, or something else. It also need documentation This is a major issue in Haskell for me. Without naming names, a lot of major or one-of-a-kind packages come with no documentation except a few function descriptions with Haddock. IMO this is a huge barrier for any beginner trying to get into Haskell.
Excellent little writeup. A nice, simple demonstration of `parallel_` from [parallel-io](http://hackage.haskell.org/package/parallel-io) and simple usage of [hxt](http://hackage.haskell.org/package/hxt). You don't even have to know about arrows, just believe that hxt magically knows how to implement `&gt;&gt;&gt;` in the way you desire.
System Requirements =================== Leksah currently requires: ... GHC version 6.10.x-6.12 
&gt; Dangling pointers are easy to introduce, and there are many places both in the compiler and the RTS itself that can violate this invariant. Tracking down these kinds of bugs can be extremely time consuming. (It is, however, one of the author‚Äôs favourite activities!) Maybe you mean: one of the author's favourite nightmares? 
linked post fo this thread sais: &gt; Changes to the last release include: &gt; * Support for GHC 7.2 and 7.4 maybe you are seing some bit of doc that needs to be updated.
I'll list any typos I spot as I go through: * In the middle of 2.3.3: "An Id is a Name with extra information: notable a type" - "notable" is probably meant to be "notably". * Second paragraph of 3.1 "the **intersted** reader" This is really nice so far, very interesting.
Yes, posting it here is fine. Job ads that don't even involve an FP language would very likely be removed, even if they were looking for Haskell programmers for whatever reason, beyond that it's up to the voting. That said, you could also look at http://www.haskellers.com/ as well as the various [mailing lists](http://www.haskell.org/mailman/listinfo). Not everyone uses reddit.
Finding bugs in the RTS might be busywork in the sense that better language support might eliminate the need to do it at all, but I disagree that it requires little mental effort. It is the most enjoyable kind of puzzle-solving, and uses most of my brain cells. That's why I love it.
Oh, I didn't mean to imply RTS bugfinding as a whole requires no mental effort ‚Äî just pointing out at least one of the reasons that tracking down low-level bugs mechanically can be enjoyable. Thanks for all your work on GHC :)
i've been meaning to look at nix. how is the windows support?
Very interesting reading. One typo on page 14 - "elimiate".
its not supported (you can try to use cygwin but even that will likely be half-broken)
I have ordered [Real World Haskell](http://www.amazon.com/Real-World-Haskell-Bryan-OSullivan/dp/0596514980/) from Amazon hoping that it's a book that will close this gap. The table of contents looks very promising.
I might get downvoted for this but out of curiosity why are you so intent on using Haskell for web development over proven technologies? Correct me if I'm wrong but at this point haskell still seems to be a very immature web development language at the enterprise level to take such a business risk. 
It surprises me that you'd think that 'mature web development languages' like ruby and php aren't a blood-chilling business risk.
But you see, PHP is a proven technology. Sure, it's proven to be *terrible*, but that's a trifling detail.
&gt; You can always fall back on using java libraries if there's no scala library to do what you want to do. It's also pretty easy in Haskell to fall back on using C libraries via the FFI if there's no native Haskell library. For some tasks (though probably not web development) that can be more useful than having access to Java-based libraries.
What haskell lacks IMO is a nice syntax for pseudo-cartesian-closed categories.
Unfortunately IIRC, the parallel cabal-install is only at an inter-package granularity, but Simon Marlow [mentioned that it would be a "fun project"](http://comments.gmane.org/gmane.comp.lang.haskell.glasgow.user/20509) to add a parallel --make to ghc (although then we'd have parallel builds in *both cabal-install and ghc one inter-package and the other intra-package I believe). Also, I think either Duncan Coutts or Edward Kismett was saying their eventual goal for cabal's hermetic build quality was to emulate the [Nix package manager's](http://nixos.org/nix/) immutable package system. (x-posted from G+)
For those wondering, a coexponential is the dual of a closure (called exponential in category theory).
The current parallel build implementation can quite easily be extended to build everything (including modules) in parallel. It has a single task queue were we could stick anything. I think we should move away from ghc --make and instead just call ghc -c repeatedly. It's much easier (and simpler) than trying to make GHC thread safe. I think the nix model is the right way to go for a shared package cache, but I think the focus on it as a goal in itself is wrong. It's just a cache. Its presence shouldn't effect how builds work (except from a performance standpoint.) Edit: s/should/shouldn't
Fantastic read. Thanks for everything, Simon(s)!
If I understand you right, I think you mean data CoExp r a b = (a, b -&gt; r) and exclude f (a, err) = Cont $ \k -&gt; runCont (f a) (either k err)
Thanks for the response. I just wanted to get some insight, having worked at a small web development company in my past, the powers that be were far more concerned with getting work done quickly and moving on to the next project. For them it was far easier to employ a .NET or Java person than go looking for a haskell person. Granite haskell is a great language there just aren't a ton of people that know the language and for you to hire someone and they leave for a better job finding a replacement that can catch up easy might be an issue. That's all I was getting at. 
Thanks Simon and Simon. I've just changed the VarIds in the embedded compiler I'm working on to include their type :-)
Windows: The Perfect Tool.
PHP is a proven technology in a trivial theory :-)
I can see how looking for Haskell developpers would get you candidates with a certain mindset (the last senior Java dev we tried to hire couldn't tell the difference between a class and an interface). However, I think you underestimate the usefulness of falling back on proven, documented libraries. As much as I find Java verbose and stagnating, it does have a solid ecosystem. Some of it even works without XML.
So, interesting, what is the practical difference between reinstalling everything, as you suggest, and `cabal upgrade`, which is supposed to install latest package versions, but in practice corrupts an installation? I guess the difference is that you advise to "reinstall the main packages", and not "all" packages, so the dependencies can get built at the non-latest package versions that the "main packages" declare? 
Now we are wondering what "dual" and "closure" mean here, or what book we need to read first. 
that was not my point, having a build/package/ci tool like nix that supports Windows, Linux and OSX all together is the perfect tool I'm looking for. Supporting Windows is a fact of my daily work life, not a personal choice.
typo, or he's [spelling phonetically](http://eggcorns.lascribe.net/english/64/granite/)
Here is something Haskell desperately needs: example code in Hackage. Most packages, even most of the popular ones, have none. Either in each package itself, or in a new "tour of major Haskell libraries" packages or book, self-contained example code showing how to use the functions and types in popular packages. 
`cabal upgrade` was deprecated some time ago, so I never understood quite how it was supposed to work. My simple minded system is NOT aimed at 'being up to date' but at getting a local system of packages that built against a single conception of hackage. In a sense the real danger is `cabal update`, which gives cabal a new conception of the possiblities: an alternative discipline with the same effect would be never to use `cabal update` Given a single conception of hackage, `cabal install` almost never fails because it is a fact *cabal is incredibly good at synchronic calculation of dependencies*. The only trouble that can arise with the particular I plan mentioned is with packages that involve subtle directives, e.g. to link to the right C libraries. So I sometimes divide it into two stages, building their conditions with `cabal install A B C`, then them, then whatever else with `cabal install D ... Z` Then of course one has to build any private packages one uses generally. (`cabal dev` is another subject) Since I use the Haskell Platform, I have a fixed pile of libraries (many involving complex C linking) in my global package registry that stays fixed. But again, doing this every once in a while is just one approach. When new versions of the Haskell Platform were coming out frequently, one didnt have this problem so much because one reinstalled ghc anyway. At the moment the platform is a year old. I am confident, from helping people on #haskell that as much as 90% of the "Oh my god cabal dependency hell" derive from this diachronic use of `cabal install`especially over a period of several months. There are of course others problems with the system, problems that can arise synchronically -- problems arising from .cabal files themselves, and the best way of specifying versions, among many others -- but they account for very little of "cabal hell" experience, which I think is mostly an illusion, or anyway a result of asking it to do something it doesn't actually do. Another way of putting the difficulty is that hackage changes too much over a period of three or four months -- certainly a year -- and cabal isn't fitted to deal with this. So, blame all the trouble on the feverish activity around hackage. It's "hackage hell" not cabal hell. That's a nice kind of hell to be in.
Scala sort of does what you want, applying conversions like toInteger with "implicits". I think you need Scala's locally declared implicits, as opposed to having them come into scope automatically, to avoid falling backwards into Javascript/PHP/Perl-style "do what you guess I might have meant". When you start taking away Haskell's ability to say "what you wrote doesn't make sense", you lose that "error correcting code" aspect, which is a significant component of Haskell's value proposition to a fallible human programmer. 
The solution I've been using... or hack if you prefer... is to use cabal-dev (or one of its friends) on each project, install the minimum GHC to get to cabal, then do the rest on a per-project basis in each directory. When temporal inconsistencies arise, I ~~reverse the polarity of the chronoton field~~ move the build artifacts out of the way and start from scratch with the latest packages, discarding the old ones if the new build works. Which it usually does. Inefficient, but I run Gentoo anyhow so meh. YMMV.
What are the advantages/features of this library vs Codec-Image-DevIL bindings? (Not, doggin ya, just curious). 
For the hell of it, I unregistered my local packages (admittedly, I'm not using ghc-7.4 for much yet) and did cabal install yesod snap-server pandoc gloss I only had the ghc boot libraries. `cabal` meditated for a minute and decided on 137 packages and the appropriate versions, including the potential gruesome GLUT and OpenGL and Platform stuff like network and text. Then I went outside to smoke and read for a bit. When I came back, alas, it still had 15 minutes to go ... as I should have expected. In the end it took 41m4.494s, a bit of a pain, but amazing considering it managed the compilation of 1,330 modules. I intervened at no point; my confidence that it would work was total. `cabal` is infallible in that kind of way. The time it takes to debug a complicated diachronic mess can easily match this, and the pain and uncertainty aren't worth it. Doing this every few months is just paying hommage to the hive of activity hackage is. 
yeah, it was versionitis all over the place...but i'm still stumped as to why the cabal install for a package fails on its dependencies, but then i can cabal install those dependencies without problem. haskell is not alone here. there are certain packages i just dread rebuilding in cpan
Hmm, I've only seen it explained as the inverse to the monoidal functor's natural transformation `m a ‚äó m b ‚Üí m (a ‚¶Å b)` where `‚¶Å` is the monoidal operation for the domain category and `‚äó` is the monoidal operator for the codomain category. Though it seems that wikipedia describes it the way you do for "[strong monads](http://en.wikipedia.org/wiki/Strong_monad)" in particular. I haven't been able to find any especially good/reliable/helpful resources on (co)strength; whence my curiosity.
Calling Java a "bad idea" is an attempt to refute billions (trillions) of dollars of value delivered by Java projects. It isn't perfect, but a lot of what we all hate about it is a necessary evil to get work done in the real world. Haskell (and OCaml, and other FP languages) certainly hasn't earned credibility as a "good idea" for projects on the scale of the entire industry (including all the work that can't be done by PhDs, because there aren't that many PhDs), and it has a long way to go (tooling, debugging, documentation, stable APIs, interoperability with the rest of the world) to earn that credibility. 
uh huh
"In the future we hope it may be possible to find a compromise solution that allows retaining ABI compatibility while still allowing some cross- module optimisation to take place." Just In Time Cross Module Optimization? 1. Always include the definition in a form that is high level enough to allow further optimization but is relatively very stable. In this case, Core seems highly appropriate. 2. Create a version of the RTS that can do cross module optimization and code generation. 3. Users can run the default implementations (no cross module inlining), or do cross module optimization at startup, based on the current versions of everything. ABI stability is never affected. 
Start here: http://www.defmacro.org/ramblings/fp.html http://mvanier.livejournal.com/998.html http://www.quora.com/What-are-the-best-languages-for-getting-into-functional-programming http://matt.might.net/articles/best-programming-languages/ (haskell, scala, ML, scheme, with further readings for each http://dl.dropbox.com/u/7810909/docs/what-does-fp-mean/what-does-fp-mean/html/index.html (incl suggested papers by Hughes, Wadler, Backus also some good threads on: http://lambda-the-ultimate.org/search/node/functional+programming -------------- A strong advocate of ML, Robert Harper: http://existentialtype.wordpress.com/ OCaml for ruby/python layperson http://enfranchisedmind.com/blog/posts/useful-things-about-static-typing/ ------------ (if you want to muddy discussion, Wishlist of everything that could be thrown into lang: http://news.ycombinator.com/item?id=3545008
Catsters are soooo cool!
You can read [Kock](http://home.imf.au.dk/kock/SFMM.pdf), which I think is the original work. He may contradict what I said. :)
cool but can you not bundle all the dependencies with the applet somehow?
I pm'ed, I'd love to chat.
&gt; The current parallel build implementation can quite easily be extended to build everything (including modules) in parallel. It has a single task queue were we could stick anything. I've thought about this quite a bit and I think it's a bit harder than this. Actually doing it in parallel once you track the dependencies properly is not so hard. Tracking the dependencies properly is hard (it looks easy at first hence the large number of so-so make replacements). &gt; I think we should move away from ghc --make and instead just call ghc -c repeatedly. It's much easier (and simpler) than trying to make GHC thread safe. Yes, absolutely. And for the performance advantages of --make, I think there's other things we can do, like making whole-package .hi files, and mmapp()ing them. &gt; I think the nix model is the right way to go for a shared package cache, but I think the focus on it as a goal in itself is wrong. It's just a cache. Its presence should effect how builds work (except from a performance standpoint.) Right, it's a mechanism not a policy. Once we have it as infrsatructure we have a lot of flexibility on policy, but we do still need to think hard about policy.
typos: &gt; p11.: optimsiation &gt; p12.: type-corect &gt; languaage &gt; attribte &gt; p13.: .... ---&gt; ... &gt; lits --&gt; list &gt; p14.: tha --&gt; that &gt; p15.: agreessively &gt; p16.: introduce by --&gt; introduced by &gt; in the pipeline --&gt; in the pipeline.
Are you planning to update the meta package's deps regularly?
I guess that's the idea =).
Yeah, that's the idea, and that's why we added those big hashes to the package ids, as one step in that direction. More generally the idea is to steal the good ideas from Nix. There's more work to do in GHC's package database before cabal can do this however. That said, we're not *that* far off. Well-Typed will hopefully be looking at this on behalf of the IHG as a future project.
i know this is a haskell thread, but real world scenario - you've got an app, part of which runs java/linux, the other part, .net/windows and maybe there's some java/windows. how to do continuous integration here is an interesting problem!
It is certainly possible, but I think it is a significantly longer process. The leap from typical languages to haskell is pretty big, whereas scala can be used as java++ right away, and then gradually you can learn more and more functional concepts as you progress. If we end up finding an awesome haskell pimp then that may be the approach we take though. But right now I'm not comfortable with being able to teach people haskell myself since my haskell powers are still weak. For someone with no background in functional programming to start coding in scala you need to cover just the basic concept of functional programming: "functions can be passed as arguments to other functions". Spend half an hour going through that and giving map/filter as examples and someone is ready to start using scala. Then they can get into other things as they come across them. With haskell they don't get to start from a familiar base of "I'll create a class with some methods" and in particular, they have to think differently about how to do things since the "create a variable, then do a bunch of stuff that changes the value of that variable" isn't an option in haskell. And monads are still intimidating even if you call them warm fuzzy elephant burritos.
So at the end there, a is the input to the "missing" function, and b-&gt;r is the computation to be applied to the the function's output. And the forall (which I think first appeared in your post?) says that a particular coexponential for a particular closure can have any return type r, whereas a and b would be determined by the type of the function that the coexponential is dual (roughly.... Trying to paraphrase sophisticated types into English is a fool's errand.. ) . Nice. 
ghaaargh. For us bilingual people it's confusing to be saying "The platform platform". 
&gt;This would be the first thing to fix. There is no reason for my learning haskell to block our hiring another developer. &gt;You can point them to Learn You a Haskell Lots of people don't learn well that way. I know more than one person who has gone through LYAH and come out the other side no better off than they went in. The reason a real live person is so useful in teaching is that they can adapt to the students needs.
Yo dawg, I herd you like platforms, so I put a Yesod in your platform so you can platform while you platform.
◊ê◊†◊ô ◊î◊ô◊ô◊™◊ô ◊ê◊ï◊û◊® ◊©◊î◊û◊ô◊ú◊î ◊î◊ô◊ê ◊®◊¶◊ô◊£, ◊ú◊ê ◊ô◊°◊ï◊ì.
I like this combinator a lot. I've done a few stack-based experiments before (before it came up on Reddit. Ha!) but I always ended up with a typeclass-ish mess when it came to lifting existing functions.
there's this http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Traversable.html#v:for. I haven't looked up the instances to make sure it works as you have defined above.
I like it. It works well when the higher-order argument is a bigger lambda.
Oh boy. Let me try my extreme layperson's understanding of this. This is all terminology that comes from [category theory](http://en.wikipedia.org/wiki/Category_theory). There is this notion called a [Cartesian Closed Category](http://en.wikipedia.org/wiki/Cartesian_closed_category) that mathematically corresponds to three things: * Certain types of abstract algebras. * Simply-typed lambda calculus (the basis for functional programming languages like Haskell) * Intuitionistic propositional logic (which is [equivalent to simply-typed lambda calculus](http://en.wikipedia.org/wiki/Curry‚ÄìHoward_correspondence)) The abstract algebras in question have operations that meet certain axioms for addition, multiplication and exponent. These axioms also happen to be sufficient to characterize conjunction ("and"), disjunction ("or") and implication ("if ... then") in logic, and correspondingly product types, variant types and function types in lambda calculus. Now, a dual in category theory is a construction that is a "mirror image" to another. The easiest example here is conjunction and disjunction: * The conjunction *A and B* is the logically weakest sentence *S* such that *A* is a consequence of *S* and *B* is a consequence of *S* * The disjunction *A or B* is the logically strongest sentence *S* such that *S* is a consequence of *A* and *S* is a consequence of *B*. Conjunction and disjunction are thus duals. This is also the case for product types and sum types (note the directions of the arrows in the types of `fst`, `snd`, `Left` and `Right`): * The type `(a, b)` can be seen as the "simplest" type `t` such that it supports total functions `fst :: t -&gt; a` and `snd :: t -&gt; b`, such that `fst (x, y) == x` and `snd (x, y) == y`. (I'm not going to define "simplest" in this context.) * The type `Either a b` is the simplest type `t` such that it supports the total functions `Left :: a -&gt; t` and `Right :: b -&gt; t`, such that `either id id (Left x) == x` and `either id id (Right y) == y`. For this reason, sum/union types are sometimes called "coproducts" (the dual of product). Exponential, as I said above, corresponds to function type. So, coexponential is to function type as product type is to union type. And that's as much as I understand about this.
&gt; So at the end there, a is the input to the "missing" function, and b-&gt;r is the computation to be applied to the the function's output. Yep, exactly. The reason I introduced the forall is that, strictly speaking, it's necessary for correctness. That is, the missing function is not allowed to know anything about `r`, and we can model this by quantifying over `r` in the right place. When we run a program the end result isn't a value within a program, it's something that lives outside of the program, in the real world. Thus, technically, the second component of the pair should be the dual of `b` ...whatever that means. Via Curry--Howard, the dual of `b` is typically interpreted to be the negative of `b`; which in turn is interpreted as an implication from `b` to false. And false is simply `forall r. r` because, if we have some function `b -&gt; forall r. r` then, via the principle of explosion, if we can locate a proof of `b`, we can prove anything; which is exactly the behavior we expect from `not b`. Of course, once we move from programming to category theory, this isn't the only interpretation of negative/dual types. In particular, what we really want as the second component of the coexponential is a continuation from `b`. That we're implementing continuations with functions is a detail that [masks what's really at stake](http://okmij.org/ftp/continuations/undelimited.html). You can see a bit of this in the `Cont r` implementation above. From within the Kleisli category for the `Cont r` monad we can't see `r`. That's why the values in the `Cont` monad must receive a function that produces an `r`. Whatever `r` is, it's something specified from outside the category. In this case, "outside" is simply the pure part of the program rather than the real world. But that's because `Cont r` is a monad for *delimited* continuations.
yes, that's what I was figuring
This assumes that the dynamic types are necessary. In my experience, with enough careful types work, you should never need `Data.Dynamic`.
Your `lift` operation is also known as functorial strength. See, e.g, here (which I only poked at the first few pages of, to be honest): http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.5365
Thanks for giving the connection back to sums and products (which I was familiar with) that motivates the vocabulary "exponential", which I had never thought about in relation to logic or programming... Except I did, since I am familiar with A^B as notation for the set of functions from set B to set A (and the "homograph" A^B for counting the number of such functions from a set of size B to a set of size A). Wow, it all ties together! (Except the ordering of writing A and B in A^B vs B-&gt;A seems a bit mixed up, alas.) 
&gt; Nitpick: It shouldn't be called haskell-mode, it's not a mode, it's the Emacs package for Haskell, which has various modes and modules! just wondering, what about `haskell-el` then?
Let us remap our caps lock key to underscore.
code quality in dbus-core is to good for this patch. the patch is working and solves the problem but in a way that is not maintainable. it (ab)uses the deprecated sendAncillary function with some magic numbers that might change (is the size of the credentials structure architecture depended? -- i don't know, i don't have access to a 32 bit FreeBSD box). The right way (tm) to do it would be FFI code. 
Alright, thanks for sharing it. By the way. How do you like taffybar? I've been meaning to try but have put it off due to lazyness :-)
it looked better than the alternatives. it still needs lots of work (esp. when running it on something that is not Linux) which is why i choose it as a project to learn haskell. (3 days ago).
I wish library writers would always leave a commented version of unoptimized code in Haddock, alongside the optimized strict hard-"Core" annotated magic. 
Haskell, being pure and concise and general and rewrite-rule friendly, is a fantastic candidate. "intentional programming" where an IDE heavily augments and transforms code for display, even interactively, to unpack tight code and specialize general code and hide performance annotations and other such translations down the cleverness stack. It is totally doable, since Haskell is so transformation friendly and strongly typed. 
OK, but that gets into the hazy boundary when talking lazy vs strict, of you are willing to accept a casual big-tent definition of "having something to do with laziness" to mean "correct program termination does depend on some appropriate evaluation strategy". But I am being deliberately and perhaps unfairly informal with my terminology (because I am not a genius/expert and I am (mentally) lazy) . It is totally fair to say, as you proved in your OCaml port, that it's not specific to Haskell's special and extreme brand of laziness, but a port to Python or Java would require a lot of evaluation-management code, and (I suspect) a port to a Lisp would require some use of delay/force or special forms or extra explicit ways of preventing some eager evaluations. 
Operation timed out after 5000 milliseconds with 0 bytes received 
Could you post some examples? I'm having trouble seeing why this is called "for" as opposed to say "with": let with = flip fmap &gt; with [1,2,3] (+1) =&gt; [2,3,4] which seems a little more of an intuitive.
Always so refreshing to see that on a blog about software design. :)
Is your time **that** valuable? How lucky you are. It is not a serious project no matter what is said in the first sentence. And if it is good, I will read it no matter what is said in the first sentence. Even more: if first 100 pages are crap, I will simply start reading from the 101^st page.
The only reason I choose the name `for` is out of analogy to `forM`/`mapM`, however I think `with` is also a good name, too. Usually it is most useful when you have a very long function you want to apply. Here's an actual example from a script I just wrote: oneTurn = map degToRad [0,1..360] best = minimumBy (comparing snd) $ for oneTurn $ \rad -&gt; let as1' = map (r ^%= (+ i) . rotateAlong v1 v2 rad . (subtract i) ) as1 in (as1', msd as1' as2) )
http://www.reddit.com/r/haskell/comments/qzeyk/using_textparsecindent_to_parse_an/c41pob4
"with" is commonly used for the "setup, callback, cleanup" patterm, like [withFile](http://www.haskell.org/ghc/docs/6.12.2/html/libraries/base-4.2.0.1/System-IO.html#v%3AwithFile).
He invented a word combining "ivory tower" and "-itis".
I'm quite sure it would work perfectly fine in Lisp without force or delay, and I'm pretty sure it would work fine with a direct translation to Python or Java or C (even replacing Integer with Int would probably work well enough for illustration purposes). I recommend you try it. 
Well, I feel that this section was intended to be a reference. ‚ÄúSpeeding Through Haskell‚Äù, remember? At least it is different from the usual chit-chat, when the discussion around `Show` doesn‚Äôt fit on a page.
I have used this to branch in quad trees: ifQuad True True thenQuad _ _ _ = thenQuad ifQuad True False _ thenQuad _ _ = thenQuad ifQuad False True _ _ thenQuad _ = thenQuad ifQuad False False _ _ _ thenQuad = thenQuad 
If Cale represented consensus, `fmap` would be called `(.)` :) I do agree with him on that point, though.
Because I trust you, I am going to transcribe to Python, making as few changes as possible to the form of the original. I'll be back. Edit: First cut: # forsome p = p(find(\a -&gt; p a))) def forsome(p): p(find_i(lambda a: p(a))) def find_i (p): if forsome (lambda a: p(zero |q| a)): return zero |q| find_i(lambda a: p( zero |q| a)) else: return one |q| find_i(lambda a: p( one |q| a)) File "./infinite-search.py", line 74, in find_i if forsome (lambda a: p(zero |q| a)): File "./infinite-search.py", line 71, in forsome def forsome(p): p(find_i(lambda a: p(a))) RuntimeError: maximum recursion depth exceeded I trust there's a away to fix it (and I might have a silly bug, not a "laziness"-related issue), but I believe it would involve `yield` or adding some `lambda`s ("delay") and explicit evaluations thereof ("force)... If anyone can post an obvious fix, without dredging through all my code, I'd like to see it. For anyone curious, my Python code is here: http://pastebin.com/KMUtKuMJ
It would be tremendous help for beginners (and intermediates?) to have the compiler emit a set of possible fixes, not just the one guess you occasionally get. For example: "f x" of type (String -&gt; Int) has no Num instance. Possible fixes: * Add a Num instance for (String -&gt; Int) * Add another argument of type String to expression "f x", to make "f x (y::String)" 
We already have a function named [on](http://www.haskell.org/hoogle/?hoogle=on).
&gt; haskel-lmode Your hyphen has an off-by-one error.
Cool, thanks for sharing.
Such reference material should be included in the appendix. If you "Speeding Through Haskell without understanding anything" - then by all means it should be left alone. However, speed can also be gained by first explaining the core concepts, or singular concept in detail, and extrapolating it to the rest.
`foldr` is "the right fold" :-) (Foldr can be readily generalized to any datatype that is the fixpoint of a functor, including trees for example. `foldl` makes sense for linear data structures only.)
Just looked at the [docs](http://hackage.haskell.org/packages/archive/plumbers/0.0.2/doc/html/Control-Plumbers.html). Great for obfuscating your code.
I often want flipped `zipWithM` (to be used like `forM`) and friends. But I don't have a good name for that.
Well a bridge between JuicyPixels and Repa exist, check out [JuicyPixels-repa](https://github.com/TomMD/JuicyPixels-repa), also available on [Hackage](http://hackage.haskell.org/package/JuicyPixels-repa), and it's compatible with the latest version :)
I think it's meant to be tongue-in-cheek.
How are you typesetting the document?
Maybe we should deprecate `map` in favour of `(\f-&gt;(&gt;&gt;=((:[]).f$))) `, which is only a few characters longer anyway.
Here is a fixed version: http://pastebin.com/QS7b2N8Q Unfortunately it was so impossibly slow that I changed the example `f`, `g`, and `h` functions to be simpler. None the less, it does work. I'm not a python programmer, so I'm sure things can be refined in order to make the code both more idiomatic and faster. Edit: http://pastebin.com/8Fwumf1E is somewhat faster.
You put it on hackage :)
Over time I've come to appreciate a relatively point-free style, avoiding bindings whenever convenient without leading to terribly obfuscated code. Which, admittedly, is in the eye of the beholder. My rule of thumb is if it looks like it came from lambdabot's @pl, it's probably out. And for an example of what I hope is a tasteful use of a "relatively point-free style", I'd point to my article in the Monad Reader Issue 14. Or, I'd consider one of the examples in the article pnorm p = pow (1 / p) . sum . map (pow p) to be in a "tasteful relatively point-free style". Will I come to like these operators? I don't know, time will tell!
`&gt;&gt;=((:[])` looks a lot like an angry monkey to me, which certainly is cute, from a distance.
Hi, we have to include similar statement, but it seems that we are not prohibited from working on the thesis for more than the last year nor could I find a requirement stating that the topic or work could not be assessed elsewhere (though I would probably ask my supervisor first if my project were to be part of GSoC or something similar). By the way (just being curious here), in your case, the topic or just the work itself can not be assessed elsewhere?
I'm probably not the best person to open his mouth but one topic I've found very interesting is the use of concurrency to manage external interfaces. A simple example is a command-line program that reads a data set from stdin, processes it, then prints the result on stdout. Rather than reading a line, processing it, then printing the result, split the functions into different threads. Reading is one thread, processing is in a second, and writing is in the third. I've found that the this separation of functions makes the program more function. There are these little bits that can be thought of imperatively from the outside but, from the inside, they are functional bits with a wrapping interface. It's hard to explain without doing an example but it does some really nifty things when it scales up. I have a mostly complete implementation of the Messaging Kit from the BeAPI where each BLooper is its own thread. The code is surprisingly simple, though using an Abstract Data Type approach is beyond GHC's ability to handle modules (too many levels of module recursion). I would imagine that there are uses when modelling certain problems, such as propagation of information between nodes on a network.
&gt; So, coexponential is to function type as product type is to union type. No, a coexponential is to exponential as union is to product.
&gt; **Post a comment** &gt; &gt; Fatal error: Call to undefined function e_() in /f5/mgsloan/public/wordpress/wp-content/themes/notes-blog-core-theme/comments.php on line 52 If your blog used Haskell with "Pointless Plumbers", this would never have happened.
OT: what I personally don't like is if there's only the choice between too small and full screen resolution for embedded videos. There should be an additional 'double size' or whatever possibilty...
Independently of all the great improvements that are imminent in cabal, I think yesod platform is a great idea. Since there are so many different packages at various levels of stability at any given time, it makes sense to have tagged sets of package versions that are known to work well together.
Like a swan from the duckling, I have made your comment... *art* http://i.imgur.com/tIlTC.jpg ...Courtesy of the instant_reddart bot
I so agree with you. I'm extremely disappointed when a speaker says "oh, I won't give the slides, because they're not as good without the context of me speaking" -- this happened for a Rich Hickey's talk recently -- because it means not seeing the slides at all. I'm considering making an exception for Bret Victor's much hyped video... but since I made that decision a week ago, I haven't yet found the time to watch it. Which is the reason why I don't even try to watch videos in the first place.
emerge emerge emerge
I've sent the organizers the slides right after the talk, I'm surprised they're not online yet. For now, you can get them [here](http://dl.dropbox.com/u/6198064/silk-fp-exchange.pdf).
That's actually a really good idea. Let's see if I can hack something together on the train next week.
Largely, yes. It's a joke that I started taking seriously because I do think that these aren't such bad operators when used in a standalone fashion, perhaps at most in pairs. The pnorm example is intended as a dense, relatively simple example of plumbers, not as a recommended use-case. "But now we can go further! Without descending into the full points-free madness of ... we can instead use the plumbers variant" is definitely a poor attempt at humor :P
Replace "package" with "language" and you'll match most people's sentiments about Haskell. Point-free style has concrete benefits. Sure, the `pnorm` example may be a bit over the top, but forming an opinion based on the Haddocks is silly when the operators all follow a simple, uniform pattern derived solely from their names.
s/ informally-specified, bug-ridden, slow / undocumented, questionably performant /
Madness? THIS IS PLUMBERS!
Not just Haskellish but better style, period. Even in C or C++ it's better to use enum BufferingMode { Unbuffered, Buffered } than bool.
I think the Haskell community needs to revoke your access to TH. (I jest, of course - [rex](http://hackage.haskell.org/package/rex) is nifty)
Having been writing a bit of javascript lately, I've also been aching for of the patterns in Haskell. Anyone know of a good example of the reactive programming mentioned?
Good talk!
Much like the other thread from the eXchange, are there slide for this one too?
You could take a look at Microsoft's Observable interface and the Javascript Rx framework that's based on it. It is an event framework similar to the one that we use to build our reactive values on.
No, our graphs are in-memory represented by simple Haskell data types and Data.Map.
It's built with normal Haskell data types representing nodes and edges. Links are done using references, and lookups in a Map. We also have Maps for extra indices into the map. We then have a query language, which is processed by a query optimizer. It tries to preserve the sorted output from the maps, merging in different result sets etc. All this is streaming if possible. The tricky thing is that because we try to infer the type of the tagged data, the sort order of the data in the Maps depends on these types. We have some interesting Ord instances...
They're [here](http://www.kosmikus.org/HaskellForDSLs.pdf).
I have to wonder, since the iPhone is a tightly managed proprietary platform. Would apps created with GHC be approved to deploy through the App Store?
Apple insisted for a short time that you're only allowed to use their sanctioned languages, compiler etc. They've reverted that step, thankfully. :)
Yes, they would, just like apps created with other runtimes, such as Mono, Lua, &lt;whatever solution Adobe is providing right now&gt;, &lt;insert favorite Scheme&gt;, etc.
Is GIF support on the horizon?
Ah-ha! somehow I missed your comment. You're right. Unfortunately it's a little late now, but next thing tomorrow is kinda re-organizing that section.
I believe there has been some work in turning GHC into "proper" cross-compiler by always building all the backends (and eventually letting the user specify the target using command-line flags.) Anyone know how that is going?
I've stuck that file into the files/ directory. I will apply them to a newer head soon, as soon as I get another chance to work on it. You apply each patch to your workspace according to the name of the patch, so, e.g. libraries_unix.patch is applied to libraries/unix/ directory in your workspace. They were created using 'git diff'.
Android compiler is next.
&gt; Replace "package" with "language" and you'll match most people's sentiments about Haskell. But the point is they would be *wrong*. Haskell does contain interesting ideas, and does solve interesting problems. If it didn't, then it wouldn't be worth any of our time. What interesting problem is solved, or idea is contained, in these operators? You could say building things in a more point-free style. Okay, then we just disagree about that. Point-free style is useful when it helps you think at a higher level of abstraction... but I can't see how these operators lead to higher levels of abstraction. Which other benefits of point-free style did you have in mind that are preserved despite the need to introduce literally hundreds of new operators for different kinds of plumbing?
Thanks for the answers... I have a hard time believing developers who have joined the program can't even run their own applications on their own devices for testing prior to submitting them to the App Store. Can that possibly be true? In any case, the $100 settles it. Once this matures a bit, I may add an option just for those students who want to jailbreak and do it. I agree about the platform myself. No way I'd do business with Apple. But I'm talking about adding capabilities for middle schoolers using the web site as an educational tool. I don't get to choose which devices they have. I'm definitely eagerly awaiting an extension of this same thing to Android, as well.
I notice you mention that JavaScript is a bit of a pain due to it's syntax. Have you considered trying Coffeescript? My experience of coffeescript has been quite pleasant. Additionally, there's a few projects out there to give Coffeescript static typing using google's closure compiler. Most of them don't seem to be complete enough for my standards (outdated, small community, ugly to use) but a few examples are: * [typeAnnotations](https://github.com/jstrachan/coffee-script/blob/master/TypeAnnotations.md) * [CLOSURE](https://github.com/vjpr/coffee-script/blob/master/CLOSURE.md) * [one using jzbuild](http://stevehanov.ca/blog/index.php?id=126) * [closure mode for coffeescript](http://bolinfest.com/coffee/features.html) &lt;-- this seems like the best, but also one of the older versions. It's no longer maintained as far as I can tell. It utilizes some of the new keywords in coffeescript (e.g. 'class') to automatically generate JSDoc that is used by the Closure compiler, a very elegant implementation. Like I said, they're a bit shaky but you sounded like a team willing to put upfront effort into writing libraries to improve the code quality (writing your own javascript module system?). And even if you don't go all the way to writing a tool to statically check the types of your coffeescript, I still recommend coffeescript since it's just neater javascript.
&gt; I have a hard time believing developers who have joined the program can't even run their own applications on their own devices for testing prior to submitting them to the App Store. Can that possibly be true? Unfortunately, it is true.
&gt; I have a hard time believing developers who have joined the program can't even run their own applications on their own devices for testing prior to submitting them to the App Store. Can that possibly be true? Not as far as I'm aware. If you're in the developer program (the $100 fee), you can self-sign for your own devices. I *believe* you get access to the simulator for absolute free these days. There's also a university program that's free for students that allows device installs.
I feel it's worth pointing out that, strictly speaking, it crashes because of an unhandled exception: ghci&gt; data Person = Person { name :: String, age :: Int } deriving Show ghci&gt; print $ Person { name = "tom" } &lt;interactive&gt;:5:9: Warning: Fields of `Person' not initialised: age In the second argument of `($)', namely `Person {name = "tom"}' In a stmt of an interactive GHCi command: it &lt;- print $ Person {name = "tom"} Person {name = "tom", age = *** Exception: &lt;interactive&gt;:5:9-31: Missing field in record construction :Interactive.age
It makes me feel like encounter Nullpointer exception. Too bad.
It is a warning and everyone should fix their warnings. You could always compile with -Werror. ghc --make -Werror test.hs [1 of 1] Compiling Main ( test.hs, test.o ) test.hs:2:15: Warning: Fields of `Person' not initialised: age In the second argument of `($)', namely `Person {name = "tom"}' In the expression: print $ Person {name = "tom"} In the definition of `main': main = print $ Person {name = "tom"} &lt;no location info&gt;: Failing due to -Werror. 
He's got a "deriving Show" on Person. You must have left that out.
Yes, I agree. It's technically the same as putting `error "whatever"` in the field, but adding such explosive thunks should be explicit, rather than implicit.
I think you get 100 codes you can give away to testers, etc. in order to use your application on multiple devices, if you are in the developer program.
It looks like she edited this after I tried it.
You'll be wanting exhaustive pattern matches next...
Simply put, haskell has non-strict semantic, and that means that functions can perfectly make sens of partly defined data structure. A partly defined data structure is a value that has some undefined fields, also known as non-computable parts. What you see here is simply a consequence of the syntactic sugar rules for records, and has no connection with the null pointer crash so common in other languages. Person { name = "tom" } == Person "Tom" (error "age is undefined"). If you try to print only the name of *Person { name = "tom" }* then everything will work as expected. But what you are trying to do here is printing the age of the person, which is obviously undefined, so you get the error. By the way this subreddit is about "Daily news and info about all things Haskell related: practical stuff, theory, types, libraries, jobs, patches, releases, events and conferences and more...". This question should not be posted here but somewhere else, see "Other community locations".
I've recently been running into the same thing. -Werror has helped me so many times that I don't want to turn it off globally (and I fear I'd forget to turn it back on). I've just taken to disabling -Werror in a specific module when I plan on doing any real experimenting. So I don't get annoyed with things like unused imports etc. (Using this to disable werror... {-# OPTIONS_GHC -Wwarn #-} 
I had a similar experience, including the patch for the phantom file.
So move the files where they belong and then apply the patches to the directories they refer to?
fwiw, I use a cabal flag for this, as I wan't `-Werror` on by default for production builds; to this end I have in my `.cabal` file: ... flag devel description: enable development mode (i.e. disable -Werror) default: False library if !flag(devel) ghc-options: -Werror ... 
You can get a similar kind of error with partial functions. fromMaybe (Just a) = a main = print $ fromMaybe Nothing Haskell let's you leave data constructors and record fields unaccounted for, but the -Wall flag will warn you.
Isn't it even better to name the function isBuffered(response); and make it return a boolean?
They're talking about underlying implementation, and the function for setting the buffering mode. an isBuffered predicate would then be easy to add: isBuffered response = getBufferingMode response == Buffered
I think a somewhat scaled back version would be good enough: Cabal doesn't need a full db of what the warnings were - just a knowledge of which files compiled with warnings. Then it could offer two commands: * cabal assert-no-warnings -- as you described, useful both for manual checking, and perhaps for presubmit scripts * cabal rebuild-warnings -- simply rebuild the files that had successfully compiled with warnings This would essentially be #3, but without the tedium, since only the files with warnings get rebuilt. It has the advantage that cabal wouldn't really need to understand anything about the warnings, or store anything more than build status. I'd worry that an outstanding warnings database could easily be out of sync with the source, and hence misleading when reviewing.
What extensions would be needed to make that first typeclass legal? Can it be done with effective type checking?
Be aware that using -Werror as a production flag can be a bit of pain for your end users. When a GHC update adds a new warning your package may no longer compile while being perfectly fine. So I'd rather see everyone testing their package using -Werror and releasing using -Wall. Personally I never use -Werror, but just manually make sure I'll never ship any warnings.
True. Also, it is probably worth pointing out that beginners should not try to solve this problem by catching the runtime exception. There is a convention of associating a defaultStuff function for each Stuff record type, with all fields set to sensible defaults. I think there is a also typeclass somewhere on Hackage for that but I can't remember where. In this case, if the age field is optional, one has to turn its type from Int to Maybe Int, to be able to always create complete records.
That isn't true. If you are in the developer program you can sign your applications and upload them directly to your device from within XCode. You can also provision it for other people's devices [iOS provisioning and distribution over the air](http://shinydevelopment.com/blog/over-the-air-ios-provisioning-and-distribution/)
Seconded. Also, if it's on Stack Overflow, I'm probably never going to see it, because I don't use that site. I don't like karma being meaningful, and restrictions based on it. I also find its UI a bit weird.
You can use weight &lt;- readLn :: Float to read your input as a float directly. Are you aware of the $-operator yet? It would make your last two let's redundant as well: ~~putStrLn ("Your BMI is " ++ show $ calcBmi height weight)~~ putStrLn $ "Your BMI is " ++ (show $ calcBmi height weight) Same goes for the line repeating your output, since you've skipped over the two initial strings you can use (show weight) and (show height) instead. Consider this version: calcBMI :: Float -&gt; Float -&gt; Float calcBMI heightIn weightLbs = (weightLbs*703)/heightIn^2 readFloat :: String -&gt; IO Float readFloat question = do putStrLn question return =&lt;&lt; readLn main = do weight &lt;- readFloat "What is your weight in pounds?" height &lt;- readFloat "What is your height in inches?" putStrLn $ concat ["Your weight is " , show weight ,"lbs. and your height is " , show height , "in."] putStrLn $ "Your BMI is " ++ (show $ calcBMI height weight) Here we've created a separate function that executes a repetitive task (Asking a question and reading the input into a float), so you don't have to write this code twice. And it's potentially useful in the future! You could use a standard (bla ++ blub ++ whatever) in the line where I've used concat (which is just [[a]] -&gt; [a]) but that's a matter of style, I think concat is a little more readable when you glue more than two substrings together :-)
Yep, tazjin got it wrong. The way `($)` works is that it evaluates everything on the each side, then applies the left to the right. So `putStrLn ("Your BMI is " ++ show $ calcBmi height weight)` tries to do `"Your BMI is " ++ show`, which doesn't make any sense, as `show` is a function, not a string. Thus the compiler error. tazjin gets it right later in his code: putStrLn $ "Your BMI is " ++ (show $ calcBMI height weight)
When posting code here, remember to add two newlines in front of it and then escape every code line with 4 spaces in the beginning :-) Did you have a look at my updated version? (I guess since I added that later you didn't see it at first)
Yeah, some subprocess of my brain crashed for a moment. I've fixed it in my first answer :]
Anyone know if/which other IDEs have this feature? Leksah? (I've tried Eclipse before, and it didn't agree with me unfortunately.) It would be nice if this functionality was available to command line users as well though.
We've looked at Coffeescript and like it, but not enough to be worth the extra complexity in the build/deploy chain. I recently ran across the static typing stuff in Google Closure. It looks interesting for us, especially since we already use Closure for crypting.
Yes, but I feel like return makes it more explicit for beginners. If you cancel out too many things to make the code appear more concise it's easier to confuse people :-)
It's just a function :] Return is just Monad m =&gt; a -&gt; m a i.e. it takes some value and returns it in its monadic context. And =&lt;&lt; is the "flipped" (arguments reversed) version of &gt;&gt;=, which takes two monadic actions, executes the first one and passes the result on to the second. So in return =&lt;&lt; readLn we execute readLn and pass the result on to return. As dmwit noted further down the return statement is not actually necessary thanks to Monad laws, but I add it most of the time because it's just a bit more explicit.
Plausible. I'd rather not grep for warnings and instead use a ghc flag and exit code, e.g. -fwarn-exit-code and exit code 1 for success but with warnings. Or something reliable like that, not grepping the stringy output.
 1. Integrate cabal-dev, cab, and the other sandboxed build tools into cabal and polish the result. 2. Add nix-style package management so built packages can be shared between sandboxes.
haskell.org has participated for years
The readFloat function is a useful abstraction for this program. How about just: readFloat question = putStrLn question &gt;&gt; readLn Seems both very concise and natural. Put the string, then read the line. 
Why limit to Hydra? There are many, many CI build tools. A Haskell specific one has the advantage of being Haskell specific, but many of the CIs have plugin systems and/or let you run arbitrary shell commands with the goal of accommodating specificity.
On the contrary, pointing this out the reduction is more likely to help them "get" monads. It is a common beginner's mistake to not realize that the commands in the do block are the same type as the entire block (modulo return value) and thus do not need to be filtered through a `return`.
I do not have any experience with Hydra. But, in terms of continuous integration testing, scoutess is designed to simulate, as closely as possible, what would happen if all the packages were actually uploaded to hackage and then end user ran 'cabal install'. It is also designed to provide additional features such as notifying the maintainers when build dependencies change on hackage, automatically generating correctly cross-reference Haddock documentation, and a number of other Haskell/Cabal specific features. And it should be easy to run the builder on Linux, OS X, FreeBSD, and Win32. (aka, easily installable via cabal install). Now, we could probably figure out some way to build this on top of Hydra. But, given the highly Haskell/Cabal specific nature of the project, it is not clear what benefits we would expect to get by using Hydra. Plus, it should be easier to extend and maintain since it is written in Haskell and not Perl. Since the target users of the system are Haskell developers, that seems rather important. There are many general purpose solutions out there already, but we are looking to get the type of power and convenience that can only come from focusing deeply in the Haskell/Cabal niche rather than trying to build something that is good for a large number of languages. 
I've idly thought that it might be neat to have a type system that not only tracked side effects, but also partiality. The innermost core would be not only pure, but total; any recursion in pure-and-total functions would have to be provably well-founded. And of course, no use of undefined, error, or records with uninitialized fields. Then at the next level out, you could have pure but possibly partial functions, then the IO monad. I wonder if this extra stratification could prove useful enough to outweigh the complexity it added.
Great Idea, How are you looking to present it? I have one up on a website, have a look and tell me how you think I should change it: [Checkitstat](http://www.checkitstat.com/bmi) Seriously though, I am trying to turn it into a useful website so please give me any feedback you can! 
data Person = Person { name :: String, age :: Int } deriving Show within this construction how you can say a field is not strict? the | is not allowed and if I break the definition into name :: String | age :: Int we are talking about different thing. 
Honestly I just did this to learn Haskell, not to present. Your site looks fine, but you might consider doing it all in JS so you don't need to do another HTTP request.
I posted it because: - it seems Go's RTS implements things similar to GHC's RTS, which means we could get similar performance in Haskell; - it is a nice exercise for someone who would want to play with high-performance network code in Haskell. Edit: [HN discussion](http://news.ycombinator.com/item?id=3733090)
Do any other CI's track dependencies down to level of the version and configuration of the GCC that compiled the version and configuration of GCC that compiled the version and configuration of GHC that compiled the version and configuration of GHC you are using so that shared dependency between projects only need to be compiled once? And by shared dependency, I don't mean just the same version of the shared dependency, but the same version of the dependency whose dependencies all have the same version and configuration and whose dependencies all the the same version and configuration all the way down to the GCC that was used to build the GCC ...
I don't know, but wouln't it be better to build that into cabal instead of a CI tool?
I always thought that ADTs + Records were a nasty idea and should be disallowed. 
I though so too, but it turns out it is impossible to generate the correct lenses at compile time (you can compose forms at runtime, after all). Without Template Haskell, it would be very verbose. Approaches exist, such as encoding the arity and types of the fields in the form, but this would make the library scary and hard to use for users who are a bit unfamiliar with type-level programming (such as me).
For this you would need to expose the idea of an "environment". With the above commands you have populated the package cache with library1 and library2, but what you see in your "global" environment (the cabal-dev set of packages when cabal-dev is not being used) needs some rethinking too. Or, maybe it doesn't. If Cabal is just a build system with package manager-like qualities, then the above will get you your build artifacts and make things work together as long as you are inside cabal's world and it's up to you and your package manager to install them into your environment for use outside Cabal's world. 
It's for exactly that reason that hackage rejects packages with -Werror.
We are building it *around* cabal, not INTO cabal. This really goes out of cabal's scope, people really don't want to make cabal something in which they just push all the features they want to handle their Haskell project. CI, for example, does go out of that scope.
&gt; Previously, the Conduit type could return a list of return values every time it was pushed to. This, however, is inadequate. If you have a Conduit That can produce large amounts of output for a single input (e.g., a decompressor), you have to allocate it all in memory. &gt; ... [the improvement is:] After being pushed new input, it can return multiple outputs separated by monadic actions, instead of returning a single list. My default Haskeller reaction was: "well, why not just use laziness!" However, after a few more seconds of thought, I remembered: the whole *point* of `conduit` is to give you more control over when effects are executed. In that light, this is an excellent improvement. &gt; Overall, this change probably complicates the writing of low-level code a bit. It may become more tedious, perhaps, but the example code given was, though tedious, *entirely* straightforward. I have practically zero experience with `conduit`, but I was able to easily follow exactly what was going on (the comments helped). I also applaud this improvement and agree it was the right choice.
New Yesod logo, yay! I feel like it's moved from the 90s to the 2000s, but it still feels slightly dated for this decade. I really like the upside-down Haskell logo idea, though, and the favicon looks great; no complaints there. This might have been a coincidence, but the little pyramid next to the Y certainly does evoke "foundation" and "framework". Maybe it would look better if it were a square instead of a circle...maybe. The landing page http://yesodweb.com could still use a designer's touch. Contrast with the sexyness of http://snapframework.com - the colors and sleekness feel a lot more professional on the snap landing page than the yesod landing page. This has absolutely nothing to do with the frameworks themselves, but nevertheless it will have an effect on how people perceive the framework. Disclaimer: I Am Not A Professional Designer. My personal tastes and sense of good design do not necessarily reflect those of other people, nor do they necessarily reflect good taste or good design in general.
So your now responsible for making sure that you uniquely label each formlet e.g. by using "fst" and "snd" as in the example?
blackh, we need two iphone.patch-files, one libffi/iphone.patch (uploaded ) and one libraries/integer-gmp/iphone.patch (not uploaded yet). Also, we can use the newest versions of both of them, so they don't need to be patched :) 
I am not a designer, but I think that the new logo would look completely modern if you just got rid of the gradient in "yesod".
Yes, and partial functions are also bad.
Advanced novice here... Warning : I am on my phone, so I might get some function or module names wrong... Use `unwords` instead of `intercalate " "`. `(Data.Function.on fst f x y)`, or similar, I think, instead of `f (fst x) (fst y) `, so you don't really need to bother with a named function `withFirst`. Much of the fun of Haskell is finding the library that already defines every general programming utility function I come up with... If you use HLint, it will give you advice like this. Something like `(/= [] ) . tail` or `not . isNothing . Safe.headMay . tail ` is more efficient than `(&gt; 1) . length` for long or infinite lists, and is perhaps more to the essential point, but doesn't matter much in usual practice for your case. I think you may as well get rid of all the names for pipeline steps, and just write the pipeline steps on one line each. 
Your withFirst can be written as withFirst = (`on` fst) or something close. Look it up on Hoogle. 
Yes I know, this is what I'm saying. Using pipe (|) it's a whole different thing. I'm learning haskell and I'm trying to figure it out how you can have a non strict field in the data construction from above. After reading more than 30 comments I can't figure it out. Well... maybe I should use the #haskell channel.
There's also `comparing`, so `sortBy (withFirst compare)` can become `sortBy (comparing fst)`, which reads very naturally. There's no `equating` though, so the `groupBy (withFirst (==))` would become `groupBy ((==) 'on' fst)`.
&gt; Haskell [...] has more libraries and more people than scala &gt; Ocaml is fine, but it has a smaller community than haskell I am genuinely puzzled about those two statements, I always thought it would be easier to hire Scala devs than Haskell (due to the strong connection with Java world) and that OCaml was more used in enterprise than Haskell: could you tell me how you did to estimate popularities/communities' size?
&gt; It's also pretty easy in Haskell to fall back on using C libraries via the FFI It's easy but not as straightforward as Scala using Java code as it comes with more boilerplate: you have to "foreign import" every single C function you want to use and (IMO the most annoying) you have to make a wrapper C-side for every function that takes/returns a structure because FFI can't handle structures on the stack and can only use pointers and atomic C types.
We had Ruby On Rails, now we've Haskell On The Rocks.
I disagree with the first point, but I don't have any experience with the second point.
Not a synonym but I always thought `screamer` was a nice name for a stream processing library.
There was actually a discussion in #haskell about "vein" and "artery" as terms, where I suggested using them to give a pull/push distinction. Jokes aside, the library in question is based on some ideas about stream-processing semantics for linear logic, and as such the current working name is "Streamline". That said, the last one on your list might be a good candidate for something else I'm hacking on, for doing certain specialized kinds of parsing on streams of `ByteString` chunks with incremental output. The bytes must flow, after all.
There is a really neat technique first described in the paper ["Functional Unparsing"](http://www.brics.dk/RS/98/12/) that may help here. Another [example] (http://www.haskell.org/pipermail/haskell-cafe/2007-July/027975.html) by Chung-chieh Shan. The point is that while you can compose forms at runtime, their *type* is always determined at compile time. 
Also, advice from hlint (I love hlint), you can also do: liftM processInput getContents (where liftM is Control.Monad.liftM)
I'm a Haskeller (and Rubyist, and ofc exp in other languages as well) in Waterloo. Not necessarily looking to commute to Toronto, but just to say (a) there are some of us around and (b) Haskell is a good choice for many applictions, and no matter what language you pick you shouldn't hire (usually) based on knowledge of a specific language, but based on being generally good. Good programmers can pick up your language. I know at PostRank we several times hired people who knew no Ruby, but were smart, and they picked it up (as well as our other tools).
Really? I find the lack of cats heartening and a nice refreshment from all the rest of reddit.
Things like langpop aren't super accurate, but they have haskell well above scala or ocaml. Between that and looking at the amount of activity in the respective communities (how many libs there are, how many new ones are being made, how many people are talking about it) I get the sense that haskell either has a notably larger community, or a dramatically more active community, especially compared to ocaml. Scala is certainly more popular specifically with web development, but it doesn't seem to be used much at all outside of that, so it appears more popular when viewed from that angle. I think if you are willing to accept people who know how to write java programs in scala then hiring scala devs is quite easy. But if you want someone who can take advantage of what scala offers, you don't have a lot of options, and it seems like most of those people also know haskell.
We're certainly open to hiring good people who don't already know haskell as long as they have enough of a functional background. But obviously someone who already knows haskell is ideal. We are trying to find good programmers, and a key thing is that knowing haskell has a strong correlation to being a good programmer. The people who have expressed interest so far (including those who have only started learning haskell) have all been far better than the best people we've seen when looking for developers through more general channels.
Sure, that's fair, looking for people through specific channels will always yield better results. Hiring is hard :(
Indeed. `not . null . tail` is better.
I would use Data.Map instead of lists when grouping things. If I'm not mistaken that's O(n*log n) instead of O(n^(2)). And the code is shorter: anagrams = filter (not . null . tail) . Map.elems . Map.fromListWith (++) . map (sort &amp;&amp;&amp; return) 
Oh, I wasn't aware of that feature. Thanks for the tip. That helps a lot.
I'm not sure I understad; are you saying that groupBy is O(n^2 )? I thought it was O(n)...
The rest is above my grade but I think Return/Empty is Constant: http://hackage.haskell.org/packages/archive/transformers/latest/doc/html/src/Data-Functor-Constant.html#Constant
Thanks! That's what I was looking for.
Thanks. I fixed it. I do know about the `Either` monad, but I never thought of it that way (i.e flipped). That's a good observation. So I guess a simpler way to formulate it would be to use a free monad over the `Await+Yield+M` functor, with `Either e r` as the return type, and then wrap that in `EitherT` for the ordinary monad and the flipped version of `EitherT` for the error monad.
Great! Though I feel like I should point out that `Pipe` and `Pipe'` are identical ‚Äî you swapped the arguments to `EitherT` but also swapped them in the left-hand side.
I was pointing out a better solution for the type of field. BTW, partial-lens has been incorporated into data-lens.
Right, I don't know what I was thinking. But note that you don't need sort either. So they are both O(n * log n).
Depends on what you mean by "only succeeds", since I'm not really intending to have any backtracking. Like I said, it's for somewhat specialized purposes. What it *does* is take a fixed size, consume it entirely, and parse that independently of anything else. The output is a stream of nested chunks (some of which may be invalid). The motivating example here being tag-length-value formats, typically a sequence of chunks labelled with a type ID and a size in bytes.
JSON is a rich and complex beastie. Ignoring the IO bits, your program should look something like: hasProperty . binValTreeFromJson . decodeJson First you parse a bytestring into a complex JSON type, then you convert that complex type into your very simple one. (And error out horribly if it fails - writing to stderr and exiting seems like an acceptable behaviour in this case, though it's not very Haskell-y.) Decode your file with aeson in GHCI, examining the aeson types interactively should make things crystal clear. Beats reading docs :)
It's pretty simple: you have to make an instance declaration of the FromJSON type class for BinValTree, as you can see from the type of decode: decode :: FromJSON a =&gt; ByteString -&gt; Maybe a The documentation of [FromJSON](http://hackage.haskell.org/packages/archive/aeson/0.6.0.0/doc/html/Data-Aeson.html#t:FromJSON) provides an example. Here's a first stab without proper testing ;): import Control.Applicative (pure) import Control.Monad (mzero) import qualified Data.Vector as V import Data.Aeson (FromJSON(..), Value(..)) import Data.Attoparsec.Number (Number(..)) data BinValTree = Zero | One | Branch [BinValTree] deriving Show instance FromJSON BinValTree where parseJSON (Array arr) = (fmap Branch) $ mapM parseJSON $ V.toList arr parseJSON (Number n) = case n of I 0 -&gt; pure Zero I 1 -&gt; pure One _ -&gt; mzero parseJSON _ = mzero BTW: Stackoverflow is a more appropriate venue for Haskell-related question. This subreddit is mostly used for Haskell-related news and announcements.
Actually, it can be considerably simpler: import Control.Monad (mzero) import Data.Aeson import qualified Data.ByteString.Lazy as BL data BinValTree = Zero | One | Branch [BinValTree] deriving Show instance FromJSON BinValTree where parseJSON (Number 0) = return Zero parseJSON (Number 1) = return One parseJSON a@(Array _) = Branch `fmap` parseJSON a parseJSON _ = mzero -- | Reads stdin, and parses as JSON encoded BinValTree main :: IO () main = BL.getContents &gt;&gt;= print . decodeBinValTree decodeBinValTree :: BL.ByteString -&gt; Maybe BinValTree decodeBinValTree = decode Notice how the parseJSON structure follows your data type's structure. As for understanding Aeson, there are really only a few common Haskell concepts and idioms that are needed, and they are worth learning early on. For this case: the use of type classes as a means of enabling a package to work on your particular data, and monadic style parsing.
I fail to see how "Either optionA optionB" is more desirable than "Either a b". The semantic meaning of the extra characters "option" is conveys no new information that you don't get from the name of the type constructor and imposes higher cognitive overhead. How about "Either theFirstOfTwoOptions theSecondOfTwoOptions"? These names have more meaning than "optionA" and "optionB". Obviously more meaning isn't always better.
&gt; For this case: the use of type classes as a means of enabling a package to work on your particular data, and monadic style parsing. Honestly, that doesn't sound too bad. Especially with those being common idioms. Those are both really helpful answers. Thanks. It's really good to be unstuck again and able to make positive progress. I'll to Stack Overflow with future questions. I'm sure this won't be the last thing I have trouble understanding.
The error in the Either example is this: You have 1, 2, or many variables. In the case where you have 1, a single-letter variable is useable. Examples are 'i' in a loop, 'a' in Maybe etc. When you have two variables, it is more questionable whether you can call them a,b x,y, or general names like that. When you have more than 2, they definitively must be named. Unfortunately, haskell tends to look ugly with long variable names. It's a shame.
Ok, then it sounds like we pretty much agree. I just dislike the sweeping generalization of "single letter symbol names are, hands down, the worst aspect of the Haskell culture" without admitting the point that they can actually be the right thing in some situations. I actually [blogged about this](http://softwaresimply.blogspot.com/2011/12/whats-in-name-lot.html) awhile back. I'm with you on function parameter names and let binding variables, as their scope puts them in the rare category.
*Login or Signup to download this file.* m(
spelling out the names of greek characters...‡≤†_‡≤† might as well just do this: data Pipe ayh bee ehm arr
It's pretty common for a chain of `types = [a, b, c]` to mean that `init types` are the inputs, and `last types` is the output. `r` is commonly used to mean "result", `e` for "error", `m` for "monad", `s` for "state". These are in no way specific conventions to pipes, they are conventions used all over Haskell. I don't think that providing longer names would make it any easier for the typical Haskeller to swallow...the problem is that there are so many type variables at all.
 Form monad index error value result This is my default assumption of what `Form m i e v a` means, and unless it deviates from this, I'd prefer the short version. :) It's better to *actually* document a thing, rather than try to make the types the entirety of the documentation. *throws pointy lambdas at fancypantalons* (don't need those for my pointless code anyways...get it? pointy? :)
Thanks! I am really looking forward to learning from these tutorials.
Dear Haskell Community, I would like to work on "cabal dependency hell" during GSoC. My name is Philipp Schuster and I am a fifth year master student at the University of Koblenz. I study Computervisualistik which is basically Computer Science with an emphasize on image processing and computer graphics. I got hooked by Haskell during my freshman year. It is my primary programming language. I write all my homework assignments and my toy projects in it. I have never written any "serious" code. I used to read everything on the mailing list, now I read everything on reddit. Concerning the cabal dependency problem here is the list of relevant material that i found so far: * [cabal of my dreams](http://blog.johantibell.com/2012/03/cabal-of-my-dreams.html) * [cabal-src](http://www.yesodweb.com/blog/2011/11/cabal-src) * [cabal-dev](http://corp.galois.com/blog/2010/12/20/cabal-dev-sandboxed-development-builds-for-haskell.html) * [discussion on google plus](https://plus.google.com/116553865628071717889/posts/WFPfB3M5C1Q) * [cabal-meta](https://github.com/yesodweb/cabal-meta) * [cabal nirvana](http://www.yesodweb.com/blog/2012/03/cabal-nirvana) * [modular-solver (pdf)](http://www.yesodweb.com/blog/2012/03/cabal-nirvana) * [Hope](https://github.com/DanBurton/Blog/blob/master/Markdown/Hope.md) * [nixos](http://nixos.org/nixos/) * [scoutess](http://alpmestan.wordpress.com/2012/03/21/scoutess-continuous-integration-cabal-and-the-google-summer-of-code/) * [hsenv](https://github.com/Paczesiowa/hsenv) * [multiple instances of a package in ghc](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Packages/MultiInstances) * [cabal is not a package manager](http://ivanmiljenovic.wordpress.com/2010/03/15/repeat-after-me-cabal-is-not-a-package-manager/) * [discussion on yesod mailing list](http://groups.google.com/group/yesodweb/browse_thread/thread/e1b6a9e867933b59?pli=1) * [the butterfly effect](http://cdsmith.wordpress.com/2011/01/17/the-butterfly-effect-in-cabal/) * [storage and identification of cabal packages](http://www.vex.net/~trebla/haskell/sicp.xhtml) Please post any links you find related. Also, may I edit the ticket?
Yes. I imagine that some people prefer the reddit format, though, and it has the added benefit of common issues getting the most upvotes and therefore the most visibility. 
It is set to self posts only at the moment. 
http://hackage.haskell.org/trac/ghc/wiki/Commentary/Packages/MultiInstances dcoutts, kosimukus and I have been discissing this plan over the last couple of days, but we're all too busy with other things to implement it. It is definitely GSoc material though, and we believe this is the right direction for solving dep hell.
Lol. I'm all with you. I hate pointless code too. Point-free code on the other hand... :) In all seriousness, every case is an individual judgment call. Point-free code can be more obfuscated, but it can also be clearer. And the nice thing about it is that clear point-free code lets you avoid having to think up parameter names where...gasp...you might be tempted to use single letter names. :D The circle of life...
I would assume that it means Form monad index error value someGeneralType Where the general type probably is the type that the form allows one to edit (that is, the "result")
How about creating a blog with about Pipes?
&gt; Example: I was burned by DiffArrays, which were documented in a wiki somewhere, only to painfully discover that the practical performance does not match up to theory, and is unusable in practice. I'm just picking this out because it's interesting... actually, in theory DiffArray is awful. You can do much better with other persistent array structures, based on fat elements and clever versioning schemes. The only possible justification for using DiffArray is that in practice they happen to work out okay for your application and have lower overhead because they aren't being so clever. So really this is more like DiffArrays being a poor idea for you both in theory and practice.
Why not simply add to / maintain intermediate section on haskell wikibooks: http://en.wikibooks.org/wiki/Haskell 
&gt; The very existence of documentation for bad ideas is cancerous; it distracts learners from the good ideas. (Generally speaking) I agree. I think the proper pedagogical approach is to start with the right answer, or if that's too complex then at least something sufficiently close yet simple enough to understand. Once you start in the right ballpark, *then* you can talk about tweaks to improve this or that aspect of things. But the real challenge with programming is always to learn what it means to be in the right ballpark. All too many programming books focus on a git-'er-done perspective, which only teaches people to get used to ad hoc and inappropriate solutions. It's better to follow the functional pearl model: here's the right answer, oh and it's easy and beautiful too.
I see it. Not sure why you can't.
As somebody who doesn't have 7.4.1 installed, can somebody explain what exactly the compile-time error is and how one would fix it? I'm not sure why those semantics keep that example from compiling.
Well, when you write `let (a, g') = random g in ...`, that expands to let e = random g a = case e of (a, g') -&gt; a g' = case e of (a, g') -&gt; g' in ... And that `g'` binding is the killer -- because `e` is polymorphic, and it's throwing away an `a`, but the `a` is the value choosing which implementation of `random` to use! So the `g` you get back is ambiguous.
One way to avoid this is to write descriptively *"how we wrote yesod"* rather than prescriptively *"how to write ..."*. The descriptive style might be a very good fit for intermediate or advanced level books which aren't obliged to start at the beginning.
`(=&lt;&lt;)` defines a functor too, and `traverse` defines a family of functors. EDIT: forgot to say that traverse only defines a family of functors for monads with an underlying commutative applicative functor.
I want this to happen! Cabal dependencies are getting more and more annoying every day. nixos and nix-pkg solves the dependency problem is a rather elegant way, if you can take the extra storage of multiple versions. Keep us up to date with your ideas! :) Edit: So today I ran into this problem: &gt; cabal configure Resolving dependencies... Configuring hindsight-0.1... Warning: This package indirectly depends on multiple versions of the same package. This is highly likely to cause a compile failure. package scrypt-0.3.2 requires entropy-0.2 package hindsight-0.1 requires entropy-0.2.1 package crypto-api-0.8 requires entropy-0.2.1 package cprng-aes-0.2.3 requires entropy-0.2.1 The hindsight package is mine, and apparently cabal cannot configure it, because my version of scrypt is conflicting with my version of cprng-aes and crypto-api. I begin to think, a better way was to just check whether the package *can* compile against a deps API and if it can, just do it. Instead of trying to track API changes through the version numbers. Edit2: You should also check out [semantic versioning](http://semver.org) and [Smackage](http://github.com/standardml/smackage).
Ignore it?
I'm sure Michael would love to have some help, maybe this will be "Even More Real World Haskell". Btw, LYAH also has a chapter on zippers, though it'd be nice to have some more in depth examples (I'd love to see a simple shell written using a zipper for the file system rep, but I'm unsure how (if) you would do it).
Is this what all lazy pattern matches expand to or just let-bound ones?
I think every monad morphism f :: m a -&gt; n a gives rise to a functor (f .) :: (a -&gt; m b) -&gt; (a -&gt; n b) between Kleisli categories. Edit: Let Mnd = category of monads and monad morphisms Cat = category of small categories Kleisli(M) be Kleisli category of the monad M Kleisli(f) be induced functor between Kleisli categories, as above I think Kleisli: Mnd -&gt; Cat is a functor.
&gt; except not in the category of Hask If only Category was Functor superclass. You could then have (.) = (&gt;=&gt;) and it would be fine. 
A monad morphism f :: m a -&gt; n a is a natural transformation that plays well with fmap, return and join. If you want (f .) to be a functor, you need (f .) return == return, i.e. f . return == return. For example, think about maybeToList :: Maybe a -&gt; [a]. Analogous laws are with fmap and join.
By the way, for implementing either of these types, I recommend my [operational][1] package, described in [The Operational Monad Tutorial][2]. The approach presented there helps a lot with clarifying the interaction with other monadic effects, like exceptions and so on. [1]: http://hackage.haskell.org/package/operational [2]: http://apfelmus.nfshost.com/articles/operational-monad.html (I'm not the OP.)
So after some thought, it turns out I've realized I was expecting `MonoLocalBinds` all along. Given how I've always considered the monomorphism restriction to be counterintuitive and confusing, this was a surprise to me! But in this case, I realized I *expected*, so fundamentally that I didn't realize I was even making an assumption there, that the compiler will settle on a specific type for `a` based on how it's used, and then use that to decide on the value of `g'`. I tried to construct a similar example with top-level bindings -- not hard at all -- and it suddenly becomes immediately obvious that of *course* there is an ambiguous type there. But in a let binding, even though I understand where it comes from, it still seems wrong. I wonder where that pattern of thinking came from.
Out of curiosity, what was the good reason motivating this change in semantics in the first place?
I'm currently also of the opinion that pipes are more natural and easier to use than conduits. But I really don't want to see conduit change to match types right away, because there are different ideas in the two packages. Maybe pipes will turn out to have some debilitating shortcomings (if that happens, I'm guessing it'll be because of either too much complexity in the error handling, or unsolvable performance problems for some use case I haven't thought of) and we'll all realize that conduit was the best we can do. This is not yet a solved problem, and variety is a good thing. In any case, it's not just the happenstance unification of types that's appealing, but also the ability to provide a more powerful interface to all of them. Conduit's separation of Source and Sink wouldn't matter all that much, except that the different pieces have interfaces that make composition more limited and difficult the further you get from the sink. This is solved by the really fundamental idea about pipes, which is that "result type" and "output type" are not the same thing. It took me a while to understand how important that is... but once you've got that down, most of the rest of it flows from that.
What he meant was &gt; except not in the category Hask Hask is this category: instance Category (-&gt;) (.) f g x = g (f x) id x = x
It seems that we can't use `ProgramT` because `Write` also needs access to `m`? I guess it would be possible to change `ProgramT` to have a constructor `Instr :: instr m a -&gt; ProgramT instr m a` to give `instr` access to `m`.
In addition to what @cdsmith said, you can add line comments for commits, or comments on an entire commit as well.
So let me ask more directly: was the filehash example the right way to demonstrate conduit and system-filepath?
Right, I interpreted IsTom as saying that it's a shame `Functor` isn't defined as class (Category (~&gt;), Category (~~&gt;)) =&gt; Functor (~&gt;) (~~&gt;) f g where fmap :: (f a ~&gt; f b) -&gt; (g a ~~&gt; g b) This would represent a functor from a category whose objects are represented by `f a` to a potentially different category with objects represented by `g a`. And then we already have: newtype Kliesli m a b = Kliesli (a -&gt; m b) instance Monad m =&gt; Category (Kliesli m) where id = return (.) = (&lt;=&lt;) Then you could write: instance (Monad m, MonadTrans t) =&gt; Functor (Kliesli m) (Kliesli (t m)) m (t m) where fmap = (lift .) I'm not saying this is a good idea. But it's how I interpreted the comment.
I don't know what the the `m ()` argument is supposed to do, so I simply swept it under the `Lift`. The idea of making `m` available to `instr` may be independently useful, though. Thanks!
When I'm using let bindings I typically have a specialized use for them in mind, which is different from typical top-level bindings.
I don't see how he addressed all the issues. $$, $= and =$ are currently different functions, and simply stating that they can be unified into a single dot operator- without any evidence for this claim- is a bit unfounded. In general, there's a large body of `conduit` code out there demonstrating how this stuff is really used, and just saying "stick it all together into a single type" doesn't at all prove your claim. I've taken a lot of input on this library, and will continue to do so. But I see little more than assertions in both the blog post ("a single datatype is better", "you can implement every piece of functionality equally well") and your statements here.
I'm not setting up rules of discussions, but I think it's fair for me to point out that this is far from a complete solution. We can discuss things, and if someone can build on what Twan has stated and build something better than conduit, I'm interested. But I think it's important to point out that what has been stated thus far is *not* a complete solution, just talk. So discuss approaches. But I would highly recommend that when you come up with a concrete proposal (which is what Twan's post appears to be), take the time to try and make the change, and see what happens when you compile. My point is you may be surprised at the number of things that don't work, and it might help you advance the design.
Well, three of those seem like a strange thing to add in. RWH and LYAH are both intended as introductory books to Haskell. Mezzo is intended to build on top of those two books. Typeclassopedia is certainly an important piece of work, but there are *lots* of blog posts and papers out there for which the same could be said. It might make sense to try and incorporate a number of those works into Mezzo, if the authors are willing to contribute them. Wikibooks is the only comparable effort I'm aware of, which is why it makes sense to discuss it.
One interesting point here, something I have run into too with frustration, is that typeclasses only admit one instance per type. Furthermore, instances can not be parameterized by runtime values, so you cannot create monoids like "log entry with timestamp closest to my timestamp of interest in this context". I'm awaiting to be proven wrong here :)
I‚Äôd say you don't need to reach any farther than [the newtypes provided in Data.Monoid](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Monoid.html) to demonstrate that problem. Has there been any ideas for resolving this? Obviously, one can replace a typeclass instance with an explicit value (record of functions), which solves the monoid problem, but is there a language in which you can define such values and then have them used implicitly when the type unambiguously selects one?
I think you made a mistake here: &gt; fmap :: (f a ~&gt; f b) -&gt; (g a ~~&gt; g b)
That's why I prefaced with "generally speaking". There are definitely cases where it's good to see the dead-ends others have tackled while exploring the design space. Especially for complex projects where the design spaces issues are about scaling up and feature interaction, but where many inappropriate approaches seem fine in the beginning.
cabal-dev and hsenv are really solutions - just like nix. Nix is just more efficient at sharing build artifacts between sandboxes.
Yuck, that's why I'm not convinced by NoMonoLocalBinds. It seems a very ad hoc solution to the GADT issue and only introduces new problems. I do work to eliminate redundant pattern matches in the first place; the compiler should not be reintroducing them.
They are isolation tools, which is fine if your dependencies are simple enough... but as I said, for any two packages with incompatible dependencies that can be built in isolation, it's trivial to build an application that depends on both of those packages but doesn't mix them, and for which the isolation tool now doesn't work even when there's no good reason the project shouldn't build. A real solution handles multiple instances of the same package by hashing identifiers for the entire build subtree, so that if two of your dependencies rely on incompatible packages, they each get their own private copy, and your program builds so long as it doesn't actually mix incompatible types/instances/etc.
But you're missing the whole point here! Certainly you can get the types to match up, I'm not talking about that at all. Can you guarantee that the semantics are still correct? Will all resources be released as early as possible? Will we still run in the smallest amount of memory possible? I'm not saying that this is impossible with your formulation, simply that it hasn't been shown yet. Additionally, all of this sidesteps the question of whether or not the new formulation is in fact a good thing. It's taken for granted in your post that a single type is better. I'm not convinced. Separate types have distinct advantages. Minor example: in [http-conduit](https://github.com/snoyberg/http-conduit/blob/master/Network/HTTP/Conduit/Response.hs#L116) we have code that modifies a `Sink`. It has to deal with just the three constructors that can logically be part of a `Sink`. Under your formulation, we'd either have to deal with a constructor that could never be (`HaveOutput`) or make it a partial function. I'm strongly of the opinion that invariants should be expressed in the type system whenever possible. `Void` is helping you to do so here, but to me, it seems an inferior solution than the separate types. Of course, this isn't a hard-and-fast rule. conduit 0.2 was able to express the no-leftovers-without-push invariant in the types themselves by having extra datatypes, and conduit 0.3 cannot. I considered that an acceptable trade-off, but only after careful thought, and thorough experimentation with the codebase.
Oleg already did it: http://okmij.org/ftp/Haskell/types.html#Prepose
&gt; We have such a conduit by default: isolate. Using it here would give almost the same result, except for one thing: there would be no way to distinguish "no more bytes from the server" versus "you consumed too many bytes already," and thus our error messages would be incorrect. But you could define a variant of isolate that does give an error message. I also believe that `checkHeaderLength` is wrong, because it allows the sink to consume more than the given length as long as it is from one chunk. Or am I missing something. &gt; But that was just one example anyway. Any time you have low-level code, you would need to explicitly deal with all of the constructors, including the ones that don't make sense. I would prefer to stay away from the constructors if at all possible. Manually unpacking the `Sink` type like this is really not something that belongs in user code. And I do consider http-conduit a user here. &gt; Look at @apfelmus's code: it's full of errors. You can prove that they are all impossible codepaths, but I prefer not needing them at all. The only reason apfelmus' code needs error is to be able to fit the types back into `Sink` and `Source`. In particular, you essentially need to convert a `ConduitClose` into a `SinkClose`. You can actually do without many of the error calls. toSinkClose (NeedInput k _) = toSinkClose (k ()) toSinkClose (HaveOutput _ _ _) = absurd toSinkClose (Finished _ _) = return () toSinkClose (PipeM _ a) = a This is not really a limitation of the Pipes idea. You could also see it as the inability of conduits to represent all possible pipes.
Clever use of top-level template haskell splices.
Because it means "early close" I think it should be `m r`.
Agda's [instance arguments](http://wiki.portal.chalmers.se/agda/agda.php?n=ReferenceManual.InstanceArguments) allow this.
Thanks! The `unused` were not necessary, they could all be given reasonable values. Also I think that source should be `type Source m o = Pipe m Void o ()` instead of `Pipe m () o ()`. I thought maybe Twan had chosen `()` instead of `Void` for a reason, but `Void` seems to work fine. All constructors (except `Finally`) have an early close field, so there's a nice `close` function, except that the close field in `HaveOutput` has to become `m r`. But that works out fine because `r` is `()` for Conduits anyway. Here's the code: https://gist.github.com/2188862
That last part is exactly correct. People that try to implement categories for iteratees always mistakenly try to implement something resembling Kleisli categories, which is not what pipes does. A more appropriate analogy is that pipes define a call stack, where each pipe is a frame in the call stack. The input type of a pipe is the type of input that particular frame demands from the lower frame, and the output is what it delivers to the higher level frame. Unlike conventional call stacks, the type of input between frame boundaries is fixed, so it would be like defining a function that could only call functions that return the same type of value. So the category of pipes is simply the category of stack frames, where composition of pipes is simply the composition of stack frames. In fact, many invariants of pipes become very intuitive and easy to explain in terms of the stack analogy. I actually realized the analogy to call stacks only after I had released v1.0. It turns out that if you follow the call stack analogy and take it to its logical extreme, you basically get the solution for how to handle errors, finalization, and premature termination. There are two quite noteworthy and elegant tricks the library uses to get around the unidirectionality of pipes (which is a requirement of the category laws) and still solve every error handling feature request I've gotten. I think that even if other iteratee libraries don't go the same path as pipes they can learn a lot from these tricks, which solve a lot of issues they grapple with. I've tested these two enhancements in isolation and they do form categories, but I haven't yet merged them and I will discuss this more once I release it. There is some interesting work in getting the call stack metaphor for work for heterogeneous types across frame boundaries, in contrast to the homogeneous streams that pipes use. See the [Coroutine](http://hackage.haskell.org/packages/archive/Coroutine/0.1.0.0/doc/html/Control-Coroutine.html) package for an example of this. If that works out well, I may copy it, but for now I'm still sticking to homogeneous streams since error/termination handling is the most demanded feature at the moment.
*Type Classes with Objects and Implicits* by Odersky et al shows type-classes implemented as a design pattern in Scala. They have their own problems with instance coherence.
What's wrong with the classic [CmdArgs](http://hackage.haskell.org/package/cmdargs-0.9.3)?
Instead of having a Template Haskell interface as shown here, cmdargs exposes a convenient, but impure interface that has to fight against the compiler to work correctly. There is also a QQ interface that maintains purity, but having a TH interface like this gives the flexibility of the default CmdArgs interface but with purity (although the interface may be slightly less convenient).
There tend to be academically-oriented conferences in functional programming in general: http://www.haskell.org/haskellwiki/Conferences and also regional hackathons and user groups: http://www.haskell.org/haskellwiki/Hackathon http://www.haskell.org/haskellwiki/User_groups
I think private deps are orthogonal to the sandbox issue. You can have private deps in a sandbox as well.
TH is impure in its own horrible way. 
It is not a change as such, just a specification of the semantics of pattern bindings in the absence of the monomorphism restriction and MonoLcalBinds. While discussing whether to drop the MR, the Haskell 2010 committee realised that we had to pick a semantics, and this one seemed simple and natural. GHC did something different before in this case, but we weren't sure *what* it was doing :) Simon PJ was arguing for MonoLocalBinds when we discovered this alternative solution.
&gt; \*sjoerd_visscher has shown... :-)
 &gt; I've asserted for a long time that the only real way to come to conclusions in a topic as complicated as what we're dealing with is to write the code. I still stand by that assertion. You see, there are other ways of understanding a complicated topic than finished production code. A large number of Haskellers have long time experience in formal methods or mathematics, as do I and I presume Twan as well. It is this experience that tells us that it is possible to recast conduits elegantly in terms of pipes, long before we see any concrete code. It's a different way of thinking, but it works. Of course, one point that may be tricky is resource usage. We have no reason to disbelieve your claim that there is something tricky going on and that you've solved it, but at the same time, we would like to have a direct theoretical understanding of what you did and why it works. Removing accidental complexity and reducing the number of data types is essential for that. What Twan did was the following: &gt; So a function exists which can convert from Pipe to Sink? OK. What advantage does this provide? With his post, Twan showed that the source code of your conduit library can be translated completely into (his variant of) pipes. He didn't spell out all the details, but it was straightforward for me to complete the translation. I didn't really feel that it was necessary to write it down, I only did it because sjoerd asked me so nicely. I was even too lazy to remove the `unused`, but from experience, I never doubted that sjoerd [could remove them][1]. [1]: https://gist.github.com/2188862 The translation shows that whatever your library can do, the pipes can do as well, simply by virtue of the translation. We don't need to know what your code actually does to know that the translated code can do the *exact* same thing. Again, this way of reasoning about code might seem unusual, but it works. 
If you fix this once and for all there will be many appreciative haskellers
i feel like tomato-rubato should be friends with reactive-banana
It's obvious that you're not understanding what I'm saying, either because I'm not being clear enough or because you don't want to hear it. Let me try one last time: Obviously you can express anything from the current types in Twan's type, I'm not arguing with the math there. I'm noting a few problems: * There's an *assertion*, which is completely unfounded, that the current connect and fuse operators can be expressed in terms of a single new operator. That has not at all been mathematically proven. * There are a number of pieces of `conduit` that are completely ignored, most notably `BufferedSource`, but also other things like `lazyConsume`. * I'm far more interested in the design of an API which is easy to use and understand than just combining types together. Everything here is predicated on the assumption that combining types is good and right. I've pointed out many reasons why that may not be true. (I haven't taken a stand either way, merely pointing out that it's not black-and-white.) If you want to continue to assert that this change is more elegant without backing that up, fine. If you want to continue to assert that this new formulation will obviously work for everything `conduit` handles- without actually addressing some of the major features in `conduit`- fine. Just don't be surprised when I'm unimpressed with the presentation.
That modular arithmetic example [in Kiselyov and Shan's paper](http://www.cs.rutgers.edu/~ccshan/prepose/prepose.pdf) is awesome.
esp ICFP and CUFP http://www.icfpconference.org/ http://cufp.org/ 
It is. But it's still a workaround for the language not directly supporting explicit passing of typeclass dictionaries to functions.
What is Conal Elliott's context monoid pattern? Google finds me examples of this phrase in source code, but no explanation of what it is.
It's in the TypeCompose package on Hackage and described in this paper: http://conal.net/papers/data-driven/paper.pdf. The pattern embeds an explicit dictionary.
&gt; It's largely the point of typeclasses to admit one instance per type [Not necessarily](http://www.cas.mcmaster.ca/~kahl/Publications/Conf/Kahl-Scheffczyk-2001.html). Named instances would be a very useful, conservative extension.
&gt;For instance, named instances + a phantom type indicating the ordering/instance selected, so you can discriminate if desired. Yes, something like this is essentially the solution I've come up with independently: in a dependently-typed setting, you can have the type of a `Set` include its `Ord` dictionary.
If I understand correctly, the TH interface of CmdArgs you are referring to is a quasi-quoted interface. This Options TH interface is not QQ. The CmdArgs QQ interface is a slight variation of the impure interface - starting with TH for this Options library has resulted in a very different interface.
&gt; There's an assertion, which is completely unfounded, that the current connect and fuse operators can be expressed in terms of a single new operator. That has not at all been mathematically proven. This is definitely true. That being said, though, I just read through the definition of those operators again in the documentation, and outside of the `BufferedSource` situation (which is it's own mess, so I'm ignoring it for now) there's nothing there that indicates that they *should* have different behavior. So maybe another way of looking at this is that *if* they are different, then you are missing some documentation about a tricky subtlety of these operators.
The nice thing of using Void on both sink and source is that you can compose a sink and a source to produce a conduit. I like that symmetry. I think that for now, we should focus on cleanly modelling streaming data processors, on not focus too much on practical concerns. When the model is settled, then we can choose what the best practical API is. F.e. it might well be that separating pipes in Source, Sink and Conduit is the best practical choice to make.
Has this been published anywhere?
I think it's cheating, but interesting nonetheless. :) If your program reads environment variables only for initial conditions, consider reading all environment variables once, then using the resulting pure hashmap for the rest of the program. If your program is expected to run continuously (e.g. web server) and needs to continuously update either its internal variables or the environment variables themselves, then your approach of making custom data types and monads for interacting with environment variables is a good solution. [RWH](http://book.realworldhaskell.org/) may have code for environment variables.
But how do I get that hashmap passed to the functions that need it? I have to thread it explicitly or use a Reader, as far as I know. 
If you only need say three specific environment variables, you can call `getEnv` three times and pass the purified strings on to the rest of the program. If you need a lot of variables, use `getEnvironment`. It returns `(key, value)` pairs for each environment variable at the time of execution. You don't need to parse raw environment variables such as `CLASSPATH=/home/mcandre/junit.jar"` because Haskell already does this for you. classpath :: [(String, String)] -&gt; String classpath = -- ... main :: IO () main = do env &lt;- getEnvironment putStrLn $ "Java classpath is " ++ classpath env You'll find these functions in the [System.Environment](http://www.haskell.org/ghc/docs/6.12.2/html/libraries/base-4.2.0.1/System-Environment.html#v%3AgetEnvironment) module.
I've needed to do this twice so far, probably not coincidentially in my two largest programs. Both times I started out manually threading information until it got too unwieldly and then refactored several thousand lines of code. I do take care to use a newtype. Should be more efficient, but what I find really useful is it makes type error messages clearer. newtype Backup a = Backup { runBackup :: StateT BackupState IO a } Two things to keep in mind: * Still write pure code whenever possible. It absolutely makes sense to retrieve something from the program's state, and manually thread it through some pure code. Similarly, when you can write a regular IO action, do so, it's probably more generic. (I checked the larger of my programs and half its top-level functions are pure, of the remainder, more than half are still generic IO, not using its custom newtype.) * Keep the state including only things that it really makes sense to have as globals throughout the program. I'll sometimes layer on another State monad if a submodule needs its own transient state. 
I'm going to disagree with the general sentiment of this thread, because I don't think it's cheating at all. An imperative programming style is theoretically sound since it is founded in monads. However, you will pay a huge performance penalty if you use imperative code everywhere because GHC is designed for optimizing pure code very aggressively.
Thanks for giving context. One case where I find the monadic code (RWS or just basic IO) helpful in a "guilty" way is that I find sequenced code easier to debug than lazy-evaluated code where GHC decides what to evaluate. Since we don't have stack traces yet in GHC, I often get in a situation where I have a failure but it is hard to pinpoint the location of my bug, because I can't say that one portion of the code executed successfully but then it went sour at point X. With pure code, I just have to build confidence in each function individually and then trust and hope that no integration bugs pop up in the large. Purity is great for minimizing integration bugs, but with laziness there are hurdles when one such bug appears. 
&gt; (lift .) defines a functor from the inner monad's Kleisli category to the outer monad's Kleisli category. In other words, (lift .) is equivalent to fmap and the monad transformer laws are just the functor laws, except not in the category of Hask wat.
I freakin love JSON, it makes Web 2 work.
When cultic_raider said he "spent a lot of time adding function arguments for 'environment data'", I don't think he was talking about the Unix environment, but rather the general programming environment his code was running in -- I'm pretty sure he knows how to pass arguments to his functions, but finds it annoying to thread the same argument to all his functions. (...and hence, uses the 'R' part of RWST to do that, instead.)
Hmm. Composition could help: main :: IO () main = do e &lt;- getEnvironment func' = func e func2' = func2 e func3' = func3 e putStrLn $ func' ++ " " ++ func2' ++ " " ++ func3' Of course, these helper functions would only be visible inside `main`.
Good, don't start yet! I was stuck on a train for 3 hours this morning, and got a huge head start! I just uploaded the code to a `pipe` branch. Please have a look there and continue. All in all: I think I like it. There are some annoyances (having to pass () to a `NeedInput` constructor for `Source`, for instance). But I've not yet run into anything which is insurmountable. This actually all still works with `BufferedSource`, but working on this gave me a different idea: maybe all we need is a connect operator that returns a new `Source`, e.g.: connectResume :: Source m a -&gt; Sink a m b -&gt; m (Source m a, b) I *think* this would completely solve the use case `BufferedSource` is intended to address. I'll need to look at this more later.
I agree that packages without Haddock documentation is unpleasant. But is RWST "abandoned", or "solved"? It is part of `mtl` package, which got upgraded to a whole new API version, and does have some nice examples, but you have to click around to find the module pages where the examples are given. R, W, and S are each documented on separate pages, example http://hackage.haskell.org/packages/archive/mtl/2.0.0.0/doc/html/Control-Monad-Reader-Class.html#t:MonadReader 
Yes. Sorry for any confusion arising from my ambiguous use of "environment". 
&gt; If you want to continue to assert that this change is more elegant without backing that up, fine. As said, my backup is not code but mathematical reasoning. How exactly would conduits three operators differ from their proposed unification? Of course I understand that nobody has written a proof that they are the same, no argument there. But that's not something I worry about. The nonexistence of this proof would have a good reason, and that's one I would like to know beforehand. The difference can't be the input/output functionality, because that is determined uniquely. So, it must be resource usage as you keep saying, but maybe you could clarify what that the difference is and why it cannot be unified? I have looked at the documentation for conduit, but I don't understand the resource model used. How can I reason about resource usage within conduit? What are the guarantees I get and how can I write proofs? Before I understand the crucial difference, I see no reason why it shouldn't be the same as a pipe implementation. At the moment, I even find that lazy lists are easier to understand, because at least I do know a model for understanding memory usage there. ----- If anything comes from this discussion, then hopefully a mutual understanding that there are different approaches to correctness: "existence of tested code" vs. "mathematical model".
We do have stack traces in GHC as of 7.4 if we have profiling enabled.
The nice thing about this variant of pipes is that NeedInput also has a source field to use when there's no more input available. So it's better to use that than to keep feeding `()`.
What are you talking about? `RWST` is part of [transformers](http://hackage.haskell.org/package/transformers) and therefore [mtl](http://hackage.haskell.org/package/mtl), almost assuredly one of the most-used packages on Hackage. It's no more abandonware than [the `State` monad](http://hackage.haskell.org/packages/archive/mtl/latest/doc/html/Control-Monad-State-Lazy.html).
Oh sweet! I knew Simon was working on it, but I didn't realize there were results yet. And I was about to add that beyond stack traces, in a lazy environment we need execution traces to understand what a program has done, but I suddenly realized that (I think) Haskell Program Coverage can be used here. Previously I had thought of it as a test-coverage tool like Java Clover, not a debugging tool. Hmm. 
`data FalseBinary = CompleteSolution | JustTalk deriving (Read, Show, Hyperbole)`
Perhaps you can add some documentation on how to use it without the Quasi Quoter. The function and module names both end in "Quote": I am very confused.
Monadic code is lazy too. The only thing that a monad enforces is the order in which operations are done (which you obviously need for IO).
My claim here is very specific: there's nothing but an assertion to back up the claim of *elegance*. I understand that it's a seemingly obvious assumption (less types == more elegant, right)? I'm trying to make clear that this may not be the case. Now that I've started to work on actual code for this, I feel much more familiar with the subject matter, but I'm still not convinced it's a slam dunk either way. I'll be uploading a blog post later.
&gt;having to pass () to a `NeedInput` constructor for `Source`, for instance I find sjoerd_visscher's arguments in reply to [this comment](http://www.reddit.com/r/haskell/comments/rbgvz/conduits_vs_pipes_using_void_as_an_input_or/c44t8z3) compelling; using `Void` as the input for a `Source` doesn't actually cause any problems, because you can simply terminate it early (as if EOF was encountered), and it lets you compose a source and a sink to produce a conduit. That seems a lot more elegant than creating an artificial infinite stream of `()`s to start the process off to me.
My thoughts: I believe this approach, including the removal of the `BufferedSource` type, makes things a lot more simple and elegant. I don't think the single-type approach will be too confusing to beginners: you can explain `Conduit a m b` as "transforms a stream of `a`s into a stream of `b`s", and `Pipe i o m r` is basically the same: "transforms a stream of `a`s into a stream of `b`s, producing a final result `r`". Indeed, when I first read up on conduit, before pipes existed, I thought that `Source`, `Sink` and `Conduit` sounded *very* similar, and I saw a lot of duplication in their constructors; perhaps my experience of not typical of a beginner, but I think the unnecessary differentiation and duplication of these concepts confused me rather than helped. I don't think `newtype` wrappers are a good idea; you'll lose the conceptual simplicity that comes from . Sure, it means you won't be able to hide the existence of `Pipe` from users, but you shouldn't do that, anyway. It's not like iteratees, where types like `Enumeratee` hid complicated underlying types; `Source`, `Sink` and `Conduit` simply become *shorthand* for established concepts. As far as the `Monad` instance, I don't think it's that big a deal. Perhaps `Conduit` should be changed so that `Conduit i o m` is `Pipe i o m ()`, to emphasise the relation between the two, and to not make people expect the `Monad` instance to behave as it previously did. I always found having the monad be the middle parameter strange, anyway, even though I understood the reason (to allow a `MonadTrans` instance); since that reason is gone, moving it to the front or the back seems like a good idea to me. Were it not for the order chosen by `Pipe` (for the same reason), I'd suggest the front, but `Conduit ByteString Blah IO` seems fine to me. It's a conduit from `ByteString`s to `Blah`s in `IO`. As I said in the previous thread, I think [`Void` is a better choice for `Source`'s input than `()`](http://www.reddit.com/r/haskell/comments/rbgvz/conduits_vs_pipes_using_void_as_an_input_or/c454vsm?context=1). (*Edit:* Although, it seems like the early termination branch when input demanded is more or less another `Source` (to allow providing output after all input is exhausted), so this might not apply.) I think that keeping around `(=$)` and `($=)` will confuse more than it helps; it provides additional specialisation of the types, but at the cost of making people think there's three different notions of composition at work, when in fact there is only one. Again, don't try to hide `Pipe` behind `Source`, `Sink`, and `Conduit`; let `Pipe` be the main concept, and the three type synonyms merely handy aliases for common uses. I realise this goes against the name of the package somewhat, but, well, "pipe" would just be confusing :) BTW, I understood your point in the original thread, and I'm not attempting to rekindle the flames ‚Äî this blog post demonstrates that things go much better when people are cooperating, after all ‚Äî but you seemed overly defensive, which (to me) seemed a strange reaction to an attempt to improve conduit. It's true that some of the claims might have been premature, but at least to me, and I expect to twanvl and the others defending the proposal in that thread, the duplication in conduit's types seemed obvious enough that it wasn't an unfounded assertion to say they could be simplified through unification, even without a proof that the composition operators could be unified. Certainly, I'd rather we have proposals for improving libraries that might make claims a little prematurely than to have people hold back on suggestions because they haven't yet rewritten an entire codebase to use them. Again, not trying to accuse anybody of anything; just offering my perspective on the disagreement. *Edit:* Also, I'm not sure why you say you can't do lazy IO with pipes. Does this translation of `lazyConsume` not work? lazyConsume :: (MonadBaseControl IO m, MonadActive m) =&gt; Source m a -&gt; m [a] lazyConsume (Done mv ()) = maybe (return []) absurd mv lazyConsume (HaveOutput src _ x) = (x:) &lt;$&gt; lazyConsume src lazyConsume (NeedInput isrc _) = lazyConsume $ isrc () lazyConsume (PipeM msrc _) = liftBaseOp_ unsafeInterleaveIO $ do a &lt;- monadActive if a then msrc &gt;&gt;= lazyConsume else return [] *Edit:* Oh, you were referring to the pipes package itself, rather than pipes in general. (This is going to get confusing...)
&gt;Thanks for the feedback here. I'm hesitant to change the order of parameters in `Conduit`, simply because if we avoid doing so old code works. It's pretty cool that a large subset of code that worked with `conduit` 0.0 still works with 0.4. Yeah, it's not a big deal, although if you're going to break backwards compatibility, now would be an excellent time :) And yes, I was only referring to the three composition operators; the others are definitely different. (I think `($$+)` would be a nicer name than `($$&amp;)`; the &amp; is too visually similar to the $.)
I would like to support suggestion of breaking compatibility. Conduits are still in flux. If new interface gives any advantages i think it's worth breaking from the old code. In the end you guys were not shy to break Yesod API many times :) But i agree that it pays off. 
I remember it used to be widespread that programmers who just moved to C from Pascal would use "#define BEGIN {" and "#define END }" macros so their programs would look more familiar to them. It's in the nature of programming languages that you can create new paradigms within them that are almost entirely different than their intended design. You can create garbage-collected functional style programs in C, and you can create imperative-style programs in Haskell. It's just a matter of a little extra notation and willingness to accept slower performance. It certainly is possible to do things like this, but the better question is should you do this. You can program to optimize machine resources consumed, source code size, machine code size, or any of a number of things. I think that good idiomatic Haskell helps you make the most of your mental resources.
One exciting thing that is possible with the pipes interface is writing conduits and sources in monadic style using the combinators yield :: o -&gt; Pipe i o m () -- = o -&gt; Conduit i m o await :: Pipe i o m i Or with the handling of closing pipes: closeOrYield :: m () -&gt; o -&gt; Pipe i o m () tryAwait :: Pipe i o m (Maybe i) Some other functions can also get a simpler type. `haveMore` no longer needs a 'next' conduit, you can just use `haveMore' close xs &gt;&gt; next`. haveMore' close = foldM (closeOrYield close) Something similar might be possible for `sequenceSink` (I don't really like the name `sequence`, because it conflicts with `Control.Monad.sequence`). sequence sink = forever $ do x &lt;- noOutput sink yield x As I write this, I realize that you need a `noOutput` function to match `noInput`, that converts a `Pipe i Void m r` into a `Pipe i o m r`. A nicer way to handle this is using a universal quantifier, type Sink m i r = forall o. Pipe m i o r type Source m i r = forall i. Pipe m i o r type Composition m r = forall i o. Pipe m i o r I am not saying you should actually do this, but it is something to consider for the pipe-theorists.
Didn't I read that there were some issues with a `yield` function? I know under `conduit` 0.3 I specifically list it as a function that can't be implemented, but that's because it would be implemented as a `Sink` providing leftovers that were never fed to it. Overall, I still prefer the approach of writing the functions directly, but that could just be because I'm more comfortable with it still. And the `forall`s definitely make me nervous, I'm far from ready to embrace that. I still want to bring the rest of the conduit code up-to-date with the current changes to make sure it actually works.
For what it's worth, when initially trying to understand the various approaches to stream processing, the need to write my own constructors and do "manual plumbing", as it were, felt very strange and low-level to me. Being able to write individual stream processors (that don't come under some common higher-order pattern) as simple loops of reading and writing values would be much better, IMO, especially since you can use standard patterns like `lift` to run an action in the base monad; since `Pipe` falls neatly into standard interfaces like `Monad` and `MonadTrans`, there's no reason to make people reimplement the instances to use the functionality. By the way, the documentation of `Pipe` in `Data.Conduit` references the `pipe` function, but it isn't exposed from that module. (Speaking of which, I feel like `pipe`'s first parameter should be polymorphic in its result type; I realise the result is discarded, but it seems strange to *require* there to be no result just because of this; it's like `(&gt;&gt;)` having the type `(Monad m) =&gt; m () -&gt; m a -&gt; m a`.)
It is probably a very stupid question but why not instead of let src = sourceSocket socket headers &lt;- src $$ getHeaders let req = makeRequest headers src app req something like this let src = sourceSocket socket let req = src $$ (getHeaders &gt;&gt;= makeRequest) app req It probably won't work exactly like this, but as far as I understand there are two ways to compose pipes. `(&amp;&amp;)` is for pumping the output stream of one pipe into another one and `(&gt;&gt;=)` lets a pipe pick up where the previous one left off. 
No, not yet.
The types don't even add up there. The type for `req` would now be `IO Request`, so presumably `Application` would be `IO Request -&gt; IO Response`. As soon as you run the action to get your `Request`, the data discarding has already occurred.
As someone who prefers not to write CPS-transformed code by hand, I'm a big fan of `await` and `yield` over using the Pipe constructors directly. Using polymorphism to express that a `Pipe` doesn't care about its input or output is cute, but I don't think it's worth the complexity in the types and the type errors. Using polymorphic conversion functions seems like a fine tradeoff. It might also improve error messages to make `NoInput` and `NoOutput` two type synonyms for `Void`. 
Since you were talking about stack traces and how confusing lazy evaluation is, I was assuming you were talking about something where evaluation order matters, like failures. Printing 7 when it should print 8 will happen with and without lazy evaluation.
Or perhaps he's thinking of pipes-core vs pipes proper?
&gt; At some point as a community we need to decide what we want. As someone who has to maintain a codebase for a living I value API stability. Let me provide the counterpoint to that. Haskell is what it is today because people are willing to rethink things. You are, after all, choosing to use a programming language where it took literally *years* of debating multiple designs to answer the question of "how do we print Hello World?" And even that is still occasionally debated to this day. Haskell is also what it is today because people are willing to think formally about semantics, and not just throw in whatever solves the specific problems they are facing right now. As a community, we look for ways to build software that satisfies expected laws of behavior, and matches the meaning expressed by simple well-described models. That's led to some really good things, and we have a better programming language for it. Not to say there isn't a trade-off here. Stability in the really basic parts of the Haskell library ecosystem is valuable, sure, and we're getting there. But the conduit library is less than three months old, and on version 0.3 now! If you're really that worried about breaking API changes in a pre-1.0 library version that has only been around for a couple of months, then I'd suggest maybe you misunderstand the basic dynamics of the community.
This was exactly my problem with the initial post, which almost felt like it was preaching some magical solution to all of our problems, without addressing any of the main motivations behind Conduit. Michael understands the issues you're having in the real world, and this is why he's made many of the decisions he has. Some of these solutions could be more elegant, but there's often a good reason for them not to be so in the long run.
I mean to say that *debugging* under lazy valuation is harder than under strict evaluation, because it's hard for me to mentally follow the execution of reductions and evaluations. In a strict language I can imagine the call tree in my head, and navigate that tree as I step in/out/over functions. In a lazy language, if I interrupt execution when I note that some output is incorrect, it's hard to know which code has already executed and may be to blame. Or if I have a suspicion that a certain part of my code is bad, it's hard to set breakpoints just before the bad part and step through it slowly. 
The Hackage link to the conduits page is broken: Missing the trailing s
I'm so glad to see someone is still working on this, I've been wanting this wort of work for a long time now. Have the LLVM guys been shown these results? I'm sure they'd be interested in why LLVM isn't producing optimal code, if we know it can.
No, you were correct the first time. You can't communicate upstream or artificially restore control upstream without violating the category laws. This always breaks the identity law, no matter how monstrous of a kludge you make idP.
The proposal that finished out that thread was to add unawait with the semantics given (loosely... I skipped plumbing of return values so these don't type-check, but it's doable) by rewrite rules like this: unawait x &gt;&gt; lift m ==&gt; lift m &gt;&gt; unawait x unawait x &gt;&gt; yield y ==&gt; yield y &gt;&gt; unawait x unawait x &gt;&gt; await ==&gt; return x So no information is really moving upstream at all. Then you define composition to just throw away the downstream unawaits (eww... I know, but it still follows the category laws), and that does end up being associative, just without an identity. So then you graft on an artificial `idP`: data Pipe a b m r where IdP :: Pipe a a m r RealPipe :: ... -&gt; Pipe a b m r Which is a silently discarded in `&gt;+&gt;`, and silently converted to `pipe id` when composed monadically. The result is a `Monad` and a `Category`, but really doesn't act like you'd hope. I left off thinking to myself that I should ask what other laws we should be expecting, that would forbid just throwing away information like that. But I never thought it through in detail.
Oh, but (and this didn't come up then, I just realized it now) this couldn't be made into a correct `MonadTrans` instance, because you have the law `lift . return == return`, and there's no way to recognize what is just a `return` in the inner monad. So you wouldn't be able to special-case `&gt;&gt;=` for `return` the way you would need to in order to keep the monad laws holding. So the `MonadTrans` instance prevents that trick. Which is fine... it was a crime against humanity to begin with. :)
I was referring to [this](http://www.reddit.com/r/haskell/comments/qq5p6/pipescore_001_released/c3zpp82).
Sure, but in neither case does `yield` break the category laws. In Paolo's release, it was `ensure` that did so. And in the haskell-cafe thread, it was `unawait`.
Don't be afraid of breaking changes. I think it's implied that if a package is v0.* to expect major changes.
Yes, it was `unawait`. I remember noticing that because it had the same problem as `enumerator`'s `yield`. Thanks!
You're correct: `Sink`s can be composed monadically exactly the way you describe. The problem is when `consumesSomeMore` is really a massive function (like a web application) that will be performing a bunch of other code, and eventually read in the request body. Under enumerator-based WAI, the solution was to have the application live in the `Iteratee` monad (equivalent to `Sink`). For various reasons, this greatly complicates things. (Mostly exception handling. But I'm kind of invoking proof-by-assertion here, just trust me that it was bad.) Also, the error message issue *is* my biggest concern right now with the unified type.
Great stuff! It seems to me that the issues could be related to LLVM detecting/using some of the optimizations related to loop induction variables. You could test some of the following flags from the LLVM documentation. * http://llvm.org/docs/Passes.html#indvars * http://llvm.org/docs/Passes.html#licm * http://llvm.org/docs/Passes.html#loop-reduce * http://llvm.org/docs/Passes.html#loop-simplify * http://llvm.org/docs/Passes.html#mem2reg * http://llvm.org/docs/Passes.html#iv-users Maybe haskell code requires a few extra of these passes? By just looking at the LLVM documentation, it seems like the following could be fixed by some of the above passes: * In the multivector dot product loop, there are three data-dependent instructions required to calculate the load into xmm2, two of the loads are from the stack, and are loop-invariant. They should be hoised out of the loop (licm), and be put in registers (mem2reg). * In most of the loops, both the upper and lower bound is checked for the loop induction variable. It looks like fixing this is the job of -indvars and -loop-simplify. * I don't think having a slightly larger loop and multiple induction variables hurts performance at all, so I wouldn't worry about it. 
Apologies, but I'm not following all of your terminology. When you say "pass state between sources," are you talking about my `BufferedSource` solution? I'm wouldn't classify is as passing state between sources, it's simply *not* closing a `Source` after the `Sink` terminates. In other words, the pattern of `conduit` is that as long as the right-hand pipe is not terminated, it tries to get data from the left-hand pipe. If the left-hand pipe terminates first, then early termination is applied to the right-hand pipe. In *normal* connecting (`$$` and the `pipe` function), if the right-hand pipe terminates first, then early termination is applied to the left-hand pipe, and any leftovers from the left-hand pipe are discarded. The connect-and-resume approach modifies that last sentence. If the right-hand pipe terminates first, then the current state of the left-hand pipe is returned, together with any leftovers from the right-hand pipe. This means we can reuse a `Source` from exactly where it left off. I haven't spent much time playing with this yet (I *was* pretty exhausted when I got home last night), but my initial reaction is that this more elegant approach is going to turn out to be less powerful in practice. Warp, for example, needs to read some headers and then pass the `Source` to an `Application` which it can't control, and which can't be trusted to return an updated `Source`. So I'll basically have to reinvent part of the `BufferedSource` idiom anyway. It might be possible to create a modified `isolate` function which wraps around an `IORef` containing a `Source` and apply connect-and-resume to it, but I only woke up 10 minutes ago, so don't trust anything I say ;).
I would agree that, from a user of Warp or http-conduit/-enumerator, there's not much of a difference between `enumerator` and `conduit`. I don't really intend `conduit` to be a package used by most programs for most tasks. For example, if you look through the Yesod book, I think there's only one example (besides the conduit chapter itself) that uses conduits. It's a low-level tool. The one demonstrable change should be the greatly simplified API we were able to add to http-conduit by getting rid of the Inversion of Control necessary for `enumerator`. That is directly caused by the move to `BufferedSource`, which is why I consider it such a central concept in the library, and why I've been so dismissive of claims to "fix" conduit which don't involve it. In other words: if you're working with xml-conduit, Warp, yaml, or any of the other few dozen packages uses conduit, I hope this discussion is completely irrelevant to you. conduit 0.0 to 0.2 introduces significant performance enhancements without modifying the external API much. It was important, but internal. 0.3 was even *more* internal: it simplified the API for those of us writing conduit code, but hardly touched user-facing code. If this set of changes goes through for 0.4 (which is *not* a given), it will be even less intrusive than the 0.3 changes. In my testing so far, all that really needs to change is the names of constructors in your code. I agree that as a community it would be great to standardize on just a single package. I tried for a while to make that package `enumerator`, but unfortunately for [various reasons](https://github.com/snoyberg/conduit) it wasn't possible. I feel pretty confident in `conduit` and its ability to solve our problems. Time will tell if the rest of the community agrees with me.
Did you read the post you replied to, or only the grandparent? What I'm talking about is the quality of documentation. This is the documentation on Hackage: http://hackage.haskell.org/packages/archive/mtl/1.1.0.2/doc/html/Control-Monad-RWS-Strict.html &gt; Inspired by the paper /Functional Programming with Overloading and Higher-Order Polymorphism/, Mark P Jones (http://web.cecs.pdx.edu/~mpj/) Advanced School of Functional Programming, 1995. That's it. Btw, the paper is nowhere on that web page. There is a link to publications among the 30+ links on that page though. It is on a sub page, the correct link is: http://web.cecs.pdx.edu/~mpj/pubs/springschool95.pdf Basically, hackage is of no help, you'll have to use Google. Now that paper is 40 pages long, including an introduction to type systems. There is no reference documentation at all. It isn't pleasant to explore this type of library unless you have 3 hours to spend reading the paper. 
Firstly, I think it might be possible to get rid of `sequenceSink` entirely if I change `Sink` to use `forall` instead of `Void`. I'll follow up on that. But I'm confused about `await`. It seems the same as `Data.Conduit.List.head`. The difference is that await returns `i`, while `head` returns `Maybe i`. So what happens with `await` when there's no more data in the stream?
In your line: almostMax (a:as) = (reverse (sort [a:as])) !! 1 When you say a:as in the RHS, you re-build the list; then you put that list into a new one-element list. It's that one-element list that gets sorted and reversed. When you do !! 1, it tries to get a second element, but that doesn't exist. Something like [(reverse (sort (a:as))) !! 1] might work better. It looks like you're using lists as a way of dealing with a value that may or may not be present. You might consider using the Maybe type for that. Also, you destructure the list then immediately reassemble it. You could just capture it in a single variable.
My apologies! Like I said, I'm a beginner. I know almost nothing about Haskell! To capture it in a single variable, would I just say "almostMax myList = ..."? Also, what do the [ ] on the outside of your example mean?
Hmm, it still seems to be telling me that my index is too large. Was I accurate with my base cases? I check to make sure they are at least size of 2 before doing any kind of list arithmetic.
You should really use sortBy with a reverse sorting funciton: almostMax l = y where x:y:z = Data.List.sortBy (\x y -&gt; if x &lt; y then GT else LT) l Or if you'd rather not use structural decomposition: almostMax l = (Data.List.sortBy (\x y -&gt; if x &lt; y then GT else LT) l) !! 1
For me, it works with your base cases and: almostMax (a:as) = [(reverse (sort (a:as))) !! 1] What error message are you seeing? For the ghci import thing, try :load. :load works for source files in this directory, and import is for installed modules. 
I'm sorry, but I think that's far beyond what my instructor has covered. Maybe once I learn my basics a lot more first!
Collapsing three operations (`$=`, `=$`, `=$=`) into one seems pretty elegant to me. Of course, if they behave slightly differently concerning resource usage, it's not going to work out, but then it's important to clarify "resource usage" to the point that one can do proofs about it. (For instance, for lazy lists, resources are like large unevaluated expressions and I can link the closure of a file to the evaluation of a constructor.)
Well, it is only for printing them to the screen, so that shouldn't be a problem!
Here's hoping that pipes and conduit will continue to have a mutually-beneficial relationship! In the end, I think our main goal should be to either 1) merge the two packages, or 2) draw a clear line between the two. That line used to be practicality vs theory, but I find it highly likely that a thoroughly thought out theory will end up being the most practical solution on all accounts. It's amazing how many phases of evolution conduits have been through in such a short time period, and I don't think it's an overstatement to say that the package has been showered with praise every step of the way. I again feel that this is a good step forward, and eagerly anticipate the next development.
This is probably what I would use: almostMax = head . tail . reverse . sort 
As a general comment, I'm finding this much more accessible than the previous Conduit implementation. Perhaps that's just because each time I run at the hill I get a little further up, but I was definitely confused by the three different types with apparent similarities before. Could you explain the role of the "leftover" field of Done? I don't see it being instantiated with a Just anywhere in the core functionality (Data.Conduit.Internal), but it is filled in with Just in a couple of places in Data.Conduit.List. How do exceptions work? In Data.Conduit, the docs for the Pipe type mention the "pipe" function, but it isn't exported by Data.Conduit. 
And that's exactly what we want to avoid. We went down that route with enumerator, and it introduces far too many complications. It becomes difficult to deal with exceptions (since `Pipe` can't be an instance of `MonadBaseControl IO`), and implementing something like an HTTP proxy is virtually impossible
I think it's either that or "resumable" pipelines via mutable state, like you said. I personally prefer the approach of just composing pipes to express the desired behavior, for the usual reasons why mutable state is bad. Note that you can do exception handling in pipes-core as if Pipe were an instance of MonadBaseControl, as you have the same primitives of lifted-base.
I agree. Also, releasing a library to hackage doesn't imply any claim that the library is usable for production code or is the best possibly solution to its use cases. It's unfortunate that hackage gets polluted over time with failed attempts and deprecated libraries, but I don't think this problem should be solved by discouraging people from releasing experimental approaches.
Regarding leftovers: there seems to be some conceptual similarity between the leftover field of Done and pipeResume. Something feels a bit strange about the design here, but I can't immediately envisage a way to simplify it.
I really liked the changes done in conduit 0.30. I think all the constructors are fairly simple and the types seems to match the use cases. As far as I can tell you only need to know the two invariants to be able to use (low-level) library - never reuse and do not yield data you have not received. Overall I think they are the simplest to understand of all the versions so far. They are a bit inconsistent though - Source m a, Conduit input m output, Sink input m output. Wouldn't it make more sense to use Source m output, Conduit input m output, Sink input m result? I have a similar complaint for their constructors - but this is all very minor details! ;) I like the new change from BufferedSource to a resumable operator. It gives rise to a bit more boilerplate, however the main invariant is never relaxed in this case, leading to simpler understanding for people who are new to the library and leading to a simpler interface for the programmer. However I don't like the combination of Sink, Source and Conduit into a single type. For example it would be possible to construct a Source that needs input (of unit type) - or perhaps even worse a "closed" conduit that still needs input. Likewise you could construct a Sink that that has output. So you would now need invariants for stuff that was forced by the type system before. Of course this is a reasonable compromise if the change leads to other benefits however I cannot see such benefits currently. Though this new Pipe is fairly simple, I do not find it as simple to use in practice as the Source, Sink and Conduit types. I would need to know in much in much more detail what a pipe is allowed and not allowed to do, thus making it harder to write code around the types. For example, when I use the "closed" pipe from a NeedInput pipe, I still need to handle the case where the returned pipe is a NeedInput too - unless I want to get a "Pattern match(es) are non-exhaustive". In this case it should probably be handled with an error, but I still need to keep track of that in my head. :) /my two cents
I think it's pretty clear you want the Maybe version of await. Paulo had it in his "guarded pipes", called tryAwait: http://pcapriotti.wordpress.com/2012/02/02/an-introduction-to-guarded-pipes/ What seems strange about the leftovers is that you can only have zero or one input element leftover. Having "unawait" would subsume leftovers, and seems more natural (if it could be made to work).
It was actually an explicit design decision to form leftovers to only every be 0 or 1 elements. This all comes down to data loss rules: even though theoretically we could use lists of values for `NeedInput` or `HaveOutput`, the entire library restricts itself to dealing with single elements at a time, to make sure we never consume too much input at one stage which is then lost. I'm not really sure how `unawait` would be implemented without having something like the current setup of the `Done` constructor, can you clarify?
 Concerning `Void` for input, the comments on Twan's post discuss that, it's not the right thing to do. If you ignore resource usage, the semantics of `Pipe` are type Pipe i o m a = [i] -&gt; m ([o],a) You can't output `Void`, so setting `o = Void` is ok. But when you get a `Void` as an *input*, you can make anything from it, literally: anything :: forall a. Void -&gt; a -- the empy line right above contains all the pattern matches -- we need to consider to define the function anything So, if you ever get `Void` as an input, you can make yourself a millionaire, among other things.
A small note concerning the original `BufferredSource`: Reading the example code in your blog post, I think that `BufferedSource` has a fundamental flaw, namely the following: Ignoring resource management, you can think of sources as representing a list of input characters. With that point of view, the code bsrc &lt;- bufferSource $ sourceSocket socket headers &lt;- bsrc $$ getHeaders let req = makeRequest headers bsrc app req looks as if you are taking a single list of characters `bsrc` and parse it twice, once via `getHeaders` and once via `makeRequest`. But that's not what happens, the call to `getHeadres` will mutate the contents of the list, namely remove the header. Obviously, that's a side effect. In constrast, your second variant returns a new "list" `src2` and no side effects are needed to describe what elements it contains. I think that's a clear advantage of your new approach. 
Scattering isn't really a problem, imho. In fact, it's better, as long as the bits of information link to each other.
After thinking and reading here is what I think the problems currently are: 1. Installing a new package that causes another one to be rebuilt with different dependencies causes other packages to break. 2. It is not possible for a package to depend on packages that depend on different versions of the same package, even if they don't expose anything from the different versions. 3. There is no way to see if the profiling version of a package was built or not. If it is missing cabal does not automatically build it. 4. It is difficult to build a package that depends on multiple packages that are not released on hackage. 5. If someone uploads a new version of a package to hackage that makes breaking changes other packages might break if they use this new version. 6. Cabal does not automatically install build tools. What else?
Finally i will be able to use hlint on my yesod app. 
What you're saying is absolutely true, and is why I prefer the new approach more. However, the mutate-the-source approach is exactly how most non-`conduit` data sources work (e.g., `Handle`, `Socket`).
To say this in more detail, the line that is the problem is: almostMax (a:as) = (reverse (sort [a:as])) !! 1 Reading that from the inside out, what that says is: 1. Start with the list `a:as` 2. Build the singleton list with only one element, which is `a:as`. You now has a list *of* *lists*, that always contains exactly one element. This is what the square brackets do. For example, `[5]` is a list with one element, which is `5`... similarly, `[a:as]` is a list with one element, which is `a:as`. 3. Sort that list... but since the list has only one element, it is already sorted by definition, so the sort does nothing. 4. Reverse that list... but since the list has only one element, its reverse is the same as itself, so this also does nothing. 5. Take the element at index 1, which is the second element... unfortunately, the list has only one element, so this produces an error. The problem, of course, is in step 2. Because `a:as` is already a list, you don't want to create a new list to contain it... you just want to use it on its own. So you'll want to use parentheses.. `(a:as)`, instead of the square brackets. almostMax (a:as) = (reverse (sort (a:as))) !! 1 Now for some stylistic things: First, you don't need the pattern `a:as` there... it's not broken since you know the list will be non-empty, but you also aren't doing anything special with the first element. It would be simpler to write the shorter expression... almostMax as = (reverse (sort as)) !! 1 Second, using a list to represent an optional value is poor form. There's a Prelude type for that already, called `Maybe a` for any type `a`. For example, the values of the type `Maybe Int` are `Nothing`, `Just 0`, `Just 1`, `Just (-1)`, `Just 2`, `Just (-2)`, and so on... The special value `Nothing` represents no answer, while every other value is prefixed by `Just`. So I might have written it this way: almostMax [] = Nothing almostMax [a] = Nothing almostMax as = Just (reverse (sort as) !! 1) Someone else suggested an inverted compare function instead of reversing the sorted list... that would be an improvement, but it's not clear how far along you are in the learning process, so if it makes sense, do it... if not, the above is at least a big improvement.
I don't know which is the better approach, but I'm not convinced by this argument. Why is it acceptable in `conduit` itself to call `absurd`, but for someone writing a `Source` it's not?
Well, the point was that you can demand a `Void`, but the conduit code will simply the *other* option it has: early termination. However, as I said in my edit, as the early termination for a demand of input is another `Pipe` with an input of `()`, this doesn't work.
Agreed, this will be very useful if it ever makes it to a GHC release. Have been implementing a lot of code on top of Repa lately and hopefully some day DPH. Very interesting read, thanks for posting.
hlint-1.8.26 is now hot off the press, with support for haskell-src-exts-1.12.0.
Well, obviously I can't convince you that it *is* a big deal. All I can say is that those of us who tried to work under the model you're describing with `enumerator` found it a sufficiently big enough deal to warrant creating `conduit` in the first place. This is why I'm so adamant that a good library for this stuff can't be written without strong motivating use cases: you won't have any real understanding of what is important and what isn't. Can you show me some code for your "split your input" approach? I have no idea how it would work.
It would be interesting to run this through Acovea and see if it can find a set of flags that work well. With enough testing, we may be able to find a set of 'good' flags that work well in lots of situations. A few months ago I took the liberty of updating Don Stewart's old acovea description for GHC and tuning it for LLVM. You can see the code here: https://github.com/thoughtpolice/ghc-acovea At the time I ran the benchmarks the results were rather inconclusive (it was an unstable build of LLVM and GHC, so there were some segfaults etc) and overall I couldn't get a huge improvement on DotP over the default, IIRC. It also takes a really, really long time for acovea to run all these tests (like, +6 hours.) It doesn't seem acovea is maintained anymore, either. Nonetheless if someone has a lot of RAM, time and examples to throw at it, it may prove useful.
Actually I agree that it's important, and in fact I claim that it is implementable in pipes, so it can't be the reason why one would choose one over the other. It's just not part of the primitives, although I suppose it could be integrated more strictly for performance reasons. The "split your input" approach I was referring to is creating a pipe of the form: Pipe ByteString ByteString m r which takes input chunks and splits them along their "natural" boundaries. For example, in an http server, you would look for the separator between header and body (double crlf I believe?) and split there, emitting the two pieces as separate chunks, so that successive stages in the pipeline don't need to bother with leftover values. If a chunk doesn't contain a separator, you'd just yield it as is, so that the pipe works as the identity (hence it doesn't have any effect, because of the category laws) away from boundaries. If this approach is not feasible for some reason, you can still use something like PutbackPipe to carry leftovers. It just requires some boilerplate to be used inside a pipeline. &gt; This is why I'm so adamant that a good library for this stuff can't be written without strong motivating use cases: you won't have any real understanding of what is important and what isn't. Yes, that's definitely true, and that's why I consider feedback from "conduit people" very valuable. However, I think the approach of starting from the "abstract" and gradually compromising towards practicality is also valid. I'm happy to see that the two approaches are converging towards some middle ground.
Thx, it works. Unfortunately haskell-src-exts still does not support DoAndIfThenElse. So i have to nudge a few of my statements before i can run hlint. 
Is there a reason not to raise some sort of error in addition to closing the pipe when an invalid constructor is encountered? This is the sort of programmer error that I think deserves a loud failure rather than a silent one.
That paper was really cool but dear god look at the hacks they had to do in the end. Type level representation of integers and abuse of the foreign function interface does not make for convenient programming.
Whether the input type is `()` or `Void`, it's not an error; in the latter case, you asked for an input, and the library replied that there is none; in the former case, you asked for an input, and the library helpfully supplies the only possible one. There's no reason you shouldn't be able to use pipes that *can* take useful input (but don't require it) as sources.
Don't forget import Data.List (sort)
My solution was almost exactly the same. And for the benefit of zsaleeba you replace friends with ["john", "pat", "gary", "michael"]
I guess mostly ease of understanding. Brevity can help with that, of course.
I think it still needs Control.Monad for `zipWithM_`.
This will print forever.
No it won't. This is a parallel list comprehension, so it will use zip instead of bind.
 msg (i,friend) = putStrLn (concat ["iteration ", show i, " is ", friend ]) friends = ["john", "pat", "gary", "michael"] main = mapM_ msg (zip [0..] friends) 
And now I have learned about -XParallelListComp.
But an experienced\* programmer will be scratching his head confused. I'd argue that it's actually less readable for him. By "most readable", do you mean for people who don't know much about the language, or people who do? \* I've barely dabbled in Python and I recognize what enumerate means. Just how much of a novice is your "novice programmer"?
 let pairs = zip [0..] (words "john pat gary michael") strings = map (\(i, name) -&gt; "iteration " ++ show i ++ " is " ++ name) pairs in mapM_ putStrLn strings
Now that you say it, I'm hesitant again. However, note that someone a source will never call `absurd`, that's only needed if you obtain a value of `Void`, which you can then convert into anything you like. One of the argument in the [comments section](http://twanvl.nl/blog/haskell/conduits-vs-pipes#comments) was actually different.
That notion of readable isn't a very useful one. Non-programmers don't handle much code, and code that's readable to them is going to lack very important features that allow composition, ease of reasoning, etc. 
A detail, really, but while I appreciate `friends` being declared upfront, I think you could improve readability by moving `format` in a `where` *after* the `zipWith`, because it's the kind of helper function for which it helps, I believe, to know about its use in context before looking in the particulars of the definition.
Chinese isn't readable to most native English speakers.
`null` is supposed to be a useful value to compare against and pass around. bottom is a runtime failure. Getting rid of runtime failures is great, but Haskell doesn't do that, and that's not what "fixing null" means. Haskell fixing null means that having a special valid empty value being passed around is allowed and type-safe.
Bad analogy here. A moderately experiences programmer can very easily read the Python version, even if he never ever saw Python before. Not so much with the Haskell versions. 
A moderately experienced *imperative* programmer can very easily read the Python version. Someone raised on functional programming might have great difficulty understanding `i += 1`. [Intuitive equals familiar.](http://www.asktog.com/papers/raskinintuit.html)
&gt;However I don't like the combination of Sink, Source and Conduit into a single type. For example it would be possible to construct a Source that needs input (of unit type) - or perhaps even worse a "closed" conduit that still needs input. Likewise you could construct a Sink that that has output. So you would now need invariants for stuff that was forced by the type system before. This is untrue. Since the output parameter for `Sink` is set to `Void`, it is impossible for it to yield any data (without resorting to `undefined`). And since the input parameter for `Source` is `Void`, you know (since there are no values of `Void`) that the output-only pipe you specify is the branch that will be taken, i.e. it's guaranteed to only receive an empty stream. When a `Sink` claims to output, the correct thing to do is to use `absurd :: Void -&gt; a` on the `Void` value it purports to give you; when a `Source` wants input, the correct thing to do is to tell it there's none. (This is actually an *advantage*, since you can use pipes which *can* use input but don't *need* it as sources.)
In the [documentation](http://www.snoyman.com/haddocks/conduit-0.4.0/Data-Conduit.html) the type is not Void, but (). I see that has been changed. Do you not still need to pattern match for those constructors? Also, will this change alter the invariants?
&gt;Do you not still need to pattern match for those constructors? Yes, but the majority of code won't have to pattern-match on the constructors at all, just like before. And, since you don't have to write the same algorithm three times (for sources, sinks and conduits), it's actually less code in the end. &gt;Also, will this change alter the invariants? I don't think so, but I'm not sure exactly which invariants you're referring to.
You are right, that you would probably never pattern match if you were using the high level API - but I don't think you would be using the constructors itself or the Pipe-type much either. So in this case I don't really think it matters which version you choose. Thus I mainly focus on writing/reading low-level code. Version 0.30 of the conduit library had very few rules for what a source/sink/conduit was allowed to do and not do - you cannot reuse them (except for BufferedSource, which I think should be removed anyways) and a sink/counduit could not yield data that had not been supplied to it. This change introduced more rules to keep track of, such as a Sink can never use the HaveOutput constructor. Before this was not possible at all, since it did not exist - but now it is possible (by using undefined), and if you are to pattern match on unless you want a GHC warning and you should treat it as an error of the pattern actually matches. Thus you have introduced more rules for using the library properly. If the goal of this change is to simplify how easy it is to understand the low-level API, then my opinion is that I think it has failed to do so. You might be right that the change leads to less code duplication and feel that it is a fair compromise - I have not yet formed an opinion on that subject. My only argument is that in my opinion the change is a complication of the datatypes themselves, not a simplification.
Indeed he does! Awesome news. And my gratitude for his quick response.
If you're going to go as far as formatting strings then you should really be using `string.format`.
I finally got to use this. [It works great](http://travis-ci.org/#!/clux/tournament.hs). Only problem is a problem with cabal itself: The cabal version which is on travis does not install dependencies listed under a Test-Suite (even with --enable-tests). To sidestep this temporarily, customize the install script in the .travis file to (for example): install: - cabal update - cabal install test-framework - cabal install test-framework-quickcheck2 - cabal install --enable-tests 
I wrote a small [demonstration](https://gist.github.com/2229180) that should fit your specification. The important parts: getHeader :: Pipe ByteString ByteString IO (Header,ByteString) webserver :: Pipe ByteString ByteString IO a -&gt; Pipe ByteString ByteString IO () webserver app = do (header,rest) &lt;- getHeader cons rest &gt;+&gt; (take (requestSize header) &gt;&gt; return ()) &gt;+&gt; (app &gt;&gt; discard) cons :: (Monad m) =&gt; a -&gt; Pipe a a m r cons x = yield x &gt;&gt; idP `getHeader` returns the parsed `Header` as well as the unparsed rest. `cons` feeds that rest to the next pipe and then acts like the identity pipe. `take` together with `discard` ensures that exactly`requestSize header` bytes are read and then the pipe terminates. The `return ()` makes explicit that if the last chunk of the request body contains parts of the next request they are discarded. You said that you don't want your request handler to be a pipe. What else should it be? Can that be lifted into a pipe?
I asked a SQL programmer last night whether she found the enumerate() version or the non-enumerate version easier to understand. She said the version without the enumerate() was clearer.
"SQL programmer"?
Never met a "HTML programmer" have we?
I've not been following the discussion closely, but it looks like things are moving quickly, and conduit is becoming more pipe-like. So what is the (latest) difference between the two again? 
Conduits indeed looks to be getting more pipe-like. There are still substantial differences, though. The design of pipes is motivated primarily by the desire to be intuitive and easy to use, which for the pipes maintainers means satisfying the natural properties one should expect for the kinds of things you're doing. Specifically: 1. You want to compose them. The natural laws for composing map-like things with domains and ranges are the category laws. So you want identities, and associative composition. 2. It's nice to be able to write single stages in a procedural style. The natural laws for this are the monad laws. So you want a monad instance with the appropriate properties. You also want the monad transformer laws. 3. (I'm starting to add this one, even though it's not traditionally been explicitly stated.) You want to be able to represent simple pure functions as pipes in a way that fits in well. The natural laws for this are that there should be a functor from Hask to the appropriate category of pipes. Once those properties are in place, there's really only one base design that makes sense, and it's the one implemented by the pipes package. The challenge is then to add the other things one might hope for -- like prompt resource finalization and handling of leftover data -- without interfering with those properties. So pipes is currently rather incomplete, and Gabriel and Paolo are working on releasing a version that handles those issues as simply as possible. Conduits, on the other hand, has taken the approach of trying to solve the practical problems right away and then refactoring the design. It is far less concerns with semantic consistency, and more on getting stuff done. It has a number of features that are there because of specific use cases. It does more, but its behavior is more ad-hoc. Some more specific differences: 1. Conduits has (indeed, was built for) an answer for prompt finalization of all resources. Pipes currently has no good answer at all for that, though it is being worked on. 2. Conduits handle leftovers by default, and automatically throw them away at certain points (mainly in downstream pipes of a composition). Pipes do not handle leftovers at all by default, and we have a pretty good understanding that a pipe that handles combining leftover values is inherently less composable than one that doesn't... so there are patterns for handling the issue, but it's kept a step above the core. 3. Pipe composition is a category. Conduit composition is strongly biased in favor of the downstream stage of the composition, and can't be made into a category with those semantics. (One consequence of pipe composition being a category, for example, is that any stage of the pipeline can terminate, resulting in the immediate termination of all upstream *and* downstream stages.) So while the two packages will be more similar now than they were, there is still quite a lot of distance between them.
Your "SQL programmer" friend most likely has some basic imperative programming background. Otherwise, "i += 1" or "$i++" would make little sense. Also, built-in iteration syntax is going to be more readable than passing a lambda expression to someone who doesn't even know the concrete syntax of the language. But it's a useful trade-off to make to simplify and empower the language. And the advantage of not having to learn lambda syntax *once* is not worth much.
&gt;Alright everyone, I'd like to make the BufferedSource thing a bit more concrete, and hopefully get some good input on something. I just did an initial port of Warp to conduit 0.4. As I expected, the tricky bit was coming up with a way to handle the request body. What we want is: * Get a Source from the Socket. * Parse the request headers from that Source, holding onto any leftover input. * Let the application parse the request body. It should be able to parse the leftover input, but should not be allowed to consume more than content-length bytes. * After the application finishes, Warp should flush the Source for any remaining bytes not consumed from the RequestBody, but should not consume any other bytes, since they may be part of a pipelined request. I wrote a small [demonstration](https://gist.github.com/2229180) that should fit your specification. The important parts: getHeader :: Pipe ByteString ByteString IO (Header,ByteString) webserver :: Pipe ByteString ByteString IO a -&gt; Pipe ByteString ByteString IO () webserver app = do (header,rest) &lt;- getHeader cons rest &gt;+&gt; (take (requestSize header) &gt;&gt; return ()) &gt;+&gt; (app &gt;&gt; discard) cons :: (Monad m) =&gt; a -&gt; Pipe a a m r cons x = yield x &gt;&gt; idP `getHeader` returns the parsed `Header` as well as the unparsed rest. `cons` feeds that rest to the next pipe and then acts like the identity pipe. `take` together with `discard` ensures that exactly`requestSize header` bytes are read and then the pipe terminates. The `return ()` makes explicit that if the last chunk of the request body contains parts of the next request they are discarded. You said that you don't want your request handler to be a pipe. What else should it be? Whatever it is, couldn't that be lifted into a pipe? 
I've put together what I believe are the final changes for this release, and I think I went with all of your recommended changes. Thank you for the input!
That's an interesting concept. However, without cooperation from the inner function (in this case, the web app, of which we do *not* assume cooperation), I think we'd still need a mutable variable to keep track of the `Source` state in. I agree with you that the `with` approach is cleaner than `BufferedSource`. I think it can be built on top of the connect-and-resume approach as well, but for now I think that the solutions I have for Warp and http-conduit are sufficient. If in the future more use cases come up, it might be worth revisiting this and seeing if we can generalize the pattern.
In Qt for example, QLineEdit has a textChanged() signal and a textEdited() signal (one is only emitted for user input, the other also for programmatic changes), or you can subclass it (or install an event filter on it...) and handle the QKeyEvents directly. Similarly for other widgets. Not familiar with GTK or Wx.
 words "john pat gary michael" That's a cute little trick :) reminds me of Ruby
As a minor nit, perhaps it might be better to use different terminology, because at first I thought you were going to talk about http://en.wikipedia.org/wiki/Bi-directional_text.
i have another vote for the ongoing use of cryptic line-noise operators in all haskell code...stop using them entirely &amp;&amp;$ '' &amp;&amp;- $$%....come on people, this is getting out of hand, i feel like i am reading the output of gzip '' from snap really gets me. i don't care who you are, when you see '', you think "empty string". who thought of using this as an operator????
This is the 3rd time in a row that reddit swallowed my submission the first time. I think it has something to do with editing or commenting on my post immediately after submission. Anyway, here is the resubmitted version and it shows ups now. I will delete the original post.
&gt; -- note: I actually use "forall r . r -&gt; m r" instead of "m ()" &gt; -- because I'm not completely convinced "m ()" is a &gt; -- monoid, but I'll keep "m ()" right now for simplicity. Well, for any monad `m`, `m ()` obviously forms a monoid with `mempty = return ()` and `mappend = (&gt;&gt;)`, per the monad laws. (Monoid in the category of endofunctors and all that...) Here's a proof: a &gt;&gt; return () = fmap (const ()) a = a -- since a :: m () return () &gt;&gt; a = return () &gt;&gt;= const a = const a () -- per left identity monad law = a (a &gt;&gt; b) &gt;&gt; c = (a &gt;&gt;= const b) &gt;&gt;= const c = a &gt;&gt;= (\x -&gt; const b x &gt;&gt;= const c) -- per associativity monad law = a &gt;&gt;= (\x -&gt; b &gt;&gt;= const c) = a &gt;&gt;= const (b &gt;&gt;= const c) = a &gt;&gt; (b &gt;&gt; c)
Not the Haskell committee, since `''` is not a valid operator name.
Are you talking about code like this? makeLens ''Rec That's template haskell and the `''` are used to quote a type or type constructor. It's syntax, not an operator. Though, I'm inclined to agree it's not very pretty ‚Ä¶ Also, the single-quote is normally a Char, not a String. There is no such thing as an "empty Char".
Someone make this guy an approved submitter, God damn it.
What do you mean? They are both input elements. The trouble starts only when you want to use text fields as output elements as well, i.e. set the text value programmatically.
I don't understand why a button emits `Event`s while a text field has an input `Behavior`.
What I was worried about is the right identity, mainly because of the existence of bottom: return _|_ &gt;&gt; return () = return () ... or using your version: fmap (const ()) (return _|_) = return () Although maybe the same objection might hold for `forall r . r -&gt; m r`. My thinking is not clear on this point. Anyway, other than that minor point I understand that the other laws check out just fine. Also, out of curiosity, why is this instance not in `Data.Monoid`?
Would `forall r . r -&gt; m r` still be a monad even if `m` didn't obey the associativity monad law? That could be a concern (I haven't read TFA yet, I admit...).
The debug tool I use the most is still Debug.Trace.trace and the following: probe msg val = trace (msg ++ show val) val and pipe a value I'm interested in through it.