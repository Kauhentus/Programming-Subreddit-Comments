I strongly disagree. As a novice Haskeller [reading LYAH](http://learnyouahaskell.com/higher-order-functions#composition) without knowing how to write Haskell at all I looked at (.) and just said, "oh, ok, function composition. cool."
I largely agree with the point you make about not swamping newcomers in superficial differences. I see this specific scenario a little differently though. Given the current meanings of `+~` and `+=`, if you are teaching lenses as one of the very first things in Haskell you probably won't be mentioning `+=` at all, as it will take a long while until you get to the `State` functor/monad. Therefore, the only syntactic distinction involved is that we use `=` in `let x = (2,4)` and `~` in `(_1 .~ 3) (2,4)`. That difference might be explained by saying that in the first case we are defining/naming something new, while in the second we are just making a modified version of something that already exists. (The third possibility, the one involving `.=`, `+=`etc., will only show up much later, as I suggested a few lines behind -- late enough that confusion will likely be much less of an issue.)
even as a visual thinker, I think I would prefer auto complete to a chart of jigsaws. still, I'd like to see it.
Are you thinking of the "`.` should be for field access" objection? If so, it is really important to distinguish newcomers to Haskell with programming experience from newcomers to programming as a whole. 
I'm going to question the wisdom of doing this too soon, and suggest that it might be too soon. Reasons: 1. Last time I looked, `record` only supports record types with up to 24 fields ([see the instances of the `Field` class](http://hackage.haskell.org/package/record-0.4.0.2/docs/Record.html)). That number needs to become much larger. 2. There's a bunch of ideas in `vinyl` that still merit exploration. See the "Records are polymorphic over functors" section of [this tutorial](http://www.jonmsterling.com/posts/2013-04-06-vinyl-modern-records-for-haskell.html) for one example.
Um, offtopic / just wondering. Why are you teaching Haskell to those people, and not some other language? It would seem that job prospects should be much more important to them – among all people – and that would make Python/Javascript a better choice? (What am I missing?)
Well, you can't be all things to all people...
there's http://hackage.haskell.org/package/interpolatedstring-perl6-1.0.0/docs/Text-InterpolatedString-Perl6.html
But he is cute.
Thank you. I assumed as much but it wasn't explicitly said in this article.
I did give it a chance, for years. I disliked it enough to switch to this frame of reference. I'm not going to change the way this works at this stage. Both camps have reasonable arguments, and switching causes transitional costs that far outweigh any benefits to be had even if I could be convinced that they existed, which I haven't.
Given that Haskell is a _functional_ language, it behooves us to make working with _functions_ as nice as possible. The existing practice around (.) fulfills that goal. I've used purescript, scala, f#, etc. where they have to use another operator. It rather drastically impacts the readability of the code when you just start to compose functions. You're welcome to disagree on that front, but regardless I'm rather thankful that this ship long ago sailed.
&gt; I think we should replace .~, +~, /~ by :=, +=, /=, etc.? The = suffix (vs. ~ suffix) is already used for modifications in an appropriate MonadState. I would not want to change that.
&gt; I completely agree with you here. Although instead of saying "first class accessors", I like the phrase "composable pointers". We could call them by their original name: ["functional references"](http://www.twanvl.nl/blog/haskell/overloading-functional-references).
I still say I wouldn't give up (.), because the very reason to learn Haskell is to start to spot those connections. If you start obscuring them you destroy the very reason we have this language in the first place. I'd love for everyone to want to write Haskell, but it isn't going to achieve popularity by just copying all the ideas everyone else has. It will succeed or fail on its own merits. To me the thing that I love about this language and this community is that I can consistently rely on them doing "the right thing" in terms of finding the right abstractions whenever those things matter. Haskell is the "shining beacon on the hill" that the other languages aspire to be like, because it takes a principled stance on things. This principled stance is what makes it so that other languages draw from Haskell. It is the very thing that provides our influence among other language communities and it is the stance that draws the sort of developer that continues to help Haskell evolve. If I was trying to teach folks from a disadvantaged background, like Amanda Laucher over at [Mined Minds](http://minedminds.github.io/index.html) as she works in Greene County, Pennsylvania helping communities that have reached 75% unemployment after the mines closed, I'd probably do what she does and teach another language. I'd teach something like F# that is connected to an ecosystem in which the audience is more likely to find immediate employment. I probably wouldn't leap directly into Haskell. Why? Because I'd be trying to maximize the chance that they could get useful employment in the short term. I've done this before with folks bootstrapping them through C# and with python and perl, and gotten old friends of mine who went into other careers early on in life to a point where they could successfully "fake it 'til they make it" as a programmer. Even if I were to leap with those students directly into Haskell I wouldn't leap directly into `lens`! `lens` is a great showcase of the power of functional abstractions, but it is an incredible source of frustration for newcomers. There are plenty of far more foundational issues to tackle first. All you can do when you are starting Haskell and encounter lens is "cargo cult" existing patterns. That isn't learning -- it is rote memorization. By the time those foundational understandings are addressed then comprehending (.) just isn't an issue, and then you can come full circle and discover how lens unifies (.) from function composition with (.) from field access by being excessively clever, but the key to ever getting there is understanding functions in the first place. Making (.) worse isn't the answer -- it just rules out more people from ever having that epiphany.
I don't understand why you think you speak for newcomers.
Author of the paper here ... It was originally done as my master thesis, I have tried to give an overview of previous proposals and come up with some reasonable syndicate. I have also modified GHC such that there was a working prototype implementation, which is something that never happened before (with exception of Conor McBrides SHE, that is a/ a preprocessor, and b/ severely limited. I have submitted it as a paper for the Haskell Symposium this year but the paper was not accepted. There are problems with e.g. multi-parameter type classes and to be honest I have no idea how it works with fancier GHC extensions, e.g. type families. Also, I have posted several mails to haskell-cafe mailing list in order to get peoples attention and find out whether they are interested but there seemed to be almost no reaction. TL;DR It is not better, it is supposed to be a reasonable compromise in between all the proposals we have seen so far, and there was a prototype implementation in GHC.
&gt; Couldn't that be disambiguated by types? The vast majority of Haskell allows type _inference_ rather than requiring type _checking_. Inference is the source of almost all of the good things that make Hindley-Milner style languages work. 99% of the time it 'just works' and avoids the `Foo foo = new Foo()` repetitive repetition problem. In the notation you mention here, that goes out the window. Unless you put a type in, (1,2,3) could be one of two very different things, meaning that map (+1) (1,2,3) would mean either (1,2,4) as a tuple or (2,3,4) as a list. A meaningful distinction, utility and readability are all lost. Nothing is gained.
&gt; Last time I looked, record only supports record types with up to 24 fields Hey, that's at least [2 more than Scala](http://www.scala-lang.org/api/current/index.html#scala.Product22). ;)
&gt;I've began to just say Haskell isn't ready yet [...] The underlying question is: ready for what? Haskell is [more ready for some tasks, less ready for others](http://www.haskellforall.com/2015/08/state-of-haskell-ecosystem-august-2015.html). Furthermore, and as we all know, it is ready for those who want to try something different and broaden their vision, having no functional programming experience. It is certainly *not* ready for those looking for a silver bullet -- but then again, which language is? &gt; [...] and they should focus in another language. Is that what we want, though? While I am enthusiastic about Haskell, I don't do proselytism. Therefore, I think that the answer to this question should depend on the reasons behind the rejection. Other points that I might have made here are already covered by [Edward's reply in the parallel subthread](https://www.reddit.com/r/haskell/comments/3k9oiq/why_dont_data_just_create_lenses/cuw2v2j).
This has the same issue, fmap now maps over the last element of the tuple which is an empty list or over the entire list.
ghc-reskin helps with syntax experimentation, but it's not the same as what I'm suggesting. If the canonical source were an AST, then it could be serialized to source code for editing as the user wishes. So, for example, someone who *doesn't* like ArgumentBlock would see trailing `do` blocks enclosed in in parenthesis (or with a $ tacked in between them, or whatever), whereas the user who does like it would have it displayed as per that extension.
Is that not somewhat similar to core?
`(&lt;|)` is analogous to `($)` not `(.)`. There is a big difference between application and composition. Look, it is clear that at this point in time, neither one of us is getting through to the other. I'm going to to away now, there are plenty of other people here to continue this discussion without me, and I can have the pleasure of my evening back. You are welcome to follow up on each of the myriad topics in this thread and have the last word if you feel the need. Good day.
Not quite. Core is what happens after a number of compiler passes have occurred. The desugaring passes could be ignored as, well, sugar; but other passes like strictness analysis, lambda lifting, etc are surely things we want to be able to ignore in our user-provided ASTs.
Suggested, then? But regardless, I think this is just a simple example of that law-of-programming-languages (google failed me) that discussions about syntax get a lot more responses than pretty much anything else. Especially if you keep suggesting new different syntaxes in different sub-threads. Don't take it too personally. I agree that data declarations should probably create lenses, probably with the IsLabel mechanism that appears to be coming with the OverloadedRecordFields. And I suspect this will be implemented, if not in GHC 8.0 then with a little TH now and a pragma in a later release. Also (&lt;|) is usually ($), not (.)
do you have a link to where they made that point? Sounds interesting.
&gt; When you once again ignore the whole answer for a nitpicked mistake that isn't relevant to the main point, it is really hard to get through you. Why haven't you said a single word about the subject of the thread in this whole debate, for example? It is not the first time I post a thread only for you to completely dismiss it for an orthogonal point, and since it is you, that becomes the new subject of the thread. Awful. [I replied to another person in this thread 4 hours ago](https://www.reddit.com/r/haskell/comments/3k9oiq/why_dont_data_just_create_lenses/cuvvyau) about the fact that we're getting overloaded record fields as part of Adam Gundry's work with SPJ as a GSoC student. As a result you'll be able to use a combinator to convert any field accessor to a lens. Is it everything I would have wanted if I had carte blanche to redesign the language as I see fit? No. But it is something that enables everyone to use lenses for field access even for data types that weren't designed with lens in mind. It remains to be seen if this is actually a net-good idea. You can't hide such instances from export lists, for instance, unlike field accessors. Had you or anyone else carried forward on that particular thread I'd have had plenty more to say on the topic, but it went largely unremarked and I went about addressing "other orthogonal points" that were being discussed in various sub-threads. As for the rest, you're filling in words about having suggesting `foldr apply` or making other concrete claims that simply aren't substantiated as they never appeared anywhere in this thread except in your head, so I'm left with nothing I am able to reply sensibly to! I can only reply to what was actually said, not the thing you intended to say but didn't say outright. Hence my earlier attempt to withdraw from this discussion, which I'm again going to attempt once more. Once more, good day.
https://www.youtube.com/watch?v=ad4BVmPni7A&amp;feature=youtu.be is one of his more recent well-articulated rants on the subject.
This is why I prefer phrasing things in terms of throughput, rather than "speedup/slowdown". One big part of the problem here is the difference between multiplicative and additive units. Acceleration (i.e. changes in velocity) is fundamentally multiplicative, whereas the terms "speedup/slowdown" are typically used in an additive manner. Another way to phrase the problem is the distinction between "percent" (multiplicative) and "percentage points" (additive), which almost noone succeeds in properly distinguishing. All this is, as /u/yitz says, a universal problem of language— not just natural language, but mathematical languages as well: which is why there's such a clamor for units-based type systems.
Keeping track of the default instances in a separate second-class bin means that should someone introduce a conflicting instance via, say, an orphan instance, you'll silently start violating coherence of instance resolution. Worse the example of `Const` runs afoul of the diamond already. `Const` is both `Applicative` and `Traversable`, which one defines your `Functor`? They tie under the rules given.
&gt;There is a valid functor instance for list that takes the first element of a list and applies f to it repeatedly. That satisfies the laws. I'm not sure which implementation you have in mind, but if it applies `f` to an element repeatedly then `f` is `a -&gt; a`, and not `a -&gt; b`, and therefore it can't be `fmap`. More generally, [all lawful implementations of `fmap` for a type produce the same results](http://article.gmane.org/gmane.comp.lang.haskell.libraries/15384) (modulo non-termination shenanigans of the sort covered by ["Fast and Loose Reasoning is Morally Correct"](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html)).
Here are my opinions: - Pure and MonadState lens operators naming could be more pedagogical if swapped around, as the former can be exposed first to novices (i.e., without monad knowledge), and would be easier to grasp due to analogy with imperative operators (`+=` instead of `+~` and so on). The tilde can be seen as an "impure setter" in that matter. - `,` makes for an intuitive cons operator since it is often used for lists in real life. That way, lists become `(1, 2, 3, [])`, and the sugared syntax could be pedagogically explained as inserting the numbers on the brackets: `(1, 2, 3, []) → (1, 2, [3]) → (1, [2,3]) → ([1,2,3]) → [1,2,3]`. - Since `:` would be free, it could be defined as `(:) = foldr id`. It would allow for things like: `object: [position.x += 3, position.y += 4]` which would read as "with object, add 3 to position.x and 4 to position.y". - `(^.)` for field access is suboptimal. I wondered if using `(.)` (and having another term for composition) could be more intuitive, so `foo^.x` becomes identical to the `foo.x` programmers are used, but losing `(.)` for composition is a big cost. I still wonder if `^.` specifically is ideal, though. ~ Now, a few disclaimers: 1. I am not proposing this change to actually be applied to Haskell since backwards compatibility would redeem it useless - I'm putting the thought in the air for it could be useful when creating new languages and DSLs. 2. I don't think debating that subject is important as it can't be changed anyway. 3. From the other thread, I understand the majority disagree with my opinions. Yet, since I split the old thread in order to promote debate about the [original subject](https://www.reddit.com/r/haskell/comments/3kbl80/should_the_data_syntax_create_lenses/), I decided to repost this one anyway as we know very well destroyed information is never good. 
That undersells `Arrow`. While applicative functors can be expressed as arrows (that is the [static arrow](https://hackage.haskell.org/package/semigroupoids-5.0.0.2/docs/Data-Semigroupoid-Static.html) presentation), and you can get an applicative functor out of an arrow, [you lose power in the latter transition](http://stackoverflow.com/a/24668518/2751851), and so they are not equivalent.
To be clear, do you mean different Lens naming conventions, or for all of Haskell? Something about your other thread and answer suggest you're talking about Lens.
&gt; It’s perfectly fine and even encouraged to arm-wave about folds or unfolds when speaking informally, but the moment someone distinguishes one particular style of fold from another via a prefix like i.e. para, I know exactly the relevant technical distinctions required to understand the discussion. So... I feel a little bad for this, but the context of the paragraph makes it pretty much mandatory. You meant '*e.g.*'
Note that `($)` is not syntax; it's an actual infix operator defined within the language, so reskins could not freely add or remove it from the syntax tree without doing some sort of static code evaluation.
Yes. Also prisms (for sum types) and isomorphisms (for newtypes)
Yeah, I realize that. The fundamental idea of just rendering or parsing "source code" based on an AST only gets us so far (though I'd argue it gets us to pretty interesting places!). On top of that, it'd also be possible to do "pattern matching" on AST to render to code that would technically have a different AST, as long as you can take it back. I don't know how feasible that is when used very broadly, but I'd love to see where people take it :)
Any idea if that will work with prisms? None of the discussion I've seen has acknowledged them, and if you use the `{ field :: Type }` syntax in a sum type that doesn't have `field` in every case currently you get partial accessors.
"Pegagogical" /= "Familiar" If you're familiar with imperative idioms, trying to recreate them in Haskell is just going to cause more confusion than necessary. Embrace the differences, if you're writing C then write idiomatic C. If you're writing Haskell then write idiomatic Haskell otherwise you'll just be fighting with the language to try and recreate a style of programming that you're used to.
I'm not sure I follow. Generating them doesn't necessarily imply exporting them from your module, right?
The problem is the way they get generated in GHC as part of the records proposal has to do with adding an instance based on the symbol for the name. You can access that instance even if I don't export the name.
Judging from the amount of pragmas in libraries, "standard Haskell" isn't actually used that much. Seems like they don't want to deal with the consequences of their approach (either by enforcing their guarantee or by stopping to make misleading claims), because they know that there are substantial unsolved issues with both... 
The time for what? To call this princess back? Nah man save yourself the trouble. In my experience people who laugh you right in the face and then leave on their high horse will never yield a satisfactery ROI. Total waste of time.
So i'm no expert but the basic gist of things is that they now use basic dpi to detect vpns and other encrypted traffic and progressively back off things like mss so that the link becomes useless within minutes. So what they have to do is mask the traffic to appear to be "normal" traffic over say https. But not only must it appear to be https but it has to behave like https traffic would. Long story short, arms race.
After building executable, stack "installs" them to a path inside the project directory, and that directory is added to the PATH when running test suites.
https://drive.google.com/file/d/0ByK3AAy5ubqaMEJtcVlBbWd4LVU/view has the slides, but they are only really used for the first 10 minutes or so. After that it is live coded, and most of that should be clear in the video.
Won't this incur an overhead? Or are the required optimization/fusions very easy?
Really not a homework? ;) Why would you expect the code to behave differently? There doesn't seem to be any comparison for equality that should result in removing the duplicate element. What happens to *swap* when it is called with (n == m) ? And what happens to swap, when there are duplicate *n* or *m* values?
The only text in Lamdu is the presentation layer rendering the ast you edit to widgets that contain text on screen.
Mixfix in Agda comes with its own quirks. For example, operator use must be spaced. 1+2 is a name whereas 1 + 2 is an operator invocation. Perhaps it can be less weird.
Lamdu already does this, so you could steal that part : http://i.imgur.com/oHBLE4H.png
Could you give us an example please?
So much the better.
You're treating lists as if they were vectors; they're not, they're simple linked lists, and most operations (including `length,` `(!!)` and most uses of `(++)`) involve traversing the whole thing, or at least everything up to the desired element. This isn't going to be noticeable with a six-element heap, but it'll grow fast! There are vector and even mutable-array packages available where you could get good performance out of this sort of technique, but for fun, here's somewhat a more idiomatic take on it. (Disclaimer: I'm no expert, and I'm sure this is still a pretty naive implementation; it's more about the approach than the specific solution.) {-# LANGUAGE DeriveFoldable #-} -- Just to make things fancy and generic data Tree a -- A heap is just a tree with extra rules. Embrace that! = Leaf | Tree a (Tree a) (Tree a) deriving (Show, Foldable) push :: (Ord a) =&gt; a -&gt; Tree a -&gt; Tree a -- Top of left side is kept larger than top of right. Makes pruning easier. push value Leaf = Tree value Leaf Leaf push value (Tree top left right) | value &gt; top = Tree value (push top right) left | ((value &lt;) &lt;$&gt; peek right) == Just True = -- Hopefully the applicative style isn't confusing there. Tree top left (push value right) | ((value &lt;) &lt;$&gt; peek left) == Just True = Tree top (push value left) right | otherwise = Tree top (push value right) left -- When bigger than left, push to right then swap. Helps balance the tree. prune :: (Ord a) =&gt; Tree a -&gt; Tree a prune (Tree _ left'@(Tree top _ _) right) = let left = prune left' in -- I'm sure this could be more compact but I should be in bed. :P case left of Tree topL _ _ -&gt; case right of Tree topR _ _ -&gt; if topR &gt; topL then Tree top right left else Tree top left right _ -&gt; Tree top left Leaf _ -&gt; Tree top right Leaf prune _ = Leaf peek :: Tree a -&gt; Maybe a peek (Tree x _ _) = Just x peek _ = Nothing -- Leaf union :: (Ord a, Foldable f) =&gt; Tree a -&gt; f a -&gt; Tree a union = foldr push makeHeap :: (Ord a, Foldable f) =&gt; f a -&gt; Tree a makeHeap = union Leaf toList :: Tree a -&gt; [a] toList (Tree top left right) = top : toList left ++ toList right -- Here, (++)'s cost is absorbed by the existing traversal. toList _ = [] -- Leaf toSortedList :: (Ord a) =&gt; Tree a -&gt; [a] toSortedList heap = case peek heap of Just x -&gt; x : toSortedList (prune heap) Nothing -&gt; [] main :: IO () main = do let x = makeHeap [1::Int,8,7,6,2] print $ toList x putStrLn " -- push 8" let y = push 8 x print $ toList y putStrLn " -- union [10, (-2)]" let z = union y $ makeHeap [10, -2] print $ toList z putStrLn " -- Tree format" print z putStrLn " -- toSortedList" print $ toSortedList z Output: [8,7,2,6,1] -- push 8 [8,8,6,1,7,2] -- union [10, (-2)] [10,8,7,2,-2,8,6,1] -- Tree format Tree 10 (Tree 8 (Tree 7 Leaf Leaf) (Tree 2 (Tree (-2) Leaf Leaf) Leaf)) (Tree 8 (Tree 6 Leaf Leaf) (Tree 1 Leaf Leaf)) -- toSortedList [10,8,8,7,6,2,1,-2]
I don’t really know yet, I guess we’ll need GLSL semantics, but with functional combinators as well.
If this can help you, I downloaded the videos and set the stereo mode to left in vlc.
I think your diagnosis is wrong, /u/Mob_Of_One explained what's really happening - it's just Henning's style. This style is also seen in http://hackage.haskell.org/package/non-empty, and other packages.
Fixed, thanks.
No need to download them, VLC can open youtube URLs directly.
&gt; And it's somewhat telling to me that out of all lens implementations we have in Haskell, the lens library is the only one with the prism construction. I admit I kinda wish it was acknowledged more openly that we all minified-lens-library-authors have more -reasons for not including prisms- than, y'know, just secretly hating them or not understanding how important invaluable etc etc they are. (I wonder whether it might finally catch on if I start calling it “Dreaded Profunctors Dependency”.) &gt; I personally don't have a plan for [merging profunctors into base] at this point [...] and we can talk more about this [after?]. If there was any followup to that, I'd love to hear about it!
I would say most people don't benefit enough from -the opportunity to make adjustments- to justify getting rid of a convenient habit (which is hard work). In fact, I guess I wouldn't be doing it either if not for some bugs Youtube has in fullscreen mode (okay, it's not exactly Youtube, it's more a combination of fullscreen mode + buggy Tree Style Tab, but whatever).
&gt; then the warning and path towards the superclass will start to happen in the next release Why the delay?
This is a strawman question. It builds in the assumption that if different naming conventions were more pedagogical to newcomers and backwards compatibility were not an issue, then we should make those changes. I disagree with that assumption. Newbie friendliness is definitely an issue that we need to take seriously. But it is not the only issue. The principal goals are for our language to be precise, expressive, readable, concise, and powerful. And yes - also newbie friendly and backwards compatible. There is always tension between all of these goals, so we have to make reasonable compromises.
Simply to not format with spaces. That way it's feasible to use a proportional typeface with higher legibility instead of perpetuating teletype legacy. The downside is that you can't do typewriter-like formatting to arrange code in a tabular style with alignment beyond indentation. /u/radix' suggestion allows editors to solve the problem by offering a configurable layout, just like word processors have layouts for bullet lists, tables, etc. instead of relying on the space bar.
why did you delete [this thread](https://www.reddit.com/r/haskell/comments/3k9oiq/why_dont_data_just_create_lenses/) and all it's comments?
I believe that provided things inline correctly, the resulting core *can* come out the same. How often it does that, I don't know.
That would not work. Code is displayed in a large number of different contexts, from version control over HTML generated by code review or documentation tools to linters, compiler error messages,... You would need your alternate formatting mechanism in all of them (with consistent implementations) for your idea to work. The IDE is just a small piece of the puzzle here and quite frankly, this very much sounds like a "cure worse than the disease" kind of situation.
That's interesting. For me there's some sort of adjustment I want to do in almost every video I watch. Either it's desynced audio/video, or brightness/contrast correction, or pan-scan to remove hard-coded letterboxing, or changing the playback speed, or convenient seeking, or stepping frame-by-frame, or one of a hundred other things heh. Not to mention that a video player like mpv is for some reason far less resource-hungry than the player in the web browser, which means less fan noise and a generally more happy kqr.
Interesting analysis and definitely far deeper than I've went.
Am I right in thinking `pure == return` now we have the AMP? Could we start to deprecate `return`?
&gt; I think : and :: should be flipped. I often wish for `:` in type declarations, but I wouldn't wish for `::` in list pattern matching. Though I guess I don't write a ton of code that pattern matches lists, it would certainly clutter up our two-line examples!
&gt; it would certainly clutter up our two-line examples! By adding a single character (or two?) to only one of the lines? I hope others have a little more tolerance for clutter. ;)
Would apply in a heartbeat if I could work in Singapore.
I realise that it's difficult to imagine elements of computing to be different than they are. There's the feeling that everything is set in stone and basics like "source code is plain text that assumes a teletype interface like in the 60s" exist because they are the best possible solution, rather than legacy from primitive technology. Computing is a young field, and I think there's plenty of room to rethink some basic assumptions. Check out https://pchiusano.github.io/unison/ and http://unisonweb.org/ for some interesting thoughts on not only code representation, but also compilation and version control.
It's a sad life up here for Haskell developers :(
I think the main things are to get rid of `String`, rename `Text` to `String` perhaps, and flip `:` and `::`.
Oh, how would I do this with mpv? (I used ffmpeg to fix it in-file. tooke me 15 minutes to find the right incantation)
Would apply if I were confident enough to write code for hundreds of users which is deployed within hours. 
The problem with tabs is that they are only ever any good for indentation, a.k.a. the easy part of the problem. For alignment you still need a different system, either spaces and a monospace font or something else that is more complex. Mixing tabs and spaces can be quite confusing to those using editors without visible whitespace so in any project with multiple team members spaces are simply the easy way to solve the problem once and for all.
Or US.
Hmm, weren't there a few in the Bay area posted recently? @Wagon for example.
&gt; impoverishment We are poor? I might be misunderstanding here.
I don't mean to offend, but if you intend to resubmit the paper, it might be worth getting the paper edited by a native or highly fluent English speaker -- a lot of the text reads very awkwardly, at least to me. I found it very hard to extract the information I wanted to know from the paper.
That's why we have types. 
You should take a look at "Polymorphic Blocks: Formalism-Inspired UI for Structured Connectors" - http://cseweb.ucsd.edu/~lerner/papers/polymorphic-blocks-chi15.pdf https://www.youtube.com/watch?v=eoilYq-jtBM
How about [`Data.Functor.Sum`](http://hackage.haskell.org/package/transformers-0.4.3.0/docs/Data-Functor-Sum.html)? The sum of two `Applicative`s is not in general `Applicative` itself because `InL f &lt;*&gt; InR x` isn't necessarily meaningful. Of course, specific combinations might have viable instances. As an example of one that probably doesn't, I don't see any sensible way to make the sum of two different fixed-length sequences `Applicative`.
That really didn't clear things up for me.
Oh no guy, you are sooo rudee .. There are more than setters and getters. There are prisms and octaedrons and all these vegetarian stuff so that we can do these beatiful expression that we can stare at for hours without doing anything useful. Aha. Now I understand. thanks.
Last time this was attempted there was an attempt to split `Profunctor` in twain, which would have cost it `dimap`, meaning two traversals and rather large slowdowns in code that is polymorphic in the profunctor when applied to any sort of recursive profunctor such as a naively encoded Mealy or Moore machine. `Profunctor` currently contains `(#.)` and `(.#)` because they are sufficient to the needs of `lens`, but they aren't the 'right' solution for mapping coercions. They don't compose. This is less of a problem for profunctors than other things, but consider if the profunctor is something like Kleisli, now to map (b -&gt; c) over the 'b' side of (a -&gt; m b) we need to coerce m b to m c, but we have no polymorphic way to do this, hence we have to map the coercion. The naive solution would be to add `fmapCoerce` to `Functor`, but that turns out to be insufficient to handle things like `Compose`! We can't even rely on representational arguments because of libraries that are 'morally' functors but contain separate constructors to manage the change rather than using Yoneda. Properly fixing the ability to map with coercions at zero cost requires the ability to lift a Coercion from a to b to possibly a Coercion from (f a) to (f b) in Functor then implementing fmapCoerce in terms of that, trying to lift the coercion to coerce directly in O(1) with perfect sharing and then falling back on `fmap coerce` if that fails. Then the `Profunctor` definition and `Contravariant` definitions can all change to suit. So, to get `Profunctor` into base in a form suitable for long term standardization without randomly causing everyone unacceptable performance regressions requires a whole bunch of changes that are not easily sold. And that ideally would require a small compiler tweak to look at the role and change the default definition of liftCoercion based on information about the role so that this 'just works' out of the box for almost all user code. This was the digression I didn't want to get into in the middle of the talk.
I wonder why there aren't more companies trying to hire Haskellers to code in Java :-)
It is really hard (at least when I was learning) to explain to beginners what a Monad is, it is way too abstract. Just take it as a magic word, and learn how to use it and don't think about how it works at first. After you know several different libs that expose Monadic interfaces and you will get the sense, though may be not precise. examples: `IO`, `ST s`, `State`, `Parser` (search "parsec" in hackage), `[]`, `Maybe`, `Either` &gt; what about when you have a function inside a container and you want to apply that to another container? I guess you probably want [Monad Transformers](https://en.wikibooks.org/wiki/Haskell/Monad_transformers). Read the following if you don't mind explanation with thoughts of imperative programming. For example, if `Foo` is a Monad that describe something stateful, then `Foo a` means a procedure that given an object of `FooStates`, it modifies its state and yields a return value `a`. A monad transformer, e.g. `FooT m a`, can be roughly understood as something that can be transformed (`lift`-ed) from a Monad that is at least as powerful as `Foo a`. (Note: to avoid misleading, `Foo` is the Monad, while `Foo a` is not) More powerful, could be roughly taken as larger-scoped or lower-leveled side-effects. You can think that within `IO`, you have the highest privilege, so `IO` is the most "powerful". So, for example, `FooT IO a` could be roughly read as "I will doing something within a `Foo` if I get the privilege level of IO". And when you see `Monad m =&gt; FooT m a`, it means that `m` can be anything. So this means it doesn't needs extra privilege. So a `FooT m _` can fit a `FooT IO _` context. 
Oh my, if this is really going to ~600 pages I'm so thrilled. What kept me away from Idris was not enough documentation and not enough examples, so a full-sized book will totally get me back.
This is pretty standard practice (for better or for worse) in OCaml I think—maybe that's where the convention comes from?
For programming languages, I believe you're looking for [Wadler's Law](http://lambda-the-ultimate.org/node/4287). But really, it's a special case of the usual bikeshed problem.
I've seen people advertise positions calling or both Haskell and PHP experience to get people to try to get talented people to write PHP. It happens. ;)
I see the negative votes, but I don't see the counter arguments. Not even non-arguments or contrary opinions. *This makes me think that I'm deadly right, unfortunately.*
Thanks a lot! For anyone else who wants to try this, it has to be mpv -af pan=1:1 (Note the equal)
And the usual 50% discount from Manning. :-D https://twitter.com/ManningBooks/status/642009196020502528
None of those people have abandoned the community.
with perhaps the exception of Simon Marlow, who still publish very interesting software and books of real world programming, yes.
That is probably because your provocation is backed only by hearsay and unsupported assertions. "Argument from hype" might be a fitting description. There is not much to counter here, though a [citation needed] tag would be very appropriate.
I still maintain that you should be able to import and export definitions from any `where` clause, not just the ones attached to modules! (Disclaimer: I haven't really worked through the implementation headaches this would entail because I'm pretty sure there are too many to bother with.)
getting downvoted without comments may well mean other things, but I will accept your "suggestion" since I have no other option, dear overlord.
Something like block/lexically scoped `open Module` could be nice. Like you say, I'm sure headaches are involved in a hypothetical implementation.
I just started messing around with Idris. It was refreshingly easy to define HLists and simpler operations on HLists compared to Scala. It's a really refreshing and invigorating language. I'm definitely going to check this out.
Dependent. Haskell. 
I will try to reeducate myself. thanks again
Yesss please merge this two wonderful technologies together :)
That happened at this place I used to work (but Ruby instead of PHP)
I'm a bit worried picking up a book for a language that is pre-1.0. Should I not be?
Indeed we are! (skedge.me)
I see, sounds good! thanks!
&gt; I used two seperate graphs to match the patterns of map. Yes, this is the right choice from a pure dataflow POV. But if you want to represent both dataflow and control flow in a single diagram, it turns out that these are dual to each other and follow quite different graphical rules. So each needs its separate 'regions', with well-defined interfaces to embed one in the other. (Pattern matching embeds a control flow 'region' as part of dataflow, whereas fork-join can be understood as embedding dataflow in control flow).
The book is estimated for Summer 2016. Does this mean Idris 1.0 is also estimated for Summer 2016? ;)
I don't think it should be impossible, even if you do use the extension. You could do something like: data InternalFoo = InternalFoo { x :: Int , y :: Int } newtype Foo = Foo InternalFoo and then even if the `x` and `y` accessors are available if all your exported functions only operate on the Foo type users can't ever get an InternalFoo to use the accessors. Not that that is ideal.
It would be possible to export, but the thing you'd be importing it from would be `function` applied to two arguments, not just `function` alone. Conceptually, I imagine this wouldn't be too different from some of the things you can do with ML modules. Practically I have no idea how you'd retrofit it onto Haskell and wouldn't seriously propose trying. (As for inner functions, it would of course be possible to not export definitions just like with modules.)
woah! apps need more discoverability. One thing I like about intelligent is (by default), it shows you a "tip of the day"
Actually, in my case both pieces of code were written simultaneously (switching back and forth, in fact, to test that one could read what the other wrote). And really, if the Haskell portion was better in any high-level sense it's only because my Python is a bit rusty. It's more that it felt a lot easier using Haskell and `lens` to just say what I meant and then tinker with the details, rather than jumping through hoops just to get the basics in place with Python. If I were less out of practice in Python I'm sure it would have come far more naturally, but these days it was kinda painful. The point is, I found using Haskell with `lens` to be more clear and intuitive because that's what I'm *currently used to* and know how to skim the lens documentation without getting baffled by meter-long type signatures. Whereas on the Python side I kept grinding my mental gears trying to translate strings of function composition and `traverse` and whatnot--you know, the natural and intuitive way to think about things!--into a language that thinks making you write out the entire word L-A-M-B-D-A is somehow reasonable.
The first chapter is excellent!
&gt; If we're too religious about adhering to existing idioms, we'll never improve them. And we all saw how religiously some language communities adhere to them with the decade lost after the Design Patterns book was first published.
I space out my operators in Haskell anyway. except when composing lens to mimic OO accessors, though I might even then.
I got it -- the first two chapters are great! I'm very excited to see how it unfolds.
Well, no, because it can't be a `Monad` let alone a `MonadPlus`. However, it is very close to an `Alternative` which is the `Applicative` version of `MonadPlus`. `(&lt;|&gt;) :: f a -&gt; f a -&gt; f a ` is morally [`unionAll`](http://hackage.haskell.org/package/opaleye-0.4.1.0/docs/Opaleye-Binary.html) but regrettably for technical reasons Opaleye needs a class constraint.
&gt; I definitely felt like a grubby "engineer" who was crude for espousing simplicity, compromise, practicality, over direct codification of high theory. Wow, this is totally the opposite of the way I see it. From my point of view lenses are simple and practical, and indeed did not arise from "theory" at all. In fact there is no particularly well-understood "theory" of lenses yet.
&gt; With a different set of compromises, you get something like microlens: an implementation not as general or as powerful, but a lot less scary. Hey, hey, hey, microlens has its own [micro-Bazaar](http://hackage.haskell.org/package/microlens-0.3.4.1/docs/src/Lens-Micro.html#line-586) too, how dare you say it's not scary! And there's also [`traversedStrictTree`](http://hackage.haskell.org/package/microlens-ghc-0.3.0.0/docs/src/Lens-Micro-GHC.html#line-247) which it took me like 2h to properly rip out of lens (along with all other stuff there). &gt; lens is uncompromising in making its types and combinators as general and widely applicable as possible And also as fast as possible, which is where some of the overengineering comes from. All in all, while it's not true that lens is overengineered *for its goals*, it is probably true that for lots of people those goals aren't immediately useful, and for *their* goals lens is overengineered indeed (but then they are recommended lens anyway when they just need to update a record, 'cause “c'mon lenses are awesome”).
I'm not really familiar with LINQ, but `Arrow` notation is already essentially `Applicative` `do` but with a weird `-&lt; ()` on the right hand side of expressions so not an awful lot will change. I should also say that if LINQ is a monad it is probably by virtue of being able to run multiple queries. By contrast an Opaleye `Query` is guaranteed to issue only one request to the database.
Famously a Real Programmer can write Fortran in any language. Apparently the same holds true of ML.
There is something fishy in section 5.2.1: default instance Fmap a where Shouldn't that be Functor?
P.S. while not related in an administrative sense, as the current sole moderator of /r/haskellgamedev, I love these guys. :)
A different thing that relates lens, isos and prisms is quite useful: You can use an iso as either a lens or a prism, making it easy to construct with or match on.
Thank you for the link! I thought I had seen a completed proof of my claim above somewhere, but I guess I thought wrong.
We have to depend on it just to get `Profunctor` and `Choice` – and it'd be alright if profunctors didn't also incur the following dependencies that you could perfectly avoid otherwise: tagged, distributive, comonad, semigroups, nats, contravariant, StateVar.
Also, even if you fix haddocks to handle this gracefully, you still have the `.C` and `.T` noise at the end of everything. We should strive to make things more readable to outsiders, not less.
Though personally I wish haddock *would* handle this more gracefully because that means it would also obviate things like "better check where this hyperlink goes so I know which `ByteString` *this* function is using".
Check out the [MIT Scratch](https://scratch.mit.edu/) project, or the [Stencyl](http://www.stencyl.com/) game engine.
&gt; Now make jigsaw puzzles patterns represent types when joining boxes together and add some functional programming related stuff. [Like this?](http://static.stencyl.com/v3/images/tour/dm.png)
Most of those packages are pretty tiny. Are you just trying to code-golf your dependency list, or is there some other reason to avoid depending on several tiny packages? Would you have preferred it if you just depended on category-extras?
For some reason, I always assumed that this well-known paper was simply introducing the list monad. That'll teach me to skimp on the classics!
One of these days, the stars will align...
While you are on this topic you may find https://www.fpcomplete.com/user/edwardk/heap-of-successes interesting.
They are tiny, but there's 8 of them (counting profunctors itself). Installing them takes 40s. There are also text and unordered-containers, which take 70s. &gt; Would you have preferred it if you just depended on category-extras? Nah. It'd be even worse as packages wouldn't be able to build in parallel. --- Now, responses to obvious objections: &gt; text and unordered-containers are in Haskell Platform, so they'll never actually be installed * I don't use Haskell Platform * nobody I personally know uses Haskell Platform (I think) * Haskell Platform isn't even available in my Linux distro's repositories * Haskell Platform has stopped being a must for Windows users (since MinGHC exists now) If it turns out now that I'm mistaken and everyone actually uses Haskell Platform, sure, I'll either include profunctors or start telling people to use lens. (Where “everyone” must include the majority of Linux and OS X users, of course; otherwise this argument could be used to justify creating a library that wouldn't be usable by OS X users *at all* and then claiming that it's still usable by the majority of users, which would be absolutely true but it won't change much.) &gt; nobody should care about text and unordered-containers, because any serious app depends on them anyway microlens isn't for “apps”. It's for libraries, and for killing the notion of Haskell having bad building times, and for small projects when you just had an idea and you want to immediately start coding and the thought -of spending 2m waiting for the sandbox to get ready- works approximately as well as a cold shower would. &gt; c'mon, 40s isn't much For a 50-line library that wants to depend on a lens implementation it seems much. (Just in case: “depend on a lens implementation” means “use lenses in code”, not “export lenses”. I know that exporting lenses can be done without depending on a lens library.) Now, there's an interesting question: is it *actually* much, or is it just some stupid feeling that some people have even tho it's not really justified? Kinda like “oh god I hate brushing teeth” despite the fact that brushing teeth takes only 2m and much more time gets wasted elsewhere – and so “objectively speaking” people *shouldn't* have such an irrational dislike? My answer is that a) I don't know, and b) it doesn't really matter. Even if I can recognise that my desire for tiny libraries is silly, and even if I manage to get rid of it, I still won't be able to eradicate it in others. So, it boils down to this: given the existence of irrational people who feel bad about depending on a library that triples their building time, and who would rather not use lenses at all than depend on profunctors, should we satisfy their preferences or not? There isn't an obvious answer to this question in general, because usually satisfying someone's preferences means harming someone else in some way. If microlens and lens happened to be rivals for some reason, I would kill microlens. However, in this particular case I don't see any tradeoffs apart from perhaps making it slightly harder for people to choose a lens library to their liking, but hey, I didn't write [this section of the microlens readme](https://github.com/aelve/microlens/#competitors) for no reason. tl;dr I'm just trying to code-golf my dependency list.
I'm pretty sure you're looking for [this](http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454).
And the Old Ones will rise from their nightmare cities to rule the world again! *Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn*
&gt; Well, no, because it can't be a Monad let alone a MonadPlus. Well, Opaleye can't be a monad because you don't want a monad-based database library.
As the author of the '50 or so amazon libraries' mentioned as reverse-deps at the start of the talk - being able to shave off lens (or conduit but-lets-not-go-there) build times would help keep the last threads of my sanity together. I'd ultimately like to just provide lens compatible signatures and let people decide on what lens library to pull in, so the art of library dependency-golf is highly valuable, it's just unfortunate(?) that Prisms are so useful in this particular case.
I think it would be great if we could have tooltip when we hover over type in haddock docs giving us detailed info of origin and definition of that type.
I wasn't joking, but of course types don't eliminate all bugs. 
This is true for various other idioms in Haskell. Just the other day I saw Haskell code incorrectly using asum on [IO (Maybe a)] expecting it to operate on the Maybes. You could use *&gt; between functions not fully applied enough and it'll type check despite being incorrect. With explicit case matches this bug may be less likely (though you could just as easily encode this bug using fmap by accident). But the code grows by a factor of 2-4 so the general idea may easily be lost causing many other kinds of bugs. Prisms as traversals like the code you posted are obviously not meant to be exhaustive. Sometimes non exhaustiveness is useful.
Yeah, I understand Review, I just haven't happened upon its usefulness much yet (a handful of times, I think). I guess for prisms that don't represent readily available constructors or very simple compositions of constructors (Json) it could come very useful.
One of the problem is that we don't yet have a mature Haskell idiom. (And Haskell seems to be the only usable static typed purely functional language, and you cannot simply borrow from Ocaml, Erlang, Clojure ones.) 
Are you saying the 1.0 language will be consistent?
&gt; ... until I stopped thinking in terms of imperative/OO idioms and really learned the "functional way." I'm quite curious about what "OO idioms" really means here. Does it mean the way to divide-and-conquer by break down bigger things into smaller ones and encapsulate by expose methods, or is it about the inheritance? If it is the about former case, I would appreciate it that you'd share about how you did it. 
laughed my tears off.
&gt; This is trivial. You just add both parametric and non-parametric quantification to the language. No, this is not "trivial". The interaction between non-parametricity, consistency and termination are very subtle and would require very careful checks. We are talking man-years of work on highly subtle type systems, here.
&gt; True...but very few people care about the ordering with Set since it is only an implementation detail. This is not merely an implementation detail, many `Data.Set` functions depend on the ordering: `lookupGT`, `split`, `findMin`, `elemAt` and `toAscList` for example.
2% of Haskellers can't tell up from down.
Okay, I didn't think of the functions that break the set abstraction.
psst, that was the author of the book and the language giving you his assurance. But to add more to my comment. I've already bought the book and read what's written so far, I believe it will be one of those books that even if idris were to disappear you would have been enlightened by the topics presented in the book if you don't know much about Dependent types etc. And since you're here in haskell everything you learn from this book will probably just be more food for thought :) Also with that discount code 50% is too tough to pass up on a book you will probably want anyway in a few months :)
Thanks. I'm not as high on Haskell as I used to be (still lurking around, obviously), but I've always enjoyed your writing. That is probably the best intro to lenses for the faint of heart--like myself, lol.
&gt; man-years of work Only a few years? Trivial :)
The alternative is that I can write a bunch of code that I can't even personally use together. I refuse to use orphan instances, so this is my only viable solution.
&gt; tagged, distributive, comonad, semigroups, nats, contravariant, StateVar. * `tagged` is basically 1 module + a module of TH support * `distributive` is basically 1 module + a module of TH support * `semigroups` is going into `base`, and consists of 2 modules. * `nats` already _has_ gone into base and so has no modules. * `StateVar` is 1 module, and I couldn't get Sven to invert the `contravariant` &lt;-&gt; `StateVar` dependency, so I'm stuck with it if I want to be able to have packages like `sdl2` avoid an entire `OpenGL` dependency. So of these `comonad` basically is the _only one_ with any real computational content! Those 8 packages consist of 8 modules + `comonad`. I've opened https://github.com/ekmett/comonad/issues/26 to consider inverting the `comonad` / `profunctors` dependency: the two packages are of comparable scale, but it would avoid a couple of other core platform packages getting caught in the `profunctors` dragnet transitively. But if I'm hearing you right, the fact that I've _already_ broken up my packages into a pretty fine grain is your problem, so I can't win.
Looks promising. I could just give the sandbox a different name, like `myprog`, and users could put it in whatever locations they wanted. The main point is not to require end-users to have Haskell installed. I should be able to safely remove `packages` and `x86_64-linux-ghc-7.10.2-packages.conf.d` folders, and `world` and `add-source-timestamps` files. EDIT. Yes, that seems to work. There's one big problem, however, the sandbox is over 750Mb, whereas my executables alone are only 30Mb. Now, given all transitive dependencies, I could try to cut some unused libs here and there, all `*.hi` files. Here's what I removed: * some unused executables, 83Mb * `logs`, 0.5Mb * `x86_64-linux-ghc-7.10.2-packages.conf.d`, 0.8Mb * `packages`, 10kB But the `lib` folder is 500Mb. Is there a way to have cabal remove the `*.hi` and `.dyn_hi` files from all libs? Also, I should probably only keep the `.so` libs and perhaps the `.a` could be removed? But it would only cut the size by half, and 250Mb is still too big.
Shouldn't you just be able to copy the binary? AFAIK the only thing's necessary is for there to be a compatible kernel/architecture/libc on the target system. Or is there something else that's making your binary non-portable? I guess `gitit` is doing something weird, I've never heard of it so I'm not sure what's going on there.
That'd be great to know, is it any more involved than simply expanding the Prism synonym?
Nothing weird at all! It just has some data files required for run-time which are kept in `share`. So they must be installed and accessible on the target machine. See the `data-files` section in [gitit.cabal](https://hackage.haskell.org/package/gitit-0.12.0.1/gitit.cabal). You'll find the `data/default.conf` file listed there, which I mentioned in the question.
What I tend to do is make a _private_ definition type Prism s t a b = forall p f. (Choice p, Applicative f) =&gt; p a (f b) -&gt; p s (f t) which I don't export to avoid collisions, then I define a version of the `prism` combinator: prism :: (b -&gt; t) -&gt; (s -&gt; Either t a) -&gt; Prism s t a b prism bt seta = dimap seta (either pure (fmap bt)) . right' {-# INLINE prism #-} Now I can define `lens` compatible prisms with just a `profunctors` dependency. The user now sees an opaque `Prism` in their haddocks, but it expands as usual when used. 4 lines of code in a utility module and no `lens` dependency is needed. If you are putting the definition into a utility module you can even make make it a normal type synonym so that it expands in the haddocks, just don't export it from your main modules.
This is my solution (and why I ported ghc to alpine linux): https://github.com/mitchty/static-ghc-linux-demo I hate dynamic linkers in general. My goal is fully static executables. Then you can just get the binaries install location or just install your stuff in a deb to /usr/share or whatever. Should "just" run on any x86_64 linux of 2.16.24ish vintage kernel. 
I use a similar solution in my own code. However, in general, that won't work because I will want to deploy other software (e.g., `gitit`), which I cannot change.
It's similar to [haskell-scratch](https://github.com/fpco/haskell-scratch). However, you don't have any `data-files` in your example, which is the problem. Nor have you any dependencies, which themselves may need `data-files` or dynamically loaded libs. 
Not quite that similar, the example there is just for show really, not meant to be comprehensive. I could expand it a bit to be more inclusive, adding dependencies just inflates the result binary size. I've used it to build a fully static pandoc that runs on any distribution. It won't cover data-files at all that is correct. It is more meant to cover glibc non portability. The whole idea is not to use dynamically loaded libraries at all and make a fully static executable. Pandoc as an example can embed data-files inside the bss segment (iir elf correct) of the executable. That saves you having to deal with packaging data-files at all. It isn't all that different really from making files you depend on into header files you compile into c then static linking the whole lot together. I've used that to great effect with just plain old c for years.
if you compile statically you only need to keep the data folders...
Yes, I tried with the following command: cabal install --disable-shared --disable-executable-dynamic --enable-relocatable --enable-library-stripping --enable-executable-stripping --disable-library-for-ghci Unfortunately, I got: Building vector-0.10.12.3... ... &lt;command line&gt;: can't load .so/.DLL for: libHSprimitive-0.6-3d4UsQu7pJCEtlsxN3gLjk.so (libHSprimitive-0.6-3d4UsQu7pJCEtlsxN3gLjk.so: cannot open shared object file: No such file or directory) and later Building profunctors-5.1.1... ... &lt;command line&gt;: can't load .so/.DLL for: libHSstm-2.4.4-C1kFMnPqFjvDhFjgMZGUpr.so (libHSstm-2.4.4-C1kFMnPqFjvDhFjgMZGUpr.so: cannot open shared object file: No such file or directory) So, it appears that these dependencies cannot be built statically...
Monad transformers are still monads, mathematically speaking. They are a convenient way to encapsulate multiple kinds of effects, but in terms of expressivity they are no more capable than monads.
I have a project called [haskell-builder](https://github.com/dkubb/haskell-builder) which is based on /u/suadade's alpine-linux docker image. The goal is to emulate [golang-builder](https://github.com/CenturyLinkLabs/golang-builder) both in behaviour and functionality. The difference from this and haskell-scratch is that it creates **fully** statically linked binaries, so there's no dependencies on external libs. You should be able to run this binary on any modern linux regardless of what libraries it has installed. You can even run these executables in a docker scratch container, which is basically a completely empty docker container. An easily overlooked detail is that it uses musl instead of glibc because glibc adds some "silent" dependencies on files even when you tell ghc to create a statically linked executable. It's possible that neither mine or /u/suadade's projects exactly meet your needs, but I'd recommend further exploration in the direction of building your portable binaries under alpine linux w/musl.
http://hackage.haskell.org/package/algebra-4.2/docs/Numeric-Algebra-Hopf.html
I'd come. But I'm not sure how you'd you inform (people like) about it :) I don't have any Haskell users in my contacts that I know of.
[Codewars](http://www.codewars.com/) and [Exercism](http://exercism.io/) are both great sources of exercises. Codewars is easier to get into, but Exercism tends to provide much richer feedback.
I also think that [hackerrank](https://www.hackerrank.com/) have some usefull exercises for haskell
Open unqualified imports make me sad. The stated reason for them (at least in the GHC project) was to avoid annoying merge conflicts. But if you use haskell-mode's `sort-imports`, `align-imports` and [resolve trivial conflicts](http://github.com/ElastiLotem/resolve-trivial-conflicts) such conflicts are a non-issue. That said, if you do use open unqualified imports, this proposal does make sense.
I agree. They make code terribly hard to read if you're not intimately familiar with all the modules which are imported. 
What was the combinator that mr. Kmett was thinking of at 34m30s? Cheers everybody!
What kind of exercises are you looking for? more "partical" ones (building useful software) or more "simple" ones? Who is the target audience? total newbies or one that studied some Haskell? Anyway, here are a few ideas: - Implement common Prelude functions (like `curry`, `uncurry`, `map`, `elem`, `(.)`) - Replicate Unix CLI utilities (like `cat`, `echo`, `yes` or `less`) - Build an RPN calculator ([solution from LYAH](http://learnyouahaskell.com/functionally-solving-problems#reverse-polish-notation-calculator)) - Download a file from the internets over HTTP - Parse a subset of markdown and generate HTML from it - ^ serve it over HTTP - Build you some parser combinators - Build something you like!
(and NICTA Course) :)
Same here. I use qualified imports and manage them automatically, so the extension is addressing a problem I believe is best solved in another way.
As an example of fully static and portable binaries outside of haskell, and note I'm a bit of a force of nature on using musl libc to create static binaries, I also use alpine linux to build a fully static tmux binary example here: https://github.com/mitchty/alpine-static-tmux/blob/master/Dockerfile That I can then use that docker file to build and then copy that tmux binary to any x86_64 linux anywhere and it "just works". The only caveat being similar to the data-files issue in that the terminfo database must be /usr/share/terminfo, OR in ~/.terminfo. Effectively the above is how I treat/use alpine linux+musl libc+ghc etc.. for my own purposes. I should probably write up a blog post on the motivation behind all this junk or put it on the wiki. Also of note, I'm working on an armhf port of ghc 7.10.2 to alpine linux but its... not cooperating so hope that isn't important.
I disagree with this, because I don't have trouble understanding where functions and types come from in other languages which only have open, unqualified imports such as, for example, C#. I think the real problem here lies with the tools. If there was an easy way to see where a symbol came from in an editor, then readability wouldn't suffer. GHC already has this information, so I think it just needs to be (better) exposed to the user. 
FWIW- I found the 10kg swiss army knife hilarious.
Thought about linking this one but didn't like the fact that the very first exercise requires writing a partial function. Also [this](https://tonymorris.github.io/blog/posts/20-intermediate-haskell-exercises/), interesting but lacking laws.
I was the author of the proposal, but FWIW I find I want this less now that Prelude has been generalized a touch. It was mainly having to hide Prelude's foldable styled functions after already having `import Data.Foldable (Foldable(..))` that really bugged me.
Elm records introduce row types and row polymorphism. ORF essentially uses typeclass constraints and existing type level features to drive instance resolution to pick a selector. It's the difference between something like this: f :: forall a. { x :: a | r } -&gt; a f r = r.x and something like this: f :: HasField "x" r =&gt; r -&gt; FieldType "x" r f r = #x r In the first case, the type system has to solve the constraint that a record must have at least field `x :: a` and some other `r` fields, and it will return the type of the field (and the order of fields doesn't matter, because record-wise concatenation is commutative - the record `{x :: Int, y :: Bool}` is equal to `{y :: Bool, x :: Int}` is equal to `{y :: Bool} ++ {x :: Int}`). In the second case, the field constraint is 'pushed out' to instance resolution: we pick the right `x :: a` selector based on the instance of `HasField "x" r`, depending on the `r`. Then we use that to pull the value out. We could add row types to Haskell (they're a powerful feature), but I don't think anyone has sorted out all the design issues that might arise, or all the backwards compatibility or type system changes, or anything like the syntactical issues - a lot of the Elm syntax is already in use by Haskell in other ways I think, and Haskell is very 'syntactically crowded' so you have to pick your battles. That's why this space is so crowded... There are dozens and dozens of possible alternative designs and implications. It's not a trivial choice to make. A lot of record proposals have boiled down to adding a bunch of new type system features and fancy machinery. I think the new design mostly manages to live with what we have now, as opposed to add tons of features, if all we really want is "just being able to overload record field names".
Yes, the compiler cannot. However, because cabal knows about the `data-files` and there's the `Paths_&lt;name&gt;` file generated, perhaps the `Paths` module could provide some interface for accessing the files listed in `data-files` section using the [file-embed](https://hackage.haskell.org/package/file-embed) package? Of course, it would only work for GHC because of TemplateHaskell but still it would be much better to have a standard way to programmatically access these files and have them embedded into the executable (with an option, of course).
Is it really that bad to have to specify which record you are talking about in a name? One of my most common things to do to make a haskell program more readable is to type synonym for BYTESTRINGLAZY and BYTESTRINGSTRICT . **It is cool they are adding this feature but from a BP POV I can't see how requiring record accessors to either have different names or live in different modules to be that terrible. Ambiguity is the enemy long term!
&gt; if you use haskell-mode I use haskell-mode, but I draw a big line there at "this isn't a problem because emacs." Getting an emacs+haskell-mode system running, and learning to use it, is too much to ask as a prerequisite to writing code in Haskell without endless futzing up at the top of the file.
You're welcome!
Man, those look nice!
Hmmm... honest question, is it possible the general verbosity of a symbol in those languages might be helping out? Seven concrete nouns is one thing, importing `?. ^. &lt;:&gt; !¡!` is quite another.
How does it handle libgmp? (Which IIRC can't be statically linked for licensing reasons.)
Just add the imports manually aligned/sorted, it's not very hard or time consuming. Reading open unqualified code though, THAT'S hard and time consuming.
That says don't use explicit import lists, not don't quality the imports. Only the point about tags applies, and it's true enough, but tags work even better with qualified imports.
I don't use qualified imports unless forced to by API design because I find that they make the code noisy to read. I write Haskell because it is pretty and appeals to my sense of aesthetics. `P.#.` isn't aesthetically pleasing, and would be necessary to appease Tekmo's concern above, which applied to an _operator_. If anything I view qualified imports of operators as syntactically harmful, especially when the operator involves a `(.)` already! `P.#.` vs. `P..#`. Both are eye-bleedingly bad. Blech. I craft the libraries I work deliberately on to avoid the need for explicitly qualified imports in practice and as a general rule I try to avoid reusing names, enabling me to better jump around between packages without consulting some local hacked up convention at the top of my module first, except when forced by surrounding conventions. This one is a matter of personal preference, but my preferences lie pretty strongly in the other direction as I find the way I work boosts my personal productivity. I'm only one developer though. When I contribute to other people's packages I try to conform to local custom.
https://github.com/1HaskellADay/1HAD/tree/master/exercises/HAD/Y2014 has many interesting exercises
Let's try baldy pinging: /u/whatarethebest What are the best Q&amp;A websites for subjective questions?
The source code seems to indicate that the @ is necessary: @/u/whatarethebest What are the best Q&amp;A websites for subjective questions?
Maybe the bot doesn't know the answer?
Thank you very much everyone! I have now collected the ideas from this page, and put them [here](http://pmikkelsen.github.io/haskell-exercises/). I will update the page whenever i find some new and useful exercises
with a good IDE, import lists are easy to manage (even some version control conflicts). with a better IDE, unqualified imports won't confuse, because of jump-to-definition.
You beat me to writing this comment. Tnx. 
Just to clarify: I don't think there's anything wrong with qualifying imports to disambiguate between symbols (e.g. for something like Data.Map). However, I do think that listing every single function and type you want to import from some module is madness.
GHC API. Anything else is fundamentally useless for serious Haskell programming (looking at you, HSE-based tools)
Thank you. Thank you thank you thank you thank you. 
For those not familiar with Reddit's oddities: the fact that https://www.reddit.com/user/whatarethebest 404's is an indication that /u/whatarethebest has been "shadowbanned," which is Reddit's unique approach to dealing with ill-behaved users. Reddit's definition of "ill-behaved" is famously unclear.
I think stack is now the "better"/recommended way to do things? I'd prefer to do it with stack, if possible, that's all. Perhaps it was because I used it stupidly, but cabal made life hard for me. 
"Every single function and type" IME usually amounts to about 1-5 names per import line. GHC can generate this list for you, with `-ddump-minimal-imports`. Using qualification, however, is about *not* specifying every name you use, but simply using: `Map.insert` instead of `insert`. Or `Set.delete` instead of `delete`. This is nice for readability, especially for lesser known modules. I agree that qualified operators are atrocious, so for those I list the names on top, or I allow myself to import only `Control.Lens.Operators` unqualified. Since I don't import anything else (but `Prelude`) unqualified, and I don't tend to declare my own operators, this is not a problem (all operators are always from there).
Hey, Slant team member here, the command to invoke is &gt; @whatarethebest and then follow it with what you are looking for, so something like this &gt; @whatarethebest Q&amp;A websites for subjective questions The thing is though, that the bot needs to be connected to a subreddit for it to work, which I do not believe /r/haskell is. Of course you are welcome to test it out somewhere we do have it currently running like on /r/testabot/ or /r/androidgaming (you'd probably want to keep anything here Android related as to not piss off the mods). *edit - It does look like some admin shadowbanned it even though it is only running in a test sub and two android subs with the mods permissions. We will get it sorted soon hopefully, sorry guys.
As a long time user of and believer in cabal-sandboxes (and a person who used to say the same thing to others) I have to say I find `stack` to be dramatically easier to use than sandboxes... 
Which then runs afoul of the laundry list of items in the GHC style guide mentioned above.
I tend to use very general combinators to avoid the need for importing some horrible monomorphic one-off combinator. I'd get very little benefit from ShortImports or LocalImports as a result. I'm writing Haskell, not ML explicitly because I prefer using typeclasses to dispatch rather tham locally opened modules. Fortunately there is a reasonable well of good core mathematical abstractions worth abstracting over and those are gradually getting encapsulated into packages. The rest wind up being rather horrible one-off APIs for working with different containers and the like. Some of that is handled by the ad hoc combinators like `at` and `ix` in `lens`, `length` moving into `Foldable`, etc.. I write fewer and fewer random local names for new data types these days -- and when I _am_ forced to write such a one-off combinator and it takes the common name I _do_ use qualified imports for working with whatever monomorphic `map` or `!` or `lookup` I wind up having to write. For me this is very much the exception rather than the rule. My preferred fix here is to look towards the hyperlinked source that is coming to hackage. I expose all the internals of my libraries in some kind of .Internal module, visible from the haddocks, so everything is exposed in a way you'd be able to walk from such hyperlinked code. Now you can retain the ability to navigate and I can maintain the ability to write.
**Update:** Back online! --- Might as well reproduce this info at top level: /u/whatarethebest appears to have been globally shadowbanned, somewhere between 5 and 7 days ago. Between Labor Day and rolling out a major revision to Slant.co itself, nobody noticed. Since it only runs when explicitly invoked, and only in subs where it has explicit moderator permission (plus /r/testabot but come on), we're hopeful for getting this overturned. Unfortunately though, this means you can't really see it in action. For posterity, [this was its last reply](https://np.reddit.com/r/AndroidGaming/comments/3jabo6//cuqkrg3?context=1) that made it to daylight.
/u/_AndrewC_ and /u/tomejaguar are shadowbanned for their atrocious spamming practices in 5... 4...
The only problem is that I'd like to implement the whole thing on the JVM. I sent an email to the author of Frege hoping he can help me somehow. To use the GHC API I would have to write some sort of server which would run outside the JVM. Not pretty, but if everything else fails...
1. Clone https://github.com/chrisdone/emacs-haskell-config 2. Edit the haskell layer of spacemacs and customize to your liking 3. Profit 4. Wait for `stack-ide` to mature Edit: Haskell-mode has much better support for stack than you might realize, you are looking to set the `haskell-process-type` variable to `'stack`. There are numerous flycheck modules that will give you the error reporting you want. I don't actually use any Haksell specific autocomplete, but if I did I would try to rely on an auto completion that the interactive haskell process provides; `stack-ide` support for autocomplete is in store for the future
I had this issue, I believe it was caused by ghc-mod but spacemacs wasn't being too clear about it. When I went into the project and ran `ghc-mod list` I was getting an error an error. Building ghc-mod from the master branch on github and having those binaries on my path instead caused `ghc-mod list` to work and spacemacs to get past the Initializing stage when opening a haskell file.
So its $80 dollars normally? Some expensive ebooks.
I did see some discussion of ghc-mod being the culprit, but I'll have to wait until I have more free time to see if that solves it.
I just executed plain `emacs` first so that it would download everything required by each layer, and then tried to edit a file with the usual ex/vim command. I tried with 24.3.1 which gave me an error about four packages only being available for 24.4, so I built 24.4 from source, got an error `Error (use-package): helm :config: Invalid function: add-function` Either way, though, it just sits at `Initializing...`. I'll try u/actionshrimp's ghc-mod suggestion at some point, maybe.
What do you see as the benefit of using your graph-based FP language? (In general, this is a worthy goal, IMO. I am not opposing it.) From reading your post, which focuses on implementation, I wonder if you are getting lost in the weeds. Do you have a clearly articulated goal that justifies the effort involved? If your primary purpose is to get experience in some implementation language(s), you might be better off not choosing a difficult research problem as the subject. See Margaret Burnett's work at Oregon State University in general and her [survey](ftp://ftp.cs.orst.edu/pub/burnett/whatIsVP.pdf) in particular.
With the next `cabal` version you won't need to use sandboxes that often anymore anyway... Stack on the other hand abides to the Law of the Instrument and takes sandboxes to the extreme and I trust its developers to double down on this technical debt and keep doing so even though `cabal` will reach a more balanced use of sandboxes thanks to the nixos-style package db reducing the need for sandboxes.
About the scalabiltiy. I'm not sure why Haskell would be different from other languages. Pretty much all main stream languages (apart from Java) don't encourage qualified import (most of them even don't allow it). That doesn't people to make large (I mean huge) applications.
Cabal needs better support for curated package sets. The no-reinstall stuff won't help any with e.g. the use case I currently use stack for, to build multiple similar projects on a Docker Jenkins slave with the sandbox packages already built. This only works if all the projects use the exact same version of every package and Cabal's giant config files to constrain all of them are less than ideal.
Is there a particular reason you want to build this on the JVM? It seems an odd choice considering there is no widely used Haskell implementation for the JVM and your system will likely only have a fraction of the users of whatever you pick as the backend Haskell implementation initially.
Native support for curated package sets in Hackage and cabal is being worked on afaik (it would help getting there faster if people would contribute to cabal rather than diverting resources to `stack` leaving us with two incomplete tools...)
It's not fair competition though if one project has all the attention (including FPCo who clearly has an agenda of making Stack appear the "best" tool ever to all new users so they get hooked on Stack, as well as getting `cabal` to loose its status being the official standard Haskell build tool, and successfully spreading FUD about Cabal) while the other project is mostly relying on contributors donating their spare time and get little gratification because the spotlights are all pointing to the new shiny Stack anyway. I just hope that Stack won't end up killing the Cabal competition...
I agree that stack makes things dead easy, but if your work flow demands an unsupported tool then cabal is not a huge leap.
Had the same problem, ghc-mod does fix the problem but you need the current git one since it hasn't been pushed to hackage.
&gt;How do you come to this questionable conclusion? Is it really necessary to hound people just because they are using a tool you don't like?
First of all, I'm not a researcher and I have no experience in implementing compilers and designing languages. I'm just a guy with some ideas... My question is "Why just text?". Developing a graph-based language is my way of trying to answer that question. I haven't read any paper on the subject and I don't intend to because I want to create something "refreshingly original". My language is almost a superset of Haskell so you are still writing Haskell code. We could say it's optionally graph-based. Everything can still be written in pure Haskell. You introduce graph elements when it suits you. Using graphs is especially good for visualizing the interaction between components, modules, for defining data structures, etc... I don't know if this is a good or bad sign, but I think that my language will be very easy to implement. I don't see any technical difficulties. Parsing Haskell would be much more difficult! You say it's a difficult research problem... maybe I'm solving another problem...
Looks neat. Will give this a try tmrw..
Well, it should certainly be easier than implementing a Haskell parser with all extensions from scratch on the JVM.
I got it for $20, which was half off.
Comma for cons seems very confusing to me. I disagree that this is the usual, everyday use of comma. To me, the natural use of comma in everyday life is not to add something to the front of an existing list, it is part of a description of one entire list (the way that it is now in Haskell). The bigger issue for me, though, is the context dependence of this. What really confuses me about it is that in something like `(1, [2,3])` the two commas mean very different things: one is a data constructor and the other is a special syntax element (as it is in Haskell now). You would also have (in my opinion) pretty strange looking things like this `[1, [2]]` and you would need to mentally type check the code to determine whether `,`-the-data-constructor is being used or `,`-the-basic-syntax-element is being used. With `:` and `,` being separate, you can just glance at it and know quickly. Also, something like `[1, [2]]` would likely be pretty misleading and surprising to someone just starting to learn the language, especially if they come from a dynamically typed language that allows heterogenous lists. While I do agree that it would be nice if `:` was used for something other than cons (personally, I would like it for type signatures) and I also believe that `::` would make a poor cons operator name, I don't feel like this would be the way to go about it. Your suggestion for `:` is interesting. It has a somewhat Smalltalk-ish feel to it, which is very intriguing to me. It makes me wonder if there might be a place for an operator that does something like that in Haskell (with a different name).
5.3.0.0. I think I compiled it with `git`, yes, although I can't be sure because I've been trying everything possible haphazardly.
cannot....resist....
What relevance does this have to r/haskell? People on IRC being dicks is not a new thing. Some of the most abusive people I've ever interacted with have been on Freenode's \#\#physics and \#\#lisp. Each channel is independently moderated. Some channels hold higher standards than others. The best you can do is reach out to the moderators and hope they have some sense of liability for what goes on in the channel. It's really unfortunate, but that's the way things tend to work. If a community's leadership and moderators don't care enough to hold their IRC channel to a high standard, it will likely degrade into a cesspool of aggression and hostility.
Thanks for working through this! Perhaps some of it can be contributed back to spacemacs, haskell-mode, etc.
You don't appear to be using ghci-ng in your haskell layer and ghc-mod doesn't currently work with stack last time I checked or maybe it now does? Is there anything you've done differently then what I mentioned here: https://www.reddit.com/r/haskell/comments/3jww0s/can_you_post_your_emacs_configuration_for/cut1zef
I've met Ryan Davis in real life, and he's generally pretty nice, but I agree with you this kind of behaviour is out of line. Just because you happen to know more about a language's features doesn't give you the right to be an asshole to them. I've been coding ruby for a long time so I'm sure there are features of ruby that I could do the same thing to him, but there's no point. It doesn't make me a better programmer, it just means I am able to memorize more or different language trivia than that person has focused on. I consider myself a beginning Haskell developer, so I am sure there are tons of things Haskell devs could school me on. So far I haven't experienced anything like that from the Haskell community. /u/dogweather If you ever have any questions about Ruby feel free to hit me up.
Sorry, I'm not sure I'm following. `app` is something that should be of type `Network.WAI.Application`. It "knows nothing" about servant on type-level, since it's purely a WAI-thing. Each microservice provides an `app`, which is launched with `warp` server to serve.
I'm not saying those languages doesn't have namespace, but that they allow and encourage unqualified import. As you said C as no namespace. C++ as namespace but as a `using` clause to avoid to use qualified name. Python normal import syntax is unqualified. I remember well, Ruby is the same. Javascript doesn't evn have a concept of module (but maybe it's not a good example of scalable languages) What I mean is : yes most of mainstream languages have a concept of namespaces, but they are only there to resolve name clashes, and people uses unqualified name when possible. I'm not saying I'm in favour of unqualified names, I'm 45/55 in fact, but I just don't buy this scalability argument as they are lots of counter examples out there. My position at the moment is EdKmett is right about qualified import being noinoisy and therefore hard to read, and Tekmo is also right , unqualified imports come from nowhere and are hard to read too. Looks like a tie, however, unqualified imports are far easier to write and maintains, which gives them a win ....
See also the comments by /u/octatoan and /u/yogsototh . They detail a Spacemacs and `ghc-mod` based approach. 
I didn't mean to include the "too" there, I just edited the comment *after* having already written about the interpreter not working. I forgot it was earlier on the list. Sorry. Everything works now. 
I'm using the `ghci-ng` shortcut (`SPC m h t`) and it works. I'll check for what you said.
:)
I wonder how backwards-compatibile it is though. In case it all still works with a cabal/sandbox workflow then it would be great to have this in spacemacs. edit: expanded bw- to backwards-
cabal/sandbox? I'm fairly sure it does, but it's hard to even get all the packages installed. (As you can see, I am no cabal ninja.)
Because my language ~~needs~~ wants to be *simple*, I'm pondering whether to add special notation for functors, applicatives and monads. Do they deserve special treatment? And what about FRP? Should I introduce special notation for a library/paradigm?
Yeah, I got it a while too late. I am not a smart man.
I think what you're describing at the end of the first paragraph could be prevented. There hasn't been a lot of work in that direction yet, but there's a *lot* you can do with those API types. We can walk them down and compare them to others, we can check whether a link to some endpoint in an API points to an endpoint that actually exists, etc. What kind of checks would one like to perform in this case (the *owlcloud-albums* example) ?
&gt; I think what you're describing at the end of the first paragraph could be prevented. You can always do a runtime check as to whether the other end is on your expected version, but then any mismatch is a _runtime_ error :-) You can't get a static guarantee of compatibility unless you always recompile and deploy simultaneously. Microservices which aren't distributed are just called modules :-)
Only the main type and class, not everything :) IORef.new is nicer than newIORef, but IORef.T is ugly :) So many of my imports are import Name (Name) in addition to the qualified import. Import * is not very common in Python.
Well, not exactly. If `owncloud-users` gets an API update, which breaks `owlcloud-albums`, you will get a compile-error when you run `stack build`. So you must update `owlcloud-albums`. I agree that my tutorial doesn't touch deploy part, which needs to be thought of in a good way, for example, to not let user decide what to deploy, but to figure it out semi-automatically, to not let deploy only `owlcloud-users` alone, deploy also must handle some intellectual way to deploy things to not have to stop all services (make it possible to keep running old users with old albums, while deploying new ones, and then do atomic switch) but that's a material for another tutorial :) UPDATE: strictly speaking, `owkcloud-lib` will get an update which breaks `owlcloud-albums`, not `owlcloud-users`, since routes are stored in `-lib`. UPDATE2: also, I just realized that maybe it makes sense to try to keep microservice-routes inside microservice itself, thus you'd manage your dependency-tree via `.cabal`-file of each microservice, which might also help with a fine-grained redeploy techniques. Just as an idea to try out.
I completely agree. I filed [#7253](https://ghc.haskell.org/trac/ghc/ticket/7253) 3 years ago. Recently Alex Rozenshteyn [volunteered](https://mail.haskell.org/pipermail/ghc-devs/2015-August/009623.html) to implement it, so there is hope.
Ah.
Nothing is a silver bullet but it is hard to argue that the state of package management on Haskell was great before stack or even now. If your main accusation is that they identified one of the largest problems new users have with Haskell from the best data available (even if that was not 100% representative) and are attempting to solve it I really do not see your point. Quite frankly to me you sound like someone who has too much emotional investment in the cabal-install project. Who cares which project solves the problem as long as it is free for use, modification and contributions by anyone? And if you are arguing that we never had a problem with package management you apparently haven't been here in the last few years.
Here's a full production example of what I do. I'm showing you: - how to create datatypes with fields which don't collide (because of their prefixes) - how to generate to/from json instances which remove those prefixes, and also convert fields to/from underscore-based form instead of camel-cased - how to generate nice "classy" lens which have same names for different types (`time`, `descriptionVal` lenses can be used for different types) - in the end you can see some examples Here's the code: data FirstVal = FirstVal { _fvTime :: UTCTime , _fvDescriptionVal :: Text } deriving (Generic) $(makeLensesWith abbreviatedFields ''FirstVal) data SecondVal = SecondVal { _svTime :: UTCTime , _svDescriptionVal :: Text } deriving (Generic) $(makeLensesWith abbreviatedFields ''SecondVal) instance ToJSON FirstVal where toJSON = genToJson 3 instance ToJSON FirstVal where toJSON = genPJson 3 instance ToJSON SecondVal where toJSON = genToJson 3 instance ToJSON SecondVal where toJSON = genPJson 3 -- utils used: genToJson :: (GToJSON (Rep a), Generic a) =&gt; Int -&gt; a -&gt; Value genToJson = genericToJSON . jsonDeriveOptions genPJson :: (GFromJSON (Rep a), Generic a) =&gt; Int -&gt; Value -&gt; Parser a genPJson n = genericParseJSON (jsonDeriveOptions n) . jsonKeysToUnderscore -- | Converts keys of json object from camelcase into underscore form, -- for example "configId" would become "config_id" jsonKeysToUnderscore :: Value -&gt; Value jsonKeysToUnderscore (J.Object hm) = J.Object . H.fromList . map (first (toText . camelToUnderscore . fromText)) . H.toList $ hm jsonKeysToUnderscore v = v jsonDeriveOptions :: Int -&gt; Options jsonDeriveOptions n = defaultOptions { fieldLabelModifier = camelToUnderscore . drop n , constructorTagModifier = camelToUnderscore , omitNothingFields = True } -- | Convert CamelCASEDString to "camel_cased_string". -- Will be available in latest aeson release as camelTo2 camelToUnderscore :: String -&gt; String camelToUnderscore = map C.toLower . go2 . go1 where go1 "" = "" go1 (x:u:l:xs) | C.isUpper u &amp;&amp; C.isLower l = x : "_" ++ u : l : go1 xs go1 (x:xs) = x : go1 xs go2 "" = "" go2 (l:u:xs) | C.isLower l &amp;&amp; C.isUpper u = l : "_" ++ u : go2 xs go2 (x:xs) = x : go2 xs -- example usages: someFunction :: IO () someFunction = do t &lt;- getCurrentTime let first = FirstVal t "first val" let second = SecondVal t "second val" putStrLn ("Time of first val is: " ++ show (first ^. time)) putStrLn ("Time of second val is: " ++ show (second ^. time)) putStrLn (show (J.encode first)) -- { "time": "...", "description_val": "first val" } 
I've been working through [Cryptopals](http://cryptopals.com/) in Haskell. Most of the work is pure, so it's a nice way to get comfortable with Haskell without having to dive into IO.
The behavior of GHCI when a value doesn't have a `Show` instance is another big annoyance of mine. It's so scary to newcomers (and even still confuses me sometimes). Related, but not exactly the same point, I would love if GHCI could display functions more usefully (not saying that they should have a `Show`, but that, as a development tool, it could do a lot more useful stuff with functions). I think it's really cool how web browser JS consoles can just display a function's source when something evaluates to a function.
&gt; they allow and encourage unqualified import The opposite is true in my experience. I can't drop the "libusb_" in [libusb_control_transfer](http://libusb.sourceforge.net/api-1.0/group__syncio.html) function. In C++ I have to qualify method call with class name (for static methods, e.g. `MyClass::test()`) or class instance (for instance methods, e.g. `myClass-&gt;test()`), and there is not a way to "open" class namespace. The same in Java. That makes name clashes less likely. In haskell type declaration doesn't form a namespace, so everything goes into global namespace, making name clash very likely. E.g. without qualified import I can't have a function `length :: PhoneCall -&gt; TimeInterval`, because it clashes with `Foldable.length`, and I can't have `PhoneCall` an instance of `Foldable`. &gt; but I just don't buy this scalability argument as they are lots of counter examples out there Sorry, I don't see these counter examples yet. &gt; My position at the moment is EdKmett is right about qualified import being noinoisy and therefore hard to read, and Tekmo is also right , unqualified imports come from nowhere and are hard to read too. I agree with that. But right now we have code like that (from `http-client` package): managerSetInsecureProxy po . managerSetSecureProxy po It combines all the drawbacks of qualified and unqualified imports -- we have to write the prefix everywhere, but still it is not clear what module these names come from. It is more readable then the following? Manager.setInsecureProxy po . Manager.setSecureProxy po Or even that (using recently proposed `LocalImports` extention): let import Manager in setInsecureProxy po . setSecureProxy po The corrent namespacing story is a [constant pain](http://blog.haskell-exists.com/yuras/posts/namespaces-modules-qualified-imports-and-a-constant-pain.html), with or without qualification. Probably it is better to introduce proper namespaces?
One thing that still doesn't work correctly with this setup is `, c c` tries to build with cabal.
FWIW, the answer to your question doesn't appear to be documented anywhere. Afer some searching, I think I've found it here: https://github.com/ruby/ruby/blob/trunk/compile.c#L2942-L3141
Got it to work.. See one of my top-level posts.
How would function definition by cases work? e.g. if I enter myadd a b | b &lt; 0 = a - b and then myadd a b = a + b 
For the most part I try to stick to writing libraries for things that are universal. This nicely lets me mostly just stick to importing pretty math libraries with nice names rather than messy qualified imports. Probably 9 out of 10 libraries I write can stick to the guidelines I mentioned above. If I were stuck working with something that messy then I'd probably switch to qualified imports for that messy thing, but the vast majority of my imports would remain unqualified. It may be unsatisfying but I don't think the "one-size fits all" solution fits the common case very well.
I do not propose to remove let syntax. Can you give an example of such definitions?
You probably have `DeriveAnyClass` enabled, otherwise `derive (ToJSON)` wont work. Occording to [docs](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/deriving.html#derive-any-class): &gt; With -XDeriveAnyClass you can derive any other class. The compiler will simply generate an empty instance. So both methods are identical.
 &gt; :set +m &gt; let a 0 = 0 | a x = 1 + b x | b x = 2 + a (x - 1) &gt; a 1 3 This is obviously contrived, but it is valid and patterns like this crop up in real code from time to time. 
Thanks. I've added a link for cabal-only instructions. Just out of curiosity, what exactly doesn't work for you in stack?
u/nicheComicsProject have some points, but you have the reason. Still doesn't change the fact that Java is hideous, ridiculous and asbsurb as a langauge. Just saying :p
Except when you have associated type defaults due to this bug https://ghc.haskell.org/trac/ghc/ticket/10361
there might be a difference with GeneralizedNewtypeDeriving rather than DeriveAnyClass, not sure.
Nope. There are no objective ways to grade programming languages just as there are no objective ways to grade natural languages. A language is as good or bad as the works it enables. In the case of programming language that means writing, debugging, testing, profiling, monitoring and maintaining software to answer certain requirements over the required lifetime of the project. So far there have been few languages with the same effectiveness as Java. Many claim they *can* be if given the chance, but until somebody actually tries, you never really know (hell, it's not even established that language abstractions -- as opposed to stuff like GC -- make a big impact at all). But if you want to argue over aesthetics, well -- to each his own. Me, I appreciate the aesthetics in algorithms or design. A beautiful program to me is one that implements a beautiful algorithm or has a beautiful design that allows great performance or maintainability. Abstraction and syntax *can* be beautiful but their effect on aesthetics is, to me at least, secondary at best. There's no doubt Haskell has far superior syntax and abstraction aesthetics to Java. But overall, I think that limited aesthetics is tarnished by the lack of impressive works Haskell has yielded. Still, some people don't like Java, as some don't like pretty much any well-used language. That's perfectly OK. It's a very personal thing.
Something to add to that is [mapM always performs poorly on large lists in both time and space.](http://stackoverflow.com/questions/24068399/haskell-performance-of-iorefs)
Curious why a Python script was submitted rather than a Haskell one, so I wrote a [Haskell one](https://github.com/FranklinChen/get-latest-stack-osx) for fun :-). {-# LANGUAGE OverloadedStrings #-} module Main where import Network.Wreq (get, asValue, Response, responseBody) import Control.Arrow ((&gt;&gt;&gt;)) import Control.Lens (firstOf, to, view, filtered) import Data.Aeson (Value) import Data.Aeson.Lens (key, _Array, _String) import qualified Data.Text as Text import qualified Codec.Compression.GZip as GZip import qualified Codec.Archive.Tar as Tar -- | For clarity. type Url = String main :: IO () main = get releasesUrl &gt;&gt;= asValue &gt;&gt;= (findFirstOsxUrl &gt;&gt;&gt; maybe (fail "No OS X download found!") (getUrlAndExtractTo ".")) releasesUrl :: Url releasesUrl = "https://api.github.com/repos/commercialhaskell/stack/releases/latest" findFirstOsxUrl :: Response Value -&gt; Maybe Url findFirstOsxUrl = firstOf $ responseBody . key "assets" . _Array . traverse . key "browser_download_url" . _String . filtered ("osx.tar.gz" `Text.isInfixOf`) . to Text.unpack getUrlAndExtractTo :: FilePath -&gt; Url -&gt; IO () getUrlAndExtractTo destDir tarGzUrl = get tarGzUrl &gt;&gt;= (view responseBody &gt;&gt;&gt; GZip.decompress &gt;&gt;&gt; Tar.read &gt;&gt;&gt; Tar.unpack destDir) 
pylint will flag it, so there is some consensus.
It's not really that. The question is whether code should be optimized for readability or writability. If it's optimized for writability we can ask the user to get better tooling for reading the code. If it's optimized for readability, then we expect the programmer to get better tooling for writing the code. In almost all large organizations, code is optimized for readability. When code is not reviewed by anyone and written by individuals, it tends to be optimized for writability, because there is no mechanism to enforce good style. 
I see, now thinking about. So if 99.9% of Java developers agree that is hideous and absurd as a language, while the remaining 0.1% (your group) thinks otherwise then my opinion will be just that. What reckless of me. &gt; So far there have been few languages with the same effectiveness as Java. I think you are referring to the platform in that regard, which would actually make sense. &gt; Many claim they can be if given the chance, but until somebody actually tries, you never really know (hell, it's not even established that language abstractions -- as opposed to stuff like GC -- make a big impact at all). Totally agree. God doesn't exists until is proven to. &gt; Me, I appreciate the aesthetics in algorithms or design. So noble of you, I envy you. Quoting u/agleiv_17, now I can understand how some people can... &gt; Write Java without throw up ever 50 seconds :) &gt; A beautiful program to me is one that implements a beautiful algorithm or has a beautiful design that allows great performance or maintainability. Where is this Alice's Wonderland where I can sing for a job? &gt; But overall, I think that limited aesthetics is tarnished by the lack of impressive works Haskell has yielded. This is not much about Haskell but others language that have both good atheistic and are actually successful apart from Java. I don't believe that Java's syntax has any influence over it success. I think the same about any PL out there. Is a shame that Java is not the whole package (by my criteria). Is always educative and entertaining to read your opinions. 
... and yet I have to use tools that exist today. I hope you agree that your objection that you can't read the code is also a tooling deficiency.
That's pretty much it. Python is nearly everywhere, no extra libraries needed. Get GHC by first installing Stack.
As a data point I have about 450+ collaborators across my various projects on github. It is definitely a team effort these days. ;) Your advice is taken in the spirit intended, but I'm not inclined to increase the burden on the development side.
I have no idea, and I don't run Haskell in production so I don't have a personal stake in it. I was more disappointed from a theoretical point of view, but that doesn't stop me from enjoying and trying to start using the language in more important roles.
`DeriveAnyClass` [takes precedence](https://downloads.haskell.org/%7Eghc/latest/docs/html/users_guide/deriving.html#derive-any-class).
ReaderT is the same as having an extra argument to every action in the monad. Not very expensive, especially if it's over IO and &gt;&gt;= is inlined. StateT has a higher cost: each action returns a new tuple containing the new state and the action's result. This tuple can result in significant amounts of garbage being created, putting additional pressure on the GC. An alternative to StateT is a ReaderT of an IORef, especially if the state rarely changes. As always, it's worth writing a criterion benchmark to see in your specific case what the relative cost is.
&gt; I don't know what you know about the Java platform, but all alternative JVM languages combined make up no more than 5-10% of the platform. I was referring to the tooling and libraries. &gt; Those numbers are based on absolutely nothing at all Ha. Of course not, I made that out. My point is that a subject matter, driven by the psych like "the horrid of Java", can become a fact if 100% of the opinion would agree. Your existence and everyone of your side of the fence are contradiction of my opinion. &gt; I can appreciate your rebelliousness against "the man" Haha. I truly apologize but this is so dramatic and hilarious. &gt; but an effective rebel is one who can recognize their opponent's strengths I do, but I am the kind of the being that cheers for the underdog. &gt; Maybe you just have a hard time accepting that great leaps in science and technology are often made by imperfect people and organizations Perfection is a illusion out context, which you can hardly get anything good from it, but at least for the future I wish that companies would cover their imperfections with a little bit of make-up and Java is not a good foundation for that, it actually make them look more uglier. It would actually make the world more pretty and exciting of what I already is, just a little bit. &gt; I'd go further and say that greatness is impossible without accepting flawed processes. Agree, wars are a example of that. Specially global. I move beyond and say that I actually need one of those little shocks in my country to move the clock forward. &gt; If you can't see it, why are you in this job? If all beauty is to be found in a language that produces no significant amount of working software, and if all amazing software Agree, I wanted beauty in every aspect of it and Java...making progress I suppose. 
I think they can be resolved. The main incompatibilities that I know of are: * The syntax of `true` and `false`. I believe there's no reason the compiler couldn't add the uppercase versions as special cases and provide an adapter function for pattern matching in both languages * The implementation of `String`. The difference in implementation between Frege and Haskell `String`s can be resolved through the use of portable adapter functions on `String`s that are implemented differently in each language. * The syntax of type classes, which can be pretty easily fixed to match Haskell syntax This means that portable code that targets both languages would be a little bit more verbose (because of the `Bool` and `String` adapter functions).
OT, but what IRC client are you using?
The two monad transformers that greatly interfere with performance are `EitherT` and "`WriterT` done wrong" (i.e. both of the `WriterT`s in `transformers`, which always leaks space). The other ones are cheap, particularly `StateT`, which is very fast.
This is basically the motivation for `pipes` and `ListT`. You get something like `mapM` except it runs very efficiently in constant space. See the [introduction to the `pipes` tutorial](http://hackage.haskell.org/package/pipes-4.1.6/docs/Pipes-Tutorial.html) which talks more about this.
agreed about the "lock" thing, but using "free", you can tie it into "free monads", which is a googleable phrase. but that indeed might be better saved for a different tutorial. the way i understand it informally, "free" is a way of freiing nyourself from dependencies, by coupling definitng things from interpreting them 
&gt; The other ones are cheap, particularly StateT, which is very fast. [chadaustin](https://www.reddit.com/r/haskell/comments/3ktmr5/tips_for_writing_efficient_haskell_code_do_you/cv0llhi) noted the tuple in [StateT's definition](https://hackage.haskell.org/package/transformers-0.4.3.0/docs/src/Control-Monad-Trans-State-Lazy.html#StateT) as being problematic for garbage collection. Is this just an issue for pervasive usage of StateT? What did you mean by "fast", what in particular is fast about it?
How should the .dir-locals.el file be changed if you don't want to use stack?
Make sure you're using the elisp code that comes with v5.3. C-h v ghc-version RET will tell you what elisp version you're using.
The `E:` is our multiplexing marker so we can separate stdout/stderr even though emacs mushes it all together into one stream. Looks like something has gone horribly wrong there O_o. If that happens again please report a bug over here: https://github.com/kazu-yamamoto/ghc-mod/issues/new 
I was skeptical about accepting the PR that put it there in the first place anyways ;) What's your rationale for removing it?
Unfortunately every beginner is pointed down this path ("learn 'wholemeal programming' with map, oh it doesn't work for monads, add an M)". We should rethink the default sequencing tutorials so beginners don't hit this and conclude that haskell is crap for performant code.
thanks ! that helped
Thank you. Please see my revised question.
Pretty much that it means ghc-mod only works with some specific version of GHC when building with stack, which is possibly not the one you meant to use.
he's got a point though... I'd expect Linux distros to neuter Stack when packaging it in a way to play nice with the system wide installed ghc library packages. Assuming it gets packaged at all.
[Slides](http://de.slideshare.net/Mittie/fregeday-roadmap-for-resolving-differences-between-haskell-and-frege-ingo-wechsung)
It doesn't have to be that heavy. You can create quite small docker images which are not much larger than necessary. Compile gitit into a static binary and put that into the docker image. Zero overhead.
&gt; at least for the future I wish that companies would cover their imperfections with a little bit of make-up and Java is not a good foundation for that, it actually make them look more uglier I have high hopes for Kotlin in that regard. At present, there are few alternatives to Java for "important" software, and none of them is an improvement syntax/abstraction-wise. But the people behind Kotlin so far seem to have the good sense of giving the industry what it actually needs rather than what they wish it would need. 
I also think removing it is better in case of ghc-mod (should work with so many different environments/GHCs). I'd explain two way to install it from the readme, w/ and w/o stack. The variant with stack has some `stack init` command as part of the install procedure.
Not sure. You might not need any...
Perhaps not exactly on topic, but has anyone tried making Android apps in Frege? How pleasant / painful is it? Does it feel like working in Haskell, or does it feel like trying to wrap a bunch of unsafe foreign code in Haskell?
Couldn't `mapM` (and other functions) have rewrite rules so that they are transformed to something faster?
As a beginner to lens, this looks awesome! skimmed through it quickly. Will more thoroughly read it later and leave my review here :)
Wouldn't it be better if I added a "compile to Frege" option?
You'd have to compile it for multiple OS and archs though... and you'd have to trust that executable not to contain malware. Just like you'll have to trust that compiled stack executable that tool fetches...
True. I'm a bit spoiled being a Windows user so I don't think about multi-platform issues as much as I should be.
You could abstract even more and require 'm' to be a monad that has some variable updating operations, maybe MonadRef or maybe your own subclass of Monad. 
I could, but like I said at the end: it's too much abstraction just for the sake of testing. It'd be another thing entirely if the abstractions paid off by making the production code itself simpler elsewhere. More importantly, it's work I simply don't have to do in a non-pure language.
I don't really understand your problem enough to help... but if you're happy with doing unit tests that involve global mutable state in D, can't you do the same in Haskell? Just use `IO`? Beyond that, if you're interested in pure ways of representing protocols with implicit global state... I'm not familiar with the protocol you're referring to... but that seems interesting. Haskell might be a good language for that. If there's a way of doing what you want in some other language, it can probably be translated to Haskell, and maybe people here would be interested in giving it a go. If you don't know how to do the thing in other languages, it's possible that you're kind of trying to solve a research problem as your very first Haskell project, which might be overly ambitious.
Have you had a look at the paper [Testing Monadic Code with QuickCheck](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.9275)? The core trick from the paper is to test for observational equivalence. The code under test gets an interface via the free monad, which the QuickCheck machinery uses to set up context for / drive the tests. After that it's all about logging the relevant changes and comparing the logs - either actual vs expected or comparing that two sequences of commands that should be equivalent (as far as the things that are logged) have equivalent output. I found the paper hard to read the first few times, but I've done a [talk based on it](https://www.youtube.com/watch?v=qNeRZS8ZCKw) - although I was a bit under-prepared and under-practiced for it - and have some blog posts in the works on the topic. Hopefully those posts are clearer than this comment :)
Do you not mock out the IO part in a non-pure language? If you do, how do you do it? Is it much different from the approach /r/augustss suggested? I'm interested, partly because I've also used MQTT and spent a couple of hours on a train making an MQTT server in Haskell. I didn't get very far! The MQTT test suite seems to have sample test clients but not sample requests and responses. (If you have any of those, could you share them please?) Are you testing the parser separately? I'd be inclined to have your `replyStream` function take an `MqttRequest` parameter that represents the result of parsing the ByteString. Then I'd test the parser function. I started using Binary for mine but with a view to switching to pipes-binary if I needed to. Like I said, I didn't get very far. :-) 
I love the Haskell wikibook. I think I learned more there than using any other resource.
How about fabulous?
I get the impression that replyStream "knows too much". Why does it have to know that such a thing as a Handle (even a polymorphic one) exists? Why not just pass a monadic generator of bytestrings? Also, perhaps making replyStream directly alter the global state is too much as well... do the handlers ever read global state, or do they only modify it? In the latter case, perhaps each emitted response could be paired with a list of "requests to modify the global state". It would be the resposibility of an external interpreter to "thread" these requests and actually perform the changes.
Great! Haskell Mode has online documentation at http://haskell.github.io/haskell-mode/manual/latest/ that could use your insight. We would love to gather all the information in one place.
&gt; Fine, so you like Haskell and don't like Java. Actually it's not about "like". For me, programming languages are just tools and I don't associate emotion with them. When I find a language more powerful, expressive and elegant than Haskell then Haskell will move to the place Common Lisp is now and Smalltalk before it (and C++ before that): a language that was powerful, useful and a joy to work in but no longer the best one. &gt;state of the industry are completely misguided. Says you, but frankly, it's people like you who hold the industry back IMO. We make shoddy products and do so extremely expensively. If the industry would embrace what rigour we do have and push further in that direction, software development could cost a small fraction of what it costs today. &gt;industry you're talking about, has given us -- just in the past decade -- novel distributed databases, Watson, Google, self-driving cars, Lord of the Rings movies, Shazam, Truffle/Graal, face.com -- most of them are written in C/C++ and Java and none of them are legacy. Irrelevant. It has cost vastly more to do those things then it needed to. No matter how awful the tools, given the money involved, people would have developed all of those things. But we've had better tools for centuries. It's time to aim them at software development. &gt;still has like 50x excellent programmers than Haskell. Perhaps, but good like finding them. &gt;Not a single large, complex software has ever been written in Haskell in spite of its 100% genius programmers. You don't have a clue if that's true or not. Are you saying no *free software* system has been written? Because trading software has been taking advantage of Haskell for years now. Further, you constant appeals to popularity don't impress me. Plenty of brilliant mathematical and scientific insights took so long to be recognised that the person or people who had them were already dead. Haskell hasn't had a reason to run out and just cobble something together and then start the hype train the way Java did. It wasn't just trying to be a better C++. &gt;Haskell has never, in its twenty years, been put to the test. So what? I don't personally think Haskell is or will be the final solution but I suspect the research being done there will drive where software development goes in the next 100+ years. Java has little to teach us that is worth learning. &gt; that industry has had some very impressive achievements What is impressive is that the industry has somewhat managed these achievements given how ad hoc and poor quality the tools are. Your argument is silly though. A couple of hundred years ago you'd be telling us about all the incredible achievements of the horse and how the automobile has never accomplished anything. &gt; But that edge is usually based on better algorithms. Algorithms have little relevance. Mostly all that matters are costs. Using languages with poor expressiveness and no possibility for rigour, your algorithms cost many times what they should. &gt;They just make a much bigger difference than better abstractions. More blub. Every algorithm you've ever conceived was built on the back of abstraction. More powerful abstractions make better algorithms, as they always have. Remember that the function (or method) itself is an abstraction we didn't always have. &gt;There just isn't any data to support that Haskell gives any significant edge over Java/Python/Clojure etc. You're looking in the wrong places. Math is a rigorous field with lots of data points to look at. Haskell is, without a doubt, closer to math than any of the languages you mention. &gt;your belief that Haskell is somehow more based on math than other verification approaches is just plain wrong. SPJ had a talk where he covered what I'm talking about. In the beginning we had the Turing machine (how to do it) and the Lambda Calculous (what to do). Turing was a machine, far from the math. The Lambda Calculous *is* math. Haskell is coming from the latter side, Java, etc. from the former. So yes, Haskell is based on math and no, Java isn't. &gt;It's just that the PL-research part of it (which is small) has yielded very few results that are applicable to the industry in the past decade or so compared with other fields. Nonsense. Every language has used their results to some degree. Even Java is finally getting (or already has perhaps) anonymous functions and so on.
Have you considered using a free monad? See e.g. http://michaelxavier.net/posts/2014-04-27-Cool-Idea-Free-Monads-for-Testing-Redis-Calls.html
&gt;A language is as good or bad as the works it enables. By that logic, all Turing compliant languages are exactly equivalent. Java "accomplished" what it has because of: * First *popular* language with a VM * First *popular and free* language with a Garbage collector * Millions, maybe billions spent hyping it up * Companies getting universities to teach the language Right place at the right time. All Java had to do at that point was not be completely unusable. &gt;So far there have been few languages with the same effectiveness as Java. There have been no languages with the amount of resources thrown at them that Java has had. If we could have thrown an equal amount of resources at nearly any language they would have accomplished at least as much. &gt;Many claim they can be if given the chance, but until somebody actually tries, you never really know This is a rather juvenile line of thought. You can't be 100% certain, but we have tools to compare languages and determine what would be possible. If a language has, at a minimum, the features of Java and at least the expressive power of Java then of course we can be as sure as anyone can ever be about anything that such a language would be as effective as Java. &gt;that allows great performance or maintainability. These are all relative measurements and smack of more blub: more maintainability is probably possible but it's hard to see when what you know is blub. &gt;Still, some people don't like Java I don't like what it's done to the industry. I don't like the cults that form around it to try and make something coherent out of a fundamentally flawed development system. I don't like the snake oil salesmen pushing crappy tools that aid in this madness. If we had a better platform to begin with, many of the "tools" you apparently think so much of wouldn't exist. They wouldn't be necessary.
I'm fairly sure I just got lucky. But I'd be happy to help!
I think you're unnecessarily constraining yourself in Haskell by refusing to have tests that use `IO`. Whatever this nice testing approach is that you would use in D, you can do the same thing in Haskell. You seem to be writing it off because of some perception that you shouldn't do that. But as you've started to see, sometimes things just have to be in `IO`. Imagine you're writing some low level networking code in Haskell. How are you going to test that? You're going to have to use `IO`. You'll probably build some test infrastructure that sets up two processes and makes them talk to one another. If you want [100% test coverage](http://buildbot.snapframework.io/job/snap-server/ghc_ver=7.8.3,variant=default/Test_coverage/), you're going to have to test your `IO` code. Edit: When I say `IO` here I'm talking about Haskell's `IO` monad.
&gt;It was never my intention to disrespect you. Don't take what I say too personally. It's the technology I hate and the cults built up around it. I dislike your packaging of inferior engineering techniques as superior right as PFP is moving closer to mainstream awareness, though if what you claim is true, then once PFP systems are given a real chance and we see success then you would become a believer. &gt;a formal specification of an algorithm and a UI for programming a physical computer. The Turing side of the camp. I started firmly in that camp but have become convinced that the way forward is for Software development to embrace the other side: Lambda Calculous and the mathematical side. Math has extraordinarily powerful tools with (in many cases) centuries of use. We should try to leverage that as much as we can. &gt;Such a language must have a mental cost, namely require the programmer to concede some convenience to the verifier. Why? Because if it didn't -- meaning the programmer could write however she thinks or normally expresses programs -- then either many useful properties could not be proven, or we have a contradiction with the halting theorem. This is not true. When building with Haskell (and especially newer languages like Idris) you create types that describe what your system can do. You should have been doing some variation of that even in Java, you just didn't have a powerful enough language to get very far without lots of manual effort. I initially had your suspicions when exploring Haskell. Then I quickly realised that the productivity you imagine for the systems you're thinking of is an illusion. Sure, in Python I can write literally anything I can think and there won't be any boring verifier to tell me why it's wrong.... I'll have to run it to find out. I'll have to manually write a bunch of tests to verify behaviour (and hope they aren't buggy themselves). In Haskell (and even more so in newer languages like Idris) instead of designing tests to "kick the tires" of the system, I can formally declare what's possible in my system (to a degree anyway, and this is getting better all the time). Most of the tests that you're busy writing are not necessary in my system because the types already show the corner cases you're verifying can't exist. The bottom line is, a language like Haskell costs more up front and less on the back side. Something like Python is the exact reverse but the overall cost must be higher because it's required to do so much manual verification. &gt;not every functionality needs to be correct Trivially disprovable. Do you seriously have functions in your system that can just return wrong answers? You have to prove them to at least appear correct even for your brand of software development. &gt;A UI either works well or doesn't, and you only know after you've put it in the field. You're trying to move the means of verification into a location that you can say your answer is the right one. In reality, we can use any comparable language to get a good estimate. If Java has accomplished X in the field then a language as expressive as Java has the exact same potential. A language that is *more* expressive can be expected to have more potential. By trying to claim that we can only know once something has happened *in a specific language* creates an unsolvable chicken-and-egg problem: no one can ever prove the language because it's unproven. In conversations like this, I feel like a scientist trapped in medieval times arguing with a "doctor" of the time about the power and success of the Humours.
I do mock out the IO in a non-pure language, which lets me not do any IO at all. In Haskell it _has_ to be in the IO monad. It ends up being, IMHO, less pure. As for requests and responses, both the unit and acceptance tests in [my Haskell implementation](https://github.com/atilaneves/mqtt_hs) have a few.
Tekmo has also has [a nice write up](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html)
I never said I don't test my IO code; I don't _unit test_ my IO code. I gave [a talk at DConf 2015](https://www.youtube.com/watch?v=bxSPCmwqgYs) on testing which explains it better than I could here. BTW, I don't have to imagine writing low level networking code in Haskell, that's what the post was about ;) My point is that in other languages I can unit test nearly all of my logic (also known as hexagonal testing or ports and adapters) and leave IO to higher level tests. I can't in Haskell, or at least I can't do it easily.
The handle is needed because a request from one client can result in replies to any number of clients, including (or not) the client that issued the request. The way I modelled it was to have the result of handling a reply be the new state plus the a list of (handle, packet) pairs. One handle for each client that needs to receive a message and the bytes to send. And yes, the handlers always read global state. They have to.
Ok, but I think my point still applies. Just s/test/unit test/. If you can do it easily in D you should be able to do it just about as easily in Haskell. There's no rule that says Haskell unit tests can't use IO.
To the contrary, it's an approach that I've used before and *only* find useful when I have more than 3 commands. If there's just one or two random API calls that I'm making, I just use `http-client`/`wreq` directly. If it's 5+ calls that I make to an API (or any IO bound, inherently non-type-safe system) and they are being chained together (not just used independently in isolated places), then I'll go in and use a free monad approach. (Actually, I use `operational`, but they're basically the same). Either free monad style approach (using `free` or `operational`) could be a good fit for your problem, but learning how to use them correctly definitely has a learning curve. That being said, if you're new to haskell, I would recommend just working in `IO` in this situation. You're dealing with networking, and there's a bunch of errors that can crop up everywhere. Harmful abstraction is possible in haskell, just like it is in ruby or java (I've seen it in all three), so remember that mocking `IO` isn't the ultimate goal. One more thought I had. Your generalized type signature (which won't work without adding constraints or changing the monad) is this: replyStream :: (Monad m) =&gt; a -&gt; (a -&gt; m BS.ByteString) -&gt; Subscriptions a -&gt; m [Response a] I have no idea what it's supposed to do, but in general, when I write testable solutions for things like this, I don't have `ByteString` anywhere in the definition. The parsing is pushed into a different place in the code and I talk about specific data (`Person`,`Dog`,`Cat`) instead of the very general `ByteString` type. If that doesn't make sense, feel free to ignore it. It's hard for me to explain without showing a lengthy example.
How do you test code in other languages? Dependency injection? Mocking? Just do the same in haskell.
I really wish I understood the MQTT stuff better because this seems like a really practical and fundamental problem that I would have thought Haskell would have a good solution for. I think the idea of using IO in your tests is to mock your interface in the same way you did in your D code. Instead of having a class interface, your interface is a function of type `x -&gt; IO y`, which in real code writes `x` over the network and produces result code `y`, but which in testing code sticks `x` into an `IORef [x]` for later use somewhere else. EDIT: it seems like in a couple of places, people are saying 'use IO', and you're saying that you don't have to do that in other languages. When I say 'use IO', at least, I mean use it for global mutable state, which is what you're doing in other languages. I don't mean literally write to sockets in your tests. IO doesn't have to be real-world-affecting - for example, you need IO to forkIO, but your code could still be, for all intents and purposes, pure. (Yes, if you were parallelising a pure function you would probably use sparks, but the point stands.)
&gt; In my entire career I've met many Java developers but probably less than five that thought the language was good. Then you haven't been around much. You do know that Amazon, Google, and Netflix are Java shops? &gt; The software I write now is much shorter than the code I'm paid to write would be for doing the same thing. And already has, as an artefact of writing it, 60-80% of the tests I would have had to write as well. I know. But the problem is that languages that work well for a single developer don't always scale well for large teams or long-lived software. You just can't extrapolate. We've learned that from hard-earned (and costly) experience. &gt; Also, as much as you poo poo on the Haskell compiler... What? I adore Haskell! It's one of the most beautiful, elegant languages I've ever seen! It's inspired and inspiring. I'm just skeptical about its utility in the industry, and whether it's even on the right path to making software development better. That doesn't mean it's not academically fascinating and aesthetically beautiful. &gt; It's still faster than anything out of Java land afaik. [Not even close](https://www.techempower.com/benchmarks/#section=data-r10&amp;hw=peak&amp;test=json), especially when you consider that the Java servers are not only (a few times) faster but provide much more functionality (monitoring, hot code swapping). 
I think the main issue in this thread is using the term 'doing IO' for both doing actual IO sideeffects and having your code in the IO monad. People are suggesting the latter. Use IO for global mutable state, not for actual networking. That part you mock.
&gt; The way I define unit tests means they don't do IO. If by "do IO" you mean communicate with external actors, that makes good sense. If by "do IO" you mean live in Haskell's `IO` monad, that's a unjustified restriction. Concurrency and exceptions were, among other things, the reason `IO` was invented and both of those should be allowed in your unit testing.
As someone who uses lenses frequently but has (or had, before reading this) NO idea whatsoever why or how they work, this is magnificent! Going trough the core types one by one has already helped me a lot.
&gt; As far as I know, the Haskell options for global mutable state are IORef, MVar and TVar. Those are all examples of mutable references, but none of them are *global*, because you have to pass them around explicitly from the point they are created to all the points at which they are used. A global reference would be a top-level value of type `IORef a`, which you can't create without `unsafePerformIO` because `newIORef` has type `a -&gt; IO (IORef a)`, not `a -&gt; IORef a`. A top-level value of type `IO (IORef a)` would not be a global variable, but a computation which produces a new global variable each time it is called. &gt; What one client sends affects what the other clients might receive. Which means I can’t just use State to thread what the subscriptions are from message to message in the stream, since every message might have to change the global state, which alters how the server processes messages from different clients. The streams aren’t independent. There are two kinds of states involved here: the state of each client, and the state of the server. The state of the server is only "global" in so far as it is accessible by all the clients, but it isn't global in the sense that it is accessible by every single line of your codebase. In particular, it should only be accessible during a particular execution of a test. And to do so, you simply need to make sure that the state of the server is threaded through the simulated interactions with all the clients, in the same way in which you are presumably already threading the client state through all the simulated interactions with a particular client. Something like this: data Request data Response data ClientState data ServerState type Client = (ClientState, State ClientState Request) type Server = (Request -&gt; State ServerState Response) testOneInteraction :: Client -&gt; Server -&gt; State ServerState (Client, Response) testOneInteraction (c,nextRequest) server = do let (request, c') = runState nextRequest c response &lt;- server request let client' = (c',nextRequest) return (client', response) testInteractions :: [Client] -&gt; Server -&gt; State ServerState [Response] testInteractions [] _ = return [] testInteractions (client:clients) server = do (client', response) &lt;- testOneInteraction client server (response:) &lt;$&gt; testInteractions (clients ++ [client]) server 
The other two responses to this are saying essentially what I'm saying. But let me say it in a different way. **Whatever you would have done in D, do it in Haskell.** If that means that the code has to live in IO or ST, that is irrelevant to the issue. It doesn't matter what your definition of "unit test" is. If your D code meets it, then Haskell code that does the same thing also meets it. Alternatively, if you really care about not being able to do arbitrary IO things, then you can create another monad that wraps around IO and only exposes things like state mutations that you deem safe. This might be a nice thing to give you an extra layer of safety. But this is above and beyond what you can do in imperative languages. Because in those languages every line of code is in IO!
That link went nowhere. I tried googling and found another link to a PDF but that also didn't work. I'll try and watch your talk when I have time.
Darnit!
Sorry, this was just an example to demonstrate a case of when you have a good idea of the system dynamics- but it sounds much more interesting than I'd thought! I will try to make this a real example later :-)
Well put. That's what I was getting at too, /u/atilaneves.
&gt; I'm seriously considering allowing functions with multiple start nodes Sounds like a neat idea to me
[Wrapping is semiautomatic](https://github.com/Frege/frege-native-gen). You still have to specify all classes which aren't pure.
Thanks for the explanation. In the case of `replyStream` those bytestrings are the input to the parser. It was a way of abstracting away `IO` so I could use pre-canned bytestrings in the unit tests with actual packet byte values. I don't know if that means your advice to not use them in the signature still stands or not.
[free monads are really nice for doing api call interfaces](https://github.com/intolerable/reddit/blob/master/src/Reddit/Types/Reddit.hs#L43-L57)
I see. I don't think I understood that point up until now (and also with @bss03 and @mightybyte's comments). It still feels... wrong. But I understand what you mean now.
I absolutely disagree on your first point. You should by all means have entire suite of services in your build tree, at least all your haskell services which depend on each other. Even more, I recommend just moving to a "monorepo" model, where you have one repo with no submodules for your projects. It's just my advice, of course, it's not required to have that. As I mentioned in previous post, `stack build` can already notice if dependents changed and if it needs to rebuild them, so, by moving route-definitions to corresponded projects, it's quite possible to imagine a build-system that would only deploy those services which are affected by changes in the codebase. Once again, I can't tell all the details because I never did that in practice, just thinking out loud. Alternative approach is just defining some `.yaml`-file with cofiguration graph (or generating one from `.cabal`-files, and adding that to build-server to check which sources changed and what should be re-deployed because of those changes, but I think previous approach is a bit easier.
This (and the other comments referencing free/operational) seems to me to be the best solution from a let's say philosophical point of view, but at the cost of complicating the code.
You could do something like this: http://andyfriesen.com/2015/06/17/testable-io-in-haskell.html
`ghc` is optimized for state-like calculations (because that's also how `IO` is implemented internally). Usually you pay no performance penalty at all for using the `StateT` abstraction.
That would also work well, too.
Yeah, "doing IO" is a bit of jargon I guess. Some people say "the IO sin bin" to refer to all the different sorts of effects that are subsumed by the `IO`type.
So how do you do OOP in a functional style?
I think that is the right way to do it, but I think he wants the guarantees that IO is only being used for global mutable state, and not anything else.
Just having it automatically imported in ghci would probably go a long way. 
I like to call it Object Oriented Programming ;p
No, the similarity is purely superficial.
A lot depends on the specifics of your application, i.e. - is the new graph just the old graph but the node value have changed? or could the new graph look very different from the old one? - do you need to keep the old graph around after computing the new one? - is the graph regular (like a grid) or even embeddable in a regular structure? More details would help.
Glad to hear that! BTW, I decided to implement everything in Haskell and ditch the JVM for this project.
[removed]
At least the SBV test system is held up by a known bug: https://github.com/LeventErkok/sbv/issues/138#issuecomment-139804285
Yes indeed, you can use parallelism any time you have a function of multiple arguments (or, conversely, one which returns multiple values) - the real challenge then is how to extract the most *useful* amount of parallelism without getting so fine-grained that the overhead becomes dominant. I'm not actually seeing how unification, specifically, is relevant here.
It's a fair critique. In the end, and as usual, it depends on personal preference. I don't like any of those options as much as how I'm able to do it in other languages. Reasonable people may disagree. The option I'm disliking the least so far is free monads. But they're still somewhat complicated; I'd have trouble explaining them to most of my colleagues.
&gt; I'd have trouble explaining them to most of my colleagues. That may well be the case. However, I wonder if the main issue here is, rather than complexity, the fact that those solutions are, at least for now, somewhat unfamiliar to you and likely far more unfamiliar to your colleagues. 
The "once it's in `IO` it's stuck there" thing is kind of a misconception, BTW. You can have as much logic as you want in pure code, it just needs to take as inputs all the stuff that needs `IO`. The only code stuck in `IO` is the code that gets the inputs and then returns the result of feeding them to the real logic. After all, the type of `main` is `IO ()`. If *everything* used from `IO` was stuck in `IO` there'd be no such thing as pure code at all! Now, arranging to have a clean separation between the IO and non-IO bits of your logic can sometimes require a different--not necessarily more complicated, just different--architecture, of which the free monad business is one example.
And for 'Koalafications.'
If I'm not mistake this is similar to one of the oldest ideas in automatic parallelism: graph reduction. Here is a [1988 paper](http://comjnl.oxfordjournals.org/content/32/2/175.full.pdf) by SPJ on it.
I know. I needed some of the other primitives that those libraries didn't expose, and I made it more as a hobby project to get acquainted with stack and tasty. Should have mentioned those projects in the post though. 
I wouldn't call it OO, since it's pretty hard to dispatch on two different types in most OO languages (keys and nonces depend on the choice of the stream cipher). Most crypto libraries for OO languages only dispatch on a key type and allow the nonce to be an array, and then check the length of the array in the actual encryption method call.
I think it is a very well written chapter, especially with the example at the start showing the problem lenses solves!
&gt;These points you've raised are why I was surprised the author went with this solution for something crypto related but it may not matter. These are bindings to a C library, and all the performance sensitive parts are written in C, so the calling overhead does not matter much at all. Also, since we use 'unsafeDupablePerformIO' there are no inlining opportunities for the optimiser anyway. I also want to point out that sensitive data isn't stored as ByteStrings. I just simplified the code a bit, since the post really wasn't about cryptography.
&gt;These are bindings to a C library, and all the performance sensitive parts are written in C, so the calling overhead does not matter much at all. I figured, which is why I wasn't too fussed about it. &gt;Also, since we use 'unsafeDupablePerformIO' there are no inlining opportunities for the optimiser anyway. Yep. &gt;I also want to point out that sensitive data isn't stored as ByteStrings. ???
 &gt;&gt;I also want to point out that sensitive data isn't stored as ByteStrings. &gt; ??? Since you mentioned that some of my choices were weird for a cryptographic library I just wanted to point out that all secret data (except for messages) isn't stored as ByteStrings, instead I use a type similar to Vincent Hanquez SecureMem datatype.
Yes, but my language compiles to Haskell so I can't use graph reduction. That would be too slow. Haskell has its own machinery for evaluating expressions and I want to reuse that. I think a good approach would be to choose a good Haskell library for concurrency/parallelism and generate code which uses it. Why reinvent the wheel?
This style is *very* different from OO.
What you describe in sure would work, but it doesn't really fit the common understanding of microservices, and it doesn't really bring any of the benefits of them, either. Your build is big and complex, your deploy requires careful synchronisation across multiple targets, and your versioning is monolithic. You avoid the costs of microservices by avoiding the benefits :)
This post will help you discover that you *don't* know what type families are (even if you think you do).
"I know that I know nothing" (Socrates Plato Jones)
Which is what the OCaml [CryptoKit](https://forge.ocamlcore.org/projects/cryptokit/) library does. eg class type hash = object method add_substring: string -&gt; int -&gt; int -&gt; unit .... 
Check out the behavior of the following programs, variants of the program `notloop` linked. I made a handwritten `mapM` to see if ghc's new definition mattered. It does, slightly. And I used a not-quite-randomly-chosen streaming library. module Main (main) where import Data.IORef import Streaming -- cabal install streaming import qualified Streaming.Prelude as S import Control.Monad main = main2 list :: [Int] list = [1..10^7] main1 = mapM newIORef list &gt;&gt;= mapM readIORef &gt;&gt;= print . length main2 = mymapM newIORef list &gt;&gt;= mymapM readIORef &gt;&gt;= print . length where mymapM act = loop where loop [] = return [] loop (x:xs) = do a &lt;- act x as &lt;- loop xs return (a:as) main3 = do n &lt;- S.length $ S.mapM readIORef $ S.mapM newIORef (S.each list) print n With these I get $ time ./test2 -- new ghc 10000000 real 0m5.033s $ time ./test2 -- dumb handwritten mapM 10000000 real 0m5.464s $ time ./test2 -- streaming 10000000 real 0m1.771s if I increase the list to 10^8, the streaming program chugs along, taking 10 times as long; the others I have to stop from the 'activity monitor' while they are said to consume some gigabytes of memory.
A lot of folks nowadays are excited about extensible effects systems, where you can say something like "This function does network IO only" or "this function mutates global state." PureScript and Idris have this model. Haskell's `IO` monad is as much a historical artifact as it is a design decision.
A very insightful anti-tutorial. It is also well worth reading the comment to GHC ticket #9840 linked halfway through the post.
Are you using a `Data.Map` map? If so, an easy performance gain might be to switch to `Data.IntMap` (using `Int` for your node ids). This is based on a Patricia trie which, while it is a tree, gives asymptotics similar to or better than hashmaps (depending on what you're doing; constant factors may vary). This is an easy change and probably easier than using vectors. It might even perform better, depending on the exact operations you'd doing and how you do them. If you want a nicer interface to graphs than rolling your own, check out the [functional graph library](https://hackage.haskell.org/package/fgl) (`fgl`). It gives you an abstract graph type that you can pattern match and recurse on, as well as a suite of common graph operations as higher-order functions. Figuring out how to use it is a bit of a mental switch, but once you get it it's really cool. Tooting my own horn a bit, take a look at my [StackOverflow](http://stackoverflow.com/questions/30464163/functional-breadth-first-search/30469287#30469287) answer on the subject and a [blog post](http://jelv.is/blog/Generating-Mazes-with-Inductive-Graphs/) I wrote covering the core ideas. It's a good way to get a handle on what all of this means! Coincidentally, the default graph implementation for the `fgl` uses Patricia tries (in the form of `IntMap`) as well. Fundamentally it can't be faster than rolling your own code on top of a trie, but the existing operations in the `fgl` might be more optimized than what you want to do by hand. (I haven't stress tested the library at all, so I don't know how fast it is in practice.) So yeah, two pieces of advice: use `IntMap` instead of `Map` and consider exploring the `fgl` package.
What's wrong with his proposal? "The bottom line to all of this is that any partial type family should be associated with a class constraint." - sounds reasonable to me.
Nah, nbd. I just wanted to throw them in your direction as things to analyze.
When I say StateT is cheap I mean in comparison to explicit state passing
`Type Families: Fundeps In Disguise`
I'm not aware of any other ML compilers that do WPO. There is some work going into OCaml under the name [flambda](https://github.com/ocaml/ocaml/pull/132) which is looking at improving inlining. I've not been following that work particularly closely so I'm not sure about specific details. 
Sorry, I'm not sure I know what are lensRules. Can you explain?
You might get som inspiration from [this paper](http://www.cse.chalmers.se/~emax/documents/svenningsson2015combining.pdf). It's based on a more standard deep embedding using HOAS, but it has a nice story for lifting tuples and lambdas. In practice, this means automatic insertion of your `Zip` nodes and that constructs like your `Map` can be represented as simple terminal symbols in the AST; something like Map :: Expr ((a → b) → Arr a → Arr b) For example, the paper uses the following constructor to represent a while loop: While :: Expr ((s → Bool) → (s → s) → s → s) (Note that it's a terminal node with no arguments.) This symbol can be automatically lifted to an overloaded higher-order function: while :: Syntactic s ⇒ (s → Expr Bool) → (s → s) → s → s while = fromFunC While The `Syntactic` class includes Haskell tuples, and can easily be extended with new high-level types as shown in the paper. The technique also supports monad reification (mentioned elsewhere in this thread).
&gt;If i recall correctly, MLKit also eliminates all module expressions statically. This is possible to do universally due to ML's phase distinction—whether it is worth the expressivity cost is another question, I am not sure, particularly in light of Edwin Brady's (and others') work on erasure without phase distinctions. Ah thank you, another example. Elision of the phase distinction is inescapable if you're trying to make a proper dependently typed language but is that really intrinsic to the module systems in ML-family languages? I had the impression first-class modules were a newer thing that came in the early 2010s. Do you have any idea how mature the elimination is in Idris? I don't know how far Edwin's and Conor's past attempts got. &gt;This mlton strategy also seems to be incompatible with separate compilation Pretty sure it is as you say, in similar spirit to GHC Haskell's.
I like this radical proposal, since I have trouble with understanding the current type families. 
Oh, didn't realize you could point the stack root somewhere else - turns out you can! Will try with STACK_ROOT set out of the cache and copy things over only one way.
Thank you so much for taking the time. :) Also, I'm not sure, but I think the nested `case`s in the `parse` function could be replaced with `&gt;&gt;=` or something. I seem to remember, from reading LYAH a year back, that that kind of thing is almost the reason we use the bind operator. Someone on #haskell said that pop should be `Stack a -&gt; (Maybe a, Stack a)` instead, so that I can use `state`. (I don't really know how to yet.) Also, any tips on how I can stop having to look at the vector in a "raw" manner in `evalStep` and instead use `push` and `pop` to get the job done?
This is a rather toxic way to refer to another programming language that people have invested time and efforts in. I get the joke, but r/haskell strives (surprisingly successfully) to maintain a high conversation quality, and we can all do better than easy bashing.
I often use a similar encoding of GADT. See this gist: https://gist.github.com/jbgi/208a1733f15cdcf78eb5 You may also be interested in https://github.com/DanielGronau/highj/ for a (ugly because Java) encoding of type constructor polymorphism. edit: see also usage of GADT to encode closed-world "typeclasses": https://github.com/functionaljava/functionaljava/issues/126
I will definitely do that. 
Thanks for this great answer. I will look into both! 
That's the actual API that is exposed to the end user, maybe I wasn't clear enough in the post. So basically we have something like this: {-# LANGUAGE RecordWildCards #-} module Crypto.Sodium.Stream.Xsalsa20 ( keyBytes , nonceBytes , Key , Nonce , streamXor ) where import qualified Crypto.Sodium.Stream.Internal as S foreign import ccall unsafe "crypto_stream_xsalsa20_keybytes" c_crypto_stream_xsalsa20_keybytes :: CInt foreign import ccall unsafe "crypto_stream_xsalsa20_noncebytes" c_crypto_stream_xsalsa20_noncebytes :: CInt foreign import ccall unsafe "crypto_stream_xsalsa20_xor" c_crypto_stream_xsalsa20_xor :: S.XorFn data Xsalsa20 type Key = S.Key Xsalsa20 type Nonce = S.Nonce Xsalsa20 xsalsa20 :: StreamCipher Xsalsa20 xsalsa20 = S.mkStream c_crypto_stream_xsalsa20_keybytes c_crypto_stream_xsalsa20_noncebytes c_crypto_stream_xsalsa20_xor S.StreamCipher {..} = xsalsa20
I'm not the author of sodium, however most of Sodium is just a portable build environment written around NaCl which in turn is authored by Dan Bernstein who is a well known cryptographer and someone I have confidence in. The sodium documentation page is hosted on gitbooks with a custom domain name, so the SSL-certificate is gitbooks certificate (CN: *.gitbook.com) which of course doesn't match the CN doc.libsodium.org. If that leads to you not using the library, then I don't believe there's much I can do to convince you.
Just a side-note on the last comment: ``` ((_, x):_) -&gt; Just x ``` is entirely valid too. There are tradeoffs between the way you write it, but I thought I'd mention it.
I actually wanted them just for that bit which was unreadable just where one wants to read.
&gt; Pop a and b, apply liftedOp and push the result back I knew that is what I had to do, but I was lazy. Okay, brb, getting annoyed :P 
Tools and languages don't have emotions, I don't feel any guilt bashing subpar technologies. Tolerating them is a sublte form of malpractice. People have invested time and efforts into it, indeed. Take for example [Paul Philips](https://github.com/paulp), the main (ex)contributor to the Scala compiler ([proof](https://github.com/scala/scala/graphs/contributors)). He's not very fond of Scala either (https://www.youtube.com/watch?v=TS1lpKBMkgg). Scala is an abomination. &gt; people have invested time Sometimes people invest time into awful things. So what?
I had no current plans to use it anyway nor would I base such a decision on that small error alone...just an observation from clicking the first link in the blog.
What should I have used instead? Array? 
"Sacrifice" is probably too strong of a word. `MonoContainer` accomplishes everything that `Element` can; the downside is that you have to remember two types instead of one. For the sake of making type families saner, however, this might be an acceptable burden.
Just a regular list is good for your supported operations. We have O(1) modification and access at the front of a list, and consing a new element is also constant time. `Vector` and `Array` are both memory-contiguous, like most vector types in imperative languages, but in Haskell we can't mutate them, so modification involves copying the whole array. 
You can use a vector, and have `O(1)` ops (aside from update) by recasting your operations to use the slice operator, which merely creates a view over the top of the underlying. That said, linked lists are the classic way of modelling a stack, and unless you had conclusive benchmarks showing that slicing a vector was much faster, I would choose the linked list every time. EDIT: this is obviously total bullshit and I don't know why I wrote it. 
http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html
Will help you out when I get home.
 parse (x:xs) = do Num nx &lt;- parse [x] Num nxs &lt;- parse xs let pow n = 10 ^ length (show n) return $ Num (nx * pow nxs + nxs)
Well, to translate directly... ... other matches ... parse (x:xs) = do Num nx &lt;- parse [x] Num nxs &lt;- parse xs return (Num (nx * pow nxs + nxs)) where pow n = 10 ^ length (show n) parse _ = Nothing However there's a bunch of other things you could do, like just making use of the stuff which already exists to parse numbers: import Text.Read parse "+" = Just Add parse "-" = Just Sub parse "*" = Just Mul parse "/" = Just Div parse xs = fmap Num (readMaybe xs) Computing the length of the show of the rest of the string repeatedly at each step will result in quadratic time performance (in general, computing the lengths of lists is something you'd always rather avoid if possible). If you want to do this by hand, perhaps: import Control.Monad import Data.Char parseDigit :: Char -&gt; Maybe Int parseDigit x | isDigit x = Just (digitToInt x) | otherwise = Nothing parseNum :: String -&gt; Maybe Int parseNum "" = Nothing -- special case parseNum xs = foldl (\r x -&gt; liftM2 addDigit r (parseDigit x)) (Just 0) xs where addDigit r d = 10*r + d Here, liftM2 :: (Monad m) =&gt; (a -&gt; b -&gt; c) -&gt; m a -&gt; m b -&gt; m c is being used with m = Maybe to combine the possibly failing results from parseDigit and the remainder of the foldl.
It sounds like it, because free monads are used as one of the foundations of the approach proposed on the paper.
I guess /u/chrisdoner has demonstrated that they do: https://www.reddit.com/r/haskell/comments/3kwvlk/emulating_higher_order_modules_in_haskell/cv26wy1
what means " (__) " in your code? never seen underscores used like that
&gt; A lot of folks nowadays are excited about extensible effects systems, And most of them are still trash, and likely will always be since monads don't compose. I've fiddled with both Idris' and Purescript's effect systems, and I gotta say that I like transformers or mtl better still, either due to flexibility or capability.
stack already works just fine with the system installed GHC. In fact, it's default behavior is to use the GHC on the PATH and not install its own GHC unless explicitly requested.
AFAIK there's no released version of cabal that has that protection. Is my information out of date? In fact, last I checked, the SSL download improvements hadn't shipped either.
I believe the reason for the split is because of there are two ways of implementing these type classes: functional dependencies (as mtl does) and type families (as monads-tf does). By having the datatypes in a shared package, we end up with more code reuse (== less ecosystem fracturing), which is a good thing. There was definitely a time when it seemed like type families would replace most usages of functional dependencies, and therefore that monads-tf would be "the future," but IME mtl has won that battle. I'm not sure if this is the official reason for the split, hopefully someone more directly involved can comment.
It is worth pointing out (in case this is not obvious) that `transformers` is pure Haskell 98 while both `mtl` and `monads-tf` rely on language extensions.
Oh interresting, thanks!
Oh that really looks great! I am verry interested in crypto myself, so i think i will go through this soon
modulo a few details I just mentioned there, I agree.
- Parse a subset of markdown and generate HTML from it, then serve it over HTTP - Build a multiplayer (over the internet) tic-tac-toe
That sounds a lot like [dataflow parallelism](http://chimera.labs.oreilly.com/books/1230000000929/ch04.html) and the [`Par` monad](http://hackage.haskell.org/package/monad-par/docs/Control-Monad-Par.html). 
to me, the reason is that, while State is an instance of MonadState, it is just one of the many possible instances... many of which have nothing to do with State or StateT. Putting MonadState in transformers would be like putting Functor or Comonad in *containers*. I mean... Map is definitely a Functor, but it's only one instance out of many, and it's a little silly to have Functor be defined in a specific package when it really isn't fundamentally related to `Map` at all. 
&gt; You might not need any... You might not need a `.dir-locals.el` at all.
I think that's enough to qualify [Trac ticket #9238](https://ghc.haskell.org/trac/ghc/ticket/9238) for having its target changed to 7.10.3 to get it onto the status page, as per Austin's instructions in the linked post. So I did that. Except it's not "highest priority", as requested by Austin. But I see there is other stuff on the status page which isn't "highest priority", and I'm afraid to mess with that.
Heh. I just started on a project like that over the weekend :) The game is just a tetris clone for now, but I'm trying to hit a lot of useful tools and techniques as I go. I'm aiming at a pedagogical tool, but there should be some FRP in there as well.
Glad to see it is helping :) Do tell us if you find any part of the text confusing. 
Building on the comment about `head` and `tail`, you should also consider `headNote` ([Safe](https://hackage.haskell.org/package/safe-0.3.9/docs/Safe.html) library). `headNote` and similar functions are appropriate when you *know* that `head` will never be given an empty list. If it is given an empty list, the program will crash and you, the programmer, will fix the bug. The note you pass to `headNote` will help you understand why you thought the list would always be non-empty, and fix the bug. At least that's how I use it.
Good link - I personally like the section title "Proposal 1: Don't do that", but there's some good practical advice on alternative methods to avoid that hack. 
Just FYI, you can already achieve that with Telegram using telegram-cli ( https://github.com/vysheng/tg ). Maybe look the wiki on github for usage info. I invoke it from emacs, and just save the files ;)
Yes, it looks like it. That's a book I need to read, by the way.
Sorry, I wrote *unification* but I meant *dataflow variables*. I've never studied logic programming and I looked at Oz so many years ago...
&gt; Or are you thinking more they just report to a given server and the data is shared through that? Yep, that. I really don't want to have to bother with all the consensus issues/etc P2P might bring.
does that include using the system installed library packages?
&gt; abbreviatedFields i just did not understand that , I thought it was some some kind of configuration for generating lenses. As far as I know its defined as [lensRules](https://hackage.haskell.org/package/lens-4.13/docs/Control-Lens-TH.html#t:LensRules)
The OP's implementation of `pop` simply modifies the `top` pointer. `push` looks like it'll indeed be _O(n)_ though.
no worries :)
I don't think removing the file is the right approach. People can always use a command like `stack build --resolver lts-3.4`
Paulp has also called scala the least bad programming language. It isn't that he thinks it is way worse than everything else it's just that he thinks it has many objectively bad parts. 
`transformers` and `mtl` are part of the same thing. In the distant past we only had `mtl`, but the `mtl` wasn't Haskell 98, so Ross Paterson, the guy who was maintaining the `mtl` at the time, split the `mtl`: 1.) He made `transformers`, which was a Haskell-98 core with just the data types to satisfy those who didn't want to use extensions at all. 2.) He made `monads-fd` which contained just the classes, with a more pragmatic set of extensions. 3.) He also made `monads-tf` which exported modules with names that conflicted with `monads-fd` just in case the type families won the war against functional dependencies "enough" that the `mtl` API had to be abandoned. He seems to have originally intended everyone to switch from `mtl` to `monads-fd` or `monads-tf`, but with `mtl`, `monads-fd` and `monads-tf` all fighting for the same module names it was a bloodbath and nothing worked together! All of hackage split 3 ways for a while. Out of that carnage `mtl` 2 was born, which was basically `monads-fd` renamed and `monads-tf` was put out to pasture. The key here is that `mtl` 2 _re-exports_ `transformers`. If you write code with `transformers` it just works with `mtl`. If you want the extra instances you import the `mtl`. If you don't believe in the `mtl` philosophy about how to deal with functional dependencies you can import one of the various `mtl-unleashed` or effect system packages that try other approaches: `transformers` doesn't care. And if you want to have 100% fully Haskell 98/2010 compliant code with absolutely no extensions whatsoever you can import just `transformers`. So the package you are looking for is `mtl` if you want those instances. The fact that `transformers` exists at all can remain forever an afterthought to you. 
I obnoxiously tried `{and: []}` because I wanted to browse them all. `InternalError "Prelude.foldl1: empty list"` :). Seriously: really cool site. How can I browse them all?
Ross wanted a Haskell 98 core for a 'next generation' `mtl`, so he split off `transformers`, and made `monads-fd` and `monads-tf` to avoid taking a stance in the language war, but when he saw how bad it was for the community that there was now a 3-way split in 'where to get `Control.Monad.State` from, he relented and made `monads-fd` into the new `mtl`, and `monads-tf` kind of died. I took over `mtl` shortly thereafter.
And I do appreciate that, since I hadn't had a runtime error to test out Rollbar monitoring until then. :) Does http://topology.jdabbs.com/spaces work?
Fun related thought: [as far as I've been able to tell](http://redd.it/3edtnb), there's no technical reason for the 'let' in a do block either.
Alright I didn't know about that option, thanks.
You can program without the type classes, too. Then you can rely exclusively on `transformers`. This can improve type inference and error messages.
Hey there. I'm still the maintainer of ghc-mod, ask me stuff :3
Well, yes, it is a rule for generating lenses indeed, at least as I understand it, I just didn't know it's named lensRules. This concrete `abbreviatedFields` is what I use, because it will remove prefixes `_fv` and `_sv`, which default one wouldn't, and it will generate a class-based instances (`HasTime` and `HasDescriptionVal`), which is what I want.
``` translate' :&lt;|&gt; detect' :&lt;|&gt; getLanguages' = client api (BaseUrl Https "www.googleapis.com" 443) ``` I'm so confused... how is this _defining_ `translate'`, `detect'`, and `getLanguages'` all on one line, as a top level definition? Isn't that a syntax error?
First of all: thanks! Then I have a question as well: how does ghc-mod detect that a project is a stack project? 
https://github.com/kazu-yamamoto/ghc-mod/blob/master/Language/Haskell/GhcMod/Cradle.hs#L81 Starting from the file under consideration we look for cabal files, traversing the directory hierarchy upwards. When we find one we start looking for stack.yaml files stating from the containing directory and if we find that we assume we're in a stack project. If this doesn't go through we try again, looking for just the cabal file this time.
https://github.com/kazu-yamamoto/ghc-mod/wiki#readpackagedb-inappropriate-type
I hear people are working around that by installing ghc-mod into .stack-work in the project they want to use it in, not sure what stack incantation you need to make that happen though. 
Thanks a lot for the work. The Haskell experience with emacs is so nice because of your work! I had earlier reported the issue #583. With the new ghc-mod, I am hit with another error "you must run ghc-mod in the project directory as returned by ghc-mod root ... ". I will file an issue later when I get some time to play with it in detail.
I think you just need to update the elisp code to fix that.
Oh. I knew `:&lt;|&gt;` was an infix data constructor, but honestly, I didn't even know `(a,b) = (1,2)` compiles. I think I've been spending too much time in ghci.
Works like a charm now! Finally back to snappy and responsive ghc-mod :)
1. You might be better off installing binaries from `~`, so stack uses the implicit global `stack.yaml`, rather than from inside a stack project. 2. `ghc-mod` can't be built by stack yet (even with the latest nightly), but in the meantime, you can add `cabal-helper-0.6.0.0` to `stack.yaml` as an extra dep and it should build fine. Edit: I take it back, `ghc-mod` itself isn't on stackage either :)
I'm not eager to decide what he thinks, I only made an observational judgement (that he isn't very fond of Scala). That's already enough to substantiate my point.
That fixed it. Thanks.
there is already a thread about this [here](https://www.reddit.com/r/haskell/comments/3kyfrl/richard_eisenberg_what_are_type_families/)
No, the 'let' is not needed in 'do' blocks nor in list comprehensions. It was a conscious design decision to use 'let' in an attempt to make it more readable and less error prone.
The code https://github.com/dmjio/google-translate/blob/master/src/Web/Google/Translate.hs is composed of two parts. The first 200 lines are code, well documented and interesting to read, except for the cryptic json and servant operators which I was unaware of (neither google / hoogle, I had to find on hayoo). Why the hell is there so much operators in haskell... ;)) The next 300 lines (so more than half of the file) is composed with the association between language codes and types plus all the the associated instances (such as Show and fromJSON). Why did you choose to use this API instead of just passing the language code directly ? How to reduce the duplication in this file ? For each language, you have a data definition (for Lang), and instances of Show and fromJSON which links the type constructor to a language code, and an instance of fromJSON which links the language code to the type constructor. Is there a way to make this more generic, for example, creating a list of [(LangConstructor, String)] (Such as [(French, "fr"), (English, "en")]) and creating the Lang data and the associated Show and fromJSON instance ?
if you just released it today it should only be in the next nightly though, shouldn't it?
This is really a matter of taste, but personally, at least for Reader/Writer/State I think there's almost always a better (but application specific) thing to do than to use mtl's type classes. The mtl type classes can be convenient in some ways. You have these operations like get and put from MonadState which you can get implementations for via newtype deriving and lifting instances. However, these don't really make for a very expressive language of operations in your monad, and they sort of reveal something about its implementation (even though you certainly could implement a MonadState instance by hand). You'll notice though that not many things really want just any MonadState s instance polymorphically. Most interesting operations you can build with get and put involve something more application-specific. Same goes for Reader and Writer at least. Control.Monad has a good bunch of control-structure like algorithms for any monad, and more exist throughout the libraries. The mtl classes don't tend to buy you as much. If you're implementing a monad, usually you have better names for the operations which act on your state than "get" and "put" -- you may also want to do something structural to restrict which states you can be in, and not provide this unfettered access -- but even when you don't, it's pretty likely you have many parts of the state you'd like to have separate operations to manipulate. Well, okay, you might say, but what about being able to write things which are polymorphic in the choice of monad, like operations which work with any MonadState, so that we know they don't use IO operations, even though our monad really involves StateT over IO? The MonadFoo type classes don't have a monopoly on polymorphism though. As an example of what you might do, rather than having a MonadState Integer m constraint on something that needed a supply of unused Integer values, you might instead just accept an argument of type m Integer which would be used to obtain the next fresh value, and then you can fill that in later with an implementation using the StateT operations, or with an implementation that updates an IORef while logging the precise times at which it needed the fresh names to do some kind of benchmark, or whatever. If you have a whole bunch of operations like that, you bundle them together into a record data structure, or if there aren't too many separate implementations to worry about, just make a type class, with an instance for your concrete monad implementation. Personally, I think that monad transformers are probably the most abused technique in Haskell (and especially so in the presence of the mtl type classes). They have their place, but part of me wishes that we could somehow avoid teaching them to every beginner. (Sadly that's kind of impractical, because they get used so often now.) They're useful when you're constructing particular sorts of libraries to get something going quickly. They're rarely a 100% ideal way to implement any given type which happens to be a monad. They also seem to give a lot of people tunnel vision about how to implement various features and route data to where it's needed. If you're using ReaderT Foo IO a or a newtyped equivalent everywhere, you really have to ask yourself whether it's *really* better than just using Foo -&gt; IO a -- sure, you save passing a parameter around explicitly, but does that make up for all the liftIO you have to do? Does it make up for the contortions you need to put yourself through whenever you use anything higher order in IO, such as forkIO or catch? There is a place for it: if you can come up with a fixed set of operations such that you could just use those operations and almost never have to use lift/liftIO, then yeah, maybe go for it, especially with a newtype. If nobody's going to know you're even using monad transformers for sure, then I think you're on the right track. Otherwise, it might work, but I'm not sure it's usually the best plan.
The PR for the last version was merged 3 days ago but it still doesn't seem to be in the latest nightly, so I have no idea how long this version will take.
Something like https://gist.github.com/NicolasT/7ad255e9eca94be21315 could help: *Langs&gt; main The code for Dutch is "nl" The code for English is "en"
That message is not intended for you but rather your frontend's author. Which one are you using?
ghcmod-vim, from this repository: https://github.com/eagletmt/ghcmod-vim It works when vim's working directory is set to the project root, otherwise I get this error message. This wasn't the case before I updated to the new version.
I provide a step-by-step for this over here: https://www.reddit.com/r/haskell/comments/3kr6m3/update_i_got_spacemacs_working_using_stack_with/cuzzbz5
Interesting, Thank you. (I need a bit of time to understand it ;)
&gt;A radical proposal Just plain no. Unassociated type families are useful for type-level programming (like `++` and other operations on type lists, used by, for example, `vinyl`), or for types that are used with more than one type class (I'm using something like this in my own code). Don't break other people's toys just because they're confusing to you.
I'm *pretty sure* the author of this article understands type families better than almost anyone else, so if something is confusing to him that's probably a bad sign. Also, you might want to reread exactly what was proposed if you think it would cause problems for `vinyl`.
Okay, sorry, I guess I have to downvote myself then. If it's only for open and type (as opposed to data) families, then I guess it's fine. I still don't see the point of forbidding them, unless this would solve the looping problem biting people. 
Hmm, I just tested your code online [here](http://www.tutorialspoint.com/compile_haskell_online.php) and it seems to work fine for 1 2 3, 2 1 3, and 3 1 2 (it returns 2 in all cases). However, it breaks for 2 3 1, because you forgot the case "between z x y". If you add it, I think the code will be correct.
you are right, It seems to fail somewhere else. I see this output : *Solution&gt; middleNumber 1 2 3 2 *Solution&gt; middleNumber 2 1 3 2 *Solution&gt; middleNumber 3 2 1 2 where on middleNumber 2 1 3 I was expecting to see 1 because that is the number between the numbers 2 and 3 
maybe I misunderstood the exercise : This is stated in the book : Suppose we are asked to return the middle of three numbers. It is clear that given the numbers 2, 4 and 3 we should return 3, but when presented with 2, 4 and 2 there are two possible responses. • We could say that 2 is the middle number because when we write the numbers in order: 2 2 4, then 2 is the number that appears in the middle. • Alternatively we could say that there is no middle number in this case, since 2 is the lower and 4 the higher, and that we therefore cannot return any result. and later : Now, the problem comes in writing down the conditions, but here we say what if we had a function to do this. Let us call it between. It has three numbers as arguments, and a Boolean result, between : : Integer -&gt; Integer -&gt; Integer -&gt; Bool and is defined so that between m n p is True if n is between m and p. We can complete the definition of middleNumber now: middleNumber x y z I between y x z = x I between x y z = y I otherwise = z 
Here's a relevant [StackOverflow question](http://stackoverflow.com/questions/4369962/what-are-some-good-example-haskell-projects/). As far as picking a project for you to work on, I'd say the most important thing is that it be something that you have a lot of intrinsic motivation for. Motivation tends to trump pretty much everything else in this area. If you're also looking to build your reputation and visibility in the Haskell community, picking significant infrastructure projects is a pretty good approach. Things like improving [cabal](https://github.com/haskell/cabal), [hackage](https://github.com/haskell/hackage-server), GHC, [refactoring tools](http://hackage.haskell.org/package/HaRe), etc. This has the benefit of getting you reading other people's code and working with other Haskell programmers who are more experienced than you. I cannot overstate the value of this.
Yeah, it seems like the book wants you to implement the median (not sure why they call it the "middle number") and it wants a different implementation of `between` than the one you have. Basically `between` should return true if a&lt;b&lt;c or a&gt;b&gt;c, and you're only handling the first case. Or to make it a bit shorter: middleNumber :: Integer -&gt; Integer -&gt; Integer -&gt; Integer middleNumber x y z | between y x z = x | between x y z = y | otherwise = z between :: Integer -&gt; Integer -&gt; Integer -&gt; Bool between a b c = (a - b) * (b - c) &gt;= 0 You can try figuring out yourself why that definition of `between` works correctly. Hope that helps!
&gt; Require closed type families to be total, with an improved termination checker. Naturally, closed type families would be unable to call partial type families. While I'm generally a fan of totality checking, I don't really think it fits as a *requirement* in Haskell. Also, because of the immediate availability of a programmer, I'd say it's better for your type language (erased things) to be partial than it is for your term language (run-time present) to be partial. It's a horrible thing to encounter partiality (esp. non-termination) when no one is there to see it but the end-user. It's not so bad if the programmer is around. It's a bit annoying to have to debug someone else's library, but it's quite a bit better than an non-programmer end-user's experience of helplessness. In Haskell, we don't ask that (non-type-level) functions or case analysis be total. I think it inconsistent and slightly unreasonable to have our closed type families (type-level functions) have totality forced on them. I'm completely comfortable with -Wall printing a diagnostic on discovery of a non-total closed type family. I quite like the other 3 suggestions.
I haven't seen this posted in this reddit yet. They recently added a few more chapters to the book covering common typeclasses. It looks like more than half the chapters are now complete. Any opinions on this book? I read the first release and am about to read it again with the new chapters.
It does work, error reporting is much better IMO than other solutions. Type information is available for local bindings and basically any expression. Auto-completion is not implemented in stack-mode yet but it is provided by ide-backend. is If you have tried FPHC, the experience is very close to that with type information. The project is maturing, so if you use it as a daily driver you'll probably encounter some bugs. We need more testers and users though! 
I've been reading this book along with doing the CIS 194 course and I have to say I'm really enjoying it. It has gotten me out of a few tight spots with regards to understanding certain HW assignments. I've only gotten to chapter 8 so far but based on what I've read, I would recommend it to anyone interested in learning Haskell. 
So far I'm liking it, haven't looked at the new chapters since they came out yesterday but I'll have a look tonight. Even though I can use haskell reasonably well, its filled in a number of gaps and dings and helped sand over the rest of my understanding of certain aspects. That and the references to other papers to dig deeper have been quite helpful too. About my only comment is there are too many dog/cat references, but that is just me being super serial about things for no real reason. Obviously it should just be cat references.
My coauthor recently got a [pair of kittens](https://twitter.com/argumatronic/status/641889368144146433) btw :) 
any local subexpression? no way. two things if you don't mind: 1 are there setup instructions somewhere (with Emacs). 2 do I need to use stack, or is a cabal file good enough? thanks
Pattern matching newteam a@(Info p x y) = case p of PlayerA -&gt; Just a otherwise -&gt; Nothing or newteam a@(Info (PlayerA) x y) = Just a newteam _ = Nothing
thanks, but how would I check everything in the constructor at once, not just PlayerA? Like if playerA, playerB, or playerC are given
 case p of PlayerA -&gt; ... PlayerB -&gt; ... ... otherwise -&gt; ... -- to be safe newteam (Info (PlayerA) _ _) = ... newteam (Info (PlayerB) _ _) = ... ... newteam _ = ... -- to be safe
The example from vinyl you gave (the `++` operator) is a a total type family (meaning that it isn't partial). This is exactly the use case that Eisenberg upholds. Also, as the other comment indicated, Richard Eisenberg is an authority on GHC Haskell's type system. He is [expecting to take on most of the work for implementing dependent types in GHC](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell), so I would not dismiss his opinions as confusion.
Oh! My bad :)
If you mean algorithm-wise – well, it just drops everything until first uppercase letter, then lowercases that letter. That's it (except for edge cases when field has no uppercase letter, or doesn't begin with underscore, not sure what it does exactly).
I believe he's also a co-author on more than one paper about type families and implemented at least one of the recent type family-related extensions.
This is really more an encoding of the Curry bool type `type Bool ~ forall a. a -&gt; a -&gt; a` than of the `Maybe` type. Here your `Maybe(p, e1, e2, ..., en)` is more like (modulo issues of purity) iif :: (a -&gt; (a -&gt; a -&gt; a)) -&gt; (a -&gt; a -&gt; a) iif p x y = p x x y rasendubiMaybe :: (a -&gt; (a -&gt; a -&gt; a)) -&gt; [a] -&gt; a rasendubiMaybe p xs = (foldr (.) id . map (iif p)) xs (last xs) whereas `Maybe a ~ forall c. (a -&gt; c) -&gt; (() -&gt; c) -&gt; c`. 
I've been writing some plugins for [gitit](http://gitit.net/) and [Pandoc](http://pandoc.org/), and recently started expanding into the main codebases. They're good examples of functional design where you can add useful features without understanding the whole codebase at first. For example I changed [the Dot plugin](http://www.gitit.net/Dot%20Plugin%20Demo) to render SVGs with clickable links. It only took a couple lines. Also wrote a (badly implemented) CSV parser. Then when you get more comfortable there are a lot of practical details of integrating Haskell with other types of code via monad transformers, calls to the command-line, a built-in GHCI instance for running the plugins, etc). I also use it every day as a lab wiki/notebook.
The book has exercises but not answers, which is probably international but a little maddening.
Two of them, in fact: closed type families *and* injective type families (along with Jan Stolarek). He's a force of nature, for certain. :)
&gt;To be sure this isn't a Haskell only problem, lots of languages have this issue. It is the difference between knowing how to write a paragraph and how to write a book. Much of how we write the book is intended to address this problem, but if you're at this stage, you have to get through the middle (currently released) and later (coming soon) parts of the book before you'll see what I mean. The progression and coverage of topics in the book is designed to hit sticking points that are sufficiently fundamental that the knowledge is reusable across most/all Haskell projects. That's partly why the book is so long, we're going from virtually nothing to, "understand common Haskell libraries of your own accord without hand-holding". You may still need to read a tutorial to figure out how to use something, but the hope is that you won't feel like you're totally drowning either and don't have to make too big of a leap to 'get it'. It would be doing y'all a disservice to write one-off tutorials for each and every library in existence when we can equip you with the fundamentals required to figure it out for yourself. I can see why they wouldn't know we're doing that based on the sample, but if you look at the [progress page](http://haskellbook.com/progress.html) you'll get an idea of where we're headed. Reader gets its own chapter, in part because it's a common component of web frameworks and client libraries. Monad transformers get their own chapter because they're common to a _lot_ of Haskell projects and libraries. Parsers get a chapter because virtually all structuring of serialized data is basically parsing (this helps with understanding Aeson as well). These fundamentals we're trying to convey include higher level intuitions (there's a section on why Applicative functors are monoidal functors) and thinking in terms of FP was well, but it's with fairly specific pedagogical &amp; practical goals in mind.
My coauthor didn't know any Haskell before joining me to write this book and until she worked on this release (Monoid, Functor, Applicative, Monad) she didn't understand those things either. The book's chapters on these topics have intuitions, application (why would I want these? how would I use these?), and writing the instances yourselves as exercises. I understand these struggles, I've taught a lot of people Haskell. That experience is why I'm writing a book, because a lot of people struggled with the existing resources. [I've written about some of this before.](http://bitemyapp.com/posts/2014-12-31-functional-education.html)
I have to say that I appreciate your posts on this and I have a lot of respect for your work teaching people and writing this book. Your posts are the reason I went through (and benefitted from) the NICTA course. I am also planning to read your book when I get the chance: it sounds great. I was just commenting here on this "capable beginner" phase.
I also want to say that I fully agree with the "do the exercises and work through it" method of learning this stuff, which is another thing that interested me in the book.
Thank you for this and your other comment, this stuff makes our day (I'm passing it on to my coauthor as well) :)
The sample content is very good indeed. I can't see it with true beginner's eyes, but it seemed very thorough and smooth; it very seldom raised a question in my mind without immediately answering it. (The introduction of do notation was one exception). The exercises looked inviting and I can see they will help a lot with learning and self-assessment. (One other minor thing: a few concepts like $ and where/let were introduced in passing during the exercises without a heading of their own.) 
except for the detail that "Slack" (like Cabal) is not really a package manager
Make sure you don't have any tabs, only spaces.
Sure, though I'd say that this view of parallelism comes from thinking more about logic/dataflow variables as things that can be iteratively refined, rather than from unification per se. When I think of unification (per se; as opposed to thinking about logic variables, etc) I think more about generalized pattern matching and/or about combining multiple sources of information. I'm at the same university as Lindsey (was) and Ryan (is), and I've spent many hours chatting with them helping them figure out the semantics of "monotonicity" and how LVars fit into that :)
I haven't tested, but [`hFileSize`](http://hackage.haskell.org/package/base-4.7.0.1/docs/GHC-IO-Handle.html#t:Handle) looks like it might help. Just `System.IO.withFile path ReadMode hFileSize`.
Interesting, this may be it! Looking at [the source code](http://hackage.haskell.org/package/base-4.7.0.1/docs/src/GHC-IO-Handle.html#hFileSize) I am not totally certain what's going on. Is this definitely O(1) or is the `flushWriteBuffer` going to do anything weird? edit: It appears that unix-compat is using this approach (see [here](https://github.com/jystic/unix-compat/blob/master/src/System/PosixCompat/Files.hsc#L346-L348)) so that seems like "the way to go". Turns out this function [appears in `System.IO`](https://hackage.haskell.org/package/base-4.8.1.0/docs/System-IO.html#v:hFileSize) as well, so I guess it is not GHC specific either. Thanks for the suggestion!
I just tried it in GHC and python, both languages interpret tabs as 8 spaces. GHC also prints a warning about it (on a successful parse). So I don't see in which way GHC is being particular, as it's interpreting tabs the same way as other whitespace-sensitive languages. I guess you haven't used whitespace-sensitive languages before?
&gt;One other minor thing: a few concepts like $ and where/let were introduced in passing during the exercises without a heading of their own. That is probably intentional. Making smaller points in passing can be less boring than pausing to discuss every little thing, and you can always return to the topic at a later point on when extra detail needs to be added. Furthermore, it is a great reason for actually doing the exercises :) 
Ah, did not catch the #ifndef, thanks for the tip!
Actually, I think that this shouldn't be an extra functionality of tuples, but simply how they are defined. It is roughly analogous to their church encoding. In principle, `(x,y) const` and `(x,y) (flip const)` should yield `x` and `y` respectively. Syntactically, this should look like `const(x,y)` and `(flip const)(x,y)`.
Why I say particular is that I was using tabs on some parts of my code when I was indenting before guards in function definitions and it worked just fine, but in the "where" block it didn't. Thanks for making me aware of the cause of the issue though. Another curious thing is that whether I used the tab key or spacebar, the line of code began at the same point, yet only one way successfully compiled. I'm using Sublime Text, by the way. 
You may have had situations where the extra indentation from the tab-space mismatch resulted in a still-valid layout, e.g. you had tabs on every line, only one line was indented, etc. It's totally normal for the layout rule (what makes the indentation work) to be confusing for a while. It seems obvious enough at first, but... A little like the where-do-curly-braces-go question in C, you kind of end up with your personal/house style for indentation. (We also periodically have flamewars here over it.) By the way, you can use curly braces and semicolons in Haskell to totally eliminate the layout rule, also.
There are two authors. One of them has been learning Haskell (Julie) as she's been writing the book, one of them is me. I've been teaching Haskell for a couple years, using it at work for about a year. The book is _greatly_ improved by having a learner do much of the editing and writing with me. My experience teaching informs a lot of what I write in the book as well, although it's a different and less interactive format so there's more care involved. [Ever seen this guide before?](https://github.com/bitemyapp/learnhaskell)
Specifically, make sure you only use tabs or only use spaces to delineate scope. Personally, I only use tabs. For me, X tabs = X levels of scope nesting. Nothing else feels comfortable for me. It looks like you're aligning visually, however. In that case, definitely do not use tabs for reasons that have been covered.
Oh right! yes that was a mistake and its now been removed. Thanks for catching that :)
Use `ghc -Wall` to display warnings. If you are using ghc 7.8 or earlier you should probably add `-fwarn-tabs`.
I think you're jumping to conclusions regarding the motivation behind that change. I believe /u/snoyberg just wants to make using Haskell easier for new users. The fact is that regardless of your opinion of architectural choice, the user experience of `stack` is currently just better than that one of `cabal`. I'm not talking about "cabal hell" or whatever troubles with the functionality. I'm talking about simple things like having commands work out of the box instead of requiring multiple invocations, having things work automatically in simple cases and having configuration for complex cases, etc. I know many people also *want* this kind of thing for cabal, but it currently doesn't have it, which makes it less friendly for new users.
So you basically consider `stack` as something like `cabal` with training wheels (if we ignore the diametrically opposed architecture underneath)?
Lindsey told me that she will try to find time to write a blog post to answer this question.
As far as I know, stack doesn't install GHC automatically. Instead it will use the system one if available, and otherwise prompt you to install it with a flag. The problem with the cabal flow is pretty simple: how would a user know what to do? If I want to build, and issue `cabal build`, it should just install the dependencies if needed. Otherwise, users will (and do) start using `cabal install` instead, since it does do that. There's some effort towards doing this, e.g. `cabal configure` is implied in many scenarios, but it's not finished. Anyway, this is just my experience. I've used Haskell commercially for about 6 years now. We started with cabal, and had a lot of problems. We started using cabal-dev when it became available, and that made things better. I spent a lot of effort trying to get cabal sandboxes to work when they came out, but they are still lacking for multi-package projects, so we stayed on cabal-dev combined with freeze files. Now we're using stack, and everything just works. I'm no longer hesitant to have non-haskell devs install our haskell tools.
&gt; I guess it really depends on your prior experience before Haskell. I have been working a lot with apt-get and autotools (./configure &amp;&amp; make install) and cabal's workflow feels obvious and natural to me. I'm afraid I've done C++ development and I've done development in a few higher level languages (Java with maven and gradel, node with npm) and I'm afraid that the model of assuming dependencies are installed in a system white manner just seemed to cause more inconveniences and problems. It may have made more sense in the past before large hard disks and fast internet, but times have changed. I wouldn't want to go back. &gt;(I would have installed a different GHC compiler myself if I were given the opportunity) etc.. Honestly, the specific compiler version *is* a dependency. It's quite possible that something that worked with 7.8 isn't going to work with 7.6 or 7.10. I don't know why more build systems don't acknowledge this. Sure, I'm happy to use the system wide compiler on the $PATH for add-hoc purposes but when I want to start a project with a proper build system, I want that build system to be doing everything is can to make sure the build is reproducible, including managing the compiler version. &gt;magic blackbox To be honest, the cabal dependency solver always seemed like the black magic thing that required the sacrifice of a chicken. Before sandboxes, `rm -rf ~/.ghc ~/.cabal` was practically part of my new-project workflow. I feel like I understand how stack is picking it's dependencies. The versions picked are those in the stackage snapshot I specify unless I say otherwise for those one or two packages I actually care about the versions of.
Not for *all* users, unless you count me out. ;-)
&gt; how would a user know what to do? If I want to build, and issue cabal build, it should just install the dependencies if needed. Oh, but that's simple. Other tools suggest what actions you *probably* want to perform next. So rather than patronizing you they educate you. Stack follows a different philosophy here, which may well be what other users may be used to. I simply prefer a more transparent UI.
&gt; Honestly, the specific compiler version is a dependency. It's quite possible that something that worked with 7.8 isn't going to work with 7.6 or 7.10. I don't know why more build systems don't acknowledge this. Sure, I'm happy to use the system wide compiler on the $PATH for add-hoc purposes but when I want to start a project with a proper build system, I want that build system to be doing everything is can to make sure the build is reproducible, including managing the compiler version. This sounds like you want to get rid of classical Linux distribution packaging, and would be better served by something more advanced like NixOS, which seems to do exactly what you're asking for. Cabal &amp; Stack on the other hand aren't designed to install missing C/C++ compilers, other system tools, missing C/C++ libraries, but NixOS is able to do just that as it isn't limited to cabalized Haskell packages.
Thanks!
Why do you use IORef instead of FRP?
That would be great. Thank you!
Why is it relevant which of the two (or both) the third most popular choice after cabal and stack (the Haskell Platform) bundles? And why is it better to attempt to force the Haskell Platform to remain (some might argue become would be a better word here) relevant than to push either of those two tools?
\#richardeisenbergfanclub
I've not really tried NixOS. Maybe I should, sooner or later. It seems like a big step to take though.
FP101x videolectures; exercises and puzzles at every stage
you mean this one : https://www.edx.org/course/introduction-functional-programming-delftx-fp101x-0
Nice! I'll be linking this from haste-lang.org, unless you have any objections. You may want to point out that you're using Haste 0.4 by the way, anyone using the 0.5 branch will be unable to compile your code without a few modifications; mainly, the DOM and event handling libraries have gotten a bit of a facelift.
&gt; A tool for finding Unicode characters by how they look (=/&gt; for ⇏, etc) or a vague description (left right arrow for ↔, gbp for £) – this one is a project of mine[3] but I would love some help. You might find [Shapecatcher](http://shapecatcher.com/) useful.
&gt; I made a PR putting stack and the stack guide on the downloads page as the primary download, maintaining the other download methods speaking of misinformation, if we compare https://www.haskell.org/downloads to your [rendered PR](http://imgur.com/NmRxqGX.jpg) you have dropped a whole section describing Hackage and how to use `cabal` to install packages from Hackage. Maybe I got the wrong impression, but that seemed to me like you wanted to drive people away from `cabal`/Hackage to `stack`/Stackage. 
That wasn't the intention, I discussed it with the original author and we both agreed that such content about downloading packages didn't belong on a page about downloading tools, and with the increased options would lead to confusion. Instead of automatically attributing some ill will to my actions, just ask, I'm usually pretty good at ~~resounding~~ responding.
[Yeah, well, that's just, like, your opinion, man](https://youtu.be/pWdd6_ZxX8c) Do you have any statistics to back up your claim that the majority of the community is preferring Stack over cabal?
Cabal hell: "well, whoops, the solution is to occasionally rm -rf your Haskell install and try again. if you never accidentally install something outside of a sandbox, this may not happen again" Stack hell: "Something might be replacing cabal! Better complain on Reddit!"
I like headings because they - cue me that we're starting a new topic, which helps me judge and manage my rate of information intake - make it much easier to scan and find that topic later - make it possible to link to that topic 
&gt; Sadly they are the very kinds of extensions that are awkward to standardize. It uses MultiParamTypeClasses and FlexibleInstances and FunctionalDependencies critically, and then has to lean on UndecidableInstances as the fuel to make it go. Was the story any better for monads-tf? With the type families I would expect that to at least remove `MultiParamTypeClasses`.
Why not just run your computations in a `MonadError` (from `Control.Monad.Except`)? That way you can specify exactly what type your errors have (which is *much* better than using strings as you would have to with the old `fail` method), and the user can choose which instance of `MonadError` to run the computations in. Granted, the only instances that `mtl` provides are for `Either` and `ExceptT`, but the user can always make `Maybe` an instance if they want to. EDIT: `ExceptT`, not `EitherT`.
Frankly, i think lashing out at stack is not healthy to this community. I do not care about tools (whether it is stack or cabal). I care about my productivity. Today i use emacs (because of haskell-mode). Tomorrow if a similar in the range of features support would be added to some other editor, i will ditch emacs in a heartbeat. Lets not turn into tooling zealots. 
Free, good Haskell books: * Learn You A Haskell For Great Good * Real World Haskell
I'd be very interested in a link to this post about `MonadError` being bad if possible. And that's a good point about the orphan instance. The other option would be to apply `either (const Nothing) Just` to the `Either` value to convert it to a `Maybe`.
[This](http://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html) is probably what I was thinking of. It only mentions it in passing.
My take on this whole thing is that people who are wanting to use haskell in production need a tool like stack and a curated package list so that they do not get bogged down chasing silly things. There is a concern that this will cut off the "do the right thing, rather wait and do it properly" people. I actually think that both can coexist, and it is good to see alternate design choices
Can you state a specific example? One of stack's goals is to be explicit, and in fact it doesn't do things (like install ghc) without explicitly asking you if it wants to. &gt; No compiler found, expected minor version match with ghc-7.8 (x86_64) (based on resolver setting in /home/b/tmp/fuf/stack.yaml). Try running stack setup to locally install the correct GHC
Notice that was written in 2012 and criticises `ErrorT`, which is now deprecated in favour of `ExceptT`.
Can you share what exactly makes them good?
That's certainly not hijacking the thread. This thread is meant for discussion about stack and other tools, not about not hijacking (I think that grammar all worked...). It's a great idea, and perhaps a brainstorming session here would be a great way to get some of the ideas out. Maybe you'd like to kick it off with some questions?
I use Stack because it saves me time and grief at work. The people working on Cabal/cabal-install are great, but lets be honest - getting anything fixed/changed in cabal-install has one of two outcomes, typically. 1. No 2. Two years from now Stack does things properly out of the box, at least for working and hobbyist Haskellers, and things like build-caching and what the stack.yaml config can do _save me time_. If it seems like cabal-install has added stuff that would outweigh those advantages, I'll use it again. Sandboxes fixed most blocker issues around dependencies people had with Cabal/cabal-install, but there's not been much that has substantially improved what Cabal is like for end-users since then and that was *two years ago*. Stack has made leaps and bounds in a matter of a couple months.
Ah right. But does the complaint (against the Error class itself) not apply in the context of MonadError?
I started to not like stack when they denied to rename it to something less overloaded. It must be a bit unreasonable to feel like this, but the naming process of stack really felt a bit arrogant to me. ("That ship has sailed.") I'm currently using stack because it's useful, but when Cabal becomes usable enough in the future I'll immediately turn back to it. That's my two cents.
But this still leaves me having to pick for consumers which mechanism gets used to signal errors. Depending on what a user is doing, they may need the error message (Either) or they may only need to know a failure happened and not care further (Maybe or List).
Hi Simon. First of all: thanks for this. It's great. Is this per the proposal as I read it a long time ago, where you enable a pragma that makes all do-syntax desugar to Applicative rather than Monad? If yes, then do you see a path forward where we may have do-syntax mean Applicative *or* Monad, with either a) an intelligent way of inferring "this should probably desugar to Monad", and "this to Applicative", or b) a manual method of saying what each do desugars to (e.g. default is Monad, but via a signature you may denote Applicative)? What are your thoughts on this? (If no, i.e. it *does* allow mixing Monad- and Applicative-Do in the same file -- how on earth did you accomplish it? :)
Because the latter imposes on the user a choice of EitherT vs ExceptT and requires more dependencies.
I don't think the `MonadError` instance for `ExceptT` requires `Error`, so no.
If you the library author have a meaningful error message to communicate, then you should use Either. If your users need that in a Maybe context, then they can use the `hush` function from errors or use a simple `either (const Nothing) Just`. In short, it's up to you to make the right choice for your situation, and your users can use existing tools to handle their needs.
Because of the name? Hmm...ok. 
Why wouldn't it allow mixing? `Applicative` is now a superclass of `Monad` and `do` notation is already defined in terms of desugaring. My understanding (from the trac wiki and previous discussions of this idea) is that it attempts to use `pure`, `fmap`, and `(&lt;*&gt;)` as much as possible when desugaring and use `join` only when necessary. After that, the constraint is whatever it's inferred to be. If the desugaring didn't need `join` you get `Applicative`, if it did need `join` you get `Monad`. Incidentally, this also has a side benefit that `(&lt;*&gt;)` can often be implemented more efficiently than the `Monad` operations, so this may improve performance in some cases even if they still need a `Monad` constraint.
I have to say I also really don't like the naming of "go". It took very long time for it to show up the top of the Google search results, and it's still harder to find the articles about Go in Hacker News because many people still just use "Go", rather than "golang". (And the Hacker News search isn't great.) "stack haskell" doesn't help, because the concepts named "stack" already exist in Haskell. You'd better invent something like "stacktool", which is similar to "golang", though being much more obscure. I don't really want to deal with this additional complexity, that's why I generally don't like overloaded terms. Why not start with less overloaded terms when there is a chance? Anyways, I'm pretty sure we'll have to agree to disagree. If you're OK with a bit more disambiguation efforts, that's fine. But what I feel about the situation is quite different.
I like `stack`. I also liked `cabal` with sandboxes. So I won't criticize any tool, but I still must ask a question that bothers me in the `stack` environment and is related with your passage &gt; And as for "a third tool": new standards aren't usually a good idea. Why a second configuration file, `stack.yaml`? As far as I know, `stack` developers were hacking on `cabal` (library). Would it be better if instead of a new file, one would create a `stack` entry inside the `.cabal` specification and get rid off a second configuration file? (Asking the question I got the reason: to release `stack` quickly, you've decided to define your own configuration file instead of changing cabal. But can we expected this unification in the future?)
In theory `transformers` could more aggressively INLINE (or more sanely INLINEABLE) the definitions of its combinators. In practice, it'd have a heck of a time doing so without causing "simplifier ticks exhausted" _somewhere_ given the wide-flung set of use-sites for the MTL.
`transformers` works on every Haskell compiler that has ever existed since Haskell 98 became a thing. `mtl` works on a wide array of compilers as well. You can run it on old versions of `hugs`, `jhc`, `uhc`, old copies of `nhc` and `yhc` if you can get your hands on them, ancient versions of `ghc` from before the split of base, etc. The `TypeFamilies` extension only works on one compiler in the universe: `ghc` 6.10+. So, no, the `monads-tf` story isn't any better in terms of "reducing extensions" except by raw count. It definitely doesn't improve portability. Implementing `TypeFamilies` is a "big deal" in a compiler: it requires a rather specific kind of type checker. The other extensions mentioned above can live in a big tent, open to many strategies. From a usability standpoint, it also doesn't help that code written using `monads-tf` _always_ has longer signatures, so user code gets more verbose. On an unrelated note, I do think if `monads-tf` had chosen to use a different module space, e.g. `Control.Monad.Family.*` or something instead of colliding directly with the `mtl` it may have been able to win some more fans though.
It's actually by design that we have a separate config file, and even if I controlled all of the libraries/tools in question and had no backwards compatibility issues to deal with, I would have done it this way. The simple answer is: a .cabal file is a package-level metadata file, and a stack.yaml file is project-level. They contain slightly different things: stack.yaml is quite hard-coded to exact dependencies, and refers to multiple local packages. A .cabal file can be more forgiving by using version bounds. This situation has existed for a long time with tools like cabal-meta, where a separate sources.txt was always required, or with Stackage Nightly/LTS Haskell, where we'd use a cabal.config separate from the .cabal file itself. There's a lot of design philosophy that goes into all of this. Some of it is [described in the guide](https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md#stackyaml-vs-cabal-files). But I think this is the right decision, and does not represent a fracture between stack and cabal.
I think stack (like cabal-meta, cabal-dev, etc before it) should be considered a temporary stopgap to get us by until the [Cabal roadmap](https://www.youtube.com/watch?v=y3nB0kH1fxw) is implemented. That approach worked well then and there's no reason it can't work well again. But unfortunately stack has not been promoted that way...hence the well-justified fears of fracturing the community.
It is surprising how quickly and deeply a name gets baked into things, and consequently how hard it is to change it after the fact.
Hm. I would personally like such a hackage feature. But since the PVP is kind of an controversial thing, I doubt the maintainers would like to perform such a 'power-move' :)
I don't think you read what I wrote. In order to configure multi-package projects with cabal sandbox, you need to make sure to call `cabal sandbox init` with the right arguments and do the right order of `cabal sandbox add-source` calls. I've seen multiple failure cases for that in companies. There's no such failure case in stack, since the multi-package configuration is a file. cabal freeze files certainly help, but they help with a different problem. And even there I can point out some shortcomings relatively to stack (namely, it's easy to accidentally leave out a dependency depending on build flags or OS, and then have unstable builds).
While we're at it (and it's a great idea host this Stack QA session!): Will Stack join forces with the [Hackage Security](http://www.well-typed.com/blog/2015/08/hackage-security-beta/) effort or keep doing its own thing?
Is there any public record of the died conversations you refer to?
Stack, for me, works without issue. It does what I expect it to do and I don't have to think about it. It does its job and gets out of the way so I can stay in the flow of what I was focused on in the first place. I like that, and I love Stackage. I love Hackage too, but I also like Stackage. We're already pretty idealistic when it comes to our choice of language. I don't think we can afford to do the same for our tools.
Obviously you can find worse naming examples than Stack but that doesn't make Stack's naming better.
What you just described (Kleisli composition) is sufficient to implement (&gt;&gt;=). That is the thing that gives us a `Monad` rather than just an `Applicative` Indeed it is sufficient to handle do b &lt;- f a g a today. =)
Fair enough thanks for the reply. I can totally appreciate the need to show the way versus sometimes fight political/human battles. My reasoning for the "third tool" option is really more to suggest an analog to ghci-ng here. Where if cabal the tool wants to take a wait and see attitude that is fine, but it would allow both cabal and stacks sets of features to be mixed together without breaking anything that isn't fully cooked. So my third tool suggestion is more in alignment with the future in that I would like to see both camps of tools to at least come up with a plan to address feature sets with working examples of how to do it. Then debate/bike shedding can start, but it will be useable at least.
I hope you can forgive me for perceiving some tones of unhappiness with the PVP as stack has been positioned against cabal-install primarily in terms of "preemptive upper bounds". Defaulting to stackage also puts stack-hosted libraries into a controlled environment that isn't exposed to the world of moving versions and breaking changes, where they are forced (by design) to use upper bounds for managing their interaction with the open community. Let's not try to agree about whether upper bounds are a good or bad thing. People will continue to have very strong differing opinions on this. Instead, now you have direct control over stack. Can you find a way to use that to make both camps happy? That's the real test :) That would be a killer feature.
I've discussed "making both camps happy" with a number of people in the past. As I mentioned above, I think tooling is the answer, namely by adding in missing upper bounds that authors haven't set themselves. I did initial work on that in stackage.org as a proof of concept, but there's was never any interest in it. So I stopped pursuing that course of action. My unhappiness with the PVP is mostly just an empirical observation: relying exclusively on the PVP doesn't work today. You can blame authors (myself included) for that, blame tooling, or blame anything else. I'm just making the simple observation: many users hit dependency problems when relying on PVP and dependency solving, and don't with Stackage/curation. My goal is help people adopt Haskell, which is why I've pushed a curation solution. If people figure out a way to make PVP/dependency solving work flawlessly, I'll be thrilled. I just don't want to sit and wait for it.
I don't disagree with that prediction - that an unenforced policy would end up being useless. But for whatever reason, the results have been quite good! I put tight bounds on all my dependencies, and have had very reproducible builds. Testament to the good-will of the community, I guess, that we follow the speed limit without needing to be hounded by the police :)
Social pressure is a loaded term. I might call it 'best practices' instead. But that's also a loaded term. We can have discussions about best practices and also work on tooling - no conflict between the two. Finding a non-adversarial way to promote curation-based *and* open-world package management may be the primary open research question of the programming language sociology field.
You may not be understanding my point. I think Stack is mature enough to _use_, but I'm not going to chuck people into it until I've run at least a couple more learners through it, noted the sticking points, and then written about them. The current alternative in recommended in the downloads page (Platform) is probably the worst option after "show them how to write makefiles and call ghc with `-I`".
I'll put it this way: the effort can certainly continue, but I'm not planning on spearheading it, I've wasted enough cycles on it already. If someone else wants to try tackling the problem, I'll be happy to participate.
As I see it, the main complain about stack is specifically that it removes a social pressure (or, if you prefer, incentive): end users will no longer be complaining to you about your package missing an upper bound. I'm unaware of any other real complaint about Stackage or stack when it comes to PVP compliance.
As you do these learner tests, please provide feedback to us (as you've been great about doing in the past)! Any improvements that can be made, should be made.
I generally agree. The desired outcome of the radical suggestion above is, as I see it, aligned with such a stance: it would minimise the risk of breakage when using cabal-install by restricting it to packages with PVP-compliant upper bounds by default. That would be, to a limited extent, analogous to what stack does by restricting itself to a specific Stackage snapshot by default. That's not to say automated enforcement of PVP bounds at Hackage wouldn't bring its own complications, such as the ones I alluded to [in the parallel subthread](https://www.reddit.com/r/haskell/comments/3lax6y/discussion_thread_about_stack/cv4wj7a).
This is a fair concern, I think. We didn't give the dollar sign its own heading because it didn't seem at the time to warrant it. `let` and `where` do have their own subheading, but you're correct that it's within the chapter exercises so it's not as easy to find for reference. I am considering ways I could restructure that section so that they have an easier-to-find heading. Thank you for this. Cheers. :) 
Some of the notes in the mailing list were added to [this wiki page](https://wiki.haskell.org/Google_summer_of_code#Accepted_GSOC2015_projects). EDIT: I also cleaned up the overview because it was annoying me.
Please, this comment is absolutely ridiculous. You implied something negative about the way we developed stack. I explained that this is because upstream contributions were not accepted. Our options were to remain blocked on upstream, or create our own project. After a lot of debate, we did the latter. I'm only now, in this thread, for the first time even mentioning that upstream integration problems were a cause of this, and only because you prompted it.
Out of curiosity, is there a reason you started an entirely new project rather than use the submitted contributions and fork cabal?
Yes: it was far easier. As a team at FP Complete, we already had experience putting together build systems for customers on multiple occasions, and in every case it turned out to be easier to start from scratch against the Cabal library instead of modify the cabal-install codebase. We could get into lots of technical discussions about what led to that. But one simple point is that there are a few fundamental design goals in stack - the most important being reproducible builds - from which a lot of the design of the system fall out from. cabal-install doesn't follow those principles, and therefore hasn't been designed to meet those goals.
Learn You A Haskell uses catchy, relatable examples and goofy clipart to maintain a casual atmosphere, bringing novices even unto *programming itself* into the fold. If you enjoyed Why's (Poignant) Guide to Ruby, you might like this. Real World Haskell draws cross-language parallels and builds *complete* examples for experienced programmers who want to convert existing paradigms to functional programming paradigms. If you enjoyed Practical Common Lisp, you might like this. Also, shit's free.
:) Here's the problem with emacs. Try to suggest it to your team of windows devs. Feel the temperature in the room suddenly drop a few degrees. 
Will do. Onboarding my coauthor soon so there'll be at least that bit of data :)
Figured it was something like that, thanks. Forking cabal would have made it easier to take a stance of "we think this is the right way, and hope to eventually re-merge with upstream cabal" but to be honest I don't think there would have been much (if any) less community strife.
&gt;You do know that Amazon, Google, and Netflix are Java shops? So what? Amazon is the kind of company that will achieve their goals no matter what they're using (and they achieve them with brute force, not elegance or efficiency). Google is one of the most overhyped companies ever. They've had one or two successes and bought everything else. Netflix I don't know much about, but I don't see what them using Java would prove. &gt;Not even close You're using a much more involved benchmark. I was talking about the original Teepeedee challenge where the server just returned some hard coded text (i.e. it tested literally nothing but the server). Teepeedee (which was written in some horrendous Common Lisp) was the fastest for a while. Then some C++ server became faster. I believe Warp held the crown briefly as well.
One way to handle that issue without changing the flow of the text might be through a back-of-the book index that included keywords and important functions. (I don't know if that is already in your plans, or how much trouble it would be setting that up for a WIP, regularly updated book.)
I could try making one again (we've been indexing, albeit not perfectly). It's just that the last couple months were a _crazy_ sprint for us so I was focused on writing &amp; teaching rather than getting a useful index in the render. I'll consider this a strong vote in favor of getting it in the next release :)
I'm no beginner and I work faster with Stack. Please be more constructive.
Modern Java is actually pretty sweet. It has types, you can make stuff immutable, awesome ide support. You can code in a functional-OOP style (object construction is just partial application, if you squint!) and write lots of tests and it's mostly good! I may only be saying as I've come off a 2 year stint on a javascript project, which is hell on earth in comparison. Java may seem bad but once you start using code generation libraries like autovalue/immutables/dagger2 to get rid of the boilerplate it's OK. Not as good as Haskell but nowhere near as bad as it could be. This is where I've arrived at after experiencing the same feelings you describe towards java about 2 years ago. Obviously if you are working with old style mutation everywhere no tests AbstractFactoryBeanServiceProviderBeanFactory java then it's a different matter. 
I've been working on a technique using a form of lens that can look into a monad transformer stack to let me run computations written for particular concrete monad transformer combinations in a larger monad transformer. This would let code be written 'generically' but still in a way that runs quickly. This isn't an `mtl` thing so much as a thing that can be written on top.
good for you, but I care about tools because I care about productivity. if some proprietary version of Emacs (that is miraculously more extensible despite costing money and being closed) comes out, I admit that I'll reluctantly use it at least part time. but I won't happen, because free/open isn't just about money/philosophy, it's also often a matter of quality. more security, more extensibility, blah blah blah. having said that, stack is open source (right?), and developed by a company staffed by frequent open source contributors. so I'm not worried yet at all. but paranoia isn't.
OK, fair enough. I wonder what use it is to have a separate 'configure' and 'build', though.
I mildly disagree. I've read a lot of people here say that stack is what finally got their teammates to try haskell. I think recommending it is good enough for now, being better than the haskell platform. relatedly, I'm someone who never experienced cabal hell (post sandboxes) till last week, and still uses cabal because it's great. I had (with explicit upper and lower version bounds, of course possibly subtly incorrect) # in sandbox $ cabal install --dependencies-only package-XYZ failed $ cabal install package-XYZ package-XYZ succeeded $ cabal install --dependencies-only all packages already installed I think cabal hell is too strong a word, but "cabal non-reproducibility" still exists. 
Cabal Hell is something a bit more specific and different from this, not to mention much more prolific when it was an ongoing issue, but I take your point. Please do not mistake my original comment for believing Cabal+sandboxes was perfect. I do almost exclusively use Stack now for a reason. 9/10 of the issues I saw post-sandboxes were down to: 1. Users forgetting to use a sandbox because the tool doesn't tell them to 2. Weird unreproducible issues like this So I do sympathize.
As an addendum to the other comments, using `(&lt;*&gt;)` and `join` instead of `(&gt;&gt;=)` means retaining the possibility of being more efficient.
In some recent discussions I've read about stack, I've seen a point that stack is obviously better than cabal-install, apparently, for all existing use cases, and for experts and novices alike. Here's one use case when stack doesn't work great: if someone wants to learn Haskell by building animations / UI apps with some high-level library. Neither gloss nor threepenny-gui are available on stackage. Last time I tried to use libraries outside of stackage with stack resulted in "stack build" spitting a bunch of lines I supposed to add to stack.yaml (manually!). When I did add them "stack build" spit out a bunch of more lines, and after couple of more iterations of that loop it finally refused to build anything at all, and I'm not even mentioning the fact that if you're using external libraries in the app you're building you have to add them both to stack.yaml and .cabal file. I'm not sure that's supposed to be beginner-friendly behaviour. (none of this seems like inherent limitation of stack, so just consider that to be semi-constructive feedback on how to make it better for some use cases)
my friend said he took a look and only things with "brief/simple constructions" were included. if anything, as a dilettante, I'd think undergraduate algebra would be the easiest to do this for.
In those cases, there's a fallback of using cabal as a dependency solver (`stack init --solver`). It's usually not necessary, and adding those lines (yes, manually) to stack.yaml will fix it. If you really hate manual changes, you can also try `stack solver --modify-stack-yaml`. I agree, the user story for packages outside of snapshots isn't nearly as nice as for packages within snapshots. I'd argue that stack is still the best tool for that situation since it will provide reproducibility once you've selected a build plan.
&gt; What you just described (Kleisli composition) is sufficient to implement (&gt;&gt;=). That is a Monad. Right, in the general case. I suppose the trick is to be able to restrict what can happen downstream of each 'statement' in a `do` block, so that only the right sort of composition is allowed. (And this in turn means that we may need something like `proc` syntax after all, because anything else could lead to ambiguity. Even though it's obviously not needed for either simple `Applicative` 's or in the case of actual Kleisli arrows.)
Firstly, *citation needed*. You keep saying this. And I keep using - and seeing others using - stack on projects both large and small. Have you, I don't know, actually *tried* stack? And anyway: who cares? Your argument is a complete non-sequitor. The fact remains that getting stack from 0 to where we need it today was cheaper than paying others to fix the cabal problems that exist. You making baseless claims about some other use cases doesn't effect that.
If only stack made it easy to jump outside curated collections! /s
The level of automated refactoring in the java ecosystem is incredible. I wish we had the same for haskell/many other languages. 
&gt; Firstly, citation needed. Um, a solver? Because stack defaults to curated collections it has completely ignored the dependency solving problem. It has no ability to make a package work with a range of versions outside what cabal-install already provides by way of its solver. This reduced scope is clearly demonstrated by the problems described by [this comment elsewhere in this thread](https://www.reddit.com/r/haskell/comments/3lax6y/discussion_thread_about_stack/cv52aei). You know this as well as I do, so why not just admit it? There's nothing wrong with attacking a reduced scope. It just means that you can't make apples-to-apples cost comparisons between what you're doing and what the bigger, more comprehensive solution is doing. Anyway, I'm done with this discussion. I'm not interested in having a discussion with people who just misinterpret and dismiss what I say at every turn. EDIT: [Citation](https://github.com/commercialhaskell/stack/issues/116#issuecomment-131542648)
I'm not that pessimistic - stack already can handle hackage dependencies, the problem is coming up a good ui for that scenario
&gt; I'd argue that stack is still the best tool for that situation since it will provide reproducibility once you've selected a build plan. *cough* `cabal freeze` *cough*
Yup. I now view and create any program as a bunch of data transformations, and it *really* helps with debugging, code reuse and readability.
&gt; stack already can handle hackage dependencies how does it do that?
Concerning JS, you might appreciate RamdaJS.
I'm very glad that stack exists. You at FP Complete have done a lot of great work at pushing this language forward and making it a candidate for production use. Stack is still at the point where I'd hesitate to drop it on a new person whom I was trying to teach Haskell, but as the early issues get worked out, I imagine that that will change. The user experience of Haskell has definitely improved over the past couple years and I'm glad to see this work continuing. One minor request: I'd like to see stack get Docker support for OS X. I'm working with a team of data scientists who are going to stick to OS X, and when I found out that HaskellR required stack's Docker support, I found out that that's only available on Linux machines. 
You may add hackage dependencies by specifying a package and version to as an entry in the extra-deps field. stack also supports pointing to arbitrary git commits. See my `stack.yaml` response to the GP.
it's glorious. and even easier to do in Haskell. the richer the static types, the more newtypes and non-empty lists and stuff you use, the more you know at build time. 
That's a great question, certainly it can be a bit of black art. I find that in most cases, I have to take maybe a few steps outside stackage to compile most projects. My brain is able to run the dependency solver for taking those few steps outside stackage with much better results than I ever had with cabal's solver. Let me put it this way, before using stack I had pretty much given up on compiling a large amount of packages. I've been on 'sales' calls where I ask the person to name a package that he's never been able to compile with cabal, to which I usually get something obscure outside of stackage like your `gloss` and `threepenny` example, in usually under a minute under pressure I've been able to get every example compiling with stack. Sorry about the non-scientific answer, I would be happy to hear someone explain that cabal install gloss and three-penny in a shared sandbox is a one-shot no thinking kind of deal. Usually I would try this things, but I've all but given up on cabal these days.
&gt; If you are unhappy with the pace of development, why don't you consider paying Well-Typed or other core Cabal contributors to work on features that you think are important? Slight tangent, but is this really the policy for how Cabal is run as an open source project? We're talking about paying someone to merge donated code?
yeah, I was writing haskell in Emacs, with only syntax highlighting and make, for months. it's the gargantuan potential that makes me sniffle. of course, someone needs to do it, and I to be involved a little when I can. but better support from the GHC API (like a type error ADT, rather than ad-hoc pretty printers) would make the work of dozens of developers much easier. 
I find that Java's functional support is poor at best. While you can do functional-like things (maps and folds via stream api, lambdas, etc.) mutability in Java makes this incredibly difficult to reason about the safety of your computations.
I think that stack has already demonstrated that better and simpler workflows are possible and many other nice things such as reuse of binaries between sandboxes. Many such things can trickle down to cabal-install if people care enough. Currently, I consider cabal-install a tool for advanced users only (i.e., people on bleeding edge). Only thing holding me back to cabal was lack of ghc-mod support, which has since been implemented. Amazing!
That's right. I gave Emacs many tries. I used it for periods of time back to my undergrad times. It never stuck with me. Currently, on Windows, I use Atom and it's spectacular, with improvements/fixes every other day. Atom with haskell-ide provides by far the most modern programming experience to me (FYI, I ditched Sublime for Atom...).
You're right. It uses `join` (although that is equivalent)
The issue with `MonadError` is the `Error` constraint on the `Monad` instance, which prevents you from storing other types of values in the "left" type parameter. `ExceptT` fixes the problem (although I still dislike the name). Oops: I meant `ErrorT`, not MonadError`
The explanations are both good and comprehensive. There are a lot of exercises that are useful for ensuring that you know the material well. The authors are testing the material on beginning Haskellers and refining the materials to ensure that beginners actually understand it.
Fine, so let me put it in another way. I do agree we should be wary of for-profit entities using embrace-extend-extinguish tactics. However, given the known facts about FPCo and stack, Occam's razor leads me to choose the "FPCo believes it can improve the Haskell tooling situation by releasing a new tool" explanation over the "FPCo is preparing a hostile takeover" one. From that stance, "I suspect they will eventually attempt to take over GHC itself to give themselves complete control of the ecosystem" can only be considered an example of FUD.
Note I said functional-OOP style not functional ;) 
Sure, the more complicated refactors are easier in haskell. But the simple ones like renaming a function or moving a function to a new package involves so much manual work :/
I switched over to Stack and have not looked back. For every workflow I do, it's hands down better than cabal-install. That's not to say that I had much trouble with cabal-install. I did waste a lot of time recompiling packages though. In particular, starting from scratch is so much nicer with Stack. It goes like this: (1) download Stack, (2) `stack build --install-ghc`, (3) there is no step three. And if that works today, it will continue to work into the future. 
The facts that I see indicate that FP Complete has been on a steady march to try to centralize the Haskell community around themselves. Since coming on the scene they have: 1. hired or attempted to hire key developers from the community 2. attempted to create the #1 Haskell editor (initially closed source and for $ only) 3. collected as much community generated tutorials as possible from community run sites onto their proprietary platform 4. made a multiple pronged effort to switch the community from cabal-install/hackage to stack/stackage It seems to me that if they controlled GHC as well, that would only make it easier for them to add the features they need to make their tooling better and would also give them significantly more credibility and authority. A repeated pattern we can see is that in everything they have done, they have attempted to move focus away from a community controlled project or resource to something they personally control. I am not convinced that fp complete is evil -- I'm just not yet convinced they aren't :) 
Also see: [Cleanest way to report errors in Haskell](http://programmers.stackexchange.com/q/252977/94478) from SO.
I agree the name could be different, but then it is quite clear what stack setup does: $ stack setup --help Usage: stack setup [GHC_VERSION] [--reinstall] [--upgrade-cabal] [--stack-setup-yaml ARG] Get the appropriate GHC for your project ... Your question "It seems that stack doesn't support sandboxes yet" indicates that you have little idea what stack is and how it works. Further, you are trying commands blindly ("stack sandbox init") and being surprised when they don't work. That's all quite natural, but it does not indicate a familiarity with the command line, where utilities require a bit of up-front work to be understood properly. This may be why others are not finding your comment constructive.
One thing we should do here is investigate ways to use `(&lt;*)` or `(*&gt;)` in the desugaring when the result isn't used, not just `(&lt;*&gt;)`. For many applicatives they can be much more efficient.
was a ticket filed upstream?
I suspect that duplode was pointing out the irony of you objecting to FUD-spreading in the middle of your FUD-spreading post.
Yup. I was fully aware of that when I posted and almost commented on it myself in the original message. While I do see the irony, I do not think it makes my feelings invalid. I do have fear, uncertainty, and doubt about what FP Complete's intentions for Haskell and I am curious why other's don't. Perhaps I am just uninformed...
&gt; A repeated pattern we can see is that in everything they have done, they have attempted to move focus away from a community controlled project or resource to something they personally control. Except, you know, Haskell. And GHC.
I took the FP101x course and a lot of the exercises were lifted directly from Graham Hutton's _Programming in Haskell_ book. 
this sounds like something you should report to the hackage trustees
That assumes there is a strong enough community still around to fork it and that there are not too many other vendor lockins in place. The tool itself is useless with out the community and infrastructure around it. Anyone can fork the reddit source code, but its unlikely they could steal reddit's userbase. If FP Complete is maintaining the most popular fork of GHC, providing the training materials, curating the package lists, providing the editor, controls the bug trackers, etc -- then I think there is little chance of a competing stackage taking hold. Now -- if they are doing a wonderful job -- then that could be great. Haskell has many shortcomings which could be solved by paying people to work on them. But if we had the keys to the kingdom to FPC and they opt to make their private investors who want short term returns now happy instead of doing what is best for Haskell in the long run, then I will be sad. FPC is no accountability to the opensource community, and legal responsibility to their shareholders. According to their website, "FP Complete is dedicated to bringing the Haskell programming language into the mainstream software market by being the leading developer of **commercial Haskell software tools and services.**" FP Complete is not primarily about building a strong open-source community, but about making money from selling Haskell. 
cabal freeze doesn't and can't solve the biggest problem we had: systems with different GHC versions esp. if devs worked on multiple projects that used different GHC versions.
Even if that were true the best solution would be to compete and provide high quality alternatives. You can't tell people to not use something without providing a suitable replacement.
Try my turtle library if you have time, which has a pretty extensive tutorial aimed at Haskell newcomers: https://hackage.haskell.org/package/turtle-1.2.1/docs/Turtle-Tutorial.html Turtle is a scripting library designed to provide the nice scripting amenities that you would expect from Ruby or Python with very lightweight syntax.
Could it be worth making this sticky, for a while at least? We get two now, and we're not using either! :)
I confess I'm slightly reluctant to do that for fear of making this whole debate seem more important than it needs to be. :P I'm still not convinced that either "living with cabal forever" or "everyone starts using stack" are the quasi-apocalyptic scenarios they're made out to be.
yeah, that worked. so you have to install the source, as well as adding it. sorry for wasting your time. is there a breakdown somewhere of exactly what steps each command (at least configure, build, install) does, at a high level? 
That is certainly the hope. The ideal case is that a commercial company is able to walk the line between making money for their investors, making their customers happy, and keeping the Haskell open-source community vibrant. Even better is that multiple companies are able to do so. But, what if they can't. What if they start selling ads on stackage so they can keep the lights on? What if they consolidate GHC and stackage development around themselves and then fold up shop leaving a vacuum in the community? What if they decide to focus their efforts on extracting as much equity as they can in the short term with no concern for the long-term health of the community? What is FP Complete's exit plan? Right now these concerns do not seem like a big deal because we still have a healthy community. But what happens in the long run if we become dependent on a single corporate sponsor? In fact, we do already have this problem -- we are dependent on Microsoft Research. Thus far they have been good to us. But eventually SPJ will retire. Having a new corporate sponsor could be a very good thing. But, having a strong community supported by multiple companies could also be a good thing. If FP Complete had been able to successful get modifications into cabal-install, or pay for the development work they desired, then that would be great. But what we have heard is that there is a private email thread where they supposedly tried to work things out and could not so they have decided to basically drive cabal-install/hackage out of business. That is less reassuring. I would like to see more reassurances that FP Complete is interested in build a strong self-sustaining community, not just a community that supports FP Complete. 
I do mostly C++ all day. One thing that haskell taught me was to think like a computer scientist again. Every problem begins with a data that transforms through an algorithm. In OOP, the two tend to get mixed up in coupled classes that have lots of shared state between the member functions. Now, I design classes are a little more than static functions that operate on some sort of product type (struct). Something more akin to haskell type classes. 
You should also not use cabal or ghc since some of the development is done by Well Typed.
Pull requests do get accepted in my experience. They get accepted slowly, and subject to a fair amount of review, which trickles in slowly. Due of course to the general fact that the people doing the review have limited time to do so.
Hah, fair enough. It's a risk I'll have to take, there just simply aren't enough hours in the day to spearhead every project that may come into existence to make sure no one causes breakage ;)
As I mentioned, before stack came into existence no more than 35% of packages on Hackage had upper bounds on all their dependencies. I don't think putting things into the build tool has worked so far either to encourage this policy.
What was the reason for the initial failure to install XYZ?
I didn't file an issue since it's a "by design" kind of thing: a cabal freeze file is just a list of constraints, but it never says "use these packages *and only these packages*." A simple workflow that breaks it is: * Work on package * Run `cabal install` * Run `cabal freeze` and check in cabal.config * Work on package more, and add a new dependency to .cabal * cabal.config no longer pins down that dependency I could file an issue about it, but I already file a lot of issues against cabal and I try to keep them focused on things that I really care to see fixed. I'm personally ambivalent about this one since I've already worked around it.
Please bring it up in an issue, we can discuss. Generally, we've avoided making anything in stack have prompts (besides --file-watch) because we want to make sure everything stays scriptable. Instead, we try to provide useful information on how to proceed.
I just fired up a fresh sandbox (using platform 2014.2, with ghc 7.8.3) and ran the following testsandbox$ cabal install gloss threepenny-gui --dry-run In order, the following would be installed (use -v for more details): SHA-1.6.4.2 base64-bytestring-1.0.0.1 blaze-builder-0.4.0.1 ... threepenny-gui-0.6.0.3 I didn't actually run the install, because I didn't want to actually install all those packages. But the solver came up with what it claimed was a workable plan basically instantly. So while you have shown you can do this with stack, that doesn't mean that one can't easily do it with cabal as well.
Looks like you need [ZipConduit](http://haddock.stackage.org/lts-3.5/conduit-1.2.5/Data-Conduit.html#v:ZipConduit) or [sequenceConduits](http://haddock.stackage.org/lts-3.5/conduit-1.2.5/Data-Conduit.html#v:sequenceConduits). Some more possibly helpful reading: * http://stackoverflow.com/questions/21671688/single-stepping-a-conduit/21684239 * https://docs.google.com/presentation/d/1RBefOCZ7AKOo4f1yiF4mtKPAT3l5vY9ky2SR02O4Vvg/edit#slide=id.g3c22e35a9_0182 (slide 28)
[Leaving it to the trustees to handle this kind of problem is not a scalable solution](https://github.com/haskell/cabal/pull/2774#issuecomment-132479035). It needs to be handled at the source by the people who are best equipped to deal with the situation...the maintainers of the packages without upper bounds. Better yet, the packages should not have been allowed into the repository without upper bounds in the first place.
&gt; I don't think putting things into the build tool has worked so far either to encourage this policy. It absolutely has. I have seen it personally in conversations with users where they have told me some variation of "you don't need upper bounds if you use stackage".
&gt; OK, again, did you read what I said? Before the existence of stack, 35% of packages had upper bounds on all their dependencies. This has nothing to do with stack or Stackage, the majority of people simply weren't putting in upper bounds at any point in time that I've checked. Dude, stop it with the "did you read what I said" while not actually reading what I said either. The users I talked to clearly linked using stackage with thinking they didn't need upper bounds. Upper bound percentages before stackage are irrelevant in the face of this information.
I really shouldn't feed trolls, but since that's how this thread got started anyway, why not? Clearly FP Complete has evil motives, and we should instead support true community projects where [the only way to get your changes included is to pay someone](https://www.reddit.com/r/haskell/comments/3lax6y/discussion_thread_about_stack/cv4zlk9). Your comment is pure FUD. FP Complete didn't take this course alone: we consulted with about a dozen companies and individuals before going the route of a new tool. Others shared the same story we had about contributions to cabal being blocked. I'm quite proud of what we've created: not just a great tool, by a flourishing open source project. We already have 60 contributors to stack, which given that it's been available for just a few months is amazing. New pull requests come in regularly. While the majority of the work is still done by FP Complete employees, many others have commit access to the project as well, and use it! If you don't want to use stack, that's your choice, and I have nothing against it. But stop with the ridiculous fear-mongering and ad hominems (against me, of course, below). The effort to work together with cabal, Hackage, and Haskell Platform is well publicized (just search for GPS Haskell). The efforts we put in to make wrappers around cabal to fix its problems are still available on Hackage. I made multiple offers to fix the insecure HTTP issue in cabal, which were blocked. So even if you want to imply I'm a complete liar and will not trust my account of what happened in non-public discussions (which, for the record, no one involved has actually disputed), there's plenty of public record to back up what I've said. tl;dr: Try harder next time, obvious troll is obvious
You can always make your own collection with custom snapshots.
&gt; the only way to get your changes included is to pay someone. This is clearly FUD. Please don't spread it, even ironically. I don't care if you could possibly read /u/mightybyte 's post to say that. He is a well known developer, but is _not_ a cabal developer, and any implication he may have given about the cabal development process is _not_ coming from someone who has been involved in it. Again, we can have it out all we want, but please, even ironically, do not contribute to the problem by repeating such groundless and poisonous accusations.
1. I don't know your background, but if you really think Haskell is "more math", than, say BASIC (or C or Java), then you've missed something fundamental. I mean, sure, it more resembles some mathematical notations in fields other than CS, but it isn't really any more based on math, you know. It also bakes a typed-logic approach to verification into the language semantics, but that's just a design choice for a verification system. And I realize this is an appeal to authority, but, like I said, you're missing something fundamental here, so just ask yourself if Haskell were really closer to math, how come virtually no algorithmic research in academia is done in Hasekll? 2. If you think I claim we don't need math in CS then you must have misunderstood me. I design algorithms for a living! I *am* saying that our programming language expressions need not necessarily be based purely on typed lambda calculus in order to best express algorithms. That is a very different statement. 3. It might surprise you to know that most other engineering disciplines (including electronics and civil engineering) today use basic calculation just as a rough foundation. Almost everything else is trial and error (in simulation). Even when circuit designers use formal methods they use something like the model checker I use to verify my code. 4. I was there when my large defense company switched most of our code from C++ to Java (including some hard-realtime safety-critical systems), and I can tell you it was not because of Java's hype (other languages around that time had as much hype, and money wasn't an issue for us). Sure, hype played some role as it always does, but if you can't see how well Java matches the needs of the industry, you just wouldn't be able to improve on it (and there's much to improve). If you base your opinions on beliefs rather than careful, methodic observation, how do you know where to go next? 5. I would seriously suggest you drop the "mediocre programmers" arrogant bullshit. One, because those "mediocre programmers" are most of the people giving us self-driving cars, and two because most "geniuses" working on the most novel algorithms in academia happen to choose the same languages the "mediocre programmers" do. 6. If you honestly believe that "hordes of mediocre programmers can achieve anything" have you considered that that might actually be a better approach given it actually works and given you believe people who don't share your tastes -- which happens to be the vast majority of CS -- are mostly mediocre (in your eyes)? If it's not the better approach then surely the small but exceptionally smart group of Haskell developers could have had *some* impressive achievements over the past twenty years. Instead you're busy giving up excuses why this group of geniuses has failed to write a single truly impressive application, while "mediocrity" is giving us machine learning and Google. So even if you're right, the smart thing to do is to use a language designed for zombie hordes, because clearly that approach works better. If not, then either prove me wrong or stop with the empty boasting. Your excuses are touching, but they don't constitute a real argument. In the time you've put into convincing yourself of your excuses, some mediocre-zombie-Go developers (a young language with a small user base) have written more impressive applications than any written in Haskell so far. My suggestions would be to actually look around and listen to what people say with an open mind rather than entrenching yourself in the belief that the choice of a specific programming style is what separates brilliant minds from the rest. Also, you should question your axiomatic belief that the programming language even matters that much, because the data we actually have seems to suggest that, at best, the choice of a programming language matters only a little. Oh, and stop repeating the "Haskell is math" thing; it's just like saying "real analysis is more math than algebra" or some similar nonsense. 
You'd think google would have had that one fixed day one. search for "go screw yourself" get 100000000000 results on go's lib-screwyourself bindings.
the students work is promising. Dominic and I are trying to find the time to verify/validate all of the students findings in the very near future. We may very well move to using split mix in random if the findings bear out!
A very interesting article about how to make reading Haskell code more approachable to programmers unfamiliar with the language. I'm not quite sure if I agree with the recommendations, but I'm looking forward to reading the discussion here on the matter.
This may be controversial for some people but I think that Haskell just has bad quality tooling. Also just seeing how slowly tools incorporate new features / fix bugs (like ghc-mod not working with a new GHC-Version for close to a year), I quite frankly don't see the harm in developing new tools. Eventually the community will accept one solution; and If it doesn't, so what? Other languages can live with multiple build tools, why should it be such a horrible thing for haskell? And besides, the user experience that cabal provides for hobbyists and people that start learning the language just isn't good. Probably anyone who tried do introduce people to haskell can confirm that. Also please note, that I don't want to whine about the quality of the work that people do for free to make tools for the community (huge thanks to the teams of cabal, ghc-mod etc.) but we can't just drop the possibility of making new tools because we have sub-par tools that do the job we want already (albeit in a somewhat unwieldy way).
Going off on a tangent: rule 2 suggests: &gt; Not okay: &gt; * `(&lt;$&gt;)` / `(&lt;*&gt;)` - Use `liftA{n}` or `ApplicativeDo` in the future How do you all see `ApplicativeDo` in relation to newcomers? My gut feeling is along the lines of "as if monadic do-block desugaring wasn't *interesting* enough to explain..." On second thought, however, I wonder if `ApplicativeDo` will be just another factor in the gradual increase of the role of `Applicative` in Haskell teaching, which is already well underway. 
I'm at rule number one, and already I am an utter failure. I hate parentheses, I love the application operator. But I guess if I were to write code for teaching beginners, I would follow these rules.
Semantically equivalent, but the version that includes an applicative is potentially more optimal.
&gt; This perception is compounded by the fact that the most significant use of the dollar symbol outside of Haskell is in Perl (a language notorious for being write-only). And [It-Which-Shall-Not-Be-Named](http://www.php.net/).
Which makes sense since Meijer suggests it as a syllabus in the beginning of the course. Also, I don't understand the downvotes; that anonymous "non constructive" button is too easy to reach for. Sure, "OP asked for a book", but we are not - I hope - finite-state automata who cannot interpret a question in a slightly less literal way.
Exactly. I think it's normal for Haskeller to love and use all the things mentioned in tutorial, but we need to realize that often we write small code-samples which will be read by newcomers (like when you try to explain something on reddit), and having these clear points might be a very powerful advice.
And don't forget that `stack` probably still doesn't download from `hackage.haskell.org` directly but has FPCo's servers inbetween you and Hackage... this requires you to put a lot of trust in them
yeah the build plan "succeeded", but then the package failed to build until explicitly installed. I didn't notice any reinstalls. maybe it was a dependency of said package that failed to build. I really don't know.
If stepcut's theory is right, that's just a matter of time ;-) They've already got snoyberg planted in the core lib committee... EDIT: /s
&gt; I made multiple offers to fix the insecure HTTP issue in cabal, which were blocked. A quick search in the cabal issue tracker reveals that HTTPS support was [merged already a couple of months ago](https://github.com/haskell/cabal/pull/2687).
Oh, but I surely don't deny his passion. I just don't like the methods, and painting it as if nobody wants to work with him so he's left no choice but to create the tooling equivalent of an anti-king and cause civil war by fragmenting the kingdom... erm.. the community.
Granted, there's also some non-beginners who prefer `stack`. But don't deny there are a lot of people preferring `cabal` over `stack`'s UI philosophy as well. We wouldn't be debating here if everyone had drunk the Stack kool-aid. The question is, can we make both groups happy with the same tool, or do we really need two competing &amp; overlapping tools which will inevitably continue to lead to religion wars?
Stop spreading FUD and actually look at what stack does
&gt; I hate parentheses, I love the application operator. Can I quietly suggest `.` as a compromise? The benefit over `$` (and function application) is that it is associative.
You know, I was starting to think your trolling was dying down a bit in this thread, and then this...
You still need to apply the function built with `(.)` at some point. Often you can get away with letting eta reduction work for you, but sometimes you can't (e.g. when feeding the argument back into the pipeline at some point). Then you're left with either putting the whole chain of functions into parentheses (ugh) or using `$`. EDIT: Another way would be to bind the resulting function to some name and then apply it in the standard whitespace way. I guess that this would be the easiest to grasp for a non-Haskeller.
I'll just assume you're genuinely unaware (rather than spreading FUD yourself) of - https://mail.haskell.org/pipermail/cabal-devel/2015-May/010151.html - http://lwn.net/Alerts/647889/
Yeah, we should just remove any upper bounds from Hackage packages and all use `stack` which doesn't need any. Then we'd finally attain Cabal Paradise. /s
Yes, I'm well aware of it, I filed that bug report. These are two different vulnerabilities. Cabal is still completely exposed to a mitm attack. If you really don't understand how, I can spell it out, but that other thread explained how already.
Java has some really good tooling. But the language is no fun to work with. Here is a (non exhaustive) list of my issues with Java: * null used for control flow * "everything" is OOP * mutability is the default * no functions (until java 8 *finally* added lambdas) * almost no operators (things like `x.plus(y.minus(z))` instead of `x + (y - z)` gets tiresome) * no REPL (though one is planned) * practically no type inference * generics was an afterthought and it shows * `equals` mixes structural and reference equality * definitions for comparators (and structural equals) yields code vomit * verbosity * abundance of repetition [1] * the culture to favor complicated things [1] Look at the amount of repetition going on in the definition of a simple class: class TrippleInt { private int x; private int y; private int z; Foo(int x, int y, int z) { this.x = x; this.y = y; this.z = z; } public getX() { return x; } public getY() { return y; } public getZ() { return z; } // repeat yet again for getters } And that's not even adding `equals`, natural comparison and `toString`. But it *is* (slowly) getting better. (So there is not much hope for it being replaced by something better in a long time.) 
 as a non haskeller my self I love these suggestions.
These are general readability tips, not only for beginners imho. I only don't agree about not using dollar operators. They make things more readable, you can think of them as unix pipes. Only order of things is different, in unix pipes data flows from left to right, while with dollar signs it flows from right to left. So maybe &amp; operator instead?
Java8's lambdas are just syntactic sugar. you still have to declare a functional interface if you want something a little more complicated than (a -&gt; b). you also need to refer to and use these as objects and not functions. Also, if you want to capture a value and to an object that might change, you have to deepcopy it prior to declaring the lambda, or it might change later.
Lots of people seem to like that one and I can I tolerate it as long as it doesn't become the, quite bizarre, foo . bar . baz . quux $ norge x
Then better don't ever use R. Try googling that :-P
It is indeed, which is why I think its wiser to name projects after little-known fish rather than with a word which is extremely overloaded in the computing world already.
You're talking as though there's a really productive community around `cabal`.
&gt; ... Well Typed ... Many members were active builders of the community in the first place and have sacrificed a lot already to build the community. Likewise FP Complete.
&gt; just to add an `s` to `https` That's a very big "just"!
Did you mean `fmap g f`? Because, with this extension, your `do` block will literally desugar to `g &lt;$&gt; f`, which is just infix for `fmap g f`. At least, that's my understanding.
I'm not very well versed with Haskell, but I don't recall ever having issues with the use of `$`. Sure, the symbol could be chosen better (for eg `&lt;|`, symmetric to `|&gt;`) but other than that it's been dandy.
On the contrary, it's a *huge* deal because do x &lt;- f return (g x) is no longer the same as do x &lt;- f let return = r in r (g x) (which is why I doubt this change has actually been made)
Yep. This post is also a must read for non haskellers since it unintendedly teach the difficult bits of the haskell syntax.
Represent values as an actual lazy and monotonically increasing sequence. Then you can stop pulling on the votes sequence after it's &gt; 50.
And also to clarify: this wasn't an apology. What I said above was obviously an ironic hyperbole against your ridiculous statement above (that paying developers to accept contributions to an open source project was a reasonable way forward) and Jeremy's silly FUD based attack. I'm saying that you implied very strongly above that paying the cabal devs was a reasonable step forward, which honestly is a much bigger slap to the cabal team than anything I've seen elsewhere. I stand by what I said: I want nothing to do with your claims above. If anyone should apologize, it's you: [I'm not the only one who made the same inference about your comments](https://www.reddit.com/r/haskell/comments/3lax6y/discussion_thread_about_stack/cv55j3d).
Regarding `$` and `.`: I still don't like neither the semantics (backwards) nor the visuals. `|&gt;` and `&gt;&gt;` for forward application and composition as introduced in Elm are much prettier, but then again I can live with what we got.
I'm happy with all the other suggestions, they mostly make code clearer for non beginners too, but I really don't like using parentheses over a function application operator. (I never liked all the parentheses in LISP either.) I totally agree that $ is a bad choice of symbol which one soon enough adapts to, but is off-putting for beginners.
This other post from DG is also my favorite for a fast introduction of non haskellers to haskell: http://www.haskellforall.com/2014/10/how-to-desugar-haskell-code.html
That defeats the purpose, which is optimization. When a partial result is available, previous partial results must be ignored. This is an optimization that must be implemented in the compiler itself.
Wouldn't a simple type clause fare better? Like, `type ConductivityMatrix = Matrix`
Right. I love `$` in my own code (parentheses are fiddly and make code harder to scan) but when I write code snippets on my slides for a talk I don't use it at all. Different code is read (and skimmed!) in different ways, so it makes sense to do things differently. This isn't just true for Haskell either; even in languages familiar to most people, I often use different indentation and naming patterns for example code as opposed to real code.
Oh... how did I miss that? Thanks! :D
Don't be an ass. Here are the recent commits https://github.com/haskell/cabal/commits/master, and here are the recent _closed_ PRs: https://github.com/haskell/cabal/pulls?q=is%3Apr+is%3Aclosed The cabal dev process, because accidentally breaking things is a particularly bad idea in a key piece like cabal, reviews things relatively slowly before accepting them, and releases less frequently than some other things release. But nonetheless cabal is actively developed, and historically has had contributions from many people. Also bear in mind that it is an inescapable fact of the lifecycle of software objects that they can begin development at a relatively active pace, and at that point add features or change design choices with relative ease. And, as they grow more mature and complicated, they hit a point at which it is _not_ so easy to add or change things, because of the much larger surface area of other functionality which one wishes to avoid regressions with.
I admit that I can't be certain, since there's not a huge amount of information here. However, if I was to take a guess, I'd guess that the selection of flags changed between the first and second call, leading to the second call succeeding and the first one failing. It's the only thing I can think of that fits the evidence. If that's in fact what's happening, curation would solve the problem by nailing down the flags in the snapshot. I'm well aware of what a guess I'm taking here. I had originally misread the report as saying different versions of the package got chosen, which upon rereading and looking at your comment I realize was a mistake on my part.
It's distracting if you're explaining code to an audience not well-versed in Haskell. I ran into this head-first when giving a presentation about FRP for a class: people couldn't read the code with `$`, which made my examples far harder to follow. So think about it as less a guideline for actual code you write and more a guideline for *example code snippets*.
Fwiw, the install-plan health for threepenny-gui got better when they started [adding proper bounds in 0.4.0.0](http://hdiff.luite.com/cgit/threepenny-gui/diff/threepenny-gui.cabal?id=0.4.0.0&amp;id2=0.3.0.1). There's just [one minor issue at the lower-bound of base affecting `threepenny-0.6.*` for GHC 7.4](https://github.com/HeinrichApfelmus/threepenny-gui/issues/120) (which would be avoidable when using Travis w/ a matrix including GHC 7.4), but other than that GHC 7.6/7.8/7.10 are fine for the last couple of major versions. So yeah, installing packages directly from Hackage is not that hopeless after all :-)
Does the `detailed` test type even work? And this situation may be explained by the `--show-details` flag of `cabal test`, which defaults to `failures`. If it defaulted to `always` instead, maybe gilmi would have seen the output they were looking for. In my opinion, it is much more reasonable to show the test output by default rather than only show it on failure.
I don't get what you're after then. As a micro optimization for avoiding some additions this is almost certainly not worth it.
The `fly` function will be more readable if you follow the other rules Tekmo is suggesting: fly :: MonadIO m =&gt; EitherT ServantError m r -&gt; EitherT ServantErr m r fly apiReq = do e &lt;- lift (runEitherT apiReq) case e of Right r -&gt; return r Left err -&gt; do liftIO (putStrLn ("Got internal-api error: " ++ show err)) left (ServantErr 500 "CyberInternal MicroServer MicroError" "" []) 
`where` is really wonderful. To me, the `let ... in ...` construct could be removed altogether from the language. Only time I use `let` at all is in `do` blocks, and then never with the `in` part. And I strongly favor let a = ... let b = ... something rather than let a = ... b = ... something in `do` blocks as long as a and b are are not mutually recursive. 
`($)`, lens operators like `(%~)` or `(^.)`, monad-related operators like `(&gt;&gt;=)` and `(&gt;=&gt;)` -- None of those are *syntax* in Haskell. They aren't part of the language, they are defined in libraries.
I don't know exactly at what point you're supposed to step in as a moderator, but there are some pretty weird comments (near the bottom) about Snoyman being "planted" in the GHC committee so that FP Complete can take over the haskell community. That comment seemed particularly malicious me.
Sure. I'm not saying "cabal test is perfect" -- like nearly every element of every piece of software ever built, there are clearly ways it could be improved. I just wanted to be clear that, to my knowledge at least, if configured properly, it at least will properly report success or failure. "lacks features" is a rather different sort of beast than "can't be trusted."
I agree! In fact, I made [Flow](http://taylor.fausak.me/2015/04/09/write-more-understandable-haskell-with-flow/), a library that provides `|&gt;` for application and `.&gt;` for composition. Unfortunately `&gt;&gt;` has to be used for monads in Haskell. 
Stack is really so cool that it's understandable. but thank you for posting this, no need for unconstructive discussions. 
Beginner here. Is that equivalent to (foo . bar . baz . quux) (norge x) ?
A lot of beginners are confused about `$` and `.` which superficially look like the same operator, and can sometimes even be used interchangeably, adding to the confusion. I myself struggled with it for at least a couple of weeks of on-and-off exercise solving, and several people I know (both fleeting acquiantances in #haskell and long-term relations) have had trouble with the same thing.
 vim :argadds **/*.hs :argdo %s/old/new/gce 
Heathen! I almost only use `let … in` and pretty much never `where`, because 1. It's not limited to some kinds of block scope – it can be used within any expression, and 2. It reads more naturally to me: top to bottom. `let … in` is also easier to consistently indent on a function level. If foo args = ---&gt;body then foo args = ---&gt;let ---&gt;---&gt;bindings ---&gt;in ---&gt;---&gt;body With `where`, it gets awkward, because you'd ideally want foo args = ---&gt;---&gt;body ---&gt;where ---&gt;---&gt;bindings but to be consistent and always indent the function body equally, you'd have functions that look like foo args = ---&gt;---&gt;body when they don't have where clauses, and that's silly. If you indent the function body only once when it doesn't have a where clause and twice when it does, you're into that "sometimes" territory /u/ocharles mentioned. I don't want that in my code. IIRC /u/chrisdoner (or am I confusing you with someone else?) "solves" this problem by introducing a "half-indentation" used *only* by the `where` keyword, so you get foo args = ---&gt;body --where ---&gt;bindings which lets you use a single indentation level for the body, but on the other hand makes the `where` keyword an anomaly. I don't find this solution satisfactory either.
I find `&gt;&gt;` to be way too noisy for composition, where `.` is actually a very elegant choice.
This may sound like nitpicking, but it's actually an important point. The fact that they're library defined means you can opt out of them. You can't opt out of syntax. (Cue everybody look angrily at the record update syntax.)
I guess so, but I don't like that it's backwards composition, which has historical reasons. I realize that I don't actually find it that noisy and appreciate the similarity to `|&gt;`. 
I was gonna pout, but then I read that last line.
If you're referring to haddock, that's not exactly "little-known", it's one of the more popular types of white fish! Standard British fish and chips uses either atlantic cod or haddock, if memory serves me.
For a basic use case like this, there are none that I know of. You can still write functions that are polymorphic over the phantom parameter and because there it only exists on the type layer, there's no runtime overhead. The only thing I can think of is that it makes your types more complex, and if you go nuts with it, it might slow down compilation by a very small amount. There are cases where you can actually run into decidability issues. You can build type level natural numbers (or just use `GHC.TypeLits`) and type families to get (almost?) complete Peano arithmetic on the type level. With that, you can encode *a lot* of information in phantom types. In turn, you need -XUndecidableInstances. The furthest I went with it was building trees that have their shape encoded at the type level. With that you can constrain your functions to pretty much arbitrary properties of trees. You can write something like `foo :: (Depth s ~ 4) =&gt; Tree s a -&gt; Fubar`, to have a function that only works on trees with depth 4. The drawback here is that it seems like there is no way to build such trees at runtime and still have the whole thing work. At least I haven't been able to figure it out yet. It works wonderfully in GHCi, but I don't know how to write a function like `fromList :: [a] -&gt; Tree s a`, because the shape `s` can't be determined. But that's borderline dependently typed programming, which Haskell doesn't support at the time anyway. There's one caveat to phantom types though. If the user has access to the data constructor, it's trivial to write a function `T a -&gt; T b`, which completely circumvents the idea of phantom types. So you probably want to make sure not to export the constructors. If it's an option, you can use GADTs to limit the constructors to certain phantom types directly, but you lose some flexibility in the process.
Okay, that's pretty obscure, yes. Can't argue with that.
Yes, but also (foo . bar . baz . quux . norge) x
I added a note to the tutorial that the quickest way to convert a `FilePath` to `Text` is to use `format fp`. I may call this out more prominently by adding a new FAQ section to the tutorial because this is the most frequently asked question. All `Pattern`s automatically capture a result (that's what the type parameter to `Pattern` means: the captured result), and your `sed` substitution would be: substitution :: Pattern Text substitution = do x &lt;- chars "/DSC_" y &lt;- chars return ("mv &amp; " &lt;&gt; x &lt;&gt; "-" &lt;&gt; y &lt;&gt; "/p") So I would write your program like this: example :: Shell Text example = sed substitution (do dir &lt;- ls "." file &lt;- ls dir guard (extension file == "jpg") return (format fp file) ) I haven't type-checked that, but it's probably close to what you want.
Eh, I think that was pretty clearly tongue-in-cheek, though not exactly constructive either. As long as stuff like that stays quarantined to this post and it doesn't get worse than that, I'll let it be.
Haha, I would call using `let` going bottom-to-top: start with the details, end with the final expression after the `in`. About indentation of `where`, why not: foo args = ---&gt;body ---&gt;where ---&gt;---&gt;bindings That's how I do it and it works really well. I don't see why `where` must be to the left of the body.
I'm OK with that, and appreciative that the topic has moderator attention.
I find it a bit weird that a keyword that "ends" a "block" (`where` ends the function body) would be on the same indentation level as the block itself. To me, that feels like writing if (predicate) { ---&gt;statements; ---&gt;} Not my personal preference, but certainly consistent so I don't mind it.
The original proposal did specially match on return, but I don't know if that's still the case.
What are the plans for the `stack image container` command? It looks very very promising. I spent some time trying to use it and also [trying to get static binaries for running in the `scratch` docker container or `haskell-scratch`](https://www.reddit.com/r/haskell/comments/3kjpwe/how_to_easily_create_portable_binaries_for_linux/). It would be **unbelievably** cool if we could easily get self-contained haskell binaries deployed into a `scratch` docker containter. By self contained, I mean including all data files in share + link everything that's possible statically + only link dynamically what's left. All that just by typing `stack image container` command :-) A related project is [haskell-builder](https://github.com/dkubb/haskell-builder). There's also an existing cabal issue [Support relocatable packages #462](https://github.com/haskell/cabal/issues/462).
If memory serves me, record syntax has a "higher precedence" than function application, which consistently surprises people and makes it easy to write very confusing-looking code!
Nice to hear!
Does this mean the above code snippet will have a `Functor` constraint instead of `Applicative`?
We've been adding features to it as we've needed them. The functionality is already at the core of one production deployment we've worked on, and I'm sure I'll be using it in the future too. More functionality for it would be great, and could be discussed on the mailing list or issue tracker. Note that I'm probably not the best person to talk with about this feature. Even though I use it, I didn't write the code for it or perform the code review.
I occasionally write code like that, and when I do it always has semantic significance. Generally, it means that the composition chain performs some meaningful operation (which could reasonably be factored out and applied to other inputs) whereas the direct application is something specific to the particular argument (which could conceivably be factored out by giving a name to the processed argument). That is, I would write code like that if and only if the most meaningful way to break it into smaller definitions was this: f x = fbbq x' where x' = norge x fbbq = foo . bar . baz. quux 
Good question!
I think you guys are both right. which leads to a point : let's be wary and value stability when it comes to things impacting tooling, which is still a major pain, albeit partially relieved by stack. 
This is a new kind of language extension: it relies on instances being law-obedient. I'm not sure whether we can standardize this at some point without altering the report to *demand* laws hold, and even then it's not an easy task because our good friend Bottom. I wrote up a proposal (and never posted it) that allows rewrite rules for class instances, but it was only a pipe dream at the time so I didn't investigate any further. https://github.com/quchen/articles/blob/master/law-rules.md
My only hard-and-fast rule is that style and structure should never obscure semantics, even if that means writing code that would, all else equal, be considered poor style.
&gt; This may sound like nitpicking And to further pick nits... `($)` does get special treatment in GHC so that we can `runST $ do`, and that's based on a syntactical form. For example, `(&amp;)` doesn't get the same treatment. Even `Prelude.($)` won't get the treatment if you pass it through a lambda (if that argument uses the documented type of `($)`). You can still opt-out of it though, which is nice.
Fixity and associativity are common in all languages with operators and are actually part of secondary school algebra.
&gt; backwards composition Are you advocating that we write function application like `(x)f` or something? If not, `(.)` is not "backwards". (`(f.g)(x)` = `f(g(x))`).
The currently available library [hastache](https://hackage.haskell.org/package/hastache) requires the IO monad for substitutions and uses a mutable IORef, which massively turned me of. This new implementation aims to be fast and easy to use by offering its own custom parsec based template parser, functions for compiling that automatically compile included templates and interoperability with [aeson](https://hackage.haskell.org/package/aeson) for input data representation. Included is a small executable for the command line for simple template substitutions using yaml or json as input. I's not 100% finished yet, and I'd very much appreciate some input on the API design. What could be done better and how. Thanks. EDIT: Pull requests and contributions to the [repository](https://github.com/JustusAdam/mustache) are very much welcome.
And (ba)sh - the original Bourne shell may well be the ultimate origin of the "variable names start with `$`" convention.
Why not just LVars?
My comment was admittedly snarky and not constructive. I'm afraid that my position on the cabal codebase is that it can't be improved much by iteration. I think it needs to be gutted and given a fresh start. However, I'm no expert on cabal and I wouldn't want to discourage people from making incremental improvements.
I was disappointed when I first noticed that I remembered that flag's name and the most useful setting for it ("always," as you say). It meant that I'd typed it too often, and should have been using something else for running tests.
Thank you for taking the time to answer. However, it doesn't really do the same. Obviously, if I'm using turtle, I don't want to generate a script shell an execute it, I want it to do what I want, ie rename the file, : not print `mv foo/DSC_bar.jpg foo_bar.jpg`), in this using the `mv` function from turtle. I can indeed convert a `FilePath` to string using `format`, but then I need to convert it back to a `FilePath` so it can be understood by `mv`. Then about `sed`. Given `foo/DSC_bar.jpg`, I'm not trying to return `mv &amp; foo_bar.jpg/p`, but return `mv foo/DSC_bar.jpg foo_bar` IF there is `DSC_` in the middle and nothing if not (that's what the `-n ... /p` combination in sed does). In any scripting language, (even Haskell using regexp) can do match a string against `(.*)/DSC_(.*)` and get, if the string matches or not, and if so, the captures ie given `foo/DSC_bar.jpg`, `["foo", "bar.jpg"]`. I would love to write shell script in Haskell, but I would expect something like my example to be something like this main = ls "." &amp; match "(.*)/DSC_(.*\.jpg)" &amp; \(s, [dir,path]) -&gt; mv s (dir &lt;/&gt; path) not having to convert back and forth between formats. ( I personally think that this FilePath safety is getting in the way, at least for throwable script). 
Funny that in many popular languages (java, c++ etc) the constructor *must* be named the same as the data type (class).
Lisps are different beasts and parens in them don't bother me that much.
This is a great article. I wholeheartedly agree with points 1 and 2. On point 1 (use of $) I found it very confusing when I was learning Haskell, especially trying to disambiguate . and $. Once I figured out $ just replaces parentheses, and learned about function composition, I admit they made a lot more sense - but even still I find usage of $ difficult to parse mentally as I'm reading code, especially lines with multiple ones. As for 2, I hate running across a fancy operator in Haskell and having to figure out where it came from and what it does. It's usually not obvious from the context, module specific, and hard to search for (even using something like Symbolhound). Especially the non-associative ones, as Gabriel points out. Using their actual function name with backticks is way more discoverable. I think it starts to make Haskell look like unreadable line noise, much like Perl (and I say this as someone that wrote a lot of Perl in the past). 
Yeah, I sometimes read it as "and then", but it doesn't always make sense. For example, `"foo" &amp; reverse`doesn't really read as "foo and then reverse". 
I don't know about that one. I understand that the $ is scary, but it's so common, I think we do more good by explaining it as if it WERE part of the language syntax, like parentheses, and doing that early. That way they don't get totally lost when reading code. 
I was, and continue to be, wary of FPCo's long term impact on the community, but they've been so much more open, welcome, and accommodating to a complete outsider who doesn't even use their products yet (me) than any of the existing organizations affiliated with the community. If they do wrest control from whoever has it now, it seems like it will be through the tactic of accepting input from the community. Perhaps you are right, and the current fair treatment they practice is a prelude to subjugation, but I'm disappointed that this is the axis they are so easily winning along.
Yup, that is definitely a subconscious inspiration of mine.
Ehm, yeah, even though that may be the case, just don't forget that secondary school algebra only means something in the US. So, a small subset of people who might be interested in learning Haskell will nod to this but there are also many people who are not from the US, who also might not even know the English words. You seem to imply that "everyone should know this" which triggered me to respond; I for one never had proper algebra at school. They did teach me that multiplication happens before addition unless parentheses tell you differently, but, no one ever told me about associativity, commutativity (commutation?) or fixity, either in Dutch or English.
That's actually a good issue that should be reported and fixed. My reproduction caused a different error, actually: [1 of 1] Compiling Test ( tests\Test.hs, dist\build\Test\Test-tmp\Test.o) [flags changed] Warning: output was redirected with -o, but no output will be generated because there is no Main module. Running 1 test suites... cabal.exe: Error: Could not find test program "dist\build\Test\Test.exe". Did you build the package first? So what must have happened for you is that the module failed to build at all, but there was an _old_ Test.exe sitting there which continued to work. The behavior of "main-is" as working to pick out different filenames, but always requiring the `Main` module name is an old problem: https://github.com/haskell/cabal/issues/172 At the time it wasn't changed because there was still a desire to keep cabal uniform across "other compilers than GHC." That ship now sailed. That said, in this case, even a better error message would probably be enough.
Beginners will have a hard time understanding what you just said. (Fd: I'm one of them.)
Are you suggesting to reimplement cabal from scratch retaining its API or something more radical that would require changes to the CABAL specification?
We should be able to get by with just adding a couple of cases for `(&lt;$)` and `(&lt;*)` based solely on if the do notation has a pattern match involved. on the thing on the right. This would leave `(*&gt;)` for the simple chain case where nothing in a prefix is pattern matched on.
I don't really want to suggest anything, I just want to point out that there's a lot of technical debt in cabal.
That karma system does more bad than good IMO, I wouldn't let a tear if karma was wholly disabled.
Fixity declarations in Haskell are you tell the compiler the rules of precedence (PEMDAS from secondary school) and whether to it associates to the left or to the right. If an operator is associative, that mean it doesn't matter which direction you associate it: (a `op` b) `op` c = a `op` (b `op` c) Multiplication and addition are associative. Exponentiation, Division, and Subtraction aren't, so you have to do them left to right: 8 / 4 / 2 = (8 / 4) / 2 = 2 / 2 = 1 8 / 4 / 2 /= 8 / (4 / 2) = 8 / 2 = 4 You may or may not have been told this concept had a name, but you were taught it in secondary school or before. Appending lists and multiplication and addition on peano naturals is denotationally associative but not operationally associative. That said, I'm not disagreeing this the guidelines in the article. For beginners, it's best if explicit parenthesis are used and they don't even have to apply something like PEMDAS; if they have to look up the precedence of `(.)` vs. `(&gt;&gt;=)` vs. `($)` to understand your code, you've lost a reader.
There is a difference between not making code unnecessarily scary to newcomers and artificially making it superficially similar to something quite different. (/u/taylorfausak: Sorry for the po-faced answer to your tongue-in-cheek question!)
I'm trying to follow up on the ticket now. I agree you ran into a bad corner-case here and I'll see what I can do to sort it out.
From many years of experience, "from scratch" is rarely the right way to clean up technical debt.
&gt; In the second example, you just read and compose the functions as you naturally read left to right. Yeah but you have to wait until the end to see what you're applying it to, which is the thing that should come *first*!
Agreed. In some sense `f &amp; g = g . f` would make more sense. Oh well.
That's a fair point of view. I guess when the dust settles on the cabal/stack saga we'll have another datapoint to advance our thinking on that. 
How about this: main = sh (do file &lt;- ls "." let s = format fp file (dir, path):_ &lt;- return (match ((,) &lt;$&gt; chars &lt;*&gt; ("/DSC_" *&gt; chars &lt;* ".jpg")) s) mv s (dir &lt;/&gt; path) )
Yes, that's nice. Now we just need to get mathematicians to agree.
Which makes me conclude that OOP is essentially opposite to Hs in this respect; "methods" somehow "belonging" to the datatypes, versus polymorphic functions that treat data with the least prejudice. We don't "overload methods", we attach extra functionality to data. Needless to say, I find the latter approach much more natural. 
Why not just imperative programming then?
That wasn't actually supposed to be a rhetorical question! I'd be interested to hear how you think LVars don't fit into a functional paradigm.
Could you? That would be nice.
Not completely. It surely looks like powerful framework and I can see my self using it in the future, but for now I need something simple that sticks to core Haskell as much as possible.
That's cool! I often wish for that in Ruby when I have to do `xs.map { |x| f(x) }` instead of `xs.map(&amp;:f)`.
There is an interesting comment by Franklin Chen about the love of `|&gt;` and `&lt;|`. And you know... (&lt;|) = ($) (|&gt;) = flip ($) p.s. I'm aware Data.Sequence already uses both of these operators.
Doesn't have to be a variant of Haskell, if you think curried functions or some other syntax is clearer. `("%s\n", "foo")printf;` or `("Hello")System.out.println;`, anyone?
You're right, I'm sorry -- I meant language-level abstractions/syntax, not extra-linguistic features that may be associated with a language or a runtime such as performance, monitoring, GC, code-swapping etc. We know that extra-linguistic features make a lot of difference. In fact, it was this exact realization (namely, that extra-linguistic features make a much bigger difference than syntactic abstractions) that contributed to Java's success. Gosling: "It was clear from talking to customers that they all needed GC, JIT, dynamic linkage, threading etc, but these things always came wrapped in languages that scared them" ([here](https://www.youtube.com/watch?v=Dq2WQuWVrgQ) 11:30-17:00) Brian has his own takeaway in the talk, but mine is simpler: if you want a language -- or any product -- to gain adoption[1] *you have to listen to your target market*; even if you think they're wrong, listen first, and only when you're certain you understand them you can decide they're wrong. Luckily, I think that PFP finally has a language designer who knows how to listen in the form of Evan Czaplicki of Elm. That's why I think it's more likely that we'll know if PFP works -- and when it works best and how it can be improved -- not through Haskell directly but through Elm. [1]: I know that Haskell isn't looking for adoption, but adoption isn't important for its own sake but to validate the claims. A language with little adoption probably won't be used for large projects, and if you're not used in large projects you don't know if your approach works in large projects, and if you don't know that then you know very little, because small projects are qualitatively different from large ones, and they don't pose a challenge to the industry at all, namely, the problem of making small projects easier doesn't have much of an economic value.
I sometimes still wish that lisk would happen.
Alright hehe fair enough. :)
Jinjing Wang is [ahead of you](https://hackage.haskell.org/package/air-2015.5.4/docs/Air-Light.html), this has been available on hackage since 2011 (looks like it's still maintained, too). You can see it used in JW's packages, eg [here](https://github.com/nfjinjing/maid/blob/master/src/maid.hs#L66).
Dan, stop trying to make Lisk happen. [It's not going to happen.](https://www.youtube.com/watch?v=Pubd-spHN-0)
&gt; Much content that was originally on community owned wikis was moved to FP Complete's servers. ... and FP Complete had them removed from the websites they were originally on and had all the Haskell wikis shut down... or not? In fact much of the content on School of Haskell didn't exist before School of Haskell invited the community to contribute it. If a company of haskell consultants set up a community-contributed website, you think that's dangerous and alarming, like Well-Typed setting up haskell.org and its committee? And you think that having a commercial organisation having control over tools is bad, like Well-Typed's aseipp holding the reins of ghc? It's not just that the conspiracy theories about FP Complete are irrational in themselves, it's that there's a history of good, positive commercial involvement in the Haskell toolchain that's ongoing. There's no sensible reason to believe that things will be any different now. &gt; Now we are talking about making stack and stackage the de facto tool and repository instead of the community owned cabal-install and hackage. You appear to be unaware that hackage is another Well Typed project and that cabal's lead developer is in fact the lead developer at S&amp;P Capital IQ. Commercial backing and involvement is both necessary and helpful. &gt; People are aware of snoyberg's claims that he attempted to work with upstream only to be rejected. I think they are just skeptical. You think he _fakes_ the mailing list archives I read? &gt; Looking at his development history, he has almost always chosen to create his own thing rather than work on an existing tool. Unlike other major haskell programmers who mainly edit other people's libraries? &gt; Once they have complete control of the toolchain they can turn stackage into the next sourceforge.net :( [actually, sourceforcge seems to have gotten a bit better recently]. A bigger concern is what happens if they consolidate all development around themselves and then go out of business leaving behind an unsustainable community. You feel that if the company had to close, they would reformat the hard drives and delete everything from github? 
Can you explain that?
How does R-pandoc differ from Rmarkdown?
Where possible I would also recommend using `let` over `where`, because it is more familiar for imperative programmers to see the thing happens first before the thing that happens second (ignoring considerations about evaluation order).
&gt; Again, are you advocating writing function application as `(x)f`? For some reason, you think reading should start with the most inner thing (the value). I do not see how any other stance on function application is consistent with what you claim is the "natural" order. That has pretty much nothing to do with what I'm saying except maybe tangentially. I don't think function application should be `(x)f`. I think it should stay the way it is right now as `f(x)` for the same reason I think function composition is currently backwards. That is because we read left to right, so it's natural to read things we write that way. If you reversed function application, then I would once again have to read right to left to read the arguments of the function in the correct order - that is to read the 1st argument, then read left to it and read 2nd argument, etc. - because for multiargument functions would be like `(z)(y)(x)f`. &gt; I don't read `foo . bar . bax . quux $ x` by tracing the `(.)` left to find the last one. I just read it left to right mentally substituting either "composed with" or "applied to the result of" for `.` and "applied to" for `$`. Sure, that might be how you do it, but if you want to read the functions in the order they are applied, then you have to read the function composition backwards, which is why we are saying that currently it is backwards.
I do look at the developer as an individual, and as an individual I prefer writing code in Java or Kotlin, or Clojure, or C, or OCaml than in Haskell. I'm pretty sure I'm not alone in that. But I also believe that working on interesting problems is much more important than working in a language you like. And there's another point to consider, which is *reading* code. In the end, you don't work alone, and you have to read lots and lots of other people's code (that, BTW, is much of what's killed C++ as an industry-wide language and what is now seriously hurting Scala). So you also have to ask yourself which language you'd rather *read*. I find Java, Go and C to be *very* readable (I haven't had the chance to look at any significant amount of Kotlin code, yet). OCaml is fine in that regard, too. Clojure (and other Lisps), while I enjoy very much writing (I think that language is a masterpiece as far as languages go), I find rather hard to read, and that's very unfortunate. Haskell is even harder (for me) to read, and Scala is near-impossible.
Awesome, tell me how it goes :)
&gt; stack setup I reported it and it got fixed in 4 hours. https://github.com/commercialhaskell/stack/issues/1004#event-413523228
If all you care about is the final result then, yeah, I'm sure there's loads of ways to do it. But the causal relationships between subexpressions are what they are, and if you interpret "lazy evaluation" as requiring that evaluation only occur when the result is demanded, I don't see how the chronological order of evaluation can differ from the causal dependencies. You have to start with `foo` in order to know whether it uses its argument or not, you have to do `bar` before `baz` for the same reason, &amp;c. The point I'm trying to make here is that looking at `foo (bar (baz (quux x)))` and thinking that `quux x` comes "first" is implicitly assuming some manner of non-lazy evaluation. Also, if you wanted to be uncooperative, you missed the opportunity to nitpick about whether `.` and `$` also qualify as "named functions". ;]
Well, I feel the wider point is that Haskell's syntax supports defining operators with essentially meaningless strings of symbols with arbitrary precedence and associativity. That leads to difficult to understand code, purely due to how it is supposed to be parsed, by both computers (and tooling) and human beings. That is a syntactical problem.
&gt; the fact that stackage puts you in a bubble that insulates you from the harmful effects of missing upper bounds ...and here is where I have to disagree with you; you feel that it's good that _all_ package downloaders should feel the pain of the existence on hackage of potential build plan failures. I think that only the package author should be hit by a problem. I can't do anything about it, and I want none of that while I'm coding, so I _love_ that stack/stackage insulates me from it. 
As this thread seems to have run its course, I'd like to thank everyone who chimed in. If we set aside the arguing parts, I come away thinking that basically everyone is right. Between /u/snoyberg /u/mightybyte and /u/sclv, I think it's clear that there are valid differences of opinion on tooling design choices, and that the perception of stagnation with cabal is sometimes exaggerated. But also that the cabal maintainers lack the hours to deal with contributions in a rapid way due to the risk of making a mistake and hurting their large user population. Since these contributors do exist and are willing to work, directing their energies to an alternative tool that isn't beholden to an already-satisfied group of users makes a lot of sense to me. If stack fizzles out, there will be an unbroken history attached to the cabal tool that stack users can return to. If stack continues as it has done and takes over, or is merged with cabal, we all get a better tool. Fragmentation does have downsides, but heterogeneity is essential for the survival of any population.
I maintain that the proposed change is worth it purely for the sake of eliminating the horrible type hack for `$` and `runST`. Ugh. Also, it would simplify the syntax rules.
&gt; The fact that they're library defined means you can opt out of them. You can't opt out of syntax. I think this is less true than implied in your comment. You can't opt out of libraries in code written by other people. And that's the problem. Every operator you define incurs a parsing overhead for both tooling and people. How does "x . f &lt;$&gt; a &lt;|&gt; b &lt;*&gt; c" parse? I don't know and I don't care. I didn't come here to solve parser puzzles.
&gt;Generally, we've avoided making anything in stack have prompts Please yes, do avoid this. Featuring `stack solver --modify-stack-yaml` could be good as others have suggested though. 
&gt; if you want to read the functions in the order they are applied Then I can read them in any order, since substitution order isn't guaranteed by the Haskell report. :P
There's a very clear lineage historically: Bourne shell -&gt; awk -&gt; perl -&gt; ~~php~~ [censored]
It actually *is* language syntax.
I find [this explanation](http://stackoverflow.com/a/24668518/3780203) to be helpful: &gt; Every applicative yields and arrow and every arrow yields an applicative, but they are not equivalent. If you have an arrow arr and a morphism arr a b it does not follow that you can generate a morphism arr o (a \to b) that replicates its functionality. Thus if you round trip through applicative you lose some features. &gt; Applicatives are monoidal functors. Arrows are profunctors that are also categories, or equivalently, monoids in the category of profunctors. There is no natural connection between these two notions. If you will excuse my flippancy: In Hask it turns out that the functor part of the pro-functor in an arrow is a monoidal functor, but that construction necessarily forgets the "pro" part. &gt; When you go from arrows to applicatives you are ignoring the part of an arrow that takes input and only using the part that deals with output. Many interesting arrows use the input part in one way or another and so by turning them into applicatives you are giving up useful stuff. &gt; &gt; That said, in practice I find applicative the nicer abstraction to work with and one that almost always does what I want. In theory arrows are more powerfull, but I don't find my self using them in practice.
Strongly recommend picking one and focusing on learning that first.
Stack helps me get on with the interesting stuff, namely writing code. I enjoy avoiding costly faffing about with ecosystem and compiler installation, wobbly library interactions, etc.
If you have any experience in imperative programming, go with Ruby until you master the Enumarable module and it's siblings. That will help you very much to understand the way Haskell works at some point. At least it helped me. If you have no experience at all, just take a dice, or ask your friends what thy use for programming and then take something you can get help with in your neighborhood. This neighborhood stuff is another thing that did help me much when learning Haskell... 
Right. I suppose you could argue that that does not make it syntax, but I have the Word of God on my side: &gt; Just think of (f $ x) as a new syntactic form -- [Simon Peyton-Jones](http://www.mail-archive.com/glasgow-haskell-users@haskell.org/msg18923.html)
They rely on mutation and their API on side effects.
Having prompts while keeping everything scriptable isn't mutually exclusive. See how `apt-get` achieves this.
What's so bad about prompts that you don't anybody else to have them?
That's definitely an improvement! That said, now I wonder how I would tell Stack where to find different GHC versions I have installed system-wide. I can tweak my `$PATH`, but that would only tell Stack one GHC at a time.
I mostly like dot chains: (f . g . h) x but I also like faking "keyword statements", like Python2's `print`, for common monadic functions, or noisy ones: example filename = do contents &lt;- liftIO$ readLine (filename ++ ".txt") tell$ filename ++ ": " ++ contents return$ length contents somewhat idiosyncratic, but if you squint, `liftIO`, `tell`, `return` look like built-in syntax, easy to skim.
it's boilerplate, but I try to export an alphanumeric for every operator, e.g. (-?-) :: Grammar a -&gt; a -&gt; Grammar a (-?-) = flip optionGrammar optionGrammar :: a -&gt; Grammar a -&gt; Grammar a optionGrammar x = ... though I prefer operators everywhere. I even like (the unary) `PostfixOperators`: (-*), manyGrammar :: Grammar a -&gt; Grammar [a] (-*) = manyGrammar manyGrammar = ... even though they need parentheses. 
I have said multiple times that I think commercial support of Haskell is vital to its success and explain why I think WT's approach stands up better to scrutiny. I am quite aware that people who later formed WT began writing hackage 2 as an open-source community project. It was later funded in part by GSoC. I myself contributed some significant patches. And it was ultimately (and only) completed because WT got funding to push it through to the final stages. They still remain actively involved and in control today. I do not think Michael lies or fakes mailing list articles. That comment was in response to a request for more information in which he said the details were 'in a private thread'. By skeptical, I only meant that some people wonder if a comprise really could have been reached in that thread or not. However, I do not wish to comment on this subject because it is pure speculation and it borders on Ad Hominum. I do not believe Michael is lying or being dishonest. FPCo is (as far as I know) a corporation with a board of directors. If they decide Aaron is not making them enough money, they can oust him and install any sort of knucklehead they see fit. I have worked for companies that were shutdown and sold off to patent trolls. I'd hate for GHC to end up in the hands of that IP management company. But, deleting all the files is an extreme example. A community is more than the code. Removing the infrastructure, people, and funding that are sustaining a community can be a pretty big blow. Additionally, it looks bad from a PR perspective if the company holding up the Haskell ecosystem fails. I am already concerned about what will happen when SPJ is eventually forced into retirement, and concerned about how even less would get done with out WTs involvement. I think we need to focus more on building up the community and diversifying where our funding and developer resources come from so that we are no longer susceptible to a 'cambridge bus accident'. There is surely a place for FP Complete to contribute to the community and there is certainly nothing wrong with their attempts to sell commercial support and development tools. I think Haskell needs more corporate sponsors paying to develop tools. And I think we need to be less dependent on any one commercial entity. That's why I see moving from a system that is trying to promote community owned resources to one that is more dependent on fpcomplete is a step in the wrong direction. We want to be in a position were the Haskell community can benefit from the contributions of Microsoft Research, Well Typed, and FP Complete, not one where we are dependent on them for survival. 
It would be unnecessary fragmentation IMO. I suspect most of us are interested in such discussions, and so it is worth it keeping them here. 
I haven't used it but I believe Rmarkdown can't be embedded in a Pandoc document (except as raw HTML). That's a dealbreaker for me because it prevents mixing other languages in the same page.
Indeed. I would add that de-emphasising composition relatively to application (as using `(.)` for application would indirectly do) would shield newcomers from an essential part of Haskell. Compared to that, left-to-right versus right-to-left is not really a big deal.
The basic scheme is incorporated into the analysis on that page. (That is the max on the bottom of the page).
https://github.com/gelisam/category-syntax#readme
 foo args = ---&gt;let ---&gt;---&gt;bindings ---&gt;in ---&gt;---&gt;body I don't actually like this, because it makes it appear that the `in` is sibling to, and not part of the `let`. I have the same issue with: if (foo) { bar(); } else { baz(); } Lisp is one of the few languages that shows this relation properly: (if condition true-result false-result) (cond (x x-result) (y y-result) (z z-result))
The more I read, the less crazy and alone I feel.
Am I the only one who hates do notation?
I'm somewhat fond of `(.&gt;)` and `(&lt;.)` for composition. I guess that would free `(.)` for reverse application.
We could just use `∘` for function composition. I'm sure that would be very popular.
Probably not. What bothers you about it?
Honestly, I wouldn't mind that. Besides, if people used real text editors (i.e. emacs) it wouldn't be hard to input.
I don't even use a "real" text editor by that measure and I wouldn't mind too much. (\^B will replace whatever is behind the cursor with something else based on an abbreviations file; I have `. = ∘` among many other things in mine.) I mean, it'd still be a terrible idea, but not because unicode is too hard :P
Well the `MkPerson` one isn't a big deal, I guess. The other one incurs a mental overhead because I have to pick names for type variables that won't clash with names for value variables, or vice versa. When it comes to that, I prefer to just use `[a..z]` for types and whatever makes sense in the context for values.
They're really useful for ~~pointless~~ point-free golfing, though!
&gt; Plus, you can write pipelines in pure functional languages without composition, as libraries like Pipes prove. *pipes* is an interesting case: it is fundamentally about composition, though the things being composed are not functions (it deals with categories other than **Hask**).
Eh, I had a background mostly with C# and a bit of Python and Scheme before learning Haskell and the use of operators never bothered me in general. Some libraries are a little too operator-happy (but I specifically don't mean `lens` here, its operators are actually very well thought out) but ditching something like `(&lt;*&gt;)` would, to my mind, make a lot of code significantly less readable for no reason.
There's some real subtle gems later on in the code, too. It's a work of art, I tells ya.
Haskell's functions shouldn't be verbs. IMO haskell's functions should be nouns. `let unifier = mostGeneralUnifier type1 type2`, not `let unifier = unify type1 type2`. This makes a lot more sense if you're reasoning about Haskell equationally, than using verbs.
Are you serious? How can you have functions that aren't verbs? `reverse`, `negate`, `sort`, `abs`, etc.
I learned ruby and perl after starting to learn haskell. I actually think it would be great to study both at the same time, but not perl. Pick a smallish project like a simple text based game or a small language parser. You'll want something with a little challenge that you will have time to implement multiple times. I basically followed this plan: 1) read up on a language 2) implement however feels most natural with your current understanding. Do not worry about it being good 3) repeat 1&amp;2 for the other language 4) compare implementations 5) remake the implementations to be more idiomatic for each language 6) compare 7) try implementing in each language with the style that is idiomatic in the other language 8) compare You can break down aspects of what is idiomatic to do finer grain comparisons. for example, imperative vs functional, fast and loose vs safe, how much you use the type system vs amount of tests. 
Will releasing "almost correct" now make releasing "all correct" more expensive later? Why make GHC 7.10 users wait for a feature that only users of 7.8 (and below) will miss?
Took me a minute to figure out that the arrows point *upstream.* I really like the sound of Duncan's solution, but I think it's worth getting this active for the big 8.0 milestone even in its current state. The problems are arguably worse than status quo when they actually come up, but seem *far* less likely to do so, to the point that I find myself doubting anyone who doesn't already know what's happening will encounter them before Duncan's patchset is ready and the answer can be "that's fixed, just `cabal update &amp;&amp; cabal install cabal-install`."
I'm not sure but it seems like [this](https://hackage.haskell.org/package/freer-0.2.2.2/docs/Data-Open-Union.html) might be useful in some way. Is extendible effects what I'm looking for?
Okay, compdata is pretty sick.
Your title reminded me of some interesting (in my view, extremely cogent) [notes on syntax](http://www.the-magus.in/Publications/notation.pdf) that Rustom Mody shared on the mailing list recently. One of his suggestions was to use a `.` for *normal* function application—`f.x` *instead of* `f x`. (The syntax is originally credited to Dijkstra.) This approach has several advantages: * application is always an operator, never syntactically magical * emphasizes that something important is going on when a function is applied * whitespace is more consistently not syntactically significant * reinforces that function application is a first-class value, just a normal operator * works with variable-width fonts (good for, at the very least, prose and examples) Obviously, we can't make this change to Haskell without creating something like OCaml's revised syntax (which almost nobody uses), but it's something that's worth thinking about anyhow.
he gave an example, but if you want other's: `head` and `tail`, `sum` and `prod`(uct). even one of your own examples, `abs`, which means "absolute value of" and isn't at all a verb!
I advocate `x |&gt; f`, yes. What other reasons are there in math, other than historical, to write `f(x)` instead of `(x)f`? It's a little unfamiliar, just like [seeing a website right-aligned](https://www.google.ae/#q=foo). Of course, we could just switch to writing text right-to-left, making it harder for 70-90% of right-handed people to hand-write.
While I was still messing with Lisp, it was a neighbour of mine who prodded me towards Haskell. Best neighbour ever.
Yeah, that's why I was putting it off.
I read "reverse" as "the reverse of", not as a verb. TBH in my preferred language would say something like `blah sorted` not `sort blah` (something like `blah sortedBy name` is much better than `sort blah by name` for understanding things equationally). Negate is just unary minus, abs is "absolute value of".
What about a hasksell-subreddit sub-reddit, to discuss about new haskell subreddit ? ;-) As other people said, I don't think the Haskell subreddit is saturated at the moment, and nobody looked or are even aware of the existing subreddit.
For some reason, I've always loved "spark" for `'`. In fact, I might start using "spark" instead of "prime". "X prime" might be more mathy, but "X spark" is just more awesome.
These are all cases resembling attributes/properties/fields/lenses.
Yeah, this is the number 1 issue with cabal-install. Don't let perfect be the enemy of good and all that. 
Please keep the Stack-debate to [this thread](https://www.reddit.com/r/haskell/comments/3lax6y/discussion_thread_about_stack/) as this thread isn't about Stack.
There was already a Haskell build system called Hake (in fact, I think there were two, but only one on Hackage).
As a general rule, I write code that *I* find easier to read, maintain and reason about. I can make an exception for didactic reasons, of course. For instance, the Elm Style Guide is almost an anti-guide to me. The only point I agree on is "Always have type annotations on top-level definitions". Code is written once and read hundreds of times, so I never sacrifice readability for easiness of editing.
I didn't want so single out ghc-mod in any way. It's a tool that I use myself (with emacs). My point is that I don't think we should set some tools in stone as a kind of de-facto standard when we can't really live up to other language's user experience in a number of ways (at least for now).
If there just were more APL keyboards in the world. 
The upgrade is more likely to work due to the no-reinstall feature too! 
I was actually using your comment as a kind of soap box for me to draw attention to the fact that for a lot of people ghc-mod is a part of the haskell ecosystem, and that perhaps it should be treated as such, or at least as a community we make sure that there is a general-purpose "haskell BIOS" for tool writers and ide/editor integrators.
Stack is a very nice exploration of the space of build tools, and its "ensure the project can always compile" philosophy has resulted in some very interesting decisions, like downloading and installing a separate GHC if necessary. Some of the design decisions could definitely be integrated with cabal.
[Have a look at this example](https://gist.github.com/spacekitteh/f4786fe4893e4e83fd21) 
&gt; I find Java, Go and C to be very readable I am the opposite, the amount of noise in those languages make it really hard for me to concentrate and read the problem. 
Applicative do would allow you to rewrite proc () -&gt; do x &lt;- query1 -&lt; () y &lt;- query2 -&lt; () z &lt;- query3 -&lt; () returnA -&lt; (x, y, z) as do x &lt;- query1 y &lt;- query2 z &lt;- query3 pure (x, y, z) So you would get a bit of a syntactic benefit.
It's very unfortunate that in GHC 7.8 you *can* implement no-reinstall but that nobody seems to realise this. You simply install each package alone in its own database and then by construction you can never have conflicting packages. Some wrapping around `ghc`, `ghci` and `cabal` would be required to enable the "correct" collection of package databases, but that's really rather simple. In fact I've successfully done this "by hand" to implement some sandboxes.
I end up using `&amp;` a lot when I want to only use `base`, but otherwise I don't think it's very intention revealing. I think `|&gt;` more clearly shows what's going to happen. Even `..` has some precedent [in Dart](https://www.dartlang.org/docs/dart-up-and-running/ch02.html#classes). 
In Haskell too. https://oleksandrmanzyuk.wordpress.com/2014/06/18/from-object-algebras-to-finally-tagless-interpreters-2/
Thank you for linking that! I have not seen it before. It is compelling and well thought out. I do have a question though: It doesn't clarify how you apply a function with multiple parameters. Would it be `f.x.y` or `f.(x, y)` or `f.(x y)` or something else? I am also happy to see it suggest `;` for composition, flowing in the more natural (to me) direction. `(f;g).x` means `g.(f.x)`. Unfortunately I think that would be a hard sell considering the semicolons use in pretty much every existing language, including Haskell. 
Interesting! I can't say I'm a fan of littering code with `to`s though. For example, `"bananas" . to reverse` would be hard to explain to someone unfamiliar with both Haskell and lenses.
Can it? What if we introduce `IO` to the mix? `readFile :: FilePath -&gt; IO String` isn't an "attribute" of `FilePath`. I suppose that's unfair. Pretty much anything in `IO` is going to be a verb rather than a noun. What about more basic things like `map`? I think of that as a verb. I read `map f xs` as "map this function over these values".
Sure, but using an operator is kicking the can down the road. How do you pronounce the `&lt;$&gt;` in `f &lt;$&gt; xs`? And furthermore, since my ultimate goal was to make Haskell *more* readable, I'd actually prefer a more procedural construct like `for xs (\ x -&gt; f x)`. Explaining `&lt;$&gt;` to someone that doesn't know Haskell sounds a little challenging. 
It's not automatically more readable. It's more readable to the vast majority of non-functional programmers. And you can still reason about `for` equationally. 
On what possible planet could "GHC to end up in the hands of that IP management company"? 
GHC is trying to leave your code polymorphic in the monad type, but the array operations aren't defined for arbitrary monads. Try picking one with a type signature. You should at least get a new and different kind of error then.
Sure, you can, but it leads people into thinking that there's something nonsymmetric about the equals sign. They think that if I write "a = b" that means "a evaluates to b", which, while possibly true, prevents you from doing equational reasoning on the other end of the equality. It's not any easier to make Haskell look like a procedural language, when Haskell's semantics are so fundamentally different. In Haskell, we care about equivalence classes in the denotation of expressions. In an imperative language, we care about some system of state transitions. Reasoning about them is fundamentally different, as I mentioned in [this old blog post](http://liamoc.net/posts/2013-11-13-imperativereasoning.html)
I like the backtick syntax when constructing a data structure like a list or a map. `union m1 m2` always confuses me which map supercedes which in case of key collisions. `m1 \`union\` m2` feels more natural.
the irony of `blah sortedBy name` is that the portion corresponding to `sort` is actually a verb because `sorted by X` is a passive relative clause with the passive verb `sorted` :)
&gt; It's not any easier to make Haskell look like a procedural language, when Haskell's semantics are so fundamentally different. Isn't that the whole point of `do` notation? Anyway, I skimmed you blog post and realized we have different goals. I am uninterested in formal program verification. I am interested in using Haskell as a general-purpose programming language, and I would like other (non-Haskell) people to be able to read the code that I write. 
Many monads simulate semantics common to procedural languages (e.g aforementioned state transition systems). Then `do` notation isn't a misleading style, because the semantics in such cases are more or less the same as a procedural language. Readability as judged by people that do not understand the semantics of the language they are reading is an often-aspired-to goal in discussions of language aesthetics, but it's fundamentally misguided, in my view. It's basically the same as COBOL's stated mission of being readable to non-programmers. It just makes life harder for the programmers who have to write it. Just as in Haskell, as someone who is more or less an expert Haskell programmer, I choose notation in Haskell that fits with my mental model of Haskell's semantics and the reasoning methods I use when writing and debugging Haskell programs.
&gt; why we ought to optimize for easy learnability by non-functional programmers Yeah, why *should* Haskell be easy for people to understand? &gt; if you cant get over using `(&gt;)` or whatever in place of `(.)`, you're not going to make it very far at all I can get over it. I have gotten over it. But I think there's room for improvement. And I think I've already made it at least a little ways. I am linked on [haskell.org](https://www.haskell.org/documentation), maintain [eight packages](http://hackage.haskell.org/user/fozworth), and have shipped Haskell in production at work. &gt; people coming to Haskell from other languages don't find `(.)` and application to be the sticking point Are you sure about that? [This question](http://stackoverflow.com/questions/940382/haskell-difference-between-dot-and-dollar-sign) about `(.)` and `($)` is the fifth most popular Haskell question of all time on Stack Overflow.
Why is installing pre-compiled packages hopeless? I do this with a tool that sits on top of Nix, and use S3 for hosting builds. The part I haven't done yet is figure out how to evict binaries from S3 after some period of time. While there are a huge number of plans for any given package, it doesn't really hurt anything to just upload any that aren't already in the binary cache. Something like a Stackage monthly would improve this even more.
I didn't even know that. Nevertheless, I'm pretty sure they were directly inspired by the Bourne shell in choosing that glyph for that functionality.
Maybe start with something simpler? There are many problems with your code which have nothing to do with Vector nor Par. I conclude that you are new to Haskell. My advice is to review the basics, making sure you're comfortable with monads and type classes before attempting to use more advanced libraries.
Agreed; read the source not the Haddocks. The lack of type annotations really improves things. notElem x ys = elem (not x) ys 
If you use sequential letters in the types and meaningful names for the variables, I'd think it does come for free unless you're writing a function that manipulates the alphabet.
I like `c` for `Char`s, which might collide with a type variable, when they are named in sequence. I also might want something like `s` for state both in the type and body.
Logically, it would be `f.x.y` if the function is curried and `f.(x, y)` if it isn't. In both cases, they're really functions of one parameter, which makes these questions easier :). Not having a special case for multiparameter functions makes Haskell quite a bit simpler.
See my suggestion below -- if we run into a situation that prior would have suggested "--force-reinstalls" we now suggest "--enable-multiple-reinstalls" instead. So there's a "gate" before doing something that could lead to confusing results, but the results of passing this gate are probably still ok, as opposed to --force-reinstalls, where the results were a big mess.
That comes from a type function not having it's parameters properly fixed. For example, if you have a type function `F a` and instance `F Int = Char`, and you have a value `gen :: F a`, you can't use `gen` where you want a `Char` because the compiler has no way to determine that `a` is `Int`. That's what it means by '... may not be injective'. I believe *this* is because you have a second monad occurance that is also not fixed inside the runPar. This one may not be as easy a fix.
What is the benefit of pushing this down into GHC? It allows GHC to load multiple instances of a package, but that seems like a very minor perk (if it is in fact a benefit at all) over the user experience of having faster builds.
Thanks for making this, it's very cool! Any plans for a gitit plugin? If not I could probably add it.
Hell, even as a more mature programmer I need an ELI5 on Haskell. Can it help me do scientific computing (numerics)?
My elevator pitch is usually that Haskell never gets in my way. When I use other languages, I constantly think, "and now I'd like to do X, but I have to do this workaround instead because the language doesn't allow me to do X." Haskell is powerful/high level enough that if I think of something I want to do, there's either a library for it or I can create one – even for things that are traditionally considered to be "built in language features". If I'm asked to show someone a hello world, I'll usually just do something like main = do putStrLn "Hello, world!" putStrLn "What's your age?" age &lt;- readLn putStrLn ("So you're " ++ show age ++ " years old?") putStrLn ("That means you shouldn't date people younger than " ++ show (daterule age) ++ " years old!") daterule years = years / 2 + 7 Basically what I'd do for any other language. There's no need to go into types and controlled side effects and that stuff when you just want to show someone some code.
Agreed, it's possible, but it's easier to start off with just one approach, and less likely to result in a mistake where something becomes nonscriptable in the future. But like I said, I'm not opposed to changes on this.
Either fire up Project Euler or grab a copy of PragProg's new book [Exercises for Programmers](https://pragprog.com/book/bhwb/exercises-for-programmers) Go through each exercise. Write one program in each language. Learn Ruby's staby lambda operator -&gt; early. You will want this to translate between the two languages. The best beginner Ruby book is the [Pickaxe](https://pragprog.com/book/ruby4/programming-ruby-1-9-2-0). My favorite Haskell book is [The Haskell Road to Logic, Maths, and Programming](http://www.amazon.com/Haskell-Programming-Second-Edition-Computing/dp/0954300696), but [Learn You a Haskell](http://learnyouahaskell.com) is good too. 
I expect it will be very difficult to explain Haskell to a newbie because it solves problems a newbie will not understand. I would rather suggest to start with Racket (truly amazing for newbies) then move to Haskell later.
Can you provide more information on this?
Putting functions before their arguments isn't "closer to natural language" at all. about 45% of all languages are SVO, which has the "function" between arguments (when there's two), and another 45% are SOV which has the "function" after both args. When there's only one arg, obviously both are SV in form, putting the function after it. about 9% of languages are VSO with the "function" first, and the remaining 1% is distributed among the other three orders, OSV, OVS, and VOS. Additionally, there are no words that really correspond to functions in general. Content words like nouns and verbs and adjectives, which make up the overwhelming majority of a language's vocabulary (somewhere over 99%, often much higher) are all predicates of various arities, at least at the semantic level, but the arguments are often not represented syntactically.
While not related to Haskell, here is a counter-example. SPJ in less than 15 minutes manages to make a very strong point of bringing the whole new curriculum to all schools of England: https://www.youtube.com/watch?v=Ia55clAtdMs If he could do that, why can't we do something "as simple" as creating a feel of why Haskell is powerful? I'm not asking to show every feature out there, just one or two examples might work. For example, showing "Maybe a" (and how Haskell solves "null" problem) type is very good, at least one of bright spots in my experience.
You don't need to explain Haskell, you just need to make as good (but without too much lies :) impression on the language as you can. Well, I wouldn't mind mentioning the bad parts also, of course, but maybe that's too much for 15 minutes or similar time-frame.
I might be completely off-base on this, but I believe SPJ had a much easier subject to communicate there. It's basically "hey we have these super important machines running all of society and none of our kids know how they work we should probably teach them that" and there's very little nuance to that, and it's very hard to take up a contrary position to. The specific solutions and tradeoffs Haskell chooses when it comes to solving programming problems are much more nuanced, there are many more competing options and the benefits and drawbacks of each option are not obviously apparent. Hell, *I'm* not convinced Haskell chooses the right tradeoffs in all cases – in some cases I'm fairly sure Haskell has chosen the *wrong* tradeoff!
Check out Project Lombok.
Is the neural network a DAG or does it have cycles? If the former you can avoid the int maps and have each node/connection contain the information it needs. This'll be nicer to work with but may not be more efficient (profiling!) 
I am 100% ok with that. Most of the INTERCAL symbol names are rather catchy, I think. Being aggressively non-standard is the only major problem.
Just had to compute combinatorial stuff with a non Haskeller. Wrote the over function to compute combinations. Used quick check in a few seconds: quickCheck (\l h -&gt; 0 &lt;= l &amp;&amp; l &lt;= h ==&gt; over h l == over h (h-l)) And it found my silly bugs quickly! That impressed the guy. I also like showing examples like: workers &lt;- replicateM 20 . forkIO . forever $ do ... Creating a thread pool with this little boilerplate is impressive when compared to Java or even Python. I also like showing things like GADTs that enforce red black tree invariants with 6-7 lines of code. And stuff like parallel annotations sprinkled is also impressive. But there's no one show stopper thing that makes Haskell great. It's a combination of things.
Thanks, didn't know that. It's even more beautiful.
[Learn Haskell and Swedish?](http://youtu.be/RqvCNb7fKsg)
FPCo is a dedicated to creating the tools required to commercialize Haskell. I think there are a number of situations in which FPCo might find it in their best interest to hire some Haskell compiler developers to hack on GHC. With SPJs eventual retirement they could easily become the center of Haskell development. They file some patents. They go out of business. Sell off the IP. This is a pretty common way for companies to die in my personal experience. I don't see FPCo hiring Haskell compiler writers to be any more absurb than FPCo creating an alternative to cabal-install/hackage. Perhaps they will grow tired of a waiting for a decent record system and will decide to release their own FPHC with an alternative record system that is not inline with GHC HQ. The state of the Haskell record system is a big wart which could block commercial adoption. Notably, none of these actions are evil or conspiratorial. Paying for Haskell development, adding features that customers want, etc, are all fine things to do. I have yet to work for a corporation that did not file patents. Companies go out of business all the time. Especially companies trying to commercialize open source -- look at the number of defunct Linux companies.
I'm not talking about whether it strictly can accommodate the use case. I'm talking about the overall general focus of the project. When you say "I personally almost never use a dependency solver", it's pretty clear that this important use case is not a priority for stack. There's nothing wrong with that. Just be up front about it.
The need to "rebuild all packages that I have installed against the new version of foo" makes sense with the current algorithm in which the solver tries to use versions already installed in the currently visible package db. However, the long-term plan is to make the solver less stateful regarding the installed package database. In other words, the package database becomes merely a kind of build-cache with no effect on the install-plan solving process.
I would spend some time talking about algebraic data types + pattern matching. Its one of the biggest advantages of typed functional languages compared to mainstream programming.
I think if you want to be able to pull in pre-compiled packages then you ultimately need to also deal with libraries that are bindings to C libraries. And if you are now deal with C libraries as well -- you might as well just use nix? The biggest issue there, is porting nix to Windows. However, I think that is less scary than it sounds. People have done test ports with good results. But nobody has been willing to really champion it.
If you're interested in solving an RL problem with NNs, and not so much on the Haskell part, consider checking out Deepmind's DQN implementations and papers (see below) - they perform much better on a range of tasks than previous RL/NN methods. If you're interested in doing work in NNs in Haskell, it's handy to separate out the execution side (which mostly involves invoking preexisting heavily optimized C/C++ and CUDA - e.g. Caffe, Torch), and the graph construction side (which can be done in an arbitrary HLL, and Haskell is a great fit for a few reasons). For example, you could use/modify https://github.com/ajtulloch/dnngraph/ to generate Torch `nngraph` networks, which you could then use the existing DQN code for the RL setup. - https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf 
When Cabal comes up with a plan that requires reinstalls and aborts, should it suggest --enable-multiple-installs
Yeah, that would be the idea. Basically whenever it suggests --force-reinstalls and warns it would be a _very bad idea_ we can suggest this and warn that it is "basically ok, but potentially confusing" or the like :-)
Oh, and that's a technique often used in makefiles, another Bell Labs language with $ for variables.
Yeah I generally agree, but remember my comment was in the context of making the language more approachable to newbies, in particular to those coming from non-FP languages.
Do you have anything useful to say?
I am not sure this is right use case for Par monad. Take a look here for examples: [slides](http://community.haskell.org/~simonmar/slides/CUFP.pdf) . You could done this example by using **parallel-io**: import qualified Data.Vector as V import qualified Data.Vector.Mutable as M import Control.Concurrent.ParallelIO.Global a :: V.Vector Int a = V.fromList [1,2,3,4,5,6] b :: IO (V.Vector Int) b = do c &lt;- V.unsafeThaw a parallel_ $ map ((\i -&gt; M.write c i 100)) [0..5] V.unsafeFreeze c Additionally, you will have to compile your program with *-threaded -rtsopts* flag and run it with *+RTS -Nx* where x is a number of cores. 
I have never seen a line of code that made me laugh so hard. 
Quite frankly nix can be safely ignored as a solution in any discussion that involves making things simpler for beginners...
okay, from `Strong p` you get `first`: first = first' from `Profunctor p` you have: rmap :: (b -&gt; c) -&gt; p a b -&gt; p a c and from `Category p` you have: id :: p b b so: arr :: (b -&gt; c) -&gt; p b c arr f = rmap f id I guess there are laws I should check...
I should add that this is mainly a problem I think for people not experienced with _any_ build system -- sbt, ant, make, etc. all in my experience end up needing the same thing, and more often to be honest. But that of course doesn't mean we can't do better. I'd lover an idea for what circumstances we could "autodetect" to suggest "have you tried to clean"
For reference: https://github.com/haskell/cabal/pull/2768 There seems to be lots of support for extracting the solver, and the usual range of concerns to sort through to make sure this is done carefully :-)
I use a variation of that technique for composable ASTs that has better performance. I've presented it at a couple talks, but don't have anything published yet. If you have a short example of what you'd like to write, I can try to find time to adapt it to my technique.
I think that Haskell is so different from most languages, that if your friend only knows basics of Java he should focus on learning Java first. If he absolutely wants to see how Haskell works I would tell him to watch this video - https://www.youtube.com/watch?v=02_H3LjqMr8 It's an hour long and explains all the basics of Haskell.
At this time.. perhaps. But ultimately, it could be pretty nice. Let's say you want to work on a project from github. You do: $ git clone https://github.com/super/awesomeproject.git $ cd awesomeproject $ nix-shell And then you end up in a shell that has everything you need to develop that project. Automatic downloading of pre-build binaries and all. If the project requires tools like nodejs, they will magically available. All without cluttering up your global package database. cabal does an acceptable job of getting the Haskell libraries installed. But if you need to install libsdl, nodejs, etc, you are left hanging. Admittedly, the documentation for using Nix+Haskell is not very good. Which is why I am working on a video series: https://www.youtube.com/channel/UCHsUOvbAHeZOo_JxWA_kUog episode 2 in the works. 
Yes, sadly. That link tells me there is no reason to use Haskell over another tool for numerics, machine learning, HPC, data science, game programming, computer vision, or embedded programming. I can only think of one project of mine in the last several years that did not fall into one of those categories... and while metaprograming in Julia taught me some things it did not ultimately require me to relearn how I think about programming just to efficiently guard against something as nasty as run-time-determined combinatorially-many edge cases. What can Haskell do for me as a scientist / computer vision engineer / game developer? 
Thanks for the link! I didn't realize how passionate the anti-library feeling was with cabal devs. There are at least two SMT solvers floating around on github that may be more likely candidates for other tools to use.
I was genuinely trying to be useful! I began by reproducing your error message and finding the cause: `The type variable m0 is ambiguous` is displayed because the operations you use are polymorphic, so your whole definition is polymorphic, but by default GHC tries to infer a concrete type for top-level definitions. I wrote a paragraph explaining that you could use the NoMonomorphismRestriction pragma to change this default behaviour, or pick a particular instance satisfying the PrimMonad constraint, such as IO. It still didn't compile, so I then looked up the Par API, with which I was not familiar, to determine what was wrong with that part of your implementation. I discovered that the Control.Monad.Par module is meant for parallelizing pure computations, but that you were trying to use it to run IO computations concurrently. I then tried to understand what was up with all the seemingly-spurious `liftM` and `return` calls, to figure out that instead of executing several mutations concurrently, you were accidentally parallelizing the construction of an IO computation which, when executed, would mutate your vector sequentially. Finally, and much later than I'd like to admit, I realized that you used `unsafeThaw a` and `return $ unsafeFreeze c` in the same computation, indicating that you were not familiar with the fact that `return` has a different meaning in Haskell than in other languages. (you have since changed your question to not include this mistake) At that point, I realized that you probably had less experience with Haskell than I had assumed up to now. As a consequence, my explanation about the monomorphism restriction would probably cause more confusion than enlightenment, so even though I had already spent quite some time on my reply, in the interest of being as useful as possible, I decided to delete my reply and to start over. My next attempt listed all the flaws I had found in your code, and asked if there was one of them about which you wanted more information. I read it over, realized that I was only listing the many ways in which you had messed up without offering any concrete advice, and decided that it might hurt your feelings. So I deleted everything and tried again. My last attempt is the one you see above, which I reworded many times in order to make it as polite as I could. I feel like I managed to offend you anyway, for which I apologize.
&gt;So, my question is: is there a "marketing tutorial", which is able to explain in 15 minutes what is Haskell and why it's good to someone who barely knows Java (very junior developer)? If there isn't – think on how would you do if you'd have those 15 minutes, do you have any bright ideas how overall nicely structured plan might look like? &gt;I believe that it would be really nice to have some strategy on how to introduce the language best, so that person "keeps it in mind" it for future, and maybe even tells their other friends without disfiguring their impression. Sorry, but no. I know it is a tempting thing to try, but I feel any such strategy would be crossing the line into evangelism, which is something we should avoid. In more practical terms, I don't think you can present Haskell in 15 minutes in a way that is at the same time general (which "tutorial" and "explain Haskell..." seem to imply), concrete (i.e. with actual code) and not disfiguring. General and abstract (as in [kqr's elevator pitch](https://www.reddit.com/r/haskell/comments/3lkli5/tutorial_request_explain_haskell_to_complete_noob/cv71b1t)) might work, and so does specific and concrete (as in [Peaker's anecdote](https://www.reddit.com/r/haskell/comments/3lkli5/tutorial_request_explain_haskell_to_complete_noob/cv736p7)). In the latter case, the key point is that, instead of jumping in armed with a "marketing tutorial", you wait until a real situation gives you the opportunity to show off some Haskell in a believable setting.
I have complete trust in Thomas and Ryan doing the sensible thing. Perhaps it's just an issue of not having another place to put this functionality.
Yep. http://www.fceia.unr.edu.ar/~mauro/pubs/Notions_of_Computation_as_Monoids.pdf works through them.
This isn't really a theorum or anything, but you can use almost any of the functions from [Control.Monad](https://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Monad.html) on your data type. Personally, I feel like `when` and `mapM` come up a lot of all sorts of monads.
Awesome, I'll look into the supply monad, seems like exactly what I need. I'd love some critique on the code if you've got the time, I just put it up on Github here : https://github.com/marcusbuffett/NEAThs. You can run `cabal repl` then `main` to see it 'in action'. Right now there's no generations, species, etc, just genomes and genes and the code for mutating the genomes with new genes.
Seems like an interesting post, I'll check it out, thanks! 
The problem with upper bound is : how do you know before a new version of a package come out, that it will break yours ? Maybe we should upgrade the PVP to semantic versionning, where in A.B.C, you'll have to bump A when introducting incompatiblities. For example, I switched to stack this week because I use Diagram and Diagram 1.3 is incompatible with 1.2. HTF I'm suppose to know that what work with Diagram 1.0 , 1.1, 1.2 will break with 1.3 ? Putting upperbounds is, with the actual PVP , close to freezing.
It might be a newbie (and orthogonal) question, but how do you get to see new comment on reddit ? I can see there are 4 new comment, but I have no way to find them and I'm probably not the only one.
I discovered UnicodeHaskell a while back and since then I've been using ∷, →, ⊛, ∘ and so on a ton. Harder to write but the code looks so gorgeous in my opinion. :D
It does look nice. And you know what they say, that code is read more often than written...
you can read the laws in Control.Monad for theorems. they can optionally be rewrite rules to speed up your code. but really, for me, it's not about "using theorems" per se. it's about the fact that you can use the 100 function that take monads (like mapM and replaicateM and &gt;=&gt; and etc) and they should "just work". generic standard library functions are easier to read too than custom ones. also, as ocharles mentions, your abstraction probably "makes sense". it means you've chiseled out a nice solid thing from the space of possible solutions. you of course want to provide additional "your type specific" operations, whether for convenience, or because the problem you're solving is messy. but anytime you don't need them (ie a monad generic function can be interpreted in an interesting way for your type, like how the monad generic join is the list specific concat) it means that things are clean and simple. 
I have another question that I haven't really seen anyone asked : How come that stack came up from nowhere and solved so easily lots of problem cabal couldn't. Is that because the code base of cabal has a heavy legacy and it was easier to start from scratch or because cabal has indeed much much more (hidden) features than stack ? (this is a genuine question, and I'm not trying to offend anyone).
Part of it is simply second system syndrome: we learned what did and didn't work with cabal, and could design something based on that experience. I really believe that the biggest change that affected everything else was reproducible builds. It's guided a massive amount of the functionality in interesting ways. Beyond that, curation by default and a strong focus on UX during design phase probably helped a lot.
There's a [link](https://github.com/haskell/cabal/issues/1597) to a related issue that starts the library process. Highlights: -1000; WONTFIX; close with prejudice; detest. Really healthy attitude there.
Yes, I meant the internal choice of the type. There are three libraries I know of: ListLike, mono-traversable, and (my own) monoid-subclasses. For the last one there's a ready parser combinator library, picoparsec. I'm not sure about the other two. I think your efficiency concerns are probably misplaced. For template parsing performance, the choice of the parsing algorithm matters much more than the input type; both Attoparsec and Picoparsec are faster than Parsec on `Text` inputs. On the output/rendering side, of most importance would be the `mappend`/`mconcat` performance, but `Text` is already not the best choice there: its `mappend` is O(n). 