Maybe I'm having a slow morning but I'm not quite getting this
Doesn't explain why we're behind math in parity. Mathematicians are not exactly renown for their fashion sense and charisma.
graphviz sounds like it’s what you’re looking for, is there any reason why you don’t want to use it? diagrams will work but unless you want to write your own graph layout algorithm you’ll probably end up using something like `diagrams-graphviz` so unless you care a lot about how your graph looks, you haven’t won much.
When you declare a module you can choose to only export some functions from it (rather than all top level functions), which is something that the module declaration is used for. The simple one is simple, though.
It's from Data.Monoid. The operator is popular, but not in Prelude, so needs to be imported.
Stack release information is here: https://mail.haskell.org/pipermail/haskell-cafe/2018-June/129260.html 
Could a source plugin be used to insert a `HasCallStack` constraint on all the functions? Or maybe to do automatic logging?
Some of the answers on there are amazing as well: https://www.codewars.com/kata/reviews/5919e5dbc2ed5e46bb00004d/groups/592feaa7a48b4d977c0001ef I did not know Lens could do that.
Aww, I'm not allowed to see it because I haven't solved that one... I'll have to take a look at it later :) 
I download a lot of papers and various other pdfs. At some point I got very annoyed at being unable to find anything in my download folder, so I wrote a small utility to help me rename/move that whole mess. This is also incidentally my first haskell program that I would consider useful for other people, so please let me hear your criticisms.
Yipp, I think ZuriHac did a fantastic job in regard to that! So I think that is something to celebrate and to honour. I definitely do. :) Nevertheless, we should always be aware that not everything that is well-meant, also has the intended effect. That's why it's important to share our experiences and discuss them, instead of ending the dialogue about cultivating diversity right there. In other words: this fantastic effort at ZuriHac should be the start of something and not the end. I am just starting a local FP-Meetup and would be grateful for advices on how to make it even more accessible. Therefore, I am happy about every constructive discussion on that topic and think that there is no shortcut to a more diverse community. There is plenty of work to do, but I think, it is so much worth effort! :) 
I guess the most effective way of creating an inclusive culture is to lead by example, from day 1. ZuriHac doesn't achieve inclusiveness by writing an extensive CoC, or actively policing participants' behavior; they did it by starting off on the right foot, and carefully growing the event in a spirit of open mindedness and focusing on the shared interest. And, again; lop-sided demographics are a symptom, but in order to figure out if anything needs to be done, or can be done, and if so, what, you need to figure out what the cause is. Just assuming a cause that fits your agenda tends to create a toxic environment and ruin everything for everyone.
Thanks for introducing me to that site. The kyu 1 are really fun.
I am looking to build a GUI where displaying a graph is one of the main parts, I was thinking of using the brick library for this but it is text only, thus displaying a graph as text would be preferable, or something that can suggest a layout and leave me to rendering that layout.
True :) Though you [would be surprised](http://www.legorafi.fr/wp-content/uploads/2017/06/Cédric-Villani-e1498035389421-820x518.jpg) But as a whole, programmers do dress in a way that does not communicate respectability, power, seduction, in sharp contrast with other professions which put a lot of effort into it. As long as that's the case, it's self evident that it won't be attractive as a profession to many people, among which a disproportionate majority of women.
&gt; I know about "things vs people" a la Damore, It's funny that you associate this with Damore. Have you never heard of [Simon Baron-Cohen](https://en.wikipedia.org/wiki/Simon_Baron-Cohen)? Relevant resources: * [https://www.youtube.com/watch?v=tiJVJ5QRRUE](Brainwash: The Gender Equality Paradox) (video) This episode of the Norwegian documentary series interviews all of the major academic sources of Damore's paper. * [Steven Pinker - The Blank Slate](https://en.wikipedia.org/wiki/The_Blank_Slate) (book) This book is the basis of the aforementioned documentary and summarizes the same research in more detail.
[removed]
What you call underrepresented I might call overrepresented, because we do not have the same priors, as exposed previously. Out of those factors that select this demographic mix you mention, some are physical and "prior-free". You mention geography. Let's imagine we create a compensation mechanism so that we can circumvent those unfair barriers. that means some poor people from Switzerland or Austria would subsidize people from China to come. So you can see how not only do we have to agree on under/overrepresentation, but we *also* have to agree on the relative price for each under/overrepresented groups. If we really want to give everyone the opportunity to enjoy haskell, and what people enjoy at Zurihac, it's best to concentrate on what's simple and common sense. For instance, help make Zurihac organization more efficient, see how we can put more of the experience online, and how it can inspire others to organize similar event with similar qualities. We have to be practical. Another idea, would it not better serve groups for which it is difficult to travel by organizing a great conference nearby them, rather than having 1 precious winner be allowed to travel or have some free kindergarten ? I would imagine so.
What’s Max doing here?
So you were actually serious about that?? I'll just point out that correlation isn't causation and that programming in Haskell is something that people do almost entirely in their own solitary alone time with the computer.
Sure, I didn’t mean to imply he came up with it. Things-people has been discussed at length even on the ssc main blog.
I am only the messenger here : People wear suits for a reason. You are right of course, programmers don't have to face customers, and putting a tie on does not make you more respectable. But if people do so, groom, etc, it's for a reason. That reason is well established to be conducive to a greater outside appeal, and a requirement for many who see it as a positive social signal.
OK. It seems derisive and misleading to credit it to some random programmer who received only negative publicity (even infamy) for his writing about it. In reality it comes from a well-respected professor who is the director of Cambridge University's autism research program. This guy's theory correctly predicted prenatal testosterone levels. Needles were stuck into pregnant women's uteruses to confirm this theory. It didn't come from a disgruntled incel at google with no scientific background.
For github pages reddit ends up using the avatar of the last committer as the preview
I've used Opaleye and Esqueleto in production and only looked at Beam and Selda. The latter didn't look promising for what I want out of a SQL EDSL. Opaleye was painful, verbose, and relatively easy to make mistakes that wouldn't show up at compile-time. I also found it tortured the semantics of SQL queries more than I would've liked in order to turn them into something else. I maintain Esqueleto because I've liked using it the last 4 years for my production Haskell work. I'll switch to something else when I find something better. I've got [plans for improving on Esqueleto](https://github.com/bitemyapp/bluetick) but none of my past or present clients have been sufficiently unsatisfied with Esqueleto to pay me to build the next iteration.
Even a totally uniform sampling of some reference population would show some odd skewed characteristic given enough characteristic dimensions
You're conflating a couple of issues here. (And I'm not sure you do understand the reason people wear suits -- it's almost always *in order to be inconspicuous*). 1. Formal dress standards These are attached to social contexts. Programmers generally conform to such social requirements as well as anyone else. 2. General "chad" attributes that make males attractive. Young men who are "on the prowl" put much effort into exhibiting these. This is where you will find programmers and nerds and solitary types, who aren't actively trying to pick up women all the time, falling short. This may include dressing more formally than others, in order to be conspicuous, which is the opposite of the normal reason for wearing a suit and something very few people do. Please don't confusing the peacocking style of dress with the normal reason people wear suits.
Just a note : Hackage download stats are almost meaningless, especially for low counts.
IMO, considering Stephen Diehl's quote: &gt; Haskell SQL libraries: endless boilerplate, opaque metaprogramming, or wall of 15 language exts and no inference. Pick two. persistent+esqueleto lets you pick 1.5 -- opaque metaprogramming and a lot of extensions. Type inference is generally quite good with it, in my experience. I've also found it to be relatively easy to contribute patches and work on the `persistent` and `esqueleto` codebases to add features.
If you look carefully, you will see that the social standards are different among the different groups mentioned. That makes some groups, even composed of inconspicuous individuals (which is not to be conflated by bad style), much more attractive to other group made of likewise inconspicuous individuals. Of course, that might not be the case everywhere, each culture has its idiosyncrasies, but we were mentioning the Swiss context here
&gt; opaque metaprogramming I usually think of metaprogramming as being things like generics or TH. Esqueleto doesn't actually use any of that. What counts as opaque metaprogramming here? The GADTs? The functional dependencies?
Anyways, as I said, correlation is not causation. 
The missing piece of the puzzle is that there’s no way to guarantee you got it right. You can’t say “I want this to be a compile time constant” and have it fail to compile.
`esqueleto` requires `persistent`, you can't really separate them out. `persistent`'s model stuff uses TH by default, and if you don't go with the "opaque metaprogramming" then you get the "endless boilerplate." No idea if it's comparable as I much prefer TH to boilerplate and have never felt the need to write it out.
The "Friends" type does not contain many friends but instead a single person and an int. Did you mean to make this a list? If not, what would combining a person and a "Friends" look like, just replacing the one friend with this new name?
Okay so i changed it now and got the function addPerson :: Person -&gt; Friends -&gt; Maybe Friends addPerson n f = Just (Friends n 0 (f)) so if i input "addPerson "Charlie" Empty" it works and outputs "Just (Friends "Charlie 0 Empty), but once i use a existing friendslist and not an empty one it wont work.Any idea why?
Two issues stick out. First, it looks like your want `Friends` to represent a whole friends list, but it only has the ability to hold a single person. There is no multiple-friends constructor. I think that's correct -- you should use an existing collection type for that -- but the type signature of `addPerson` seems to that a `Friends` can hold more than one. Second, assuming `0` means "not friends" rather than "enemies", you have encoded _three separate ways_ of indicating non-friendship! There's the `Int`, the `Empty` constructor, and the fact that `addPerson` returns `Maybe`. You only need one of these, and I'd favor the one that uses the outer structure over encoding a null state in your data type. If you're not planning on adding any more data to a `Friends` value, I'd just do this: ``` import qualified Data.Set as S type Person = String type Friends = S.Set Person addPerson :: Person -&gt; Friends -&gt; Friends addPerson = S.insert ``` Or, if you do wind up attaching more data to a friendship: ``` import qualified Data.Map.Strict as M type Person = String data Friend = Friend {- extra friendship data, NOT including name -} type Friends = M.Map Person Friend addPerson :: Person -&gt; Friend -&gt; Friends -&gt; Friends addPerson = M.insert ```
I think it'd be really hard to make a DSL for generating another human readable programming language as output, and not have that involve metaprogramming, being that the explicit purpose here is to generate code. So, whether or not this was the intent of the original quote, the only way to resolve the issue of 'opaque metaprogramming' in this problem space would pretty much have to be addressing the opacity.
Loud applauds. I thought of this so many times... so many times, but never got started.
It's not because it's expecting a string! `print` is `putStrLn . show` so anything which has an instance of `Show` can be printed (and ByteStrings definitely do). The problem here is that `simpleHttp` returns an `IO ByteString` where `print` expects a nice pure value. So what you need to do is somehow take this function that expects a pure value, and instead pass it a with context. Enter `fmap` or its operator equivalent `&lt;$&gt;`. `print &lt;$&gt; getHaskell` will give you the output you want
Name inspired by Atlanta?
Won't you obain a \`IO (IO ())\` ? which I don't think is a wanted output. I think you need \`print =&lt;&lt; getHaskell\`
Yeah, you're totally right, thanks! Long day and out of habit I always try to reach for functor and applicative before monad :)
What is the argument of type c for
I don't think I understand your restrictions - what code golf madness is this? That said, fold f b c as = foldl (\b' a -&gt; f b' c a) b as And :t fold fold :: Foldable t1 =&gt; (t2 -&gt; t3 -&gt; t4 -&gt; t2) -&gt; t2 -&gt; t3 -&gt; t1 t4 -&gt; t2
You could do this? fold' :: (b -&gt; c -&gt; a -&gt; b) -&gt; b -&gt; c -&gt; [a] -&gt; b fold' f b c as = foldl (aug f c) b as where aug g n = (backwards g) n backwards g' x y = g' y x I'm not sure if you're asking the question you've meant to ask, as it's really hard to figure out what you could do with `c` in your example other than just overload your original argument with it. AFAICT, that's actually the -only- thing we could do with it given that type signature. 
I have the same problem with my heap of pdfs, so thanks for solving it!
I mean it will involve something that feels like metaprogramming I guess, but there is definitely no fundamental reason you truly need TemplateHaskell. I think better support for flexible anonymous structural types + their introspection and manipulation would perhaps make it easier. 
I'm a couple of cups of tea in, but I have a few questions: 1. What is the purpose of the `data Defn = Defn` declaration, and why does it appear in the newtype data constructors like `newtype Reverse xs = Reverse Defn`? It it that it makes the newtype declarations of library-specific definitions (like `IsNil`, `IsCons`, `Reverse`) coercible in and out of `Defn`? Then, because you don't export the constructors of these library-specific definitions, the only way to get the exact properties you're interested in is to use the library correctly? (The library author is allowed to use `defn :: Defining f =&gt; a -&gt; (a ~~ f)`?) 2. Why do the natural deduction functions like `andIntro :: p -&gt; q -&gt; Proof (p &amp;&amp; q)` use bare `p` and `q` as arguments and not `Proof p`/`Proof q`? 3. The `ListCase` type and the `classify` function in figure 11 smells like there should be some generic way to lift case-analysis. Take a data type `Foo = Bar | Baz`, generate `IsBar`/`IsBaz` definitions and `FooCase = IsBar (Proof IsBar) | IsBaz (Proof IsBaz)` witnesses, and so on. 4. Are there plans to make a library out of this stuff?
Beam does not use template haskell for anything. It is certainly not fundamental. Beam is also usable without any meta-programming at all, although I certainly provide a lot of helpers that do rely on meta-programming via Generics.
&gt; How do Esqueleto queries compose? Like a Monad Only in the most liberal sense. For example, suppose you had an esqueleto query `myEsqueletoQuery`. Now suppose you had two higher-order queries top5 q = do q E.limit 5 next10 q = do q E.offset 10 Intuitively, `top5` returns the first five results of the given query, and `next10` drops the first ten. Suppose `myEsqueletoQuery` returns 20 rows by itself, ordered and labeled R1 - R20. Then, I would expect `top5 (next10 myEsqueletoQuery)` to return R11-R15. This is what esqueleto would return, because it would generate a `LIMIT 5 OFFSET 10` clause (or the equivalent), However, `next10 (top5 myEsqueletoQuery)` ought to return zero rows, because if you select the top 5 you are left with 5 rows and then if you offset by 10 you end up with zero. While this is a well documented part of esqueleto, it will inevitably lead to surprising results as you abstract queries. This was my initial motivation to write beam -- correctness in the face of composability. Beam correctly respects ordering and will generate a subquery in the second case to get around the `LIMIT/OFFSET` behavior. I don't mean this as a criticism, because I've used esqueleto quite extensively. However, &gt; If somebody implements something better than Esqueleto I will happily move over, but AFAICT most of the people implementing the newer SQL EDSL libraries hadn't used or learned Esqueleto before they set out to write a new library I don't think this is true at all, and honestly I think this statement is rather. The various other SQL libraries all have various different foci. I've personally used esqueleto, opaleye, and played around with selda and squeal. They're all pretty nifty. I think it's unfair to claim that we haven't used Esqueleto or didn't use it correctly, when there are clearly some deficiencies for certain use cases. For example, here is a query, that I can't figure out how to represent in esqueleto SELECT "t3"."Name" AS "res0", "t2"."Title" AS "res1", "t1"."Name" AS "res2" FROM (SELECT "t0"."AlbumId" AS "res0", "t0"."GenreId" AS "res1", COUNT(*) AS "res2", MAX(COUNT(*)) OVER (PARTITION BY "t0"."AlbumId") AS "res3" FROM "Track" AS "t0" GROUP BY "t0"."AlbumId", "t0"."GenreId") AS "t0" INNER JOIN "Genre" AS "t1" ON ("t0"."res1") = ("t1"."GenreId") INNER JOIN "Album" AS "t2" ON ("t0"."res0") = ("t2"."AlbumId") INNER JOIN "Artist" AS "t3" ON ("t2"."ArtistId") = ("t3"."ArtistId") WHERE (("t0"."res2") = ("t0"."res3")) AND (("t0"."res0") = ANY (SELECT "sub_t0"."AlbumId" AS "res0" FROM "Track" AS "sub_t0" GROUP BY "sub_t0"."AlbumId" HAVING (COUNT(DISTINCT "sub_t0"."GenreId")) &gt; (1) ORDER BY "sub_t0"."AlbumId" ASC)) This query over the Chinook example database first associates one genre with each album in the database based on which genre has the most tracks in a given album. It then filters out all albums that do not have more than one genre, and then returns the artist name, album title, and genre of each album. In english, it returns the album name, artist name, and most common genre of each album that contains tracks of more than one genre. This query is taken from a set of real queries performed against this database. I have no idea how to represent it in esqueleto. On the other hand, in beam 0.8.0.0 (on github), it's straightforwards, and we get to abstract out subqueries freely. do let albumGenreCnts = aggregate_ (\t -&gt; ( group_ (trackAlbumId t) , group_ (trackGenreId t) , as_ @Int countAll_ )) $ all_ (track chinookDb) albumAndRepresentativeGenres = withWindow_ (\(albumId, _, _) -&gt; frame_ (partitionBy_ albumId) noOrder_ noBounds_) (\(albumId, genreId, trackCnt) albumWindow -&gt; (albumId, genreId, trackCnt, max_ trackCnt `over_` albumWindow)) $ albumGenreCnts (albumId@(AlbumId albumIdColumn), genreId, _, _) &lt;- filter_' (\(_, _, trackCnt, maxTrackCntInAlbum) -&gt; just_ trackCnt ==?. maxTrackCntInAlbum) $ albumAndRepresentativeGenres genre_ &lt;- join_' (genre chinookDb) (\g -&gt; genreId ==?. just_ (primaryKey g)) album_ &lt;- join_' (album chinookDb) (\a -&gt; albumId ==?. just_ (primaryKey a)) artist_ &lt;- join_ (artist chinookDb) (\a -&gt; albumArtist album_ `references_` a) -- Filter out all albums with tracks of only one genre guard_' (albumIdColumn ==*. anyOf_ (orderBy_ asc_ $ fmap (\(AlbumId albumIdRaw, _) -&gt; albumIdRaw) $ filter_ (\(_, genreCntByAlbum) -&gt; genreCntByAlbum &gt;. 1) $ aggregate_ (\t -&gt; let GenreId genreId = trackGenreId t in ( group_ (trackAlbumId t) , as_ @Int (countOver_ distinctInGroup_ genreId))) $ all_ (track chinookDb))) pure ( artistName artist_, albumTitle album_, genreName genre_ )
&gt; the people implementing the newer SQL EDSL libraries hadn't used or learned Esqueleto before they set out to write a new library I used Esqueleto long enough to find this SQL crash bug https://github.com/prowdsponsor/esqueleto/issues/41 and this failure of referential transparency https://github.com/prowdsponsor/esqueleto/issues/40 
I'd be happy to provide details on beam. I keep a complete compatibility matrix of all SQL features here: http://tathougies.github.io/beam/about/compatibility/ . There are also a lot of features in the beam-0.8.0.0 branch. Beam also supports a lot of extensions in its various backends, like Postgres LOCKING, Postgres JSON, MySQL JSON, etc. Beam's support is rather complete, and the SQL generated very straightforwards. For example, I don't think any other library supports window functions (I could be wrong). For example, here's a rather complicated beam query: do let albumGenreCnts = aggregate_ (\t -&gt; ( group_ (trackAlbumId t) , group_ (trackGenreId t) , as_ @Int countAll_ )) $ all_ (track chinookDb) albumAndRepresentativeGenres = withWindow_ (\(albumId, _, _) -&gt; frame_ (partitionBy_ albumId) noOrder_ noBounds_) (\(albumId, genreId, trackCnt) albumWindow -&gt; (albumId, genreId, trackCnt, max_ trackCnt `over_` albumWindow)) $ albumGenreCnts (albumId@(AlbumId albumIdColumn), genreId, _, _) &lt;- filter_' (\(_, _, trackCnt, maxTrackCntInAlbum) -&gt; just_ trackCnt ==?. maxTrackCntInAlbum) $ albumAndRepresentativeGenres genre_ &lt;- join_' (genre chinookDb) (\g -&gt; genreId ==?. just_ (primaryKey g)) album_ &lt;- join_' (album chinookDb) (\a -&gt; albumId ==?. just_ (primaryKey a)) artist_ &lt;- join_ (artist chinookDb) (\a -&gt; albumArtist album_ `references_` a) -- Filter out all albums with tracks of only one genre guard_' (albumIdColumn ==*. anyOf_ (orderBy_ asc_ $ fmap (\(AlbumId albumIdRaw, _) -&gt; albumIdRaw) $ filter_ (\(_, genreCntByAlbum) -&gt; genreCntByAlbum &gt;. 1) $ aggregate_ (\t -&gt; let GenreId genreId = trackGenreId t in ( group_ (trackAlbumId t) , as_ @Int (countOver_ distinctInGroup_ genreId))) $ all_ (track chinookDb))) pure ( artistName artist_, albumTitle album_, genreName genre_ ) Intuitively, it returns a list of albums which contain tracks of more than one genre, along with the name of the most common genre in that album and the name of the album's primary artist. The SQL generated is what you'd expect, but beam's type inference and composability mean you can abstract out the various subqueries (`albumGenreCnts`, etc). The following query is what is in the auto generated documentation for the latest beam branch (beam-0.8.0.0, on github). SELECT "t3"."Name" AS "res0", "t2"."Title" AS "res1", "t1"."Name" AS "res2" FROM (SELECT "t0"."AlbumId" AS "res0", "t0"."GenreId" AS "res1", COUNT(*) AS "res2", MAX(COUNT(*)) OVER (PARTITION BY "t0"."AlbumId") AS "res3" FROM "Track" AS "t0" GROUP BY "t0"."AlbumId", "t0"."GenreId") AS "t0" INNER JOIN "Genre" AS "t1" ON ("t0"."res1") = ("t1"."GenreId") INNER JOIN "Album" AS "t2" ON ("t0"."res0") = ("t2"."AlbumId") INNER JOIN "Artist" AS "t3" ON ("t2"."ArtistId") = ("t3"."ArtistId") WHERE (("t0"."res2") = ("t0"."res3")) AND (("t0"."res0") = ANY (SELECT "sub_t0"."AlbumId" AS "res0" FROM "Track" AS "sub_t0" GROUP BY "sub_t0"."AlbumId" HAVING (COUNT(DISTINCT "sub_t0"."GenreId")) &gt; (1) ORDER BY "sub_t0"."AlbumId" ASC)) I wrote beam to be able to write queries like this in Haskell, by building them up from smaller building blocks.
Hi, I'm now learning about Haskell and have made a library, simple number guessing game. Now I'm going to try to build web application by Haskell. But there are lots of web application frameworks. https://wiki.haskell.org/Web/Frameworks Which haskell web app framework should I learn?
Thanks for the sharing :)
As with so many things, it depends: * Yesod is a full-stack framework with embedded DSLs for routes, persistence, templates, Javascript, and stylesheets. It heavily uses Template Haskell for code generation and compresses resourdes. This means that runtime speed is probably decent. In return there are cases where compile speed might get slow. Use it if you want an integrated full-stack framework. * Servant makes it possible to specify a REST API using the type signatures of your routes. You implement them, and you can also extract a documentation for your API. Sounds good for designing REST applications and Microservices. * Snap is the least principled. You get a request object and you create a response to return. There are lots of extras and middlewares, like a routing filter and static file handlers. If you have experience with microframeworks you'll get up to speed quickly.
Drat, this is the one I picked when I had to find a Markdown library a few weeks ago.
FYI, this is happening: https://github.com/jgm/commonmark-hs It is nicely split out into different packages with the core parsing one having dependencies that all already ship with GHC. It is also nicely extensible, supporting custom inline and block elements. The author is John MarFarlane (also author of Pandoc and the Commonmark spec). I hope the Haskell community bands behind this package the way it has banded behind `prettyprinter` for pretty-printing.
Libraries such as `cmark` or `sundown` unfortunately rely on C routines for parsing markdown which limit their applicability in terms of portability (and there's also security aspects to be considered). Mark's recent blogpost [Announcing MMark](https://markkarpov.com/post/announcing-mmark.html) gives a nice overview of the existing markdown implementations as well as motivating why the (imho very promising) [`mmark`](http://hackage.haskell.org/package/mmark) library was created. 
Indeed. That's why we need linear types :)) But there is tons of evidence of causation on that one, it's a broad social behavior which I imagine has been extensively studied in experimental psychology. People tend to thrive for something "better", but usually do not know what it is better. So they use proxy, vague, erroneous approximation to a fault, to the point where it's engrained in societal standards. Sending the opposite signal is a material, observable cause for a strong repulsive effect for a general audience, whatever reason one can find to explain it (solitary activity, etc..)
&gt;But there is tons of evidence of causation on that one, Huh? How is there even *any*? &gt;Sending the opposite signal is a material, observable cause for a strong repulsive effect for a general audience, whatever reason one can find to explain it (solitary activity, etc..) You are missing the point. There is no social signal in a solitary activity. So it isn't an explanation. Men are sitting alone at home with their laptops, programming in Haskell. Women aren't doing the same thing. The men aren't doing it because they find anyone attractive and the women aren't doing something else because they find anyone repulsive. Men don't attend things like Haskell conferences to meet women. They are a total sausage fest. Horrible way to meet women. Something inside the men makes them want to attend even in spite of the lack of sexual opportunity. 
I'll definitely try your solution, thank you=) On a side note, did you try using any reference management utilities? IIRC, the title/author suggestions by Mendeley were quite good (except for djvu converted to pdf, of course). And if you are keep on downloading new pdfs, Zoltero should be quite useful, too.
Is there a stylistic or other preference between code that: combines monad and functor, main = do all_phrases &lt;- lines &lt;$&gt; readFile "src/words.txt" return () Or else uses a monad, with non-monadic expression, main1 = do s &lt;- readFile "src/words.txt" let all_phrases = lines s return () 
Start with scotty. By the time you hit its limitations you'll be able to move unto Spock or Yesod. Or Servant, if you need a backend-only server. Alternatively, you can go with raw WAI to get a grip on how haskell web goes on the low level.
Why limit yourself to Edward's way of structuring programs?
At least it does that only once per major snapshot version...
&gt;I hope the Haskell community bands behind this package the way it has banded behind prettyprinter for pretty-printing. I don't think it has banded behind prettyprinter at all. But settling on a single pretty printing package would be neat.
&gt; Libraries such as cmark or sundown unfortunately rely on C routines for parsing markdown which limit their applicability in terms of portability Specifically, GHCJS won't be able to easily use these libraries.
I'd use the sdl2 package. This [trivial early 90's style web browser](https://github.com/chrisdone/vado#building) is buildable on Windows, Linux and OS X (incl. high DPI support), with instructions for each.
It's actually from an old NES game that I had as a kid, but I like the idea that it has multiple references :)
I haven't tried out too many other utilities yet, because I couldn't find any as minimalist as I wanted. But I'll check out the ones you named to see if I can steal some ideas!
It's pretty clear that neither of us have ever been to Berkley, given the very tame nature of my initial statements. I do concur we should, as a community, talk about all these interrelated facts often. What I see as an opportunity, a pressing need in fact, is to create documentation and examples that cater to a broader audience; more languages, more disciplinary backgrounds. This could be one way forward to broaden adoption.
Thanks for the link, I'd completely missed that blog post.
That's a great link, thanks! I'm still trying to wrap my head around the different options John is providing between mmark, cmark, and this repo.
Hi Chris, It's very hard for me to respond to these challenges without more details. Last time we discussed this on Twitter you kindly offered to write a blog post going into more detail. I don't think one has been forthcoming but it would be great to read one so that I can know how to improve the library or its documentation. &gt; How do Esqueleto queries compose? Like a Monad. The problem with using a monad for representing queries (as opposed to an arrow) is that unless you are careful (like Beam and Selda are) you [end up permitting crashing queries](https://github.com/prowdsponsor/esqueleto/issues/41). &gt; Familiarity with HaskellDB would help avoid some unforced errors too. Could you go into more detail? I was intimately familiar with HaskellDB before writing Opaleye. The problems are that it (like Selda) doesn't allow you to store rows in arbitrary Haskell types (you have to use `HDBRec`) and it's riddled with bugs: * https://github.com/m4dc4p/haskelldb/issues/22 * https://github.com/m4dc4p/haskelldb/issues/18 * https://githhttps://github.com/m4dc4p/haskelldb/issues/15ub.com/m4dc4p/haskelldb/issues/17 * 
I have had problems when compiling in Windows 10: - Backpack uses very long paths that reach the maximum Windows length. I have tried disabling the MAX_PATH limitation in Windows but it doesn't solve the problem. - Sometimes I get "folder not empty" errors when building projects. - There are problems when linking with network: C:\Users\Whatever\AppData\Roaming\cabal\store\ghc-8.4.3\network-2.7.0.0-9128498960cdf352573e0b25abee5ccd62d3a339\lib/libHSnetwork-2.7.0.0-9128498960cdf352573e0b25abee5ccd62d3a339.a(Socket.o):fake:(.text+0x12dfc): undefined reference to `acceptNewSock' C:\Users\Whatever\AppData\Roaming\cabal\store\ghc-8.4.3\network-2.7.0.0-9128498960cdf352573e0b25abee5ccd62d3a339\lib/libHSnetwork-2.7.0.0-9128498960cdf352573e0b25abee5ccd62d3a339.a(Socket.o):fake:(.text+0x12f62): undefined reference to `newAcceptParams'
Yes, there is one more wrapping/unwrapping to do if you use `&lt;$&gt;`. But that should only be a concern if you use deep monad transformer stack or within a tight loop. Inlining could also matter. Stylistically, I'd prefer to spend a short identifier to keep a complex line simple. Especially if the first argument to `&lt;$&gt;` is not trivial.
&gt; Then, because you don't export the constructors of these library-specific definitions, the only way to get the exact properties you're interested in is to use the library correctly? (The library author is allowed to use `defn :: Defining f =&gt; a -&gt; (a ~~ f)`?) Yes, exactly. It gives a mechanism for library authors to introduce names, without extending that capability to the users of those libraries (for those same names, anyway). &gt; Why do the natural deduction functions like `andIntro :: p -&gt; q -&gt; Proof (p &amp;&amp; q)` use bare `p` and `q` as arguments and not `Proof p`/`Proof q`? It might be that you can do it either way, I'm not entirely sure. Here, I was thinking of a bare `p` as a premise to an argument, and `Proof p` as a completed proof of `p`. The distinction is a bit more useful in rules like `not_intro :: (p -&gt; Proof FALSE) -&gt; Proof (Not p)`, which you could read as "If you can prove `FALSE` by assuming `p`, `not_intro` lets you prove `Not p`." &gt; The ListCase type and the classify function in figure 11 smells like there should be some generic way to lift case-analysis. Absolutely! I should add that bit of TH to the library. &gt; Are there plans to make a library out of this stuff? Yes indeed! I have a `gdp` library where I was doing some of the initial experimentation for this paper. I'm currently cleaning out old, failed experiments and adding documentation for a release. Hopefully later this week.
As for the first issue, a common workaround is to place the nix-style store to a shorter base-path, as in e.g. cabal --store-dir=C:\SR new-build There's ongoing work to reduce the impact PATH-related limits on windows in future GHCs, but for now we still need these workarounds. As for the other two issues, I'll ask around.
Thanks! I'm using cabal new-build, with [this project](https://github.com/danidiaz/thrifty-sailor/tree/0.2.1.0) (without the cabal-freeze). A branch without Backpack but which still suffers from the linking problem can be found [here](https://github.com/danidiaz/thrifty-sailor/tree/no-backpack) For the network linking issue, perhaps I could get around using the network that comes with the full platform.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [danidiaz/thrifty-sailor/.../**0803db5d8cd60358076e6322ee280284443b932a** (no-backpack → 0803db5)](https://github.com/danidiaz/thrifty-sailor/tree/0803db5d8cd60358076e6322ee280284443b932a) ---- 
Only looking for a place to start since I'm new to the language. Do you have any suggestions? Doesn't have to be one of Edward's projects.
OP here. I was planning to take the approach of this post and create a walk-through for other libraries I am exploring. Any feedback on this approach is much appreciated.
This is a summary of reading/skimming through [Snoyman's blog post](https://www.snoyman.com/blog/2018/06/deprecating-haskell-markdown-library), [Hackage's list of Markdown-related packages](https://hackage.haskell.org/packages/search?terms=markdown), [Wikipedia's article on Markdown standardisation](https://en.wikipedia.org/wiki/Markdown#Standardization), [GitHub's GFM spec](https://github.github.com/gfm/) (GFM is a superset of CommonMark). &gt;There are too many different flavors of markdown floating around today, and I'm contributing to the problem by having yet another flavor. This is a very reasonable argument. &gt;I'm also open to alternative solutions here, like using the markdown package namespace for a better implementation. The best default choice would be a pure Haskell implementation of CommonMark. According to u/newtyped, [John MacFarlane's `commonmark`](https://github.com/jgm/commonmark-hs) (not his `cmark`) is the best candidate for that. But Hackage/Stackage doesn't support package aliases, does it? I'd probably like the name `commonmark` better than to have that implementation put in a new major version of `markdown`. Here's a quick review of packages related to Markdown on Hackage: * `cmark` is the most popular C wrapper for CommonMark. * `cmark-gfm` is a fork of `cmark` that handles GitHub's GFM superset of CommonMark. * `sundown` is another popular C wrapper for GitHub's Markdown. The last commit to the C code is six years old, but GitHub's GFM was standardised in 2017, so the implementation is not necessarily on par with the spec. Also, `cmark`'s documentation says that it "raised a malloc error when compiled into our benchmark suite." * `discount` and `hdiscount` are wrappers around [a C library called discount](http://www.pell.portland.or.us/~orc/Code/discount/). * `cheapskate` is pure and forgiving, but does not advertise that it follows any standards and is labelled as "Experimental markdown processor". It appears robust and definitely serves a purpose, but is perhaps not the least opinionated choice. * `mmark` is pure and unforgiving, it explicitly deviates from both CommonMark and GFM, but states exactly how. It's a solid piece of software, but is also not standardised or the least opinionated choice. * `markdown-pap` is very alpha and supports a limited subset of Markdown. * `comark-parser` / `comark-syntax` is another CommonMark library. By the way, I noticed that * `yesod-markdown` depends on the `pandoc` package * `yesod-text-markdown` depends on the `markdown` package
I like this kind of tutorials, please continue.
Thanks to all the library authors for taking on this hard problem. My work relies on having such libraries available, otherwise, I wouldn't be able to use Haskell.
You're welcome! If you're ever using Opaleye and need any help feel free to [file an issue](https://github.com/tomjaguarpaw/haskell-opaleye/issues/new) or [drop me an email](https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/README.md#contact-the-author).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/haskell-opaleye/.../**README.md#contact-the-author** (master → d336711)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/d336711d5df7ed4865107e27b37d147240141504/README.md#contact-the-author) ---- 
* [Matterhorn](https://github.com/matterhorn-chat/matterhorn) - A mattermost terminal client in Haskell using the brick library. * [Ninjas](http://hackage.haskell.org/package/Ninjas) - A simple multiplayer game using Gloss. 
I believe we're in the process of banding to prettyprinter. I recently accepted a PR that changed the pretty printer in `logging-effect` to use `prettyprinter` (https://github.com/ocharles/logging-effect/pull/25). I encourage others to make the change, it's a bloody lovely library.
In a case like: data Todo = Todo [String] newtype Todo2 = Todo2 [String] Is there any practical reason to prefer 'Todo' instead of 'Todo2?' I have seen this question before but the answers usually descend into people quibbling about bottom, and I've yet to be able to translate those discussions into practically applicable advice.
I think both are doable. For automatic logging, take a look at [`what-it-do`](https://github.com/ocharles/what-it-do).
Well the only difference between the two _is_ bottom so those "quibbling"s are on point. The question you might want to ask is when a lifted bottom is useful. Consider knot tying as one case where having a lazy structure is handy.
Thanks! All of these look interesting!
So, phrased another way, the whole 'equality of run-time representation' thing would mean that the evaluation of the 'contents' of the newtype constructor would by definition, need to have the same strictness semantics as the 'unwrapped' representation?
Yes, unfortunately my predecessor wasn't as bothered by that stuff as I am. In practice, I've found Esqueleto more productive and reliable for my work. I wish it were not so. I'd happily use something more sound. A friend of mine took an interest in Bluetick recently so maybe I'll be able to get something that matches the safety of HaskellDB, the composability of Esqueleto and Opaleye, and that is a good citizen on top of Persistent. Here's hoping.
fwiw, that doesn't seem to be the network that comes with the full platform -- that's one coming from new-build, since it is in the `store`. Also, it is 2.7, and the platform still ships 2.6.3.5. So the issue is specifically that the new-built 2.7 appears to build but not link. Hmm.. 
I did the decision about a year ago and I have decided against doing it all haskell. - the SSL layer in haskell is made completely in haskell (which doesn't mean it's bad - but it's a question if you want to use it as a central proxy). - the proxying code is supposed to do all kinds of work/checks and it takes a lot of time to make it work right After doing some research I ended up deploying `haproxy`. People seem to be happy with it, it has all the bells and whistles regarding TLS support, service health discovery and it is widely used. I'm running it on AWS, the microservices have a cron job running every minute, updating the data in a DynamoDB table. The proxy is periodically checking the table; when it discovers a new service, it creates a DNS record (using `route53`) and waits for it to be propagated, asks for letsencrypt certificate and sets up the haproxy configuration properly. It also periodically runs renew on the enabled certificates. I did this in haskell; I had some problems with haproxy config reloads, I used the let's encrypt's `certbot` and didn't like it (there are various other clients available now, I'd suggest using something else). Apart from one problem of the process writing to dynamodb getting stuck (added debug prints, hasn't happen since then), it works nicely.
I am not trying to explain anything, I am informing you : well groomed, stylish, good smelling people play a role in social desirability. Just like the converse, lack thereof, plays a massive negative role. You don't even have to take my word for it.
Ok, I'm tracking down a theory. Can you check your cabal config file (C:\users\$NAME\AppData\Roaming\cabal\config) and provide the values for extra-include-dirs, extra-lib-dirs and extra-prog-path? 
Hi, thanks for looking at this. I have checked config and the values are: -- extra-include-dirs: -- extra-lib-dirs: extra-prog-path: C:\Users\$NAME\AppData\Roaming\cabal\bin Also, I have installed the full Haskell platform in another windows machine, and "pinned" the versions of native-dependent packages like `directory` and `network` to [those provided by the platform](https://www.haskell.org/platform/contents.html). That way it worked fine from cmd, I didn't have to compile using cygwin. I haven't encoutered the "folder not empty error" again, either.
The installer is supposed to automatically update the config file for you -- and I checked and it typically does (So I don't know what happened here -- you may have opted out). The values you want to add are: -extra-prog-path: C:\Program Files\Haskell Platform\{{hpVersion}}\msys\usr\bin -extra-lib-dirs: C:\Program Files\Haskell Platform\{{hpVersion}}\mingw\lib -extra-include-dirs: C:\Program Files\Haskell Platform\{{hpVersion}}\mingw\include You don't need to remove the existing ones entry for extra-prog-path. That should teach cabal how to find all the proper bits and pieces to allow linking against msys libraries.
If only we could settle on a single randomness implementation and a single recursive-descent parser impementation as well.
Good job. This is what I would to write myself, API looks really accurate.
Indeed, adding those entries makes linking work. Thanks again for you help.
&gt;If somebody implements something better than Esqueleto I will happily move over, but AFAICT most of the people implementing the newer SQL EDSL libraries hadn't used or learned Esqueleto before they set out to write a new library. Familiarity with HaskellDB would help avoid some unforced errors too. I wrote [sql-fragment](http://maxigit.github.io/sql-fragment/index.html) (documentation [there](http://maxigit.github.io/sql-fragment/index.html)) a fews years ago, as a proof of concept that a type safe SQL EDSL can be written as simple SQL combinators, using only operators instead monad or arrows. My main motivation at the time was to implement a proof of concept of what I call autojoin hoping that it will get some interest from the community. ( I had at the time a few emails with the funder of Esqueletto about it, but it didn't seem interested). The idea of autojoin is, instead of having to rewrite all the time the same joins in different query. Let's write once for a big join-graph of all the tables then just pick and chose the fields that we need and let the system include the required tables as well as the join between them. As I said, previously, this can be done in a type safe way, with just simple operators. The main idea behing sql-fragment (which makes it different from other approach), is instead of defining table, one define individuals fields. For example we can define a user name as userName = "[user.name](https://user.name)" :: SQlFragment '\[Text\] '\[\] This mean the field \`name\` from the table \`user\`. \`'\[\] '\[\]\` are phantom types to specify that the fragment returns one fied of type \`Text\` and doesn't require any parameter. We can add a few more fields userId = "[user.id](https://user.id)" :: SqlFragment '\[UserId\] '\[\] postComment = "post.comment" :: SqlFragment '\[Text\] '\[\] postUser = "post.user\_id" :: SqlFragment '\[UserId\] '\[\] We can now find all the comment joined with the user name with universe = userId !&lt;--&gt;! postUser -- we join [user.id](https://user.id) to post.user\_id toSelectQuery ( postComment !#! userName \`join\` universe) (The \`!#!\` operator just concatene 2 fragments. Fragments can come from different tables, select some columns, filter some rows, or a mix of both etc ....). Also, one fragment can select all the columns of table and be used to load a full record if necessary, and the autojoin can work with more than two tables (if A is joined to B and B is joined to C. select something from A and C Will automatically include B). Fragment could be generated from a record using TH, or the other way around. I have a script which generate fragments definition from a database. Under the hood, a fragment is just a \`Map Section \[Text\]\` where section can be a column, a where clause, a group clause etc ... Unfortunately, this project is half-dead for a few reasons : \- Type inference appeared much more difficult than expected. To be usable (without having to write every single type signature by hand), I needed injective type family. This extension didn't exist at the time, so I had to give up sql-fragment for the project I was working on and therefore don't really need it anymore. \- Defining a join graph for a real world db, is in practice pretty difficult as it needs to acyclic. The DB I was workin at lot of cycles, and the way to solve the problem was either to define a few big-graph or a lots of small ones. \- I tried to deal with units (via the \`dimensional\` package). which unfortunately introduced a major release with breaking changes. I haven't managed to update \`sql-fragment-mysql\` with the new version of \`dimensional\`, so I can't use it for my new projects. I don't have much time for it and as nobody is interested ... (even me : I manage at the moment with just Persistent or MySQL-simple depending on the projects) there is no real motivation to try to improve it. Having said that, I would love some of its idea to be reused elsewhere ;-) I wish I could give a try Esqueleto, but unless things have changed, it required postgre and I have to use MySQL.
Why switching from Groundhog to Persistent ? I was thinking doing the otherway around ....
I was wondering the same and I think I have figured out the answer. A linear function returning `Unrestricted` guarantees that it can't secretly return its argument. In particular the continuation passed to `newMArray` cannot leak the `MArray` that it's given. There is no linear function that takes an `MArray` and returns `Unrestricted MArray`, and you can't use the constructor `Unrestricted` on it, because said constructor is not a linear function so, from the point of view of the type system, might consume its argument multiple times (it doesn't, but the type checker trusts the types). So there is no way to restrict a function to always return a linear value, but you can restrict it to _not_ return a linear value.
Is there a difference (semantic or performance etc ...) between a plain product type and a newtype of the equivalent tuple ? (example `data A = A Int Double` and `newtype = A (Int, Double)` ?
What an abstract algebra library not written by Ed Kemet??? I love to use of doctest in the instances!
&gt; It might be that you can do it either way, I'm not entirely sure. I haven't had time to try, but I read the "assuming" out of being on the left hand of the `(-&gt;)`. `Proof p -&gt; Proof q -&gt; Proof (p &amp;&amp; q)` reads to me like "if you can prove `p` and you can prove `q`, then you can combine them to prove `p &amp;&amp; q`. Similarly for a hypothetical `not_intro :: (Proof p -&gt; Proof FALSE) -&gt; Proof (Not p)`: "if you assume you can prove `p` and from that can prove `FALSE`, then you have a proof of `Not p`. If it works, making all the arguments have the `Proof p` shape means that if some other type `t` leaks into your statements, you can't accidentally prove it by using a value of type `t` as witness. I could write `andIntro 3 "Hello" :: Proof (Int &amp;&amp; String)` with the current toolkit, whatever that means. It might also be worth trying to collect the logic bits into a single data constructor and using `-XDataKinds`? Then your natural deduction toolkit can only operate on proofs of the correct kind. (Keep `Proof` poly-kinded, I think, because then you can use it for other logics?)
Q. Why have the laws been duplicated for the different instances? The excess comments on the right hand side seem a bit distracting... For example, [here](https://hackage.haskell.org/package/semilattices-0.0.0.0/docs/Data-Semilattice-Meet.html), I understand that IntSet doesn't have an upper bound, hence it doesn't have the identity law but I'm not sure if it is worth duplicating all the information except for tricky cases, if any.
I believe there is no difference because there is no difference between a 2-tuple and a data type with two fields and newtypes don't affect bottom or performance, so transitively a 2-tuple wrapped in a newtype should be equivalent
Without loading the code into GHCi myself, I think the issue is with `hash \`div\` numLetters`. `div` has type `Integral a =&gt; a -&gt; a -&gt; a`, meaning that both `a`s need to be the same, but `numLetters` isn't guaranteed to be an `Int32`.
I'm glad to have found out about [`prettyprinter`](http://hackage.haskell.org/package/prettyprinter). It does look very nice. Thanks.
What are the differences between this and [Algebra.Lattice](http://hackage.haskell.org/package/lattices-1.7.1/docs/Algebra-Lattice.html) from lattices?
unicode operators ∧ and ∨ ?
I think this is a good question. Anyone who has search hackage knows how many duplicate libraries there because everyone has to have their own version. Not saying this one is bad, just saying it might be unnecessary and confusing to people who are looking for a semilattice library. 
You can also do case expressions for non-bool situations.
What would those look like 
Hmm, https://www.haskell.org/tutorial/patterns.html this seems to cover it all pretty well.
doesnt the fact that I have this type definition `type Hash = Int32` `baseNLetters :: Integral a =&gt; a -&gt; Hash -&gt; a -&gt; a -&gt; a` mean that its guaranteed to be an `Int32` or else the function wont even bother looking at it?
In practice I rarely if ever use if-then-else. Case expressions tend to do the job the vast majority of the time since most conditional checks are structural and not value based.
Don't forget guards too! They are like generalized if's.
Thank you for answering. I got a step up road image. scotty sounds samll and suitable for my first step. But I have a question. Is WAI framework the de facto standard or something like that?
Thank you answering. I got detailed comparison between them. I'm interested in snap, but I heard it does not based on WAI? I don't know WAI well. Is that important?
Not just "like". You can straight up use them that way if you want: case () of () | condition1 -&gt; foo | condition2 -&gt; bar | otherwise -&gt; baz 
If you want to use guards in an expression you can use the `MultiWayIf` syntactic extension. https://downloads.haskell.org/~ghc/8.4.1/docs/html/users_guide/glasgow_exts.html#multi-way-if-expressions if | condition1 -&gt; foo | condition2 -&gt; bar | otherwise -&gt; baz 
Yeah, `MultiWayIf` is really nice. One of those really nice language extensions that only has upsides. 
Is there a straightforward to use string formatting library, similar in spirit to Python's built in "string.format(...)` method? I am talking of run time operation, not _(or not only)_ compile time. My desires are that it supports both `Text` and `String`, and that it looks nice and tidy in code. My use case is formatting errors and other user interaction messages. I am currently writing along the lines of `error $ "Error at " ++ show offset ++ ": "Expected " ++ show expected ++ " while received " ++ show actual ++ "."` and I hate it.
The [author writes](https://twitter.com/rob_rix/status/1007652612433502208) &gt; Join isn’t a superclass of Lower, Meet isn’t a superclass of Upper, etc. I'm not entirely convinced this is good. The documentation for `Upper` has laws like: &gt; If s is Bounded, upperBound = maxBound &gt; If s is a Meet semilattice upperBound \/ a = a These are all reasonable things to declare, but conditional laws on type classes make them very hard to reason about.
That was fixed last year [here](https://github.com/chrisdone/formatting/commit/7bd7a6138e157c1d67cf5b35e129443faa92b5b3). Maybe just update to formatting-6.3.4? Pop `formatting-6.3.4` in your `extra-deps` in your `stack.yaml`. By the way, your approach is fine. It could be a bit shorter: later (\n -&gt; bprint ((if n&lt;0 then "-" else "") % commas) (abs n)) `bprint` is kind of `id` in formatting (literally: `bprint m = runFormat m id`), so it's not heavy.
The benefit seems to be that WAI has become a de-facto standard for a HTTP inteface in the Haskell ecosystem. I presume you'd have less trouble porting WAI apps to another WAI platform.
Pretty much exactly the sort of thing I was wondering how to do this morning for a little toy project. Nice one mate.
There's a difference for Generics, which will see either a constructor with two fields or a constructor with one field, which can result in different behaviors when deriving instances via Generics.
What do you think of fmt?
I think it is not aesthetically pleasant. I understand it may have its benefits. But I cannot like it. Compare "Error at {}: Expected..." [offset, ...] — With: "Error at"+|offset|+": Expected..." The latter is not much nicer than what I would write with `&lt;&gt;` or `++` operators. I am looking at `text-format-simple`. Maybe I would extend it a bit and put it to use. As little known and used as it is, I find it to be without peer.
Yeah, that functor thing was totally my fault. Added it in in only one version, but still kicking myself about it.
I hope you dont mind me saying so but it would have been better to record the slides with audio alone or even edited the video with slides and audio. It's going to be tricky coordinating slides while watching a video as is which is a shame because the speakers gave brilliant talks.
Yeah, I totally agree, it's the first time we've had that issue with our cameras. For the next year we're going to record the slides through an HDMI proxy.
I see. The list makes me uneasy but it doesn't seem possible to match that simplicity with a total interface.
Perhaps an overstatement, but I feel like this could be revolutionary. I can think of so many use cases...
How is this related to the ["ghosts of departed proofs"](https://www.reddit.com/r/haskell/comments/8qn0wr/safe_api_design_with_ghosts_of_departed_proofs/) approach?
Good question. I guess I've found another paper to read.
I agree, conditional laws are tricky. But as a `lattice` maintainer I have to comment that its hierarchy isn't perfect either. As I mentioned on twitter, there might be a semilattice with both unit and annihilating element, but without dual operation. We'd need a language where defining an instance will impose new (law) proof obligations, also dependent on other existing instances. Then we could have Applicative f = (Pointed f, Apply f) (and also other means to make granular class hierarchies convenient &amp; practical to work with)
Excellent!! The world needs more tutorials like this!
May I ask you to elaborate? Specifically, which list do you mean and what is the matter with the simplicity and total interface? I find it hard to follow your thought.
You can write a single branch multi way if for partiality I think 
I don't think Esqueleto currently requires any particular DB. Opaleye requires Postgres, but Esqueleto supports anything persistent supports, albeit without portability guarantees.
So... printf from Text.Printf in base? printf "Hello %s, I am %d months old.\n" "kindaro" (32*13) Hello kindaro, I am 416 months old.
Eh... What was the matter with it? 😯
There is a good part about `printf` in its creative usage of type classes to well-type a polyary function. It makes it intuitive and concise. But there is also a downside, in that you are forced to learn arbitrary and cryptic format string notation, and even more so when you want to extend it with your custom types. In my use case, I have a type for hexadecimal values that, however simple, has its own parse and display idiosyncrasies. I do not want to go to the length of adapting `printf` to it. But overall, `printf` is nothing but an intricate and ugly remnant of the epoch that is long gone. It has no place in a reasonable and modern language.
Glad it worked well! &gt; It turns out that what I wrote was probably slow because I set up around 60k rules and not so much because of FilePath. Yep, it's currently _O(n^2)_ in some cases. I have a plan to optimise that to _O(n)_ but for real build systems I've seen it's never been an issue. Now I'll push it up the list. &gt; I imagine the Path wrapper is nice and self contained - fancy releasing it? I meant the Shake/Path wrapper combination :)
you can use fmap to construct a value inside of the Refined wrapper that no longer satisfies the predicate, p. &gt;&gt;&gt; :set -XTemplateHaskell &gt;&gt;&gt; :set -XDataKinds &gt;&gt;&gt; foo = $$(refineTH 5) :: Refined Positive Int &gt;&gt;&gt; unrefine foo 5 &gt;&gt;&gt; unrefine $ fmap negate foo -5 This value was never supposed to be NonPositive; using fmap can make it so.
Thanks. I was just surprised that you actually have to run the formatter (even with `id`) as I thought that Formatter was just a `a -&gt; Builder` in disguise. I'll probably switch to 6.3.4 as suggested. However, I still need the `later`, `bprint` combination as I was defining my own formatter `commasFixed` which displays a floating value with commas (and 2 decimals if needed) : ideal for currency amount. Here is the code in case you are interested (it could of course be adapted to chose the number of decimals ) commasFixed = later go where go x = let (n,f) = properFraction x :: (Int, Double) frac = floor (100 * abs f) fracB = if frac &lt; 1 then fconst mempty else "." % left 2 '0' %. int b = (commas % fracB) in bprint b n frac 
Ok, I'll have a go then .
I'm not sure why this has to be a separate library from refined, instead of just enriching the API of refined. All new ideas look like they can be fitted onto refined without much hassle.
I think you'll agree though that right now they are quite different. For me it was like: well, I have a completely different design for this in mind, what will I do? 1) I'll try to develop something without constraints associated with making it fit into an existing and quite different library 2) come to maintainers of `refined` and tell them that, now that they have recently (almost) re-wrote the library, it's time to redesign it again :) I chose 1. I just checked on `refined` on github, I see you started porting at least some ideas from `facts` to it, which is a good thing, because I used `refined` as an inspiration too. I think we should have both libraries for a little diversity. What I published today is just a proof-of-concept and I have no clue what mature version of `facts` will look like, so while I'm getting there, I don't want to lose this little playground and constrain my imagination.
Ah, fair enough. Choice (1) is definitely more flexible. Yeah, I've added Generic to all predicates (something small I noticed in &lt;code&gt;facts&lt;/code&gt; that I noticed I wasn't doing), and added IdPred. I've been wanting to write a better way to compose predicates/remove redundant predicates, and &lt;code&gt;facts&lt;/code&gt; provides a good way to do that. Though, I am concerned about runtime refinements being inefficient for large enough sets of predicates. i really like the ideas in &lt;code&gt;facts&lt;/code&gt; and i look forward to seeing it mature.
Fair point.
A History of Haskell: Being Lazy With Class mentions that Haskell 1.3 (May 1996) got monadic IO and do-notation.
The do-notation was invented by Mark Jones and first appeared in Gofer, IIRC. He invented the do-notation to make writing monadic code more palatable. It also resembles monad comprehension, that were invented by Phil Wadler. 
Haskell 1.3 introduced do-notation. The release notes are still online. It's a pretty entertaining history lesson: Haskell 1.3 introduced a lot of things that we take for granted today. https://www.haskell.org/definition/from12to13.html
&gt; only has upsides It messes up alignment with multiples of 2 😛.
Is there any source?
I save frames in my mellow library but it doesn't save a gloss rendering, just an RGBA which is the entirely of the gloss picture for that use case: do jpg &lt;- toJuicyRGBA &lt;$&gt; rend st -- render state to RGBA let bs = imageToJpg 98 (ImageRGBA8 jpg) t &lt;- getCurrentTime Lazy.writeFile (show t ++ ".jpg") bs return st I think you just need to get the maintainer of gloss-juicy to respond.
The source is my memory, but maybe you can find some old Gofer/Hugs releases. 
That is a bunch of interesting things! "After much discussion, we have decided to avoid the issues of inheritance or object oriented programming for the moment" Will come any release now.
text-format-simple's [`format`](https://hackage.haskell.org/package/text-format-simple-1.1.0/docs/Text-Format.html) takes a list of strings, implying that it will accept lists that are longer or shorter than what is actually expected without any compile-time error. It actually doesn't fail at runtime (which is what I mean by a total interface, I had wrongly assumed `format` isn't total) so that's not bad but I was wondering about a good way to reflect the number of holes in the format string in its type.
We have `bool` in `Data.Bool` which is the same but with the correct parameter order. ``` data Bool = False | True deriving (Eq, Ord) bool onFalse onTrue = \case { False -&gt; onFalse; True -&gt; onTrue } ``` Notice that the order of parameters matches the order of constructors. That's because `bool` is the eliminator for `Bool`.
I have this: https://github.com/elaforge/karya but you would probably have to be interested in music or audio synthesis. I guess most projects would appreciate contributors, so you can just look on hackage or github and pick whatever you like. The shortage is in contributors, not projects!
These aren't type-changing; it is the set of variables that are in scope that changes. Also, would https://arxiv.org/abs/1804.00119 be helpful?
I knew n+k patterns were disliked, but I didn't know the disdain for them was that old.
Thank you, now I understand.
If you are compiling with `-Wall` then you will get a warning. I ship code with `-Wall` but always compile locally with `-Wall -Werror`.
The Gofer 2.30 release notes: 2.8 The do notation \-------------------- Gofer 2.30 supports a new, experimental syntax for monad comprehensions which we will refer to as \`do {...} notation'. To maintain compatibility with previous releases of Gofer, the do notation is only supported if the system is compiled with the DO\_COMPS flag in prelude.h set to to 1, and the DO\_COMPS section of parser.y included.
[removed]
BTW- I think I may have mentioned this before, but is there any chance we could get around to refactoring levity polymorphism RuntimeRep a bit to make it so that there was a nice way to boundedly refer to things that are just the `LiftedRep` and `UnliftedRep` cases? e.g. something like data RuntimeRep = HeapPtr HeapRep | IntRep | ... data HeapRep = LIFTED | UNLIFTED pattern Lifted = HeapRep LIFTED pattern Unlifted = HeapRep UNLIFTED with the patterns just to preserve existing usage. It seems to me this would be sufficient to allow for things like pattern Heaped a = TYPE (HeapPtr a) Maybe :: Heaped a -&gt; Type -- [] :: Heaped a -&gt; Type as the data constructors there shouldn't care about the choice of thing you put into them as long as they are heap pointers. This would allow `Maybe (MutVar# s a)`, `Maybe (MVar# s a)`, etc.
There's [intero-neovim](https://github.com/parsonsmatt/intero-neovim) and there's also an open issue for [supporting vim 8.1](https://github.com/parsonsmatt/intero-neovim/issues/149) which might interest you!
Now that I know there's an open issue I can totally get started on it. Thanks!
&gt; As I mentioned on twitter, there might be a semilattice with both unit and annihilating element, but without dual operation. I wondered what the example is (and couldn't find the tweet). Then come up with the following: * Let I be the half-open interval (0,1] of real numbers. Consider a set &gt; S = I×I ∪ { (0,-1), (-1, 0), (-1, -1) } And the partial order on it: &gt; (x,y) &lt;= (x', y') iff x &lt;= x' and y &lt;= y' Then ∧ can be defined on S, there are the maximum element (1,1) and the minimum element (-1,-1), but no (-1,0) ∨ (0, -1) can be found.
That'd be sweet. I've been meaning to try out intero again.
I would like to fetch some information for website that does not have RSS. That means I will have to parse the HTML in order to extract it. What library should I use for this? 
Well, the runtime data associated with each variable can also change, so I think "type-changing" is a reasonable way to put it (I meant to draw an analogy to the type-changing ability of lenses, which also form a category). I didn't intend "type" to mean the type of term in its type system (i.e.: not Haskell's type system, but the type system of the language you are implementing). Thanks for the paper link, I'll check it out.
See this great introduction to using Haskell with nix: https://github.com/Gabriel439/haskell-nix
&gt; It also resembles monad comprehensions What are the main differences between do notation and monad comprehensions? Or are they just syntactically different forms of the same approach?
I already love it, thank you!
There are so many ways to do that that I don't dare say what is the best for your use case. Here's a quick and dirty way; add a callHackage version to the let bindings in your shell.nix: `beam-postgres = haskellPackages.callHackage "beam-postgres" "0.3.2.0" {};` Similarly, as you yourself suggested, you could have an overlay that more or less globally adds any package you care about in the same fashion. You will probably need to chase dependencies for a bit if there are more packages missing in the 18.03 set that I assume you are using. Or you could locally use a newer nixpkgs for your shell environment by importing nixpkgs from a tarball from github from the unstable channel or even master.
&gt;newtype Type = Type { unType :: String } Username checks out...
WAI isn't a framework exactly, but a bag of common types and utilities, not unlike the WSGI of the Python fame. This gives an opportunity to mix and match middleware, servers, frameworks etc. and contribute to the ecosystem not limited to a single framework.
Having done a bit of work with Nix on a Reflex project (not NixOS though) I have solved dependency issues like srhb describes. Putting that in the my-project.nix file and having the default.nix and shell.nix use that let's one pin dependencies like any other dependency manager, which is nice. On the topic of NixOS, how was the installation? Did you use any guide or have any notes from your install process? I have been wanting to try it out for a long time now, but the expected time to debug the installation/get WiFi to work etc is putting me off.
The installation took a bit of trial and error since I hadn't ever manually partitioned or anything like that but was broadly fine and I got it done in a day from starting reading up on the subject to having a working installation and all my stuff restored from my backup. I just followed [Chris Martin's writeup](https://chris-martin.org/2015/installing-nixos) plus some tutorials about gdisk since I'd never used it. I think I had to reinstall about three times before I got it right but I got there in the end.
Although I'd read and worked through a lot of that I completely missed the mention of the [styx](https://github.com/jyp/styx) tool until now. It seems to do what I'm looking for.
I know of: - [HaskTags](https://hackage.haskell.org/package/hasktags) (probably mostly useful for Emacs/VIM) - [Haskell-Ide-Engine](https://github.com/haskell/haskell-ide-engine) - [Intero](https://hackage.haskell.org/package/intero) Personally I use **Intero** with Spacemacs and VS.code (using Haskero Plugin there) but I did hear a lot of good things about the IDE-Engine und I think a lot of Haskellers just use things like hasktags with just syntax-highlighting and probably GHCid too.
intero just works out of the box
Every so often someone will post a thread asking how to get involved with Haskell open source; you can probably find some of those and bulk out your list a bit. Haskell Weekly also has a package of the week and call for contributions.
Tagsoup is pretty straightforward and intuitive [https://hackage.haskell.org/package/tagsoup](https://hackage.haskell.org/package/tagsoup)
The only reason that I use haskero in vscode is the autocomplete feature. In intero-neovim, the dev said that we should use neco-ghc for autocompletion and I think that just doesn't make sense when intero is already capable doing that out of the box. *PS: I'm a vim user*
Sorry, I don't use intero or vim for haskell development and am not affiliated with the intero-neovim project. Perhaps you'll get more information on that if you open an issue on github?
Ah sorry, I just want to share my opinion because you suggest intero-neovim.
Did I see you there?
I've been happier with haskell-ide-engine than anything else. It's also an LSP implementation, so we'd probably do well to make that the future as a community.
Chris Martin also made a 4 hour tutorial on nix a couple of days ago. https://www.youtube.com/watch?v=DQ44q2aIP48&amp;list=PLcAu_kKy-krz3t2teYyCM0Lt4015DF-Zp&amp;index=22&amp;t=0s
#### [Chris Martin - Deploying Haskell to the Cloud (part 1 &amp; 2 / 4)](https://www.youtube.com/watch?v=DQ44q2aIP48&amp;list=PLcAu_kKy-krz3t2teYyCM0Lt4015DF-Zp&amp;index=22&amp;t=0s) ##### 77 views &amp;nbsp;👍2 👎0 *** Description: Slides: https://monadic-party.chris-martin.or... *Monadic Warsaw, Published on Jun 15, 2018* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details. Respond 'delete' to delete this.) ^(|) [^(Opt Out)](http://np.reddit.com/r/YTubeInfoBot/wiki/index) ^(|) [^(More Info)](http://np.reddit.com/r/YTubeInfoBot/)
I use generic autocomplete mode in Emacs. It seems to work fine for Haskell out of the box. I would be surprised if there isn't anything easy also for Vim.
In emacs I use a package called company-ghci. This gives autocompletion from the ghci session for that file. For me this works well as I allways have ghci running when working on something.
Yes, all you need is LSP config for your editor of choice plus a single executable, `hie`, and you're set to go. The functionality is there and working, and it could, like you say, use more help to grind away the sharp edges, and make it more efficient and complete. With that being said, I'm not sure if LSP has a REPL concept yet. So you might want to hang on to GHCid or Dante (if Intero is too slow or buggy). In Emacs there's the main emacs-lsp project for integration and the new, light-weight eglot.el. VSCode has built-in LSP IIUC and for Vim there's LanguageClient-neovim. I only have experience with Emacs lsp-mode and eglot and both work for the basic functionality that the LSP protocol, so far, has defined. It's clear that LSP has been focused on languages that didn't require features like your interactive Lisp or Haskell development experience you're used to. LSP will also fail to provide any standard way for theorem prover integration ala Idris. But LSP and haskell-mode go hand in hand and there's no reason idris-mode cannot delegate stuff supported by LSP and focus on editing and filling theorem holes dynamically. Personally I'm excited that I can rename identifiers properly, without string search and replace. And maybe LSP will add a protocol for renaming modules and your editor could take care of shuffling around files and directories for Haskell's Java-like hierarchy in the filesystem. It's something Java IDEs have supported for a long time and rather mechanical and easy to automate in principle.
I'm sorry, what do you mean by _generic autocomplete_? complete-at-point?
That makes me very happy to hear!
When I start typing the first few characters of a symbol, a menu of matching symbols pops up.
Monad comprehensions look exactly like list comprehensions, so it’s just different syntax for the same thing. 
I use dynamic abbreviations in Emacs (M-/). It works very, very well. 
But some of us have use the cryptic formatting strings for 30-40 years already, and we know them well. 
Thanks for the link. I'll try to find the courage to leave my comfy Ubuntu setup for NixOS sometime during the summer.
Sweet! Thank you for the link!
I can’t think of any (non-contrived) reason to use Todo. 
Afaik eta uses hackage to get its dependencies so I'd expect them to. I'm sure they'll implement the reports, not like there's a new one every year. The name is because of a rebranding. Haskell sadly carries some negative connotations, especially in the enterprise programming world. Main issues are probably the "ivory tower" perception and the near mythical status of monads as something cryptic.
One thing I'm a bit sour about using `persistent` is the fact that you essentially have to define your DB records with TH, and the rest of the codebase needs to play with whatever comes out of it. This means that you'll probably be using the `persistent` generated types for interacting with the DB, but you'll be using redundant isomorphic types defined elsewhere to be used in the rest of the codebase, and maintain sets of functions to translate between these two. Based on a skim through the `beam` tutorials, it seems to me like you could actually define your records in a module that knows nothing about `beam` and a DB module could define type aliases or newtypes over your records and adapt them to beam after the fact. This would be much better for reducing boilerplate and having more consistency in your codebase. Am I right that `beam` can be used this way?
To me, Gabriel Gonzalez is an exemplary maintainer, whose projects I very much enjoyed contributing to. You can find his projects (that I'm aware of) at https://github.com/Gabriel439/ and at https://github.com/dhall-lang.
Ah, well then what about the formatting library? Prelude Formatting&gt; :set -XOverloadedStrings Prelude Formatting&gt; format ("Hello " % text % ", I am " % int % " months old") "Kindaro" (33*13) "Hello Kindaro, I am 429 months old"
Ah, well then what about the formatting library? Prelude Formatting&gt; :set -XOverloadedStrings Prelude Formatting&gt; format ("Hello " % text % ", I am " % int % " months old") "Kindaro" (33*13) "Hello Kindaro, I am 429 months old"
no problem. i just didn't want to give the wrong impression.
Right. I don't think of it as "forced to learn" so much as "get to reuse knowledge".
What about Postgrest? https://postgrest.org/en/v5.0/ It's also available on Hackage as a library. Re. zero-downtime deployment, what do you mean exactly? A combination of container scheduling and a reverse proxy that switches accordingly ? I'd say just use Kubernetes for that and don't reinvent the wheel. 
I'm working on a similar task, but I'm curious what an interface for defining higher order to lattices would even look like. I want to construct many type lattices and don't know off good lattice construction primitives.
There are no data structures in Algebra.Lattice, only a typeclass. You have to write all the code for representing and computing with whatever lattice you're thinking of yourself.
Slightly off-topic, but if you're looking at lattices of types, are you familiar with Stephen Dolan's work adding subtyping to Hindley-Milner? Rough idea: given any two types `a` and `b`, rather than consulting the type hierarchy, we simply say that their meet and join are `a &amp; b` and `a | b`. For example: select p v d = if p v then v else d Hindley-Milner would type this as select :: (a -&gt; Bool) -&gt; a -&gt; a -&gt; a But Dolan gives a more general type: select :: (a -&gt; Bool) -&gt; a -&gt; b -&gt; a | b Note that `|` is not the same as `Either`, rather, it means "an `a` or a `b`, but I'm not telling you which". But if your subtype hierarchy happens to have `a ≤ c` and `b ≤ c`, then you have `a | b ≤ c`. Note also that `|` only appears in positive positions and `&amp;` in negative. Anyway, check out his [POPL talk video](https://www.youtube.com/watch?v=-P1ks4NPIyk) or [full dissertation](https://www.cl.cam.ac.uk/~sd601/thesis.pdf).
You can try [https://github.com/kquick/vernix](https://github.com/kquick/vernix) as well.
System F:&lt; (FSub) is the main implementation of the Typed Lambda Calculus with Subtyping.
Maybe we can collaborate on this. I need multiple type hierarchies represented as lattices with optional top and bottom elements to allow strict or non strict (levitated and non levitated) lattices.
As I understand it, subtyping in FSub is not decidable? I believe it is in Dolan's system.
I did ask in Haskell Cafe Mailing List a couple of days ago but not any answers there yet.
I guess you're objecting that `if` with one space after it is 3 characters, so if you start the conditions on the same line as the `if` *and* align the pipes on following lines, you end up aligning them on an odd column rather than an even one. If that's actually your complaint... why not just add a second space after the `if`?
Any experience on HIE + VSCode + NixOS? I just can't get it working.
Wondering if implementation could be based similar to Haskell Data.Graph [https://hackage.haskell.org/package/containers-0.5.11.0/docs/Data-Graph.html](https://hackage.haskell.org/package/containers-0.5.11.0/docs/Data-Graph.html) How to calculate the LUB (Least Upper Bound) in another thing though. The other implementation is as a matrix of Booleans. Again calculating the LUB is difficult a saturated LUB like solution can be calculated by a power function by the number of routes of the matrix itself. But this saturated and I cannot find a binary 'boundary' function for binary matrices.
It’s clearly really useful and clear, I’m waiting for more. When I got feedbacks, I post it here. Thanks !
I would love autocomplete in intero-neovim but I don't know how to make it work. 
That would be great! Thanks for putting the time in :) 
Yeah your right its not decidable. Oh Dolan's Thesis looks good :- [https://www.cl.cam.ac.uk/\~sd601/thesis.pdf](https://www.cl.cam.ac.uk/~sd601/thesis.pdf) I will have to take some time to print and read it. He also has a paper on "MLSub" with an implementation ! [https://www.cl.cam.ac.uk/\~sd601/papers/mlsub-preprint.pdf](https://www.cl.cam.ac.uk/~sd601/papers/mlsub-preprint.pdf) [https://github.com/stedolan/mlsub](https://github.com/stedolan/mlsub) Cool, great lead !
Trust me,it isn't only you."Couldn't start hie". Forever and ever.
There's an online demo too: https://www.cl.cam.ac.uk/~sd601/mlsub/
What's your preferred front end for haskell-ide-engine? If the spacemacs integration is reliable, I might try it out.
Yea, you have to use NixOS's way of managing vscode plugins.
I just use vscode because it's the most stable. I've always used emacs because it does a lot, not because I think it's a good editor. Editors like vscode are starting to catch up in terms of doing a lot.
So the answer was in the link suggested by u/bradley_hardy, which I'd read previously but missed the crucial section, all along. For anyone with the same problem I had the section on [Changing Versions](https://github.com/Gabriel439/haskell-nix/blob/master/project1/README.md#changing-versions) is the bit to really internalize. Particularly useful is cabal2nix's ability to write a derivation for a remote package: `cabal2nix cabal://turtle-1.3.2 &gt; turtle.nix`
During the 4th iteration, you wrote : « The wrapping of Maybe at this point may not add much value, since a Right value would indicate error anyways. » Don’t you mean, success instead of error ? If I got it wrong, sorry 
Thanks Ashley this is exactly what I needed !
There are two cool functions (`shellFor` and `packageSourceOverrides`) in nixpkgs that are not well documented anywhere yet. From the comments [here](https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/haskell-modules/make-package-set.nix#L210): ``` # Returns a derivation whose environment contains a GHC with only # the dependencies of packages listed in `packages`, not the # packages themselves. Using nix-shell on this derivation will # give you an environment suitable for developing the listed # packages with an incremental tool like cabal-install. # # # default.nix # with import &lt;nixpkgs&gt; {}; # haskellPackages.extend (haskell.lib.packageSourceOverrides { # frontend = ./frontend; # backend = ./backend; # common = ./common; # }) # # # shell.nix # (import ./.).shellFor { # packages = p: [p.frontend p.backend p.common]; # withHoogle = true; # } # # -- cabal.project # packages: # frontend/ # backend/ # common/ # # bash$ nix-shell --run "cabal new-build all" ```
I think `shellFor` and `packageSourceOverrides` do the same job as styx (see my [other comment](https://www.reddit.com/r/haskell/comments/8rpvua/what_is_the_correct_haskell_nix_workflow/e0ubh4j/)). I think they will work better in the long run as they keep the configuration in nix. For instance you can use `builtins.filterSource` to filter the source directories to prevent unwanted rebuilds.
Sounds like what company.el does and probably alternatives. I'm curious what you use is all built-in 26.1 functionality.
I want to provide confirmation that lsp-mode and haskell-mode work in Emacs. Any limitation I've encountered is inherent to the limited functionality of LSP the protocol. Just saying you don't have to switch to VSCode to use HIE.
I commented with the following stack in another thread with similar question https://www.reddit.com/r/haskell/comments/8bfjy5/comment/dx6eqnk?st=JIJIUNU9&amp;sh=134d6e39 Check other answer too as they are useful as well I once heard about “keter” for deploying yesod application, but I don’t have much experience with it. However I Guess it’s better to just use Kubernetes bcs it is application-independent thus your learnings will be transferable suppose in the future you use other technologies than Haskell
What is the year of that release? I can't find the year anywhere...
As an alternative you could try \`fmt\` formatting library: * [https://github.com/aelve/fmt](https://github.com/aelve/fmt) It's more beginner friendly. And also faster.
Sounds good. I shoud start from WAI based one. Now I will start from scotty. Thank you again. 
I see. So I will start from scotty, dpwiz recommended, and step up to Yesod or something. Thanks again.
The page claims to have last been updated on September 16th, 1998. Looks like IO's 20th is coming up pretty soon. :-)
[My article on using Nix for Haskell development might also help.](https://www.fosskers.ca/blog/nix-en.html)
I am using VScode with HIE via hie-nix https://github.com/domenkozar/hie-nix and everything works fine. Could you give that a try?
Did you try: https://www.reddit.com/r/haskell/comments/8rq7ih/what_is_the_status_of_haskell_autocompletion/e0unibe/ ?
*(fwiw, I recommend `lib.cleanSourceWith` over `builtins.filterSource`, as it does allows nesting filters, i.e. `cleanSourceWith { filter = ...; src = cleanSourceWith ...; }`, whereas `filterSource (...) (filterSource ...)` is not allowed.
Just beware of https://github.com/haskell/haskell-ide-engine/issues/439#issuecomment-359801662 You need a binary for every ghc major-version atm. That given script is easily extendable until hie has proper detection (if ever, because internals..).
I really want to use the NixOS module system (or something like it) for generalized nix workflows. The NixOS module file is a really good unit of configuration. Since it's modular, you can just import complex projects (e.g. `reflex-platform` could be made just one big module) without having to change configuration systems. Then you just need a command line shortcut that automatically evaluates the module system, and you could use it for modular, declarative configuration of all kinds of things (not just haskell). Plus you get automatic documentation of all the configuration options. The main problem is that its concept of `imports` is a little bad if you need a complex graph of third party modules. For example, how should a reflex-platform module import a GHCJS module? Potentially, other third party modules may also depend on the GHCJS module, and they all need to agree on a version. It's almost like you need a dependency manager for your dependency manager... Which is obviously too complex.
The latest version of HIE now comes with an executable `hie-wrapper` that handles that for you
I've tried that + the vs-code-hie-server plugin, but auto completion, type hins etc. just won't work (no errors in plugin developer console either).
I'm not familiar with company.el; I'm using the autocomplete package. I don't have emacs 26 yet on any of the platforms I use. Some of them don't even have 24 yet.
Also assembler, I assume?
Is there per-platform automated testing of the platform yet? We've been talking about it for years. It's pretty important for a tool intended to serve such a fundamental role.
This argument is nice, but not applicable to the design and use of a modern string template library. In that sense, it is not right for the purpose of this thread. I can also explain how it is ad hominem and against progress, if the explanation is necessary. You (and /r/augustss) are right in that I am being judgemental. Do you disagree with the particular judgement that `printf`'s format strings are intricate and ugly?
Of course I’ve written assembly language. For instance, I’ve written a `printf` in assembly. And a C compiler. But I’m not sure how that’s related to what we are discussing. 
Like many things (Haskell included), there are arbitrary decisions in the design of `printf`. I don’t find the format strings intricate nor ugly, but that’s because I know them well. For anyone who doesn’t already know them, I’d probably suggest learning something nicer and more extensible. Since you don’t like `printf`, don’t use it. 
Thank you. If I may ask, how do you think a format string (I suppose we may call it a string template) might look like in modern Haskell? Specifically: * Hard-coded format specifiers and modifiers are only applicable to a closed number of types. Does it make sense to keep some of them, or better to just get rid of them altogether and let the user choose the format explicitly, for example by means of a `newtype` a with custom `Show` instance? In some JavaScript and Python template engines that I had an experience using, there is only one format specifier: curly braces, like this: `{replace me}`. Is this _"fit"_ or _"enough"_ for a typed language, like Haskell? * Should the format string actually be a string, or an `IsString`, or rather a richer Haskell type?
This project sounds ideal for a first real-world Haskell app. There are many mature and robust choices for each of the things you are looking for. 1. Web+db frameworks: We are using yesod + persistent + esqueleto. It is a classic rails-django-like MVC approach, but type-safe, and with some optional but really nice easy-to-use DSLs. Very will documented, easy to set up and get started if you have some experience in web programming (but not necessarily in Haskell). Mature and robust surrounding ecosystem. Scales to high load, high availability, large content. In use by some of the world's largest enterprises. Some other popular approaches are snap and servant for the web framework, opaleye for the DB framework. 2. Automated deployment: We are using keter. It grew out of the yesod ecosystem and most often used with yesod, but in fact it is not tied to yesod in any way. It works equally well not only for non-yesod web frameworks, but also for non-Haskell frameworks, as long as your app compiles to a stand-alone executable that doesn't require running inside a separate web server or app container. It is simple to set up and use, scalable, and reliable. I recommend it if you are targeting a single Linux deployment platform. Some other popular alternatives are docker, heroku.
I have no particular opinion. I use `printf`. :)
It somewhat shows that the `printf` mindset is more along the lines of the mindset of a low level language than a high level one, like Haskell. I also meant to point out that your sentence may be interpreted as an *ad hominem* argument in favour of `printf`, while in truth the experience and proficiency of any one programmer is not an argument in a discussion of timeless qualities. As Assembler does not directly inform the design of Haskell, in the same way `printf` does not inform the design of a modern string templating engine. We learn from experience not merely to replicate it, but also to build upon it something better and different.
My comment was not meant as an ad hominem argument. I was merely pointing out that if you already know `printf` then it doesn’t seem awkward. I wrote the Haskell `printf` for my own benefit, and I figured there might be others out there who might like it too. Those who don’t like it can use something else (hopefully type safe). It is indeed a design from a bygone era. Perhaps you should design a modern formatting library?
Could you please elaborate? By now, I am just installing the hie-server-plugin from the integrated VSCode plugin management system.
I am sorry. I should not have worded my opinion about it in such a harsh fashion. When my design is prototyped, can I send it to you for a review?
I'm interested by this thread, I was / am in the same situation as you. After looking around, I finally chose to go for Servant. This web library/framework let you learn more the Haskell language than learning just another web framework FMPOV.
This is a pretty experimental, naive, hacky, whatever attempt to provide a GHCi-based Haskell integration for VSCode. It's still very feature-poor currently, but I'm sharing it here in case anyone is interested. This project is similar in goal with [jyp/dante](https://github.com/jyp/dante), but it's VSCode. I have no previous experience with this kind of things before, so I really appreciate any feedback. :)
Oh, nice. That's more ergonomic than auto-complete.
How's the memory usage of this compared to the other tools?
I’m curious, your readme mentions that you use \`:all-types\` to get the type of a selection. Is there a reason why you are using that over the \[\`:type-at\`\]([https://downloads.haskell.org/\~ghc/latest/docs/html/users\_guide/ghci.html#ghci-cmd-:type-at](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#ghci-cmd-:type-at)) command? I would expect that the latter might be more efficient than dumping all types and it should also save you the work of having to find the matching source span.
I think `:type-at` is not what I want. You see that in the demo screenshot, I only selected a part of the function application, but the entire thing is highlighted. I would really love that 'smartness', but as far as I've tried, that's not the behavior of ':type-at:.
I haven't done any measurements, so I don't know for sure. The extension is (unfortunately) currently one GHCi per file, plus the extension code itself (it's all JavaScript with no more external deps). As you know, GHCi currently only supports loading one file interpreted, so it seemed at first appropriate to have one instance per file. I think I might be able to change that to use only one GHCi and switch between files, if/when I figure it out.
Does this do anything on top of Haskero for vscode?
I think that it differs mostly in terms of cpu/power usage. I've recently tried `intero` again and at first were happy with it, but then I figures that it made my laptop hotter and less responsive when it was working, so I switched back to `ghcid` (which provides much less features than `intero` but doesn't heat my laptop up).
Nope. If Haskero has been working well for you then you probably want to stay with it. As I've mentioned, it's similar in goal to [dante (which is for Emacs)](https://github.com/jyp/dante), in that it depends on no external things other than GHC, so it's probably simpler. And as I've mentioned in the readme, this is also me toying around with new GHC features: &gt; Since around GHC 8, the compiler GHC and its interactive REPL GHCi has gained various tooling-related features. These allow for more tooling that communicate with the compiler using text IO and files, instead of a Haskell API. This project aims to explore the possibilities provided by said features, by implementing Haskell tooling within the editor VSCode. 
Good point! I was definitely missremembering how well `:type-at` works. I wonder if we could change this upstream. At least I would find the behavior you chose a lot more intuitive.
I'm delighted to announce that the next Mathematics of Program Construction (MPC) conference will be held in the historic city of Porto, Portugal in October 2019, co-located with Formal Methods (FM). Please share, and submit your best papers!
Please join https://github.com/haskell/haskell-ide-engine instead of duplicating efforts.... 
I believe GHC/GHCi has `:set +c`, `-fsource-spans`, `-ddump-json` etc. for a reason...
I think this effort is intentionally (and valuably) different: a problem with a lot of the other haskell tools (including ghc-mod, and haskell-lsp) is that it's often hard to get the tooling up and running, dealing with different compiler versions, etc. So I'm glad there's a thin wrapper that uses text interfaces with the repl's built in features, since it's a wonderful thing to fall back on if, for example: - I have a GHC version that's newer than ghc-mod supports - I'm dicking around with GHC and I broke the API - I just can't get the other tooling working and I have no idea why (can you tell me honestly that you've never been in that situation?)
I would like to try the Hood debugger (http://ku-fpg.github.io/software/hood/). I try to run the example on their page in ghci, but it fails and I don't understand why. Here is the script I load into ghci: import Debug.Hood.Observe {- al :: [Int] -} {- al = [0..9] -} ex2 = print . reverse . (observe "intermediate") . reverse $ [1..4] n = runO ex2 Then I get the following error: hoodTests.hs:10:10: error: • Ambiguous type variable ‘a0’ arising from the literal ‘1’ prevents the constraint ‘(Num a0)’ from being solved. Probable fix: use a type annotation to specify what ‘a0’ should be. These potential instances exist: instance Num Integer -- Defined in ‘GHC.Num’ instance Num Double -- Defined in ‘GHC.Float’ instance Num Float -- Defined in ‘GHC.Float’ ...plus two others (use -fprint-potential-instances to see them all) • In the expression: 1 In the second argument of ‘($)’, namely ‘[1 .. 4]’ In the expression: print . reverse . (observe "intermediate") . reverse $ [1 .. 4] Failed, modules loaded: none. When I change the script such that the type ambiguity is resolved: import Debug.Hood.Observe al :: [Int] al = [0..9] ex2 = print . reverse . (observe "intermediate") . reverse $ al n = runO ex2 The error is gone but the intermediate result is not shown and the output is not the same as in the example. hoodTests.hs:10:10: error: • Ambiguous type variable ‘a0’ arising from the literal ‘1’ prevents the constraint ‘(Num a0)’ from being solved. Probable fix: use a type annotation to specify what ‘a0’ should be. These potential instances exist: instance Num Integer -- Defined in ‘GHC.Num’ instance Num Double -- Defined in ‘GHC.Float’ instance Num Float -- Defined in ‘GHC.Float’ ...plus two others (use -fprint-potential-instances to see them all) • In the expression: 1 In the second argument of ‘($)’, namely ‘[1 .. 4]’ In the expression: print . reverse . (observe "intermediate") . reverse $ [1 .. 4] Failed, modules loaded: none. 
&gt; A History of Haskell: Being Lazy With Class mentions that Haskell 1.3 (May 1996) got monadic IO and do-notation. Section 9.3 in the same paper mentions the following: &gt; Moving to take a post-doctoral post at Yale in 1992, Jones continued to develop and maintain Gofer, adding support for constructor classes (Section 6.4) in 1992–93 and producing the first implementation of the `do`-notation in 1994.
The Haskell version 1.3 report came out in May 1996 according to *A History of Haskell: Being Lazy With Class* and [this page](https://www.haskell.org/definition/).
Can we expect to have this as a GHC extension one day? It would be really nice
true.
that's true; however, how are we ever going to get into mature tooling? I checked the HIE again today and I was amazed how much it progressed since the last time I looked. Multiple editors integrated, stack, cabal, many GHC versions, ... amazing effort! Now, if some features can be done in a more lightweight way, why not improve the HIE implementation to reduce the reliance on heavy weight tools, like ghc-mod. Maybe, at some point, ghc-mod could become optional, required only for most advanced features. 
I agree, and I'm really glad this particular alternative exists, but you have to admit the editor situation in Haskell is getting a bit comical.
Sure, I’ll take a look. 
I'm playing with [Brick](https://hackage.haskell.org/package/brick), and I'm in the EventM monad: e -&gt; do liftIO $ hPutStrLn stderr $ "unhandled VtyEvent: " ++ show e halt state But the text doesn't appear when the TUI ends. Why is this, and how do I fix it?
Thanks, I'll have a look
I'm still trying to figure out if there isn't a common interface, and the sole reason I configured company.el is that so many of the major language modes use it. I wish, and maybe there is, a way to choose completiong engine and have it plugged into current mode. Anyway, thanks for looking it up. It confirms my suspicion that you're using more than defaults of `M-/`. Regarding Emacs version, I can say that Emacs has so few releases that I consider it easy to build and package for the platforms I need it on. It's the opposite of browsers that update every 2 weeks and would make rebuilding a PITA.
A few comments on this list: As some people have mentioned, I've been working on a [pure Haskell commonmark parser](https://github.com/jgm/commonmark-hs). My design goals: - BSD-licensed - minimal dependencies - flexible and extensible - tracks source positions - conforms to commonmark spec and passes test suite - handles pathological input well (linear time) The API isn't stabilized, and some more work is needed before it's ready to publish. (I'd welcome feedback from anyone about the design.) `cheapskate` is an old project of mine that I haven't been actively maintaining. It has some parsing bugs -- I'm sorry, I can't remember the details, but I gave up working on it when I started working on commonmark. `comark-parser` appears to have started out as a modification of `cheapskate`. It's faster than my `commonmark` library and consumes less memory, but it gave me a stack overflow on some of the pathological input my parser is designed to handle in linear time. It doesn't track source positions, and isn't as easily extensible as `commonmark`. `mmark` actually departs quite a lot from both traditional Markdown and from commonmark. For example, setext-style (underlined) headers are not supported. And the following is parsed as two block quotes instead of one: &gt; This is my &gt; block quote. I could give many more examples. So really `mmark` implements a new syntax that shares a lot with Markdown, but is far from being backwards compatible. When it comes to the wrappers around C libraries, I can only recommend `cmark` (which wraps my `libcmark`, the reference implementation for commonmark) or `cmark-gfm` (which wraps the fork of `libcmark` that GitHub uses). These C libraries are robust and well tested. `sundown` is the old GitHub Markdown library, but GitHub doesn't use it any more. (It had too many parsing bugs.) Now they use the fork of `libcmark` that is wrapped by `cmark-gfm`. `sundown` would be a poor choice for anyone, I think. I don't think that the underlying C library is actively maintained. And I don't think there's any good reason to use `discount` instead of `cmark`. `cmark` has much better performance and conforms to the commonmark standard. So, the bottom line: - If you want something standard and don't mind C dependencies, I'd recommend using `cmark` or `cmark-gfm`. - If you want a more flexible, pure Haskell library, the upcoming `commonmark` library will be a good choice. - If you need pure Haskell but can't wait, `cheapskate` might be good enough for the short term. 
This is likely because you are not redirecting `stderr` output. What happens is that the output *does* go to the terminal, but because the terminal is immediate redrawn with your UI, you don't see the output text (or if you do, only very briefly).
You can do “zero-downtime” in Erlang because you can literally load new code into BEAM and start using new requests through to that. You can achieve something similar in most dynamically loaded languages (everything from Ruby to Java) using similar techniques, if not nearly as elegantly. It _can_ be done in Haskell (there’s a a Facebook library that does it) but normally Haskell, like C++, is just a compiled executable. So yeah, the reply above is the standard way to do it (and works for anything, not just Haskell).
Thanks to whoever created this. I can’t get `haskell-ide-engine` to work with VSCode and GHC 8.4.x: https://github.com/alanz/vscode-hie-server/issues/89 Depending only on GHCi should help with getting newer GHC versions up and running with IDEs.
Everything is a void *
I am enjoying this greatly. The article asks, "did you know that some types are secretly product types in disguise?" Is the word "some" in that supposed to be "sum"? And by "are secretly" should I be reading "are isomorphic to"? That is, is the claim intended to be that *all* sums can be expressed as products, or only some of them? The article goes on to explain that `Either a a` is equivalent to `(Bool, a)`, with which I agree. But how would you express `Either a b` as a product? (You could write a surjection `(Bool, a, b) -&gt; Either a b`, but it's not invertible. You could write an injection `Either a b -&gt; (Maybe a, Maybe b)` but it's not surjective.)
Thanks! The meaning of "did you know that some types are secretly product types in disguise?" is that a lot of types that you work with every day are actually product types, but you just might not be aware that they are product types. `Person` was an example of such a "hidden" product, and `Either a a` is another "hidden" product. `Either a b` can be expressed as a product between `()` and `Either a b` (`((), Either a b)`), but that's probably not the one you are talking about :) You might be thinking about a product where one of the sides is `a`. To figure this out, the simplest way is to follow the algebra: a + b = a * q (a + b) / a = q q = (a + b) / a q = 1 + b/a Which says that `a + b` is a product between `a` and some other type if there exists a "quotient" between `b` and `a`. Quotients on Haskell don't exist for all types, so it's impossible to write a fully generic and polymorphic decomposition. However, we can look at `b / a` to imagine what sort of `b`s and `a`s will give meaningful types: 1. If `b` is also `a`, then we have `q = 1 + a/a = 2`. And that's exactly what we mean when we say that `Either a a` is isomorphic to `(Bool, a)` --- it's `2 * a`! 2. If `b` is `Void`, then we have `q = 1 + 0/a = 1`. So this means that `Either a Void` is isomorphic to `((), a)`. 3. If `a` is `()`, we have `q = 1 + b/1 = 1 + b`. This means that `Either () b` is isomorphic to `((), Either () b)`. We can rearrange some things to try to find more interesting examples. Let's say there exists a type `w` where `b = w*a`: q = 1 + b/a q = 1 + (w*a)/a q = 1 + w So this means that if `b` can be decomposed as a product between some other type and `a`, then `Either a b` is isomorphic to `(a, Either () w)`, or `(a, Maybe w)`. Intuitively, you can think of this as saying that `Either a (a,w)` is isomorphic to `(a, Maybe w)`. The `Either` "always has" an `a`, but it can sometimes have an `(a,w)` as well (the `Just` case). The examples above can be explained in this light -- (1) is `b = ((), a)` and (2) is `b = Void * a`.
I just do this if | cond -&gt; res | cond -&gt; res And I'm probably not the only human on the world to do this: fn = go whatever where go x = foo $ bar ++ x It really works well
How do I make the FFI work with C functions that take/return structs as values?
How would the new version of `inserting` look after applying all the tricks from section 5?
Hijacking OP's post with somewhat similar question. Wanted to learn haskell along with some "new" technologies e.g. couchdb. But when I search in hackage the latest couchdb package was updated at 2015, should I worry about it?
Here's one way to do it, using `gdp-0.0.0.2` from Hackage. A map API with a simpler `insert` instead of `inserting`: {-# LANGUAGE TypeOperators #-} module JMap where import Prelude hiding (lookup) import GDP import Data.Map (Map) import qualified Data.Map as M newtype x ∈ xs = Element Defn newtype Keys m = Keys Defn lookup :: Ord k =&gt; (k ~~ key ::: key ∈ Keys m) -&gt; (Map k v ~~ m) -&gt; v lookup k m = case M.lookup (the k) (the m) of Just v -&gt; v Nothing -&gt; error "unreachable" member :: Ord k =&gt; (k ~~ key) -&gt; (Map k v ~~ m) -&gt; Maybe (k ~~ key ::: key ∈ Keys m) member k m = const (k ...axiom) &lt;$&gt; M.lookup (the k) (the m) -- Add a definition so we can write down lemmas about `insert`. newtype InsertedTo k m = InsertedTo Defn insert :: Ord k =&gt; (k ~~ key) -&gt; v -&gt; (Map k v ~~ m) -&gt; (Map k v ~~ InsertedTo key m) insert k v m = defn (M.insert (the k) v (the m)) -- And now, the lemmas: insert_key_present :: Proof (k ∈ Keys (InsertedTo k m)) insert_key_present = axiom insert_keys_superset :: (k ∈ Keys m) -&gt; Proof (k ∈ Keys (InsertedTo k' m)) insert_keys_superset _ = axiom And a usage example: import Prelude hiding (lookup) import JMap import GDP example :: IO () example = name M.empty $ \m0 -&gt; -- name an empty map name3 "a" "b" "c" $ \k1 k2 k3 -&gt; do -- name three keys -- Make some different maps let m1 = insert k1 "alpha" m0 m2 = insert k2 "beta" m1 m3 = insert k3 "gamma" m2 -- We know that k1, k2, and k3 are all present in m3. Let's prove -- it and then apply `lookup` to print the corresponding values. putStrLn $ "key: " ++ the k3 ++ " value: " ++ lookup (k3 ...insert_key_present) m3 putStrLn $ "key: " ++ the k2 ++ " value: " ++ lookup (k2 ...( insert_key_present |$ insert_keys_superset)) m3 putStrLn $ "key: " ++ the k1 ++ " value: " ++ lookup (k1 ...( insert_key_present |$ insert_keys_superset |. insert_keys_superset)) m3 
Thanks! So essentially, the name of the updated map becomes a list of all keys inserted since the original map was named.
Yes, exactly. You could also mix and match this approach with the original `inserting` to get a nice balance. For example, this version wouldn't need a named key: insert :: k -&gt; v -&gt; (Map k v ~~ m) -&gt; (forall m' key. (Map k v ~~ m' ::: Keys m ⊆ Keys m') -&gt; (k ? KeyOf m') -&gt; t) -&gt; t (? is from http://hackage.haskell.org/package/gdp-0.0.0.2/docs/Data-Refined.html#g:3) In this version, you wouldn't need to name the key before using `insert`, and the lemmas you need would be derived from the evidence that the key was added (`k ?KeyOf m'`) along with more general lemmas about key sets, such as subset_keys :: (xs ⊆ ys) -&gt; KeyOf xs k -&gt; Proof (KeyOf ys k) 
Shameless plug for my preferred setup https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8
Are you the author?
That is going to have dramatically bad performance with non-trivial projects. What do you mean "GHCi only supports loading one file interpreted"? I'm pretty sure you can load as many files as you want.
Does that mean I need to save up all the stuff I want to print til after the UI loop ends, and then print it?
That's a funny way to spell `int` :)
No, it just means that you'll need to redirect `stderr` when you run your program from the shell, e.g., ``` $ some-brick-program 2&gt;log.txt ``` This way, you could `tail` the log in another window to watch what happens as you interact with the Brick program: ``` $ tail -f log.txt ```
I mean that there can only be one `*` module per instance. Maybe using only one can work; as I said I'll try that.
100%. The reason i moved from intero to dante is because i got tired of having to depend on other programs for my editor features and all that comes with it. Namely having to wait before i start coding a new project so the tooling can install itself which also requires an internet connection, or not being able to move to a newer ghc version until the tooling catch up. Dante might have less features but it doesn't bother me with all this unrelated stuff. Now if only ghci can be more robust and provide better support for ghci based editor plugins...
Same as with any other language, only after i wrote a few programs with it.
There isn't one correct approach so much as different approaches with different trade-offs. In my experience, the most reliable way is to have a \`stack.yaml\`, then generate the Nix files using \`stackage2nix\`. The advantage of this is that it lets you reuse the Stackage LTS snapshots, extra-deps, etc., so you don't have to deal with dependency hell. Note that you'll want to install \`stackage2nix\` using \*Nix\*, not Stack/Cabal, since the Nix expression for it actually downloads the package index as well as building it. If you want a more vanilla experience cabal2nix can work too, but you'll be stuck with whatever version of a given library is in nixpkgs. If you go this route, make sure you pin nixpkgs, otherwise you'll lose all the nice isolation Nix provides as you'll be using whatever version of nixpkgs happens to be available.
&gt;3- How should I learn the type system? I get the basics (data types and classes (which aren't like c# classes at all)) but it still feels strange and vast to me. &amp;nbsp; See a function in another language in your day to day work? Attempt to write its type signature. Then write the function in Haskell and hit :t to see how you did.
&gt; I don't really know how to design a full program in Haskell. Don't worry. Nobody in Haskell world knows for sure how to do this. That's why there exist multiple ways with different tradeoffs to write big applications. Couple of them: * Beginner: https://jaspervdj.be/posts/2018-03-08-handle-pattern.html * More advanced: http://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html * Even more advanced: http://degoes.net/articles/modern-fp Try to write some simple terminal (console) application. Such applications are usually not that big. But still they are real complete projects. So you will have a feeling of what it's like to write Haskell application. The more you learn the language and approaches the more you will be capable of thinking in big. 
I like this to get the basics of functor, applicative and monad http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html Explained in C# : I'd see Haskell-data close to C#-struct/class without methods and Haskell-class close to C#-interfaces. They're not the same thing tho, but that's the most similar concepts in C#
One of the best advice I've seen somewhere (don't remember where, reference appreciated) was basically : Write some stuff, however ugly or imperative it is (do everything in a IO monad if needed) and refactor it if you feel the need to or just carry on different projects. That way, you'll get your hand dirty, have to think about how to solve things, and help you understand when you hit a wall why people end up with abstraction that you might not really understand until you actually need them. 
sed s:int:long:g
After I understood this I feel more confident now: 0. https://www.youtube.com/watch?v=PlFgKV0ZXoE 1. http://blog.tmorris.net/posts/20-intermediate-haskell-exercises/ 2. https://savanni.luminescent-dreams.com/page/haskell-app-monad 3. http://www.lulu.com/shop/bartosz-milewski/category-theory-for-programmers/hardcover/product-23389988.html (haven't read fully).
Nobody outside of Haskell knows how to write big programs either
Niiice! I like your declarative interface. And I will try to give some more feedback + code review later.
https://haskell-miso.org
Servant is perfectly fine. Just return HTML instead of JSON in your routes.
&gt; I find a big issue I have is the type System: often when I try to write a function type signature it doesn't work yet if I write the function it will work. Can you give an example? We may be able to tell where you're going wrong.
nope
Yes, I don't know why a lot of people think servant only works well for web services, but I'd love to address this and make it a lot clearer that any content type / format works fine. I've even seen an application that define a `Zip` content-type and produces `.zip` archives on the fly from the data returned by handlers (through a suitable `MimeRender` instance).
This is basically a short simple introduction to "trees that grow" right? I like it, but why the title "trees that shrink"? It makes it sound like it is going to explore some dual concept or something.
I think the biggest takeaway from Haskell is how immutability, composability, purity and a declarative style can make your program more robust. The rest is just different techniques to get there. 
In this example I shrink the syntax tree by desugaring let-expressions, so I thought it was a fun title to use.
 int-graph.cabal src/ Data/ IntGraph.hs IntGraph/ Undirected.hs Directed.hs Algorithm.hs README.md LICENSE `stack sdist`, to take a look at the package as it would be released. `stack upload .` to upload on Hackage (for an additional sanity check you can also [upload a package candidate](https://hackage.haskell.org/upload#candidates)). Then to register it on Stackage see their [`README`](https://github.com/commercialhaskell/stackage) (mainly, make a PR to add your package to the `build-constraints.yaml`). I don't bother adding `stack.yaml`, to avoid checking in system-specific tweaks (e.g., using local development versions of packages), but it might make sense to add a template `default-stack.yaml` to the repository to make it easier for contributors to set it up by just copying it.
I've been quite happy with Scotty and Blaze.
The "Trees that Grow" paper really should have used `()` for most of the applications of `Void`, and compensated for reviewer complaints by making the annotations that get added strict. Then it could properly use Void for things like the `Exp` case where uninhabitedness is truly warranted. This would also make it much easier to do things like derive show, etc, while avoiding the ugliness that using Void has w.r.t. deriving Show, etc. GHC has enough random bottoms buried like landmines inside of the trees it builds. Why add more?
I ran into this when trying to derive `Show` for my AST type :). There was [some discussion about this](https://www.reddit.com/r/haskell/comments/5iovx9/trees_that_grow_interesting_paper_about_extending/dbai9k4) but it didn't seem like a good idea to deviate from the paper's choice to use `Void` instead of `()`.
Maybe "Shrinking Trees That Grow"? Anyway, I always appreciate writing that introduces a topic by solving real problems a thanks!
If I had thought of this I would definitely have used it :)!
The key is using ! annotations. With those data ExpX i a = LitX !(XLit i a) a | VarX !(XVar i a) | AbsX !(XAbs i a) (ExpX i a) | AppX !(XApp i a) (ExpX i a) (ExpX i a) | ExpX !(XExp i a) Shayan's argument deflates completely and you can use () off the shelf.
We tried using this for our compiler with a type family for tree annotations which change with compiler passes but that needs a lot more boilerplate as you can't get a functor instance for the annotations because type families cannot be partially applied. More generally, this approach seems to quickly lead to long compile times once you start increasing the number of family instances and AST types (you just have Expr here). I don't know how the GHC devs work around this.
Do you also then have to use a `Lazy` constructor for extensions as mentioned [here](https://www.reddit.com/r/haskell/comments/5iovx9/trees_that_grow_interesting_paper_about_extending/dil13t9), or is that not necessary?
&gt; here Depends. Typically the sort of data you're baking into these terms is stuff you're going to be strict in anyways, but if you really do want something lazy, then yes.
The newtype is, at runtime, exactly identical to a plain old `(Int, Double)`. The distinction only exists for the sake of typechecking. When using the `data` keyword, your type gets its own representation. This means there's a small semantic difference when it comes to `bottom` - see [here](https://wiki.haskell.org/Newtype#The_messy_bits).
Scotty, Blaze, and Shakespeare are definitely awesome
Thanks, I've updated the blog post with this approach at the end!
&gt; When did Haskell "click" for you? When i stopped reading about monads and started to just use them. Turns out it's way easier to grok theory when you have some intuitive understanding than other way around.
What's a monad transformer?
Miso for single page applications is Good ! 
You can simplify a lot of the pattern synonym noise in that version: pattern LitLet a &lt;- LitX _ a where LitLet a = LitX () a becomes pattern LitLet a = LitX () a
Servant is actually pretty bad for user facing pages IMO. For example: a 404 page with a nice error page (including HTML; which could do IO) which is correct for all content types. It's stupidly hard to get it to work for both errors inside handlers, and routing errors. 
It keeps clicking every time.
If you're only interested in static websites, consider [Hakyll](https://jaspervdj.be/hakyll/)!
See this: [https://www.kovach.me/posts/2017-02-03-haskell-bits-application-beginnings.html](https://www.kovach.me/posts/2017-02-03-haskell-bits-application-beginnings.html) It was enough for me but in haskell I was coding small programs so far.
It actually "clicked" when I started to understand the advantages of static typing. Coming from python it felt like magic, when my compiler told me things, that used to be runtime errors in python. ("Index must be a number" etc.)
Doesn't a middleware address both types of errors? You look at the response, if it's a 404 you grab the text and put it under a suitable disguise, may it be some JSON structure or an HTML page. Now, being able easily configure these things is something I have wanted for a while. Right now we're preparing another major change though (see [here](https://github.com/haskell-servant/servant/issues/841) -- this by the way changes the story of errors inside handlers). But this lack of configurability (tracked [here](https://github.com/haskell-servant/servant/issues/689) and some other issues) might very well be one of the things we look at next.
They just pretend they do 
Little bit later than "this week", but my stuff is here: https://github.com/duijf/stm-course
Global module namespace prefixes are overrated. Drop that `Data.*` bit.
Probably not right? A middleware is essentially agnostic to the site content in question. But a 404 page should hold links, potential suggestions, all sorts of images and items which are derived form the site's content. I agree with [charles](https://duckduckgo.com/?q=charles+ofarrel+web+lambda&amp;t=ffsb&amp;atb=v119-6&amp;ia=videos&amp;iax=videos&amp;iai=WhUFaZMFt6A) on this one. Just functions... functions work.
No, a middleware can look at the request and gather the information you need to offer a context-aware error, in a format that matches your content type, etc. Several people have done this in the past (including me). It's not great but like I said, your problem is on the roadmap :-)
Not sure about how to get `intero` to work in vim but I recently switch from using `intero` in emacs to [`ghcid`](https://github.com/ndmitchell/ghcid). This [blog post](https://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html) has some nice tips on how to use `ghcid`. I have one terminal running `ghci` which I can use to inspect types and another running `ghcid` which is essentially a much faster `stack build --file-watch`.
Sure, we're clever people, we know we bash past a lot of dumb issues. But I would suggest watching the talk I link to. I think it speaks to the essence of Haskell, functions and data types go a long way.
Is there a specific chunk of the type system that's giving you headache? The type system is a very broad topic and there are lots of subtle places to get stuck, but a little help can go a long way, so let us know. The best general advice I can give is to use the REPL and `:t` liberally, and also, check out typed holes, which you can use to help you figure out how to get a concrete term into the right type. If you keep getting stuck, folks are super friendly on IRC, and there is a great monthly sticky post for any questions. Also, the /r/haskellquestions subreddit exists for more long-form help. Don't despair - Haskell is largely not about complex topics so much as it is about the extremely far reaching implications of very simple topics. Revelations may be closer then they appear. 
No spoilers thanks ! ;)
When one day I was confused about monad transformers and I mentioned it on this subreddit. Someone put a long form comment that totally explained it for me, someone else took the comment and put it on the haskell wiki. Before I learned this, I was always stymied by libraries which used their own monadic type, and also failing to add effects into my code cleanly like randomness.
By hoping this will be resolved soon : https://ghc.haskell.org/trac/ghc/ticket/8095
Do you know why open type families are used for everything? I'd prefer something like this: ``` data ExprTag = LitTag | VarTag | AbsTag | AppTag type family Field decor (tag :: ExprTag) a data Expr decor a = Lit !(Field decor 'LitTag a) a | Var !(Field decor 'VarTag a) Int | Abs !(Field decor 'AbsTag a) (Expr decor a) | App !(Field decor 'AppTag a) (Expr decor a) (Expr decor a) data NoDecor type family FieldNoDecor tag a where FieldNoDecor 'LitTag _ = () FieldNoDecor 'VarTag _ = () FieldNoDecor 'AbsTag _ = () FieldNoDecor 'AppTag _ = () type instance Field NoDecor tag a = FieldNoDecor tag a ```
Is the comment about servant's approach in general? f not, and if this is still about custom error pages etc, a middleware _is_ a simple function. If it is about servant's approach in general, then I'm afraid the approach outlined in the talk just doesn't fulfill the requirements we had when we wrote servant. Servant isn't just a library for writing web applications, it's a way of describing web applications that you can use to implement servers, derive clients and all that. We wanted strongly typed handlers and client functions that are dependent on the description of a web application, and we needed to be able to add new "words" to the description vocabulary and be able to modularly explain to ghc what the effect of those new words should be when it comes to server-side handlers, client functions, etc. When putting it all together, only the current approach was left as a reasonably attractive option, among the many, many approaches we've read about and played with. And this was all dictated by the needs we had at Zalora at the time, we would certainly not have gone "this far" into type-level land without very good reasons to do so. =)
I can agree with this. I'm still without a doubt a complete beginner but I just wrote my first "useful" (to me) Haskell program and despite it only being a few tens of lines long, I learned a phenomenal amount just by sitting down and trying to do something (besides exercises from books or Project Euler problems) from start to finish. I feel like it's "clicking" in waves. I also highly recommend #haskell-beginners on Freenode. The guys there are incredibly helpful and will often take whatever time you need to help you figure out your problems and then even refactor your existing code, explaining different concepts along the way.
&gt; Based on a skim through the beam tutorials, it seems to me like you could actually define your records in a module that knows nothing about beam and a DB module could define type aliases or newtypes over your records and adapt them to beam after the fact. Almost. You have to use beam's 'higher-kinded data' constructor `Columnar` for each field you want to be a column. However, if you parameterize it with `Identity`, then you get back the regular Haskell data type, without any wrapping. Parameterization and such work as expected. The module you define them in must know a little about beam (really just the Columnar constructor) because a type cannot be parameterized by a type family (yet, anyway). On the other hand, your common data type module only needs to know about `beam-core`. `beam-core` compiles on old GHCs and GHCJSs and probably any other GHC derivative (like Eta), and I try to keep its dependencies minimal. &gt; DB module could define type aliases or newtypes over your records and adapt them to beam after the fact. If you're asking what I think you're asking, then yes, you could do this, but you may be stuck having to write `Beamable` instances by hand. This isn't difficult, just something to be aware of.
\&gt; and compensated for reviewer complaints about () having "two members" \_|\_ and () . . .
I fully agree with Edward. In the GHC's implementation (and any of my other presentations and implementations of TTG), we use a unit type and an empty type to correspondingly express a "no field extension" and a "no constructor extension". We could as well use the `!` annotated version. For the details on the current implementation approach in GHC, see https://ghc.haskell.org/trac/ghc/wiki/ImplementingTreesThatGrow/TreesThatGrowGuidance and https://ghc.haskell.org/trac/ghc/wiki/ImplementingTreesThatGrow As mentioned, bottoms also cause problems with automatic deriving. 
That's kind of my question. I understand that newtype are not lifted but data are. However, `(int, Double)` should be equivalent to `data Pair a b= Pair a b`, so am I right to believe that `newtype (Int, Double)` is then strictly equivalent to `data A = A Int Double` whereas `data B = B (Int, Double)` introduces a second level of lifting (ie a second chance to have a bottom)?
What do you mean by `*` module?
Everyone learns differently but this would be my recommendation as well. It's quite easy to burn significant quantities of time reading but get relatively little understanding in return. Code, on the other hand, makes you think about the ideas in concrete terms, which I think is generally more conducive to understanding.
To be honest, the syntax isn't all that different either. Result before pipe at the beginning instead of after `return` at the end; square brackets instead of curly; comma instead of semicolon. Anything else? Yet one looks kinda-like a snippet in an imperative programming language, and the other looks kinda-like a typical set definition in mathematics. It's cool that the two are actually so similar, and that they are actually just two different styles of talking about the same concept.
I use spock for a few simple apps amd am happy with it. If you want a more thorough overview [watch this video](https://twitter.com/taylorfausak/status/1002607257446703104?s=19).
We abandoned this "dispatcher" type family solution, like `:@` explained in the earlier version of TTG [1], since it does not scale well to support GADTs and existential types. In existential types, each constructor may have a set of a varying number of existential variables (with different kinds). We toyed with the idea of using type-level lists (or hlists), but in practice, we didn't see any benefits in not using separate type families (e.g., they are just simpler). Closed type families can also be used in a different way: to provide instances for a family of extension descriptors. For instance, see how we use the closed type family `XAppGHC` in https://ghc.haskell.org/trac/ghc/wiki/ImplementingTreesThatGrow/HandlingSourceLocations Above has the downside of coupling GHC-specific decorations together, hence making, for instance, the definition of the tree in the parsing phase `Exp (GHC Ps)` depend on the definition of `Type` defined in the type checking phase. [1] https://youtu.be/DSWoGdfYt68?t=893 
That sounds right to me, yeah, but I don't actually think I know more than you do. 
The title of the post is already a spoiler. The abstract of the paper itself doesn't mention Hindley-Milner.
I don’t think that Haskell is any different in that matter. Lack of maintenance and activity probably means you might run into trouble with the library pretty quickly. The less popular the platform, the more abandoned libraries there is. 
I've taught a bunch of people Haskell using this strategy and I think it's superb. I think most people should start by getting what they need to map their existing programming expertise onto Haskell. Typically, this means mastering IO and tail-recursive loops. Once you've got that, you're a Haskell programmer. The only thing left to learn is a truckload of ways to save yourself work. :)
The solver is written in Haskell.
I'm involved with two FLO projects: * Snowdrift ([site](https://snowdrift.coop), [community](https://community.snowdrift.coop), [code)(https://git.snowdrift.coop)). It's mission is to address issues of cooperation that are at the heart of the funding problem for free/libre/open software (and other public goods). * Databrary ([site](https://nyu.databrary.org/), [code](https://github.com/databrary/databrary)). This site helps researchers share their primary data (videos) to improve reuse and reproducibility. Ping me for more information about both of them. Snowdrift has a much better community for interested Haskellers, but Databrary has a lot more working code that could use some eyes and ideas.
But people do write big programs. It's just that if you look for the "right" way do do it before you even start, then you'll never get started. The Haskell community selects for a certain personality type, that values correctness more than many other programmers. That can be great... sometimes. But it can also lead to a form of paralysis, which definitely exists in Haskell to an extent that would be laughable in Python or Java. While we're debating whether we've got the semantics right for simultaneous events in FRP, the Java programmer will have already written a prototype of an application using event handlers and callback loops. There will be many things wrong with their implementation; but it will exist. In this spirit, I'd answer question #2 like this: absolutely, 100&amp;#37;, start doing side projects NOW. Maybe you'll just be writing "C in Haskell" (our word for the style of Haskell where everything is done with the IO monad and MVars instead of writing pure functional code). If it works, that's fine. You might be embarrassed by your code in a year's time; it doesn't matter. If you do not build things until you know how to do it right, then you will never have learned enough to build anything at all.
If you want to insinuate something, at least tell us what you're insinuating.
Awesome, thank you!
What's wrong with Yesod? It can be quite minimal if you want: https://github.com/parsonsmatt/yesod-minimal/blob/master/src/Minimal.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [parsonsmatt/yesod-minimal/.../**Minimal.hs** (master → 3dc0b1e)](https://github.com/parsonsmatt/yesod-minimal/blob/3dc0b1ea3c81c17f86acfa91e37d866d17744b29/src/Minimal.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e0xv9z6.)
&gt; you can't get a functor instance for the annotations because type families cannot be partially applied Perhaps you could get around this by using a Backpack signature instead of a collection of type families. The decorated AST would depend on a signature which declared one abstract type per constructor. Each "phase" would be represented by an implemention of the signature. Possible disadvantages: it would require more modules and internal libraries. Backpack doesn't like pattern synonyms on implementation modules. And the constructors would need to be qualified because they would not belong to completely different types.
A functor, as commonly used, for TTG makes sense when *all* constructors have a uniform common type variable. For this specific case, a function like`fmap`should be definable. Earlier, we had speed problems when defining class instances in a generic way (e.g., parametrised over all extensions using the `ForallX` synonyms defined in the paper). However, we solved this by providing explicit instances instead. You may see https://ghc.haskell.org/trac/ghc/wiki/ImplementingTreesThatGrow/Instances#PLANB As Alan also mentioned, there are some general (not specific to TTG) works to improve the performance of GHC when it comes to the constraint solving. 
Servant's intended use case (well-typed handlers) has some major problems with returning HTML as you'd like to handle it most web applications. The biggest one is that templates usually require more information than "just" the type you're returning. So for a JSON API, it's OK to write: type API = "users" :&gt; Capture "userId" Int :&gt; Get '[JSON] User Because the `User` record contains everything you want to see. However, if you want to render an HTML page for this route, you need to provide much more information, and typically based on state -- is the end user logged in? Are they an admin? etc. So you can't just write: type API = "users" :&gt; Capture "userId" Int :&gt; Get '[JSON, HTML] User because the `ToHTML` instance for `User` is essentially a function `toHtml :: User -&gt; Html`. No extra information allowed. You can get around this with a type like data Response a = Response a ExtraMetadata instance ToJSON a =&gt; ToJSON (Response a) where toJSON (Response a _) = toJSON a instance ToHTML (Response User) where toHtml (Response a metadata) = makeTemplate a metadata which uses `ExtraMetadata` for the `ToHTML` instances and delegates to the `ToJSON` instance for `a`. Servant does not make things like sessions or cookies convenient, which you want in webapps. Servant does not allow you to see the request, at all, except by the type classes you define. This makes it difficult and annoying to write things like "When this form is submitted with errors, redirect to the form page and render the errors; when the form is submitted successfully, redirect to the route provided." Redirects are also handled in the exception system (`throwError err302 { set the appropriate header here }`) which has pretty bad ergonomics for something you need to regularly. Sure, you *can* use `servant` to write a standard webapp. It's just an immense pain in the ass compared to Yesod which handles this use case extremely well. Since Servant and Yesod are both fantastic libraries at their intended use cases, and [they're trivially easy to combine](http://www.parsonsmatt.org/2016/12/18/servant_in_yesod_-_yo_dawg.html), there is no point in trying to contort Servant's API focus to rendering HTML pages.
Piling onto an already excellent comment: Servant’s API churn is atrocious because he project is actively experimenting with advanced type level programming. For example, [check out this pull request swapping Servant out for wreq in an API client](https://github.com/haskell-works/hw-kafka-avro/pull/11/files). If you want to support multiple versions of Servant you’re stuck adding a lot of `CPP`. Even if you don’t want to support everything though, this gives you a good idea of what you’ll have to modify just to keep up to date.
You're welcome. Please feel free to get in touch directly if you need any Brick help!
I had a look and unless I'm mistaken `fmt` is not (I don't know how to call it) a formatter ala printf, where you postpone all value at the end of the format (`"(%d)" a_complex_pression`, but a "in place" formatter, you format things as you go, ie pretty much equivalent to `"(" &lt;&gt; show a_complex_expression &lt;&gt; ")"`. I use `formatting` (and `printf`) because I want have a format as close as possible as the final result and don't want to intersperse Haskell expression in the middle. So, unfortunately I don't think `fmt` is the right library for me.
[Scotty tutorial](https://www.reddit.com/r/haskell/comments/8rapv4/read_you_a_scotty/) posted just the other day.
If the route returns HTML then it doesn't make sense to pretend to return a User. What does that even mean? The route result would be something like `Get '[HTML] Html`. And I'm not really sure what you mean by "no extra information allowed". The `ToHTML` instance isn't the right place to put your route logic. The right place is the route handler which has access to everything you've captured in the API. We usually have special API components that let us capture the current session, user role etc. The redirection stuff _is_ a bit gross though. That's definitely true.
When I saw how people who don't use Haskell suffer.
As others have pointed out Servant is perfectly capable of serving HTML. I will add that if you want an API start with PostgREST. It is easiest and pushes you towards a well designed solution.
re-read my question
⊥
⊥
&gt; If the route returns HTML then it doesn't make sense to pretend to return a User. What does that even mean? well, er, that's the point of servant! you return a well-typed value, and it handles the encoding/decoding for you. consider this edit: &gt; If the route returns JSON then it doesn't make sense to pretend to return a User. What does that even mean? It means: "I return a `User`, and the content-type of the request determines how the API viewer will see the response." Duplicating all of your routes is another work-around for Servant's poor suitability for websites, but you're throwing away the entire point of Servant's typed handlers, and now you can't generally reuse the handler functions (whereas a `foo :: Int -&gt; Handler User` can be easily reused elsewhere in the codebase, a `foo :: Int -&gt; Handler Html` can't). You're also now duplicating all of your routes, one with an `Get '[HTML] Html` endpoint and one with a `Get '[JSON] ActualResource` endpoint. If you're gonna duplicate all the route logic, then just use Yesod, which is great for this stuff. If you don't need the API side of things, then Servant provides very little benefit for a rather extreme complexity cost.
&gt; I don't want to use Yesod. Ok, why not? 
&gt; I don't know why a lot of people think servant only works well for web services All of the examples posted online are JSON APIs. I don't think I've seen a single fleshed-out example of a web app using Servant to render HTML, and the several times I've tried to do this, it was an awful experience compared to other libraries. As fantastic as Servant is for JSON APIs, it just doesn't support the needs of traditional websites very well right now.
Routes that return HTML are inherently different from ones that return JSON values. I really don't think there's any situation in which the same route should be return both JSON and HTML. I also don't agree that the "return types" are the _entire_ point of servant. As you say yourself, servant handles encoding _and_ decoding for you. You still get to use the decoding part. You still get to specify the route components and parameters at the type level. As for reusebility, the same reasoning could be extended to claim that since `main` has the type `IO ()` you can't reuse it, but if it instead was `IO SomethingElse` you could. There's nothing stopping you from defining your `Int -&gt; m User` function and reusing that in the HTML route or somewhere else in your codebase. I don't see why it matters that you can't reuse the handler itself.
Such laziness...
After about 5,000 lines of written code.
So if you're using the `temporary` package you provide a "template" for file names i.e.: Prelude System.IO.Temp λ&gt; temp1 &lt;- createTempDirectory "." "" Prelude System.IO.Temp λ&gt; temp1 "./9631" Prelude System.IO.Temp λ&gt; temp2 &lt;- createTempDirectory "." "cabal-tmp-" Prelude System.IO.Temp λ&gt; temp2 "./cabal-tmp-9631" So possibly in the stack version of the code there's no template being passed and so you end up with just plain numbers.
... what? Most advice I see in this thread is to write code. I don't get what your speculations are based off. People are upset about the state of software and try to find better ways. They still write the bad code, that's why they're upset. 
Unix tools even when they should work don't work well on Windows. Been my experience for decades. There are all sorts of subtle bugs. Microsoft used to be very focused on creating a POSIX compatible environment that really worked: https://en.wikipedia.org/wiki/Windows_Services_for_UNIX and of course Cygwin used to be a focus. I really felt this was the right direction get a workable Unix running on the NT kernel and a workable Unix that could use of all NT rather than slowly getting one tool at a time to work. Perl never quite got there despite a massive effort. 
**Windows Services for UNIX** Windows Services for UNIX (SFU) is a discontinued software package produced by Microsoft which provided a Unix environment on Windows NT and some of its immediate successor operating-systems. SFU 1.0 and 2.0 used the MKS Toolkit; starting with SFU 3.0, SFU included the Interix subsystem, which was acquired by Microsoft in 1999 from US-based Softway Systems as part of an asset acquisition. SFU 3.5 was the last release and was available as a free download from Microsoft. Windows Server 2003 R2 included most of the former SFU components (on Disk 2), naming the Interix subsystem component Subsystem for UNIX-based Applications (SUA). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; All of the examples posted online are JSON APIs. Right, the docs just go with JSON for simplicity and because ever since servant came out, it does seem to be the most common use case. And almost everyone is familiar with aeson so we don't need to explain much there. Maybe it's time to enhance them (or the cookbook) with more examples of other content types, with a special place for HTML which is probably the most common, but including perhaps dynamically generated CSS, zip archives, CSV and what not. &gt; I don't think I've seen a single fleshed-out example of a web app using Servant to render HTML And this ties back into my previous answer: we should make it clearer that it's possible and show an example or two. All the examples I've seen were either opensource projects from all sorts of Haskellers on github, or private (work) projects. Github's search shows quite a few such applications (and real ones, written by other people, not just the toy kind that we have to show in the tutorial to keep things short and simple): using [servant-lucid](https://github.com/search?q=%22import+Servant%22+lucid&amp;type=Code), [servant-blaze](https://github.com/search?q=%22import+Servant%22+blaze&amp;type=Code) and apparently even [servant-ede](https://github.com/search?p=3&amp;q=%22import+Servant%22+ede&amp;type=Code). Ideally we would have examples for each of those in the cookbook, for visibility and so that we get a chance to explain how these libraries can be used through a concrete example, differently than the haddocks. &gt; As fantastic as Servant is for JSON APIs, it just doesn't support the needs of traditional websites very well right now. Given what I said above, I don't think this is correct. However, if all you're going to do for an application is have a web application that serves dynamically generated HTML (and associated static files), without the need to hit the said pages from Haskell/JS/etc code, etc, then I think servant does not have any particular edge compared to the alternatives. More importantly, you'd be going through the trouble of describing your entire web application structure as a type for close to no gain. But it is certainly possible to use servant for non-JSON APIs and people build all sorts of "toolboxes" to address their specific needs the way they want to. I definitely agree though that it would help a lot if this was all a lot more documented and advertised.
I pretty much agree with your suggestions. If one can't write `ToHTML SomeType` because the instance would need some context (e.g the current date, to be displayed somewhere on the page), then you either wrap `SomeType` in something like the `Response` type above or you just generate your HTML from the handler and just return an `Html` value there, splitting the work between a few reusable functions along the way. Both approaches are somewhat equivalent and are in fact exactly what we would be doing with most other web frameworks, I think?
Thanks, I've made this change as well :).
It turns out it isn't even a bug in autoconf, but documented behavior, which our tooling needs to work around :-/ http://lists.gnu.org/archive/html/bug-autoconf/2018-06/msg00002.html
You know, once upon a time, when folks submitted forms, instead of calling 'preventDefault' and hijacking the browsers native behaviors, we'd just let the browser send the form data to the server at the specified URL, and then use that to drive a parametric HTML generation process in the response. We didn't even have to write out client side handlers for the response data , the browser just loaded the HTML right there on the spot. It was almost as if someone built an implementation of a set of software protocols to support that specific interaction, instead of expecting you to suppress an existing set of behaviors and implement a set of protocols on top of an existing protocol layer designed for a totally different purpose.
i wish all websites worked like this :(
And now we have WSL.
I wish more that we'd recognized the need for a broader set of universal RPC protocols, etc, for client driven user interactions and dynamic content generation instead of trying to over-load the concept of resource identifiers and gluing crap together in a half-baked scripting language. Browsers should be able to natively support parametrically dynamic dom components - If my database API can handle `userFetch = Statement.prepare("select email from User u where u.name =?")` why can't my browser handle `genBanner = document.prepare("&lt;h1&gt;Welcome,&lt;/?&gt;&lt;/h1&gt;",parentElement)` without 12 third party libraries and 3,500 transitive dependencies? I suspect web assembly is going to end up solving this problem sort of accidentally as they start implementing browser API interactions that don't need to be bootstrapped with JS, but, perhaps I'm being too optimistic.
https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#ghci-scope
 dividedBy :: Integral a =&gt; a -&gt; a -&gt; Maybe (a, a) dividedBy _ 0 = Nothing dividedBy num denom = Just $ go num 0 where go n count | sgn n == sgn denom &amp;&amp; abs n &lt; abs denom = (count, n) | otherwise = go (n - sgn n * abs denom) (count + sgn n * sgn denom) sgn :: (Ord a, Num a) =&gt; a -&gt; a sgn x | x &lt; 0 = -1 | x == 0 = 0 | x &gt; 0 = 1
That's a really good reason to prefer \`formatting\` over \`fmt\`! You should never use \`printf\` formatting, because it's unsafe (we actually had bad bug in production because of that and we spent a lot of time to find it). So I like \`formatting\` because it provides type-safe interface for \`printf\`-like formatting. \`fmt\` is slightly better than \`"(" &lt;&gt; show a\_complex\_expression &lt;&gt; ")"\` because you don't need to call \`show\`. And \`fmt\` can be much shorter in code like this: "There are "+|n|+" million bicycles in "+|city|+"." With \`formatting\` it will be something like this: stext ("There are "&amp;#37;int&amp;#37;" million bicycles in "&amp;#37;text&amp;#37;".") n city Unless you're currying arguments. In that case \`formatting\` might be more actually more readable and prettier. And once you need complex formatters like padding or block lists, you won't have text close to desired output anyway, unfortunately :(
Also I think sgn == signum but I'm not sure
Yes you're right, didn't notice that. Thanks!
The inconsistent indentation has something to do with the default config of haskell-vim. Will try ot change that, thanks!
There was never a defining moment. I just slowly got better through practise.
bad bot
Thank you, yitz, for voting on WikiTextBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
If this was the case, then these kinds of file paths should be generated _everywhere_. However, it only seems to happen on AppVeyor. Also, to my knowledge, we always pass in template names. Finally, the path segment is reliable `\1\`, not a randomly generated value.
You are correct. Thank you for pointing that out. I will fix that shortly.
They mean, to make stuff on reddit appear in a code block, indent it four extra spaces instead of using ```` ``` ```` which isn't supported
You can also watch a video lecture version of 4: [Part 1](https://www.youtube.com/playlist?list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_), [Part 2](https://www.youtube.com/playlist?list=PLbgaMIhjbmElia1eCEZNvsVscFef9m0dm)
&gt; :module supports the * modifier on modules, which opens the full top-level scope of a module, rather than just its exports.
This matches my experience with `scotty`, `spock`, and `yesod` as well. You'll spend more time up front configuring `scotty` and `spock` to do what you want while `yesod` has just about everything you want as opt-in features. `yesod` can be quite minimal as the example above shows. I'd suggest reading the [scaffold sources](https://github.com/yesodweb/yesod-scaffold/) in addition to the docs to get a good picture of what the possibilities are and try/enable/bring each feature in at your own pace if needed. 
Ah, the joy of stringly-typed programming!
As an added bonus, try to implement `dividedBy` using implementations of `quot` and `rem` with your constraints. You'll notice that `divMod` is exactly `quotRem` unless the signs of the inputs are opposite. And you'll get the added benefit of learning about threading a `Maybe` through multiple functions.
Interesting but that seems like it is just a VM. Am I reading that wrong? 
Oof ok that sounds scary!
You’re welcome. Again great piece !
because
I can't really speak for anyone else, but I know I suffer from analysis paralysis sometimes. I often tell people to just be practical and code something, but I guess one some level I'm really telling myself.
Monad transforms and the readerT pattern (which I started using before I even realised it had a name). I normally have an `AppContext` type that has my config object, the random number generator, the database connection pool, or whatever else and use readerT to make it accessible everywhere. It's flexible enough of a structure that I can start most projects with it and not worry too much unless something specific comes up. 
Ahh, `:load *file1.hs *file2.hs` and `:module *A *B` works differently. Spoiler: the first one doesn't do it, the second does.
I know I shouldn't use `printf` but I do because, well, I'v been using for 20 years, and I know how to get padding, precision and stuff and it's really concise. What I like with `formatting` is the ability to define by own formatter, but I don't like the `"%...%"` noise. Maybe we need a TH version of formatting ;-). However, I generally use formatters when I need to define a format somewhere, and use it later. When I use `&lt;&gt; show ... &lt;&gt;` (which also happend) I probably should try `fmt` instead ;-).
Hey, maxigit, just a quick heads-up: **happend** is actually spelled **happened**. You can remember it by **ends with -ened**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Nope not a VM at all. The Windows 10 kernel natively supports the Linux kernel API as an equal peer to the NT kernel API. This has been a fully supported feature of mainstream Windows 10 releases for more than a year. And the Windows native "command line prompt" window has been vastly improved, now quite usable when you're running a Unix shell. Times are changing...
Hmm, does this mean AppVeyor is intentionally creating folder names that contain backslashes? If so, that's awful. Or is it some weird interaction between AppVeyor and the way autoconf is set up in the network library?
Why is using a Haskell framework such as keter "reinventing the wheel", whereas using a non-Haskell framework such as kubernetes not "reinventing the wheel"?
Who ever said this? I only argued from my personal experience; unfortunately I haven't used keter yet.
Hey [u/maxigit](https://reddit.com/u/maxigit), it's perfectly okay to misspell things. I *actually* hope that you have a nice day. ***** I am a bot.
WSL’s pretty good, but every so often it bites you. And file interactions are definitely on that list.
OP asked: &gt; Is there any solution at all, which offers streamlined zero-downtime deployments... more tightly related to the Haskell out there? Your answer was: &gt; I'd say just use Kubernetes for that and don't reinvent the wheel. But in fact, there is quite a bit of tooling available out-of-the-box that is tightly integrated with Haskell build toolchains. The one I know best is keter, and there is also a lot available for docker-based workflows, and some for heroku. Probably other stuff, too. Is there out-of-the-box tooling for kubernetes-based automated Haskell hot-swapping build-and-deploy? If so, we would be very happy to hear about it. 
&gt; Is there out-of-the-box tooling for kubernetes-based automated Haskell hot-swapping build-and-deploy? If so, we would be very happy to hear about it. We would be happy to hear about this too! 
It's a thing that turns a monad into another monad. Example: newtype StateT s m a = StateT { runStateT :: s -&gt; m (a, s) } Now you can take any monad - say, the `Maybe` monad - and turn it into a monad that also has state: type MaybeState s a = StateT s Maybe a Of course, for this to work, we need `Functor`, `Applicative` and `Monad` instances for the `StateT` type: instance Functor m =&gt; Functor (StateT s m) where fmap f (StateT g) = StateT $ \s -&gt; fmap (first f) (g s) -- first is from the Bifunctor class I'm not going to show the `Applicative` and `Monad` instances here - see if you can figure them out yourself! We can also write: class MonadTrans t where lift :: Monad m =&gt; m a -&gt; t m a instance MonadTrans (StateT s) where lift m = StateT $ \s -&gt; fmap (\x -&gt; (x,s)) m Now we can start working with our `MaybeState` type: type UserMap = Map Int User lookupName :: Int -&gt; MaybeState UserMap Name lookupName userId = do userMap &lt;- get -- get :: StateT s m s, pretend we've implemented this user &lt;- lift $ lookup userId userMap -- lookup :: k -&gt; Map k v -&gt; Maybe v, we lift it to StateT return $ getName user -- assuming User is a record with a field getName
A quick search found at least four different libraries for couchdb. However, none of them look very active now. It looks like there was a burst of interest in couchdb five or six years ago, but interest has waned since then and many people moved on to other kinds of no-SQL persistence. Some of these libraries actually look really nice, written by people who in general are quite active. If you are a couchdb enthusiast, you might rekindle the spark.
If you're looking for some TemplateHaskell-like solution I could recommend recently announced `PyF` package: * https://github.com/guibou/PyF
I think I experience "analysis paralysis" more frequently when writing Haskell than in any other language. However, I feel like while I spend more time thinking about the problem and designing a rough solution in my head, when I end up writing it, it's fast and easy to write, especially considering that it is very often essentially correct as soon as the type checker accepts it. I have no empirical data to support it, but I tell myself that the time spent staring into space and thinking is less than the time I'd have spent fixing bugs or refactoring to accommodate new requirements in other languages. 
Oops missed that call, although on new reddit it seems just fine? Anyway fixed that, thanks for clarifying for me.
Alternative approach: ```hs db2 :: forall a. Integral a =&gt; a -&gt; a -&gt; Maybe (a, a) db2 _ 0 = Nothing db2 num denom = Just $ f num denom where f :: a -&gt; a -&gt; (a, a) f n d | d &lt; 0 = negDenom (f (n) (0-d)) f n d | n &lt; 0 = negNum (f (0-n) d) f n d = simple 0 n absDenom = abs denom negDenom (n, d) = (0-n-1, d-absDenom) negNum (n, d) = (0-n-1, absDenom - d) simple :: a -&gt; a -&gt; (a, a) simple count num | num &gt;= 0 &amp;&amp; num &lt; absDenom = (count, num) | otherwise = simple (count+1) (num-absDenom) ```
Haskell's already got a *Space Force*!
&gt; load up GHCi in gdb (that’s a bit fiddly and I won’t go into the details here) Does anyone know some details for doing this?
GHCi is a script that invokes the real binary, that's part of the problem. You have to invoke the real binary and pass the correct flags, particularly \`-B/path/to/ghc/lib\`. If GHC is dynamically linked (which it usually is) you also need to \`set environment LD\_LIBRARY\_PATH /some/huge/list:/of/paths\`. I normally put all this in a \`.gdbinit\` file so I don't have to repeat it, and I've also made a script to generate the \`.gdbinit\` file for a particular ghci invocation.
Author here: I think it fills a gap in haskell's documentation. I'm thinking of doing the same for Maybe. What do you think ?
Can someone explain this combination of module exports and imports in the file src/Lets.hs of the lets-lens project? ```haskell module Lets ( module L ) where import Lets.Data as L import Lets.GetSetLens as L() import Lets.Lens as L() import Lets.OpticPolyLens as L() import Lets.StoreLens as L() import Lets.Existential as L() ```
Not sure what you mean by "file interactions". I use Ubuntu running on WSL for Haskell development all day long. Since WSL became a stable release I have had no issues. It's the best of both worlds. The only thing is that you need to use an Ubuntu build of GHC compiled with a flag set that prevents GHC from pre-allocating a terabyte of memory. /u/hvr_ provides those GHC builds in his [PPA for GHC on WSL](https://launchpad.net/~hvr/+archive/ubuntu/ghc-wsl).
Interested indeed (especially the padding section)
I usually add the `gdb` invocation into the wrapper script by manually editing it. I have written up the process in https://ghc.haskell.org/trac/ghc/wiki/DWARF#DebuggingwhilebootstrappingGHC
&gt; &gt; [] == mempty &gt; True This one looks different than the others, but I understand why: if you just type `mempty`, you'd get an ambiguity error, or type defaulting would cause the result to be `()` instead of `[]`. How about using a type annotation instead: &gt; &gt; mempty :: [Int] &gt; []
That's a nice trick.
Why can't you use \`gdb attach\`, or \`gdb -p pidOfRunningGHCI\`?
For what it's worth, I have a [script](https://github.com/bgamari/ghc-utils/blob/master/debug-ghc) which makes this task significantly more convenient. Specifically, I would write debug-ghc ghc --interactive ... 
Except for the first one, these imports use explicit import lists. import Lets.GetSetLens as L () This imports no identifier from `Lets.GetSetLens`, thus there is no conflict. Empty import lists can be used to only import instances (because they are always imported). I'm not sure why they'd use empty import lists here though.
Awesome work. Maybe I don’t have to restart ghci as often after these fixes. It tends to get slower and slower over time (just like web browsers). 
Well, there is a web framework just named `simple` http://hackage.haskell.org/package/simple-0.11.2/docs/Web-Frank.html
There's a super common bug here I'll let you figure out (one that I've written myself, and fixed in two other unrelated haskell libraries). Try this: \*Main&gt; (negate maxBound :: Int) \`dividedBy\` 5
IsList and IsString? 
Have you seen [rank2classes](http://hackage.haskell.org/package/rank2classes-1.0.2#readme), [conkin](http://hackage.haskell.org/package/conkin), and similar approaches? I think they would let you lose some of the boilerplate. 
Yes, you can also do that. Sometimes it's more convenient to have the wrapper though, e.g. if you want to have gdb feed the input, or if you want to stop it before it gets to the prompt.
I had no problem using the GHC that Stack downloads, which I believe does the preallocation.
Gotta love TRAC issues. Inconclusive discussion, multi year gaps and then sometimes “I’ve pushed a 10,000 line change that should address this”
It's not the backslashes, it's a backslash followed immediately by a numeric which triggered the bug.
Why only base? There's more in the prelude even.
It's [this](https://media.giphy.com/media/3rgXBrg00rx6gmU7lK/giphy.gif) , but with monads instead of cats.
A friend of mine is trying to implement some algorithm involving \[Vector Addition Systems with State\]([https://en.wikipedia.org/wiki/Vector\_addition\_system](https://en.wikipedia.org/wiki/Vector_addition_system)). At some point in the algorithm, he would need to compute SCCs of the underlying graph. He wants to implement this in Haskell. What would be a recommended way to represent his abstract machine?
**Vector addition system** A vector addition system (VAS) is one of several mathematical modeling languages for the description of distributed systems. Vector addition systems were introduced by Richard M. Karp and Raymond E. Miller in 1969, and generalized to vector addition systems with states (VASS) by John E. Hopcroft and Jean-Jacques Pansiot in 1979. Both VAS and VASS are equivalent in many ways to Petri nets introduced earlier by Carl Adam Petri. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Because I do not know Haskell enough... I'm covering the [TypeClassOPedia](https://wiki.haskell.org/Typeclassopedia) classes for now.
Looks great, but I think you meant for the section before Traversable to be Foldable, not MonadPlus
Fixed. Thanks.
I don't understand this comment. `Prelude` is *in* `base`.
GHC.Base is something else. It doesn't have Eq for List, for instance. 
Do you have anything specific in mind? From what I can see `Validatable` type family already does a pretty good job in that department. Or are you referring to the internal code? I'd like to avoid dependencies as much as possible and make things simple for newcomers. I'm actually already refactoring initial code and have managed to make it much cleaner than it already was :)
&gt; It's a thing that turns a monad into another monad. Oh god. I think I'm gonna have to study up on monads a bit more before any of this makes sense to me :P Thanks for the explanation, I'll definitely come back to it!
Microsoft has been working hard on improving the situation with this incompatibility (which also affects the go compiler btw). The normal GHC build has worked for quite a while, but both GHC itself and binaries created by GHC caused a long latency at startup. Last I checked, it was about 12 seconds on my PC. But that was over a month ago, before I sent in my Windows PC for a warranty replacement, and before the latest Windows OS update. Perhaps there has been a further improvement with the current Windows build.
Perhaps not after all, at least if you wish to have distinct error types for different fields. If you're willing to simplify the bunch of error types to something generic like `[String]`, you could use `Rank2.pure` and record update to easily construct a Validator for a small subset of fields. As for dependencies, `text` is already bringing way more transitive dependencies your way than `rank2classes` would. 
Well, the whole idea is to have everything be as generic as possible. It is not necessary to use `[String]`, you can use anything you want as an error but if you want to run multiple distinct checks they have to be "combineable" / make a `Semigroup` in some way. It is perfectly possible to have different error types for different fields. Maybe I haven't made that clear enough in the documentation.
doesn't `rr record sh -c ghci` work? you should be able to pick the direct invocation out of `rr ps` and then replay it with `rr replay -p &lt;pid&gt;`. also doesn't `gdb --args sh -c ghci` work as well? with `fork-follow-mode` set to `child`?
Yes, learn monads before monad transformers ;) 
That's the specific path that triggered the error. But Nick Bowler's response to the upstream bug was that the root cause is autoconf's documented inability to deal with paths that contain backslashes and other nasties. So if AppVeyor does that and then we rely on autoconf, it's a toxic mix. Nick says that a PR to add that to autoconf would be graciously accepted. Hey, m4 is a functional language. Good luck.
I'm not sure I understand. This article seems to cover things that are not defined in \`GHC.Base\`, like \`Foldable\`. You can verify this by starting running \`ghci -XNoImplicitPrelude\`, \`import\`ing \`GHC.Base\` and trying \`:i Foldable\` (it won't find \`Foldable\`). Likewise for \`Traversable\`. (Also just saying "base" usually refers to the entire standard library, since \`base\` is the package name for the standard library.)
It doesn't matter. They're doing a good job and they're doing more. 
If you want url routing (with some friendly sugar), but essentially just a Request -&gt; IO Response server (using WAI), we wrote http://hackage.haskell.org/package/fn for essentially this. (it's used in production as well, so not a toy). 
Did you use the `Unboxed` or `Storable` variants?
No, just the "regular" boxed versions (since in general the type 'r' will be types such as Rational or some arbitrary precision Real number type) 
I'm trying to make a generator using Hedgehog with IO, following along with [https://teh.id.au/posts/2017/04/23/property-testing-with-hedgehog/index.html](https://teh.id.au/posts/2017/04/23/property-testing-with-hedgehog/index.html) for examples. When I have something like the code in the article: genWord :: MonadIO m =&gt; GenT m Text genWord = **do** ws &lt;- T.lines &lt;$&gt; liftIO (T.readFile "/usr/share/dict/words") Gen.element ws and try to use it in ghci, I get: \*Main Lib&gt; Gen.sample genWord &lt;interactive&gt;:22:12: error: • Could not deduce (MonadIO Data.Functor.Identity.Identity) arising from a use of ‘genWord’ from the context: MonadIO m bound by the inferred type of it :: MonadIO m =&gt; m Text at &lt;interactive&gt;:22:1-18 • In the first argument of ‘Gen.sample’, namely ‘genWord’ In the expression: Gen.sample genWord In an equation for ‘it’: it = Gen.sample genWord I'm still somewhat new to monad transformers -- does anybody have an idea what I need to do here to get this to work?
The simplest possible way to represent this seems to be a rose tree of `[Int]` with smart constructors to ensure a fixed length. You could go -MUCH- fancier here, but your signal to noise ratio could suffer, and it seems extremely unlikely that you'd need to for such a fixed problem domain unless your use-cases got very strained. I'd advise not doing so unless your friend deliberately wants to flex their Haskell muscles - keeping things simple seems to be the best way to focus on playing with the algorithm, which I assume is the point. 
Thank you for the word. I decided to go slowly and built my own lib for couchdb.
\&gt; However, just sorting a list of two dimensional vectors is about four to five time as slow as a direct implementating using 'Linear.V2' Can you provide a gist or a link to a branch where someone else can run these benchmarks? I have some ideas, but I want to make sure they do better in your actual benchmark.
of course! this actually has a lot of applications. Heterogenous collections, monad/functors of things that require constraints (sets for example). There a lot of things where you can think of a way to define monads/functors, but (-&gt;) is too flexible and would break it.
Some strange defaulting is going on, I'm not sure why, but you can try using a type annotation. &gt; Gen.sample genWord :: IO [Text] Or define &gt; let sample :: GenT IO a -&gt; IO [a] ; sample = Gen.sample &gt; sample genWord
The type of [sample](https://hackage.haskell.org/package/hedgehog-0.6/docs/Hedgehog-Gen.html#v:sample) has changed since then, and only accepts `Gen a`, i.e., `GenT Identity a`. I'm not sure why. For this example, a better way to write it is to separate reading the file from running the generator. &gt; ws &lt;- T.lines &lt;$&gt; T.readFile "/..." &gt; sample (Gen.element ws)
Couldn't `Functor` be: class (Category source, Category target) =&gt; Functor source target f where fmap :: source a b -&gt; target (f a) (f b) Not sure if this is abstract nonsense either! But I have a feeling it could be handy for modelling compilation at least!
That is a BIG statement
Not exactly what you're asking for, but `mmorph` has a class for [monads in the category of monads](https://hackage.haskell.org/package/mmorph-1.1.2/docs/Control-Monad-Morph.html#t:MMonad).
Sure, all of these work. However, the script allows me to avoid all of these headstands. I can just replace `gdb` with `debug-ghc` and go on my merry way.
Every path on Windows contains a backslash in it. The [PR that landed](https://github.com/haskell/cabal/pull/5388) faked this by converting backslashes to forward slashes. But AppVeyor isn't doing anything wrong with backslashes, as it's inherent to Windows.
WSL is a _bit_ slow for me, but pretty good overall. I give Microsoft huge props for doing a great job on this. But I totally disagree with you on one point: the Windows console hasn't gotten any better at all, you're crazy ;). I downloaded a third-party console to deal with it.
The Haskell type system do facilitate extensive refactoring, so it's actually *easier* to do something that works and change it to a more 'correct' approach later.
With the relocatable builds coming out of hadrian, this should now be almost as easy as ``` gdb ghc -- --interactive ```
&gt; continually failed to add effects into my code cleanly like randomness. I was JUST trying to figure out how to do this. I tried googling and found a lot of things, but I wasn't quite sure how to make sense of what they were saying. Any tips on where to go to get random numbers in my program? 
I can't see how to express: - the category of functors - the identity endofunctor … without which it's hard to go on and define Monad in terms of an endofunctor and two natural transformations.
Codewars is really fun. One thing which sometimes frustrates me are the random tests which fail without informing you for which inputs it is failing. I later realized that this was intentional and is this way to encourage users to write more tests apart from what is provided as a basic test suite. I think I am getting a hang of it now.
Hi, I'm looking for someone to help me maintaining this [https://github.com/blender/Rome/](https://github.com/blender/Rome/) There is plenty to do :)
I’ve also been a beginner for the last months and this is what I ended up doing. There’s only so much theory and blog posts one can read. In the end, reading was just making me feel impatient to use the things I read about - but the other pieces of the puzzle were missing. Most of the time, that meant that whatever I had read about a few weeks ago, I forgot. I strongly feel that unless you apply what you learn and see it in action, in your own code, you probably will not remember it. Reading about what you already know or reading as you go, on the other hand, is quite beneficial too. I just think one needs to strike a balance. Haskell as a language has so much to offer and for most people it’s such a paradigm shift, that it’s easy to lose yourself in theory and never really apply it. 
Richard Bird says here that the do notation was "suggested by John Launchbury in 1993 and was first implemented by Mark Jones in Gofer." [https://books.google.com/books?id=iGenBAAAQBAJ&amp;pg=PA275&amp;lpg=PA275&amp;dq=richard+bird+Mark+Jones+John+Launchbury+do+notation&amp;source=bl&amp;ots=TH2c426A2-&amp;sig=Q8Tqu1xt\_su7Y1EAnhgiefcoyKE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjRqqSzk-TbAhXIxFQKHdFdDxkQ6AEIKTAA#v=onepage&amp;q=richard&amp;#37;20bird&amp;#37;20Mark&amp;#37;20Jones&amp;#37;20John&amp;#37;20Launchbury&amp;#37;20do&amp;#37;20notation&amp;f=false](https://books.google.com/books?id=iGenBAAAQBAJ&amp;pg=PA275&amp;lpg=PA275&amp;dq=richard+bird+Mark+Jones+John+Launchbury+do+notation&amp;source=bl&amp;ots=TH2c426A2-&amp;sig=Q8Tqu1xt_su7Y1EAnhgiefcoyKE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjRqqSzk-TbAhXIxFQKHdFdDxkQ6AEIKTAA#v=onepage&amp;q=richard%20bird%20Mark%20Jones%20John%20Launchbury%20do%20notation&amp;f=false)
Here you go: https://github.com/noinia/hgeometry/tree/fasterpoint Just running the default benchmark (which is Algorithms.Geometry.ConvexHullBench) gives the results I mentioned above. In the last commit of that branch I tried to rip out other stuff in hgeometry to minimize the dependencies.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [noinia/hgeometry/.../**5d33065ebd3e5a60ed3893824ddd05ed975218e9** (fasterpoint → 5d33065)](https://github.com/noinia/hgeometry/tree/5d33065ebd3e5a60ed3893824ddd05ed975218e9) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e11bcos.)
The implementation is slightly clunky, but essentially it's just a function from monad to monad - i.e., you give me a monad and I'll give you another monad. A common example might be `WriterT`, which has the essence "you give me any monad, and I'll add the ability to keep a running log to that monad."
Is it possible to convert windows iso 1250 to utf8 in haskell?
Why stackage does not find `Data.List.isInfixOf` when searching for `String -&gt; String -&gt; Bool`? Hoogle does not have any problem with it!!
Cmd window is now fully VT100+ compliant, including colors. And cut-and-paste works normally now, not the old awkward way. Windows 10 GUI is growing a general "tabbed window" API, and Cmd window will be one of the first to support it. There are many other improvements; a dedicated team at MS has been working on it for about two years. What do you find still missing? They answer feature requests on GitHub, so if there's something important that they're not already working on, submit it there. Does this sound to you like the old Microsoft we all know and love?
`Eq`, `Ord`, `Show`, and `Read`. But OP is focusing on [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia) classes.
You now added a comparison to `Maybe`. That's nice! I suggest making the cases more parallel though. For example, for `Monad` this would be more parallel: List: &gt; mul xs ys = do x&lt;-xs; y&lt;-ys; return (x*y) &gt; mul [1,2] [3,4,5] [3,4,5,6,8,10] Maybe: &gt; mul x' y' = do x&lt;-x'; y&lt;-y' return (x*y) &gt; mul (Just 2) (Just 5) Just 10
Thanks for the answer, so something like this: ``` data UserT' a f = User { someField :: Columnar f a , someOtherField :: Columnar f Int , yetAnotherField :: Maybe a } -- newtype so that I can have instances, could I somehow switch the order of `a` -- and `f` so that I can use `type` instead and get rid of the wrapping? newtype User a = UserT' a Identity type UserT = UserT' T.Text ``` This way, users of `User` wouldn't have any idea that beam is involved. I think this is a huge step up from what we have with `persistent` in terms of flexibility. But I'd love to be able to completely isolate `beam` while defining the type. That would be a killer feature.
Well, that's not annoying to work around *at all*.
I don't know anything about Blockchain. Can I still apply? 
Yes, you can apply
I like the [hask](https://github.com/ekmett/hask/blob/cd4d30e7911dd7cc2da78383fd833272b1ff9303/src/Hask/Category.hs#L88) definition type Cat ob = ob -&gt; ob -&gt; Type class (Category (Dom f), Category (Cod f)) =&gt; Functor (f :: d -&gt; c) where type Dom f :: Cat d type Cod f :: Cat c fmap :: Dom f a b -&gt; Cod f (f a) (f b) As a [gist](https://gist.github.com/ekmett/b26363fc0f38777a637d), but the `fmap` method is wrong
I would like to replace the prelude `Functor` with it :] the source (`Dom f`) and target (`Cod f`) categories can be defaulted type family Hask :: Cat ob where Hask = ((-&gt;) :: Cat Type) class .. Functor (f :: d -&gt; c) where type Dom f :: Cat d type Dom f = Hask type Cod f :: Cat c type Cod f = Hask so instance declarations would stay the same instance Functor Maybe where fmap :: (a -&gt; b) -&gt; (Maybe a -&gt; Maybe b) -- -XInstanceSigs fmap f = \case Nothing -&gt; Nothing Just a -&gt; Just (f a)
Referring to the old `Functor` would be -- OldFunctor :: (Type -&gt; Type) -&gt; Constraint type OldFunctor f = FunctorOf (-&gt;) (-&gt;) f
Because the instance search algorithm can only determine that there _is_ an instance. It can't determine there isn't one. Due to the open world assumption, you can always define new instances which the search would then pick up. It would be pretty undesirable is adding new instances suddenly changed the behavior of already existing code.
Would you be able to plonky in the new `fmap` in your `class .. Functor`?
I don't remember exactly what it was that felt wrong. Certainly the choice of colors was very poor; the Ubuntu I had running in WSL had basically unreadable output from `ls`. I may have just assumed otherwise that the console was the same as it ever was. &gt; Does this sound to you like the old Microsoft we all know and love? I'll say the same thing I told some people after the Github acquisition: I don't trust Microsoft, but there aren't really any large companies I trust. It seems to be in Microsoft's business interests right now to make really good tech and be friendly to open source, which is about the best we can hope for. I _won't_, however, believe that they've somehow become true friends and advocates of FLOSS or anything of the like.
[An example](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf)
This can already happen with -XOverlappingInstances and its variants.
Hmm, you're right. However, I find this behavior to be pretty desirable so perhaps there's a more precise statement to be made. Overlapping instances allow you to override instances with more specific ones. 
It really tripped me up for awhile. Once I got the hang of it it was no big deal. In any function where which you anticipate using randomness, make sure you write it monadically with do notation. It's type would go from `a -&gt; b`, to `Monad m =&gt; a -&gt; m b`, but should do exactly the same thing. You can fix m as Identity and use runIdentity to achieve that. Then you add the `MonadRandom` library to your project, and now you'll go `runRand foo)`, and ensure that the type of your functions are now `MonadRandom m =&gt; a -&gt; m b`. It's as easy as that. Now you can use any of the functions in `Control.Monad.Random.Class`, such as `getRandomR`, `fromList`, etc. calcDamage :: PlayerWeaponDamage -&gt; MonsterArmour -&gt; MonsterHP -&gt; MonsterHP calcDamage wd arm mh = mh - (wd + variation - arm) where variation = 0 calcDamageM :: Monad m =&gt; PlayerWeaponDamage -&gt; MonsterArmour -&gt; MonsterHP -&gt; m MonsterHP calcDamageM wd arm mh = do let variation = 0 return $ mh - (wd + variation - arm) calcDamageR :: MonadRandom m =&gt; PlayerWeaponDamage -&gt; MonsterArmour -&gt; MonsterHP -&gt; m MonsterHP calcDamageR wd arm mh = do variation &lt;- getRandomR (-3,3) :: MonadRandom m =&gt; Int return $ mh - (wd + variation - arm) 
These "file system conventions" are hilarious!
There is also [`ProfunctorMonad`](https://hackage.haskell.org/package/profunctors-5.2.2/docs/Data-Profunctor-Monad.html#t:ProfunctorMonad) -- ProfunctorFunctor :: ((Type -&gt; Type -&gt; Type) -&gt; (Type -&gt; Type -&gt; Type)) -&gt; Constraint -- ProfunctorMonad :: ((Type -&gt; Type -&gt; Type) -&gt; (Type -&gt; Type -&gt; Type)) -&gt; Constraint class ProfunctorFunctor t =&gt; ProfunctorMonad t where proreturn :: Profunctor p =&gt; p :-&gt; t p projoin :: Profunctor p =&gt; t (t p) :-&gt; t p -- (:-&gt;) :: (Type -&gt; Type -&gt; Type) -&gt; (Type -&gt; Type -&gt; Type) -&gt; Type type pro :-&gt; pro' = forall xx yy. pro xx yy -&gt; pro' xx yy
Wow! This is actually the best example of randomness in Haskell that I’ve ever seen. I really loved how you progressively transformed it from a pure function to the end product. Thank you so much!
Wow! This is actually the best example of randomness in Haskell that I’ve ever seen. I really loved how you progressively transformed it from a pure function to the end product. Thank you so much!
I also heard this advice and couldn't find the source. My paraphrase is here: https://www.reddit.com/r/haskell/comments/5uijni/haskell_projects_for_beginners/dduz1yo/?context=1
I don't really understand the affine iterator section. If I understand linear types correctly then stream2 = mapIterator f stream1 means `stream1 is consumed exactly once if stream2 is consumed exactly once`. We can't give a strong guarantee that stream2 will be evaluated because of async exceptions. So important resources require something like ResourceT. So if we have an explicit map of linear resources anyway why can't we build affine streams on top of that? This would leave us with a method like `free :: Stream s .-&gt; IOL ()` - the implementation would be necessarily unsafe and nonlinear but user code could use it safely.
You never know how much that reading actually fuels your eventual understanding, though. I think reading about them is probably a good thing even if it feels like you aren't getting anywhere.
I don't really understand your comment tbh. if it's backtracking, it still can only determine whether there is an instance. Isn't backtracking independent from the open-world assumption?
The problem is that with such a `free` function, you can drop things which are not resources (in the `ResourceT` sense, if you will). Basically, you can turn `free` into oops :: a -&gt;. IOL () oops x = free (yield x) And this interferes negatively with all the pure interface that rely on linearity (like gelisam's example to use linearity to ensure, by typing, that a surface can be 3d printed). So, unfortunately, `free` would not be safe to use.
Hello! One of the authors here. The problem is: If a stream reference is an affine argument, how can the type checker remind the user to invoke `free`? If the user forgets to invoke `free`, the reference will have to wait for the garbage collector to release it or it will have to wait for the end of the current dynamic scope (ResourceT) to be reached, which in turn means that any Java references in it will have to wait the same. We've been arguing against waiting so long to release Java references in one of the referred earlier posts.
No because it doesn't know when to backtrack. 
When I was playing with linear streams I ended up with something like yield :: a -&gt; Stream a yieldL :: a -&gt;. (a -&gt;. IOL ()) -&gt; Stream a This still has problems because there is a tradeoff between promptness and convenient api. Also, annotating Stream with multiplicities via type families got really gross but multiplicity polymorphism hopefully will fix that. Also, for pure linear data I used `unsafeDrop :: a -&gt;. IO ()` which is horrible. A typeclass like `Droppable m a =&gt; a -&gt;. m` might be cleaner or at least hide the ugliness.
If you can backtrack if the instance fails to resolve, then now in theory I can make a new orphan instance in code you've never seen and change the semantics of code you've already written. This attacks separate compilation and the open world assumption that code can be actually compiled and stay compiled as you add more modules to the system.
My bad, by affine I meant something like data Affine a = Affine (forall r. Either (a -&gt;. r) r -&gt; r) So something that is always freeable but which still forces the user to invoke free.
Rust seems to handle this in an acceptable way: if two instances could overlap, then the compiler requires an instance that specifies behaviour when both conditions are met. It could still change behaviour when you add an instance, but it still feels fairly principled.
There is a subtlety. To define the category of functors, first you need to know that `Category` in Haskell is too weak, so you have to fix that up by giving better control over the objects. The same `hask` code linked above gives a more nuanced `Category`. It uses a constraint to model the source and target maps for the category, and letting the object -&gt; arrow identity map take a constraint to model the set of objects. class Category (k :: i -&gt; i -&gt; *) where type Ob k :: i -&gt; Constraint src :: k a b -&gt; Dict (Ob k a) tgt :: k a b -&gt; Dict (Ob k b) id :: Ob k a =&gt; k a a (.) :: k b c -&gt; k a b -&gt; k a c -- .. other stuff With that in the category [C,D] of functors from C -&gt; D, given a natural transformation from f to g, we can know f and g are functors from C to D, but we can also show that in this more proper encoding of natural transformations the arrows themselves have to carry the witness that the endpoints are functors from C -&gt; D to supply the src and tgt mappings. Then there is the issue of figuring out to embed `Compose` in multiple kinds, which you can attack through open data kinds today, rather than using the comparatively awful machinery I used in `hask` in ~7.10. https://github.com/ekmett/hask/blob/master/src/Hask/Tensor/Compose.hs#L16
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/hask/.../**Compose.hs#L16** (master → cd4d30e)](https://github.com/ekmett/hask/blob/cd4d30e7911dd7cc2da78383fd833272b1ff9303/src/Hask/Tensor/Compose.hs#L16) ---- 
That `Hask` defaulting trick is kinda gorgeous!
This works, of course. It isn't very convenient to use in practice, but up to syntactic inconvenience it's an encoding of affine types in linear types.
So if orphans were disallowed, would that change things?
Hello everyone! I noticed that people experience difficulties when they start working with Haskell packages. There was recently this rant on Reddit: * https://www.reddit.com/r/haskell/comments/8ll3sh/dear_haskell_its_not_you_its_your_tooling/ Also looks like people struggle with creating a project for Haskell: * https://www.reddit.com/r/haskell/comments/8ouarg/anyone_wanna_help_out_with_this_new_art_library/e0724u0 And they seem to be confused by overwhelming amount of configuration files for Haskell projects: * https://stackoverflow.com/questions/50889497/what-is-package-yaml-stack-yaml-setup-hs-the-project-name-cabal-files I always thought this happens because of lack of tutorials. So I decided to write one. When I was teaching Haskell to students I made some brief instructions on how to build Haskell projects. Now I decided to create more complete guide yet still only for basic workflow. The blog post covers workflows for both `cabal` and `stack`. But if you're interested in more details about `cabal` I highly recommend this episode of Haskell At Work: * https://haskell-at-work.com/episodes/2018-05-13-introduction-to-cabal.html This is my first blog post. So any feedback is appreciated! If you find any issues, you can even submit them directly on Github: * https://github.com/kowainik/kowainik.github.io/issues
Hello everyone! I noticed that people experience difficulties when they start working with Haskell packages. There was recently this rant on Reddit: * https://www.reddit.com/r/haskell/comments/8ll3sh/dear_haskell_its_not_you_its_your_tooling/ Also looks like people struggle with creating a project for Haskell: * https://www.reddit.com/r/haskell/comments/8ouarg/anyone_wanna_help_out_with_this_new_art_library/e0724u0 And they seem to be confused by overwhelming amount of configuration files for Haskell projects: * https://stackoverflow.com/questions/50889497/what-is-package-yaml-stack-yaml-setup-hs-the-project-name-cabal-files I always thought this happens because of lack of tutorials. So I decided to write one. When I was teaching Haskell to students I made some brief instructions on how to build Haskell projects. Now I decided to create more complete guide yet still only for basic workflow. The blog post covers workflows for both `cabal` and `stack`. But if you're interested in more details about `cabal` I highly recommend this episode of Haskell At Work: * https://haskell-at-work.com/episodes/2018-05-13-introduction-to-cabal.html This is my first blog post. So any feedback is appreciated! If you find any issues, you can even submit them directly on Github: * https://github.com/kowainik/kowainik.github.io/issues
liftA foldl seems close enough (actual type will have c-&gt;b rather than b-&gt;c)
This looks great. Thanks for stepping up and trying to fill the gap!
Done ! Thanks. 
Thank you! That makes sense -- my specific use case was that I want to make a random UTCTime that's after now, your response has convinced me that it'll be much better to separate out the IO and adjust the Gen with current time outside of the main specification.
Maybe. I'd still be worried you might have the ability to make a new class to hang an instance off or something creating new backtracking paths, but it may work out that that is perfectly sufficient.
I don't know if it officially qualifies as "backtracking" (I'm a bit hazy both on the precise meaning of that term, as well as on what exactly Rust does), but Rust does something like this under the banner of ["negative reasoning"](https://aturon.github.io/blog/2017/04/24/negative-chalk/), which does in fact follow as a consequence from the fact that it strictly forbids orphans.
(For anyone else reading: this is with regards to "specialization" (the Rust word for "overlapping instances", basically), which is *still* an unstable feature, and not the ["negative reasoning"](https://www.reddit.com/r/haskell/comments/8sqzop/why_are_we_not_backtracking_when_picking_instances/e1230j3/) it employs when deciding whether to accept polymorphic instances such as the OP's example.)
Depending on how evil you feel like getting, you could always just parse the argument as a string and then evaluate that as Haskell using template Haskell. Otherwise it'll just be creating a normal language and then writing a parser that takes their string and turns it into an ast. It'll be a simple language, but easily embeddable really depends on your experience more than anything. Haskell is one of the best languages to write this sort of thing in, but that doesn't mean it won't be tricky the first few times you figure it out :) Luckily for you, "let's parse arithmetic expressions!" Is literally every single how-to-dsl tutorial *ever*, so there's no shortage of copy and paste examples to learn from. You have a variable in your example, though, which is interesting. What would that be used for? I can see that complicating things
How Do I compile and run my code with -O2 with stack on windows?
`FunctorOf` is basically type FunctorOf f dom cod = (Functor f, Dom f ~ dom, Cod f ~ cod) but we cannot partially apply this definition. For that we need a [`class synonym`](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70#constraint-synonym-encoding-or-class-synonym) class (Functor f, Dom f ~ dom, Cod f ~ cod) =&gt; FunctorOf dom cod f | f -&gt; dom cod instance (Functor f, Dom f ~ dom, Cod f ~ cod) =&gt; FunctorOf dom cod f
If you fully apply `User`, then you can use `type`. `type User a = UserT' a Identity`. If you're asking for partial application at the type level, then perhaps this will one day be possible with Dependent Haskell, but it is not right now. 
If you fully apply `User`, then you can use type: `type User a = UserT' a Identity`. If you're asking about flipping the order of arguments of a type without wrapping, that's not possible due to Haskell's nominal typing semantics. Perhaps it will be possible with dependent Haskell, but I'm not sure. 
Thanks for this. This is Haskell's biggest area to grow in; not only the maturity of its tooling but perhaps more importantly the knowledge of the dev community about said tooling.
This seems a good place to mention [this small tutorial](https://github.com/danidiaz/really-small-backpack-example) I wrote on how to use Backpack (the [first chapter](https://github.com/danidiaz/really-small-backpack-example/tree/master/lesson0-convenience-libraries) also touches on internal convenience libraries in Cabal, which are generally useful).
That's not really a great argument. Since even without orphans, OverlappingInstances is straight up incoherent and requires just as much care as unsafePerformIO. See [here](https://www.reddit.com/r/haskell/comments/7nre75/fixing_overlapping_instances/?st=JIOZVHPX&amp;sh=133f1b8d) for an explanation.
You should wear something like [this ](https://pbs.twimg.com/media/DgPnyy9U8AAfJ7f.jpg)! I'll wear a matching one at the next Lambdaconf. Deal?
Giving an opinion about when to use stack or cabal would be nice (assuming that you write for beginners, they don't know). It would also be nice if you'd explain why nix is an popular "alternative" even though its (much) harder to use.
Why certain libraries doesn't have documentations in this link? [http://hackage.haskell.org/package/base](http://hackage.haskell.org/package/base) Example: GHC.Base where all the fundamental type classes(Monad, etc) are defined
Small question. If I have a small experimental library without any guarantees on API or overall stability; but if it compiles with the Stackage snapshots, should I submit it to Stackage anyway? Or is it only for "stable" packages?
The FFI doesn't support passing structs by value. In general you will have to allocate on the heap, and pass around the corresponding pointers.
Have you read the [maintainers agreement](https://github.com/commercialhaskell/stackage/blob/master/MAINTAINERS.md)? In particular, you should roughly follow the PVP so any users that update within the same LTS-series don't experience any breakage.
tesla save the planet for great good :D
At the end of the blog post I gave recommendation to try both and choose which one you like more. It's hard to give recommendation without being opinionated. I use both tools. And I love both. But also both of them have some drawbacks in comparison to each other. So it's hard to say that one is strictly better than the other. I wanted to write side-by-side comparison. But didn't figure out how to do this in neutral way. So maybe next time :) &gt; It would also be nice if you'd explain why nix is an popular "alternative" I never used `nix` for building Haskell projects so it's hard for me to describe in simple words why you might want to prefer `nix`. Probably someone with more expertise can write beginner-friendly blog post about nix workflow for Haskell. But for now I think the following repository is pretty good in explaining `nix` for Haskell (and that's why I provided link to it in the blog post itself): * https://github.com/Gabriel439/haskell-nix
Sounds like a plan.
i’ll check it out. Thanks for contributing to the Haskell community!
A lot of people have given this advice, and I'm going to echo it: build something real. After a few tutorials and some exercises on codewars.com and exercism.io, I took a contract gig to write a reddit bot in whatever language I wanted as long as they had a full-timer who could maintain it if necessary. One of the guys has the Haskell logo for his Slack avatar, so I jumped on the opportunity. (Incidentally, I am now also full time with the same company, and we're in the process of moving most of our backend to Haskell.) Getting this done involved the [`reddit`](https://hackage.haskell.org/package/reddit) library, [`postgresql-simple`](https://hackage.haskell.org/package/postgresql-simple) for some basic persistence, and [`http-conduit`](https://hackage.haskell.org/package/http-conduit) to make outgoing requests (somewhat similar to /u/wikibot). There followed about three weeks of staring alternately at code examples and type errors, which was admittedly daunting (I not have made it without the ability to verify incremental progress in `GHCi`), but I came out the other end knowing that I definitely could build things in Haskell, and with strong intuitions for: - monad transformers - using `&lt;$&gt;` and `=&lt;&lt;` to enhance or replace `do` notation - the relation between `.`, `$`, point-free style, and refactoring - using `undefined`, [typed holes](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#typed-holes), and [partial type signatures](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#partial-type-signatures) to build an application from the top down, rather than starting out in the weeds Three years on, the type system is still strange and vast, but it turns out you don't need most of it just to get things working. I'm picking it up bit by bit; last week I wrote my first existential GADT... but then promptly found a simpler solution! (Related: contravariant functors are _the best._) I had no problem turning out real applications before I even figured out `mtl`; it's great, but `transformers` will get you to production.
so.. why the reference to /r/factorio?
You don't need the &amp; in the examples.
I think you will have to have explicit streaming constructs for handling files larger than RAM. In the case of the lens-aeson, let's say if you have a large top-level object, even if the thing is smart enough not to force values in the hashmap, it will have to validate the json format, thus reading it till the end, meaning it will all be in memory at once.
How was that Mag functor instance not law abiding?
The functor law states that `fmap id = id`. This does not hold for the `Mag` instance: fmap id a -- definition of fmap = Mag id a /= a = id a
But they are isomorphic, are they not? I think that's _good enough_ even if they aren't exactly equal.
That depends on your definition of isomorphic. If the only way to take apart a `Mag` (some kind of `run` function) doesn't distinguish between `Mag id a` and `a`, then the instance is morally law-abiding; there are many such instances in various packages. That is in fact why the author of the linked post isn't in favor of adding the `Representational1` constraint to `Functor`.
Yeah, that's pretty much what I meant.
[Data.Graph](http://hackage.haskell.org/package/containers-0.6.0.1/docs/Data-Graph.html) from containers offers functions for calculating SCCs.
I don't think they are. You can't write the other half; you can't write a function that *undoes* `fmap id`, because it can't detect that the function in _ is `id` in order to reduce back to `One`
Would this be a slightly less hand wavy proof that all law abiding Functors are Representational1? -- Given: coerce . coerce = id fmap coerce . fmap coerce = fmap (coerce . coerce) = fmap id = id I think it's a *little* less hand wavy, as it's more obvious that `coerce . coerce = id` I'd really like to see this as a super class of Functor, as it would give us the ability to improve a lot of Functor-abstract code. Functors that break this law are few and far between (and pretty much always have bad performance), and this would serve as a neat little proof that the Functor laws hold.
we really should not be pointing beginners to global IORefs. The correct answer ere is using an `IORef` that is passed around manually, *or* using a `ReaderT` to provide that ambient local context.
That doesn't really have anything to do with the JSON format. It's entirely possible to lazily generate and consume a representation of anything that can be logically subdivided, most certainly including JSON. It's just not possible to do so in Aeson because they've chosen a strict intermediary representation. I don't think there even is a way to avoid memory exaustion if the Aeson representation of the JSON you're parsing is big enough to exhaust memory - Wouldn't the strictness annotations force the evaluation of the whole object, and therefor prevent you from doing clever MMAP-y things with it?
&gt; It's entirely possible to lazily generate and consume a representation of anything that can be logically subdivided, most certainly including JSON. Only if you accept asynchronous exceptions! How would you handle parse/deserialization errors otherwise? 
Related : [this](http://hackage.haskell.org/package/hw-json). It should be very efficient for moving through a JSON structure, and would probably work well with mmaped stuff (never tested it though).
He told me that he doesn't understand about ReaderT pattern. I guess it's something like a shortcut for him to fix his program. 
Additionally, we could have some convenient rewrite rules for \`Functor\` if we required following the laws. With \`ApplicativeDo\`, we kind of already push in the direction of "functor/applicative/monad instances must be law-abiding". I'm against adding \`Representational1\` as a superclass of \`Functor\` because it adds complexity for the sole purpose of solving a problem that already has a solution (the solution being \`-XDeriveTraversable\`). But, I like the idea of having an \`fmapCoerce\` that's usable with any \`Functor\`. I'm not sure if there's a good way to do this without the superclass.
Generally speaking, most people will advise you not to use record syntax like this. It's preferred to only use it when the type has one constructor: data Pair a b = Pair { fst :: a, snd :: b } Or, as you say, when the fields are mimicked across cases: data Tree a = Leaf { val :: a } | Bin { val :: a, _l :: Tree, _r :: Tree a } As it happens, GHC *can* warn you about it, with [-Wpartial-fields](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/using-warnings.html#ghc-flag--Wpartial-fields). I don't know why this isn't included in -Wall.
&gt; I'm against adding `Representational1` as a superclass of `Functor` because it adds complexity for the sole purpose of solving a problem that already has a solution (the solution being `-XDeriveTraversable`). I think it's solving a great deal more than that. Coercing Functors has a pretty broad range of applications beyond deriving Traversable. The added complexity is definitely unfortunate. It might be one thing too many to have to explain to newcomers just getting their feet wet with Functor/Applicative/Monad... :/
What? I don't need to do that when lazily picking a single line out of a large file. This is the same idea, it's just using a more complicated concept of value separation than subdividing on `'\n'`. You'd need a lazy parser, and a lazy intermediary representation, with a strict lens. What you do for exception handling is... You don't model parsing errors as exceptions, because why the hell would you do that in Haskell? 
Here's a concrete example: compare the before and after in this diff, which really should just read something like `cont (Map $ M.map coerce m)`. The "after" version is all of the hoop-jumping I had to do to get that effect, all due to the possibility that `f`'s type parameter was nominal. https://github.com/matt-noonan/justified-containers/commit/bd8397f29229eadad63c9ce4a99bf1b0809c1afc#diff-2ecda9c11ab2994deb5ee5e26ef1b387L306
I really should do more research on why it is not included in `-Wall`, as I think the problem it solves is very critical. Do you know how to ask a question to GHC developers? Thank you very much!
If you model parsing errors as a sum type, you need to parse the whole thing to know if it succeeded. You don't do that with lines because there are only lines in a file. 
That's sounds like a good bug report to make: https://ghc.haskell.org/trac/ghc/wiki/ReportABug
There's a pragma to make haddock skip it: `{-# OPTIONS_HADDOCK hide #-}` It still generates links to the source https://hackage.haskell.org/package/base-4.11.1.0/docs/src/GHC.Base.html
Is this less handwavy than requiring `coerce = id`?
The hand waviness I was resolving was the hand wavy Functor law `fmap coerce = coerce`, so I just wanted to reduce the LHS to `fmap id` in order to use the actual Functor law verbatim. I think your suggestion is fundamentally the same.
Obviously using them as functions is not a great idea. However they can be used safely via pattern matches: fromMyMaybe :: a -&gt; MyMaybe a -&gt; a fromMyMaybe x None = x fromMyMaybe _ Some{ value = x } = x Doesn't do much for a small type like this, of course. But for larger constructors with many fields this can be a big help for readability. And then going the other way, constructing a value using the record syntax, is also safe. This pairs well with the somewhat-controversial `RecordWildCards`. Here is a project where I used this extensively: * [big state-machine-like type](https://github.com/mtolly/glory/blob/93d2181d5cec2449d372fa8d325c88dae1b3c24c/src/Core.hs#L12) * [big update function pattern matching on it](https://github.com/mtolly/glory/blob/93d2181d5cec2449d372fa8d325c88dae1b3c24c/src/Update.hs#L71) Notice how you can write things like `case x of Con1{..} -&gt; Con2{ foo = 5, .. }` which means "fill in the rest of Con2's fields with whatever it shares with Con1", no partial functions involved. (This will correctly warn with `-Wall` if there's a missing field.) I found this really nice for writing state machine logic with minimal ceremony. Of course `RecordWildCards` gets fair criticism for its confusing shadowing behavior so YMMV.
Here is a longer answer now that I have an actual keyboard. Let's say you want to do that: {"a":5,"b":6],"c":7} &amp; key "c" .~ 9 How do you parse the json ? I think there are only three ways: * `parse :: [(k,v)]`, using error * `parse :: Either Error [(k,v)]`, but then you have to reach the end of the file and parse it all * using explicit streams, but then you will have to convert your lens syntax into something that is more like a parser, and wonder if you would like to support modifying `"b"` in the previous example
I think one of the mailing lists is possibly a good option? There are actually several things which aren't in -Wall that don't make a lot of sense to me. [This](https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/) blog post mentions a few of them.
&gt;Of course `RecordWildCards` gets fair criticism for its confusing shadowing behavior so YMMV. I see this criticism all the time, but I've never found the issues to be that large. If you're using `RecordWildCards` and design your types for them, it really cleans things up.
Using record accessors directly with sum types is dangerous as you said. If you want totality, you can do something like this data MySum = One One_ | Two Two_ data One_ = One_ { oneX :: Int } data Two_ Two {twoX :: Int, twoY :: String} That said, the partial way is nice if you never use them, but instead use the names with `RecordWildCards` or `NamedFieldPuns`
Never knew about `-Wpartial-fields`. Thanks!
I totally agree that parsing arithmetic expressions is really not hard. I know because that's what I'm currently doing (using megaparsec's makExprParser). Though, I couldn't shake the feeling that I was re-inventing the wheel. The software provides some values that the user can use in her expressions. 
This is almost always a bad idea. But if you have to do it it's REALLY IMPORTANT to add a noinline pragma : {-# NOINLINE onceUniqueRef #-} onceUniqueRef = unsafePerformIO $ newIORef Nothing
Why wouldn’t State have been sufficient?
Wouldn’t you want StateT for the mutable aspect of a global state?
Packages need not be stable in order to be included in Stackage, as long as they continue to build and (ideally) pass their test suites.
The objection isn't just that names are shadowed but that you can't tell locally *which* names. With NamedFieldPuns you can still see what names are bound just looking at the pattern. With RecordWildCards you have to look at the data constructor definition too, and changes to the constructor can change the meaning of code using wildcards. (I actually agree that RWC is pretty tame, but it does have some complications that aren't present with just field puns.)
&gt; using template Haskell I don't see how that could work; Template Haskell runs at compile time, but command-line arguments are available at runtime. How about using [`hint`](http://hackage.haskell.org/package/hint) to interpret a pure Haskell expression?
&gt; Also, do people feel the same way about NamedFieldPuns? They also allow the same shadowing, correct? I hate RecordWildCards but love NamedFieldPuns. Explicit is usually better than implicit.
&gt; [global-variables](http://hackage.haskell.org/package/global-variables): Namespaced, global, and top-level mutable variables without unsafePerformIO. They [use `unsafeDupablePerformIO`](http://hackage.haskell.org/package/global-variables-1.0.1.1/docs/src/Data-Global-Registry.html#declareIORef) instead :)
I can see a few more ways to model this problem, but I think you're fundamentally right in that they really make the behavior of the lens quite a bit more tricky - I don't see a way to do that in constant space without re-inventing something that's very similar to but not quite a lens. And also I think your original premise is correct, in that explicit streaming constructs are the way to go. But ultimately none of this will work without another way to represent the underlying JSON - You just can't build Aeson's representation atomically per node, and you'd need to be able to do that in some fashion to support this for unbounded input. 
Looks like `safe-globals` [takes care of that](http://hackage.haskell.org/package/safe-globals-0.1.1/docs/src/Data-Global.html#line-120).
`ReaderT (IORef Int)` is what I meant. `StateT` is useful for thread-local pure state, but if you're in IO, you probably want to use a reference with the concurrency properties you care about.
It was basically because I was trying to think of some sort of name for a tool for "refactoring" (though I realize that name is not quite right) arbitrary files. I had recently purchased the game so it sprang to mind and the way you compose powerful automation from small pieces reminded me of composing a big lens to do something powerful from a standardized set of smaller pieces.
Derp. I got it in my mind that I needed special syntax to allow people to involve the extra operators (eg. `*~`) but obviously that's not necessary at all. I'll remove them when I merge down the `automate-examples` branch.
Thanks for calling attention to this. I was not aware of it and it definitely looks interesting for this scenario.
I see a Simon, I upvote! Seriously though, thanks for the continued improvements to GHC.
I kinda like the rep1 type class idea. But I then wonder if it’s useful to not just talk about representational isomorphisms/ equivalences, but computational ones as well? Granted computationally relevant vs irrelevant (aka erasable ) equivalence proofs are tricky 
This has caused me trouble too. [`Distributive`](https://hackage.haskell.org/package/distributive-0.5.3/docs/Data-Distributive.html) is another example that can hopefully be derived if `Functor` has a `Representable1` superclass (with no modification to the class itself) -- Distributive :: (Type -&gt; Type) -&gt; Constraint class Functor d =&gt; Distributive d where distribute :: Functor f =&gt; f (d a) -&gt; d (f a) .. Just like `traverse` can be replaced by a 'GND-friendly' method: (= `mapTraverse id`) mapTraverse :: Applicative f =&gt; (t b -&gt; r) -&gt; (a -&gt; f b) -&gt; (t a -&gt; f r) so could we derive `Distributive` if `distribute` were replace by (= `collect id`) collect :: Functor f =&gt; (a -&gt; d b) -&gt; (f a -&gt; d (f b))
Yeah, I've found it to be pretty useful personally and haven't had many issues. The obvious pitfall is if a field isn't extracted when you expect it to be, or vice versa, then you'll get an error along the lines of `Couldn't match 'Rec -&gt; Field' with 'Field'` where it'd be nice GHC could identify that RWC might be the culprit and include that in the message. But I tend to keep the scope of the shadowing small and obvious so there's less potential for errors of that form.
flexible attitude toward workers' rights a big plus
[@taylorfausak's latest tweet](https://i.imgur.com/GJ6cF1Z.jpg) [@taylorfausak on Twitter](https://twitter.com/taylorfausak) - ^I ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
So, `-XNoSelectorFunctions`, anyone?
What about using something like flatpak? It would work ok on multiple distros. Recently I wanted to check how does it work with Haskell.
Seems weird that there is not even a small difference in Runtime. I've wondered about all these zero words in Info tables just a few days before your patch hit head to remove them.
Thanks for letting me know my plans for tomorrow.
I agree that they are useful. I've needed this before as well. And I don't mind prohibiting instances that only law abiding "up to observation". I find it unfortunate that we would go from: class Functor f where ... To class (forall a b. Coercible a b =&gt; Coercible (f a) (f b)) =&gt; Functor f where ... It's not the use of `QuantifiedContexts` that bothers me (I'm really looking forward to this extension). It's that `Coercible` is a magic typeclass. `Functor` has roots in category theory. `Coercible` does not. For some reason, even though this is basically equivalent, the following formulation seems more pleasing to me: data Coercion1 f where Coercion1 :: (forall a b. Coercible a b =&gt; Coercible (f a) (f b)) =&gt; Coercion1 f class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b (&lt;$) :: a -&gt; f b -&gt; f a fcoercion :: Coercion1 f This has an extra indirection, and it doesn't work quite as nicely (you cannot just use `coerce` directly), but it avoids adding a very GHC-specific superclass to `Functor`.
It's a binding form that doesn't tell you which variables it binded.
Thats a great beginner example to template Haskell.
I see the superclass just as a proof of the laws, not as some quirky thing.
This is not true. Recursive functions do not typically get evaluated at compile time, even if they are fully applied to statically known arguments.
Thank you, Simon!
Pet peeve about: What are the error bars on those numbers? Without those it's basically impossible to know whether this is actually an improvement.
Cool library, but why use an entire library in the place of a single NOINLINE pragma?
The library prevents you from declaring a polymorphic global, which could be used to break type safety like so: global :: IORef [a] global = unsafePerformIO (newIORef []) {-# NOINLINE global #-} main = do writeIORef global [(+ 3)] x &lt;- readIORef global print (x + 7) -- boom The example is a bit contrived, but it still demonstrates the issue.
Thanks Simon, &gt; The SRT field for each code block can be 32 bits, not 96 Could we not do this on the old representation as well? 32 bits for the SRT pointer (instead of 64 bits), then a bitmap, so 64 bits instead of 96. (still not as good as the new representation but still an improvement that applies to previous representation as well) &gt; We can drop duplicate references from an SRT If I get this right, I think this problem doesn't exist in the previous representation where a single large SRT contained _all_ static references in a module. So this seems to me like fixing a problem that new representation has. &gt; We never need a singleton SRT This also seems to me like something we could do on the previous representation. Overall as someone who works on the RTS these days I really like the simplifications on code this patch brings, so thanks for this!
We do not have a good alternative to record syntax yet. For large data types it is not feasible yet to refer to fields by constructor pattern matching alone. Also, `-Wpartial-fields` should make it obvious when partial record fields are declared.
Hey, it's a blog post, not a paper! The full nofib results (with standard deviations) are here: [https://phabricator.haskell.org/P176](https://phabricator.haskell.org/P176) Don't pay any attention to the runtime results though, it was done on my laptop with a variable CPU speed. Basically the only way this could affect runtime is by * instruction cache effects, and those should be in our favour since we made the code smaller, and * GC time improvements. I measured what should be the worst case for this - doing many old-gen collections in GHC itself - and the differences were within the variability of the benchmark (which was quite wide) So I'm satisfied that this doesn't make runtime worse in general, and likely makes it a bit better. Of course if I was writing this up for a paper I'd do more rigorous experiments, but I doubt it's worth it.
Yes we could have saved 32 bits with the old representation, but unless you save a full 64 bits in the info table you don't get any savings (info tables need to be an integral number of words). &gt;If I get this right, I think this problem doesn't exist in the previous representation where a single large SRT contained *all* static references in a module. So this seems to me like fixing a problem that new representation has. Right, the point is that the new representation plus a handful of sensible optimisations gives better results than the old representation plus a handful of different optimisations. Some of the new optimisations came for free with the old representation (and the reverse is also true, in fact). &gt;This also seems to me like something we could do on the previous representation. Not without complicating the representation, because you would need to distinguish between a pointer to the SRT table and a pointer to a closure.
I think this is a great advice. Sometimes it is a good idea to write C-like IO-based code using the knowledge you already have, then you can find how could it be improved using pure functions and more advanced types. That's a wonderful experience, reasoning each step and giving more value to beautiful pure code, because it helps in various ways.
Unfortunately, I can't see those without a login. Thanks, though, at least others can look at them. As I say, it really is a pet peeve of mine that people throw around numbers like these without at least some way to tell how likely they are to be actually correct. Another is using standard deviation rather than e.g. 95% confiedence intervals for communicating what the likely error is -- it's much easier to make intuitive sense of a 95% conf. interval rather than a standard deviation. Of course, not everybody has time to do a rigorous statistical analysis, but one assumes that this is something nofib should do by default.
I wish `distribute` would just be defined that way.
What I meant is, can I flip the order of the parameters on the `UserT'` type and still use it with `beam`, so that I can have `type User a = UserT' Identity a` instead of `newtype`. The main thing I'm trying to achieve is to minimize the impact of `beam` on the `User` type, the one that the rest of the codebase sees. The rationale is the fact that data persistence is just one of your concerns, and I'm trying to isolate it from the other concerns. In summary, instead of wrapping `User` with `newtype`, I'd prefer to wrap `UserT`.
For what it's worth, we could give `Mag` representational type parameters efficiently if only we had a way to unpack representational equality the way we can unpack nominal equality. It still wouldn't be a lawful `Functor`, but it would be able to get a `Representational1` instance.
I have two questions : How do I tell Haskell that the `f` in the following definition should only be used with `Functor`s : `data Free f a = Free (f (Free f a)) | Pure a` (shamelessly copied from [http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html](here)) And suppose that I want to write a `Show` instance for this. I will assume that `a` is `Show`able and that `f a` is `Show`able if `a` is. But something like `instance (Show a) =&gt; (Show (f a)) =&gt; Show (Free f a) where ..` doesn't work right? How do I express that then?
I think it's probably because it is hard to happen by accident. I mean, sure, you will have to know it to avoid it, but you don't get warnings for using \`==\` on floating-point numbers too, right? On the other hand, you're probably right that we should have deprecated partial fields.
..lenses?
Is there really enough in there for it to be a proof of the laws in general? I confess I haven't thought very hard about this before posting, but wouldn't a `Representational1` superclass merely prove that the instances preserve (a particular family of) isomorphisms?
Yes I also don't want that choice to be so consequential
To expand on this: consider the difference in behaviour if, during a long process, your function throws an exception.
Yes I am a bit confused how to proceed based on the existing Haskell Lattice classes. My older ideas were based on implementation of boolean dependency matrix which gives the dependencies but does not really allow Least Upper Bounds or Greatest Lower Bounds to be found easily and efficiently. The following papers give good solutions to calculating implementation a data structures that allow easy calculation of LUG and GLB and also Subsumption by And's, Or's, and a subset NotAnd combination respectively. These can be calculated from a boolean dependency matrix or directly from subtype graph data structures. Incremental Encoding of Multiple Inheritance Hierarchies Supporting Lattice Operations [http://www.ep.liu.se/ea/cis/2000/001/cis00001-body.ps](http://www.ep.liu.se/ea/cis/2000/001/cis00001-body.ps) Efficient implementation of lattice operations [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.4911&amp;rank=1](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.4911&amp;rank=1) Efficient Management of Transitive Realationships in Large Data and Knowledge Bases [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.454.3687&amp;rep=rep1&amp;type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.454.3687&amp;rep=rep1&amp;type=pdf)
Making an account is free. Probably the least you could do, compared to the amount of work you are requesting, much of which is already done.
I had a weird interaction between lens naming schemes and -fdefer-typed-holes RecordWildCards before: data MyStruct = MyStruct { _myStructField ::Int } foo (MyStruct{..}) = _myStrucField The mistyped identifier is now interpreted as a typed hole and only generates a warning. 
I don't want yet another account. It has nothing to do with how easy/hard they are to make.
any good example project of how 'three layer cake' applied?
I've recently started using haskell and I found it amazing, I solved 142 project euler problems and some problema here and there, this makes me interested in learning about compilers about which I have no knowledge apart from surface working.
gdb /bin/sh run /usr/bin/ghci 
That example did not typecheck in my head, so I tested it in GHCi and it didn't typecheck there either. GHC catches that (x + 7) doesn't make sense as there's no instance for Num [a]. However the type inference with global IORefs is weird, if I try to add a typed hole (just to see what GHC thinks the type signature of x is), it says it's [a], even though we just wrote to it a function of type (Num a =&gt; a -&gt; a). 
&gt; Unfortunately, I can't see those without a login Hmm, this is weird. I wonder if this is intentional. I think phabricator.haskell.org should be open to everyone to see.
Oops, I missed that; I was tired. I edited in a fix.
This is the relevant snippet of nofib from the link /u/simonmar gave (as of June 23, 2018). The percentages are all standard deviations, the decimals are very small standard deviations, and `bspt` is the odd one out, no idea why. -------------------------------------------------------------------------------- Program Size Allocs Runtime Elapsed TotalMem -------------------------------------------------------------------------------- CS -1.5% 0.0% 0.182 0.182 0.0% CSD -1.5% 0.0% +24.7% +25.5% 0.0% FS -1.5% 0.0% +8.5% +8.6% 0.0% S -1.5% 0.0% +15.4% +15.5% 0.0% VS -1.5% 0.0% +8.7% +8.8% 0.0% VSD -1.5% 0.0% 0.010 0.010 0.0% VSM -1.5% 0.0% +18.8% +18.9% 0.0% anna -3.2% 0.0% 0.072 0.072 0.0% ansi -1.6% 0.0% 0.000 0.000 0.0% atom -1.6% 0.0% -6.2% -6.1% 0.0% awards -1.6% 0.0% 0.000 0.000 0.0% banner -1.4% 0.0% 0.000 0.000 0.0% bernouilli -1.7% 0.0% 0.111 0.111 0.0% binary-trees -1.8% 0.0% +10.0% +10.6% 0.0% boyer -1.5% 0.0% 0.029 0.029 0.0% boyer2 -1.6% 0.0% 0.005 0.005 0.0% bspt 949483 11024288 0.007 0.007 0 cacheprof -2.3% -0.0% +1.7% +1.6% 0.0% calendar -1.5% 0.0% 0.001 0.001 0.0% cichelli -1.6% 0.0% 0.054 0.054 0.0% circsim -1.7% 0.0% +16.8% +16.8% 0.0% clausify -1.5% 0.0% 0.024 0.024 0.0% comp_lab_zift -2.0% 0.0% 0.111 0.111 0.0% compress ----- ----- ----- ----- ----- compress2 -1.9% 0.0% 0.129 0.129 0.0% constraints -1.5% 0.0% +65.2% +67.0% 0.0% cryptarithm1 -1.5% 0.0% +24.5% +25.3% 0.0% cryptarithm2 -1.4% 0.0% 0.004 0.004 0.0% cse -1.5% 0.0% 0.001 0.001 0.0% digits-of-e1 -1.6% 0.0% -9.0% -9.1% 0.0% digits-of-e2 -1.6% 0.0% +1.4% +1.4% 0.0% eliza -1.5% 0.0% 0.001 0.001 0.0% event -1.5% 0.0% 0.127 0.127 0.0% exact-reals -3.2% 0.0% -2.0% -2.0% 0.0% exp3_8 -1.5% 0.0% -21.1% -21.7% 0.0% expert -1.7% 0.0% 0.000 0.000 0.0% fannkuch-redux -1.5% 0.0% +11.1% +11.2% 0.0% fasta -1.6% 0.0% -8.2% -8.5% 0.0% fem -2.2% 0.0% 0.014 0.014 0.0% fft -1.6% 0.0% 0.040 0.040 0.0% fft2 -1.6% 0.0% 0.042 0.042 0.0% fibheaps -1.5% 0.0% 0.018 0.018 0.0% fish -1.5% 0.0% 0.005 0.005 0.0% fluid -2.2% 0.0% 0.005 0.005 0.0% fulsom -1.7% 0.0% 0.200 0.200 0.0% gamteb -1.7% 0.0% 0.027 0.027 0.0% gcd -1.6% 0.0% 0.025 0.025 0.0% gen_regexps -1.5% 0.0% 0.000 0.000 0.0% genfft -1.6% 0.0% 0.037 0.037 0.0% gg -2.0% 0.0% 0.007 0.007 0.0% grep -1.6% 0.0% 0.000 0.000 0.0% hidden -1.8% 0.0% -9.2% -9.7% 0.0% hpg -2.0% 0.0% 0.153 0.154 0.0% ida -1.6% 0.0% 0.059 0.059 0.0% infer -1.7% 0.0% 0.035 0.035 0.0% integer -1.5% 0.0% -17.4% -17.4% 0.0% integrate -1.5% 0.0% 0.083 0.083 0.0% k-nucleotide -1.6% +0.0% -8.6% -8.7% 0.0% kahan -1.6% 0.0% +24.4% +24.5% 0.0% knights -1.6% 0.0% 0.002 0.002 0.0% lambda -1.6% 0.0% -42.3% -42.8% 0.0% last-piece -1.6% 0.0% -25.0% -25.5% 0.0% lcss -1.5% 0.0% -39.3% -39.8% 0.0% life -1.6% 0.0% 0.136 0.136 0.0% lift -1.7% 0.0% 0.001 0.001 0.0% linear -1.6% 0.0% -7.5% -7.6% 0.0% listcompr -1.5% 0.0% 0.060 0.060 0.0% listcopy -1.5% 0.0% 0.061 0.061 0.0% maillist -1.6% -0.0% 0.048 0.049 -40.4% mandel -1.6% 0.0% 0.034 0.034 0.0% mandel2 -1.5% 0.0% 0.003 0.003 0.0% mate -1.7% 0.0% +6.9% +6.9% 0.0% minimax -1.6% 0.0% 0.002 0.002 0.0% mkhprog -1.8% 0.0% 0.001 0.001 0.0% multiplier -1.8% 0.0% 0.068 0.068 0.0% n-body -1.6% 0.0% -8.5% -8.6% 0.0% nucleic2 -1.3% 0.0% 0.048 0.048 0.0% para -2.3% 0.0% 0.197 0.198 0.0% paraffins -1.5% 0.0% 0.081 0.081 0.0% parser -2.2% 0.0% 0.026 0.027 0.0% parstof -2.2% 0.0% 0.003 0.003 0.0% pic -1.7% 0.0% 0.004 0.004 0.0% pidigits -1.6% 0.0% +6.6% +6.3% 0.0% power -1.8% 0.0% -8.3% -8.3% 0.0% pretty -1.6% 0.0% 0.000 0.000 0.0% primes -1.5% 0.0% 0.054 0.054 0.0% primetest -1.6% 0.0% 0.072 0.073 0.0% prolog -1.5% 0.0% 0.001 0.001 0.0% puzzle -1.6% 0.0% 0.084 0.084 0.0% queens -1.5% 0.0% 0.010 0.010 0.0% reptile -2.2% 0.0% 0.006 0.006 0.0% reverse-complem -1.5% 0.0% 0.085 0.085 0.0% rewrite -1.8% 0.0% 0.010 0.010 0.0% rfib -1.5% 0.0% 0.014 0.014 0.0% rsa -1.6% 0.0% 0.019 0.020 0.0% scc -1.5% 0.0% 0.000 0.000 0.0% sched -1.5% 0.0% 0.014 0.014 0.0% scs -2.2% 0.0% +24.7% +24.9% 0.0% simple -2.8% 0.0% -36.6% -37.2% 0.0% solid -1.5% 0.0% 0.107 0.107 0.0% sorting -1.5% 0.0% 0.001 0.001 0.0% spectral-norm -1.6% 0.0% +4.5% +4.6% 0.0% sphere -1.5% 0.0% 0.026 0.026 0.0% symalg -2.0% 0.0% 0.008 0.008 0.0% tak -1.5% 0.0% 0.007 0.007 0.0% transform -2.1% 0.0% +4.8% +4.7% 0.0% treejoin -1.6% 0.0% 0.117 0.118 0.0% typecheck -1.5% 0.0% 0.200 0.201 0.0% veritas -3.2% 0.0% 0.001 0.001 0.0% wang -1.6% 0.0% 0.060 0.060 0.0% wave4main -1.6% 0.0% 0.134 0.134 0.0% wheel-sieve1 -1.6% 0.0% +12.1% +12.3% 0.0% wheel-sieve2 -1.5% 0.0% 0.147 0.147 0.0% x2n1 -1.6% 0.0% 0.002 0.002 0.0% -------------------------------------------------------------------------------- Min -3.2% -0.0% -42.3% -42.8% -40.4% Max -1.3% +0.0% +65.2% +67.0% 0.0% Geometric Mean -1.7% -0.0% -1.0% -1.1% -0.5% 
&gt; I don't see how that could work; Template Haskell runs at compile time, but command-line arguments are available at runtime. D'oh, that was a brain fart on my part. I was just thinking "metaprogramming" and TH fits the bill for that :) Hint looks pretty badass, though; not sure how heavyweight it is, but it definitely looks cool.
&gt; The software provides some values that the user can use in her expressions. That's not too bad then, and it can be represented in an AST fairly simply. I'd say that makeExprParser was built for basically this exact sort of thing, so if you're using it, you wouldn't be reinventing the wheel at all.
So -Wpartial-fields warns whenever you give a record field in a sum type that does not have that field in all constructors a name? Wouldn't it make more sense to warn when they are actually used with record syntax to update or reference the record? Names can also be useful in e.g. Show output.
oh yeah, nice. thanks the example. mad scary
It is open, but some parts are gated.
It's an internship, so that's pretty much given
Yes, `-Wpartial-fields` warns when you define* a partial field. But only if their name does not start with an underscore. That should cover the cases where partial record fields are useful for documentation and serialization. *) I guess the warning is generated at definition sites to reduce noise. Users of partial functions cannot do much to fix the problems associated with them after all. because the API. It is the task of the module author to expose a safe API.
 sounds like a money maker
This is an ad. There's no reason for this to be on /r/Haskell
# 1. You don't write that in the data type, you write it on all the `Free`-related functions that need `Functor f`. # 2. Use `(Show a, Show (f (Free f a)))`.
I imagine their tool is written in Haskell since it’s FP Complete, and the page mentions Cardano SL, also written in Haskell. But yeah, not necessarily relevant for r/Haskell 
have you heard of data types? tuples don't have clear sematics/meanings
Wow that's new. Thanks!
Yeah, I was referring to the plain haskell example in the block post. I changed the type from `Maybe a` to `Integer` but should have said so.
Another person posted this exact comment on stackoverflow. Link to the question OP asked: [https://stackoverflow.com/questions/51003679/recursively-getting-elements-from-one-list-using-elements-from-another-list](https://stackoverflow.com/questions/51003679/recursively-getting-elements-from-one-list-using-elements-from-another-list)
Come on... I'm aware some people don't like FP Complete despite em being one if not the single biggest contributor to Haskell. But please cut them some slack. Have you even read it? https://www.cardano.org/wp-content/uploads/2018/04/FP-Complete-Cardano-Report-12-April-2018.pdf 
I know, but we are supposed to use it that way.
https://github.com/mikeizbicki/subhask/blob/master/examples/example0002-monad-instances-for-set.lhs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mikeizbicki/subhask/.../**example0002-monad-instances-for-set.lhs** (master → f53fd8f)](https://github.com/mikeizbicki/subhask/blob/f53fd8f465747681c88276c7dabe3646fbdf7d50/examples/example0002-monad-instances-for-set.lhs) ---- 
As others have commented already, you should prefer data types to tuples. This might include using dedicated types for things like the `id`s, and `time`. Each of which are something like `Integer` right now (`Num a =&gt; a`, really). For example: {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Id = Id Int deriving Show newtype Time = Time Int deriving (Show, Num) data Berth = Berth { bId :: Id , bOpen :: Time , bClose :: Time } deriving Show berthTime :: Berth -&gt; Time berthTime berth = bClose berth - bOpen berth You can of course disregard the language extension I used, then you would have to explicitly pattern match, or define things as newtype Time = Time { getTime :: Int } deriving Show berthTime :: Berth -&gt; Time berthTime berth = Time $ getTime (bClose berth) - getTime (bOpen berth) for example. As a further point, I would strongly suggest against using partial functions like `(!!) :: Int -&gt; [a] -&gt; a`, which couldn't possibly give you an `a` for each index, after all. Suggestions: You might be able to apply more things you know. For instance, the port seems to have two berths, which you might use to define a different type, or a pair if you want to stick to tuples for some reasons. I don't know the exact wording of your assignment though. Alternatively, you could really only talk about a partial mapping from berth ids to zero or more ship ids, which you could model with lookup :: Eq a =&gt; a -&gt; [(a, b)] -&gt; Maybe b Likewise, you would have a mapping from ship Id to time for each berth. This mapping would probably be total, defaulting to zero. This is your assignment of course, and people will not solve it for you. But you should note that there is things we think about and would like to express, and we do so using types. A list is not just a collection of some things, but more specifically zero (!) or more values that denote the same concept. If that is not exactly the case, we express this also using types, the simplest of which is `Either`. If we can't afford there being zero values, we use data NonEmpty a = a :| [a] which is just a list without the empty constructor. So all in all, making the types more concrete is in my opinion the best advice people could give you at this stage. "Figuring out the types" is more or less half of what people do in this subreddit.
:(
I generally use `-Weverything`, and then selectively disable warnings I disagree with. My default is something like: ghc-options: - -Weverything - -Wno-implicit-prelude - -Wno-missing-import-lists - -Wno-safe - -Wno-unsafe - -Wno-missing-local-signatures I try to do this in other compiled languages as well (`clang` also has `-Weverything`, for instance). Not only do you catch more stuff, but even the "super pedantic" warnings might teach you something new about the language, at least.
Just because they do a lot of important Haskell work doesn't mean everything they do is haskell news. This post is an ad and isn't Haskell.
I'd say for tech internships it's not really a given. A lot of tech internships pay above household median income, which is a ton for a single, often pretty young, individual.
What exactly is the deal with `Eq1`, `Ord1`, `Show1` and the like? Why do we need them? What wouldn't be possible without them?
I don't think many people would pay to have their blockchain tech audited 😂😭
Nice to see this!
&gt; https://www.cardano.org/wp-content/uploads/2018/04/FP-Complete-Cardano-Report-12-April-2018.pdf That pdf does mention Haskell, but the ad doesn't. &gt; Have you even read it? The ad includes an animated gif of a phone scrolling through the pdf, were they supposed to read it through that? The ad doesn't seem to link to the pdf version. In fact, the pages linked from the ad don't link to the pdf version, and neither do the pages linked from those pages. Seriously, even after seeing the URL, the best path I could find was: from the ad, click on "[Cardano uses FP complete](https://www.fpcomplete.com/blog/fp-complete-and-cardano-blockchain-audit-partnership)", then on "[Cardano Foundation](https://cardanofoundation.org/)", and that was a dead end because I couldn't find a link from `cardanofoundation.org` to `cardano.org`. Once at `cardano.org`, though, finding the report is quite easy: click on ["Transparency / Audit Report](https://www.cardano.org/en/cardano-audit-reports/)", then "[Download Report](https://dev-cardanoorg.pantheonsite.io/wp-content/uploads/2018/04/FP-Complete-Cardano-Report-12-April-2018.pdf)". &gt; But please cut them some slack. "Them"? So you, the OP, are not affiliated with FP Complete? That makes the post a lot less objectionable: this is you sharing a piece of news about a big Haskell player, rather than FP Complete trying to sell us a product which isn't related to Haskell.
I believe Hoogle only searches the libraries in the Haskell platform, whereas Stackage searches ... Stackage. Which is way more. I'm pretty sure `Data.List.isInfixOf` would show at some page. But since the type isn't `String -&gt; String -&gt; Bool`, you will have to skip a lot of hits like `Int -&gt; Int -&gt; Bool` first. If you give it `Eq a =&gt; [a] -&gt; [a] -&gt; Bool`, the result will show up immediately. Remember that `type String = [Char]`.
Because they are startups who don't have that kind of money to waste, or because most blockchain techs wouldn't pass an audit and their stewards know they wouldn't? If the latter, then audits sound like a very valuable signal for the savvy customers who know that most blockchain techs wouldn't pass an audit and are looking for one which would.
The `iconv` bindings might do the trick. Maybe even the `text-icu` package.
Luckily, Cardano did the responsible thing and considered such an audit important enough to pay FP Complete to certify their blockchain implementation. If faced with the choice between multiple vendors I'd definitely prefer the ones certified by FP Complete.
I don't know how you'd get the impression I was affiliated with FP Complete. I certainly am not (yet). Am I grateful for FP Complete having made it possible to use Haskell in a mission-critical commercial setting? Hell yes!
The latter, and yeah I agree. 
Please don't make me install a spam filter on /r/haskell
Do you have any citations for FP Complete being one of the biggest contributors to Haskell? Preferably something not from their own marketing material or from highly partisan sources. 
A nice Stack Overflow Q&amp;A that discusses some of that: [*What are FromJSON1 and ToJSON1 used for in aeson?*](https://stackoverflow.com/q/49188247/2751851)
At our company we have `three-layer` repository which is a template for other projects. I'm not sure it follows Three Layer Cake approach completely, but it's pretty decent to me.
I'm working through the Haskell from First Principles book. Currently at chapter 11, where you have to use as-patterns. I'm asked to do the following exercise: &gt; Use as-patterns in implementing the following functions. &gt; &gt; This should return True if (and only if) all the values in the first list appear in the second list, though they need not be contiguous. &gt; isSubseqOf :: (Eq a) =&gt; [a] -&gt; [a] -&gt; Bool Now, I wrote a little bit of code that solves that problem without using as-patterns: isSubseqOf :: (Eq a) =&gt; [a] -&gt; [a] -&gt; Bool isSubseqOf [] _ = True isSubseqOf _ [] = False isSubseqOf (x:xs) (y:ys) = if x == y then isSubseqOf xs ys else isSubseqOf (x:xs) ys My question is: How would an as-pattern make this simpler? How or why would I even use it here?
The number of packages that [Michael Snoyman](http://hackage.haskell.org/user/MichaelSnoyman) maintains might be a good start. I wish we could reliably calculate the percentage of haskell users that use stack/stackage.
See how you had to manually reconstruct `(x:xs)` there in the last line? If you had bound a name to that list using an as-pattern, e.g. `isSubseqOf l@(x:xs) (y:ys) = …`, then you could have written `isSubseqOf l ys` instead.
Oh man, I didn't even notice that! It's useful after all. Thanks!
Just one individual that FP Complete have pissed off, to the point of no return, has done more for Haskell than all of FP Complete has done, and will do, for the remainder of all of our lives.
&gt; pay FP Complete to certify their blockchain implementation That's maybe what fpco wants you to believe but it's more like Cardano was approached by fpco who desperately wanted to be part of the cardano hype-train. So they offered to audit Cardano free of charge but with the benefit of being allowed to [brag about it](https://www.fpcomplete.com/blog/fp-complete-and-cardano-blockchain-audit-partnership). You have to admit it's kinda impressive how they manage to present themselves as an experienced authority on blockchain tech certification after having merely published a single code review.
&gt; Just one individual that FP Complete have pissed off, to the point of no return, has done more for Haskell than all of FP Complete has done, and will do, for the remainder of all of our lives. Are you referring to [Edward Kmett](https://twitter.com/kmett/status/770088956415401984)?
This but unironically 
Great, good luck! Looking forward to seeing the new couchdb library!
Is Cardano with FPC now? I know they were with Serokell last year and I thought they’d switched to Well Typed.
About the possibility of `De` having a monad instance: iirc that is not allowed since it would mean you can get values “from the future”?
I've got currently a problem on fedora 27 when switching to the current nightly :( $ stack build Downloaded nightly-2018-06-23 build plan. No setup information found for ghc-8.4.3 on your platform. This probably means a GHC bindist has not yet been added for OS key 'linux64-ncurses6', 'linux64-tinfo6'. Supported versions: ghc-7.8.4, ghc-7.10.2, ghc-7.10.3, ghc-8.0.1, ghc-8.0.2, ghc-8.2.1, ghc-8.2.2, ghc-8.4.1, ghc-8.4.2 
Lol, how did I miss this one? This is perfect material for /r/programmingcirclejerk ! Also this can be generalized so I quickly made [this little mockup](https://www.reddit.com/user/snoyjerk/comments/8tgq1g/fpcomplete_launches_new_universal_auditing/). Enjoy!
Good post. Quick clarification: The definition of `NonEmpty` was just to show you how it works. The type itself is defined in the base library in `Data.List.NonEmpty`, with a lot of tooling and utilities supporting it, so you should use that. Similarly, `lookup` is an existing function in `Prelude`, not something you should define.
`RecordWildCards` can often make code almost completely unreadable, e.g. if you have several large record types in play in the same module. There are other use cases where it can be quite helpful though. It's an extension that can be valuable but must be used sparingly and with great care.
&gt; Yes we could have saved 32 bits with the old representation, but unless you save a full 64 bits in the info table you don't get any savings (info tables need to be an integral number of words). Why is that? I'm aware of pointer tagging but I thought GHC doesn't do that for StackRep info tables?
could you please add a link?
There is so much fun stuff, I think you'll enjoy the journey. Here is a [talk by Simon Peyton Jones](https://www.youtube.com/watch?v=uR_VzYxvbxg), it discusses the translation of Haskell (big, complicated) into Core (small, typed intermediate language)
*[[8:16](https://www.youtube.com/watch?v=uR_VzYxvbxg&amp;t=8m16s)]* And the other thing is that **the design of the Core language is itself a powerful sanity check on the design of the source language**. So the Core language has this small type system .. while the source language has this large, complex type system. But just as if I can translate some kind of new source language.. dynamic you know operational construct into Core that suggests that is just syntactic sugar. **If I can design some new source language type system feature and I can compile that into Core's type system that's a sanity check that suggests that the feature in the source language actually is not really adding anything fundamentally new**.
Isn't there a middle ground between `Get [HTML] User` and `Get [HTML] HtmlPage` ? Like `Get [HTML] UserDetailsPage`?
Ah, sorry. I was about to post the link but forgot. * https://github.com/holmusk/three-layer
I recently got interested in the [conduit](http://hackage.haskell.org/package/conduit) library, so upon reading the [docs](http://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit.html), I wrote the following barebones code as an experiment import Conduit import Control.Concurrent (threadDelay) import Control.Monad -- I want to add a producer' :: ConduitT a Int IO () producer :: ConduitT a Int IO () producer = forever $ do yield 5 lift $ putStrLn "Produced!" lift $ threadDelay 1000000 consumer :: ConduitT Int a IO () consumer = awaitForever (\elem -&gt; lift $ putStrLn $ "Received " ++ show elem) -- and make this accept more than 1 producer. run = runConduit $ producer .| consumer But suppose I want to add one more producer, and they can both feed values into the consumer (in any order). Can `Conduit` be used to do something like that? If not, how do we do that (probably using other libraries)? [Sorry for asking too many questions!]
What kind of approach did you have in mind? An update from my side: I realized that I could try to cheat: [this](https://github.com/noinia/hgeometry/blob/fasterpoint/src/Data/Geometry/Vector/VectorFamily8.hs) implementation uses a combination of unsafeCoerce + SPECIALIZE instance pragmas all over the place. The implementation is kind of horrible, but that seems to get me close to the Linear.V2 speed. The biggest performance increase came from also coercing the Ord (Vector d r) instance to select the right Ord instance directly rather than going through Applicative (Vector d) and (Foldable (Vector d). I've been trying to see if I can make this slightly more safe by having a single function that does the actual selection based on d (and has all the scary unsafeCoerce business), and that the rest of the implementation would just use this. That is not really working yet though.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [noinia/hgeometry/.../**VectorFamily8.hs** (fasterpoint → 50ce0f8)](https://github.com/noinia/hgeometry/blob/50ce0f8d612b51dab0b1e508911ece649189e708/src/Data/Geometry/Vector/VectorFamily8.hs) ---- 
The `(forall b. Eq b =&gt; Eq (m b))` syntax mentioned in that Q&amp;A is scheduled to be in GHC 8.6, so eventually Eq1 and its ilk won't be needed anymore.
Thanks for the name drop! This is some awesome work. 
Are there helper libs to use hakyll with servant?
This is essentially the `Get '[HTML, JSON] (WithTemplateInfo User)` trick. You can do it, but it's not terribly pleasant.
The way I was thinking was to use `K` as a type parameter rather than nesting to signal multiple steps into the future. So there’d be no function `Delay (S K) a -&gt; Delay K a`, but you could do `join`. I meant to explore it more in the post, but I’ll be busy with work for the next few weeks so I decided to just put out what I had. I might be able to get back to it in a month or so.
I don't think so, they don't list any in their libraries page.
Yeah, I see how it breaks the beautiful pattern of how servant works. It feels like a very ugly hack in ab otherwise beautiful scheme.
There's a reason people tend to make JSON APIs with servant -- it's really good at it, and a SPA can sidestep the context-of-a-page with many requests. But, ugh, SPAs are awful if you don't need them, and 99% of SPAs I interact with would be simpler, faster, easier, more accessible, and less broken if they were just simple webpages rendered on the server.
You most likely want to have a look at `ZipSink`, `ZipSource` and `ZipConduit`. The [readme](https://github.com/snoyberg/conduit#zipsink) has some usage examples. And you probably now this, but you can of course also write consumer = awaitForever $ \elem -&gt; do {...} just like you did for `producer`.
True, but because of backwards compatibility you are likely you encounter them in library code for a long time to come.
I want to run ghci and build with ghc in 32bit (x86) mode on Windows, and I want to use the benefits of the Haskell Platform 8.4.3, which still doesn't have a 32bit version for Windows again. How could I go about it? Installing HP 8.4.3 allows me to e.g. use the module Data.Vector.Mutable from the respective "extra lib". Overwriting stuff in the install folder with the official GHC 8.4.3 x86 release leads to GHC not finding those extra libs anymore -- I guess it's because their object files are compiled for x64 and/or because their interfaces are not linked up to that install then... Yeah... Hm. :\\ How can I cleanly reinstall all the extra libs I'm used to in a 32bit version?
A paper I wrote last year on working with highly asynchronous and event driven applications in pure functional languages; it also covers a novel infrastructure for querying extensions in a typesafe way using monoids.
SIMD seems to be coming up a lot recently. One of the key difficulties seems to be that the way we write programs normally just seems to have all the data layed out in completely the wrong shape (and in fact, it seems to be the wrong shape for non-simd, too, but at least you're legally allowed to speak of it in the wrong shape in the normal model). So regarding this "data layout" problem, how viable is a "rank2 types"-style approach to address this? For example, where we have data Point f = Point { x :: f Float , y :: f Float , z :: f Float } And then we can instantiate `f` with array or whatever. Could systematically doing things in this way solve the layout problem?
Thanks for the resources and the tip! I knew that but for some reason beautiful Haskell escapes me when I'm actually writing code. Thanks for pointing that out. :)
When I saw the real (in-place) QuickSort implementation :)
As recently shown in [this user survey](https://www.fpcomplete.com/hubfs/Haskell-User-Survey-Results.pdf) and also in this [other independent user survey](http://taylor.fausak.me/2017/11/15/2017-state-of-haskell-survey-results/#question-23) the vast majority of commercial Haskell users rely on Stack and Stackage. You (and u/dalaing) could argue those numbers might be subject to statistical fluctuations and you'd be probably right. But does this have any significance for the overall picture when it comes to a 80%-90% majority?
We've pointed this out to you before. Both of those surveys show severe selection bias. A poll by the Stackage devs, and by an independent Stackage contributor who has alienated contact with non Stackage supporters is about as biased as it gets.
You've brought up those links before. Aside from the fact that survey design is hard, one of those surveys is from FP Complete and one of them is from someone who I would consider very partisan in these kind of discussions. Neither of those links do anything to back up your original claim, either. 
Do you have any proof for this or are you just spreading FUD about FP Complete in order to make em look like con artists?
Do you generally distrust FP Complete to publish honest statistics or is there anything FP Complete could do so you'd trust their numbers? Why should we trust a survey organized by haskell.org more than one done by fpcomplete.com?
It's not about fp complete. Any survey by a party that is partial to a particular view is going to have selection bias. In this case, it's pretty extreme. It should not be surprising to think that fp complete has much better outreach to Stackage users than to non Stackage users.
Still, how do you suggest we can get our hands on statistics all sides accept as truth? 
Yep https://www.cardano.org/en/cardano-audit-reports/
We transpose a `[Point Identity]` into `Point Array` to get our arrays-of-fields, nice. Can `f` be something unboxed, like UArray? Floa would need to be hoisted out too, or Point parametrized with some type family, right?
Just have them pass around a parameter for an array of references. This is like a dumb object with a few mutable fields.
After more tinkering, I've noticed that I was wrong to assume Data.Vector being in containers. It's in vector. ghc-pkg sees and lists the target package, but GHC can't access the contained modules when compiling, nor can GHCI see them. Can loading the packages somehow fail silently? I have the impression that I'm missing something really important there.
I'm am not suggesting such a thing. I am suggesting those surveys aren't helping.
Unfortunately, it looks like you have fallen prey to the "[monad tutorial fallacy](https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/)". Many Haskellers go through that phase: once they finally realize how simple monads are, they think they can do the world a favour by writing a blog post explaining how simple monads are; unfortunately, that doesn't work. Sorry! Also, your blog post contains a few factual errors. I should stress that those factual errors aren't what is preventing your blog post from illuminating the masses; but still, for your own enlightenment, here are the errors I have spotted in your post. &gt; The first, `return`, takes a value and applies a data constructor to it. Not necessarily! [`ContT`'s `return`](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/src/Control.Monad.Trans.Cont.html#line-176), for example, is `return x = ContT ($ x)`, not `return x = ContT x`. And for lists, `return x = (:) x []`, not `return = (:) x`. &gt; Why would you use monads in your code? The simple answer is so you can do useful work such as manipulation and management of input and output and asynchronous programming. That's not quite right. You could do all those things with type-specific versions of `return` and `(&gt;&gt;=)`. The reason to use a typeclass is to write generic code which works with any instance of that typeclass. &gt; For example, applying a function to two lists since each list has an instance of `Functor`. It is the `[]` type constructor which has a `Functor` instance, not the two list arguments of `liftA2 f`. Also, the `Functor` instance isn't really relevant here. &gt; Adding instances of these typeclasses to an instance of `Monad` enables you to do more with your data types. Again, adding an instance to a type does not allow you to do more with a type, since you could do the same things to that type using type-specific versions of the methods in the typeclass. Adding an instance to a type allows you to take a piece of generic code which is polymorphic over this typeclass, and to instantiate it at that type. Furthermore, since the typeclasses in question are `Functor` and `Applicative`, which as superclasses of `Monad`, not giving `Functor` and `Applicative` instances to a type which has a `Monad` instance doesn't just mean that we would be able to instantiate fewer polymorphic functions at that type, it means that the instance will be rejected. The reason for this is that the behaviour of the methods of a typeclass is given in terms of the behaviour of the other methods of the typeclass and in terms of the behaviour of the methods in the superclasses. &gt; `Just` acts like a function and when we try to `bind` a value to a name or use the value somehow `Just` will either give us our value or will `fail` and return `Nothing`. How could `Just` return `Nothing`? &gt; Along the same lines as the last implementation this `return` wraps a value in the data constructor `List`. Not at all; `return` creates a list with one element, that value. The outermost data constructor for such a list is `(:)`, a.k.a. "`Cons`", not `List`. `List` is not a even a data constructor. Perhaps you meant the type constructor `[]`, a.k.a. `List`? &gt; This can be thought of as similar to a for loop for `[]`. See, this is a very good example of the monad tutorial fallacy. You know what `(&gt;&gt;=)` does, you know that you would implement it as a for loop in an imperative language: output = [] for x in input: output += f(x) return output But many other functions would be implemented as a for loop `fmap`, `traverse`, `zip`, etc. So the explanation that `(&gt;&gt;=)` behaves like a for loop is transparent to you, but opaque to your audience, who doesn't know which for loop you're talking about. Worse, maybe they think of the for loop corresponding to `fmap`, and so they will have to unlearn that incorrect piece of knowledge next time they try to learn something related to monads.
This is sweet! 
I [wrote up a gist](https://gist.github.com/andrewthad/46dde4c28b4ea48c18f9c31087282316) as an example of what I had in mind. Note that there are four files in the gist, and the `Nat` type I use isn't the magic one provided by GHC. I haven't benchmarked this, but I think that if you put `INLINABLE` pragmas on the functions in `Fancy.hs` that have `ImplicitNat` constraints, it should perform well.
Nix is interesting to me because it's not a Haskell build tool, but it can build Haskell products. This means I can use it as the dependency managing evolution of a Makefile on a polyglot project. I often wind up with a mix of languages for data processing, server/client split, or inherited tooling that needs to be built. Nix offers a way to manage that, including pulling, building, and caching dependencies, in a language neutral way. And I can get a reasonably minimal docker image out of it at the end of it all. It's definitely much harder to use. Much, much harder.
To answer my own question about prior art, I [just found hawk](https://github.com/gelisam/hawk) which is similar.
Was thinking about this the other day. This will be so useful
You might just be better off making two separate types for `Y` and `Z` but if you really want them to be one type, you could do this with GADTs: data X (a :: Bool) where Y :: X 'True Z :: X 'False -&gt; X 'True
What you are trying to do here is unclear. Are Y and Z types themselves, or constructors? The way you have it formatted is confusing: It says that "Y is a constructor for type X, which takes no inputs" and "Z is a constructor for type X which takes an input of type Y", while type Y has never been mentioned before. Are you sure you don't just want two different types, with `data Y = Y`, `data Z = Z Y`, and `type X = Either Y Z`?
Thanks! I'll play with that. I've amended my original post to include the more complicated thing I'm actually interested in representing. It includes a constraint on the length of an argument to a constructor; can that also fit in the GADT framework?
Thanks. I've amended my original post by adding the real-world problem that motivates the simpler one, including the need for them to be the same type.
This smells of [Quotient Inductive Inductive Types](https://arxiv.org/pdf/1612.02346.pdf), which afaik is still an emerging research area, and definitely not expressible in Haskell.
Wow, it works! (A note to readers: I had to use `{-# LANGUAGE GADTs, KindSignatures, DataKinds #-}`.) My question about the length constraint remains.
I love this idiom! Is there a way to express the following (as valid Haskell)? ``` {-# LANGUAGE GADTs, KindSignatures, DataKinds #-} data XConstructor = XA | XB | XC data X (a :: XConstructor) where A :: X 'XA B :: X 'XB C :: X '(XA or XB but not XC) -&gt; X 'XC ``` That is, you can build a `C` from an `A` or a `B` but not from a `C`.
I guess so yes. :)
That's great
Solved it -- but it's awkward. ``` {-# LANGUAGE GADTs, KindSignatures, DataKinds #-} data XConstructor = XA | XB | XC data AorB = AorB | NotAorB data X (a :: XConstructor) (b :: AorB) where A :: X 'XA 'AorB B :: X 'XB 'AorB C :: X a 'NotAorB -&gt; X 'XC 'NotAorB ``` If there were more subsets to keep track of (e.g. AorC, BorC) this could get way out of hand.
I've see the following text (which variations in language) in relation to monads in multiple places. &gt;That way you can only chain computation on the monad but you never have direct access to the impure value and that keeps all your functions pure. It might seem like a hassle but the language has a *bind* operator and some other syntax sugar to make things simple. \[1\] My question is: How does never having direct access to the impure value (e.g. in, say, An `IO Int` monad) but only being able to chain computations lead to preserving purity? \[1\] [https://medium.com/real-world-fsharp/understanding-monads-db30eeadf2bf](https://medium.com/real-world-fsharp/understanding-monads-db30eeadf2bf)
It is isn't it. I even suppose I may have seen you there as well then. All in all pretty cool stuff!
You could extract the two fields of the `RelationshipFlavor` tag into its own record `data Flavor = MkFlavor Arity Label` and then reference that twice in your `HypergraphNode`. What is the semantics for the arity of a `RelationshipFlavor` outside of a `Relationship` anyway?
I'd define the pieces separately first: import Data.Vector.Sized data Atom = Atom Text data Flavor (n :: Nat) = Flavor Label data Relationship where Relationship :: Flavor n -&gt; Vector HypergraphNode n -&gt; Relationship data HypergraphNode where AAtom :: Atom -&gt; AAtom SomeFlavor :: Flavor n -&gt; SomeFlavor ARelationship :: Relationship -&gt; ARelationship You might notice that SomeFlavor is different. To put it in the list we have to forget its type level arity. We could put a singleton copy at the value level to do runtime checks but do a check whether it's a valid HypergraphNode first. Also, written on phone so it probably doesn't quite compile.
The code needs to be aligned on an 8-byte boundary, so that means the info table also needs to be aligned on an 8-byte boundary. If we relaxed the alignment requirements to 4 bytes then we could have 20-byte info tables, but that question is academic now that info tables are always 16 bytes anyway. We do have different formats for info tables - functions, stack frames, and constructors all have slightly different info table layouts. I'm not sure how pointer tagging is relevant here, so perhaps I'm misunderstood your question though.
The link to the nofib results was in the diff summary, which was linked from the original post.
How about something like [this](https://gist.github.com/Tayacan/d3e63e5cbc42d67e5bbf31450cb0a91b)? Idea taken from [here](https://www.schoolofhaskell.com/user/konn/prove-your-haskell-for-great-safety/dependent-types-in-haskell) - he uses the `singletons` package, but I figured I'd keep it simple.
\`\`\` {-# LANGUAGE GADTs, KindSignatures, DataKinds, TypeOperators #-} import Data.Type.Bool import Data.Type.Equality data XConstructor = XA | XB | XC data X (a :: XConstructor) where A :: X 'XA B :: X 'XB C :: (t == 'XA || t == 'XB) \~ 'True =&gt; X t -&gt; X 'XC \`\`\` which gives: \`\`\` \*Main&gt; :t A A :: X 'XA \*Main&gt; :t B B :: X 'XB \*Main&gt; :t C C :: (((t == 'XA) || (t == 'XB)) \~ 'True) =&gt; X t -&gt; X 'XC \*Main&gt; :t C A C A :: X 'XC \*Main&gt; :t C B C B :: X 'XC \*Main&gt; :t C (C A) &lt;interactive&gt;:1:1: error: • Couldn't match type ‘'False’ with ‘'True’ arising from a use of ‘C’ • In the expression: C (C A) \`\`\`
Thanks for the response! &gt; The code needs to be aligned on an 8-byte boundary, so that means the info table also needs to be aligned on an 8-byte boundary. But **why** does code need to be aligned on a 8-byte boundry? I tried messing up the alignment for labels with StackRep tables by hand (added 4byte before the table) and everything still worked fine? I'm likely just missing something fundamental but I can't say what. I know there is a performance hit when loading across across cache lines, but that seems orthogonal. &gt; I'm not sure how pointer tagging is relevant here, so perhaps I'm misunderstood your question though. I was just thinking if we had tagged pointers to either the table or code then we would need to have appropriate alignment as well. So yes probably not relevant but was just a reason I could think of why we would need 8 byte alignment. &gt; If we relaxed the alignment requirements to 4 bytes then we could have 20-byte info tables, but that question is academic now that info tables are always 16 bytes anyway. Are there not 24Byte ones as well? From some code compiled with head: .section .text .align 8 .align 8 .quad 4294967301 .quad 0 .long 14 .long z_closure-(foo_info)+0 .globl foo_info foo_info: _c6KX: .... 
This is the right approach in my opinion. Except we should be getting better syntax for type-level and term-level nats and promotion/demotion between them, and a standard length-indexed vector type with simple syntax. Can someone please summarize the status of those things?
Took her Haskell class at my College which was a pretty great opportunity. We covered LH for a couple weeks, but most of the class was just studying Haskell up to using monads. Favorite part of the classes were really interesting guest lecturers.
\&gt; But **why** does code need to be aligned on a 8-byte boundry? Performance only, I believe. IIRC this was the Intel recommendation, but it's a while since I looked at it. We have a hard requirement on at least a 2-byte alignment because we use the LSb in the GC to mark closures that have been evacuated. \&gt; Are there not 24Byte ones as well? Yes there are. You aren't letting me get away with skipping any details here :) Info tables have a fixed part which is always 16 bytes (32 bytes when profiling), and an "extra" part that depends on the type of closure or stack frame. For a function (as in your example), the extra part is 8 bytes. (all these sizes apply to 64-bit builds only, divide by 2 for 32-bit builds). Currently the total size of the info table is always a multiple of the word size.
A quick google search [brought this up](http://hackage.haskell.org/package/language-javascript), which looks quite good.
Anyone? Mmm, a quick search through [hackage](https://hackage.haskell.org/packages) turned up at least three serious native Haskell JS parsers that are actively supported, some by large teams; several older native Haskell JS parsers that might be unsupported; and at least five or six packages that include bindings to some external JS parser. I probably missed more libraries. Here are the actively supported native Haskell parsers I found: [language-javascript](https:/hackage.haskell.org/package/language-javascript), [language-js](https:/hackage.haskell.org/package/language-js), [language-ecmascript](https:/hackage.haskell.org/package/language-ecmascript)
Read it, loved it. Splendid book. It taught be things about basic mathematics that I thought I knew, but apparently I had missed some subtleties. It is written fully from the 'math' point of view though. So translating your newfound insights to being applicable to Haskell programming will still require effort.
And because I asked (on twitter), here are the slides: [https://www.slideshare.net/ekmett/revisiting-combinators-edward-kmett](https://www.slideshare.net/ekmett/revisiting-combinators-edward-kmett)
my beautiful artwork is being destroyed by the site layout :(((
Got it working: 1. Set up GHC x86 2. Set up cabal 3. Installed the set of packages with their revisions by hand using cabal So the key info here is the list of curated packages from the Haskell Platform.
I'm sorry for my crap layout, a friend of mine actually send that picture and I thought it was so funny that it motivated me to write this post. I'm not talented like you :((
Oh, I must have missed that. Sorry about that.
Data.Vector.Unboxed takes this from the other direction by mapping to the transposed representation with a data family. So with Repa Array U Z (Int, Int, Int) would be a `Point Identity`while Array U Dim1 (Int, Int, Int) would be `Point Data.Vector.Unboxed`. I think accelerate is also unboxed and shape polymorphic.
May someone explain me the monad instance (more precisely the lambda case) on the second slide?
It substitutes terms for variables. https://www.schoolofhaskell.com/user/edwardk/bound
The reasoning is that with \`IO Int\` you don't have an \`Int\`, just a plan on getting that \`Int\`. The plan itself is pure (well, it's an other question that you can't observe it, unless the monad is a free-monad-like interpretable construct). It's only the runtime system (RTS) that will execute the plan, thus the pesky impure details are left to the impure RTS. Apart from being a nice mind exercise, I don't really see the point in emphasizing this. While technically true, it reminds me the "if it compiles it works" slogan.
Doesn't this syntactically conflict with @-patterns?
Yes, GHC takes into account equality constraints when checking for functional dependencies. I haven't found any detailed documentation, but this was introduced in https://git.haskell.org/ghc.git/commitdiff/fe61599ffebb27924c4beef47b6237542644f3f4.
You can actually observe the value (its the value passed to the bind (the function on the right of `&gt;&gt;=`. For this particular thing, I'm think I'm going to persist to find out why the statement in my question in true. Monads are very important in Haskell (obviously) and I want to know why Monads give rise to purity even in impure situations. 
There's a GHC proposal for or patterns, pending committee review: https://github.com/ghc-proposals/ghc-proposals/pull/43.
Thank you, I think I get the essence of it. 
&gt; IIRC this was the Intel recommendation It still is, there are some situations on newer CPU's where it still leads to worse performance. And it is generally bad on older (Core2 and older) ones. But I'm wondering how the penalty compares to the cost we pay for the code size.
I'm afraid I don't quite follow the issue. Perhaps you could summarize what specifically you have done and what you are seeing?
Didn't know about ``-Weverything` - thanks!
Sweet!
You might want to record information like "The RelationshipFlavor [needs] implies an opposite relationship [helps]," or "if X is a boring thing, hide it from views constructed in the default manner." (The second statement is a relationship involving the "is a" RelationshipFlavor and the node "boring thing".) Any information regarding RelationshipFlavors can be recorded by putting those RelationshipFlavors in relationships, and I believe it's the most natural way to do it.
I hope I understand your question: A RelationshipFlavor like "_ needs _" has arity 2. A RelationshipFlavor like "_ uses _ to achieve _" has arity 3. "Maybe _" has arity 1. Keeping the arity attached to the flavor ensures that it can't be used the wrong way.
Completely agree. I wanted to add that it is also very well written and was easy to follow. The best introductory book on CT I know of.
To be fair, `language-ecmascript` supports ES3 (from 1999), the read me for `language-js` says that it's still a work in progress and `language-javascript` has no specified ES version. 
Ah awesome, together with INLINE that indeed seems to do the trick :D. (just INLINABLE is not sufficient apparently). In the benchmark below sort is directly using Linear.V2 , Family8 the scary unsafecoerce everywhere, Family6 an implementation of your approach using singletons + wrapping GHC.Typelits on top of it, and fancy your implementation. benchmarking convexHullBench/build/2000/sort time 1.324 ms (1.285 ms .. 1.381 ms) 0.988 R² (0.979 R² .. 0.996 R²) mean 1.367 ms (1.336 ms .. 1.403 ms) std dev 110.9 μs (88.71 μs .. 132.5 μs) variance introduced by outliers: 61% (severely inflated) benchmarking convexHullBench/build/2000/sortFamily8 time 1.573 ms (1.512 ms .. 1.644 ms) 0.990 R² (0.985 R² .. 0.994 R²) mean 1.587 ms (1.551 ms .. 1.628 ms) std dev 135.0 μs (113.8 μs .. 163.5 μs) variance introduced by outliers: 62% (severely inflated) benchmarking convexHullBench/build/2000/sortFamily6 time 1.362 ms (1.327 ms .. 1.403 ms) 0.991 R² (0.986 R² .. 0.995 R²) mean 1.387 ms (1.356 ms .. 1.429 ms) std dev 115.4 μs (94.58 μs .. 150.0 μs) variance introduced by outliers: 63% (severely inflated) benchmarking convexHullBench/build/2000/sortFancy time 1.374 ms (1.342 ms .. 1.412 ms) 0.994 R² (0.992 R² .. 0.997 R²) mean 1.381 ms (1.350 ms .. 1.412 ms) std dev 104.3 μs (85.50 μs .. 133.4 μs) variance introduced by outliers: 57% (severely inflated) 
Actually, both language-ecmascript and language-js reference ECMA-262 from 2017, the latest spec. In language-js, the doc file that links the grammar line-by-line to the spec has not been kept up to date, but the name of the grammar module for the parser was recently updated from `Language.JavaScript.Parser.Grammar5` to `Language.JavaScript.Parser.Grammar7`, so it's not far behind.
This sounds like it's based on a new GHC feature. Is there a link to its documentation?
I don't think there is any documentation for source plugins yet as they will be in the forthcoming 8.6 release. There is documentation in the manual for them in the release though.
Thank you!
Isn't that `(=&lt;&lt;)`?
ooops yes thanks! fixed
[removed]
Let alone packages which do fancy tricks with IO, for example Reflex or Reactive-banana! 
You're doing god's work, sir.
Unwrapping an `Int` from an `IO Int` is what causes the side effects to be executed. Let's say you could just access the `Int` inside an `IO Int` anywhere in your code. That would mean that whatever function that unwraps that `Int` would also have side effects, so we don't allow this. Using `&gt;&gt;=` allows you to combine different `IO a` values without actually having to unwrap them. Note that if you never have to unwrap an `IO a` it's still pure. `main` has type `IO ()` which means that even `main` doesn't end up unwrapping the `()` from the `IO ()`. It just combines different `IO a` values to make a single `IO ()`. The unwrapping occurs magically when the program is executed. That means everything you type in Haskell is pure.
Have you send an email to `admin@hackage.haskell.org` as mentioned on the [registration page](https://hackage.haskell.org/users/register-request)?
Nice article, thanks! Your post comes very timely as u/chessai and I are about to publish [a little utility library for working with `Coercible`](https://github.com/sjakobi/coercible-utils). My understanding of roles was very fuzzy until now! :) What do you mean by "in most cases" here: &gt; If f :: Coercible a b =&gt; a -&gt; b (common if f is a newtype un/wrapper, or composition of such), then fmap f O(n) is equivalent to coerce O(1) in most cases.
My memories on this are pretty fuzzy but iirc evaluating the combinator calculus depends pretty heavily on sharing the subexpression duplicated by S. But not all duplications are visible so evaluation order can change the sharing oportunities that are available. Could the just-in-time simplifier that the gc would do help improve that sharing? Or does the environment merging already make the runtimes practical?
Probably because it relies on `fmap id == id` which isn't necessarily true in the most direct sense for some functors.
The talk dove into a way to recover sub-expression sharing even "at a distance" by using hash-consing.
I'll add a footnote, thanks! What I mean is that this is only true if `f`'s type parameter isn't `nominal` :)
It really really really really should be, or else you've violated the functor laws and everything goes terribly wrong.
It's not really rare to see constructs that satisfy laws as far as the exposed API is concerned, but violate them internally. Then again, in those cases you won't get a `Coercable` instance in the first place since the constructors won't be exported.
Awesome! Thanks for benchmarking this. I’m glad it worked, and good luck with your experiments!
I see this happen occasionally with \`Eq\`. The \`Eq\` instances in \`Data.Map\` and \`Data.Set\` are written like this. I've never seen anyone do this with \`Functor\` other than on the \`Mag\` data type.
&gt;Apart from being a nice mind exercise, I don't really see the point in emphasizing this. While technically true, it reminds me the "if it compiles it works" slogan. I'd say it does matter -- not so much because you can claim that `putStrLn :: String -&gt; IO ()` is pure in some sense, but because an `IO a -&gt; a` function would break referential transparency.
IIRC the functor law follows from parametricity as long as the data type isn't a GADT, and there aren't that many of those around.
The `primes 10000` conduit is essentially a list of primes that gets computed only once, and reused by `replicateM_`. You can split up the application of `runPrimes` so that every iteration recomputes it from scratch, for example, by putting the arguments in a list: runX x = Data.Foldable.for_ (replicate x 3000) ConduitBench.runPrimes That seems to work in this case but it is not a robust method in the presence of fusion rules. `criterion` provides `whnf` and `nf` as a general way to ensure the compiler doesn't optimize the application we want to run multiple times as a single thunk.
I don't think that's related to GADTs. The composition law follows from the identity law by parametricity, but we cannot get both for free because of counterexamples such as: bad_fmap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b bad_fmap _ _ = Nothing
I can't find anything special about these `Eq` instances. What's the trick(?) your talking about?
I completely grok why 'x:xs' is a common pattern. My question is why 'xs' or 'ys' etc? Should I read it as "x's" (as in, multiple x's)? I'm conceptually solid, just curious about the origin of 'xs' and others, and how it should be read.
"many x's" is my interpretation as well.
Consider the following piece of code (the code is full of irrelevant details because it is from some code I'm writing currently, sorry for that!). Is it safe to assume that by the time the recursive call to `feedForward` happens, `parsedComp` has been garbage-collected (atleast on average). It is important to me because `parsedComp` is potentially a _huge_ object, and I would prefer it is GC'ed. feedForward :: [[ClassName]] -&gt; (ClassName -&gt; Maybe RawClassFile) -&gt; (ClassName -&gt; Maybe (V.Vector ConstantInfo)) -&gt; (FieldDB, MethodDB, FieldNullabilityDB, MethodNullabilityDB) -&gt; (FieldDB, MethodDB, FieldNullabilityDB, MethodNullabilityDB) feedForward [] _ _ dbs = dbs feedForward (comp : rest) rcf cpoolf (fDB, mDB, fDB_n, mDB_n) = let parsedComp = map (fromJust . rcf) comp mthds = concatMap getMethods parsedComp flds = concatMap getFields parsedComp cnames = map (javaNamify . thisClass) parsedComp mids = map toMethodID mthds fids = map toFieldID flds loadedThings = M.fromList $! map (\(i, d) -&gt; (EFieldID i, EFieldData d)) (zip fids flds) ++ map (\(i, d) -&gt; (EMethodID i, EMethodData d)) (zip mids mthds) loadedStatus = M.fromList $! map (\i -&gt; (EFieldID i, NotAnalyzed)) fids ++ map (\i -&gt; (EMethodID i, NotAnalyzed)) mids (_, fDB', mDB', fDB_n', mDB_n') = analyseAll cpoolf loadedThings loadedStatus fDB mDB fDB_n mDB_n in feedForward rest rcf cpoolf (fDB', mDB', fDB_n', mDB_n') 
It's possible for two `Map`s that aren't structurally equal to be considered equal by the `Eq` instance. The API provided by `Data.Map` makes it impossible to distinguish such maps from one another (except by using `showTree`).
&gt; Haskell's expressiveness allows coding in a very concise style. For example it renders comments redundant and makes you able to call all your types `T` and typeclasses `C` and them import them qualified. Salty! 
You can obtain information about `a` using `typeRep` (will require a `Typeable a` constraint). `TypeRep` should contain enough information to construct a TH `Type`.
Textfiles on straight Github repos is a very low-level, but comfortable way of publishing things. Almost zero maintenance, especially when typos are caught and even often fixed by the readers. 
what is Mag?
&gt;I want to know why Monads give rise to purity even in impure situations. It's probably helpful to really understand why things are called pure or impure. A monad is pure, its values are pure, and the results from a monad are pure; it's purity the whole way down. Why? Because of referential transparency--given the same input, you get the same output. This is true even in the case of the IO monad, it's just that IO means that your input environment is implicitly "what you gave the function + the outside world." A better description for the IO monad would be "highly unpredictable", not "impure". (There's a great comment on this subreddit that goes into more detail about this but I can't find it for some reason...) The only reason that monads are actually used for Haskell's IO is that they are a very convenient solution for a lazy language (which has no concept of "order of evaluation") to sequence things and say "this happens, and then this, and then this..." This sequencing guarantee can then allow you to implement IO by using that same pattern and have the IO behave in an intuitive way. Monads themselves really have nothing--inherently--to do with impurity, so it makes sense that values of a monadic nature are still pure in a language that is entirely pure. Maybe is a monad, for example; so are lists, so are like 80% of the datatypes in the Prelude. &gt; My question is: How does never having direct access to the impure value (e.g. in, say, An IO Int monad) but only being able to chain computations lead to preserving purity? The short answer would be "If you don't have a way to construct a function of type `IO a -&gt; a`, there's no way to have a function that can perform undeclared side-effects." But this answer has nothing to do with monads, or with understanding them; mostly because monads are unfairly lumped together with the idea of "getting impurity" and don't have much to do with IO in haskell.
Interesting, I think when I was trying something similar with pipes this behaviour didn't occur. So I'm not sure where the difference would be. But it gives me some ideas on what I can investigate. I'll look further into this tomorrow.
That may be because in conduit, producers are constructed with [`HaveOutput`](https://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit-Internal.html#t:Pipe), so they have a list-like runtime representation HaveOutput :: Pipe l i o u m r -&gt; o -&gt; Pipe l i o u m r whereas in pipes, which is bidirectional, [the tail of a producer is a continuation](https://hackage.haskell.org/package/pipes-4.3.9/docs/Pipes-Internal.html#t:Proxy) (with `b' ~ ()`), preventing it from being thunked: Respond :: b -&gt; (b' -&gt; Proxy a' a b' b m r) -&gt; Proxy a' a b' b m r 
"The Stackage ecosystem" might be more appropriate. Several layers here that aren't necessary if you just use cabal-install. Also, I think you accidentally flipped Hackage and Stackage in the circle diagram.
The set of nested circles doesn't quite make sense to me. I think it's because those things have different "types". Hackage and Stackage have the same types, so a Venn diagram makes sense for those two. But GHC is a compiler, Cabal is a Haskell library, and stack and cabal-install are command line build tools.
Oleg [recently posted an LC-&gt;SKI translator](http://okmij.org/ftp/tagless-final/cookbook.html#ski) that seems relevant—not sublinear but he says it's easily optimizable.
Good points. And you are correct about be flipping hackage and stackage.
Notwithstanding my error in naming on the circle diagram, I think this circle diagram would have been quite useful to me when I was first using Stack. I will consider your point when I redo the diagram.
You might be interested in `ad`. If you can generalize your `goal` to something like this: data Dist a d = MkDist [(a, d)] goal :: Floating d =&gt; Dist Color d -&gt; d then `ad` can plug in some fancy types of numbers in place of `d` to compute derivatives (forward or backwards) that you can use to implement gradient descent.
Yeah I can generalize like that. And I've seen libraries for gradient descent. That still requires a bunch of complexity to keep track of the edges, because I want the solution to be polymorphic over different base types (deriving Bounded and Enum), not just `Color`.
Beautiful.
Then having different type though, doesn’t mean they can’t depend on each other. Or is it more specifically with using a Venn diagram to illustrate what they encapsulate? 
UPDATES: I changed the title to reflect how the article is really about Stack, not just Haskell. I fixed the diagram thanks to /u/MidnightMoment's sharp eye.
https://github.com/ekmett/bifunctors/blob/ffc6b5c0a7fe8f3e1c3603e54a4025f06d7bcfba/src/Data/Biapplicative.hs#L212-L222 It was mentioned in [the article for this recent post](https://www.reddit.com/r/haskell/comments/8t0hs2/quantifiedconstraints_and_the_trouble_with/).
From ghc-devs: &gt; I was recently doing some GHC Core related stuff, and I found that it might be useful to have a theoretical understanding of the type system for writing better practical code. Therefore I started to read papers about GHC Core, or System FC, following the publication timeline, to try to understand each design choice involved in the type system. &gt; I wrote a simple literature review that I think might be useful for people like me. If you're thinking about learning System FC, you might find it helpful: https://github.com/xnning/GHC-Core-Literature-Review/blob/master/doc/doc.pdf
Oh, right! The next `ghc-prim` release will actually [mention](https://github.com/ghc/ghc/blob/3d002087dce9c61932dd17047902baa83581f4df/libraries/ghc-prim/GHC/Classes.hs#L133-L134) that convention. It's called _substitutivity_ there.
Yea, but what do you think of Google-discoverability of Github markdown files? For example when I searched for `The Cont monad explained` your post was way below the first page, and did not even include the search query (probably because of missing it in the page title).
nivr
Everything will probably just be thunked before the next call to `feedForward`. `let` doesn't force anything (even the tuple pattern binding at the end). These `$!` also aren't useful, because `M.fromList` is already strict. You at least need a bang or a `seq` on the last tuple binding, but whether that's sufficient depends on whether `analyseAll` produces the tuple after having fully evaluated its contents. You should replace this tuple with a custom data type anyway, and then you can make it strict.
I'm confused at how the following four passages can all coexist: data Vect (n :: Nat) a where ... type Arity = SNat type Flavor n = (Arity n, Label) data HypergraphNode where ... Relationship :: Flavor n -&gt; Vect n HypergraphNode -&gt; HypergraphNode Both appearances of `n` in the last line, I assume, refers to the same thing -- that's how we guarantee the length of the Vect matches the Flavor of the Relationship. But in `Flavor n`, `n` is an `SNat`, whereas in `Vect n`, `n` is a `Nat`. The code *does* compile -- but what's going on?
[As Syrak notes](https://www.reddit.com/r/haskell/comments/8u3x6x/maximize_a_goal_function_over_probability/e1cf0fr/) automatic differentiation and gradient descent will likely solve your needs -- the only strong complication is keeping the distribution actually a distribution. A naive gradient estimate will push the distribution off of summing to 1. Renormalizing at each step is one approach, but better would be not to move much in that direction in the first place. I would like to point out that your `Dist` data type is poor because `MkDist` admits multiple representations for the same distributions (reordering), unclear interpretation when there isn't exactly one entry per constructor, as well as non-probably distributions (numbers being negative, or not adding to 1). Depending on your use cases, some may be worth fixing, others not. Reordering can be solved by simply having a fixed required order. If you're content to only work with a single data type, use `data ColorDist d = {red, green, blue :: d }`, or just a tuple `(d,d,d)`and modify your goal function accordingly: goal dist = ... where r = red dist or goal (r, g, b) = ... If you really want to be somewhat generic over the underlying type, this becomes extremely similar to the problem generic memoization libraries need to solve of associating a value with each member of a type, so stealing from one of them is also a good option. They typically need a few extra guarantees on the type. If you want to roll your own, you can change data Color = Red | Green | Blue to data Color = Red | Green | Blue deriving (Enum, Bounded, Ix, Ord, Eq) and use an array or whatever fancy equivalent is in vogue these days to store the values. In ancient Haskell: type Dist a d = Array a d mkDist values = accumArray (flip const) 0 (minBound, maxBound) values and then use as goal dist = ... where r = dist ! Red This will make it clear that you take the last value at construction, and that a missing value is 0. Summing all the values could instead be done with: mkDist values = MkDist $ accumArray (+) 0 (minBound, maxBound) values. To handle the probabilities not adding up to 1, you could store the logs of values, and explicitly calculate a normalization constant and the actual probabilities in the goal function. This is essentially the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function). AD will pass through, and thus not strongly push the gradients in such a way as to alter the normalization -- because it won't alter the probabilities. To maintain good precision, just keep the stored numbers around 0, perhaps by subtracting the mean after each gradient step. As a bonus, this will automatically exclude negative numbers, though there may be issues around 0 so the the boundary of the space of probabilities might not be accessible. Rolling your own is one option, as is the Haskell package [log-domain](https://hackage.haskell.org/package/log-domain), though I haven't tested it with any automatic differentiation libraries.
These kinds of writeups are very useful, thanks!
It's possible to parameterize probability distributions over a cube of dimension one less than the size of the support in a continuous way that preserves maximus and minimums. Gradient decent over that space solves your problem.
This is an excellent overview!
&gt;located in Boston CA Did you mean Boston MA?
There's scarcely more convincing evidence on hackage that a robust and expressive programming language can't save you from bad taste.
I live in Somerville (Next to Boston) and mostly program in Erlang. Are you using Cloud Haskell for your Backend? I have an adequate understanding of Haskell up to Monads and New Types (and willing to learn more), but have not used Cloud Haskell or Haskell otherwise for backend services. Would knowledge along those lines be beneficial? Currently I'm a Master's student at Tufts, but I only have a single class and my thesis to do over the next year. I'd love to be working while doing those things. The class I'm taking is on program analysis and verification, which is probably what your company is largely using to find security holes.
How? The only way I can think of is "generate n-1 numbers between 0 and 1, sort them, add 0 at the beginning and 1 at the end, then use the gaps between consecutive numbers as probabilities". But that's not differentiable (think about what happens when two numbers change places), so I think gradient descent will behave weirdly. Is there a better parameterization?
Eh I have to go sign up a paper I wrote for a class now. The mapping goes like this N-Cube with side lengths 1 -&gt; A certain N-Simplex with most of side lengths of 1 -&gt; a probability distribution Every probability distribution over N elements can be viewed as a simplex of N-1 dimensions where all but one of your dimensions corresponds to a dimension of the simplex. The last element is 1 minus the sum of the remaining probabilities. So now you just have to figure out how to nicely map and N-cube to an N-simplex. I found an analytical way to accomplish this as part of a project I worked on but I'd have to dig up the exact formula.
Yeah, that's very similar to what I described. If you take a point in the cube and sort its coordinates, you end up in a certain simplex. But that's not differentiable at the largest face of the simplex. Can it be made differentiable?
"Exes". Yep. I'm pretty certain that's the reason. 
Ok, I see what you're getting at a little more now, but a Venn diagram still doesn't seem right to me. A dependency graph seems more appropriate.
In a way, the flipped version is less accurate. All of hackage is not contained in stackage. It's actually the other way around. Every package in stackage is contained in hackage. However, this new relationship does work as a dependency tree.
Thanks for the pointer to softmax! It's not quite what I'm looking for, because in many cases the maximum will be on the boundary, but it seems like the right answer should be something like that.
Yep. My transformation was just a series of multiplications and additions as I recall. Let me look it up. Also keep in mind you don't need a space to be differentable for gradient decent to work if you use down hill simplex. Your proposed transformation plus down hill simplex would be sufficient I think.
Technically, Cabal-ized packages (including those on Hackage) can be used with other compilers as well. It's just that GHC has been the de-facto standard for quite some time.
Wow, nice! I'll try implementing the Nelder-Mead thing later today. If you could look up your transformation that would also be great.
Ok I dug it up Assuming $v \\in \\prod\_{i=1}\^N \[0, 1\]$ you can map $v$ onto a simplex by mapping each dimension $v\_n$ onto $(\\prod\_{i=n}\^N v\_i) - (\\prod\_{i=n-1}\^N v\_i) = v'\_n$. Don't ask me how I found that, I don't recall. I was just playing around with low dimensional examples and some visualizations and got it to work finally. My paper included the following fairly trivial proofs a) $0 \\le v'\_n \\le 1$ b) $\\sum\_{i=1}\^N v'\_i = 1 (e.g. v' lies on a simplex) c) the map from $v$ to $v'$ is surjective (meaning no point in the simplex is missed) That's sufficient to ensure that you can now use basic calculus on the N-cube input domain to perform optimizations on probability distributions (which was more or less what my paper was about). I was trying (but I didn't accomplish) to prove that Jeffery conditioning minimized KL-divergence. I proved it for 2 variables in the support but I don't think I had to turn the paper in with just that case proved.
Thanks for the review. I'm moving to Poland and just missed it. I will try to go next year :)
Nice! So you take N numbers between 0 and 1 and use them to build this non-decreasing sequence: 0, product of all numbers, product of all but the first, product of all but the first two, ..., product of none (which is 1). Then consecutive differences between these will give N+1 probabilities. I'll try this as well, thanks a lot for the help!
I never thought about it like that but that's an excellent way of thinking about it. As far as I know I was the first to want to do this badly enough to work it out or at the very least I don't know the right words to google to find it. I'm sure there's some better thing that explains why it hasn't ever been needed before but I'm not aware of why exactly. Tell me if you find something better!
Sure :-)
The conduit types are still wrapped in a continuation with \[ConduitT\]([https://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit.html#t:ConduitT](https://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit.html#t:ConduitT)), but due to the inline annotations on the various conduit combinators it seems that this is indeed what is happening. Thanks for the help! Btw, I was doing benchmarking using \`nfIO\` from criterion which caused me to come across this. Is this case something I should mention as an issue? Or is this the expected behaviour?
Thanks for replying /u/ondrap. If I may ask, what were the problems you saw with `certbot`? 
If you don't require that `handle (pure (Right a)) f = pure (Right a)` then `handleA` is really a valid implementation of `handle` for _any_ applicative, right? Doesn't this mean that `Applicative` is at least as strong as `Selective`? Seems more useful to include that law since that lets you express things you otherwise couldn't with just `Applicative`.
So I think the main problem is that the numbers are squished toward zero. But that should be fixable by applying a curve, forcing them to be distributed like [order statistics](https://en.wikipedia.org/wiki/Order_statistic). Something like this: unorderedToOrdered :: Int -&gt; [Double] -&gt; [Double] unorderedToOrdered = f 0 1 where f _ _ _ [] = [] f a b n (x:xs) = let y = a + (b-a)*x^n in y : f y b (n-1) xs It takes N unordered numbers between 0 and 1 and returns N ordered numbers between 0 and 1. From that I can get a distribution by taking consecutive differences. Does that make sense?
**Order statistic** In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference. Important special cases of the order statistics are the minimum and maximum value of a sample, and (with some qualifications discussed below) the sample median and other sample quantiles. When using probability theory to analyze order statistics of random samples from a continuous distribution, the cumulative distribution function is used to reduce the analysis to the case of order statistics of the uniform distribution. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I think the main problem is that the numbers in the sequence are squished toward zero. But that should be fixable by applying a curve, making them distributed like order statistics. Something like this: cubeToSimplex :: Int -&gt; [Double] -&gt; [Double] cubeToSimplex = f 0 where f _ _ [] = [] f a n (x:xs) = y : f y (n-1) xs where y = 1-(1-a)*(x**(1/(fromIntegral n))) It takes N numbers between 0 and 1 and returns N non-decreasing numbers between 0 and 1. From that I can get N+1 probabilities by taking consecutive differences. Does that make sense?
Yes, it is tempting to require that unnecessary effects must be skipped, and indeed one of the earlier versions of the laws included `handle (Right &lt;$&gt; x) f = x`. Unfortunately, this forbids a useful `Const` instance, because it cannot distinguish between `Left` and `Right` and will thus have to skip all effects -- the only allowed instance would then be `handle (Const m) _ = Const m`. This does mean that `handle = handleA` is a valid implementation, but in general it is not the only valid one, as demonstrated by `Validation`. I don't know what to do with the fact that this makes it look that `Applicative` is as strong as `Selective`. Intuitively, this is not the case, because `Selective` has a choice what to do with effects whereas `Applicative` doesn't, but I can't formulate my intuition in more concrete terms.
A less restrictive implementation directly implies a less expressive interface. What I'd personally like, is a way to write functions that I can't express with just `Applicative`, but still be able to analyze their structure. It's a shame about `Const` but it does make sense. Applicative effects can depend on previous effects, but not on previous values. Monadic effects can arbitrarily depend on previous effects _and_ values. We want a middle ground where we can depend on previous values, but in a limited, inspectable way. Since `Const` doesn't actually have any values, it can't satisfy the required interface.
The type of `handle` along with 'skipping if Right' makes me think you're really looking to say that you want `f` to be costrong. This is doable with a `Traversable` constraint, as in `category-extras`. I haven't checked if all the laws hold, but you can at least get "effect skipping" this way: costrength :: Traversable f =&gt; f (Either a b) -&gt; Either a (f b) costrength = sequenceA flipEither :: Either a b -&gt; Either b a flipEither (Right x) = Left x flipEither (Left x) = Right x costrFlipped :: Traversable f =&gt; f (Either a b) -&gt; Either b (f a) costrFlipped = costrength . fmap flipEither handleTrav :: Traversable f =&gt; Applicative f =&gt; f (Either a b) -&gt; f (a -&gt; b) -&gt; f b handleTrav x f = either replace (f &lt;*&gt;) (costrFlipped x) where replace v = fmap (const v) x I hope that's readable. I'm posting from my phone. 
I think the killer is that OP wants this to be sort of in between Applicative and Monad. e.g. they want `instance Selective IO where handle = handleM`.
&gt; implies a less expressive interface I think we might want to "express" three different ideas: (1) Express the requirement that all effects must be performed. This corresponds to `Applicative`. There is no way to distinguish necessary effects from unnecessary in this interface. (2) Express the requirement that unnecessary effects **must** be skipped. This is a stricter version of `Selective`. (3) Express the requirement that unnecessary effects **may** be skipped. This is the version of `Selective` from the blog post. I think both (2) and (3) are interesting and useful. An example where (3) may be preferable: if you have parallelism, you might in some cases prefer to start the evaluation of both arguments of `handle` in parallel, so that you can speed up the error branch. (2) forbids any such optimisations. 
Oh, I get what the intended use case is. I was just trying to point out a possible connection to other known structures, since it still seems somewhat unclear exactly where in the typeclass hierarchy `Selective` would go. 
Without more context I can't tell.
This is fun, but doesn't seem to work :) handle [Left 1, Right 2, Left 3] [(*10)] = [10,2,30] but handleTrav [Left 1, Right 2, Left 3] [(*10)] = [2,2,2] I haven't checked which laws are violated by this, but I hope some are!
Fixed, thanks
If I run a benchmark by doing ``` main2 = defaultMain [ bench "primes_10000" (nfIO (ConduitBench.runPrimes 10000)) ] ``` Then criterion reports that the function takes 34.65 ms (32.65 ms .. 36.85 ms) to run. While if I time `main1 = ConduitBench.runPrimes 10000` using the time command it takes about 4.5 seconds. So it seems that criterion is measuring the post-thunk evaluated `primes 10000` situation.
&gt; Are you using Cloud Haskell for your Backend? I wish! No, our microservices are communicating using json messages, often via websockets. We sometimes put a typed library on top to make exchanging those messages look like regular IO calls.
Oh, you're right. Brain fart on my part. We need a map `f (Either a b) -&gt; Either (f a) (f b) `, that preserves both sides. Then you could recover a sensible `handle`. That map should exist, I feel like. Need to think about it some more. 
&gt; I have an adequate understanding of Haskell up to Monads and New Types (and willing to learn more), but have not used Cloud Haskell or Haskell otherwise for backend services. Would knowledge along those lines be beneficial? After monads, learning how to use [monad transformers](http://hackage.haskell.org/package/transformers) and then how to use them in the [mtl](https://hackage.haskell.org/package/mtl) style would be a must, as we are making heavy use of this style in our codebase. The backend-specific libraries we happen to be using are [yesod](http://hackage.haskell.org/package/yesod) and [websockets](http://hackage.haskell.org/package/websockets), but I think those libraries are easy to learn on the job, I think it would be much more beneficial to focus on more generally-applicable skills like how to use streaming libraries such as [conduit](http://hackage.haskell.org/package/conduit), and some SQL.
It seems to me that `coerce` is not as safe as it's advertised, simply because it relies on programmer annotations to work safely and inferred annotations too permissive. In the `Data.Map` example mentioned in the article, if we forget to add `type role Map nominal representational` `coerce (m :: Map Int Int) :: Map MyInt Int` is allowed, which breaks all kinds of invariants.
&gt; The class I'm taking is on program analysis and verification, which is probably what your company is largely using to find security holes. Not really; we're not helping our customers to find or patch security holes, we're helping them to practice the scenario in which attackers manage to get in anyway. Due to zero-day exploits, the sad reality is that if you're a big enough target, they're going to get in no matter how well you protect yourself, so you also need to know how to kick them out before they do too much damage.
Oh I see. I guess you could argue for an `(a -&gt; IO b) -&gt; a -&gt; Benchmarkable` function or consider running the conduit purely with `nf` instead.
I absolutely do not want to keep the whole of `parsedComp` in memory at any point of time, because it is too large. Is there way a way to do what I have done (the lists derived from `parsedComp` are small, so there is no problem)? I mean something like evaluating and storing one value at a time, without evaluating the whole of parsedComp. Thanks for the response! 
Changing this line (_, fDB', mDB', fDB_n', mDB_n') = analyseAll to (using the `BangPatterns` extension and the `deepseq` library) !(_, fDB', mDB', fDB_n', mDB_n') = Control.DeepSeq.force $ analyseAll should ensure the tuple holds no reference to `parsedComp`.
This might be important for your application, as it can result in security holes.
I will need to derive `NFData` before doing that, right? I'll try that out! Thanks!
I've added a footnote to my blog post, hopefully clarifying this issue: https://blogs.ncl.ac.uk/andreymokhov/selective/#footnotes
If you don't care that unnecessary effects are skipped, `Applicative` already offers the necessary tools. So I think (2) is strictly superior to (3): if I see an instance of `Selective` with (2), then I have learned something interesting and useful about the type that is an instance, whereas if I see it in situation (3) I have learned nothing. Or, to put it another way: without the law, I can't actually trust the `Selective` instance to do anything good, and must read and deeply understand the source every time. Ouch! For parallelism, and for `Const`, I *want* to be forced to use the `Applicative` interface rather than `Selective`, so that I must admit I am okay with doing extra effects.
Also see the RESTLESS vulnerability for more examples of (non-haskell) code with such problems. https://www.danieldent.com/blog/restless-vulnerability-non-browser-cross-domain-http-request-attacks/
Thanks! I do agree that the strict version (2) makes it easier to understand what `handle` does when you see one in the code. Note that if `f` is a `Monad`, you do know that unnecessary effects are skipped thanks to the law `handle = handleM`, so you really only have to dive deeply in the code for a few cases when `f` is not a `Monad`. But this gives you a great bonus -- static analysis! Strict selective functors cannot be statically analysed: they insist that you give them actual values in order to decide which effects to perform, hence both `Const` and `Validation` are lost (the latter is lost only partially: it will need to terminate the static analysis as soon as an opaque condition is met). But let's assume that we have `class Selective f =&gt; StrictSelective f`. What would be additional laws for `StrictSelective`? I actually haven't yet figured it out yet. The following, for example, doesn't work: handle (Left &lt;$&gt; x) f = flip ($) &lt;$&gt; x &lt;*&gt; y handle (Right &lt;$&gt; x) f = x These two laws are contradictory, e.g. when `x = Nothing`.
Haskellers generally know about this, and don't write code that does it. A sum type with a record is one of the things that immediately fails code review at every place I've worked. I do agree that it should not be allowed *at all* by the compiler and that `-Wpartial-fields` should be part of `-Wall`.
Hi, I have a question about the usage of the applicative instance of the Gabriel Gonzalez foldl library let's say I have the data type `data Foo = Foo a b c` and these folds `f1 :: Fold x a` `f2 :: Fold x b` `f3 :: Fold x c` I can build the fold `fFoo :: Fold x Foo` `fFoo = Foo &lt;$&gt; f1 &lt;*&gt; f2 &lt;*&gt; f3` Problem: if my folds return a pair how can i combine them in a general way with the applicative style? `f1' = Fold x (a,a)` `f2' = Fold x (b,b)` `f3' = Fold x (c,c)` `fFoo' :: Fold x (Foo,Foo)` `fFoo' = ???`
Aw, you could have put it in the title that remote excludes anything outside of USA and Canada. I'd love to have a Haskell job but I'm not going to leave Europe.
Loved this reply. Makes a lot of sense. Thank you,
Thank you for posting these articles. Very helpful. I am not yet through with the Haskell book I am reading, but I am getting the feeling that Haskell might not be the right choice to work with. It seems rough when it comes to the build system (well, at least when compared to Rust, which I am also looking at this very moment), and layer by layer of concepts that, at least at a first glance, show the beginner little entry point how to make something 'useful'... you know, a program that does IO.
In general the bullet point stuff stuff is a bit inaccurate: "Cabal is a package specification developed and maintained by the core Haskell group." No. Cabal is a specification _and library_ _and executable_. All three are managed by the Cabal maintainers. "The Hackage repository is a huge repository of open-source libraries for Haskell, managed by the core Haskell group." No. Hackage is maintained by the haskell infrastructure admin team, developed by the hackage developers, and with day-to-day operations delegated to the hackage trustees: https://github.com/haskell-infra/hackage-trustees "The Stackage repository is a compatibility-tested repository of Hackage packages managed by FPComplete, a private company." This is not correct. The repository is managed by the stackage curators: https://github.com/commercialhaskell/stackage/blob/master/CURATORS.md I think that what one can say about FPComplete is that it plays an important role, and in particular devotes resources to the maintenance and administration of stackage. "The Commercial Haskell Group is a group of people and organizations that manage Stack." This is not correct. Stack is managed by the stack maintainers. Stack attributes its license to the commercial haskell group, but the commercial haskell group, to my knowledge, does not really exist in a meaningful way anymore. It is just a very sparely posted-to mailinglist. "If you manage packages in Haskell, you’ll probably end up using Stack." Nope, many people use cabal-install and don't use stack. Many others do use stack. There's no "probably" either way here. 
&gt; "If you manage packages in Haskell, you’ll probably end up using Stack." &gt; &gt; Nope, many people use cabal-install and don't use stack. Many others do use stack. There's no "probably" either way here. Even though these are the only ones we have so far, [this user survey](https://www.fpcomplete.com/hubfs/Haskell-User-Survey-Results.pdf) and also this [other independent user survey](http://taylor.fausak.me/2017/11/15/2017-state-of-haskell-survey-results/#question-23) do make it sound quite plausible that the vast majority of commercial Haskell users would probably end up using Stack and Stackage.
visa vis the relationship of stack to the CHG, let's just take it from stack's own documentation! "Stack is provided by a team of volunteers and companies under the auspices of the Commercial Haskell group." The people who manage stack are under the "auspices" of the CHG, which is a vague statement. But the CHG doesn't do anything directly with regards to stack. The stack team does. This should be uncontroversial! vis a vis the survey stuff, people answered you on this just a few days ago: https://www.reddit.com/r/haskell/comments/8tc8pr/fp_complete_launches_new_blockchain_auditing/e1878ox/
I'd like to use a 32bit version of the current Haskell Platform for Windows, but because it's still only available in 64bit due to packaging issues (on your side?), I had to fiddle around some. :) 
 module Main where main :: IO () main = putStrLn "I'm doing IO!"
OK, I accept that my comment were a bit like a rant. By no means I wanted to talk down the usefulness of Haskell. I just wanted to vent my experiences in the difficulties of the language, and the additional difficulties with the tooling. It may have been worse, but even these days it seems to me (a beginner), that there is a lot of friction for entry level programmers to build projects. 
I was being a bit facetious, I grant that there are some hurdles.
&gt;facetious No worries. Few died on this planet because of jokes.
Interesting. Could you explain why you think a map `Traversable f =&gt; f (Either a b) -&gt; Either (f a) (f b)` doesn't exist? It's not immediately obvious me. 
&gt; Use where like it creates a submodule. Use submodules extensively to structure your code. This is how GHC is written. Some people really seem to like it. I think it would be fine if all the local definitions had type signatures and documentation.
Your link seems to broken on my end.
Think of `[Left 'a', Right True]`. There's no good way to choose between producing `Left ['a']` or `Right [True]`.
That's a great explanation, thank you!
Yes. I was frustrated too to wait until week 8 of a course before I could leave sandbox. In fact I remember looking up myself how to do IO because I didn't want to write in GHCI. In these I'm coming from a different direction. The goal isn't learning Haskell, but being able to use Haskell. I think all given examples were in IO.
You keep posting those links, despite people having pointed out issues with them.
So I had it right the first time... but on accident...
https://gfycat.com/gifs/detail/JointHiddenHummingbird This is a friendly reminder that it's "by accident" and not "on accident". ***** ^(Downvote to 0 to delete this comment.)
Wow I can't catch a break today
Yep, that's it. In fact, if it makes it clearer, apostrophes are allowed in variable names in Haskell, so you can call your variable `x's` if you want.
Furthermore, the tarversable laws ultimately dictate that any result of `traverse` will have the same shape (in the case of lists, length) as the input. So for `[Left foo, Right bar]`, there is no way to use `traverse` to choose between Lefts or Rights, because it would require changing the length.
Hello, I'm new to Haskell and learning about state monad. It is useful. So, I'm now making simple blackjack game with it but got a question. Is there any sophisticated way to compose 2 different type of state monad? for example, there is a card deck and a player holding his hand. These 2 objects are of different types. And now, the player draws a card from the card deck. This action updates the states of both of 2 objects. I have solved this problem by writing primitive state update functions, in the style of "\s -&gt; (a, s)", and making a state monad consists of the 2 objects from those functions. But if there is a way to compose state monads, the code will be more sophisticated, I think. Thank you.
IIUC, GHC can output a very different ABI after minor code changes. I'm not sure if they've fixed the nondeterminism in the ABI or not, but if not then you can't even expect the *same* code to produce the same ABI twice. The ABI includes things like the names of the symbols, the size of data types, the types of functions, etc.. Plus, GHC inlines a ton of code across package boundaries, so changing a dependency without recompiling a dependent could result in the two having different opinions e.g. on how a particular instance works, which would be massively breaking. In short, **do not** expect GHC to output an ABI that lets you swap a lib out with a newer version in place. If a dependency is recompiled, all its dependents must be as well. However, you *can* link dynamically just to save disk space. Linking several Haskell executables against the same tree of dynamic libraries will make the executable binaries smaller in total.
Servant is known to use advanced type level programming and requires many language extensions. There are many other web "framework" for Haskell. Scotty, for example, is much simpler, IMO.
Really done well 
Really done well 
You mentioned the word 'joke'. Chuck Norris doesn't joke. Here is a fact about Chuck Norris: &gt;In an act of great philanthropy, Chuck made a generous donation to the American Cancer Society. He donated 6,000 dead bodies for scientific research.
Floating includes things like complex numbers, which aren't quite the same as `Float`. `realToFrac` gets you what you are asking for in that it provides the most general way to convert something to a `Float`, but you may want to just plumb `Float`s all the way through.
Let's look at your function `singleRoot`. data Roots = Roots Float Float | SingleRoot Float deriving Show singleRoot :: Floating a =&gt; a -&gt; a -&gt; Roots singleRoot a b = SingleRoot (- b / (2 * a)) The problem here is in the call to `SingleRoot`. Your `SingleRoot` constructor is of type `Float -&gt; Roots`. However, you are trying to pass it `(- b / 2 * a)`, which has the same type as `a` and `b`. But according to your type signature, `a` and `b` can be any `Floating` type, not just `Float`! This is why the type checker is complaining. I can think of two fixes to this. Fix 1: data Roots = Roots Float Float | SingleRoot Float deriving Show singleRoot :: Float -&gt; Float -&gt; Roots singleRoot a b = SingleRoot (- b / (2 * a)) Here, `a` and `b` are `Float`s, so `(- b / 2 * a)` is a `Float`, so you can now apply SingleRoot to it. Fix 2: data Roots a = Roots a a | SingleRoot a deriving Show singleRoot :: Floating a =&gt; a -&gt; a -&gt; Roots a singleRoot a b = SingleRoot (- b / (2 * a)) Here, I have modified the definition of `Roots`, so it now has a type variable. Now `SingleRoot` has type `a -&gt; Roots a`. Since `a` and `b` have type `a`, `(- b / 2 * a)` has type a, so `SingleRoot (- b / 2 * a)` has type `Roots a`. Note that `Roots Float` is your original `Roots` type. The problem in the function `dualRoots` is the same. Once you fix those two functions, your code should typecheck. I don't think you're handling the `a = 0` case correctly though.
it seems it's not under drafts put under posts now: https://abhinavsarkar.net/posts/fast-sudoku-solver-in-haskell-1/
The article on the 'fix' function gives an overview of different uses, but does not explain why you would or should use it. I've often written functions using the "go pattern" with 'go' explicitly calling itself. Any reason to use 'fix' besides aesthetic?
&gt; which is more explicit about what the two constructors represent. Ugh, no! `Left` is *exactly* what it represents -- the left type variable. `Right` is *precisely* what it means -- the right type variable.
Don't this way give you "two levels" of errors? You get the explicit errors captured by your custom data type, but all the stuff that can go wrong in `IO` can _still_ go wrong even though you wrap it in `ExceptT`. I see you mention it briefly in "If the monad m isn’t `IO` then we have a good degree of confidence that none of the base exceptions will be present.", but what if the underlying monad _is_ `IO`? I've been wrestling with this one myself, so any thoughts on elaborating on that would be interesting. In your examples I think you should add using `mtl` typeclasses to limit effects for the exception part as well. Making it something like `(MonadError AppError m) =&gt; SomeData -&gt; m SomeResult` and stuff that need `IO` get the additional `MonadIO m` constraint like you did. This gives a great level of control of effects and information in the type signature.
That seems a bit harsh. `Left` represents failure in the `Monad` instance for no other reason that the order of the type variables.
It doesn't represent failure. That's a carry-on from the insistence on interpreting `Left` as failure. It is a *short-circuiting* constructor. Short-circuiting can mean failure, but it can also mean early-success or [breaking from a loop](http://www.haskellforall.com/2012/07/breaking-from-loop.html). Just like monads aren't burritos, `Either` isn't a failure monad.
One way people solve this is by composing the different types of state: drawFromDeck :: State Deck Card addToHand :: Card -&gt; State Hand () newtype PlayerId = PlayerId { getPlayerId :: Int } focus :: (s -&gt; t) -&gt; (t -&gt; s -&gt; s) -&gt; State t a -&gt; State s a focus get set ma = State $ \s -&gt; let (a, t) = runState ma (get s) in (a, set t s) drawCard :: PlayerId -&gt; State (Deck, IntMap Hand) () drawCard (PlayerId i) = do card &lt;- focus fst (,) drawFromDeck focus (\(_,hs) -&gt; hs ! i) (\h (d, m) -&gt; (d, insert i h hs)) (addToHand card) This sort of plumbing is what lead to the development of the various lens libraries, 
Nice! I really like how you showed the various intermediate results. &gt; Also, it turns out that we don’t need to write the back-transform function. `subGridsToRows` is its own back-transform: My favorite sentence in the whole post.
Hey, if you need help getting around actually building stuff in haskell (including build tools and stuff), feel free to pm me and i'll try to help out. I don't think these things are too complicated and there's no reason they should hinder your progress.
That's a damn good reason though, because it isn't just soft documentation that could go wrong or be forgotten, it's guaranteed by definition to work that way because of the way `Monad` works.
I just want to shows a simple but non-trivial web app using Servant for a non-API use case. Someone mentioned writing Zip endpoint in the Servant thread recently, which reminds me of this project I wrote some time ago. I use it to practice Haskell and Servant, so please let me know if I did something wrong or need improvement.
That's all true, in which case That seems a bit harsh. `Left` represents short circuiting in the Monad instance for no other reason that the order of the type variables which is by no means explicit. 
I was a bit generous calling it a "reason". It's not a reason, it's an accident. The current situation is not a design, it's just expedience. Better names (with regard to the `Monad` instance) would be `ShortCircuit` and `Continue`. 
A small nitpick. With the classy prisms approach, you can have done :: (MonadError AppError m, ...) =&gt; m () instead of done :: (MonadError e m, AsFrobError e, AsWebError e, AsFooError e, AsWhateverError e, ...) =&gt; m () as long as `AppError` has instances for those classes.
Using type variables to reason about things is super common in Haskell, Monad Either fits perfectly into that. Particularly with the functor hierarchy, I knew how Miso's Effect instances worked based purely on the type variables.
I think we've maybe got a little off track. The original discussion is whether data Either a b = Left a | Right b communicates just as well as data Result a b = Error a | OK b as a type to represent possibly failing computations. It seems that you think it does. I disagree. (NB all the benefits you state about reasoning about type variables hold just as well for the latter as for the former)
You can get past the composability and polymorphism problems with [something like ether](https://github.com/int-index/ether). There's plenty of ways to work around those issues. I'll concede that the branching is a problem but IIRC it's not such a big deal because of some of the things STG can do. I'd love to see a benchmark. Haskell [can also get quite nice actual exceptions](https://www.well-typed.com/blog/2015/07/checked-exceptions/).
Indeed! Not sure now why I got stuck here. So, in case of `Either` traversing with an applicative allows one to detect/collect errors, whereas `handle` essentially traverses by fixing all errors and always returning a `Right` value.
&gt; The original discussion is whether &gt; data Either a b = Left a | Right b &gt; communicates just as well as &gt; data Result a b = Error a | OK b Right. I like the former much more for a variety of reasons. Of course if `Either` was ONLY ever used for short circuiting errors then the latter would make sense. But `Either` is so much more, it's a general purpose sum type, a short circuiting on failure type, a short circuiting on success type, or whatever else you want to do with the various instances. Realistically to have any "improvement" in the situation you would need both the standard either, and ALSO `Error a | OK b` AND `Done a | TryAgain b`, or perhaps `ShortCircuit a | Continue b`. ALL of these types would have the exact same monad and the like instances. So I honestly think Haskell's current situation is pretty much optimal with regards to Monad Either, given Haskell's semantics that is.