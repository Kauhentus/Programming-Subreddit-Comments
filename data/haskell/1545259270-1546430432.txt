&gt; It claims to be resilient to unplugging the power cord, but I doubt that, as it's not resilient to Ctrl-C: This is rather unfair, as the issue linked is resolved, even if it hasn't been marked so. Partially written entries are ignored, which is about the best one can do.
I have been busy and have only gone through a few chapters. It has held up perfectly fine thus far. I would say the quality seems good. Hope you can get some reviews from readers who have gone through more of the book though.
`acid-state` data loss was actually the major reason I moved from `darcs` on `patchtag` to `github` back in the day. I lost something like 10 repositories, right as I was switching machines, learned `git` and never looked back. At the risk of sounding FUDdy, this has colored my impression of `acid-state` ever since. =/
Start here: [https://wiki.haskell.org/Performance](https://wiki.haskell.org/Performance) Well-Typed has great course material on this. It's possibly reserved paying clients, but you could always ask /u/kosmikus or /u/adamgundry.
&gt; I think some criticisms of acid state don't entirely acknowledge the motivating factors behind it, which are 1) to provide a way of persisting the models and structures that you actually want to use without forcing you to write your entire application around SQL and 2) to remove the need for an interface, that will of necessity be bug prone and awkward, between the richly typed internal world of Haskell and the sparsely typed world of an external persistence layer. I think a lot of people agree this is a serious problem. For evidence we just have to look at the proliferation of libraries to interface with SQL databases. Despite excellent attempts even the best ones have drawbacks. So it's good that experiments like acid-state exist to explore doing things differently. However, there's a difference between "this is a good experiment", and "yes, I think it's a good choice to bet multiple people's livelihoods on this". DB choice can cause real problems for companies. Criticism is useful to remind people that though something may be a cool project, it still has drawbacks and (and worse, unknowns!) that aren't present for boring solutions with millions more users like Postgres / SQLite.
I don't really know much about acid-state, or what an entry even is. What is an entry, and does it ensure that you will simply get the previous version of the state back in the event of ctrl-c or power failure? It kinda sounds like you're saying you'll get incomplete data instead, which is *not* acid.
Out of curiosity, why Redis and not acid state?
Well if a write isn't complete, it rolls back to the last complete write. That sounds ACID to me?
It's not resolved. The commit you're referring to is in a fork of `acid-state`, and the discussion in the thread indicates that fix won't be merged in.
Agreed on the whole, I was sort of responding to the last sentence of the posted link: &gt; The benefits are just... *really* small, and the costs are massive. It's not a good choice. On the general question of whether acid state should be used in production I think the answer should be 'it depends'. I have been using it in production for 4 or 5 years without any significant issues and in that time I have learned to work around its flaws. But I think it is the kind of package that you should get to know pretty well before committing to it, so I would hesitate to recommend it unreservedly, even for projects where it might otherwise be a good fit. I wouldn't have chosen it for cardano, for example, unless I was planning to put some good tooling around it so that developers wouldn't misuse it. Some of the gripes in the post - that you get problems if you change acidic functions for example - demonstrate a lack of understanding at some level of the development process. Which is fair enough, because I don't think this stuff is particularly well documented, but they aren't so much an argument for not using it as an argument for not using it incorrectly.
Ahh alright, as long as it does that rollback.
An entry is the log of a specific function / event. Acid state stores a serialised log of all events (effectively just a reference to a specific state monad function along with arguments). When you restore a database it replays all the events stored in the log. This means that yes, you will get the previous version of the state back - the partially written entry will be treated as if it never happened, which is the correct behaviour I think.
Well that sucks! Do you recall what caused the data loss? 
Redis works with everything and damn near everyone had a nice integration with it. Once it's in redis you can dump it to nginx or to elastisearch or to grafana or why not one of the million language libraries that integrate with redis? It makes things a lot easier from an interop situation where the Haskell code might be just one part of the picture. Acid state is purely haskell, with all the benefits and drawbacks that brings.
In general, the answer to "why is it so hard to make reasonable errors" is usually overloading. The more overloading you have in your language, the more ways something can be interpreted, and therefore, the harder it is to give a reasonable error message. This uses "overloading" in a broad sense, though. In particular, Haskell has: * Partial application. Because applying too few arguments to a function might be intended as a partial application, the error message is generally bad. * Parametric polymorphism. This limits the information the compiler has about intended types. * Ad hoc polymorphism. This tends to make things even worse. Code that's heavy on type classes produces some of the least helpful error messages. If you want better error messages, your first tools should be to fix more type information by adding type annotations (especially for top-level definitions), and avoiding overly general types when you don't need them. This phenomenon isn't unique to Haskell. All major C++ compilers, for example, give notoriously awful error messages for template-heavy code. It's just a question of how much the compiler knows about the code, and how much it's left to infer through many layers of indirection (and therefore not realize when something went wrong until it's layers of indirection away from the cause).
That's simply untrue. Recursion-schemes was designed so that you don't have to rewrite existing data in order to provide it's instances.
&gt; Two of them, though, are not really fair &gt; &gt; On keeping everything in memory - yes, acid state is limited to either high memory environments or smallish databases. But for many projects this might be entirely acceptable, and the performance benefits might be worth the cost of an extra 8GB of RAM. It is very situational is my point, and not an argument against the approach in general. A company [literally switched from Haskell to Go](https://making.pusher.com/latency-working-set-ghc-gc-pick-two/) because GHC's GC didn't handle large working sets well. Keeping your entire dataset in memory isn't a good idea when the GC has to walk it all of it. A commenter on GitHub pointed out that you can use `compact-regions` for the acid-state main database, so you can avoid the above problem if you're willing to essentially do manual memory management on your database. Sure, there are apps where performance just doesn't matter, and the dataset will always be tiny. In that case, any slow or resource-intensive database is just fine. But I don't really think it's "unfair" to say "`acid-state` has serious perf issues in these ways" when your rebuttal is essentially "sometimes perf doesn't matter." &gt; On querying efficiency. A similar objection could be made to any relational database if your data isn't structured in such a way that the database can easily construct queries against it. In practice what people end up doing is rewriting their model to fit their database, rather than their database doing an amazing job of efficiently querying any arbitrary structure. I'm not really sure how my claim is unfair at this point. I mention how `acid-state` encourages you to work (single big nested record), I talked about how you can work around those problems (record of `ix-set` "tables") and get pseudo-relational performance and flexibility out of it, and I mentioned the downsides of doing so. In my experience, the relational model is almost always the right way to deal with data. It is occasionally not -- sometimes graph databases, append-only distributed logs, document stores, or time-series databases work out better for very specific use-cases. It's *only* benefit is that you can use ordinary Haskell types as your data model, without having to worry about making them conform to tables or any other way of modeling data. If performance doesn't matter, then who cares if you're `O(n)` in the size of your entire data store (or `O(n^2)` if you need to do a pessimal join). If performance does matter, then `acid-state`'s gonna require you to massage your data into a flexible format for querying, and then you're right back to the whole "I have to care about how my data is stored" thing that `acid-state` is supposed to protect you from. &gt; 2) to remove the need for an interface, that will of necessity be bug prone and awkward, between the richly typed internal world of Haskell and the sparsely typed world of an external persistence layer. Nope. Not at all. In fact, it makes it even worse! With Postgres, or Mongo, or Neo4j, I have some "neutral" database program that can view and query all my data. It may not be in Haskell-types, but it's some structured format, and I can poke it, prod it, etc. I can even interact with it from other languages! We do have a downgrade from Haskell types to Postgres types, and that is unfortunate, but that's a cost of translating to some other language. But with `acid-state`, we go from Haskell-types to raw binary blobs. The external persistence layer is a file of binary data, and the *only* way to view it is by opening the database with exactly the Haskell program that generated it, or one that has a successful migration path defined. There is no way to independently view this data. There is no way to independently verify it.
Have seen them already. Records containing a super-set of fields with a bunch of `Maybe`s. It is left up to the application developer to figure out which field of which API response will by a `Just` vs a `Nothing`.
“Novadiscovery” link at the start is broken for me
I'm an utter newbie to Haskell (like 4 days) and I was wondering if there was any "optimize" this little function (to be more idiomatic?) canReachEnd :: [Int] -&gt; Bool canReachEnd list = sum list &gt;= length list I don't know if this is even worth pursuing but I would prefer to not see `list` in my function (that's partial application I think) There were patterns like this in the Learn You A Haskell tutorial's chapter on partial application, where repeated use of a function parameter was replaced with partial application. Is there any pattern in Haskell that allows me to use two functions on one list like that, or is that just not possible? (This is my attempt at Daily Coding Problem #192 btw)
What I meant was: stack-installer (sh) -&gt; stack (haskell binary) -&gt; GHC For ghcup we have: ghcup (sh) -&gt; GHC Rewriting ghcup in haskell, which some people seem to think is worthwhile, would then require: ghcup-installer (sh) -&gt; ghcup (haskell binary) -&gt; GHC which makes little sense, since ghcup is in functionality very slim.
&gt; I don't know if this is even worth pursuing but I would prefer to not see list in my function It's not worth pursuing, especially if you are new. That said, if you wanted you could write your function as: canReachEnd :: [Int] -&gt; Bool canReachEnd = liftA2 (&gt;=) sum length It won't optimize it exactly though, it will still (e.g.) traverse the list twice. For that, you'd need something like http://hackage.haskell.org/package/foldl
`Scope` is a [`Free` monad](https://hackage.haskell.org/package/free-5.1/docs/Control-Monad-Free.html#t:Free) for the functor `f :+: Const Int {- Bound -} :+: (,) (Int, n) {- Binder -}`.
It is, kind of, but bind has to shift indices of substituted terms, so you can't use Free.
Oh I see, that's too bad. I'll try again!
Splitting off the I/O manager is tricky as you'll create a three way mutual dependency between base, rts and the new I/O manager package. Which means more dangling symbols and more hacks on platforms where shared libs aren't allowed to have dangling symbols. 
To me, the bigger problem is expressing queries. It's really damn hard to offer a haskell interface that is both typesafe and able to express as many features of the underlying database implementation as possible.
yeah, ive been wondering about an easier/better way to do it.
It's a useful observation!
My vague recollection is that it was an unclean shutdown.
Doubling down on the free monad idea, perhaps we can keep the `Free` representation, but define substitution differently, parameterizing it by the way each constructor affects the context, I'm thinking of a function `(&gt;&gt;=?) :: (forall a. f (Int -&gt; a) -&gt; Int -&gt; f a) -&gt; Free f a -&gt; (a -&gt; Free f b) -&gt; Free f b` (where `f` stands for the whole sum with `Bound` and `Binder` above, and the `Int` gives the "current DeBruijn level"). There may be a more general class of operations `forall a. f (g a) -&gt; g (f a)` with properties that have been written about but I have no idea where. Or, thinking about tracking the DeBruijn indices in types, similar ideas might motivate some kind of "free indexed monad"...
Thanks, should be fixed now
That looks cool :) &amp;#x200B; Indeed, we need a custom server, which is a bit annoying. But in our case that was kinda mandatory anyways because we wanted the process of adding new results from the CI to be a simple \`aws s3 cp\` or a \`POST\` request with the new results (unless you have a nice way of doing this with a static page, in which case I'm interested)
Google scholar only has three citations for that article, so I don't think there are responses from basically anyone :-) (https://scholar.google.com/scholar?hl=en&amp;um=1&amp;ie=UTF-8&amp;lr&amp;cites=6738691474593962450)
`Just :: a -&gt; Maybe a`
 f2 x y = if x then y x else Just means f2 x y z = (if x then y x else Just) z which is the same as f2 x y z = if x then y x z else Just z
It's because you left your `Just` hanging. `Just` is a function that takes an argument of type `a` and return something of type `Maybe a`. Since the outcome of `f2` must have the same type no matter what the result of the `if` test is, then it propagates the signature of `Just` to `y`. If you meant to return `Nothing`when the test fails, then change your `Just` to a `Nothing`: `f2 x y = if x then y x else Nothing` Otherwise, if you meant you have a default return value for when the test fails, you need to specify that value: `f2 x y = if x then y x else (Just defaultValue)` 
You could use a fold where the accumulator is a pair (total sum and length) so it only traverses the list once, then you compose it with uncurry &gt;= to return the bool applied to the pair: canReachEnd :: [Int] -&gt; Bool canReachEnd = uncurry (&gt;=) . foldr go (0,0) where go n (sum, len) = (sum + n, len + 1) The initial accumulator is (0, 0) so that it also works as expected on the base case, i.e., empty list. Hopefully that helps!
&gt; ease of multithreading with phones doing 8 cores Concurrency != Parallelism. The GHC runtime's scheduler is perfectly capable of running multiple Haskell threads on a single core. &gt; with DOM usually being resource hog that can't be parallelized The Web APIs to mutate the DOM can't be invoked in a Worker, so the "rendering" calls can't be parallelized indeed. However, if you're writing a virtual DOM framework, it's possible to parallelize the workload of producing/diffing the virtual DOM trees. Different web apps have different workloads, and when the bottleneck isn't in the rendering part, parallelism still helps. &gt; Webassembly doesn't support concurrency yet V8 already ships thread/atomics related opcodes which can be switched on by `--experimental-wasm-threads` flag. See the tracking [issue](https://bugs.chromium.org/p/v8/issues/detail?id=6532) for details. In short, multiple wasm instances can run in distinct workers and synchronize via an imported linear memory backed by a `SharedArrayBuffer`.
Adding new results is for me also a matter of copying new data files; the "frontend code" (the mess of Javascript) does not need to be updated. All it does is read an index file somewhere, which then points to various other JSON files (in a bespoke format) that define the results. The way it works is that we keep some [raw data](https://futhark-lang.org/benchmark-results/), one JSON file per commit, which is produced by CI jobs and copied to a central location. This is then refined by a script run in another CI job to [processed data](https://futhark-lang.org/benchmark-results-processed/), which is then also copied to some public location. This location is known by the frontend, and read on every load (modulo normal HTTP caching). Presumably in contrast to Tweag.io, we're a small research group with no ops capability and little competence in systems administration. Hence, we wanted to minimise our infrastructure and the number of moving parts, and in particular wanted to avoid having to maintain new servers or systems. Our solution, albeit limited, depends only on our existing CI and static HTTP servers.
The comments in [`GHC.Base`](https://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.Base.html) is a good resource.
&gt; The Web APIs to mutate the DOM can't be invoked in a Worker, so the "rendering" calls can't be parallelized indeed Isn't possible for two threads to modify two different DOM branches?
Thank you! This is quite comprehensive and neatly explains what the foundation is and what's build on top of it.
https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Functions_and_classes_available_to_workers
On performance: the comparison between acid state and, say, postgres is not just about memory performance. It is memory vs doing disk IO, query generation, interfacing with an external program on every query, and marshalling between Haskell types and db types. My perception has always been that this was in favour of acid state, despite the problems of GC, but I haven't actually benchmarked them against each other directly so I may be wrong. I would be interested to see a benchmark if you have one? On querying: Your paragraph on this concluded "reinventing SQL poorly so that you can use less efficient data structures in Haskell". The extent to which this is the case will depend heavily on your model and what kind of queries you want to execute. Your counter is that in most cases the relational model is the right way to handle data. But why do you say that? I suspect that part of it is precisely because we currently have access to highly optimised relational query databases, not because the relational model somehow maps so beautifully onto most problem domains. You are right, of course, that there will always be some trade off between modelling accuracy and performance. I'm just not convinced that it is trivially obvious that the optimal trade off is almost always the relational model, and I think it only seems that way because we have good implementations of relational models to work with. This is sort of a circular argument I think. &gt; Nope. Not at all. In fact, it makes it even worse! I think overall that not having an external view on persisted data is the most serious problem with acid state. But that is not what I was referring to - the cost of the acid-state/binary interface is in external viewing, which may or may not be a requirement. The cost of the Haskell-SQL interface is present in every interaction with the database - it is right at the core of everything you are doing. 
Have you tried advertising on Functional Works?
I'm not suggesting rewriting ghcup in Haskell so I don't know why you're saying this. The stack installer is approximately the same size as ghcup, both POSIX, so whatever argument is being used against writing ghcup as a POSIX shell script, must also be applied to the stack installer. In my opinion, a POSIX shell script, and a Windows .bat equivalent, is the ideal delivery mechanism for ghcup.
Slightly harder to see what's happening but quite neat: canReachEnd = (&gt;=0) . sum . map pred or a monoid version, with &amp;&amp;&amp; from Control.Arrow: canReachEnd = uncurry (&gt;=) . foldMap (Sum &amp;&amp;&amp; const (Sum 1))
These are good reasons for sure.
Sounds very plausible!
Thanks a lot. I did not known the Worker API. However GHCI does not use that API, so the ghci threads have no such limitation. Another question is if the thread block the entire DOM when it is updating some branch. I think that this is unlikely. The Worker limitation is more like a functional requirement of the javascript philosophy rather than a tecnically insurmountable limitation. But that is only my opinion. 
Maybe that is why I loved Smalltalk so much... everything is an object and object can only communicate via messages. That nice category-theory-ish way of thinking is enforced by the language.
Sounds great, but what they have done with GC? There is no GC implementation from WA itself AFAIK.
&gt; None of this is compiler-checked, and it's caused a major and difficult-to-fix bug in almost every single release of cardano-wallet. How could such a terrible design decision happen in the first place? Is the rationale for this decision known? Is there even a process for vetting dependencies ? Has it been considered to use a more commonly used storage technology such as SQLite which has become the industry standard for embedded databases?
Currently the storage manager only allocates new space without reclaiming space. But a proper GC is among our top priorities, and we're working on it :)
ha, I see, no-op GC :D Are you working on it yourself or it somehow related to WA discussions/proposals?
Is this related to GHCJS in any way?
I see, thanks a lot :)
This is incredibly exciting!
It might complement or make unnecessary the GHCJS project, but I doubt that will happen for a very long time (as GHCJS is pretty well established and this is new and untested).
It'll be a homebrew gc instead of using wasm's gc opcodes directly. We once [considered](https://github.com/tweag/asterius/issues/35) wasm gc though, see linked issue for details.
Thanks for your work, keep going guys!
Not yet. But we're totally open to that possibility :) Also me and Luite had pleasant discussions in St.Louis this year, so probably "related" in that sense?
Could you elaborate on why you are emitting from Cmm instead of starting from the Stg IR. Did you consider the alternative? Is it possible to reuse some of the already existing Cmm code used by the native backend? If Cmm is used as a starting point, the runtime can probably behave more like the native runtime. However there is also the WebGHC project, which directly compiles from native code. In contrast, going from Stg might allow more fine tuning for WASM and would allow to reuse the GC infrastructure. In particular, implementing a performant GC with generational collection, concurrent mark and sweep etc, is lot of work and needs a lot of code increasing the size of the binaries. So it seems, the choice is between easier/more direct codegen+writing custom gc, versus more involved codegen from Stg+using WASM GC.
If you were to decorate your function w. it's intended type signature, I believe you'd end up with a compilation error. That might be wise in the future. I could see in a large project, where the usage of f2 causes an error but you don't get the direct benefit of knowing f2 was incorrect in the first place, you just see that your usage of f2 is incorrect.
You can also use semicolons in place of new lines. For example \`f :: Int; f = 4\` works in ghci.
&gt; Is it possible to reuse some of the already existing Cmm code used by the native backend? We already compile and use `.cmm` code in `rts`, with minor patching of course. &gt; Did you consider the alternative? We did. Short reason: it's *easier* to start from Cmm, since it's more bare-metal and maps to wasm much more easily than STG. Also, for performance, we don't lose ghc's own optimization passes on Cmm. Using Wasm GC doesn't necessarily indicate starting from STG; we need some patches in GHC's backend to allow hooking into StgCmm and changing some bits though; a more pragmatic choice at this point is starting from a homebrew gc, but the other possibility is not closed yet.
Not a pitfall, but instead an insight that has only grown stronger with time - **composition** is at the heart of functional programming. Composition is important because you can build complex behavior out of simpler parts. This allows you to think about a *single* problem at a time, and then glue the solutions together. See [Why functional programming matters](https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf). As functional programmers, we can compose functions, we can compose data types, and we can compose computations. For each of those categories of things, we have tools to ensure that they can be composed. For functions, we have currying and partial application to make sure that every function is a one-argument function, ensuring that they can be glued together. For data types, we have algebraic data types. For computations, we have monads. The above is a summary of [The Power of Composition](https://youtu.be/WhEkBCWpDas). Composition is important in every language / paradigm, but I think it is particularly important in FP because it gives you the raison d'etre for other concepts that newcomers seem to quickly gain a distaste for.
my issue is line length, i only used python but i'm convinced that line length is a universal standard in coding. can't have super long lines that you have to scroll..
&gt;what situations really call for Seq over list &gt; &gt; "a finite number of sequential elements, providing fast access to both ends of the sequence as well as efficient concatenation." Basically `Seq` is always better than `[]`. Both access to head, tail and random indexing are much more efficient than with `[]`. The same goes for random modifications. The reason `[]` is so ubiquitous and useful however is that in Haskell it is often used as basically a memoizing version of an `Iterable` or `Stream`. Specifically the power of `Iterable` and similar interfaces is that elements are created lazily as needed. These structures are rarely accessed or modified randomly but usually wholesale (the entire structure) with functions such as `filter` and `map`. For these cases `[]` is much better than `Seq` because `[]` uses much less "bookkeeping" structure per element than `[]` making modifying traversals much faster (specifically tose that change the amount of elements. In addition GHC contains a lot of optimizations specifically targeting stream-like modifications to `[]`. Also unlike `Seq` `[]` can actually be infinite which is sometimes useful for instance in a simple enumerator like `enumerate = zip [0..] :: [a] -&gt; [(Int, a)]`
The first talk was great; going to see the second one once I get a bit of time. Thanks for the links.
I think this is about the cleanest you can get: canReachEnd :: [Int] -&gt; Bool canReachEnd = (&gt;=) &lt;$&gt; sum &lt;*&gt; length 
Thanks for the awesome work! On a side note, the linked TODO example at [https://www.tweag.io/wasm-todomvc/](https://www.tweag.io/wasm-todomvc/) is not working.
How many "primitives" (or "assembly instructions") does Haskell =&gt; WASM need to support before it is reasonably complete from a compiler perspective?
For anyone curious - here's the JS file that has been generated - https://www.tweag.io/wasm-todomvc/todomvc.js - it's ~360k uncompressed and ~52k compressed.
Hm. Works for me. What error do you see?
The linked example doesn't work for me: https://www.tweag.io/wasm-todomvc/#/ I can type in the input and click checkmark on items, but I can't switch from active/all/completed or add new items or mark things as completed.
Ah, our emitted code currently relies on `BigInt`, and SpiderMonkey is still working to deliver it.
"For reference, 30L in INR is about 42,000 USD." Requirements: We are looking for someone who has extensive Haskell experience architected large scale applications in Haskell an understanding of the current best practices for writing production quality code knows how to optimise Haskell code for performance can guide less experienced Haskell developers on the team. Moving to India is not a selling point. Good luck.
I assume you mean primops in `GHC.Prim`. I'd like to roughly group them into three kinds: 1. Relatively pure primops, e.g. arithmetic, load/stores, etc. They map to their wasm opcode counterparts directly, require little assistance from runtime. This part is mostly already working well. 2. Side-effecting primops that doesn't necessarily involve multi-threading. They are backed by rts cmm/c code and we are gradually improving on this front. 3. Thread-related primops: MVars, STM, Sparks, etc. We don't have Haskell multi-threading yet; we surely will implement threads some time from now, but before that, we'd like to make sure 2 is reliably delivered, and gc is functioning. From a compiler's perspective, if you go to the extremes and only allow importing `GHC.Prim` and writing fib of unboxed ints, then other than a bunch of rts interfaces, we won't need to implement too many primops to get a result. I won't call it "reasonably" complete though :)
When building an `Element` you get back an `IO JSVal` so I take it that it's currently impossible to write typesafe ala typescript native javascript code? I think were I'm getting at is that I'd expect to get back a `newtype JSElement = JSElement JSVal` with associated lenses to access the parts that make it a [`HTMLElement`](https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement). Any plans on something like that?
The question is about using web assembly which runs faster than js, and yes the idea is to give right of way to DOM updates deferring computations. I heard that workers are fairly useless unless you decided to go mine concurrency on browser.
I assume sharedArrayBuffer efficient enough that you would run computations outside the DOM thread?
Yes. It is a little bit confused when browse Haskell info resources. They are decentralized. It need some other way of study then used before. Maybe the way is more open and depends on eager`s patience and perseverance. After some milestone Haskell will be revealed and you are in. I suppose it may happen after first Monad understanding.
Is it a lot?
Javascript error: ReferenceError: BigInt is not defined\[Learn More\] Also, I get a 404 for the learn.json file when loading the page. Marking the task as Done works, but anything else doesn't. I'm using Firefox 63.0.3 on Windows, btw.
There's noting to do the rollback *to* - start up involves replaying events since the last snapshot. If the incomplete event is ignored, the state is simply unchanged.
I made a small lib a while ago, which has a pure js impl if BigInt is not available. Furthermore small integers are optionally stored as unboxed numbers. See [https://github.com/minad/bigint-unboxed](https://github.com/minad/bigint-unboxed). However I guess you would rather want a lower-level lib for wasm, maybe gmp/libtommath compiled for wasm. Why do you use BigInt? Do you always cross the wasm/js language boundary for integer operations?
Yes, if you have a different type you can do different things. My example did not include a typeclass constraint, so what I said holds. I don't understand what you are trying to say. I will intentionally exaggerate the point you seem to be making to explain how I understand what you are trying to say. It comes across as if you rebuttal is "Yeah, but if we know it's an int then we can create two from thin air". I know, but in my example it is an a, not an into nor a (Default a) =&gt; a. &gt; Aside from the fact that the typeclass would be present in the function type Going back to the example of reverse, there is no typeclass constraint so I don't understand why you say that there is. 
We are in agreement. I have worded things very poorly.
That does look like what I'm looking for actually! Because this is for practice, though, I'd prefer to know why I'm doing something that works, not just make it work. Do you mind summarizing what "applicative style" means, because I have not yet encountered that as of yet. Thanks again!
Whoa yeah that is a bit harder to understand. What exactly is \`pred\` doing? It does look better than mine, but I really don't get how it works 😅
I was thinking of something like that actually! I don't know why I thought you couldn't use a tuple as an accumulator. I come from a Rust background, so I guess I just didn't think tuples had the same flexibility in Haskell. Haskell never fails to both surprise and impress me. Thank you for your answer!
JS is single-threaded outside Workers, and the DOM is only available on the main thread, so you can't have multiple threads modify the DOM in parallel. Any concurrency in DOM manipulation would come from the GHCJS runtime's task scheduler splitting main thread time among multiple concurrent tasks on a single thread.
What the hell is anyone doing with the DOM that needs 8 cores?
Ah I see. I kinda get what the \`liftA2\` function is doing, but I tried looking at its signature, and I don't really understand some parts of it. liftA2 :: (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c So I can see that you passed `(&gt;=)` as the `(a -&gt; b -&gt; c)` because it's kind of a binary operator That takes types `a` and `b` and returns type `c` Then you passed `sum` and `length` as whatever `f a` and `f b` means? I can see how the types match up and everything, but I don't get what the type "is". Does `f x` mean a function that takes type `x`? Or is it something else? Nonetheless, thank you very much for your response! This was such a small program that I wasn't looking to any external dependencies, but at least now I know that that library exists. Thanks!
`pred` is just predecessor, i.e. subtract 1. `map pred` subtracts 1 from each item in the list. So it works because checking if `sum [x, y, z]` is &gt;= 3 is the same thing as checking if `sum [x-1, y-1, z-1]` is &gt;= 0. (If you're a beginner you should probably ignore my second implementation until you know about Monoid and Foldable.)
Oh wow yeah that was literally the second function in my tutorial. I totally forgot about it. That solution is absolutely genius, I never would have thought of it. (And you're right about that, I have no idea what those are)
I'm using it for ` f ~ (-&gt;) [Int]`. Like I said, don't worry about it until you get more comfortable with Haskell, particularly applicative functors.
Fortune favours the bold ... and patient.
I've had reflex+ghcjs projects use 1.5 mb. 
Yeah I still don’t get it, but you’re definitely right I just need to get more experience with Haskell. Thanks you very much!
I heard that -at least in chrome- execution of events is multithreaded although the user has no access to the thread mechanism. Much like a web server is multithreaded, besides each single handler is not. So ghch should make use of this.
Had the pleasure of meeting and working with the Hasura team in Chennai a few years ago. They were a very fun and welcoming group, doing really interesting work.
It seems that it uses BigInt, that is not available in Firefox for the moment. See the answer to Leshow above. For the moment you have to test it in Chromium.
It seems kind of obvious, but I actually think this is a great way to get going with a GC language. As long as you don't run out of memory it works amazingly well as a GC! :) BTW, scala.native did something very similar before going to the Immix (IIRC) GC.
Yeah, but still it feels a bit not natural anyway - possibly I'm a bit old fashioned, id you get something - don't forget to return :)
I was on mobile, so I didn't provide a full explanation. Let's see if I can do better with a full-sized keyboard: You recognize that `a`, `b`, and `c` are type variables, meaning they can stand it for different types at different times. `f` is another type variable. But, you recognized that `f` isn't used like a type on it's own, instead it's used like a function and `f a` or `f b`. Depending on your background you might have heard of "generics" (java.util.ArrayList) or "class templates" (std::vector) or even "parameteric types" (the list type, []), because of how `f` is used when know it's one of these; it doesn't have normal values itself, instead acts like a type-level function: you give it a type (java.lang.String, int, or Integer, resp.) and it outputs a type (java.util.ArrayList&lt;java.lang.String&gt;, std::vector&lt;int&gt;, or [Integer]). So far my examples of what `f` are all "containers". That's a fine intuition for now, but not entirely true. Now, I think you left out an important part of the type signature for `liftA2`. `liftA2 :: Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c` That `Applicative f` part is a *context* or *constraint*. It tells us some interesting things about `f`. `Applicative` is a standard type class (think interface, for now), that extends the standard `Functor` type class, with two new operations `pure :: Applicative f =&gt; a -&gt; f a` and `ap :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b`. The `Functor` type class provides `fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b`. `fmap` certainly aligns with our earlier "container" intuition. We can just apply the function to each thing we contain, and indeed you'll find most containers in Haskell have a `Functor` instance. `pure` also aligns with the whole container idea. In many cases, it's a way to create a single-element container from just an element. Some containers have this, but some don't; for example, a key-value map/dictionary also needs a key for that single element. `ap` is a little more nebulous, although it is very important for what we are doing. When using the container analogy, it's taking the functions in one container and applying them to the values in another container to generate a container of results. Now, just reemphasizing that while we can build analogies to containers, `Functor` can be provided for *anything* that lets us define `fmap`, and `Applicative` can be provided for *any* `Functor` that also lets us define `pure` and `ap`. --- Haskell syntax has both *operators* that are called infix like the `+` in `1 + 2` and *functions* that are called prefix like `compare 3 5`. But, it also lets us convert one to the other and vice-versa. To call a function infix we surround it with backticks ``2 `compare` 3``; to treat an operator as a function -- either to call prefix or to pass to another function -- we surround it with parens `(+) 5 8` or `foldr (+) Z`. We can also do the later with `-&gt;`, the name of function types. So, `(-&gt;) Int Bool` is the same as `Int -&gt; Bool`. It turns out that `(-&gt;) e` is an applicative functor: instance Functor ((-&gt;) e) where -- fmap :: (a -&gt; b) -&gt; ((-&gt;) e a) -&gt; ((-&gt;) e b) -- fmap :: (a -&gt; b) -&gt; (e -&gt; a) -&gt; (e -&gt; b) fmap f g = \x -&gt; f (g x) -- fmap = (.) instance Applicative ((-&gt;) e) where -- pure :: a -&gt; ((-&gt;) e a) -- pure :: a -&gt; (e -&gt; a) pure x = \_ -&gt; x -- pure = const -- ignore the environment -- ap :: ((-&gt;) e (a -&gt; b)) -&gt; ((-&gt;) e a) -&gt; ((-&gt;) e b) -- ap :: (e -&gt; (a -&gt; b)) -&gt; (e -&gt; a) -&gt; (e -&gt; b) ap gf gx = \x -&gt; (gf x :: a -&gt; b) (gx x :: a) -- feed the environment into both the function generator on the left -- and the value generator on the right. Now, unless you think function *contain* their return values, this isn't a container, but it has `Functor` and `Applicative` instances. I used the `Applicative` instance for `(-&gt;) [Int]`. I treated `sum :: Num a =&gt; [a] -&gt; a` as `sum :: (-&gt;) [Int] Int`. I treated `length :: [a] -&gt; Int` as `length :: (-&gt;) [Int] Int`. I treated `(&gt;=) :: Ord a =&gt; a -&gt; a -&gt; Bool` as `Int -&gt; Int -&gt; Bool`. When I fed them into `liftA2` it worked because there was a consistent assignment of all the variables in it's type with specific types (a ~ Int, b ~ Int, c ~ Bool, and f ~ (-&gt;) [Int]) and the compiler was able to find instances to satisfy the context `Applicative ((-&gt;) [Int])` (this instance is in the standard library). `liftA2` is just a combination of `pure` and `ap`: liftA2 :: Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c liftA2 x l r = fmap f l `ap` r Since `ap` for functions "just" feeds the environment to each side separately, the `liftA2` version still processes the list twice. --- As a side note, `liftA2` is like `fmap` but for binary functions instead of unary ones, but it is also exactly has powerful as `ap` (it is inter-definable with it). Applicative functors are exactly those contexts that let us fmap / lift functions of any arity, including "nullary functions" (`pure` lifts those). HTH
 (&lt;$&gt;) = fmap (&lt;*&gt;) = ap They are operators so they get written infix instead of prefix though.
Probably just the fact that we depend on GC for everything, so we don't have to worry about the tuple lifetime w.r.t the lifetime of the values it contains. In addition to possible GC costs, tuples can sometimes introduce too much laziness.
Thanks! What matters is that things are progressing fast!
Thanks a lot!
I'm sure what old-fashioned-ness has do with it, but... ok?
No it really is not but the runtime is not complete yet.
Ah okay that makes more sense. Your answer is very helpful, so thank you, but think I'm going to stick to the simpler solutions simply because I'm just not quite there yet with Haskell. Thank you so much though!
Does this make any use of the GHC LLVM back-end, or is it a totally new back-end? 
Yeah, this is the solution I'm using now. Thank you again!
Wow. This is great. Is there a goal to support one of the existing APIs e.g. jsaddle? What I really mean to ask is, when will it run my reflex application?!
That's an order of magnitude smaller than reflex/ghcjs.
As a guy who is currently being force-fed the Java ecosystem through a fire-hose: Oracle's shit doesn't smell nicer than anybody else's. When I have an configuration issue on a non-java development project, I fix it using the same tools that I've been using for the last 2 decades to fix technical issues with my OS/Environment. When I have an issue on a java project, it involves digging through stack overflow and settings menus, getting insanely frustrated, finding out that whatever auto-magical thing was supposed to be there to solve my problem doesn't work with X version of Y, uninstalling, reinstalling, gutting a cat in the light a full moon and consulting the pattern of it's entrails, etc. This isn't a dig against IDEs - People should use tools they find comfortable when they can. I'm just saying that way of doing things also comes with it's own problems and it's own learning curves, and nobody ever seems to remember the time they spent getting frustrated with Eclipse as a beginner when they move into a new ecosystem. I remember being new to development, and then being new to development on windows, and then being new to X language... All of those experiences had some serious pain, and Java certainly isn't any different. 
It also doesn't include the massive Haskell dependency graph that any project like Reflex is bound to use
In a *.hs file, you wouldn't have ghci trying to execute each line and `:{` / `:}` would probably be a syntax error. While I am also a advocate for some firm line length (I prefer 80, but I'm flexible.), I don't mind long command-lines or long lines in a ghci session.
Because cross-compiling `gmp` or something similar is going to take a lot of extra work; we decided to start with `BigInt` first. Indeed `Integer` operations always call into js. We use a tag bit to identify small Integers, and the small ones are only regular Haskell heap objects without extra storage burden at the js side.
Hard to give a timeframe for reflex support at the moment. Before that happens, we need: * Improve runtime, properly add gc, exceptions and other services * Improve Cabal support so we can `cabal install` stuff instead of including the world in our repo * Some degree of ghcjs compat layer We'll evetually run your favorite reflex stack, it just takes some more time. Stay tuned :)
&gt; It claims to be resilient to unplugging the power cord, but I doubt that, as it's not resilient to Ctrl-C: https://github.com/acid-state/acid-state/issues/79 I really love the idea behind `acid-state`, but I can't believe that bug still lingers. It completely destroys the D in ACID and turns `acid-state` into a student-project-grade library.
With Rails 3.x we were forced to create a "delayed job" (basically offload work to an async queue) for any long-running request, because of how the Rails concurrency model works. When we started writing web-services in Haskell we were tempted to simple `forkIO` for a async processing, but decided against it. We ended up writing our own DB-based job-queue even in Haskell. Reasons: - What happens if the Haskell process crashes? You lose all these ephemeral background thread. - What happens if you need to push a new version of your code and manually kill the Haskell process? Both of these are handled very well with writing to a queue and letter queue/job workers do their thing. Wrt Kafka, etc , my personal thumb-rule is to NOT introduce moving parts into production till absolutely necessary. Have you outgrown a simple DB-based job-queue? If yes, move the storage backend to Redis and see if that solves the problem.
I recently reinstalled Windows, and installed the latest Haskell Platform (GHC 8.6.3, Cabal 2.4.1.0), and now when I try to run certain commands in ghci using `cabal new-repl` (e.g. `Data.HashMap.Strict.fromList []`) or build certain dependencies with `cabal new-build` (e.g. `math-functions`), ghc will go into what appears to be an infinite loop. What did I break?
In the past I've successfully used https://hackage.haskell.org/package/websockets.
There are a couple things that will not work here. The first, is that f is return both "Int" and "Int -&gt; Int". For any fold to work f needs to be either (a -&gt; b -&gt; b) or (b -&gt; a -&gt; b). The second issue, is that using a fold to both parse and evaluate a mathematical expression tends to be very limiting. Nonetheless, if you want to get around the problem that f needs to have a consistent type, you need to use a Abstract Data Type. Here is a quick example of how an ADT can encode the (-), (+) or Number in a single return type: data Stack a = Val a | Intr Op a data Op = Plus | Minus f :: Stack Int -&gt; Char -&gt; Stack Int f (Val a) x | x == '+' = Intr Plus a | x == '-' = Intr Minus a | otherwise = Val a f s@(Intr Plus a) x | x == ' ' = s | otherwise = Val $ a + (read $ pure x :: Int) f s@(Intr Minus a) x | x == ' ' = s | otherwise = Val $ a - (read $ pure x :: Int) evalExpr :: String -&gt; Int evalExpr xs = coerce $ foldl f (Val 0) xs where unStack (Val a) = a unStack (Intr _ a) = a main = putStrLn $ show $ evalExpr "0+1+2+3"
&gt; For example you could simply input that you have a dell xps laptop and it would be able to generate a minimal linux config for that hardware. This sounds very ambitious, althought maybe that's a feature not a bug 😅. One (wild?) option would be to model the problem as one of type checking/inference, more specifically a record system (your kernel configurator will be more advanced than GHC :P). * Record labels are configuration keys, types are configuration values. * The expected output type for the entire program (config) is a supertype of a minimal config (or maybe a sum type of bunch of alternative minimal configs). * High level statements can be thought of as extra constraints (e.g. systemd config could be a subtype of your config). * You want to infer types for the entire program based on a few number of inputs. * Disallowed simultaneous configurations end up assigning different types to the same label, creating a type error. &lt;-- I'm concerned if this scales ok or it becomes N^2 in interacting pieces. * Typing judgements produce constraints and tracking constraint sources can help you give advice/error messafes. * For composability, you can allow row concatenation/projection of sub-rows/update/extension. * If it type checks it must be correct (i.e. no brick guarantee). * Do you have recursive configuration? That would be wild. If this sounds interesting and you have a bunch of time to spare, check out Morris and McKinna's "Abstracting Extensible Data Types". If you don't have time to spare, my suggestion would be still try and separate out the constraint generation and constraint solving bits (regardless of whether you model it as a type system or not). If there is some neat algebraic structure underneath, your repetitive code will probably make you think of it sooner or later. If you try to impose structure early on and later discover that it only works in 80-90% of cases, you might have to redo a bunch of work. I believe I first heard this from a talk by Sandi Metz, "Duplication is far cheaper than using the wrong abstraction". Good luck!
What happens when you try `cabal v2-repl --build-depends="tidal"`? Reading the docs, I get the impression that `v2-install` can modify global state external to the Nix-like immutable store used by the v2- commands. This global state includes links to executables, and also the "package envionment" that is available to GHC by default. By contrast, `v2-repl` works exclusively with the Nix-like store, it doesn't touch anything in the GHC installation itself.
&gt; Which exactly is the correct way ? The **correct** way to **make a library available to all future `ghci` sessions** is the 4th: ghcup install ghcup set 8.4.4 ghcup install-cabal cabal new-update # running `cabal new-install cabal-install` is unnecessary, since # ghcup install-cabal installed the latest version cabal new-install --lib tidal The v2 commands will drop the `new-` prefix in the next major cabal version (3.0) and will become default, but you should use them already unless you run in some obscure edge case where only the v1 commands work (if you do, report it, please!). You should avoid v1 commands, since they pollute the global package database. &gt; Can someone maybe explained a little about the differences and take a look at the mix results of the various combinations I've tested below? &gt; what exactly is the reason why Tidal works in some of the scenarios listed below and not others? Which exactly is the correct way ? `new-install` requires `--lib` when installing *libraries*. It's explicit because it's almost always *not* what you want, except in rare cases like tidal, which requires its library to be available in ghci. In v1-`install`, `--lib` does not exist. &gt; What's the reason for these no symlink warning when I use no prefix? and does this affect anything? I think the warning is there because cabal new-install uses symlinks to install executables, and this leaked into v1-install which copies the exe instead. So there's a file conflict, but you can ignore it. &gt; why no prefix creates files/folders in ~/.cabal while using new- creates none? but if the lib is not created in ~/.cabal where does it reside? `new-*` does create files in `~/.cabal`. It's just all isolated in `~/.cabal/store`, the new [nix](https://nixos.org/nix)-style package store which allows cabal to avoid package conflicts between separate projects while still caching things globally. --- Finally, there's another quicker way if you only want to test a library: `cabal new-repl --build-depends tidal` will open a repl with the tidal library loaded, but it will not make it available to all future ghci sessions, only that one. 
My friend, you should use the [Hackage search](https://hackage.haskell.org/packages/search?terms=Websocket) bar next time.
Any idea what the Asterius vs Ghcjs numbers would be for a Reflex based TodoMVC?
thank you so much for clarifying the differences. really appreciate it! :) 
I mean, which is recommended for production usage?
solid advice!
Nixos kindoff has done this already: https://github.com/NixOS/nixpkgs/blob/master/pkgs/os-specific/linux/kernel/generic.nix Maybe you should look at their work and see what you want different in specific?
I'm using [https://hackage.haskell.org/package/websockets](https://hackage.haskell.org/package/websockets) in production. It's being used as part of a real time status overview for a FreeSWITCH system. Maximum amount of clients connected at any given time is approx 15. I haven't had any problems with it.
Not really a Haskell podcast but: http://typetheorypodcast.com
&gt; Thread-related primops: MVars, STM, Sparks, etc. We don't have Haskell multi-threading yet; we surely will implement threads some time from now, but before that, we'd like to make sure 2 is reliably delivered, and gc is functioning. Does WASM have anything analogous to "OS capabilities" (if I got the term right?) Loosely, the number of underlying "real" OS threads that can be spawned?
Do you make any distinction between the types of jobs added? For example, sending an email versus processing a .csv file for an upload? (I don't know your app, so I can't pick better examples) I like the consistency of treating each job as something to place on the queue. 
For me it is more a question of not interrupting the code with absurd events and callbacks whenever waiting or displaying something is necessary. Think in a program which display two counters, each one in a loop, with a threadDelay. 
Are you sure \`evalExpr\` is correct? Maybe you wanted to give \`coerce\` and \`unStack\` the same name?
Great question. To answer your specific examples: - Sending and email =&gt; Create a job -- it will automatically take care of retrying due to temporary network errors. - Processing a CSV file =&gt; Depends. Due to the power of Haskell concurrency, _other_ web requests won't get blocked, even if one request is waiting for a large CSV file to be processed. However, if the processing is going to take longer than the request timeout, then we have a problem. Thumb-rule: - Anything for which the user is not waiting for an immediate response, create a job. Especially stuff that is making network calls to other services. eg. sending email, sending SMS, sending mobile notification, etc. - Anything for which the user is waiting for an immediate response AND it can be reasonably completed within the timeout window AND the corresponding UI is well engineered [1] you may do synchronous processing. - Anything which is potentially going to take longer than the request timeout =&gt; create a job and expose and endpoint which can tell you the progress of the job. Get the UI to poll the job status OR use websockets. [1] Well engineered UI = "loading / processing" UI feedback given to the user AND user has the ability to retry the action if it times out, in rare cases. 
No. Not sure if asterius is able to compile Reflex yet
I would definitely go for the "separate process with persistent queue in between" option, so that you can 1. scale your background workers separately from your web serving tier. 2. Deploy workers and web servers independently without losing any currently running jobs. 3. Have some protection if the haskell process crashes and/or some of the servers involved experience hardware failure. With regards to Redis/Kafka/etc, I would take the advice of /u/saurabhanda and delay introducing new moving parts until it is necessary. I quite like Redis personally, and we've had good success with AWS SQS at $DAYJOB (though that presupposes you run on AWS).
It will have WebWorkers soon, but the implementation is pretty terrible. It's enough for pthreads in emscripten though, so it should be enough for Haskell. Chrome is the only browser with support so far I think.
This is similar to what I've been thinking; thanks for responding. &amp;#x200B; I was hoping there was a generic solution or library that I simply hadn't been unable to uncover up until now and I wouldn't have to write my own. &amp;#x200B; I did stumble upon [yesod-job-queue](http://hackage.haskell.org/package/yesod-job-queue). All of my services are built using Servant, and since we're already using Redis, I thinking of writing my own version of this for our applications. We have deployments in Haskell and Python and I think that will continue for the foreseeable future, until I can get more of the team comfortable writing Haskell. &amp;#x200B; Another idea I had, since we're deploying into Kubernetes, is to use the control plane API to create Kubernetes Job objects. Another option is to look at something like Apache Thrift and "one way" (or fire-and-forget) messaging. Another alternative is to go down the RabbitMQ path (or any of the \*MQ variants), but I'm not looking forward to keeping RabbitMQ running and I haven't used any of the other variants. Yet another alternative is to use some cloud product as a job queue and have some process that listens for events on the queue, whatever the interface to it may be. Of course, Kafka comes up a lot when going down this path as well. &amp;#x200B; I'd like to use something that can be used in a variety of situations for both point-to-point and pub/sub messaging (for the latter, I think I'd be happy with Redis) regardless of language. However, I don't have anywhere near the scale that would justify using Kafka, and I don't foresee having those problems in the future either. &amp;#x200B; I realized, rereading here, that this is just me thinking out loud. Thanks again for replying.
Meh. Hackage search isn't overly helpful or insightful for package discovery
Thank you for the response, good advice.
6 hours (!!) of video footage? Was this a 2-3 day workshop?
There are certainly times where it's better and times where it's worse, but in this case it's pretty good. The first result is the same library everyone is recommending, it has a history of updates, including within recent months, it has a high download count including plenty of recent downloads, at least one of the maintainers is well known in the community, it's well documented and it comes with examples. What more could you ask for?
I was trying out rewrite rules for the first time and having difficulty getting anything to fire. Someone interested in taking a look at my repo and give me some tips? https://github.com/philzook58/not-bad-ccc/blob/master/src/CCC.hs What is the best Haskell chat place these days? IRC? Slack? Something else? 
Yeah, we actually are using Redis already and I've used it for many years in a variety of situations, so I think that's going to be part of our infrastructure, moving forward. &amp;#x200B; I've used SQS as well, actually, in a previous role and it did work pretty well. My current role is in an organization that has everything deployed onto Azure and they have a product called "Service Bus", which is apparently used a lot internally by Microsoft but which I've heard from multiple sources is being de-emphasized (and receiving less investment moving forward) in favor of their Kinesis-equivalent Event Hubs. 
Isn't this basically what package managers do?
Yeah, sure. I guess what I was getting at here is that webdev shouldn't be so concerned with performance, not that Haskell is an awkward fit. UI work gets it's own thread: good. UI work gets 8 threads: what the hell are you doing?
Have you used `ghcid`? It's pretty much `:r` automatically, plus some other niceness.
You can always use PostgreSQL to do this if your needs aren't incredibly "Web Scale" and you literally just need a transactional queue. In particular you need PostgreSQL 9.5 or later, but it's relatively easy to model a job queue if you have it. Do not roll out Kafka just for the sake of it, unless you specifically want to build familiarity with it, or you really need it (which is totally fine! Just know what problem you're actually trying to solve). Measure twice, cut once, etc. &amp;#x200B; [https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/](https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/) &amp;#x200B; I'd probably suggest you write most of your queue logic inside the database as a few simple functions (\`CREATE FUNCTION\`) wrapped up with interfaces to do all the work like scanning open jobs/marking rows as completed, and stuff like that. Then any client library can just call an abstract interface (across any language) to do the dirty work. A simple implementation is probably 30 lines of PSQL, a more robust/domain-specific one is probably a bit more than that. &amp;#x200B; (You can also use extensions such as \`pg-cron\` if you need to occasionally insert jobs based on a fixed schedule, entirely in the database.)
That's an interesting suggestion. We do use Postgresql, but I didn't realize we could use it in this way. Thank you.
I would second this. Do not underestimate the power of your RDBMS (especially if it's PostgreSQL). We've been using it as a job-queue for 4+ years without any problems. Also, an RDBMS makes job creation a part of your original main, so you don't have to deal with the edge case of the original transaction succeeding, but job creating (in an external queue) failing.
This comment is purely focused on shifting from imperative to functional when solving algorithm problems, and its more focused on functions than the type aspect of haskell. I have used haskell for backends and web database applications in prod for a few years. I did algorithm and high throughput work in the past before that using c# and scala. A few months ago, I spent a few weeks trying to complete lots of easy HackerRank puzzles at max speed in HackerRank, after having been around Haskell for a while now (and scala, clojure, scheme before that for many years; my first exposures to programming were basic, c++ and scheme). Even to this day, I still gravitate towards writing the imperative version first and feel faster in imperative. I can almost get the functional solution in the same amount of time, but I think being exposed to c++ early biased me to it. My advice is to try the exercise I did on a recurring basis, forcing yourself to use uncomfortable functional approach and see if you can build up some speed and instincts there. Overall, it's a small part of all the other skills one should master to program and design in a functional style, but it does help rewire your brain a bit.
Thank you, this was very helpful for me today. My team has spent the past day trying to get HIE and other plugins working with VS Code unsuccessfully, then I tried running these commands and Haskero worked!
Alas, that version of GHC is affected by a [bug](https://ghc.haskell.org/trac/ghc/ticket/16057) that causes any compilation that uses Template Haskell to hang. The `Data.HashMap.Strict.fromList []` stuff is also quite concerning, I have reproduced it as well. I wouldn't recommend people to move to this version of the Haskell Platform. 
Yeah, it was really good! The few hours are about the organization, then the next few are about the major milestones of the big type-checker changes and the papers behind them. The closing intros where GHC is going, specifically mentioning some dependent types work and the complications therein, such as complications in the kind system. Seems like a significant backwards-incompatible change will need to occur to have Really Good dep. types.
I thought this was a good presentation: the category diagrams were good illustrations and linked really well to the code. It's not easy to give a presentation that helps the audience build intuition for topics like this. Well done.
whoops! Thank you!
Hey I actually am going to go into this (and a bunch of other more practical things I skipped over in part 4). I'd never heard of [`ghcid`](https://github.com/ndmitchell/ghcid) until now, I was actually planning of looking into `stack install --file-watch` due to it's use in [this talk on Building Terminal User Interfaces given by FPComplete](https://www.youtube.com/watch?v=qbDQdXfcaO8). I'll include a note on `ghcid` in Part 4 as well, thanks!
Strong recommendation! It wraps the stack functionality and a load more. :D
&gt; Comments always start with a hyphen in Haskell. 1. Learn Haskell 2. Don't write any more tutorials
I'm not OP, but Intellisense is a Visual Studio/C# feature (not exclusive to C#, mind you). It's a code hinting feature (among other things), basically. 
In order to circumvent the perennial "record problems", I'm considering vinyl. How can I go about learning vinyl to use it effectively, beyond just the simple use cases that Data.Vinyl.Tutorial.Overview covers? I find the documentation and type signatures rather abstract and hard to translate to working knowledge for writing code. I'm sure if I spend enough time meditating on the documentation, experimenting with code and making mistakes, I'll eventually get it, but I'm looking for the most effective way to get familiar with it. &amp;#x200B; Are there in-depth tutorials that cover it? Or, relevant concrete code examples with descriptive comments to help understanding? Any tips for learning it?
If you're OK with limiting yourself to `base &gt;= 4.11`, you can do this: canReachEnd = sum &lt;&amp;&gt; (&gt;=) &lt;*&gt; length Which I think is closer to the ideal "idiom brackets" style: canReachEnd = [| sum &gt;= length |]
I think most of reflex hinges on jsaddle, so once that is in place I think most things are there - but it sounds like there's a way to go yet, even so - very exciting! &amp;#x200B; I have an image annotation application which I think would benefit from wasm greatly.
There's the whole react native thing, from what I understand js that happens to use multi-threaded implementation of renderer. Those 8 cores are weak and some apps do take a while to load even on wifi.
What is the type of `sum`? Let's ignore all the `Num` polymorphism. sum :: [Int] -&gt; Int `sum` is function from lists of numbers to numbers. But also: type Ctx = (-&gt;) [Int] sum :: Ctx Int `sum` *is*, metaphorically, *a* number. `sum` can be seen as *an* `Int` that happens to depend on some "environment" of type `[Int]`. Also: length :: Ctx Int `length` is also a number which depends on its environment. If `sum` and `length` are just `Int`s, then I should be able to just `&gt;=` them, like `sum &gt;= length`, but the little `Ctx` gets in the way. So, we can use the `(&lt;&amp;&gt;)` and `(&lt;*&gt;)` operators in between. sum &lt;&amp;&gt; (&gt;=) &lt;*&gt; length `(&lt;&amp;&gt;)` is somewhat new to the `base` library, and `(&lt;&amp;&gt;) = flip (&lt;$&gt;)` in the same way `(&amp;) = flip ($)`, so you can also write (&gt;=) &lt;$&gt; sum &lt;*&gt; length The idea is that you're just supposed to read this as if all the `Applicative` and `Functor` operators aren't there: (&gt;=) sum length And their presence tells you something extra is happening. Bonus: take a look at the [`foldl`](https://hackage.haskell.org/package/foldl-1.4.5/docs/Control-Foldl.html) library. import Control.Foldl(Fold) import qualified Control.Foldl as L instance Applicative (Fold a) L.sum :: Num a =&gt; Fold a a L.length :: Fold a Int The idea is that a `Fold a b` is a special kind of function `[a] -&gt; b`. The `Applicative` instance combines such functions together in about the same way as for the previous `(-&gt;) a` instance, except it ensures only one pass over the list is needed. You can write `canReachEnd` in basically the exact same way while making it more performant: canReachEnd :: Fold Int Bool canReachEnd = L.sum &lt;&amp;&gt; (&gt;=) &lt;*&gt; L.length
if i'm using cabal new-install --lib tidal for installing these library files, how would I do an update? say for example when tidal went from version 1.0.3 to 1.0.4 (which it just did lol)
Google for "list fusion." 
It's also worth noting that list comprehensions don't compile into loops. Iirc the default desugaring translates into build/foldr, with monadcomprehensions into &gt;&gt;= and some combinators.
It's more like what distro installers do (and they do a very good job of it in recent years).
Same author, I think. 
Only if optimizations are turned on. With `-O0`, they just become pretty efficient loops.
i never understood the motivation for comprehension, i see it as unnecessary syntax complication 
Doesn’t List Comprehension allow you to do map and filter in only one iteration?
Haskell is lazy so composing map, filter, etc is only one iteration.
It's about expressiveness. List comprehension when put to good use, is incredibly beautiful. It is almost an exact mirror of set builder notation in mathematics as I'm sure you already know. I think it's sort of like a ternary operator. The goal is expressiveness but it can be overdone and abused. 
I'm trying to understand the GHC build process. This post [Building GHC: The Stages](https://medium.com/@zw3rk/building-ghc-the-stages-2c6cf6fc4b29) has been helpful, but I still have some questions. The version of `base` corresponding to the version of GHC we want to build is first built by the stage 1 compiler, is that correct? The stage 1 compiler is built by the stage 0 compiler using an earlier version of `base`. Also, the post mentions the concept of "bootstrap packages": &gt; As the compiler quite often depends on features of libraries it depends on that are not guaranteed to be new enough in the bootstrap compilers package database, the first step is to augment the bootstrap compilers package database with those required packages to build the Stage 1 compiler. To do this, we compile this set of bootstrap packages with the bootstrap compiler. Do these "bootstrap packages" need to be the exact same version of the packages that are built by the stage 1 compiler and shipped with the final stage 2 compiler? Or is it enough that they should be modern enough to compile stage 1?
I have a question about the[Building GHC: The Stages](https://medium.com/@zw3rk/building-ghc-the-stages-2c6cf6fc4b29) post. We have stage 0, stage 1 and stage 2. When is the version of `base` that ships with stage 2 first built? Is it first built by stage 1, or by stage 0 as a "bootstrap package"? Also, do the "bootstrap packages" mentioned in the post need to be the exact same version than the versions of the packages that are built by the stage 1 and ship with stage 2?
IRC (#haskell on freenode) and Slack (#haskell on functionalprogramming) are both pretty active.
Thank you so much. Never new this existed but have wanted it for so long.
reflex don't do exceptions: https://github.com/reflex-frp/reflex-dom/issues/114
yes, but this relies on garbage collecting to be done just in time to be efficient. We can actually explicitly request strictly one traversal by writing everything in foldrs and composing the reducers a-la simple DIY transducers: &amp;#x200B; mapping f cons x r = cons (f x) r filtering p cons x r | p x = cons x r | otherwise = r foo f p g = map f . filter p . map g = foldr (mapping g . filtering p . mapping f $ (:)) \[\] &amp;#x200B; &amp;#x200B;
You shouldn't worry about performance---the two versions are likely to end up generating pretty similar code. You should choose based on which version expresses your intention more directly. Often I find a list comprehension to be much more readable, especially if the functions involved (the omitted parameters in the question) are not already named.
The standard ca answer to infix parsing is the [shanting yard algorithm](https://en.m.wikipedia.org/wiki/Shunting-yard_algorithm).
Eh. The amount of time spent on the work is the same as if it were two iterations. With laziness, the whole concept of an iteration gets very murky. The only thing laziness is buying here is the potential ability to use constant space, but it puts pressure on the GC to do so
It's a notation borrowed from math. The point is to mimic [set-builder notation](https://en.wikipedia.org/wiki/Set-builder_notation), more or less.
The set of all even integers, expressed in set-builder notation.
**Set-builder notation** In set theory and its applications to logic, mathematics, and computer science, set-builder notation is a mathematical notation for describing a set by enumerating its elements or stating the properties that its members must satisfy.Defining sets by properties is also known as set comprehension, set abstraction or as defining a set's intension. Set-builder notation is sometimes simply referred to as set notation, although this phrase may be better reserved for the broader class of means of denoting sets. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
/u/FunCicada is apparently a worse version of WikiTextBot. (Open the wiki page and see that this sentence appears in the aside to the right.)
Is most of that from the mammoth amount of generated code in jsaddle-dom? I'm guessing most of that never gets used - is it a dead code elimination problem?
Boot libraries only get compiled once, by the stage 1 compiler. The versions of bootstrap libraries are pinned, that's why development of them mostly happens in submodules of the GHC repository.
I too use a Postgres-based job queue for my Haskell applications, and I've made the Postgres highly-available with [Stolon](https://github.com/sorintlab/stolon). I would recommend this to anyone, for most use cases.
It's super pretty though: &gt; let fib = 0:1:[ a + b | (a,b) &lt;- zip fib (tail fib) ] &gt; &gt; Prelude&gt; take 8 fib &gt; [0,1,1,2,3,5,8,13] 
At Freckle we use Faktory, which is a language agnostic job server from the creator of sidekiq. I've been very pleased with it. http://hackage.haskell.org/package/faktory https://github.com/contribsys/faktory/
If you enable optimizations, GHC will do this for you anyway.
If my understanding is correct, because of how Haskell rearranges your code it won't actually iterate over the list 3 times, it'll instead iterate over the list once and apply all 3 operations when evaluated.
&gt; Because the function go over the list 3 times this doesn't happen because lists are lazy. 
They generalize to monad comprehensions. 
this is what they call a premature optimization
You can write the same as &gt; let fib = 0:1:zipWith (+) fib (tail fib)
`(map f . filter p . map g)` isn't slower than list comprehensions, actually. It's actually possible that they compile over to the same thing :) `map f . map g` actually never generates an intermediate list -- the final list is generated directly from the first list, because of how lazy lists work: map f [] = [] map f (x:xs) = f x : map f xs so consider: head (map f (map g (x:xs)) this becomes: head (map f (g x : map g xs)) which becomes: head (f (g x) : map f (map g xs)) which becomes: f (g x) So, the list is not traversed twice -- it's only traversed once :)
&gt;Did those people not do a good enough job convincing you? If not, why not? Not sure about op, but I wasn't able to be convinced because it was recommended to me through someone else who didn't know haskell.
You definitely can. And yet, the first contains the expression `a + b`. If you instead wanted `2 * a + b`, or `foo a + bar b`, it's clear where to make that change. To accomplish the same in the latter case, you'd want an explicit lambda: `let fib = 0 : 1 : zipWith (\a -&gt; a + b) fib (tail fib)`. That's a bit uglier than the list comprehension syntax. And with parallel list comprehensions, it's `let fib = 0 : 1 : [ a + b | a &lt;- fib | b &lt;- tail fib ]`, and I find that abundantly clear. There's definitely an aesthetic choice involved. I recognize that others prefer a style of code with a lot of little combinators -- zips, folds, maps, filters, fst and snd, etc. I tend to find code more readable when it expresses the basic data flow with pattern matching and expression structure, instead of hiding that structure behind shallow combinators.
This is the worst paying Haskell job I saw, including every entry-level position. I suspect it may be good for someone already living in India (or even worse paid country) but probably doesn't make sense for most...
I agree, especially since that `do` notation achieves the same thing.
&gt; You should also be willing to work with us on a short term engagement to see if we are a good fit for each other.
Oh, til. The translation is described in chapter 7 of 'The implementation of functional programming languages' and [summarized here](https://github.com/ghc/ghc/blob/509d5be69c7507ba5d0a5f39ffd1613a59e73eea/compiler/deSugar/DsListComp.hs#L158). If someone else is interested the tldr isn't too complex. First the common list comp rewrite rules - specialize `[x | v&lt;- a:as, qs]` into `[x|qs][a/v] ++ [x|v&lt;-as, qs]` and `[x | ps, v &lt;- [], qs]` into `[]` - specialize `[x | ps, True, qs]` and `[x | ps, False, qs]` - translate leftover `[x | ]` into `[x]` Then rewrite` [x | p &lt;- xs, qs]` into `concatMap (\ case {p -&gt; [x | qs]; _ -&gt; []) xs` and guards into if statements. I think these rules are standard, I saw them in several other works about optimizing various comprehensions. The next part is more interesting because it basically hardcodes optimizations for the -O0 case: -inline concatMap - float the rhs in `concatMap f xs ++ rhs` to the tail position, effectively replacing the nil constructor
Is that what GHC actually uses? The desugaring given in the report is slightly different, but the report doesn't require the exact transformation it documents, just those semantics.
&gt; Before you start writing the hello world program, Please make sure Haskell Platform installation on your machine. Haskell Platform contains GHC - Glasgow Haskell Compiler and Cabal (Common Architecture for Building Applications and Libraries).This post will not cover the Haskell platform installation I'd argue that the point of the traditional "Hello, world!" program is to introduce a person into the programming language, and that without covering the installation of the tooling it loses a big part of what it's for.
what is the practical point of mimicing set builder notation? 
so it is applicable widely; but that does not make it useful
The actual implementation in GHC does not use ++ or concatMap. It just writes s custom loop that does e exactly one cons per element.
The point is that mathematicians have already come up with a standard notation which they find to be clear and concise. So why not use it? I admit, though, that I rarely use list comprehensions. Still, I'm making arguments if their favor because I do not find them as useless as some others here seem to find them.
I was about to answer that it optimizes to the same thing but without rewrite rules concatMap actually isn't tail recursive. I feel like vectorization could help here but since the whole thing only happens without optimizations enabled that is kind of a non sequitur. 
So, I've been working with continuations recently and have hit upon a problem which would be well suited to a solution with a MonadFix-style approach. Without going into too much detail, what I want to find is a function `mfix :: (a -&gt; (a -&gt; r) -&gt; r) -&gt; (a -&gt; r) -&gt; r` that works like `fix :: (a -&gt; a) -&gt; a`, but with continuations. So far, I have not managed to implement a function that works as `mfix` for this signature. Googling that problem a bit I've found claims that `mfix` is not definable for continuations, but I have not understood that (yet). Can anybody help me with this?
I've finally bothered to take the time to understand why `List`, for example, is considered non-deterministic. This leads me to a different question, though: If Haskell was/to be come a dependently typed language, would List still be non-deterministic?
`[]` is considered one way to model non-determination. `[]` itself is completely deterministic, but it ('s Monad instance) can be used to implement backtracing with relatively few lines or code. (It's not always the most runtime efficient, though...) In DT Haskell, `[]` would be essentially unchanged, so it can still model non-determinism. (The NP type, not the QM type.) In strict languages (like Idris or most other langauges), lists are not a practical model of non-determinism. In lazy but total languages like Agda (or Coq?), list data can still be used in many cases, but not all because all data is finite. They might have a list-y codata type[1] that can be used in mostly the same way as Haskell lists, though productivity checking might still get in your way. [1] Not the common "stream" example, since it models *only* infinite lists.
We built this: http://hackage.haskell.org/package/hworker to solve essentially this problem. It allows arbitrary languages to put jobs in, though we didn't actually need that (the jobs are JSON). We really just wanted to be able to process lots of jobs (a backend request could generate tens of thousands) using a set of processing servers, and have some guarantees about reliability.
Just looking at the type signature have for `mfix` there, it doesn't appear to be implementable. Where would you get an `a` so you can call either the first or second argument? --- Also, it doesn't look quite like `fix`. If you substitute `a` =&gt; `c -&gt; r` in the type signature for `fix` you get `fix :: ((c -&gt; r) -&gt; c -&gt; r) -&gt; c -&gt; r` which *does* have an implementation: fix :: ((c -&gt; r) -&gt; c -&gt; r) -&gt; c -&gt; r fix f = fp where fp x = f fp x And, it's not *entirely* undefined. `fix (flip const) = id`, I believe. There might be other implementations, too. --- If you instead substitute `a` =&gt; `(c -&gt; r) -&gt; r` into the `fix` type, you get `fix :: (((c -&gt; r) -&gt; r) -&gt; (c -&gt; r) -&gt; r) -&gt; (c -&gt; r) -&gt; r`. That *also* has _the same_ implementation: fix :: (((c -&gt; r) -&gt; r) -&gt; (c -&gt; r) -&gt; r) -&gt; (c -&gt; r) - r fix f = fp where fp g = f fp g Not sure if there's a *useful* way to call it.
I would have agreed with you for a long time, but I've started using them on occasion and have started to enjoy using them more. No math background either :)
Because Math is in some ways a declarative language and as such similar to Haskell sometimes. And if this notation has stood the test of time, why try to reinvent the wheel?
What is the reason for these boot libraries? Instead of allowing users to configure every library fully?
What text was this from?
Some links here: https://github.com/soupi/haskell-study-plan#performance
Okay, that makes sense and, in retrospect, the answer to my question is pretty obvious. Thank you!
Custom loops is the way to go if you do this transformation directly. I believe ghc goes via foldr/build so list comprehension fuse with other list operations. 
The same reason we have `do` notation: Because it communicates quite a lot of information (semantics), in a short and easily-read way. A good rule of thumb I find when writing Haskell is that you are writing your code with the intent of making it *as readable as possible*. GHC is good at reading and it's quite hard to accidentally make something nonperformant (in practical code, anyway). I think that one of Haskell's strengths is the ability to produce really readable code.
OP's `mfix` is [MonadFix(mfix)](http://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad-Fix.html#t:MonadFix) for `Cont r`. I don't know the reason why `Cont r` can't have a `MonadFix` instance, but "can't be implemented without using `fix`" is not a sufficient reason. `MonadFix Identity` *is* `fix` and it's a prototypical example of `MonadFix`.
What is the issue with `Data.HashMap.Strict.fromList []` on Windows? Does it loop? If it is then it may be helpful to update #16057 with that info because people are assuming the issue is with TH, but maybe it's more general than that.
Oh my goodness! Thank you, I've tried the other commonly recommended VSCode extensions and none of them have worked. This one finally does, and it's good enough for me!
You mention you like the workflow of using something like Kafka, but can't justify hosting it; have you considered using one of Google's managed alternatives? Pubsub and Task Queues operate very similarly to Kafka with "at least once" delivery. Google pubsub you pay per message (no hosting cost) so if your volume is small it's really inexpensive, and it's language agnostic so you can pull from or push to any server. Good luck!
It seems that the issue has already been updated. It's a bug in the runtime.
This pay is really good in India, mainly due to very cheap cost of living, cheaper markets and inexpensive housing compared to the valley. A person getting $100k in valley is equivalent to one getting paid ₹1.5mn (USD 25k) per year in India due to reasons mentioned above, leading to similar standard of living. The values may fluctuate a bit, but I hope you get the essence of cost of living for similar expenses.
I’ve encountered a similar issue (https://github.com/dramforever/vscode-ghc-simple/issues/3). You might want to install version 0.0.5 (https://github.com/dramforever/vscode-ghc-simple/releases/tag/v0.0.5) and try that out. And, if that doesn’t work, I recommend you open an issue on GitHub, so the extension can be further improved (and you can get something that works for you).
op here. i've been using distributed-process on a project at work and wanted to share some testing techniques i found useful. these might be bleedingly obvious to everyone else but past me would have been very grateful to have found this post. 
That's the best Christmas present!
Why did you switch from `brick` to `ncurses`?
Have you searched on Hackage?
I'm not sure if this counts, but Hasura is built on Haskell afaik: https://github.com/hasura/graphql-engine
I once wrote a [tool](https://github.com/gelisam/cabal-rangefinder) which tries a lot of versions in order to find the bounds for you, but it takes a while to run and has quite a lot of bitrot at this point. What I do instead these days is I find the earliest stackage lts with which my code compiles, and I use the versions provided by that lts as my lower bounds. The result isn't optimal, in the sense that some of the bounds could be lower, but at least [it's easy to configure CI](https://github.com/gelisam/hawk/issues/192#issuecomment-441400539) so that it warns me if I make a change which breaks my lower-bounds.
For performance, basically. I think it's a consequence of the design approach I used, but Brick started to use a **lot** of CPU when 'nothing' was happening. I'll try to explain: The basic design for the game uses a timer 'tick' that fires every 1/60th of a second. This is essentially the frame-rate for the game. Now, depending on the level, the pac-man moves a 'n' frames, and the ghosts at 'm' frames, but it might be different if they are fleeing, and one of the ghosts moves slightly faster than the rest. Thus, the main game code needs to be called for every frame, but nothing may have happened for that frame. However, the way Brick seems to work is these 'ticks' are merged into the same handler as a keyboard/mouse/other event handler. So Brick calls the tick handler and then evaluates whether anything has moved on it's 'virtual' display by re-doing the widgets. Unfortunately all this detection uses up a lot of CPU and thus made the game very inefficient. I love the Brick model; it just didn't fit my application because of a decision I made to run it using constant ticks. I *could've* used a variable timer and worked out what needed to be moved next, but it seemed complex. Therefore, I moved to ncurses. As a bonus, as ncurses is not packaged on Stackage, I had to learn how to pull that across into my Stack build as well! So to cut a long story short; because Brick didn't match up to the design decisions that I thought I needed to make to try to replicate as much of Pac-man as I could.
It looks like that you initial the first element of the String to 0. If I put evalExpr "1+2+3" , it does not work anymore. 
Hey, testing info *and* a nice example to get started with `distributed-process`! Thanks for the post. 
I thought that in the f ( Val a) x, "otherwise = Val a" might be changed to "otherwise = Val (read $ pure x :: Int)". However, I still have a question, why I cannot just use "read x :: Int". How the "pure" works here? 
This is where running the matrix against release candidates would be nice. Instead of fiddling around you could just let a robot tell you what breaks. http://matrix.hackage.haskell.org
And already in haskell-ide-engine. https://github.com/haskell/haskell-ide-engine/pull/1007
my pleasure!
Let f and g be functions (a -&gt; b) and p a predicate (a -&gt; Bool): O(map f (map g (filter p xs))) = O([f(g(x)) | x &lt;- xs, p x])?
Hi Thanks for your feedback, I am new to haskell. I will correct if anything misleaded information exists
The way I search for packages is usually to go to http://hackage.haskell.org/packages/ and CTRL-F for stuff. It looks like there are a few graphql-related packages, but I don't have experience with them. On a different note, there are some openapi/swagger/JSON Schema packages that looked pretty cool, and are in my queue to evaluate more closely for a project. This is another (complementary?) point in the "modern" web dev design space
You still need to specify the version constraints with stack. When you're building a library other users will often not use the same resolver as you, and your library's stack.yaml won't be considered. Even for an executable explicit bounds indicate which versions you support for changing resolvers. You can leave the bounds of for a short-term time savings but at a long-term cost to you and immediate cost to your users. You can generate bounds using: cabal gen-bounds
Intero too! https://github.com/commercialhaskell/intero/pull/598
What's the idiomatic way to work with Persistent models? Do I use the database models everywhere in my code or should I declare my domain models normally and translate to/from Persistent entities at database interaction boundaries?
I've thought only a little of going this route. We're deploying into Kubernetes on Azure and they have a product called Event Hubs, which I was considering but all the documentation talks about how it's great for IoT, billions of events, etc. I should probably reconsider it. Thank you.
This is pretty cool. Thanks for telling me about it. If I were going to write something myself, it would have been something like what you have here. Instead, I'll take a look at your project. Thanks again.
In my experience it's quite easy to use just a single service from google cloud platform; and you can get $300 free credit when you sign up to try it out. Hope something works for you!
Does anyone have any tips on fault finding a HIE Windows 10 build? I'm running the build-all.ps1 in powershell and it seems to be unable to build fclabels-2.0.3.3. I can see ghc running in the task manager, it just never finishes. If I stop it and re-run, it builds other dependencies but still ends up stuck waiting for fclabels to finish.
Interesting video, but like lots of materials, it mostly seems targeted towards an audience familiar with blockchain trying to get a sense of what Haskell is and why it's useful. But I'm in the opposite camp. I know what Haskell is and why it's useful; I know about lambda calculus and Turing completeness, about buffer and arithmetic overflows, basic principles of type systems, etc. etc., but I don't have any intuition for what "running my program on the blockchain" even means. In fact, for me this phrase breaks the entire metaphor of cryptocurrency: the analogy I'm familiar with for what blockchain is is, well, currency, where I have a wallet and I transfer money between wallets, and "running a program" in this context is a meaningless statement that makes no sense to me. I can't "run a program" on the US dollar, so this completely breaks the existing metaphor I have for blockchain. So from a high level, for an audience like me, what is the actual programming model here? I'm used to writing Haskell programs and (reluctantly) running them on AWS. What does it mean for me to write a program and run it on a blockchain? How do I even upload it there? Computation isn't free, I assume, so how does payment for the computation itself work? If I write an infinite loop, will it just burn the funds in my account until it's empty? But... where did that money go? To the party who was performing the evaluation? How do I deal with data storage and retrieval? Surely there's no Postgres instance for me to connect to, so... is data itself stored in the blockchain as well? Finally, how fast does stuff run? Distributed consensus is notoriously slow, so much so that many platforms just outright abandon data consistency so they can have reasonable performance - and that's with running native code on real hardware owned by a single party. Anyway, sorry for being a bit long-winded, but I see so much information about this project dedicated to informing non-Haskellers about Haskell, but so little dedicated to explaining to Haskellers what exactly this thing *is* and how to use it compared to how we normally use Haskell. I did check out the Plutus Playground posted here last week, but the vocabulary being used was foreign enough to me that I couldn't get any intuition for what was going on beyond just "Yup, that is Haskell code alright."
Useless questions for Haskell developers. It would be more interesting to know what can I actually do with Cardano.
Could someone explain why it's so important?
\&gt; write a program and run it on a blockchain? Typically you need a lot of infrastructure outside the scripts on the blockchain. For example, if you take the [lightning.network](https://lightning.network), all its bitcoin scripts are available\[0\] , as you can see there are not many lines of code . But to use them correctly, you might need 50 kloc (Golang), at least that is the rough size of one implementation (lnd). The scripts are just for consensus (conditions for a coin being spent). You can store other data, like private keys, IP's of peers and account history separately. Other projects (Bitcoin Cash, Ethereum) seem to dream about storing everything on-chain, (which seems crazy to me, but whatever. Why replicate worldwide unless necessary, you gotta pay for it with lack of decentralization). On bitcoin you can lock your funds conditioned on a hashed script, which means you won't even reveal the program until you spend it. Which of course then would need software to keep track of where the money was locked up. \&gt; will it just burn the funds in my account until it's empty? On Ethereum, there is "gas" which is consumed during execution, but you allot a fixed value when you submit the tx. On bitcoin it is simpler, you just pay for the size in bytes. Bigger script, larger costs. \&gt; is data itself stored in the blockchain as well? Yes. But you can design a protocol with oracles that can link cryptographic commitments to real-world events. Projects like Augur, Bitcoin Hivemind and Discrete Log Contracts attempt this. But since this would mean centralization when done naively, it isn't a native feature. Bitcoin script is a Forth dialect with very few operations. Many people in Bitcoin consider the Ethereum project ridiculous since they have grand promises like "world computer" that havn't been delivered on. Since people believe Bitcoin can serve as a "store-of-value" (whatever that means) without having a powerful script, powerful scripting is being developed very slowly. There is some work, Russell O'Connor is doing "Simplicity"\[1\], but the goals are probably modest compared to Cardano. I see Cardano as more rigorous attempt to do what Ethereum wanted, which is to make a powerful programming platform for contracts. More powerful than what is possible on Bitcoin, but not with footguns like Solidity has (there was a major vulnerability and the Ethereum project reversed history\[2\]). Don't ask me for any current actually useful applications of "smart contracts" (I am active in Lightning but I would not call it a smart contract), but the pipe dream is to "code as law", better in theory because code is clearer than laws. With a proper programming language, you should be able to avoid reversing history like Ethereum did, which violated that philosophy. \[0\] [https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md](https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md) \[1\] [https://github.com/ElementsProject/simplicity/blob/pdf/Simplicity-TR.pdf](https://github.com/ElementsProject/simplicity/blob/pdf/Simplicity-TR.pdf) \[2\] [https://en.wikipedia.org/wiki/The\_DAO\_(organization)](https://en.wikipedia.org/wiki/The_DAO_(organization)) &amp;#x200B;
Usually in this context we’re talking about so-called “smart contracts”. See this description (from here which is a good survey of smart contract languages https://eprint.iacr.org/2018/192.pdf): &gt; The term “smart contract” was conceived in [43] to describe agreements between two or more parties, that can be automatically enforced without a trusted in- termediary. Fallen into oblivion for several years, the idea of smart contract has been resurrected with the recent surge of distributed ledger technologies, led by Ethereum and Hyperledger. In such incarnations, smart contracts are rendered as computer programs. Users can request the execution of contracts by sending suitable transactions to the nodes of a peer-to-peer network. These nodes col- lectively maintain the history of all transactions in a public, append-only data structure, called blockchain. The sequence of transactions on the blockchain de- termines the state of each contract, and, accordingly, the assets of each user. A crucial feature of smart contracts is that their correct execution does not rely on a trusted authority: rather, the nodes which process transactions are as- sumed to be mutually untrusted. Potential conflicts in the execution of contracts are resolved through a consensus protocol, whose nature depends on the specific platform (e.g., it is based on “proof-of-work” in Ethereum). Ideally, contracts execute correctly whenever the adversary does not control the majority of some resource (e.g., computational power for “proof-of-work” consensus). Every node runs the code, and they all ought to produce the same result, except when they don’t and that’s why you have a consensus. The questions you have about infinite loops and storage, side-effects, etc. are the kinds of design questions that people like Wadler and cryptocurrency authors are thinking about. By making a language not Turing complete, for example, that’s one easy way to avoid infinite loops. But they are fraught with bugs. 
Thanks for your hard work, guys!
A lot of people (myself included) enjoy managing our compilers through `stack`. It makes a lot of things simple, especially if your system's package manager also provides a version of GHC and `cabal`, etc etc. So, having an LTS that provides `8.6.3` means we can start basing our projects off it and get all the newest goodies. Yes this has always been possible if you're not a `stack` person and have managed a working balance, but plenty enjoy `stack` for these things, and that's okay.
Oh indeed! I use `stack` myself and I'm just fairly ignorant of said goodies from 8.6.3
&gt; What does it mean for me to write a program and run it on a blockchain? Perhaps surprisingly, a block in a blockchain such as Bitcoin's doesn't contain ledger entries such as "send X bitcoins to Y", but rather scripts in a programming language specified by the blockchain project, such as [Bitcoin Script](https://en.bitcoin.it/wiki/Script). So writing a program for a blockchain means writing a script in the programming language specified by that blockchain project, or in a language which compiles to that language. Any machine which wishes to examine the blockchain in order to determine how many bitcoin each address holds must execute all those scripts in order to compute the answer. &gt; How do I even upload it there? You send it to the pool of miners, one of whom will accept it and add your script to their block. You thought you were asking the pool of miners to accept your transaction, but in fact your bitcoin wallet was sending them a short script, not a transaction! &gt; Computation isn't free, I assume, so how does payment for the computation itself work? The amount of computation per block is limited. Miners are compensated for their work in the usual way, by receiving a reward if they are chosen to sign the next block. Then every other miner has to execute the chosen block as well without compensation, and that's part of the business cost of being a miner. In addition, some blockchain projects such as Ethereum allow you to upload long-lived programs which do _not_ get run automatically. After uploading it, you may send money to that program to pay for it to be executed for a few more steps (or a lot more steps if you send it a lot of money). Again, all the nodes must run your program for that many steps even if they don't get chosen to sign the block in which you send the money to the program. &gt; If I write an infinite loop, will it just burn the funds in my account until it's empty? It will burn through the funds you sent to that program, not the funds in your account. &gt; But... where did that money go? To the party who was performing the evaluation? The money disappears forever, it has been spent. Despite the fact that cryptocurrency has monetary value, it does not mean that it never gets created nor destroyed. On the contrary, it constantly gets created each time a miner receives a reward, and constantly gets destroyed whenever somebody sends money to a program (and in other circumstances too). As the amount of cryptocurrency available changes, it affects its value, as per the usual offer vs demand curves. &gt; How do I deal with data storage and retrieval? Surely there's no Postgres instance for me to connect to, so... is data itself stored in the blockchain as well? Some blockchain languages store data as part of their source code, while others give programs access to some data addressing scheme, which means that all the nodes which run all the programs will also have to store all the data of all those programs. Blockchain languages can choose whatever semantic they want, so it would be totally possible to give an SQL primitive, which would in turn require all nodes to store the data of all the programs in something like a postgres instance. It's also possible to provide primitives which access data from some globally-available source, e.g. "get the Dow Jones at the close of day X", or "get the text of page 6 of the New York Times on day Y", but obviously, writing data to a single global place would be more difficult since there would be many nodes attempting to write to the same place. &gt; Finally, how fast does stuff run? Distributed consensus is notoriously slow, so much so that many platforms just outright abandon data consistency so they can have reasonable performance - and that's with running native code on real hardware owned by a single party. Much, much slower than that.
&gt;constantly gets destroyed whenever somebody sends money to a program Is this how it works on Ethereum? Because on Bitcoin, miners collect the fees. Also worth mentioning is that the supply function might not be constant. With Bitcoin, miners will have to do with fees in the future, since rewards converge to zero...
I built with stack. How the hell does `cabal gen-bounds` know which versions of dependencies the package was built with?
&gt; Perhaps surprisingly, a block in a blockchain such as Bitcoin's doesn't contain ledger entries such as "send X bitcoins to Y", but rather scripts in a programming language specified by the blockchain project, such as Bitcoin Script.... Any machine which wishes to examine the blockchain in order to determine how many bitcoin each address holds must execute all those scripts in order to compute the answer. That is indeed surprising, and makes this whole thing much clearer! &gt; In addition, some blockchain projects such as Ethereum allow you to upload long-lived programs which do not get run automatically. After uploading it, you may send money to that program to pay for it to be executed for a few more steps See this makes me question the choice of a high-level programming language like Haskell, because now it sounds like I’m suppose to have some sort of awareness for how much “gas” my program needs to run to completion, and that’s an extremely difficult question to answer. I can reason about whether my code has the right asymptotics, but in terms of predicting the actual number of primitive operations, I have no idea how to reason about that in this language. Or is there a convenient way to spend “up to” some amount of effort on a program? If you can’t “overspend” effort on running a program, maybe it’s ok. &gt; Much, much slower than that. Ya, with replicating all computation and all data across all nodes, that sounds *extremely* expensive. I assume there’s efforts towards using homomorphic encryption to at least memoise the computation?
&gt;See this makes me question the choice of a high-level programming language like Haskell, because now it sounds like I’m suppose to have some sort of awareness for how much “gas” my program needs to run to completion Plutus is simply a DSL encoded in Haskell. What would be a better candidate? I don't know of any programming languages that have execution time in the type system.
https://github.com/urbint/graphql-meta is faithful to the spec 
I’m not saying I have a perfect answer, just that if extreme frugality with resources is important, then usually you don’t choose a lambda calculus based approach, and given the very high cost of executing computation in this environment, it does seem like a valid concern. That said, if correctness is paramount, then sure, I do think this is best approach.
I'm not a cryptocurrency person, but I was in your situation but long ago. This basic paper explained it pretty well: https://github.com/ethereum/wiki/wiki/White-Paper
&gt; Or is there a convenient way to spend “up to” some amount of effort on a program? It sounds like it would be easy to automatically reimburse the unused gas, so maybe that's already how it works I'm not familiar with the details, and they're likely to differ from one blockchain project to another anyway. &gt; I assume there’s efforts towards using some homomorphic encryption-ish thing to at least memoise the computation? I have no idea what that would look like and I have no idea what kind of research is currently under way, sorry. I know just enough to give a broad overview.
&gt; I have no idea what that would look like It was just a shot in the dark—homomorphic encryption lets me give you an encrypted program, you run it and give me a result, and I can verify that the result is correct without actually running the program myself. My idea was just that something like this could allow one node to do the computation, then the others can just steal that answer and only need to bother performing a quick check of correctness. No idea whether that would actually work or not 🤷🏼‍♀️ 
&gt; Is this how it works on Ethereum? Because on Bitcoin, miners collect the fees. I haven't studied either Bitcoin nor Ethereum in details, I tried to give a broad overview which answered all the questions, but I might have gotten a few details wrong.
Protip: create a `.ghci` file in the folder you're starting ghci from and throw your settings in there. It will stop you from reentering those commands. 
It's done :) https://www.lulu.com/shop/sandy-maguire/thinking-with-types/hardcover/product-23922949.html
All done! https://www.lulu.com/shop/sandy-maguire/thinking-with-types/hardcover/product-23922949.html
All done! https://www.lulu.com/shop/sandy-maguire/thinking-with-types/hardcover/product-23922949.html
There are epubs now! You can get them off leanpub alongside the pdf version :)
All done! https://www.lulu.com/shop/sandy-maguire/thinking-with-types/hardcover/product-23922949.html
It just uses the plan it comes up with in `cabal new-configure` and assumes you can build with that plan, then suggests tight bounds on the major versions of the dependencies. i.e. you have to test that it's correct with `cabal new-build` and make sure everything builds correctly.
Even if you’re not using Stack, it’s a good indicator about the level of support for GHC 8.6 within the Haskell ecosystem.
What metrics are measurably improved by this optimization? Is peak FPS improved at all? Or just idle CPU?
The GHC 8.6.3 release cycle is finally complete! Only when a LTS has been released the GHC is considered stable and ready for production.
Some cool feature like deriving-via come in 8.6, which could help reduce boilerplate (i.e options for making bugs) in some cases.
What's the benefit of haskell-ide-engine when we already got intero?
Both, depending on your CPU. With my (old) MacBook Pro, it was maxing out a single core and visibly slowing the game down, and (by that stage) I'd only got the pacman moving and displaying static ghosts. Now on the same MacBook Pro, CPU utilisation sits at about 4% for a single core during game play. I'm unsure if it's 'skipping' frames if all the ghosts and pac-man move during a frame (and thus the ncurses library needs to update all of their position), but overall it gives a smooth game play. But, to answer your question, idle CPU utilisation is much improved.
I can't honestly disagree, but I also can't substantiate the claim. Can you?
IMO it's a lot more sophisticated than intero, and the architecture is much more robust. Intero is fairly stack-tied, despite its exe's supposed independent of stack, which means I can't really use it unless I'm using a standard, non-trivial stack setup.
Are you usinh GHC 8.6.3? https://www.reddit.com/r/haskell/comments/a1u9qj/monthly_hask_anything_december_2018/ec9toka
[https://github.com/arybczak/haskellx18/blob/master/code.hs#L93-L106](https://github.com/arybczak/haskellx18/blob/master/code.hs#L93-L106)
I’m not sure I understand the question. GHC has a dependency on some libraries. Now, because we can only depend on a single version of a library, depending on the ghc package means that you’ll freeze the version of boot libraries to those which ghc was compiled with. That’s especially true when using a stack LTS. You could, in theory, treat the ghc library like any other library. But ghc doesn’t use a plain cabal build system (plus, it takes time even to build stage 1), so nobody has invested the time to make it possible.
My bad, I had understood writing API servers, not clients. Here's an attepmt to solve the problem using generic-lens and generic-sop: https://github.com/danidiaz/subrec
My favorite comparison is git. It is a linked list where each node is identified by the hash of it's content and contains the hash of its parent node. But instead of text files the block chain stores programs for a virtual machine. To get the state of the block chain you start at the root and start evaluating the transactions (aka vm programs). For bitcoin this currently takes a week, I think. After that you can do incremental updates. 
See https://github.com/commercialhaskell/lts-haskell and therein linked [blogpost](https://www.fpcomplete.com/blog/2014/12/backporting-bug-fixes): &gt; LTS (Long Term Support) Haskell is a curated set of packages, a more **stable** companion to Stackage Nightly. &gt; In an LTS release, bug fixes are backported to a **stable** version for an extended period of time. This allows users to have **stability** without stagnation. Over the next month, a few of us in the community will be working towards the goal of an experimental "LTS Haskell" kind of project
Let's unpack this a bit &gt; IMO it's a lot more sophisticated than intero Why is more complexity a good thing? &gt; , and the architecture is much more robust. In what way is Intero less robust? It's been working fairly well for me. Whereas I couldn't get haskell-ide-engine working at all. &gt; Intero is fairly stack-tied, despite its exe's supposed independent of stack, which means I can't really use it For the sake of the argument if I'm using what is currently considered the best and most popular build tool for Haskell, haskell-ide-engine has no benefit to me, no? &gt; unless I'm using a standard, non-trivial stack setup. What do you mean by "non-trivial" here? In my experience Stack is fairly easy to use and doesn't require you fussing around to figure out the correct GHC version to download and install it as you have to when you don't use Stack. 
Why, you heathen, blockchain is a means AND an end. It's enough to mention it enough times and .. uh, "disruption" or something happens.
Rich is very invested in his current PL
lol, I won't deny the thought has crossed my mind. But given the number of people I already respected who are involved in the engineering of this project, I'm willing to give it a fair shot at trying to understand what it is and how it might be useful.
List Comprehension has one big pro against lambdas and other functions, you can access more then the built in functions give you as parameters, for example if you want to access the whole List while filtering.
I'm not sure what the default version is, I'm guessing that comes from the stack installer? I installed the newest stack (1.9.3) via their installer, cloned haskell-ide-engine, stack cabal install, cabal update and then ran the build-all script. After multiple attempts all of which got stuck at the fclabels point, I binned all the installed stuff and re-installed stack, edited the build-all script and removed the 8.6.3 target. I'm now trying to "build-all" currently. I can presumably just add 8.6.3 back later, it seemed fairly tolerant of me squashing the previous builds and picking up where it left off. I have no idea how to fault find the "hung" ghc build of fclabels though.
If I’m not mistaken, Intero is only for Emacs. HIE can be used in any editor, that understands the Language Server Protocol. Imagine telling someone, who’s learning Haskell, to switch to Emacs and in addition to Haskel learn that. I think it would be unnecessarily discouraging.
I was under the impression that compiling with GHC required you to use the same versions of the boot libraries that GHC itself was compiled with. Even if you don’t depend on the GHC package. In the case that you actually are depending on the GHC package itself (and not just building with it) the current situation seems reasonable. I would like to be able to independently vary my GHC version and the versions of the boot libraries I depend on, to basically treat GHC as a stand-alone binary that doesn’t come with library versioning strings. Besides `base` (or ideally not even that, just `ghc-prim` or similar). 
- Note that **Extended period of time** is about 6 months, lately. - *Anything saying it has LTS should have an EOL (end-of-life) date.*
Hm, thanks for the info. Seems I have to restructure my program, then.
When you run a program on AWS, you usually don't get the output yourself, instead you're usually providing a service to others. Developer -&gt; AWS -&gt; User But the users can't see what code is running, let alone provenance of the data or access control policy for it, and have no contractual agreement with AWS at all. So this is one reason running on a blockchain is different
When you run a program on AWS, you usually don't get the output yourself, instead you're usually providing a service to others. Developer -&gt; AWS -&gt; User But the users can't see what code is running, let alone provenance of the data or access control policy for it, and have no contractual agreement with AWS at all. So this is one reason running on a blockchain is different... users get some visibility and guarantees about what program they're interacting with. It's clearly important for currency, but also for other applications involving shared data like auctions, voting, name registries, etc
[no, no ..]
《 oh, oh 》😶
《 oh, oh 》😶
I'm not sure that it should be possible, but it certainly was possible to specify `extra-deps` on i.e. `containers` last time I checked. `base` is a different story, as the implementation is specific to GHC. I'm not sure if there are any kind of backwards/forwards compatibility wrt. `base` and the GHC internals it uses. Apart from this, It's just not how GHC manages its dependencies, I'm afraid. GHC tracks them by submodules (`base` isn't even a submodule, but versioned alongside GHC), so it's pretty much like GHC specifies its own `extra-deps` from a specific git repository. I agree that this could be more integrated into hackage. Maybe it's because I'm not particularly affiliated with the devops stuff that I don't see an obvious reason, but when in doubt, there's always lack of interest and/or man power.
The book [Category Theory for the Sciences](http://math.mit.edu/~dspivak/teaching/sp13/CT4S.pdf) also goes into this connection. It's a nice book to get introduced to this stuff.
You might be able to just build using GHC 8.6.2 instead, which doesn't hang on Windows.
I'm pretty sure almost nothing ever gets backported to old LTSes.
Sophisticated doesn't mean complexity. I meant HIE supports a more powerful feature set. It's not any harder to use (once you have it set up, which I'll admit is more difficult). HIE has benefits other than working with tools other than stack. It has a different feature set that I much prefer. I meant to say trivial. Once your setup needs anything nontrivial, I find stack and intero fall short extremely quickly
To whoever reads the post in its 2.5 hour entirety: Please make yourself known here so I can upvote that chutzpah 
Hahaha maybe it's a bit long lol, but that's life 😁
No no no it's not too long! The entire post has a different audience than the abstract. And it has my favorite opening sentence of anything I've read in a long time
Bahahaha I'm glad to read that, I think 😅
Windows users take care, it seems GHC 8.6.3 has a problem with Template Haskell, as in it segfaults. https://ghc.haskell.org/trac/ghc/ticket/16057
There is both [a version of intero for vim](https://www.reddit.com/r/haskell/comments/9fmca7/intero_for_vim_8_1/) and [a version of intero for neovim](https://github.com/parsonsmatt/intero-neovim).
[removed]
Haha, yeah. I'll read this *later*. RemindMe! 3 years
I will be messaging you on [**2021-12-25 19:07:41 UTC**](http://www.wolframalpha.com/input/?i=2021-12-25 19:07:41 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/a9ffe0/graphs_are_to_categories_as_lists_are_to_monoids/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/a9ffe0/graphs_are_to_categories_as_lists_are_to_monoids/]%0A%0ARemindMe! 3 years) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
It seems 8.6.4 will address this issue.
Isn't it more "graphs are to categories as sets are to monoids"?
I love Haskell libraries like this which provide a nice interface to an external library. Ollie always designs beautiful interfaces as well. Amazing work!
Thank you! I have to emphasize it took multiple tries to get here, but it just goes to show that it's always worth asking and reasoning "ok but what really *is* this?". It was only after spending months asking that I noticed the possibility to have STM like transactions. Thanks for the kind words!
I find the first part very important, with age I did start to think that quantity was a very thin projection of information and that counting was more related to topology/graphs than the numeric reduction we tag at the end.
Location matters. Why not advertise people move to Venezuela and offer like 1000 USD a year: that's 3 times more than programmers there seem to make, you'll have a high standard of living... "Move to cheap place to make relatively more money" is just a really poor pitch in my eyes. If I can have same standard of living in the valley, I know which I'll pick.
Monadic recursion schemes? Interesting. What's the other difference between this and the typical `recursion-schemes` library?
Sounds like a build system...so, [shake](http://hackage.haskell.org/package/shake)?
This sounds like a build system. You should definitely check out shake. You may not actually need everything shake provides, or even need to use shake, but I highly recommend reading their paper on its design, or even their latter paper, Build systems a la cart. 
I want to create a new configuration management system similar to SaltStack and Puppet. I liked SaltStack's dependency management. But, I didn't like YAML. YAML is fragile.
It was an analogy of SaltStack. I want to create a minimal alternative to SaltStack in haskell, clojure, or hylang.
Is “dependency injection” just another name for... capabilities? (In the operating systems/programming languages literature sense, not the Linux sense.)
What I would like to know from Stackage curators is their first experiences on whether the faster GHC release cycle made the upgrade from lts-12 to lts-13 less work or not, and a summary of pain points (if any).
Sounds like you want a DAG, which is what they use in Airflow to determine task dependencies. I would start thinking about how to represent your tasks as graphs.
... well *fuck*, I really could have used this a month ago!
Have you seen Nix or Dhall?
Although I wanted to create a configuration management system, after watching https://www.youtube.com/watch?v=BQVT6wiwCxM, I concluded that the paper, `Build Systems a la carte` will help me create a configuration management system.
I watched https://www.youtube.com/watch?v=BQVT6wiwCxM and decided to follow the footstep of Cloud Shake.
Yeah, I don't get it. You would need a unique generator to impose a list-like structure on a monoid, and even then it's actually a cycle rather than a list. Unless I'm totally missing the point. Disclaimer: I didn't read the whole article.
How distinct is the low-level problem space? I've never heard of the solver you're using, but it looks like you're having to go through some (de)serialization layer to use it. Is the problem space distinct enough that it's worth that? I say this partly because sometimes [simply accessing the solver through these serialization layers](https://github.com/ekmett/ersatz/issues/16) can dwarf the cost of actually solving the problem! As far as solvers with direct (no serialization) interactions go, I've gotten good mileage out of [glpk-hs](https://hackage.haskell.org/package/glpk-hs) for linear problems, and of course, soon we will have [guanxi](https://github.com/ekmett/guanxi), which also uses a native solver with no serialization layer sitting in the middle. Anyway, the problem space you're exploring looks fun - I need to investigate more!
read accepts a "String", which in Haskell is eq to "[Char]". "[]' is a member of the type class Applicative, so when we look up the type signature of pure, we see, pure :: Applicative f =&gt; a -&gt; f a Thus, when pure is applied to a "Char", the type returned is "[Char]", type synonym of "String'. An alternative expression would be "read [x] :: Int". If that explaination doesn't make sense, read up on "Type Synonyms" and "Type Classes". 
There's also Haskero for [Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=Vans.haskero).
If D really depends on A and B and E on B and C then most likely there genuinely is something they consume that is produced by the others. Thus write them as functions in some monad a :: M A b :: M B c :: M C d :: A -&gt; B -&gt; M D e :: B -&gt; C -&gt; M E job :: M (D, E) job = do a' &lt;- a b' &lt;- b c' &lt;- c d' &lt;- d a' b' e' &lt;- d b' c' return (d', e') 
When you have the tasks represented as a DAG, a topological sort will give you an execution order
I'm afraid I don't really know what you mean by "distinct" in this context. However, I can answer the serialization stuff. &amp;#x200B; There is (naturally) some type of serialization happening, as Fast Downward is a standalone executable that I can only communicate with through file formats. The serialisation is fairly light weight though, so the actual encoding/decoding of that is unlikely to be the problem. \[Here's how the gripper problem is encoded\]([https://gist.github.com/ocharles/900ffc2990f166fe2ce633c1a3614f3e](https://gist.github.com/ocharles/900ffc2990f166fe2ce633c1a3614f3e)). The bigger problem will be the enumeration I describe \[here\]([https://ocharles.org.uk/blog/posts/2018-12-25-fast-downward.html#behind-the-scenes](https://ocharles.org.uk/blog/posts/2018-12-25-fast-downward.html#behind-the-scenes)). Currently, the algorithm to find the fixed is very naive - it simply runs everything until it stops changing. This could be made faster if I used some type of propagation network so I would know that if variable X changes, then effects A, B and C need to be re-ran. Naive enumeration has been fine so far, including for solving problems in the larger domain at work (which still produces answers in under half a second). &amp;#x200B; Thanks for introducing me to \`glpk-hs\`, I only knew of \`sbv\` before, so it's nice to expand my toolkit!
&gt; I'm afraid I don't really know what you mean by "distinct" in this context. I just meant that if your current solver substially exploits “domain specific awareness” of your problem space, then using a different solver might be a disadvantage even if it elided the need for serialization. &gt; Thanks for introducing me to _glpk-hs Ya it’s good, just a bit dated—I had to make patches for the recent MonadFail changes. I should probably be a good citizen and submit them as PR.
You want to do minimal configuration (eg lots of defaults), or do you want to keep the system to a minimum? Anyway, I'd recommend nix: https://ocharles.org.uk/posts/2014-02-04-how-i-develop-with-nixos.html
I haven't found a system that I liked. I don't want to spend too much time on creating a program. So, I will probably create a minimal configuration management system. Otherwise, coding will take a lot more time than actual configuration management.
&gt; [...] but I don't have any intuition for what "running my program on the blockchain" even means. In fact, for me this phrase breaks the entire metaphor of cryptocurrency: the analogy I'm familiar with for what blockchain is is, well, currency, where I have a wallet and I transfer money between wallets, and "running a program" in this context is a meaningless statement that makes no sense to me. The key word here is *transfer*, because a *program* is exactly what decides who's able to transfer money from one wallet to another. When Satoshi Nakamoto was writing the original Bitcoin client, he kept encountering new and useful transaction forms. First there's the obvious "single recipient", but it's also useful for multiple recipients to be able to mutually own currency. In addition, constraints like "is locked until March 21st 2019" also proved useful, and the more he worked with it the more useful transaction types he came up with. The solution to this problem was a "transaction script". Rather than predefining a bunch of fixed transaction types, each transaction contains a so-called "script", which is executable code that -- when combined with the script of a redeeming transaction -- evaluates to some value. The value this script evaluates to determines whether a "redeemer" (someone who wishes to transfer to a different wallet) is eligible to do this. In order to prevent other people from transferring money from your wallet to theirs, these scripts usually contain some sort of public key, and a requirement that the redeeming transaction contain a signature that verifies against this public key. For a node in the a blockchain P2P network, it's necessary to verify the transfers, in order to know whether the given chain is valid. This is done by, for each transaction, executing the transaction script ("running a program on the blockchain").
Ya, your and gelisam's clarification that transactions are actually just scripts makes the whole thing make much more sense. From the video, my understanding is that Cardano's scripting language is indeed Turing complete, which means it presumably does encounter the questions I raised about "how do you actually know how much gas you need to run your program before you run it?", which as you've described, does not seem to be an issue in Bitcoin's much simpler scripting language. &gt; Consensus is probabilistic (at least with Bitcoin), but it's in the order of hours. Wow, that's slower than I expected. I assume there are attempts to bring this down some, but ya, that does kinda shoot a hole in the World Computer pipe dream: if I have to wait 3 hours for my RedditCoin comment to appear, I'm just going to use the proprietary, centralized, ad-filled Reddit.
I'd love to see this, but like… Rust has a CloudABI target and not much has been built using it… let's make CloudABI itself more popular first? Oh, and a prerequisite for building any reasonable CloudABI apps is getting everyone to use directory descriptors — `openat(dir, "some_file")` instead of `open(join(dir_path, "some_file"))`. (And `connectat`/`bindat` for UNIX sockets, `renameat`, `unlinkat`, etc.) I don't think I've seen anything do that in Haskell libraries :( Let's start with that?
A fairly simple solution that I quite like is to use `async` such that every node in a computation graph runs asynchronously. Each of these nodes has a `TMVar` that it fills when it's complete. Now sequencing tasks is simply reading everyone else's `TMVar`s before you do any work - the runtime system and scheduler can take care of the rest!
You "OOP" solution doesn't mention anything object-oriented, and in fact it is exactly how you would do it in Haskell - except that you would use a recursive IO action instead of a while-loop (but then again, the monad-loops package implements loops as abstractions of monadic patterns, or you can trivially implement that abstraction yourself, so you can in fact use a while-loop after all). The trickiest part here is that you need two threads: one for the "every second" part, and one for "get input and check against 'E'" - but again, this is the same problem regardless of FP.
I think you meant imperative, not OOP
The point isn’t “have no state”, but rather “be conservative and explicit about your use of state”. In Haskell, I’d have two threads and an MVar to hold the “E was pressed” state.
yes, thank you.
Hopefully this is not homework. I was curious to see how hard it is to implement [my understanding of] this spec, without threads or exotic libs. FWIW here's what I came up with. Unsolved: it should discard all extra input each time so that an E is processed right away. import Control.Concurrent import Control.Exception import Control.Monad import Data.Time import System.IO import System.Console.ANSI main = do bracket_ (hideCursor &gt;&gt; hSetEcho stdout False &gt;&gt; hSetBuffering stdin NoBuffering) (showCursor &gt;&gt; hSetEcho stdout True) loop loop = do threadDelay 1000000 t &lt;- getCurrentTime hasinput &lt;- hReady stdin c &lt;- if hasinput then getChar else return ' ' putStrLn $ show t ++ " " ++ [c] if (c == 'E') then putStrLn "end of story" else loop 
This is an interesting suggestion but I wouldn't trust myself to not create `waiting on MVar` issues. What do you recommend to keep yourself out of trouble?
Well if you do some kind of depth first traversal of the graph and only ever add dependencies on things "before" you (reachable from the vertex you started and not passing through the vertex you are at) I think you're in safe territory. I'll admit I haven't given it a huge amount of thought!
Off topic: I am also pondering the following puzzles observed when running the above with ghcid -T: 1. why does it receive junk input ? ("INTERNAL_GHCID...") stack exec -- ghcid a.hs -Tmain Loading ghci -fno-break-on-exception -fno-break-on-error -v1 -ferror-spans -j a.hs ... GHCi, version 8.6.2: http://www.haskell.org/ghc/ :? for help Loaded GHCi configuration from /Users/simon/.ghci [1 of 1] Compiling Main ( a.hs, interpreted ) Ok, one module loaded. All good (1 module, at 15:03:25) Running test... 2018-12-26 15:03:26.859998 UTC I 2018-12-26 15:03:27.861969 UTC N 2018-12-26 15:03:28.863842 UTC T 2018-12-26 15:03:29.86539 UTC E end of story &lt;interactive&gt;:24:1-19: error: Not in scope: ‘RNAL_GHCID.putStrLn’ Perhaps you meant ‘INTERNAL_GHCID.putStrLn’ (imported from System.IO) No module named ‘RNAL_GHCID’ is imported. 2. Why does it hang after the above output and error message ? 3. Why do I have to press control-C not once but twice to kill it ? 4. Why in this case does it fail to restore the terminal's cursor visibility ? 
First of all, both Data.Map and Data.HashMap are implemented as trees (Data.Map is a binary tree, Data.HashMap is a trie). Secondly, you *can’t* actually have a structure with constant-time updates of random elements in pure FP. You can get close, with very large n for some log_n, but true O(1) access/update has been proven impossible (I don’t have the paper handy, but it’s a well-known result). The solution to this problem is usually to restructure your algorithm to not rely on this particular data structure. If you tell me what the algorithm is, I can try figure out a way to do that. Unfortunately, though, for some algorithms it is impossible. Finally, to answer your actual question: Data.Map is a size-balanced binary tree. That means that every node has to maintain its size, so taking the size of the tree is O(1). In Data.HashMap’s current design (emphasis on current there, there is talk of this changing), it simply doesn’t need to maintain the size for bookkeeping. So yes, you could simply add a counter for the size (and AFAIK there’s a package on hackage which does this), but what’s provided in Data.HashMap is the “minimal” version of the structure, which imposes minimal overhead.
The last build I did with the 8.6.3 target removed completed fine so it has all versions of GHC except the newest. That will probably be fine for the time being given that I'm still a Haskell beginner. Thanks again.
Coming from the Python / Julia world, I was also pretty frustrated with this. What I ended up finding acceptable was VS Code with Haskelly (some have recommended Haskero, I haven't tried that), and just adding a "main" function in each script I was fiddling with, which lets you run it. You can also use the REPL to load scripts and play with the functions there. Programming with the REPL alone is kind of unacceptable, and getting involved with building projects is usually overkill if you're hacking around. &amp;#x200B; There's a Haskell kernel for jupyter (IHaskell), but it is tedious to set up on Windows so I haven't used it.
Thanks for the well thought-out reply. &gt;Secondly, you can’t actually have a structure with constant-time updates of random elements in pure FP. You can get close, with very large n for some log\_n, but true O(1) access/update has been proven impossible (I don’t have the paper handy, but it’s a well-known result). What if they implemented `Data.HashMap` by wrapping some C code? &gt;In Data.HashMap’s current design (emphasis on current there, there is talk of this changing), it simply doesn’t need to maintain the size for bookkeeping. Who do I lobby to have this changed??
&gt; I was under the impression that compiling with GHC required you to use the same versions of the boot libraries that GHC itself was compiled with. Even if you don’t depend on the GHC package. This is not exactly true, but the story is a bit more complicated than that. Essentially though, you are able to vary non-wired boot libraries. This is why e.g. Even though win32 and process are boot libraries you can use any version you want in your own programs. Wired in package such as the rts, base, ghc-prim, etc can't be changed because of the ABI issues between the different versions. Base can never be varied due to the various hooks from the rts into it. The rts initializes calls on it's own back into haskell world, it does this through closures we hardwire into the rts. If those are missing or have a different ABI things will fall apart, either loudly or subtlely. This is why even the build system treats rts and base special. Conversely base has method calls back into the rts, these calls aren't optional. If they're missing things will just not work. So since every ghc haskell program requires the rts, you are bound to the base and ghc-prim version that goes with it. This is why base isn't a submodule. Semantic versioning and dynamic libraries may give you some degree of movement, but the minor version of base changes almost every release as APIs are added and changed. So likely you'd still be stuck at the same version. 
\&gt; in pure FP as they said. Once you factor in the FFI all those assumptions go out the window and of course its possible.
&gt;in pure FP as they said. Once you factor in the FFI all those assumptions go out the window and of course its possible. 
&gt; I guess compiler is not supporting stdcalls anymore, but what should I do if that's what Excel is using? Also should I be worried if Export Viewer can't see anything? The stdcall calling convention doesn't exist anymore on 64 bit windows. The only calling convention on there is the "x64 calling convention" (no name) which most Compilers will use when you use the ccall. You will still need stdcall for the 32 bit version of the caller is expecting it (in which case you need to use the 32 bit version of ghc). 
&gt; What if they implemented Data.HashMap by wrapping some C code? That's a good question, and gets to the core objective of "pure" data structures. The problem is that (in Haskell) we can't even *talk* about mutation. In other words, if you have: xs = newArray [1..10] ys = updateAt 0 (\x -&gt; x + 1) xs `xs` *has* to be kept around in case you want to look at it. The only way to do this consistently is by taking a full copy of the array for *each* update operation. This is why we don't really use arrays that much in pure FP: not because we can't implement them, but rather because the semantics of the language doesn't allow for the destructiveness that makes them fast. So the problem is that we need the old versions of structures around, which isn't solved by calling out to C. This is also why using something like `ST` can often solve the problem: its semantics precisely allow for destructive changes. (It's also what linear types aim to give us) For your second question, check out the [github repository](https://github.com/tibbe/unordered-containers). I'm afraid it looks like [this issue](https://github.com/tibbe/unordered-containers/issues/138) has already been discussed and decided against, though!
\&gt; gold has gotten pretty stable as a proper ld replacement, as I understand it. For mainstream features, perhaps. However while it links fast the resulting code is often behind and lacks a lot of features. e.g. as far as I know it doesn't support any of the new GNU notes tags. Judging from the fact that the gold sub dir hardly gets any updates [https://github.com/bminor/binutils-gdb/tree/master/gold](https://github.com/bminor/binutils-gdb/tree/master/gold) I'd say it'll always be quite far behind. So yes it's a replacement for fairly straight forward compilations (why glibc or the linux kernel can't use it afaik). \&gt; But I believe that using clang as the driver instead of gcc works pretty much perfectly with lld, and clang is almost a perfect substitute for gcc Only for assembly code generated by clang. gas/ld still support more relocations than clang/lld. \&gt; So other than historical baggage, the only reason lld isn’t the default is because clang isn’t the default. Well, not really. a patch to support \`-fuse-ld=lld\` in GCC has been floating around for ages, it was never accepted because LLD had bugs that the maintainers weren't in a hurry to fix. These have now been done and GCC 9 will support \`lld\` as a linker. So why couldn't GHC have switched on it's own? well because on certain platforms we're sort of tied to how ld produces symbols for image-base etc. It wouldn't have been easy to support until this incompatibilities were taken care of. \&gt; As for gold, I’m not sure why it’s not default. I can't speak for everyone else, but for me making it the default means actively supporting a linker that isn't in very active support upstream on it's own. It's trivial to use it yourself anyway with \`-fuse-ld=gold\`. \&gt; I’m guessing there are still some minor compatibility quirks with niche use cases or something. Or maybe it targets fewer platforms. Well yes, gold is \`ELF\` only, so wouldn't be a usable on \`MacOS\` or \`Windows\`. \`LLD\` is cross platform but up until quite recently had bugs and an incompatible CLI. e.g. no Linker scripts on Windows etc. At the end of the day, the reason LLD and GOLD are that much faster is that they have less abstractions. They support a fraction of the platforms ld support and they support a mainstream subset of the features bfd does. But I do suppose that for the majority of programs people write using GHC they should be stable enough. If they work. &amp;#x200B;
Never thought I'd see a question I could actually answer properly here! I've been working on a PR for [this](https://github.com/tibbe/unordered-containers/pull/170) for well over a year (open-source takes time that most people already don't have in abundance). I'm reasonably sure every change that PR introduces does not break anything (tests are useful for this, I can go into more detail if you want), but the main problem is performance. The `unordered-containers` library performs quite well, and if a change introduces even a minor penalty in performance, its benefits have to be weighed VERY carefully with this penalty. In this case, I modified the `HashMap` datatype to carry size information, which means ALL users of the library have to "pay" for this change when they use the datatype, but not all users would find a constant-time `size` useful. Furthermore, actually fixing these performance penalties is nontrivial sometimes, which is actually the case here. Specifically, I've been trying to understand why `foldl` and `foldlWithKey` become MUCH slower, in spite of not caring about the hashmap's size at all. In any case, I hope this answers your question, at least with regards to `HashMap`. It's difficult to have the best of both worlds: speed and minimal overhead, or these utility "quality of life" features.
&gt; In `Data.HashMap` [...] `update` is O(1). Where did you get that figure? According to [the documentation of `update`](http://hackage.haskell.org/package/unordered-containers-0.2.9.0/docs/Data-HashMap-Strict.html#v:update), it's O(log n). Which makes sense, because `update` is a pure function, and therefore `HashMap` is a [persistent data structure](https://en.wikipedia.org/wiki/Persistent_data_structure) which is "modified" by creating a new value which shares a lot of data with the original value. If you instead want a mutable hashmap with O(1) updates, you need to use a library like [`hashtables`](http://hackage.haskell.org/package/hashtables) whose update functions have `IO` or `ST` side-effects.
&gt; Secondly, you can’t actually have a structure with constant-time updates of random elements in pure FP. This appears true outside the realm of FP, too. Real RAM has a small O(lg n) factor based on the total addressable amount.
&gt;Where did you get that figure? From my memory, which is why I got it wrong. I was thinking of `Data.IntMap`, whose [`update`](http://hackage.haskell.org/package/containers-0.6.0.1/docs/Data-IntMap-Lazy.html#g:6 function) runs in O(min(n,W)) time, which I consider constant because I take _W_ to be fixed.
This is the sort of thing where there’s enough surface area that either Somone needs to be paid to do it, or it solves a huge hurdle in their own prjects 
This is because ghcid's `--test` flag is for running unit tests -- which are expected not to try to interact with stdin. If you just want to run the program you should use `stack runghc ./a.hs`.
[Apparently](https://github.com/ndmitchell/ghcid/blob/master/src/Language/Haskell/Ghcid.hs#L91) when you use `--test=foo`, ghcid sends ghci something like the lines `import Prelude as INTERNAL_GHCID`, `foo`, and `INTERNAL_GHCID.putStrLn "GHCID COMMAND FINISHED"`. This normally causes ghci to run `foo` and then print `GHCID COMMAND FINISHED`, so ghcid waits until it sees `GHCID COMMAND FINISHED` in ghci's stdout in order to confirm that your `foo` code has completed. But if `foo` reads a line from stdin, it will read `INTERNAL_GHCID.putStrLn "GHCID COMMAND FINISHED"` (that answers question #1). And if you do, ghci won't interpret that line, so ghcid won't know that `foo` has completed (that answers question #2). On my machine, with ghcid-0.6.8 and ghc-8.4.4, a single Ctrl-C is sufficient. But I know that in Haskell, the default handler for Ctrl-C is to send an exception to the main thread (so it can attempt to clean up gracefully) and to change the handler so that a second Ctrl-C kills the program without waiting for the main thread. So I am guessing that your version of ghcid has a different cleanup process than mine. On my machine, my cursor also disappears (type `reset` to restore it). I am guessing that restoring the cursor is part of the cleanup process which got skipped.
Here it is using `whileM` from the `monad-loops` package to discard the extra input: import Control.Concurrent import Control.Exception import Control.Monad.Loops import Data.Bool import Data.Time import System.Console.ANSI import System.IO main = bracket_ (hideCursor &gt;&gt; hSetEcho stdout False &gt;&gt; hSetBuffering stdin NoBuffering) (showCursor &gt;&gt; hSetEcho stdout True) loop loop = do threadDelay 1000000 t &lt;- getCurrentTime exit &lt;- ('E' `elem`) &lt;$&gt; getStr putStrLn (show t) if exit then putStrLn "end of story" else loop getStr = whileM (hReady stdin) getChar 
I think [RRB-Trees](http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf) should give you the time bounds you want. It's technically log base 32 (or 64) of, n but for all intents and purposes that's 5. Also: https://cstheory.stackexchange.com/questions/1539/whats-new-in-purely-functional-data-structures-since-okasaki in general.
I would write a thin IO layer around a pure core. Usually this means writing a bit of imperative code which reads in the whole input, passes it to a pure transformation (the core of the program) and then imperatively prints the whole output. But in this case, the core of your program is how it interacts with the outside world, so I understand why you found it difficult to express this program in a pure style. In those cases, I still write a thin IO layer around a pure core, but this time the thin IO layer cannot read all of the input all at once, it must read it gradually and feed it to the pure core as it happens, and it can't print the whole output at the end of the program, it must print it gradually as it is received from the pure core. One good example of this API is the [`gloss`](http://hackage.haskell.org/package/gloss-1.13.0.1/docs/Graphics-Gloss.html#v:play) library, which allows you to write interactive graphical program by writing a few pure functions indicating how you want to render your state, and how you want to update your state in reaction to key presses and to the passage of time. Since you want this to be a terminal application, I can't use `gloss` as-is. I could use [`brick`](http://hackage.haskell.org/package/brick), which brands itself as the `gloss` of terminal applications, but I don't find it as simple to use as `gloss`, so I'll write my own small wrapper instead, based on /u/simonmic's solution. {-# LANGUAGE ScopedTypeVariables #-} import Control.Concurrent import Control.Exception import Control.Monad import Data.Monoid import Data.Time import System.IO import System.Console.ANSI play :: forall s . Int -- ^ number of ticks per second -&gt; s -- ^ initial state -&gt; (s -&gt; [String]) -- ^ messages to print -&gt; (s -&gt; Bool) -- ^ whether to keep going -&gt; (Char -&gt; s -&gt; s) -- ^ react to keys -&gt; (UTCTime -&gt; s -&gt; s) -- ^ react to time passing (called once per tick) -&gt; IO () play fps initialState render keepGoing handleChar handleTick = do bracket_ (hideCursor &gt;&gt; hSetEcho stdout False &gt;&gt; hSetBuffering stdin NoBuffering) (showCursor &gt;&gt; hSetEcho stdout True) (go initialState) where go :: s -&gt; IO () go s = do threadDelay (1000000 `div` fps) t &lt;- getCurrentTime let s' = handleTick t s s'' &lt;- processChars s' mapM_ putStrLn (render s'') when (keepGoing s'') (go s'') processChars :: s -&gt; IO s processChars s = do hasInput &lt;- hReady stdin if hasInput then do c &lt;- getChar processChars (handleChar c s) else do pure s Like in `gloss`, my wrapper asks for a pure function which "renders" the current state into a form which I can display; `gloss` asks for a geometric description of a picture, I ask for a list of strings. `gloss` and I also both ask for functions which react to keypresses and to the passage of time, but my version provides the current time instead of the number of seconds which have passed, as this is better-suited for the program you want to write. Unlike `gloss`, which exits when `ESC` is pressed, I ask for a function describing the condition upon which the program should exit. Now that the thin IO layer is written, I can use this API to implement the pure core of my program: a value which keeps track of the current time and of whether `E` has been pressed yet. Note that all the state transitions are described by pure functions, not IO actions. So even though we do keep track of a piece of state, this part seems pretty purely-functional to me. data MyState = MyState { currentTime :: Last UTCTime , eWasReceived :: Any } instance Semigroup MyState where MyState x1 x2 &lt;&gt; MyState y1 y2 = MyState (x1 &lt;&gt; y1) (x2 &lt;&gt; y2) instance Monoid MyState where mempty = MyState mempty mempty myRender :: MyState -&gt; [String] myRender myState = [show t | Last (Just t) &lt;- [currentTime myState]] ++ ["end of story" | Any True &lt;- [eWasReceived myState]] myKeepGoing :: MyState -&gt; Bool myKeepGoing = not . getAny . eWasReceived myHandleChar :: Char -&gt; MyState -&gt; MyState myHandleChar 'e' myState = myState &lt;&gt; MyState mempty (Any True) myHandleChar 'E' myState = myState &lt;&gt; MyState mempty (Any True) myHandleChar _ myState = myState myHandleTick :: UTCTime -&gt; MyState -&gt; MyState myHandleTick t myState = myState &lt;&gt; MyState (Last (Just t)) mempty main :: IO () main = play 1 mempty myRender myKeepGoing myHandleChar myHandleTick 
&gt; Who do I lobby to have this changed?? As described above, if you really want to carry around the size, just carry it around in half of a tuple, and wrap the size-changing operations so that they change this size as well. If you _really_ care about performance, you can use an unboxed strict custom tuple instead of a standard one, but you shouldn't need that, most likely.
You think you're accidentally going to write a task `A` which only starts when task `B` is done, and a task `B` which only starts when task `A` is done? You can easily write a task `B` which depends on `A`: withAsync doA $ \a -&gt; do withAsync (wait a &gt;&gt; doB) $ \b -&gt; do ... But you can't write `wait b &gt;&gt; doA` because `b` is not yet in scope at that point. So you'd have to work pretty hard to shoot yourself in the foot in that way!
Does this API allow assigning a cost to each Effect? What is is the cost function that the solver seeks to minimize? I see the reference to "real cost" in [the definition of `CostType`](https://hackage.haskell.org/package/fast-downward-0.1.0.0/docs/FastDownward-Exec.html#t:CostType), but what does that mean? This is a very cool library!
The solver can attach costs to transitions, but I haven't yet worked out how that would look in this DSL. I use this library to minimize the number of transitions needed to reach the goal state (shortest path where all edges have length 1 essentially)
So that's the "mindset of FP" for many lovers and haters of FP: to waste time in exercises like this. I'm sorry but all of this is deeply wrong and does not help the widespread of Haskell 
Man, I wish I was allowed to talk about the details of what we do at work-- I guess all I can say is that we wrote a DSL (in Haskell, of course) which is very relevant to this comment. [Apply to SimSpace](https://angel.co/simspace/jobs/64261-software-engineer-backend) if you want to hear more :)
It's not surprising that it's hard to think functionally about this program, as the functional core of it is very small, most of the functionality is around IO. I think simonmic's answer is clearly the best, which is an imperative program. No need to shoot yourself in the foot by forcing in functional concepts. If you really wanted to then in general the "proper FP" way of thinking about a problem is to try to separate out the purely mathematical part, and afterwards wire it up with a thin layer of IO. It's sometimes useful to express this pure part in terms of some form of DSL, and the IO wiring as an interpreter of said DSL. In this case the pure part is a little state machine that decides what to do based on certain external events, namely a clock trigger and when 'E' is pressed. It's literally what you described, written down as a pure Haskell function that expresses the state transitions and actions to do on certain transitions. If you want to go full FP-berserk you can invent a little DSL fully capturing the domain of the program, but this would mostly be unnecessary noise. See gelisam's answer as an example.
Trying to get as close as the data definitions given by the author, I've come up with this: &amp;#x200B; data Coord = One | Two | Three deriving (Enum, Eq) data Symb = O | X | Nada deriving (Show, Eq) type TicTacToe = Coord -&gt; Coord -&gt; Symb showBoard :: TicTacToe -&gt; String showBoard b = unlines $ map showLine [One .. Three] where showLine c = unwords $ map (showHouse $ b c) [One .. Three] showHouse b' c = show $ b' c composeBoard :: TicTacToe -&gt; TicTacToe -&gt; TicTacToe composeBoard b1 b2 = b where b c1 c2 | b1 c1 c2 == Nada = b2 c1 c2 | b2 c1 c2 == Nada = b1 c1 c2 | otherwise = b1 c1 c2 play :: TicTacToe -&gt; Coord -&gt; Coord -&gt; Symb -&gt; TicTacToe play b c1 c2 player | b c1 c2 == Nada = composeBoard b b' | otherwise = b where b' c1' c2' = if c1'==c1 &amp;&amp; c2'==c2 then player else Nada emptyBoard :: TicTacToe emptyBoard = (const . const) Nada main = do let b1 = play emptyBoard Two Two O b2 = play b1 One One X b3 = play b2 One One O putStr $ showBoard b3 &amp;#x200B;
Ah no, I think what you're suggesting is what I do. Indeed, no need for extra variables.
I enjoyed it in it's entirety. Definitly skipped over some parts, but it has amazing depth to it. I have something for you to think about since you are better skilled in Agda than I am: Most (well-designed) functional programming languages can be modeled in Categories. Can we thus design an interface that feels like writing a normal program but actually interprets the result in any Category that models the underlying features? As motivation, it would be cool to output a (subset of) Agda programs as a graph to show dependencies etc.. or in the same way, interpret them in another language merely by a Functor. I hope it's clear enough what I mean by this. Some reading https://hal.inria.fr/hal-00706562v4/document http://conal.net/papers/compiling-to-categories/
That's correct! However, I'm suggesting the more useful insight: all monoids are lists with extra equations, likewise all categories are path categories with extra equations =) Happy reading!
I'm glad you've enjoyed it 😁 I've dabbled with the idea of having a multi-perspective language with the ability to choose an input interface at varying levels of complexity and abstraction. Unfortunately, I changed jobs and this intra-company project has been low priority, last I heard. I've read the second link, but not the first. Thankyou! In turn, you might find this enjoyable: https://blog.functorial.com/posts/2017-10-08-HOAS-CCCs.html Happy reading 😊
I'm not saying this is the simplest or best way of doing it, but it seemed fun to combine the effectful streams of [streaming](http://hackage.haskell.org/package/streaming) along with the dead-easy concurrency of [async](http://hackage.haskell.org/package/async-2.2.1/docs/Control-Concurrent-Async.html#v:concurrently_) and the terminal goodness of [haskeline](http://hackage.haskell.org/package/haskeline): {-# LANGUAGE NumDecimals #-} module Main where import Data.Monoid import Data.IORef import Control.Monad import Control.Concurrent (threadDelay) import Control.Concurrent.Async (concurrently_) import Data.Time (UTCTime, getCurrentTime) import Streaming import qualified Streaming.Prelude as S import System.Console.Haskeline main :: IO () main = do let keypresses :: Stream (Of Char) (InputT IO) x keypresses = S.concat (S.repeatM (getInputChar "")) delays :: Stream (Of ()) IO x delays = S.repeatM (threadDelay 1e6) times :: Stream (Of UTCTime) IO x times = S.repeatM getCurrentTime untilTrue :: MonadIO m =&gt; IORef Any -&gt; Stream (Of Any) m () untilTrue = void . S.break getAny . S.repeatM . liftIO . readIORef shutdown &lt;- newIORef (Any False) concurrently_ (S.mapM_ print (S.zipWith3 (\_ _ t -&gt; t) (untilTrue shutdown) delays times)) (runInputT defaultSettings (S.mapM_ (\c -&gt; liftIO (modifyIORef' shutdown (`mappend` (Any (c == 'k'))))) (S.zipWith (\_ c -&gt; c) (untilTrue shutdown) keypresses))) 
Ok, so that's "path categories are to categories as lists are to monoids" :-P
Hahaha I suppose so, good catch! 😁
I would very much want that.
I'm a fan of [Tarjan's Algorithm](https://vaibhavsagar.com/blog/2017/06/10/dag-toolkit/) for this.
&gt;Since there should be no state in FP This is a often cited misconception. FP does not eliminate state management. It just makes it explicit. You still have global variables. The only difference is that your code that reads or writes into those global variables has a type that informs programmers that the code does read or write from a global variable. That's it. You would do exactly the same thing in haskell as you would in imperative languages. 
It's called --test, but it runs any GHCI command you like (and therefore any command, eg doctest: -T ':!doctest file.hs'). There's no reason it should generate junk on stdin.
Informative, thank you! ghc 8.6.3 and ghcid 0.7.1 here.
Oh, nice, good to know. I guess it's just not expecting something that interacts with stdin then. The first ctrl-c is necessary to kill the inner read loop and the second to kill ghcid itself.
I would very much not want that. Overloading shouldn't be as loose and uncontrolled as that.
Could be an extension then
I personally am not a fan of this overall approach. Just like how shadowing is a warning I can only imagine and hope that -Wall warns when this kicks in, so this will not help me in production -Wall and -Werror code. For various similar functions/values the correct (IMO) way to avoid name collisions is to use a generic typeclass such as Functor or Witherable or Align. For field name collisions I thing the right approach is in the vein of OverloadedLabels + true row/record/variant types, perhaps with dot syntax as shorthand for `get #foo`. The remaining collisions are when you have two very different functions/values with the same name. In this case I think a qualified or explicit import or just hiding one of the collisions works just fine. With that said just for ghci use it could be helpful, just like how defaults and shadowing are very useful in ghci.
Can you elaborate on that functor/witherable/align approach?
So a lot of the name collisions that I see are different types implementing essentially the same function but just in the way most appropriate for their type. For example union on IntMap and Map, map on Map, Seq, IntMap and [], filter on Map, Set, Seq, IntMap and []. For these functions they should not be exported (or not imported) from Data.Map and Data.IntMap and so on. These structures should just implement the Align, Functor and Witherable classes respectively, so you can call the generic functions on all of them with no name collisions to worry about. 
In the DSL, it would make sense to have `addCost :: Cost -&gt; Effect ()`, with the property that `addCost x &gt;&gt; addCost y` has the same cost as `addCost (x+y)`, and `addCost 0` has the same cost as `return a`; i.e. a default cost of 1, as under the current implementation. The other option would be to embed it in the return type and have a different `solve` function that took `[Effect (a, Cost)]` as its second input; but that seems less correct to me, for lack of composition.
I think this would be a tax on people reading your code.
Your suggestion that is two suggestions is exactly why I haven't done it, it's unclear exactly what the best decision is :) if this is important to you though please open an issue and I can see what is best!
Are there any well-documented or recommended 3d physics libraries for Haskell? I’m only 3 months in with seriously using Haskell, and I'm porting a simple game using Apecs and SDL2 that uses 3d collision. This would most likely only require AABB and spheres, and need to support basic solid collision and objects pushing each other. Most libraries I’ve found when searching had little documentation or were abandoned, and I couldn’t find one that fit. Am I just doing a terrible job searching?
How do I tell stack to build shared libraries &amp; link dynamically to packages with LGPL licences?
As far as I can tell, most of the criticism here is unfounded. Is there actually any downside to this? I guess you might use the wrong argument for such an identifier. Mixing up your qualified imports at the same time is probably a lot less likely. But especially working with several different containers should be much easier since you don't need any more qualified imports (e.g. `fromList`, `insert`, `lookup`).
&gt; Just like how shadowing is a warning Shadowing is not a bad thing—it’s even encouraged in some languages (eg, Agda). The reason it’s dangerous in Haskell is because of unrestricted recursion in let bindings, which makes it very easy to accidentally make an infinite loop (it’s actually not hard to make this mistake even with shadow warnings). Personally, I wish Haskell would disallow recursion in let bindings. I have never intentionally used it, and it’s bitten me several times when I used it by accident.
Is this a work-around for the absence of method syntax?
&gt; Records/OverloadedRecordFields/MagicClasses allow duplicate record names to be disambiguated. While there is some overlap, there are also some important differences, as they are solving different problems: Does this mean that this proposal would *not* allow the compiler to infer the correct record selector from the types? Because currently OverloadedRecordFields does not work in the majority of cases.
The link to the original proposal seems to be a dead link. 
&gt; so this will not help me in production -Wall and -Werror code. You know you can disable Werror for specific errors, it's not an all or nothing proposition.
I realize that. I guess an implied part of what I said was that since I WANT it to be a part of Wall, I’m implying that it isn’t a very bulletproof and robust feature that I would want in production code bases. 
I am a fan of consistency. So my opinion is that you either commit to shadowing or you commit to recursive bindings: Shadowing: All declarations will by default have some sort of implied order, where things that come “after” shadow the things before. This annoys me becomes sometimes I feel as though defining things somewhat “backwards” is convenient, such as utility functions at the bottom of the file / where clause. But sometimes “forwards” is convenient such as building progressively bigger and more complicated things on top of primitives at the top. You now need special syntax for when you do want mutual recursion, which I don’t love. You now also have to think some more about how you handle monadfix blocks, as they make heavy use of mutual recursion and so shadowing would probably be confusing. Mutual recursion: No implicit order for anything, things on the same level are equal and swapping them is always safe. No extra syntax. Some argue that it’s more error prone due to accidental mutual recursion. Haskell now: Mostly in the latter category, although I personally wish it was committed to more aggressively. Monad blocks have shadowing which I would prefer to be an error telling you to turn the block into a MonadFix block. Also TemplateHaskell complicating things. 
As in a haskell library licensed with LGPL? You're going to have a very hard time doing what you want. GHC is unfortunately nondeterministic even in its ABI, so your users won't be able to build that package from source. You'll have to figure out a way to ship your build of the library without making the rest of the product derivative of it. You'll also have to make absolutely certain that none of the code from the library gets inlined into your code by the optimizer.
yes, FSM, wonder how didn't I think of it. Thank you! also I found something called "table-driven", I guess that maybe also something dealing with states.
If you are actually interested in learning FP, pick a different task. What you describe is not a particularly hard problem, but it does not show off the benefits of Haskell and FP more generally - purity and immutablity. &gt; while-loop to check that variable. While loops translate very easily into recursive functions. 
&gt; FP does not eliminate state management. It just makes it explicit. Since you replace it with immutable data, it is kind of different. 
What is the point of consistency if leads to bad UX? If you mandate that `where` blocks correspond to mutually+self recursive lets (note that modules begin with `module Foo where`) and have plain `let` be non-shadowing then most of the problems both you and /u/dnkndnts raised go away.
Just what Haskell's syntax needs: more ambiguity and less explicitness. We shouldn't be adding superficial things to the language, we should be taking them away. Readability &gt; writability Editor tooling &gt; writability
I don't like your tone, but I kinda agree. If we're going to make things less explicit we first need to invest in the tooling first. There's an example I'd like to give when talking about explicit vs. implicit in language design. Normally you'd think this is more explicit: ``` foo :: Int -&gt; Int foo x = ... ``` than ``` foo x = ... ``` However suppose I have the tooling that can show me type of `foo`. Now if I want I can now hover my cursor over `foo` and see the type in an overlay. Alternatively I can click to a button and all top-level definition types can be shown, similar to how IntelliJ and some other IDEs how values of variables inline when debugging etc. (or how parameter names of a function are shown in call sites) This is now as explicit as the one before, but now I need to type less which IMO is a plus (because it doesn't take away from readability).
Extensions are more costly than you'd think, for one we already have a lot: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html and their interaction makes writing tooling for haskell tricky.
Fantastic work! A few notes: * Try turning on `-XNoMonomorphismRestriction`---this should let you infer `toCCC` types in the examples. * You can avoid the incoherent instances by writing `IsTup` as a type family: type family IsTup a :: Bool where IsTup (a, b) = 'True IsTup a = 'False I'd love to help---is performance your #1 issue right now?
This has strayed pretty far from my core point which is that IMO the proposal isn’t a great thing to use in prod. But regardless you already get most of what you are asking for by just writing `let` multiple times, but you can maintain the mutual / order independent behavior with a big block under a single let. I personally don’t love that there is both `let` and `where` and would prefer a unified syntax that allowed for easy blocks of declarations anywhere with minimal noise.
I would really hate for this to become the defacto standard way to share syntax for functions like `fromList`, `insert` and `lookup`. And I mean really really hate. `fromList` `insert` and `lookup` all should absolutely be typeclass methods with various laws and various methods built generically on top of them. Honestly reading your comment made me far more against this suggestion than I originally was. Functions that essentially mean the same thing but just specific to a certain container type (like the ones you gave) are the EXACT functions that benefit by far the most from type classes and building generic methods on top of them. This is the same reason I don’t like various extensions that improve “traditional records”, as they make a proper solution like true rows and records less likely and desired.
I actually saw your comment about my tone before you edited it. I actually thought it was fair, and should clarify I'm not annoyed at all at the proposal's author. Naturally people are going to write up suggestions for haskell they think would be cool. I am annoyed at how little consideration the our community gives to editor tooling maintainers. Elm and PureScript both take editor tooling seriously and consider it when making language design decisions. Haskell just gives editor tooling volunteers the GHC API and a list of 100+ language extensions and says "good luck". The mixed results this gets are predictable, make haskell harder to use than it should be, and hurt adoption. There are a few possible solutions to this: release a new, streamlined standard, stop adding so many extensions, improve the GHC API (or at least stop making breaking changes to it so much), etc. I'm not sure which is best, but I hope they're being looked at seriously.
I would personally like more frequent Haskell reports and for those reports to really focus on getting a core set of cohesive and tractable extensions standards that handle 99% of production needs. I would then like tooling to be built around really nailing extension-less Haskell. So then Haskell with extensions is the “research language” and Haskell without is the “industry language” without all the damage a true language fork would IMO cause. Right now extensions are just too key for writing good Haskell to pull that off. The biggest roadblock will probably be TemplateHaskell, with how incredibly practical and harshly intractable it is. So IMO it should probably stay as an extension, something like true rows and records can severely reduce its need (no more Lens/DB library TH strictly needed, with no or only a slight verbosity increase).
I would be careful to not over-optimize for IDE’s, as I quite enjoy the low friction of Haskell coding via just ghc + vim alone. One option is to pick a nice sweet spot that works well in text editors, and build a nice IDE that allows you to quickly code-gen the parts that you don’t want to type (that some consider too explicit) and provide tooltips for the parts the user might consider too implicit.
I 100% agree. It’s superficial and it also would potentially decrease motivation for non-superficial solutions to problems. Such as true rows, records and variants and better typeclass container hierarchies (or even embracing the existing ones like Witherable and Align and Functor and such). It’s a superficial partial solution to three different situations: Overlapping names for functions that do the “same” thing but for different types. Like Map and IntMap and Set and Seq name collisions. Much much better solved by typeclasses. Overlapping field names. Much better solved with proper rows, records and variants. This better solution would also remove the need for traditional record syntax, a sizeable quantity of record related extensions, Generic, and a lot of TemplateHaskell. Truly different things with coincidentally identical names. The rarest of the 3 collisions and explicit / qualified imports are more than good enough of a solution, and far more tractable. 
&gt; Haskell just gives editor tooling volunteers the GHC API and a list of 100+ &gt; language extensions and says "good luck" Agreed. However from where I sit I see the situation differently. I probably know all of the people who are paid to work on GHC (except any people that may be working on closed-source forks), and I'm one of those people, and we very rarely work on a user-facing new feature (e.g. a new pragma). I think no one implemented a new pragma as a part of the paid work. Those come from contributors who are all about new features but never about refactoring, benchmarking and profiling to make things faster, or testing. Those are boring and thankless tasks (except if you can make things faster dramatically, which never happens) that make people burn out in a few months. Yet these are the things that we get most complaints about. So more specifically &gt; stop adding so many extensions This would mean rejecting most of the volunteer work. &gt; improve the GHC API No volunteer would do this because it's long, tedious, and thankless work. The manpower we have is barely enough for providing stable releases in the presence of the new features we ship with every major release. See 8.6 as an evidence The first two releases are unusable, and the third release is still unusable on Windows (but not on Linux, I don't know about OSX). For the IDE/tooling, the work needed to even start implementing the most basic tools is not something for contributors, because it requires huge amounts of thankless work that'd take at least a year of full-time work (I'd expect more, but some say I'm a bit pessimistic on these kinds of things). So unless someone volunteers to hire people and pay for the IDE/tooling work for a few years, I think it's safe to assume that this is not going to happen.
What I am surprised by is that Google X is involved in this. Last time I checked Haskell was not an approved language at Google.
I have a question about monadic zipping. I have a function `f :: a-&gt; a -&gt; m a`, which looks something like `f a b = do c :: Int &lt;- read &lt;$&gt; getLine; return (a*b*c)`. I want to write something like `mydata = [1,2,3,4] res = 2 : zipWithM f mydata res` However, of course, this does not typecheck. How should I define res?
Seems like Google X doesn't follow a lot of the rules that the rest of Google has.
Having ci for a big patch has been great for me already :)
One thing I would suggest is waiting for / focusing on proper row/record/variant support, along with any associated syntax sugar from dot syntax to OverloadedLabels. Done properly I could see it mostly obviating this proposal, as well as much of the existing language from Generic to a lot of existing TH to various record based extensions. I would say it's more or less agreed that row/record/variants would be good for Haskell, and that people would want to use them in production code bases. However I know I am not the only one who would not want to allow for usage of the above extension. So even if it turns out that people are 60-40 or 70-30 or whatever in favor of this, I would strongly suggest waiting to see what people's opinions are after the record problem is more or less solved.
&gt; Who do I lobby to have this changed?? You can file a bug report upstream, but it's less rude to submit a patch yourself. 
Are issues still going through trac?
Yeah issues are not migrated yet.
To be fair, Gitlab allows you to just plug in a Kubernetes cluster running on whatever for your CI tasks. It's pretty hands-off for the provider.
You don't need to change your mindset. Your algorithm is inherently sequential. You don't need to contort it in some form of fake declarative or functional stuff which would complicate everything and would deceive yourself and others into thinking that your are using a supposed "FP mindset". That is deeply wrong. That's one reason because programmers don't use Haskell.
Nothing in what you wrote *proves* your last sentence. "Can be" does not mean "always is". Consider as = map g xs bs = map f as main1 = print bs &gt;&gt; print as main = print bs What you describe is a compiler *optimization*.
if g is `a -&gt; b` then `f` must be `b -&gt; c`. And, yes. 
Avoiding infinite loops is a special case of the more general problem which is removing or renaming an inner scoped variable, without the aid of editor support, inevitably leaves names captured by the outer scope without the programmer noticing, provided the types still match up. Regarding let binding recursion, I'd agree, I'd prefer if it was explicitly enabled by e.g. `let ~x = ...` or `~let x = ...`. 
?? My point was Google was taking interest in a Haskell project. I have heard about pretty nasty politics at Google when they were trying to get Haskell approved as a production language. 
Ah, I was not aware of that. Although Haskell as an approved language and providing raw compute power is probably a different bureaucratic path
&gt; That's one reason why programmers don't use Haskell. I should ask my employer to change my job title then :)
Aren't type classes less explicit than functions? Should we take them away?
I've read your comments before but what I would actually like to hear is why you have that opinion. I can't even find a single argument for your position in your post. &gt; `fromList` `insert` and `lookup` all should absolutely be typeclass methods with various laws and various methods built generically on top of them. There already are some type classes for the above (IsList, At, Ixed). This will lead to a ton of single definition type classes (i.e. namespace bloat). I don't see the advantage because most of the time I don't need to write code that is polymorphic about the type of container I use.
There are some remaining issues with the migration, such as the times of issue events not being properly migrated. Once these are sorted out the migration will be of a very high quality. You can see the remaining issues here: https://github.com/bgamari/trac-to-remarkup
[https://gitlab.haskell.org/ghc/ghc/wikis/welcome#status-of-the-trac-migration](https://gitlab.haskell.org/ghc/ghc/wikis/welcome#status-of-the-trac-migration)
I honestly didn't think I needed to back that point up. Without a typeclass there are no shared laws or sub/super classes or any ability to build generic functions on top of it. It also makes it easier for users to pick up new types. With the approach in the proposal something as simple as tiny generic helper functions cannot be created, `insertLookup` or what have you are unimplementable. One example is that with a generic `adjust` and `lookup` class you can build `Ixed` from Control.Lens on top of it freely, likewise with `insert` and `delete` as a sub-class you can build `At` from Control.Lens on top of it freely. Another example is various queue based or graph algorithms or any number of other things that have different time complexity / space complexity etc. with different underlying data structures. The ease advantage is not to be ignored either, now that I am familiar with some generic typeclasses for data structures, I don't think to think much if I import a new one, I just re-use all the existing methods and I know exactly how they should work and their type. I haven't used `HashMap` but since I've use `Map` and `Ixed` I know exactly what `myHashMap ^? ix "foo"` is going to do. If we continue down the "building typeclasses for containers is noisy and a waste of time" we'll pretty quickly hit typeclasses like `Functor`, `Traversable`, `Foldable`, `Monoid`, `IsList` (some more tied to containers than others) and I would absolutely hate to start throwing those out, and I think most of the Haskell community would agree that most/all of them are great to have and would be very poorly replaced purely by this proposal. Building and using these elegant and disciplined typeclass hierarchies is a large part of why I like Haskell, I would love for it to be pushed even further in that direction over the coming years, with various ekmett libraries and projects like SubHask. A huge regression like everyone using this proposal instead would be a major blow to my love of this language.
Thanks for writing this up. It was a useful read.
&gt; Again, it seems the default Enum implementations for the base numeric types do not account for this use case, and `[10..0]` will result in `[]`. You can have decreasing ranges if you use the step variant: `[10,9..0]`
I think this is a good introduction to type-classes, but the explanation of visible type applications is a little misleading. It gives the false impression that type applications only work with type-classes. Type applications allow instantiating **any** type variable with a type argument, whether mentioned in a constraint or not. Ex: Prelude&gt; :t const const :: a -&gt; b -&gt; a Prelude&gt; :t const @Int @String const @Int @String :: Int -&gt; String -&gt; Int In GHCI you can see the variables that are available for type application by enabling `-fprint-explicit-foralls` and using `:type +v` instead of `:type`. All the variables mentioned in the `forall`, except those displayed in curly braces, are available for type application. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#visible-type-application
I think a mutable array/vector with logarithmic growth would satisfy the requirements? AFAICT that's the only possible way to do it without *some* leeway on constant-time access (i.e. average or amortized constant). The VList data structure is a possible candidate with average-constant-time random access.
I'd love to know why Turing completeness was a requirement..
&gt; I could not find any function to parse numbers. Try [`integer`](http://hackage.haskell.org/package/parsers-0.12.9/docs/Text-Parser-Token.html#v:integer) from the `parsers` library
Nice, good read!
Thank god I'm not the only one who gave up on number six from last year.
&gt; Just remember to always add the required catch-all cases, lest you write a partial function. In my opinion, a total function which returns the wrong answer on some inputs is worse than a partial function. I would instead write the following instead: -- partial if the input list has fewer than 5 elements process g (a1:a2:a3:a4:a5:as) = g a1 a2 a3 a4 a5 : process g (a2:a3:a4:a5:as) process g [a1,a2,a3,a4,a5] = [g a1 a2 a3 a4 a5] process _ xs = error "process: input has fewer than 5 elements" This way if you accidentally call this function with an invalid input, you'll get an error telling you that you did and you can immediately proceed to trying to find and fix that call site. With the total-but-wrong variant, you'll instead get some wrong answer, which will get fed to the rest of your code, and eventually you'll get a final answer which the adventofcode website will tell you is incorrect, and you'll have no idea where you made a mistake: maybe you misunderstood the problem statement? Maybe there is a corner case which your algorithm doesn't handle? Maybe you implemented the algorithm wrong? It will take a lot of debugging before you can figure out that the problem is that you gave an invalid input to `process`.
Have you tried using typed holes and hole fit proposals? They are very useful if you like having a dialogue with the compiler. They are like a supercharged lint :-)
Thank you so much. It never occurred to me to look in the \`Token\` module. (not OP btw)
You can have a look at `Sequence` from `containers`. It provides `log` access and appending and performs well enough, see https://github.com/haskell-perf/sequences I'm also abusing `Map Int t` or `HashMap Int t` from `containers` and `unordered-containers` to get (sparse?) indexed with `log` access and appending. Please note that `HashMap` from `unordered-containers` uses a big logarithm base, so access are faster than what you can hope from a base 2 log. If you want true `O(1)` access and (amortized) append, I'm not aware of a functional (i.e. persistant) datastructure which offer this. Have a look at mutable vectors in `vector` which provides `O(1)` access and modification in `ST` or `IO`. However they does not have any support for resizing and you'll have to code that by yourself with over allocation to get the amortized behavior you're looking for. I was sure hackage had a package for growing vectors, but I cannot find it.
&gt; For the first time GHC has fully-automated testing, proposed patch CI, and release generation across the full range of Tier 1 configurations it supports, with passing builds in all cases. This is huge. The effects will be glorious!
If you are using `ale` and `stack` then I'd recommend building tools with `stack build [package] --copy-compiler-tool` and configuring `stack` as your executable in `ale`. See my `ftplugin/haskell.vim` for an example. https://github.com/eborden/dotfiles/blob/master/.vim/ftplugin/haskell.vim
I forgot a fifth option, which could be also myopic: 5) a non zero integer type..
Have you considered not using `--optimize`? IMO interpreted Haskell is plenty fast enough.
&gt; I would consider myself an advanced functional programmer (...) and intermediate Haskeller I'm immediately skeptical of anyone who claims this, especially when they then go on to describe how they learned `Lens` for the first time and have yet to use `Data.Sequence` or `Control.Monad.ST`. &gt; After a bit of research, I found that the `try` combinator allows you to backtrack away from a failing parser, thus allowing the alternative branches to proceed as expected. This is an extremely `Parsec`-centric worldview and does not take into account the numerous backtracking parser libraries that are available, such as `ReadP` and `attoparsec` &gt; Finally, one thing that I’m probably doing wrong: I could not find any function to parse numbers. Attoparsec, for example, has `decimal` and `signed` combinators for this.
Just want to share something I wish I could tell my past self: POSIX scripting is good. Bash scripting is also good. Thing about ecosystems, even VERY big ones, is that there are happy paths, which is what people use the technology for, and straying from the happy path will sooner or later bump you into problems. Lispy things that are good specifically for scripting, from this standpoint, are guile, chicken scheme, and picolisp. Of these, chicken has a great number of packages and an active community; Guile a bit less so, but still very reasonable in practice; Picolisp is a curious piece of technology with a long history, again a lot of tooling around processes and such. All of these have 0.00x startup time. However in the end of the day shell languages were made specifically for shell scripting, and this is the only place where you can run executables as if it's a function: \`ln -s $orig $targ\`, instead of \`system("ln -s $orig $targ")\` -- and that's with pseudo string interpolation :) To overcome the ickyness and find the courage to face your old horrifying scripts in \~/bin/, invest some time into becoming the best POSIX / Bash expert on planet Earth. Read the manuals, they are fun! There are many cool tricks with piping things into each other. Dash is faster and POSIX, Bash has arrays, dicts, [process substitution](https://unix.stackexchange.com/questions/254820/xdotool-how-to-search-for-window-by-title-and-class-with-different-patterns-si/255643#255643), ...; Use proper tooling: autoformat ( [https://github.com/mvdan/sh](https://github.com/mvdan/sh) ), lint ( [https://github.com/koalaman/shellcheck](https://github.com/koalaman/shellcheck) Haskell! ). A cool thing I realized recently is that POSIX does not have arrays because pipes are arrays. \`\`\` targets() { echo \~/dir1 echo \~/dir2 } targets | while read -r targ; do ls -la $targ done \`\`\` Another cool thing about shell scripting and pipes is that in the current year there's a number of cool utils. [Standalone frecency tracker](https://github.com/ccheek21/fe) can be coupled with a [fuzzy filter](https://github.com/junegunn/fzf) to filter the results of [fd](https://github.com/sharkdp/fd); Of course we can do this in any language, but normally outside of shell languages piping one process to another is not a one-character type of deal.
I have done 22/25 in Haskell this year, and did over half in Elm last year. Honestly, I find that just doing splits, pattern matching, and read is the fastest way to get most problems parsed. The inputs are often simple to the point that a “read” on the third and fifth “words” of the “lines” is quick and painless.
It isn't but there's a good amount of Haskell libraries in hackage made by Google. They're using Haskell for something in there.
Your job is "FP mindsetted programmer"? 😃
The links to the original TDNR proposal are dead. Looks like this is the right link: https://wiki.haskell.org/TypeDirectedNameResolution
Why not just write an IDE plugin that transforms from/to the desired (TDNR) representation into valid Haskell? I see no need for this Haskell-to-Haskell transformation to be in GHC.
Do you know if anyone has written a quasiquoter for running shell commands? That would definitely make Haskell suitable and would avoid "most" of the problems that bash scripting normally suffers from. On the other hand you would also lose some of the niceness of Haskell ie type safety.
I liked the presentation as well, but I find LambdaWorld seems to be very sparce on supporting materials. There are no links to the slides, nor links to the other resources she suggests at the end of the talk. Nor links to her web pages (which might have those materials, as well as other work she has done that might be of interest). 
An intermediate is, to me, someone who is beyond beginner. And if this person is still considered a beginner, then the Haskell community does indeed have a problem with elitism.
``` playerTurn :: Player -&gt; State Game () playerTurn p = do -- be careful with p here! return () turn :: State Game () turn = do players &lt;- get mapM_ playerTurn players ``` so one fix would be to iterate through the player indices, rather than the players, and to fetch the current value for a player just in time. ``` turn = do numPlayers &lt;- length &lt;$&gt; get forM_ [0..numPlayers - 1] $ \i -&gt; playerTurn =&lt;&lt; (!!i) &lt;$&gt; get ``` But you'd still have two values for the current player - the one in the state and the one you were currently using. Perhaps a better approach would be to use a zipper.
would you describe someone who had been programming for 25 days to be a beginner?
Could anyone show me a simple example of using `mapM_` with `State s a`?
IIRC, Real World Haskell doesn't use Rank2Types (granted it is a bit old at this point). While intermediate is certainly a broad term, if someone's able to solve all the AoC 2018 problems using Haskell, they're certainly not a beginner given the complexity of the problems involved and the variety of techniques required, compared to using an imperative language where you'd stick to mutable vectors + sets + dictionaries.
 Expected type: State MessageState () Actual type: ((), MessageState) but where does the `((), MessageState)` come from? Hint, replace `s` with `MessageState`: runState :: State s a -&gt; s -&gt; (a, s) --- Your intention with `processStream` is not clear - is it (A) a state update or (B) do you just want to use state internally? If A, then why `runState` inside? If B, why `processStream` is `:: ByteString -&gt; State MessageState ()` instead of something like `:: ByteString -&gt; MessageState`?
I'm trying to use State internally for processing a bytestring where some bytes are control codes that modify the buffer as it is being processed. OK, so looking at the signature of `runState`, it makes sense that the result would be `((), MessageState)`. I guess I want to do something like: processStream :: ByteString -&gt; MessageState processStream bs = let stream = BS.unpack bs startingState = MessageState T.empty Normal (_, msgState) = runState (mapM_ handleStream' stream) startingState in msgState 
Would you agree with their description of themselves as an advanced functional programmer?
Expressivity question: In a few places I have read that Datalog cannot compute the difference of two relations. So I found a Datalog implementation somewhere, and queried for `all (X,Y) such that P(X,Y) and not( Q(X, Y) )`, and it worked. Maybe I don't know what the difference of two relations is. But more importantly, are there any limits to the queries that this implementation of Datalog will let you express?
Type classes have huge downsides, but pull their weight by letting us express awesome stuff like `Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b`. I'm not convinced that type directed name resolution adds a similar level of expressiveness to the language.
&gt; Those come from contributors who are all about new features but never about refactoring, benchmarking and profiling to make things faster, or testing. Those are boring and thankless tasks (except if you can make things dramatically faster, which never happens) that make people burn out in a few months. Yet these are the things that we get most complaints about. i actually guessed this would be the case, it's a common pattern for open source :( However, if it has an upside it means that saying "no" to an extension won't cost essential refactoring/internals volunteer work, because they usually don't come bundled together. &gt; The manpower we have is barely enough for providing stable releases in the presence of the new features we ship with every major release. See 8.6 as an evidence. The first two releases are unusable, and the third release is still unusable on Windows (but not on Linux, I don't know about OSX). Has the community considered trying to raise money for another FTE on GHC? As far as I can tell the number of Haskell companies is growing, maybe they could support that. As a sidenote is there a way to donate to GHC development right now? There's this page: https://wiki.haskell.org/Donate_to_Haskell.org , but that's a broader scope than just GHC. &gt; For the IDE/tooling, the work needed to even start implementing the most basic tools is not something for contributors, because it requires huge amounts of thankless work that'd take at least a year of full-time work (I'd expect more, but some say I'm a bit pessimistic on these kinds of things). Agreed editor tooling is some of the most thankless work out there, but for some reason our community has a fair amount of people who are willing to do it. We have ghc-mod, Intero, haskell-ide-engine, etc. So the person-hours are there. IMHO the needs of these people should be taken into account (not as an overriding concern, but as a real if small concern) when designing the language. I know the GHC folks don't want to see it this way, but for the forseeable future GHC *is* haskell, and the editor tooling people need to support each new extension added to GHC is they want their users to have a good experience.
I bet there is a very good reason for it, but it doesn't feel very intuitive :( &amp;#x200B;
If you don't need a widely adopted implementation in Haskell, and assuming you're talking about immutable arrays, you can have a look at: * Array mapped tries, package [persistent-vector](https://hackage.haskell.org/package/persistent-vector). *Effectively* constant (logarithmic in fact, but with guaranteed small tree hight) or amortized constant for most operations, including random lookup and adding at the end. * [RRB vectors](https://infoscience.epfl.ch/record/213452/files/rrbvector.pdf), no relevant Haskell implementation that I know of. It adds effectively or amortized constant concat and splitting (also right splitting aka `drop`). &amp;#x200B;
This proposal would impose a cost for readability and add one more thing that people coming to the language eventually have to learn. When reading raw source code, e.g. browsing Github or reading blog posts, it is already relatively difficult to identify where a given name is coming from. With this, except in variant 1 (only annotated types allowed), the reader potentially needs to infer the type of the name in order to identify what it is, rather than the status quo where often a qualified name helps the reader to infer the types of its arguments. And, more important, it adds another set of rules to the language semantics, and now there are *two* ways to have name overloading. As others have commented, extensions are not optional for a reader of code using the extension. Would overloading only be allowed in separate modules? Could one module export multiple values with the same name? If not, why not? I'd hate to go the way of C/C++, where the "type signature" can be necessary to uniquely identify a function. I don't personally find the motivating example compelling. I assume `Button.reset` and `Canvas.reset` are intended to have the same basic semantics ("the same thing"); otherwise they shouldn't share the same name. Why shouldn't `reset` be a typeclass function? It even has appropriate laws, `reset x &gt;&gt; reset x === reset x` for example, and probably some more meaningful domain-specific laws. The semantics of the function can be documented in one place, and if you're reading Haddocks you also get some amount of useful cross-referencing via the instance list.
Someone who has 25 days of programming in Go is an expert
What algorithm are you looking to implement that makes you want this? (you may also be interested in a random access list, which doesn't quite get you there but its nice for plenty of scenarios: http://hackage.haskell.org/package/random-access-list-0.2/docs/Data-RandomAccessList.html)
I just stumbled over https://mail.haskell.org/pipermail/ghc-devs/2017-July/014424.html. So, the situation should improve for the non-wired-in packages.
I think it's borrowed from maths. In maths, the range [10, 1] is empty, but the range [1, 10] has at least two elements. `[` and `]` are inclusive range endpoints, `(` and `)` are exclusive range endpoints. Since `Enum` (where `enumFromTo` resides) also provides a conversion to/from `Int`, it would be possible to have `enumFromTo` determine the "right direction"; `enumFromThenTo` already uses the conversion to `Int` to determine the gap between the "from" and the "then". I don't think any of them are on hackage, but I do have programs around that depend on `[1..0]` being empty, not `[1, 0]`, so changing `[n..m]` to use something other than `enumFromTo` or changing the semantics of `enumFromTo` would definitely be breaking. It might still be worth it, but definitely breaking.
I've actually seen people returning nonsense in the middle of critical production code instead of throwing an error just to be "pure". Does this cognitive bias have a name?
&gt; Personally, I wish Haskell would disallow recursion in let bindings. Interestingly, GHC Core has both `let` and `letrec`, but the Haskell keyword `let` translates to GHC Core's `letrec`.
I would say someone that hasn't used Lens OR Sequence OR ST may very well still be a beginner. I think RWH uses ST, Lens is used by a LOT of stuff, and Sequence (or something very much like it) usually gets mentioned as soon as you talk about purely functional data structures. Heck, ST is in base! Hackage is big and it's easy to not know about a package, even a popular one, until you get exposed to it some other way, but base is shipped with every (?) Haskell compiler and while it is big for a package, but it's about the size of the C stdlib, small compared to the C++ stdlib, and absolutely tiny compared to JavaSE or the Python standard library.
Yes. But how is that relevant? On the odd chance that you didn't read the article I'll just leave this here &gt;*To put things into context a little bit, I’ve been programming for almost 20 years, and* [*used to code competitively*](http://stats.ioinformatics.org/people/3996) *back in High School. I started with functional programming a couple of years back, made my way through Elm and then progressed into Haskell. I did the* [*CIS-194 Spring ’13*](http://www.cis.upenn.edu/~cis194/spring13/) *course by Brent Yorgey and I’m* [*(very) slowly making my way*](https://github.com/mvaldesdeleon/haskell-book) *through* [*Haskell Programming from first principles*](http://haskellbook.com/)*. I would consider myself an advanced functional programmer, advanced challenge solver and intermediate Haskeller, so the lessons learned should be considered from that point of view.* &amp;#x200B;
I believe this is the correct link: https://wiki.haskell.org/TypeDirectedNameResolution
&gt; Data.Array, Data.Vector or Data.Sequence. Which one, and when, I’ve yet to read about. For fixed-length arrays, use Data.Vector. Only use Data.Array when Data.Vector is unavailable or if non-Int indexing is absolutely essential for your own usability. (IMO, Data.Array is mostly still around because it is part of the '98 report.) Data.Sequence is for flexible sized arrays, though it's quite a bit slower that Data.Vector, and for some loads slower than [a]. (Asymptotically optimal doesn't actually mean it performs well on *any* reasonable workload -- certainly not necessarily *your* workload.)
This just got my attention via Haskell Weekly. I suspect the motivation is [licensing issues with ghc-mod](https://github.com/DanielG/ghc-mod/issues/638), but I'm not too sure.
&gt; Map Int t I think `IntMap t` will be faster, normally, even if your indexes are sparse.
no, I was just wondering what people would think about that. I think someone can learn quite a bit of programming in 25 days, and we only would think they're a beginner because we've been programming for years
This post implements pure Datalog which can only have monotonic queries that is to say if you have a set of known facts, KB, and a query, Query, and you compute a result, Query(KB), then if you later add new facts, Delta, to KB, it is necessarily the case that Query(KB) is a subset of Query(KB union Delta). In your query, if you add more to the relation Q, the result of the query might shrink, so it is not expressible in pure Datalog. Pure Datalog is basically a first-order logic + transitive closure (Kleene star) and a special condition that the signature for FOL doesn't have any function symbols (or only has nullary function symbols i.e. constants). Nearly all Datalog implementations have some form of negation meaning it can do non-monotonic reasoning. So your understanding of "the difference of two relations" is correct, this is just a very simple Datalog, that's all =)
TDNR in Idris is quite awesome, and I'd love to see it in Haskell, too. That said, I already have problems tracking down the definition of a symbol in existing Haskell code if it was part of a "wildcard" non-qualified import. And, TDNR is just going to make that harder. Sure, when I'm in my development environment, jump-to-definition might work, but a lot of times I'm doing investigation from the (HTML-formatted) source code I'm seeing on hackage -- outside of hyperlinking everything, cross-package, with a jump-to-definition link we'll need good ways for name resolution to be done manually without tooling. (Also, for type class methods something I was jump-to-declaration instead of jump-to-definition, but I really need both.) Finally, in a lot of cases, it's better to use type classes in Haskell. They were invented as a way to do ad-hoc name overloading.
I think your post is mostly fair and a reasonable exposure to the `base` libraries is an important part of honing Haskell skills. That being said, to make it clear for folks: you can very well be an expert and not know all of these things, there is no checklist of ‘must know’ items to be an expert in functional programming or Haskell. I have a PhD in FP and am on the Haskell language committee and I don’t know the `lens` package at all. There are also folks who know all the type wizardry and have very little intuition about laziness and how GHC’s runtime works. They are experts too! There is no one true body of knowledge that all experts must posses.
Have you tried Classy Prelude/mono-traversable?
&gt; For various similar functions/values the correct (IMO) way to avoid name collisions is to use a generic typeclass such as Functor or Witherable or Align. Agreed, mostly. As a counter-point, I'd like to show you some (bad) Idris: namespace NN plus : Nat -&gt; Nat -&gt; Nat mult : Nat -&gt; Nat -&gt; Nat namespace NP plus : Nat -&gt; Pos -&gt; Pos mult : Nat -&gt; Pos -&gt; Nat namspace PN plus : Pos -&gt; Nat -&gt; Pos mult : Pos -&gt; Nat -&gt; Nat namespace PP plus : Pos -&gt; Pos -&gt; Pos mult : Pos -&gt; Pos -&gt; Pos Now, when I use `plus` or `mult`, the return type is automatically the most informative that the argument types allow. (Or vice-versa, if I need a `Pos` and am writing it as a product, both factors have to be `Pos`, but if I'm writing it as a sum, only one summand has to be `Pos`.) Assuming I wanted to do something similar in Haskell with type classes, I'd have to use two separate multi-parameter type classes, and eight (four each) instances, and a stack of functional dependencies to make sure type inference could make progress. Actually, I'm not sure what are the required functional dependencies are, so I'm not sure if they are sufficient, particularly for reasoning from known return type to unknown argument types. Any sort of qualification (which basically is the same as giving them all different names) seems to be WET, encoding type information into the name. Plus, it would basically not be name overloading, which is really what we want; it's not incidental name overlapping. (There's two other moderately good reasons for having a `Pos` type distinct from but mutually recursive with `Nat`, so this isn't a completely empty problem.) I'm sure TDNR can and will be abused (if I haven't already), but I do think it deserves a place in Haskell even though we already have qualified imports (to split up incidental overlaps) and type classes (for most principled overloading). (I wonder if closed type classes would help here.)
I haven’t used them much myself, but should probably try them out more seriously at some point. 
What's the difference between that and Data.Sequence?
sequence is a 2-3 fingertree which has better asymptotic bounds on a broader range of things (including concatenation), but also higher overheads.
http://www.haskellforall.com/2015/01/use-haskell-for-shell-scripting.html
&gt; There is no one true body of knowledge that all experts must posses. Agreed. But, we aren't talking be being an expert, just being a non-beginner. To me, a Haskell programmer not knowing about Data.Array is equivalent to a Java programmer not knowing about java.util.Vector. And, a Haskell programmer not knowing about Data.Vector is equivalent to a Java programmer not knowing about java.util.ArrayList. Sure, you can get by with the built-in `[]` type in Haskell (/ built-in arrays in Java), but going beyond what's built-in is necessary to stop being a beginner. You have to begin to learn the ecosystem on your own to stop being a beginner.
I feel like a lot of Haskell devs occasionally have doubts about typeclasses from time to time due to their implicitness and the amount of effort needed to make them work. But then I know I personally get flashbacks of working in OCaml and I jump right back on the typeclasses bandwagon as they really do seem to be worth their massive weight in gold. I cannot say the same about type directed name resolution, they seem to have very superficial benefits. 
Sure, but that's a straight-up reimplementation. I was wondering if anyone had tried making a quasiquoter which wraps the shell.
"The C Programming Language"? ;) There's a Debian package-system check to make sure C libraries don't call `exit` or `_exit` because it used to be so common.
I’m with you, and I agree. I just feel compelled to say something along the lines I wrote whenever the topic of ‘learnedness’ comes up in Haskell. Too many people who casually read this think that all the Haskell experts are advocating some specific list or ladder of knowledge. You weren’t doing that, but I wanted to emphasize that it’s a spectrum of knowledge just in case people felt you were.
There are a bunch of good reasons. For example, `[1..n]` has always `n` elements. And it matches more closely `[1..]`, which is *always* ascending. So if you think of `[1..n]` as `[1..]` with a cut-off, it makes sense. But yeah, there is always wiggle room in how to design things, and you are just trading oddities…
I thought the same recently, so I benchmarked it. The benchmark showed really aweful performance for `IntMap`, so I concluded that `IntMap` was slow. This discussion makes me had another look at the benchmark and I realized that `IntMap.size` was `O(1)`, and that was the reason for the slow behavior of my benchmark. Here is the updated benchmark: https://gist.github.com/guibou/81daacbf5540a0f73182d432be7ec35c In substance, we get: ``` benchmarking test/Data.Vector.Unboxed.Mutable time 3.034 ms (3.015 ms .. 3.049 ms) benchmarking test/Data.Vector.Mutable time 20.28 ms (19.99 ms .. 20.61 ms) benchmarking test/Data.IntMap time 314.3 ms (311.8 ms .. 316.6 ms) benchmarking test/Data.Map time 294.9 ms (293.8 ms .. 296.1 ms) benchmarking test/Data.HashMap time 162.8 ms (162.2 ms .. 163.2 ms) benchmarking test/Data.Sequence time 447.4 ms (446.2 ms .. 449.4 ms) ``` I'm actually impressed by `Data.HashMap` in this context. 
I don't know how it's called, but I think it's a common overreaction to learning about total functions. A total function which returns a correct result for every input is definitely better than a function which only returns a correct result on some inputs and fails at runtime otherwise, and so once you learn about the difference between total and partial functions, it makes a lot of sense to strive to write total functions whenever possible. As a result, many of us went through a phase in which we tried to make our functions total even when it was _not_ possible. The way out of that phase is to realize that the correct way to make our functions total is not to make them return dummy values when we receive illegal inputs, but to use precise types to make illegal inputs unrepresentable.
(Purely speculating here) Maybe because the OP is coming from Elm and I think you can't have partial functions in Elm, so they're trying to aggressively avoid partial functions in Haskell out of habit?
That's also what I typically use, but I think it's a bad habit I've kept from my ruby days of munging the string into a useable format in whichever way seemed the most expedient. Then someone on twitter (I couldn't find the tweet, sorry) claimed that using a parser combinator library is not just clearer (which is obviously true) but also just as shorter to write (which is less obvious). So I tried it on whichever hack I had last written using the split and read approach, and was surprised to discover that (at least on this case) the claim was correct!
&gt; *Use of AGPL restricts use of software licensed under it at corporations, since AGPL is not well defined and leads to ambiguity, which corporations' lawyers do not like.* lolwut... sure... let's make corporations and their overpaid lawyers happy!! And if we're *really* lucky they'll actually use Haskell and contribute back by filing support requests or requesting features in Haskell's issue trackers! Profit! 
I've cloned GHC HEAD and I have a profiling build using `BuildFlavour = bench` locally. Now I'd like to try to 1. Build some packages (e.g. lens HEAD) and see what parts are taking long, ideally on a module-by-module basis. 2. Avoid installing GHC HEAD system-wide using `make install` (so that it doesn't mess with my existing GHC installation from the PPA). What is the easiest way to do so?
How do you use it? I tried `hppc check File.hs` in a cabal project and it didn't work due to not picking up dependencies. Same with stack.
Did you try annotating `f` with the signature you think it has? You'll see that it doesn't work, and ghci will tell you the right signature, it will be `f :: Int -&gt; Int -&gt; IO Int` (doesn't work for any monad `m`) because `getLine :: IO String`. &gt;&gt;&gt;:set -XScopedTypeVariables &gt;&gt;&gt;:{ &gt;&gt;| f a b = do &gt;&gt;| c :: Int &lt;- read &lt;$&gt; getLine &gt;&gt;| return (a*b*c) &gt;&gt;| :} &gt;&gt;&gt;-- :t is short for :type &gt;&gt;&gt;:t f f :: Int -&gt; Int -&gt; IO Int Okay, now we have this. What is the signature for `zipWithM f mydata mydata` going to be (ignoring that the second one is res in your example)? Try using ghci again and that might give you a hint for why you can't treat that as a plain list (i.e. why you can't directly prepend an element to the result using `:`).
Its possible with mutability under the hood something like this (code is untested and assumed to contain bugs). Are there any production ready implementations of this concept? {-# Language RecordWildCards #-} module AppendArray where import Data.IORef import Data.Array.IO import System.IO.Unsafe data AppendArray a = AppendArray { size :: Int , nextUnfilled :: IORef Int , storage :: IOArray Int a } access AppendArray{..} i = if i &lt; size then unsafePerformIO (storage `readArray` i) else error "out of bounds" append AppendArray{..} v = unsafePerformIO $ do storeSize &lt;- rangeSize &lt;$&gt; getBounds storage canMutate &lt;- if (size &lt; storeSize) then -- If space we can extend only if we are the first -- Atomically claim the first spot if avaliable atomicModifyIORef' nextUnfilled (\f -&gt; if f == size then (f+1, True) else (f,False) ) else return False if canMutate then do writeArray storage size v return $ AppendArray (size+1) nextUnfilled storage else do na &lt;- newArray_ (0, ( (size+1) * 2) ) mapM_ (\i -&gt; readArray storage i &gt;&gt;= writeArray na i) [0..size-1] writeArray na size v nref &lt;- newIORef (size+1) return $ AppendArray (size+1) nref na newAppendArray :: AppendArray a newAppendArray = unsafePerformIO $ do arr &lt;- newArray_ (0,1) ior &lt;- newIORef 1 return $ AppendArray 1 ior arr instance Show a =&gt; Show (AppendArray a) where show AppendArray{..} = show $ unsafePerformIO $ do take size &lt;$&gt; getElems storage 
I've seen this before too and agree that it is bad enough to deserve a name. But it also reminded me of the funny trick in [Seemingly Impossible Functional Programs](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/), where they implement a total function of the form `find :: (X -&gt; Bool) -&gt; X` that searches an (infinite!) domain `X` for an element that satisfies the predicate. Of course, there might not be such an element, and in that case they just have `find` return some other value of type `X`. The punchline is that this kind-of-buggy `find` can be used to compute not-at-all-buggy functions `forAll :: (X -&gt; Bool) -&gt; Bool` and `thereExists :: (X -&gt; Bool) -&gt; Bool` like this: thereExists p = p (find p) forAll p = (not . p) (find (not . p))
Thanks! -XNoMonomorphismRestriction does work. I tried using the IsTup type family to be sure and it compiles but does not work as is. It can't infer the needed types internal to the hackery. I think the fundamental issue is the weirdness of trying to detect polymorphic positions. We need to detect when we hit a polymorphic type variable `a` in our tree traversal, but is the type variable `a` going to unify later into `(b,c)`? The type family has to halt in its track here until it can guarantee `a` won't become a tuple. But I need that type family to evaluate to 'False so that I can force `a` to unify to a non tuple. The Incoherent instance just grabs the most general option and does the trick. There really is only one consistent choice of that bool flag, all told, but the typeclass resolution isn't clever enough to know that. Perhaps there is another approach. I was emboldened by finding an Oleg Kiselyov post where he does exactly this, but I believe that post predates closed type families. I'd say performance is my #1 concern now that I've gotten the proof of concept. The performance is frankly abysmal, whereas in the original compiling to categories plugin approach it is one of the big features. If you (or anyone) knows about GHC rewrite rules that would be a big help. It seems like the most obvious, if finicky, way of performing some of the needed optimizations. If you check out my repo here https://github.com/philzook58/not-bad-ccc/blob/master/src/CCC.hs I attempted some of the necessary rewrites, but don't know how to get them to fire.
&gt; type applications only work in function contexts This is incorrect: {-# LANGUAGE TypeApplications #-} -- not a function foo :: a foo = undefined -- typechecks! bar = foo @Int Type application doesn't work on literals though.
 &gt; The effects will be glorious And hopefully made explicit by the type system
What do you mean? You have access to the whole list while filtering even without list comprehensions: sillyFilter xs = filter (`elem`xs) xs
Thank you! I just learned the Functors and Applicative Functors, so now that makes sense.
``` class Semiring a b where type AddResult a b :: * type MultResult a b :: * add :: a -&gt; b -&gt; AddResult a b mult :: a -&gt; b -&gt; MultResult a b instance Semiring Nat Nat where type AddResult Nat Nat = Nat type MultResult Nat Nat = Nat instance Semiring Nat Pos where type AddResult Nat Pos = Pos type MultResult Nat Pos = Nat instance Semiring Pos Nat where type AddResult Pos Nat = Pos type MultResult Pos Nat = Nat instance Semiring Pos Pos where type AddResult Pos Pos = Pos type MultResult Pos Pos = Pos ```
Another tip: they way I would have figured that out, is to just search hoogle, https://www.haskell.org/hoogle/?hoogle=pure, then look at the type signatures of the sub-expressions in questions.
I don't believe this has good "backwards" type inference -- from the return type to the argument types.
I don't see how you would even express said backwards inference? Because knowing the result type and one of the argument types isn't sufficient to know the other argument type in general. You would need to be able to say and verify things like `MultResult = Pos -&gt; a = Pos AND b = Pos`. Since such a thing is inexpressible in current Haskell, that means that the proposal is fundamentally extending the type checker if it does in fact give you that inference, which means that said extensions could easily be brought back into type families / fun-deps to get the same benefits. If there are non-superficial things in the proposal then I would be interested in having those parts pulled out and discussed, but the overarching ability to just have names collide without thinking about it and have GHC pull you from the jaws of defeat is something I am not interested in outside of GHCi.
I've been coding in Haskell for ~10 years, and have very rarely used lenses or ST, and only just learned about Sequence from advent of code. I should have read all of base, in fact I probably did but you don't retain all that until you have a need for it. And base is only a small part of what you need for real world Haskell programming. There's so much to learn, it's easy to never get around to a comprehensive survey.
Negation is an extension to plain datalog and requires stratification -- in a nutshell, you can't have recursion that goes through negation.
&gt; I think that Elm has a great role to play in it. I don't think Elm is going to do much of anything. Way too user-hostile and with the new breaking changes pretty much everyone is trying to move away from it - often to migrate to PureScript. 
[removed]
I already use fish for scripting tasks. But, for slightly more complex tasks that involve text manipualtion, a proper scripting language is more beneficial. &gt; guile, chicken scheme, and picolisp What about racket and common lisp? It seems common lisp has been advertised as a good scripting language for the last few years.
[removed]
What breaking changes do you mean? I would say that the latest release has actually brought a lot of improvements. Native modules are easily replaced with ports and custom operators with non-symbolic names
I reaLly hate shell scripting because piping doesn't work half the time. So I have to check the output format, input format and use some cryptic invocation to translate them. This usually involves a stack overflow search - I probably could do it myself but this is faster and less error prone. Powershell also has a peculiar syntax but is a lot more fun to write for me because piping just works. It also has really good discovery so you can figure stuff out as you need them.
Perhaps, the motivation was simply the desire of the author.
Late to the party, but I have done DepTrack https://github.com/lucasdicioccio/deptrack-project for this type of "SaltStack" replacements. Have a look at the examples directory. Another system in the area is https://hackage.haskell.org/package/propellor
You could recover most of the parallelism with -XApplicativeDo but there are some mixed opinions on breaking the laws that way.
I've recently read "Type-Driven Development in Idris". In there, the author modeled the State-type in an interpreter-like fashion as I would describe it. I implemented the same thing in Haskell (see below). My question is: What are the downsides/upsides of this implementation? Is it a good idea to model state (or any other monad) in this fashion in Haskell as well? {-# LANGUAGE GADTs #-} {-# LANGUAGE KindSignatures #-} module State where import Control.Monad data State :: * -&gt; * -&gt; * where Get :: State st st Set :: st -&gt; State st () Pure :: a -&gt; State st a Bind :: State st a -&gt; (a -&gt; State st b) -&gt; State st b instance Functor (State st) where fmap = liftM instance Applicative (State st) where pure = Pure (&lt;*&gt;) = ap instance Monad (State st) where return = pure (&gt;&gt;=) = Bind runState :: State st a -&gt; st -&gt; (a, st) runState Get st = (st,st) runState (Set newSt) _ = ((),newSt) runState (Pure x) st = (x, st) runState (Bind stm f) st = let (res, st') = runState stm st in runState (f res) st' -- example counter :: State Int () counter = replicateM_ 3 $ do c &lt;- Get Set $ c + 2 Get &amp;#x200B;
Getting rid of operators, new FFI... I'm not going to get into whether they're improvements, but if you have 30,000 lines of code in Elm it's a pain in the ass to port them because the upstream compiler developer wants to make things more elegant.
&gt; Way too user-hostile I understood that first time user had less trouble than with Haskell
It think this is a very unfortunate attitude to have :( Not to start a license discussion, but does it really matter that anyone makes profit of the software you create? If anything, it will help them generate the revenue needed to fund time on developing further on the tool and further to the community. E.g. the scenario that was brought up: A web IDE for Haskell that would utilize ghc-mod. If this needed to pull in ghc-mod as a library, GPL would infest all their code. They now has to loose competitive advantage --&gt; less money generated --&gt; less money to spend on dev time working on improving the tools they use for their product, e.g. ghc-mod. Now, does this really happen? Yes, just take a look at https://github.com/lspitzner/brittany/issues/164, which bars the profilic ndmitchell from spending company hours on contributing to Brittany, and essentially barring him from using it at his company. If you take a look at the major contributors to the top languages, you'll see they are almost all the time hired by a company that invests in the language or framework—and it sure is no secret that we could use more corporate backing for our Haskell tools.
Quick to write, sure. But be warned that the performance of `read` is **utterly** atrocious. I really mean just awful. So if you're doing any sort of bulk processing you **really** want to use one of the proper parsers.
For the level-above-shellscripting both are valid choices (racket VM is in 0.180'ish start times on v7.0), there's also Gambit which [deserves mention](https://ecraven.github.io/r7rs-benchmarks/) and is more system-programming focused than Racket; CL is I feel like a great tool to have in your toolbox, but not something I would recommend for my mom to pick up for a slightly complex task that involves text manipulation. On Common Lisp front, highly recommend [Vlime](https://github.com/l04m33/vlime) if you're using Vim: the starting experience is very polished, complete with a step-by-step tutorial going through building a fibonacci generator. Without editor integration, CL experience is questionable and repl's are mind-blowingly barebones (because people don't really use them directly). Outside of lisp lands, I think OCaml deserves a mention: types, 0.00x startup, a [whole book on unix programming](https://ocaml.github.io/ocamlunix/), one of the best repl's if not the best (it sort of has autosuggestions like zsh/fish), autoformat &amp; language-server. Another interesting choice is [D](https://tour.dlang.org/tour/en/gems/functional-programming), and I'm sure a case for Nim/Crystal could also be made. Factor has a great repl experience and a very productive community for how small it is, however for any problems you'd have to contact the community directly because the internet is of not much help. [Minlang](https://min-lang.org/) provided a much smoother scripting experience in my case. Concatenative scripting sort of ruined lisps for a while for me, with how succinct it is and how basically the whole flow is one pipe. On the darker side of the force, Python + mypy + black is quite a pleasant experience, and there's also Go which has one of the most active hacker communities on github (with approximately zero interest in functional programming however). Both these tools happy path's involve the level-above-shellscripting kind of programming.
If you keep the return values symbolic then you can just generate a syntax tree and interpret in parallel if appropriate. &gt; tasks have invisible data dependencies like writing file another task reads. Naturally the aim of my approach is to get away from exactly that sort of implicit behaviour!
I already know how to write scripts in haskell and clojure. But, scripts written in haskell and clojure don't start up fast. Among the languages you introduced, I would consider OCaml and a lisp implementation. In particular, I want any language that satisfies the following conditions. * Automatically downloads and loads a script's dependencies, or easy to manage local dependencies. * Loads a script fast. * Has a useful standard library, or a large collection of packages in central repository. * Is a fine language
I don't know if this has been posted here before, but I couldn't find it on this sub. It's probably a little more novel to the Python/R/Matlab/Julia crowd, but still a very nice read. Great to see Haskell in production at Target and know a little more clearly the classess of problems they solve with it 💪🎉
How would you make this into a syntax tree? foo = do x &lt;- a y &lt;- b if x then bar y else z x needs to be a concrete value before you can start the rest of the computation so you can't parallelize a and b. You could use rebindable syntax and lift the entire prelude but that's a lot of work.
I would say never, in data science you have a few things you need to do: 1. Quick scripts to test something * Here the type system works against you 2. Math Heavy problems with thousands of parameters * Here haskell is too slow and inefficient 3. Graph based knowledge systems * This is the best case for haskell however interop with other languages might be too much of burden, I would also prefer too use Rust for this as Haskell is too memory inefficient. In general haskell works best for problems where CPU/Memory is in abundance. Data science does not have such problems. 
There is no new FFI, ports were always the recommended way of doing FFI. Native modules were undocumented and at your own risk. I was porting code base around that size and neither of those were a problem. You can write a regex to replace your operators globally if you've overused them.
It's certainly possible to implement monads this way in Haskell, but it's not always the best choice because it degrades performance. Note that in Haskell, we usually use a slightly different approach - `Pure` and `Bind` are a separate data type which takes `State s` as a parameter. For more concrete examples, look at `free`, `freer`, and related packages.
If you just want the final state, you can use `execState` instead of unwrapping the tuple from `runState`.
Haskell has several deep problems that makes it fundamentally unfit for being a significant contender in the data science field. The lack of tooling and libraries is already mentioned often enough, so I'll try to touch on some other points: - the libraries that exist are not directed towards neophytes. Haskell is often used in academic/very cs circles who love abstraction, and someone who doesn't have a cs background can only handle so much cs abstraction at once, which is most often the case for data scientists. The *ability* to abstract is great, but most often than not we're just not given the choice. - not only the libraries prove to be somewhat convoluted, they also often make no effort to address that in their documentation. Most often the only docs available are the type signatures, and a very brief and obscure description of the functions exported. A data scientist just discovering the language has all the chances to lose any hope to get proficient at the language. - Haskell gives the impression to not be mature as a language. For a language that prides itself on rigorousness, its swath of extensions feel more like cheat codes and place holders waiting for a more permanent solution. - lack of industry support, or from the right industry. From what I understand Haskell is very popular in the banking sector but it doesn't contribute anything to the community. With a lack of industry support, the tooling and the libraries are bound to be lacking. This is a period all recent languages have to go through, but Haskell is already 30 years old. And again there seems to be little effort from the community to change this situation. TL;DR: Haskell has some fundamental problems preventing it to "make it" in the data science field, problems that the community doesn't seem to be willing to address atm (despite the brave effort of data-haskell)
I think this could do a good job of addressing the space where you want to overload a name, but not in a truly type-polymorphic manner. A lot of people are saying "use type classes!" but type classes often *fail* a little litmus test I like to use, which is to ask the question "when will I use this method without knowing the type?". For example, `mempty` is obviously a great idea, because I write a lot of functions with types like `foo :: Monoid c =&gt; A -&gt; B -&gt; c`. However, I can also see myself wanting to write some functions `flob :: A | B | C -&gt; String`, where I only ever use `flob` on values that have a concrete type - that is, are *not* polymorphic.
Indeed, if `x` is symbolic then you can't inspect it with `if` and you would have to write an `ifThenElse` combinator.
One of the things I use it for is for parsing input data into a useful structure. Monadic parsers are head and shoulders above most other solutions. If getting the data into the system is a bottleneck, Haskell can help with that.
I would really love to use haskell for data-science tasks. A lot of the involve complex transformations of data, which turn out to be unbearable complex in python and would be really quick and easy in haskell. &amp;#x200B; I don't share the negative view that haskell is unfit or it's impossible for haskell. I think that haskell will probably never be a mainstream data-science language, but can probably carve out a nice niche for dependable, fast and understandable data-transformations. If you could couple this with some basic data-analysis and visualisation tools, it would probably go a long way. &amp;#x200B; I think a more complex niche, but with a better value-proposition, would be neural networks. It's all function composition! Haskell is not there yet, but making progesss. I am a research-assistant and I am working on trying to build new basic building-blocks that satisfy certain mathematical properties. I often get lost in all the reshaping and because it's not tied to a specific dataset and abstract, it's hard to keep track what shapes come in and what come out (this operation returns 1/10th of the indices of the 2rd. dimension etc. this operation expects data-tensors without channels, I need upper triangular matrices here, etc.). Solid, type level dimensions and type-level computations of them would be really help me. I admit it's also more niche, but thanks to hasktorch and progress on more type-level stuff I think we are slowly getting there. I am excited what the landscape looks like next year!
None of these seem fundamental. Big and important yes, but nothing about these problems is intrinsic to the language itself.
The other day I was looking to use a simple data frames library (like Pandas in Python or I believe Owl in OCaml) and the only one I saw was far from simple, I had no idea of how to start making head or tail of it. 
Rust comes to mind. But it's too low level for fast tasks. Unless you're into that sort of thing. Nothing auto-downloading the deps comes to mind. I would be interested to hear on that as well. Options that I listed above are pretty much all fine languages that load fast and have some sort of package management, I listed those specifically because I find them good options. Two more to mention are Perl5, large community / package library, is fine as long as you restrict yourself from too much abstraction, keeping closer to shell style; And Lua's + [penlight](https://github.com/stevedonovan/Penlight) library. Again, I feel compelled to stress the point that.. Programming is a *dirty thing*. Particularly the system-management kind of programming. And it is okay. You know what else is dirty? Mathematics. A newborn universe meditating in a void of nothingness is kind of okay. The cleanest thing however is to embrace the dirt.
And I think that's okay. If the choice is to dilute and/or fragment the language (Haskell is already pretty fragmented due to extensions) for it to be widely adopted then I would rather choose it not to be widely adopted. On the other question of ecosystem, I think we can certainly do better. I think Haskell also has a barrier to try. How simple is Python to try? Super. It's available by default on most Linux distributions and also on MacOS. Haskell even with Stack or Nix it requires some knowledge to get started. 
Lens is actually pretty straightforward once you get the hang of it, and despite the obtuse-ness of the types, it's a lot more useful in real-world contexts than in theoretical contexts. The basic intuition I'd give is if you have a type like data Blah a b c = Blah { fielda :: a , fieldb :: b , fieldc :: c , someText :: Text } You can easily make a `Functor` instance for this, but it only "works" on the last parameter, but... that's kind of silly, since really it's a functor in all 3 of those parameters (individually and simultaneously!), but unfortunately you just can't "say" that. Well, this is what `lens` gives you! You can just derive the lenses, then say `over fielda f` and it will map a function over `fielda` just like `fmap` would. You can then think about how this generalizes in all sorts of directions: this type is also `Traversable` where we encounter the same problem - `Traversable` requires us to have an unwanted magical bias towards the final type parameter `c`, but algebraically, `a` and `b` are just as valid to traverse over. Well, lens gives you that in the same way: `traversed fielda f` allows you to traverse over `fielda`. But what about that `someText`? It's not a `Functor` at all in the usual Haskell sense, but... actually it kinda is -- I mean if instead of saying the objects are types and the arrows are functions from type to type, what if we say there's only 1 object and the morphisms are all functions from `Text -&gt; Text`. Well in this world, `someText` actually is a functor, and... in fact, lens supports this, too! If you have `f :: Text -&gt; Text`, you can say `over someText f`! But what about `Contravariant`? And what about sum types? Or recursion patterns? We can continue in these directions with this same kind of thinking. And that's basically what the `Lens` library is.
In my case, the technique used by the proposal (if exactly one type-checks use that one) corresponds exactly with the inference I want. I agree it's not general, and mostly superficial, but I'm still in favor of it, though in many case using type classes is a better approach. (Closed type classes or negative instances could be helpful in getting the inference I want.)
Why? What advantage does it have over just using the tools binary directly?
[removed]
One thing the article doesn't discuss is the level of intelligence and skill of Tikhon, both in data science and engineering. How many teams will have people that strong to "save the day" when they hit a complex performance issue like he mentions? Maybe a similar team can get by with one expert or a consultant on retainer 
I (and I suspect many others as well) would like to hear more about your experience regarding usability and performance of current Haskell DS libraries.
I was sloppy in my expression, Haskell does have some fundamental problems as exposed by u/erzain. However you're right that *there* I didn't list any. Sorry about that!
If you're proficient in the scientific python ecosystem, using Haskell feels like plummeting into the dark ages. There are no libraries. You can't prototype at the level you need to quickly. You have to do tons of type-paperwork to do something simple like access some data, etc. In general, few people are contributing to it and the people that are are apparently not thinking about it from the perspective of "average" working users rather than type theorists golfing, so it's a huge mismatch and generally getting worse as time goes on and it falls farther behind.
Corrected thanks!
&gt; Quick scripts to test something &gt; Here the type system works against you I don't see how.
This post is basically multiple paragraphs of saying "We have to be as nice to corporations as possible or else they won't like us." What you're actually describing is called a power asymmetry, where one party gets to use vast resources in order to dangle carrots in your face. This is how all work is in a sense, but you're making it very explicit. The reality is developers simply do not pay for development tools (developers are, almost universally, cheap as hell) so the price value of all tooling is driven to zero over time. This is precisely why the "open core" model is becoming more popular for large, venture funded software. They get bled dry otherwise, so you 'force' people to pay. It is also why developers are deeply encouraged to license under the most permissive licenses possible, all other considerations being ignored, because the "acquisition cost" of licenses like the GPL means your "price isn't low enough" to drive adoption. It's similar to just burning money in a startup: you burn cash immediately to get a foothold in a chosen market, establish yourself -- which allows you to later "extract" value from users in order to further sustain yourself (by consulting, by offering services, whatever). All of this is a byproduct of a mindset where you are primarily trying to *sell a product to a market* as opposed to *creating a tool to be used to solve a problem*. Developers subconsciously desire this today, no matter how much they deny it, except they work in a market where people don't pay money. But the idea of "the market" persists -- who your users are, what they want, how to deliver it, etc. In the tooling case, licensing is still important but mostly a political position about (re)distribution. In the product case, licensing is a business concern, above all else. But again: you're doing "business" in a "market" where everyone is cheap. In your particular example, Neil works at a company that has nearly 100 million in funding split over 3 rounds of investment. I'm not picking on anyone in particular here, but in this example, do you think they couldn't just afford to *write* their own equivalent to brittany? Of course they could. They could put it under whatever terms they want and it would likely be a drop in their overall finances. But that's a cost center, no matter how small, and nobody pays for tools. So instead, it would be better to just reuse something. Except they won't allow it unless it's exactly on all their terms (no AGPL, or whatever weird requirements lawyers think up). So it's again not a question of "tools to solve problems", it's one of "products we can sell, and buy, at the lowest cost", above all else. This is exactly the power asymmetry I was talking about: the possibility of asking corporations to adjust their scope or demands is out of the question and not even mentioned, as if it is an immutable material fact of the universe, as opposed to a social argument we have decided to give up on, in the name of a different approach -- a material *choice*, not immutable fact. In your example you should also ask the opposite question: how many developers who liberally licensed their software successfully barely get anything from it? Apparently we should feel shameful every time our choices cause a random corporation to stop their employees from doing something because it would cause "less money to be generated", but if a developer creates a highly successful project (with real, material value, and material results) and gets basically nothing out of it except bug reports from users (which happens), well, that's just the cost of doing business! The vast majority of open source software is funded by corporate books, this is true. There's no point in denying it and it's quite easily arguable that a lot of *good software* has come from the injection of such money into the system. It also won't change because I make a Reddit post, so you can rest assured things will continue as they were. But you can't talk about money without actually *following it* and seeing where it leads (or, more accurately, where it doesn't.) There is certainly a wind of change coming with regards to how free software can sustain itself. If you believe that the rhetorical question "does it matter if someone profits off your software?" is the important question, and you're talking about licensing particulars -- note I didn't talk about licenses so much as social dynamics -- your analysis is fundamentally shallow, and your question is misguided. The more important question is: what material desires and power structures drive and influence our choice of software licenses, a choice that is inherently political? That's a much more interesting and meaningful question in the long run.
julia will probably take that seat, it's halfway between rigourous and straightforward
For data manipulation, standard haskell is IMO a lot better than python. When it comes to the data science part, the library support is much worse, but compiling to python is an option. I've successfully used [TypedFlow](https://github.com/GU-CLASP/TypedFlow) which takes this route, and I think expanding support for most of tensorflow contrib using that approach is fairly low effort.
100% agreed.
Julia is indeed a very nice language! I wish I had the time to write something like [coconut](http://coconut-lang.org/). I think it'd be a nice compromise
Three that I know of. 1. `--copy-compiler-tool` binds the binary to the specific LTS you are using. Most of these tools are loosely bound to a specific version of GHC, so managing them alongside your GHC version is helpful. This is mentioned in Alexis King's fantastic post: [https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/](https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/) 2. You can have different versions of these tools for different projects. Varying projects may require incompatible versions of a styling tool. For example, at Freckle we use `brittany-0.11.0.0` and we confirm proper styling with CI. I however contribute to `brittany` and often have `HEAD` laying around. 3. You can pin the version of a specific tool you are using in your `stack.yaml` and avoid conflicts from many different version. This means that your project can have a `setup` script that installs all necessary tooling via `stack` and those tools are sandboxed for your given project. There are probably others, but these are the important ones for me.
When the program is small types are not necessary, as such dealing with them can only serve to slow you down. 
&gt; lack of industry support, or from the right industry. From what I understand Haskell is very popular in the banking sector but it doesn't contribute anything to the community. This is not true. Many big industrial users of haskell, including in the banks, have made significant contributions back.
There's also `ghci -e` for those of us that don't use stack.
If you have "actionable" (ugh) feedback, suggestions, concrete pain points, or just wish to discuss, you're welcome to join DataHaskell on github and gitter
If you write a script to test something and you've made a conceptual error that invalidates your script that types would have caught, then I don't see how types slow you down. How do you figure this? Haskell doesn't have libraries to do serious data science so where did you get your data? Did you try to write such a set of tools and gave up because of the reasons you gave?
Doing something about the compile time memory usage explosion, that seems to get worse over time, e.g.: https://ghc.haskell.org/trac/ghc/ticket/15455 . The other probably even more important: Tooling! It is a shame that such a young language like PureScript has so much better tooling than Haskell. I would be very delighted if there were to happen some awesome improvements to hie during GSoC!
As a counterpoint, I have used `haskell-gi` on windows pretty extensively and found it to be a largely positive experience. Installation was difficult but not a whole lot more so than resolving anything else on windows that needs to involve the FFI in any non-trivial way. It was initially pretty obnoxious to sort out, but once I had my environment up and running everything worked quite nicely and the API and documentation are pretty top notch. The documentation instructions [here](https://github.com/haskell-gi/haskell-gi/wiki/Using-haskell-gi-in-Windows) now capture all of the issues I originally experienced, and a user following those instructions can probably expect no significant problems. 
&gt; Your program structure will then resemble C/C++ code... I found this to be largely inaccurate. The code necessary to instantiate widgets and what not is generally still pretty declarative (and if you're using glade + gtk much of it can just be XML markup), and though there are some exceptions, it's usually possible and fairly painless to wrap or interface with the more procedural bits with something more idiomatic. It all depends on how much logic your presentation layer needs and how complicated your user interactions are, the more complex the GUI, the more time you'll probably need to spend wrapping up the nasty bits, but if you're presenting an inherently simple interface on top of a complex process, your code will stay mostly painless. For a middle of the road use-case, what you'll probably end up with is a lot of pretty nice declarative pure code, with a small handful of some fairly nasty code wrapping a particularly complicated widget API that was pretty hard to think about. 
Am curious about what companies use Haskell and to what they contribute, do you know if there’s such a list somewhere?
I think the [typeclassopedia](https://wiki.haskell.org/Typeclassopedia#Traversable) explains it pretty well.
I imagine the type of situation they are talking about is something like reading JSON where in a dynamic language, like say python, you just do something like `json.reads(blob)` and then `x["foo"]` as opposed to creating a data type and specifying the `foo` field's type, etc. Of course there are ways around this (eg `aeson-lens`) but the general type of problem still occurs frequently -- eg what if you are reading a CSV or HDF5 instead... there may or may not be an equivalent of `aeson-lens` for each format, and if there is you have to search them out and figure out how to use them instead of using the ubiquitous "read it into a dictionary and then always access it the same way" approach of python and friends.
 traverse :: Applicative f =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) Apply a function that operates in an applicative context (`a -&gt; f b`) at each "a" hole in the "t" and stitch the resulting "b"s together into a "t" that has the same shape as the input "t" all within a similar applicative context. Has to work for any applicative context. The "shape" of the "t" might change, but the laws (naturality/identity/composition) mean the shape change is rather limited. Traversables can be thought of as finite containers, for most proposes. They are of limited utility when side-effects are unrestricted. However, when effects are considered important enough to track, many effects (and combinations of effects) are applicative contexts. `traverse` is an "effectful" `fmap` in this sense. (When side-effects are unrestricted, `fmap` can be used directly.) HTH
That's quite reasonable. That would be an important component of a data science ecosystem in Haskell. I don't think it's actually impossible to support, though.
Odd... I tried following those instructions and had no success. Maybe I'll try again later.
I think part of the problem is that the Haskell community is largely working to solve problems that random script-writing data-munging prototypers don't have. When I did this professionally we had two basic modes of operation. Cowboy Blitzkrieg and Complete Rewrite. The first was a rush to demo an idea on ridiculously short notice and the second was to make something that had to continue to work in a year when some other team has taken it over. Often we never made it to the second mode because it was a one-off question we were answering and we answered it or because the people paying us decided to do the rewrite themselves in-house in $ENTERPRISE_FRAMEWORK. Neither was a good fit for Haskell. Cowboy was too slow for the reasons I mentioned above (and others have elsewhere in this thread). Rewrite was always C++ or Java or whatever because enterprises would rather have code that they can hire anyone to work on than code that is the best it can be.
I find "sequence" easier to think about than "traverse". List is probably the easiest traversable to wrap your head around: if you have a list of applicatives, sequence transforms it to an applicative of a list, aggregating the effects of the original elements into one applicative in the same order as they appear in the list. For non-list traversables, they're foldable and can be to-listed, the way sequence aggregates the effects is the same as it would for its to-list I believe.
Sequence mechanics makes sense to me, traverse is what I'm really shaky on. I'm still hazy on the usefulness of sequence though, do you have any examples of where it comes in handy? 
Sure, lets say I have a list of strings and I want to print them in order, each on their own line. I have putStrLn :: string -&gt; IO (), but that's just for one string. I can map it over the list (putStrLn &lt;$&gt; mylist :: \[ IO () \] which gives me a list of IO actions, then I call give that to sequence to get me a single IO action. Alternatively, I can do (traverse putStrLn myList) :: IO \[()\] ).
Julia doesn't nead coconut actually. It's pretty Lispy itself. For more functional abstraction like currying , ... you can just use macro facilities of it. 
https://stackoverflow.com/questions/7460809/can-someone-explain-the-traverse-function-in-haskell Scroll down to the example of using `Maybe` in `traverse half [2,4..10]`. I like this because it keeps it simple and uses `Maybe` to illustrate how the short circuiting applicative works within the traverse to give you back all successful applications of calling the `half` function wrapped back up in `Just`, or simply returns `Nothing` if 1 call is "bad". That's about as much intuition I have for traverse at the moment but can see how it could be useful
Nothing is impossible, Haskell is Turing complete. And so is assembly.
I'd say these problems aren't limited to just data science field.
Haskell and clojure don't load fast. But, stack script downloads packages, and `clojure` does, too. I'm ok without auto-downloading if I can just download a package in a directory and allow a script to depend on a local package in a relative path. `clojure` supports this although it takes 4 seconds to load the simplest script.
My advise is just to familiarize yourself with why `mapM` is useful, and then think of `traverse` as a generalization of that in a few dimensions. (i.e. it works over many "containers" not just lists, and also it works with things that may not be monads but only applicative).
I think it could benefit from, say, purity by default (at least in the user's scope), more elegant pattern matching system with an ML syntax, `const` by default (not sure about that though, Haskell has plenty of optimization for immutable structures that I just can't fit), and easier access to lazyness
this! I can get type-on-hover with intero+haskero, but I'd like to be able to see haddock docs and language-aware autocomplete
Btw u/dev_matan_tsuberi where are you in your project of building a better programming language for data science/machine learning
Do you have experience with [clojurescript on node](https://github.com/anmonteiro/lumo)? Claims fast startup. Wrt deps, depending on which problem you are interested in solving, two things: statically compiled binaries (a widespread practice/requirement &amp; something that is google'able for pretty much everything listed above); Nixos; From the standpoint of developer experience I think Common Lisp again deserves mention because the api for installing packages is customarily accessed directly from your repl as you hack on your project.
Well, as with all dll hell scenarios, the state of your environment before you begin matters, and can change your experience. The lynchpin for me, and what gave me all the trouble last time and has now been documented, is the part about the zlib dll bundled with GHC - That being in conflict with the dll bundled with the `mingw gtk` package caused me a lot of headache, and it took awhile for me to figure out exactly what was happening and how to fix it. The recommended solution was what I eventually ended up doing to solve my issue as well. Your method of using `stack exec` to invoke pacman and install the `gtk` dependencies was also the tactic I took for success, and I consider it better advice than the documented method of using a separate MSYS installation.
&gt; You can write a regex to replace your operators globally if you've overused them. That's the incredibly user-hostile community I'm talking about...
I don't see how Turing completeness is relevant.
Yes, I have. It starts up faster than clojure although start-up time increases linearly with each added dependency.
&gt; Your method of using `stack exec` to invoke pacman and install the `gtk` dependencies was also the tactic I took for success, and I consider it better advice than the documented method of using a separate MSYS installation. In my opinion, this feature is one of the best features of Stack. Even better, you can run `stack exec mintty` to start up Stack's MinGW shell! It would be nice if this feature was documented somewhere though, as it's incredibly useful.
Closed type classes + families with the ability to specify the level of inference (default open world vs explicit injectivity vs maximum inference) seems like it would be a more powerful and less sketchy way to implement what you desire. You could then have the inference you want while still retaining the ability to define functions like the ones I gave above. Plus no excessive implicitness. 
IOHK has developed their own programming language Plutus based on Haskell and training around 10k developers in Barbados and Ethiopia in Haskell. I see it as good contribution
Honestly I don’t think it’s ok. It would be one thing if these were fundamental limitations inherent to the language. I wouldn’t be willing to compromise on the language itself for the sake of some specific group / popularity. But basically every single one of these problems can be solved at the library / tooling level. We should all want better documented libraries. Also `.Basic` modules with less abstraction built on top of the elegant more complicated abstractions seem like a great idea too. Even the extensions thing is a very reasonable issue, we should try and have more frequent reports that standardize the extensions that are most important / cohesive / tractable. So that way it is actually feasible to write extension-less projects if desired.
Ok that makes a lot of sense
Here's an example I used during the Advent Of Code. Say I had a grid represented by a list of lists: [[a_0_0,a_0_1,a_0_2,a_0_3,a_0_4,…,a_0_n] ,[a_1_0,a_1_1,a_1_2,a_1_3,a_1_4,…,a_1_n] ⋮ ,[a_m_0,a_m_1,a_m_2,a_m_3,a_m_4,…,a_m_n] ] And I want to get all southeast corners of the grid, that is `[[a_i_j | j ∈ [q,n]] | i ∈ [p,m]]` for all `p` and `q`. [[a_p_q,a_p_q1,…,a_p_n] ,[a_p1_q,a_p1_q1,…,a_p1_n] ⋮ ,[a_m_q,a_m_q1,…,a_m_n] ] `tails` will get me part of the way there, giving all the southern sections of the grid tails :: [a] -&gt; [[a]] tails [r_0,r_1,…,r_m] == [[r_0,r_1,…,r_m],[r_1,…,r_m],…,[r_m],[]] (init . tails) [r_0,r_1,…,r_m] == [[r_0,r_1,…,r_m],[r_1,…,r_m],…,[r_m]] One way to get all the eastern sections of the grid is to use `transpose` to flip the grid, using `tails` to get all the eastern sections, then use `transpose` again to flip it back: fmap transpose . init . tails . transpose :: [[a]] -&gt; [[[a]]] ghci&gt; (fmap transpose . init . tails . transpose) [[0,1,2] ,[3,4,5] ] [[[0,1,2] ,[3,4,5] ] ,[[ 1,2] ,[ 4,5] ] ,[[ 2] ,[ 5] ] ] `transpose` can be implemented in terms of `traverse` using the `ZipList` applicative: transpose = getZipList . traverse ZipList And using `traverse` gives a simpler expression to get the eastern sections of the grid without having to flip and flip back getZipList . traverse (ZipList . init . tails) :: [[a]] -&gt; [[[a]]] ghci&gt; (getZipList . traverse (ZipList . init . tails)) [[0,1,2] ,[3,4,5] ] [[[0,1,2] ,[3,4,5] ] ,[[ 1,2] ,[ 4,5] ] ,[[ 2] ,[ 5] ] ]
Tooling assisted refactoring that warns you any place there is a potential change in semantics would be great too (e.g field names when there is a Generic instance). 
A simple way to explain traverse: it lets you have a 'map with effects'. So you if you have f: (a -&gt; IO b), and xs: [a], you can get IO [b]. Traversable functors are those which have zero or more elements, and an order to those elements, so you can just apply an effectful function in order.
mapM is IMO no simpler to understand. I personally kind of wish it was deprecated. 
Also, I think closed type classes can be made modular, unlike existing type classes. But, we don't have closed type classes, yet, although it's possible to emulate some aspects of them. I should try that out some. I'm certainly fine with TDNR, even after we get closed type classes, but I will admit that my example is probably better solved, at least in Haskell, with closed type classes.
I’m not sure I agree with the idea that they are more modular? Typeclasses allow new data types to inhabit them which is key for modular code. I’m personally not fine with TDNR (or at least this variant of it, overloaded labels type of thing I quite like), as I think it will lead to it being used and relied on, which I think is almost always the wrong thing to do, and can get in the way of real solutions. 
It seems like a language like Haskell with huge amounts of static knowledge really shouldn’t be compiling much of that in, only the parts that are actually used?
If Haskell had a comprehensive angle into deep learning, I’d stop using anything else for data science/ML. But so far as I can tell, none of the shim libraries are close yet. Grenade had me excited for a bit, but realistically, the only industrial strength solution here is to hook into one of the big frameworks.
I think you're missing part of that description: What do you expect the value of `x["foo"]` to be? It's probably some closed world of Python types (numbers, strings, etc), right? But is that closed world the exact same when you do `csv.reads(blob)`, for instance? I just don't know what you'd want? I'm 100% sure you can create a Haskell type identical to that Python closed world (dicts and all!) and make it used equivalently (down to the dynamic-typed-ness of it). But I suspect trying to type those Python programs would be informative.
It tries it's best, but only at the linker level, not core, so it's not great at it. But even if it were optimal about that, Haskell codesize is still huge generally. Plus inlining happens a lot and duplicates a massive amount of code.
Yes, of course it can be done -- Aeson's `Value` type is it really, but the point is that type is ubiquitous in python but not in Haskell. 
How do we go about fixing that issue? I don’t want to be in one of those, codesize, composeability, pick one, situations
Not sure honestly. IMO, GHC should do whole world optimization, and that could make this a ton better. But that would essentially mean rewriting the entire backend
Wouldn’t that significantly increase compile times / hurt incremental building?
I think there are ways to make it work. If you hash the expression trees, I believe you can cache optimizations by these hashes, which would allow for Nix-like sharing and garbage collection of optimizations. Should make things fairly incremental. Bonus, if you use De Bruijn indexing to eliminate names, you get even more sharing!
I am avoiding throwing and catching custom async exceptions because it just feels dirty and wrong. I'm writing bindings to the ODBC C library, which uses return codes to signal success or failure, and I think it is better to use Either in this case, as failure is pretty normal e.g. wrong login credentials, incorrect SQL syntax, etc. I have a custom error type - `OdbcException` - so all my low-level IO functions return `IO (Either OdbcException a)`, where `a` can be various things depending on the function and context (often `()`, but also various handle types, etc). What I've read so far: * https://softwareengineering.stackexchange.com/questions/252977/cleanest-way-to-report-errors-in-haskell * https://www.fpcomplete.com/blog/2016/11/exceptions-best-practices-haskell * https://stackoverflow.com/questions/25752900/exceptions-and-monad-transformers * https://ocharles.org.uk/blog/posts/2012-07-24-in-praise-of-EitherT.html * https://stackoverflow.com/questions/13252889/elegant-haskell-case-error-handling-in-sequential-monads * https://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html I think I want to use some version of EitherT, but which one? Do I want: * https://hackage.haskell.org/package/errors * https://hackage.haskell.org/package/transformers-either * https://hackage.haskell.org/package/mtl-2.2.2/docs/Control-Monad-Except.html (ExceptT) * https://hackage.haskell.org/package/either (from category-extras) (but versions &gt;= 5 no longer include Control.Monad.Trans.Either - very confused; where is it now?) * http://hackage.haskell.org/package/EitherT (only provides Control.Monad.Trans.Either, but is deprecated in favour of either, which does not contain Control.Monad.Trans.Either !?) 
mapM is map, which ideally is straightforward, and then "but also, doing something effectful at each step." So, `mapM print` should be something that people can easily grasp, for example.
Yes, as far as I can tell: mapM =&gt; traverse mapM_ =&gt; traverse_ ...etc... ... and as far as I can tell `mapM` etc continue to exist solely *because* they enforce `Monad` rather than `Applicative` and that replacing `mapM` with `traverse` breaks existing code under some conditions that I don't understand. Since my *Haskell-fu* is not strong enough to understand those cases, it feels like a wart to me, and I would rather that the `Monad` functions go away or are just aliases for the `Applicative` versions. Can anyone explain?
This used to be a good list, but since there's _so much_ haskell in industry its not even remotely up to date anymore: https://wiki.haskell.org/Haskell_in_industry As to what each contributes, there's no central list or even a pretense to one. But I do know that a lot of the teams at major corporations (google, facebook, banks) have contributed a fair amount back either just from individual team members, or some times in terms of projects, and certainly in terms of bugfixes, patches, etc. At this point I think its less likely that the bl*kchain stuff has contributed much back, but that's not unexpected. 
10k developers!? those numbers cannot be right. do you have a source? I think something must have gotten lost in translation...
but that's exactly what `traverse` is...?
(Open) Type classes are anti-modular because they require global properties (uniqueness, or at least coherence) or you can get wrong results from implementations like Data.Map (if somehow the Ord instance used for one call differs from the Ord instance on a second call, the second operation may give incorrect results.) Instances also can't be local; they are always exported and always imported, even if no symbols are. Closed Type classes *can be* isolated and do not rely on global properties.
But think how useless closed `Eq`, `Ord`, `Functor`, `Monoid`, `FromJSON`, `ToJSON`, `Monad`, `Traversable`, `Witherable` and so on would be.
Where the C functions return error codes, or have relevant output parameters, use `Either`. Where the C functions use errno (or some other global), use IOExceptions. Use `throwIO` not `throw` when you do throw. Avoid async exceptions at all costs. Better to live in IO than to throw an exception from pure code.
&gt; (don't take the formatting seriously, I'm going through a phase) I love you. Don't ever change. (Also, great post!)
https://www.youtube.com/watch?v=bcMNm6QeGkY&amp;t=4979s . Just a correction. They will be training 10k in the next 12-18 months but have trained a few already 
Thanks. I have embraced using `Either` in the API wrappers, but I am struggling to get the client code to look nice. This is how it currently looks (yes, I like qualified imports): import qualified Data.Either as Either import qualified Control.Monad.IO.Class as MonadIO import qualified Control.Error as Error -- from errors package import qualified Control.Monad as Monad import qualified Database.ODBC.Internal as Internal -- ODBC C API wrapper ... let action :: Error.ExceptT Internal.OdbcException IO (Either Internal.OdbcException ()) action = do eenv &lt;- MonadIO.liftIO Internal.allocEnv Monad.when (Either.isLeft eenv) (Error.throwE (Either.fromLeft undefined eenv)) let env = Either.fromRight undefined eenv MonadIO.liftIO (Internal.setOdbcVer env) econn &lt;- MonadIO.liftIO (Internal.allocConn env) Monad.when (Either.isLeft econn) (Error.throwE (Either.fromLeft undefined econn)) let conn = Either.fromRight undefined econn eresult &lt;- MonadIO.liftIO (Internal.connect conn connstr) Monad.when (Either.isLeft eresult) (Error.throwE (Either.fromLeft undefined eresult)) let normstr = Either.fromRight undefined eresult MonadIO.liftIO (putStrLn normstr) MonadIO.liftIO (Internal.disconnect conn) MonadIO.liftIO (Internal.freeConn conn) MonadIO.liftIO (Internal.freeEnv env) eresult &lt;- Error.runExceptT action case eresult of Left exc -&gt; putStrLn (show exc) Right _ -&gt; return () I have avoided having many levels of indented nested cases, deconstructing the `Either` results, but the price seems to be (1) having to sprinkle `liftIO` around like fairy dust, and (2) using `isLeft`/`isRight` for the success and failure cases (because I often need the value carried by the `Right` for a subsequent API call). 
I think there was a lot of discussion around this change elsewhere already and many pros and cons were given for both sides of the argument. If this makes you more compelled to use PureScript, then perhaps that's not such such a bad outcome? Almost everybody moving away for that reason sounds a bit overstated, especially if you're talking about larger code bases.
How would the `NotFun` part (or something equivalent that finds the "shape" of the function) be implemented in Idris?
According to [*3.2.1 Variable-arity `zipWith`*](https://cs.brynmawr.edu/~rae/papers/2016/thesis/eisenberg-thesis.pdf) &gt; type family &gt; CountArgs (f :: Type) :: Nat where &gt; CountArgs (_ -&gt; b) = Succ (CountArgs b) &gt; CountArgs a = O &gt; &gt; The ability to write this function is unique to Haskell, where pattern-matching on proper types (of kind **Type**) is allowed.
Thanks. I seem to remember that Idris can use something called the [universe pattern](https://www.youtube.com/watch?v=AWeT_G04a0A) to handle "pattern matching on a type"-like problems, but I don't know if that technique is applicable here.
[I implemented a similar thing a while ago](https://doisinkidney.com/snippets/nary-uncurry.html) trying to get a variable-arity `liftA` (also using Richard Eisenberg’s thesis).
Not if you use another function or lambda instead of currying
Just decode your JSON into a `Data.Aeson.Value` and you can do that easily. Except Haskell won’t hide the assumptions you make about the type(s) within the JSON object (unless you enable `-fdefer-type-errors`). The more I use Haskell the more I realize that it just exposes the runtime errors of popular languages as compile-time errors. 
You can generally replace `unsafeCoerce` directly with `coerce` at the cost of an extra `Coercible` constraint (which might not always be solvable). gzip' :: forall f' f n a. ( Applicative f' , Coercible (Listify(n) f' a) (Listify(n) f a) ) =&gt; Arity(n) a =&gt; a -&gt; Listify(n) f a gzip' fs = coerce @(Listify(n) f' a) @(Listify(n) f a) (listApply @f' (pure fs) (getNumArgs @n @a)) Another way, that results in a more flexible interface, is to insert coercions between the "user-facing" functor `f` (e.g., `[]`) and the "`Applicative`-carrying" `f'` (e.g., `ZipList`) as needed inside the implementation (in this case `listApply`), so that the coercion constraint is as simple as possible (`Coercible f' f`). listApply :: forall f' f a n. (Applicative f', Coercible f f') -- extra Coercible =&gt; f' a -&gt; NumArgs(n) a -&gt; Listify(n) f a listApply fs = \case NumArgsO -&gt; coerce fs -- &lt;- extra coerce NumArgsS numArg -&gt; \arg -&gt; listApply @f' @f (fs &lt;*&gt; coerce arg) numArg -- &lt;- extra coerce 
sorry this is a little too much, got a simpler example to get a feel for it?
You wrote: &amp;#x200B; instance (CNumArgs n b, a\_b \~ (a -&gt; b)) =&gt; CNumArgs (S n) a\_b where &amp;#x200B; why can't we write: &amp;#x200B; instance (CNumArgs n b) =&gt; CNumArgs (S n) (a -&gt; b) where
This seems like a reasonable way to think about it for lists. If you generalise that `map` to `fmap` and that `IO` to any `Applicative` then you have `Traversable`.
`mapM` is less polymorphic, which may make it easier to form an intuition for. `mapM :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]` `traverse :: (Traversable t, Applicative f) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b)`
I am looking for a project to contribute to. I have completed a few small projects on my own, but I still consider myself I beginner. Where should I look to find work?
I'm not making this complaint about Haskell, I prefer Haskell and don't want to go back to python dictionaries, I'm trying to help people understand the difference in experience that already exists that turns people who aren't yet convinced away. The situation in python is that every library returns the ubiquitous dictionary type, and then access is always the same regardless of what data format you read. In Haskell only Aeson returns `Value`s. `Value` is simply not a ubiquitous like python's dictionaries are. There's nothing stopping people from writing a bunch of parsers for other formats that also return aeson `Values` but a) it hasn't been done, and b) it's unlikely to be done on a wide scale because Haskell developers are already converts and prefer the typed approach to decoded most of the time. Maybe it would benefit the Haskell ecosystem if Haskell Prime included Aeson's `Value` type natively so that more people would be likely to follow Aeson's lead and use it to produce an intermediate "dynamic-ish" format, but that seems unlikely.
This is a common trick to improve type inference. If it's ever ambiguous whether `a_b` is a function or not, your version would not be picked because the instance only works for functions. Whereas with the original version, the instance says it works for any type `a_b`, so the instance would be picked, and then the `a_b ~ (a -&gt; b)` would clarify that `a_b` is, indeed, a function, which may get type inference to progress somewhere else. Whenever you know that a type must have a particular shape, it's good practice to use this trick.
This says 'a hundred' [https://www.busiweek.com/ethiopian-government-cardano-technology-team-up-on-blockchain/](https://www.busiweek.com/ethiopian-government-cardano-technology-team-up-on-blockchain/)
What kind of projects (e.g. web dev, CLI apps) are you interested in? I've been working on a [help viewer](https://github.com/theindigamer/ozil) -- if that seems interesting to you (and the code doesn't look very intimidating), I can carve out beginner friendly issues over the next 1-2 days.
Wow, thanks. I did not know that!
 eresult &lt;- liftIO (Internal.setOdbcVer env) when (isLeft eresult) (throwE (fromLeft undefined eresult)) A value of type `ExceptT e IO r` is the value constructor `ExceptT` wrapping an action of type `IO Either e r`. It seems that you IO actions fit that mold. So you could simply do result &lt;- ExceptT Internal.allocEnv and you wouldn't need the whens and `liftIO`s. Also, if you decide to use an error monad transformer, I believe you should choose [`ExceptT`](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Except.html#t:ExceptT) because it's the one in the popular mtl/transformers libraries. 
It's described [here](https://chrisdone.com/posts/haskell-constraint-trick) (and in my [personal notes](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70#blog-reddit-constraint-trick-for-instances)) Instance resolution completely ignores the context `(CNumArgs n b, a_b ~&gt; (a -&gt; b))` before committing to an instance, it only tries to match the instance head ?? =&gt; CNumArs (S n) a_b and it's only once that matches that the context is taken into account
It's a bit hacky since it relies on the details of the resolution algorithms, but it works well
You can always point-ify things to get access to the list at whatever place in the pipeline you want. There is nothing special about list comprehensions here. 
That’s not the type or `mapM`
https://www.haskell.org/hoogle/?hoogle=mapM
/u/ocharles has been around for a while: https://hackage.haskell.org/package/base-4.7.0.0/docs/Control-Monad.html#g:4
I really wish `traverse` were called `mapA`.
http://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#v:mapM
I see. How strange
I suspect once pre-AMP base is no longer supported then `mapM = traverse` will be fine.
Oh goodness, yes. I certainly wouldn't want to *only* have closed type classes. I do think that it might be better to have distinguished necessarily unique instances (Functor, e.g.) from possibly non-unique instances (Ord, e.g.) and treated them differently. For example, having non-unique instance be first-class and have things like `Map` that need to consistently operate with one of the many carry it around. But, I think that's too large a change for Haskell (plus, even with dependent typing, equality of instances generally depends on function equality which is it's own bag of snakes).
This is the right way to think about it.
It seems like there are not that many Haskell benchmarks, although when considering the benchmarkgame the performances seem either comparable to java or significantly worse. I'm pretty sure Haskell actually has better performances than that but the lack of benchmarks makes this difficult to verify
There's a number of approaches. Before making any changes, I'd say that you seem to make almost forgotten the behavior of bind (`&gt;&gt;=`) for `ExceptT` does. If it's first argument throws, the result will throw and the second argument won't get executed. You seem to be writing code to do that over and over instance of just using the existing bind. You only need to deal with the `Either` explicitly when you can turn some `Left` into a `Right` by recovering. Firstly, I'd get rid of some of your repeated code by using `ExceptT :: IO (Either e a) -&gt; ExceptT e IO a`. Definitely avoid fromLeft -- partial functions as basically as bad as async exceptions. Secondly, if you find the liftIO calls onerous, then export lifted versions of your functions, either instead of or in addition to your unlifted versions. For listed version of functions that aren't yours, just do the lifting once and give it a name, probably won't export it, but that doesn't prevent you from using it internally. For cleanup functions like disconnect/freeConn/freeEnv, I strongly recommend use of bracket or something like bracket, rather than just piling them at the end of your block. HTH
I also recommend `ExceptT` over `ErrorT`, but the later has `MonadBaseControl` instances and the former doesn't. (Unless, I'm missing something.)
&gt; Haskell is already pretty fragmented due to extensions Can you expand a little on why you say this? I don't understand why everyone thinks extensions are a problem at all. There are a few weird ones that nobody uses that do cause conflicts and break backwards compatibility but almost all of the extensions that I see in the wild could just be thought of as "the syntax to use this feature is [whatever the immediate syntax is] plus [this extra syntax you have to type at the top of the file (or in your .cabal file)]." and the situation would be no different than just saying "the language has this feature and this is the syntax to use it" other than the fact that some of the syntax has to be typed at the top of the file.
Let me understand, you work for a consultancy that sells data science services in the form of software? Then it makes sense for your customers to "bet safe"; I've been talking to a few "data" consultancies and their tech choices seemed very conservative indeed (Microsoft stuff, SAP, Java, C++ etc.), precisely due to the high turnover of people and projects. I'd say Haskell is a completely sensible choice if you (company) are making and selling your own software and your architecture allows for language and tooling experimentation (for example if the thing is based on microservices). Oh and if the senior devs don't point their feet because they don't "get it".
Let's leave performance aside then (but see github.com/haskell-perf for a growing list of benchmarks); it would be great if we could start an informed discussion around experiences and needs for the Haskell numerical/data science part of the ecosystem. In fact, that's the whole purpose of github.com/DataHaskell .
I'm not a fan of monad-control. [This post](http://blog.ezyang.com/2012/01/monadbasecontrol-is-unsound/) and [this video](https://youtu.be/KZIN9f9rI34?t=471) touch on some of the problems with the library.
As my hero always said "need input". There are lots of projects and it really helps if you are interested in the end result and the community. Do you want to work with other people or just a pre-existing codebase? Do you like graphics? Developer tooling? Data processing? AI? Image processing? Cryptography? Networking? CLI? Web development? Chat apps? Developer tools?
But, by that logic, \`O(log n)\` is also sort of like \`O(1)\`. You can just assume that \`n\` will never be greater than \`10\^73\`, the number of particles in the universe, so \`log n\` will always be lower than \`73\`. Actually, while evaluating the feasibility of an approach with a crude first-pass analysis in my mind, I do treat \`O(log n)\` as \`O(1)\`.
I was looking at Google X, and they use Haskell for compilers.
Sure. I'm familiar with the problems. (At least in the abstract, they've never affected me.) Still, the instance would be nice to have for lifting things like bracket instead of having to do it manually yourself.
I hear a lot of people want better IDE integration. Maybe work on one of those projects? Improving the LSP daemon for Haskell, or better integrating the LSP into your favorite IDE/editor?
Specific, concrete, measurable (achievable) goals are key for GSoC projects. Can you maybe identify one or two HIE improvements that would have the most value for you?
&gt;Let me understand, you work for a consultancy that sells data science services in the form of software? Yes. Small company. Initial phone call to confidence-instilling demo was usually 2 weeks or so. Projects typically lasted a year or so with lots of back and forth with customer about intermediate results, use cases etc. We were usually functioning as accelerators for their in-house researcher teams (of "regular" scientists) because they know that if they ask their own dev teams to "prototype" an idea it will take a two years, cost $30M and they'll have no opportunity to change the requirements as they go along. &gt;I'd say Haskell is a completely sensible choice if you (company) are making and selling your own software and your architecture allows for language and tooling experimentation (for example if the thing is based on microservices). Often it's an interactive data exploration tool. GUI programming in Haskell isn't impossible, but it's definitely not at the level of C++ or Python. I have trouble imagining the equivalent of yhat or rstudio or canopy or whatever in Haskell because the ecosystem is so disjoint. We haven't even agreed on how to do file io yet. If you're writing a bunch of business logic microservices then I agree, you can sneak Haskell in without it being too painful, but in my experience it wasn't very common that we needed to stand up a microservice that just did some math and spit out the answer. We used the C ffi in Python or just wrote it in C/Fortran/whatever as needed when that happened. Maybe others have had different experiences. We weren't writing "web scale" apps that need to do linear regression over a billion records. We had "medium data." We were doing "implement this research teams' crazy idea and provide a way for them to iterate." &gt; Oh and if the senior devs don't point their feet because they don't "get it". This is a problem, but it's a separate problem.
It's almost as though there is too much variety of knowledge and human experience to arrange on a linear scale quantized scale with only three choices or something!
Thanks for the link. It also says "up to" and that the first class, which will be for 30, has not started yet...
I want to do class Empty a where prove :: a -&gt; b instance Empty a =&gt; Empty (a , b) where prove = undefined instance Empty b =&gt; Empty (a , b) where prove = undefined so, "if either a or b is empty, then the product of a and b is empty." But GHC complains about this, and I can't seem to find any extension that will make it stop complaining. Is there one?
I have some experience or interest in Data Processing, AI, Networking, and Developer tools
Thanks for the suggestion. I'm taking a look at the code base now and I'll get back to you after if I want to contribute.
Those make a lot of sense and I wasn't aware of them. Thank you!
Thanks for the shoutout Leander. If you haven't seen it already, you might find this gaussian process example interesting: [https://github.com/hasktorch/hasktorch/blob/master/examples/gaussian-process/Main.hs](https://github.com/hasktorch/hasktorch/blob/master/examples/gaussian-process/Main.hs) Small but practical example of how typed FP can make an algorithm's compositional structure transparent. Even at this early stage the compiler helped me understand the gaussian process model better, something I've not experienced outside Haskell which makes me think this is a thread that's worth pulling on more. Would be interested to hear more about what research you're doing. There's a lot going on now w/ transfer learning that's pretty exciting in terms of what composition-via-FP can bring to the table.
* AI - [grenade](https://github.com/HuwCampbell/grenade) is the go-to RNN right now. Tensorflow, hasktourch and hlearn are also big and alive. * Networking - there's some great packages ranging from the low level (network, hans), to protocol specific (http, websockets, wreq, servant, zeromq), to database (cql-io, redis), and services (aws, github, vcs-*). * Developer tools - I think the communicty focus is on [HIE](https://github.com/haskell/haskell-ide-engine) but tools like ghc-mod, ghcid, and hdevtools are also around. * Data Processing - I know other people claim Haskell data processing is alive but last time I looked around F# served me better. For example, best fit lines and curves for sample data still has no go-to package in Haskell. That means there is space for something like a .Net Numerics knock off if you feel like starting it. 
Note: The guarantee of small tree height there is simply nonsense. Repeated concatenation can make that as tall as you want growing linearly in height in operation count. /O(log_32 n)/ is still /O(log_32 n)/.
 type family Or (a :: Constraint) (b :: Constraint) :: Constraint where Or () b = b Or a () = a class Empty a where prove :: a -&gt; b instance (Or (Empty a) (Empty b)) =&gt; Empty (a , b) where prove = undefined Inspired by [this](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70#github-or-and-for-constraints).
 HashMap gets away with a significantly shallower tree, as it uses a 16-way branching factor. However, it nybble-reverses the keys. http://hackage.haskell.org/package/discrimination-0.3/docs/Data-Discrimination-Internal-WordMap.html provides a more direct implementation of the same idea for ints/words without the nybble-reversal, but without that it needs to do path compression rather than hope for good distribution like hashmap does.
I am already following it! And thanks for your awesome work!
I think looking only at the structure of the instance head during selection is pretty fundamental to the instance search and not just an implementation detail. I wouldn't really call it hacky.
Thanks!
Conceptual errors don't happen at the type level. Types usually only help detect very basic errors, ones that usually aren't expensive to fix - at least compared to development effort required to model everything as types. One should aim to think and minimize errors that are the most expensive - see [this image](https://raw.githubusercontent.com/matthiasn/talk-transcripts/master/Hickey_Rich/EffectivePrograms/00.27.19.png). The most common misconception in Haskell is that *if it compiles, it works!*. It only says it *may* work and, frankly, I've seen many examples of Haskell code that compiled and didn't work. On a side note, the second most popular misconception is that *types document everything* - which they also don't. A similar statement often exists in other languages, i.e. *good code documents itself*. Why is it untrue? Because code only shows the *result* of a thought process and not the *reason* - what the author wanted to express. In Haskell this behaviour is often exacerbated, because types are essentially a black box.
This is my concern. Elm a great onboarding tool, but there's a real risk of thinking that you have all the benefits of FP. 
Disagree. Back when I worked in dynamic languages I distinctly recall writing unit tests for throwaway scripts when the stakes were high. In one haskell-in-prod talk (I think it's the skedge.me one), they talk about how the type system let them write their quick hacky scripts faster and more hacky than they could otherwise.
https://www.youtube.com/watch?v=bcMNm6QeGkY&amp;t=4979s. Here is the link
Ak, more precisely around 1:28:00. They haven't done much yet, but just stated that they hope to have a MOOC as a result which in turn could train a large number of developers. It seems like a very maximal plan, of which very little has been done yet, and even the speaker seems to note that they have a "fallback" plan should that mainly not work.
how old are you?
I went looking for the skedge.me thing because I was interested and here is the link to the time where he tells the story: https://youtu.be/BveDrw9CwEg?t=725
[Maybe one of these](https://github.com/soupi/haskell-study-plan/blob/master/README.org#a-few-cool-open-source-applications)
&gt; I'm opposed to changing language at all, in general. I know, right? Shakespeare in particular totally bastardized the English language with all of his new-fangled words and shifted meanings. Call me a traditionalist, but I think all of us can agree that early hominid grunting noises are the only *real* form of communication. The rest is liberal nonsense.
Yeah, it's [societal pressure](https://www.smbc-comics.com/comic/2010-05-16) without a doubt
Carnegie Melon has reached gender parity in CS. They have a very active SWE chapter, do lots of high school and middle school outreach, and in general have been working hard to identify the various factors that have driven women away from the field.
All the problems of Haskell can be summarized in a deeper problem: the abandonment of functional programming in favor of typeful programming which is a very different thing that is not orthogonal: the more types are introduced, the more the chances are that composability would be hindered by accidental complexity introduced by spurious type adaptors. In many cases composability is not even possible. Take for example the main building block of haskell programming: the huge variety of monad transformers and free monads created for managing 4 (four) very simple effects. None of them compose well. That's is beyond than ironic in a language that is supposed to summum of functional programming. 
Thanks. Now it looks like this, which is more along what I was hoping for with ExceptT (no need to be so liberal with `liftIO` either): eresult &lt;- runExceptT $ do env &lt;- ExceptT Internal.allocEnv ExceptT (Internal.setOdbcVer env) conn &lt;- ExceptT (Internal.allocConn env) normstr &lt;- ExceptT (Internal.connect conn connstr) liftIO (putStrLn normstr) ExceptT (Internal.disconnect conn) ExceptT (Internal.freeConn conn) ExceptT (Internal.freeEnv env) case eresult of Left exc -&gt; putStrLn (show exc) Right _ -&gt; return () I'm not comfortable with using `ExceptT`, and was vaguely aware that ExceptT could be used to process a monadic block of actions returning `Either`s, but all of the examples I've found have manual unpacking of the `Either`. How would I integrate with `bracket`? It won't work out of the box because `bracket` is directly in IO, and expects the value to be returned directly i.e. not wrapped with `Either`. I've tried using `generalBracket` from exceptions, but I do not understand how this is meant to be used with monad transformers (and I get loads of type errors).
`ExceptT e IO a` and `IO (Either e a)` and intraconvertable (in fact, I think isomorphic), so you can always convert to IO for feeding into `bracket` if need be. There might be a better approach, though.
I think in this case, you might be better off passing around an explicit dictionary rather than trying to use instance resolution as a proof assistant. (It can do some cases, but not many.)
For people not working with Elm, what happened to operators?
Ya, that's kinda what I expected. Just wanted to make sure I wasn't missing some obvious way to make this work.
Thanks. This looks nicer tbh
`IO` is an instance of `MonadMask`, and `ExceptT` over any instance of `MonadMask` is also a `MonadMask`. That's what the following instances mean: MonadMask IO MonadMask m =&gt; MonadMask (ExceptT e m) `generalBracket` is usually only needed when defining your own `MonadMask` instance. For normal use, you can turn to [`Control.Monad.Catch.bracket`](http://hackage.haskell.org/package/exceptions-0.10.0/docs/Control-Monad-Catch.html#v:bracket) (also form "exceptions") which is more similar to conventional `bracket`. The signature, when specialized to `ExceptT e IO`, is: bracket :: ExceptT w IO a -- allocate resource -&gt; (a -&gt; ExceptT w IO c) -- cleanup allocated resource -&gt; (a -&gt; ExceptT w IO b) -- process allocated resource -&gt; ExceptT w IO b Notice that all the actions and callbacks are lifted to `ExceptT`. This means that if your cleanup action is a simple `IO`, you will have to use `liftIO` to make the signatures match.
[some resources to learn from](https://libeako.github.io/c/ID_253191122.html)
I've edited my comment and tried to alert about the loose term. I'm still curious to know if that kind of data structures would be competitive in practice for a wide range of problems, as the authors claim they are in Scala or Clojure.
Don't get me wrong. RRB-vectors are a fine general purpose data structure, but the asymptotics are worse than they are touted. Anything that was "actually" constant that supported all of the operations they support would violate some age old results from Tarjan about possible confluent persistent data structures, so its kind of important that the distinction be kept, which is the main reason I get on my high horse about them.
Elm 0.19 removes the syntax for user-defined operators, the reasoning can be found here: https://gist.github.com/evancz/769bba8abb9ddc3bf81d69fa80cc76b1
...While at the same time approaches that are genuinely functional programming such are continuations, that would allow such composability by eliminating the callback hell, are completely abandoned.
Same as class Eq a where (==) :: a -&gt; a -&gt; Bool a == a' = not (a /= a') (/=) :: a -&gt; a -&gt; Book a /= a' = not (a == a') {-# Minimal (==) | (/=) #-} Both methods have default definitions, which means that the user must implement `(==)` *or* `(/=)`, this is indicated by the [`Minimal` pragmas](https://downloads.haskell.org/~ghc/8.6.3/docs/html/users_guide/glasgow_exts.html#minimal-pragma). In your case the pragma ís {-# Minimal (writer | tell), listen, pass #-} which says you must implement `listen` and `pass` but it's up to you if you implement `writer` or `tell`. Hope this helps.
Ah, this is very interesting! I have some questions. - In the [linked paper](https://home.uni-leipzig.de/gkobele/files/papers/Kobele16CooperStorageIdiom.pdf) it is mentioned as an advantage that "generation can be done in polynomial time". Why is is considered as an advantage? Is it because lower computational complexity is intrinsically regarded as better or more frugal, or because it has implications for some software application inspired by the theory? - If I wanted to write some [SHRDLU](https://en.wikipedia.org/wiki/SHRDLU)-like program for some toy world, would these theories be useful for that purpose?
**SHRDLU** SHRDLU was an early natural language understanding computer program, developed by Terry Winograd at MIT in 1968–1970. In it, the user carries on a conversation with the computer, moving objects, naming collections and querying the state of a simplified "blocks world", essentially a virtual box filled with different blocks. SHRDLU was written in the Micro Planner and Lisp programming language on the DEC PDP-6 computer and a DEC graphics terminal. Later additions were made at the computer graphics labs at the University of Utah, adding a full 3D rendering of SHRDLU's "world". *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; Not to start a license discussion,... is the best phrase to start it &gt; but does it really matter that anyone makes profit of the software you create? In some countries, hiring unpaid interns is illegal.
[removed]
How about data Foo2 a b = Foo2 a b data Foo1 b = Foo1 b instance {-# OVERLAPPABLE #-} Empty a =&gt; Empty (Foo2 a b) where prove = undefined instance {-# OVERLAPPING #-} Empty b =&gt; Empty (Foo2 a (Foo1 b)) where prove = undefined instance Empty (Foo2 a (Foo1 b)) =&gt; Empty (a , b) where prove = undefined
Either * GTK * writing a GUI in OpenGL with libraries like GLOSS * Threepenny-GUI and the like, where you write a web app but is rendered in a browser locally (handled for you, you don’t require a user to navigate to local host) * doing a web app
Still in my opinion a very good project and plan for Haskell community in general. But you are right still need to be completed. 
You can also check out: * [http://hackage.haskell.org/package/fltkhs](http://hackage.haskell.org/package/fltkhs) * [https://github.com/reflex-frp/reflex](https://github.com/reflex-frp/reflex) (also deployable through embedded browser, like Threepenny-GUI) * and on the GTK+ front, I feel obliged to mention/self-plug [https://owickstrom.github.io/gi-gtk-declarative/](https://owickstrom.github.io/gi-gtk-declarative/) (although it's experimental at this point)
Thanks - hmm, while I would sometimes appreciate less of cryptic custom operators in Haskell's libraries, I would really miss them in DSLs and data structures...
Don't know if it's common knowledge or not, but today I've found a simple way of making constraints on type lists: Because constraint tuples (`()`, `(Read a, Show a)`, ..) have the same kind `Constraint` as other constraints, they can be arbitrarily nested. Now, if I treat 2-tuples and 0-tuple as `Cons` and `Nil`, I can easily build "constraint-lists": {-# LANGUAGE DataKinds, KindSignatures, TypeFamilies, TypeOperators, UndecidableInstances #-} import Data.Kind type family All (c :: Type -&gt; Constraint) (ts :: [Type]) :: Constraint where All c '[] = () All c (t:ts) = (c t, All c ts) Now I can e.g. implement `Show` on `HList` using just one instance, without overlapping: instance All Show ts =&gt; Show (HList ts) where showsPrec _ HNil = showString "HNil" showsPrec p (x:*xs) = let mp = 3 in showParen (p &gt; mp) $ showsPrec (mp + 1) x . showString " :* " . showsPrec (mp + 1) xs (What I don't know is if this approach may affect performance - will GHC check constraints just once and then happily use them for showing sublists, or will it evaluate all "tails" on subsequent calls of `show`?)
I can understand that. To some extent that's achievable as they are allowed in packages published in "elm" and "elm-explorations" groups.
&gt; In some countries, hiring unpaid interns is illegal. Which is honestly a weird comparison to make. Volunteer work is a more direct comparison, which is also *usually* unpaid, but sometimes can be paid, from e.g. donations..a lot like OSS. You don't get fired for not delivering on your OSS project, you do from your internship.
Isn't that what TypedFlow does?
Oskar's `gi-gtk-declarative` is well worth a look: https://owickstrom.github.io/gi-gtk-declarative/
I've never tried any graphical programming in Haskell so I'm curious as to why your answer is significantly more upvoted
You can find this type family (with a different name) in the `vinyl` package, so it certainly isn't an unknown technique. It shouldn't affect performance adversely too much, but I'm not confident in that claim.
Nice, thanks :D
I've had the most success with reflex and reflex-dom. That being said, I've had lots of issues as well, not a good developer experience altogether so far. I've yet to see GUI solutions as polished (and as free-of-bugs, and as working-at-all-on-a-basic level even on occasion) as they exist in other language ecosystems. I'm even considering to migrate one of my app's front-end off of reflex-dom to something else while keeping its back end in Haskell. Maybe it's just too much work to put in to get a good GUI library that no one has done fully yet, but as I said, not a very good experience using these tools so far. 
What is your opinion about doing webapp instead?
Much better. I've wrote a few websites using Yesod. It gives you much more in terms of what you expect from such a library/framework, and usually works as expected. More on par with what other ecosystems give you, sometimes even better. E.g. as you might imagine, templates are type-checked at compile time. So if a basic, click-to-navigate or at most submit-form level of GUI is acceptable for you, then Yesod will take you there. If you want anything more, you'll have to keep looking for something in addition to Yesod or different altogether.
FYI, having the same problems with `Cabal`. I tried LTS-11 (GHC 8.2.2) and LTS-12 (GHC 8.4.3), to no avail. I reverted back to LTS-9 (GHC 8.0.2) and that seems to work.
The problem with that code is that GHC only matches instances using their heads, not including constraints on the instances. So this two instances are the same pattern as far as GHC is concerned. An `Either` for constrains sounds like a good idea though, considering we have a `(,)` for them already. Not sure if there's a way to emulate this right now
I'm personally strongly in favor of only using Haskell's exceptions to indicate an error should not be recovered from, and the task should gracefully shut down (task being an abstract term; for instance an individual http request in a server). In all other scenarios, the error should be present in the type system with Either or ExceptT.
Maybe this completely misses your question but if you can live with a UI running in a terminal you should definitely check out [Brick](http://hackage.haskell.org/package/brick). It's very nice to work with and building something with it served as a nice learning experience for me a while ago.
My suggestion would be `reflex` and `reflex-dom`. With this stack you get a web app, a desktop app, and native Android and iOS apps out of the box. There are Haskell libraries for other desktop GUI toolkits, but it's hard to beat the flexibility that you get with Reflex.
I used `qtah-qt5` in production and added support for Qt Graphics View Framework to it. This library is one of the most impressive pieces of code I've ever seen. The author was readily answering my questions and provided all the support I needed. Sometimes you have to add bindings by yourself, though. But overall, I had some really good experience with this library.
When I had to do this I ended up wrapping the mow level GTK bindings and created [frpnow-gtk3](http://hackage.haskell.org/package/frpnow-gtk3). It is somewhat similar to reflex, but frpnow is much simpler and interacts quite well with low-level IO code when needed. An application using it can be found [here](https://github.com/george-steel/maxent-learner).
Seems like a dream, write once and run it on most popular platforms + its Haskell!
Is Threepenny-GUI something like Electron?
What are the advantages of Reflex over a nice Model/View/Update framework for a beginner working on their first haskell project? 
Or you can use `miso` and bundle it with Electron to make a web-based desktop app. Read more on Functional Reactive Programming if you're not already familiar with it. The best way would be to actually learn Elm in my opinion - which architecture inspired `miso` Depending on your goals: 1. ghcjs + reactive-banana -&gt; just FRP library with raw DOM manipulation 2. ghcjs + reflex + reflex-dom -&gt; FRP with better DOM manipulation utils + widget abstraction (best tooling and infrastructure like mentioned above, however they're all web-based actually. For the android and ios support, it's haskell code compiled to native code for the logic but the UI is still web-based. Also can be painful to set up because the stack relies heavily on nix, so yet another thing to setup. If I'm not wrong the author released internal tool called `obelisk` to make things easier). 3. ghcjs + miso -&gt; more like a framework with opinionated approach. comes with virtual dom + diffing algo so you don't have to manipulate the DOM directly (like React, IMO cleanest of all) If you want to learn Haskell, `miso` is probably the best because it leverages the type-system the most where as reflex and reactive-banana, you need to care about the javascript side and also they're more about FRP rather than Haskell
Nice! I wonder tough if you can use DerivingVia to prevent having DBFieldWrap in your datatypes.
I admittedly haven’t used it, but...crossing code boundaries like that looks very unpleasant : the RNN example has as much Python as Haskell. Tensorflow is also a hot mess of an API, if you can even call it that, and maintaining essentially a DSL to generate Tensorflow models in anything beyond these simple examples just sounds like a special kind of hell. Comprehensive SWIG-like auto-generated bindings to Pytorch or mxnet’s underlying C/C++ libraries, and a separate library for building up abstractions in Haskell (E.g. “hasktorch-auto” and “hasktorch”) is my first impulse, but I’m a full-time nlp academic researcher, and this would be waaaaay beyond “neat side-project”!
Sort of. threepenny-gui is basically just a small webserver running on localhost, so you can access the GUI it creates via a web browser. (You can however _use_ it with Electron to create a GUI; see the GitHub page for details).
The tooling. Reflex comes with infrastructure tooling that makes deployment easier and support for multiple platforms. Also it's been tested for real-world production projects. Watch the following talk if you're interested ;) https://m.youtube.com/watch?v=riJuXDIUMA0 Also if I assumed right, the MV* model you're talking about would involve a lot of direct mutations which can be easier for beginners coming from OOP but not really the Functional Programming way. In FP, Functional Reactive Programming is usually the technique being used and GUI libraries in Haskell are usually based around that idea. Because again the MV* model encourages direct mutation which is discouraged in Haskell
I recently wrote up a fairly comprehensive overview of Haskell GUI libraries for [this question](https://www.reddit.com/r/haskell/comments/a6bvr2/graphics_in_haskell/) - you can find it [here](https://www.reddit.com/r/haskell/comments/a6bvr2/graphics_in_haskell/ebxti0v/). For ease of reading I'll copy it into this comment: &gt; I struggled with this for a while - there are many old GUI packages, and many of them don't work or can't be installed on Windows (which I use). However, the major packages tend to work (although it tends to be easiest to install them using `stack`, not `cabal`): &gt; &gt; * The [`threepenny-gui`](http://hackage.haskell.org/package/threepenny-gui) library is best if you just want a simple way to make GUIs. However, it doesn't really do *desktop* GUIs as such: it displays your GUI as a webpage running on `localhost`. (However, it does have [Electron integration](https://github.com/HeinrichApfelmus/threepenny-gui/blob/master/doc/electron.md), which can be used to make a desktop application.) &gt; * If you want a simple way to make a desktop GUI, [`fltkhs`](http://hackage.haskell.org/package/fltkhs) \- which binds to the FLTK library - is a good bet, as it's the easiest to install of all the Haskell desktop GUI libraries (even on Windows!). Unfortunately, the FLTK library doesn't create the prettiest GUIs in the world (e.g. see the [GitHub page](https://github.com/deech/fltkhs/tree/master/images)). If you care about such things, the author has also released the [`fltkhs-themes`](https://hackage.haskell.org/package/fltkhs-themes-0.1.0.1/docs/Graphics-UI-FLTK-Theme-Light.html) library, which provides a set of widgets with a much nicer style - see [the GitHub page](https://github.com/deech/fltkhs-themes-demo) for a showcase. &gt; * The standard way to make GUIs in Haskell today seems to be using the GTK library. (This is what Leksah uses.) There are two widely-used libraries available: [`gtk3`](http://hackage.haskell.org/package/gtk3) and [`gi-gtk`](http://hackage.haskell.org/package/gi-gtk]). Although the latter seems to be recommended these days, I have found it to be almost impossible to install on Windows (at least, I haven't succeeded yet), and I haven't found any problems with `gtk3` yet. And by the way - if you're on Windows, don't be fooled by the dense [Haskell Wiki installation page](https://wiki.haskell.org/Gtk2Hs/Installation)! If you're using `stack`, then you can install GTK on Windows by going through the [GTK+ Download page](https://www.gtk.org/download/windows.php), but ignoring the instructions to install MSYS2 and prepending `stack exec --` to each command, so e.g. `stack exec -- pacman -S mingw-w64-x86_64-gtk3`; after this, you should be able to use `gtk3`! (It should be easier on other platforms - simply follow the [appropriate download page for the OS](https://www.gtk.org/download/index.php), and then you should be able to use `gtk3`. &gt; - If you just want to draw stuff on a window, then have a look at [`gloss`](http://gloss.ouroborus.net/) (a very simple yet useful interface to OpenGL) and [`sdl2`](http://hackage.haskell.org/package/sdl2) (which gives bindings to the SDL library).
I learnt Haskell by writing a webapp and I think it's better than trying to learn thru GUI programming. If you decide to go this path, I'd recommend using `scotty` instead of `yesod` because with `scotty` you'd write a more idiomatic Haskell and you'd have more freedom in writing your custom logic. With `yesod` a lot of things have been built for you and it comes with the framework's specific DSLs so it may not be very good for learning. There are other microframeworks as well but I like `scotty` because it doesn't use any custom operator to do type-level hacking so it's most idiomatic IMO. Try the following stack: `scotty` + `blaze-html` + `aeson` + `sqlite-simple`
My impression was that the tooling required you to buy into Nix, which seems like a pretty heavyweight solution for a beginner. I could be wrong about this though. By MV* I don't mean mutation at all, but rather the Elm/Brick/Gloss design: data Model = Model ... -- state of the app, we'll create new ones but we won't mutate them init :: Model -- initial model update :: Input -&gt; Model -&gt; Model view :: Model -&gt; Html -- or a gloss Picture, brick Widget, etc The details vary depending on the framework, for instance you can see the above mentioned gi-gtk-declarative's MVU strategy here: https://owickstrom.github.io/gi-gtk-declarative/app-simple/ IMHO this is probably easier for most people to get started with than FRP.
Just a quick correction: Miso is using The Elm Architecture (TEA) - not FRP.
Maybe it could even work with generalized new-type deriving? I'm not sure, I don't understand the ins and outs of these language extensions. I'm pretty sure the derived instances would still be orphans though (in this case).
Are you already familiar with FRP? I might be particularly dense but it's taken me a while to get to grips with Reflex due to my unfamiliarity with the model. If I were in your position _and_ unfamiliar with FRP I probably wouldn't go this route if you're trying to learn Haskell at the same time. As I say, I may just not be the sharpest tool in the box ;)
Ah you're right. However to me the Elm architecture is just FRP with abstractions though - the essence is still there, though the event network stuffs are hidden away from the user.
GHC will get the constraint for the tail `tl` by pattern-matching on `All Show (hd ': tl)`, which isn't too costly, but not free either. At runtime you will have two lists, the `HList` you want to show, and the `All Show ts` dictionary, which is a list of `Show` dictionaries.
Yeah nix is actually a turn-off for me as well, yet another thing to setup. Ah I see, I have never looked into anything other than web-based libraries so I'm not really familiar with those. Thanks! It's something new to learn. I agree that FRP is hard to grasp especially with Haskell syntax. I'm quite lucky that I came from JavaScript background where I have played quite a lot with Reactive Programming libraries and Cycle.js which helped me grasped the idea better when I tried to learn reactive-banana.
I would need to play a bit more with it. My projects are all nix and cabal new-build based and I could not get it to do anything with neovim yet. Probably because nix based builds and cabal new-build are not properly supported yet? If that's the case then adding nix and cabal new-build support would definitely have the most value for me.
I'm not convinced that that is a good way to learn Haskell unless you are already very familiar with functional programming. There will be so many unfamiliar concepts and terms that you will rapidly get lost and just give up. The whole way the app is structured will be different to python. In python io goes anywhere. In Haskell you start with io and follow from there. You can't really just map the python to the Haskell. I would suggest working through a book and it's exercises so you understand these before embarking an a larger project. 
I see your point, but the point for me is that If Haskell and it's concepts are not useful to solve my carrier problems then I should stop and ask an Important question from myself, something like "why should I bother myself to learn something that is likely less useful?", I tend to think that it is useful and to grasp the benefits from the language I should have a Reformation in my problem solving skills. Maybe I should skim through all the Haskell concepts then make my decision about it.
Indeed we have gone down the route of code generation with hasktorch (codebase LOC is probably 80% codegen), wouldn't be feasible without auto-generated bindings as a force multiplier. Next iteration will be even more so. You'd be surprised at what can be accomplished as a "neat side-project" when Haskell is being used :)
[https://github.com/bigos/cairo-snake](https://github.com/bigos/cairo-snake)
They're the author of the top voted library in the thread so far. Also, people psychologically upvote posts with actionable links more than posts with just text :)
Elm is not FRP with abstractions. Elm consciously moved away from FRP a couple years ago. https://elm-lang.org/blog/farewell-to-frp
I'm glad to hear it! :)
For what it's worth, I consider the similarity between Brick and Gloss to be pretty superficial, including only the pure functional state update. I compare Brick to Gloss to make that point, but the comparison definitely ends there.
Oh cool, I just threw that name out there, must be a high-probability portmanteau. I’ll check this out!
Yeah, depending on the complexity of what you're drawing, the Vty image diffing that happens on each redraw might end up costing enough that it would be tough to use for high frame rates. That isn't to say it couldn't be improved, though. It's also possible that Brick's core rendering loop could be improved. I've done some work to investigate that but I haven't had good results isolating the various factors yet.
I wonder if a good way to make it possible to write in Haskell but have impact throughout data science would be to have a way to expose Haskell APIs as Python libraries. I had a similar situation in grad school, where I needed to implement something super-efficiently, so C++, but ultimately needed a nice Python package for a collaborator. Very smooth sailing by just setting up SWIG to generate the Python side, and focus on writing C++. With all the code introspection power, could a GHC plugin do something like this?
Thanks!
I couldn’t read this article on my phone. Too much ads. :-(
I agree with what others have said here -- imagine if you wanted to call map on a structure, but the function you were mapping over did something effect-y (like IO). traverse is exactly the function that lets you do this.
In my understanding volunteer work should be for non-profit. When the volunteer work is compensated, the work is still done to benefit non-profit causes. Otherwise it's work. I.e. - "can you make the feature A so we can use it my lib B" **is different from** - "can you make the feature A so we can use it in our (proprietrary) product C" Maybe for you the difference is irrelevant, but for others it well may be not. If person releases something under license A, asking them to re-license under (more permissive) license B is very a like as "saying this costs too much". Thus my internship comparison: "We cannot pay you the proper wage, do you want to intern for free?" Maybe HHP is a free market reaction for the "high price", maybe not. Also there's `hindent` doing similar thing as `brittany`, and if copyleft of `brittany` is a problem, one can still consider `hindent`. I still wonder why AGPL of brittany or ghc-mod is a problem, getting a signoff shouldn't be harder than asking the developer(s) to relicense. Emacs is GPL'd, which additional clauses of Affero make brittany or ghc-mod harder to use for developing own products. It's clear that the "code written" by brittany or ghc-mod (i.e. reformatted code by brittany or case-split by ghc-mod) doesn't need to be AGPL'd. *Maybe* there should be "bison exception" -like exception to make it super clear (if either of tools uses some kind of templates); but this is my speculation. Note, there's a lot of GPL software we use: `git`, gold linker, linux kernel... so again, what's more tricky about Affero? 
If it's not doing unnecesarry constraint resolution on every sublist, it's fine - at the end, if I were to write performant code, I probably wouldn't use HLists altogether - from my understanding, I would probably always end up passing class dictionaries somewhere at the runtime, because of their nature... Thanks for clarifying :)
Opaleye encourages (but does not require) you to do this via a [product-profunctors `Default`](https://hackage.haskell.org/package/product-profunctors-0.10.0.0/docs/Data-Profunctor-Product-Default.html) instance. For example, for a type defined like data Birthday f = Birthday { bdName :: TableField f String SqlText NN Req , bdDay :: TableField f Day SqlDate NN Req } you write an instance Default p (Birthday a) (Birthday b) and then the instances for (the equivalent of) `HasSqlEqualityCheck`, `HasSqlValueSyntax`, `FromBackendRow`, `HasDefaultSqlDataTypeConstraints`, `HasDefaultSqlDataType`, ..., come for free. The `Default` instance can even be generated by TH or Generic (although no one's implemented support for HKD types yet ... volunteers welcome). So what did we gain? * No orphans * Boilerplate has been reduced to just *generating* the Default instance (with TH or Generic) * No need to unwrap and wrap at the call sites (Opaleye queries) * No need to depend on lens * The Opaleye schema is no more verbose What did we lose? Nothing, I don't think!
You may find this list useful: * https://github.com/willbasky/Awesome-list-of-Haskell-mentors
I reduced some settings in google adsense. It should be less intrusive now. 
There is one paper which addresses the paper FP vs OOP by Meyers in some way. In [F is for Functor](http://www.cs.ox.ac.uk/people/daniel.james/functor/functor.pdf) Meyer makes a cameo as "Betrand" :D
I wanted to do this: newtype BdName = BdName String newtype Dob = Dob Day Birthday { bdName :: TableField f BdName SqlText NN Req , bdDay :: TableField f Dob SqlDate NN Req } So that my queries don't get meshed up (in opelaye terms). Anyway the point of the article isn't about beam, it is to show a neat little trick you can do with the `Wrapped` type class so share instances across newtypes without needing access to the definition. 
You can go forward with this approach and use indexed monads to implement state machines with type level assurances. There is a blog post about it by Oscar Wickstrom: [https://wickstrom.tech/finite-state-machines/2017/11/19/finite-state-machines-part-2.html](https://wickstrom.tech/finite-state-machines/2017/11/19/finite-state-machines-part-2.html) Another approach is to use free categories which I described here: [https://coot.me/posts/finite-state-machines.html](https://coot.me/posts/finite-state-machines.html) 
Interesting. I wouldn't suggest that with Opaleye. I would suggest newtype BdName f n r = BdName (TableField f String SqlBdName n r) newtype Dob f n r = Dob (TableField f Day SqlDob n r) -- ^ Both of those can be given Wrapped instances, -- if you like data Birthday f = Birthday { bdName :: BdName f NN Req , bdDay :: Dob f NN Req } I'd probably suggest the same for Beam too, but I don't really know Beam, so maybe I'm wrong.
I was interested to read about your approach and the `Wrapped` class, but it seems like significantly more work than our tagged/phantom approach and with no upside that I can discern. If you define one-newtype-to-rule-them-all, with [the same definition as from the `tagged` package](https://www.stackage.org/haddock/lts-13.0/tagged-0.8.6/Data-Tagged.html): newtype Tagged (s :: k) a = Tagged { unTagged :: a } only in your own codebase to avoid orphans, now you can define every instance you're going to want in one go with GeneralizedNewtypeDeriving. Every individual type you want to be checked as a unique newtype is instead just a new type synonym that fixes the type parameters of this `Tagged`. The phantom type parameter is kind-polymorphic – its easiest to use a string with the name of the type synonym. You've already gone most of the way with your special newtype to handle all the instances. The advantages to going a step further to this approach are: * It is much nicer to have your instances at the level of the types you actually use, rather than needing explicit wrapping/unwrapping to get things to work. With our approach you wouldn't need to look at `DBFieldWrap _` in so many places, because the newtype that has the instances is the same one that you use for type safety. * Its a lot easier to do the equivalent of defining new newtypes. Especially if you'd need to add a long list of derived type classes for each one. * Its nicer to only deal with 1 new name per type instead of 3 – Just the type name, because the constructor and accessor names are the same. Same goes for the lens, if you use it. 
Makes me want to do a proper comparison between various db systems, alas that has to wait.
That would be interesting to see! If you decide to try it then let me know if you need any assistance with the Opaleye parts :)
I’d import this if you threw it up on hackage. 
I'm sorry I have a hard time keeping up with all the concepts you're talking about. If I understand correctly, you suggest to introduce a newtype specifically for beam in this case, and then use type synonyms with a (phantom) datakind to distinct them? I find this approach interesting, but it has to wait. I have more pressing matters to attend to, and I'm reasonably happy with my approach (although yes the wrapping and unwrapping is unfortunate indeed). 
You're right, sorry
The `log` in complexity is normally `log_2`, rather than `log_10`. Still, 2^73 is sufficiently huge that you can keep treating it as more or less constant time.
[Done!](http://hackage.haskell.org/package/arity-generic-liftA)
What is the state of dependent types in Haskell? How usable are they inside the ecosystem?
How does composable data types, or data types a la carte, fit with optics? Does the question even make sense?
AFAIU it, it is 2018, and no one advocates for global installs anymore. They are tricky and break things often. It's difficult for developers and authors alike to manage global state like that. That's why most build-tool related how-to's I've seen in the modern era (post `v2-*` commands in Cabal, and more recently with LexiLambda's [opinionated guide](https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/)), using the explicit `install` commands for _either_ build tool is discouraged. I understand your gripes about `uninstall`'s, but they are moot in a world where no one _needs_ to uninstall, so perhaps that's why you haven't seen much effort placed in that area. PR's are always welcome, though, and you are free to contribute code and discuss it with the team!
If you really want to uninstall, you can use ghc-pkg unregister, and then just delete the files. While this would be easy to implement, as /u/emilypii already pointed out, it's a bit of a moot point when everyone's already only installing into sandboxes anyway, and has no need to uninstall anything. In recent years, I don't even bother having a globally installed GHC, and just use nix to put me in a shell with appropriate dependencies for whatever I'm working on.
Don't people usually resort to using a `Makefile`?
TL;DR: Both tools have no uninstall command, and have finicky CLIs. Pretty surface level complaints IMO. It'd be nice if no one had issues with these things, but they hardly seem like the leading issues with these tools, and definitely don't seem like serious reasons to denounce them.
As I understand it, the main problem with uninstalling is what to do with (transitive) dependencies. But isn't this exactly the problem that nix also faces? In which case, couldn't we take a page from the nix book and have some sort of garbage-collection command? `cabal v2-uninstall` could remove packages from the live set (maybe displayed via `cabal v2-list-installed`) and some new `cabal v2-garbage-collect` could do the work of deleting packages which are not in the live set or its transitive dependencies. I could see myself using `uninstall` and `garbage-collect` as ways of reclaiming my disk space.
&gt; Monadic parsers are head and shoulders above most other solutions. Still relatively new to Haskell here. Care to explain why?
I'm not a Haskell user, I'm just posting a user experience I've read of someone I don't know because it's almost exactly the same one I had the many times I tried it. It's not just the `uninstall` thing, there are several points there, explicit or implied. I went through the pretty stone age way of nuking `~/.cabal` to fix things, and I've seen a lot of devs doing and advocating that. You still have to manage cached artifacts when using sandboxes, which easily grow to several GBs. At least the time I used, and in this post too, Stack is viewed as a Cabal replacement to fix all the things, but then you learn you have to deal with the two. I once watched [this 2 hour long tutorial](https://www.youtube.com/watch?v=sRonIB8ZStw) on Stack, watching an expert get confused on dependencies getting built multiple times, in a very slow process of downloads and builds, this is far far from a good first time experience at all, I'm glad 99% of the language setups don't make you go through that. I went through several other bad experiences like always breaking code completion support due to ghc-mod incompatibilities with newer GHCs, etc. I've summarized part of my own experience in the past [elsewhere](https://www.reddit.com/r/haskell/comments/5quf5y/learn_purescript_or_haskell_first/dd5fvba/), which can be used for comparison. When two completely strange persons agree on such number of issues, I think that's indication of something. As a beginner (and currently non-user), I'm not going to invest time in fixing or PR'ing all this. These things have not been fixed for decades, and will stay unfixed.
https://www.reddit.com/r/haskell/comments/aba7gb/why_i_am_not_a_fan_of_cabal_or_stack/ecz2r6s
&gt; I could see myself using `uninstall` and `garbage-collect` as ways of reclaiming my disk space. Even if you don't mind disk space, you may still want to claim commands dropped in your PATH.
I suspect you are being downvoted because: &gt; When two completely strange persons agree on such number of issues, I think that's indication of something. This is a terrible statistic. If the recent [Haskell survey](https://taylor.fausak.me/2018/11/18/2018-state-of-haskell-survey-results/#question-041) is any indicator, there are people happily using these tools without complaint in the community. &gt; As a beginner (and currently non-user), I'm not going to invest time in fixing or PR'ing all this. These things have not been fixed for decades, and will stay unfixed. Many of us use these tools professionally and aren't seeing the same problems you appear to have. The fact that you intend to "fix it all" as a self declared beginner and non-user seems naive at best. I do wish you the best of luck if you do end up forking and improving either project. I'm sure if you get your PRs merged it means the community agrees with the direction you believe things should go.
Haskell is one of those odd languages where if you are prepared to invest time and effort into learning a new paradigm, unlearning imperative expectations, and avoiding the idea that you can "learn it in a few days", the benefits begin to outpace the drawbacks of the long ramp up. If you need to ship software tomorrow then it's probably not a great choice. If you need to write software over the next year, and more importantly support it for years to come, Haskell is ultimately a fantastic choice in my humble opinion.
I also found stack to be very confusing when using it at first. Intuitively for me I assumed `stack install` would install in the local project and not globally, for example. It would be nice if you could add packages by doing `stack install --local` or something similar. Also the fact that there are multiple configs and that you have to add a bunch of stuff in extra-deps confused me a bunch at the start. It is also not clear where I can find all resolvers. Otherwise it is quite a powerful tool, it just never seems to follow my intuition. I think the learning curve could be reduced quite a bit by doing some user research but then again that could be said for the entire language ecosystem...
What about a version that infers the other way? When we see \`lift f xs ys zs\`, where \`xs :: f a\`, and we see that the result should have type \`f d\`, we can infer that \`f :: a -&gt; b -&gt; c -&gt; d\` for some \`b\` and \`c\`. The arrow-counting version seems to rely on knowing the exact type of \`f\`, which feels un-Haskellian somehow.
Read the post I'm providing an answer for, that point you're considering naive is just a response to what was suggested as solution: fix it yourself. I also do know professionals are blind to this, specially in the Haskell community, I've seen they advocate nuking and wasting hours downloading and building for getting slightly more than a working "hello world". If it's all properly cached and versioned in your professional machine, then great. They also seem not to care for the POV of "non-professionals" or beginners, or even the image Haskell tooling will pass to that public in comparison to other languages.
It looks like `self-healing` might be the keyword I'm after.
It's pretty serious that these issues are unsolved for years or maybe a decade now.
I dunno that it's "serious". Frustrating, maybe, but they still don't seem like reasons to denounce the tools IMO.
Garbage collection a la nix is the only answer here. The trouble is determining what is and isn't installed. Local projects need to declare their GC roots somehow. I find Nix's approach to this problem satisfactory (having a GC roots directory in the store with symlinks to hardcoded path's to symlinks in your project directory pointing to the store), but it's a little brittle. Alternatively, cabal-install could just keep a database of installed and uninstalled packages that is updated by any command that messed with dependencies in any way. The downside of this is that uninstalling a local project dependency is no longer as simple as deleting a symlink
&gt; Intuitively for me I assumed stack install would install in the local project and not globally, for example. I don't think that's intuition. I think that is learned behavior from other package managers like NPM that misuse the word "install."
I think they're genuine reasons when they significantly impact the new user experience. Stack and Cabal are part of what makes it hard to attract people to Haskell.
One thing that would help is to better standardize our container typeclass hierarchy. There are some good classes out there that make interacting with a variety of containers much nicer, from Witherable to Ixed to Subhask. If everyone got used to a nice set of container classes that handled indexing and filtering and mapping and conversions and so on then parsing libraries can return whatever data type they want and you can just start playing around with them. There is still the situation of how best to handle indexing into something that could be a Number or a Bool (such as with Value), but I think the existing situation of either using Prisms or pattern matching is not bad and when combined with some container classes should work well. You can also always read in your data as `Map String Value` to at least unwrap the first dict Layer straight away. 
You can always delete the executables or symlinks if that's all you want.
You must be fairly new to Haskell if you think types only detect very basic errors. Look at things like Servant or binary trees that the type system guarantees are balanced for examples of types doing much more. You can convert most runtime errors / incorrect runtime behavior into types, with the exceptions being things that Haskell’s type system isn’t powerful enough to enforce like “no `undefined` or infinite loops”.
Curious, is Stack backwards compatible with Cabal? Can Stack packages be published to Hackage and installed via Cabal?
The potential for generative testing is, Imo, the big win with spec. It's also more composable.
The principle of least astonishment depends on the audience and their experience. Arguments that one’s experience is more valid than another’s should be justified.
Sure, sure, yeah, being certified of each command that's dropped on a given install command to undo it, sounds nothing like bare bones rudimentary. 
Aha! Here's an option with rather different trade-offs: class Functor f =&gt; Roob a f r | r -&gt; a where oop :: f a -&gt; r instance Functor f =&gt; Roob a f (f a) where oop = id instance (Applicative f, Roob b f q) =&gt; Roob (a -&gt; b) f (f a -&gt; q) where oop fs xs = oop (fs &lt;*&gt; xs) liftN :: Roob b f r =&gt; (a -&gt; b) -&gt; f a -&gt; r liftN f fa = oop (fmap f fa) &amp;#x200B; Note that it's *not* possible to write a type family calculating `a` from `r`, which makes this a somewhat peculiar use of fundeps.
I mostly agree with you but the haskell survey is not a good point of reference because it's going to explicitly select for users that are already a part of the community. The lost flux of new users is not going to be reflected. I see both sides here. The user experience leaves a lot to be desired. On the other hand, these tools are made by volunteers, so I'd rather see criticism raised in a less entitled tone. Not that it's something to strive for, but python regularly spits out exceptions + stack traces as the standard user experience for its package manager and it has picked up lots of users. I suppose that reflects a separate dimension of how motivated new users are to be onboarded.
Ah, I missed the generalised `bracket`. Thanks. Code looks like this now, which seems reasonable: eresult &lt;- Error.runExceptT $ do Catch.bracket (ExceptT Internal.allocEnv) (\env -&gt; ExceptT (Internal.freeEnv env)) (\env -&gt; do ExceptT (Internal.setOdbcVer env) Catch.bracket (ExceptT (Internal.allocConn env)) (\conn -&gt; do connected &lt;- MonadIO.liftIO (IORef.readIORef (connConnected conn)) Monad.when connected (ExceptT (Internal.disconnect conn)) ExceptT (Internal.freeConn conn) ) (\conn -&gt; do normstr &lt;- ExceptT (Internal.connect conn connstr) MonadIO.liftIO (putStrLn normstr) ) ) case eresult of Left exc -&gt; putStrLn (show exc) Right _ -&gt; return () 
For the moment, they are clumsy and involve the use of libraries like `singletons`, which may be not completely natural for standard Haskell users. This is the Haskell part, which complements the fact the that are not many know design patterns for standard programming with dependent types. In the future, dependent Haskell may be a reality.
Cabal drives me nuts whenever I have to use it (thankfully not too often these days). I understand that installing is not ideal and should be avoided, but just look at what happens when you try to build instead of install: ``` $ cabal new-build Resolving dependencies... Build profile: -w ghc-8.6.3 -O1 In order, the following will be built (use -v for more details): - totally-sane-0.1.0.0 (exe:totally-sane) (first run) Configuring executable 'totally-sane' for totally-sane-0.1.0.0.. Preprocessing executable 'totally-sane' for totally-sane-0.1.0.0.. Building executable 'totally-sane' for totally-sane-0.1.0.0.. [1 of 1] Compiling Main ( Main.hs, /home/omer/haskell/totally-sane/dist-newstyle/build/x86_64-linux/ghc-8.6.3/totally-sane-0.1.0.0/x/totally-sane/build/totally-sane/totally-sane-tmp/Main.o ) Linking /home/omer/haskell/totally-sane/dist-newstyle/build/x86_64-linux/ghc-8.6.3/totally-sane-0.1.0.0/x/totally-sane/build/totally-sane/totally-sane ... ``` I honestly don't want to use a tool that was designed by people who thought that path is OK. To make the matters worse it doesn't have a flag for where to copy the bin. stack has a flag for that, but it's not documented in `--help`. So good luck with "buildling" (instead of installing). Whenever a beginner writes these kinds of blog posts/comments etc. experienced Haskellers start explaining things. The thing is there's nothing the explain; these tools suck. You just get used to them after a while.
Piggybacking off of what a lot of others have said, these methods for handling packages are pretty much unused now (see /u/emilypii’s comment), and a read through of [this explanation from the Stack docs](https://docs.haskellstack.org/en/stable/stack_yaml_vs_cabal_package_file/) should help clear up exactly the way Stack, Cabal, Hackage, and Stackage relate to each other.
People who hack on GHC - what is your editor setup? The only feature I'm looking for is goto definition.
There are only "cabal packages". There is no such thing as a "stack package." `cabal-install` and `stack` are both tools for working with cabal packages.
As someone who's used both \`cabal\` and \`stack\`, I've recently started using bash commands defined in a nix-shell that invoke \`runghc Setup.hs (build/configure/clean)\` and it's been great. But yea, nix solved a lot of problems here regarding install / uninstall with its pure model.
First off the paths have been reduced somewhat in the latest version afaik. Secondly, why do you care about the path? It's for the machine to worry about that, not you! Also `cabal new-install` does have such a flag. New-build doesn't because it isn't supposed to install a bun anywhere, just copy it. Finally, the fact that the bin is in a hard to find place is a known problem and its one of the reasons that new-build hasn't become just "build" yet! People know this is an experimental feature that hasn't had the UI polished yet, and it is not great to complain as though people thought that this was the way things _should_ be -- just the state they're in. 
If I shouodn’t care about the ridiculous path than _why is the tool printing it out_?
I'm not a great Haskell programmer by any means, but learning Haskell has really improved my JavaScript - which is what I use at my day job, so it's been worth it!
What is a short snappy adjective that means "potentially empty" in the context of data structures? Compare with "nonempty" for a container that has at least one item. The only related word I can think of is "nullable".
Does it matter? What I try to convey is that, just as I expect the cross at the top right of a window to mean close, running stack install would install the package locally. If all the common package installers have that system then most people will expect that behavior
I notice the url of the post \http://www.rntz.net/post/2018-05-18-**why-i-am-not-a-fan-of-stack**.html implies the original title of the post was actually "Why I am not a fan of Stack"... =)
Huh. I guess Idris (or sometimes Agda) is the better alternative until then. A way to use Haskell packages in Idris would be also quite useful.
Last time people here tried to tell me that Haskell has got the most advanced package management. All package managers I know are able to track dependencies among packages and can help to uninstall stuff properly. What you just suggested is doing both things manually. Sorry, it's too difficult for me to track dependencies and to know if I can just delete something. It's even worse. Since you can install multiple versions and they overwrite each other, but do not uninstall previous files, you're polluting installations on whole new ways and it's not trackable what to delete and what not. Additionally it can break updates, if packages load any resources dynamically from their installation directories.
That linked documentation doesn't do a very good job in being fair to cabal's features. For instance, it says &gt; Stack is a build tool that works on top of the Cabal build system, and defines a new concept called a project. as Cabal too defines a "concept called a project" with those `cabal.project` files. ---- PS: There's a couple more confusing statements mentioned throughout that document such as &gt; why doesn't Stack automatically add `acme-missiles` to `build-depends` in your cabal file if you add it as an extra-dep? There are a surprising number reasons actually: &gt; &gt; * The cabal spec doesn't support anything like that Wat? So the spec doesn't explicitly mention that you could insert a `build-depends: acme-missiles` line into cabal files via textfile editing and therefore it's not supported by the spec... makes sense!
Do you consider [Nix](https://nixos.org/nix/) an advanced package manager? 
If anything people would resort to a [`Shakefile`](http://shakebuild.com/)!
&gt;cabal-run Sounds like your friend reinvented \`cabal v2-exec\`. You're not supposed to poke inside the \`dist-newstyle\` directory. But if you really must, then use \`cabal v2-exec which -- name\` to find where \`name\` is. I agree that the verbose output is not useful: so I suggest you send a PR to simplify it. As a very recent beginner to Haskell, I read the manuals for both \`cabal\` and \`stack\` (and \`ghc\` and Emacs). That's where I learnt these things. I'm sure both you and the poster have your reasons for why you didn't read the manuals, or even the \`--help\`, where these things \*are\* documented (I just checked). In cabal's case, you're looking for \`--bindir=\` / \`--symlink-bindir=\`.
&gt; use `cabal v2-exec which -- name` to find where `name` is. Wow. That's a nifty trick!
Avoiding the type level naturals from deriving Arbitrary via (Between 1900 2100) where 1900 is a type of kind Nat satisfying constraint KnownNat 1900, with between :: Int -&gt; Int -&gt; Arbitrary.Dict Int between = ... instance Arbitrary Year = coerce (between 1900 2000) where 1900 is just a normal integer, really drives the point home that the notion of *phase* is fundamental. many exemples of type level magic would become far easier if we had this notion baked in. It's also of prime usage for dependent types, which are allowing to making even more distinction based on values (aka : values ..from which phase?) Here's a few links that look relevant around this idea : http://lucacardelli.name/Papers/PhaseDistinctions.A4.pdf https://edwinb.wordpress.com/2015/02/28/practical-erasure-in-dependently-typed-languages/
I don't think optics make much difference for data types à la carte or most other alternatives. They probably bring as much convenience to those as they do to normal types.
&gt; If I understand correctly, you suggest to introduce a newtype specifically for beam in this case Well, the opposite actually. I'm suggesting a single newtype for nearly *all* type definitions that would otherwise be newtypes. Not just Beam-related stuff. Put all the instances for all the libraries you use on the one newtype. I say "nearly" all because some types you may want to restrict the interfaces that are implemented, and/or you may want it to be more explicit and noticeable when wrapping/unwrapping than the above way. So you use a standard newtype in that case. Otherwise literally all newtypes definitions could instead be defined this way. &gt;and then use type synonyms with a (phantom) datakind to distinct them? Exactly right.
A - Those tools are a far cry from what was there before. B - But they are even more of a far cry from where they should be. The point A (which praises, not denounce, current tools) is no reason to not talk about point B. Some other languages have excellent UI for tools, excellent workflow in them, excellent editor integration, excellent refactoring capabilities, etc..
I don't know. I tried to install NixOS one time, but something failed to build during the initial installation steps when I followed the documentation. So I cannot really tell. I heard Nix manages multiple instances of packages differently. But I still would like to be able to uninstall old dangling packages without worries. I don't know if Nix can do it. 
Yes, I did take a look at how Vty worked, and couldn't work out how to essentially signal to Brick/Vty that the UI needs updating. It would be nice if essentially there was a `noop` continue statement. However, I wasn't sure how to achieve it, so just went back to basics. I've used Brick for another mini project and it was great! I think it's a really nice library to work with; just due to some design choices I made, it just wasn't quite the right fit for this one.
For the use case of initializing the dependency constraints and for updating them, I have written the tool [cabal-bounds](https://github.com/dan-t/cabal-bounds).
Build ghc with haddock highlighting enabled, and browse the html?
You can build tags https://wiki.haskell.org/Tags#GHC_development
Stack does add the `stack.yaml` file for describing extra stackage information. So there is a stack package, but it's very lightweight around a core cabal package. 
&gt; If all the common package installers act like that then most people will expect that behavior. But they don’t all (and of those I know, most install globally, for better or for worse). The `install` command in Bundler, Cargo, pip, and RubyGems all install globally by default. Of course, NPM/Yarn installs locally by default because of how Node was designed (although Ryan Dahl [somewhat regrets this decision](https://youtu.be/M3BM9TB-8yA?t=756)).
&gt; It's even worse. Since you can install multiple versions and they overwrite each other, but do not uninstall previous files, you're polluting installations on whole new ways and it's not trackable what to delete and what not. Additionally it can break updates, if packages load any resources dynamically from their installation directories. Keeping track of and rolling back large transitive dependencies is what I was addressing initially with nix-style GC. Also, if you are using the `v2-` commands, you shouldn't encounter breakage from things being overridden (except of course the finally symlink for binaries).
Is there any working IDE-like support for Reflex-based projects? e.g., take a skeleton project as described here: [https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md](https://github.com/reflex-frp/reflex-platform/blob/develop/docs/project-development.md). It depends on Nix and on cabal new-style build commands. According to my own attempts, HIE is not able yet to work in such a setup. But I might be overlooking something. &amp;#x200B; I am using VSCode. Any input on the question of developing Reflex applications with some level of IDE support is highly welcome. Currently, ghcid is the closest solution to the problem I was able to get working. &amp;#x200B;
That's a stack *project*.
&gt; Helping each other is a good thing, no? That's of course great, what I mean is there's a reason for every broken feature and experienced Haskellers start listing them every time someone complains. Cabal install doesn't have a way of uninstalling? Well that's because you shouldn't be using cabal install at all ... Cabal build doesn't work? Well that's because you should be using cabal new-build ... New-build paths are crazy? Well that's because you aren't supposed to be using it, it's in development ... In practice all this boil down to "cabal sucks".
&gt; But they are even more of a far cry from where they should be. 100% This! I totally believe we're currently prevented from having the tooling we deserve due to the [build-tooling fragmentation we're suffering](https://www.reddit.com/r/haskell/comments/9fefoe/if_you_had_the_ultimate_power_and_could_change/e5w6aeh/). I repeat [my request](https://www.reddit.com/r/haskell/comments/9fefoe/if_you_had_the_ultimate_power_and_could_change/e5w6aeh/) &gt; just pick a "winner" among Stack and Cabal! Flip a coin or make a poll... it doesn't really matter which one we pick. Officially declare the loser as discontinued in favor of the winner and shift all resources into making the winner the best tool we can come up with. Everybody wins! 
I really only want/use syntax highlighting and I grep around the code using git grep. Most of the solutions always choke on something somewhere so I don't bother (this is true for any other software I work on as well). I usually finish typing something before the IDE can pop up suggestions anyway.. 
Mainly two reasons: 1. They can be written in a very straightforward way, clearly conveying your intention instead of trying to conform to implementation details of the parser. Regular expressions for example are famously difficult to read, especially when they grow big. 2. Since a parser is just a function, they can be combined with higher order functions. It is very easy to combine many small parsers into a bigger one. This makes the whole much more comprehensible. Take for example this parser for the solution of [day 8 of the Advent of Code 2018 problems](https://adventofcode.com/2018/day/8): node :: Parser Node node = do numNodes &lt;- integer &lt;* space numMetadata &lt;- integer &lt;* space nodes &lt;- count numNodes node metadata &lt;- count numMetadata (integer &lt;* space) return $ Node (Header numNodes numMetadata) nodes metadata data Node = Node { header :: Header , subNodes :: [Node] , metadata :: [Int] } deriving (Show,Eq) data Header = Header { numNodes :: Int , numMetadata :: Int } deriving (Show,Eq) Notice how the data structures are straight from the description in the problem and the parser is very straightforward as well: - First parse an integer followed by a space, ignore the space and keep the integer in a variable called `numNodes`. (we could also have separated `integer &lt;* space` into a separate function but I was lazy at the time) - Then do the same for a variable called `numMetadata`. - The `count n p` combinator will apply the parser `p` n times. Since the subnodes have the same structure as any other node, we can recurse using the `node` parser without any problems. This is true even if the subnodes have subnodes of their own. This is much less straightforward with most other parsing methods I know of. - We do a similar thing for the metadata numbers, but we supply the `(integer &lt;* space)` parser again instead of the node parser. - Finally we construct a Node out of all the data we have gathered and return it. Being able to apply higher order functions to parsing is **really** powerful.
I spent the last 2-3 days reading through Monad Transformers from haskell first principles book, but I have not understood them properly yet and struggled through the exercises at the end of the chapter. What are some good resources to understand Monad Transformers properly?
With regards "Projects are an unnecessary barrier to entry" I believe I have [a solution](https://www.ahri.net/practical-haskell-programs-from-scratch/)
This is also what I expect `make install` to do when building something from source; to install my binaries into a globally-accessible directory.
I read your comment from last year, and I see you did try many install options. I can see your frustration, because I (and many others) have been through it as well. However venting on reddit just because you see others clashing against the same UX issues doesn't really help anybody. Open source functions on the goodwill of its developers and of its community. It's not fun to see the work you give away for free publicly thrashed because it doesn't correspond to someone's mental model. As /u/emilypii explained, you shouldn't need to uninstall dependencies. Yes, it leads to space usage, but it's a much safer option than having other projects who use those same dependencies break at random. Stack does make the process of building and caching Haskell straightforward; I use it exclusively but I hear many have good experiences with Nix for doing the same thing.
While I like `reflex`, I would quite certainly not recommend that as a starter for Haskell beginners. I program Haskell full time for many years, and have used reflex for a commercial application, and still haven't figured out some key things. Orthogonally, the combination of learning Haskell, nix, and reflex's FRP from scratch to high efficiency can easily take you a year of ramp-up time. There are also remaining technical shortcomings, such as no way to know when an element has actually been added to the DOM (what React calls "lifecycle"). See [issue](https://github.com/reflex-frp/reflex-dom/issues/110#issuecomment-445495094), which means you currently have to resort to racy `sleep`s in the code, which isn't straightforward. So, by all means try reflex, but do it if you're ready for a new challenge, not when getting started with Haskell.
&gt; Cabal install doesn't have a way of uninstalling? Well that's because you shouldn't be using cabal install at all ... Nobody said not to use it... it's just that most people don't understand why you'd want an "uninstall". I'm sure you could send a very simple and small PR that deletes the symlink from `~/.cabal/bin` if you feel that strongly about it. The upcoming garbage collector will ensure that all the dependencies are cleaned up. &gt; Cabal build doesn't work? Well that's because you should be using cabal new-build ... You don't need an experienced Haskeller to tell you this. The `cabal build` command will tell you this already. Cabal 3.0 is going to use the `v2-*` commands by default, so you won't even have to read what it says. We're in a transitionary phase: if people want things to improve they have to be prepared for changes. It is intellectually dishonest to simultaneously complain that everything is broken, and that everything is changing. &gt; New-build paths are crazy? Well that's because you aren't supposed to be using it, it's in development ... This is incorrect. Beginners shouldn't be poking into the internals of the cache, whether it's stable or in development. I've done a lot of Free Software volunteering and contributing in other languages, and it's frankly people like you who made me give up. There is always some random dude (like the original poster, or you) who complains loudly that its all terrible and broken, when in fact they just haven't bothered to read the manuals or seek assistance. Some people just get on with it... and send PRs when they fix problems, or write documentation where there is none. Be better, be like those people.
That's what I was doing before posting this :). I wasn't looking for auto-complete (which seems hard to fit in with a non-standard build system), only for goto def.
Oh neat, thanks!
I wouldn't be surprised if `{-# Language QuantifiedConstraints #-}` had a role to play forall f f'. Coercible f f' =&gt; Coercible (Listify(n) f a) (Listify(n) f' a) but they are frustrating with type families &amp; `Coercible`
For what it's worth, `new-build` is coming on 3 years now as of its release, and the newest version of cabal (coming up) will have all the `v2-*` and `new-*` commands as the standard (instead of having to prefix). It seems there's an accumulation of knowledge derived from outdated blog posts and other poor sources, as opposed actually reading the documentation for the tools. It's right there in the documentation. Heck, it tells you not to use the `v1` commands _from the tool itself_. It's really no fault on the part of Cabal developers, other than perhaps not providing adequately watered down and digestible sources for people who don't want to read the documentation. But then, PR's and 3rd-party tutorials are always welcome...
I wrote a blogpost recently about how it all fits together: https://medium.com/@fommil/why-not-both-8adadb71a5ed The answer to the question "is stack backwards compatible with Cabal" is technically "yes", but I suspect you're not asking the question you think you're asking :-) The post will make it clear.
I'd say it depends on what you're doing. You're never going to have sound theorem proving in Haskell anyway, so if that's what you want, use Idris or Agda. If what you want is Haskell, with coherent type classes and laziness, and also with dependent types for some things, use `singletons`.
garbage-collect is planned: https://github.com/haskell/cabal/issues/3333
Interesting! You could do even more laws: \`zip nil x = nil\` and some way to say that \`align (repeat a) x\` does not contain any \`That\`s.
Or zero!
stack.yaml is comparable to cabal.project
That documentation about stack defining "a concept called a project" was probably written before cabal.project existed.
&gt; Whenever a beginner writes these kinds of blog posts/comments etc. experienced Haskellers start explaining things. The thing is there's nothing the explain; these tools suck. You just get used to them after a while. Explaining is something that one can actually do to help someone with a few minutes of effort and little coordination. Fixing the issues so that these tools suck less is requires much more time, effort, and debate to accomplish. (Of course, one can do both.)
&gt; That documentation about stack defining "a concept called a project" was probably written before cabal.project existed. According to Git it was [committed on Jan 7, 2018](https://github.com/commercialhaskell/stack/commit/af5c97f2aa96862068a969f426a2523ba51137b5) whereas Googling tells us that `cabal.project` files existed at least since 2016: http://blog.ezyang.com/2016/05/announcing-cabal-new-build-nix-style-local-builds/
Weird. I thought I read that particular doc sometime in like 2015. Maybe it was copy pasted or something. Or, I could be wrong.
I found the "Monad transformers step by step" paper very helpful (a google search will give you the pdf, I don't have the link handy now)
&gt; The task requires I use fmap to implement the function, and frankly I'm stumped. Is it that requirement which causes you problems? Are you able to implement `convert` without using `fmap`?
lovely post. thanks for sharing.
You could try the wiki book (https://en.wikibooks.org/wiki/Haskell/Monad_transformers)
"Emptyable" isn't a real word, but gets the meaning across pretty unambiguously IMO.
I am going to assume a couple of things regarding your task but I think you are requested to use `fmap` because you need to map over `Maybe` Functor. Saying that one possible implementación of `convert` would be: ``` convert :: (a -&gt; b) -&gt; Thing a -&gt; Thing b convert f t = t { amount = f &lt;$&gt; amount t } ``` Here `&lt;$&gt;` is an alias of `fmap` and `t { ... }` is a short hand for copy a record and assign a new value to a specific field. There are shorter ways to do this but this is basic use of fmap in Maybe Functor. Hope this can help you. 
repost btw https://www.reddit.com/r/haskell/comments/8kg8yj/why_i_am_not_a_fan_of_cabal/
Nullable works fine. “Null set” is already used in math to describe empty sets: https://en.m.wikipedia.org/wiki/Empty_set 
Desktop link: https://en.wikipedia.org/wiki/Empty_set *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^228859
It was asinine of me to preemptively promise to not respond. And it was stupid to take the exchange ad hominem without being extremely careful. I was feeling superior, in retrospect. Sorry. I said "brutally honest" instead of "lazy and sloppy". I'll try again: Your exchanges with the other commenters seemed stalled due to rationally reasoning from incompatible assumptions. From those exchanges I inferred that you didn't seem to feel underprivileged as a member of this community and moreover that you had a "blindspot" that so many of us today have. So I shared the logic and link that had most starkly revealed to me that I have that same blindspot. But then I called out that blindspot in a way that amounts to publicly shaming you. I am ashamed. Sorry again. I hope the logic and link I shared help you consider that blindspot -- or if you actually don't have it, helped you see how at least some of your exchanges on this post might suggest that blindspot.
I don't think you are. one of stack main reason of existence was to handle projects correctly, IIRC
I don't mean to offend, I know it's all developed with goodwill, and I thought that twice before linking it (I'm not the author of the post), but I imagined that it to be expressed out outweighed the urge for not passing it on.
Thanks. I searched before linked but failed due to changed title.
I would second the idea that projects are an unnecessary barrier to entry. Especially when each language has its own way of specifying dependencies. Sometimes convoluted, non composable, or inspectable.. The lack of honest solution might be why we need a big hammer like nix. 
Folks, this sounds so messy....
Did you try looking yourself???? https://www.yesodweb.com/book-1.6
Do you mean my solution is dishonest in some way? I'm not quite sure what nix offers over it except in cases where non Haskell dependencies are required too.
Consider something like∀`f. fmap (mapThat f) = id` to ensure there are no `That`s
If you're doing raw Vty programming, updating the display is a manual step. If you're using Brick, calling `continue` causes a display update. If by "noop continue statement" you mean "continue but do not redraw", that could be added. If you are still interested, please feel free to open a ticket.
What about Interlaced as a name? And I guess you'd call it a "latticeal" or "interlaced" functor. It's a little bit odd because of the "ambiguity" (there's two functions from `These a a` to `a`).
Great post! Have you looked into `conduit` at all? It's a more principled approach to streaming than plugging things together with explicit loops. The `conduit-extra` package already has functions to stream from a network socket and to repeatedly apply an attoparsec parser.
On that note, many packages provide [iteratee](http://okmij.org/ftp/Haskell/Iteratee/describe.pdf), for example there's a pipes-attoparsec and streaming-attoparsec. Has anyone explored making the consumer agnostic to the type of iteratee with something like backpack?
Idiom brackets seem worth mentioning. [Functional pearl: Applicative programming with effects](http://www.staff.city.ac.uk/~ross/papers/Applicative.pdf), Conor McBride, Ross Paterson. &gt; pure f &lt;*&gt; u1 &lt;*&gt; ... &lt;*&gt; un &gt; &gt; This canonical form captures the essence of Applicative programming: computations &gt; have a fixed structure, given by the pure function, and a sequence of subcomputations, &gt; given by the effectful arguments. We therefore find it convenient, at &gt; least within this paper, to write this form using a special bracket notation, &gt; &gt; [[ f u1 ... un ]] There's also the [*each* library](https://hackage.haskell.org/package/each) which provides a similar Template Haskell notation.
https://imgur.com/a/jVqKW7h is how I see zip, align and some other monoidal stuff. And that I completely understand LatticeLike.
Yeah I’ve written several binary protocol servers using conduit-extra and its attoparsec support. Nowadays I can’t imagine whipping something up without it, it’s a huge level up. Aside from having all those things out of the box, your parser is testable because a conduit is agnostic of its input, so it can do an end to end connection handshake from a socket or a pure list of bytes in a test suite or in the REPL. I have an unpublished blog post coming up about using it to write a fake Microsoft SQL Server service. I have a WIP X11 server in Haskell here https://github.com/chrisdone/hex-server written as a learning exercise.
I want to avoid stack because I tried it in 2018 and it did not work for me (wild errors which turned out to be stack bugs). The generic linux distribution of GHC has been working for years for me.
Have you tried using `ghcup`? It's recommended in the passage you quoted, even!
I'm still unconvinced about visible type applications. The current situation with parameter order strikes me as very precarious and confusing. This isn't surprising, really, since Haskell was designed under the assumption that types are always inferred, so there is no defined order of type variables. The type application extension adds a new subset of type variables which suddenly have a defined order so they are subject to visible type applications... but they are fragile; seemingly innocent operations can accidentally unify them with non-explicit type variables, leading to surprising errors. &amp;#x200B; I understand there is some advanced type-trickery where type annotations are not sufficient to convey type information and uglier tricks like \`Proxy\` are needed instead. But until you get there, I wouldn't encourage anyone to use visible type applications when it's avoidable. Partial type signatures are a great solution for many simpler cases where you want to add enough type information to resolve a type class ambiguity, but still take advantage of inference.
What do I get with ghcup? I don't want "just ghc &amp; stack" but the "posix-full" variant of the haskell plattform (which was in the past provided by the installer).
Historically I've mostly installed the ghc binary distribution (rather than the whole Haskell Platform), which has always been simple and straightforward. It will update `ghc` etc symlinks to point to latest installed, but multiple ghcs play along nicely alongside one another, and you can select different compilers with cabal. https://www.haskell.org/ghc/download.html
But I can not find the PDF
The poster is not the author. You could have read that. As the poster you may read in some of my answers I'm not a Haskell user atm, I'm not trying Haskell for the nth time since the same pain points I've gone through are still there, I can see it from the experience reflected in the post. Even if you're pointing there's one fix for one issue someone mentioned, you can read there, here and in the links I've sent in responses, that there are many issues open.
We recommend ghcup even though yes that replicates the core rather than full installer. We’ve been encouraging people to use core rather than full for some years now! With pervasive sandboxing and the cabal v2 commands becoming standard, shipping with prebuilt libraries makes much less sense...
Great, I did not know that!
A null set isn’t necessarily empty, as it says in your link.
Either for constraints is `IncoherentInstances`. E.g. what to do for `Empty (Void, Void)`
What do you mean by `fst x` as applied to an element of `Fin n`?
If you're on Ubuntu, there's a great ppa to easily install GHC: https://launchpad.net/~hvr/+archive/ubuntu/ghc sudo add-apt-repository ppa:hvr/ghc
Are you familiar with the BST removal algorithm? Asking so I only mention it here if you are not familiar with the algo.
No, I'm not. The only algorithms I know are merge sort, insertion sort and quick sort, I think they only work with lists though 
(post 1/2) If you were to invert your typeclass hierarchy to `Top f =&gt; Zippable f` and `Bottom f =&gt; Align f`, here's what they'd be: First, notice that `These a Void ~ These Void a ~ a`, hence `Void` is the unit for `These`, so `These` gives a monoidal structure on the category of types. We have two symmetric monoidal categories with diagonals, with the cartesian product `(Type, (,), ())` and `(Type, These, Void)`, which is symmetric (`swapThese :: These a b -&gt; These b a`) and has diagonals (`dupThese :: x -&gt; These x x`), but fails to be cartesian since it doesn't have total projections (`fstThese :: These a b -&gt; Maybe a`; `sndThese :: These a b -&gt; Maybe b`), and because it isn't the cartesian product (duh). Both typeclasses say that `f` is a lax functor from `(Type, (,), ())` to either category, preserving the symmetric monoidal structure with diagonals. -- the same as the Pointed class everyone yet no-one wants. class Top f where repeat :: a -&gt; f a empty :: () -&gt; f () repeat x = fmap (const x) (empty ()) empty _ = repeat () class Bottom f where nil :: f a void :: () -&gt; f Void nil = absurd $ void () void _ = nil -- @Void -- f is a lax monoidal functor from (Type, (,), ()) to (Type, (,), ()) -- this is equivalent to Applicative because of the structure of Type, -- and exactly what people mean when they say -- "an Applicative functor is a lax monoidal functor". class Top f =&gt; Zippable f where zip :: (f a, f b) -&gt; f (a, b) -- zip . pair (fmap f) (fmap g) = fmap (pair f g) . zip -- Unit: -- unit . zip . pair id empty = unit -- or: -- zip . pair id (repeat a) = fmap (\x -&gt; (x,a)) -- Diagonals: -- zip . dup = fmap dup -- Associativity: -- zip . pair id zip . assoc = fmap assoc . zip . pair zip id -- Symmetry: -- zip . swap = fmap swap . zip -- f is a lax monoidal functor from (Type, (,), ()) to (Type, These, Void) -- while f is still an endofunctor from Type to Type, as all Functor instances are, -- it is not a *monoidal* endofunctor anymore, as it maps between two different -- monoidal structures on Type. -- This makes it easier to see why all the unit, dup, assoc and swap stuff is necessary. class Bottom f =&gt; Align f where align :: (f a, f b) -&gt; f (These a b) -- align . pair (fmap f) (fmap g) = fmap (these f g) . align -- Unit: -- unitThese . align . pair id void = unit -- or: -- align . pair id nil = fmap This -- Diagonals: -- align . dup = fmap dupThese -- Associativity: -- align . pair id align . assoc = fmap assocThese . align . pair align id -- Symmetry: -- align . swap = fmap swapThese . align -- bimap for (,) pair :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; (a,b) -&gt; (c,d) pair f g (x,y) = (f x, g y) dup :: a -&gt; (a,a) unit :: (a,()) -&gt; a swap :: (a,b) -&gt; (b,a) assoc :: ((a,b),c) -&gt; (a,(b,c)) -- bimap for These these :: (a -&gt; c) -&gt; (b -&gt; d) -&gt; These a b -&gt; These b d these f g (This a ) = This (f a) these f g (That b) = That (g b) these f g (These a b) = These (f a) (g b) dupThese :: a -&gt; These a a dupThese x = These x x unitThese :: These a Void -&gt; a unitThese (This a ) = a unitThese (That v) = absurd v unitThese (These a v) = absurd v -- while a would typecheck and still be correct in a total or strict language, -- bottoms in Haskell make it so that you could trick that implementation, -- so we just let it spin forever if you try. -- these functions are long and boring/unenlightening to write out. swapThese :: These a b -&gt; These b a assocThese :: These a (These b c) -&gt; These (These a b) c
 (post 2/2) `(Zippable f, Align f)`, then, says that `f` is able to preserve the structure in both ways. At this point, I'm pretty tired and the category theory becomes too hard for me to chase, so I can't really state the laws. From here, we can go and add additional structure to f. The easiest way to go is to note that `Void` is the zero of `(,)`, so `These` should be like addition/disjunction, and `(,)` should be like multiplication/conjunction. By leveraging the distributivity between `(,)` and `These`, we can go and define a `RingLike` class that preserves it. At this point, `f` is some kind of lax(?) (endo?)functor on (Type, (,), (), These, Void) as a "rig"/semiring/bimonoidal category. class (Zippable f, Align f) =&gt; RingLike f where -- /u/sjoerd_visscher's laws go here. -- something about distribPairThese and fmaps, I dunno -- what is this?!? distribPairThese :: (a, These b c) -&gt; These (a,b) (a,c) distribPairThese (a, t) = these ((,) a) ((,) a) t We can also go and implement the actual `LatticeLike` structure you want by stating absorption laws. class (Zippable f, Align f) =&gt; LatticeLike f -- your and /u/sjoerd_visscher's laws go here. I'm sure I'm wrong on details in multiple places, but the reasoning should be sound, all of this should answer your question number 1. TL;DR: some kinda functor on a bimonoidal category.
I think that's a theorem, not an axiom. fst &lt;$&gt; zip nil x [fst . second f = fst] = fst . second That &lt;$&gt; zip nil x [parametricity (zip)] = fst &lt;$&gt; zip nil (fmap That x) [nil is the unit of align] = fst &lt;$&gt; zip nil (align nil x) [absorption] = nil And I believe `fmap f x = nil ==&gt; x = nil` also holds, but have not reached to proof yet. Similarly, fstThese &lt;$&gt; align (repeat a) x = fstThese &lt;$&gt; align (repeat a) (fmap (snd . (a,)) x) = fstThese &lt;$&gt; align (repeat a) (fmap snd . fmap (a,) x) = fstThese &lt;$&gt; align (repeat a) (fmap snd (zip (repeat a) x)) = fstThese . second snd &lt;$&gt; align (repeat a) (zip (repeat a) x) = fstThese &lt;$&gt; align (repeat a) (zip (repeat a) x) = Just &lt;$&gt; repeat a = repeat (Just a)
Awesome! The idea is to remove the element if the structure of BST is not violated after the removal. You have already covered one of those cases (when there is only one element in the BST). Can you think of other cases? However, there are cases when removing an element could violate the structure of a BST: BST (BST Empty 5 Empty) 8 (BST Empty 10 Empty) In the tree above, removing 8 directly will result in something that doesn't look like a BST, so what do we do now? The idea is to replace it with something else, and remove the element that replaced the original item you were trying to remove. So, the above tree could look like the tree below: BST Empty 5 (BST Empty 10 Empty). Is it obvious what happened here? Without giving you the direct answer, I replaced it something that a) would maintain the structure of the tree and b) would be easy to remove. Are these hints enough to help you with removal? Please ask again if not!
Re: Edit: Yeah, `Kleisli Maybe =&gt; Hask` is a clear cut, but it does not allow current instances of `[]`, `Seq`, etc. The reason is, to be a lax monoidal functor `Kleisli Maybe =&gt; Hask`, following "naturality" law must hold. forall f :: a -&gt; Maybe a' g :: b -&gt; Maybe b' x :: f a y :: f b. align (mapMaybe f x) (mapMaybe g y) = mapMaybe (f *** g) align x y where (***) :: (a -&gt; Maybe a') -&gt; (b -&gt; Maybe b') -&gt; These a b -&gt; Maybe (These a' b') But `[]`, `Seq`, etc. don't satisfy this law. Like what u/phadej wrote in the comment, these list-like instances perfectly make sense, so it's not good to leave off them.
Thanks! So "lax monoidal functor which preserves diagonal" is each half of `LatticeLike`. I'll try another try of searching with that keyword.
The exact notion of symmetric monoidal category with diagonals seems to have some connections to relevance logic, so it's listed as [relevance monoidal category](https://ncatlab.org/nlab/show/relevance+monoidal+category) on nlab. I don't know how useful that might be for you. Having diagonals at all seems to be pretty important for your use case though, so maybe searching for [semicartesian monoidal categories](https://ncatlab.org/nlab/show/semicartesian+monoidal+category) might be more useful. What would be most useful though, is to just search for bimonoidal categories, you clearly want a pair of monoidal categories of some kind to have some shared structure. I know semiring-like categories exist as I mentioned above, and lattices aren't that different structure-wise from semirings, you just have to figure out how to state absorpion and you'd be there. Even better, you seem to have some intuition on how the laws should look, so you may just want to categorize those instead.
[rig category, bimonoidal category](https://ncatlab.org/nlab/show/rig+category)? Thanks, it's very helpful! (And I totally missed "symmetric" part!) My minor worry is in bimonoidal category ⊗ distributes over ⊕, so with absorption it would be always "distributive lattice". Maybe I shouldn't worry much since lattice structure of `[]` is (Nat, min, max), `Map k` is (FinSet, ∩, ∪), both of them are distributive.
I wish this were less true, but I think you're right. It's less true than people say because moving within an established codebase is actually relatively easy (i.e., when you're onboarding juniors, the types help a lot), but initial ramp-up is still a bit sticky. I don't see a good way to smooth the on-ramp without compromising on the rigour that makes Haskell such a great language for ongoing maintenance.
Personally I would avoid running the actual test suite as part of the execution of the program that's being tested. If I understand your situation correctly, you want your program to detect whether or not the website has the expected structure of HTML that you know your scraper will handle correctly. That seems like a useful feature for a scraper, so it seems to me like it's worth putting that validation logic into a module within the program itself (i.e. within `src`). Your test suite can then include tests for that validation module. You could have one set of fixtures taken from the latest version of the website, representing the expected structure that the validation functions should return no errors for. Then you can have other fixtures with different structures that should result in various errors from the validation functions. This is not far from what you proposed, but keeps all the logic that your program relies on in the code of the program itself, with no dependency on the test suite. I think that if you approach this as a conventional "validation problem" then the solution can be kept quite simple. 
I'm trying to run profiling on a binary that I'm compiling with stack. I'm providing the --profile option to stack which, as I understand it, enables library and executable profiling. I have these values in my cabal file: ghc-options: -threaded -g -fprof-auto -rtsopts cpp-options: +RTS -DS and I'm running my executable with stack exec -- my-executable +RTS -p and while a profiling file IS being produced, it never gets populated with anything. I'm sort of at a loss at how to proceed. Would love some help.
 Indent for formatting data BTree a = Empty | Node a (BTree a) (BTree a) remove :: Ord a =&gt; a -&gt; BTree a -&gt; BTree remove x (Node r Empty Empty) | x==r = Empty remove x (Node r e d) | x==r = ??? | x&lt;r = Node r (remove x e) d | x&gt;r = Node r e (remove x d)
[removed]
I read the relevant chapter in Real World Haskell 4-5 times and used ReaderT and StateT a bunch before it "clicked".
I'm surprised that that page doesn't mention https://www.haskell.org/ghc/ . I've been installing GHCs from there since years and it never failed me. Just get a tarball there, do `./configure &amp;&amp; make install` and you're done. Then get a cabal binary from https://www.haskell.org/cabal/download.html and you're good to go.
It's worth noting that `Zippable` is basically `Apply` but with more laws, and `Top` is `Applicative` but with more laws. In fact some existing `Apply`/`Applicative` instances having zipping semantics, like `IntMap`, `Map` and `ZipList`. Similarly `Monoid` and `Alternative` have some overlap with `Align` and `Bottom`, `empty`/`mempty` are usually going to coincide with `nil`, and `alignWith (these id id const)` coincides with a lot of the existing map-like `Monoid` instances, `alignWith (these id id (&lt;&gt;))` coincides with some alternative map-like `Monoid` instances that some people would prefer. Not that I am saying you shouldn't try and make and use these classes, but its worth thinking about exactly what is different about these classes and if there are any ways to reconcile them.
I think you're confused about `cpp-options`; that's for "C preprocessor" options, not for runtime options. Also, I doubt you want to pass `-DS` to to the runtime because (1) that only works with debug runtime (not the default, and if that's the whole of `ghc-options` you're using than you're not enabling it) (2) it enables garbage collector sanity checks which is for debugging and makes your program really slow for no reason (unless you're debugging GHC). At some point adding `-fprof-auto` to `ghc-options` worked, but I think these days you should add it to [`ghc-prof-options`](https://www.haskell.org/cabal/users-guide/developing-packages.html?highlight=ghc%20prof#pkg-field-ghc-prof-options). &gt; it never gets populated with anything Is the file completely empty or do you at least get the standard file header? I'd try finding the patch to your executable (maybe try `find . my-executable`) running it directly, removing stack from the equation. Then try `./my-executable +RTS --info`, you should see a line like `("RTS way", "rts_p")` (the "rts_p" says "profiling runtime"). If you don't see a "p" there then there's something wrong with building. Otherwise maybe your program is really not doing anything, or failing with a segfault or something.
For goto definition I use [this command](https://github.com/osa1/rcbackup/blob/master/.zshrc#L204) (see also other related commands like `alltags`, `allrtstags` etc.) + vim's built-in tags support.
I think I can see the intention: the existential is not a `Nat` anymore but a pair of `n` and `m` which sort of a makes sense as a the length of the cartesian product of lists If that's the case, then the `Fin n` isn't really a `Fin n` any more but a pair of `(Fin n, Fin m)` It's playing a little fast and loose with the notation + definition but it seems like a good intuition to me. 
It's not `fst: Fin n -&gt; ...`; it is `fst : Fin (n * m) -&gt; Fin n`
&gt; Also, I doubt you want to pass -DS to to the runtime because (1) that only works with debug runtime (not the default, and if that's the whole of ghc-options you're using than you're not enabling it) I had the debug runtime enabled, but when i switched over to ghc 8.6 I started getting a linking error from ld so I removed it. &gt; Is the file completely empty or do you at least get the standard file header? Completely empty &gt; I'd try finding the patch to your executable (maybe try find . my-executable) running it directly, removing stack from the equation. Have tried this, results were the same, though again, it does create the file. &gt; Then try ./my-executable +RTS --info, you should see a line like ("RTS way", "rts_p") (the "rts_p" says "profiling runtime") Ahh this is useful. I'll try running with these flags. &gt; Otherwise maybe your program is really not doing anything, or failing with a segfault or something. It is definitely running and doing a bunch of stuff, as I can see it doing so.
Arguably "Monoidal". You can think of the binary operation and corresponding laws as giving your notion of "empty" some ballast.
&gt; Stack is viewed as a Cabal replacement to fix all the things, but then you learn you have to deal with the two. Stack is not a cabal replacement, as far as I know. Which is a good thing because it does not break the compatibility. There are still people out there who uses cabal-install as their primary tool.
Thanks! Good perspective. I should be able to reuse code between validations like these and the test suite.
I experimented with that [here]( https://github.com/danidiaz/streamy). In theory, it would be enough for clients t depend on the abstract signature. Things like parsers could be implemented on top of the signature. In practice, there are many concrete implementations already and they work well, so there's not much point.
&gt; Completely empty This may mean your program is crashing instead of exiting gracefully. I'd also check the exit code (try `echo $?` when your program exits).
&gt; Stack is not a cabal replacement Well, that's not what Stack developer want you to think! For years they've been trying to sell you Stack as a Cabal replacement. See https://begriffs.com/posts/2015-06-22-haskell-stack-build-tool.html where &gt; Dan Burton gave this talk at Bayhac 2015. In it he introduces **Stack, a candidate replacement for Cabal**. The tool provides an easy one line command to install Haskell packages. It also installs any missing tools onto the system (GHC, Cabal, and libraries like alex, happy and cpphs). https://github.com/commercialhaskell/stack/wiki/New-to-Haskell says &gt; The cabal executable, more technically known as cabal-install. stack is a replacement for this executable, and it is not needed when using stack And you'll see statements in that vein from Stack developers or their fan base especially when explaining things to newcomers. Just wait for the next newcomer running into issues while using cabal and you'll inevitably see certain people trying to convert them to Stack before even assessing what the problem was they were struggling with. 
It was. the entire motivation for stack was to improve on workflow reliability/UI and provide this project feature. The authors analyzed it was an impossibility to get it done in cabal (to which they had contributed before). The real mess is to not have articulated well between the engineering/practical impetus of stack on one hand, and the forward-looking features of cabal on the other. There are far fewer people involved in this than folks having opinion on it.
This discussion already has links to [good explanation](https://medium.com/@fommil/why-not-both-8adadb71a5ed) (and there more links which I have not checked) about how cabal and stack relate to each other. I don't think I can add anything to those. I don't think the way you are describing it is relevant. I myself mostly use stack as the build frontend, especially for Windows.
Not at all, it's a great solution. What I was pointing at is that each language has its own notion of "required package environment". Then for each case where it needs to be solved is usually tied to that use case ("to compile, read the .cabal file and do etc..."). So those dependencies are explicit for that use case, but quite implicit for some external, language agnostic, tool. If you want to use that code in another context (say in a script like you do), we have to adapt the tools (say dante) and teach it to use that context (from the script header). likewise for literate programming, which has to maintain some form of environment if we want to run code alongside. Nix gives a universal, because it is very stupid, way to track all dependencies, as it goes down bit level. but it also is tied to a particular usage of running things, when the dependencies collected could be used to populate an IDE completion (?) Ideally, we might want some form of low key convention (like what stack does for its package in your solution) universal across languages to express the dependencies, pin them, etc.. but at a higher level than just bits. (I think bazel is in that vein for the purpose of building stuff)
Interesting. Your solution should work with [Cabal's shebang script feature](https://cabal.readthedocs.io/en/latest/nix-local-build.html#cabal-new-run) as well, no?
The "original poster" is the blog author, obviously. I don't understand why you've jumped on a thread where one person deleted all their comments, adding your interpretation of what you think they were saying. I'm fully aware of what they said in this thread: and it wasn't what you're postulating: they didn't read the documentation and then complained about not knowing how to do anything. I hope they've read the (very good) manuals.
I've read it before deletion. You have one interpretation of that quote, I have another and think you lost the author. That's it.
Did you use "original poster" in the text you just edited? If not, that's being bad intentioned. 
Not a big fan of nullable, to me that feels like the null is something outside the possible values of the data structures. E.g. a string is potentially empty, I would expect a "nullable string" to be able to distinguish "" from null. Of the suggestions so far I think I prefer "emptyable". I'll offer "maybe-empty", but it's not short or snappy.
I updated the text to reflect what you asked me to do. I have learnt my lesson: I shall not make the mistake again of trying to keep you happy!
OK, it didn't look so, it looked like you were simply quoting (since it's quoting, must be original text, but isn't) to point how it was originally obvious and my clarification on that unnecessary.
I do think it's relevant, in fact pretty dawn relevant. Why? Because you may point a link explaining the actual relation between cabal and stack, but which doesn't necessarily acts to clarify a confusion created by the tool authors or the community, which is exactly what's done above.
Hey, Haskell/Type Theory/Automated Theorem Proving newbie here. Could you please elaborate as to why Haskell will never get sound theorem proving? Do you mean it in terms of the (re-)engineering involved or is there something inherent to System FC that might not allow it? 